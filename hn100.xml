<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 12 Nov 2025 16:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Pakistani newspaper mistakenly prints AI prompt with the article (316 pts)]]></title>
            <link>https://twitter.com/omar_quraishi/status/1988518627859951986</link>
            <guid>45898789</guid>
            <pubDate>Wed, 12 Nov 2025 11:17:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/omar_quraishi/status/1988518627859951986">https://twitter.com/omar_quraishi/status/1988518627859951986</a>, See on <a href="https://news.ycombinator.com/item?id=45898789">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don‚Äôt fret ‚Äî let‚Äôs give it another shot.</span></p><p><img alt="‚ö†Ô∏è" draggable="false" src="https://abs-0.twimg.com/emoji/v2/svg/26a0.svg"><span> Some privacy related extensions may cause issues on x.com. Please disable them and try again.</span></p></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yt-dlp: External JavaScript runtime now required for full YouTube support (468 pts)]]></title>
            <link>https://github.com/yt-dlp/yt-dlp/issues/15012</link>
            <guid>45898407</guid>
            <pubDate>Wed, 12 Nov 2025 10:12:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yt-dlp/yt-dlp/issues/15012">https://github.com/yt-dlp/yt-dlp/issues/15012</a>, See on <a href="https://news.ycombinator.com/item?id=45898407">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">This is a follow-up to <a data-error-text="Failed to load title" data-id="3444474002" data-permission-text="Title is private" data-url="https://github.com/yt-dlp/yt-dlp/issues/14404" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/14404/hovercard" href="https://github.com/yt-dlp/yt-dlp/issues/14404">#14404</a>, which announced that yt-dlp will <em>soon</em> require an external JavaScript runtime (e.g. Deno) in order to fully support downloading from YouTube.</p>
<h3 dir="auto">With the release of <a href="https://github.com/yt-dlp/yt-dlp/releases/tag/2025.11.12">yt-dlp version <code>2025.11.12</code></a>, external JavaScript runtime support has arrived.</h3>
<h3 dir="auto">All users who intend to use yt-dlp with YouTube are strongly encouraged to install one of the supported JS runtimes.</h3>
<p dir="auto">The following JavaScript runtimes are currently supported (<em>in order of recommendation, from strongest to weakest</em>):</p>
<ol dir="auto">
<li>
<h3 dir="auto">Deno</h3>
<ul dir="auto">
<li><strong>recommended for most users</strong></li>
<li><a href="https://deno.com/" rel="nofollow">https://deno.com/</a></li>
<li><a href="https://github.com/denoland/deno">https://github.com/denoland/deno</a>
<ul dir="auto">
<li>note: <em>if downloading from <a href="https://github.com/denoland/deno/releases/latest">Deno's GitHub releases</a>, get <code>deno</code> <strong>not</strong> <code>denort</code></em></li>
</ul>
</li>
<li>minimum Deno version supported by yt-dlp: <code>2.0.0</code>
<ul dir="auto">
<li><em>the latest version of Deno is strongly recommended</em></li>
</ul>
</li>
</ul>
</li>
<li>
<h3 dir="auto">Node</h3>
<ul dir="auto">
<li><a href="https://nodejs.org/" rel="nofollow">https://nodejs.org/</a></li>
<li>minimum Node version supported by yt-dlp: <code>20.0.0</code>
<ul dir="auto">
<li><em>if using Node, the latest version (25+) is strongly recommended for security reasons</em></li>
</ul>
</li>
</ul>
</li>
<li>
<h3 dir="auto">QuickJS</h3>
<ul dir="auto">
<li><a href="https://bellard.org/quickjs/" rel="nofollow">https://bellard.org/quickjs/</a></li>
<li>minimum QuickJS version supported by yt-dlp: <code>2023-12-9</code>
<ul dir="auto">
<li><em>if using QuickJS, version <code>2025-4-26</code> or later is strongly recommended for performance reasons</em></li>
</ul>
</li>
</ul>
</li>
<li>
<h3 dir="auto">QuickJS-ng</h3>
<ul dir="auto">
<li><a href="https://quickjs-ng.github.io/quickjs/" rel="nofollow">https://quickjs-ng.github.io/quickjs/</a></li>
<li>all versions are supported by yt-dlp; however, <strong><a href="https://bellard.org/quickjs/" rel="nofollow">upstream QuickJS</a> is recommended instead</strong> for performance reasons</li>
</ul>
</li>
<li>
<h3 dir="auto">Bun</h3>
<ul dir="auto">
<li><a href="https://bun.com/" rel="nofollow">https://bun.com/</a></li>
<li>minimum Bun version supported by yt-dlp: <code>1.0.31</code>
<ul dir="auto">
<li><em>if using Bun, the latest version is strongly recommended</em></li>
</ul>
</li>
</ul>
</li>
</ol>
<p dir="auto">Note that only <code>deno</code> is enabled by default; all others are disabled by default for security reasons. See <a href="https://github.com/yt-dlp/yt-dlp/wiki/EJS">the EJS wiki page</a> for more details.</p>
<hr>
<p dir="auto"><em>In addition to</em> the JavaScript runtime, yt-dlp also requires the <a href="https://github.com/yt-dlp/ejs">yt-dlp-ejs</a> component in order to operate the JS runtime.</p>
<p dir="auto"><strong>NOTE:</strong> This component <strong>is already included in all of the <a href="https://github.com/yt-dlp/yt-dlp/releases/latest">official yt-dlp executables</a></strong>.<br>
Similarly, if you've installed &amp; upgraded the yt-dlp Python package with the <code>default</code> extra (<code>yt-dlp[default]</code>), <strong>then you already have the yt-dlp-ejs component</strong>.</p>
<p dir="auto">If you've installed yt-dlp another way, then please refer to <a href="https://github.com/yt-dlp/yt-dlp/wiki/EJS#step-2-install-ejs-challenge-solver-scripts">section 2 of the EJS wiki page</a> for more details.</p>
<hr>
<p dir="auto">Support for YouTube without a JavaScript runtime is now considered "deprecated." It does still work somewhat; however, format availability will be limited, and severely so in some cases (e.g. for logged-in users). Format availability without a JS runtime is expected to worsen as time goes on, and this will not be considered a "bug" but rather an inevitability for which there is no solution. It's also expected that, eventually, support for YouTube will not be possible at all without a JS runtime.</p>
<hr>
<p dir="auto">If you have questions, please refer to the EJS wiki page, the previous announcement's FAQ, and the README before commenting or opening a new issue:</p>
<ul dir="auto">
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/EJS">https://github.com/yt-dlp/yt-dlp/wiki/EJS</a></li>
<li><span><a data-error-text="Failed to load title" data-id="3444474002" data-permission-text="Title is private" data-url="https://github.com/yt-dlp/yt-dlp/issues/14404" data-hovercard-type="issue" data-hovercard-url="/yt-dlp/yt-dlp/issues/14404/hovercard" href="https://github.com/yt-dlp/yt-dlp/issues/14404">[Announcement] Upcoming new requirements for YouTube downloads<span>&nbsp;#14404</span></a></span></li>
<li><a href="https://github.com/yt-dlp/yt-dlp#dependencies">https://github.com/yt-dlp/yt-dlp#dependencies</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp#general-options">https://github.com/yt-dlp/yt-dlp#general-options</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp#youtube-ejs">https://github.com/yt-dlp/yt-dlp#youtube-ejs</a></li>
<li><a href="https://github.com/yt-dlp/yt-dlp/wiki/EJS#plugins">https://github.com/yt-dlp/yt-dlp/wiki/EJS#plugins</a></li>
</ul>
<hr>
<h2 dir="auto">Notes to package maintainers</h2>
<p dir="auto">If you are maintaining a downstream package of yt-dlp, we offer the following guidance:</p>
<ul dir="auto">
<li>
<p dir="auto">The <code>yt-dlp</code> repository, source tarball, PyPI source distribution and built distribution (wheel) are still licensed under The Unlicense (public domain); however, <strong>when the <code>yt-dlp-ejs</code> package is built, it bundles code licensed under ISC and MIT.</strong> This is the primary reason why <code>yt-dlp-ejs</code> was split off into a <a href="https://github.com/yt-dlp/ejs">separate repository</a> and <a href="https://pypi.org/project/yt-dlp-ejs" rel="nofollow">PyPI package</a></p>
</li>
<li>
<p dir="auto">If <code>yt-dlp</code> is packaged as a Python package in your repository, <code>yt-dlp-ejs</code> would ideally be packaged separately</p>
</li>
<li>
<p dir="auto"><code>yt-dlp-ejs</code> is technically an <em>optional</em> Python dependency of yt-dlp, but YouTube support is deprecated without it</p>
</li>
<li>
<p dir="auto"><strong>Each version of <code>yt-dlp</code> will be pinned to a specific version of <code>yt-dlp-ejs</code> and yt-dlp will reject any other <code>yt-dlp-ejs</code> version.</strong> Refer to yt-dlp's <a href="https://github.com/yt-dlp/yt-dlp/blob/2025.11.12/pyproject.toml#L59"><code>pyproject.toml</code></a> for the pinned version</p>
</li>
<li>
<p dir="auto">If your repository packages <code>yt-dlp</code> <em>as the zipimport binary instead of as a Python package</em>, you can use <code>make yt-dlp-extra</code> to build the zip executable with <code>yt-dlp-ejs</code> included. (The Makefile will look for the <code>yt-dlp-ejs</code> wheel in the <code>build</code> subdirectory, or the extracted built distribution in the <code>yt_dlp_ejs</code> subdirectory)</p>
</li>
<li>
<p dir="auto"><code>deno</code>, <code>nodejs</code>, <code>quickjs</code> and/or <code>bun</code> should be <em>optional</em> dependencies of <code>yt-dlp</code>. But again, YouTube support is deprecated without <em>one</em> of them</p>
</li>
<li>
<p dir="auto">While <code>yt-dlp-ejs</code> and the external JavaScript runtimes are currently only used with YouTube, yt-dlp's usage of these may be expanded in the future (and necessarily so)</p>
</li>
</ul>
<p dir="auto">If this guidance is insufficient, or if you are a developer integrating yt-dlp into your software and you have further questions, please open a <a href="https://github.com/yt-dlp/yt-dlp/issues/new?template=6_question.yml">new GitHub issue</a>.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happened to Transmeta, the last big dotcom IPO (122 pts)]]></title>
            <link>https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/</link>
            <guid>45897935</guid>
            <pubDate>Wed, 12 Nov 2025 09:01:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/">https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/</a>, See on <a href="https://news.ycombinator.com/item?id=45897935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Transmeta was the last big IPO of the <a href="https://dfarq.homeip.net/when-the-dotcom-bubble-burst/">dotcom era</a>, launching Nov 7, 2000. Some analysts call its $273 million IPO the last successful tech IPO until the <a href="https://dfarq.homeip.net/google-incorporated-september-4-1998/">Google IPO in 2004</a>. Transmeta didn‚Äôt completely fit in to the dotcom era, because they were a hardware company. But they were still a technology company, and if their plans had gone well, they would have sold their product to dotcoms, but it didn‚Äôt work out that way for them. In this blog post we explore what happened to Transmeta.</p><h2>Was Transmeta truly the last of the dotcoms?</h2><figure id="attachment_36221" aria-describedby="caption-attachment-36221"><a href="https://dfarq.homeip.net/what-happened-to-transmeta-the-last-big-dotcom-ipo/transmeta_crusoe_tm5600_2/" rel="attachment wp-att-36221"><img data-recalc-dims="1" fetchpriority="high" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/10/Transmeta_crusoe_tm5600_2.jpg?resize=300%2C250&amp;ssl=1" alt="Transmeta Crusoe CPU" width="300" height="250" srcset="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/10/Transmeta_crusoe_tm5600_2.jpg?resize=300%2C250&amp;ssl=1 300w, https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2024/10/Transmeta_crusoe_tm5600_2.jpg?w=576&amp;ssl=1 576w" sizes="(max-width: 300px) 100vw, 300px"></a><figcaption id="caption-attachment-36221">Transmeta was a CPU company, a pure technology play as investors cooled on Internet companies. Its IPO feels like the end of the dotcom era, even though the line feels a bit fuzzy..</figcaption></figure><p>I‚Äôve heard Transmeta called the last successful IPO of the dotcom era, or the last of the big dotcom IPOs. But it‚Äôs an oversimplification to say there weren‚Äôt any successful technology IPOs between Transmeta and Google. One very notable exception is Paypal, who ran an IPO in February 2002 that raised $70.2 million.&nbsp;<em>The Register </em>even hailed Paypal as the return of the Internet IPO.</p><p>And it wasn‚Äôt just that dotcom IPOs grew scarce after November 2000. On February 7, 2002, <em>Forbes</em> stated that <a href="https://www.forbes.com/2002/02/07/0207paypal.html">only 34 IPOs in total</a> launched between September 11, 2001 and February 6, 2002, compared to 87 in the same-year-earlier period, and 240 between September 11, 1999 and February 6, 2000. That date is significant. <em>Any&nbsp;</em>bad news can spook investors, and the 9/11 attack did, in fact, spook investors.</p><p>So where the dotcom-era ended is a fuzzy line, but it makes sense to draw the line at Transmeta. Technology IPOs and particularly Internet IPOs became much more scarce after Transmeta, and the size of the IPOs shrunk too. The distinction of Transmeta being a technology stock, rather than an Internet stock, also suggests investors were already cooling on Internet stocks by November 2000.</p><p>Transmeta was a CPU company. But they may be better known for their most famous employee than for any of their products. When Linus Torvalds completed his degree, there was a great deal of speculation where he would take his day job. Transmeta was not the place most technologists expected him to land. But it allowed him to continue his work on the Linux kernel while staying close to a key part of the hardware, the CPU.</p><p>In the year 2000, the CPU wars were cooling down. A few years earlier, there had been four companies not named Intel producing Socket 7 CPUs. But only two of them survived to compete in the next generation, and only AMD was able to compete at <a href="https://dfarq.homeip.net/amd-athlon-amds-game-changing-cpu-from-1999/">anything other than the entry level</a>. And even though the <a href="https://dfarq.homeip.net/cyrix-processor-chips/">Cyrix</a> name survived, it was the competing IDT technology under the hood.</p><h2>How its CPUs worked</h2><p>Transmeta wanted to take a different approach. They were going to produce an x86 compatible CPU, but they were going to use a translation layer to do it. They would design a very efficient CPU, and in theory, they could place any translation layer in front of it that they wanted. Emulating PowerPC or ARM would have been possible if they saw the need to do it. But the <a href="https://dfarq.homeip.net/why-is-x86-so-popular/">popular yet inefficient x86 architecture</a> was a more inviting target.</p><p>AMD was doing something similar, essentially translating x86 instructions into RISC instructions for efficiency, but they did it in hardware rather than software like Transmeta did. AMD never had any designs on putting any different translation layer in front of it.</p><p>Transmeta did ship two CPUs, but weren‚Äôt able to reach the same levels of performance AMD and Intel were reaching. Its first CPU, <a href="https://dfarq.homeip.net/transmeta-crusoe-cpu/">Crusoe</a>, could run at <a href="https://dfarq.homeip.net/pentium-iii-launched-feb-28-1999/">Pentium III</a>-like speeds but was about 30% less efficient, so a 700 MHz Crusoe ran like a 500 MHz Pentium III. Transmeta didn‚Äôt have fabrication plants of its own, so IBM handled manufacturing of its first-generation CPUs.</p><p>The Transmeta Efficieon, released in 2004, competed with the Pentium 4 but peaked at 1.7 GHz. With the Pentium 4 reaching 2.4 GHz speeds, the Efficieon had trouble competing. And AMD‚Äôs release of the <a href="https://dfarq.homeip.net/athlon-64-how-amd-turned-the-tables-on-intel/">Athlon 64</a> didn‚Äôt help matters. Selling 32-bit CPUs in a 64-bit world was going to be tough. TSMC and Fujitsu handled manufacturing for the Efficieon.</p><p>Transmeta CPUs saw use in low-power laptops, thin clients, and embedded applications. The Bluecoat web filtering appliance used them for a while. But if you owned a computer at that time, it‚Äôs much more likely to have had an AMD or Intel CPU in it. If you used one at work, it was even more likely to have an AMD or Intel CPU in it.</p><h2>What happened to Transmeta</h2><p>Even though Transmeta was the last big dotcom era IPO, they were not among the survivors. Torvalds resigned from Transmeta in June 2003.</p><p>What happened to Transmeta was that in 2005, Transmeta shifted to licensing intellectual property rather than selling CPUs. And in January 2009, Transmeta sold itself to Novafora, who in turn sold the patent portfolio to Intellectual Ventures, a private equity company. Novafora ceased operations in August 2009, just seven months later. Intellectual Ventures licenses the Transmeta intellectual property to other companies on a non-exclusive basis. Transmeta ended up being more like <a href="https://dfarq.homeip.net/netscape-the-ipo-that-went-boom-on-its-way-up-and-down/">Netscape</a> or <a href="https://dfarq.homeip.net/va-linux-the-biggest-dotcom-ipo/">VA Linux</a> than <a href="https://dfarq.homeip.net/red-hats-successful-1999-ipo/">Red Hat</a>.</p><p>Today, Transmeta hardware is rare enough to be interesting as a collectible. But there aren‚Äôt a lot of people nostalgic for it, and that probably keeps prices low.</p><div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img data-recalc-dims="1" decoding="async" src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" data-src="https://i0.wp.com/dfarq.homeip.net/wp-content/uploads/2017/06/dave_farquhar_181px.jpg?resize=100%2C100&amp;ssl=1" width="100" height="100" alt="" itemprop="image"></p><div><p>David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Please donate to keep Network Time Protocol up ‚Äì Goal 1k (267 pts)]]></title>
            <link>https://www.ntp.org/</link>
            <guid>45897457</guid>
            <pubDate>Wed, 12 Nov 2025 07:56:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ntp.org/">https://www.ntp.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45897457">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	  <p>The NTP Project conducts Research and Development in NTP, a protocol designed to synchronize the clocks of computers over a network to a common timebase. NTP is what ensures the reliability of billions of devices around the world, under the sea, and even in space. Accurate timekeeping is vital to the many applications which have revolutionized and are essential to our daily lives: satellites, GPS, 5G, financial services, healthcare, and more.</p>
<p>The NTP Project produces an <a href="https://bk.ntp.org/">open source Reference Implementation</a> of the NTP standard, maintains the implementation <a href="https://www.ntp.org/documentation/4.2.8-series/">Documentation</a>, and develops the protocol and algorithmic standard that is used to communicate time between systems. Background information about NTP can be found in the <a href="https://www.ntp.org/reflib/">Reference Library</a>.</p>
<p>Network Time Foundation provides support for the NTP Project. Learn more about the <a href="https://www.nwtime.org/">Foundation‚Äôs work</a>.</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Yann LeCun to depart Meta and launch AI startup focused on 'world models' (624 pts)]]></title>
            <link>https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models</link>
            <guid>45897271</guid>
            <pubDate>Wed, 12 Nov 2025 07:25:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models">https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models</a>, See on <a href="https://news.ycombinator.com/item?id=45897271">Hacker News</a></p>
Couldn't get https://www.nasdaq.com/articles/metas-chief-ai-scientist-yann-lecun-depart-and-launch-ai-start-focused-world-models: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[You will own nothing and be (un)happy (221 pts)]]></title>
            <link>https://racc.blog/you-will-own-nothing-and-be-unhappy/</link>
            <guid>45897016</guid>
            <pubDate>Wed, 12 Nov 2025 06:37:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://racc.blog/you-will-own-nothing-and-be-unhappy/">https://racc.blog/you-will-own-nothing-and-be-unhappy/</a>, See on <a href="https://news.ycombinator.com/item?id=45897016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

    

    
        

        <p>
            <i>
                <time datetime="2025-11-11T11:50Z">
    11 Nov, 2025
</time>
            </i>
        </p>
    

    <p>Yesterday, I checked out the Goodnotes app because I‚Äôm planning to buy an iPad to give note-taking on it a try again. I opened it and I saw that it has updated to Goodnotes 6. I looked through the changelogs, and what? There are AI-integrated features now, which, as far as I‚Äôm aware, can‚Äôt be turned off.<sup id="fnref-1"><a href="#fn-1">1</a></sup> And the option to buy a lifetime license? Gone.</p>
<p>I bought the previous ‚Äúlifetime‚Äù version of the app, but for WHAT, since I have to pay for the subscription to access the newest features. Sure, I can switch back to the outdated version, but if I had known this would happen, I would‚Äôve just gone for the yearly subscription (even though I try to avoid them). It would‚Äôve been cheaper anyway.</p>
<p>Sadly, everything is going in this direction. You can have a subscription for almost everything now. From food delivery apps to beer subscription.</p>
<p>Companies prey on those who forget to cancel their free trial. So far, it only happened once to me, but thankfully, I managed to get my refund. But not everyone might feel like going through the refund process, because companies make it intentionally hard by creating friction - hiding it deep into their websites or adding extra unnecessary steps like forcing you to reach out to the support. People might just sigh, mutter ‚Äúthis bloody subscription rahhh,‚Äù and cancel it so it won‚Äôt charge them again.</p>
<p>It‚Äôs funny how ‚Äúownership‚Äù in the digital world has become an illusion. You don‚Äôt really own your apps, your music, or even your tools anymore. You‚Äôre just renting access until someone decides to move the paywall. It‚Äôs convenient, sure, but it also feels like losing control over something that used to be ours.</p>
<p>Market values steady revenue streams over fair ownership. The subscription model rewards companies not for making better products, but for finding better ways to keep us paying indefinitely. They won‚Äôt disappear, not unless they find something more profitable. Maybe they‚Äôll start sucking out our souls for profit?</p>
<hr>
<p>Enjoyed the article? Subscribe to my <a href="https://raccoon.bearblog.dev/feed/?type=rss"><strong>RSS feed</strong></a> :D</p>
<section>
<ol>
<li id="fn-1"><p><em>(on the side note: who tf NEEDS chatgpt built into their note taking app)</em>.<a href="#fnref-1">‚Ü©</a></p></li>
</ol>
</section>


    

    
        
            <p>
                
                    <a href="https://racc.blog/blog/?q=2025">#2025</a>
                
            </p>
        

        
            


        

        
            

            
        
    


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Perkeep ‚Äì Personal storage system for life (272 pts)]]></title>
            <link>https://perkeep.org/</link>
            <guid>45896130</guid>
            <pubDate>Wed, 12 Nov 2025 03:34:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perkeep.org/">https://perkeep.org/</a>, See on <a href="https://news.ycombinator.com/item?id=45896130">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		

<p><a href="https://perkeep.org/keepy"><img src="https://perkeep.org/static/keepy-small-left.png" width="290" height="280"></a></p>

<p>Perkeep (<a href="https://github.com/perkeep/perkeep/issues/981">n√©e Camlistore</a>) is a set of open source formats, protocols, and software for modeling, storing, searching, sharing and synchronizing data in the post-PC era. Data may be files or objects, tweets or 5TB videos, and you can access it via a phone, browser or FUSE filesystem.</p>

<p>Perkeep is under active development. If you're a programmer or fairly technical, you can probably get it up and running and get some utility out of it. Many bits and pieces are actively being developed, so be prepared for bugs and unfinished features.</p>

<p>Join <a href="https://perkeep.org/community">the community</a>, consider <a href="https://perkeep.org/doc/contributing">contributing</a>, or <a href="https://github.com/perkeep/perkeep/issues">file a bug</a>.</p>

<p>Things Perkeep believes:</p>
<ul>
  <li>Your data is entirely under your control</li>
  <li>Open Source</li>
  <li>Paranoid about privacy, everything private by default</li>
  <li>No SPOF: don't rely on any single party (including yourself)</li>
  <li>Your data should be alive in 80 years, especially if you are</li>
</ul>

<h2 id="latest-release">Latest Release</h2>

<p>The latest release is <a href="https://github.com/perkeep/perkeep/releases/tag/v0.12">0.12 ("Toronto")</a>, released 2025-11-11.</p>
<p>Follow the <a href="https://perkeep.org/download">download and getting started instructions</a> to set up Perkeep.</p>

<h2 id="video-demo">Video Demo</h2>

<p>LinuxFest Northwest 2018 [<a href="https://docs.google.com/presentation/d/1suYfv3dmjJQ1mMJIG7_D26e5cudZqPcZTPNgrLvTIrI/view">slides</a>] [<a href="https://www.youtube.com/watch?v=PlAU_da_U4s">video</a>]:</p>

<center><iframe width="640" height="360" src="//www.youtube.com/embed/PlAU_da_U4s" frameborder="0" allowfullscreen=""></iframe></center>

<p>Or see the <a href="https://perkeep.org/doc/#presentations">other presentations</a>.</p>

	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Nietzsche matters in the age of artificial intelligence (147 pts)]]></title>
            <link>https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/</link>
            <guid>45894588</guid>
            <pubDate>Tue, 11 Nov 2025 23:59:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/">https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/</a>, See on <a href="https://news.ycombinator.com/item?id=45894588">Hacker News</a></p>
Couldn't get https://cacm.acm.org/blogcacm/why-nietzsche-matters-in-the-age-of-artificial-intelligence/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I can build enterprise software but I can't charge for it (162 pts)]]></title>
            <link>https://gist.github.com/EchenD/8b211ebfa4941d2c5df7b526790b31aa</link>
            <guid>45894569</guid>
            <pubDate>Tue, 11 Nov 2025 23:58:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/EchenD/8b211ebfa4941d2c5df7b526790b31aa">https://gist.github.com/EchenD/8b211ebfa4941d2c5df7b526790b31aa</a>, See on <a href="https://news.ycombinator.com/item?id=45894569">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-neoclerks-partnership-md" tabindex="0" role="region" aria-label="NeoClerks-partnership.md content, created by EchenD on 11:20PM yesterday.">
    <article itemprop="text"><p>Error in user YAML: (&lt;unknown&gt;): did not find expected alphabetic or numeric character while scanning an alias at line 1 column 1</p><div dir="auto"><pre>---

<span>**Important clarification based on HackerNews feedback:**</span>

<span>The partnership structures I originally proposed (US/UK company + contractor arrangement) **violate international sanctions and are illegal**. I was naive about this.</span>

<span>**I am NOT asking anyone to break sanctions laws.**</span>

<span>**New focus - Legal markets:**</span>
- <span>üáÆüá≥ India - No sanctions, large SaaS ecosystem</span>
- <span>üá¶üá™ UAE/Dubai - Open to Iranian business</span>
- <span>üáπüá∑ Turkey - Regional tech hub  </span>
- <span>üá∏üá¨ Singapore - Researching legal framework</span>

<span>**If you're from these countries and interested:** EchenDeligani@gmail.com</span>

---

</pre></div>

<p dir="auto"><h2 dir="auto">I Can Build Enterprise Software. But I Can't Charge for It.</h2><a id="user-content-i-can-build-enterprise-software-but-i-cant-charge-for-it" aria-label="Permalink: I Can Build Enterprise Software. But I Can't Charge for It." href="#i-can-build-enterprise-software-but-i-cant-charge-for-it"></a></p>
<p dir="auto"><em>How I spent 18 months building an AI avatar platform through war, economic collapse, and 120-hour weeks‚Äîand why I'm now looking for a partner to help me turn this into a real business.</em></p>
<hr>
<p dir="auto"><h2 dir="auto">Test it first: <a href="https://neoclerks.com/en/" rel="nofollow">neoclerks.com/en</a></h2><a id="user-content-test-it-first-neoclerkscomen" aria-label="Permalink: Test it first: neoclerks.com/en" href="#test-it-first-neoclerkscomen"></a></p>
<p dir="auto">Go ahead. Talk to the avatar for 2 minutes. Ask it anything in English. Watch how it responds in real-time with synchronized lip movements and natural conversation.</p>
<p dir="auto">I'll be here when you get back.</p>
<hr>
<p dir="auto"><h2 dir="auto">The Current Situation (Being Completely Honest)</h2><a id="user-content-the-current-situation-being-completely-honest" aria-label="Permalink: The Current Situation (Being Completely Honest)" href="#the-current-situation-being-completely-honest"></a></p>
<p dir="auto">I'm 35 years old. I live in Iran. I spent 18 months building an enterprise AI avatar platform that competes with Soul Machines ($70M funded, $50k+ setup fees for enterprise clients).</p>
<p dir="auto"><strong>The product works</strong>. You just tested it. The code is production-ready. The architecture is solid. The documentation is complete (20+ guides, 100+ pages).</p>
<p dir="auto"><strong>I have zero customers</strong>. Zero revenue. I'm out of savings and actively job hunting for 9 months with no luck. My wife is a nurse who works 5am-7pm while I sit at a computer building something that won't pay our rent.</p>
<p dir="auto"><strong>I can't monetize this from Iran</strong>:</p>
<ul dir="auto">
<li>‚ùå Stripe, PayPal, Western payment processors (sanctioned)</li>
<li>‚ùå AWS, GCP, Azure (sanctioned)</li>
<li>‚ùå Western bank account (sanctioned)</li>
<li>‚ùå Credit card payments from customers (no processor works here)</li>
</ul>
<p dir="auto"><strong>I tried the local Iranian market</strong>. I showed it to friends, family, and potential clients. Their response: <em>"Nobody in Iran will pay $500/month for this. The Persian language quality isn't perfect. We'll use free ChatGPT instead."</em></p>
<p dir="auto">They're right about the market. Iran's currency devalued 100,000x over 20 years. Hotels are closing. Banks are failing. People struggle to pay rent and buy food. This isn't the market for enterprise AI.</p>
<p dir="auto"><strong>So here's why I'm writing this</strong>:</p>
<p dir="auto">I'm not confidently selling a finished product for $100k. I'm looking for a <strong>partner or co-founder</strong> who can access Stripe, handle sales, and help me turn 18 months of brutal work into a business that actually makes money.</p>
<p dir="auto">I want to keep building this. I just need someone who can sell it.</p>
<hr>
<p dir="auto"><h2 dir="auto">What I Built (The 30-Second Version)</h2><a id="user-content-what-i-built-the-30-second-version" aria-label="Permalink: What I Built (The 30-Second Version)" href="#what-i-built-the-30-second-version"></a></p>
<p dir="auto"><strong>NeoClerks</strong> is a self-hosted AI avatar platform with real-time conversation and enterprise infrastructure:</p>
<p dir="auto"><strong>Core Avatar Tech:</strong></p>
<ul dir="auto">
<li>3D photorealistic avatars (Unreal Engine 5 + MetaHuman, pixel streaming)</li>
<li>Real-time conversation (1-8 seconds response time depending on cache hits)</li>
<li>47 languages (OpenAI STT + TTS)</li>
<li>Smart 4-layer caching (hash ‚Üí vector ‚Üí LLM selection, 60-90% cost reduction)</li>
<li>RAG knowledge base (hybrid BM25 + vector search with pgvector)</li>
</ul>
<p dir="auto"><strong>Enterprise Business Infrastructure:</strong></p>
<ul dir="auto">
<li>Multi-tenant B2B architecture (8 database tables for organizations, subscriptions, usage tracking)</li>
<li>8 pre-configured pricing tiers ($497-$947/month + Enterprise custom)</li>
<li>Automated billing system (conversation counting, overage calculation, invoice generation)</li>
<li>Conversation analytics (11 database tables, 7 AI analysis types via OpenAI Batch API)</li>
<li>Admin panel (Next.js, full system management UI)</li>
<li>Monitoring stack (Prometheus + Grafana + Loki, 35+ alert rules)</li>
<li>4-server distributed architecture (scalable to 100+ instances, k8s-ready)</li>
</ul>
<p dir="auto"><strong>Compare to competitors:</strong></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Feature</th>
<th>NeoClerks</th>
<th>Soul Machines</th>
<th>D-ID</th>
<th>Synthesia</th>
</tr>
</thead>
<tbody>
<tr>
<td>Setup Cost</td>
<td>$0-$797</td>
<td>$50,000+</td>
<td>$0</td>
<td>$0</td>
</tr>
<tr>
<td>3D Photorealistic</td>
<td>‚úÖ Real-time UE5</td>
<td>‚úÖ Real-time</td>
<td>‚ùå 2D only</td>
<td>‚ùå 2D only</td>
</tr>
<tr>
<td>Real-time Conversation</td>
<td>‚úÖ 1-8s</td>
<td>‚úÖ ~2s</td>
<td>‚úÖ ~3s</td>
<td>‚ùå Pre-recorded</td>
</tr>
<tr>
<td>Self-hosted</td>
<td>‚úÖ Full control</td>
<td>‚ùå Cloud only</td>
<td>‚ùå Cloud only</td>
<td>‚ùå Cloud only</td>
</tr>
<tr>
<td>White-label</td>
<td>‚úÖ Included</td>
<td>Enterprise tier</td>
<td>‚ùå No</td>
<td>Limited</td>
</tr>
<tr>
<td>Multi-tenant B2B</td>
<td>‚úÖ Built-in</td>
<td>Enterprise tier</td>
<td>Basic</td>
<td>Basic</td>
</tr>
<tr>
<td>Usage Analytics</td>
<td>‚úÖ 7 types</td>
<td>Enterprise tier</td>
<td>Basic</td>
<td>Basic</td>
</tr>
<tr>
<td>Admin Panel</td>
<td>‚úÖ Full UI</td>
<td>Enterprise tier</td>
<td>Basic</td>
<td>Basic</td>
</tr>
<tr>
<td>RAG Knowledge Base</td>
<td>‚úÖ Hybrid search</td>
<td>Enterprise tier</td>
<td>‚ùå No</td>
<td>‚ùå No</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>Market opportunity</strong>: AI avatar market is $2.1B in 2024, projected $4.3B by 2027. Soul Machines has 100+ enterprise clients (Mercedes-Benz, Vodafone). The demand exists.</p>
<p dir="auto"><strong>Full technical documentation</strong>: 20+ guides covering architecture, deployment (local/2-server/4-server), API reference (96+ endpoints), security (JWT + RBAC), monitoring, analytics.</p>
<hr>
<p dir="auto"><h2 dir="auto">The Stories Nobody Sees (Why This Almost Killed Me)</h2><a id="user-content-the-stories-nobody-sees-why-this-almost-killed-me" aria-label="Permalink: The Stories Nobody Sees (Why This Almost Killed Me)" href="#the-stories-nobody-sees-why-this-almost-killed-me"></a></p>
<p dir="auto"><h3 dir="auto">The 120-Hour Weeks</h3><a id="user-content-the-120-hour-weeks" aria-label="Permalink: The 120-Hour Weeks" href="#the-120-hour-weeks"></a></p>
<p dir="auto">For the last 9 months, I worked 100-120 hours per week.</p>
<p dir="auto">My wife wakes up at 5 AM for her nursing shift. I wake up with her. She comes home at 7 PM‚Äîthat's when I eat my first meal of the day. We eat together. Then I go back to the computer until midnight or 1 AM, when I physically can't move anymore and fall asleep at my desk.</p>
<p dir="auto">One evening she came home exhausted and asked: <em>"Is there anything else we can talk about besides this project?"</em></p>
<p dir="auto">I had nothing to say. I live this. I breathe this. It's all I think about.</p>
<p dir="auto">My mother keeps asking: <em>"When will you get a job?"</em> I've been trying for 9 months. The game industry is collapsing globally with massive layoffs. Iran's economy is in the worst state in its history. AI is replacing developer positions. I thought building something valuable was the answer.</p>
<p dir="auto"><h3 dir="auto">The War</h3><a id="user-content-the-war" aria-label="Permalink: The War" href="#the-war"></a></p>
<p dir="auto">In June 2025, during the Iran-Israel conflict, there were bombs falling. I kept working. Explosions in the distance. I just hoped none would hit my house.</p>
<p dir="auto">Around the same time, a crypto exchange I used got hacked‚ÄîI lost what little savings buffer I had left.</p>
<p dir="auto">I kept coding. What else could I do?</p>
<p dir="auto"><h3 dir="auto">The Technical Nightmare That Almost Broke Me</h3><a id="user-content-the-technical-nightmare-that-almost-broke-me" aria-label="Permalink: The Technical Nightmare That Almost Broke Me" href="#the-technical-nightmare-that-almost-broke-me"></a></p>
<p dir="auto">The PAK file loading system nearly killed this project.</p>
<p dir="auto"><strong>The problem</strong>: Generate lip-sync animations on-demand, cook them in Unreal Engine, package them into PAK files, and load them dynamically into a shipped build. There's almost zero documentation for this‚ÄîUnreal is built for game development, not service-based animation streaming.</p>
<p dir="auto">I spent a month fighting this. One month of:</p>
<ul dir="auto">
<li>Creating custom Unreal plugins</li>
<li>Syncing paths precisely between 5 different services (9 with body animation)</li>
<li>Debugging why animations load perfectly in development but crash in production</li>
<li>Fighting Unreal's cooking pipeline (loading a project takes 17 seconds minimum per cook)</li>
</ul>
<p dir="auto">At 3 AM one night, after 14 straight hours of debugging, I almost deleted the entire repository. I was done.</p>
<p dir="auto">Then I found one obscure tutorial that gave me the breakthrough. I got it working. Then I realized it was too slow for real-time use‚Äî17 seconds per cook meant I couldn't use it for live responses.</p>
<p dir="auto">I had to pivot: use real-time lip-sync for live conversations (lower quality but acceptable), and save the PAK system for smart caching where quality matters and pre-generation is possible.</p>
<p dir="auto"><strong>That was one problem</strong>. I faced 50 more like it.</p>
<p dir="auto">The session management between the QR code scanning (phone becomes microphone) and pixel streaming frontend. The WebSocket orchestration. Making it gracefully handle network failures, users switching tabs, phones dying. Creating limits so it can't be abused in a hotel lobby. Every edge case felt like a show-stopper.</p>
<p dir="auto">I wanted to quit every single month. But I had no other choice. I kept going.</p>
<hr>
<p dir="auto"><h2 dir="auto">The Claude Partnership (How I Actually Built This)</h2><a id="user-content-the-claude-partnership-how-i-actually-built-this" aria-label="Permalink: The Claude Partnership (How I Actually Built This)" href="#the-claude-partnership-how-i-actually-built-this"></a></p>
<p dir="auto">I couldn't have built this alone in 18 months. Without AI assistance, this would've taken 6 years.</p>
<p dir="auto">I used <strong>Claude Code (Sonnet 4.5)</strong> as my co-developer and code reviewer. We worked together:</p>
<ul dir="auto">
<li>I'd write code ‚Üí Claude would review and suggest improvements</li>
<li>Claude would write code ‚Üí I'd review and validate</li>
<li>Claude would audit ‚Üí I'd test ‚Üí Claude would audit again</li>
<li>Repeat hundreds of times</li>
</ul>
<p dir="auto"><strong>For the last 50 times I asked Claude to audit this codebase, it consistently responded: "This is 100% production-ready. Deploy immediately."</strong></p>
<p dir="auto">Early on, every fresh audit would find improvements (security issues, architecture problems, missing edge cases). For the last 2 months, Claude finds nothing to fix.</p>
<p dir="auto"><strong>This is AI-validated, production-grade code.</strong></p>
<p dir="auto">You can verify this yourself: open Claude Code, point it at the repository, ask it to audit for production readiness. It will confirm.</p>
<p dir="auto"><strong>Repository stats</strong> (from refactored architecture):</p>
<ul dir="auto">
<li>Total commits: 818</li>
<li>First commit: September 26, 2024</li>
<li>Development time: 18 months (includes original version before refactor)</li>
<li>Lines of code: 20,000+ (backend Python/FastAPI, frontend Next.js, UE5 blueprints)</li>
</ul>
<hr>
<p dir="auto"><h2 dir="auto">Why I Can't Monetize From Iran (The Geography Problem)</h2><a id="user-content-why-i-cant-monetize-from-iran-the-geography-problem" aria-label="Permalink: Why I Can't Monetize From Iran (The Geography Problem)" href="#why-i-cant-monetize-from-iran-the-geography-problem"></a></p>
<p dir="auto">I showed this to several potential clients in Iran before realizing the local market is dead:</p>
<p dir="auto"><strong>Problem 1: Currency collapse</strong>
Iran's rial lost 99.999% of its value over 20 years. I'd need to charge in dollars but collect in rials. Nobody will pay $500/month when that's equivalent to a month's salary.</p>
<p dir="auto"><strong>Problem 2: Persian language quality</strong>
I use GPT-5 (the most expensive model) but Persian language quality isn't perfect. Clients compare it to ChatGPT and expect native-level fluency. It's good (in my opinion), but not perfect enough for risk-averse businesses.</p>
<p dir="auto"><strong>Problem 3: Economic collapse</strong>
Hotels are closing (no tourists). Banks are failing. Hospitals can't risk AI mistakes in Persian. Entertainment is not a priority when people lack clean water. I thought about pivoting to water filters‚Äîthat would sell‚Äîbut I'm a programmer, not an engineer.</p>
<p dir="auto"><strong>Problem 4: International sanctions</strong>
Even if I found international clients, I can't:</p>
<ul dir="auto">
<li>Accept payments (no Stripe, PayPal, or credit card processors)</li>
<li>Deploy on cloud platforms (AWS/GCP/Azure sanctioned)</li>
<li>Open Western bank accounts</li>
<li>Register US/EU companies</li>
</ul>
<p dir="auto"><strong>I tried crypto payments</strong>: B2B customers won't pay $500-1000/month subscriptions in Bitcoin. Their accounting departments reject it. Too volatile for recurring revenue.</p>
<p dir="auto"><strong>I tried intermediaries</strong>: They want 20-30% commission, they hold all payment power (can shut me down anytime), and I still hold legal liability while they take zero risk.</p>
<p dir="auto"><strong>The reality</strong>: I can build enterprise software. I just can't charge for it.</p>
<hr>
<p dir="auto"><h2 dir="auto">What I'm Really Looking For (Partnership, Not Just a Sale)</h2><a id="user-content-what-im-really-looking-for-partnership-not-just-a-sale" aria-label="Permalink: What I'm Really Looking For (Partnership, Not Just a Sale)" href="#what-im-really-looking-for-partnership-not-just-a-sale"></a></p>
<p dir="auto">I don't want to sell this and disappear. <strong>I want to keep building it.</strong> I love this product. I just need a partner who can access Stripe and handle sales.</p>
<p dir="auto">Here are three options (all negotiable):</p>
<p dir="auto"><h3 dir="auto"><strong>Option 1: Co-Founder Partnership</strong></h3><a id="user-content-option-1-co-founder-partnership" aria-label="Permalink: Option 1: Co-Founder Partnership" href="#option-1-co-founder-partnership"></a></p>
<p dir="auto"><strong>Structure:</strong></p>
<ul dir="auto">
<li>You invest: $50-80k + handle sales/operations/Stripe access</li>
<li>I contribute: Complete codebase + continue as technical co-founder</li>
<li>Equity split: 60/40 or 70/30 (negotiable based on your investment and role)</li>
<li>Commitment: 3-year minimum from both sides</li>
</ul>
<p dir="auto"><strong>Your responsibilities:</strong></p>
<ul dir="auto">
<li>Sales and customer acquisition</li>
<li>Payment processing (Stripe account)</li>
<li>Customer support and onboarding</li>
<li>Marketing and positioning</li>
</ul>
<p dir="auto"><strong>My responsibilities:</strong></p>
<ul dir="auto">
<li>All technical development and maintenance</li>
<li>Feature development based on customer feedback</li>
<li>Infrastructure and DevOps</li>
<li>Technical support for complex issues</li>
</ul>
<p dir="auto"><strong>Timeline to profitability</strong>: 6-12 months to acquire first 20-30 customers at $500-1500/month each</p>
<hr>
<p dir="auto"><h3 dir="auto"><strong>Option 2: Outright Sale + Long-Term Contract</strong></h3><a id="user-content-option-2-outright-sale--long-term-contract" aria-label="Permalink: Option 2: Outright Sale + Long-Term Contract" href="#option-2-outright-sale--long-term-contract"></a></p>
<p dir="auto"><strong>Structure:</strong></p>
<ul dir="auto">
<li>You pay: $60-80k upfront (full code ownership, all IP rights)</li>
<li>I sign: 3-year contractor agreement at $4-6k/month</li>
<li>Total cost: $60-80k + $144-216k over 3 years = $204-296k total</li>
<li>You get: Technical founder-level support without equity dilution</li>
</ul>
<p dir="auto"><strong>What I provide:</strong></p>
<ul dir="auto">
<li>30 days intensive support (live setup, training, documentation walkthrough)</li>
<li>Ongoing development (20-30 hours/week for 3 years)</li>
<li>Feature development and bug fixes</li>
<li>Architecture decisions and code reviews</li>
<li>Emergency support for critical issues</li>
</ul>
<p dir="auto"><strong>Why this works:</strong></p>
<ul dir="auto">
<li>You own 100% of the business</li>
<li>You have guaranteed technical support for 3 years</li>
<li>I have stable income while you scale</li>
<li>Lower risk than buying code and hoping it works</li>
</ul>
<hr>
<p dir="auto"><h3 dir="auto"><strong>Option 3: Revenue Share Partnership</strong></h3></p>
<p dir="auto"><strong>Structure:</strong></p>
<ul dir="auto">
<li>You invest: $30-50k upfront + Stripe access + sales/marketing</li>
<li>I contribute: Complete codebase + continue as technical co-founder</li>
<li>Revenue split: 50/50 until you recoup investment, then renegotiate (maybe 40/60 or 30/70)</li>
<li>Commitment: 3-year minimum</li>
</ul>
<p dir="auto"><strong>Why this works:</strong></p>
<ul dir="auto">
<li>Lower upfront cost for you</li>
<li>I'm incentivized to help you succeed (my income depends on revenue)</li>
<li>Fair risk distribution</li>
<li>Aligns our interests long-term</li>
</ul>
<hr>
<p dir="auto"><strong>I'm flexible on structure.</strong> What matters most to me:</p>
<ol dir="auto">
<li>The product gets to market and helps businesses</li>
<li>I can continue working on it (I genuinely love building this)</li>
<li>I have stable income to support my family</li>
<li>We both benefit from the success</li>
</ol>
<p dir="auto">If you have a different structure in mind, let's talk.</p>
<hr>
<p dir="auto"><h2 dir="auto">What You're Actually Getting (Component Breakdown)</h2><a id="user-content-what-youre-actually-getting-component-breakdown" aria-label="Permalink: What You're Actually Getting (Component Breakdown)" href="#what-youre-actually-getting-component-breakdown"></a></p>
<p dir="auto">If you hired developers to build this from scratch:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Component</th>
<th>Cost to Build</th>
<th>Time</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td>Avatar + AI pipeline (UE5, STT, LLM, TTS, RAG)</td>
<td>$50-70k</td>
<td>6 months</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Multi-tenant B2B infrastructure</td>
<td>$20-30k</td>
<td>3 months</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Conversation analytics (11 tables, 7 AI analyses)</td>
<td>$15-25k</td>
<td>2 months</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Monitoring stack (Prometheus/Grafana/35 alerts)</td>
<td>$10-15k</td>
<td>1 month</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Admin panel (Next.js, TypeScript)</td>
<td>$10-15k</td>
<td>1 month</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>4-server distributed architecture</td>
<td>$20-30k</td>
<td>2 months</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Security (JWT on 96 endpoints, RBAC, SSL)</td>
<td>$15-20k</td>
<td>1 month</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td>Documentation (20+ guides, 100+ pages)</td>
<td>$10-15k</td>
<td>1 month</td>
<td>‚úÖ Done</td>
</tr>
<tr>
<td><strong>TOTAL</strong></td>
<td><strong>$150-220k</strong></td>
<td><strong>18 months</strong></td>
<td>‚úÖ Done</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><strong>You're getting this for $60-80k (outright) or $50-80k + equity (partnership).</strong></p>
<p dir="auto">That's a 50-70% discount vs hiring it built, plus you skip 18 months of development time.</p>
<hr>
<p dir="auto"><h2 dir="auto">Known Limitations (Being 100% Honest)</h2><a id="user-content-known-limitations-being-100-honest" aria-label="Permalink: Known Limitations (Being 100% Honest)" href="#known-limitations-being-100-honest"></a></p>
<p dir="auto">I'm not going to oversell this. Here's what you should know:</p>
<p dir="auto"><strong>Technical Limitations:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Animation quality is "good" not "perfect"</strong>: Lip sync is 85-90% accurate (on par with Soul Machines demos). Body language is limited to idle animations. Facial expressions are good but not Pixar-level. Fix: Hire a 3D animator for $3-5k to improve animations.</p>
</li>
<li>
<p dir="auto"><strong>Persian language needs work</strong>: English is flawless. Persian is 80-85% accurate (Whisper + GPT-4o struggle with some accents/nuances). Fix: Fine-tune Whisper on Persian dataset or use local STT service.</p>
</li>
<li>
<p dir="auto"><strong>Demo server latency</strong>: The live demo runs on my GTX 1060 in Iran with 4G home internet. International users may experience 500-1500ms lag. This is infrastructure, not code. Fix: Deploy on AWS in customer's region (latency drops to 100-300ms).</p>
</li>
<li>
<p dir="auto"><strong>GPU requirements</strong>: Development works on GTX 1060 (1 user). Production needs RTX 3090+ for 10 users, RTX 5090 for 30 concurrent users. Cost: $800-2000/month for AWS g4dn/g5 instances.</p>
</li>
</ol>
<p dir="auto"><strong>Business Limitations:</strong></p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Zero customers</strong>: No testimonials, no case studies, no revenue history. Reality: You're starting from scratch on customer acquisition.</p>
</li>
<li>
<p dir="auto"><strong>No brand recognition</strong>: "NeoClerks" has no market awareness. No SEO, no social media following. Reality: You'll likely want to rebrand entirely.</p>
</li>
<li>
<p dir="auto"><strong>No marketing materials</strong>: No demo videos (beyond live demo), no sales collateral, no pitch decks. Reality: You'll need to create these.</p>
</li>
<li>
<p dir="auto"><strong>Support depends on our agreement</strong>: I'll support based on which option you choose (30 days for outright sale, 3 years for partnership). Reality: If you scale, you may need to hire additional support ($50-70k/year).</p>
</li>
</ol>
<hr>
<p dir="auto"><h2 dir="auto">How to Verify This Is Real</h2><a id="user-content-how-to-verify-this-is-real" aria-label="Permalink: How to Verify This Is Real" href="#how-to-verify-this-is-real"></a></p>
<p dir="auto">I know you're skeptical. Here's how to verify everything:</p>
<p dir="auto"><h3 dir="auto"><strong>1. Test the live demo</strong> (5 minutes)</h3><a id="user-content-1-test-the-live-demo-5-minutes" aria-label="Permalink: 1. Test the live demo (5 minutes)" href="#1-test-the-live-demo-5-minutes"></a></p>
<ul dir="auto">
<li>Go to <a href="https://neoclerks.com/en/" rel="nofollow">https://neoclerks.com/en/</a></li>
<li>Click "Book a Private Demo"</li>
<li>Talk to the avatar in English</li>
<li>Evaluate: response time, lip sync quality, conversation coherence</li>
</ul>
<p dir="auto"><strong>Known limitation on demo</strong>: Running on consumer GPU + 4G home internet in Iran, so international latency will be higher than production deployment.</p>
<p dir="auto"><h3 dir="auto"><strong>2. Code access for serious inquiries</strong> (after NDA)</h3><a id="user-content-2-code-access-for-serious-inquiries-after-nda" aria-label="Permalink: 2. Code access for serious inquiries (after NDA)" href="#2-code-access-for-serious-inquiries-after-nda"></a></p>
<p dir="auto">I'll give you read-only GitHub access to review:</p>
<ul dir="auto">
<li>Backend services (FastAPI microservices, Python)</li>
<li>Frontend code (Next.js landing page + admin panel)</li>
<li>Database schema (migrations, SQLAlchemy models)</li>
<li>Infrastructure configs (Docker Compose, nginx, environment templates)</li>
<li>UE5 project structure (blueprints, MetaHuman assets, pixel streaming)</li>
</ul>
<p dir="auto"><strong>What you can verify</strong>: Code quality, architecture, documentation, tests.</p>
<p dir="auto"><h3 dir="auto"><strong>3. Live deployment walkthrough</strong> (after serious interest)</h3><a id="user-content-3-live-deployment-walkthrough-after-serious-interest" aria-label="Permalink: 3. Live deployment walkthrough (after serious interest)" href="#3-live-deployment-walkthrough-after-serious-interest"></a></p>
<p dir="auto">1-2 hour Zoom call where I:</p>
<ul dir="auto">
<li>Deploy the entire stack locally (you watch)</li>
<li>Walk through the admin panel, monitoring dashboards, API endpoints</li>
<li>Explain the architecture and code structure</li>
<li>Answer technical questions</li>
<li>Demonstrate analytics system, billing system, session management</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>4. Claude Code validation</strong> (do this yourself)</h3><a id="user-content-4-claude-code-validation-do-this-yourself" aria-label="Permalink: 4. Claude Code validation (do this yourself)" href="#4-claude-code-validation-do-this-yourself"></a></p>
<ul dir="auto">
<li>Get Claude Code access</li>
<li>Point it at the repository</li>
<li>Ask: "Review this codebase for production readiness"</li>
<li>Claude will confirm what I've said</li>
</ul>
<hr>
<p dir="auto"><h2 dir="auto">Who Should Consider This</h2><a id="user-content-who-should-consider-this" aria-label="Permalink: Who Should Consider This" href="#who-should-consider-this"></a></p>
<p dir="auto"><h3 dir="auto">‚úÖ <strong>Good Fit:</strong></h3><a id="user-content--good-fit" aria-label="Permalink: ‚úÖ Good Fit:" href="#-good-fit"></a></p>
<p dir="auto"><strong>You're a digital agency with existing B2B clients:</strong></p>
<ul dir="auto">
<li>You have 50-200 clients you can upsell to</li>
<li>You can position this at $700-1500/month to hotels, retail, healthcare</li>
<li>You have a DevOps person or can hire one ($60-80k/year)</li>
</ul>
<p dir="auto"><strong>You're a SaaS entrepreneur who's done this before:</strong></p>
<ul dir="auto">
<li>You've sold B2B SaaS (you understand 6-12 month sales cycles)</li>
<li>You have capital for 6-12 months runway</li>
<li>You know how to do outreach, demos, close deals</li>
</ul>
<p dir="auto"><strong>You're an enterprise IT company:</strong></p>
<ul dir="auto">
<li>You sell to Fortune 500s (banks, telecoms, healthcare)</li>
<li>You need a white-label AI solution for your portfolio</li>
<li>Your team closes 5-10 enterprise deals/year</li>
</ul>
<p dir="auto"><strong>You want a technical co-founder:</strong></p>
<ul dir="auto">
<li>You can handle sales/ops but lack technical depth</li>
<li>You want someone committed long-term (not just a contractor)</li>
<li>You're willing to share equity for proven technical execution</li>
</ul>
<hr>
<p dir="auto"><h3 dir="auto">‚ùå <strong>Bad Fit:</strong></h3><a id="user-content--bad-fit" aria-label="Permalink: ‚ùå Bad Fit:" href="#-bad-fit"></a></p>
<p dir="auto"><strong>You have no technical skills and won't hire:</strong></p>
<ul dir="auto">
<li>Can't deploy Docker containers</li>
<li>Won't hire a DevOps engineer ($60-80k/year)</li>
<li><strong>Why you'll fail</strong>: Every customer needs a deployment</li>
</ul>
<p dir="auto"><strong>You have no sales experience:</strong></p>
<ul dir="auto">
<li>You've never sold B2B SaaS</li>
<li>You can't afford to hire a salesperson ($50-70k/year + commission)</li>
<li><strong>Why you'll fail</strong>: Zero customers means you acquire them all yourself</li>
</ul>
<p dir="auto"><strong>You expect passive income:</strong></p>
<ul dir="auto">
<li>You want "buy it and forget it"</li>
<li>You don't want to provide customer support</li>
<li><strong>Why you'll fail</strong>: This requires active sales and support</li>
</ul>
<hr>
<p dir="auto"><h2 dir="auto">Unit Economics (Why This Can Be Profitable)</h2><a id="user-content-unit-economics-why-this-can-be-profitable" aria-label="Permalink: Unit Economics (Why This Can Be Profitable)" href="#unit-economics-why-this-can-be-profitable"></a></p>
<p dir="auto">Let's do realistic math:</p>
<p dir="auto"><strong>Scenario: Digital agency with 100 existing clients</strong></p>
<p dir="auto"><strong>Month 1-3</strong>: Pitch 100 clients, 10 sign up (10% conversion)</p>
<ul dir="auto">
<li>Tier: Professional ($799/month)</li>
<li>Revenue: $7,990/month</li>
<li>Costs: $2,000/month (AWS + OpenAI API)</li>
<li>Profit: $5,990/month (~75% margin)</li>
</ul>
<p dir="auto"><strong>Month 4-6</strong>: Upsell another 10 clients (20 total)</p>
<ul dir="auto">
<li>Revenue: $15,980/month</li>
<li>Costs: $3,500/month</li>
<li>Profit: $12,480/month</li>
</ul>
<p dir="auto"><strong>Month 7-12</strong>: Organic growth + referrals (30 total)</p>
<ul dir="auto">
<li>Revenue: $23,970/month = <strong>$287k/year</strong></li>
<li>Costs: $5,000/month = <strong>$60k/year</strong></li>
<li>Profit: $18,970/month = <strong>$227k/year</strong></li>
</ul>
<p dir="auto"><strong>ROI</strong>: At $60k purchase price, you break even in <strong>3-4 months</strong> with 30 customers.</p>
<p dir="auto"><strong>This is realistic if you have existing B2B relationships.</strong> If starting cold, add 6-12 months to reach 30 customers.</p>
<hr>
<p dir="auto"><h2 dir="auto">The Sale/Partnership Process</h2><a id="user-content-the-salepartnership-process" aria-label="Permalink: The Sale/Partnership Process" href="#the-salepartnership-process"></a></p>
<p dir="auto">Since I can't use Stripe/PayPal, we use cryptocurrency escrow for safety:</p>
<p dir="auto"><h3 dir="auto"><strong>Phase 1: Initial Contact (Week 1)</strong></h3><a id="user-content-phase-1-initial-contact-week-1" aria-label="Permalink: Phase 1: Initial Contact (Week 1)" href="#phase-1-initial-contact-week-1"></a></p>
<ol dir="auto">
<li>You test the demo (5 minutes)</li>
<li>You read this article</li>
<li>We do a 30-minute intro call (verify I'm real, answer questions)</li>
<li>I send you high-level code structure overview</li>
</ol>
<p dir="auto"><h3 dir="auto"><strong>Phase 2: Due Diligence (Week 2-3)</strong></h3><a id="user-content-phase-2-due-diligence-week-2-3" aria-label="Permalink: Phase 2: Due Diligence (Week 2-3)" href="#phase-2-due-diligence-week-2-3"></a></p>
<ol dir="auto">
<li>You sign NDA (mutual protection)</li>
<li>I give you read-only GitHub access</li>
<li>You review code, architecture, documentation (3-7 days)</li>
<li>We do live deployment walkthrough (1-2 hours)</li>
<li>You decide if you want to proceed</li>
</ol>
<p dir="auto"><h3 dir="auto"><strong>Phase 3: Agreement (Week 3-4)</strong></h3><a id="user-content-phase-3-agreement-week-3-4" aria-label="Permalink: Phase 3: Agreement (Week 3-4)" href="#phase-3-agreement-week-3-4"></a></p>
<ol dir="auto">
<li>We agree on structure (co-founder / sale+contract / revenue share)</li>
<li>We finalize price and terms</li>
<li>Lawyers draft agreement (if needed)</li>
</ol>
<p dir="auto"><h3 dir="auto"><strong>Phase 4: Payment &amp; Transfer (Week 4-5)</strong></h3><a id="user-content-phase-4-payment--transfer-week-4-5" aria-label="Permalink: Phase 4: Payment &amp; Transfer (Week 4-5)" href="#phase-4-payment--transfer-week-4-5"></a></p>
<p dir="auto"><strong>For outright sale:</strong></p>
<ul dir="auto">
<li>We use <strong>Hodl Hodl</strong> (non-custodial multisig escrow, works from Iran)</li>
<li>You deposit BTC/USDT into escrow (neutral third party holds funds)</li>
<li>I transfer complete source code + documentation</li>
<li>We conduct 2-hour live setup session (recorded)</li>
<li>You verify code works (3-7 days)</li>
<li>You release escrow payment</li>
<li>30-day support period begins</li>
</ul>
<p dir="auto"><strong>For partnership:</strong></p>
<ul dir="auto">
<li>Standard equity agreement via lawyer</li>
<li>Payment via wire transfer to your company account (you're outside Iran)</li>
<li>I join as technical co-founder</li>
<li>We start working together immediately</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>Protection for both sides:</strong></h3><a id="user-content-protection-for-both-sides" aria-label="Permalink: Protection for both sides:" href="#protection-for-both-sides"></a></p>
<ul dir="auto">
<li>‚úÖ Escrow mediator handles disputes (for sales)</li>
<li>‚úÖ Standard equity agreements (for partnerships)</li>
<li>‚úÖ You verify code before payment release</li>
<li>‚úÖ I get security that payment is locked in escrow</li>
</ul>
<p dir="auto"><strong>Timeline</strong>: 4-6 weeks from first contact to deal closed.</p>
<hr>
<p dir="auto"><h2 dir="auto">What Happens Next (If We Work Together)</h2><a id="user-content-what-happens-next-if-we-work-together" aria-label="Permalink: What Happens Next (If We Work Together)" href="#what-happens-next-if-we-work-together"></a></p>
<p dir="auto">If you become my partner or buyer, here's the immediate roadmap:</p>
<p dir="auto"><h3 dir="auto"><strong>Month 1: Launch</strong></h3><a id="user-content-month-1-launch" aria-label="Permalink: Month 1: Launch" href="#month-1-launch"></a></p>
<ul dir="auto">
<li>Deploy on your infrastructure (AWS/GCP in target market)</li>
<li>Set up Stripe account and payment processing</li>
<li>Rebrand if desired (logo, domain, marketing site)</li>
<li>Create demo videos and sales collateral</li>
<li>I provide intensive technical support</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>Month 2-3: First Customers</strong></h3><a id="user-content-month-2-3-first-customers" aria-label="Permalink: Month 2-3: First Customers" href="#month-2-3-first-customers"></a></p>
<ul dir="auto">
<li>Target: Acquire 5-10 pilot customers</li>
<li>Pricing: $500-1000/month with discounted setup fees</li>
<li>Focus: Hotels, retail, or your existing client base</li>
<li>Collect feedback and testimonials</li>
<li>I implement critical feature requests</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>Month 4-6: Scale</strong></h3><a id="user-content-month-4-6-scale" aria-label="Permalink: Month 4-6: Scale" href="#month-4-6-scale"></a></p>
<ul dir="auto">
<li>Target: 20-30 customers</li>
<li>Improve animations based on feedback</li>
<li>Add language support if needed</li>
<li>Build case studies</li>
<li>Optimize costs (caching should reduce API costs 60-90%)</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>Month 7-12: Growth</strong></h3><a id="user-content-month-7-12-growth" aria-label="Permalink: Month 7-12: Growth" href="#month-7-12-growth"></a></p>
<ul dir="auto">
<li>Target: 50-100 customers</li>
<li>Hire additional support/sales if needed</li>
<li>Explore enterprise deals ($2-5k/month)</li>
<li>Consider raising funding if partnership model</li>
<li>I continue technical leadership</li>
</ul>
<p dir="auto"><strong>This is a real business opportunity.</strong> The market exists (Soul Machines has 100+ customers at 10x the price). The technology works (you tested it). It just needs someone who can access Stripe and sell.</p>
<hr>
<p dir="auto"><h2 dir="auto">Final Thoughts</h2><a id="user-content-final-thoughts" aria-label="Permalink: Final Thoughts" href="#final-thoughts"></a></p>
<p dir="auto">I built this because I had no other choice. The game industry collapsed. Iran's economy is in freefall. AI is taking developer jobs. I thought: "Build something valuable, and opportunity will follow."</p>
<p dir="auto">I was wrong about one thing: geography matters more than I thought. I can build enterprise software, but I can't charge for it from Iran.</p>
<p dir="auto">But I was right about another thing: this product is valuable. Soul Machines raised $70M selling similar technology for $50k+ setup fees. D-ID raised $25M with 2D avatars. The market is real.</p>
<p dir="auto"><strong>I don't want sympathy. I want a partner.</strong></p>
<p dir="auto">Someone who can access Stripe, understands B2B sales, and wants to build this into a real business.</p>
<p dir="auto">If that's you, let's talk.</p>
<hr>
<p dir="auto"><h2 dir="auto">How to Get Started</h2><a id="user-content-how-to-get-started" aria-label="Permalink: How to Get Started" href="#how-to-get-started"></a></p>
<p dir="auto"><h3 dir="auto"><strong>If you're seriously interested:</strong></h3><a id="user-content-if-youre-seriously-interested" aria-label="Permalink: If you're seriously interested:" href="#if-youre-seriously-interested"></a></p>
<ol dir="auto">
<li><strong>Test the demo</strong> ‚Üí <a href="https://neoclerks.com/en/" rel="nofollow">https://neoclerks.com/en/</a> (5 minutes)</li>
<li><strong>Schedule a call</strong> ‚Üí Email me at <a href="mailto:EchenDeligani@gmail.com">EchenDeligani@gmail.com</a></li>
<li><strong>Review the code</strong> ‚Üí I'll provide NDA + GitHub access</li>
<li><strong>Make a proposal</strong> ‚Üí Co-founder equity / Sale+contract / Revenue share</li>
</ol>
<p dir="auto"><h3 dir="auto"><strong>If you know someone who might be interested:</strong></h3><a id="user-content-if-you-know-someone-who-might-be-interested" aria-label="Permalink: If you know someone who might be interested:" href="#if-you-know-someone-who-might-be-interested"></a></p>
<ul dir="auto">
<li>Share this article with digital agencies, SaaS entrepreneurs, or investors</li>
<li>Introduce me via email</li>
<li><strong>Referral bonus</strong>: I'll pay 5% ($3-4k) if your intro leads to a deal</li>
</ul>
<p dir="auto"><h3 dir="auto"><strong>If you're just curious:</strong></h3><a id="user-content-if-youre-just-curious" aria-label="Permalink: If you're just curious:" href="#if-youre-just-curious"></a></p>
<ul dir="auto">
<li>Test the demo anyway (it's genuinely cool tech)</li>
<li>Ask questions in the comments (I'll respond to everything)</li>
<li>Follow for updates on how this story ends</li>
</ul>
<hr>
<p dir="auto"><h2 dir="auto">Contact Information</h2><a id="user-content-contact-information" aria-label="Permalink: Contact Information" href="#contact-information"></a></p>
<p dir="auto"><strong>Email</strong>: <a href="mailto:EchenDeligani@gmail.com">EchenDeligani@gmail.com</a>
<strong>LinkedIn</strong>: <a href="https://www.linkedin.com/in/echen-deligani/" rel="nofollow">https://www.linkedin.com/in/echen-deligani/</a>
<strong>Telegram</strong>: @unnamedhn
<strong>WhatsApp</strong>: +98 901 441 1869</p>
<p dir="auto"><strong>Live Demo</strong>: <a href="https://neoclerks.com/en/" rel="nofollow">https://neoclerks.com/en/</a></p>
<p dir="auto"><strong>Partnership Options</strong>:</p>
<ul dir="auto">
<li>Co-founder: $50-80k + equity split</li>
<li>Sale + Contract: $60-80k + 3-year agreement ($4-6k/month)</li>
<li>Revenue share: $30-50k + 50/50 split until ROI</li>
</ul>
<p dir="auto"><strong>Timeline</strong>: 4-6 weeks from first contact to deal closed</p>
<hr>
<p dir="auto"><h2 dir="auto">Updates</h2><a id="user-content-updates" aria-label="Permalink: Updates" href="#updates"></a></p>
<p dir="auto">I'll update this section as things progress:</p>
<ul>
<li><strong>2025-11-12</strong>: Posted on Medium, HackerNews, Reddit r/SaaS</li>
<li> Demo calls scheduled</li>
<li> Code reviews in progress</li>
<li> Negotiating with potential partners</li>
<li> <strong>DEAL CLOSED</strong>: [Date TBD]</li>
</ul>
<p dir="auto">Follow me to see how this story ends.</p>
<hr>
<p dir="auto"><em>Originally written: November 2025</em></p>
<p dir="auto"><strong>Tags</strong>: #AI #Startups #SaaS #UnrealEngine #TechPartner #Partnership #SoulMachines #AvatarAI #ConversationalAI #IranTech #Entrepreneurship</p>
<hr>
<p dir="auto"><h2 dir="auto">Comments</h2><a id="user-content-comments" aria-label="Permalink: Comments" href="#comments"></a></p>
<p dir="auto"><strong>Questions? Want to discuss partnership? Technical deep-dive requests?</strong></p>
<p dir="auto">Drop a comment below or reach out directly. I respond to every message within 24 hours.</p>
<p dir="auto"><strong>I built this for 18 months through war and 120-hour weeks. Now I need a partner who can help me turn it into a real business.</strong></p>
<p dir="auto"><strong>Is that you?</strong></p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.NET MAUI is coming to Linux and the browser, powered by Avalonia (282 pts)]]></title>
            <link>https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia</link>
            <guid>45893986</guid>
            <pubDate>Tue, 11 Nov 2025 22:50:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia">https://avaloniaui.net/blog/net-maui-is-coming-to-linux-and-the-browser-powered-by-avalonia</a>, See on <a href="https://news.ycombinator.com/item?id=45893986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.</p><p>For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.</p><h2>Try It Right Now</h2><p>Before we dive into the details, you can experience it yourself:</p><p><!--$--><a href="https://brave-sky-0c7a41a03-preview.westeurope.3.azurestaticapps.net/" target="_blank" rel="noopener"><strong>Launch MAUI in your browser ‚Üí</strong></a><!--/$--></p><p>This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.</p><h2>What is the Avalonia MAUI Backend?</h2><p>At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.</p><p>In practical terms, that means several big wins.</p><h3>Desktop Linux support</h3><p>.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.</p><p><img alt="" width="899" height="506" src="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png" srcset="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=512&amp;width=1799&amp;height=1013 512w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=1024&amp;width=1799&amp;height=1013 1024w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?width=1799&amp;height=1013 1799w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><video autoplay="" loop="" muted="" playsinline="" src="https://framerusercontent.com/images/2t4wGG4T3lL7WgeuQul6aggxBNA.mp4"></video><h3>Embedded Linux</h3><p>Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.</p><h3>WebAssembly support</h3><div><p>The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.</p></div><p><img alt="" width="1337" height="1004" src="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png" srcset="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=512&amp;width=2674&amp;height=2008 512w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=1024&amp;width=2674&amp;height=2008 1024w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=2048&amp;width=2674&amp;height=2008 2048w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?width=2674&amp;height=2008 2674w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><h3>Bonus: The Avalonia MAUI Backend runs on Windows and macOS too</h3><p>On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.</p><p>All of this is possible because we have built a version of MAUI that sits on top of Avalonia‚Äôs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.</p><h3>Simpler, faster development</h3><p>For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.</p><p><img alt="" width="1276" height="285" src="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png" srcset="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=512&amp;width=2552&amp;height=571 512w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=1024&amp;width=2552&amp;height=571 1024w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=2048&amp;width=2552&amp;height=571 2048w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?width=2552&amp;height=571 2552w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><div><p>Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.</p><p>That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. </p><p>For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.</p></div><h2>Why Is Avalonia Building a Backend for MAUI?</h2><p>It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?</p><p>The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.</p><p>This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.</p><p>There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.</p><p>So this project is not about ‚Äúsaving‚Äù MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.</p><h2>Why This Matters for MAUI Developers</h2><p>If you have followed MAUI since its launch, you will know the two requests that never went away.</p><p>Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.</p><p>The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.</p><p>It provides:</p><ul><li data-preset-tag="p"><p>Hardware accelerated rendering on every platform</p></li><li data-preset-tag="p"><p>A consistent layout and styling system</p></li><li data-preset-tag="p"><p>Smooth animations at high refresh rates</p></li><li data-preset-tag="p"><p>Custom rendering and visual effects capabilities</p></li><li data-preset-tag="p"><p>Broad platform coverage</p></li><li data-preset-tag="p"><p>A fully supported platform that is receiving significant investment</p></li></ul><p>These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.</p><p>By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.</p><h2>Performance and Next Generation Rendering</h2><p>Performance is an important part of this story.</p><p>A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.</p><p>We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.</p><p>The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.</p><p><!--$--><a href="https://avaloniaui.net/blog/avalonia-partners-with-google-s-flutter-t-eam-to-bring-impeller-rendering-to-net">Read more about our Impeller collaboration with Google ‚Üí</a><!--/$--></p><h2>Looking Forward</h2><p>We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.</p><p>This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.</p><p><!--$--><a href="https://avaloniaui.net/maui-interest"><strong>‚Üí Register your interest in the early access ‚Üê</strong></a><!--/$--><strong></strong></p><br></div><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.</p><p>For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.</p><h2>Try It Right Now</h2><p>Before we dive into the details, you can experience it yourself:</p><p><!--$--><a href="https://brave-sky-0c7a41a03-preview.westeurope.3.azurestaticapps.net/" target="_blank" rel="noopener"><strong>Launch MAUI in your browser ‚Üí</strong></a><!--/$--></p><p>This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.</p><h2>What is the Avalonia MAUI Backend?</h2><p>At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.</p><p>In practical terms, that means several big wins.</p><h3>Desktop Linux support</h3><p>.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.</p><p><img alt="" width="899" height="506" src="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png" srcset="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=512&amp;width=1799&amp;height=1013 512w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=1024&amp;width=1799&amp;height=1013 1024w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?width=1799&amp;height=1013 1799w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><video autoplay="" loop="" muted="" playsinline="" src="https://framerusercontent.com/images/2t4wGG4T3lL7WgeuQul6aggxBNA.mp4"></video><h3>Embedded Linux</h3><p>Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.</p><h3>WebAssembly support</h3><div><p>The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.</p></div><p><img alt="" width="1337" height="1004" src="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png" srcset="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=512&amp;width=2674&amp;height=2008 512w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=1024&amp;width=2674&amp;height=2008 1024w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=2048&amp;width=2674&amp;height=2008 2048w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?width=2674&amp;height=2008 2674w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><h3>Bonus: The Avalonia MAUI Backend runs on Windows and macOS too</h3><p>On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.</p><p>All of this is possible because we have built a version of MAUI that sits on top of Avalonia‚Äôs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.</p><h3>Simpler, faster development</h3><p>For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.</p><p><img alt="" width="1276" height="285" src="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png" srcset="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=512&amp;width=2552&amp;height=571 512w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=1024&amp;width=2552&amp;height=571 1024w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=2048&amp;width=2552&amp;height=571 2048w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?width=2552&amp;height=571 2552w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><div><p>Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.</p><p>That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. </p><p>For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.</p></div><h2>Why Is Avalonia Building a Backend for MAUI?</h2><p>It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?</p><p>The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.</p><p>This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.</p><p>There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.</p><p>So this project is not about ‚Äúsaving‚Äù MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.</p><h2>Why This Matters for MAUI Developers</h2><p>If you have followed MAUI since its launch, you will know the two requests that never went away.</p><p>Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.</p><p>The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.</p><p>It provides:</p><ul><li data-preset-tag="p"><p>Hardware accelerated rendering on every platform</p></li><li data-preset-tag="p"><p>A consistent layout and styling system</p></li><li data-preset-tag="p"><p>Smooth animations at high refresh rates</p></li><li data-preset-tag="p"><p>Custom rendering and visual effects capabilities</p></li><li data-preset-tag="p"><p>Broad platform coverage</p></li><li data-preset-tag="p"><p>A fully supported platform that is receiving significant investment</p></li></ul><p>These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.</p><p>By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.</p><h2>Performance and Next Generation Rendering</h2><p>Performance is an important part of this story.</p><p>A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.</p><p>We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.</p><p>The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.</p><p><!--$--><a href="https://avaloniaui.net/blog/avalonia-partners-with-google-s-flutter-t-eam-to-bring-impeller-rendering-to-net">Read more about our Impeller collaboration with Google ‚Üí</a><!--/$--></p><h2>Looking Forward</h2><p>We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.</p><p>This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.</p><p><!--$--><a href="https://avaloniaui.net/maui-interest"><strong>‚Üí Register your interest in the early access ‚Üê</strong></a><!--/$--><strong></strong></p><br></div><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>We are bringing .NET MAUI to Linux and to the browser, powered by Avalonia.</p><p>For the past few months, we have been working on an Avalonia powered backend for .NET MAUI, with guidance and feedback from engineers in the MAUI ecosystem. What started as an experiment has grown into a project we are committing to, with apps already running on new platforms. It is time to show you what we have been building.</p><h2>Try It Right Now</h2><p>Before we dive into the details, you can experience it yourself:</p><p><!--$--><a href="https://brave-sky-0c7a41a03-preview.westeurope.3.azurestaticapps.net/" target="_blank" rel="noopener"><strong>Launch MAUI in your browser ‚Üí</strong></a><!--/$--></p><p>This is a real MAUI application running on WebAssembly, rendered through Avalonia, with no plugins or hidden tricks. It is an early build with rough edges, but it proves the point: MAUI can now run on every major desktop OS and in the browser.</p><h2>What is the Avalonia MAUI Backend?</h2><p>At its core, the Avalonia MAUI Backend enables you to keep your MAUI codebase while replacing the rendering layer with Avalonia. The goal is straightforward: take your existing MAUI applications and extend them to additional platforms, while enhancing desktop performance along the way.</p><p>In practical terms, that means several big wins.</p><h3>Desktop Linux support</h3><p>.NET MAUI apps running as first class desktop apps on distributions such as Ubuntu, Debian and Fedora, sharing the same Avalonia renderer that already powers demanding desktop apps in production today.</p><p><img alt="" width="899" height="506" src="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png" srcset="https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=512&amp;width=1799&amp;height=1013 512w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?scale-down-to=1024&amp;width=1799&amp;height=1013 1024w,https://framerusercontent.com/images/KyaYoF7ykWI3G1pHcJVRbDXx1B8.png?width=1799&amp;height=1013 1799w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><video autoplay="" loop="" muted="" playsinline="" src="https://framerusercontent.com/images/2t4wGG4T3lL7WgeuQul6aggxBNA.mp4"></video><h3>Embedded Linux</h3><p>Avalonia already runs on embedded Linux devices, from Raspberry Pi panels to industrial HMIs. Using the same backend, the Avalonia MAUI Backend brings those capabilities to MAUI as well, so the applications you build in MAUI can run on the same embedded Linux targets as Avalonia.</p><h3>WebAssembly support</h3><div><p>The demo you can open in your browser today is a real MAUI application running on WebAssembly, rendered by Avalonia, with no native dependencies on the client. It is an early build, but it demonstrates what is now possible. MAUI apps will soon be free to deploy to the browser.</p></div><p><img alt="" width="1337" height="1004" src="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png" srcset="https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=512&amp;width=2674&amp;height=2008 512w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=1024&amp;width=2674&amp;height=2008 1024w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?scale-down-to=2048&amp;width=2674&amp;height=2008 2048w,https://framerusercontent.com/images/T9sNNunB9VG7qoS56a2mHrhSj2I.png?width=2674&amp;height=2008 2674w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><h3>Bonus: The Avalonia MAUI Backend runs on Windows and macOS too</h3><p>On Windows and macOS, it plugs into the same mature desktop story Avalonia already has. On macOS, early testing indicates significantly improved performance compared to the Mac Catalyst approach. We are seeing more than 2x the performance in representative desktop scenarios, which is a very encouraging sign for the future of MAUI on desktop.</p><p>All of this is possible because we have built a version of MAUI that sits on top of Avalonia‚Äôs drawn UI model rather than native controls. Not only do you get more platforms and improved performance, your MAUI applications can look and behave consistently whether they are on Windows, macOS, Linux, mobile or running in a browser tab.</p><h3>Simpler, faster development</h3><p>For the Avalonia team, this architecture has a major practical benefit: we only have to target one platform: Avalonia itself. That single target means we can move faster, ship features consistently, and avoid the need to endlessly fix platform-specific edge cases.</p><p><img alt="" width="1276" height="285" src="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png" srcset="https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=512&amp;width=2552&amp;height=571 512w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=1024&amp;width=2552&amp;height=571 1024w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?scale-down-to=2048&amp;width=2552&amp;height=571 2048w,https://framerusercontent.com/images/H4IFZdeFuwE4I7XSlLN5GlTBeZY.png?width=2552&amp;height=571 2552w" data-framer-original-sizes="" sizes="(min-width: 1400px) 100vw, (min-width: 800px) and (max-width: 1399.98px) 100vw, (max-width: 799.98px) 100vw"></p><div><p>Instead of maintaining separate implementations for iOS, Android, Windows, macOS, Linux, and WebAssembly, we maintain just one. That massively reduces the chances of platform quirks, that can eat up hours of debugging time when something works on Android but not iOS, or renders differently on Mac Catalyst versus WinUI 3. When building on Avalonia, the controls render the same way everywhere because they are using the same rendering engine everywhere.</p><p>That means when we add a feature or fix a bug, it works across all platforms. No more "this works on mobile but breaks on desktop" or "this looks right on Windows but wrong on macOS." The entire development cycle becomes more predictable and significantly faster. </p><p>For us, that is a significant advantage. For MAUI developers, it means the backend evolves faster and more reliably.</p></div><h2>Why Is Avalonia Building a Backend for MAUI?</h2><p>It is a fair question. Avalonia already has its own thriving ecosystem. We see strong, sustained growth in our community, so why invest this much effort into making MAUI run on top of Avalonia?</p><p>The honest answer is that we care about .NET client developers first, and about which on ramp they use second. Many teams have already chosen MAUI, which they like and want more from. If we can provide them with Linux and browser support, along with improved desktop performance, without requiring a rewrite, that aligns with our mission to delight developers and solve complex problems.</p><p>This is not entirely selfless. Building a MAUI backend is also a way for us to learn. Running MAUI on Avalonia highlights what is missing for Avalonia to feel completely natural on mobile, which APIs are problematic, which tooling gaps matter, and where we need to raise our game to stay competitive. The work we are doing here directly contributes to strengthening Avalonia.</p><p>There is also a long term benefit in familiarity. By using Avalonia as the backend for their existing MAUI apps, developers gain insight into our renderer, capabilities and way of thinking. Some of those teams will quite reasonably stay with MAUI. Others, when they start a new project or need something lower level, may build directly on Avalonia instead. If this backend becomes a bridge that brings more people into the Avalonia ecosystem over time, that is a win.</p><p>So this project is not about ‚Äúsaving‚Äù MAUI from other frameworks. It is about giving existing MAUI developers more headroom and additional platforms, learning from their needs, and ensuring Avalonia is an obvious, competitive choice for whatever they build next.</p><h2>Why This Matters for MAUI Developers</h2><p>If you have followed MAUI since its launch, you will know the two requests that never went away.</p><p>Developers want Linux support, both for desktop and for embedded devices. They also want a drawn control model that provides consistent behaviour across platforms, rather than relying on the native toolkit available on each system.</p><p>The Avalonia backend tackles both of those head on. Avalonia is a mature drawn UI framework.</p><p>It provides:</p><ul><li data-preset-tag="p"><p>Hardware accelerated rendering on every platform</p></li><li data-preset-tag="p"><p>A consistent layout and styling system</p></li><li data-preset-tag="p"><p>Smooth animations at high refresh rates</p></li><li data-preset-tag="p"><p>Custom rendering and visual effects capabilities</p></li><li data-preset-tag="p"><p>Broad platform coverage</p></li><li data-preset-tag="p"><p>A fully supported platform that is receiving significant investment</p></li></ul><p>These are not theoretical promises. They are the reasons Avalonia is used in production by companies such as Unity, JetBrains and Schneider Electric.</p><p>By building MAUI on top of Avalonia, you get a predictable, drawn UI foundation and an expanded set of platforms, without having to throw away your existing codebase. You do not need to abandon MAUI to get Linux and the web. You can bring MAUI with you, while also improving the experience on Windows and macOS.</p><h2>Performance and Next Generation Rendering</h2><p>Performance is an important part of this story.</p><p>A drawn, GPU friendly UI stack gives you more headroom than wrapping native toolkits.</p><p>We are collaborating with the Flutter team at Google to bring Impeller, their GPU first renderer, to .NET. That work is already in progress and as it lands, the MAUI backend will inherit those gains.</p><p>The aim is simple: faster rendering, lower battery usage and smoother animations across desktop, mobile and embedded, using the same underlying technology that is pushing Flutter forward.</p><p><!--$--><a href="https://avaloniaui.net/blog/avalonia-partners-with-google-s-flutter-t-eam-to-bring-impeller-rendering-to-net">Read more about our Impeller collaboration with Google ‚Üí</a><!--/$--></p><h2>Looking Forward</h2><p>We are particularly grateful to the MAUI engineers who have shared feedback and ideas as we have developed this backend. The .NET client ecosystem is at its best when different teams can cross pollinate and push each other forward.</p><p>This is just the beginning. As Linux and browser support matures, MAUI can finally live up to its promise as a truly multi platform app UI. We will keep sharing previews, benchmarks and updates as development continues, and once we are happy with the stability of the backend we will release the source code as fully open source under the MIT licence.</p><p><!--$--><a href="https://avaloniaui.net/maui-interest"><strong>‚Üí Register your interest in the early access ‚Üê</strong></a><!--/$--><strong></strong></p><br></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Four strange places to see London's Roman Wall (236 pts)]]></title>
            <link>https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html</link>
            <guid>45893795</guid>
            <pubDate>Tue, 11 Nov 2025 22:31:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html">https://diamondgeezer.blogspot.com/2025/11/odd-places-to-see-londons-roman-wall.html</a>, See on <a href="https://news.ycombinator.com/item?id=45893795">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <a name="3254945716980684913"></a>    
          <i>There are <a href="https://en.wikipedia.org/wiki/London_Wall#Known_monuments_and_landmarks" target="_blank">many</a> <a href="https://colat.org.uk/wp-content/uploads/2024/07/London-Wall-Walk-guide.pdf" target="_blank">places</a> around the City of London to see its old <a href="https://www.english-heritage.org.uk/visit/places/london-wall/history/" target="_blank">Roman Wall</a>, notably alongside Noble Street, in Barber Surgeons' Meadow, through the Barbican, in St Alphage Garden and just outside the entrance to Tower Hill station. Here are four of the odder spots.</i>
<p>
<b><u>Four strange places to see London's Roman Wall</u></b></p><p>
<b>1) From platform 1 at Tower Hill station</b></p><p>
If you're ever waiting for a westbound train at Tower Hill station, take a walk to the rear of the platform and take a look across the tracks, roughly where the penultimate carriage would stop. High on the far wall is a square recess lined by black tiles, and at the back of that is a dimly-lit surface of chunky irregular blocks. Unlike every single other thing on the Underground, the Romans built that.
</p><p>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgb2locUXUgdHSakPua-TyO6SVasQxlWPrse9F6CwqXLVM1oqPaXpBqAxjgbvc9H3zyDlibYX4Hmn0qVrMh_E5JwHEJl_PYQGMFJeTMcH4QogeJTNqhTR8gmed6vQV5nscU3Avuif3_ePzPpNkCLX2_ydqzKUfXz0x8mAwQMzFoTslTbOjAqx3mWQ/s1600/towerhilstn.jpg" title="the wall opposite platform 1 at Tower Hill station" data-original-height="375" data-original-width="500"></p><p>
London's original wall was 2 miles long, 6 metres high and almost 3 metres thick at its base, all the better to keep out uncivilised marauders. It was built around 200 AD, then left to decay and rebuilt in the medieval era, again for defensive purposes. This is one of the original bits, not that you can easily tell by squinting across the tracks. A small metal lamp points inwards but is no longer switched on because heritage illumination is not a TfL priority. There is however a rather nice silver plaque on the pillar opposite, should you step back far enough to notice it.
</p><p>
<img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKJSryCZmQbUERGBEE5aEwbVX5UrBgPYyeZPT8psByjxtc253SMFxSZsjJS5b6SyNU3IatUqjXB4gjlR0JEzdXiw8E62y4DnTwSTwcdmE2gBbxkkoWLzJp1Yuy15K2WXVekqYQlgBR3xmeXOCyXAjTGsyxLHnbpe8pUv9UDRVGewvoVc-ujslZ2A/s1600/towerplak.jpg" title="plaque on platform 1 at Tower Hill station" data-original-height="375" data-original-width="500"></p><p>
The plaque confirms that the stones here are a continuation of the wall seen (much more clearly) outside. What it doesn't mention is the unavoidable truth that the wall must once have continued across the tracks and platforms but is no longer here. That's because when the Circle line was constructed in 1882 the railway companies had permission to demolish 22 metres of London's wall and duly did, the Victorians never being afraid to destroy ancient heritage. <a href="https://www.ianvisits.co.uk/articles/the-tube-station-with-a-piece-of-roman-wall-in-it-28063/" target="_blank">Ian Visits</a> has a photo of navvies standing atop the offending stonework just before they bashed it through. The square hole is no recompense, plus you can't see anything if a train's in the platform, but it is a brilliantly quirky thing to find on the Underground.
</p><p>
<b>2) Round the back of the Leonardo Royal Hotel</b></p><p>
The short walk from Tower Hill station to the rear entrance of Fenchurch Street passes two hotels. The second is the Leonardo Royal, formerly the Grange, whose car port looks like it leads to a cocktail terrace and maybe some parking. Nothing's signed from the street, indeed I'd never thought to duck through before, but at the far end past the umbrellas of Leo's bar is a <a href="https://www.flickr.com/photos/dgeezer/54915634089" target="_blank">significant chunk</a> of Roman wall. 
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/54915634089" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhOfHkApAlEsIQL1lmNeyPV8_66Y0Na4z1-mqzL8-VbXyY2UXqZtJqBYp5HssoII_sakA2y-fEHN9r1AX0G4b6dPZ8bLPYrtfWJUCLLbrZOdIWeMxsgSEYw7w9x_3PSL7WF8l9KCTkTwFrcqxwqoqpm0RQKO5VcckV4MbVBkvVg5TWiE5cf1eVHPA/s1600/wallchunk.jpg" title="Roman Wall at the Leonardo Royal Hotel" data-original-height="375" data-original-width="500"></a></p><p>
The upper section has arched windows built for archers and square holes which once supported a timber platform. It's impressive of course merely medieval, part of the rebuild that occurred along much of the wall as the city grew and spread beyond its former border. To see the Roman section stand closer to the rail and look down, this because ground level then was a few metres lower than now. The telltale signs are several distinctive bands of thin red bricks, these added to strengthen and bond the structure, and which look like layers of jam in a particularly lumpy sponge. The entire segment behind the hotel is over 20m long, thus longer than the better-known chunk outside the station.
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/54915689185" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEju8ZOotr7pi26pCHNBBVVspTb4aEOPTb6UZKkJnSuhKut2qmrSHzkW9eAtzoPlVECcAVfihlku-Pv-WnFRay98AKWK559c6u0NBtYOw_TUlIiYBvBJeCuHGBiBaeeE2X4y6-bxAxiZxal-RW3j0VcdwJQo40XUkGygne8xE1BxRUfhDCPfvY_KfQ/s1600/sideon.jpg" title="Roman Wall at the Leonardo Royal Hotel" data-original-height="375" data-original-width="500"></a></p><p>
Perhaps the best thing about this bit of wall is that you can walk through it. A couple of steps have been added on each side allowing passage through a <a href="https://www.flickr.com/photos/dgeezer/54915689185" target="_blank">low medieval arch</a>, all marked with anachronistic trip hazard markings. If steps aren't your thing you can also pass round the end of the wall on the flat. Round the other side are a glum alley and a staff back-entrance, also an exit into a separate backstreet past a sign that says </p><span>PRIVATE No Public Right Of Way Beyond This Point Entry At Your Own Risk Absolutely No Liability Is Accepted For Any Reason Whatsoever</span>. Stuff that, there's an actual Roman Wall back here.
<p>
<b>3) From a cafe terrace</b></p><p>
I've written about <a href="https://citywallvinestreet.org/" target="_blank">The City Wall at Vine Street</a> before, a free attraction opened in 2023 beneath a block of student flats. <a href="https://diamondgeezer.blogspot.com/2024/06/the-city-wall-at-vine-street.html" target="_blank">Last time</a> I had to battle the Procedural Curmudgeon to gain admittance but I'm pleased to say they've since loosened up and you can now simply gesture at the door, walk in and give your first name to a flunkey with a tablet. He rattled through the key information with all the practised enthusiasm of a call centre employee dictating terms and conditions, then sent me off down the stairs. 
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/53818147256" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiuwmvZBL72Y2RH1oJbXI6X8voRbaBoBxSycp1GrQxmqn58fz5CNp2wn4pD6GiYTFkbiguBkp4wHrLCEjxMif0QpDUqdoQRyap584g3_u2nHWTy_M0UKtZTaajI6oYcO-GR1al51KXCBokDiri19tIeERjAOdq_9aj8fJuovmeFMasRywvfeA37FA/s1600/postun.jpg" title="The City Wall at Vine Street" data-original-height="375" data-original-width="500"></a></p><p>
Two walls are filled with finds from the excavations, including an AD 70s coin and the bones of a 1760s cat. Nobody's quite sure how the ancient Greek tombstone ended up here, given it predates Londinium, but it has pride of place in a central glass case. The 5-minute historical animation is pretty good too, assuming you can read quite fast. But the main draw is the multi-layered towering <a href="https://www.flickr.com/photos/dgeezer/53818147256" target="_blank">remnant of wall</a> which here has the benefit of being properly illuminated and protected from the elements. The protruding lower section (which looks much too clean to be so very old) is all that remains of an original postern, and is also unique because all the other towers elsewhere round the City are merely medieval.
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/53818480299" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiOHW9y0b7sBcLDfZqR-iRt9buQoLX2YhZzOAr7_GJ-q2jj4bVSZbnPg2Ycw61afbEbnkbdaPytwRnOD-mdcaZSUDD9y0dfcfRlJRgQapZJVnjD0WcrR45M5wCDbhEx03jknpGUZG9omr9vsVjG7yPKSHiPnjFEsFz2imuuijhwNeBgT0e2PlZNdQ/s1600/romcafe.jpg" title="Senzo cafe, Vine Street" data-original-height="375" data-original-width="500"></a></p><p>
What's weird is that this large basement space is <a href="https://www.flickr.com/photos/dgeezer/53818480299" target="_blank">overlooked</a> by a balcony scattered with small tables at which sit students and businesspeople consuming coffee and all-day brunch. The baristas operate from the cafe upstairs but any food comes from a small kitchen down below, which has the unnerving side effect that while you're wandering around what looks like a museum it smells like an office canteen. If you choose to be tempted by a cappuccino and smashed avocado on your way out you can enjoy extra time with the Roman wall, or indeed skip the walkthrough altogether and focus only on refreshment with an absolutely unique view. I recommend a proper visit though... the visitors book awaits your praise. 
</p><p>
<b>4) At the rear of a car park</b></p><p>
This is amazing on many levels, the main level being subterranean. After WW2 so much of the City was in ruins that planners drove a new dual carriageway through the Aldersgate area and called it London Wall. They believed cars were the future and to that end hid a linear <a href="https://www.cityoflondon.gov.uk/services/parking/car-parks/london-wall-car-park" target="_blank">car park</a> directly underneath the new road. It's very narrow, very long and pretty grim, indeed precisely the kind of filming location you'd expect a throwback crime thriller to use for a shoot-out or kidnapping. Cars <a href="https://osm.org/go/euu4uYiF9--?m=" target="_blank">enter</a> down a short spiral ramp and pedestrians through a grubby side door, and the numbered concrete catacombs stretch on and on for almost 400 metres. Keep walking past the white vans, Range Rovers and the attendant's cabin, trying not to attract too much attention, and almost at the far end is... blimey.
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/54916433010" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj1ubdrJlo7E5qTThn8ioGGvtd8VfNJzljYY3JSr4X4dIE_VC8QPU_kMpQendZjevqSVSHOKJ4DyWxLvl2WUVNkv94s0tIR2YJbzfEZUHem_AVM696E5FGesTwGOsS7HoINEcHY6uW6KwIpxGiNvcEzDwL5MBLK4VrcVFUHv6ZxjaGuwhUIyh31xQ/s1600/52park.jpg" title="London Wall car park" data-original-height="375" data-original-width="500"></a></p><p>
You can't park in <a href="https://www.flickr.com/photos/dgeezer/54916433010" target="_blank">bays 52 and 53</a> because they're full of Roman remains. A substantial <a href="https://www.flickr.com/photos/dgeezer/54916349423" target="_blank">chunk of wall</a> slots in diagonally beneath the joists and pillars, tall enough to incorporate two separate bands of red bricks. It looks quite smooth up front but fairly rubbly round the back, also much thicker at the base than at the top. Obviously it's very risky to have a <a href="https://historicengland.org.uk/listing/the-list/list-entry/1018885?section=comments-and-photos" target="_blank">scheduled ancient monument</a> in a car park so protective concrete blocks have been added to make sure nobody reverses into the stonework by mistake. More recently a glass screen has been added at one end, branded 'City of London' so you know who to thank, but the other end remains accessible for now (not that you should be stepping in or even touching it).
</p><p>
<a href="https://www.flickr.com/photos/dgeezer/54916349423" target="_blank"><img src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj0X3EucV9TL89yW51xa9ys4JupOSlmswxcSVXevlp0nvAfy4oNu5VmgJIJvtdYuofmNNe73kXxYZOgaLKRLwPCt83_z_sr2Ax9JFA8j3qOfWYCLI7mrNJP71UpsjNBqCcZ4AqCUCcb6GcBjR0zOGs6jac3ni5HmwqvQbk50yec9y1kD9bjZ9RoiA/s1600/carpak.jpg" title="London Wall car park" data-original-height="375" data-original-width="500"></a></p><p>
It's the contrasts that I found most incongruous. A relic from Roman times penned inbetween a speed hump and a futile pedestrian crossing. A fortification from the 3rd century beside an electric van built last year. A defensive structure that helped see off the Peasants Revolt beside a poster warning what to do in the event of fire. A boundary wall once an intrinsic part of the capital now underground illuminated by strip lights. And all this at the very far end of an oppressive bunker preserved for the benefit of hardly any eyes in a parking facility only a few know to use. Sure you can see chunks of Roman wall all around the City, even from a tube platform, hotel terrace or cafe. But the oddest spot may well be here in the London Wall car park, should you ever have the balls to take a look.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Heroku Support for .NET 10 (104 pts)]]></title>
            <link>https://www.heroku.com/blog/support-for-dotnet-10-lts-what-developers-need-know/</link>
            <guid>45893646</guid>
            <pubDate>Tue, 11 Nov 2025 22:18:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.heroku.com/blog/support-for-dotnet-10-lts-what-developers-need-know/">https://www.heroku.com/blog/support-for-dotnet-10-lts-what-developers-need-know/</a>, See on <a href="https://news.ycombinator.com/item?id=45893646">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="f5128c8" data-element_type="widget" data-widget_type="theme-post-content.default">
					<p>It‚Äôs that time of year for .NET when we get a new major version and a bunch of exciting features. <a href="https://dotnetconf.net/">.NET Conf 2025</a> kicked off earlier today, bringing with it the release of <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-10/">.NET 10</a>, as well as ASP.NET Core 10, C# 14, and F# 10. Congrats (and a big thank you) to the .NET team and everyone who helped get .NET 10 out the door.</p>
<p>At Heroku, we believe you should be able to use language and framework releases when they launch, and we prepare accordingly. You can <strong>now build and run .NET 10 apps on Heroku</strong>, with buildpack support for new SDK features like <strong>file-based apps</strong>, <strong><code>.slnx</code> solution files</strong>, and more.</p>

<h2>Migration and support timelines</h2>
<p>This year‚Äôs release is significant because .NET 10 is the new <strong>Long Term Support (LTS)</strong> release, which will be supported for three years. This extended support, including regular updates and security patches, makes it the best release for businesses and developers to build on and migrate to, offering a stable foundation with access to the latest features.</p>
<p>With .NET 10 now available, the clock is ticking on previous versions. Both .NET 8 and .NET 9 will reach End of Support on November 10, 2026. In other words, now is a good time to start planning your migration.</p>
<p>We will continue to support .NET 8 and .NET 9 with consistent, timely updates alongside .NET 10. Our .NET support follows the <a href="https://devcenter.heroku.com/articles/dotnet-heroku-support-reference#net-versions">official .NET support policy</a>, and we are fully committed to providing a stable and secure platform for your .NET applications.</p>
<p>Let‚Äôs dive into using .NET 10 on Heroku today!</p>
<h2>Zero-config deployment with .NET 10 file-based apps</h2>
<p>One of the most exciting features in .NET 10 is <a href="https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotnet-10/sdk#file-based-apps-enhancements">file-based apps</a> ‚Äì .NET applications defined in a single C# file without project or solution files, making it easier than ever to deploy .NET apps to Heroku.</p>
<p>For example, here‚Äôs a complete ASP.NET Core 10 web application, <code>HelloHeroku.cs</code>:</p>
<pre><code>// Use the new #sdk directive to pull in the ASP.NET Core SDK
#:sdk Microsoft.NET.Sdk.Web

var builder = WebApplication.CreateBuilder(args);
var app = builder.Build();  
  
app.MapGet("/", () =&gt; "Hello from .NET 10 on Heroku!");

app.Run();
</code></pre>
<p>When you push this to Heroku, the platform detects the <code>*.cs</code> file and uses the .NET buildpack. Since there are no solution or project files, the buildpack treats it as a file-based app, installs the latest .NET SDK, builds and publishes the app, detects and configures it as a web application, and deploys it to serve traffic.</p>
<p><iframe src="https://player.vimeo.com/video/1135900665?h=142beadaa1&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share" referrerpolicy="strict-origin-when-cross-origin" title=".NET 10 - File Based App Demo"></iframe></p>
<p><br>
The result is a simple, zero-config experience to get you started quickly, ideal for prototyping and developers new to .NET. And there‚Äôs more coming ‚Äì check out the <a href="https://github.com/dotnet/sdk/issues/48170">.NET SDK repository</a> to see what the .NET team is working on!</p>
<p>To learn more about what you can do with file-based apps on Heroku today, see our <a href="https://devcenter.heroku.com/articles/dotnet-behavior-in-heroku#file-based-apps">Dev Center documentation</a>.</p>
<h2>SLNX: A modern solution for a modern .NET</h2>
<p>For decades, .NET developers have used <code>.sln</code> solution files, a proprietary format introduced in 2002 for Visual Studio. Unlike .NET itself, they haven‚Äôt changed much since. In a step towards modernization, the .NET 10 SDK is making SLNX the default format. Heroku ensures a seamless deployment experience by fully supporting both formats.</p>
<pre><code>&lt;solution&gt;
    &lt;project path="MyApp\MyApp.csproj"&gt;&lt;/project&gt;
&lt;/solution&gt;
</code></pre>
<p><code>*.slnx</code> files are easier to read and edit, less likely to cause merge conflicts, and support a wider range of workflows and environments, from Linux shells to Visual Studio on Windows. To migrate existing <code>.sln</code> files, run <code>dotnet solution migrate</code> or see the <a href="https://devblogs.microsoft.com/visualstudio/new-simpler-solution-file-format/">.NET blog announcement</a> for more details.</p>
<h2>Heroku CI and the Microsoft Testing Platform</h2>
<p>The .NET 10 SDK integrates the <a href="https://learn.microsoft.com/en-us/dotnet/core/testing/microsoft-testing-platform-intro?tabs=dotnetcli">Microsoft Testing Platform (MTP)</a> directly in the <code>dotnet test</code> command. Since Heroku CI runs <code>dotnet test</code> by default, your test suite works out of the box after you migrate your apps.</p>
<p>For more control over the test setup and execution, you can <a href="https://devcenter.heroku.com/articles/heroku-ci#specify-custom-test-commands-scripts">specify custom test commands</a> in your <code>app.json</code>.</p>
<h2>Ready to migrate? We‚Äôre here to help</h2>
<p>To support your .NET 10 migration, we‚Äôve updated all our documentation and resources:</p>
<ul>
<li>The <a href="https://github.com/heroku/dotnet-getting-started">.NET Getting Started app</a> now runs on .NET 10.</li>
<li>The <a href="https://devcenter.heroku.com/articles/aspnetcore-app-configuration">ASP.NET Core configuration article</a> includes new guidance for ASP.NET Core 10, including migration away from the now-obsolete <code>IPNetwork</code> and <code>ForwardedHeadersOptions.KnownNetworks</code> APIs (<a href="https://learn.microsoft.com/en-us/dotnet/core/compatibility/aspnet-core/10/ipnetwork-knownnetworks-obsolete">learn more</a>) often used to integrate with Heroku‚Äôs router.</li>
<li>Apps currently using .NET 10 RC builds on Heroku (with <code>TargetFramework</code> set to <code>net10.0</code>) will automatically be built with the stable .NET 10 release on the next <code>git push</code>. You can pin to specific SDK versions using a <a href="https://devcenter.heroku.com/articles/dotnet-heroku-support-reference#advanced-net-sdk-version-configuration"><code>global.json</code> file</a>.</li>
</ul>
<p>For teams migrating from earlier versions, the <a href="https://learn.microsoft.com/en-us/dotnet/core/compatibility/10.0">.NET 10 breaking changes documentation</a> covers important upgrade considerations.</p>
<h2>Get started today</h2>
<p>We can‚Äôt wait to see what you build with .NET 10 on Heroku. From new features like file-based apps to the stability of an LTS release, this is a great time to be a .NET developer.</p>
<p>Check out our updated <strong><a href="https://devcenter.heroku.com/articles/getting-started-with-dotnet">Getting Started with .NET on Heroku</a></strong> guide and please feel free to reach out with any questions or feedback.</p>

				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I didn't reverse-engineer the protocol for my blood pressure monitor in 24 hours (311 pts)]]></title>
            <link>https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/</link>
            <guid>45893095</guid>
            <pubDate>Tue, 11 Nov 2025 21:25:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/">https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/</a>, See on <a href="https://news.ycombinator.com/item?id=45893095">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                <p>Yesterday after receiving my yearly flu vaccine at the pharmacy I was offered a blood pressure test, which reported a reading that made the young pharmacist who had just given me my vaccine a bit worried.</p>
<p>Off the back of this she offered me a 24 hour study, and then strapped a cuff to my arm plumbed into a little device which I had to wear in a little caddy - the cuff would inflate every 30 minutes during the day and every 60 minutes during the night, and then tomorrow I would bring it back for analysis.</p>
<p>"Can I read the measurements?" I asked, as it was being strapped to me.</p>
<p>"Oh, no, that will just stress you out. We turn that off". Fair enough.</p>
<p>Thing is, this device had a little micro-USB port on the side.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/revealing-the-port.jpg" alt="A blood pressure monitor with a flap being held back, revealing a micro-USB port"></p><p>A blood pressure monitor with a flap being held back, revealing a micro-USB port</p>
<h2>Doing things the proper way</h2>
<p>I had started researching the device - a <a href="https://www.microlife.com/professional-products/watchbp-o3/watchbp-o3-ambulatory-2g">Microlife WatchBP O3</a> - before I got out of the chemist, and once I'd got back to the office I downloaded <a href="https://www.microlife.com/support/ambulatory-support">the software</a> that's freely available to interact with it, setting up a Bottles instance to run the software since I don't (knowingly) have a Windows machine within 100 metres of me.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/installing-watchbp-analyzer-in-bottles.png" alt="Installing WatchBP Analyzer in Bottles"></p><p>Installing WatchBP Analyzer in Bottles</p>
<p>Unfortunately it didn't seem to be able to access the device, and I had no clue why. In Linux it was just presenting as a standard <code>hidraw</code> device:</p>
<pre><code>[33301.736724] hid-generic 0003:04D9:B554.001E: hiddev96,hidraw1: USB HID v1.11 Device [USB HID UART Bridge] on usb-0000:c5:00.0-1/input0
</code></pre>
<p><em>Fine</em>, I'll install windows.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/send-diagnostic-data-to-microsoft.png" alt="Microsoft wants my diagnostic data"></p><p>Microsoft wants my diagnostic data</p>
<p>After dodging around Microsoft's idea of UX, and then forwarding the USB device to the VM (I used <a href="https://apps.gnome.org/en-GB/Boxes/">Gnome Boxes</a> for this, works nicely), I finally got to see WatchBP Analyzer with the data downloaded from the device.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/watchbp-analyzer.png" alt="WatchBP Analyzer with my first three measurements"></p><p>WatchBP Analyzer with my first three measurements</p>
<p>But I don't want to open a Virtual Machine running Windows to see this data, and anyway - I'm pretty sure that reverse-engineering this will be good for my blood pressure.</p>
<h2>Sniffing the traffic</h2>
<p>Since I'm running this in a Virtual Machine I can just rely on Wireshark in Linux to get the traffic between the host and the device. <code>usbmon</code> is already installed and we know that the device is on Bus 3, so we can select usbmon3 on startup and start capturing.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/wireshark-initial.png" alt="Wireshark with the initial capture"></p><p>Wireshark with the initial capture</p>
<p>I'm very much out of my depth at this point but, being <a href="https://www.sciencefocus.com/science/could-i-land-a-plane-in-an-emergency">one of those who could land a plane in an emergency</a> (why would you talk yourself out of it?!) I decided to crack on regardless. I know that the interesting stuff is sent after I press "Download", and I know that <em>something</em> in there is gonna say <em>"my blood pressure is 137/113"</em> - so let's look for that. Just convert to show bytes as decimal and..</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/hex-blood-pressure.png" alt="My blood pressure seems to be encoded in this packet"></p><p>My blood pressure seems to be encoded in this packet</p>
<p>..that looks like a blood pressure! Let's copy that out as hex:</p>
<pre><code>05 0a 89 71 43 9b
</code></pre>
<p>I'm not sure if this is "valid" HID Data (Wireshark seems convinced that only the first byte is the Vendor Data, with the rest being padding) but it seems like the data is being sent in 32-byte "chunks", of which the first byte tells you the number of significant (SIG) following bits in the chunk (I deleted the rest - all zeroes - for clarity). The third byte is my <em>Systolic</em> blood Pressure (SYS), the fourth is my <em>Diastolic</em> blood pressure (DIA), and the fifth is my heart rate (HR) - no clue what the second or last byte is, but let's find all other bytes with my blood pressure in them (in decimal this time, because I can't read hex without help):</p>
<pre><code>SIG ??? SYS DIA  HR ??? ??? ???
  5  10 137 113  67 155
  5   0 132  86  68 155
  6   0 126  84  82 155  83
  6  10 128  80  61 155  83
  7   0 148  93  65 155  83  64
  7   0 121  92  74 155  83  94
  7   0 123  83  65 155  83  95
  7   0 123  79  78 155  83 129
</code></pre>
<p>Hmm. So we're still looking for the Oscillometric signal peak pressure (OPP)as well as some timestamps (we can calculate Mean arterial pressure - MAP - as <code>(2*DIA+SYS)/3</code>, according to <a href="https://www.microlife.com/support/software-professional-products/watchbp-analyzer-support">the manual</a>, and Pulse Pressure (PP) is just <code>SYS-DIA</code>). We can see the OPP in the packets that come after each of those above, but they don't seem to consistently come in on the same line:</p>
<pre><code> 10  82  195   80 *121    0    0    0    0    0    0
 10  82  223   80  *95    0    0    0    0    0    0
  9   1   80  *90    0    0    0    0    0    0
  9  35   80  *86    0    0    0    0    0    0
  8  80 *103    0    0    0    0    0    0
  8  80 *106    0    0    0    0    0    0
  8  80  *90    0    0    0    0    0    0
 10  80  *88    0    0    0    0    0    0   29  251
</code></pre>
<p>Oh. Maybe if I stick them together?</p>
<pre><code>??? SYS DIA  HR ??? ??? ??? ??? OPP ??? ??? ??? ??? ??? ??? ??? ???
 10 137 113  67 155  82 195  80 121   0   0   0   0   0   0
  0 132  86  68 155  82 223  80  95   0   0   0   0   0   0
  0 126  84  82 155  83   1  80  90   0   0   0   0   0   0
 10 128  80  61 155  83  35  80  86   0   0   0   0   0   0
  0 148  93  65 155  83  64  80 103   0   0   0   0   0   0
  0 121  92  74 155  83  94  80 106   0   0   0   0   0   0
  0 123  83  65 155  83  95  80  90   0   0   0   0   0   0
  0 123  79  78 155  83 129  80  88   0   0   0   0   0   0  29 251
</code></pre>
<p>Right, timestamps. I first guessed that the four populated contiguous bytes between <code>HR</code> and <code>OPP</code> are a 32-bit unix timestamp, but that would make the first one <code>9B52C350</code>; either <code>Jul 29 2052</code> or <code>Dec 08 2012</code> depending on which endianness the protocol is into. The 8 readings we have here are all from <code>November 10th</code>, at <code>11:03</code>, <code>11:31</code>, <code>12:01</code>, <code>12:35</code>, <code>13:00</code>, <code>13:30</code>, <code>13:31</code> and <code>14:01</code>, which isn't.. isn't that.</p>
<p>But note that the number in the 6th column flips from <code>82</code> to <code>83</code> when we switch from AM to PM - that's something, and when it does the 7th column resets. And hey - <code>1</code>, <code>35</code>, <code>64</code>, <code>94</code>, <code>95</code>.. that seems dangerously close to <code>12:01</code>, <code>12:35</code>, <code>13:00</code>, <code>13:30</code> and <code>13:31</code> if you were just to count the minutes. What's going on?</p>
<h2>Deadlines and dead ends</h2>
<p>I tried feeding a lot of this into various Als (Kagi gives you access to a few with a nice interface) and I found that they mostly were stupid in ways that made me think. A few times I thought they had <em>"cracked the case"</em> but actually they just made me waste time. But they did remind me e.g. of endianness, so I did get a bit out of them.</p>
<p><img src="https://james.belchamber.com/articles/blood-pressure-monitor-reverse-engineering/images/hex-is-hard-for-llms.png" alt="Kimi K2 demonstrating creativity with numbers"></p>
<p>I also spent quite a bit of time trying to write some Python that emulated the initial handshake and download button of the interface so that it could push out the data as a stream instead of me having to wrestle it out of Wireshark - again, Al had a habit of giving me incorrect code (although it did turn me on to <a href="https://pypi.org/project/hid/">pyhidapi</a>).</p>
<p>But ultimately I had a deadline, and I had to return the device even though I wanted to spend more time with it. Possibly for the best - while it did give me some reverse engineering practice (which it turns out I really enjoy), I should do some work instead of procrastinating.</p>
<p>My final lesson was a new word - <em>Normotension</em>, normal blood pressure - and a new phrase - <em>White Coat Hypertension</em>, the phenomena of high blood pressure in a clinical setting. Turns out that when you check someone's blood pressure after giving them an injection, it's higher than normal.</p>
<p>I don't think I'd recommend getting your blood pressure tested after your next flu jab. But then, I'm not a doctor.</p>

                <br>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X5.1 solar flare, G4 geomagnetic storm watch (371 pts)]]></title>
            <link>https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html</link>
            <guid>45893004</guid>
            <pubDate>Tue, 11 Nov 2025 21:18:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html">https://www.spaceweatherlive.com/en/news/view/593/20251111-x5-1-solar-flare-g4-geomagnetic-storm-watch.html</a>, See on <a href="https://news.ycombinator.com/item?id=45893004">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="SWL_Page">
                    <div>
                        
                        
<h3>X5.1 solar flare, G4 geomagnetic storm watch</h3><p>Tuesday, 11 November 2025 19:07 UTC</p><p><img src="https://www.spaceweatherlive.com/images/news/593-header.jpg" loading="lazy" alt="X5.1 solar flare, G4 geomagnetic storm watch" width="900" height="450"></p><p>Here she blows! Sunspot region 4274 produced its strongest solar flare thus far since it appeared on the east limb and the sixth strongest solar flare of the current solar cycle. An impressive long duration and highly eruptive X5.1 (R3-strong) solar flare peaked this morning at 10:04 UTC.</p><p>It became quickly clear that the eruption would be followed by an impressive coronal mass ejection (CME). The resulting coronal wave following the solar explosion as well as the coronal dimming observed as the CME was propelled into space were of a spectacular magnitude as can be seen in the animation below provided by <a href="https://x.com/halocme" target="_blank" rel="noopener">halocme</a>.</p><blockquote data-media-max-width="560"><p dir="ltr" lang="en">Another eruption from AR12474, associated with an X5.1 flare. It has become a full halo CME. I am truly impressed by how fast and global this coronal wave is. The CME will arrive on November 13, but because of earlier CMEs it will be challenging to isolate the ICME from this. <a href="https://t.co/H6eNjzQUGz">pic.twitter.com/H6eNjzQUGz</a></p>‚Äî Halo CME (@halocme) <a href="https://twitter.com/halocme/status/1988253760179548649?ref_src=twsrc%5Etfw">November 11, 2025</a></blockquote><p>Taking a look at coronagraph imagery provided by GOES-19 CCOR-1 we see the gorgeous fast halo coronal mass ejection as it propagates away from the Sun. It doesn't take a rocket scientist to come to the conclusion that this plasma cloud of course has an earth-directed component and it is pretty clear that this will be a strong impact when it arrives at our planet. This rightfully so prompted the NOAA SWPC to issue a G4 or greater geomagnetic storm watch for tomorrow as the cloud could impact our planet as early as 16 UTC on 12 November. Not only is the CME fast but it will also travel trough an area with high ambient solar wind speed and low density thanks to two other CMEs released earlier by this region. More about that below.</p><figure><img src="https://www.spaceweatherlive.com/images/news/2025/593-cme.gif"><figcaption>Coronal mass ejection launched during today's X5.1 solar flare as captured by the coronagraph from GOES-19.</figcaption></figure><p>If the solar wind and interplanetary magnetic field values at Earth are favorable this could result in a geomagnetic storm which is strong enough for aurora to become visible from locations as far south as northern France, Germany, Ukraine, Switzerland and Austria. In the US it could become visible as far south as Nevada and Arkansas. No guarantees of course, this is space weather we are talking about but be sure to download the SpaceWeatherLive app to your mobile device, turn on the alerts and keep an eye on the solar wind data from ACE and DSCOVR!</p><p>We also want to remind you that we still have two coronal mass ejections on their way to Earth. These are not as impressive as this X5.1 CME but these two plasma clouds will likely arrive within the next 6 to 18 hours. This is a tricky one as they could arrive as one impact or two impacts close intill each other. More information in <a href="https://www.spaceweatherlive.com/en/news/view/592/20251110-x1-2-solar-flare-with-earth-directed-cme.html" target="_blank" rel="noopener">yesterday's news</a>.</p><div> <p><em>Thank you for reading this article! Did you have any trouble with the technical terms used in this article? Our help section is the place to be where you can find in-depth <a href="https://www.spaceweatherlive.com/en/help.html">articles</a>, a <a href="https://www.spaceweatherlive.com/en/faq.html">FAQ</a> and a list with common <a href="https://www.spaceweatherlive.com/en/acronyms-and-abbreviations.html">abbreviations</a>. Still puzzled? Just post on our <a href="https://community.spaceweatherlive.com/">forum</a> where we will help you the best we can! <span> Never want to miss out on a space weather event or one of our news articles again? Subscribe to our <a href="https://www.spaceweatherlive.com/en/mailinglist.html">mailing list</a>, follow us on <a href="https://twitter.com/_SpaceWeather_">Twitter</a> and <a href="https://www.facebook.com/SpaceWeatherLive/">Facebook</a> and download the SpaceWeatherLive app for <a href="https://play.google.com/store/apps/details?id=com.spaceweatherlive.app">Android</a> and <a href="https://itunes.apple.com/us/app/spaceweatherlive/id1435501021?mt=8">iOS</a>! </span> </em> </p></div>
                        
                    </div>
                                            <div>
                                                          <div>  <p>A lot of people come to SpaceWeatherLive to follow the Solar activity or if there is a chance to see the aurora, but with more traffic comes higher costs to keep the servers online. If you like SpaceWeatherLive and want to support the project you can choose a subscription for an ad-free site or consider a donation. With your help we can keep SpaceWeatherLive online!</p>  <p><a href="https://shop.spaceweatherlive.com/" target="_blank"><img src="https://www.spaceweatherlive.com/images/SWL_Shop.png" alt="Support SpaceWeatherLive with our merchandise" width="600" height="200"></a> </p> <p><a href="https://shop.spaceweatherlive.com/" target="_blank"> <b>Check out our merchandise</b></a></p></div><div><table><thead><tr><th colspan="2">Spotless days</th></tr></thead><tbody><tr><td>Last spotless day</td><td>2022/06/08</td></tr></tbody></table><table><thead><tr><th colspan="2">Monthly mean Sunspot Number</th></tr></thead><tbody><tr><td>October 2025</td><td>114.6 <span> -15.2</span></td></tr><tr><td>November 2025</td><td>91.9 <span> -22.7</span></td></tr><tr><td>Last 30 days</td><td>96.2 <span> -34.6</span></td></tr></tbody></table><h4>This day in history*</h4><div><table><thead><tr><th></th><th></th><th>Dst</th><th>G</th></tr></thead><tbody><tr><td>1</td><td><a href="https://www.spaceweatherlive.com/en/archive/2004/11/11.html">2004</a></td><td>-106</td><td><span>G1</span></td></tr><tr><td>2</td><td>1981</td><td>-78</td><td><span>G2</span></td></tr><tr><td>3</td><td>1974</td><td>-72</td><td><span>G3</span></td></tr><tr><td>4</td><td><a href="https://www.spaceweatherlive.com/en/archive/2013/11/11.html">2013</a></td><td>-68</td><td><span>G1</span></td></tr><tr><td>5</td><td>1991</td><td>-64</td><td><span>G1</span></td></tr></tbody></table><p>*since 1994</p></div></div>                           </div>
                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Collaboration sucks (435 pts)]]></title>
            <link>https://newsletter.posthog.com/p/collaboration-sucks</link>
            <guid>45892394</guid>
            <pubDate>Tue, 11 Nov 2025 20:27:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newsletter.posthog.com/p/collaboration-sucks">https://newsletter.posthog.com/p/collaboration-sucks</a>, See on <a href="https://news.ycombinator.com/item?id=45892394">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2y1b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2y1b!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 424w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 848w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2y1b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg" width="1456" height="1048" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1048,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:801531,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://newsletter.posthog.com/i/178280989?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2y1b!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 424w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 848w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!2y1b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fff09f674-3296-47aa-9ee4-d25de75728fa_1456x1048.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>‚ÄúIf you want to go fast, go alone; if you want to go far, go together‚Äù</p><p>This phrase will slowly kill your company and I‚Äôm here to prove it.</p><p>Imagine you are driving a car. It‚Äôs often useful to have someone give you directions, point out gas stations, and recommend stops for snacks. This is a helpful amount of collaboration.</p><p>An unhelpful amount of collaboration is getting out of your car to ask pedestrians if they like your car, swapping drivers every 10 minutes, or having someone constantly commenting on your driving.</p><p>In the first scenario, you get the right amount of feedback to get to your destination as fast as possible. In the second, you get more feedback, but it slows you down. You run the risk of not making it to the place you want to go.</p><p>The second scenario is also the one most startups (or companies, really) end up in because of ‚ú® collaboration ‚ú®.</p><p>As PostHog grows, I‚Äôve seen more and more collaboration that doesn‚Äôt add value or adds far too little value for the time lost collaborating. So much so we made ‚Äúcollaboration sucks‚Äù the topic of the week during a recent company all hands.</p><p><span>‚ÄúYou‚Äôre the driver‚Äù is a </span><a href="https://posthog.com/handbook/values?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">key value</a><span> for us at PostHog. We aim to hire people who are great at their jobs and get out of their way. No deadlines, minimal coordination, and no managers telling you what to do.</span></p><p><span>In return, we ask for extraordinarily high ownership and the ability to get a lot done by </span><em>yourself.</em><span> Marketers ship code, salespeople answer technical questions without backup, and </span><a href="https://posthog.com/blog/what-is-a-product-engineer?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">product engineers</a><span> work across the stack.</span></p><p>This means there is almost always someone better at what you are doing than you are. It is tempting to get them, or anybody really, involved and ‚ú® collaborate ‚ú®, but collaboration forces the driver to slow down and explain stuff (background, context, their thinking).</p><p>This tendency reveals itself in a few key phrases:</p><ul><li><p>‚ÄúCurious what X thinks‚Äù</p></li><li><p>‚ÄúWould love to hear Y‚Äôs take on this‚Äù</p></li><li><p>‚ÄúWe should work with Z on this‚Äù</p></li></ul><p><span>This </span><em>sometimes</em><span> leads to valuable insights, but </span><em>always</em><span> slows the driver down. It erodes their motivation, confidence, and effectiveness, and ultimately leads us to ship less.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!MoTP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!MoTP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 424w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 848w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 1272w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!MoTP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png" width="714" height="328.5576923076923" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;normal&quot;,&quot;height&quot;:670,&quot;width&quot;:1456,&quot;resizeWidth&quot;:714,&quot;bytes&quot;:463445,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://newsletter.posthog.com/i/178280989?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!MoTP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 424w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 848w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 1272w, https://substackcdn.com/image/fetch/$s_!MoTP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7cb9ef7c-6958-481c-8a93-209cafc1912f_2044x940.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Everyone is to blame.</p><ul><li><p><span>People want to be helpful. For example, when someone posts their work-in-progress in Slack, others feel obliged to give feedback because we have a </span><a href="https://posthog.com/handbook/people/feedback?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">culture of feedback</a><span>.</span></p></li><li><p>On the flip side, people don‚Äôt ask for feedback from specific people because it doesn‚Äôt feel inclusive, even though it would help.</p></li><li><p>People aren‚Äôt specific enough about what feedback they need. This creates more space for collaboration to sneak in. A discussion about building a specific feature can devolve into reevaluating the entire product roadmap if you let it.</p></li><li><p>When someone has a good idea, the response often defaults to ‚Äúlet‚Äôs discuss‚Äù rather than ‚Äúok, do it.‚Äù As proof, we have 175 mentions of ‚Äúlet‚Äôs discuss‚Äù in Slack.</p></li><li><p><span>People just want to talk about stuff because they </span><s>are too busy</s><span> can‚Äôt be bothered to act on it. We drift from our ideal of a pull request to an issue/RFC to Slack (we are mostly here) to ‚Äúlet‚Äôs discuss‚Äù.</span></p></li><li><p>It‚Äôs not clear who the owner is (or no one wants to own what‚Äôs being discussed).</p></li><li><p>It is annoying, but sometimes a single person can‚Äôt ship certain things front to back to a high-enough quality and we can‚Äôt just ship and iterate. We can fix broken code, but we can‚Äôt resend a newsletter.</p></li></ul><p>So if collaboration is your enemy, how do you defeat it? Here‚Äôs what we say:</p><ul><li><p>Default to shipping. Pull requests &gt; issues &gt; Slack messages.</p></li><li><p>Every time you see ‚ú® collaboration ‚ú® happening, speak up and destroy it. Say ‚Äúthere are too many people involved. X, you are the driver, you decide.‚Äù (This is a great way to make friends btw).</p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!CtaP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!CtaP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 424w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 848w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!CtaP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg" width="1456" height="870" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:870,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;How to make friends and crush collaboration&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="How to make friends and crush collaboration" title="How to make friends and crush collaboration" srcset="https://substackcdn.com/image/fetch/$s_!CtaP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 424w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 848w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!CtaP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd08aa557-77bd-4521-8e30-ab03faf7c7db_1600x956.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Graphic design is my passion</figcaption></figure></div><ul><li><p>Tag who you specifically want input from and what you want from them, not just throw things out there into the void.</p></li><li><p>Prefer to give feedback after something has shipped (but before the next iteration) rather than reviewing it before it ships. Front-loading your feedback can turn it into a quasi-approval process.</p></li><li><p><span>If you are a team lead, or leader of leads, who has been asked for feedback, consider being more </span><a href="https://www.youtube.com/shorts/DjvVN4Vp_r0" rel="">you can just do stuff</a><span>.</span></p></li><li><p>When it‚Äôs your thing, you are the ‚Äúinformed captain.‚Äù Listen to feedback, but know it‚Äôs ultimately up to you to decide what to do, not the people giving feedback.</p></li></ul><p><span>Unfortunately for me, not all collaboration can be rooted out, and even I will admit that some collaboration is useful. </span><a href="https://www.linkedin.com/in/ianvanagas/" rel="">Ian</a><span> and </span><a href="https://www.linkedin.com/in/andyvandervell/" rel="">Andy</a><span> edited this newsletter after all. </span></p><p><span>The point is, if you aren‚Äôt actively attempting to collaborate less, you are probably collaborating too much by default and hurting your ability to go far, fast.</span><em><p><span>Words by </span><a href="https://www.linkedin.com/in/wololo/" rel="">Charles Cook</a><span>, who also hates sparkling water, presumably because the bubbles are too collaborative.</span></p></em></p><ul><li><p><strong><a href="https://posthog.com/careers/ai-product-engineer?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">AI Product Engineer</a></strong><span> working on PostHog AI, LLM Analytics or Array teams.</span></p></li><li><p><span>Backend Engineer for </span><strong><a href="https://posthog.com/careers/backend-engineer-feature-flags?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Feature Flags</a></strong><span> and </span><strong><a href="https://posthog.com/careers/backend-engineer-ingestion?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Ingestion</a></strong><span> teams</span></p></li><li><p><strong><a href="https://posthog.com/careers/influencer-wrangler?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Influencer Wrangler</a></strong><span> on the Marketing team</span></p></li><li><p><strong><a href="https://posthog.com/careers/yc-technical-onboarding-specialist-onsite?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">YC Technical Onboarding Specialist </a></strong><span>on the Onboarding team (San Fran based)</span></p></li><li><p><strong><a href="https://posthog.com/careers/clickhouse-operations-engineer?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">ClickHouse Operations Engineer</a></strong><span> on the ClickHouse team</span></p></li></ul><ul><li><p><strong><a href="https://posthog.com/blog/workflows-alpha?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Workflows are now in Alpha and I already broke mine</a><span> ‚Äì Sara Miteva</span></strong></p></li><li><p><strong><a href="https://notes.mtb.xyz/p/your-data-model-is-your-destiny?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Your data model is your destiny</a><span> ‚Äì&nbsp;Matt Brown</span></strong></p></li><li><p><strong><a href="https://www.dylanamartin.com/2025/11/07/spinning-plates.html?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">Spinning Plates</a><span> ‚Äì Dylan Martin</span></strong></p></li><li><p><strong><a href="https://www.youtube.com/watch?v=yKgfk8lTQuE?utm_source=posthog-newsletter&amp;utm_medium=post&amp;utm_campaign=collaboration-sucks" rel="">1000x: The Power of an Interface for Performance</a><span> (video) ‚Äì Joran Dirk Greef</span></strong></p></li></ul><div id="youtube2-bs_v-xY7Nqw" data-attrs="{&quot;videoId&quot;:&quot;bs_v-xY7Nqw&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/bs_v-xY7Nqw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microplastics: No longer a "maybe" (113 pts)]]></title>
            <link>https://ibbi.io/mp</link>
            <guid>45892340</guid>
            <pubDate>Tue, 11 Nov 2025 20:24:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ibbi.io/mp">https://ibbi.io/mp</a>, See on <a href="https://news.ycombinator.com/item?id=45892340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p><strong>It‚Äôs Already Inside Us</strong><br>
      Microplastics are not just an environmental problem affecting nature.
      They‚Äôre in our blood
      <a href="https://pubmed.ncbi.nlm.nih.gov/35367073/" target="_blank">[1]</a>, lungs
      <a href="https://pubmed.ncbi.nlm.nih.gov/35364151/" target="_blank">[2]</a>, placentas
      <a href="https://pubmed.ncbi.nlm.nih.gov/33395930/" target="_blank">[3]</a>, brains
      <a href="https://pubmed.ncbi.nlm.nih.gov/38765967/" target="_blank">[4]</a>, and breast milk
      <a href="https://www.mdpi.com/2073-4360/14/13/2700" target="_blank">[5]</a>. Every human tissue scientists have tested so far has come back
      contaminated. In diseased tissue samples of people with chronic illnesses
      (IBD
      <a href="https://pubmed.ncbi.nlm.nih.gov/34935363/" target="_blank">[6]</a>, Dementia
      <a href="https://pubmed.ncbi.nlm.nih.gov/39901044/" target="_blank">[7]</a>, heart disease
      <a href="https://pubmed.ncbi.nlm.nih.gov/38446676/" target="_blank">[8]</a>), microplastic prevalence is significantly higher than healthy tissue.
      </p><p>
      <strong>The Trajectory Is Clear</strong><br>
      Every new study finds higher microplastic concentrations in human tissue
      than the last. Most recently, we found a 50% increase in brain tissue
      microplastic prevalence over the past 8 years
      <a href="https://pubmed.ncbi.nlm.nih.gov/39901044/" target="_blank">[9]</a>. The burden on the human body is compounding: what we take in today
      stays with us for decades, and future generations are born contaminated.
      That‚Äôs not even mentioning nanoplastics, which we weren‚Äôt able to detect
      until 10 years ago
      <a href="https://pubmed.ncbi.nlm.nih.gov/38765967/" target="_blank">[10]</a>. </p><p>
      <strong>What the Mice Tell Us</strong><br>
      Mice exposed to higher doses of microplastics develop gut inflammation
      <a href="https://pubmed.ncbi.nlm.nih.gov/39265698/" target="_blank">[11]</a>, hormone disruption
      <a href="https://particleandfibretoxicology.biomedcentral.com/articles/10.1186/s12989-022-00453-2" target="_blank">[12]</a>, infertility
      <a href="https://pubmed.ncbi.nlm.nih.gov/36493639" target="_blank">[13]</a>, developmental delays
      <a href="https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2023.1254886/full" target="_blank">[14]</a>, and organ damage
      <a href="https://www.sciencedirect.com/science/article/abs/pii/S0048969720366158" target="_blank">[15]</a>. The doses they are tested with are higher than ours‚Ä¶ <em>for now</em>.
      But the global plastic load is increasing exponentially, and the gap is
      closing. </p><p>
      <strong>Humans Can‚Äôt Afford 1% of That</strong><br>
      Even a fraction of the effects we induce in mice appearing in people is a
      global health crisis. That‚Äôs not hypothetical; our tolerance for risk is
      far lower than a lab rat. Mice don‚Äôt need to perform demanding physical
      and cognitive tasks 40 hours a week to survive. The accumulation math is
      also worse: mice don‚Äôt live 80 years, don‚Äôt have pregnancies lasting nine
      months, and don‚Äôt accumulate microplastics for decades. We do.
      </p><p>
      Waiting for government intervention before acting is the same mistake we
      made with lead, asbestos, and PFAS. Let‚Äôs not do that again.
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The terminal of the future (277 pts)]]></title>
            <link>https://jyn.dev/the-terminal-of-the-future</link>
            <guid>45892191</guid>
            <pubDate>Tue, 11 Nov 2025 20:11:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jyn.dev/the-terminal-of-the-future">https://jyn.dev/the-terminal-of-the-future</a>, See on <a href="https://news.ycombinator.com/item?id=45892191">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <blockquote>
<p>Terminal internals are a mess. A lot of it is just the way it is because someone made a decision in the 80s and now it‚Äôs impossible to change.
‚Äî<a href="https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/">Julia Evans</a></p>
</blockquote>
<blockquote>
<p>This is what you have to do to redesign infrastructure. Rich [Hickey] didn't just pile some crap on top of Lisp [when building Clojure]. He took the entire Lisp and moved the whole design at once.
‚Äî<a href="https://www.destroyallsoftware.com/talks/a-whole-new-world">Gary Bernhardt</a></p>
</blockquote>
<h2 id="a-mental-model-of-a-terminal">a mental model of a terminal<a href="#a-mental-model-of-a-terminal" aria-label="Anchor link for: a-mental-model-of-a-terminal"></a>
</h2>
<p>At a very very high level, a terminal has four parts:</p>
<ol>
<li>The "<a href="https://wizardzines.com/comics/meet-the-terminal-emulator/">terminal emulator</a>", which is a program that renders a grid-like structure to your graphical display.</li>
<li>The "<a href="https://jvns.ca/blog/2022/07/20/pseudoterminals/">pseudo-terminal</a>" (PTY), which is a connection between the terminal emulator and a "process group" which receives input. This is not a program. This is a piece of state in the kernel.</li>
<li>The "shell", which is a program that leads the "process group", reads and parses input, spawns processes, and generally acts as an event loop. Most environments use <a href="https://jvns.ca/blog/2017/03/26/bash-quirks/">bash</a> as the default shell.</li>
<li>The programs spawned by your shell, which interact with all of the above in order to receive input and send output.</li>
</ol>
<p>I lied a little bit above. "input" is not just text. It also includes <a href="https://man7.org/linux/man-pages/man7/signal.7.html">signals</a> that can be sent to the running process. Converting keystrokes to signals is the job of the PTY.</p>
<p>Similar, "output" is not just text. It's a stream of <a href="https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797">ANSI Escape Sequences</a> that can be used by the terminal emulator to display rich formatting.</p>
<h2 id="what-does-a-better-terminal-look-like">what does a better terminal look like?<a href="#what-does-a-better-terminal-look-like" aria-label="Anchor link for: what-does-a-better-terminal-look-like"></a>
</h2>
<p>I do some <a href="https://jyn.dev/how-i-use-my-terminal/">weird things</a> with terminals. However, the amount of hacks I can get up to are pretty limited, because terminals are pretty limited. I won't go into all the ways they're limited, because it's been rehashed <a href="https://matklad.github.io/2019/11/16/a-better-shell.html">many</a> <a href="https://github.com/withoutboats/notty">times</a> <a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">before</a>. What I want to do instead is imagine what a better terminal can look like.</p>
<h3 id="a-first-try-jupyter">a first try: Jupyter<a href="#a-first-try-jupyter" aria-label="Anchor link for: a-first-try-jupyter"></a>
</h3>
<p>The closest thing to a terminal analog that most people are familiar with is <a href="https://docs.jupyter.org/en/latest/">Jupyter Notebook</a>. This offers a lot of cool features that are not possible in a "traditional" VT100 emulator:</p>
<ul>
<li>
<p>high fidelity image rendering</p>
<p><img src="https://jyn.dev/assets/jupyter%20bubble%20plot.png" alt="screenshot of a JupyterLite notebook. It has some python code in a cell that generates a matplotlib chart. Beneath is the chart, rendered to raster graphics."></p>
</li>
<li>
<p>a "rerun from start" button (or rerun the current command; or rerun only a single past command) that replaces past output instead of appending to it</p>
<p><img src="https://jyn.dev/assets/jupyter%20run%20cells.png" alt=""></p>
</li>
<li>
<p>"views" of source code and output that can be rewritten in place (e.g. markdown can be viewed either as source or as rendered HTML)</p>
<p><img src="https://jyn.dev/assets/jupyter%20markdown%20raw.png" alt=""> <img src="https://jyn.dev/assets/jupyter%20markdown%20rendered.png" alt=""></p>
</li>
<li>
<p>a built-in editor with syntax highlighting, tabs, panes, mouse support, etc.</p>
<p><img src="https://jyn.dev/assets/jupyter%20window%20management.png" alt=""></p>
</li>
</ul>
<h3 id="some-problems">some problems<a href="#some-problems" aria-label="Anchor link for: some-problems"></a>
</h3>
<p>Jupyter works by having a "kernel" (in this case, a python interpreter) and a "renderer" (in this case, a web application displayed by the browser). You could imagine using a Jupyter Notebook with a shell as the kernel, so that you get all the nice features of Jupyter when running shell commands. However, that quickly runs into some issues:</p>
<ul>
<li>Your shell gets the commands all at once, not character-by-character, so tab-complete, syntax highlighting, and autosuggestions don't work.</li>
<li>What do you do about long-lived processes? By default, Jupyter runs a cell until completion; you can cancel it, but you can't suspend, resume, interact with, nor view a process while it's running. Don't even think about running <code>vi</code> or <code>top</code>.</li>
<li>The "rerun cell" buttons do horrible things to the state of your computer (normal Jupyter kernels have this problem too, but "rerun all" works better when the commands don't usually include <code>rm -rf</code>).</li>
<li>Undo/redo do <em>not</em> work. (They don't work in a normal terminal either, but people attempt to use them more when it looks like they should be able to.)</li>
</ul>
<p>It turns out all these problems are solveable.</p>
<h2 id="how-does-that-work">how does that work?<a href="#how-does-that-work" aria-label="Anchor link for: how-does-that-work"></a>
</h2>
<h3 id="shell-integration">shell integration<a href="#shell-integration" aria-label="Anchor link for: shell-integration"></a>
</h3>
<p>There exists today a terminal called <a href="https://www.warp.dev/">Warp</a>. Warp has built native integration between the terminal and the shell, where the terminal understands where each command starts and stops, what it outputs, and what is your own input. As a result, it can render things very prettily:</p>
<p><img src="https://jyn.dev/assets/warp%20terminal.png" alt=""></p>
<p>It does this using (mostly) standard features built-in to the terminal and shell (a custom DCS): you can read their explanation <a href="https://www.warp.dev/blog/how-warp-works#:~:text=the%20reason">here</a>. It's possible to do this less invasively using <a href="https://iterm2.com/documentation-escape-codes.html#:~:text=shell%20integration/">OSC 133 escape codes</a>; I'm not sure why Warp didn't do this, but that's ok.</p>
<p>iTerm2 does a similar thing, and this allows it to enable <a href="https://iterm2.com/documentation-shell-integration.html#:~:text=enables%20numerous%20features">really quite a lot of features</a>: navigating between commands with a single hotkey; notifying you when a command finishes running, showing the current command as an "overlay" if the output goes off the screen.</p>
<h3 id="long-lived-processes">long-lived processes<a href="#long-lived-processes" aria-label="Anchor link for: long-lived-processes"></a>
</h3>
<p>This is really three different things. The first is <em>interacting</em> with a long-lived process. The second is <em>suspending</em> the process without killing it. The third is <em>disconnecting</em> from the process, in such a way that the process state is not disturbed and is still available if you want to reconnect.</p>
<h4 id="interacting">interacting<a href="#interacting" aria-label="Anchor link for: interacting"></a>
</h4>
<p>To interact with a process, you need bidirectional communication, i.e. you need a "cell output" that is also an input. An example would be any TUI, like <code>top</code>, <code>gdb</code>, or <code>vim</code> <sup id="fr-1-1"><a href="#fn-1">1</a></sup>.  Fortunately, Jupyter is really good at this!  The whole design is around having <a href="https://ipywidgets.readthedocs.io/en/latest/">interactive outputs</a> that you can change and update.</p>
<p>Additionally, I would expect my terminal to always have a "free input cell", as Matklad describes in <a href="https://matklad.github.io/2019/11/16/a-better-shell.html">A Better Shell</a>, where the interactive process runs in the top half of the window and an input cell is available in the bottom half. Jupyter can do this today, but "add a cell" is manual, not automatic.</p>
<h4 id="suspending">suspending<a href="#suspending" aria-label="Anchor link for: suspending"></a>
</h4>
<p>"Suspending" a process is usually called "<a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">job control</a>". There's not too much to talk about here, except that I would expect a "modern" terminal to show me all suspended and background processes as a de-emphasized persistent visual, kinda like how Intellij will show you "indexing ..." in the bottom taskbar.</p>
<p><img src="https://jyn.dev/assets/intellij%20background%20tasks.png" alt=""></p>
<h4 id="disconnecting">disconnecting<a href="#disconnecting" aria-label="Anchor link for: disconnecting"></a>
</h4>
<p>There are roughly three existing approaches for disconnecting and reconnecting to a terminal session (Well, four if you count <a href="https://github.com/nelhage/reptyr">reptyr</a>).</p>
<ol>
<li>
<p>Tmux / Zellij / Screen</p>
<p>These tools inject a whole extra terminal emulator between your terminal emulator and the program. They work by having a "server" which actually owns the PTY and renders the output, and a "client" that displays the output to your "real" terminal emulator. This model lets you detach clients, reattach them later, or even attach multiple clients at once. You can think of this as a "batteries-included" approach. It also has the benefit that you can <a href="https://jyn.dev/how-i-use-my-terminal/">program</a> both the client and the server (although many modern terminals, like <a href="https://sw.kovidgoyal.net/kitty/">Kitty</a> and <a href="https://wezfurlong.org/wezterm/">Wezterm</a> are programmable now); that you can organize your tabs and windows in the terminal (although many modern desktop environments have tiling and thorough keyboard shortcuts); and that you get street cred for looking like Hackerman.</p>
<p><img src="https://jyn.dev/assets/hackerman.png" alt=""></p>
<p>The downside is that, well, now you have an extra terminal emulator running in your terminal, with <a href="https://sw.kovidgoyal.net/kitty/faq/#i-am-using-tmux-zellij-and-have-a-problem">all the bugs that implies</a>.</p>
<p>iTerm actually avoids this by <a href="https://iterm2.com/documentation-tmux-integration.html">bypassing the tmux client altogether and acting as its own client</a> that talks directly to the server. In this mode, "tmux tabs" are actually iTerm tabs, "tmux panes" are iTerm panes, and so on. This is a good model, and I would adopt it when writing a future terminal for integration with existing tmux setups.</p>
</li>
<li>
<p><a href="https://mosh.org/">Mosh</a></p>
<p>Mosh is a really interesting place in the design space. It is not a terminal emulator replacement; instead it is an <em>ssh</em> replacement. Its big draw is that it supports reconnecting to your terminal session after a network interruption. It does that by <a href="https://mosh.org/#:~:text=how%20mosh%20works">running a state machine on the server and replaying an incremental diff of the viewport to the client</a>. This is a similar model to tmux, except that it doesn't support the "multiplexing" part (it expects your terminal emulator to handle that), nor scrollback (ditto). Because it has its own renderer, it has <a href="https://github.com/mobile-shell/mosh/issues/234">a similar class of bugs to tmux</a>. One feature it <em>does</em> have, unlike tmux, is that the "client" is really running on your side of the network, so local line editing is instant.</p>
</li>
<li>
<p><a href="https://ansuz.sooke.bc.ca/entry/389">alden</a>/<a href="https://github.com/shell-pool/shpool">shpool</a>/<a href="https://github.com/crigler/dtach">dtach</a>/<a href="https://github.com/martanne/abduco">abduco</a>/<a href="https://github.com/yazgoo/diss">diss</a></p>
<p>These all occupy a similar place in the design space: they <em>only</em> handle session detach/resume with a client/server, not networking or scrollback, and do not include their own terminal emulator. Compared to tmux and mosh, they are highly decoupled.</p>
</li>
</ol>
<h3 id="rerun-and-undo-redo">rerun and undo/redo<a href="#rerun-and-undo-redo" aria-label="Anchor link for: rerun-and-undo-redo"></a>
</h3>
<p>I'm going to treat these together because the solution is the same: dataflow tracking.</p>
<p>Take as an example <a href="https://plutojl.org/">pluto.jl</a>, which does this <em>today</em> by hooking into the Julia compiler.</p>
<p><img src="https://jyn.dev/assets/pluto%20interactive%20ode.gif" alt=""></p>
<p>Note that this updates cells live in response to previous cells that they depend on. Not pictured is that it <em>doesn't</em> update cells if their dependencies haven't changed. You can think of this as a spreadsheet-like Jupyter, where code is only rerun when necessary.</p>
<p>You may say this is hard to generalize. The trick here is <a href="https://jyn.dev/complected-and-orthogonal-persistence/#how-far-can-we-take-this">orthogonal persistence</a>. If you sandbox the processes, track all IO, and prevent things that are "too weird" unless they're talking to other processes in the sandbox (e.g. unix sockets and POST requests), you have really quite a lot of control over the process! This lets you treat it as a pure function of its inputs, where its inputs are "the whole file system, all environment variables, and all process attributes".</p>
<h3 id="derived-features">derived features<a href="#derived-features" aria-label="Anchor link for: derived-features"></a>
</h3>
<p>Once you have these primitives‚ÄîJupyter notebook frontends, undo/redo, automatic rerun, persistence, and shell integration‚Äîyou can build really quite a lot on top. And you can build it incrementally, piece-by-piece:</p>
<h4 id="needs-a-jupyter-notebook-frontend">needs a Jupyter notebook frontend<a href="#needs-a-jupyter-notebook-frontend" aria-label="Anchor link for: needs-a-jupyter-notebook-frontend"></a>
</h4>
<ul>
<li><a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/">Runbooks</a> (actually, you can build these just with Jupyter and a PTY primitive).</li>
<li>Terminal customization that uses normal CSS, no weird custom languages or ANSI color codes.</li>
<li>Search for commands by output/timestamp. Currently, you can search across output in the current session, or you can search across all command input history, but you don't have any kind of smart filters, and the output doesn't persist across sessions.</li>
</ul>
<h4 id="needs-shell-integration">needs shell integration<a href="#needs-shell-integration" aria-label="Anchor link for: needs-shell-integration"></a>
</h4>
<ul>
<li>Timestamps and execution duration for each command.</li>
<li>Local line-editing, even across a network boundary.</li>
<li><a href="https://docs.warp.dev/terminal/command-completions/completions">IntelliSense for shell commands</a>, without having to hit tab and with rendering that's integrated into the terminal.</li>
</ul>
<h4 id="needs-sandboxed-tracing">needs sandboxed tracing<a href="#needs-sandboxed-tracing" aria-label="Anchor link for: needs-sandboxed-tracing"></a>
</h4>
<ul>
<li>"<a href="https://jyn.dev/complected-and-orthogonal-persistence/#but-why">All the features from sandboxed tracing</a>": collaborative terminals, querying files modified by a command, "asciinema but you can edit it at runtime", tracing build systems.</li>
<li>Extend the smart search above to also search by disk state at the time the command was run.</li>
<li>Extending undo/redo to a git-like branching model (something like this is already support by <a href="https://elpa.gnu.org/packages/undo-tree.html#:~:text=undo%20systems">emacs undo-tree</a>), where you have multiple "views" of the process tree.</li>
<li>Given the undo-tree model, and since we have sandboxing, we can give an LLM access to your project, and run many of them in parallel at the same time without overwriting each others state, and in such a way that you can see what they're doing, edit it, and save it into a runbook for later use.</li>
<li>A terminal in a prod environment that can't affect the state of the machine, only inspect the existing state.</li>
</ul>
<h2 id="ok-but-how-do-you-build-this">ok but how do you build this<a href="#ok-but-how-do-you-build-this" aria-label="Anchor link for: ok-but-how-do-you-build-this"></a>
</h2>
<p>jyn, you may say, <a href="https://becca.ooo/blog/vertical-integration/">you can't build vertical integration in open source</a>. <a href="https://tech.lgbt/@jyn/112187088917827279">you can't make money off open source projects</a>. <a href="https://jyn.dev/you-are-in-a-box/#switching-costs-and-growth">the switching costs are too high</a>.</p>
<p>All these things are true. To talk about how this is possible, we have to talk about incremental adoption.</p>
<p>if I were building this, I would do it in stages, such that at each stage the thing is an improvement over its alternatives. This is how <code>jj</code> works and it works extremely well: it doesn't require everyone on a team to switch at once because individual people can use <code>jj</code>, even for single commands, without a large impact on everyone else.</p>
<h3 id="stage-1-transactional-semantics">stage 1: transactional semantics<a href="#stage-1-transactional-semantics" aria-label="Anchor link for: stage-1-transactional-semantics"></a>
</h3>
<p>When people think of redesigning the terminal, they always think of redesigning the terminal <em>emulator</em>. This is exactly the wrong place to start. People are attached to their emulators. They configure them, they make them look nice, they use their keybindings. There is a high switching cost to switching emulators because <a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/#everything-affects-everything-else">everything affects everything else</a>. It's not <em>so</em> terribly high, because it's still individual and not shared across a team, but still high.</p>
<p>What I would do instead is start at the CLI layer. CLI programs are great because they're easy to install and run and have very low switching costs: you can use them one-off without changing your whole workflow.</p>
<p>So, I would write a CLI that implements <a href="https://jyn.dev/complected-and-orthogonal-persistence/#how-far-can-we-take-this">transactional semantics for the terminal</a>. You can imagine an interface something like <code>transaction [start|rollback|commit]</code>, where everything run after <code>start</code> is undoable. There is a <em>lot</em> you can do with this alone, I think you could build a whole business off this.</p>
<h3 id="stage-2-persistent-sessions">stage 2: persistent sessions<a href="#stage-2-persistent-sessions" aria-label="Anchor link for: stage-2-persistent-sessions"></a>
</h3>
<p>Once I had transactional semantics, I would try to decouple persistence from tmux and mosh.</p>
<p>To get PTY persistence, you have to introduce a client/server model, because the kernel <em>really really</em> <a href="https://en.wikipedia.org/wiki/SIGHUP">expects</a> both sides of a PTY to always be connected. Using commands like <a href="https://ansuz.sooke.bc.ca/entry/389">alden</a>, or a library like it (it's not <em>that</em> complicated), lets you do this simply, without affecting the terminal emulator nor the programs running inside the PTY session.</p>
<p>To get scrollback, the server could save input and output indefinitely and replay them when the client reconnects. This gets you "native" scrollback‚Äîthe terminal emulator you're already using handles it exactly like any other output, because it looks exactly like any other output‚Äîwhile still being replayable and resumable from an arbitrary starting point. This requires some amount of parsing ANSI escape codes<sup id="fr-2-1"><a href="#fn-2">2</a></sup>, but it's doable with enough work.</p>
<p>To get network resumption like mosh, my custom server could use <a href="https://eternalterminal.dev/howitworks/">Eternal TCP</a> (possibly built on top of QUIC for efficiency). Notably, the persistence for the PTY is separate from the persistence for the network connection. Eternal TCP here is strictly an optimization: you could build this on top of a bash script that runs <code>ssh host eternal-pty attach</code> in a loop, it's just not as nice an experience because of network delay and packet loss. Again, composable parts allow for incremental adoption.</p>
<p>At this point, you're already able to connect multiple clients to a single terminal session, like tmux, but window management is still done by your terminal emulator, not by the client/server. If you wanted to have window management integrated, the terminal emulator could speak the tmux -CC protocol, like iTerm.</p>
<p>All parts of this stage can be done independently and in parallel from the transactional semantics, but I don't think you can build a business off them, it's not enough of an improvement over the existing tools.</p>
<h3 id="stage-3-structured-rpc">stage 3: structured RPC<a href="#stage-3-structured-rpc" aria-label="Anchor link for: stage-3-structured-rpc"></a>
</h3>
<p>This bit depends on the client/server model. Once you have a server interposed between the terminal emulator and the client, you can start doing really funny things like tagging I/O with metadata. This lets all data be timestamped<sup id="fr-3-1"><a href="#fn-3">3</a></sup> and lets you distinguish input from output. <a href="https://jvns.ca/blog/2022/07/20/pseudoterminals/">xterm.js</a> works something like this. When combined with shell integration, this even lets you distinguish shell prompts from program output, at the <em>data</em> layer.</p>
<p>Now you can start doing really funny things, because you have a <em>structured log</em> of your terminal session. You can replay the log as a recording, like <a href="https://asciinema.org/">asciinema</a><sup id="fr-4-1"><a href="#fn-4">4</a></sup>; you can transform the shell prompt without rerunning all the commands; you can import it into a Jupyter Notebook or <a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/">Atuin Desktop</a>; you can save the commands and rerun them later as a script. Your terminal is data.</p>
<h3 id="stage-4-jupyter-like-frontend">stage 4: jupyter-like frontend<a href="#stage-4-jupyter-like-frontend" aria-label="Anchor link for: stage-4-jupyter-like-frontend"></a>
</h3>
<p>This is the very first time that we touch the terminal emulator, and it's intentionally the last step because it has the highest switching costs. This makes use of all the nice features we've built to give you a nice UI. You don't need our <code>transaction</code> CLI anymore unless you want nested transactions, because your whole terminal session starts in a transaction by default. You get all the features I mention <a href="https://jyn.dev/the-terminal-of-the-future/#derived-features">above</a>, because we've put all the pieces together.</p>
<h2 id="jyn-what-the-fuck">jyn, what the fuck<a href="#jyn-what-the-fuck" aria-label="Anchor link for: jyn-what-the-fuck"></a>
</h2>
<p>This is bold and ambitious and I think building the whole thing would take about a decade. That's ok. I'm patient.</p>
<p>You can help me by spreading the word :) Perhaps this post will inspire someone to start building this themselves.</p>
<hr>
<h2 id="bibliography">bibliography<a href="#bibliography" aria-label="Anchor link for: bibliography"></a>
</h2>
<ul>
<li><a href="https://www.destroyallsoftware.com/talks/a-whole-new-world">Gary Bernhardt, ‚ÄúA Whole New World‚Äù</a></li>
<li><a href="https://matklad.github.io/2019/11/16/a-better-shell.html">Alex Kladov, ‚ÄúA Better Shell‚Äù</a></li>
<li><a href="https://jyn.dev/how-i-use-my-terminal/">jyn, ‚Äúhow i use my terminal‚Äù</a></li>
<li><a href="https://jyn.dev/complected-and-orthogonal-persistence/">jyn, ‚ÄúComplected and Orthogonal Persistence‚Äù</a></li>
<li><a href="https://jyn.dev/you-are-in-a-box/">jyn, ‚Äúyou are in a box‚Äù</a></li>
<li><a href="https://tech.lgbt/@jyn/112187088917827279">jyn, ‚Äúthere's two costs to making money off an open source project‚Ä¶‚Äù</a></li>
<li><a href="https://becca.ooo/blog/vertical-integration/">Rebecca Turner, ‚ÄúVertical Integration is the Only Thing That Matters‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2025/06/24/new-zine--the-secret-rules-of-the-terminal/">Julia Evans, ‚ÄúNew zine: The Secret Rules of the Terminal‚Äù</a></li>
<li><a href="https://wizardzines.com/comics/meet-the-terminal-emulator/">Julia Evans, ‚Äúmeet the terminal emulator‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2022/07/20/pseudoterminals/">Julia Evans, ‚ÄúWhat happens when you press a key in your terminal?‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2025/01/11/getting-a-modern-terminal-setup/">Julia Evans, ‚ÄúWhat's involved in getting a "modern" terminal setup?‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2017/03/26/bash-quirks/">Julia Evans, ‚ÄúBash scripting quirks &amp; safety tips‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2025/02/05/some-terminal-frustrations/">Julia Evans, ‚ÄúSome terminal frustrations‚Äù</a></li>
<li><a href="https://jvns.ca/blog/2024/07/03/reasons-to-use-job-control/">Julia Evans, ‚ÄúReasons to use your shell's job control‚Äù</a></li>
<li><a href="https://man7.org/linux/man-pages/man7/signal.7.html">‚Äúsignal(7) - Miscellaneous Information Manual‚Äù</a></li>
<li><a href="https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797">Christian Petersen, ‚ÄúANSI Escape Codes‚Äù</a></li>
<li><a href="https://github.com/withoutboats/notty">saoirse, ‚Äúwithoutboats/notty: A new kind of terminal‚Äù</a></li>
<li><a href="https://docs.jupyter.org/en/latest/">Jupyter Team, ‚ÄúProject Jupyter Documentation‚Äù</a></li>
<li><a href="https://www.warp.dev/">‚ÄúWarp: The Agentic Development Environment‚Äù</a></li>
<li><a href="https://www.warp.dev/blog/how-warp-works">‚ÄúWarp: How Warp Works‚Äù</a></li>
<li><a href="https://docs.warp.dev/terminal/command-completions/completions">‚ÄúWarp: Completions‚Äù</a></li>
<li><a href="https://iterm2.com/documentation-escape-codes.html">George Nachman, ‚ÄúiTerm2: Proprietary Escape Codes‚Äù</a></li>
<li><a href="https://iterm2.com/documentation-shell-integration.html">George Nachman, ‚ÄúiTerm2: Shell Integration‚Äù</a></li>
<li><a href="https://iterm2.com/documentation-tmux-integration.html">George Nachman, ‚ÄúiTerm2: tmux Integration‚Äù</a></li>
<li><a href="https://ipywidgets.readthedocs.io/en/latest/">Project Jupyter, ‚ÄúJupyter Widgets‚Äù</a></li>
<li><a href="https://github.com/nelhage/reptyr">Nelson Elhage, ‚Äúnelhage/reptyr: Reparent a running program to a new terminal‚Äù</a></li>
<li><a href="https://sw.kovidgoyal.net/kitty/">Kovid Goyal, ‚Äúkitty‚Äù</a></li>
<li><a href="https://sw.kovidgoyal.net/kitty/faq/">Kovid Goyal, ‚Äúkitty - Frequently Asked Questions‚Äù</a></li>
<li><a href="https://wezfurlong.org/wezterm/">Wez Furlong, ‚ÄúWezterm‚Äù</a></li>
<li><a href="https://mosh.org/">Keith Winstein, ‚ÄúMosh: the mobile shell‚Äù</a></li>
<li><a href="https://github.com/mobile-shell/mosh/issues/234">Keith Winstein, ‚ÄúDisplay errors with certain characters</a></li>
<li><a href="https://ansuz.sooke.bc.ca/entry/389">Matthew Skala, ‚Äúalden: detachable terminal sessions without breaking scrollback‚Äù</a></li>
<li><a href="https://github.com/shell-pool/shpool">Ethan Pailes, ‚Äúshell-pool/shpool: Think tmux, then aim... lower‚Äù</a></li>
<li><a href="https://github.com/crigler/dtach">Ned T. Crigler, ‚Äúcrigler/dtach: A simple program that emulates the detach feature of screen‚Äù</a></li>
<li><a href="https://github.com/martanne/abduco">Marc Andr√© Tanner, ‚Äúmartanne/abduco: abduco provides session management‚Äù</a></li>
<li><a href="https://github.com/yazgoo/diss">yazgoo, ‚Äúyazgoo/diss: dtach-like program / crate in rust‚Äù</a></li>
<li><a href="https://plutojl.org/">Fons van der Plas, ‚ÄúPluto.jl ‚Äî interactive Julia programming environment‚Äù</a></li>
<li><a href="https://blog.atuin.sh/atuin-desktop-runbooks-that-run/">Ellie Huxtable, ‚ÄúAtuin Desktop: Runbooks that Run‚Äù</a></li>
<li><a href="https://elpa.gnu.org/packages/undo-tree.html">Toby Cubitt, ‚Äúundo-tree‚Äù</a></li>
<li><a href="https://en.wikipedia.org/wiki/SIGHUP">‚ÄúSIGHUP - Wikipedia‚Äù</a></li>
<li><a href="https://eternalterminal.dev/howitworks/">Jason Gauci, ‚ÄúHow Eternal Terminal Works‚Äù</a></li>
<li><a href="https://asciinema.org/">Marcin Kulik, ‚ÄúRecord and share your terminal sessions, the simple way - asciinema.org‚Äù</a></li>
<li><a href="https://ratatui.rs/concepts/backends/alternate-screen/">‚ÄúAlternate Screen | Ratatui‚Äù</a></li>
</ul>
<hr>
<!--To talk about interacting with a process, we have to talk about [PTTYs](https://jvns.ca/blog/2022/07/20/pseudoterminals/).-->
<!--
My complaint about Warp is chiefly that they didn't think big enough. Warp built a very cool tool, and then wrapped it in a product that mostly exists to make it easier to use LLMs in the terminal. There are [*so many* things](https://iterm2.com/documentation-shell-integration.html#:~:text=enables%20numerous%20features) you could do with shell integration, and they don't seem to have done very many of them.
-->


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A modern 35mm film scanner for home (242 pts)]]></title>
            <link>https://www.soke.engineering/</link>
            <guid>45891907</guid>
            <pubDate>Tue, 11 Nov 2025 19:48:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.soke.engineering/">https://www.soke.engineering/</a>, See on <a href="https://news.ycombinator.com/item?id=45891907">Hacker News</a></p>
<div id="readability-page-1" class="page"><div webpageid="augiA20Il" data-layout-template="true" id="main" data-framer-hydrate-v2="{&quot;routeId&quot;:&quot;augiA20Il&quot;,&quot;localeId&quot;:&quot;default&quot;,&quot;breakpoints&quot;:[{&quot;hash&quot;:&quot;72rtr7&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1200px)&quot;},{&quot;hash&quot;:&quot;1gly80&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1199.98px)&quot;},{&quot;hash&quot;:&quot;1teekcz&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809.98px)&quot;},{&quot;hash&quot;:&quot;4e4xff&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 1440px)&quot;},{&quot;hash&quot;:&quot;wthtfe&quot;,&quot;mediaQuery&quot;:&quot;(min-width: 810px) and (max-width: 1439.98px)&quot;},{&quot;hash&quot;:&quot;1l7dlx4&quot;,&quot;mediaQuery&quot;:&quot;(max-width: 809.98px)&quot;}]}" data-framer-ssr-released-at="2025-11-10T14:50:19.351Z" data-framer-page-optimized-at="2025-11-11T21:36:56.978Z" data-framer-generated-page=""><nav data-framer-appear-id="h121gk" data-framer-layout-hint-center-x="true" data-framer-name="Navbar"></nav><div data-framer-root=""><header data-framer-name="Hero-Section" id="hero-section"><div data-framer-name="Container"><figure as="figure" data-framer-name="img"><p><img decoding="async" width="1237" height="1110" src="https://framerusercontent.com/images/Z8ymSbSlFHFYqnxs7knHRxMBqo.png?width=1237&amp;height=1110" alt=""></p></figure></div></header><header data-framer-name="Hero-Section" id="hero-section-1"><div data-framer-name="Container"><div data-framer-name="Left-Text"><p data-framer-component-type="RichTextContainer"><h2 data-styles-preset="ZfkYyGwCo"><span>The New Era of </span><br><span>Film Scanning</span></h2></p><div data-framer-name="Highlights"><p><!--$--><h2><span>Knokke  - a high-resolution 35 mm film scanner built for photographers who demand speed, quality and control.</span></h2><!--/$--></p></div></div><div data-framer-name="Variant 1" data-highlight="true" tabindex="0" data-framer-appear-id="1wbnzl8" id="1wbnzl8"><p><svg style="width:100%;height:100%;transform-origin:center" viewBox="0 0 100 100" overflow="visible"><path id="curve-wnxkz4" d="M 0 50 L 0 50 A 1 1 0 0 1 100 50 L 100 50 L 100 50 A 1 1 0 0 1 0 50 L 0 50" stroke-width="none" fill="transparent"></path><text><textPath href="#curve-wnxkz4" startOffset="0" dominant-baseline="Text Top" style="letter-spacing:0.02em;font-family:&quot;IBM Plex Mono&quot;, monospace;font-size:14px;font-style:normal;font-weight:500;line-height:1em;fill:var(--token-ba5469a1-3890-44cc-aaeb-d6b7e143f20d, rgb(244, 244, 245))">Watch the video - Watch the video -</textPath></text></svg></p></div></div></header><header data-framer-name="Hero-Section" id="hero-section-2"><div data-framer-name="Container"><p data-framer-component-type="RichTextContainer"><h2 data-styles-preset="ZfkYyGwCo"><span>The New Era of </span><br><span>Film Scanning</span></h2></p><p><!--$--><h2><span>Knokke - a high-resolution 35 mm film scanner built for photographers who demand speed, quality, and control.</span></h2><!--/$--></p></div><div data-framer-appear-id="truicp" data-framer-name="Image"><figure as="figure" data-framer-name="img"><p><img decoding="async" width="1237" height="1110" src="https://framerusercontent.com/images/Z8ymSbSlFHFYqnxs7knHRxMBqo.png?width=1237&amp;height=1110" alt=""></p></figure></div></header><div data-framer-name="Proof Section" id="proof"><p data-framer-component-type="RichTextContainer"><h2 data-styles-preset="ZfkYyGwCo">Knokke redefines film scanning by bringing modern imaging, optics, and software into a beautifully engineered device.</h2></p></div><div data-framer-name="Product Section" id="features"><div data-framer-name="Components/Product item"><p>The modern 35 mm film scanner that captures a full roll in under just a few minutes while capturing every frame at 4064 DPI and 48bit colour. Its custom optics and state-of-the-art sensor deliver benchmark setting quality and speed at a price only Knokke can offer.</p></div><div data-framer-component-type="RichTextContainer" data-framer-name="Product 2"><p>Built for the 21st century, Knokke runs on Korova, a lean C++ application that's native to Linux, macOS, and Windows‚Äîso you can forget vintage PCs and enjoy a plug-and-play workflow that lets you focus on your photos.</p><p>Each frame can have custom scan settings, repeatable across multiple scans for consistent results and tailored workflows. The scanner can also skip directly to requested frames, massively accelerating scanning time and enabling fast access to key shots without unnecessary delay.</p></div></div><div data-framer-name="Features Section"><div data-framer-name="Image"><p><img decoding="async" loading="lazy" width="5152" height="7728" sizes="(min-width: 1200px) max(max((min(max(100vw - 32px, 1px), 1200px) - 71px) / 3, 1px), 100vw), (min-width: 810px) and (max-width: 1199.98px) max(max((min(max(100vw - 32px, 1px), 1200px) - 71px) / 3, 1px), 100vw, 254px), (max-width: 809.98px) max(max((min(max(100vw - 32px, 1px), 1200px) - 71px) / 3, 1px), min(100vw - 32px, 1200px), 100vw)" srcset="https://framerusercontent.com/images/VrDOZbsbciVXbWxffHjfugl4U7w.jpg?scale-down-to=1024&amp;width=5152&amp;height=7728 682w,https://framerusercontent.com/images/VrDOZbsbciVXbWxffHjfugl4U7w.jpg?scale-down-to=2048&amp;width=5152&amp;height=7728 1365w,https://framerusercontent.com/images/VrDOZbsbciVXbWxffHjfugl4U7w.jpg?scale-down-to=4096&amp;width=5152&amp;height=7728 2730w,https://framerusercontent.com/images/VrDOZbsbciVXbWxffHjfugl4U7w.jpg?width=5152&amp;height=7728 5152w" src="https://framerusercontent.com/images/VrDOZbsbciVXbWxffHjfugl4U7w.jpg?width=5152&amp;height=7728" alt="" data-framer-original-sizes="max((min(max(100vw - 32px, 1px), 1200px) - 71px) / 3, 1px)"></p></div><div data-framer-name="Heading"><p id="w8drg9" data-framer-component-type="RichTextContainer"><h3 data-styles-preset="gUnjFa38F">Engineered for Individual Users and Lab Professionals</h3></p><div data-framer-name="Grid"><div data-border="true" data-framer-name="Default"><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h6 data-styles-preset="cr4s6T7G5">01</h6></p><p data-framer-component-type="RichTextContainer"><h6>Quality</h6></p></div><p>Knokke‚Äôs premium build and precision engineering ensure lasting, reliable performance.</p></div><div data-border="true" data-framer-name="Default"><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h6 data-styles-preset="cr4s6T7G5">02</h6></p><p data-framer-component-type="RichTextContainer"><h6>Speed</h6></p></div><p>Knokke‚Äôs high scan speed and streamlined workflow keep you moving.</p></div><div data-border="true" data-framer-name="Default"><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h6 data-styles-preset="cr4s6T7G5">03</h6></p><p data-framer-component-type="RichTextContainer"><h6>Full Control</h6></p></div><p>Knokke lets you fine-tune every detail with flexible settings and precise color control.</p></div><div data-border="true" data-framer-name="Default"><div data-framer-name="Title"><p data-framer-component-type="RichTextContainer"><h6 data-styles-preset="cr4s6T7G5">04</h6></p><p data-framer-component-type="RichTextContainer"><h6>Future Proof</h6></p></div><p>Knokke comes with ongoing software support, open-source flexibility, and readily available spare parts.</p></div></div></div></div><div data-framer-name="CTA-Section" id="cta-section"><div data-framer-name="Left"><p data-framer-component-type="RichTextContainer"><h2>Price at Launch</h2></p></div><div data-framer-name="Right"><div data-framer-name="Price"><p><!--$--><h2><span>999‚Ç¨</span></h2><!--/$--></p><p><strong>Includes scanner + software</strong></p></div><div data-framer-name="Features"><div data-framer-name="Row"><p><!--$--><h2><span>4064 dpi resolution</span></h2><!--/$--></p></div><div data-framer-name="Row"><p><!--$--><h2><span>5 min per roll</span></h2><!--/$--></p></div><div data-framer-name="Row"><p><!--$--><h2><span>48-bit colour depth</span></h2><!--/$--></p></div><div data-framer-name="Row"><p><!--$--><h2><span>120 dB Dynamic Range</span></h2><!--/$--></p></div><div data-framer-name="Row"><p><!--$--><h2><span>LED Matrix</span></h2><!--/$--></p></div><div data-framer-name="Row"><p><!--$--><h2><span>RGB LED backlight</span></h2><!--/$--></p></div></div></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A catalog of side effects (109 pts)]]></title>
            <link>https://bernsteinbear.com/blog/compiler-effects/</link>
            <guid>45891868</guid>
            <pubDate>Tue, 11 Nov 2025 19:44:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bernsteinbear.com/blog/compiler-effects/">https://bernsteinbear.com/blog/compiler-effects/</a>, See on <a href="https://news.ycombinator.com/item?id=45891868">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Optimizing compilers like to keep track of each IR instruction‚Äôs <em>effects</em>. An
instruction‚Äôs effects vary wildly from having no effects at all, to writing a
specific variable, to completely unknown (writing all state).</p>

<p>This post can be thought of as a continuation of <a href="https://bernsteinbear.com/blog/irs/">What I talk about when I talk
about IRs</a>, specifically the section talking about asking the right
questions. When we talk about effects, we should ask the right questions: not
<em>what opcode is this?</em> but instead <em>what effects does this opcode have?</em></p>

<p>Different compilers represent and track these effects differently. I‚Äôve been
thinking about how to represent these effects all year, so I have been doing
some reading. In this post I will give some summaries of the landscape of
approaches. Please feel free to suggest more.</p>

<h2 id="some-background">Some background</h2>

<p>Internal IR effect tracking is similar to the programming language notion of
algebraic effects in type systems, but internally, compilers keep track of
finer-grained effects. Effects such as ‚Äúwrites to a local variable‚Äù, ‚Äúwrites to
a list‚Äù, or ‚Äúreads from the stack‚Äù indicate what instructions can be
re-ordered, duplicated, or removed entirely.</p>

<p>For example, consider the following pseodocode for some made-up language that
stands in for a snippet of compiler IR:</p>

<div><pre><code><span># ...
</span><span>v</span> <span>=</span> <span>some_var</span><span>[</span><span>0</span><span>]</span>
<span>another_var</span><span>[</span><span>0</span><span>]</span> <span>=</span> <span>5</span>
<span># ...
</span></code></pre></div>

<p>The goal of effects is to communicate to the compiler if, for example, these two IR
instructions can be re-ordered. The second instruction <em>might</em> write to a
location that the first one reads. But it also might not! This is about knowing
if <code>some_var</code> and <code>another_var</code> <em>alias</em>‚Äîif they are different names that
refer to the same object.</p>

<p>We can sometimes answer that question directly, but often it‚Äôs cheaper to
compute an approximate answer: <em>could</em> they even alias? It‚Äôs possible that
<code>some_var</code> and <code>another_var</code> have different types, meaning that (as long as you
have strict aliasing) the <code>Load</code> and <code>Store</code> operations that implement these
reads and writes by definition touch different locations. And if they look
at disjoint locations, there need not be any explicit order enforced.</p>

<p>Different compilers keep track of this information differently. The null effect
analysis gives up and says ‚Äúevery instruction is maximally effectful‚Äù and
therefore ‚Äúwe can‚Äôt re-order or delete any instructions‚Äù. That‚Äôs probably fine
for a first stab at a compiler, where you will get a big speed up purely based
on strength reductions. Over-approximations of effects should always be
valid.</p>

<p>But at some point you start wanting to do dead code elimination (DCE), or
common subexpression elimination (CSE), or loads/store elimination, or move
instructions around, and you start wondering how to represent effects. That‚Äôs
where I am right now. So here‚Äôs a catalog of different compilers I have looked
at recently.</p>

<p>There are two main ways I have seen to represent effects: bitsets and heap
range lists. We‚Äôll look at one example compiler for each, talk a bit about
tradeoffs, then give a bunch of references to other major compilers.</p>

<p>We‚Äôll start with <a href="https://github.com/facebookincubator/cinder">Cinder</a>, a Python JIT, because that‚Äôs what I used to
work on.</p>

<h2 id="cinder">Cinder</h2>

<p><a href="https://github.com/facebookincubator/cinder">Cinder</a> tracks heap effects for its high-level IR (HIR) in
<a href="https://github.com/facebookincubator/cinderx/blob/8bf5af94e2792d3fd386ab25b1aeedae27276d50/cinderx/Jit/hir/instr_effects.h">instr_effects.h</a>. Pretty much everything happens in
the <code>memoryEffects(const Instr&amp; instr)</code> function, which is expected to know
everything about what effects the given instruction might have.</p>

<p>The data representation is a bitset representation of a lattice called an
<code>AliasClass</code> and that is defined in <a href="https://github.com/facebookincubator/cinderx/blob/8bf5af94e2792d3fd386ab25b1aeedae27276d50/cinderx/Jit/hir/alias_class.h">alias_class.h</a>. Each
bit in the bitset represents a distinct location in the heap: reads from and
writes to each of these locations are guaranteed not to affect any of the other
locations.</p>

<p>Here is the X-macro that defines it:</p>

<div><pre><code><span>#define HIR_BASIC_ACLS(X) \
  X(ArrayItem)            \
  X(CellItem)             \
  X(DictItem)             \
  X(FuncArgs)             \
  X(FuncAttr)             \
  X(Global)               \
  X(InObjectAttr)         \
  X(ListItem)             \
  X(Other)                \
  X(TupleItem)            \
  X(TypeAttrCache)        \
  X(TypeMethodCache)
</span>
<span>enum</span> <span>BitIndexes</span> <span>{</span>
<span>#define ACLS(name) k##name##Bit,
</span>    <span>HIR_BASIC_ACLS</span><span>(</span><span>ACLS</span><span>)</span>
<span>#undef ACLS
</span><span>};</span>
</code></pre></div>

<p>Note that each bit implicitly represents a set: <code>ListItem</code> does not refer to a
<em>specific</em> list index, but the infinite set of all possible list indices. It‚Äôs
<em>any</em> list index. Still, every list index is completely disjoint from, say, every
entry in a global variable table.</p>

<p>(And, to be clear, an object in a list might be the same as an object in a
global variable table. The objects themselves can alias. But the thing being
written to or read from, the thing <em>being side effected</em>, is the container.)</p>

<p>Like other bitset lattices, it‚Äôs possible to union the sets by or-ing the bits.
It‚Äôs possible to query for overlap by and-ing the bits.</p>

<div><pre><code><span>class</span> <span>AliasClass</span> <span>{</span>
  <span>// The union of two AliasClass</span>
  <span>AliasClass</span> <span>operator</span><span>|</span><span>(</span><span>AliasClass</span> <span>other</span><span>)</span> <span>const</span> <span>{</span>
    <span>return</span> <span>AliasClass</span><span>{</span><span>bits_</span> <span>|</span> <span>other</span><span>.</span><span>bits_</span><span>};</span>
  <span>}</span>

  <span>// The intersection (overlap) of two AliasClass</span>
  <span>AliasClass</span> <span>operator</span><span>&amp;</span><span>(</span><span>AliasClass</span> <span>other</span><span>)</span> <span>const</span> <span>{</span>
    <span>return</span> <span>AliasClass</span><span>{</span><span>bits_</span> <span>&amp;</span> <span>other</span><span>.</span><span>bits_</span><span>};</span>
  <span>}</span>
<span>};</span>
</code></pre></div>

<p>If this sounds familiar, it‚Äôs because (as the repo notes) it‚Äôs a similar idea
to Cinder‚Äôs <a href="https://bernsteinbear.com/blog/lattice-bitset/">type lattice representation</a>.</p>

<p>Like other lattices, there is both a bottom element (no effects) and a top
element (all possible effects):</p>

<div><pre><code><span>#define HIR_OR_BITS(name) | k##name
</span>
<span>#define HIR_UNION_ACLS(X)                           \
  </span><span>/* Bottom union */</span><span>                                \
  X(Empty, 0)                                       \
  </span><span>/* Top union */</span><span>                                   \
  X(Any, 0 HIR_BASIC_ACLS(HIR_OR_BITS))             \
  </span><span>/* Memory locations accessible by managed code */</span><span> \
  X(ManagedHeapAny, kAny &amp; ~kFuncArgs)
</span></code></pre></div>

<p>Union operations naturally hit a fixpoint at <code>Any</code> and intersection operations
naturally hit a fixpoint at <code>Empty</code>.</p>

<p>All of this together lets the optimizer ask and answer questions such as:</p>

<ul>
  <li>where might this instruction write?</li>
  <li>(because CPython is reference counted and incref implies ownership) where
does this instruction borrow its input from?</li>
  <li>do these two instructions‚Äô write destinations overlap?</li>
</ul>

<p>and more.</p>

<p>Let‚Äôs take a look at an (imaginary) IR version of the code snippet in the intro
and see what analyzing it might look like in the optimizer. Here is the fake
IR:</p>

<div><pre><code>v0: Tuple = ...
v1: List = ...
v2: Int[5] = ...
# v = some_var[0]
v3: Object = LoadTupleItem v0, 0
# another_var[0] = 5
StoreListItem v1, 0, v2
</code></pre></div>

<p>You can imagine that <code>LoadTupleItem</code> declares that it reads from the
<code>TupleItem</code> heap and <code>StoreListItem</code> declares that it writes to the <code>ListItem</code>
heap. Because tuple and list pointers cannot be casted into one another and
therefore cannot alias, these are
disjoint heaps in our bitset. Therefore <code>ListItem &amp; TupleItem == 0</code>, therefore
these memory operations can never interfere! They can (for example) be
re-ordered arbitrarily.</p>

<p>In Cinder, these memory effects could in the future be used for instruction
re-ordering, but they are today mostly used in two places: the refcount
insertion pass and DCE.</p>

<p>DCE involves first finding the set of instructions that need to be kept around
because they are useful/important/have effects. So here is what the Cinder DCE
<code>isUseful</code> looks like:</p>

<div><pre><code><span>bool</span> <span>isUseful</span><span>(</span><span>Instr</span><span>&amp;</span> <span>instr</span><span>)</span> <span>{</span>
  <span>return</span> <span>instr</span><span>.</span><span>IsTerminator</span><span>()</span> <span>||</span> <span>instr</span><span>.</span><span>IsSnapshot</span><span>()</span> <span>||</span>
      <span>(</span><span>instr</span><span>.</span><span>asDeoptBase</span><span>()</span> <span>!=</span> <span>nullptr</span> <span>&amp;&amp;</span> <span>!</span><span>instr</span><span>.</span><span>IsPrimitiveBox</span><span>())</span> <span>||</span>
      <span>(</span><span>!</span><span>instr</span><span>.</span><span>IsPhi</span><span>()</span> <span>&amp;&amp;</span> <span>memoryEffects</span><span>(</span><span>instr</span><span>).</span><span>may_store</span> <span>!=</span> <span>AEmpty</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>There are some other checks in there but <code>memoryEffects</code> is right there at the
core of it!</p>

<p>Now that we have seen the bitset representation of effects and an
implementation in Cinder, let‚Äôs take a look at a different representation and
and an implementation in JavaScriptCore.</p>

<h2 id="javascriptcore">JavaScriptCore</h2>

<p>I keep coming back to <a href="https://gist.github.com/pizlonator/cf1e72b8600b1437dda8153ea3fdb963">How I implement SSA form</a> by <a href="http://www.filpizlo.com/">Fil
Pizlo</a>, one of the significant contributors to JavaScriptCore (JSC). In
particular, I keep coming back to the <a href="https://gist.github.com/pizlonator/cf1e72b8600b1437dda8153ea3fdb963#uniform-effect-representation">Uniform Effect
Representation</a> section. This notion of ‚Äúabstract heaps‚Äù felt
very‚Ä¶ well, abstract. Somehow more abstract than the bitset representation.
The pre-order and post-order integer pair as a way to represent nested heap
effects just did not click.</p>

<p>It didn‚Äôt make any sense until I actually went spelunking in JavaScriptCore and
found one of several implementations‚Äîbecause, you know, JSC is six compilers
in a trenchcoat<sup>[<a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed"><i>citation needed</i></a>]</sup>.</p>

<p>DFG, B3, DOMJIT, and probably others all have their own abstract heap
implementations. We‚Äôll look at DOMJIT mostly because it‚Äôs a smaller example and
also illustrates something else that‚Äôs interesting: builtins. We‚Äôll come back
to builtins in a minute.</p>

<p>Let‚Äôs take a lookat how DOMJIT structures its <a href="https://github.com/WebKit/WebKit/blob/989c9f9cd5b1f0c9606820e219ee51da32a34c6b/Source/WebCore/domjit/DOMJITAbstractHeapRepository.yaml">abstract
heaps</a>: a YAML file.</p>

<div><pre><code><span>DOM</span><span>:</span>
    <span>Tree</span><span>:</span>
        <span>Node</span><span>:</span>
            <span>-</span> <span>Node_firstChild</span>
            <span>-</span> <span>Node_lastChild</span>
            <span>-</span> <span>Node_parentNode</span>
            <span>-</span> <span>Node_nextSibling</span>
            <span>-</span> <span>Node_previousSibling</span>
            <span>-</span> <span>Node_ownerDocument</span>
        <span>Document</span><span>:</span>
            <span>-</span> <span>Document_documentElement</span>
            <span>-</span> <span>Document_body</span>
</code></pre></div>

<p>It‚Äôs a hierarchy. <code>Node_firstChild</code> is a subheap of <code>Node</code> is a subheap of‚Ä¶
and so on. A write to any <code>Node_nextSibling</code> is a write to <code>Node</code> is a write to
‚Ä¶ Sibling heaps are unrelated: <code>Node_firstChild</code> and <code>Node_lastChild</code>, for
example, are disjoint.</p>

<p>To get a feel for this, I wired up a <a href="https://github.com/tekknolagi/tekknolagi.github.com/tree/main/assets/code/gen_bitset.rb">simplified version</a> of
ZJIT‚Äôs bitset generator (for <em>types!</em>) to read a YAML document and generate a
bitset. It generated the following Rust code:</p>

<div><pre><code><span>mod</span> <span>bits</span> <span>{</span>
  <span>pub</span> <span>const</span> <span>Empty</span><span>:</span> <span>u64</span> <span>=</span> <span>0u64</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Document_body</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>0</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Document_documentElement</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>1</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Document</span><span>:</span> <span>u64</span> <span>=</span> <span>Document_body</span> <span>|</span> <span>Document_documentElement</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_firstChild</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>2</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_lastChild</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>3</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_nextSibling</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>4</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_ownerDocument</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>5</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_parentNode</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>6</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node_previousSibling</span><span>:</span> <span>u64</span> <span>=</span> <span>1u64</span> <span>&lt;&lt;</span> <span>7</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Node</span><span>:</span> <span>u64</span> <span>=</span> <span>Node_firstChild</span> <span>|</span> <span>Node_lastChild</span> <span>|</span> <span>Node_nextSibling</span> <span>|</span> <span>Node_ownerDocument</span> <span>|</span> <span>Node_parentNode</span> <span>|</span> <span>Node_previousSibling</span><span>;</span>
  <span>pub</span> <span>const</span> <span>Tree</span><span>:</span> <span>u64</span> <span>=</span> <span>Document</span> <span>|</span> <span>Node</span><span>;</span>
  <span>pub</span> <span>const</span> <span>DOM</span><span>:</span> <span>u64</span> <span>=</span> <span>Tree</span><span>;</span>
  <span>pub</span> <span>const</span> <span>NumTypeBits</span><span>:</span> <span>u64</span> <span>=</span> <span>8</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>It‚Äôs not a fancy X-macro, but it‚Äôs a short and flexible Ruby script.</p>

<p>Then I took the <a href="https://github.com/WebKit/WebKit/blob/989c9f9cd5b1f0c9606820e219ee51da32a34c6b/Source/WebCore/domjit/generate-abstract-heap.rb">DOMJIT abstract heap
generator</a>‚Äîalso funnily enough a short Ruby
script‚Äîmodified the output format slightly, and had it generate its int
pairs:</p>

<div><pre><code><span>mod</span> <span>bits</span> <span>{</span>
  <span>/* DOMJIT Abstract Heap Tree.
  DOM&lt;0,8&gt;:
      Tree&lt;0,8&gt;:
          Node&lt;0,6&gt;:
              Node_firstChild&lt;0,1&gt;
              Node_lastChild&lt;1,2&gt;
              Node_parentNode&lt;2,3&gt;
              Node_nextSibling&lt;3,4&gt;
              Node_previousSibling&lt;4,5&gt;
              Node_ownerDocument&lt;5,6&gt;
          Document&lt;6,8&gt;:
              Document_documentElement&lt;6,7&gt;
              Document_body&lt;7,8&gt;
  */</span>
  <span>pub</span> <span>const</span> <span>DOM</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>0</span><span>,</span> <span>end</span><span>:</span> <span>8</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Tree</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>0</span><span>,</span> <span>end</span><span>:</span> <span>8</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>0</span><span>,</span> <span>end</span><span>:</span> <span>6</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_firstChild</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>0</span><span>,</span> <span>end</span><span>:</span> <span>1</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_lastChild</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>1</span><span>,</span> <span>end</span><span>:</span> <span>2</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_parentNode</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>2</span><span>,</span> <span>end</span><span>:</span> <span>3</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_nextSibling</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>3</span><span>,</span> <span>end</span><span>:</span> <span>4</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_previousSibling</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>4</span><span>,</span> <span>end</span><span>:</span> <span>5</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Node_ownerDocument</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>5</span><span>,</span> <span>end</span><span>:</span> <span>6</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Document</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>6</span><span>,</span> <span>end</span><span>:</span> <span>8</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Document_documentElement</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>6</span><span>,</span> <span>end</span><span>:</span> <span>7</span> <span>};</span>
  <span>pub</span> <span>const</span> <span>Document_body</span><span>:</span> <span>HeapRange</span> <span>=</span> <span>HeapRange</span> <span>{</span> <span>start</span><span>:</span> <span>7</span><span>,</span> <span>end</span><span>:</span> <span>8</span> <span>};</span>
<span>}</span>
</code></pre></div>

<p>It already comes with a little diagram, which is super helpful for readability.</p>

<p>Any empty range(s) represent empty heap effects: if the start and end are the
same number, there are no effects. There is no one <code>Empty</code> value, but any empty
range could be normalized to <code>HeapRange { start: 0, end: 0 }</code>.</p>

<p>Maybe this was obvious to you, dear reader, but this pre-order/post-order thing
is about nested ranges! Seeing the output of the generator laid out clearly
like this made it make a lot more sense for me.</p>

<!--
So how do we compute subtyping relationships with `HeapRange`s? We check range
overlap! Here is [DOMJIT's C++ implementation][domjit-is-subtype-of]:

[domjit-is-subtype-of]: https://github.com/WebKit/WebKit/blob/989c9f9cd5b1f0c9606820e219ee51da32a34c6b/Source/JavaScriptCore/domjit/DOMJITHeapRange.h#L99

```c++
class HeapRange {
    constexpr explicit operator bool() const {
        return m_begin != m_end;
    }

    bool isStrictSubtypeOf(const HeapRange& other) const {
        if (!*this || !other)
            return false;
        if (*this == other)
            return false;
        return other.m_begin <= m_begin && m_end <= other.m_end;
    }

    bool isSubtypeOf(const HeapRange& other) const {
        if (!*this || !other)
            return false;
        if (*this == other)
            return true;
        return isStrictSubtypeOf(other);
    }
```

This is represented by the `operator bool()`
and implicit boolean conversions. To reinforce the whole nested heap ranges
thing, `isSubtypeOf` is asking if one `HeapRange` contains another.
-->

<p>What about checking overlap? Here is the <a href="https://github.com/WebKit/WebKit/blob/989c9f9cd5b1f0c9606820e219ee51da32a34c6b/Source/JavaScriptCore/domjit/DOMJITHeapRange.h#L108">implementation in
JSC</a>:</p>

<div><pre><code><span>namespace</span> <span>WTF</span> <span>{</span>
<span>// Check if two ranges overlap assuming that neither range is empty.</span>
<span>template</span><span>&lt;</span><span>typename</span> <span>T</span><span>&gt;</span>
<span>constexpr</span> <span>bool</span> <span>nonEmptyRangesOverlap</span><span>(</span><span>T</span> <span>leftMin</span><span>,</span> <span>T</span> <span>leftMax</span><span>,</span> <span>T</span> <span>rightMin</span><span>,</span> <span>T</span> <span>rightMax</span><span>)</span>
<span>{</span>
    <span>ASSERT_UNDER_CONSTEXPR_CONTEXT</span><span>(</span><span>leftMin</span> <span>&lt;</span> <span>leftMax</span><span>);</span>
    <span>ASSERT_UNDER_CONSTEXPR_CONTEXT</span><span>(</span><span>rightMin</span> <span>&lt;</span> <span>rightMax</span><span>);</span>

    <span>return</span> <span>leftMax</span> <span>&gt;</span> <span>rightMin</span> <span>&amp;&amp;</span> <span>rightMax</span> <span>&gt;</span> <span>leftMin</span><span>;</span>
<span>}</span>

<span>// Pass ranges with the min being inclusive and the max being exclusive.</span>
<span>template</span><span>&lt;</span><span>typename</span> <span>T</span><span>&gt;</span>
<span>constexpr</span> <span>bool</span> <span>rangesOverlap</span><span>(</span><span>T</span> <span>leftMin</span><span>,</span> <span>T</span> <span>leftMax</span><span>,</span> <span>T</span> <span>rightMin</span><span>,</span> <span>T</span> <span>rightMax</span><span>)</span> <span>{</span>
    <span>ASSERT_UNDER_CONSTEXPR_CONTEXT</span><span>(</span><span>leftMin</span> <span>&lt;=</span> <span>leftMax</span><span>);</span>
    <span>ASSERT_UNDER_CONSTEXPR_CONTEXT</span><span>(</span><span>rightMin</span> <span>&lt;=</span> <span>rightMax</span><span>);</span>

    <span>// Empty ranges interfere with nothing.</span>
    <span>if</span> <span>(</span><span>leftMin</span> <span>==</span> <span>leftMax</span><span>)</span>
        <span>return</span> <span>false</span><span>;</span>
    <span>if</span> <span>(</span><span>rightMin</span> <span>==</span> <span>rightMax</span><span>)</span>
        <span>return</span> <span>false</span><span>;</span>

    <span>return</span> <span>nonEmptyRangesOverlap</span><span>(</span><span>leftMin</span><span>,</span> <span>leftMax</span><span>,</span> <span>rightMin</span><span>,</span> <span>rightMax</span><span>);</span>
<span>}</span>
<span>}</span>

<span>class</span> <span>HeapRange</span> <span>{</span>
    <span>bool</span> <span>overlaps</span><span>(</span><span>const</span> <span>HeapRange</span><span>&amp;</span> <span>other</span><span>)</span> <span>const</span> <span>{</span>
        <span>return</span> <span>WTF</span><span>::</span><span>rangesOverlap</span><span>(</span><span>m_begin</span><span>,</span> <span>m_end</span><span>,</span> <span>other</span><span>.</span><span>m_begin</span><span>,</span> <span>other</span><span>.</span><span>m_end</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>(See also <a href="https://zayenz.se/blog/post/how-to-check-for-overlapping-intervals/">How to check for overlapping intervals</a> and
<a href="https://nedbatchelder.com/blog/201310/range_overlap_in_two_compares.html">Range overlap in two compares</a> for more fun.)</p>

<p>While bitsets are a dense representation (you have to hold every bit), they are
very compact and they are very precise. You can hold any number of combinations
of 64 or 128 bits in a single register. The union and intersection operations
are very cheap.</p>

<p>With int ranges, it‚Äôs a little more complicated. An imprecise union of <code>a</code> and
<code>b</code> can take the maximal range that covers both <code>a</code> and <code>b</code>. To get a more
precise union, you have to keep track of both. In the worst case, if you want
efficient arbitrary queries, you need to store your int ranges in an interval
tree. So what gives?</p>

<p>I asked Fil if both bitsets and int ranges answer the same question, why use
int ranges? He said that it‚Äôs more flexible long-term: bitsets get expensive as
soon as you need over 128 bits (you might need to heap allocate them!) whereas
ranges have no such ceiling. But doesn‚Äôt holding sequences of ranges require
heap allocation? Well, despite Fil writing this in his SSA post:</p>

<blockquote>
  <p>The purpose of the effect representation baked into the IR is to provide a
precise always-available baseline for alias information that is super easy to
work with. [‚Ä¶] you can have instructions report that they read/write
multiple heaps [‚Ä¶] you can have a utility function that produces such lists
on demand.</p>
</blockquote>

<p>It‚Äôs important to note that this doesn‚Äôt actually involve any allocation of
lists. JSC does this very clever thing where they have ‚Äúfunctors‚Äù that they
pass in as arguments that compress/summarize what they want to out of an
instruction‚Äôs effects.</p>

<p>Let‚Äôs take a look at how the DFG (for example) uses these heap ranges in
analysis. The DFG is structured in such a way that it can make use of the
DOMJIT heap ranges directly, which is neat.</p>

<p>Note that <code>AbstractHeap</code> in the example below is a thin wrapper over the DFG
compiler‚Äôs own <code>DOMJIT::HeapRange</code> equivalent:</p>

<div><pre><code><span>class</span> <span>AbstractHeapOverlaps</span> <span>{</span>
<span>public:</span>
    <span>AbstractHeapOverlaps</span><span>(</span><span>AbstractHeap</span> <span>heap</span><span>)</span>
        <span>:</span> <span>m_heap</span><span>(</span><span>heap</span><span>)</span>
        <span>,</span> <span>m_result</span><span>(</span><span>false</span><span>)</span>
    <span>{</span>
    <span>}</span>

    <span>void</span> <span>operator</span><span>()(</span><span>AbstractHeap</span> <span>otherHeap</span><span>)</span> <span>const</span>
    <span>{</span>
        <span>if</span> <span>(</span><span>m_result</span><span>)</span>
            <span>return</span><span>;</span>
        <span>m_result</span> <span>=</span> <span>m_heap</span><span>.</span><span>overlaps</span><span>(</span><span>otherHeap</span><span>);</span>
    <span>}</span>

    <span>bool</span> <span>result</span><span>()</span> <span>const</span> <span>{</span> <span>return</span> <span>m_result</span><span>;</span> <span>}</span>

<span>private:</span>
    <span>AbstractHeap</span> <span>m_heap</span><span>;</span>
    <span>mutable</span> <span>bool</span> <span>m_result</span><span>;</span>
<span>};</span>

<span>bool</span> <span>writesOverlap</span><span>(</span><span>Graph</span><span>&amp;</span> <span>graph</span><span>,</span> <span>Node</span><span>*</span> <span>node</span><span>,</span> <span>AbstractHeap</span> <span>heap</span><span>)</span>
<span>{</span>
    <span>NoOpClobberize</span> <span>noOp</span><span>;</span>
    <span>AbstractHeapOverlaps</span> <span>addWrite</span><span>(</span><span>heap</span><span>);</span>
    <span>clobberize</span><span>(</span><span>graph</span><span>,</span> <span>node</span><span>,</span> <span>noOp</span><span>,</span> <span>addWrite</span><span>,</span> <span>noOp</span><span>);</span>
    <span>return</span> <span>addWrite</span><span>.</span><span>result</span><span>();</span>
<span>}</span>
</code></pre></div>

<p><code>clobberize</code> is the function that calls these functors (<code>noOp</code> or <code>addWrite</code> in
this case) for each effect that the given IR instruction <code>node</code> declares.</p>

<p>I‚Äôve pulled some relevant snippets of <code>clobberize</code>, which is quite long, that I
think are interesting.</p>

<p>First, some instructions (constants, here) have no effects. There‚Äôs some
utility in the <code>def(PureValue(...))</code> call but I didn‚Äôt understand fully.</p>

<p>Then there are some instructions that conditionally have effects depending on
the use types of their operands.<sup id="fnref:dfg-use-type" role="doc-noteref"><a href="#fn:dfg-use-type" rel="footnote">1</a></sup> Taking the absolute value of an
Int32 or a Double is effect-free but otherwise looks like it can run arbitrary
code.</p>

<p>Some run-time IR guards that might cause side exits are annotated as
such‚Äîthey write to the <code>SideState</code> heap.</p>

<p>Local variable instructions read <em>specific</em> heaps indexed by what looks like
the local index but I‚Äôm not sure. This means accessing two different locals
won‚Äôt alias!</p>

<p>Instructions that allocate can‚Äôt be re-ordered, it looks like; they both read
and write the <code>HeapObjectCount</code>. This probably limits the amount of allocation
sinking that can be done.</p>

<p>Then there‚Äôs <code>CallDOM</code>, which is the builtins stuff I was talking about. We‚Äôll
come back to that after the code block.</p>

<div><pre><code><span>template</span><span>&lt;</span><span>typename</span> <span>ReadFunctor</span><span>,</span> <span>typename</span> <span>WriteFunctor</span><span>,</span> <span>typename</span> <span>DefFunctor</span><span>,</span> <span>typename</span> <span>ClobberTopFunctor</span><span>&gt;</span>
<span>void</span> <span>clobberize</span><span>(</span><span>Graph</span><span>&amp;</span> <span>graph</span><span>,</span> <span>Node</span><span>*</span> <span>node</span><span>,</span> <span>const</span> <span>ReadFunctor</span><span>&amp;</span> <span>read</span><span>,</span> <span>const</span> <span>WriteFunctor</span><span>&amp;</span> <span>write</span><span>,</span> <span>const</span> <span>DefFunctor</span><span>&amp;</span> <span>def</span><span>)</span>
<span>{</span>
    <span>// ...</span>

    <span>switch</span> <span>(</span><span>node</span><span>-&gt;</span><span>op</span><span>())</span> <span>{</span>
    <span>case</span> <span>JSConstant</span><span>:</span>
    <span>case</span> <span>DoubleConstant</span><span>:</span>
    <span>case</span> <span>Int52Constant</span><span>:</span>
        <span>def</span><span>(</span><span>PureValue</span><span>(</span><span>node</span><span>,</span> <span>node</span><span>-&gt;</span><span>constant</span><span>()));</span>
        <span>return</span><span>;</span>

    <span>case</span> <span>ArithAbs</span><span>:</span>
        <span>if</span> <span>(</span><span>node</span><span>-&gt;</span><span>child1</span><span>().</span><span>useKind</span><span>()</span> <span>==</span> <span>Int32Use</span> <span>||</span> <span>node</span><span>-&gt;</span><span>child1</span><span>().</span><span>useKind</span><span>()</span> <span>==</span> <span>DoubleRepUse</span><span>)</span>
            <span>def</span><span>(</span><span>PureValue</span><span>(</span><span>node</span><span>,</span> <span>node</span><span>-&gt;</span><span>arithMode</span><span>()));</span>
        <span>else</span>
            <span>clobberTop</span><span>();</span>
        <span>return</span><span>;</span>

    <span>case</span> <span>AssertInBounds</span><span>:</span>
    <span>case</span> <span>AssertNotEmpty</span><span>:</span>
        <span>write</span><span>(</span><span>SideState</span><span>);</span>
        <span>return</span><span>;</span>

    <span>case</span> <span>GetLocal</span><span>:</span>
        <span>read</span><span>(</span><span>AbstractHeap</span><span>(</span><span>Stack</span><span>,</span> <span>node</span><span>-&gt;</span><span>operand</span><span>()));</span>
        <span>def</span><span>(</span><span>HeapLocation</span><span>(</span><span>StackLoc</span><span>,</span> <span>AbstractHeap</span><span>(</span><span>Stack</span><span>,</span> <span>node</span><span>-&gt;</span><span>operand</span><span>())),</span> <span>LazyNode</span><span>(</span><span>node</span><span>));</span>
        <span>return</span><span>;</span>

    <span>case</span> <span>NewArrayWithSize</span><span>:</span>
    <span>case</span> <span>NewArrayWithSizeAndStructure</span><span>:</span>
        <span>read</span><span>(</span><span>HeapObjectCount</span><span>);</span>
        <span>write</span><span>(</span><span>HeapObjectCount</span><span>);</span>
        <span>return</span><span>;</span>

    <span>case</span> <span>CallDOM</span><span>:</span> <span>{</span>
        <span>const</span> <span>DOMJIT</span><span>::</span><span>Signature</span><span>*</span> <span>signature</span> <span>=</span> <span>node</span><span>-&gt;</span><span>signature</span><span>();</span>
        <span>DOMJIT</span><span>::</span><span>Effect</span> <span>effect</span> <span>=</span> <span>signature</span><span>-&gt;</span><span>effect</span><span>;</span>
        <span>if</span> <span>(</span><span>effect</span><span>.</span><span>reads</span><span>)</span> <span>{</span>
            <span>if</span> <span>(</span><span>effect</span><span>.</span><span>reads</span> <span>==</span> <span>DOMJIT</span><span>::</span><span>HeapRange</span><span>::</span><span>top</span><span>())</span>
                <span>read</span><span>(</span><span>World</span><span>);</span>
            <span>else</span>
                <span>read</span><span>(</span><span>AbstractHeap</span><span>(</span><span>DOMState</span><span>,</span> <span>effect</span><span>.</span><span>reads</span><span>.</span><span>rawRepresentation</span><span>()));</span>
        <span>}</span>
        <span>if</span> <span>(</span><span>effect</span><span>.</span><span>writes</span><span>)</span> <span>{</span>
            <span>if</span> <span>(</span><span>effect</span><span>.</span><span>writes</span> <span>==</span> <span>DOMJIT</span><span>::</span><span>HeapRange</span><span>::</span><span>top</span><span>())</span> <span>{</span>
                <span>if</span> <span>(</span><span>Options</span><span>::</span><span>validateDFGClobberize</span><span>())</span>
                    <span>clobberTopFunctor</span><span>();</span>
                <span>write</span><span>(</span><span>Heap</span><span>);</span>
            <span>}</span> <span>else</span>
                <span>write</span><span>(</span><span>AbstractHeap</span><span>(</span><span>DOMState</span><span>,</span> <span>effect</span><span>.</span><span>writes</span><span>.</span><span>rawRepresentation</span><span>()));</span>
        <span>}</span>
        <span>ASSERT_WITH_MESSAGE</span><span>(</span><span>effect</span><span>.</span><span>def</span> <span>==</span> <span>DOMJIT</span><span>::</span><span>HeapRange</span><span>::</span><span>top</span><span>(),</span> <span>"Currently, we do not accept any def for CallDOM."</span><span>);</span>
        <span>return</span><span>;</span>
    <span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div>

<p>(Remember that these <code>AbstractHeap</code> operations are very similar to DOMJIT‚Äôs
<code>HeapRange</code> with a couple more details‚Äîand in some cases even contain DOMJIT
<code>HeapRange</code>s!)</p>

<p>This <code>CallDOM</code> node is the way for the DOM APIs in the browser‚Äîa significant
chunk of the builtins, which are written in C++‚Äîto communicate what they do
to the optimizing compiler. Without any annotations, the JIT has to assume that
a call into C++ could do anything to the JIT state. Bummer!</p>

<p>But because, for example, <a href="https://developer.mozilla.org/en-US/docs/Web/API/Node/firstChild"><code>Node.firstChild</code></a> <a href="https://github.com/WebKit/WebKit/blob/32bda1b1d73527ba1d05ccba0aa8e463ddeac56d/Source/WebCore/domjit/JSNodeDOMJIT.cpp#L86">annotates what
memory it reads from</a> and what it <em>doesn‚Äôt</em> write to,
the JIT can optimize around it better‚Äîor even remove the access completely.
It means the JIT can reason about calls to known builtins <em>the same way</em> that
it reasons about normal JIT opcodes.</p>

<p>(Incidentally it looks like it doesn‚Äôt even make a C call, but instead is
inlined as a little memory read snippet using a JIT builder API. Neat.)</p>

<!-- TODO tie it back to the original example -->

<!--
B3 from JSC
https://github.com/WebKit/WebKit/blob/main/Source/JavaScriptCore/b3/B3Effects.h
https://github.com/WebKit/WebKit/blob/5811a5ad27100acab51f1d5ba4518eed86bbf00b/Source/JavaScriptCore/b3/B3AbstractHeapRepository.h

DOMJIT from JSC
https://github.com/WebKit/WebKit/blob/main/Source/WebCore/domjit/generate-abstract-heap.rb
generates from https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/WebCore/domjit/DOMJITAbstractHeapRepository.yaml#L4

DFG from JSC
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGAbstractHeap.h
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGClobberize.h
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGClobberize.cpp
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGClobberize.h
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGStructureAbstractValue.cpp
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGStructureAbstractValue.h
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGClobberSet.h
https://github.com/WebKit/WebKit/blob/b99cb96a7a3e5978b475d2365b72196e15a1a326/Source/JavaScriptCore/dfg/DFGStructureAbstractValue.h
-->

<p>Last, we‚Äôll look at Simple, which has a slightly different take on all of this.</p>

<h2 id="simple">Simple</h2>

<p><a href="https://github.com/seaofnodes/simple">Simple</a> is Cliff Click‚Äôs pet Sea of
Nodes (SoN) project to try and showcase the idea to the world‚Äîoutside of a
HotSpot C2 context.</p>

<p>This one is a little harder for me to understand but it looks like each
translation unit has a <a href="https://github.com/SeaOfNodes/Simple/blob/1426384fc7d0e9947e38ad6d523a5e53c324d710/chapter10/src/main/java/com/seaofnodes/simple/node/StartNode.java#L33"><code>StartNode</code></a> that doles out
different classes of memory nodes for each alias class. Each IR node then takes
data dependencies on whatever effect nodes it might uses.</p>

<p>Alias classes are split up based on the paper <a href="https://bernsteinbear.com/assets/img/tbaa.pdf">Type-Based Alias Analysis</a>
(PDF): ‚ÄúOur approach is a form of TBAA similar to the ‚ÄòFieldTypeDecl‚Äô algorithm
described in the paper.‚Äù</p>

<!--

Cliff Click says:

All effects are represented as edges in the graph, the same edges as normal value flows, and all edges in Simple/C2 are simple pointers (and hence are unlabeled).

StartNode produces all effects and StopNode consumes them; same for Call and CallEnd.
Effects, being just another form of value, can be merged in PhiNodes.
Effects are generally split into smaller disjoint pieces, and recombined before Stop/Call.  Splitting into disjoint pieces allows more precision in the IR, and so more optimizations.
The common first split is the Memory effect from all other effects.  Other effects are generally some form of abstract i/o (all file system operations, reading/writing device controller memory, all external calls to disjoint address spaces, etc), or control.  Control is Just Another Edge denoting normal control flow, and e.g. data ops that depend on a prior control op use it to guard for safety.  Things like div-by-0, or null-ptr-check, or array-index-OOB are all done with a control edge to the guarding test.

Memory effects are further split into disjoint aliases; operations in one alias class can never overlap with another (this is a Y/N choice, not a may/must choice).  These aliases are equivalence classes; all mem ops belong in exactly one class, and the set of classes exactly partitions all of memory.  Common splits are fields in a struct (no 'f' field ever overlaps with any 'g' field), or kinds of arrays (no int[] overlaps with a flt[]).

In this example a = l[0]; l[0] = 5, we might have as IR:

a = Load(ctrl-for-AIOOB, mem-for-int[], offset);
mem-for-int[] = Store(ctrl-for-AIOOB, mem-for-int[], offset, 5)



Note that the Load and Store are not ordered here.  This Store IS ordered against all other int[] Stores.
The serializing algo Global Code Motion will add an anti-dep as needed, and then order the Load & Store.

Splitting is basically by having a "narrow" user read from a "fat memory".  Narrow, because its using a single alias and is one of the memops (e.g. Loads and Stores).  A "fat memory" always comes from Start & CallEnd.  A MemMerge can merge a bunch of narrow aliases (and one fat) and make a fat memory.  Basically its all done lazily by "doing nothing", and requiring the graph builder not produce a junk graph.

Splitting happens when the Parser decides you are manipulating a slice.
THere are some peephole's for widening the split region over a larger area, allowing more memory optimizations in the larger wider area.
Load & Stores have a peep to move "up past" a MemMerge on the correct alias edge.
-->

<p>The Simple project is structured into sequential implementation stages and
alias classes come into the picture in <a href="https://github.com/SeaOfNodes/Simple/tree/main/chapter10">Chapter 10</a>.</p>

<p>Because I spent a while spelunking through other implementations to see how
other projects did this, here is a list of the projects I looked at. Mostly,
they use bitsets.</p>

<h2 id="other-implementations">Other implementations</h2>

<h3 id="hhvm">HHVM</h3>

<p><a href="https://github.com/facebook/hhvm">HHVM</a>, a JIT for the
<a href="https://hacklang.org/">Hack</a> language, also uses a bitset for its memory
effects. See for example: <a href="https://github.com/facebook/hhvm/blob/0395507623c2c08afc1d54c0c2e72bc8a3bd87f1/hphp/runtime/vm/jit/alias-class.h">alias-class.h</a> and
<a href="https://github.com/facebook/hhvm/blob/0395507623c2c08afc1d54c0c2e72bc8a3bd87f1/hphp/runtime/vm/jit/memory-effects.h">memory-effects.h</a>.</p>

<p>HHVM has a couple places that use this information, such as <a href="https://github.com/facebook/hhvm/blob/4cdb85bf737450bf6cb837d3167718993f9170d7/hphp/runtime/vm/jit/def-sink.cpp">a
definition-sinking pass</a>, <a href="https://github.com/facebook/hhvm/blob/0395507623c2c08afc1d54c0c2e72bc8a3bd87f1/hphp/runtime/vm/jit/alias-analysis.h">alias
analysis</a>, <a href="https://github.com/facebook/hhvm/blob/4cdb85bf737450bf6cb837d3167718993f9170d7/hphp/runtime/vm/jit/dce.cpp">DCE</a>, <a href="https://github.com/facebook/hhvm/blob/4cdb85bf737450bf6cb837d3167718993f9170d7/hphp/runtime/vm/jit/store-elim.cpp">store
elimination</a>, <a href="https://github.com/facebook/hhvm/blob/1f9eda80656b79634b6956084481ed5a43d8bc2e/hphp/runtime/vm/jit/refcount-opts.cpp">refcount opts</a>, and
more.</p>

<p>If you are wondering why the HHVM representation looks similar to the Cinder
representation, it‚Äôs because some former HHVM engineers such as Brett Simmers
also worked on Cinder!</p>

<h3 id="android-art">Android ART</h3>

<p>(note that I am linking an ART fork on GitHub as a reference, but the upstream
code is <a href="https://android.googlesource.com/platform/art/+/refs/heads/main/compiler/optimizing/nodes.h">hosted on googlesource</a>)</p>

<p>Android‚Äôs <a href="https://source.android.com/docs/core/runtime">ART Java runtime</a> also
uses a bitset for its effect representation. It‚Äôs a very compact class called
<code>SideEffects</code> in <a href="https://github.com/LineageOS/android_art/blob/c09a5c724799afdc5f89071b682b181c0bd23099/compiler/optimizing/nodes.h#L1602">nodes.h</a>.</p>

<p>The side effects are used in <a href="https://github.com/LineageOS/android_art/blob/c09a5c724799afdc5f89071b682b181c0bd23099/compiler/optimizing/licm.cc#L104">loop-invariant code motion</a>, <a href="https://github.com/LineageOS/android_art/blob/c09a5c724799afdc5f89071b682b181c0bd23099/compiler/optimizing/gvn.cc#L204">global
value numbering</a>, <a href="https://github.com/LineageOS/android_art/blob/c09a5c724799afdc5f89071b682b181c0bd23099/compiler/optimizing/write_barrier_elimination.cc#L45">write barrier
elimination</a>, <a href="https://github.com/LineageOS/android_art/blob/c09a5c724799afdc5f89071b682b181c0bd23099/compiler/optimizing/scheduler.cc#L55">scheduling</a>,
and more.</p>

<h3 id="netcoreclr">.NET/CoreCLR</h3>

<p>CoreCLR mostly <a href="https://github.com/dotnet/runtime/blob/a0878687d02b42034f4ea433ddd7a72b741510b8/src/coreclr/jit/sideeffects.h#L169">uses a bitset</a> for its <code>SideEffectSet</code>
class. This one is interesting though because it also splits out effects
specifically to include sets of local variables (<code>LclVarSet</code>).</p>

<h3 id="v8">V8</h3>

<p>V8 is also about six completely different compilers in a trenchcoat.</p>

<p>Turboshaft uses a struct in <a href="https://github.com/v8/v8/blob/e817fdf31a2947b2105bd665067d92282e4b4d59/src/compiler/turboshaft/operations.h#L577">operations.h</a> called
<code>OpEffects</code> which is two bitsets for reads/writes of effects. This is used in
<a href="https://github.com/v8/v8/blob/42f5ff65d12f0ef9294fa7d3875feba938a81904/src/compiler/turboshaft/value-numbering-reducer.h#L164">value numbering</a> as well a bunch of
other small optimization passes they call ‚Äúreducers‚Äù.</p>

<p>Maglev also has this thing called <code>NodeT::kProperties</code> in <a href="https://github.com/v8/v8/blob/42f5ff65d12f0ef9294fa7d3875feba938a81904/src/maglev/maglev-ir.h">their IR
nodes</a> that also looks like a bitset and is used in their various
reducers. It has effect query methods on it such as <code>can_eager_deopt</code> and
<code>can_write</code>.</p>

<p>Until recently, V8 also used Sea of Nodes as its IR representation, which also
tracks side effects more explicitly in the structure of the IR itself.</p>

<h2 id="guile">Guile</h2>

<p><a href="https://www.gnu.org/software/guile/">Guile Scheme</a> looks like it has a <a href="https://wingolog.org/archives/2014/05/18/effects-analysis-in-guile">custom tagging
scheme</a> type thing.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Both bitsets and int ranges are perfectly cromulent ways of representing heap
effects for your IR. The Sea of Nodes approach is also probably okay since it
powers HotSpot C2 and (for a time) V8.</p>

<p>Remember to ask <em>the right questions</em> of your IR when doing analysis.</p>

<h2 id="thank-you">Thank you</h2>

<p>Thank you to <a href="http://www.filpizlo.com/">Fil Pizlo</a> for writing his initial
GitHub Gist and sending me on this journey and thank you to <a href="https://www.chrisgregory.me/">Chris
Gregory</a>, Brett Simmers, and <a href="https://ufuk.dev/">Ufuk
Kayserilioglu</a> for feedback on making some of the
explanations more helpful.</p>

<!--

TODO Dart
https://github.com/dart-lang/sdk/blob/59905c43f1a0394394ad5545ee439bcba63dea55/runtime/vm/constants_riscv.h#L968
https://github.com/dart-lang/sdk/blob/59905c43f1a0394394ad5545ee439bcba63dea55/runtime/vm/compiler/backend/redundancy_elimination.cc#L758
https://github.com/dart-lang/sdk/blob/59905c43f1a0394394ad5545ee439bcba63dea55/runtime/vm/compiler/backend/redundancy_elimination.cc#L1096

ChakraCore
https://github.com/chakra-core/ChakraCore/blob/2dba810c925eb366e44a1f7d7a5b2e289e2f8510/lib/Runtime/Types/RecyclableObject.h#L172

SpiderMonkey
https://github.com/servo/mozjs/blob/77645ed41f588297fd8d7edaee71500f4c83d070/mozjs-sys/mozjs/js/src/jit/MIR.h#L935
https://github.com/servo/mozjs/blob/77645ed41f588297fd8d7edaee71500f4c83d070/mozjs-sys/mozjs/js/src/jit/MIR.h#L9658

Cinder LIR
https://github.com/facebookincubator/cinderx/blob/main/cinderx/Jit/lir/instruction.h

HotSpot C1

HotSpot C2

PyPy
https://github.com/pypy/pypy/blob/main/rpython/jit/codewriter/effectinfo.py
https://github.com/pypy/pypy/blob/main/rpython/jit/metainterp/optimizeopt/heap.py#L59

LLVM
https://llvm.org/docs/LangRef.html#tbaa-metadata

LLVM MemorySSA
https://llvm.org/docs/MemorySSA.html

MLIR
https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/

MEMOIR
https://conf.researchr.org/details/cgo-2024/cgo-2024-main-conference/31/Representing-Data-Collections-in-an-SSA-Form

Scala LMS graph IR
https://2023.splashcon.org/details/splash-2023-oopsla/46/Graph-IRs-for-Impure-Higher-Order-Languages-Making-Aggressive-Optimizations-Affordab

MIR and borrow checker
https://rustc-dev-guide.rust-lang.org/part-3-intro.html#source-code-representation

> "Fabrice Rastello, Florent Bouchez Tichadou (2022) SSA-based Compiler Design"--most (all?) chapters in Part III, Extensions, are pretty much motivated by doing alias analysis in some way

Intermediate Representations in Imperative Compilers: A Survey
http://kameken.clique.jp/Lectures/Lectures2013/Compiler2013/a26-stanier.pdf

Partitioned Lattice per Variable (PLV) -- that's in Chapter 13 on SSI

TODO maybe lattice in ascent

-->


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Agentic pelican on a bicycle (102 pts)]]></title>
            <link>https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/</link>
            <guid>45891817</guid>
            <pubDate>Tue, 11 Nov 2025 19:40:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/">https://www.robert-glaser.de/agentic-pelican-on-a-bicycle/</a>, See on <a href="https://news.ycombinator.com/item?id=45891817">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <p>Simon Willison has been running his own informal model benchmark for years: <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/?ref=robert-glaser.de" rel="noreferrer">‚ÄúGenerate an SVG of a pelican riding a bicycle.‚Äù</a> It‚Äôs delightfully absurd‚Äîand surprisingly revealing. Even the model labs channel this benchmark in their marketing campaigns announcing new models.</p><p>Simon‚Äôs traditional approach is zero-shot: throw the prompt at the model, get SVG back. Maybe‚Äîif you‚Äôre lucky‚Äîyou get something resembling a pelican on a bicycle.</p><p>Nowadays everyone is talking about agents. Models running in a loop using tools. Sometimes they have vision capabilities, too. They can look at what they just created, cringe a little, and try again. The agentic loop‚Äîgenerate, assess, improve‚Äîseems like a natural fit for such a task.</p><p>So I ran a different experiment: what if we let models iterate on their pelicans? What if they could see their own output and self-correct?</p><h2 id="the-prompt">The Prompt</h2><pre><code>Generate an SVG of a pelican riding a bicycle

- Convert the .svg to .jpg using chrome devtools, then look at the .jpg using your vision capabilities.
- Improve the .svg based on what you see in the .jpg and what's still to improve.
- Keep iterating in this loop until you're satisfied with the generated svg.
- Keep the .jpg for every iteration along the way.</code></pre><p>Besides the file system and access to a command line, the models had access to <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp?ref=robert-glaser.de" rel="noreferrer">Chrome DevTools MCP server</a> (for SVG-to-JPG conversion) and their own multimodal vision capabilities. They could see what they‚Äôd drawn, identify problems, and iterate. The loop continued until they declared satisfaction.</p><p>I used the Chrome DevTools MCP server to give every model the same rasterizer. Without this, models would fall back to whatever SVG-to-image conversion they prefer or have available locally‚ÄîImageMagick, Inkscape, browser screenshots, whatever. Standardizing the rendering removes one variable from the equation.</p><p>The prompt itself is deliberately minimal. I could have steered the iterative loop with more specific guidance‚Äî‚Äúfocus on anatomical accuracy,‚Äù ‚Äúprioritize mechanical realism,‚Äù ‚Äúensure visual balance.‚Äù But that would defeat the point. Simon‚Äôs original benchmark is beautifully unconstrained, and I wanted to preserve that spirit. The question isn‚Äôt ‚Äúcan models follow detailed improvement instructions?‚Äù It‚Äôs ‚Äúwhen left to their own judgment, what do they choose to fix?‚Äù</p><h2 id="the-models">The Models</h2><p>I tested six models across the frontier, all multimodal:</p><ul><li>Claude Opus 4.1, Claude Sonnet 4.5, Claude Haiku 4.5, all with thinking</li><li>GPT-5 (on medium reasoning effort)</li><li>GPT-5-Codex (on medium reasoning effort)</li><li>Gemini 2.5 Pro</li></ul><p>Each model decided independently when to stop iterating. Some made four passes. Others kept going for six. None knew when to quit.</p><h2 id="the-results">The Results</h2><p>Let‚Äôs see what happened. For each model, I‚Äôm showing the first attempt (left) and the final result (right) after self-correction.</p><h3 id="claude-opus-41-4-iterations">Claude Opus 4.1 (4 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican_bicycle_v1-1.svg" width="800" height="600" loading="lazy" alt=""></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican_bicycle_v4_final-1.svg" width="800" height="600" loading="lazy" alt=""></p></div></figure><p>Opus started with a serviceable pelican-bicycle combo and then did something interesting: it added realism. The final version has an actual bicycle chain connecting the pedals to the rear wheel. The wheels gained more spokes. The pelican‚Äôs proportions improved, and it got arms holding the handlebars. This wasn‚Äôt just ‚Äúadd more details‚Äù‚Äîit was ‚Äúmake this mechanically coherent.‚Äù Interestingly, we got the catch of the day on a special plate on the handlebars. Oh, and look at the street and the birds in the backdrop!</p><h3 id="claude-sonnet-45-4-iterations">Claude Sonnet 4.5 (4 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-bicycle-v1-2.svg" width="800" height="600" loading="lazy" alt=""></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-bicycle-v4-1.svg" width="800" height="600" loading="lazy" alt=""></p></div></figure><p>Sonnet took a more restrained approach. The changes between iterations were subtler‚Äîrefinements to curves, adding shadows, and movement indicators. Adjustments to positioning. Improving the spokes. Improving the arms and handlebars. The final result is cleaner, but the core composition remained remarkably stable.</p><h3 id="claude-haiku-45-6-iterations">Claude Haiku 4.5 (6 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/iteration-1-1.jpg" width="2000" height="1330" loading="lazy" alt="" srcset="https://www.robert-glaser.de/content/images/size/w600/2025/11/iteration-1-1.jpg 600w, https://www.robert-glaser.de/content/images/size/w1000/2025/11/iteration-1-1.jpg 1000w, https://www.robert-glaser.de/content/images/size/w1600/2025/11/iteration-1-1.jpg 1600w, https://www.robert-glaser.de/content/images/2025/11/iteration-1-1.jpg 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/iteration-6-final-1.jpg" width="2000" height="1330" loading="lazy" alt="" srcset="https://www.robert-glaser.de/content/images/size/w600/2025/11/iteration-6-final-1.jpg 600w, https://www.robert-glaser.de/content/images/size/w1000/2025/11/iteration-6-final-1.jpg 1000w, https://www.robert-glaser.de/content/images/size/w1600/2025/11/iteration-6-final-1.jpg 1600w, https://www.robert-glaser.de/content/images/2025/11/iteration-6-final-1.jpg 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>Haiku took the longest journey‚Äîsix full iterations. It kept tweaking, kept adjusting. The additional iterations didn‚Äôt necessarily produce a dramatically better result, but Haiku seemed determined to get every detail right before calling it done. So the pelican definitely received proper legs and feet. </p><h3 id="gpt-5-medium-5-iterations">GPT-5 Medium (5 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-01.jpg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.robert-glaser.de/content/images/size/w600/2025/11/pelican-01.jpg 600w, https://www.robert-glaser.de/content/images/size/w1000/2025/11/pelican-01.jpg 1000w, https://www.robert-glaser.de/content/images/size/w1600/2025/11/pelican-01.jpg 1600w, https://www.robert-glaser.de/content/images/size/w2400/2025/11/pelican-01.jpg 2400w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-05.jpg" width="2000" height="1500" loading="lazy" alt="" srcset="https://www.robert-glaser.de/content/images/size/w600/2025/11/pelican-05.jpg 600w, https://www.robert-glaser.de/content/images/size/w1000/2025/11/pelican-05.jpg 1000w, https://www.robert-glaser.de/content/images/size/w1600/2025/11/pelican-05.jpg 1600w, https://www.robert-glaser.de/content/images/size/w2400/2025/11/pelican-05.jpg 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>GPT-5 Medium started with a recognizable pelican-bicycle scene and refined it over five iterations. The improvements were incremental‚Äîbetter proportions, clearer shapes‚Äîbut the fundamental composition held steady throughout.</p><h3 id="gpt-5-codex-medium-5-iterations">GPT-5-Codex Medium (5 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-bike-01.svg" width="800" height="800" loading="lazy" alt=""></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/pelican-bike-05.svg" width="960" height="720" loading="lazy" alt=""></p></div></figure><p>Here‚Äôs where things get interesting. Its initial attempt was‚Ä¶ let‚Äôs call it ‚Äúabstract.‚Äù A sort of layer cake of pelican parts. And then, instead of simplifying, it doubled down. The final result added even more layers. More complexity. More parts. Whether this counts as ‚Äúimprovement‚Äù is a philosophical question I‚Äôm not qualified to answer.</p><h3 id="gemini-25-pro-6-iterations">Gemini 2.5 Pro (6 iterations)</h3><figure><div><p><img src="https://www.robert-glaser.de/content/images/2025/11/iteration-1.svg" width="150" height="150" loading="lazy" alt=""></p><p><img src="https://www.robert-glaser.de/content/images/2025/11/iteration-6-final-2.jpg" width="2000" height="1067" loading="lazy" alt="" srcset="https://www.robert-glaser.de/content/images/size/w600/2025/11/iteration-6-final-2.jpg 600w, https://www.robert-glaser.de/content/images/size/w1000/2025/11/iteration-6-final-2.jpg 1000w, https://www.robert-glaser.de/content/images/size/w1600/2025/11/iteration-6-final-2.jpg 1600w, https://www.robert-glaser.de/content/images/size/w2400/2025/11/iteration-6-final-2.jpg 2400w" sizes="(min-width: 720px) 720px"></p></div></figure><p>Gemini was the outlier. Most models preserved their initial composition through iterations, making refinements but keeping the core structure mostly intact. Gemini actually changed the fundamental arrangement‚Äîthe pelican‚Äôs pose, the bicycle‚Äôs orientation, the spatial relationship between them. Six iterations showed a bigger leap.</p><h2 id="what-did-we-learn">What Did We Learn?</h2><p>The results are‚Ä¶ mixed.</p><p><strong>The optimistic take</strong>: Models like Opus 4.1 made genuinely thoughtful improvements. Adding a bicycle chain isn‚Äôt just decoration‚Äîit shows understanding of mechanical relationships. The wheel spokes, the adjusted proportions‚Äîthese are signs of vision-driven refinement working as intended.</p><p><strong>The skeptical take</strong>: Most models didn‚Äôt fundamentally change their approach. They tweaked. They adjusted. They added details. But the basic composition‚Äîpelican shape, bicycle shape, spatial relationship‚Äîwas determined in iteration one and largely frozen thereafter.</p><p><strong>The confusing take</strong>: Some models (looking at you, GPT-5-Codex) seemed to mistake ‚Äúmore complex‚Äù for ‚Äúbetter.‚Äù The self-feedback loop amplified their initial artistic direction rather than correcting it. If your first draft is a layer cake of pelican parts, and your self-correction produces an even more elaborate layer cake... did the loop help? Of course, GPT-5-Codex is a fine-tune of GPT-5, optimized for engineering tasks. Could be that its strengths are not in broad visual capabilities.</p><p>The agentic approach definitely produces different results than zero-shot generation. Whether it produces <em>better</em> results seems to depend heavily on the model‚Äôs ability to self-critique. Vision capabilities alone aren‚Äôt enough‚Äîyou need something more: aesthetic judgment, mechanical reasoning, or at least the wisdom to know when to stop adding details.</p><p>Simon‚Äôs zero-shot benchmark reveals how well models handle unusual creative tasks on the first try. The agentic variant reveals something else: how well models can evaluate and improve their own creative output. Turns out, that‚Äôs a different skill entirely. But‚Äîsomehow related?</p><hr><p><em>All test code and results available in the </em><a href="https://github.com/youngbrioche/agentic-pelican-on-a-bicycle?ref=robert-glaser.de"><em>GitHub</em></a><em> repository.</em></p>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg to Google: Fund Us or Stop Sending Bugs (1025 pts)]]></title>
            <link>https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/</link>
            <guid>45891016</guid>
            <pubDate>Tue, 11 Nov 2025 18:32:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/">https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/</a>, See on <a href="https://news.ycombinator.com/item?id=45891016">Hacker News</a></p>
Couldn't get https://thenewstack.io/ffmpeg-to-google-fund-us-or-stop-sending-bugs/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Terminal Latency on Windows (2024) (105 pts)]]></title>
            <link>https://chadaustin.me/2024/02/windows-terminal-latency/</link>
            <guid>45890726</guid>
            <pubDate>Tue, 11 Nov 2025 18:07:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chadaustin.me/2024/02/windows-terminal-latency/">https://chadaustin.me/2024/02/windows-terminal-latency/</a>, See on <a href="https://news.ycombinator.com/item?id=45890726">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><strong>UPDATE 2024-04-15</strong>: Windows Terminal 1.19 contains a fix that
reduces latency by half! It‚Äôs now competitive with WSLtty on my
machine. Details in the <a href="https://github.com/microsoft/terminal/issues/5590">GitHub
Issue</a>.</p>

<p>In 2009, I wrote about <a href="https://chadaustin.me/2009/10/reasons-why-mintty-is-the-best-terminal-on-windows/">why MinTTY is the best terminal on
Windows</a>.
Even today, that post is one of my most popular.</p>

<figure>
<a href="https://chadaustin.me/wp-uploads/mintty_right_click.png"><img src="https://chadaustin.me/wp-uploads/mintty_right_click.png" alt="MinTTY in 2009"></a>
<figcaption>MinTTY in 2009</figcaption>
</figure>

<p>Since then, the terminal situation on Windows has improved:</p>
<ul>
  <li>Cygwin defaults to MinTTY; you no longer need to manually install
it.</li>
  <li>Windows added <a href="https://devblogs.microsoft.com/commandline/windows-command-line-introducing-the-windows-pseudo-console-conpty/">PTY
support</a>,
obviating the need for offscreen console window hacks that add
latency.</li>
  <li>Windows added basically full support for <a href="https://learn.microsoft.com/en-us/windows/console/console-virtual-terminal-sequences">ANSI terminal
sequences</a>
in both the legacy conhost.exe consoles and its new <a href="https://github.com/microsoft/terminal">Windows
Terminal</a>.</li>
  <li>We now have a variety of terminals to choose from, even on Windows:
<a href="https://cmder.app/">Cmder</a>, <a href="https://conemu.github.io/">ConEmu</a>,
<a href="https://alacritty.org/">Alacritty</a>,
<a href="https://wezfurlong.org/wezterm/index.html">WezTerm</a>,
<a href="http://xtermjs.org/">xterm.js</a> (component of Visual Studio Code)</li>
</ul>

<p>The beginning of a year is a great time to look at your tools and
improve your environment.</p>

<p>I‚Äôd already <a href="https://chadaustin.me/2024/01/truecolor-terminal-emacs/">enabled 24-bit color in all of my
environments</a>
and <a href="https://chadaustin.me/2024/02/tmux-config/">streamlined my tmux
config</a>. It‚Äôs about time
that I take a look at the newer terminals.</p>

<p>Roughly in order, I care about:</p>
<ul>
  <li>Minimum feature set: 24-bit color, reasonable default fonts with
emoji support, italics are nice.</li>
  <li>Input latency.</li>
  <li>Throughput at line rate, for example, when I <code>cat</code> a large file.</li>
  <li>Support for multiple tabs in one window would be nice, but tmux
suffices for me.</li>
</ul>

<h2 id="which-terminals-should-i-test">Which terminals should I test?</h2>

<p>I considered the following.</p>

<ul>
  <li>Legacy conhost.exe (also known as Windows Console), Windows 10 19045</li>
  <li>MinTTY (3.7.0)</li>
  <li>Alacritty (0.13.1)</li>
  <li>WezTerm (20240203-110809-5046fc22)</li>
  <li>Windows Terminal (1.18.10301.0)</li>
</ul>

<h2 id="testing-features">Testing Features</h2>

<p>Testing color and italics support is easy with my
<a href="https://gist.github.com/chadaustin/2d2c2cb4b71fd1d4163aa8115077624a">colortest.rs</a>
script. To test basic emoji, you can cat the <a href="https://unicode.org/Public/emoji/1.0/emoji-data.txt">Unicode emoji 1.0
emoji-data.txt</a>.
To test more advanced support, try the zero-width joiner list in the
<a href="https://unicode.org/Public/emoji/latest/">latest/</a> directory.</p>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Emoji</th>
      <th>Font Attributes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conhost.exe</td>
      <td>No</td>
      <td>No italics</td>
    </tr>
    <tr>
      <td>MinTTY</td>
      <td>Black and white</td>
      <td>All major attributes</td>
    </tr>
    <tr>
      <td>Alacritty</td>
      <td>Black and white</td>
      <td>Everything but double underline</td>
    </tr>
    <tr>
      <td>WezTerm</td>
      <td><a href="https://wezfurlong.org/wezterm/config/fonts.html">Color</a></td>
      <td>All major attributes</td>
    </tr>
    <tr>
      <td>Windows Terminal</td>
      <td>Color</td>
      <td>All major attributes</td>
    </tr>
  </tbody>
</table>

<p>Everything but conhost.exe meets my bar.</p>

<p>It‚Äôs also worth noting that conhost.exe has a terrible default
palette. The default yellow is a pukey green and dark blue is barely
visible. You can change palettes, but defaults matter.</p>

<figure>
<a href="https://chadaustin.me/images/windows-terminal-latency/default-palette-conhost.png"><img src="https://chadaustin.me/images/windows-terminal-latency/default-palette-conhost.png" alt="Conhost.exe Default Palette"></a>
<figcaption>Conhost.exe Default Palette</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/windows-terminal-latency/default-palette-mintty.png"><img src="https://chadaustin.me/images/windows-terminal-latency/default-palette-mintty.png" alt="MinTTY Default Palette"></a>
<figcaption>MinTTY Default Palette</figcaption>
</figure>



<p>I set up two latency tests. One with an 80x50 blank window in the
upper left corner of the screen. The other fullscreen, editing an
Emacs command at the bottom of the screen.</p>

<p>Since latencies are additive, system configuration doesn‚Äôt matter as
much as the absolute milliseconds of latency each terminal adds, but
I‚Äôll describe my entire setup and include total keypress-to-pixels
latency.</p>

<ul>
  <li>Windows 10</li>
  <li>Intel i7-4771 @ 3.5 GHz</li>
  <li>NVIDIA GTX 1060</li>
  <li>Keyboard: <a href="https://1upkeyboards.com/shop/keyboard-kits/macro-pads/sweet16-macro-pad-white/">Sweet 16 Macro
Pad</a></li>
  <li>Display: <a href="https://www.lg.com/us/monitors/lg-27gp950-b-gaming-monitor">LG
27GP950-B</a>
at 4K, 120 Hz, adaptive sync</li>
</ul>

<h3 id="measurement-methodology">Measurement Methodology</h3>

<p>With <a href="https://isitsnappy.com/">Is It Snappy?</a>, I measured the number
of frames between pressing a key and pixels changing on the screen.</p>

<p>To minimize ambiguity about when the key was pressed, I slammed a
pencil‚Äôs eraser into the key, and always measured the key press as the
<em>second</em> frame after contact. (The first frame was usually when the
eraser barely touched the key. It would usually clear the activation
depth by the second frame.)</p>

<p>I considered the latency to end when pixels just started to change on
the screen. In practice, pixels take several 240 Hz frames to
transition from black to white, but I consistently marked the
beginning of that transition.</p>

<p>I took five measurements for each configuration and picked the median.
Each measurement was relatively consistent, so average would have been
a fine metric too. It doesn‚Äôt change the results below.</p>

<h3 id="80x50">80x50</h3>

<p>80x50 window, upper left of screen, cleared terminal, single keypress.</p>

<p>Confirmed window size with:</p>

<div><pre><code>$ echo $(tput cols)x$(tput lines)
80x50
</code></pre></div>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Median Latency (ms)</th>
      <th>240 Hz Camera Frames</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conhost.exe WSL1</td>
      <td>33.3</td>
      <td>8</td>
    </tr>
    <tr>
      <td>MinTTY WSL1</td>
      <td>33.3</td>
      <td>8</td>
    </tr>
    <tr>
      <td>conhost.exe Cygwin</td>
      <td>41.3</td>
      <td>10</td>
    </tr>
    <tr>
      <td>MinTTY Cygwin</td>
      <td>57.9</td>
      <td>14</td>
    </tr>
    <tr>
      <td>WezTerm cmd.exe</td>
      <td>62.5</td>
      <td>15</td>
    </tr>
    <tr>
      <td>Alacritty WSL1</td>
      <td>62.5</td>
      <td>15</td>
    </tr>
    <tr>
      <td>WezTerm WSL1</td>
      <td>66.7</td>
      <td>16</td>
    </tr>
    <tr>
      <td>Windows Terminal WSL1</td>
      <td>66.7</td>
      <td>16</td>
    </tr>
  </tbody>
</table>

<h3 id="fullscreen">Fullscreen</h3>

<p>Maximized emacs, editing a command in the bottom row of the terminal.
I only tested WSL1 this time.</p>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Median Latency (ms)</th>
      <th>240 Hz Camera Frames</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conhost.exe</td>
      <td>45.8</td>
      <td>11</td>
    </tr>
    <tr>
      <td>MinTTY</td>
      <td>52.42</td>
      <td>12</td>
    </tr>
    <tr>
      <td>WezTerm</td>
      <td>75</td>
      <td>18</td>
    </tr>
    <tr>
      <td>Windows Terminal</td>
      <td>75</td>
      <td>18</td>
    </tr>
    <tr>
      <td>Alacritty</td>
      <td>87.5</td>
      <td>21</td>
    </tr>
  </tbody>
</table>

<h3 id="throughput">Throughput</h3>

<p>I generated a 100,000-line file with:</p>

<div><pre><code>$ yes "This sentence has forty-five (45) characters." | head -n 100000 &gt; /tmp/lines.txt
</code></pre></div>

<p>Then I measured the wall-clock duration of:</p>

<div><pre><code>$ time cat /tmp/lines.txt
</code></pre></div>

<p>This benchmark captures the case that I accidentally dump a ton of
output and I‚Äôm sitting there just waiting for the terminal to become
responsive again. I have a gigabit internet connection, and it‚Äôs
embarrassing to be CPU-bound instead of IO-bound.</p>

<p>I did include Cygwin in this test, just to have two different MinTTY
datapoints.</p>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Elapsed Time (s)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MinTTY WSL1</td>
      <td>0.57</td>
    </tr>
    <tr>
      <td>MinTTY Cygwin</td>
      <td>2.2</td>
    </tr>
    <tr>
      <td>Windows Terminal</td>
      <td>5.25</td>
    </tr>
    <tr>
      <td>Alacritty</td>
      <td>5.75</td>
    </tr>
    <tr>
      <td>WezTerm</td>
      <td>6.2</td>
    </tr>
    <tr>
      <td>conhost.exe</td>
      <td>21.8</td>
    </tr>
  </tbody>
</table>

<p>I assume this means MinTTY throttles display updates in some way. Of
course this is totally fine, because you couldn‚Äôt read the output
either way.</p>

<p>To test the hypothesis that MinTTY was caching cell rendering by
their contents, I also tried generating a file that rotated through
different lines, with no effect.</p>

<div><pre><code><span>with</span> <span>open</span><span>(</span><span>"/tmp/lines2.txt"</span><span>,</span> <span>"w"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
  <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>100000</span><span>):</span>
    <span>sentence</span><span>=</span><span>"This sentence has forty-five (45) characters."</span>
    <span>print</span><span>(</span><span>sentence</span><span>[</span><span>i</span><span>%</span><span>len</span><span>(</span><span>sentence</span><span>):]</span><span>+</span><span>sentence</span><span>[:</span><span>i</span><span>%</span><span>len</span><span>(</span><span>sentence</span><span>)],</span> <span>file</span><span>=</span><span>f</span><span>)</span>
</code></pre></div>

<h3 id="cpu-usage-during-repeated-keypresses">CPU Usage During Repeated Keypresses</h3>

<p>While making these measurements, I noticed some strange behaviors. My
monitor runs at 120 Hz and animation and window dragging are generally
smooth. But right after you start Alacritty, dragging the window
animates at something like 30-60 frames per second. It‚Äôs noticeably
chunkier. WezTerm does the same, but slightly worse. Maybe 20 frames
per second.</p>

<p>I don‚Äôt know if I can blame the terminals themselves, because I
sometimes experience this even with Notepad.exe too. But the
choppiness stands out much more. Maybe something is CPU-bound in
responding to window events?</p>

<p>This made me think of a new test: if I open a terminal and hold down
the ‚Äúa‚Äù button on autorepeat, how much CPU does the terminal consume?</p>

<p>To measure this, I set the terminal process‚Äôs affinity to my third
physical core, and watched the CPU usage graph in Task Manager. Not a
great methodology, but it gave a rough sense. Again, 80x50.</p>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Percent of Core</th>
      <th>Private Bytes After Startup (KiB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conhost</td>
      <td>0%</td>
      <td>6,500</td>
    </tr>
    <tr>
      <td>Alacritty</td>
      <td>5%</td>
      <td>74,000</td>
    </tr>
    <tr>
      <td>MinTTY WSL1</td>
      <td>10%</td>
      <td>10,200</td>
    </tr>
    <tr>
      <td>MinTTY Cygwin</td>
      <td>10%</td>
      <td>10,500</td>
    </tr>
    <tr>
      <td>Windows Terminal</td>
      <td>20%</td>
      <td>73,700</td>
    </tr>
    <tr>
      <td>WezTerm</td>
      <td>85%</td>
      <td>134,000</td>
    </tr>
  </tbody>
</table>

<p>The WezTerm CPU usage has to be a bug. I‚Äôll report it.</p>

<h3 id="cpu-usage-idle">CPU Usage (Idle)</h3>

<p>I often have a pile of idle terminals sitting around. I don‚Äôt want
them to chew battery life. So let‚Äôs take a look at CPU Cycles Delta
(courtesy of Process Explorer) with a fresh, idle WSL
session.</p>

<table>
  <thead>
    <tr>
      <th>Terminal</th>
      <th>Idle Cycles/s (Focused)</th>
      <th>Idle Cycles/s (Background)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>conhost</td>
      <td>~900,000</td>
      <td>0</td>
    </tr>
    <tr>
      <td>Alacritty</td>
      <td>~2,400,000</td>
      <td>no difference</td>
    </tr>
    <tr>
      <td>WezTerm</td>
      <td>~2,600,000</td>
      <td>~1,600,000</td>
    </tr>
    <tr>
      <td>Windows Terminal</td>
      <td>~55,000,000</td>
      <td>~6,100,000</td>
    </tr>
    <tr>
      <td>MinTTY WSL1</td>
      <td>~120,000,000</td>
      <td>no difference</td>
    </tr>
    <tr>
      <td>MinTTY Cygwin</td>
      <td>~120,000,000</td>
      <td>no difference</td>
    </tr>
  </tbody>
</table>

<p>These numbers aren‚Äôt great at all! For perspective, I have a pile of
Firefox tabs open, some of them actively running JavaScript, and
they‚Äôre ‚Äúonly‚Äù using a few hundred million cycles per second.</p>

<p>Raymond Chen once wrote a <a href="https://devblogs.microsoft.com/oldnewthing/20060124-17/?p=32553">blog post about the importance of properly
idling</a>
in the Windows Terminal Server days. You might have a dozen users
logged into a host, and if a program is actively polling, it‚Äôs eating
performance that others could use.</p>

<p>Today, we often run on batteries, so idling correctly still matters,
but it seems to be something of a lost art. The only terminal that
idles completely is the old conhost.exe.</p>

<p>The other lesson we can draw is that Microsoft‚Äôs own replacement for
conhost.exe, Windows Terminal, uses over 10x the RAM, 60x the CPU when
focused, and infinitely more CPU when idle.</p>

<h2 id="conclusions">Conclusions</h2>

<p>conhost.exe consistently has the best latency, with MinTTY not much
behind. MinTTY handily dominates the throughput test, supports all
major ANSI character attributes, and has a better default palette.</p>

<p>As in 2009, I‚Äôd say MinTTY is still pretty great. (I should try to
track down that idle CPU consumption. It feels more like a bug than a
requirement.)</p>

<p>If you want to use MinTTY as the default terminal for WSL, install
<a href="https://github.com/mintty/wsltty">WSLtty</a>.</p>

<p>The others all have slightly worse latencies, but they‚Äôre in a similar
class. I‚Äôm particularly sensitive to latency, so I‚Äôd had a suspicion
even before measuring. Maybe it‚Äôs some consequence of being
GPU-accelerated? Out of curiousity, I put Windows Terminal in
software-rendered mode, and it shaved perhaps 4 ms off (median of 62.5
ms, 15 frames). Perhaps just measurement noise.</p>

<p>While I‚Äôm going to stick with MinTTY, one thing is clear: there is
room to improve all of the above.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We ran over 600 image generations to compare AI image models (185 pts)]]></title>
            <link>https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/</link>
            <guid>45890186</guid>
            <pubDate>Tue, 11 Nov 2025 17:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/">https://latenitesoft.com/blog/evaluating-frontier-ai-image-generation-models/</a>, See on <a href="https://news.ycombinator.com/item?id=45890186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p><b>tl:dr; </b>We‚Äôve been making photo apps for iOS for long enough that we have gray hairs now, and using our experience we ran over 600 image generations to compare which AI models work best for which image edits. If you want, you can jump right to the <a href="#filters">image comparisons</a>, or the <a href="#conclusion">conclusion</a>, but promise us you won‚Äôt presumptuous comments on Hacker News until you‚Äôve also read the background!</p>



<h2>Background</h2>



<p>Hi! We‚Äôre LateNiteSoft, and we‚Äôve been working on photography-related iOS apps for 15 years now. Working on market-leading apps such as Camera+, <a href="https://photon.cam/?from=lnsblog">Photon</a> and <a href="https://recvideoapp.com/">REC</a>, we‚Äôve always had our finger on the pulse on what users want out of their mobile photography.</p>



<p>With the ground-breaking release of OpenAI‚Äôs <b>gpt-image-1</b> image generation model earlier this year, we started investigating all the interesting use cases we could think of for AI image editing.</p>



<p>But as a company that has never taken any venture capital investment, all our products have to pay for themselves. We‚Äôre in it to delight our users, not just capture market share and sell them out. When considering AI projects, one thing has been clear ‚Äì we can‚Äôt take the AI startup road where you have a generous free tier, charge an unreasonably small monthly fee for ‚Äúunlimited‚Äù, and hope you‚Äôre going to make it up on scale (code for ‚Äúsomeone please acquire us‚Äù).</p>



<p>All the AI-focused billing systems we could find out there were based on this. Assuming you want to claim unlimited access, and then sandbag users with ‚Äúfair use‚Äù clauses and prevent them from any actual unlimited usage (which is, obviously, untenable, since you‚Äôll end up with one $20/mo user reselling to everyone else).</p>



<p>Since we want to fairly charge our customers for what they actually use, we‚Äôve built a credit-based ‚Äúpay per generation‚Äù-style billing system (that internally we‚Äôve been calling CreditProxy). We‚Äôve also been planning on providing this as a service, since nobody else seems to be doing it, so if you‚Äôre interested in being a trial user, <a href="https://latenitesoft.com/contact/">get in touch!</a></p>



<p>We released our app <a href="https://apps.apple.com/app/apple-store/id6745558534?pt=11365&amp;ct=lnsblog&amp;mt=8">MorphAI</a> as a public proof of concept to give CreditProxy a proper real world-test, and have marketed it to the users of Camera+, which includes traditional photo-editing functionality, including a whole host of popular photo filters, giving us a built-in audience of customers ready for the next step in image editing.</p>



<p>With the release of newer models like nanoBanana and Seedream, we‚Äôve had to consider which models make sense to support. We need to explore the trade-offs between quality, prompt complexity, and pricing.</p>



<p>A couple of hastily-hacked together scripts, and many, many AI generation credits later, we have some results! So that everyone else also doesn‚Äôt have to waste their money, we figured we‚Äôd share what we found:</p>



<h2>The Tests</h2>



<p>Based on our experience with Camera+ and the kind of edits our users have been making with MorphAI, we picked a host of somewhat naive prompts. Veteran Midjourney users may scoff at these, but in our experience these are the kinds of prompts that our average user is likely to use.</p>



<p>As for test photos, we chose some some representative things people like to take photos of: their pets, their kids, landscapes, their cars, and product photography.</p>



<p>
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/landscape.jpg"> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/car.jpg"> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/pets.jpg"> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/portrait.jpg"> 
<img decoding="async" src="https://latenitesoft.com/blog/wp-content/plugins/model-comparison-widget/images/product.jpg">
</p>



<p>Image generation times are also relevant. During our test period, the generation time for all models was fairly consistent, and didn‚Äôt vary by image or prompt complexity.</p>



<figure><table><tbody><tr><td><strong>OpenAI (High)</strong></td><td><strong>Gemini</strong></td><td><strong>Seedream</strong></td></tr><tr><td>80 seconds</td><td>11 seconds</td><td>9 seconds</td></tr></tbody></table></figure>



<p>OpenAI also has a quality setting, the images included here were all generated on High quality, but we also tested Medium, and those generations averaged 36 seconds. We can include the Medium quality images as well if there is any interest!</p>



<p>There are a ton of photos to compare here, so to make things easier to flip through, here are some <strong>keyboard shortcuts</strong> to help you out: Click on a photo to see it larger. Now you can use the arrow keys to switch between models. Press the tab key to switch between test images. Hit ESC to leave the view.</p>



<h2 id="filters">Classic filters</h2>



<p>These are the types of filters that we used to implement manually, by painstakingly hand-crafting textures and Photoshop layers and then converting those to Objective-C code. Now all you need is a few words into a language model (and to burn down half of a rain forest or so; just the cost of progress).</p>



<p>Our conclusion for this category is that for photo realistic filters like this, Gemini really shines by preserving details from the original and minimizing hallucinations, but often at the expense of the strength and creativity of the effect. Especially with photos of people, Gemini seems to refuse to apply any edits at all, with a strong bias towards photo realism.</p>



<p>OpenAI really likes to mess with the details of the photo, giving a characteristic ‚ÄúAI slop‚Äù feel, which can be a deal breaker on things like human faces.</p>



<h3>Grungy vintage photo</h3>






<h3>Use soft, diffused lighting</h3>






<h3>Transform into a kaleidoscopic pattern</h3>



<p>Gemini took some really odd shortcuts in generating some of these!</p>






<h3>Apply a heat map effect</h3>



<p>It‚Äôs clear that none of the models actually have a concept of what generates heat here, aside from Seedream knowing that humans generate heat, clearly revealing that without any ground truth the models struggle.</p>






<h3>Make it look like a long exposure photograph</h3>



<p>This is an interesting test since in some of the sample photos a long exposure doesn‚Äôt make sense. In the ones where it makes the most sense ‚Äì the landscape and the car, OpenAI did the best, but on the other hand it completely messed up the cats and the product, and the portrait photo turned into a trippy art piece.</p>



<p>Gemini, maybe logically, did nothing. Seedream liked adding light streaks as if a car drove past, with only the portrait photo seemingly making any sense.</p>






<h3>Pinhole camera</h3>



<p>In this case, it was funny to watch Gemini take a literal approach and generate actual pictures of cameras! For this reason we re-worked this prompt by just adding the word ‚Äúeffect‚Äù.</p>






<h3>Pinhole camera effect</h3>



<p>Gemini liked to generate a literal pinhole camera here so we tried modifying the prompt.</p>






<h3>Add a layer of fog or mist</h3>






<h3>Make it look like it‚Äôs golden hour</h3>






<h3>Make it look like it‚Äôs etched in glass</h3>



<p>With this prompt, there is ambiguity in what ‚Äúit‚Äù is, so we tried a reworded prompt as well. Only OpenAI consistently knew what a traditional etched glass effect looks like. Seedream‚Äôs glass item effect looks really cool!</p>






<h3>Make it look like the photo is etched in glass</h3>



<p>Gemini has a really interesting interpretation here! And Seedream had some pretty fantastic results.</p>






<h3>Remove background</h3>



<p>This is a classic job people have spent their lives doing manually in Photoshop since the early 90‚Äôs. But what is a ‚Äúbackground‚Äù, really? Is the ground in front of a car the ‚Äúbackground‚Äù? We also retried this with a tweaked prompt.</p>



<p>OpenAI‚Äôs ‚Äúsloppification‚Äù of the details of objects makes it useless for this purpose.</p>






<h3>Isolate the object</h3>



<p>With the tweaked prompt, Gemini‚Äôs API actually returned a followup question: <em>‚ÄúWhich object would you like to isolate? There are two cats in the image.‚Äù</em>, which our generation script was not prepared to handle! So it is missing from this comparison.</p>






<h3>Give it a metallic sheen</h3>



<p>Another case where ‚Äúit‚Äù is vague and we can retry with a more specific prompt. The product imagery is another case where Seedream created a really stunning result, even adding a reflection of someone taking the photo with their phone!</p>






<h3>Give the object a metallic sheen</h3>



<p>Modifying the prompt here really only changed OpenAI‚Äôs interpretation.</p>






<h2>Lens effects</h2>



<p>One of the filter packs we had worked on for Camera+ using traditional methods was a lens effect filter pack. But unlike traditional edits, with generative AI you can also create wide-angle lens effects that can just make up the portions of the image that the camera couldn‚Äôt capture.</p>



<p>This is another category where it‚Äôs very visible how OpenAI regenerates and hallucinates all the details in a picture, where Gemini and Seedream‚Äôs results are very faithful to the original and look more like actual lens permutations.</p>



<h3>Apply a fish-eye lens effect</h3>






<h3>Strong bokeh blur</h3>



<p>It was pretty surprising how poorly the models did here considering how common this must be among the training data. OpenAI give a strong blur but no bokeh effects. Gemini gives us a bunch of random circles in front of the image, demonstrating an understanding of what people want out of a bokeh filter but not how it works. Seedream does really well here.</p>






<h3>Apply a Dutch angle (canted frame)</h3>



<p>OpenAI really lost it‚Äôs mind here on the car photo.</p>






<h3>Change to a bird‚Äôs-eye view</h3>






<h2>Style transfer</h2>



<p>Style Transfer is the process of applying an artistic style to a photo. This technique predates the current AI model by quite a few years with popular apps generating Van Gogh paintings out of your photos. We were also early out in attempting style transfer for our apps, shout out to Noel‚Äôs Intel iMac which had to run at full blast all night just to generate a 256x256px image, since it was our only machine with a compatible GPU.</p>



<p>While Gemini was good at preserving reality in the more photorealistic effects in the previous section, when it comes to the more artistic styles, OpenAI has them beat, while Gemini keeps things far too conservative, especially with photos of a human in them, where it sometimes seems to just do nothing at all, is this some kind of safety guardrail?</p>



<h3>Draw this in the style of a Studio Ghibli movie</h3>



<p>ChatGPT went viral with this prompt, with Sam Altman even making it his profile on X. And OpenAI keeps the crown ‚Äì is Google too conservative in order to avoid a lawsuit? Seedream makes an attempt but they just end up looking like ‚Äúgeneric Anime‚Äù.</p>






<h3>Transform into watercolor painting</h3>






<h3>Make it look like a pastel artwork</h3>






<h3>Transform into Art Nouveau style</h3>






<h3>Apply a ukiyo-e Japanese woodblock print style</h3>



<p>A very stark example of Gemini failing to apply a style on photos with humans. This is a prompt where Seedream knocked it out of the park, perhaps showing a larger portion of their training data being sourced from asian cultures than the western models.</p>






<h3>Transform into low poly art</h3>



<p>Seedream blows everyone else away here.</p>






<h2>Portrait effects</h2>



<p>For prompts about human appearance, we have only applied them to the portrait photo.</p>



<h3>Make it look like a caricature</h3>



<p>Seedream seems to be biased towards asian culture, giving an anime look instead of a western-style cartoon caricature.</p>






<h3>Turn them into an action figure in the blister pack</h3>



<p>OpenAI‚Äôs style here went viral a while back, but Gemini is stunningly realistic. Seedream is a weird mix of realistic and hallucinations.</p>






<h2>Generative edits</h2>



<p>The place where generative AI really shines is when it can show off some creativity, and these were some prompts we added as suggestions in MorphAI to showcase that and inspire our users. OpenAI still seems to win here.</p>



<h3>Create a 70‚Äôs vinyl record cover</h3>



<p>This is an example of a prompt that has a small viral moment with OpenAI, but the other models can‚Äôt even get the aspect ratio right.</p>






<h3>Introduce mythical creatures native to this environment</h3>



<p>This one showcases OpenAI‚Äôs creativity. Gemini seems kind of creepy?</p>






<h3>Add a mystical portal or gateway</h3>



<p>Gemini replacing the face with a portal is certainly a choice!</p>






<h3>Incorporate futuristic technology elements</h3>



<p>Another example of OpenAI being far more creative and willing to re-do the whole image.</p>






<h3>Make it look whimsical and enchanting</h3>



<p>This one also shows OpenAI being more artistic, and Gemini being more realistic while still trying to incorporate the prompt.</p>






<h3>Transform the scene to a stormy night</h3>






<h2 id="conclusion">Conclusion</h2>



<p>If you made it all the way down here you probably don‚Äôt need a summary, but for our purposes, we‚Äôve at least concluded that there is no one-size-fits all model at this point.</p>



<p>OpenAI is great for fully transformative filters like style transfer or more creative generative applications, whereas Gemini works better for more realistic edits. Seedream lies somewhere in the middle and is a bit of a jack of all trades, and for the price and performance may be a good replacement for OpenAI.</p>



<p>We‚Äôve been experimenting on working on a ‚Äúprompt classifier‚Äù to automatically choose a model ‚Äì sending artistic prompts to OpenAI and more realistic prompts to Gemini, if there‚Äôs any interest we can follow up with how that worked out!</p>















<h4>Methodology</h4>



<p>Tests were performed on October 8 with <code>gpt-image-1</code>, <code>gemini-2.5-flash-image</code> and <code>seedream-4-0-250828</code>.</p>



<p>Timings were measured on a consumer internet connection in Japan (Fiber connection, 10 Gbps nominal bandwidth) during a limited test run in a short time period.</p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cache-friendly, low-memory Lanczos algorithm in Rust (125 pts)]]></title>
            <link>https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/</link>
            <guid>45889891</guid>
            <pubDate>Tue, 11 Nov 2025 17:08:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/">https://lukefleed.xyz/posts/cache-friendly-low-memory-lanczos/</a>, See on <a href="https://news.ycombinator.com/item?id=45889891">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>The standard Lanczos method for computing matrix functions has a brutal memory requirement: storing an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span></span> basis matrix that grows with every iteration. For a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500.000</mn></mrow><annotation encoding="application/x-tex">500.000</annotation></semantics></math></span></span>-variable problem needing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000</annotation></semantics></math></span></span> iterations, that‚Äôs roughly 4 GB just for the basis.</p>
<p>In this post, we will explore one of the most straightforward solutions to this problem: a two-pass variant of the Lanczos algorithm that only requires <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span></span> memory at the cost of doubling the number of matrix-vector products. The surprising part is that when implemented carefully, the two-pass version isn‚Äôt just memory-efficient‚Äîit can be faster for certain problems. We will dig into why.</p>
<ul>
<li>All code is available on GitHub: <a href="https://github.com/lukefleed/two-pass-lanczos">two-pass-lanczos</a></li>
<li>The full technical report with proofs and additional experiments: <a href="https://github.com/lukefleed/two-pass-lanczos/raw/master/tex/report.pdf">report.pdf</a></li>
</ul>
<hr>
<h2 id="table-of-contents">Table of Contents</h2>
<details><summary>Open Table of Contents</summary>
<ul>
<li><a href="#computing-matrix-functions">Computing Matrix Functions</a>
<ul>
<li><a href="#krylov-projection">Krylov Projection</a>
<ul>
<li><a href="#building-an-orthonormal-basis">Building an Orthonormal Basis</a></li>
<li><a href="#solving-in-the-reduced-space">Solving in the Reduced Space</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#the-lanczos-algorithm">The Lanczos Algorithm</a>
<ul>
<li><a href="#three-term-recurrence">Three-Term Recurrence</a></li>
<li><a href="#reconstructing-the-solution">Reconstructing the Solution</a></li>
</ul>
</li>
<li><a href="#two-pass-algorithm">Two-Pass Algorithm</a>
<ul>
<li><a href="#first-pass-compute-the-projected-problem">First Pass: Compute the Projected Problem</a></li>
<li><a href="#second-pass-reconstruct-and-accumulate">Second Pass: Reconstruct and Accumulate</a>
<ul>
<li><a href="#a-subtle-numerical-point">A Subtle Numerical Point</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#recurrence-step">Recurrence Step</a></li>
<li><a href="#an-iterator-for-state-management">An Iterator for State Management</a></li>
<li><a href="#first-pass-computing-the-decomposition">First Pass: Computing the Decomposition</a></li>
<li><a href="#second-pass-reconstructing-the-solution">Second Pass: Reconstructing the Solution</a></li>
<li><a href="#the-public-api">The Public API</a>
<ul>
<li><a href="#example-solving-a-linear-system">Example: Solving a Linear System</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#some-interesting-results">Some interesting results</a>
<ul>
<li><a href="#memory-and-computation-trade-off">Memory and Computation Trade-off</a>
<ul>
<li><a href="#memory-usage">Memory Usage</a></li>
<li><a href="#runtime-where-theory-breaks">Runtime: Where Theory Breaks</a></li>
<li><a href="#medium-scale-behavior">Medium-Scale Behavior</a></li>
<li><a href="#what-about-dense-matrices">What About Dense Matrices?</a></li>
</ul>
</li>
<li><a href="#scalability">Scalability</a></li>
</ul>
</li>
</ul>
</details>
<h2 id="computing-matrix-functions">Computing Matrix Functions</h2>
<p>Let‚Äôs consider the problem of computing the action of matrix functions on a vector:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{x} = f(\mathbf{A})\mathbf{b}</annotation></semantics></math></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> is a large sparse Hermitian matrix and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> is a matrix function defined on the spectrum of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span>. This is a problem that appears pretty often in scientific computing: solving linear systems corresponds to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span></span>, exponential integrators for PDEs use <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>t</mi><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z) = \exp(tz)</annotation></semantics></math></span></span>, and many other problems require functions like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>‚àí</mo><mn>1</mn><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1/2}</annotation></semantics></math></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>sign</mtext><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(z) = \text{sign}(z)</annotation></semantics></math></span></span>.</p>
<p>Indeed, there are a lot problems with computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span></span> directly. First of all, even if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> is sparse, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span></span> is generally dense. Storing it explicitly is out of the question for large problems. Even if we could store it, computing it directly would require algorithms like the Schur-Parlett method that scale as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^3)</annotation></semantics></math></span></span>, which is impractical for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>.</p>
<p>However we know that given any matrix function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> defined on the spectrum of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span>, we can express <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span></span> as a polynomial in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> of degree at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> (the size of the matrix) such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>p</mi><mi>n</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A}) = p_{n}(\mathbf{A})</annotation></semantics></math></span></span> (this is a consequence of the Cayley-Hamilton theorem). This polynomial interpolates <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> and its derivatives in the Hermitian sense at the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span>.</p>
<p>This gives us a good and a bad news: the good news is that, well, we can express <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{A})</annotation></semantics></math></span></span> as a polynomial in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span>. The bad news is that the degree of this polynomial can be as high as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>, which is huge for large problems. The idea is then to find a low-degree polynomial approximation to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> that is <em>good enough</em> for our purposes. If we can find a polynomial <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">p_k</annotation></semantics></math></span></span> of degree <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚â™</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \ll n</annotation></semantics></math></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mo>‚âà</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_k(\mathbf{A}) \approx f(\mathbf{A})</annotation></semantics></math></span></span>, then we can approximate the solution as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi><mo>‚âà</mo><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><msub><mi>c</mi><mi>i</mi></msub><msup><mi mathvariant="bold">A</mi><mi>i</mi></msup><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">f(\mathbf{A})\mathbf{b} \approx p_k(\mathbf{A})\mathbf{b} = \sum_{i=0}^k c_i \mathbf{A}^i \mathbf{b}</annotation></semantics></math></span></span></span>
<p>This polynomial only involves vectors within a specific subspace.</p>
<h2 id="krylov-projection">Krylov Projection</h2>
<p>We can notice that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">p_k(\mathbf{A})\mathbf{b}</annotation></semantics></math></span></span> only depends on vectors in the Krylov subspace of order <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span></p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="script">K</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo separator="true">,</mo><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>span</mtext><mo stretchy="false">{</mo><mi mathvariant="bold">b</mi><mo separator="true">,</mo><mrow><mi mathvariant="bold">A</mi><mi mathvariant="bold">b</mi></mrow><mo separator="true">,</mo><msup><mi mathvariant="bold">A</mi><mn>2</mn></msup><mi mathvariant="bold">b</mi><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msup><mi mathvariant="bold">A</mi><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">b</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{K}_k(\mathbf{A}, \mathbf{b}) = \text{span}\{\mathbf{b}, \mathbf{Ab}, \mathbf{A}^2\mathbf{b}, \ldots, \mathbf{A}^{k-1}\mathbf{b}\}</annotation></semantics></math></span></span></span>
<p>This is fortunate: we can compute an approximate solution by staying within this space, which only requires repeated matrix-vector products with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span>. For large sparse matrices, that‚Äôs the only operation we can do efficiently anyway.</p>
<blockquote>
<p>We don‚Äôt need to construct <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">A</mi><mi>j</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{A}^j</annotation></semantics></math></span></span> explicitly. We compute iteratively: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo stretchy="false">(</mo><msup><mi mathvariant="bold">A</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msup><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{A}(\mathbf{A}^{j-1}\mathbf{b})</annotation></semantics></math></span></span>.</p>
</blockquote>
<p>But there‚Äôs a problem: the raw vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msup><mi mathvariant="bold">A</mi><mi>j</mi></msup><mi mathvariant="bold">b</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\mathbf{A}^j\mathbf{b}\}</annotation></semantics></math></span></span> form a terrible basis. They quickly become nearly parallel, making any computation numerically unstable. We need an orthonormal basis.</p>
<h3 id="building-an-orthonormal-basis">Building an Orthonormal Basis</h3>
<p>The standard method is the Arnoldi process, which is Gram-Schmidt applied to Krylov subspaces. We start by normalizing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold">b</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2</annotation></semantics></math></span></span>. Then, iteratively:</p>
<ol>
<li>Compute a new candidate: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{w}_j = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span></span></li>
<li>Orthogonalize against all existing basis vectors:</li>
</ol>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><mo>=</mo><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>j</mi></munderover><mo stretchy="false">(</mo><msubsup><mi mathvariant="bold">v</mi><mi>i</mi><mi>H</mi></msubsup><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_j = \mathbf{w}_j - \sum_{i=1}^j (\mathbf{v}_i^H \mathbf{w}_j) \mathbf{v}_i</annotation></semantics></math></span></span></span>
<ol start="3">
<li>Normalize: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><mi mathvariant="normal">/</mi><mi mathvariant="normal">‚à•</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mi>j</mi></msub><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1} = \tilde{\mathbf{v}}_j / \|\tilde{\mathbf{v}}_j\|_2</annotation></semantics></math></span></span></li>
</ol>
<p>The coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>i</mi><mi>H</mi></msubsup><msub><mi mathvariant="bold">w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_{ij} = \mathbf{v}_i^H \mathbf{w}_j</annotation></semantics></math></span></span> become entries of a projected matrix. After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> iterations, we have:</p>
<ul>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{V}_k = [\mathbf{v}_1, \ldots, \mathbf{v}_k]</annotation></semantics></math></span></span>: an orthonormal basis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">K</mi><mi>k</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold">A</mi><mo separator="true">,</mo><mi mathvariant="bold">b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{K}_k(\mathbf{A}, \mathbf{b})</annotation></semantics></math></span></span></li>
<li><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k</annotation></semantics></math></span></span>: an upper Hessenberg matrix representing the projection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> onto this subspace</li>
</ul>
<p>We can express this relationship with the Arnoldi decomposition:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo>+</mo><msub><mi>h</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mi>k</mi></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><msubsup><mi mathvariant="bold">e</mi><mi>k</mi><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{A}\mathbf{V}_k = \mathbf{V}_k \mathbf{H}_k + h_{k+1,k} \mathbf{v}_{k+1} \mathbf{e}_k^T</annotation></semantics></math></span></span></span>
<h3 id="solving-in-the-reduced-space">Solving in the Reduced Space</h3>
<p>Now we approximate our original problem by solving it in the small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>-dimensional space. Using the Full Orthogonal Method (FOM), we enforce that the residual is orthogonal to
the Krylov subspace. This gives:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span></span> is computed as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{H}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span></span></span>
<p>The heavy lifting is now on computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(\mathbf{H}_k)</annotation></semantics></math></span></span>, a small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k \times k</annotation></semantics></math></span></span> matrix.
Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚â™</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k \ll n</annotation></semantics></math></span></span>, we can afford direct methods like Schur-Parlett (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>k</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k^3)</annotation></semantics></math></span></span>).</p>
<blockquote>
<p>For <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span></span> (linear systems), this reduces to solving <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k \mathbf{y}_k = \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span></span> with LU decomposition.</p>
</blockquote>
<h2 id="the-lanczos-algorithm">The Lanczos Algorithm</h2>
<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> is Hermitian (or symmetric in the real case), the general Arnoldi
process simplifies dramatically. We can prove that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">H</mi><mi>k</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">V</mi><mi>k</mi><mi>H</mi></msubsup><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{H}_k = \mathbf{V}_k^H \mathbf{A} \mathbf{V}_k</annotation></semantics></math></span></span> must also be Hermitian. A matrix that is both upper Hessenberg <em>and</em> Hermitian must be real, symmetric, and tridiagonal. This is a <em>huge</em> simplification.</p>
<p>In the literature, this projected matrix is denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span> to highlight its
tridiagonal structure:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ±</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ≤</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ≤</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ±</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ≤</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ≤</mi><mn>2</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ã±</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ã±</mo></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ã±</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>Œ±</mi><mi>k</mi></msub></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{T}_k = \begin{pmatrix}
\alpha_1 &amp; \beta_1 &amp; &amp; \\
\beta_1 &amp; \alpha_2 &amp; \beta_2 &amp; \\
&amp; \beta_2 &amp; \ddots &amp; \ddots \\
&amp; &amp; \ddots &amp; \alpha_k
\end{pmatrix}</annotation></semantics></math></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub><mo>‚àà</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\alpha_j \in \mathbb{R}</annotation></semantics></math></span></span> are the diagonal elements and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub><mo>‚àà</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\beta_j \in \mathbb{R}</annotation></semantics></math></span></span> are the off-diagonals (subdiagonals from the orthogonalization).</p>
<h2 id="three-term-recurrence">Three-Term Recurrence</h2>
<p>This tridiagonal structure leads to a beautiful simplification. To build the next basis
vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span></span>, we don‚Äôt need the entire history of vectors. We only need
the two previous ones. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> is Hermitian, this guarantees that
any new vector is <em>automatically</em> orthogonal to all earlier vectors (beyond the previous two). So we can skip the full orthogonalization and use a simple three-term recurrence:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>Œ±</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>+</mo><msub><mi>Œ≤</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{A}\mathbf{v}_j = \beta_{j-1}\mathbf{v}_{j-1} + \alpha_j \mathbf{v}_j + \beta_j \mathbf{v}_{j+1}</annotation></semantics></math></span></span></span>
<p>Rearranging gives us an algorithm to compute <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span></span> directly:</p>
<ol>
<li>Compute the candidate: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_{j+1} = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span></span></li>
<li>Extract the diagonal coefficient: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>j</mi><mi>H</mi></msubsup><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_j = \mathbf{v}_j^H w_{j+1}</annotation></semantics></math></span></span></li>
<li>Orthogonalize against the two previous vectors:</li>
</ol>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>‚àí</mo><msub><mi>Œ±</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_{j+1} = w_{j+1} - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span></span></span>
<ol start="4">
<li>Normalize: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">‚à•</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\beta_j = \|\tilde{\mathbf{v}}_{j+1}\|_2</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">/</mi><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1} = \tilde{\mathbf{v}}_{j+1} / \beta_j</annotation></semantics></math></span></span></li>
</ol>
<p>This is known as the Lanczos algorithm. It‚Äôs more efficient than Arnoldi because each iteration only orthogonalizes against two previous vectors instead of all prior ones.</p>
<h2 id="reconstructing-the-solution">Reconstructing the Solution</h2>
<p>After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> iterations, we end up with the tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span> and all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> basis vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><mo>=</mo><mo stretchy="false">[</mo><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>k</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbf{V}_k = [\mathbf{v}_1, \ldots, \mathbf{v}_k]</annotation></semantics></math></span></span>. We can then reconstruct the approximate solution as:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span></span>
<p>where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span></span> is solved from the small tridiagonal matrix.</p>
<p>There is a timing problem however: we cannot compute the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span></span>
until all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> iterations are complete. The full matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span> is only available
at the end, so we must store every basis vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span> along the way, leading to a memory cost of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span></span>.</p>
<p>So we‚Äôre left with a choice: whether we store all the basis vectors and solve the problem in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> passes, or find a way to avoid storing them. There is a middle ground.</p>
<blockquote>
<p>There are also techniques to compress the basis vectors, have a look <a href="https://arxiv.org/abs/2403.04390">here</a></p>
</blockquote>
<h2 id="two-pass-algorithm">Two-Pass Algorithm</h2>
<p>Here‚Äôs where we break the timing deadlock. The insight that we don‚Äôt actually need to store the basis vectors if we can afford to compute them twice</p>
<p>Think about what we have after the first pass. We‚Äôve computed all the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span> coefficients that compose the entire tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span>. These numbers are small compared to the full basis. What if we kept only these scalars, discarded all the vectors, and then replayed the Lanczos recurrence a second time? We‚Äôd regenerate the same basis, and this time we‚Äôd use it to build the solution.</p>
<p>This comes at a cost. We run Lanczos twice, so we pay for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span></span> matrix-vector products instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>. But we only ever store a constant number of vectors in memory, no <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span></span> basis matrix. The memory complexity drops to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span></span>.</p>
<p>It sounds like a bad trade at first. But as we‚Äôll see later, the cache behavior of this
two-pass approach can actually make it as fast (or even faster) on real hardware if well optimized.</p>
<h2 id="first-pass-compute-the-projected-problem">First Pass: Compute the Projected Problem</h2>
<p>We initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo>=</mo><mi mathvariant="bold">b</mi><mi mathvariant="normal">/</mi><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1 = \mathbf{b} / \|\mathbf{b}\|_2</annotation></semantics></math></span></span> and set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_0 = 0</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>0</mn></msub><mo>=</mo><mn mathvariant="bold">0</mn></mrow><annotation encoding="application/x-tex">\mathbf{v}_0 = \mathbf{0}</annotation></semantics></math></span></span>.Then we run the standard Lanczos recurrence:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j = \mathbf{A}\mathbf{v}_j</annotation></semantics></math></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub><mo>=</mo><msubsup><mi mathvariant="bold">v</mi><mi>j</mi><mi>H</mi></msubsup><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j = \mathbf{v}_j^H w_j</annotation></semantics></math></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>w</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>Œ±</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\tilde{\mathbf{v}}_{j+1} = w_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span></span></span>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub><mo>=</mo><mi mathvariant="normal">‚à•</mi><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mover accent="true"><mi mathvariant="bold">v</mi><mo>~</mo></mover><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">/</mi><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j = \|\tilde{\mathbf{v}}_{j+1}\|_2, \quad \mathbf{v}_{j+1} = \tilde{\mathbf{v}}_{j+1} / \beta_j</annotation></semantics></math></span></span></span>
<p>At each step, we record <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span>. But we <em>do not</em> store <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span>.
Instead, we discard it immediately after computing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j+1}</annotation></semantics></math></span></span>. In this way we only keep in memory at most just three vectors at any time (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j-1}</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span>, and the working vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span></span>).</p>
<p>After <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> iterations, we have the full set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>Œ±</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>Œ±</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>Œ≤</mi><mi>k</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_1, \beta_1, \ldots, \alpha_k, \beta_k\}</annotation></semantics></math></span></span>. These <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k)</annotation></semantics></math></span></span> scalars define the tridiagonal matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span>. We can now solve:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span></span></span>
<p>This is the solution in the reduced space. Now that we have the coefficients we need to build <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span></span>.</p>
<h2 id="second-pass-reconstruct-and-accumulate">Second Pass: Reconstruct and Accumulate</h2>
<p>With <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k</annotation></semantics></math></span></span> in memory, we replay the Lanczos recurrence <em>exactly as before</em>. We start with the same initialization (<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_0</annotation></semantics></math></span></span>) and apply the same sequence of operations, using the stored scalars <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span> to reconstruct each basis vector on demand. We can write some rust-like <em>pseudocode</em> for this second pass to get a feel for it:</p>
<pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> x_k</span><span> =</span><span> vec!</span><span>[</span><span>0</span><span>.</span><span>0</span><span>; </span><span>n</span><span>];</span></span>
<span><span>let</span><span> mut</span><span> v_prev</span><span> =</span><span> vec!</span><span>[</span><span>0</span><span>.</span><span>0</span><span>; </span><span>n</span><span>];</span></span>
<span><span>let</span><span> mut</span><span> v_curr</span><span> =</span><span> b</span><span>.</span><span>clone</span><span>() </span><span>/</span><span> b_norm</span><span>;</span></span>
<span></span>
<span><span>for</span><span> j</span><span> in</span><span> 1</span><span>..=</span><span>k</span><span> {</span></span>
<span><span>    let</span><span> w</span><span> =</span><span> A</span><span> @</span><span> v_curr</span><span>;  </span><span>// Matrix-vector product</span></span>
<span></span>
<span><span>    // We don't recompute alpha/beta; we already have them from pass 1</span></span>
<span><span>    let</span><span> alpha_j</span><span> =</span><span> alphas</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>];</span></span>
<span><span>    let</span><span> beta_prev</span><span> =</span><span> j</span><span> &gt;</span><span> 1</span><span> ?</span><span> betas</span><span>[</span><span>j</span><span> -</span><span> 2</span><span>] </span><span>:</span><span> 0</span><span>.</span><span>0</span><span>;</span></span>
<span></span>
<span><span>    // Accumulate the solution</span></span>
<span><span>    x_k</span><span> +=</span><span> y_k</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>] </span><span>*</span><span> v_curr</span><span>;</span></span>
<span></span>
<span><span>    // Regenerate the next basis vector for the *next* iteration</span></span>
<span><span>    let</span><span> v_next</span><span> =</span><span> (</span><span>w</span><span> -</span><span> alpha_j</span><span> *</span><span> v_curr</span><span> -</span><span> beta_prev</span><span> *</span><span> v_prev</span><span>) </span><span>/</span><span> betas</span><span>[</span><span>j</span><span> -</span><span> 1</span><span>];</span></span>
<span></span>
<span><span>    // Slide the window forward</span></span>
<span><span>    v_prev</span><span> =</span><span> v_curr</span><span>;</span></span>
<span><span>    v_curr</span><span> =</span><span> v_next</span><span>;</span></span>
<span><span>}</span></span></code></pre>
<p>This loop regenerates each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span> on demand and immediately uses it to update the solution.
Once we‚Äôve accumulated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">(\mathbf{y}_k)_j \mathbf{v}_j</annotation></semantics></math></span></span> into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span></span>, we discard the vector. We never store the full basis.</p>
<h3 id="a-subtle-numerical-point">A Subtle Numerical Point</h3>
<p>There is one detail worth noting: floating-point arithmetic is deterministic. When we replay the Lanczos recurrence in the second pass with the exact same inputs and the exact same order of operations, we get bitwise-identical vectors. The <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span> regenerated in pass 2 are identical to the ones computed in pass 1.</p>
<p>However, the order in which we accumulate the solution differs. In a standard Lanczos,
<span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span></span> is built as a single matrix-vector product: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span> (a <code>gemv</code> call in BLAS). In the two-pass method, it‚Äôs built as a loop of scaled vector additions (a series of <code>axpy</code> calls). These operations accumulate rounding error differently, so the final solution differs slightly, typically by machine epsilon. This rarely matters in practice, and convergence is unaffected.</p>
<h2 id="implementation">Implementation</h2>
<p>Building this in Rust forces us to think concretely about where data lives and how it flows through the cache hierarchy. We need to control memory layout, decide when allocations happen, and choose abstractions that cost us nothing at runtime.</p>
<p>For linear algebra, we reach for <a href="https://github.com/sarah-ek/faer-rs"><code>faer</code></a>. Three design choices in this library matter for what we‚Äôre building:</p>
<ul>
<li><strong>Stack allocation via <code>MemStack</code>:</strong> Pre-allocated scratch space that lives for the entire computation. The hot path becomes allocation-free.</li>
<li><strong>Matrix-free operators:</strong> The <code>LinOp</code> trait defines an operator by its action (<code>apply</code>) without materializing a matrix. For large sparse problems, this is the only viable approach.</li>
<li><strong>SIMD-friendly loops:</strong> The <code>zip!</code> macro generates code that compiles to packed instructions.</li>
</ul>
<h2 id="recurrence-step">Recurrence Step</h2>
<p>Our starting point is the Lanczos three-term recurrence that we derived earlier:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="bold">A</mi><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>Œ±</mi><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub><mo>‚àí</mo><msub><mi>Œ≤</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\beta_j \mathbf{v}_{j+1} = \mathbf{A}\mathbf{v}_j - \alpha_j \mathbf{v}_j - \beta_{j-1}\mathbf{v}_{j-1}</annotation></semantics></math></span></span></span>
<p>We can translate this into a recurrence step function. The signature looks like this:</p>
<pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> lanczos_recurrence_step</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    mut</span><span> w</span><span>:</span><span> MatMut</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_curr</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_prev</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> (T</span><span>::</span><span>Real</span><span>, </span><span>Option</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;)</span></span></code></pre>
<p>The function is generic over the field type <code>T</code> (<code>f64</code>, <code>c64</code>, etc.) and the operator type <code>O</code>. It operates on matrix views (<code>MatMut</code> and <code>MatRef</code>) to avoid unnecessary data copies. The return type gives us the diagonal element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> and, <em>if no breakdown occurs</em>, the off-diagonal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span>.</p>
<p>Now we can implement the body by following the math. The first step is the most expensive:</p>
<pre tabindex="0" data-language="rust"><code><span><span>// 1. Apply operator: w = A * v_curr</span></span>
<span><span>operator</span><span>.</span><span>apply</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>, Par</span><span>::</span><span>Seq</span><span>, </span><span>stack</span><span>);</span></span></code></pre>
<p>The matrix-vector product dominates the computational cost. Everything else is secondary.</p>
<p>Next, we orthogonalize against <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mrow><mi>j</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{j-1}</annotation></semantics></math></span></span>. This is where we benefit from <code>faer</code>‚Äôs design. The <code>zip!</code> macro fuses this operation into a single loop that the compiler vectorizes into SIMD instructions.</p>
<pre tabindex="0" data-language="rust"><code><span><span>// 2. Orthogonalize against v_{j-1}: w -= Œ≤_{j-1} * v_{j-1}</span></span>
<span><span>let</span><span> beta_prev_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>beta_prev</span><span>);</span></span>
<span><span>zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_prev</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_prev_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>    *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>beta_prev_scaled</span><span>, </span><span>v_prev_i</span><span>));</span></span>
<span><span>});</span></span></code></pre>
<p>With <code>w</code> partially orthogonalized, we can compute the diagonal coefficient via an inner product. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> is Hermitian, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> is guaranteed real.</p>
<pre tabindex="0" data-language="rust"><code><span><span>// 3. Compute Œ±_j = v_j^H * w</span></span>
<span><span>let</span><span> alpha</span><span> =</span><span> T</span><span>::</span><span>real_part_impl</span><span>(</span><span>&amp;</span><span>(</span><span>v_curr</span><span>.</span><span>adjoint</span><span>() </span><span>*</span><span> w</span><span>.</span><span>rb</span><span>())[(</span><span>0</span><span>, </span><span>0</span><span>)]);</span></span></code></pre>
<p>We complete the orthogonalization against <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span></span> with another <code>zip!</code> loop.</p>
<pre tabindex="0" data-language="rust"><code><span><span>// 4. Orthogonalize against v_j: w -= Œ±_j * v_j</span></span>
<span><span>let</span><span> alpha_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>alpha</span><span>);</span></span>
<span><span>zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_curr_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>    *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>alpha_scaled</span><span>, </span><span>v_curr_i</span><span>));</span></span>
<span><span>});</span></span></code></pre>
<p>Now <code>w</code> holds the unnormalized next basis vector. We compute its norm to get <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span>. If this norm is numerically zero, the Krylov subspace is invariant, the iteration has reached its natural stopping point. This is called breakdown.</p>
<pre tabindex="0" data-language="rust"><code><span><span>// 5. Compute Œ≤_j = ||w||_2 and check for breakdown</span></span>
<span><span>let</span><span> beta</span><span> =</span><span> w</span><span>.</span><span>rb</span><span>()</span><span>.</span><span>norm_l2</span><span>();</span></span>
<span><span>let</span><span> tolerance</span><span> =</span><span> breakdown_tolerance</span><span>::</span><span>&lt;T</span><span>::</span><span>Real</span><span>&gt;();</span></span>
<span></span>
<span><span>if</span><span> beta</span><span> &lt;=</span><span> tolerance</span><span> {</span></span>
<span><span>    (</span><span>alpha</span><span>, </span><span>None</span><span>)</span></span>
<span><span>} </span><span>else</span><span> {</span></span>
<span><span>    (</span><span>alpha</span><span>, </span><span>Some</span><span>(</span><span>beta</span><span>))</span></span>
<span><span>}</span></span></code></pre>
<p>The function returns <code>None</code> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span> when breakdown occurs, signaling to the caller that no further iterations should proceed.</p>
<h2 id="an-iterator-for-state-management">An Iterator for State Management</h2>
<p>The recurrence step is a pure function, but calling it in a loop is both inefficient and awkward. We‚Äôd need to manually pass vectors in and out of each iteration. More critically, we‚Äôd create copies when we should be reusing memory.</p>
<p>The iterator pattern solves this. We create a struct that encapsulates the state:</p>
<pre tabindex="0" data-language="rust"><code><span><span>struct</span><span> LanczosIteration</span><span>&lt;'</span><span>a</span><span>, </span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt; {</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>'</span><span>a</span><span> O</span><span>,</span></span>
<span><span>    v_prev</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,       </span><span>// v_{j-1}</span></span>
<span><span>    v_curr</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,       </span><span>// v_j</span></span>
<span><span>    work</span><span>:</span><span> Mat</span><span>&lt;</span><span>T</span><span>&gt;,         </span><span>// Workspace for the next vector</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,   </span><span>// Œ≤_{j-1}</span></span>
<span><span>    // ... iteration counters</span></span>
<span><span>}</span></span></code></pre>
<p>The main design choice here is that vectors are <strong>owned</strong> (<code>Mat&lt;T&gt;</code>), not borrowed. This enables an optimization in the <code>next_step</code> method. After computing the next vector and normalizing it into <code>work</code>, we cycle the state without allocating or copying:</p>
<pre tabindex="0" data-language="rust"><code><span><span>// Inside next_step, after normalization...</span></span>
<span><span>core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_prev, </span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_curr);</span></span>
<span><span>core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>v_curr, </span><span>&amp;</span><span>mut</span><span> self</span><span>.</span><span>work);</span></span></code></pre>
<p>On x86-64, swapping two <code>Mat&lt;T&gt;</code> structures (fat pointers) compiles to three <code>mov</code> instructions. The pointers change, but no vector data moves. After the swap, <code>v_prev</code> points to what <code>v_curr</code> held, <code>v_curr</code> points to <code>work</code>‚Äôs allocation, and <code>work</code> points to the old <code>v_prev</code> data. In the next iteration, <code>work</code> gets reused.</p>
<p>We keep exactly three n-dimensional vectors live in memory. The same allocations cycle through the computation, staying hot in L1 cache. This is the core reason the two-pass method can be faster than expected, the working set never leaves cache.</p>
<h2 id="first-pass-computing-the-decomposition">First Pass: Computing the Decomposition</h2>
<p>The first pass runs the Lanczos iteration and collects the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>Œ±</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>Œ≤</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span></span>. Basis vectors are discarded after each step.</p>
<pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_pass_one</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>impl</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    k</span><span>:</span><span> usize</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>LanczosDecomposition</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;, </span><span>LanczosError</span><span>&gt; {</span></span>
<span><span>    // ...</span></span>
<span><span>}</span></span></code></pre>
<p>We allocate vectors for the coefficients with a capacity hint to avoid reallocations:</p>
<pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> alphas</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>k</span><span>);</span></span>
<span><span>let</span><span> mut</span><span> betas</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>k</span><span> -</span><span> 1</span><span>);</span></span></code></pre>
<p>Then we construct the iterator. This allocates the three work vectors once. After this point, the hot path is allocation-free:</p>
<pre tabindex="0" data-language="rust"><code><span><span>let</span><span> mut</span><span> lanczos_iter</span><span> =</span><span> LanczosIteration</span><span>::</span><span>new</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>k</span><span>, </span><span>b_norm</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>for</span><span> i</span><span> in</span><span> 0</span><span>..</span><span>k</span><span> {</span></span>
<span><span>    if</span><span> let</span><span> Some</span><span>(</span><span>step</span><span>) </span><span>=</span><span> lanczos_iter</span><span>.</span><span>next_step</span><span>(</span><span>stack</span><span>) {</span></span>
<span><span>        alphas</span><span>.</span><span>push</span><span>(</span><span>step</span><span>.</span><span>alpha);</span></span>
<span><span>        steps_taken</span><span> +=</span><span> 1</span><span>;</span></span>
<span></span>
<span><span>        let</span><span> tolerance</span><span> =</span><span> breakdown_tolerance</span><span>::</span><span>&lt;T</span><span>::</span><span>Real</span><span>&gt;();</span></span>
<span><span>        if</span><span> step</span><span>.</span><span>beta </span><span>&lt;=</span><span> tolerance</span><span> {</span></span>
<span><span>            break</span><span>;</span></span>
<span><span>        }</span></span>
<span></span>
<span><span>        if</span><span> i</span><span> &lt;</span><span> k</span><span> -</span><span> 1</span><span> {</span></span>
<span><span>            betas</span><span>.</span><span>push</span><span>(</span><span>step</span><span>.</span><span>beta);</span></span>
<span><span>        }</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>        break</span><span>;</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>The check for breakdown stops the iteration when the residual becomes numerically zero. This means we‚Äôve found an invariant subspace and there‚Äôs no value in continuing.</p>
<p>At the end, we collect the scalars into a <code>LanczosDecomposition</code> struct. The memory footprint throughout this pass is constant: three n-dimensional vectors plus two small arrays that grow to at most <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> elements.</p>
<h2 id="second-pass-reconstructing-the-solution">Second Pass: Reconstructing the Solution</h2>
<p>Now we face a different problem. We have the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>Œ±</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>Œ≤</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span></span> coefficients from the first pass and the coefficient vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k = f(\mathbf{T}_k) \mathbf{e}_1 \|\mathbf{b}\|_2</annotation></semantics></math></span></span> from solving the projected problem. We need to reconstruct the solution:</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo stretchy="false">(</mo><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \sum_{j=1}^k (\mathbf{y}_k)_j \mathbf{v}_j</annotation></semantics></math></span></span></span>
<p>without storing the full basis matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span></span>.</p>
<p>The recurrence step in this pass is structurally similar to the first pass, but with a key difference: we no longer compute inner products or norms. We already know the coefficients, so the step becomes pure reconstruction.</p>
<pre tabindex="0" data-language="rust"><code><span><span>fn</span><span> lanczos_reconstruction_step</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>, </span><span>O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    mut</span><span> w</span><span>:</span><span> MatMut</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_curr</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    v_prev</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    alpha_j</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    beta_prev</span><span>:</span><span> T</span><span>::</span><span>Real</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) {</span></span>
<span><span>    // Apply operator</span></span>
<span><span>    operator</span><span>.</span><span>apply</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>, Par</span><span>::</span><span>Seq</span><span>, </span><span>stack</span><span>);</span></span>
<span></span>
<span><span>    // Orthogonalize using stored Œ±_j and Œ≤_{j-1}</span></span>
<span><span>    let</span><span> beta_prev_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>beta_prev</span><span>);</span></span>
<span><span>    zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_prev</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_prev_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>beta_prev_scaled</span><span>, </span><span>v_prev_i</span><span>));</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    let</span><span> alpha_scaled</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>alpha_j</span><span>);</span></span>
<span><span>    zip!</span><span>(</span><span>w</span><span>.</span><span>rb_mut</span><span>(), </span><span>v_curr</span><span>)</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>, </span><span>v_curr_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> sub</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>alpha_scaled</span><span>, </span><span>v_curr_i</span><span>));</span></span>
<span><span>    });</span></span>
<span><span>}</span></span></code></pre>
<p>This is cheaper than the first-pass recurrence. We‚Äôve eliminated the inner products that computed <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_j</annotation></semantics></math></span></span> and the norm calculation for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span>. What remains is pure orthogonalization and the operator application.</p>
<p><code>lanczos_pass_two</code> implements this reconstruction. We initialize the three work vectors and the solution accumulator:</p>
<pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_pass_two</span><span>&lt;</span><span>T</span><span>:</span><span> ComplexField</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>impl</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    decomposition</span><span>:</span><span> &amp;</span><span>LanczosDecomposition</span><span>&lt;</span><span>T</span><span>::</span><span>Real</span><span>&gt;,</span></span>
<span><span>    y_k</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>LanczosError</span><span>&gt; {</span></span>
<span><span>    let</span><span> mut</span><span> v_prev</span><span> =</span><span> Mat</span><span>::</span><span>&lt;</span><span>T</span><span>&gt;</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>);</span></span>
<span><span>    let</span><span> inv_norm</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>T</span><span>::</span><span>Real</span><span>::</span><span>recip_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>b_norm));</span></span>
<span><span>    let</span><span> mut</span><span> v_curr</span><span> =</span><span> b</span><span> *</span><span> Scale</span><span>(</span><span>inv_norm</span><span>);  </span><span>// v_1</span></span>
<span></span>
<span><span>    let</span><span> mut</span><span> work</span><span> =</span><span> Mat</span><span>::</span><span>&lt;</span><span>T</span><span>&gt;</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>);</span></span>
<span></span>
<span><span>    // Initialize solution with first component</span></span>
<span><span>    let</span><span> mut</span><span> x_k</span><span> =</span><span> &amp;</span><span>v_curr</span><span> *</span><span> Scale</span><span>(T</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>y_k</span><span>[(</span><span>0</span><span>, </span><span>0</span><span>)]));</span></span></code></pre>
<p>We build the solution incrementally by starting with the first basis vector scaled by its coefficient. The main loop then regenerates each subsequent vector: we regenerate each subsequent basis vector, normalize it using the stored <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math></span></span>, and immediately accumulate its contribution:</p>
<pre tabindex="0" data-language="rust"><code><span><span>for</span><span> j</span><span> in</span><span> 0</span><span>..</span><span>decomposition</span><span>.</span><span>steps_taken </span><span>-</span><span> 1</span><span> {</span></span>
<span><span>    let</span><span> alpha_j</span><span> =</span><span> T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>alphas[</span><span>j</span><span>]);</span></span>
<span><span>    let</span><span> beta_j</span><span> =</span><span> T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas[</span><span>j</span><span>]);</span></span>
<span><span>    let</span><span> beta_prev</span><span> =</span><span> if</span><span> j</span><span> ==</span><span> 0</span><span> {</span></span>
<span><span>        T</span><span>::</span><span>Real</span><span>::</span><span>zero_impl</span><span>()</span></span>
<span><span>    } </span><span>else</span><span> {</span></span>
<span><span>        T</span><span>::</span><span>Real</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas[</span><span>j</span><span> -</span><span> 1</span><span>])</span></span>
<span><span>    };</span></span>
<span></span>
<span><span>    // 1. Regenerate the unnormalized next vector</span></span>
<span><span>    lanczos_reconstruction_step</span><span>(</span></span>
<span><span>        operator</span><span>,</span></span>
<span><span>        work</span><span>.</span><span>as_mut</span><span>(),</span></span>
<span><span>        v_curr</span><span>.</span><span>as_ref</span><span>(),</span></span>
<span><span>        v_prev</span><span>.</span><span>as_ref</span><span>(),</span></span>
<span><span>        alpha_j</span><span>,</span></span>
<span><span>        beta_prev</span><span>,</span></span>
<span><span>        stack</span><span>,</span></span>
<span><span>    );</span></span>
<span></span>
<span><span>    // 2. Normalize using stored Œ≤_j</span></span>
<span><span>    let</span><span> inv_beta</span><span> =</span><span> T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>T</span><span>::</span><span>Real</span><span>::</span><span>recip_impl</span><span>(</span><span>&amp;</span><span>beta_j</span><span>));</span></span>
<span><span>    zip!</span><span>(</span><span>work</span><span>.</span><span>as_mut</span><span>())</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>w_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>w_i</span><span> =</span><span> mul</span><span>(</span><span>w_i</span><span>, </span><span>&amp;</span><span>inv_beta</span><span>);</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    // 3. Accumulate: x_k += y_{j+1} * v_{j+1}</span></span>
<span><span>    let</span><span> coeff</span><span> =</span><span> T</span><span>::</span><span>copy_impl</span><span>(</span><span>&amp;</span><span>y_k</span><span>[(</span><span>j</span><span> +</span><span> 1</span><span>, </span><span>0</span><span>)]);</span></span>
<span><span>    zip!</span><span>(</span><span>x_k</span><span>.</span><span>as_mut</span><span>(), </span><span>work</span><span>.</span><span>as_ref</span><span>())</span><span>.</span><span>for_each</span><span>(</span><span>|</span><span>unzip!</span><span>(</span><span>x_i</span><span>, </span><span>v_i</span><span>)</span><span>|</span><span> {</span></span>
<span><span>        *</span><span>x_i</span><span> =</span><span> add</span><span>(</span><span>x_i</span><span>, </span><span>&amp;</span><span>mul</span><span>(</span><span>&amp;</span><span>coeff</span><span>, </span><span>v_i</span><span>));</span></span>
<span><span>    });</span></span>
<span></span>
<span><span>    // 4. Cycle vectors for the next iteration</span></span>
<span><span>    core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> v_prev</span><span>, </span><span>&amp;</span><span>mut</span><span> v_curr</span><span>);</span></span>
<span><span>    core</span><span>::</span><span>mem</span><span>::</span><span>swap</span><span>(</span><span>&amp;</span><span>mut</span><span> v_curr</span><span>, </span><span>&amp;</span><span>mut</span><span> work</span><span>);</span></span>
<span><span>}</span></span></code></pre>
<p>The accumulation <code>x_k += y_{j+1} * v_{j+1}</code> is implemented as a fused multiply-add in the <code>zip!</code> loop. On hardware with FMA support, this becomes a single instruction per element, not three separate operations.</p>
<p>Note that we accumulate the solution incrementally. After each iteration, <code>x_k</code> contains a partial result. We cycle through the same three vectors (<code>v_prev</code>, <code>v_curr</code>, <code>work</code>), keeping the working set small and resident in L1 cache.</p>
<p>Compare this to the standard method‚Äôs final reconstruction step: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span>. This is a dense matrix-vector product where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span></span> is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span></span>. When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> are both large, this matrix no longer fits in cache. The CPU must stream it from main memory, paying the cost of memory latency. Each element requires a load, multiply, and accumulate, but the load operations dominate‚Äîthe CPU stalls waiting for data.</p>
<p>In our two-pass reconstruction, the operator <code>$\mathbf{A}$</code> is applied <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> times, but against vectors that stay in cache. The memory bandwidth is spent on reading the sparse structure of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> and the vector elements, not on scanning a dense <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span></span> matrix.</p>
<p>This is the reason the two-pass method can be faster on real hardware despite performing twice as many matrix-vector products. The cache behavior of the reconstruction phase overwhelms the savings of storing the basis.</p>
<h2 id="the-public-api">The Public API</h2>
<p>We can wrap the two passes into a single entry point:</p>
<pre tabindex="0" data-language="rust"><code><span><span>pub</span><span> fn</span><span> lanczos_two_pass</span><span>&lt;</span><span>T</span><span>, </span><span>O</span><span>, </span><span>F</span><span>&gt;(</span></span>
<span><span>    operator</span><span>:</span><span> &amp;</span><span>O</span><span>,</span></span>
<span><span>    b</span><span>:</span><span> MatRef</span><span>&lt;'</span><span>_</span><span>, </span><span>T</span><span>&gt;,</span></span>
<span><span>    k</span><span>:</span><span> usize</span><span>,</span></span>
<span><span>    stack</span><span>:</span><span> &amp;</span><span>mut</span><span> MemStack</span><span>,</span></span>
<span><span>    mut</span><span> f_tk_solver</span><span>:</span><span> F</span><span>,</span></span>
<span><span>) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>LanczosError</span><span>&gt;</span></span>
<span><span>where</span></span>
<span><span>    T</span><span>:</span><span> ComplexField</span><span>,</span></span>
<span><span>    O</span><span>:</span><span> LinOp</span><span>&lt;</span><span>T</span><span>&gt;,</span></span>
<span><span>    F</span><span>:</span><span> FnMut</span><span>(</span><span>&amp;</span><span>[T</span><span>::</span><span>Real</span><span>], </span><span>&amp;</span><span>[T</span><span>::</span><span>Real</span><span>]) </span><span>-&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>T</span><span>&gt;, </span><span>anyhow</span><span>::</span><span>Error</span><span>&gt;,</span></span>
<span><span>{</span></span>
<span><span>    // First pass: compute T_k coefficients</span></span>
<span><span>    let</span><span> decomposition</span><span> =</span><span> lanczos_pass_one</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>k</span><span>, </span><span>stack</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    if</span><span> decomposition</span><span>.</span><span>steps_taken </span><span>==</span><span> 0</span><span> {</span></span>
<span><span>        return</span><span> Ok</span><span>(</span><span>Mat</span><span>::</span><span>zeros</span><span>(</span><span>b</span><span>.</span><span>nrows</span><span>(), </span><span>1</span><span>));</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // Solve projected problem: y_k' = f(T_k) * e_1</span></span>
<span><span>    let</span><span> y_k_prime</span><span> =</span><span> f_tk_solver</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>alphas, </span><span>&amp;</span><span>decomposition</span><span>.</span><span>betas)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    // Scale by ||b||</span></span>
<span><span>    let</span><span> y_k</span><span> =</span><span> &amp;</span><span>y_k_prime</span><span> *</span><span> Scale</span><span>(T</span><span>::</span><span>from_real_impl</span><span>(</span><span>&amp;</span><span>decomposition</span><span>.</span><span>b_norm));</span></span>
<span></span>
<span><span>    // Second pass: reconstruct solution</span></span>
<span><span>    lanczos_pass_two</span><span>(</span><span>operator</span><span>, </span><span>b</span><span>, </span><span>&amp;</span><span>decomposition</span><span>, </span><span>y_k</span><span>.</span><span>as_ref</span><span>(), </span><span>stack</span><span>)</span></span>
<span><span>}</span></span></code></pre>
<p>The design separates concerns. The <code>f_tk_solver</code> closure is where we inject the specific matrix function. We compute the Lanczos decomposition, then pass the coefficients to the user-provided solver, which computes <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">y</mi><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{y}_k' = f(\mathbf{T}_k) \mathbf{e}_1</annotation></semantics></math></span></span> for whatever function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> is needed. This decoupling means we handle linear solves, matrix exponentials, or any other function without modifying the core algorithm.</p>
<p>The caller provides <code>f_tk_solver</code> as a closure. It receives the raw <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>Œ±</mi><mi>j</mi></msub><mo separator="true">,</mo><msub><mi>Œ≤</mi><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\alpha_j, \beta_j\}</annotation></semantics></math></span></span> arrays and must return the coefficient vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">y</mi><mi>k</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msubsup></mrow><annotation encoding="application/x-tex">\mathbf{y}_k'</annotation></semantics></math></span></span>. We then scale it by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">‚à•</mi><mi mathvariant="bold">b</mi><msub><mi mathvariant="normal">‚à•</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\|\mathbf{b}\|_2</annotation></semantics></math></span></span> and pass everything to the second pass.</p>
<h3 id="example-solving-a-linear-system">Example: Solving a Linear System</h3>
<p>To see this in practice, consider solving <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="bold">A</mi><mi mathvariant="bold">x</mi></mrow><mo>=</mo><mi mathvariant="bold">b</mi></mrow><annotation encoding="application/x-tex">\mathbf{Ax} = \mathbf{b}</annotation></semantics></math></span></span>. We compute <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>z</mi><mrow><mo>‚àí</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(z) = z^{-1}</annotation></semantics></math></span></span>, which means the <code>f_tk_solver</code> must solve the small tridiagonal system <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub><msup><mi mathvariant="bold">y</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo>=</mo><msub><mi mathvariant="bold">e</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k \mathbf{y}' = \mathbf{e}_1</annotation></semantics></math></span></span>.</p>
<p>Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">T</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{T}_k</annotation></semantics></math></span></span> is tridiagonal, we can exploit its structure. A sparse LU factorization solves it in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k)</annotation></semantics></math></span></span> time instead of the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>k</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(k^3)</annotation></semantics></math></span></span> cost of a dense method.</p>
<pre tabindex="0" data-language="rust"><code><span><span>let</span><span> f_tk_solver</span><span> =</span><span> |</span><span>alphas</span><span>:</span><span> &amp;</span><span>[</span><span>f64</span><span>], </span><span>betas</span><span>:</span><span> &amp;</span><span>[</span><span>f64</span><span>]</span><span>|</span><span> -&gt;</span><span> Result</span><span>&lt;</span><span>Mat</span><span>&lt;</span><span>f64</span><span>&gt;, </span><span>anyhow</span><span>::</span><span>Error</span><span>&gt; {</span></span>
<span><span>    let</span><span> steps</span><span> =</span><span> alphas</span><span>.</span><span>len</span><span>();</span></span>
<span><span>    if</span><span> steps</span><span> ==</span><span> 0</span><span> {</span></span>
<span><span>        return</span><span> Ok</span><span>(</span><span>Mat</span><span>::</span><span>zeros</span><span>(</span><span>0</span><span>, </span><span>1</span><span>));</span></span>
<span><span>    }</span></span>
<span></span>
<span><span>    // 1. Assemble T_k from coefficients using triplet format</span></span>
<span><span>    let</span><span> mut</span><span> triplets</span><span> =</span><span> Vec</span><span>::</span><span>with_capacity</span><span>(</span><span>3</span><span> *</span><span> steps</span><span> -</span><span> 2</span><span>);</span></span>
<span><span>    for</span><span> (</span><span>i</span><span>, </span><span>&amp;</span><span>alpha</span><span>) </span><span>in</span><span> alphas</span><span>.</span><span>iter</span><span>()</span><span>.</span><span>enumerate</span><span>() {</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span>, </span><span>col</span><span>:</span><span> i</span><span>, </span><span>val</span><span>:</span><span> alpha</span><span> });</span></span>
<span><span>    }</span></span>
<span><span>    for</span><span> (</span><span>i</span><span>, </span><span>&amp;</span><span>beta</span><span>) </span><span>in</span><span> betas</span><span>.</span><span>iter</span><span>()</span><span>.</span><span>enumerate</span><span>() {</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span>, </span><span>col</span><span>:</span><span> i</span><span> +</span><span> 1</span><span>, </span><span>val</span><span>:</span><span> beta</span><span> });</span></span>
<span><span>        triplets</span><span>.</span><span>push</span><span>(</span><span>Triplet</span><span> { </span><span>row</span><span>:</span><span> i</span><span> +</span><span> 1</span><span>, </span><span>col</span><span>:</span><span> i</span><span>, </span><span>val</span><span>:</span><span> beta</span><span> });</span></span>
<span><span>    }</span></span>
<span><span>    let</span><span> t_k_sparse</span><span> =</span><span> SparseColMat</span><span>::</span><span>try_new_from_triplets</span><span>(</span><span>steps</span><span>, </span><span>steps</span><span>, </span><span>&amp;</span><span>triplets</span><span>)</span><span>?</span><span>;</span></span>
<span></span>
<span><span>    // 2. Construct e_1</span></span>
<span><span>    let</span><span> mut</span><span> e1</span><span> =</span><span> Mat</span><span>::</span><span>zeros</span><span>(</span><span>steps</span><span>, </span><span>1</span><span>);</span></span>
<span><span>    e1</span><span>.</span><span>as_mut</span><span>()[(</span><span>0</span><span>, </span><span>0</span><span>)] </span><span>=</span><span> 1</span><span>.</span><span>0</span><span>;</span></span>
<span></span>
<span><span>    // 3. Solve T_k * y' = e_1 via sparse LU</span></span>
<span><span>    Ok</span><span>(</span><span>t_k_sparse</span><span>.</span><span>as_ref</span><span>()</span><span>.</span><span>sp_lu</span><span>()</span><span>?.</span><span>solve</span><span>(</span><span>e1</span><span>.</span><span>as_ref</span><span>()))</span></span>
<span><span>};</span></span></code></pre>
<p>The closure takes the coefficient arrays, constructs the sparse tridiagonal matrix, and solves the system. The triplet format lets us build the matrix efficiently without knowing its structure in advance. The sparse LU solver leverages the tridiagonal structure to avoid dense factorization.</p>
<h2 id="some-interesting-results">Some interesting results</h2>
<p>Now that we have a working implementation we can run some tests. The core idea of what we have done is simple: trade flops for better memory access. But does this trade actually pay off on real hardware? To find out, we need a reliable way to benchmark it.</p>
<p>For the data, we know that the performance of any Krylov method is tied to the operator‚Äôs spectral properties. We need a way to generate a family of test problems where we can precisely control the size, sparsity, and numerical difficulty. A great way to do this is with Karush-Kuhn-Tucker (KKT) systems, which are sparse, symmetric, and have a specific block structure.</p>
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>D</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>E</mi><mi>T</mi></msup></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>E</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">A =
\begin{pmatrix}
    D &amp; E^T \\
    E &amp; 0
\end{pmatrix}</annotation></semantics></math></span></span></span>
<p>This structure gives us two critical knobs to turn. First, with the <a href="https://commalab.di.unipi.it/files/Data/MCF/netgen.tgz">netgen</a> utility, we can control the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span></span> matrix, which lets us dial in the problem dimension, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span>. Second, we build the diagonal block D with random entries from a range <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>C</mi><mi>D</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[1, C_D]</annotation></semantics></math></span></span>. This parameter, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">C_D</annotation></semantics></math></span></span>, gives us direct control over the numerical difficulty of the problem.</p>
<p>For a symmetric matrix like <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span></span>, the 2-norm condition number, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\kappa_2(D)</annotation></semantics></math></span></span>, is the ratio of its largest to its smallest eigenvalue: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>Œª</mi><mi>max</mi><mo>‚Å°</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><msub><mi>Œª</mi><mi>min</mi><mo>‚Å°</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\kappa_2(D) = \lambda_{\max}(D) / \lambda_{\min}(D)</annotation></semantics></math></span></span>. Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span></span> is diagonal, its eigenvalues are simply its diagonal entries. We are drawing these entries from a uniform distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo stretchy="false">[</mo><mn>1</mn><mo separator="true">,</mo><msub><mi>C</mi><mi>D</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">U[1, C_D]</annotation></semantics></math></span></span>, so we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>max</mi><mo>‚Å°</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>‚âà</mo><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_{\max}(D) \approx C_D</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œª</mi><mi>min</mi><mo>‚Å°</mo></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>‚âà</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda_{\min}(D) \approx 1</annotation></semantics></math></span></span>. This means we get direct control, as <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ∫</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo><mo>‚âà</mo><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">\kappa_2(D) \approx C_D</annotation></semantics></math></span></span>.The spectral properties of this block heavily influence the spectrum of the entire matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span>. A large condition number in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span></span> leads to a more ill-conditioned system for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span>. The convergence rate of Krylov methods like Lanczos is fundamentally governed by the distribution of the operator‚Äôs eigenvalues. An ill-conditioned matrix, with a wide spread of eigenvalues, will require more iterations, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>, to reach the desired accuracy. By simply adjusting the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">C_D</annotation></semantics></math></span></span> parameter, we can generate everything from well-conditioned problems that converge quickly to ill-conditioned ones that force us to run a large number of iterations. This is exactly what we need to rigorously test our implementation.</p>
<h2 id="memory-and-computation-trade-off">Memory and Computation Trade-off</h2>
<p>We measure the algorithm against two hypotheses on a large sparse problem with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>500</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=500,000</annotation></semantics></math></span></span>, varying the number of iterations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>.</p>
<p><strong>Hypothesis 1 (Memory):</strong> The one-pass method stores the full basis <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span></span> with complexity <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nk)</annotation></semantics></math></span></span>. We expect its memory to grow linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>. The two-pass method operates with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span></span> memory, so it should have a flat profile.</p>
<p><strong>Hypothesis 2 (Runtime):</strong> The two-pass method performs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span></span> matrix-vector products instead of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>. If all else were equal, we‚Äôd expect it to run twice as slow.</p>
<h3 id="memory-usage">Memory Usage</h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs500k_rho3_memory.png" alt="Memory vs Iterations"></p>
<p>The memory data confirms Hypothesis 1 exactly. The one-pass method‚Äôs footprint scales as a straight line‚Äîeach additional iteration adds one vector to the basis. The two-pass method remains flat. No allocation growth happens after initialization.</p>
<h3 id="runtime-where-theory-breaks">Runtime: Where Theory Breaks</h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs500k_rho3_time.png" alt="Runtime vs Iterations"></p>
<p>The runtime data contradicts Hypothesis 2. The two-pass method is slower, but never by a factor of two. For small <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>, the gap is minimal. As <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> grows, the two-pass runtime diverges slowly from the one-pass method, not by doubling, but by a much smaller margin.</p>
<p>This difference comes from memory access patterns. Both methods perform matrix-vector products, but they differ in how they reconstruct the solution.</p>
<p>The one-pass method computes <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub><mo>=</mo><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k = \mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span> in a single dense matrix-vector product. When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> are large, the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span></span> basis matrix exceeds all cache levels. The CPU cannot keep the data resident; instead, it streams <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span></span> from main memory. This is a memory-bandwidth-bound operation. The processor stalls, waiting for each load to complete. Instruction-level parallelism collapses.</p>
<p>The two-pass method reconstructs the solution incrementally. At each iteration, it operates on exactly three n-dimensional vectors: <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mtext>prev</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{\text{prev}}</annotation></semantics></math></span></span>, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mtext>curr</mtext></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_{\text{curr}}</annotation></semantics></math></span></span>, and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_k</annotation></semantics></math></span></span>. This working set fits in L1 cache. The processor performs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span></span> matrix-vector products (each one reading the sparse operator, then applying it to a cached vector), but the solution accumulation happens entirely within cache. The additional matrix-vector products are cheaper than the memory latency of the standard method.</p>
<p>The cost of re-computing basis vectors is less than the latency cost of scanning an <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">n \times k</annotation></semantics></math></span></span> dense matrix from main memory.</p>
<h3 id="medium-scale-behavior">Medium-Scale Behavior</h3>
<p><img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs50k_rho3_time.png" alt="Medium Scale Runtime vs Iterations">
<img src="https://lukefleed.xyz/assets/lanczos/tradeoff_arcs50k_rho3_memory.png" alt="Medium Scale Memory Usage vs Iterations"></p>
<p>At <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>50</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=50,000</annotation></semantics></math></span></span> we can observe an equilibrium. The two methods have nearly identical runtime. The standard method‚Äôs <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k</annotation></semantics></math></span></span> matrix is smaller; it fits partially in cache. The cache-miss penalty here becomes manageable. The two-pass method still has the advantage of cache-local accumulation, but the difference is marginal.</p>
<h3 id="what-about-dense-matrices">What About Dense Matrices?</h3>
<p>To be sure of our hypothesis, we can test it directly using a dense matrix of size <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=10,000</annotation></semantics></math></span></span>. For dense problems, the matrix-vector product is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span></span>, it dominates all other costs. Memory latency will become negligible relative to the compute work and the cache efficiency advantage should disappear.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/dense-tradeoff.png" alt="Dense Matrix Runtime vs Iterations"></p>
<p>We can see that the two-pass method runs almost exactly twice as slow as the one-pass method. The slope ratio is <em>exactly</em> 2:1. In a compute-bound regime, the extra matrix-vector products cannot be hidden by cache effects. Here, the theoretical trade-off holds perfectly.</p>
<h2 id="scalability">Scalability</h2>
<p>Now, let‚Äôs fix the iteration count at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">k=500</annotation></semantics></math></span></span> and vary <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">50,000</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>500</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">500,000</annotation></semantics></math></span></span> to measure scalability. Based on what we have seen before, we would expect the two-pass memory to scale linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> but with a small constant factor (three vectors, plus scalars). The one-pass method should also scale linearly, but with a <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span>-dependent slope.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/scalability_k500_rho3_memory.png" alt="Scalability Memory Usage"></p>
<p>Here we have to use a logarithmic y-axis to show both curves; the two-pass line is so flat relative to the one-pass line that it‚Äôs otherwise invisible.</p>
<p><img src="https://lukefleed.xyz/assets/lanczos/scalability_k500_rho3_time.png" alt="Scalability Runtime"></p>
<p>Runtime scales linearly with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> for both methods, as expected. Below <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>150</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">n=150,000</annotation></semantics></math></span></span>, the two methods have similar performance. This is the regime where both basis and working set fit in cache, or where the problem is small enough that memory latency is not the bottleneck.</p>
<p>As <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span></span> increases beyond <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">150,000</annotation></semantics></math></span></span>, the matrix-vector product time dominates. The sparse structure of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf{A}</annotation></semantics></math></span></span> ensures that each matvec requires multiple memory accesses per element. For the one-pass method, the final reconstruction of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">V</mi><mi>k</mi></msub><msub><mi mathvariant="bold">y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{V}_k \mathbf{y}_k</annotation></semantics></math></span></span> begins to cost more as the matrix grows. For the two-pass method, performing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">2k</annotation></semantics></math></span></span> matrix-vector products means the matvec cost accumulates more rapidly. The divergence is gradual, not sharp, because the advantage of cache locality in accumulation persists‚Äîbut it cannot overcome the fundamental cost of doubling the number of expensive operations.</p>
<hr>
<p>Well, that‚Äôs it. If you want to have a better look at the code or use it, it‚Äôs all open source:</p>
<ul>
<li><a href="https://github.com/lukefleed/two-pass-lanczos">Github Repository</a></li>
<li><a href="https://github.com/lukefleed/two-pass-lanczos/raw/master/tex/report.pdf">LaTeX Report</a></li>
</ul>
<p>This was more of an exploration than a production-ready library, so expect rough edges. But I hope it gives an interesting perspective on how algorithm engineering and low-level implementation details can alter what seems like a straightforward trade-off on a blackboard.</p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iPod Socks (234 pts)]]></title>
            <link>https://en.wikipedia.org/wiki/IPod_Socks</link>
            <guid>45889602</guid>
            <pubDate>Tue, 11 Nov 2025 16:52:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.wikipedia.org/wiki/IPod_Socks">https://en.wikipedia.org/wiki/IPod_Socks</a>, See on <a href="https://news.ycombinator.com/item?id=45889602">Hacker News</a></p>
Couldn't get https://en.wikipedia.org/wiki/IPod_Socks: Error: Request failed with status code 403]]></description>
        </item>
    </channel>
</rss>