<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Dec 2024 10:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Diátaxis – A systematic approach to technical documentation authoring (192 pts)]]></title>
            <link>https://diataxis.fr/</link>
            <guid>42325011</guid>
            <pubDate>Thu, 05 Dec 2024 04:35:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diataxis.fr/">https://diataxis.fr/</a>, See on <a href="https://news.ycombinator.com/item?id=42325011">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
        <a href="#">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div>
          

          <p><label for="__toc">
            <p>Toggle table of contents sidebar</p>
            <i><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </p></div>
        <article role="main">
          <section id="diataxis">
<span id="id1"></span>

<p>A systematic approach to technical documentation authoring.</p>
<hr>
<p>Diátaxis is a way of thinking about and doing documentation.</p>
<p>It prescribes approaches to content, architecture and form that emerge from a systematic approach to understanding the needs of documentation users.</p>

<p>Diátaxis identifies four distinct needs, and four corresponding forms of documentation - <em>tutorials</em>, <em>how-to guides</em>, <em>technical reference</em> and <em>explanation</em>. It places them in a systematic relationship, and proposes that documentation should itself be organised around the structures of those needs.</p>
<img alt="Diátaxis" src="https://diataxis.fr/_images/diataxis.png">
<p>Diátaxis solves problems related to documentation <em>content</em> (what to write), <em>style</em> (how to write it) and <em>architecture</em> (how to organise it).</p>
<p>As well as serving the users of documentation, Diátaxis has value for documentation creators and maintainers. It is light-weight, easy to grasp and straightforward to apply. It doesn’t impose implementation constraints. It brings an active principle of quality to documentation that helps maintainers think effectively about their own work.</p>
<hr>
<section id="contents">
<h2>Contents<a href="#contents" title="Link to this heading">¶</a></h2>
<p>This website is divided into two main sections, to help apply and understand Diátaxis.</p>
<div>
<p><em>Start here.</em> These pages will help make immediate, concrete sense of the approach.</p>

<p>This section explores the theory and principles of Diátaxis more deeply, and sets forth the understanding of needs that underpin it.</p>

</div>
<hr>
<p>Diátaxis is proven in practice. Its principles have been adopted successfully in hundreds of documentation projects.</p>
<blockquote>
<div><p>At Gatsby we recently reorganized our open-source documentation, and the Diátaxis framework was our go-to resource
throughout the project. The four quadrants helped us prioritize the user’s goal for each type of documentation. By
restructuring our documentation around the Diátaxis framework, we made it easier for users to discover the
resources that they need when they need them.</p>
<p>—<a href="https://hachyderm.io/@meganesulli">Megan Sullivan</a></p>
</div></blockquote>
<blockquote>
<div><p>While redesigning the <a href="https://developers.cloudflare.com/">Cloudflare developer docs</a>, Diátaxis became our north star for information architecture. When we weren’t sure where a new piece of content should fit in, we’d consult the framework. Our documentation is now clearer than it’s ever been, both for readers and contributors.</p>
<p>—<a href="https://github.com/adamschwartz">Adam Schwartz</a></p>
</div></blockquote>

</section>
</section>

        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitcoin is over $100k (122 pts)]]></title>
            <link>https://www.tradingview.com/symbols/BTCUSD/</link>
            <guid>42324263</guid>
            <pubDate>Thu, 05 Dec 2024 02:41:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tradingview.com/symbols/BTCUSD/">https://www.tradingview.com/symbols/BTCUSD/</a>, See on <a href="https://news.ycombinator.com/item?id=42324263">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-query-type="media" data-props-id="fuP9Tl" id="tv-content" aria-label="Main content" tabindex="-1"><div><p><span><div><p><span><p>On December 5, Bitcoin reached $100,000, up from $90,000 on November 12. The U.S. experienced over $31 billion in net inflows from Bitcoin ETFs, raising its market cap to $2 trillion.</p></span><span><p>Bitcoin crossed the $100,000 mark, recording a 5.5% gain over a 24-hour period, with its current price at $101,345.</p></span><span><p>Bitcoin's price is influenced by short-term demand and long-term supply, with profit-taking affecting it. Analysts indicate that inflows from Bitcoin spot ETFs may drive the price closer to $100,000.</p></span></p></div></span><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP%3ABTCUSD"><span>Analyze the impact</span></a><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP%3ABTCUSD"><span>Analyze the impact</span></a></p></div><div data-container-name="company-info-id"><p><span>Bitcoin is the world’s most traded cryptocurrency, and represents the largest piece of the crypto market pie. It was the first digital coin and as such, remains the most famous and widely-adopted cryptocurrency in the world. It's the original gangster in whose footsteps all other coins follow. The birth of Bitcoin was the genesis of an entirely new asset class, and a huge step away from traditional, centrally controlled money. Today, many advocates believe Bitcoin will facilitate the next stage for the global financial system, although this — of course — remains to be seen.</span></p></div><div data-container-name="symbol-faq-widget-id"><div><div id="Accordion-details::Rmr:" inert=""><p>The current price of </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) is </p><!-- --><p>103,282</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p> — it has fallen </p><!-- --><p>−0.79</p><!-- --><p>% in the past 24 hours. Try placing this info into the context by checking out what coins are also <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-gainers/">gaining</a> and <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-losers/">losing</a> at the moment and seeing </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>.</p></div><div id="Accordion-details::R1mr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) trading volume in 24 hours is </p><!-- --><p>‪61.06 B‬</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p>. See how often other coins are traded in <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-most-traded/">this list</a>.</p></div><div id="Accordion-details::R26r:" inert=""><p>Bitcoin</p><!-- --><p> price has risen by </p><!-- --><p>3.53</p><!-- --><p>% over the last week, its month performance shows a </p><!-- --><p>38.46</p><!-- --><p>% increase, and as for the last year, </p><!-- --><p>Bitcoin</p><!-- --><p> has increased by </p><!-- --><p>134.27</p><!-- --><p>%. See more dynamics on </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>. <br>Keep track of coins' changes with our <a href="https://www.tradingview.com/heatmap/crypto/?color=change&amp;dataset=Crypto&amp;group=no_group&amp;size=market_cap_calc">Crypto Coins Heatmap</a>.</p></div><div id="Accordion-details::R2mr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) reached its highest price on </p><!-- --><p>Nov 22, 2024</p><!-- --><p> — it amounted to </p><!-- --><p>99,800</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p>. Find more insights on the </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>. <br>See the list of <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-gainers/">crypto gainers</a> and choose what best fits your strategy.</p></div><div id="Accordion-details::R36r:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) reached the lowest price of </p><!-- --><p>2</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p> on </p><!-- --><p>Oct 20, 2011</p><!-- --><p>. View more </p><!-- --><p>Bitcoin</p><!-- --><p> dynamics on the <a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">price chart</a>. <br>See the list of <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-losers/">crypto losers</a> to find unexpected opportunities.</p></div></div><div><div id="Accordion-details::R1ar:" inert=""><p>Bitcoin</p><!-- --><p> has the limit of </p><!-- --><p>‪21.00 M‬</p><!-- --><p> coins. No matter how the currency evolves, no new coins will be released after this number is reached.</p></div><div id="Accordion-details::R1qr:" inert=""><p>The safest choice when buying </p><!-- --><p>BTC</p><!-- --><p> is to go to a well-known crypto exchange. Some of the popular names are Binance, Coinbase, Kraken. But you'll have to find a reliable broker and create an account first. You can trade </p><!-- --><p>BTC</p><!-- --><p> right from TradingView charts — just <a href="https://www.tradingview.com/brokers/">choose a broker</a> and connect to your account.</p></div><div id="Accordion-details::R2ar:" inert=""><p>Crypto markets are famous for their volatility, so one should study all the available stats before adding crypto assets to their portfolio. Very often it's technical analysis that comes in handy. We prepared <a href="https://www.tradingview.com/symbols/BTCUSD/technicals/?exchange=BITSTAMP">technical ratings</a> for </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>): today its technical analysis shows the buy signal, and according to the 1 week rating </p><!-- --><p>BTC</p><!-- --><p> shows the buy signal. And you'd better dig deeper and study 1 month rating too — it's strong buy. Find inspiration in </p><a href="https://www.tradingview.com/symbols/BTCUSD/ideas/?exchange=BITSTAMP">Bitcoin<!-- --> trading ideas</a><p> and keep track of what's moving crypto markets with our <a href="https://www.tradingview.com/markets/cryptocurrencies/news/">crypto news feed</a>.</p></div><div id="Accordion-details::R2qr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) is just as reliable as any other crypto asset — this corner of the world market is highly volatile. Today, for instance, </p><!-- --><p>Bitcoin</p><!-- --><p> is estimated as </p><!-- --><p>5.66</p><!-- --><p>% volatile. The only thing it means is that you must prepare and examine all available information before making a decision. And if you're not sure about </p><!-- --><p>Bitcoin</p><!-- --><p>, you can find more inspiration in our <a href="https://www.tradingview.com/sparks/crypto/">curated watchlists</a>.</p></div><div id="Accordion-details::R3ar:" inert=""><p>You can discuss </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) with other users in our public chats, Minds or in the comments to <a href="https://www.tradingview.com/symbols/BTCUSD/ideas/?exchange=BITSTAMP">Ideas</a>.</p></div></div></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bringing K/V context quantisation to Ollama (149 pts)]]></title>
            <link>https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/</link>
            <guid>42323953</guid>
            <pubDate>Thu, 05 Dec 2024 01:40:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/">https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/</a>, See on <a href="https://news.ycombinator.com/item?id=42323953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Explaining the concept of K/V context cache quantisation, why it matters and the journey to <a href="https://github.com/ollama/ollama/pull/6279">integrate it into Ollama</a>.</p><p><a href="https://github.com/ollama/ollama/pull/6279"><img loading="lazy" src="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/ollama-release.png" alt="Release the Ollamas"></a></p><hr><h2 id="why-kv-context-cache-quantisation-matters">Why K/V Context Cache Quantisation Matters</h2><p>The <a href="https://github.com/ollama/ollama/pull/6279">introduction</a> of K/V context cache quantisation in Ollama is significant, offering users a range of benefits:</p><ul><li>• <strong>Run Larger Models</strong>: With reduced VRAM demands, users can now run larger, more powerful models on their existing hardware.</li><li>• <strong>Expand Context Sizes</strong>: Larger context sizes allow LLMs to consider more information, leading to potentially more comprehensive and nuanced responses. For tasks like coding, where longer context windows are beneficial, K/V quantisation can be a game-changer.</li><li>• <strong>Reduce Hardware Utilisation</strong>: Freeing up memory or allowing users to run LLMs closer to the limits of their hardware.</li></ul><p>Running the K/V context cache at Q8_0 quantisation effectively halves the VRAM required for the context compared to the default F16 with minimal quality impact on the generated outputs, while Q4_0 cuts it down to just one third (at the cost of some noticeable quality reduction).</p><p><img loading="lazy" src="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/llm-vram-components.svg" alt="What makes up a LLMs memory usage?"></p><p>Consider running a 8b parameter model with a 32K context size, the vRAM required for the context could be as follows:</p><ul><li>• <strong>F16</strong> K/V: Around <strong>6GB</strong>.</li><li>• <strong>Q8_0</strong> K/V: Around <strong>3GB</strong>, (50%~ saving).</li><li>• <strong>Q4_0</strong> K/V: Around <strong>2GB</strong>, (66%~ saving).</li></ul><p>Saving that 3GB of vRAM by using Q8_0 could be enough to either allow you to double the context size to 64K, or perhaps to run a larger parameter model (e.g. 14B instead of 8B).</p><hr><h2 id="interactive-vram-estimator">Interactive VRAM Estimator</h2><p>I’ve built an interactive VRAM estimator to help you understand the impact of K/V context cache quantisation on your VRAM usage. You can adjust the model size, context size and quantisation level to see how it affects the memory requirements.</p><hr><h2 id="enabling-kv-context-cache-quantisation-in-ollama">Enabling K/V Context Cache Quantisation in Ollama</h2><p>This is covered in the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache">Ollama FAQ</a>, but here’s a quick guide:</p><ul><li>• Build the latest version of Ollama from the main branch or download the pre-release binaries from Ollama’s <a href="https://github.com/ollama/ollama/releases">releases page</a>).</li><li>• Make sure you’re running Ollama with <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention">Flash Attention enabled</a> (<code>OLLAMA_FLASH_ATTENTION=1</code>), <em>Note: This should become the default behaviour in the near future as there’s no reason not to use it.</em></li><li>• Set the K/V cache quantisation to Q8_0 by adding <code>OLLAMA_KV_CACHE_TYPE="q8_0"</code> to the environment variables you run Ollama with.</li></ul><p>Start Ollama and Q8_0 quantisation will be used for the K/V context cache by default.</p><h3 id="implementation-limitations">Implementation Limitations</h3><p>See <a href="#what-wasnt-included-in-the-pr">what wasn’t included in the PR</a> for more information on these limitations.</p><ul><li>You cannot set the quantisation level in a model’s Modelfile (this would be really good to add back in).</li><li>You cannot set the K and V caches to different quantisation levels.</li><li>You cannot request a quantisation level via the Ollama API, or on the command line.</li></ul><hr><h2 id="understanding-kv-context-cache-quantisation">Understanding K/V Context Cache Quantisation</h2><p>K/V context quantisation is completely separate from model quantisation, which is the process of reducing the precision of the model’s weights and biases to save memory and improve performance. Instead of compressing the model itself, K/V context cache quantisation focuses on reducing the memory footprint of the context cache used during text generation.</p><blockquote><p><em>Matt Williams kindly featured the PR on his YouTube channel, which generated a lot of interest and feedback from the community.</em>
<a href="https://www.youtube.com/watch?v=RFaMiQ97EoE"><img loading="lazy" src="https://img.youtube.com/vi/RFaMiQ97EoE/0.jpg" alt="Matt William’s YouTube video on the PR"></a></p></blockquote><h3 id="kv-context-cache">K/V Context Cache</h3><p>You can think of the K/V (key-value) context as the ‘working memory’ of an LLM. It’s it needs to keep at front of mind as you interact with it. This cache can get <em>very</em> large - in the order of many gigabytes.</p><p>In simple terms, the K/V context cache acts as the memory of an LLM during text generation. It stores the essential information from the preceding text, allowing the model to maintain context and generate coherent responses.</p><h3 id="quantisation">Quantisation</h3><p>Quantisation <em>(or ‘quantization’ to our American friends)</em> can be thought of as <em>compression</em>, it works by reducing the precision of the numerical values stored within it. Think of it like rounding numbers - you lose a tiny bit of detail, but you save a lot of space.</p><p>When quantisation is applied to the K/V context cache it greatly reduces the memory requirements, allowing users to run larger models or use larger context sizes on their existing hardware.</p><p>The most commonly used quantisation levels for the K/V are Q8_0 and Q4_0, unquantised is referred to as F16 (or F32 although for inference you would not run F32).</p><h3 id="performance">Performance</h3><p>Quantisation of the K/V context cache has minimal impact on performance, with quantising the K cache slightly improving performance while quantising the V cache may have a slight negative impact. The overall performance impact is negligible, especially when weighed against the significant reductions in VRAM usage.</p><h3 id="quality">Quality</h3><p>• Q8_0 - Minimal quality impact for normal text generation models, suitable for most users to be enabled by default.
Perplexity measurements on an early implementation showed it added around 0.002~ perplexity to the model.</p><p>• Q4_0 - Some noticeable quality reduction, but still usable for those without much vRAM or working on creative tasks where quality is less critical.
In early testing, Q4_0 added around 0.206~ perplexity to the model.</p><p>As stated in the FAQ, it is not recommended to use K/V cache quantisation for embedding models as these are more sensitive to quantisation. The same may apply to vision/multi-modal models although I have not looked into this.</p><p>If you run into issues with quality however - you can simply disable K/V context cache quantisation by setting <code>OLLAMA_KV_CACHE_TYPE</code> environment variable to <code>f16</code> or not setting it at all. This is even easier if you run Ollama in a container as you can simply run two containers with different configurations (as there’s practically no overhead to running multiple containers).</p><p>Note that the ability to set the K/V cache quantisation level in a model’s Modelfile was <a href="#what-wasnt-included-in-the-pr">removed from the PR</a>, but I hope that Ollama will reconsider this in the future.</p><hr><h2 id="compatibility">Compatibility</h2><p>K/V context cache quantisation requires <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention">Flash Attention</a> to be enabled. Enabling Flash Attention has no negative impacts and is something I expect to become the default behaviour in the near future with Ollama.</p><p>While practically all modern models support Flash Attention, if a model is loaded that - or if your hardware doesn’t support Flash Attention, Ollama will automatically fall back to the default F16 quantisation. You’ll see a warning in the logs if this happens.</p><p>Supported Hardware:</p><ul><li>• Apple Silicon (Metal): Works on Apple Silicon devices.</li><li>• NVIDIA GPUs: Works on all NVIDIA GPUs with CUDA support, Pascal and newer.</li><li>• AMD: Works on most AMD GPUs with ROCm support, although ROCm in general is not as well supported as CUDA or Metal and performance may vary.</li></ul><hr><h2 id="the-journey-to-integration">The Journey to Integration</h2><p>The journey to integrate K/V context cache quantisation into Ollama took around 5 months.</p><p>The hard work was done up front by <a href="https://github.com/ggerganov/">ggerganov</a> in the underlying <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, which Ollama uses as it’s primary inference engine.</p><p>My PR integrated that functionality into Ollama which involved not just supporting the required configuration, but implementing memory estimations for layer placement, error and condition handling, ensuring compatibility with the existing codebase and a lot of testing.</p><h3 id="successes">Successes</h3><ul><li>• It’s merged!</li><li>• Extensive testing and feedback from the community.</li><li>• Once the PR gained traction with the Ollama team, <a href="https://github.com/jmorganca">jmorganca</a> and <a href="https://github.com/jessegross">jessegross</a> were both incredibly helpful in providing feedback and guidance, especially as I am not a Golang developer.</li><li>• The PR became so popular I knew of <em>many</em> people running Ollama from the feature branch, this is not something I wanted to see in the long term, but it was a good sign that people were interested in the feature.</li><li>• I have been building Ollama successfully at least twice, often much more every day for 5 months without major issues.</li></ul><h3 id="challenges">Challenges</h3><ul><li>• Explaining the concept and benefits of K/V context cache quantisation to the community.</li><li>• Addressing merge conflicts due to updates in the Ollama main branch as time passed.</li><li>• Adapting to the new CGO server implementation and refactoring the code to accommodate new runners.</li><li>• Ensuring compatibility with the existing codebase as it changed over time.</li><li>• Finding the right balance between allowing users to configure Ollama to meet their needs while maintaining simplicity and ease of use.</li><li>• The noise in the PR from folks either piling on or trying to help, which made it hard to keep track of the actual changes.</li><li>• Github’s PR interface which can be a bit clunky when dealing with large PRs.</li><li>• Daily battles with my (International / British English) spell checker to keep the Americanised spelling of words consistent with the rest of the codebase.</li></ul><p><em>It took 5 months, but we got there in the end.</em></p><hr><h2 id="definitions">Definitions</h2><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>LLM</td><td>Large Language Model, a type of AI model capable of understanding and generating human-like text.</td></tr><tr><td>vRAM</td><td>Video RAM, the memory used by your graphics card. LLMs require significant vRAM, especially for larger models and context sizes.</td></tr><tr><td>Context Size</td><td>The amount of text the LLM can “remember” and consider when generating a response including both the user’s inputs and the models own outputs. Larger context sizes allow for more nuanced and relevant output.</td></tr><tr><td>Quantisation</td><td>A technique for reducing the precision of numerical values, resulting in smaller data sizes.</td></tr><tr><td>Q8_0 &amp; Q4_0</td><td>Different levels of quantisation, with Q8_0 halving the VRAM usage of the context and Q4_0 reducing it to one third compared to F16 (unquantised).</td></tr><tr><td>llama.cpp</td><td>The primary underlying inference engine used by Ollama.</td></tr><tr><td>Flash Attention</td><td>A technique used to reduce the memory requirements of LLMs by only <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention">attending</a> to a subset of the context at a time.</td></tr><tr><td>ROCm</td><td>The AMD Radeon Open Compute platform, an open-source platform for GPU computing</td></tr><tr><td>CUDA</td><td>A parallel computing platform and application programming interface model created by Nvidia</td></tr><tr><td>Metal</td><td>A low-level, low-overhead hardware-accelerated graphics and compute application programming interface developed by Apple.</td></tr></tbody></table><hr><h2 id="what-wasnt-included-in-the-pr">What wasn’t included in the PR</h2><p>Originally I had several features which in the PR that were not included in the final version as Ollama wanted to minimise the configuration exposed to users and not introduce API changes.</p><ul><li>• All the K/V quantisation types supported by llama.cpp (Q4_1, IQ4_NL, Q5_0, Q5_1).</li><li>• Ability to set the quantisation level in a models Modelfile (I’m hoping this will be added back in the future).</li><li>• API parameters that allowed setting the quantisation level when making requests.</li><li>• CMD line parameters that allowed setting the quantisation level when running Ollama.</li></ul><p>Additionally the ability to set different quantisation levels for the K and V caches was not included, this might be nice to add back in the future, as to quote <a href="https://github.com/JohannesGaessler">JohannesGaessler</a> [<a href="https://github.com/ggerganov/llama.cpp/pull/7412#issuecomment-2120427347">May 2024</a>] while measuring the quality impact of K/V context cache quantisation using an older implementation:</p><blockquote><p><em>• The K cache seems to be much more sensitive to quantization than the V cache. However, the weights seem to still be the most sensitive.</em></p><p><em>• Using q4_0 for the V cache and FP16 for everything else is more precise than using q6_K with FP16 KV cache.</em></p><p><em>• A 6.5 bit per value KV cache with q8_0 for the K cache and q4_0 for the V cache also seems to be more precise than q6_K weights. There seems to be no significant quality loss from using q8_0 instead of FP16 for the KV cache.</em></p></blockquote><hr><h2 id="reporting-issues">Reporting Issues</h2><p>If you find a bug with K/V context cache quantisation it could be either in Ollama or, perhaps more likely - in the underlying llama.cpp project.</p><p>Remember to test using the latest Ollama release or main branch build and to test with the feature disabled to ensure it’s actually related to K/V context cache quantisation.</p><p>It’s likely to be Ollama if it’s related to:</p><ul><li>• Enabling/disabling the feature.</li><li>• Memory estimations (e.g. how many layers are offloaded to each GPU).</li></ul><p>It’s more likely to a bug in llama.cpp if it’s related to:</p><ul><li>• Performance.</li><li>• ROCm support.</li><li>• Model compatibility.</li><li>• Quality issues.</li></ul><p>When logging a bug:</p><ul><li>• You should first <a href="https://github.com/ggerganov/llama.cpp/issues?q=sort%3Aupdated-desc+is%3Aissue+k%2Fv+quantization">search for existing issues in the llama.cpp project</a>.</li><li>• Reach out to the community to see if others have experienced the issue.<ul><li>• Ollama has a <a href="https://discord.gg/ollama">Discord server</a> where you can discuss issues, although be mindful that <strong>Discord is an information black hole and is not well suited to knowledge discovery or issue tracking</strong>.</li><li>• If you find an issue do not add a comment such as “+1” or “I have this issue too” - instead use an emoji reaction to indicate your support and only comment if you have valuable information to add.</li></ul></li></ul><p>Finally, if you can’t find an existing issue, you can create an issue (in the relevant project), ensuring you include your hardware configuration, software versions, environmental settings, the model you’re using, the context size, the quantisation level and any other relevant information.</p><div><p>Remember: You’re <em>logging a bug</em> to a free and open source project, not requesting <em>support</em> from a paid service.</p><p>Be patient, respectful and provide as much information as you can to help the developers diagnose and fix the issue if they have the time and resources to do so.</p></div><hr><h2 id="further-reading">Further Reading</h2><ul><li>• <a href="https://github.com/ollama/ollama/pull/6279">The PR to add K/V context cache quantisation to Ollama.</a></li><li>• <a href="https://www.youtube.com/watch?v=RFaMiQ97EoE">Matt William’s YouTube video on the PR</a></li><li>• <a href="https://smcleod.net/2024/07/understanding-ai/llm-quantisation-through-interactive-visualisations/">Understanding Quantisation</a></li><li>• Ollama<ul><li>• <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache">Ollama FAQ</a></li><li>• <a href="https://ollama.com/blog">Ollama Blog</a></li><li>• <a href="https://github.com/ollama/ollama/releases">Ollama Releases</a></li></ul></li><li>• <a href="https://huggingface.co/blog/kv-cache-quantization">HuggingFace’s blog post on K/V cache quantisation</a>, which provides a more technical deep dive into the topic in the context of Transformers</li><li>• Related llama.cpp PRs and performance measurements (note: these are now quite old and things have likely improved since):<ul><li>• <a href="https://github.com/ggerganov/llama.cpp/pull/7412#issuecomment-2120427347">ggerganov/llama.cpp#7412</a></li><li>• <a href="https://github.com/ggerganov/llama.cpp/pull/7527#issuecomment-2132341565">ggerganov/llama.cpp#7527</a></li></ul></li></ul><hr><p>Discuss this post on:</p><ul><li>• <a href="https://news.ycombinator.com/item?id=42323953">HackerNews</a></li><li>• <a href="https://x.com/ollama/status/1864487185443115378">Twitter</a></li><li>• <a href="https://www.linkedin.com/feed/update/urn:li:activity:7270248388057542656/">LinkedIn</a></li><li>• <a href="https://aus.social/@s_mcleod/113597789410718453">Mastodon</a></li><li>• <a href="https://www.reddit.com/r/LocalLLaMA/comments/1h62u1p/ollama_has_merged_in_kv_cache_quantisation/">Reddit</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Native dual-range input (175 pts)]]></title>
            <link>https://muffinman.io/blog/native-dual-range-input/</link>
            <guid>42320516</guid>
            <pubDate>Wed, 04 Dec 2024 18:39:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://muffinman.io/blog/native-dual-range-input/">https://muffinman.io/blog/native-dual-range-input/</a>, See on <a href="https://news.ycombinator.com/item?id=42320516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page"><p><a href="#content">Jump to content</a></p><section id="content" role="main" tabindex="-1"><svg viewBox="0 0 1000 40"><path d="M 0.25 -5 L 0,0 L 500 40 L 1000,0.25 L 1000,-5 z"></path><path d="M-2000 -160 L 500 40 L 3000,-160 L 3000 42 L -2000 42"></path><path d="M-2000 -160 L 500 40 L 3000,-160" fill="none" vector-effect="non-scaling-stroke"></path></svg><div><p>I just released <a href="https://github.com/stanko/dual-range-input">@stanko/dual-range-input</a> - a native dual-range input. Here is how it looks with the default styles:</p><p>The "native" part is somewhat open for discussion. I call it native because the library uses two native HTML range inputs. This means that all of the native interactions and accessibility features are preserved.</p><p>Native inputs allow us not to reinvent the wheel. There is about <a href="https://cdn.jsdelivr.net/npm/@stanko/dual-range-input/dist/index.js">fifty lines of JavaScript</a> to synchronize the inputs, along with some CSS to ensure everything looks polished.</p><p>In my book, that is <em>native enough</em>.</p><h2 id="why">Why<a aria-label="Anchor link for: why" title="Anchor link for: why" href="#why"> </a></h2><p>When I create <a href="https://muffinman.io/art">my generative drawings</a>, I use a tool I built myself. This tool includes a UI for tweaking parameters, and I often have minimum and maximum sliders for certain parameters. I thought it would be nice to have a dual-range slider for these. However, most existing solutions rely heavily on JavaScript and reimplement dragging and accessibility features.</p><p>So, I set my own set of requirements:</p><ul><li>It should use native HTML range inputs.</li><li>When you click on the track, the closer of the two thumbs should jump to that value.</li></ul><p>Hopefully, after you read these two requirements, my solution will make sense.</p><h2 id="how-it-works">How it works<a aria-label="Anchor link for: how-it-works" title="Anchor link for: how-it-works" href="#how-it-works"> </a></h2><p>There are two inputs placed next to each other. When either of the inputs is changed, the library calculates a midpoint between the two selected values. Then the <code>min</code> and <code>max</code> attributes are set to the midpoint, and the width of both inputs is updated to match.</p><p>Here is an unstyled example, which will hopefully illustrate this well:</p><p>Even like this, it works reasonably well. We'll style it later to make it look nicer.</p><h3 id="resizing-the-inputs">Resizing the inputs<a aria-label="Anchor link for: resizing-the-inputs" title="Anchor link for: resizing-the-inputs" href="#resizing-the-inputs"> </a></h3><p>There's a small trick involved in calculating input widths. This is because the range input's track is actually shorter than the input's total width. All browsers leave enough space on the sides so the thumb doesn't stick out.</p><p>Here is a screenshot from Firefox (other browsers work similarly), where you can see that the track is shorter than the width. I've emphasized the space the browser leaves for the thumb.</p><p><img alt="Screenshot showing that the range input's track is shorter than its total width" src="https://muffinman.io/img/dual-range-input/thumb-width.png"></p><p>If we take an example where the inputs need to be in a 1:3 ratio, simply setting their widths to 25% and 75% isn't enough. We also need to account for the thumb width. Instead of calculating the exact ratio, I simplified the math by adding the thumb width to each input's width:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span>input</span><span><span><span>:</span>first-child</span></span> </span><span>{</span>
</span><span> <span><span>width</span></span><span>:</span><span> <span><span>calc</span><span><span>(</span><span>25<span>%</span></span> <span>+</span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-thumb-width</span></span></span><span><span>)</span></span></span></span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span><span>input</span><span><span><span>:</span>last-child</span></span> </span><span>{</span>
</span><span> <span><span>width</span></span><span>:</span><span> <span><span>calc</span><span><span>(</span><span>75<span>%</span></span> <span>+</span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-thumb-width</span></span></span><span><span>)</span></span></span></span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>If you thought, <em>Wait, this adds up to more than 100%</em>, you'd be 100% right. That's why I applied a small trick: I added padding to the inputs' wrapper to accommodate the extra width for the thumbs.</p><p><img alt="Screenshot showing padding on the wrapper to accommodate extra width" src="https://muffinman.io/img/dual-range-input/padding.png"></p><p>This makes the math simpler while keeping the input sizing correct. It took me forever to explain this properly, and I'm still not sure if I succeeded.</p><h3 id="move-the-thumb-closer-to-the-click">Move the thumb closer to the click<a aria-label="Anchor link for: move-the-thumb-closer-to-the-click" title="Anchor link for: move-the-thumb-closer-to-the-click" href="#move-the-thumb-closer-to-the-click"> </a></h3><p>Because the inputs are resized to meet at the midpoint, whenever you click between the thumbs, the one closer to the click will move to that value.</p><p> and the midpoint will be easier to see.</p><p>If there's an odd number of steps between the thumbs, the last-used input is favored. Try it out with debug mode on, and you'll see what I mean.</p><p>With that, both requirements are satisfied. The only thing left is to style it properly.</p><h2 id="styling">Styling<a aria-label="Anchor link for: styling" title="Anchor link for: styling" href="#styling"> </a></h2><p>All browsers allow us to style range inputs using CSS. That made styling of the tracks and thumbs pretty straightforward. I just ensured that the tracks didn't have a border radius in the middle where they connect.</p><h3 id="theming">Theming<a aria-label="Anchor link for: theming" title="Anchor link for: theming" href="#theming"> </a></h3><p>I exposed several variables to make theming easier. Here's the complete list with their default values:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span><span>.</span></span><span>dual-range-input</span> </span><span>{</span>
</span><span>  <span><span><span>--</span><span>dri-height</span></span></span><span>:</span><span> <span>1<span>.</span>5<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-thumb-width</span></span></span><span>:</span><span> <span>1<span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-height</span></span></span><span>:</span><span> <span>1<span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-thumb-color</span></span></span><span>:</span><span> <span><span>#</span>ddd</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-hover-color</span></span></span><span>:</span><span> <span><span>#</span>a8d5ff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-active-color</span></span></span><span>:</span><span> <span><span>#</span>4eaaff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-border-color</span></span></span><span>:</span><span> <span><span>rgba</span><span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0<span>.</span>1</span></span><span><span>)</span></span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-border-radius</span></span></span><span>:</span><span> <span>1<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-track-height</span></span></span><span>:</span><span> <span>0</span><span><span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-color</span></span></span><span>:</span><span> <span><span>#</span>ccc</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-filled-color</span></span></span><span>:</span><span> <span><span>#</span>0084ff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-border-radius</span></span></span><span>:</span><span> <span>1<span>rem</span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>To create your own theme, simply override these variables.</p><h3 id="gradients">Gradients<a aria-label="Anchor link for: gradients" title="Anchor link for: gradients" href="#gradients"> </a></h3><p>One thing I thought was cool is how I used CSS gradients to paint the selected range in both inputs. I set the gradients to use the <code>--dri-gradient-position</code> variable, then updated that variable in the code along with the widths.</p><p>Here's how the CSS looks for one of the inputs:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span>input</span><span><span><span>:</span>first-child</span></span><span><span>::</span><span>-moz-</span>range-track</span> </span><span>{</span>
</span><span>  <span><span>background-image</span></span><span>:</span><span> <span><span>linear-gradient</span><span><span>(</span>
</span></span></span></span><span><span><span><span>    <span>to</span> <span>right</span><span>,</span>
</span></span></span></span><span><span><span><span>    <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-track-color</span></span></span><span><span>)</span></span></span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-gradient-position</span></span></span><span><span>)</span></span></span><span>,</span>
</span></span></span></span><span><span><span><span>    <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-track-filled-color</span></span></span><span><span>)</span></span></span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-gradient-position</span></span></span><span><span>)</span></span></span>
</span></span></span></span><span><span><span><span>  </span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>Again,  and the semi-transparent thumbs will make the gradients easier to see.</p><h2 id="conclusion">Conclusion<a aria-label="Anchor link for: conclusion" title="Anchor link for: conclusion" href="#conclusion"> </a></h2><p>I had to write this post as <del>a brain dump</del> a way to consolidate my thoughts, so I hope it's not too convoluted.</p><p>Thank you for following along, and I hope this inspires you to try it out and consider using more native elements before opting for custom libraries.</p></div></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Outerbase Studio – Open-Source Database GUI (245 pts)]]></title>
            <link>https://github.com/outerbase/studio</link>
            <guid>42320032</guid>
            <pubDate>Wed, 04 Dec 2024 17:55:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/outerbase/studio">https://github.com/outerbase/studio</a>, See on <a href="https://news.ycombinator.com/item?id=42320032">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Outerbase Studio</h2><a id="user-content-outerbase-studio" aria-label="Permalink: Outerbase Studio" href="#outerbase-studio"></a></p>
<p dir="auto"><strong>Outerbase Studio</strong> is a lightweight, browser-based GUI for managing SQL databases, designed for simplicity and versatility. Initially built for LibSQL and SQLite, it now supports a broad range of databases, including:</p>
<p dir="auto"><strong>Supported Databases:</strong></p>
<ul dir="auto">
<li><strong>SQLite-based Database</strong>
<ul dir="auto">
<li>Turso/LibSQL</li>
<li>SQLite (local files)</li>
<li>Cloudflare D1</li>
<li>rqlite</li>
<li>StarbaseDB</li>
<li>Val.town</li>
</ul>
</li>
<li>MySQL (beta, limited features)</li>
<li>PostgreSQL (beta, limited features)</li>
</ul>
<hr>
<p dir="auto">Give it a try directly from your browser</p>
<p dir="auto"><a href="https://libsqlstudio.com/" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/4539653/350291679-5d92ce58-9ce6-4cd7-9c65-4763d2d3b231.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMzNTE3MDMsIm5iZiI6MTczMzM1MTQwMywicGF0aCI6Ii80NTM5NjUzLzM1MDI5MTY3OS01ZDkyY2U1OC05Y2U2LTRjZDctOWM2NS00NzYzZDJkM2IyMzEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMDRUMjIzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YjlhNmFkMTE1ODc1ZDljOTFmN2Q3MzdkZjgxNzcyMTgwZDNmNWUzOTZiMzdjZWNiOGIxYTliMTMzM2MyNmFkMSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.VdECOwaLsFE_cScRvcLG0fw-0atqrvFGub02Lf3g5QA" alt="LibSQL Studio, sqlite online editor" secured-asset-link=""></a>
<a href="https://libsqlstudio.com/playground/client?template=chinook" rel="nofollow"><img src="https://private-user-images.githubusercontent.com/4539653/350291645-dcf7e246-fe72-4351-ab10-ae2d1658087d.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMzNTE3MDMsIm5iZiI6MTczMzM1MTQwMywicGF0aCI6Ii80NTM5NjUzLzM1MDI5MTY0NS1kY2Y3ZTI0Ni1mZTcyLTQzNTEtYWIxMC1hZTJkMTY1ODA4N2QucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMDRUMjIzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MmJhOWYyZmU3NmNmYWI0MTE2ZWUyZmIzZDFjY2MyMGZhYTIzMTljYzM0M2FkYzkzM2JlY2EzYzIxYjI4MDliNCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iXRst9DCrdQ2NBSYBHjXk6SPBM8Unf7KtyiYoiK6U8w" alt="Libsql studio playground" secured-asset-link=""></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Desktop App</h2><a id="user-content-desktop-app" aria-label="Permalink: Desktop App" href="#desktop-app"></a></p>
<p dir="auto">You can download <a href="https://github.com/outerbase/studio-desktop/releases/">Windows and Mac desktop app here</a>.</p>
<p dir="auto">Outerbase Studio Desktop is a lightweight Electron wrapper for the Outerbase Studio web version. It enables support for drivers that aren't feasible in a browser environment, such as MySQL and PostgreSQL.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/4539653/350292159-1d7a3d90-61e3-4a77-83a5-4bb096bbfb4b.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMzNTE3MDMsIm5iZiI6MTczMzM1MTQwMywicGF0aCI6Ii80NTM5NjUzLzM1MDI5MjE1OS0xZDdhM2Q5MC02MWUzLTRhNzctODNhNS00YmIwOTZiYmZiNGIuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMDRUMjIzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDJlNDU4MmQ3MWM5YzM4NjM2MWVhYjE3N2ZiZTA1MDU1NjUyZjE4NDZiNGU1OWI1YjRhZjNlNTkyNDAwYTA0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.3IPl5luif-NcmkAKuqYJv5YnrXGAlp3zbInQtAgqjKU"><img src="https://private-user-images.githubusercontent.com/4539653/350292159-1d7a3d90-61e3-4a77-83a5-4bb096bbfb4b.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMzNTE3MDMsIm5iZiI6MTczMzM1MTQwMywicGF0aCI6Ii80NTM5NjUzLzM1MDI5MjE1OS0xZDdhM2Q5MC02MWUzLTRhNzctODNhNS00YmIwOTZiYmZiNGIuZ2lmP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTIwNCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEyMDRUMjIzMDAzWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZDJlNDU4MmQ3MWM5YzM4NjM2MWVhYjE3N2ZiZTA1MDU1NjUyZjE4NDZiNGU1OWI1YjRhZjNlNTkyNDAwYTA0NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.3IPl5luif-NcmkAKuqYJv5YnrXGAlp3zbInQtAgqjKU" alt="libsqlstudio-git-preview (7)" data-animated-image=""></a></p>
<ul dir="auto">
<li><strong>Query Editor</strong>: It features a user-friendly query editor equipped with auto-completion and function hint tooltips. It allows you to execute multiple queries simultaneously and view their results efficiently.</li>
<li><strong>Data Editor</strong>: It comes with a powerful data editor, allowing you to stage all your changes and preview them before committing. The data table is highly optimized and lightweight, capable of rendering thousands of rows and columns efficiently.</li>
<li><strong>Schema Editor</strong>: It allows you to quickly create, modify, and remove table columns with just a few clicks without writing any SQL.</li>
<li><strong>Connection Manager</strong>: It includes a flexible connection manager, allowing you to store your connections locally in your browser. You can also store them on a server and share your connections across multiple devices.</li>
</ul>
<p dir="auto">The features mentioned above are just a few of the many we offer. Give it a try to explore everything we have in store.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grifters, believers, grinders, and coasters (211 pts)]]></title>
            <link>https://www.seangoedecke.com/programmer-archetypes/</link>
            <guid>42319997</guid>
            <pubDate>Wed, 04 Dec 2024 17:52:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/programmer-archetypes/">https://www.seangoedecke.com/programmer-archetypes/</a>, See on <a href="https://news.ycombinator.com/item?id=42319997">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>Why do engineers get mad at each other so often?<sup id="fnref-1"><a href="#fn-1">1</a></sup> </p>
<p>I think a lot of programmer arguments bottom out in a cultural clash between different kinds of engineers: believers vs grifters, or coasters vs grinders. I’m going to argue that good companies actually have a healthy mix of all four types of engineer, so it’s probably sensible to figure out how to work with them.</p>
<p>Despite the names, I think grifters and coasters can be as good at their jobs as believers and grinders. I’m naming them this way because these are the names you’d give them when you’re complaining about your coworkers, and this article is really aimed at people who are trying to have a bit more empathy for the assholes they work with. I myself fall mostly in the grifter + coaster quadrant, and <a href="https://www.seangoedecke.com/how-to-ship">I think I’m great at my job</a>. Here’s a beautiful diagram:</p>
<p><span>
      <a href="https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/081d5/quadrants.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="quadrants" title="quadrants" src="https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/fcda8/quadrants.png" srcset="https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/12f09/quadrants.png 148w,
https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/e4a3f/quadrants.png 295w,
https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/fcda8/quadrants.png 590w,
https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/efc66/quadrants.png 885w,
https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/c83ae/quadrants.png 1180w,
https://www.seangoedecke.com/static/463a9c7a46c66e8232c46aaad5523766/081d5/quadrants.png 1264w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>These aren’t immutable aspects of your personality. They’re more categories for how you approach the job of software engineering - you’ll move around between quadrants as you change your approach to work, for all the usual reasons.</p>
<h2>Grifters</h2>
<p>Grifters play the game to win. They think carefully about the image they’re presenting to leadership, they make tactical decisions around performance cycles, and they are comfortable speaking the language of the organization. They respect the company mission as written, but what they really value is what the leaders of the company show they care about. Despite the name, grifters are not <em>frauds</em>. In an organization that rewards shipping good code and delighting customers, grifters will ship good code and delight customers. But they won’t sacrifice their own interests to ship good code and delight customers in an organization that rewards other behavior.</p>
<p>Grifters are good at getting stuff done in large organizations. You want a grifter leading a complicated engineering project, because a grifter understands the levers of power in a large organization and how to use them. Without any grifter involvement, projects tend to mysteriously stall out from lack of buy-in. However, Grifters aren’t very good at changing the culture of their organizations. They tend to go with the current instead. If all you have are grifters, your organization will probably naturally devolve into a lowest-common-denominator culture. Grifters also aren’t good at important work that’s unrewarding. In most software companies, you really want a handful of engineers obsessing about issues like accessibility, security and performance all the time, even when the organization as a whole doesn’t care about it. Those people probably won’t be grifters. </p>
<h2>Believers</h2>
<p>Believers just want to do good work. They think making tactical decisions and “managing up” is slimy, and they don’t do it. They truly value the company mission, or they wouldn’t be there. Typically they’re heavily invested in product decisions, and prioritize the user experience over profit. Believers are often under-promoted, either due to alienating leadership or just not investing enough effort into the promotion cycle. They’re not <em>suckers</em>, though - they know the political cost they’re paying by refusing to play the game. They’re willing to pay that cost in order to work in accordance with their values.</p>
<p>Believers are good at keeping an organization focused on the customer. They’re out there walking the walk and talking the talk, drumming into new hires that At This Company We Do Things Right. They’re also good at keeping code quality up and obsessing over issues that require long-term maintenance (like performance). However, they can struggle when an organization changes focus. In my experience, as companies grow and move more into the enterprise market, there’s usually a cohort of alienated believers who are made very unhappy. If your entire organization is believers, you’ll be very well-equipped to execute but have zero flexibility. You’re going to stick to the mission that everyone believes in, even if it runs the org directly into the ground.</p>
<h2>Coasters</h2>
<p>Coasters are chill. They work enough to get the job done, but typically no further - above all else they avoid work they see as unnecessary. They typically do a lot of “hammock time”: non-work activities where they ponder a work problem in the back of their minds (this is particularly true for senior+ engineers). They still take work seriously: when they’re working, work has their full attention. But they don’t force themselves to produce code when they’re really not feeling it. Why go to a ton of effort to push out mediocre work when they can come back to it later and do it right?</p>
<p>Coasters are good at maintaining a calm, safe environment on teams. They’re also good for teams that have a lot of last-minute requests or questions, because coasters have “slack in the system”: they’ll rarely be completely consumed by a particular task for days. But they’re not as good for teams that have a lot of well-defined work queued up - for that, you want a grinder. A software engineering org could survive with all coasters, but when the pressure ramps up it’s easier to have some grinders around.</p>
<h2>Grinders</h2>
<p>Grinders are locked in. There’s always something that needs doing and the grinder is ready to do it. They just love the mechanics of the job - writing code, reviewing PRs, answering questions on Slack - or at least they love being useful. They’re always heads-down on a problem, sometimes to the point where they can’t see the forest for the trees. Still, if you need something done fast, give it to a grinder.</p>
<p>The strengths of a grinder are obvious: they do a lot of work. The weaknesses are also pretty straightforward: when a grinder burns out, they burn out <em>hard</em>. In my experience, grinders also tend to be pretty high-strung, since operating with intensity for a long time puts strain on the whole system. If it’s not obvious from the way I’m writing about this, I don’t think being a grinder is sustainable as a general mode of operating (of course I’d say that, since I’m a coaster by nature). I’ve met a lot of junior engineers who are grinders, but very few staff+ engineers. I think that’s no accident. You either learn to dial it back a bit in the first 5-10 years or you flame out of the industry entirely.</p>
<h2>Summary</h2>
<ul>
<li>If you’re a grifter, you need to figure out how to work with believers, because an all-grifter company isn’t the kind of place you want to work.</li>
<li>If you’re a believer, you don’t have to figure out how to work with grifters, but you’ll probably have to be very selective about the companies you work for.</li>
<li>If you’re a grinder, you need to figure out how to work with coasters, because you’re likely to become one at some point if you want to stay in the industry (or at minimum many of your peers will).</li>
<li>If you’re a coaster, you should try and be understanding about grinders running around stressing everyone out. You’re only able to coast because someone is willing to occasionally grind.</li>
</ul>
</section><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI helps researchers dig through old maps to find lost oil and gas wells (172 pts)]]></title>
            <link>https://newscenter.lbl.gov/2024/12/04/ai-helps-researchers-dig-through-old-maps-to-find-lost-oil-and-gas-wells/</link>
            <guid>42319969</guid>
            <pubDate>Wed, 04 Dec 2024 17:50:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newscenter.lbl.gov/2024/12/04/ai-helps-researchers-dig-through-old-maps-to-find-lost-oil-and-gas-wells/">https://newscenter.lbl.gov/2024/12/04/ai-helps-researchers-dig-through-old-maps-to-find-lost-oil-and-gas-wells/</a>, See on <a href="https://news.ycombinator.com/item?id=42319969">Hacker News</a></p>
Couldn't get https://newscenter.lbl.gov/2024/12/04/ai-helps-researchers-dig-through-old-maps-to-find-lost-oil-and-gas-wells/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[UnitedHealthcare CEO fatally shot in midtown Manhattan (189 pts)]]></title>
            <link>https://www.cnn.com/2024/12/04/us/brian-thompson-united-healthcare-death/index.html</link>
            <guid>42317968</guid>
            <pubDate>Wed, 04 Dec 2024 14:52:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/12/04/us/brian-thompson-united-healthcare-death/index.html">https://www.cnn.com/2024/12/04/us/brian-thompson-united-healthcare-death/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=42317968">Hacker News</a></p>
Couldn't get https://www.cnn.com/2024/12/04/us/brian-thompson-united-healthcare-death/index.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A pro-science, pro-progress, techno-optimistic health textbook from 1929 (126 pts)]]></title>
            <link>https://moreisdifferent.blog/p/a-pro-science-pro-progress-techno</link>
            <guid>42317952</guid>
            <pubDate>Wed, 04 Dec 2024 14:50:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://moreisdifferent.blog/p/a-pro-science-pro-progress-techno">https://moreisdifferent.blog/p/a-pro-science-pro-progress-techno</a>, See on <a href="https://news.ycombinator.com/item?id=42317952">Hacker News</a></p>
Couldn't get https://moreisdifferent.blog/p/a-pro-science-pro-progress-techno: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Genie 2: A large-scale foundation world model (1020 pts)]]></title>
            <link>https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/</link>
            <guid>42317903</guid>
            <pubDate>Wed, 04 Dec 2024 14:45:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/</a>, See on <a href="https://news.ycombinator.com/item?id=42317903">Hacker News</a></p>
Couldn't get https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Nearly half of teenagers globally cannot read with comprehension (118 pts)]]></title>
            <link>https://ourworldindata.org/data-insights/nearly-half-of-teenagers-globally-cannot-read-with-comprehension</link>
            <guid>42317442</guid>
            <pubDate>Wed, 04 Dec 2024 13:56:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ourworldindata.org/data-insights/nearly-half-of-teenagers-globally-cannot-read-with-comprehension">https://ourworldindata.org/data-insights/nearly-half-of-teenagers-globally-cannot-read-with-comprehension</a>, See on <a href="https://news.ycombinator.com/item?id=42317442">Hacker News</a></p>
Couldn't get https://ourworldindata.org/data-insights/nearly-half-of-teenagers-globally-cannot-read-with-comprehension: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I combined spaced repetition with emails so you can remember anything (176 pts)]]></title>
            <link>https://www.ginkgonotes.com/</link>
            <guid>42317393</guid>
            <pubDate>Wed, 04 Dec 2024 13:50:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ginkgonotes.com/">https://www.ginkgonotes.com/</a>, See on <a href="https://news.ycombinator.com/item?id=42317393">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><section><div><p>Add some notes and we'll email them to you on specific days using spaced repetition. This personalized schedule ensures you remember.</p><div><p><span>100+</span> people remember their notes</p></div></div><div><p><span>Try adding a note 🪄</span><img alt="arrow" loading="lazy" width="80" height="80" decoding="async" data-nimg="1" srcset="https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farrow.e13ecc6a.png&amp;w=96&amp;q=75 1x, https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farrow.e13ecc6a.png&amp;w=256&amp;q=75 2x" src="https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Farrow.e13ecc6a.png&amp;w=256&amp;q=75"></p></div></section><section><p>"Spaced repetition" is the best proven method to remember new information. That's why we built this app around it.</p><div><div><p><span>📚</span></p><h3>You spend hours reading and listening</h3></div><svg viewBox="0 0 138 138" fill="none" xmlns="http://www.w3.org/2000/svg"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M72.9644 5.31431C98.8774 43.8211 83.3812 88.048 54.9567 120.735C54.4696 121.298 54.5274 122.151 55.0896 122.639C55.6518 123.126 56.5051 123.068 56.9922 122.506C86.2147 88.9044 101.84 43.3918 75.2003 3.80657C74.7866 3.18904 73.9486 3.02602 73.3287 3.44222C72.7113 3.85613 72.5484 4.69426 72.9644 5.31431Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M56.5084 121.007C56.9835 118.685 57.6119 115.777 57.6736 115.445C59.3456 106.446 59.5323 97.67 58.4433 88.5628C58.3558 87.8236 57.6824 87.2948 56.9433 87.3824C56.2042 87.4699 55.6756 88.1435 55.7631 88.8828C56.8219 97.7138 56.6432 106.225 55.0203 114.954C54.926 115.463 53.5093 121.999 53.3221 123.342C53.2427 123.893 53.3688 124.229 53.4061 124.305C53.5887 124.719 53.8782 124.911 54.1287 125.015C54.4123 125.13 54.9267 125.205 55.5376 124.926C56.1758 124.631 57.3434 123.699 57.6571 123.487C62.3995 120.309 67.4155 116.348 72.791 113.634C77.9171 111.045 83.3769 109.588 89.255 111.269C89.9704 111.475 90.7181 111.057 90.9235 110.342C91.1288 109.626 90.7117 108.878 89.9963 108.673C83.424 106.794 77.3049 108.33 71.5763 111.223C66.2328 113.922 61.2322 117.814 56.5084 121.007Z"></path></g></svg><div><p><span>😮‍💨</span></p><h3>You create notes on important insights</h3></div><svg viewBox="0 0 138 138" fill="none" xmlns="http://www.w3.org/2000/svg"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M72.9644 5.31431C98.8774 43.8211 83.3812 88.048 54.9567 120.735C54.4696 121.298 54.5274 122.151 55.0896 122.639C55.6518 123.126 56.5051 123.068 56.9922 122.506C86.2147 88.9044 101.84 43.3918 75.2003 3.80657C74.7866 3.18904 73.9486 3.02602 73.3287 3.44222C72.7113 3.85613 72.5484 4.69426 72.9644 5.31431Z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M56.5084 121.007C56.9835 118.685 57.6119 115.777 57.6736 115.445C59.3456 106.446 59.5323 97.67 58.4433 88.5628C58.3558 87.8236 57.6824 87.2948 56.9433 87.3824C56.2042 87.4699 55.6756 88.1435 55.7631 88.8828C56.8219 97.7138 56.6432 106.225 55.0203 114.954C54.926 115.463 53.5093 121.999 53.3221 123.342C53.2427 123.893 53.3688 124.229 53.4061 124.305C53.5887 124.719 53.8782 124.911 54.1287 125.015C54.4123 125.13 54.9267 125.205 55.5376 124.926C56.1758 124.631 57.3434 123.699 57.6571 123.487C62.3995 120.309 67.4155 116.348 72.791 113.634C77.9171 111.045 83.3769 109.588 89.255 111.269C89.9704 111.475 90.7181 111.057 90.9235 110.342C91.1288 109.626 90.7117 108.878 89.9963 108.673C83.424 106.794 77.3049 108.33 71.5763 111.223C66.2328 113.922 61.2322 117.814 56.5084 121.007Z"></path></g></svg><div><p><span>😔</span></p><h3>Days later, you've forgotten most of it</h3></div></div></section><div id="demo"><p><h2>Be more knowledgeable with less effort</h2></p></div><div id="faq"><div><p>FAQ</p><p>Frequently Asked Questions</p></div><ul><li></li><li></li><li></li><li></li><li></li><li></li></ul></div><div id="pricing"><div><p>Pricing</p><h2>Notes you’ll never lose track of</h2></div><div><div><div><p>Ginkgo Notes</p><p>Simple pricing. All Ginkgo Notes features.</p></div><p><label for="pricing-switch">Billed yearly</label></p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Create up to 1000 new notes per month</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Customizable repetition settings</span></li></ul></div><div><p><span>POPULAR</span></p><div><div><p>Pay once</p><p>Your long-term learning companion</p></div><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Unlimited notes</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Customizable repetition settings</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Early access to new features</span></li></ul><div><p>Pay once. Access forever.</p></div></div></div><div><div><p>Enterprise</p><p>For educational institutions and large teams</p></div><p>Custom</p><ul><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Batch note creation</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Branded note templates</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Admin dashboard</span></li><li><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M16.704 4.153a.75.75 0 01.143 1.052l-8 10.5a.75.75 0 01-1.127.075l-4.5-4.5a.75.75 0 011.06-1.06l3.894 3.893 7.48-9.817a.75.75 0 011.05-.143z" clip-rule="evenodd"></path></svg><span>Priority support</span></li></ul></div></div></div><div><h2>Forget about forgetting</h2><p>Don't waste time learning new information that you'll forget.</p></div></section><div><p><a aria-current="page" href="https://www.ginkgonotes.com/#"><img alt="GinkgoNotes logo" fetchpriority="high" width="24" height="24" decoding="async" data-nimg="1" srcset="https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ficon.aed309e0.png&amp;w=32&amp;q=75 1x, https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ficon.aed309e0.png&amp;w=48&amp;q=75 2x" src="https://www.ginkgonotes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ficon.aed309e0.png&amp;w=48&amp;q=75"></a></p><p>Remember your notes with science</p><p>Copyright © <!-- -->2024<!-- --> - All rights reserved</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[They don't make them like that any more: the Yamaha DX7 keyboard (168 pts)]]></title>
            <link>https://kevinboone.me/dx7.html</link>
            <guid>42316902</guid>
            <pubDate>Wed, 04 Dec 2024 12:46:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kevinboone.me/dx7.html">https://kevinboone.me/dx7.html</a>, See on <a href="https://news.ycombinator.com/item?id=42316902">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">





<p><img src="https://kevinboone.me/img/notes.gif"></p>
<p>The sound of the Yamaha DX7 synthesizer defined 1980s popular music.
While the DX7 could, in principle, produce a limitless range of sound
textures, few musicians used more than the 32 built-in present sounds.
As a result, the sound of the DX7 is immediately recognizable. For me,
the theme from the TV show <em>Twin Peaks</em> typifies the DX7 sound
but, frankly, you’ll be hard-pressed to find a successful album from the
mid-80s where you don’t hear it. It would be pointless to list the bands
and artists that used the DX7 – it would be easier to make a list of the
ones that didn’t. I’m told that the ‘Electric Piano 1’ preset alone
appeared in over 60% of album releases of 1986.</p>
<figure>
<img src="https://kevinboone.me/img/dx7.jpg" alt="The Mark I DX7 in all its brutal glory">

</figure>
<p>The original, Mark I DX7 was a huge, uncompromising lump of ironwork,
made to be thrown in the back of a van. Its membrane keypad controls
were horrible to use, but were good at resisting beer spills. When you
turned up for practice with a DX7, everybody knew you meant
business.</p>
<p>What made the DX7 special was its novel approach to sound generation.
Although electronic keyboards were not new when the DX7 was released in
1983, they nearly all used an analog method of sound production.
Typically an electronic oscillator would generate a tone whose pitch was
set by the key pressed, and then a variety of filters and amplitude
modifiers would work on the sound to change its properties.</p>
<p>This is a completely logical way to approach simulating real musical
instruments electronically. Real instruments have something that
generates a tone – reed, string, whatever – and something that modifies
the characteristics of the tone – pipe, soundbox, and so on. Each
instrument has a characteristic amplitude envelope. For example, a note
on a piano has an initial louder, percussive peak in volume, followed by
a gradual decay. Analog synthesizers tried to simulate these acoustic
properties using electronics.</p>
<p>Most analog synthesizers of the 70s and early 80s were monophonic,
that is, they could only play one note at a time. Hitting a new key
while the sound of the first was playing would simply cut off the
original tone. You couldn’t play chords on an analog synthesizer, which
made it a ‘lead’ instrument in popular music – something that carried a
distinct tune rather than providing an accompaniment.</p>
<p>Synthesizer designers capitalized on this role, by providing their
instruments with <em>expression wheels</em>, which the musician would
operate with the left hand, while playing the melody with the right.
These wheels could vary the pitch of the note, like ‘bending’ the string
on a guitar, or alter the characteristic of a filter. A good-quality
analog synthesizer like the infamous Minimoog was as expressive as a
guitar or a violin and, frankly, easier to become proficient with. The
Minimoog was the signature sound of 1970 ‘prog’ rock. The DX7 continued
the tradition of left-hand expression wheels, as other instruments did,
and some still do.</p>
<p>Building an analog synthesizer that was polyphonic – where many keys
could be played at the same time – was stupendously expensive. The same
electronic circuitry had to be duplicated many times, sometimes for each
key. These instruments were mostly of academic interest – I can’t think
of any band that used one on stage (but I could be wrong).</p>
<p>The DX7 worked in a completely different way from the analog
synthesizers of the 70s. First, it was all-digital. But it didn’t try to
simulate digitally the tone generation methods of existing analog
synthesizers. Conceptually, the basis of its sound generation was
<em>frequency modulation</em>. That is, one sinewave tone generator
modified the pitch of another, which perhaps modified the pitch of
another. If this kind of modulation is at a low frequency, the result is
a rather conventional vibrato effect. In the DX7, though, the tones that
modulated one another were all in the audible range, producing an effect
that was not at all conventional.</p>
<p>The frequency modulator components in the DX7 were called
<em>operators</em>. For each of the sixteen notes that the instrument
could play polyphonically, there were six of these operators, making 96
in all. The DX7 could combine the operators in many different ways, with
the output of one feeding the inputs of others. These arrangements of
operators were known as ‘algorithms’. The standard algorithms were shown
on the top of the DX7’s cabinet, as you can see in the photo below.</p>
<figure>
<img src="https://kevinboone.me/img/dx7_detail.jpg" alt="Detail of the DX7 Mark II’s algorithm selection">

</figure>
<p>Algorithms 31 and 32 have all the operators contributing directly to
the output. Setting the pitches of the six operators to different
multiples of the basic note pitch produced the kinds of sound that could
be produced by a draw-bar organ, but with somewhat more flexibility, as
each operator could be programmed with a different amplitude
envelope.</p>
<p>The more novel sounds, however, came from algorithms in which the
operators all modified one another. Algorithms 16-18 all take their
final output from a single operator, whose pitch gets modified by the
other five operators, all interacting with one another.</p>
<p>As a result, the DX7 could produce everything from smooth, organ-like
sounds, to abrasive, ringing electronic sounds that we had never heard
before.</p>
<p>I said that the DX7 used frequency modulation ‘conceptually’. The
reality was that the tone generation was completely mathematical.
Custom-made micro-controllers did the math digitally, and fed the result
to a digital-to-analog converter. The sampling rate of the instrument as
a whole was about 60kHz, allowing for frequencies beyond the limit of
human hearing. It was this prevalence of high-frequency components, I
guess, that gave the DX7 it’s signature ‘brilliant’ sound.</p>
<p>Yamaha’s ‘operator’ technology eventually spread outside the DX7,
particularly to PC sound cards. An interesting development was the
discovery that, if the operators started with square, rather than sine,
waveforms, it didn’t take as many operators to create interesting
sounds. Yamaha went on to market two-operator and four-operator sound
chips – the OPL2 and OPL3 respectively; the latter was part of the
hugely popular SoundBlaster 16 sound card.</p>
<p>In 1984 Yamaha released the CX5M – a complete music composition
workstation that included an operator synthesis module. The CX5M was a
remarkable piece of equipment in its own right. I owned one myself in
the late 80s and, while I’d like to write an article about it for this
series, I was stoned out of my tree for the whole time I had it, and
don’t remember much about it.</p>
<p>Yamaha went on to sell 150,000 of the original DX7s – a staggering
number for a keyboard instrument: ten times as many as the Minimoog,
which was itself considered popular. The DX7 was was not cheap – in the
UK it sold for £1300 in 1983 (equivalent to about £5000 in 2024) – but
it was still a whole lot cheaper than a polyphonic analog
synthesizer.</p>
<p>The DX7, for all its success, had some oddities.</p>
<p>The instrument was praised for it’s ‘brilliant’ sound but, weirdly,
that brilliance diminished for notes at the right-hand end of the
keyboard – the highest octave could sound somewhat dull. Possibly this
was the result of careless design of the digital-to-analog converter
(DAC) but, given the care that Yamaha took with the rest of the design,
I suspect the DAC had to be tuned this way to overcome limitations
elsewhere in the processing chain.</p>
<p>For example, the DX7 only had a 12-bit DAC. It would definitely have
made the math easier and faster, if the designers had anticipated that
it only had to produce a result to 12-bit precision. 16-bit DACs were
available in 1983, because this is the format used by audio CDs. So I
guess the limitation was in the internal computation, not the choice of
DAC itself. But I don’t know for sure.</p>
<p>The real problem with the DX7, though, was programming it – which is
why almost nobody did. Partly the limitation was the user interface,
which had only a two-line display, and all the push-buttons had at least
four functions. But mostly it was almost impossible to work out how to
configure the operators to get a particular sound. The tiniest change in
the settings could have a radical effect on the output. Analog
synthesizers were never particular good at generating the sounds of real
instruments, but at least we could predict what the effect of tweaking a
control would be. With the DX7 it was a lottery.</p>
<p>What most people did – certainly what I did – was to fiddle with the
settings more-or-less at random, and then store any configuration that
sounded good. You could give the sound configuration a name, which would
have been handy, if anybody had the patience to enter a name using the
horrible membrane keypad. You could save your sound configurations to a
memory cartridge, which meant that you could use somebody else’s DX7 –
perhaps one in a recording studio – with your own settings. I certainly
remember trading DX7 cartridges with other keyboard players, back in the
day.</p>
<p>Yamaha did produce further models in the DX7 line-up. The Mark IID
was notable for having a split keyboard, so you could assign different
sounds to different parts of the keyboard. It also used a 16-bit DAC.
Whether the internal math used a higher precision, or Yamaha just
realized they’d used an inferior DAC in earlier models, I don’t know. A
potential advantage of the later models was that they had real
mechanical controls, not the nasty membrane keypad of the Mark I.
Whether they were as beer-resistant, I don’t know. There was even a
model with a floppy disk drive.</p>
<p>Sadly, none of these later models enjoyed even close to the success
of the Mark I, and the decline of the range was almost as rapid as its
rise had been. I bought my Mark I DX7 second-hand in 1990 for a few
hundred quid. I was still playing it in bands until the mid-90s, by
which time it was already a historical curiosity.</p>
<p>What killed the DX7, and other instruments like it, was the falling
cost of microprocessors and memory. Yamaha’s FM synthesis was arcane,
but it was comparatively easy to implement with the digital technology
of the early 80s. Given how successful analog synthesizers like the
Minimoog had been, it’s interesting to speculate why instrument
designers like Yamaha didn’t try to model analog sound generation
digitally. This would have been much more comprehensible to musicians
than FM synthesis. The answer, I think, is that technology was not ready
for that in the early 80s.</p>
<p>When technology did reach that point, it had become possible to do
sound synthesis using sampling techniques. The use of mathematics to
produce sounds resembling real instruments quickly became obsolete once
we could just go out and sample the real thing. Sampling is a boring,
brute-force approach to sound synthesis, but it’s a relatively
straightforward one, now we have the computing power and memory.</p>
<p>And once we have sampling technology, there’s little point using
computational techniques to make the once-novel sounds of earlier
synthesizers: we can just sample the instruments themselves.</p>
<p>There remains a niche interest in generating instrumental sounds
using mathematical modelling, but almost all digital music production
these days is based on sampling. My Kawai stage piano has about forty
different instrumental sounds, all produced from samples of real
instruments. Apart from the string sounds: I believe it creates those by
tormenting cats on Xanax.</p>
<p>With hindsight, the DX7 and its ilk look like a weird diversion in
the path of musical progress. Operator synthesis makes no logical sense.
It became popular because we had the capability to do digital sound
generation, but lacked the technology to do it in a comprehensible,
systematic way. So the DX7 enjoyed huge popularity for a few years,
before fading into obscurity.</p>
<p>If you want the sound of a DX7 from your modern keyboard on stage,
it’s easier to sample a real one, than to do the math that the DX7 did.
For studio work, you can get software DX7 emulators for popular audio
workstations. This allows us to create all-new DX7-style sounds, which
is something we can’t do using sample.</p>
<p>Oddly, though, <em>anything</em> produced by operator synthesis
sounds like Brian Eno.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Speeding up Ruby by rewriting C in Ruby (249 pts)]]></title>
            <link>https://jpcamara.com/2024/12/01/speeding-up-ruby.html</link>
            <guid>42316799</guid>
            <pubDate>Wed, 04 Dec 2024 12:31:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jpcamara.com/2024/12/01/speeding-up-ruby.html">https://jpcamara.com/2024/12/01/speeding-up-ruby.html</a>, See on <a href="https://news.ycombinator.com/item?id=42316799">Hacker News</a></p>
<div id="readability-page-1" class="page"><section><p><img src="https://cdn.uploads.micro.blog/98548/2024/yjitvsc.drawio.png" alt=""></p>
<p>There is a recent <a href="https://github.com/bddicken/languages">language comparison repo</a> which has been getting shared a lot. In it, CRuby was the third slowest option, only beating out R and Python.</p>
<p>The repo author, <a href="https://x.com/BenjDicken">@BenjDicken</a>, <a href="https://x.com/BenjDicken/status/1861072804239847914">created a fun visualization</a> of each language’s performance. Here’s one of the visualizations, which shows Ruby as the third slowest language benchmarked:</p>

<blockquote>
<p>The code for this visualization is from <a href="https://benjdd.com/languages/">https://benjdd.com/languages/</a>, with permission from <a href="https://x.com/BenjDicken/status/1862623583803253149">@BenjDicken</a></p>
</blockquote>
<p>The repository describes itself as:</p>
<blockquote>
<p>A repo for collaboratively building small benchmarks to compare languages.</p>
</blockquote>
<p>It contains two different benchmarks:</p>
<ol>
<li>“Loops”, which “Emphasizes loop, conditional, and basic math performance”</li>
<li>“Fibonacci”, which “Emphasizes function call overhead and recursion.”</li>
</ol>
<p>The loop example iterates 1 billion times, utilizing a nested loop:</p>
<div><pre tabindex="0"><code data-lang="ruby">u <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>].</span>to_i       
r <span>=</span> rand(<span>10_000</span>)                          
a <span>=</span> Array<span>.</span>new(<span>10_000</span>, <span>0</span>)                 
	
(<span>0</span><span>...</span><span>10_000</span>)<span>.</span>each <span>do</span> <span>|</span>i<span>|</span>                     
  (<span>0</span><span>...</span><span>100_000</span>)<span>.</span>each <span>do</span> <span>|</span>j<span>|</span>               
    a<span>[</span>i<span>]</span> <span>+=</span> j <span>%</span> u                     
  <span>end</span>
  a<span>[</span>i<span>]</span> <span>+=</span> r                      
<span>end</span>
	
puts a<span>[</span>r<span>]</span>
</code></pre></div><p>The Fibonacci example is a basic “naive” Fibonacci implementation<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>:</p>
<pre><code>def fibonacci(n)
  return 0 if n == 0
  return 1 if n == 1
  fibonacci(n - 1) + fibonacci(n - 2)
end

u = ARGV[0].to_i
r = 0

(1...u).each do |i|
  r += fibonacci(i)
end

puts r
</code></pre>
<p>Run on <a href="https://x.com/BenjDicken">@BenjDicken</a>’s M3 MacBook Pro, Ruby 3.3.6 takes 28 seconds to run the loop iteration example, and 12 seconds to run the Fibonacci example. For comparison, node.js takes a little over a second for both examples - it’s not a great showing for Ruby.</p>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Fibonacci</th>
      <th>Loops</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ruby</td>
      <td>12.17s</td>
      <td>28.80s</td>
    </tr>
    <tr>
      <td>node.js</td>
      <td>1.11s</td>
      <td>1.03s</td>
    </tr>
  </tbody>
</table>
<p>From this point on, I’ll use benchmarks relative to my own computer. Running the same benchmark on my M2 MacBook Air, I get 33.43 seconds for the loops and 16.33 seconds for fibonacci - even worse 🥺. Node runs a little over 1 second for fibonacci and 2 seconds for the loop example.</p>
<table>
  <thead>
    <tr>
      <th></th>
      <th>Fibonacci</th>
      <th>Loops</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Ruby</td>
      <td>16.33s</td>
      <td>33.43s</td>
    </tr>
    <tr>
      <td>node.js</td>
      <td>1.36s</td>
      <td>2.07s</td>
    </tr>
  </tbody>
</table>
<h3 id="who-cares">Who cares?</h3>
<p>In most ways, these types of benchmarks are meaningless. Python was the slowest language in the benchmark, and yet at the same time it’s the <a href="https://github.blog/news-insights/octoverse/octoverse-2024/">most used language on Github as of October 2024</a>. Ruby runs some of the <a href="https://x.com/tobi/status/1863935229620363693">largest web apps in the world</a>. I ran a <a href="https://x.com/jpcamara/status/1849984009515966958">benchmark recently of websocket performance between the Ruby Falcon web server and node.js</a>, and the Ruby results were close to the node.js results. Are you doing a billion loop iterations or using web sockets?</p>
<p>A programming language should be reasonably efficient - after that the usefulness of the language, the type of tasks you work on, and language productivity outweigh the speed at which you can run a billion iterations of a loop, or complete an intentionally inefficient implementation of a Fibonacci method.</p>
<p>That said:</p>
<ol>
<li>The programming world loves microbenchmarks 🤷‍♂️</li>
<li>Having a fast benchmark may not be valuable in practice but it has meaning for people’s interest in a language. Some would claim it means you’ll have an easier time scaling performance, but that’s arguable<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>It’s disappointing if your language of choice doesn’t perform well. It’s nice to be able to say “I use and enjoy this language, and it runs fast in all benchmarks!”</li>
</ol>
<p>In the case of this Ruby benchmark, I had a feeling that YJIT wasn’t being applied in the Ruby code, so I checked the repo. Lo and behold, the command was as follows:</p>
<pre><code>ruby ./code.rb 40
</code></pre>
<p>We know my results from earlier (33 seconds and 16 seconds). What do we get with YJIT applied?</p>
<pre><code>ruby --yjit ./code.rb 40
</code></pre>
<table>
  <thead>
    <tr>
      <th></th>
<th>Fibonacci</th>
<th>Loops</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ruby</td>
<td>2.06s</td>
<td>25.57s</td>
</tr>
</tbody>
</table>
<p>Nice! With YJIT, Fibonacci gets a massive boost - going from 16.88 seconds down to 2.06 seconds. It’s close to the speed of node.js at that point!</p>
<p>YJIT makes a more modest difference for the looping example - going from 33.43 seconds down to 25.57 seconds. Why is that?</p>
<h3 id="a-team-effort">A team effort</h3>
<p>I wasn’t alone in trying out these code samples with YJIT. On twitter, <a href="https://x.com/bsilva96">@bsilva96</a> had asked the same questions:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/screenshot-2024-12-01-at-8.38.46pm.png" alt=""></p>
<blockquote>
<p><a href="https://x.com/bsilva96/status/1861136096689606708">https://x.com/bsilva96/status/1861136096689606708</a></p>
</blockquote>
<p><a href="https://bsky.app/profile/k0kubun.com">@k0kubun</a> came through with insights into why things were slow and ways of improving the performance:</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/screenshot-2024-12-01-at-8.41.03pm.png" alt=""></p>
<blockquote>
<p><a href="https://x.com/k0kubun/status/1861149512640979260">https://x.com/k0kubun/status/1861149512640979260</a></p>
</blockquote>
<p>Let’s unpack his response. There are three parts to it:</p>
<ol>
<li><code>Range#each</code> is still written in C as of Ruby 3.4</li>
<li><code>Integer#times</code> was converted from C to Ruby in Ruby 3.3</li>
<li><code>Array#each</code> was converted from C to Ruby in Ruby 3.4</li>
</ol>
<h3 id="1-rangeeach-is-still-written-in-c-which-yjit-cant-optimize">1. <code>Range#each</code> is still written in C, which YJIT can’t optimize</h3>
<p>Looking back at our Ruby code:</p>
<pre><code>(0...10_000).each do |i|                     
  (0...100_000).each do |j|               
    a[i] += j % u                     
  end
  a[i] += r                      
end
</code></pre>
<p>It’s written as a range, and range has its own <code>each</code> implementation, which is apparently written in C. The CRuby codebase is pretty easy to navigate - let’s find that implementation 🕵️‍♂️.</p>
<p>Most core classes in Ruby have top-level C files named after them - in this case we’ve got <code>range.c</code> at the root of the project. CRuby has a pretty readable interface for exposing C functions as classes and methods - there is an <code>Init</code> function, usually at the bottom of the file. Inside that <code>Init</code> our classes, modules and methods are exposed from C to Ruby. Here are the relevant pieces of <code>Init_Range</code>:</p>
<pre><code>void
Init_Range(void)
{
  //...
  rb_cRange = rb_struct_define_without_accessor(
    "Range", rb_cObject, range_alloc,
    "begin", "end", "excl", NULL);

  rb_include_module(rb_cRange, rb_mEnumerable);
  // ...
  rb_define_method(rb_cRange, "each", range_each, 0);
</code></pre>
<p>First, we define our <code>Range</code> class using <code>rb_struct_define...</code>. We name it <code>“Range”</code>, with a super class of <code>Object</code> (<code>rb_cObject</code>), and some initialization parameters (<code>“begin”</code>, <code>“end”</code> and whether to exclude the last value, ie the <code>..</code> vs <code>...</code> range syntax).</p>
<p>Second, we include <code>Enumerable</code> using <code>rb_include_module</code>. That gives us all the great Ruby enumeration methods like <code>map</code>, <code>select</code>, <code>include?</code> and <a href="https://docs.ruby-lang.org/en/3.3/Enumerable.html">a bajillion others</a>. All you have to do is provide an <code>each</code> implementation and it handles the rest.</p>
<p>Third, we define our <code>“each”</code> method. It’s implemented by the <code>range_each</code> function in C, and takes zero explicit arguments (blocks are not considered in this count).</p>
<p><code>range_each</code> is hefty. It’s almost 100 lines long, and specializes into several versions of itself. I’ll highlight a few, collapsed all together:</p>
<pre><code>static VALUE
range_each(VALUE range)
{
  //...
  range_each_fixnum_endless(beg);
  range_each_fixnum_loop(beg, end, range);
  range_each_bignum_endless(beg);
  rb_str_upto_endless_each(beg, sym_each_i, 0);
  // and even more...
</code></pre>
<p>These C functions handle all the variations of ranges you might use in your own code:</p>
<div><pre tabindex="0"><code data-lang="ruby">(<span>0</span><span>...</span>)<span>.</span>each
(<span>0</span><span>...</span><span>100</span>)<span>.</span>each
(<span>"a"</span><span>...</span><span>"z"</span>)<span>.</span>each
<span># and on...</span>
</code></pre></div><p>Why does it matter that <code>Range#each</code> is written in C? It means YJIT can’t inspect it - optimizations stop at the function call and resume when the function call returns. C functions are fast, but YJIT can take things further by creating specializations for hot paths of code. There is a great article from Aaron Patterson called <a href="https://railsatscale.com/2023-08-29-ruby-outperforms-c/">Ruby Outperforms C</a> where you can learn more about some of those specialized optimizations.</p>
<h3 id="2-optimizing-our-loop-integertimes-was-converted-from-c-to-ruby-in-ruby-33">2. Optimizing our loop: <code>Integer#times</code> was converted from C to Ruby in Ruby 3.3</h3>
<p>The hot path (<em>where most of our CPU time is spent</em>) is <code>Range#each</code>, which is a C function. YJIT can’t optimize C functions - they’re a black box. So what can we do?</p>
<blockquote>
<p>We converted Integer#times to Ruby in 3.3</p>
</blockquote>
<p>Interesting! In Ruby 3.3, <code>Integer#times</code> was <a href="https://github.com/ruby/ruby/pull/8388">converted from a C function to a Ruby method</a>! Here’s the 3.3+ version - its pretty simple:</p>
<pre><code>def times
  #... a little C interop code
  i = 0
  while i &lt; self
    yield i
    i = i.succ
  end
  self
end
</code></pre>
<p>Very simple. It’s just a basic while loop. Most importantly, it’s all Ruby code, which means YJIT should be able to introspect and optimize it!</p>
<h3 id="an-aside-on-integersucc">An aside on <code>Integer#succ</code></h3>
<p>The slightly odd part of that code is <code>i.succ</code>. I’d never heard of <code>Integer#succ</code>, which apparently gives you the “successor” to an integer.</p>
<p><img src="https://cdn.uploads.micro.blog/98548/2024/0c8bd56f64.png" alt=""></p>
<blockquote>
<p>I’ve never seen this show, and yet it’s the first thing I thought of when I learned about this method. Thanks, advertising.</p>
</blockquote>
<p>There was a PR to improve the performance of <code>Integer#succ</code> in early 2024, which helped me understand why anyone would ever use it:</p>
<blockquote>
<p>We use Integer#succ when we rewrite loop methods in Ruby (e.g. Integer#times and Array#each) because opt_succ (i = i.succ) is faster to dispatch on the interpreter than putobject 1; opt_plus (i += 1).</p>
<p><a href="https://github.com/ruby/ruby/pull/9519">https://github.com/ruby/ruby/pull/9519</a></p>
</blockquote>
<p><code>Integer#success</code> is like a virtual machine cheat code. It takes a common operation (adding 1 to an integer) and turns it from two virtual machine operations into one. We can call <code>disasm</code> on the <code>times</code> method to see that in action:</p>
<pre><code>puts RubyVM::InstructionSequence.disasm(1.method(:times))
</code></pre>
<p>The <code>Integer#times</code> method gets broken down into a lot of Ruby VM bytecode, but we only care about a few lines:</p>
<pre><code>...
0025 getlocal_WC_0   i@0
0027 opt_succ        &lt;calldata!mid:succ, ARGS_SIMPLE&gt;[CcCr]
0029 setlocal_WC_0   i@0
...
</code></pre>
<ul>
<li><code>getlocal_WC_0</code> gets our <code>i</code> variable from the current scope. That’s the <code>i</code> in <code>i.succ</code></li>
<li><code>opt_succ</code> performs the <code>succ</code> call in our <code>i.succ</code>. It will either call the actual <code>Integer#succ</code> method, or an optimized C function for small numbers</li>
<li>In Ruby 3.4 with YJIT enabled, small numbers get optimized even further into machine code (just a note, not shown in the VM machine code)</li>
<li><code>setlocal_WC_0</code> sets the result of <code>opt_succ</code> to our local variable <code>i</code></li>
</ul>
<p>If we change from <code>i = i.succ</code> to <code>i += 1</code>, we now have two VM operations take the place of <code>opt_succ</code>:</p>
<pre><code>...
0025 getlocal_WC_0        i@0
0027 putobject_INT2FIX_1_
0028 opt_plus             &lt;calldata!mid:+, argc:1, ARGS_SIMPLE&gt;
0029 setlocal_WC_0        i@0
...
</code></pre>
<p>Everything is essentially the same as before, except now we have two steps to go through instead of one:</p>
<ul>
<li><code>putobject_INT2FIX_1_</code> pushes the integer <code>1</code> onto the virtual machine stack</li>
<li><code>opt_plus</code> is the <code>+</code> in our <code>+= 1</code>, and calls either the Ruby <code>+</code> method or an optimized C function for small numbers</li>
<li>There is probably a YJIT optimization for <code>opt_plus</code> as well</li>
</ul>
<p>If there is nothing else to learn from this code, it’s this: the kinds of optimizations you do at the VM and JIT level are <em>deep</em>. When writing general Ruby programs we typically don’t and <em>shouldn’t</em> consider the impact of one versus two <em>machine code instructions</em>. But at the JIT level, on the scale of millions and billions of operations, it matters!</p>
<h3 id="back-to-integertimes">Back to <code>Integer#times</code></h3>
<p>Let’s try running our benchmark code again, using <code>times</code>! Instead of iterating over ranges, we simply iterate for <code>10_000</code> and <code>100_000</code> <code>times</code>:</p>
<div><pre tabindex="0"><code data-lang="ruby">u <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>].</span>to_i        
r <span>=</span> rand(<span>10_000</span>)        
a <span>=</span> Array<span>.</span>new(<span>10_000</span>, <span>0</span>)
	
<span>10_000</span><span>.</span>times <span>do</span> <span>|</span>i<span>|</span>
  <span>100_000</span><span>.</span>times <span>do</span> <span>|</span>j<span>|</span>
    a<span>[</span>i<span>]</span> <span>+=</span> j <span>%</span> u
  <span>end</span>
  a<span>[</span>i<span>]</span> <span>+=</span> r
<span>end</span>
	
puts a<span>[</span>r<span>]</span>
</code></pre></div><table>
<thead>
<tr>
<th></th>
<th>Loops</th>
</tr>
</thead>
<tbody>
<tr>
<td>Range#each</td>
<td>25.57s</td>
</tr>
<tr>
<td>Integer#times</td>
<td>13.66s</td>
</tr>
</tbody>
</table>
<p>Nice! YJIT makes a much larger impact using <code>Integer#times</code>. That trims things down significantly, taking it down to 13.66 seconds on my machine. On <a href="https://bsky.app/profile/k0kubun.com">@k0kobun</a>’s machine it actually goes down to 9 seconds (and 8 seconds on Ruby 3.4).</p>
<blockquote>
<p>It’s probably Ruby 3.5’s job to make it faster than 8s though.</p>
</blockquote>
<p>We might look forward to even faster performance in Ruby 3.5. We’ll see!</p>
<h3 id="3-arrayeach-was-converted-from-c-to-ruby-in-ruby-34">3. <code>Array#each</code> was converted from C to Ruby in Ruby 3.4</h3>
<p>CRuby continues to see C code rewritten in Ruby, and in Ruby 3.4 <code>Array#each</code> was one of those changes. Here is an <a href="https://github.com/ruby/ruby/pull/6687/files">example of the first attempt at implementing it</a>:</p>
<pre><code>def each
  unless block_given?
    return to_enum(:each) { self.length }
  end
  i = 0
  while i &lt; self.length
    yield self[i]
    i = i.succ
  end
  self
end
</code></pre>
<p>Super simple and readable! And YJIT optimizable!</p>
<p>Unfortunately, due to something related to CRuby internals, it contained <a href="https://jpcamara.com/2024/06/23/your-ruby-programs.html#race-conditions">race conditions</a>. A later implementation <a href="https://github.com/ruby/ruby/pull/11955">landed in Ruby 3.4</a>.</p>
<pre><code>def each
  Primitive.attr! :inline_block, :c_trace

  unless defined?(yield)
    return Primitive.cexpr! 'SIZED_ENUMERATOR(self, 0, 0, ary_enum_length)'
  end
  _i = 0
  value = nil
  while Primitive.cexpr!(%q{ ary_fetch_next(self, LOCAL_PTR(_i), LOCAL_PTR(value)) })
    yield value
  end
  self
end
</code></pre>
<p>Unlike the first implementation, and unlike <code>Integer#times</code>, things are a bit more cryptic this time. This is definitely not pure Ruby code that anyone could be expected to write. Somehow, the <code>Primitive</code> module seems to allow evaluating C code from Ruby, and in doing so avoids the race conditions present in the pure Ruby solution<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.</p>
<p>By fetching indexes and values using C code, I think it results in a more atomic operation. I have no idea why the <code>Primitive.cexpr!</code> is used to return the enumerator, or what value <code>Primitive.attr! :inline_block</code> provides. Please comment if you have insights there!</p>
<p>I was a little loose with my earlier <code>Integer#times</code> source code as well. That actually had a bit of this <code>Primitive</code> syntax as well. The core of the method is what we looked at, and it’s all Ruby, but the start of the method contains the same <code>Primitive</code> calls for <code>:inline_block</code> and returning the enumerator:</p>
<pre><code>def times
  Primitive.attr! :inline_block
  unless defined?(yield)
    return Primitive.cexpr! 'SIZED_ENUMERATOR(self, 0, 0, int_dotimes_size)'
  end
  #...
</code></pre>
<p>Ok - it’s more cryptic than <code>Integer#times</code> was, but <code>Array#each</code> is mostly Ruby (on Ruby 3.4+). Let’s give it a try using arrays instead of ranges or <code>times</code>:</p>
<div><pre tabindex="0"><code data-lang="ruby">u <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>].</span>to_i
r <span>=</span> rand(<span>10_000</span>)
a <span>=</span> Array<span>.</span>new(<span>10_000</span>, <span>0</span>)
	
outer <span>=</span> (<span>0</span><span>...</span><span>10_000</span>)<span>.</span>to_a<span>.</span>freeze
inner <span>=</span> (<span>0</span><span>...</span><span>100_000</span>)<span>.</span>to_a<span>.</span>freeze
outer<span>.</span>each <span>do</span> <span>|</span>i<span>|</span>
  inner<span>.</span>each <span>do</span> <span>|</span>j<span>|</span>
    a<span>[</span>i<span>]</span> <span>+=</span> j <span>%</span> u
  <span>end</span>
  a<span>[</span>i<span>]</span> <span>+=</span> r
<span>end</span>
	
puts a<span>[</span>r<span>]</span>
</code></pre></div><p>Despite the embedded C code, YJIT still seems capable of making some hefty performance optimizations. It’s within the same range as <code>Integer#times</code>!</p>
<table>
<thead>
<tr>
<th></th>
<th>Loops</th>
</tr>
</thead>
<tbody>
<tr>
<td>Range#each</td>
<td>25.57s</td>
</tr>
<tr>
<td>Integer#times</td>
<td>13.66s</td>
</tr>
<tr>
<td>Array#each</td>
<td>13.96s</td>
</tr>
</tbody>
</table>
<h3 id="microbenchmarking-ruby-performance">Microbenchmarking Ruby performance</h3>
<p>I’ve forked the original language implementation repo, and created my own repository called “Ruby Microbench”. It takes all of the examples discussed, as well as several other forms of doing the iteration in Ruby: <a href="https://github.com/jpcamara/ruby_microbench">https://github.com/jpcamara/ruby_microbench</a></p>
<p>Here is the output of just running those using Ruby 3.4 with and without YJIT:</p>
<table>
<thead>
<tr>
<th></th>
<th>fibonacci</th><th>array#each</th>
<th>range#each</th>
<th>times</th>
<th>for</th>
<th>while</th>
<th>loop do</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ruby 3.4 YJIT</td>
<td>2.19s</td>
<td>14.02s</td>
<td>26.61s</td>
<td>13.12s</td>
<td>27.38s</td>
<td>37.10s</td>
<td>13.95s</td></tr>
<tr>
<td>Ruby 3.4</td>
<td>16.49s</td>
<td>34.29s</td>
<td>33.88s</td>
<td>33.18s</td>
<td>36.32s</td>
<td>37.14s</td>
<td>50.65s</td>
</tr>
</tbody>
</table>
<p>I have no idea why the <code>for</code> and <code>while</code> loop examples I wrote seem to be so slow. I’d expect them to run much faster. Maybe there’s an issue with how I wrote them - feel free to open an issue or PR if you see something wrong with my implementation. The <code>loop do</code> (taken from <a href="https://bsky.app/profile/timtilberg.bsky.social">@timtilberg</a>’s <a href="https://x.com/timtilberg/status/1861194052516864004">example</a>) runs around the same speed as <code>Integer#times</code> - although its performance is <em>awful</em> with YJIT turned off.</p>
<p>In addition to running Ruby 3.4, for fun I have it using <code>rbenv</code> to run:</p>
<ul>
<li>Ruby 3.3</li>
<li>Ruby 3.3 YJIT</li>
<li>Ruby 3.2</li>
<li>Ruby 3.2 YJIT</li>
<li>TruffleRuby 24.1</li>
<li>Ruby Artichoke</li>
<li>MRuby</li>
</ul>
<p>A few of the test runs are listed here:</p>
<table>
<thead>
<tr>
<th></th>
<th>fibonacci</th>
<th>
array#each
</th>
<th>range#each</th>
<th>times</th>
<th>for</th>
<th>while</th>
<th>loop do</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ruby 3.4 YJIT</td><td>2.19s</td>
<td>14.02s</td>
<td>26.61s</td>
<td>13.12s</td>
<td>27.38s</td>
<td>37.10s</td>
<td>13.95s</td></tr>
<tr>
<td>Ruby 3.4</td>
<td>16.49s</td>
<td>34.29s</td><td>33.88s</td>
<td>33.18s</td>
<td>36.32s</td>
<td>37.14s</td>
<td>50.65s</td>
</tr>
<tr><td>TruffleRuby 24.1</td><td>0.92s</td>
<td>0.97s</td>
<td>0.92s</td><td>2.39s</td>
<td>2.06s</td><td>3.90s</td>
<td>0.77s</td>
</tr><tr>
<td>MRuby 3.3</td>
<td>28.83s</td>
<td>144.65s</td>
<td>126.40s</td>
<td>128.22s</td>
<td>133.58s</td><td>91.55s</td>
<td>144.93s</td>
</tr>
<tr>
<td>Artichoke</td>
<td>19.71s</td>
<td>236.10s</td><td>214.55s</td>
<td>214.51s</td>
<td>215.95s</td>
<td>174.70s</td>
<td>264.67s</td>
</tr>
</tbody>
</table>
<p>Based on that, I’ve taken the original visualization and made a Ruby specific one here just for the <code>fibonacci</code> run:</p>

<h3 id="speeding-up-rangeeach">Speeding up <code>range#each</code></h3>
<p>Can we, the non <a href="https://bsky.app/profile/k0kubun.com">@k0kobun</a>’s of the world, make <code>range#each</code> faster? If I monkey patch the <code>Range</code> class with a pure-ruby implementation, things <em>do</em> get much faster! Here’s my implementation:</p>
<pre><code>class Range
  def each
    beginning = self.begin
    ending = self.end
    i = beginning
    loop do
      break if i == ending
      yield i
      i = i.succ
    end
  end
end
</code></pre>
<p>And here is the change in performance - 2 seconds slower than <code>times</code> - not bad!</p>
<table>
<thead>
<tr>
<th>				
</th>
<th>
Time spent
</th>
</tr>
</thead>
<tbody>
<tr>
<td>Range#each in C</td>
<td>25.57s</td>
</tr>
<tr>
<td>Range#each in Ruby</td>
<td>16.64s</td>
</tr>
</tbody>
</table>
<p>This is obviously over-simplified. I don’t handle all of the different cases of <code>Range</code>, and there may be nuances I am missing. Also, most of the Ruby rewritten methods I’ve seen invoke a <code>Primitive</code> class for certain operations. I’d love to learn more about when and why it’s needed.</p>
<p>But! It goes to show the power of moving things <em>out</em> of C and letting YJIT optimize our code. It can improve performance in ways that would be difficult or impossible to replicate in regular C code.</p>
<h3 id="yjit-standard-library">YJIT standard library</h3>
<p>Last year Aaron Patterson wrote an article called <a href="https://railsatscale.com/2023-08-29-ruby-outperforms-c/">Ruby Outperforms C</a>, in which he rewrote a C extension in Ruby for some GraphQL parsing. The Ruby code outperformed C thanks to YJIT optimizations.</p>
<p>This got me thinking that it would be interesting to see a kind of “YJIT standard library” emerge, where core ruby functionality run in C could be swapped out for Ruby implementations for use by people using YJIT.</p>
<p>As it turns out, this is almost exactly what the core YJIT team has been doing. In many cases they’ve completely removed C code, but more recently they’ve created a <code>with_yjit</code> block. The code will only take effect if YJIT is enabled, and otherwise the C code will run. For example, this is how<code>Array#each</code> is implemented:</p>
<pre><code>with_yjit do
  if Primitive.rb_builtin_basic_definition_p(:each)
    undef :each

    def each # :nodoc:
      # ... we examined this code earlier ...
    end
  end
end
</code></pre>
<p>As of Ruby 3.3, YJIT can be lazily initialized. Thankfully the <code>with_yjit</code> code handles this - the appropriate <code>with_yjit</code> versions of methods will be run once YJIT is enabled:</p>
<pre><code># Uses C-builtin
[1, 2, 3].each do |i|
  puts i
end

RubyVM::YJIT.enable

# Uses Ruby version, which can be YJIT optimized
[1, 2, 3].each do |i|
  puts i
end
</code></pre>
<p>This is because <code>with_yjit</code> is a YJIT “hook”, which is called the moment YJIT is enabled. After being called, it is removed from the runtime using <code>undef :with_yjit</code>.</p>
<h3 id="investigating-yjit-optimizations">Investigating YJIT optimizations</h3>
<p>We’ve looked at Ruby code. We’ve looked at C code. We’ve looked at Ruby VM bytecode. Why not take it one step deeper and look at some <em>machine code</em>? And maybe some Rust code? Hey - where are you going! Don’t walk away while I’m talking to you!</p>
<p>If you <em>haven’t</em> walked away, or skipped to the next section, let’s take a look at a small sliver of YJIT while we’re here!</p>
<p>We can see the machine code YJIT generates 😱. It’s possible by building CRuby from source with YJIT debug flags. If you’re on a Mac you can see <a href="https://jpcamara.com/2024/12/02/my-macos-setup.html">my MacOS setup for hacking on CRuby</a> or <a href="https://jpcamara.com/2024/11/27/my-docker-setup.html">my docker setup for hacking on CRuby</a> for more elaborate instructions on building Ruby. But the simplified step is when you go to <code>./configure</code> Ruby, you hand in an option of <code>--enable-yjit=dev</code>:</p>
<pre><code>./configure --enable-yjit=dev
make install
</code></pre>
<p>Let’s use our <code>Integer#times</code> example from earlier as our example Ruby code:</p>
<div><pre tabindex="0"><code data-lang="ruby">u <span>=</span> <span>ARGV</span><span>[</span><span>0</span><span>].</span>to_i
r <span>=</span> rand(<span>10_000</span>)
a <span>=</span> Array<span>.</span>new(<span>10_000</span>, <span>0</span>)
	
<span>10_000</span><span>.</span>times <span>do</span> <span>|</span>i<span>|</span>
  <span>100_000</span><span>.</span>times <span>do</span> <span>|</span>j<span>|</span>
    a<span>[</span>i<span>]</span> <span>+=</span> j <span>%</span> u
  <span>end</span>
  a<span>[</span>i<span>]</span> <span>+=</span> r
<span>end</span>
	
puts a<span>[</span>r<span>]</span>
</code></pre></div><p>Because you’ve built Ruby with YJIT in dev mode, you can hand in the <code>--yjit-dump-disasm</code> flag when running your ruby program:</p>
<pre><code>./ruby --yjit --yjit-dump-disasm test.rb 40
</code></pre>
<p>Using this, we can see the machine code created. We’ll just focus in on one tiny part - the machine code equivalent of the Ruby VM bytecode we read earlier. Here is the original VM bytecode for <code>opt_succ</code>, which is generated when you call <code>i.succ</code>, the <code>Integer#succ</code> method:</p>
<pre><code>...
0027 opt_succ        &lt;calldata!mid:succ, ARGS_SIMPLE&gt;[CcCr]
...
</code></pre>
<p>And here is the machine code YJIT generates in this scenario, on my Mac M2 arm64 architecture:</p>
<pre><code># Block: times@&lt;internal:numeric&gt;:259 
# reg_mapping: [Some(Stack(0)), None, None, None, None]
# Insn: 0027 opt_succ (stack_size: 1)
# call to Integer#succ
# guard object is fixnum
0x1096808c4: tst x1, #1
0x1096808c8: b.eq #0x109683014
0x1096808cc: nop 
0x1096808d0: nop 
0x1096808d4: nop 
0x1096808d8: nop 
0x1096808dc: nop 
# Integer#succ
0x1096808e0: adds x11, x1, #2
0x1096808e4: b.vs #0x109683048
0x1096808e8: mov x1, x11
</code></pre>
<p>To be honest, I about 25% understand this, and 75% am combining my own logic and AI to learn it 🤫. Feel free to yell at me if I get it a little wrong, I’d love to learn more. But here’s how I break this down.</p>
<pre><code># Block: times@&lt;internal:numeric&gt;:259
</code></pre>
<p>👆🏼This roughly corresponds to the line <code>i = i.succ</code> in the <code>Integer#times</code> method in <code>numeric.rb</code>. I say roughly because in my current code I see that on line 258, but maybe it shows the end of the block it’s run in since YJIT compiles “blocks” of code:</p>
<pre><code>256: while i &lt; self
257:   yield i
258:   i = i.succ
259: end

# reg_mapping: [Some(Stack(0)), None, None, None, None]
# Insn: 0027 opt_succ (stack_size: 1)
# call to Integer#succ
</code></pre>
<p>👆🏼I have no idea what <code>reg_mapping</code> means - probably mapping how it uses a CPU register? <code>Insn: 0027 opt_succ</code> looks very familiar! That’s our VM bytecode! <code>call to Integer#succ</code> is just a helpful comment added. YJIT is capable of adding comments to the machine code. We still haven’t even left the safety of the comments 😅.</p>
<pre><code># guard object is fixnum
</code></pre>
<p>👆🏼This is interesting. I can find a corresponding bit of Rust code that maps directly to this. Let’s take a look at it:</p>
<pre><code>fn jit_rb_int_succ(
  //...
  asm: &amp;mut Assembler,
  //...
) -&gt; bool {
  // Guard the receiver is fixnum
  let recv_type = asm.ctx.get_opnd_type(StackOpnd(0));
  let recv = asm.stack_pop(1);
  if recv_type != Type::Fixnum {
    asm_comment!(asm, "guard object is fixnum");
    asm.test(recv, Opnd::Imm(RUBY_FIXNUM_FLAG as i64));
         asm.jz(Target::side_exit(Counter::opt_succ_not_fixnum));
  }

  asm_comment!(asm, "Integer#succ");
  let out_val = asm.add(recv, Opnd::Imm(2)); // 2 is untagged Fixnum 1
  asm.jo(Target::side_exit(Counter::opt_succ_overflow));

  // Push the output onto the stack
  let dst = asm.stack_push(Type::Fixnum);
  asm.mov(dst, out_val);

  true
}
</code></pre>
<p>Oh nice! This is the actual YJIT Rust implementation of the <code>opt_succ</code> call. This is that optimization <a href="https://bsky.app/profile/k0kubun.com">@k0kobun</a> made to further improve <code>opt_succ</code> performance beyond the bytecode C function calls. We’re in the section that is checking if what we’re operating on is a Fixnum, which is a way small integers are stored internally in CRuby:</p>
<pre><code>if recv_type != TypeFixnum 
  asm_comment!(asm, "guard object is fixnum");
  asm.test(recv, Opnd::Imm(RUBY_FIXNUM_FLAG as i64));
  asm.jz(Target::side_exit(Counter::opt_succ_not_fixnum));
}
</code></pre>
<p>That becomes this machine code:</p>
<pre><code># guard object is fixnum
0x1096808c4: tst x1, #1
0x1096808c8: b.eq #0x109683014
</code></pre>
<p><code>asm.test</code> generates <code>tst x1, #1</code>, which according to an AI bot I asked is checking the least significant bit, which is a Fixnum “tag” that indicates this is a Fixnum. If it’s Fixnum, the result is 1 and <code>b.eq</code> is false. If it’s not a Fixnum, the result is <code>0</code> and <code>b.eq</code> is true and jumps away from this code.</p>
<pre><code>0x1096808cc: nop 
0x1096808d0: nop 
0x1096808d4: nop 
0x1096808d8: nop 
0x1096808dc: nop 
</code></pre>
<p>🤖 “NOPs for alignment/padding”. Thanks AI. I don’t know why it is needed, but at least I know what it probably is.</p>
<p>Finally, we <em>actually</em> add 1 to the number.</p>
<pre><code>asm_comment!(asm, "Integer#succ");
let out_val = asm.add(recv, Opnd::Imm(2)); // 2 is untagged Fixnum 1
asm.jo(Target::side_exit(Counter::opt_succ_overflow));

// Push the output onto the stack
let dst = asm.stack_push(Type::Fixnum);
asm.mov(dst, out_val);
</code></pre>
<p>The Rust code generates our <code>Integer#succ</code> comment. Then, to add 1, because of the “Fixnum tag” data embedded within our integer, actually means we have to add 2 using <code>adds x11, x1, #2</code> 😵‍💫. If we overflow the space available, it exits to a different code path - <code>b.vs</code> is a branch on overflow. Otherwise, it stores the result with <code>mov x1, x11</code>!</p>
<pre><code># Integer#succ
0x1096808e0: adds x11, x1, #2
0x1096808e4: b.vs #0x109683048
0x1096808e8: mov x1, x11
</code></pre>
<p>😮‍💨. That was a lot. And it seems like <em>alot</em> of working is being done, but because it’s such low level machine code it’s presumably super fast. We examined a teensy tiny portion of what YJIT is capable of generating - JITs are complicated!</p>
<p>Thanks to <a href="https://bsky.app/profile/k0kubun.com">@k0kobun</a> for providing me with the commands and pointing me at the <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT docs</a> which contain tons of additional options as well.</p>
<h3 id="the-future-of-cruby-optimizations">The future of CRuby optimizations</h3>
<p>The irony of language implementation is that you often work less in the language you’re implementing than you do in something lower-level - in Ruby’s case, that’s mostly C and some Rust.</p>
<p>With a layer like YJIT, it potentially opens up a future where more of the language becomes plain Ruby, and Ruby developer contribution is easier. Many languages have a smaller low level core, and the majority of the language is written in itself (like Java, for instance). Maybe that’s a future for CRuby, someday! Until then, keep the YJIT optimizations coming, YJIT team!</p>


<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Naive in this case meaning that there are more efficient ways to calculate fibonacci numbers in a program&nbsp;<a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>MJIT, the precursor to YJIT, made Ruby much faster on certain benchmarks. But on large realistic Rails applications it actually <a href="https://bugs.ruby-lang.org/issues/14490">made things <em>slower</em></a>&nbsp;<a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>When C code is running, it has to opt-in to releasing the GVL, so it’s more difficult for threads to corrupt or modify data mid-operation. The original Ruby version could yield the GVL at points that would invalidate the array. That’s my understanding of the situation anyways.&nbsp;<a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</section></div>]]></description>
        </item>
    </channel>
</rss>