<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Dec 2024 18:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Federal Court Says Dismantling a Phone to Install Firmware Isn't a 'Search' (161 pts)]]></title>
            <link>https://www.techdirt.com/2024/12/04/federal-court-says-dismantling-a-phone-to-install-firmware-isnt-a-search-even-if-was-done-to-facilitate-a-search/</link>
            <guid>42329005</guid>
            <pubDate>Thu, 05 Dec 2024 15:18:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2024/12/04/federal-court-says-dismantling-a-phone-to-install-firmware-isnt-a-search-even-if-was-done-to-facilitate-a-search/">https://www.techdirt.com/2024/12/04/federal-court-says-dismantling-a-phone-to-install-firmware-isnt-a-search-even-if-was-done-to-facilitate-a-search/</a>, See on <a href="https://news.ycombinator.com/item?id=42329005">Hacker News</a></p>
Couldn't get https://www.techdirt.com/2024/12/04/federal-court-says-dismantling-a-phone-to-install-firmware-isnt-a-search-even-if-was-done-to-facilitate-a-search/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Waymo announces Miami as its next ride hailing city (177 pts)]]></title>
            <link>https://waymo.com/blog/2024/12/next-stop-miami/</link>
            <guid>42328971</guid>
            <pubDate>Thu, 05 Dec 2024 15:14:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waymo.com/blog/2024/12/next-stop-miami/">https://waymo.com/blog/2024/12/next-stop-miami/</a>, See on <a href="https://news.ycombinator.com/item?id=42328971">Hacker News</a></p>
Couldn't get https://waymo.com/blog/2024/12/next-stop-miami/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Americans React to UnitedHealthcare CEO's Murder: 'My Empathy Is Out of Network' (117 pts)]]></title>
            <link>https://gizmodo.com/bitter-americans-react-to-unitedhealthcare-ceos-murder-my-empathy-is-out-of-network-2000534520</link>
            <guid>42327272</guid>
            <pubDate>Thu, 05 Dec 2024 11:52:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/bitter-americans-react-to-unitedhealthcare-ceos-murder-my-empathy-is-out-of-network-2000534520">https://gizmodo.com/bitter-americans-react-to-unitedhealthcare-ceos-murder-my-empathy-is-out-of-network-2000534520</a>, See on <a href="https://news.ycombinator.com/item?id=42327272">Hacker News</a></p>
Couldn't get https://gizmodo.com/bitter-americans-react-to-unitedhealthcare-ceos-murder-my-empathy-is-out-of-network-2000534520: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Diátaxis – A systematic approach to technical documentation authoring (401 pts)]]></title>
            <link>https://diataxis.fr/</link>
            <guid>42325011</guid>
            <pubDate>Thu, 05 Dec 2024 04:35:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diataxis.fr/">https://diataxis.fr/</a>, See on <a href="https://news.ycombinator.com/item?id=42325011">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page">
        <a href="#">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div>
          

          <p><label for="__toc">
            <p>Toggle table of contents sidebar</p>
            <i><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </p></div>
        <article role="main">
          <section id="diataxis">
<span id="id1"></span>

<p>A systematic approach to technical documentation authoring.</p>
<hr>
<p>Diátaxis is a way of thinking about and doing documentation.</p>
<p>It prescribes approaches to content, architecture and form that emerge from a systematic approach to understanding the needs of documentation users.</p>

<p>Diátaxis identifies four distinct needs, and four corresponding forms of documentation - <em>tutorials</em>, <em>how-to guides</em>, <em>technical reference</em> and <em>explanation</em>. It places them in a systematic relationship, and proposes that documentation should itself be organised around the structures of those needs.</p>
<img alt="Diátaxis" src="https://diataxis.fr/_images/diataxis.png">
<p>Diátaxis solves problems related to documentation <em>content</em> (what to write), <em>style</em> (how to write it) and <em>architecture</em> (how to organise it).</p>
<p>As well as serving the users of documentation, Diátaxis has value for documentation creators and maintainers. It is light-weight, easy to grasp and straightforward to apply. It doesn’t impose implementation constraints. It brings an active principle of quality to documentation that helps maintainers think effectively about their own work.</p>
<hr>
<section id="contents">
<h2>Contents<a href="#contents" title="Link to this heading">¶</a></h2>
<p>This website is divided into two main sections, to help apply and understand Diátaxis.</p>
<div>
<p><em>Start here.</em> These pages will help make immediate, concrete sense of the approach.</p>

<p>This section explores the theory and principles of Diátaxis more deeply, and sets forth the understanding of needs that underpin it.</p>

</div>
<hr>
<p>Diátaxis is proven in practice. Its principles have been adopted successfully in hundreds of documentation projects.</p>
<blockquote>
<div><p>At Gatsby we recently reorganized our open-source documentation, and the Diátaxis framework was our go-to resource
throughout the project. The four quadrants helped us prioritize the user’s goal for each type of documentation. By
restructuring our documentation around the Diátaxis framework, we made it easier for users to discover the
resources that they need when they need them.</p>
<p>—<a href="https://hachyderm.io/@meganesulli">Megan Sullivan</a></p>
</div></blockquote>
<blockquote>
<div><p>While redesigning the <a href="https://developers.cloudflare.com/">Cloudflare developer docs</a>, Diátaxis became our north star for information architecture. When we weren’t sure where a new piece of content should fit in, we’d consult the framework. Our documentation is now clearer than it’s ever been, both for readers and contributors.</p>
<p>—<a href="https://github.com/adamschwartz">Adam Schwartz</a></p>
</div></blockquote>

</section>
</section>

        </article>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bitcoin is over $100k (134 pts)]]></title>
            <link>https://www.tradingview.com/symbols/BTCUSD/</link>
            <guid>42324263</guid>
            <pubDate>Thu, 05 Dec 2024 02:41:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tradingview.com/symbols/BTCUSD/">https://www.tradingview.com/symbols/BTCUSD/</a>, See on <a href="https://news.ycombinator.com/item?id=42324263">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-query-type="media" data-props-id="fuP9Tl" id="tv-content" aria-label="Main content" tabindex="-1"><div><p><span><div><p><span><p>On December 5, Bitcoin reached $100,000, up from $90,000 on November 12. The U.S. experienced over $31 billion in net inflows from Bitcoin ETFs, raising its market cap to $2 trillion.</p></span><span><p>Bitcoin crossed the $100,000 mark, recording a 5.5% gain over a 24-hour period, with its current price at $101,345.</p></span><span><p>Bitcoin's price is influenced by short-term demand and long-term supply, with profit-taking affecting it. Analysts indicate that inflows from Bitcoin spot ETFs may drive the price closer to $100,000.</p></span></p></div></span><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP%3ABTCUSD"><span>Analyze the impact</span></a><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP%3ABTCUSD"><span>Analyze the impact</span></a></p></div><div data-container-name="company-info-id"><p><span>Bitcoin is the world’s most traded cryptocurrency, and represents the largest piece of the crypto market pie. It was the first digital coin and as such, remains the most famous and widely-adopted cryptocurrency in the world. It's the original gangster in whose footsteps all other coins follow. The birth of Bitcoin was the genesis of an entirely new asset class, and a huge step away from traditional, centrally controlled money. Today, many advocates believe Bitcoin will facilitate the next stage for the global financial system, although this — of course — remains to be seen.</span></p></div><div data-container-name="symbol-faq-widget-id"><div><div id="Accordion-details::Rmr:" inert=""><p>The current price of </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) is </p><!-- --><p>103,282</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p> — it has fallen </p><!-- --><p>−0.79</p><!-- --><p>% in the past 24 hours. Try placing this info into the context by checking out what coins are also <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-gainers/">gaining</a> and <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-losers/">losing</a> at the moment and seeing </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>.</p></div><div id="Accordion-details::R1mr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) trading volume in 24 hours is </p><!-- --><p>‪61.06 B‬</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p>. See how often other coins are traded in <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-most-traded/">this list</a>.</p></div><div id="Accordion-details::R26r:" inert=""><p>Bitcoin</p><!-- --><p> price has risen by </p><!-- --><p>3.53</p><!-- --><p>% over the last week, its month performance shows a </p><!-- --><p>38.46</p><!-- --><p>% increase, and as for the last year, </p><!-- --><p>Bitcoin</p><!-- --><p> has increased by </p><!-- --><p>134.27</p><!-- --><p>%. See more dynamics on </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>. <br>Keep track of coins' changes with our <a href="https://www.tradingview.com/heatmap/crypto/?color=change&amp;dataset=Crypto&amp;group=no_group&amp;size=market_cap_calc">Crypto Coins Heatmap</a>.</p></div><div id="Accordion-details::R2mr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) reached its highest price on </p><!-- --><p>Nov 22, 2024</p><!-- --><p> — it amounted to </p><!-- --><p>99,800</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p>. Find more insights on the </p><a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">BTC<!-- --> price chart</a><p>. <br>See the list of <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-gainers/">crypto gainers</a> and choose what best fits your strategy.</p></div><div id="Accordion-details::R36r:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) reached the lowest price of </p><!-- --><p>2</p><!-- -->&nbsp;<!-- --><p>USD</p><!-- --><p> on </p><!-- --><p>Oct 20, 2011</p><!-- --><p>. View more </p><!-- --><p>Bitcoin</p><!-- --><p> dynamics on the <a href="https://www.tradingview.com/chart/?symbol=BITSTAMP:BTCUSD">price chart</a>. <br>See the list of <a href="https://www.tradingview.com/markets/cryptocurrencies/prices-losers/">crypto losers</a> to find unexpected opportunities.</p></div></div><div><div id="Accordion-details::R1ar:" inert=""><p>Bitcoin</p><!-- --><p> has the limit of </p><!-- --><p>‪21.00 M‬</p><!-- --><p> coins. No matter how the currency evolves, no new coins will be released after this number is reached.</p></div><div id="Accordion-details::R1qr:" inert=""><p>The safest choice when buying </p><!-- --><p>BTC</p><!-- --><p> is to go to a well-known crypto exchange. Some of the popular names are Binance, Coinbase, Kraken. But you'll have to find a reliable broker and create an account first. You can trade </p><!-- --><p>BTC</p><!-- --><p> right from TradingView charts — just <a href="https://www.tradingview.com/brokers/">choose a broker</a> and connect to your account.</p></div><div id="Accordion-details::R2ar:" inert=""><p>Crypto markets are famous for their volatility, so one should study all the available stats before adding crypto assets to their portfolio. Very often it's technical analysis that comes in handy. We prepared <a href="https://www.tradingview.com/symbols/BTCUSD/technicals/?exchange=BITSTAMP">technical ratings</a> for </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>): today its technical analysis shows the buy signal, and according to the 1 week rating </p><!-- --><p>BTC</p><!-- --><p> shows the buy signal. And you'd better dig deeper and study 1 month rating too — it's strong buy. Find inspiration in </p><a href="https://www.tradingview.com/symbols/BTCUSD/ideas/?exchange=BITSTAMP">Bitcoin<!-- --> trading ideas</a><p> and keep track of what's moving crypto markets with our <a href="https://www.tradingview.com/markets/cryptocurrencies/news/">crypto news feed</a>.</p></div><div id="Accordion-details::R2qr:" inert=""><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) is just as reliable as any other crypto asset — this corner of the world market is highly volatile. Today, for instance, </p><!-- --><p>Bitcoin</p><!-- --><p> is estimated as </p><!-- --><p>5.66</p><!-- --><p>% volatile. The only thing it means is that you must prepare and examine all available information before making a decision. And if you're not sure about </p><!-- --><p>Bitcoin</p><!-- --><p>, you can find more inspiration in our <a href="https://www.tradingview.com/sparks/crypto/">curated watchlists</a>.</p></div><div id="Accordion-details::R3ar:" inert=""><p>You can discuss </p><!-- --><p>Bitcoin</p><!-- --><p> (</p><!-- --><p>BTC</p><!-- --><p>) with other users in our public chats, Minds or in the comments to <a href="https://www.tradingview.com/symbols/BTCUSD/ideas/?exchange=BITSTAMP">Ideas</a>.</p></div></div></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VectorChord: Store 400k Vectors for $1 in PostgreSQL (145 pts)]]></title>
            <link>https://blog.pgvecto.rs/vectorchord-store-400k-vectors-for-1-in-postgresql</link>
            <guid>42324059</guid>
            <pubDate>Thu, 05 Dec 2024 02:01:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pgvecto.rs/vectorchord-store-400k-vectors-for-1-in-postgresql">https://blog.pgvecto.rs/vectorchord-store-400k-vectors-for-1-in-postgresql</a>, See on <a href="https://news.ycombinator.com/item?id=42324059">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><p>We’re pleased to announce our new vector search extension for PostgreSQL, providing a highly cost-effective way to manage large vectors. Using <a target="_blank" href="https://github.com/tensorchord/VectorChord">VectorChord</a>, you can achieve a QPS of 131 with 0.95 precision on 100 million 768-dimensional vectors for the top 10 queries. This setup costs only $250 monthly and can be hosted on a single machine.</p>
<p>This means you can <strong>store 400k vectors for only $1</strong>, allowing you to save significantly: 6x more vectors compared to Pinecone (storage optimized instance) and 26x more than pgvector/<a href="http://pgvecto.rs/" target="_blank">pgvecto.rs</a> for the same price.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733194327962/8b0f2610-af64-4104-98db-7f94416f354e.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>In the monthly cost comparison for storing vectors, based on <a target="_blank" href="https://myscale.github.io/benchmark/#/">MyScale benchmark data</a>, the chart highlights how <a target="_blank" href="https://github.com/tensorchord/VectorChord">VectorChord</a> emerges as an affordable option, priced at just $247 for storing 100 million vectors. In contrast, Pinecone, despite its optimized storage, costs $1,600 per month, while Qdrant is priced at $4,374. pgvector/<a target="_blank" href="http://pgvecto.rs/">pgvecto.rs</a> has a considerably higher cost of $6,580.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733194599461/eb1cf21d-4eb0-4427-8b39-bbb96d125182.png?auto=compress,format&amp;format=webp" alt=""></p>


<h2 id="heading-hnsws-problem">HNSW’s problem</h2>
<p>As the successor to <a href="http://pgvecto.rs/" target="_blank">pgvecto.rs</a>, VectorChord has gained valuable insights from its predecessor. While many vector databases or extensions (including <a href="http://pgvecto.rs/" target="_blank">pgvecto.rs</a>) perform well with datasets of around 1 million, they often struggle when scaling up to larger sizes, such as from 10 million to 100 million. Traditional HNSW-based vector databases face specific challenges with larger datasets:</p>
<ul>
<li><p><strong>Long index build time:</strong> It typically takes over 2 hours to build an index for 5 million records.</p>
</li>
<li><p><strong>High memory requirements:</strong> Storing a dataset of 10 million vectors can require as much as 40GB of memory.</p>
</li>
</ul>
<h2 id="heading-vectorchords-solution-disk-friendly-ivfrabitq">VectorChord’s solution: Disk-friendly IVF+RabitQ</h2>
<p>VectorChord employs IVF (Inverted File Index) along with <a target="_blank" href="https://arxiv.org/pdf/2405.12497">RaBitQ</a>[1] quantization to provide fast, scalable, and accurate vector search capabilities. This method <strong>compresses 32-bit vectors into compact bit representations</strong>, significantly reducing computation demands. Most comparisons are conducted using these compressed vectors, while full-precision calculations are reserved for an adaptive reranking phase applied to a smaller subset, ensuring both speed and recall are preserved.</p>
<p>Many people think that IVF has a less favorable recall/speed tradeoff than HNSW and involves many configurations for optimization. However, it's a complex issue, and we’ll explain it briefly now, with a detailed post on the topic coming later.</p>
<h3 id="heading-ivf-vs-hnsw">IVF vs HNSW</h3>
<p>A significant portion of the time taken by vector search algorithms is dedicated to distance computation. To enhance speed, it's essential to minimize distance comparisons as much as possible. The original IVF struggles in this area, usually necessitating a scan of 1% to 5% of the total vectors, which is considerably higher than what HNSW requires.</p>
<p>However, RabitQ presents an innovative approach that allows for the compression of a 32-bit vector into just 1 bit. While this compression results in some loss of precision, it greatly reduces computational requirements. With the fast scan optimization, we can achieve calculations that are <strong>over 100 times faster than traditional vector distance computations</strong>.</p>
<p>You might wonder about the recall. We can rerank additional vectors to enhance the recall rate, and full precision distance computation is only necessary during the reranking phase. RaBitQ guarantees a sharp theoretical error bound and provides good empirical accuracy at the same time. This is why IVF can be faster than HNSW.</p>
<p>Here are some initial benchmark results for the GIST dataset, which consists of 1 million vectors in 960 dimensions. With equivalent recall, VectorChord's QPS could be twice that of pgvector. More details will be provided in the Benchmark section.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733288263095/3f2dcc75-a3e9-4d86-9515-7ab120f73c0c.png?auto=compress,format&amp;format=webp" alt=""></p>
<h3 id="heading-external-index-build">External Index build</h3>
<p>The original IVF method typically needs to scan 1–5% of the dataset, which can be slow. By using RaBitQ and fast scan optimization, VectorChord aims to speed up calculations by cutting down the number of full precision vectors that need to be fully compared. This approach helps create a stable and scalable vector search system that works well with the PostgreSQL storage system. As a result, users can use <strong>physical replication and other PostgreSQL features</strong> along with VectorChord.</p>
<p>Built on IVF, VectorChord <strong>allows KMeans clustering to be conducted externally</strong> (such, as on a GPU) and easily imported into the database. We performed tests to measure indexing and insert time on an AWS i4i.large instance, which has 2 vCPUs, and 16 GB of RAM. The dataset used for this test was GIST 1M. We inserted 700,000 vectors, built the index, and then added another 300,000 vectors. After warming up the system, we performed queries using a single thread. During this process, we evaluated both the index build time and the insert time. Here are the results:</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733302602422/57f0e7d6-5f48-484f-ac2a-5e999b7cd0fe.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>VectorChord takes 186 seconds to build the index by utilizing a separate machine for KMeans clustering, making it <strong>16 times faster than pgvector</strong>. Additionally, the insert time is also 14 times faster than that of pgvector. As we know, indexing is the most resource-intensive part of vector databases, requiring significant computation and increasing the demand for CPUs and memory. By utilizing a more capable machine to build the index and then importing it to a smaller machine for querying, it becomes possible to support billions of vectors on a single machine.</p>
<h2 id="heading-benchmark">Benchmark</h2>
<p>We conducted additional experiments to assess performance and costs more thoroughly using the LAION 5M and 100M datasets.</p>
<h3 id="heading-laion-5m">LAION 5M</h3>
<p>We had an experiment using the LAION 5M dataset, and the results were encouraging for Vectorchord. It consistently achieved higher queries per second (RPS) compared to other platforms. While many databases struggle to maintain a balance between speed and accuracy as recall increases, Vectorchord managed to remain efficient even at higher recall levels. This characteristic could make it a suitable option for applications needing both quick responses and precision.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733288538606/7f7ae3ef-5bda-4b3c-ace7-cf269511472f.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The experiment utilizes the <a target="_blank" href="https://myscale.github.io/benchmark/#/">Myscale single thread benchmark</a>, and we conducted it on an r6a.xlarge machine, which features 4 vCPUs, 32GB of memory, and 200GB of EBS storage. The parameters set for the experiment include an nlist of 8192, a shared buffer of 28GB, JIT disabled, and an effective I/O concurrency of 200. We ran the experiment twice without prewarming.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733288473788/e47f3d44-84fc-4dc7-900b-a6b0cbe4db92.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>The machine we used, an <code>r6a.xlarge</code>, costs around $165.56 per month, while the Zilliz Performance 4CU is priced at approximately $460 per month. Notably, we achieved comparable performance in the Top 100 on the LAION 5M dataset.</p>
<h3 id="heading-laion-100m-on-a-single-machine">LAION 100M, on a single machine</h3>
<p>Furthermore, due to its disk-friendly indexing, increasing a single machine's disk capacity can proportionally enhance the maximum number of vectors VectorChord can hold, potentially allowing for storage of 1 billion or more.</p>
<p>To assess scalability, we performed experiments on the LAION 100M dataset (768 dimensions) using an AWS <code>i4i.xlarge</code> instance, which is an economical configuration priced at $250 per month.</p>
<p><a target="_blank" href="https://instances.vantage.sh/aws/ec2/i4i.xlarge?region=us-east-1&amp;os=linux&amp;cost_duration=monthly&amp;reserved_term=Standard.noUpfront"><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733213906554/5021cb87-7337-4fe3-b29f-7b0fd5f28a08.png?auto=compress,format&amp;format=webp" alt=""></a></p>
<p>It features just 4 CPUs and 32 GB of memory, with 937 GB of SSD used to store the 100 million vectors. In this setup, we achieved a QPS of 16.2 @ recall 0.95 for the top 10 results and 4.3 @ recall 0.95 for the top 100 results with a single-thread query. Here are the impressive results:</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733215378789/abdae62d-0085-4b38-97d3-43409d3d3f7b.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>While maintaining a recall greater than 0.95, we also assessed the multi-thread QPS on this 4-vCPU machine. In this scenario, as the number of requested threads rises from 1 to 8, the QPS for vector queries can increase linearly. This indicates that VectorChord demonstrates excellent scalability.</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1733288585680/c6de7df3-facd-456c-8268-b6799586bc6c.png?auto=compress,format&amp;format=webp" alt=""></p>
<h2 id="heading-summary">Summary</h2>
<p>VectorChord is a new PostgreSQL extension designed for efficient vector search. It allows users to store 400,000 vectors for just $1, significantly cheaper than competitors. By utilizing IVF and RaBitQ quantization, VectorChord optimizes search speed and memory usage, making it suitable for large datasets.</p>
<p>We offer cloud-managed services for VectorChord at <a target="_blank" href="https://cloud.pgvecto.rs/">PGVecto.rs Cloud</a>. Our platform simplifies deployment and management, enabling you to scale your vector database solutions with ease and efficiency. If you have any questions about VectorChord, please feel free to reach out. We're here to assist you! You can either open an issue in our repository or email us at <a target="_blank" href="mailto:vectorchord-inquiry@tensorchord.ai">vectorchord-inquiry@tensorchord.ai</a>.</p>


<h2 id="heading-references">References</h2>
<p>[1] Gao, Jianyang, and Cheng Long. "RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search." <em>Proceedings of the ACM on Management of Data</em> 2.3 (2024): 1-27.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bringing K/V context quantisation to Ollama (205 pts)]]></title>
            <link>https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/</link>
            <guid>42323953</guid>
            <pubDate>Thu, 05 Dec 2024 01:40:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/">https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/</a>, See on <a href="https://news.ycombinator.com/item?id=42323953">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Explaining the concept of K/V context cache quantisation, why it matters and the journey to <a href="https://github.com/ollama/ollama/pull/6279">integrate it into Ollama</a>.</p><p><a href="https://github.com/ollama/ollama/pull/6279"><img loading="lazy" src="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/ollama-release.png" alt="Release the Ollamas"></a></p><hr><h2 id="why-kv-context-cache-quantisation-matters">Why K/V Context Cache Quantisation Matters</h2><p>The <a href="https://github.com/ollama/ollama/pull/6279">introduction</a> of K/V context cache quantisation in Ollama is significant, offering users a range of benefits:</p><ul><li>• <strong>Run Larger Models</strong>: With reduced VRAM demands, users can now run larger, more powerful models on their existing hardware.</li><li>• <strong>Expand Context Sizes</strong>: Larger context sizes allow LLMs to consider more information, leading to potentially more comprehensive and nuanced responses. For tasks like coding, where longer context windows are beneficial, K/V quantisation can be a game-changer.</li><li>• <strong>Reduce Hardware Utilisation</strong>: Freeing up memory or allowing users to run LLMs closer to the limits of their hardware.</li></ul><p>Running the K/V context cache at Q8_0 quantisation effectively halves the VRAM required for the context compared to the default F16 with minimal quality impact on the generated outputs, while Q4_0 cuts it down to just one third (at the cost of some noticeable quality reduction).</p><p><img loading="lazy" src="https://smcleod.net/2024/12/bringing-k/v-context-quantisation-to-ollama/llm-vram-components.svg" alt="What makes up a LLMs memory usage?"></p><p>Consider running a 8b parameter model with a 32K context size, the vRAM required for the context could be as follows:</p><ul><li>• <strong>F16</strong> K/V: Around <strong>6GB</strong>.</li><li>• <strong>Q8_0</strong> K/V: Around <strong>3GB</strong>, (50%~ saving).</li><li>• <strong>Q4_0</strong> K/V: Around <strong>2GB</strong>, (66%~ saving).</li></ul><p>Saving that 3GB of vRAM by using Q8_0 could be enough to either allow you to double the context size to 64K, or perhaps to run a larger parameter model (e.g. 14B instead of 8B).</p><hr><h2 id="interactive-vram-estimator">Interactive VRAM Estimator</h2><p>I’ve built an interactive VRAM estimator to help you understand the impact of K/V context cache quantisation on your VRAM usage. You can adjust the model size, context size and quantisation level to see how it affects the memory requirements.</p><hr><h2 id="enabling-kv-context-cache-quantisation-in-ollama">Enabling K/V Context Cache Quantisation in Ollama</h2><p>This is covered in the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache">Ollama FAQ</a>, but here’s a quick guide:</p><ul><li>• Build the latest version of Ollama from the main branch or download the pre-release binaries from Ollama’s <a href="https://github.com/ollama/ollama/releases">releases page</a>).</li><li>• Make sure you’re running Ollama with <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention">Flash Attention enabled</a> (<code>OLLAMA_FLASH_ATTENTION=1</code>), <em>Note: This should become the default behaviour in the near future as there’s no reason not to use it.</em></li><li>• Set the K/V cache quantisation to Q8_0 by adding <code>OLLAMA_KV_CACHE_TYPE="q8_0"</code> to the environment variables you run Ollama with.</li></ul><p>Start Ollama and Q8_0 quantisation will be used for the K/V context cache by default.</p><h3 id="implementation-limitations">Implementation Limitations</h3><p>See <a href="#what-wasnt-included-in-the-pr">what wasn’t included in the PR</a> for more information on these limitations.</p><ul><li>You cannot set the quantisation level in a model’s Modelfile (this would be really good to add back in).</li><li>You cannot set the K and V caches to different quantisation levels.</li><li>You cannot request a quantisation level via the Ollama API, or on the command line.</li></ul><hr><h2 id="understanding-kv-context-cache-quantisation">Understanding K/V Context Cache Quantisation</h2><p>K/V context quantisation is completely separate from model quantisation, which is the process of reducing the precision of the model’s weights and biases to save memory and improve performance. Instead of compressing the model itself, K/V context cache quantisation focuses on reducing the memory footprint of the context cache used during text generation.</p><blockquote><p><em>Matt Williams kindly featured the PR on his YouTube channel, which generated a lot of interest and feedback from the community.</em>
<a href="https://www.youtube.com/watch?v=RFaMiQ97EoE"><img loading="lazy" src="https://img.youtube.com/vi/RFaMiQ97EoE/0.jpg" alt="Matt William’s YouTube video on the PR"></a></p></blockquote><h3 id="kv-context-cache">K/V Context Cache</h3><p>You can think of the K/V (key-value) context as the ‘working memory’ of an LLM. It’s it needs to keep at front of mind as you interact with it. This cache can get <em>very</em> large - in the order of many gigabytes.</p><p>In simple terms, the K/V context cache acts as the memory of an LLM during text generation. It stores the essential information from the preceding text, allowing the model to maintain context and generate coherent responses.</p><h3 id="quantisation">Quantisation</h3><p>Quantisation <em>(or ‘quantization’ to our American friends)</em> can be thought of as <em>compression</em>, it works by reducing the precision of the numerical values stored within it. Think of it like rounding numbers - you lose a tiny bit of detail, but you save a lot of space.</p><p>When quantisation is applied to the K/V context cache it greatly reduces the memory requirements, allowing users to run larger models or use larger context sizes on their existing hardware.</p><p>The most commonly used quantisation levels for the K/V are Q8_0 and Q4_0, unquantised is referred to as F16 (or F32 although for inference you would not run F32).</p><h3 id="performance">Performance</h3><p>Quantisation of the K/V context cache has minimal impact on performance, with quantising the K cache slightly improving performance while quantising the V cache may have a slight negative impact. The overall performance impact is negligible, especially when weighed against the significant reductions in VRAM usage.</p><h3 id="quality">Quality</h3><p>• Q8_0 - Minimal quality impact for normal text generation models, suitable for most users to be enabled by default.
Perplexity measurements on an early implementation showed it added around 0.002~ perplexity to the model.</p><p>• Q4_0 - Some noticeable quality reduction, but still usable for those without much vRAM or working on creative tasks where quality is less critical.
In early testing, Q4_0 added around 0.206~ perplexity to the model.</p><p>As stated in the FAQ, it is not recommended to use K/V cache quantisation for embedding models as these are more sensitive to quantisation. The same may apply to vision/multi-modal models although I have not looked into this.</p><p>If you run into issues with quality however - you can simply disable K/V context cache quantisation by setting <code>OLLAMA_KV_CACHE_TYPE</code> environment variable to <code>f16</code> or not setting it at all. This is even easier if you run Ollama in a container as you can simply run two containers with different configurations (as there’s practically no overhead to running multiple containers).</p><p>Note that the ability to set the K/V cache quantisation level in a model’s Modelfile was <a href="#what-wasnt-included-in-the-pr">removed from the PR</a>, but I hope that Ollama will reconsider this in the future.</p><hr><h2 id="compatibility">Compatibility</h2><p>K/V context cache quantisation requires <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-enable-flash-attention">Flash Attention</a> to be enabled. Enabling Flash Attention has no negative impacts and is something I expect to become the default behaviour in the near future with Ollama.</p><p>While practically all modern models support Flash Attention, if a model is loaded that - or if your hardware doesn’t support Flash Attention, Ollama will automatically fall back to the default F16 quantisation. You’ll see a warning in the logs if this happens.</p><p>Supported Hardware:</p><ul><li>• Apple Silicon (Metal): Works on Apple Silicon devices.</li><li>• NVIDIA GPUs: Works on all NVIDIA GPUs with CUDA support, Pascal and newer.</li><li>• AMD: Works on most AMD GPUs with ROCm support, although ROCm in general is not as well supported as CUDA or Metal and performance may vary.</li></ul><hr><h2 id="the-journey-to-integration">The Journey to Integration</h2><p>The journey to integrate K/V context cache quantisation into Ollama took around 5 months.</p><p>The hard work was done up front by <a href="https://github.com/ggerganov/">ggerganov</a> in the underlying <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, which Ollama uses as it’s primary inference engine.</p><p>My PR integrated that functionality into Ollama which involved not just supporting the required configuration, but implementing memory estimations for layer placement, error and condition handling, ensuring compatibility with the existing codebase and a lot of testing.</p><h3 id="successes">Successes</h3><ul><li>• It’s merged!</li><li>• Extensive testing and feedback from the community.</li><li>• Once the PR gained traction with the Ollama team, <a href="https://github.com/jmorganca">jmorganca</a> and <a href="https://github.com/jessegross">jessegross</a> were both incredibly helpful in providing feedback and guidance, especially as I am not a Golang developer.</li><li>• The PR became so popular I knew of <em>many</em> people running Ollama from the feature branch, this is not something I wanted to see in the long term, but it was a good sign that people were interested in the feature.</li><li>• I have been building Ollama successfully at least twice, often much more every day for 5 months without major issues.</li></ul><h3 id="challenges">Challenges</h3><ul><li>• Explaining the concept and benefits of K/V context cache quantisation to the community.</li><li>• Addressing merge conflicts due to updates in the Ollama main branch as time passed.</li><li>• Adapting to the new CGO server implementation and refactoring the code to accommodate new runners.</li><li>• Ensuring compatibility with the existing codebase as it changed over time.</li><li>• Finding the right balance between allowing users to configure Ollama to meet their needs while maintaining simplicity and ease of use.</li><li>• The noise in the PR from folks either piling on or trying to help, which made it hard to keep track of the actual changes.</li><li>• Github’s PR interface which can be a bit clunky when dealing with large PRs.</li><li>• Daily battles with my (International / British English) spell checker to keep the Americanised spelling of words consistent with the rest of the codebase.</li></ul><p><em>It took 5 months, but we got there in the end.</em></p><hr><h2 id="definitions">Definitions</h2><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>LLM</td><td>Large Language Model, a type of AI model capable of understanding and generating human-like text.</td></tr><tr><td>vRAM</td><td>Video RAM, the memory used by your graphics card. LLMs require significant vRAM, especially for larger models and context sizes.</td></tr><tr><td>Context Size</td><td>The amount of text the LLM can “remember” and consider when generating a response including both the user’s inputs and the models own outputs. Larger context sizes allow for more nuanced and relevant output.</td></tr><tr><td>Quantisation</td><td>A technique for reducing the precision of numerical values, resulting in smaller data sizes.</td></tr><tr><td>Q8_0 &amp; Q4_0</td><td>Different levels of quantisation, with Q8_0 halving the VRAM usage of the context and Q4_0 reducing it to one third compared to F16 (unquantised).</td></tr><tr><td>llama.cpp</td><td>The primary underlying inference engine used by Ollama.</td></tr><tr><td>Flash Attention</td><td>A technique used to reduce the memory requirements of LLMs by only <a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention">attending</a> to a subset of the context at a time.</td></tr><tr><td>ROCm</td><td>The AMD Radeon Open Compute platform, an open-source platform for GPU computing</td></tr><tr><td>CUDA</td><td>A parallel computing platform and application programming interface model created by Nvidia</td></tr><tr><td>Metal</td><td>A low-level, low-overhead hardware-accelerated graphics and compute application programming interface developed by Apple.</td></tr></tbody></table><hr><h2 id="what-wasnt-included-in-the-pr">What wasn’t included in the PR</h2><p>Originally I had several features which in the PR that were not included in the final version as Ollama wanted to minimise the configuration exposed to users and not introduce API changes.</p><ul><li>• All the K/V quantisation types supported by llama.cpp (Q4_1, IQ4_NL, Q5_0, Q5_1).</li><li>• Ability to set the quantisation level in a models Modelfile (I’m hoping this will be added back in the future).</li><li>• API parameters that allowed setting the quantisation level when making requests.</li><li>• CMD line parameters that allowed setting the quantisation level when running Ollama.</li></ul><p>Additionally the ability to set different quantisation levels for the K and V caches was not included, this might be nice to add back in the future, as to quote <a href="https://github.com/JohannesGaessler">JohannesGaessler</a> [<a href="https://github.com/ggerganov/llama.cpp/pull/7412#issuecomment-2120427347">May 2024</a>] while measuring the quality impact of K/V context cache quantisation using an older implementation:</p><blockquote><p><em>• The K cache seems to be much more sensitive to quantization than the V cache. However, the weights seem to still be the most sensitive.</em></p><p><em>• Using q4_0 for the V cache and FP16 for everything else is more precise than using q6_K with FP16 KV cache.</em></p><p><em>• A 6.5 bit per value KV cache with q8_0 for the K cache and q4_0 for the V cache also seems to be more precise than q6_K weights. There seems to be no significant quality loss from using q8_0 instead of FP16 for the KV cache.</em></p></blockquote><hr><h2 id="reporting-issues">Reporting Issues</h2><p>If you find a bug with K/V context cache quantisation it could be either in Ollama or, perhaps more likely - in the underlying llama.cpp project.</p><p>Remember to test using the latest Ollama release or main branch build and to test with the feature disabled to ensure it’s actually related to K/V context cache quantisation.</p><p>It’s likely to be Ollama if it’s related to:</p><ul><li>• Enabling/disabling the feature.</li><li>• Memory estimations (e.g. how many layers are offloaded to each GPU).</li></ul><p>It’s more likely to a bug in llama.cpp if it’s related to:</p><ul><li>• Performance.</li><li>• ROCm support.</li><li>• Model compatibility.</li><li>• Quality issues.</li></ul><p>When logging a bug:</p><ul><li>• You should first <a href="https://github.com/ggerganov/llama.cpp/issues?q=sort%3Aupdated-desc+is%3Aissue+k%2Fv+quantization">search for existing issues in the llama.cpp project</a>.</li><li>• Reach out to the community to see if others have experienced the issue.<ul><li>• Ollama has a <a href="https://discord.gg/ollama">Discord server</a> where you can discuss issues, although be mindful that <strong>Discord is an information black hole and is not well suited to knowledge discovery or issue tracking</strong>.</li><li>• If you find an issue do not add a comment such as “+1” or “I have this issue too” - instead use an emoji reaction to indicate your support and only comment if you have valuable information to add.</li></ul></li></ul><p>Finally, if you can’t find an existing issue, you can create an issue (in the relevant project), ensuring you include your hardware configuration, software versions, environmental settings, the model you’re using, the context size, the quantisation level and any other relevant information.</p><div><p>Remember: You’re <em>logging a bug</em> to a free and open source project, not requesting <em>support</em> from a paid service.</p><p>Be patient, respectful and provide as much information as you can to help the developers diagnose and fix the issue if they have the time and resources to do so.</p></div><hr><h2 id="further-reading">Further Reading</h2><ul><li>• <a href="https://github.com/ollama/ollama/pull/6279">The PR to add K/V context cache quantisation to Ollama.</a></li><li>• <a href="https://www.youtube.com/watch?v=RFaMiQ97EoE">Matt William’s YouTube video on the PR</a></li><li>• <a href="https://smcleod.net/2024/07/understanding-ai/llm-quantisation-through-interactive-visualisations/">Understanding Quantisation</a></li><li>• Ollama<ul><li>• <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-set-the-quantization-type-for-the-kv-cache">Ollama FAQ</a></li><li>• <a href="https://ollama.com/blog">Ollama Blog</a></li><li>• <a href="https://github.com/ollama/ollama/releases">Ollama Releases</a></li></ul></li><li>• <a href="https://huggingface.co/blog/kv-cache-quantization">HuggingFace’s blog post on K/V cache quantisation</a>, which provides a more technical deep dive into the topic in the context of Transformers</li><li>• Related llama.cpp PRs and performance measurements (note: these are now quite old and things have likely improved since):<ul><li>• <a href="https://github.com/ggerganov/llama.cpp/pull/7412#issuecomment-2120427347">ggerganov/llama.cpp#7412</a></li><li>• <a href="https://github.com/ggerganov/llama.cpp/pull/7527#issuecomment-2132341565">ggerganov/llama.cpp#7527</a></li></ul></li></ul><hr><p>Discuss this post on:</p><ul><li>• <a href="https://news.ycombinator.com/item?id=42323953">HackerNews</a></li><li>• <a href="https://x.com/ollama/status/1864487185443115378">Twitter</a></li><li>• <a href="https://www.linkedin.com/feed/update/urn:li:activity:7270248388057542656/">LinkedIn</a></li><li>• <a href="https://aus.social/@s_mcleod/113597789410718453">Mastodon</a></li><li>• <a href="https://www.reddit.com/r/LocalLLaMA/comments/1h62u1p/ollama_has_merged_in_kv_cache_quantisation/">Reddit</a></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HowStuffWorks founder Marshall Brain sent final email before sudden death (121 pts)]]></title>
            <link>https://arstechnica.com/ai/2024/12/web-pioneer-marshall-brain-dies-suddenly-at-63-amid-ethics-battle/</link>
            <guid>42323599</guid>
            <pubDate>Thu, 05 Dec 2024 00:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/ai/2024/12/web-pioneer-marshall-brain-dies-suddenly-at-63-amid-ethics-battle/">https://arstechnica.com/ai/2024/12/web-pioneer-marshall-brain-dies-suddenly-at-63-amid-ethics-battle/</a>, See on <a href="https://news.ycombinator.com/item?id=42323599">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>Brandon Kashani, a former student of Brain's and startup mentor at NC State's entrepreneurship clinic, recalled to the Technician about how he met with Brain on November 15. "He felt like his reputation was tarnished, like everything he worked for was ruined, and the root of all that was he didn't get any support from the University," Kashani <a href="https://www.technicianonline.com/news/popular-nc-state-professor-marshall-brain-dies-alleges-retaliation-for-ethics-complaints/article_152e5c80-ac2e-11ef-8b3f-036ac3c8d9bf.html">told</a> the newspaper.</p>
<p>In his email, Brain wrote that the school's head of the Department of Mechanical and Aerospace Engineering later informed him the department would stop recommending students for Brain's Engineering Entrepreneurs Program. According to Brain's account, this led to disciplinary action against Brain for "unacceptable behavior."</p>
<p>"My career has been destroyed by multiple administrators at NCSU who united together and completely ignored the EthicsPoint System and its promises to employees," Brain wrote. "I did what the University told me to do, and then these administrators ruined my life for it."</p>
<h2>Unanswered questions remain</h2>
<p>In recent years, Brain <a href="https://ece.ncsu.edu/people/mdbrain/">directed</a> NC State's <a href="https://entrepreneurship.ncsu.edu/engineering-entrepreneurs-program/">Engineering Entrepreneurs Program</a>, where he mentored students and supported innovation in Research Triangle Park.</p>
<p>So far, Brain's death on campus has come as a shock to students and colleagues. Dror Baron, an NCSU professor of Electrical and Computer Engineering, <a href="https://x.com/BaronDror/status/1861985286873375165">wrote</a> on X, "A professor I know died following various investigations. I know the people mentioned here, and call for a transparent and independent investigation."</p>
<p>So far, that investigation has not been forthcoming. University spokesperson Mick Kulikowski declined to comment to The Technician about Brain's death or the allegations. To date, the university has not issued a public statement about Brain's death.</p>
<p>Barry and Kashani expressed disappointment in the university's lack of public response. "It's been six days now," Kashani said at the time to the school newspaper. "There hasn't been any acknowledgment of mistakes that were made, systems that failed, no resignations, not even a call to celebrate Marshall's achievements."</p>
<p>Brain’s friends and family plan to celebrate his achievements. His wife, Leigh Ann, their four children—David, Irena, Johnny, and Ian—and family dog Summer survive him. The family will host a <a href="https://www.dignitymemorial.com/obituaries/cary-nc/marshall-brain-12105251">Celebration of Life</a> on December 8, 2024, at Brown-Wynne Funeral Home in Cary, North Carolina.</p>
<p><em>If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number, 1-800-273-TALK (8255), which will put you in touch with a local crisis center.</em></p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Oracle files notice of appearance for JavaScript trademark [pdf] (102 pts)]]></title>
            <link>https://deno.com/blog/deno-v-oracle/20241204-notice-of-appearance.pdf</link>
            <guid>42323158</guid>
            <pubDate>Wed, 04 Dec 2024 23:24:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deno.com/blog/deno-v-oracle/20241204-notice-of-appearance.pdf">https://deno.com/blog/deno-v-oracle/20241204-notice-of-appearance.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=42323158">Hacker News</a></p>
Couldn't get https://deno.com/blog/deno-v-oracle/20241204-notice-of-appearance.pdf: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Native dual-range input (243 pts)]]></title>
            <link>https://muffinman.io/blog/native-dual-range-input/</link>
            <guid>42320516</guid>
            <pubDate>Wed, 04 Dec 2024 18:39:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://muffinman.io/blog/native-dual-range-input/">https://muffinman.io/blog/native-dual-range-input/</a>, See on <a href="https://news.ycombinator.com/item?id=42320516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page"><p><a href="#content">Jump to content</a></p><section id="content" role="main" tabindex="-1"><svg viewBox="0 0 1000 40"><path d="M 0.25 -5 L 0,0 L 500 40 L 1000,0.25 L 1000,-5 z"></path><path d="M-2000 -160 L 500 40 L 3000,-160 L 3000 42 L -2000 42"></path><path d="M-2000 -160 L 500 40 L 3000,-160" fill="none" vector-effect="non-scaling-stroke"></path></svg><div><p>I just released <a href="https://github.com/stanko/dual-range-input">@stanko/dual-range-input</a> - a native dual-range input. Here is how it looks with the default styles:</p><p>The "native" part is somewhat open for discussion. I call it native because the library uses two native HTML range inputs. This means that all of the native interactions and accessibility features are preserved.</p><p>Native inputs allow us not to reinvent the wheel. There is about <a href="https://cdn.jsdelivr.net/npm/@stanko/dual-range-input/dist/index.js">fifty lines of JavaScript</a> to synchronize the inputs, along with some CSS to ensure everything looks polished.</p><p>In my book, that is <em>native enough</em>.</p><h2 id="why">Why<a aria-label="Anchor link for: why" title="Anchor link for: why" href="#why"> </a></h2><p>When I create <a href="https://muffinman.io/art">my generative drawings</a>, I use a tool I built myself. This tool includes a UI for tweaking parameters, and I often have minimum and maximum sliders for certain parameters. I thought it would be nice to have a dual-range slider for these. However, most existing solutions rely heavily on JavaScript and reimplement dragging and accessibility features.</p><p>So, I set my own set of requirements:</p><ul><li>It should use native HTML range inputs.</li><li>When you click on the track, the closer of the two thumbs should jump to that value.</li></ul><p>Hopefully, after you read these two requirements, my solution will make sense.</p><h2 id="how-it-works">How it works<a aria-label="Anchor link for: how-it-works" title="Anchor link for: how-it-works" href="#how-it-works"> </a></h2><p>There are two inputs placed next to each other. When either of the inputs is changed, the library calculates a midpoint between the two selected values. Then the <code>min</code> and <code>max</code> attributes are set to the midpoint, and the width of both inputs is updated to match.</p><p>Here is an unstyled example, which will hopefully illustrate this well:</p><p>Even like this, it works reasonably well. We'll style it later to make it look nicer.</p><h3 id="resizing-the-inputs">Resizing the inputs<a aria-label="Anchor link for: resizing-the-inputs" title="Anchor link for: resizing-the-inputs" href="#resizing-the-inputs"> </a></h3><p>There's a small trick involved in calculating input widths. This is because the range input's track is actually shorter than the input's total width. All browsers leave enough space on the sides so the thumb doesn't stick out.</p><p>Here is a screenshot from Firefox (other browsers work similarly), where you can see that the track is shorter than the width. I've emphasized the space the browser leaves for the thumb.</p><p><img alt="Screenshot showing that the range input's track is shorter than its total width" src="https://muffinman.io/img/dual-range-input/thumb-width.png"></p><p>If we take an example where the inputs need to be in a 1:3 ratio, simply setting their widths to 25% and 75% isn't enough. We also need to account for the thumb width. Instead of calculating the exact ratio, I simplified the math by adding the thumb width to each input's width:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span>input</span><span><span><span>:</span>first-child</span></span> </span><span>{</span>
</span><span> <span><span>width</span></span><span>:</span><span> <span><span>calc</span><span><span>(</span><span>25<span>%</span></span> <span>+</span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-thumb-width</span></span></span><span><span>)</span></span></span></span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span><span>
</span><span><span><span>input</span><span><span><span>:</span>last-child</span></span> </span><span>{</span>
</span><span> <span><span>width</span></span><span>:</span><span> <span><span>calc</span><span><span>(</span><span>75<span>%</span></span> <span>+</span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-thumb-width</span></span></span><span><span>)</span></span></span></span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>If you thought, <em>Wait, this adds up to more than 100%</em>, you'd be 100% right. That's why I applied a small trick: I added padding to the inputs' wrapper to accommodate the extra width for the thumbs.</p><p><img alt="Screenshot showing padding on the wrapper to accommodate extra width" src="https://muffinman.io/img/dual-range-input/padding.png"></p><p>This makes the math simpler while keeping the input sizing correct. It took me forever to explain this properly, and I'm still not sure if I succeeded.</p><h3 id="move-the-thumb-closer-to-the-click">Move the thumb closer to the click<a aria-label="Anchor link for: move-the-thumb-closer-to-the-click" title="Anchor link for: move-the-thumb-closer-to-the-click" href="#move-the-thumb-closer-to-the-click"> </a></h3><p>Because the inputs are resized to meet at the midpoint, whenever you click between the thumbs, the one closer to the click will move to that value.</p><p> and the midpoint will be easier to see.</p><p>If there's an odd number of steps between the thumbs, the last-used input is favored. Try it out with debug mode on, and you'll see what I mean.</p><p>With that, both requirements are satisfied. The only thing left is to style it properly.</p><h2 id="styling">Styling<a aria-label="Anchor link for: styling" title="Anchor link for: styling" href="#styling"> </a></h2><p>All browsers allow us to style range inputs using CSS. That made styling of the tracks and thumbs pretty straightforward. I just ensured that the tracks didn't have a border radius in the middle where they connect.</p><h3 id="theming">Theming<a aria-label="Anchor link for: theming" title="Anchor link for: theming" href="#theming"> </a></h3><p>I exposed several variables to make theming easier. Here's the complete list with their default values:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span><span>.</span></span><span>dual-range-input</span> </span><span>{</span>
</span><span>  <span><span><span>--</span><span>dri-height</span></span></span><span>:</span><span> <span>1<span>.</span>5<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-thumb-width</span></span></span><span>:</span><span> <span>1<span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-height</span></span></span><span>:</span><span> <span>1<span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-thumb-color</span></span></span><span>:</span><span> <span><span>#</span>ddd</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-hover-color</span></span></span><span>:</span><span> <span><span>#</span>a8d5ff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-active-color</span></span></span><span>:</span><span> <span><span>#</span>4eaaff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-border-color</span></span></span><span>:</span><span> <span><span>rgba</span><span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0<span>.</span>1</span></span><span><span>)</span></span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-thumb-border-radius</span></span></span><span>:</span><span> <span>1<span>rem</span></span></span><span>;</span>
</span><span>
</span><span>  <span><span><span>--</span><span>dri-track-height</span></span></span><span>:</span><span> <span>0</span><span><span>.</span>25<span>rem</span></span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-color</span></span></span><span>:</span><span> <span><span>#</span>ccc</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-filled-color</span></span></span><span>:</span><span> <span><span>#</span>0084ff</span></span><span>;</span>
</span><span>  <span><span><span>--</span><span>dri-track-border-radius</span></span></span><span>:</span><span> <span>1<span>rem</span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>To create your own theme, simply override these variables.</p><h3 id="gradients">Gradients<a aria-label="Anchor link for: gradients" title="Anchor link for: gradients" href="#gradients"> </a></h3><p>One thing I thought was cool is how I used CSS gradients to paint the selected range in both inputs. I set the gradients to use the <code>--dri-gradient-position</code> variable, then updated that variable in the code along with the widths.</p><p>Here's how the CSS looks for one of the inputs:</p><pre data-lang="scss"><code data-lang="scss"><span><span><span>input</span><span><span><span>:</span>first-child</span></span><span><span>::</span><span>-moz-</span>range-track</span> </span><span>{</span>
</span><span>  <span><span>background-image</span></span><span>:</span><span> <span><span>linear-gradient</span><span><span>(</span>
</span></span></span></span><span><span><span><span>    <span>to</span> <span>right</span><span>,</span>
</span></span></span></span><span><span><span><span>    <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-track-color</span></span></span><span><span>)</span></span></span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-gradient-position</span></span></span><span><span>)</span></span></span><span>,</span>
</span></span></span></span><span><span><span><span>    <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-track-filled-color</span></span></span><span><span>)</span></span></span> <span><span>var</span><span><span>(</span><span><span>--</span><span>dri-gradient-position</span></span></span><span><span>)</span></span></span>
</span></span></span></span><span><span><span><span>  </span><span><span>)</span></span></span></span><span>;</span>
</span><span><span>}</span>
</span></code></pre><p>Again,  and the semi-transparent thumbs will make the gradients easier to see.</p><h2 id="conclusion">Conclusion<a aria-label="Anchor link for: conclusion" title="Anchor link for: conclusion" href="#conclusion"> </a></h2><p>I had to write this post as <del>a brain dump</del> a way to consolidate my thoughts, so I hope it's not too convoluted.</p><p>Thank you for following along, and I hope this inspires you to try it out and consider using more native elements before opting for custom libraries.</p></div></section></div></div>]]></description>
        </item>
    </channel>
</rss>