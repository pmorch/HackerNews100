<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 18 Nov 2024 04:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Reverse Engineering iOS 18 Inactivity Reboot (208 pts)]]></title>
            <link>https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html</link>
            <guid>42167633</guid>
            <pubDate>Sun, 17 Nov 2024 21:50:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html">https://naehrdine.blogspot.com/2024/11/reverse-engineering-ios-18-inactivity.html</a>, See on <a href="https://news.ycombinator.com/item?id=42167633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-3312850032502776883">
<p>iOS 18 introduced a new inactivity reboot security feature. What does it protect from and how does it work? This blog post covers all the details down to a kernel extension and the Secure Enclave Processor.</p><h2>Security Before First Unlock / After First Unlock</h2><p>Did you know that entering your passcode for the first time after your phone starts is something very different then entering it later on to unlock your phone?</p><p>When initially entering your passcode, this unlocks a key store in the Secure Enclave Processor (SEP) that encrypts your data on an iPhone.</p><p>The state before entering your passcode for the first time is also called <b>Before First Unlock</b> (BFU). Due to the encrypted user data, your iPhone behaves slightly differently to later unlocks. You'll see that Face ID and Touch ID won't work and that the passcode is required. But there's more subtle things you might notice: Since Wi-Fi passwords are encrypted, your iPhone won't connect to Wi-Fi networks. If your SIM is not PIN-protected, your iPhone will still connect to cellular networks. That means, technically, you can still receive phone calls. Yet, if you receive a call, even if that number is in your contacts, the contact name won't be shown, as the contacts haven't been decrypted yet. Similarly, when you receive notifications about new messages, you'll see that you got messages, but you won't see any message previews. You can easily try this yourself!</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEihQqOut8zLHRxuz_g8ornTF1A-X69IoWpX8lZPnZBisvc1t80zBRGebsCj4x4Vz_6C_i_ImG5AszaaQ-rXnFEJeB1-Dfaj4bSBwajxyKLYoKQNdJ8dZZtaqhexYUc1rZL7w_6yXkMj5APTe30fHOMRlg3da55UCxSZhmTFRLWmBEh_iWVgPSyKuVCEQ_48"><img alt="" data-original-height="3936" data-original-width="7142" height="352" src="https://blogger.googleusercontent.com/img/a/AVvXsEihQqOut8zLHRxuz_g8ornTF1A-X69IoWpX8lZPnZBisvc1t80zBRGebsCj4x4Vz_6C_i_ImG5AszaaQ-rXnFEJeB1-Dfaj4bSBwajxyKLYoKQNdJ8dZZtaqhexYUc1rZL7w_6yXkMj5APTe30fHOMRlg3da55UCxSZhmTFRLWmBEh_iWVgPSyKuVCEQ_48=w640-h352" width="640"></a></p><p>In the <b>After First Unlock</b> (AFU) state, user data is decrypted. You can imagine this like a key safe that is kept open while iOS is running. Even when you see a lock screen, certain keys remain available to the operating system. This way, you stay connected to Wi-Fi networks and receive message notification previews, even when your iPhone is locked.</p><p>While it's more convenient, the AFU state is more susceptible to attacks. An attacker who can somehow bypass the lock screen can get access to decrypted data on the iPhone. To bypass the lock screen, an attacker does not necessarily need to know the passcode. Security vulnerabilities within iOS can allow attackers to get code execution and extract from an iPhone, even while it appears to be "locked".</p><p>Attackers with physical access to an iPhone have more security vulnerabilities to choose from. The attack surface is larger, as such attackers can exploit vulnerabilities in the USB stack or within wireless protocols, such as Wi-Fi, Bluetooth, or cellular, or even more invasive hardware attacks that involve opening the device. This larger attack surface tends to make exploits for these vulnerabilities cheaper on the gray market, as there's potentially more supply. Another factor that makes attacks cheaper is time – vulnerabilities that are publicly known by the vendor and patched in more recent software versions won't unlock new iPhones, but can unlock iPhones that were kept in AFU state for a long time that didn't get any software updates.</p><h2>Rumors about Rebooting iPhones</h2><p>In law enforcement scenarios, a lot of the forensically relevant data is available in the AFU state. Law enforcement takes advantage of this and often keeps seized iPhones powered on, but isolated from the Internet, until they can extract data. This time might be necessary to wait for an exploit to be available or for legal reasons, such as getting a warrant.</p><p>However, thieves and other criminals are also interested in getting this kind of access after stealing a device. It gives them access to bank accounts and other valuable information, by far exceeding what the iPhone itself would be worth, or which might be used for blackmail. People reuse their passwords often, and getting access to the iCloud account may allow a thief to reset activation lock for the device, increasing the resale value.</p><p>A recent&nbsp;<a href="https://www.404media.co/police-freak-out-at-iphones-mysteriously-rebooting-themselves-locking-cops-out/">news article by 404 media</a>&nbsp;(while paywalled, the most important information is also contained in the related&nbsp;<a href="https://x.com/josephfcox/status/1854615490087551327">Tweet</a>)&nbsp;reported on a law enforcement document about suspicious iPhone reboots. This document makes two interesting claims:</p><ol><li>iPhones on iOS 18 will reboot, even when completely isolated from wireless networks.</li><li>iPhones on iOS 18 will tell other iPhones on lower iOS versions to reboot – wirelessly!</li></ol><p>Especially the second claim would be huge if true. If anyone figured out how this works, they could build a large TV-Be-Gone for iPhones, forcing reboots over the air on hundreds of iPhones simultaneously. Would Apple really build such a feature into an iPhone?</p><p>Knowing a thing or two about the Apple wireless ecosystem, my interest was piqued, and I had to go down the rabbit hole!</p><h2>Discovery of Inactivity Reboot</h2><p>When Apple adds new features, they usually don't hide this very well. Apple software contains a lot of debug strings, which hint at new functionality. Blacktop maintains a <a href="https://github.com/blacktop/ipsw-diffs">git repository</a> of strings found in iOS, which keeps a nice version history. I decided to do the most low-effort thing I could think of: just search for "reboot".</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEi4XBZbJShDJ9GPps2OujHTkV_KZn-KcijaJX34M5GsyKk_TMRsUrYkxkft2LiAMa8V62VKXagqP5Q2lQntBlRo9lobsf-7qo39M-DzpWwGgfOioKq8LQs8xAnKIRNrC0b9RlA4FZWYQg4Q1yvcILbr_Lp_Nm0ULdRG9l13_sSB1R5JIP7eYJzYwy9QAwU2"><img alt="" data-original-height="1544" data-original-width="3000" height="330" src="https://blogger.googleusercontent.com/img/a/AVvXsEi4XBZbJShDJ9GPps2OujHTkV_KZn-KcijaJX34M5GsyKk_TMRsUrYkxkft2LiAMa8V62VKXagqP5Q2lQntBlRo9lobsf-7qo39M-DzpWwGgfOioKq8LQs8xAnKIRNrC0b9RlA4FZWYQg4Q1yvcILbr_Lp_Nm0ULdRG9l13_sSB1R5JIP7eYJzYwy9QAwU2=w640-h330" width="640"></a></p><br></div><p>Bingo, that third hit looks good: "inactivity_reboot". The fact that it's in <span>keybagd</span> is interesting: this daemon is related to the key store that is unlocked on the first unlock.</p><p>A second search for only inactivity reboot shows the string starts occurring in iOS 18.1 and iOS 18.2. In iOS 18.2, the string changed from "inactivity_reboot" to "inactivity_reboot_enabled", hinting towards more potential changes in the latest iOS 18.2 betas.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgBjM82vk9aCJNzlk-9gbRKwc9y83Z9g9WZsPDUQXxB4oYXsuA-QntGrp4pMalJFUPFqETW0fxy3IfCrlziX946r_ymJ8G0a-mtIkFEzp1DoXPRcq9RUi1KOOtjgK8XfRJJcghzLLgNsM7JfvByjxqfWz2JqHPGEEkXpttCbMvdoCW7FPrfDU-psLHbdgD-"><img alt="" data-original-height="1194" data-original-width="2334" height="328" src="https://blogger.googleusercontent.com/img/a/AVvXsEgBjM82vk9aCJNzlk-9gbRKwc9y83Z9g9WZsPDUQXxB4oYXsuA-QntGrp4pMalJFUPFqETW0fxy3IfCrlziX946r_ymJ8G0a-mtIkFEzp1DoXPRcq9RUi1KOOtjgK8XfRJJcghzLLgNsM7JfvByjxqfWz2JqHPGEEkXpttCbMvdoCW7FPrfDU-psLHbdgD-=w640-h328" width="640"></a></p><p>Something that was still unclear to me at that point is: How long does it take for inactivity reboot to be triggered? A new&nbsp;<a href="https://www.404media.co/apple-quietly-introduced-iphone-reboot-code-which-is-locking-out-cops/">article by 404 media</a>&nbsp;claimed that it was 3-4 days. So I updated my SRD to the latest beta and made a time lapse.</p><p><iframe allowfullscreen="" height="266" src="https://www.youtube.com/embed/QOe2rDKOWMk" width="320" youtube-src-id="QOe2rDKOWMk"></iframe></p><p>Turns out, the inactivity reboot triggers exactly after 3 days (72 hours). The iPhone would do so despite being connected to Wi-Fi. This confirms my suspicion that this feature had nothing to do with wireless connectivity.</p><h2>Reverse Engineering Inactivity Reboot</h2><p>Let's reverse engineer what's changed! Which security guarantees does it provide?</p><p>Here is a high-level overview of what I found:</p><ul><li>The Secure Enclave Processor (SEP) keeps track on when your phone was last unlocked. If that last unlock time exceeds 3 days, the SEP tells the <span>AppleSEPKeyStore</span> kernel module that the time was exceeded.</li><li>The <span>AppleSEPKeyStore</span> kernel module informs user space to initiate a reboot. <span>SpringBoard</span> will then gracefully terminate all user-space processes. This prevents potential data loss upon reboot.</li><li>If the <span>AppleSEPKeyStore</span> kernel module finds the iPhone to still be powered on after it should have rebooted, the kernel will panic. This case should never happen, unless someone tries to tamper with inactivity reboot.</li><li>The <span>AppleSEPKeyStore</span> kernel module writes an NVRAM variable <span>aks-inactivity</span>. After the iPhone rebooted, <span>keybagd</span> reads this variable and, if set, sends an analytics event to Apple including how long the iPhone was not unlocked.</li></ul><p>The remainder of this post shows how I figured this out and what security implications the underlying design has.</p><h2>Indicators in Sysdiagnose</h2><p>From my search in ipsw-diffs, I knew there were some log messages that are printed on reboot. At the same time as I started looking them statically, I knew I had to see them actually logged for myself.</p><p>After my phone rebooted after three days, I took a sysdiagnose and searched for these messages. When doing this yourself, make sure that you unlocked the device before making the sysdiagnose. Otherwise, events from before the reboot will be missing.</p><p>In the&nbsp;<span>AppleSEPKeyStore</span>&nbsp;messages, there are the following entries around the inactivity reboot:</p><p><span>default</span><span>	</span><span>2024-11-17 01:35:14.341697 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":3846:0: <span>notifying user space of inactivity reboot</span><br></span><span>default</span><span>	</span><span>2024-11-17 01:35:14.341766 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":12598:31: operation failed (sel: 35 ret: e00002f0)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:14.342053 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":12598:31: operation failed (sel: 35 ret: e00002f0)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:34.958218<br></span><span><span>[reboot occurs]</span><br>+0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":331:0: starting (BUILT: Oct 26 2024 08:16:35) ("normal" variant 🌽 , 1827.60.43)<br></span><span>default</span><span>	</span><span>2024-11-17 01:35:34.958381 +0100</span><span>	</span><span>kernel</span><span>	</span><span>"AppleSEPKeyStore":476:0: _sep_enabled = 1</span></p><p>For more context, these are the unfiltered log messages before the reboot is initiated:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjh3CycIuHLRgh46JB32Vr3WAFbEPzGzusiH_hWUUO248aoq27ntRBvDJuUhwVzF2O1p3BIYYhHoYam1IssPyBeI581bGm7frA4yFzIFA8gWerWJyuHfxDhRstUeo9eUcjG0xHcYoFeFi0S67dtuFRezYoJsz3ZMuhL5nce9o6jE7pH_Qb7tSedUNY0TxDn"><img alt="" data-original-height="1848" data-original-width="2518" height="470" src="https://blogger.googleusercontent.com/img/a/AVvXsEjh3CycIuHLRgh46JB32Vr3WAFbEPzGzusiH_hWUUO248aoq27ntRBvDJuUhwVzF2O1p3BIYYhHoYam1IssPyBeI581bGm7frA4yFzIFA8gWerWJyuHfxDhRstUeo9eUcjG0xHcYoFeFi0S67dtuFRezYoJsz3ZMuhL5nce9o6jE7pH_Qb7tSedUNY0TxDn=w640-h470" width="640"></a></p><h2>Reverse Engineering the <span>SEPKeyStore</span> Kernel Extension</h2><p>The latest iOS kernel can be downloaded using the following <a href="https://github.com/blacktop/ipsw"><span>ipsw</span></a> command:</p><p><span>ipsw download appledb --device iPhone17,3 --os iOS --version '18.2 beta 2' --kernel</span></p><p>This will download and decompress the kernel. For further analysis, I loaded the whole kernel cache into Binary Ninja. <span>ipsw</span> also supports splitting the kernel into its modules (called "extensions" on iOS). The latest version of Ghidra also has decent support for the iOS kernel. So there's a lot of tools to choose from for this analysis.</p><p>I also downloaded an older kernel where Apple accidentally included symbols and manually diffed these versions with a focus on the code related to inactivity reboot. The kernel has three strings relating to the feature:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh0ZxgYUnJU1elCjWJXqzi-qkQEehkAKd3dkIH4UAUmxk6dwL3iiZ2LFlo3IvYug9d_qFuuiV6AnibEN8o8cEN-76IfazheZdPhIkm88ZYS59KADrKHJcup3X1mwGQ-9BCkdOJt7L83mSLBSAiayIIxM4AXVjMtAKRHwYaccHqKMeYAejLtvi9TGxkrY0Xl"><img data-original-height="356" data-original-width="1386" height="164" src="https://blogger.googleusercontent.com/img/a/AVvXsEh0ZxgYUnJU1elCjWJXqzi-qkQEehkAKd3dkIH4UAUmxk6dwL3iiZ2LFlo3IvYug9d_qFuuiV6AnibEN8o8cEN-76IfazheZdPhIkm88ZYS59KADrKHJcup3X1mwGQ-9BCkdOJt7L83mSLBSAiayIIxM4AXVjMtAKRHwYaccHqKMeYAejLtvi9TGxkrY0Xl=w640-h164" width="640"></a></p><p>"notifying user space of inactivity reboot" is the string we already know from the sysdiagnose. It belongs to the function <span>AppleKeyStore::handle_events</span>, which polls for SEP events in the background. The following screenshot shows it in more context after reverse engineering and some renaming of functions.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhqa9iNrqdm95LKnxB9k2Nbw9Pp2lDkiYg6NVgZW3PFiO_0DntNVYm6QRwV-0Er5puxU_Y7m3kS1zEbuxcU1IGOOTU8ZKwZsfmLEPiZy_XJb1KWi1_dQAI7_iaV7pmymZS0NtE2CLVTfH0Km5pfKeETc27gLtKDgPXAgKmwZEQESfk0No_7H-CEqVZ5U33e"><img alt="" data-original-height="1936" data-original-width="3024" height="410" src="https://blogger.googleusercontent.com/img/a/AVvXsEhqa9iNrqdm95LKnxB9k2Nbw9Pp2lDkiYg6NVgZW3PFiO_0DntNVYm6QRwV-0Er5puxU_Y7m3kS1zEbuxcU1IGOOTU8ZKwZsfmLEPiZy_XJb1KWi1_dQAI7_iaV7pmymZS0NtE2CLVTfH0Km5pfKeETc27gLtKDgPXAgKmwZEQESfk0No_7H-CEqVZ5U33e=w640-h410" width="640"></a></p><p>The first string, "max inactivity window expired, failed to reboot the device", is the kernel panic in case that the iPhone failed to reboot.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgKeMPdV7mTxwu4QbCAEk4DnF6wdxUtpApFTkmHV7aYJjkkyVz4BdCN5tuFIJ5FsfvYqhjbDtdGic19Rt5cHbdoZ6S5Jl_lCTd7JhkuG8IzWCyg5h0WgXNKYa3xuaqlTOwuoG7SoATovv-m6nXthdMMaIpWeQ90xCh1IrqfzQorVcuFuH7TPuop6A6ysjBS"><img alt="" data-original-height="166" data-original-width="1338" height="80" src="https://blogger.googleusercontent.com/img/a/AVvXsEgKeMPdV7mTxwu4QbCAEk4DnF6wdxUtpApFTkmHV7aYJjkkyVz4BdCN5tuFIJ5FsfvYqhjbDtdGic19Rt5cHbdoZ6S5Jl_lCTd7JhkuG8IzWCyg5h0WgXNKYa3xuaqlTOwuoG7SoATovv-m6nXthdMMaIpWeQ90xCh1IrqfzQorVcuFuH7TPuop6A6ysjBS=w640-h80" width="640"></a></p><p>For more context, the panic is called by the function&nbsp;<span>AppleKeyStore::handle_device_state_return</span>. There are multiple paths that invoke this handler through many layers of abstraction, which have to do with the UserClient but also SEP states.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEgi_v0P9i541Jg-d9THXz-ka-CvldLwbzLqWkCDFt6rEuKszDs8BQFtfhbYU2sXAqeOkMKkCrG47wShd3bzw-PurLLi766x86d1d3K_7T6oP_7R6y6bhBmCfAnKPnyuQLRVqlxjOaiJPkXl1UW0Yg5asqovDlzSJVqJdsks2pfF0oDKTw55nKSZ6EAq90yO"><img alt="" data-original-height="1584" data-original-width="1886" height="537" src="https://blogger.googleusercontent.com/img/a/AVvXsEgi_v0P9i541Jg-d9THXz-ka-CvldLwbzLqWkCDFt6rEuKszDs8BQFtfhbYU2sXAqeOkMKkCrG47wShd3bzw-PurLLi766x86d1d3K_7T6oP_7R6y6bhBmCfAnKPnyuQLRVqlxjOaiJPkXl1UW0Yg5asqovDlzSJVqJdsks2pfF0oDKTw55nKSZ6EAq90yO=w640-h537" width="640"></a></p><p>With the <a href="https://github.com/elbiazo/calltree">calltree plugin</a>, we can see all the incoming calls to this function.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjfKMpgWgr5MpdI1gKiXgnP860eGS0w4h7DqW8nWfOX5D2I8WiUPGHIGjfFeDOwJ_zKqjjv59fIAdeZQNQ-OPCzdNTM08N12sMg2Cg4M042iaWFkkgYlYeUsfk7njj6QwChF61fnDebQbbbDx1u6QKj33-cJnAqNBLo4f8LL68GNiodwTMwEfuptfnsadrL"><img alt="" data-original-height="660" data-original-width="1054" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEjfKMpgWgr5MpdI1gKiXgnP860eGS0w4h7DqW8nWfOX5D2I8WiUPGHIGjfFeDOwJ_zKqjjv59fIAdeZQNQ-OPCzdNTM08N12sMg2Cg4M042iaWFkkgYlYeUsfk7njj6QwChF61fnDebQbbbDx1u6QKj33-cJnAqNBLo4f8LL68GNiodwTMwEfuptfnsadrL=w640-h400" width="640"></a></p><p>Now to the last string, "aks-inactivity". We can see that this is a property that is set in the IORegistry.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEh3-JpRsuVcoQvKveSUKY47zOeJl3FVm3VAvtmG17u_f_wdfYYvQaS3JXA7kIAequNfdiQ9s-tSHQrOAtlRdVoPmJD1lJ2K5RZExe5vNgHNMZsv_JPgY5k-RflZTVqnC1-_FdZCmn6EZnE_fPiIWTIYPMKPh_oN_EkQP5YEk9eE1i-x-Wy-_YCEC9GvNus5"><img alt="" data-original-height="1000" data-original-width="2948" height="218" src="https://blogger.googleusercontent.com/img/a/AVvXsEh3-JpRsuVcoQvKveSUKY47zOeJl3FVm3VAvtmG17u_f_wdfYYvQaS3JXA7kIAequNfdiQ9s-tSHQrOAtlRdVoPmJD1lJ2K5RZExe5vNgHNMZsv_JPgY5k-RflZTVqnC1-_FdZCmn6EZnE_fPiIWTIYPMKPh_oN_EkQP5YEk9eE1i-x-Wy-_YCEC9GvNus5=w640-h218" width="640"></a></p><p>Its counterpart is in <span>keybagd</span> in user space. When <span>keybagd</span> is initialized, it checks for this variable, issues an analytics event, and then deletes it. This analytics event probably helps Apple optimize the time window, but we can ignore it for the core functionality.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiDi2dNE_vIsKY6UD2LhnRy31qXgNMeBdWA-Bk52lQV7aK6VWZqxudXe0wJRPDHV4rPDxi44CUOyKQpm7-Yks93eATITRqqbSkOME6xM9o7W7N1NYKsBBP65yS97cKtq6TLP6ChFQxF1gFg-NK84_q0lCXOubU1ZNr9-XIxdEKjE9dc3QwXRKe31KJ9j5oH"><img alt="" data-original-height="804" data-original-width="1692" height="304" src="https://blogger.googleusercontent.com/img/a/AVvXsEiDi2dNE_vIsKY6UD2LhnRy31qXgNMeBdWA-Bk52lQV7aK6VWZqxudXe0wJRPDHV4rPDxi44CUOyKQpm7-Yks93eATITRqqbSkOME6xM9o7W7N1NYKsBBP65yS97cKtq6TLP6ChFQxF1gFg-NK84_q0lCXOubU1ZNr9-XIxdEKjE9dc3QwXRKe31KJ9j5oH=w640-h304" width="640"></a></p><p>Something that I couldn't find in the kernel, even with the knowledge that it was 72 hours, was this particular time window. I couldn't find any numbers that matched 72 hours. So how does the phone know when to reboot?</p><p>While there are some references to time-related functionality in the <span>SEPKeyStore</span> kernel extension, none of these compare a value to 72 hours. These references were quite simple to find and did not differ much from the older kernel version without inactivity reboot, so it doesn't seem like the functionality was added here.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEitDjU-0SbfmaNcNosZlWHx0dTUnfmIjSVxTW_xA1bgEVwSmgp_uob_P62GyCveo8CdmN7MwQ8rX_J1qI1U4KMTPxgUigfGwgRYlWzvkmhoqepnpr-KkBDhQ2WakbMxs_YGsuiOIMVoXq19cu4TEzqBHHxwQEMJrR1xAVTF0LRk6_2-W9L_m4k7LvDJw9Bb"><img alt="" data-original-height="936" data-original-width="1610" height="372" src="https://blogger.googleusercontent.com/img/a/AVvXsEitDjU-0SbfmaNcNosZlWHx0dTUnfmIjSVxTW_xA1bgEVwSmgp_uob_P62GyCveo8CdmN7MwQ8rX_J1qI1U4KMTPxgUigfGwgRYlWzvkmhoqepnpr-KkBDhQ2WakbMxs_YGsuiOIMVoXq19cu4TEzqBHHxwQEMJrR1xAVTF0LRk6_2-W9L_m4k7LvDJw9Bb=w640-h372" width="640"></a></p><p>However, the <span>SEPKeyStore</span> communicates with the SEP co-processor. In the functions I identified, reboots are related to some SEP states. Could it be the SEP itself that checks the time?</p><h2>Reverse Engineering the Secure Enclave Processor</h2><p>The SEP is one of Apple's most protected secrets. In contrast to most other firmware on the iPhone, the firmware for the SEP is encrypted.</p><p>Luckily for us, <a href="https://twitter.com/nyan_satan">@nyan_satan</a> recently leaked <a href="https://theapplewiki.com/wiki/Keys:CrystalBSeed_22B5069a_(iPhone16,1)">SEP firmware encryption keys for iOS 18.1 beta 6</a>, just eta wen Apple introduced inactivity reboot. (Thank you!! 🎉 And Apple, if you're reading this, why not ship the SEP unencrypted?) Using <span>ipsw</span>, we can download the SEP firmware as follows:</p><p><span>ipsw download appledb --device iPhone16,1 --os iOS --version '18.1 beta 6' --pattern "sep-firmware.d83.RELEASE.im4p"</span></p><p>With the leaked keys, we can decrypt the firmware:</p><p><span>pyimg4 im4p extract --iv 6705fb216080e19667dbcf71f532ae73 --key 4ea9db4c2e63a316a6854c83e2f5c81fd102ad40160b8998b5f9b16838b7116e -i sep-firmware.d83.RELEASE.im4p -o sep-firmware.d83.RELEASE.im4p.e</span></p><p>Loading this into Binary Ninja is a bit tricky. We can guess that the architecture is 64-bit ARM little endian. But there's no metadata where the firmware has to be loaded to. Being lazy and not wanting to spend time on writing a firmware loader, I used Binary Ninja's Triage feature to auto-detect the most likely address. Note that the firmware seems to have multiple fragments and there's multiple potential load addresses. I picked&nbsp;0x80090000ffc80000, which worked well for me.</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEguwmjRE1k8u5npWzXhq5cs8lRMkyQqgECTzTuq_zaLuWDr5J8AyZtKkN64XixVpkczXFfSsQknPPclpjICoz7GrfRWBINwETutIXB-RF45-vlem8PavZPTyOTK4k6BmjTyY2l7xcTbmC1g6gooiaAe9DtqaiGavX0ajfnofIghAADa8xtmsLPe7bMNN-cW"><img alt="" data-original-height="580" data-original-width="514" height="400" src="https://blogger.googleusercontent.com/img/a/AVvXsEguwmjRE1k8u5npWzXhq5cs8lRMkyQqgECTzTuq_zaLuWDr5J8AyZtKkN64XixVpkczXFfSsQknPPclpjICoz7GrfRWBINwETutIXB-RF45-vlem8PavZPTyOTK4k6BmjTyY2l7xcTbmC1g6gooiaAe9DtqaiGavX0ajfnofIghAADa8xtmsLPe7bMNN-cW=w355-h400" width="355"></a><span>&nbsp;</span></p><p>There's only little known about the SEP. The best information I could come up with is a <a href="https://www.blackhat.com/docs/us-16/materials/us-16-Mandt-Demystifying-The-Secure-Enclave-Processor.pdf">presentation dating back to 2016</a> – but that's better than nothing! What's good to know is that the SEP firmware is structured into apps, so I'm guessing the other base addresses the triage found may correspond to the other apps' address spaces. The app that communicates with the SEPKeyStore is called <span>sks</span> (see slide 86 of the presentation). Not a lot of information, but enough to start reverse engineering!</p><p>Looking at strings, it looks like the architecture of apps running inside the SEP hasn't changed much since 2016. The <span>SEPKeyStore</span>-related app is still called <span>sks</span>:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhfPY-EilBBTqlkWuLpWvFXx3bok5syhn8KZO15LWApKKMIN2wgcAMK-znjMMpk5Kv3xq0uVQOBI8UK4Ld8QpEmDQGCOpEq4shM1Td8ywQ3xHmdjl8Fi7lrCYg7iWTsGxY5fA-GcTTk-3z6Z4qb4FCGZ7AE1Lbcxy_KHPQ9-blxMVrSjLRtSUNdZxk8iluZ"><img alt="" data-original-height="436" data-original-width="1248" height="224" src="https://blogger.googleusercontent.com/img/a/AVvXsEhfPY-EilBBTqlkWuLpWvFXx3bok5syhn8KZO15LWApKKMIN2wgcAMK-znjMMpk5Kv3xq0uVQOBI8UK4Ld8QpEmDQGCOpEq4shM1Td8ywQ3xHmdjl8Fi7lrCYg7iWTsGxY5fA-GcTTk-3z6Z4qb4FCGZ7AE1Lbcxy_KHPQ9-blxMVrSjLRtSUNdZxk8iluZ=w640-h224" width="640"></a></p><p>The SEP has almost no debug strings, making it tougher to reverse engineer. Here is what the initialization function for the <span>SEPKeyStore</span> looks like after some manual annotations ("sth" stands for "something" – I didn't go too deep into understanding the specifics here):</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhPrpu1jil4mrXVyRP1LvrLjsTVNQ2jC3QHXLJGZaSZ8oAFgGC97jZf0T00HGkzTiANO1MlZA6KSoUmiDzhEkx0Zk0GQcYD6MCAa1RuX9cJnQNBFWblQ_k5j0ZqBUltQ1B2Wfks6vmIcHu6MsDrJmmtw270W3SQ9bBTTcXai7Sc4g3Y6Uj-3PJQP71Z9zbE"><img alt="" data-original-height="458" data-original-width="1614" height="182" src="https://blogger.googleusercontent.com/img/a/AVvXsEhPrpu1jil4mrXVyRP1LvrLjsTVNQ2jC3QHXLJGZaSZ8oAFgGC97jZf0T00HGkzTiANO1MlZA6KSoUmiDzhEkx0Zk0GQcYD6MCAa1RuX9cJnQNBFWblQ_k5j0ZqBUltQ1B2Wfks6vmIcHu6MsDrJmmtw270W3SQ9bBTTcXai7Sc4g3Y6Uj-3PJQP71Z9zbE=w640-h182" width="640"></a></p><p>Within its main function, we can find multiple other functions executed before a service workloop starts. However, there's plenty of code. How do we focus on things that are related to the inactivity reboot?</p><p>Let's recall that we're looking for something that resembles 72 hours. In the kernel, times are usually measured in seconds or in microseconds. For example 72 hours are&nbsp;259200 seconds (0x3f480). But looking for this value (or for 259200000000, in microseconds, or any other sensible units) in the binary won't return any matches.</p><p>Using the <a href="https://godbolt.org/">compiler explorer</a>, we can see why: Optimizations...</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEjiVp3eTE2aXqnNiZMIJu7uRjTEpEahpDlVxzexq0iVsFmD01Ux8xTq8OUcNECXwZ099DnhfdpBvb_ztjNnzHlQ6XV9nbIOOYmabejFVA-D3Iq-cAt_nK2IktXQ7I0XF-6xONp4MgNgcjsCa1_mRTAiJ0WgeKRiac-QQpo8SBthBPDV3aK2UrLwNS7nUtRk"><img alt="" data-original-height="1062" data-original-width="3024" height="225" src="https://blogger.googleusercontent.com/img/a/AVvXsEjiVp3eTE2aXqnNiZMIJu7uRjTEpEahpDlVxzexq0iVsFmD01Ux8xTq8OUcNECXwZ099DnhfdpBvb_ztjNnzHlQ6XV9nbIOOYmabejFVA-D3Iq-cAt_nK2IktXQ7I0XF-6xONp4MgNgcjsCa1_mRTAiJ0WgeKRiac-QQpo8SBthBPDV3aK2UrLwNS7nUtRk" width="640"></a></p><p>Rather than looking the full time in bytes in reverse byte order, we're looking for assembly instructions that load parts of the timespan into a register.</p><p>Binary Ninja knows how to reverse this optimization, and allows us to search in its intermediate representations, instead of looking for raw bytes. In our case, we know that we're looking for a constant.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEiAksVrxNBr5xGLwoRNEbxzVIMoOMMapRcHKVGaB_R_O3AWmrvNcMwINtPo1zaXnlNxbv0tV8EWXAg0jnHgskU22-Ui71tPAFkaoXud_POuCAdbcoAwPg8HNbjfeo6qYKmsaM9IlQR6muWihOSzBzauHF4PitCFVl-DfjRt6RUUZrBEPLU3Gu2WEusxU8hE"><img alt="" data-original-height="808" data-original-width="1100" height="294" src="https://blogger.googleusercontent.com/img/a/AVvXsEiAksVrxNBr5xGLwoRNEbxzVIMoOMMapRcHKVGaB_R_O3AWmrvNcMwINtPo1zaXnlNxbv0tV8EWXAg0jnHgskU22-Ui71tPAFkaoXud_POuCAdbcoAwPg8HNbjfeo6qYKmsaM9IlQR6muWihOSzBzauHF4PitCFVl-DfjRt6RUUZrBEPLU3Gu2WEusxU8hE=w400-h294" width="400"></a>&nbsp;</p></div><p>We find only two matches:</p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhJwjCy5EDHg2kK4t-h4GW_KqEKu7X_1d-jfbjm1LeYIMKF_YEA4oAZaWK20ZujWctbqOhXaseMbH1DvWp6H-O4flgmN419gJorpk653jeG90Jr540JGOV92Up4e7jyUq9CW1X2VPhFEncwOT-qWzB0ZFm6BN3vLcA01fZVH6WR9jJ5ywFRevYjyFVX4g5c"><img alt="" data-original-height="250" data-original-width="1632" height="98" src="https://blogger.googleusercontent.com/img/a/AVvXsEhJwjCy5EDHg2kK4t-h4GW_KqEKu7X_1d-jfbjm1LeYIMKF_YEA4oAZaWK20ZujWctbqOhXaseMbH1DvWp6H-O4flgmN419gJorpk653jeG90Jr540JGOV92Up4e7jyUq9CW1X2VPhFEncwOT-qWzB0ZFm6BN3vLcA01fZVH6WR9jJ5ywFRevYjyFVX4g5c=w640-h98" width="640"></a></p><p>And here it is – a function that compares various times, including 3 days, which is related to the <span>sks</span> application's main function. The result of this time comparison is used to create a message, which is likely sent to the <span>SEPKeyStore</span> kernel extension. Creating a new enum makes it more readable:</p><p><span>enum times : uint32_t</span></p><p><span>{</span></p><p><span>    _3_days = 0x3f480,</span></p><p><span>    _2_days = 0x2a301,</span></p><p><span>    _1_days = 0x15181,</span></p><p><span>    `_2.5h` = 0xe11</span></p><p><span>};</span></p><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEg0JC9kKOelUjXI7AmxnIJf4160QuVQ6JibF1gYvk_QIc3d-0mDP29IvKaxuYQS9Hou8u3gfcpJHf11qznWDYY2QQKSZhDWcGusssdg_kxo4fH9aJbveSJxRdo5OpLS7az18e9wfpKxsjTZqKeQdA6eB45tfr4y0offM4mmbADozT7PH3H71ypvq_83cql6"><img alt="" data-original-height="1114" data-original-width="1330" height="535" src="https://blogger.googleusercontent.com/img/a/AVvXsEg0JC9kKOelUjXI7AmxnIJf4160QuVQ6JibF1gYvk_QIc3d-0mDP29IvKaxuYQS9Hou8u3gfcpJHf11qznWDYY2QQKSZhDWcGusssdg_kxo4fH9aJbveSJxRdo5OpLS7az18e9wfpKxsjTZqKeQdA6eB45tfr4y0offM4mmbADozT7PH3H71ypvq_83cql6=w640-h535" width="640"></a></p><p>This function is used in a context to initialize a struct, which is likely a message being sent from the SEP to the kernel extension.</p><div><p><a href="https://blogger.googleusercontent.com/img/a/AVvXsEhIi1ytOnwIHz3FfehB3gzVzc4s2L1oOKyB_huAhQ_tMDIPvTFs3ffHMb8sICRwqbc8_dDY3r340BZpYdyjx9n1hRnMUcHIxRUEhcoPrBiiTA4tevJ3E_X8rxujzB1hq2Ncm0mLX3nzlHc41BfoAMsJcqZZWcaEg0_ElGxbhQS86UZeo9TXH4PdrzeFqIza"><img alt="" data-original-height="1626" data-original-width="1780" height="584" src="https://blogger.googleusercontent.com/img/a/AVvXsEhIi1ytOnwIHz3FfehB3gzVzc4s2L1oOKyB_huAhQ_tMDIPvTFs3ffHMb8sICRwqbc8_dDY3r340BZpYdyjx9n1hRnMUcHIxRUEhcoPrBiiTA4tevJ3E_X8rxujzB1hq2Ncm0mLX3nzlHc41BfoAMsJcqZZWcaEg0_ElGxbhQS86UZeo9TXH4PdrzeFqIza=w640-h584" width="640"></a></p><p>I didn't end up reverse engineering much more of the SEP, but this seems to confirm that it's really the SEP that keeps track of how long the phone hasn't been unlocked. This design makes sense to me, since the SEP is involved in every unlock, and is also hardened against tampering, even if an exploit against the main kernel is used, so it's a good place to anchor a mitigation like this.</p></div><div><h2>A Mitigation Only Against Cops?</h2><h2><p>While the media coverage so far framed this mitigation as primarily targeting law enforcement, it also a huge security improvement against theft. Outdated law enforcement equipment often finds its way to eBay and other similar platforms for rather cheap price tags. However, thieves won't have the financial and legal means to obtain up-to-date exploits to unlock iPhones within 3 days of getting them. That's another reason why it's important to keep your device updated!</p><p>On the other hand, law enforcement can and will have to adjust their process, and act faster than before. The first forensic tooling companies already announced that they're able to coordinate these steps within 24 hours! (Note that this also indicates that they only have exploits for AFU state... 🤡)</p></h2><h2>Key Takeaways</h2></div><div><p>This feature is not at all related to wireless activity. The law enforcement document's conclusion that the reboot is due to phones wirelessly communicating with each other is implausible. The older iPhones before iOS 18 likely rebooted due to another reason, such as a software bug.</p><p>The time measurement and triggering of the reboot is in the SEP, which communicates with the <span>SEPKeyStore</span> kernel extension to perform the reboot. It is likely that using an external time source provided over the Internet or cellular networks to tamper with timekeeping will not influence the 3-day timer.</p><p>Security-wise, this is a very powerful mitigation. An attacker must have kernel code execution to prevent an inactivity reboot. This means that a forensic analyst might be able to delay the reboot for the actual data extraction, but the initial exploit must be run within the first three days.</p><p>Inactivity reboot will change the threat landscape for both thieves and forensic analysts, but asymmetrically so: while law enforcement is under more time pressure, it likely completely locks out criminals from accessing your data to get into your bank accounts and other valuable information stored on your iPhone.</p><p>Interested in reverse engineering? Follow me on <a href="https://www.youtube.com/@jiskac">YouTube</a> and <a href="https://bsky.app/profile/naehrdine.bsky.social">BlueSky</a> for updates.</p></div></div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why did Windows 95 setup use three operating systems? (175 pts)]]></title>
            <link>https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507</link>
            <guid>42166606</guid>
            <pubDate>Sun, 17 Nov 2024 19:54:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507">https://devblogs.microsoft.com/oldnewthing/20241112-00/?p=110507</a>, See on <a href="https://news.ycombinator.com/item?id=42166606">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="single-wrapper">
    
    <article data-clarity-region="article" id="post-110507">
        <div data-bi-area="body_article" data-bi-id="post_page_body_article">
            <p>Twitter users @tthirtle asked why Windows 95 setup goes through three operating systems: MS-DOS, Windows 3.1, and then Windows 95. Why not go from MS-DOS straight to Windows 95?</p>
<blockquote data-conversation="none">
<p dir="ltr" lang="en">Here’s another good question. Why does Windows 95 setup use 3 different UI’s. DOS,Win3.x,and Win9x?</p>
<p>— Thomas (@tthirtle) <a href="https://twitter.com/tthirtle/status/1809777917565767993?ref_src=twsrc%5Etfw">July 7, 2024</a></p></blockquote>

<p>Windows 95 setup could upgrade from three starting points: MS-DOS, Windows 3.1, or Windows 95. (Yes, you could upgrade Windows 95 to Windows 95. You might do this to repair a corrupted system while preserving data.)</p>
<p>One option is to write three versions of Windows 95 setup: One for setting up from MS-DOS, another for setting up from Windows 3.1, and a third for setting up from Windows 95.</p>
<p>This was not a pleasant option because you basically did the same work three times, but implemented separately, so you have to do three times the coding.</p>
<p>A better option is to just write one version of Windows 95 setup and use it for all three starting points. So now you get to choose the platform on which to base your code.</p>
<table>
<tbody>
<tr>
<th rowspan="2">From</th>
<th colspan="3">App type</th>
</tr>
<tr>
<th>MS-DOS</th>
<th>16-bit GUI</th>
<th>32-bit GUI</th>
</tr>
<tr>
<td>MS-DOS</td>
<td>•</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Windows 3.1</td>
<td>•</td>
<td>•</td>
<td>&nbsp;</td>
</tr>
<tr>
<td>Windows 95</td>
<td>•</td>
<td>•</td>
<td>•</td>
</tr>
</tbody>
</table>
<p>If you write Windows 95 setup as an MS-DOS app, then it runs on all three platforms. That’s great! You need to write only one setup program. The downside is that it’s going to be a text-mode setup program, which looks ugly and gives a poor initial impression of what is supposed to be a brand new GUI world.</p>
<p>At the other extreme, you can write Windows 95 setup as a 32-bit GUI program, but that means that if the user is starting from MS-DOS or Windows 3.1, you have to install Windows 95 before you can run Windows 95 setup, which is a bit of a catch-22.</p>
<p>In the middle is the happy medium: You can have the MS-DOS setup program install a minimal version of Windows 3.1, just barely enough to support what the 16-bit GUI setup program needs.¹ This tiny version is small enough to be copied and installed off a small number of floppy disks. Once that’s done, boot into the tiny version of Windows 3.1 and run the 16-bit GUI setup program.</p>
<p>Okay, so now we have three setup programs. The first one is used if you’re installing from MS-DOS: It installs the tiny version of Windows 3.1, and then boots into Windows 3.1 to continue to the next step.</p>
<p>The second setup program runs as a 16-bit Windows app, either in the miniature copy of Windows 3.1 (if the user is upgrading from MS-DOS), the real copy of Windows 3.1 (if the user is upgrading from Windows 3.1), or the real copy of Windows 95 (if the user is upgrading from Windows 95). This second setup program is the one that does almost all of the real work: It does the initial interaction with the user to gather information about how to install Windows 95, like asking which optional components to include, and does hardware detection to decide which drivers to install.² And then it copies the drivers and Windows 95 files onto the system, migrates your old settings to the new operating system, and boots into Windows 95.</p>
<p>The third setup program runs as a 32-bit Windows app. It is running in the real Windows 95 system and does some final steps that require operation a live running system, like installing printers.</p>
<table>
<tbody>
<tr>
<td>Starting from MS-DOS →</td>
<td>Install mini Windows 3.1</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>MS-DOS app</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Boot into mini-Windows 3.1</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>Starting from Windows 3.1 →</td>
<td>Gather information</td>
<td rowspan="7">&nbsp;</td>
<td rowspan="7">&nbsp;</td>
<td rowspan="7">16-bit Windows app</td>
</tr>
<tr>
<td>or Windows 95</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Detect hardware</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Copy drivers and<br>
Windows 95 files</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Migrate settings and configure drivers</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Boot into Windows 95</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>↓</td>
</tr>
<tr>
<td>&nbsp;</td>
<td>Final setup</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>Windows 95 app</td>
</tr>
</tbody>
</table>
<p>So that’s why Windows 95 setup is really three setup programs chained together. It allows a single copy of the code to be used for all three of the installation scenarios. Each program takes you one step closer to the goal. And everything got implemented only once.</p>
<p>¹ There was existing precedent for a tiny version of Windows that is barely enough to run a single program. The original Windows version of Microsoft Excel <a href="https://en.wikipedia.org/w/index.php?title=Microsoft_Excel&amp;oldid=166461028"> came with a runtime version of Windows 2.1</a>, so that customers who didn’t have Windows could still use Excel.</p>
<p>² This hardware detection code that Setup uses is the same code that runs when you do hardware detection from within Windows 95 itself, so even that code needed to be written only once. It did have some runtime checks to change behavior slightly depending on whether it’s running in Windows 3.1 or Windows 95, but the vast majority of the code is identical.</p>
        </div><!-- .entry-content -->

        <!-- AI Disclaimer -->
            </article>
    
</div><div><!-- Author section -->
            <h2>Author</h2>
            <div><div><p><img src="https://devblogs.microsoft.com/oldnewthing/wp-content/uploads/sites/38/2019/02/RaymondChen_5in-150x150.jpg" alt="Raymond Chen"></p></div><p>Raymond has been involved in the evolution of Windows for more than 30 years. In 2003, he began a Web site known as The Old New Thing which has grown in popularity far beyond his wildest imagination, a development which still gives him the heebie-jeebies. The Web site spawned a book, coincidentally also titled The Old New Thing (Addison Wesley 2007). He occasionally appears on the Windows Dev Docs Twitter account to tell stories which convey no useful information.</p></div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Humans have caused 1.5 °C of long-term global warming according to new estimates (374 pts)]]></title>
            <link>https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</link>
            <guid>42166030</guid>
            <pubDate>Sun, 17 Nov 2024 18:49:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates">https://www.lancaster.ac.uk/news/humans-have-already-caused-15-c-of-long-term-global-warming-according-to-new-estimates</a>, See on <a href="https://news.ycombinator.com/item?id=42166030">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                    <img src="https://cisweb.lancaster.ac.uk/img/cwip/cisweb.lancaster.ac.uk/EventsMedia/earth-adobestock242647178-smaller-638669192826825152.jpg?mode=crop&amp;width=874&amp;height=492&amp;center=0.50%2c0.50" alt="Earth from space" loading="lazy">
                                        <p>A new study published today in Nature Geoscience by Dr Andrew Jarvis at Lancaster University and Professor Piers Forster at the University of Leeds shows that humans may have already caused 1.5 °C of global warming when measured from a time genuinely before the industrial revolution and the start of large-scale carbon emissions.</p><p>The Paris Climate Agreement from 2016 established a long-term temperature goal of “limiting global temperature increase well below 2 degrees Celsius, while pursuing efforts to limit the increase to 1.5 degrees.” The 1.5 °C of warming figure has since become the yardstick to judge progress, or the lack of it, on climate change.</p><p>The human-induced contribution to global warming is currently put at 1.31 °C, but with an uncertainty range of 1.10 to 1.60 °C, according to the Intergovernmental Panel on Climate Change (IPCC’s) preferred methods. This means it is unclear, from the IPCC’s adopted estimates, whether the 1.5 °C boundary has been breached or not. </p><p>Crucially, the IPCC’s preferred methods use temperature records from 1850-1900 as their ‘pre-industrial’ baseline for their calculations. They do this because this is when the first temperature records were taken, although the exact way to measure global temperature increases has never been defined within the climate negotiations. </p><p>Using this same 1850-1900 baseline, Dr Jarvis and Professor Forster’s method more than halves the uncertainty in the current human-caused warming estimate, thereby showing human-caused global warming currently remains below 1.5 °C if measured this way. On this measure, crossing the 1.5 °C Paris guardrail is under 10 years away at current warming rates. </p><p>However, Dr Jarvis and Professor Forster go further. Their method makes a more accurate estimate of the true long-term human contribution to global warming by pushing the base period from which the global temperature change is measured back to before 1700.</p><p>The authors find that when measured from this earlier more accurate definition of pre-industrial time, the long-term human contribution to warming was 1.49 °C ± 0.11 °C in 2023 and is now above 1.5 °C. This reveals that there is almost 0.2 °C of warming within the 1850-1900 baseline currently being used to define the warming.</p><p>Dr Jarvis, lead author of the study, said: “Measuring human-caused global warming is a difficult task because it forces us to compare today’s temperature with what it was in pre-industrial times – we call this the pre-industrial baseline. The closest we come to pre-industrial global temperature measurements are from the middle of the 1800’s, and unsurprisingly, these data are somewhat patchy and the Industrial Revolution was well underway by then. So using these early temperature data as a baseline as previous methods do not only ignores the warming that was already underway, it also bakes significant uncertainty into warming estimates.”</p><p>This new study instead uses CO<sub>2</sub> records from air bubbles trapped in ice-cores to establish a pre-1700 baseline for temperature. These records stretch back thousands of years, well before the Industrial Revolution and the effects of human-derived carbon emissions. The scientists are able to use the CO<sub>2</sub> record to anchor global warming estimates because of what they say is an overlooked relationship between the two.</p><p>“If you plot global temperatures against the concentration of CO<sub>2</sub> in the atmosphere, they both fall on a remarkably straight line, much straighter than current theory would predict,” said Dr Jarvis. “That line tells you not only how much the Earth has warmed since pre-industrial times, but also how much of that warming can be blamed on human activity.</p><p>“The climate is unimaginably complex, so perhaps it isn’t so surprising that such a direct method for accurately measuring the warming humans are responsible for has been overlooked,” Dr Jarvis added.</p><p>The scientists believe their new method is a strong candidate for measuring progress against the 1.5 and 2.0 degree Paris yardsticks.</p><p>Dr Jarvis said: “Our method has a number of strengths. Firstly, it directly tackles the problem of how to establish a robust pre-industrial baseline, although it functions equally well with the 1850-1900 baseline. Secondly, it produces estimates of human-caused warming that are at least 30 percent more certain than current methods. Finally, it is easy and quick to apply, meaning that we can produce warming estimates as soon as the CO<sub>2</sub> and temperature data become available without having to re-run complex climate models. This also means the results are transparent, making them far easier to communicate to non-specialists.”</p><p>Professor Forster, co-author of the study, said: “Our study shows that human societies have caused more than 1.5 degrees of long-term global warming. However, this does not necessarily mean that the Paris Agreement’s 1.5 temperature guardrail is breached, as we find that 0.18 °C of warming happened before global temperature records began, and this baked-in warming would not have been factored into the Paris Agreement.</p><p>“Policy makers set the Paris temperature goal to limit the devasting climate impacts many around the world are already experiencing. It was set to push countries to higher national ambition. It is clear we need to do more. The ambition set out in the goal for “pursuing efforts to limit the increase to 1.5 degrees” was not vainglorious when set, rather countries did not match it with their efforts. Urgent actions can slow warming rates and push back the time of breaching the Paris 1.5 degree limit. Although breaching the limit is now inevitable, delivering action commensurate with the noble Paris goal is more important than ever.” he added.</p><p>Although useful for measuring current levels of human-induced global warming, the researchers caution against using their method for making predictions on future warming.</p><p>Dr Jarvis, said: “Although atmospheric CO<sub>2</sub> is responsible for the bulk of human-induced warming so far, it is not solely responsible, and we know other factors such as methane could become increasingly important in the future, especially if we encounter climate tipping points. This means we need to keep an eye on our analysis. Fortunately, departures from the current linear regime could provide valuable early warning of such a change.”</p><p>The study is outlined in the paper ‘<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">Estimated human-induced warming from a linear temperature and atmospheric CO<sub>2</sub> relationship</a>’ published in Nature Geoscience.</p><p>DOI:<a href="https://www.nature.com/articles/s41561-024-01580-5" rel="noopener noreferrer" target="_blank">10.1038/s41561-024-01580-5</a></p>          <a href="https://www.lancaster.ac.uk/news/">Back to News</a>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Creating a QR Code step by step (146 pts)]]></title>
            <link>https://www.nayuki.io/page/creating-a-qr-code-step-by-step</link>
            <guid>42165862</guid>
            <pubDate>Sun, 17 Nov 2024 18:26:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nayuki.io/page/creating-a-qr-code-step-by-step">https://www.nayuki.io/page/creating-a-qr-code-step-by-step</a>, See on <a href="https://news.ycombinator.com/item?id=42165862">Hacker News</a></p>
Couldn't get https://www.nayuki.io/page/creating-a-qr-code-step-by-step: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lucid dreaming app triples users' awareness in dreams, study finds (111 pts)]]></title>
            <link>https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/</link>
            <guid>42165849</guid>
            <pubDate>Sun, 17 Nov 2024 18:24:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/">https://www.psypost.org/lucid-dreaming-app-triples-users-awareness-in-dreams-study-finds/</a>, See on <a href="https://news.ycombinator.com/item?id=42165849">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In a recent study published in <a href="https://www.sciencedirect.com/science/article/abs/pii/S1053810024001260"><em>Consciousness and Cognition</em></a>, researchers at Northwestern University showed that a smartphone app using sensory cues can significantly increase the frequency of lucid dreams—dreams in which a person is aware they are dreaming while still asleep. This study marks the first attempt to apply a lucid-dreaming method called Targeted Lucidity Reactivation outside of a lab environment, demonstrating that even a simple at-home approach can help users experience more lucid dreams.</p><p>Lucid dreaming has drawn increasing public interest for its potential benefits, including enhancing creativity, overcoming nightmares, and providing a space for personal growth and skill practice. Traditional techniques for inducing lucid dreams involve cognitive exercises, such as keeping a dream journal, performing reality checks, and practicing intention-setting before sleep.</p><p>Although these techniques can be effective, they require significant dedication and consistency. A streamlined, at-home approach could make lucid dreaming more accessible to the general public. The researchers wanted to explore whether a simplified, app-based approach using Targeted Lucidity Reactivation—a method previously successful in a controlled lab setting—could be adapted for use outside the lab with minimal technical requirements.</p><p>In Targeted Lucidity Reactivation, participants undergo training to associate a sound cue—such as a tone or melody—with becoming aware that they are dreaming. This same sound is then played during sleep to prompt lucidity within a dream, leveraging the brain’s ability to recognize the cue and reawaken a state of self-awareness while dreaming.</p><p>“I have always been fascinated in lucid dreaming because it provides a space to experience yourself in an entirely new way,” said study author Karen Konkoly, a postdoctoral psychology fellow and member of <a href="https://pallerlab.psych.northwestern.edu/" target="_blank" rel="noopener">Ken Paller’s Cognitive Neuroscience Laboratory</a>.</p><p>“Looking around in a lucid dream, you realize that everything before you is generated by your mind, including your sense of self. Moreover, lucid dreaming is a fascinating model for studying consciousness. Since we’ve been developing more effective ways to induce lucid dreams in the sleep laboratory, we wanted to take a step towards making these advances available for individuals to use on their own.”</p><p>The study consisted of two experiments to test whether a smartphone-based Targeted Lucidity Reactivation method could increase lucid dreaming frequency. The first experiment involved 19 participants, all Android users with a history of dream recall, who completed a one-week protocol involving nightly training with a specialized app. In the second experiment, the researchers recruited a larger sample of 416 participants who used the app for multiple nights, allowing for a more comprehensive look at the effectiveness of Targeted Lucidity Reactivation cues in a diverse group.</p><p>In Experiment 1, participants used an app that played specific sounds—a sequence of beeps or a violin tone—to create an association between these sounds and a lucid state of awareness. Each night before sleep, participants completed a 20-minute training exercise where the app’s sound cues prompted them to enter a “lucid mindset.”</p><p>Once asleep, the app replayed these cues intermittently after a six-hour delay, using a gradual volume increase to avoid sudden awakenings. Participants reported any dreams they remembered each morning, noting if they experienced lucid awareness or incorporated the sound cue into their dream.</p><p>This first experiment found that using the app increased lucid dreaming frequency from an average of 0.74 dreams per week (prior to the study) to 2.11 during the week of app use. Many participants credited the app’s cues with prompting lucidity directly or indirectly.</p><p>Experiment 2 built on these results with a more complex design. This time, participants were divided into three groups to clarify the effect of the Targeted Lucidity Reactivation cues. The first group, receiving cues every night, served as the main experimental group. The second and third groups were control conditions: one received “untrained” cues—sounds not used in the pre-sleep training—and the other received no sound cues on alternate nights. This design helped distinguish increases in lucid dreaming due specifically to Targeted Lucidity Reactivation and those possibly caused by arousal from sound cues in general.</p><p>With 50 participants completing the full seven-night protocol, the study provided insight into the distinct impact of Targeted Lucidity Reactivation cues compared to other sounds. Participants receiving these cues reported significantly more lucid dreams on training nights than those receiving untrained or no cues, reinforcing that pairing cues with lucidity training was essential for effective lucid dreaming.</p><p>“Tweaking sleep opens the door for people to change their dreaming,” Paller said. “We are taking a sleep-engineering approach to using sleep for personal benefits, for practicing skills, solving problems, and for spiritual and personal growth.”</p><p>The combined results from both experiments support Targeted Lucidity Reactivation’s potential as an accessible, smartphone-based method for promoting lucid dreaming. While the cues showed a clear benefit, the study also highlighted challenges, such as the potential for cues to disrupt sleep if mistimed, since the app could not detect when participants entered rapid eye movement (REM) sleep.</p><p>To improve the precision of Targeted Lucidity Reactivation in the future, researchers are considering incorporating wearable technology capable of detecting REM sleep, allowing cues to play at the optimal time for triggering lucidity without disturbing sleep. The Northwestern team has already begun collaborating with InteraXon, the company behind the Muse-S headband, which could allow for more precise sleep-stage detection and improve the effectiveness of the Targeted Lucidity Reactivation method.</p><p>“The app used in this research is under continual development and the latest version can be found in the Google Playstore: <a href="https://play.google.com/store/apps/details?id=com.neurelectrics.dive&amp;hl=en_US&amp;gl=US">https://play.google.com/store/apps/details?id=com.neurelectrics.dive&amp;hl=en_US&amp;gl=US</a>,” Konkoly said. “We are working on making better methods available to the public, but we aren’t there yet. We hope to have more options for people next year.”</p><p>The study, “<a href="https://doi.org/10.1016/j.concog.2024.103759" target="_blank" rel="noopener">Provoking lucid dreams at home with sensory cues paired with pre-sleep cognitive training</a>,” was authored by Karen R. Konkoly, Nathan W. Whitmore, Remington Mallett, Christopher Y. Mazurek, and Ken A. Paller.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AlphaProof's Greatest Hits (160 pts)]]></title>
            <link>https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html</link>
            <guid>42165397</guid>
            <pubDate>Sun, 17 Nov 2024 17:20:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html">https://rishimehta.xyz/2024/11/17/alphaproofs-greatest-hits.html</a>, See on <a href="https://news.ycombinator.com/item?id=42165397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      

      <main aria-label="Content">
        <div>
          <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    



<p>Here I’ll try to explain the coolest ideas in each of <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">AlphaProof</a>’s IMO 2024 solutions. AlphaProof produces proofs in <a href="https://leanprover.github.io/">Lean</a>, and each Lean proof is composed of a series of tactics. So I’ll pick out the tactics that correspond to these ideas in the proofs for problems 1, 2 and 6 (the three problems that AlphaProof solved). AlphaProof has developed its own proving style, so figuring out what it’s doing can involve some detective work.</p>

<!--more-->

<p>If you’re not familiar with the problems already, I recommend trying them yourself, and then maybe reading <a href="https://web.evanchen.cc/exams/IMO-2024-notes.pdf">Evan Chen’s solution notes</a>, or watching these <a href="https://youtube.com/playlist?list=PLSa4NIW1yxdg7xoEL2x8wGzci0t8h51nN&amp;si=-GB9q9hreFBD7eOI">videos</a> that give an intuition for some of the human solutions. The full AlphaProof solutions, annotated by <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=Oliver%20Nash%2C%20Bhavik%20Mehta%2C%20Paul%20Lezeau%2C%20Salvatore%20Mercuri%2C%20Lawrence%20Wu%2C%20Calle%20Soenne%2C%20Thomas%20Murrills%2C%20Luigi%20Massacci%20and%20Andrew%20Yang%20advised%20and%20contributed%20as%20Lean%20experts">Lean experts</a>, are available <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/index.html">here</a> - I will only include snippets of the proofs in this post.</p>

<ul id="markdown-toc">
  <li><a href="#problem-1" id="markdown-toc-problem-1">Problem 1</a>    <ul>
      <li><a href="#problem" id="markdown-toc-problem">Problem</a></li>
      <li><a href="#solution" id="markdown-toc-solution">Solution</a></li>
    </ul>
  </li>
  <li><a href="#problem-2" id="markdown-toc-problem-2">Problem 2</a>    <ul>
      <li><a href="#problem-1" id="markdown-toc-problem-1">Problem</a></li>
      <li><a href="#solution-1" id="markdown-toc-solution-1">Solution</a></li>
    </ul>
  </li>
  <li><a href="#problem-6" id="markdown-toc-problem-6">Problem 6</a>    <ul>
      <li><a href="#problem-2" id="markdown-toc-problem-2">Problem</a></li>
      <li><a href="#solution-2" id="markdown-toc-solution-2">Solution</a></li>
    </ul>
  </li>
  <li><a href="#postscript" id="markdown-toc-postscript">Postscript</a></li>
</ul>

<h2 id="problem-1">Problem 1</h2>

<h3 id="problem">Problem</h3>
<p>Determine all real numbers $\alpha$ such that, for every positive integer $n$, the integer</p><p>

\[\lfloor \alpha \rfloor + \lfloor 2\alpha \rfloor + \dots +\lfloor n\alpha \rfloor\]

</p><p>is a multiple of $n$. (Note that $\lfloor z \rfloor$ denotes the greatest integer less than or equal to $z$. For example, $\lfloor -\pi \rfloor = -4$ and $\lfloor 2 \rfloor = \lfloor 2.9 \rfloor = 2$.)</p>

<h3 id="solution">Solution</h3>

<p>The answer turns out to be the set of even integers. Reminder that the way AlphaProof solves these problems is by proposing many solution candidates, attempting to prove and disprove each of them, and then eventually finding a proof only for the correct answer. The proof we’re looking at here is the one that proves that the answer is the set of even integers.</p>

<p>Showing that the even integers satisfy the given property is trivial, the hard part of this proof is to show that no $\alpha$ except the even integers can satisfy it. AlphaProof does this in an interesting (if convoluted) way. It first sets up an integer $\ell$ such that $2\ell = \lfloor\alpha\rfloor + \lfloor2\alpha\rfloor$. This is possible because we know the RHS is even by substituting $n=2$ into the given property.</p>

<figure><pre><code data-lang="lean"><span>exists</span><span>λ</span><span>x</span> <span>L</span><span>=&gt;</span>(<span>L</span> <span>2</span> <span>two_pos</span>)<span>.</span><span>rec</span> <span>λ</span><span>l</span> <span>Y</span><span>=&gt;</span><span>?</span><span>_</span></code></pre></figure>

<p><code>L 2</code> is where the given property is used with $n=2$. As an aside, AlphaProof often mashes several tactics together in a single line like this. A more comprehensible version that accomplishes the same thing is</p>

<figure><pre><code data-lang="lean"><span>constructor</span>
<span>·</span> <span>intro</span> <span>x</span> <span>L</span>
  <span>obtain</span> <span>⟨</span><span>l</span>, <span>Y</span><span>⟩</span> := <span>L</span> <span>2</span> (<span>by</span> <span>exact</span> <span>two_pos</span>)</code></pre></figure>

<p>Note that we’ve also renamed $\alpha$ to <code>x</code>.
Next, it claims (and goes on to prove) that for all natural numbers $n$,</p><p>

\[\lfloor (n + 1)\:\alpha \rfloor = \lfloor \alpha \rfloor + 2n\:(\ell - \lfloor \alpha \rfloor) \tag{1} \label{eq:claim}\]

</p><figure><pre><code data-lang="lean"><span>suffices</span>: <span>∀</span> (<span>n</span> : <span>ℕ</span>),<span>⌊</span>(<span>n</span><span>+</span><span>1</span>)<span>*</span><span>x</span><span>⌋</span> <span>=</span><span>⌊</span> <span>x</span><span>⌋</span><span>+</span><span>2</span> <span>*</span> <span>↑</span> (<span>n</span> : <span>ℕ</span>) <span>*</span> (<span>l</span><span>-</span>(<span>⌊</span>(<span>x</span>)<span>⌋</span>))</code></pre></figure>

<p>From this, it is able to show that $\alpha = 2\,(\ell-\lfloor\alpha\rfloor)$.</p>

<figure><pre><code data-lang="lean"><span>use</span>(<span>l</span><span>-</span><span>⌊</span><span>x</span><span>⌋</span>)<span>*</span><span>2</span></code></pre></figure>

<p>which has to be an even integer (since it’s 2 multiplied by an integer).</p>

<p>The way in which it proves these things involves some rather elaborate simplifications. But setting up the claim in \eqref{eq:claim} is the impressive bit that makes the rest of the proof work. To my eyes, the motivation for this claim is rather unintuitive, and the fact that it all works out is almost magical.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P1/index.html">here</a>.</p>

<h2 id="problem-2">Problem 2</h2>

<h3 id="problem-1">Problem</h3>

<p>Find all positive integer pairs $(a,b)$ such that there exist positive integers $g$ and $N$ where</p><p>

\[\gcd\,(a^n + b, b^n + a) = g\]

</p><p>holds for all integers $n \geq N$.</p>

<h3 id="solution-1">Solution</h3>

<p>AlphaProof correctly proposes that $(1, 1)$ is the only solution. To show that no other solution can work, it asks us to consider the number $ab + 1$. It claims (and later proves) that $ab + 1$ must divide $g$.</p>

<figure><pre><code data-lang="lean"><span>suffices</span>:<span>b</span><span>.1*</span><span>b</span><span>.2+</span><span>1</span><span>∣</span><span>Y</span></code></pre></figure>

<p>Note that in its infinite wisdom, AlphaProof decides to rename pairs $(a, b)$ to <code>b</code>, so that it must reference the elements as <code>b.1</code> and <code>b.2</code>. It has also chosen, for reasons best known to itself, to rename the variable $g$ to <code>Y</code>.</p>

<p>Now, selecting $n=N \phi(ab+1)$, we get</p><p>

\[(ab + 1) \mid (a^{N\phi(ab + 1)} + b) \text{ and } (ab + 1) \mid (b^{N\phi(ab + 1)} + a)\]

</p><p>The fact that $ab + 1$ is coprime to $a$ and $b$ makes it so that we can apply <a href="https://en.wikipedia.org/wiki/Euler%27s_theorem">Euler’s theorem</a>, ie</p><p>

\[a^{\phi(ab+1)} \equiv 1 \pmod{ab+1}\]

\[b^{\phi(ab+1)} \equiv 1 \pmod{ab+1}\]

</p><p>So we have $ab + 1 \mid 1 + b$ and $ab + 1 \mid 1 + a$, from which it follows that $a = b = 1$.</p>

<p>This strategy closely follows human proofs to this problem. The choice to consider $ab + 1$ is the clever idea that sets up the proof.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P2/index.html">here</a>.</p>

<h2 id="problem-6">Problem 6</h2>

<h3 id="problem-2">Problem</h3>
<p>Let $\mathbb{Q}$ be the set of rational numbers. A function $f: \mathbb{Q} \to \mathbb{Q}$ is called <em>aquaesulian</em> if the following property holds: for every $x,y \in \mathbb{Q}$,</p><p>

\[f(x+f(y)) = f(x) + y \quad \text{or} \quad f(f(x)+y) = x + f(y).\]

</p><p>Show that there exists an integer $c$ such that for any aquaesulian function $f$ there are at most $c$ different rational numbers of the form $f(r) + f(-r)$ for some rational number $r$, and find the smallest possible value of $c$.</p>

<h3 id="solution-2">Solution</h3>

<p>AlphaProof finds the answer $c=2$. It breaks the proof into two parts. First, it shows that $c \leq 2$, by showing that $f(r) + f(-r)$ can either be $0$ or some singular other value. This part of the proof is quite elaborate, and makes clever use of the given aquaesulian properties.</p>

<p>Once this is done, $c$ can be either $1$ or $2$. To show that $c=2$, AlphaProof proposes an aquaesulian function $f$ that takes on two different values for  $f(r) + f(-r)$.</p><p>

\[f(x) = -x + 2⌈x⌉\]

</p><figure><pre><code data-lang="lean"><span>specialize</span> <span>V</span> <span>$</span> <span>λ</span> <span>N</span><span>=&gt;-</span><span>N</span><span>+</span><span>2</span> <span>*</span><span>Int</span><span>.</span><span>ceil</span> <span>N</span></code></pre></figure>

<p>It then shows that $f(-1) + f(1) = 0$ and $f(1/2) + f(-1/2) = 2$, which gives us the two distinct values we need.</p>

<figure><pre><code data-lang="lean"><span>use</span> <span>Finset</span><span>.</span><span>one_lt_card</span><span>.2</span><span>$</span> <span>by</span> <span>exists</span><span>@</span><span>0</span>,<span>V</span><span>.1.</span><span>mem_toFinset</span><span>.2</span> (<span>by</span> <span>exists</span><span>-</span><span>1</span>),<span>2</span>,<span>V</span><span>.1.</span><span>mem_toFinset</span><span>.2</span> (<span>by</span> <span>exists</span> <span>1</span><span>/</span><span>2</span>)</code></pre></figure>

<p>Again, lots of stuff mashed into one line, but <code>by exists -1</code> and <code>by exists 1/2</code> are where it shows the two distinct values.</p>

<p>This is a remarkable function to have constructed! And it’s pretty hard to find. Only 5/509 participants solved P6, and notably Tim Gowers gave it a bit of a shot while judging this solution and didn’t find a function that gave two distinct values.</p>

<p>AlphaProof’s full solution is <a href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/imo-2024-solutions/P6/index.html">here</a>.</p>

<h2 id="postscript">Postscript</h2>

<p>Having now digested several AlphaProof solutions, I still find it remarkable that a machine is able to find proofs like this. Finding these mathematical ideas is hard enough, and having to prove them in Lean makes things even harder.</p>

<p>At an IMO afterparty, some of us from the AlphaProof team had fun with signing the best tactic from each proof.</p>

<p><img src="https://rishimehta.xyz/assets/p1_tactic.jpg" alt="p1 tactic">
<img src="https://rishimehta.xyz/assets/p2_tactic.jpg" alt="p2 tactic">
<img src="https://rishimehta.xyz/assets/p6_tactic.jpg" alt="p6 tactic"></p>

<p>I discussed the P6 solution in some more detail on the <a href="https://youtu.be/uX6ceY1vcUg?si=d9LY1QvdQ-yeppPK&amp;t=1845">No Priors podcast</a>.</p>

<p>Thanks to the <a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/#:~:text=Oliver%20Nash%2C%20Bhavik%20Mehta%2C%20Paul%20Lezeau%2C%20Salvatore%20Mercuri%2C%20Lawrence%20Wu%2C%20Calle%20Soenne%2C%20Thomas%20Murrills%2C%20Luigi%20Massacci%20and%20Andrew%20Yang%20advised%20and%20contributed%20as%20Lean%20experts">Lean experts</a> who translated the cryptic Lean proofs into English, and Bhavik, Oliver and Paul in particular for suggesting impressive lines in the proofs.</p>

  </div>
</article>

        </div>
      </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Good Software Development Habits (274 pts)]]></title>
            <link>https://zarar.dev/good-software-development-habits/</link>
            <guid>42165057</guid>
            <pubDate>Sun, 17 Nov 2024 16:34:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://zarar.dev/good-software-development-habits/">https://zarar.dev/good-software-development-habits/</a>, See on <a href="https://news.ycombinator.com/item?id=42165057">Hacker News</a></p>
<div id="readability-page-1" class="page">
  
  <header>
    <a href="https://zarar.dev/">
      <h2>
        Zarar's blog
      </h2>
    </a>
    <nav>
      <p><a href="https://zarar.dev/">Home</a>
<a href="https://zarar.dev/me/">Me</a>
<a href="https://zarar.dev/subscribe/">Subscribe</a></p>

    </nav>
  </header>
  <main>
    

    
        
    

    
        

        <p>
            <i>
                <time datetime="2024-09-05T13:00Z">
                    05 Sep, 2024
                </time>
            </i>
        </p>
    

    <p>This post is not advice, it's what's working for me.</p>
<p>It's easy to pick up bad habits and hard to create good ones. Writing down what's working for me helps me maintain any good habits I've worked hard to develop. Here's an unordered list of 10 things that have helped me increase speed and maintain a respectable level of quality in the product I'm currently developing.</p>
<ol>
<li><p>Keep commits small enough that you wonder if you're taking this "keep commits small" thing a little too far. You just never know when you have to revert a particular change and there's a sense of bliss knowing where you introduced a bug six days ago and only reverting that commit without going through the savagery of merge conflicts. My rule of thumb: compiling software should be commitable.</p>
</li>
<li><p>Live Kent Beck's <a href="https://x.com/KentBeck/status/250733358307500032?lang=en">holy words of wisdom</a>: "for each desired change, make the change easy (warning: this may be hard), then make the easy change". Aim for at least half of all commits to be refactorings. Continuous refactoring is thinking of changes I can make in under 10 minutes that improve something. Doing this pays off whenever a bigger requirement comes in and you find yourself making a small change to satisfy it only because of those smaller improvements. Big refactorings are a bad idea.</p>
</li>
<li><p>All code is a liability. Undeployed code is the grim reaper of liabilities. I need to know if it works or at least doesn't break anything. Tests give you confidence, production gives you approval. The hosting costs might rack up a little with so many deploys but it's a small price to pay for knowing the last thing you did was a true sign of progression. <em>Working software is the primary measure of progress</em>, says one of the <a href="https://agilemanifesto.org/principles.html">agile principles</a>. Working and progress are doing a lot of heavy lifting in that sentence, so I've defined them for myself. Working is something being working enough to be deployed, and if it's code that's contributing to a capability, that's progress.</p>
</li>
<li><p>Know when you're testing the framework's capability. If you are, don't do it. The framework is already tested by people who know a lot more than you, and you have to trust them that the <code>useState()</code> hook does what it's supposed to do. If you keep components small, then you reduce the need for a lot of tests as the framework will be doing most of the heavy lifting in the component. If the component is big, then you introduce more complexity and now you need to write a lot of tests.</p>
</li>
<li><p>If a particular function doesn't fit anywhere, create a new module (or class or component) for it and you'll find a home for it later. It's better to create a new independent construct than to jam it into an existing module where you know deep down it doesn't make sense. Worst comes to worst, it lives as an independent module which isn't too bad anyway.</p>
</li>
<li><p>If you don't know what an API should look like, write the tests first as it'll force you to think of the "customer" which in this case is you. You'll invariably discover cases that you would not have thought of if you had just written the code first and tests after. You don't have to be religious about TDD and it's OK to work in larger batches (e.g., write more than just a couple lines of code before making it pass).  The amount of code to write in a red/failing state doesn't always have to be small. You know what you're doing, don't let dogma get in the way of productivity.</p>
</li>
<li><p>Copy-paste is OK once. The second time you're introducing duplication (i.e., three copies), don't. You should have enough data points to create a good enough abstraction. The risk of diverging implementations of the same thing is too high at this point, and consolidation is needed.  It's better to have some wonky parameterization than it is to have multiple implementations of nearly the same thing. Improving the parameters will be easier than to consolidate four different implementations if this situation comes up again.</p>
</li>
<li><p>Designs get stale. You can slow the rate at which they get stale by refactoring, but ultimately you'll need to change how things work. Don't feel too bad about moving away from something that was dear to you a while ago and something you felt proud about at the time.  You did the right thing then and shouldn't beat yourself up for not getting it right enough that you wouldn't need to change anything. Most of the time writing software is changing software. Just accept it and move on. There's no such thing as the perfect design, and change is at the core of software development. How good you are at changing things is how good you are at software development.</p>
</li>
<li><p>Technical debt can be classified into three main types: 1) things that are preventing you from doing stuff now, 2) things that will prevent you from doing stuff later, and 3) things that <em>might</em> prevent you from doing stuff later. Every other classification is a subset of these three. Minimize having lots of stuff in #1 and try to focus on #2. Ignore #3.</p>
</li>
<li><p>Testability is correlated with good design. Something not being easily testable hints that the design needs to be changed. Sometimes that design is your test design. As an example, if you find yourself finding it difficult to mock <code>em.getRepository(User).findOneOrFail({id})</code>, then chances are you either need to put that call into its own function that can be mocked, or write a test utility which allows for easier mocking of the entity manager methods. Tests go unwritten when it's hard to test, not because you don't want to test.</p>
</li>
</ol>
<p>There's probably a lot more, but 10 is a nice number.</p>


    

    
        

        
            


        
    


  </main>
  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything Is Just Functions: Mind-Blowing Insights from SICP and David Beazley (315 pts)]]></title>
            <link>https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</link>
            <guid>42164541</guid>
            <pubDate>Sun, 17 Nov 2024 15:07:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df">https://ezzeriesa.notion.site/1-week-with-David-Beazley-and-SICP-4c440389cf1e43f48fe67c969967f655#58ee6b0435b24e26bd624b33ffed94df</a>, See on <a href="https://news.ycombinator.com/item?id=42164541">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The myth that you can’t build interactive web apps except as single page app (200 pts)]]></title>
            <link>https://htmx.org/essays/you-cant/</link>
            <guid>42164154</guid>
            <pubDate>Sun, 17 Nov 2024 13:44:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://htmx.org/essays/you-cant/">https://htmx.org/essays/you-cant/</a>, See on <a href="https://news.ycombinator.com/item?id=42164154">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    

  
  
    <address>Tony Alaribe</address>
    <p><time>September 20, 2024</time></p><h3 id="an-ode-to-browser-advancements"><a href="#an-ode-to-browser-advancements" aria-label="Anchor link for: an-ode-to-browser-advancements">#</a><strong>An Ode to Browser Advancements.</strong></h3>
<p>I often encounter discussions on Reddit and YCombinator where newer developers seek tech stack advice. Inevitably,
someone claims it’s impossible to build a high-quality application without using a single-page application (SPA)
framework like React or AngularJS. This strikes me as odd because, even before the SPA revolution, many popular
multi-page web applications offered excellent user experiences.</p>
<p>Two years ago, I set out to build an <a rel="noopener" target="_blank" href="https://apitoolkit.io/">observability platform</a> and chose to experiment with a
multi-page application (MPA) approach using HTMX. I wondered: Would a server-rendered MPA be inadequate for a data-heavy
application, considering that most observability platforms are built on ReactJS?</p>
<p>What I discovered is that you can create outstanding server-rendered applications if you pay attention to certain
details.</p>
<p><strong>Here are some common MPA myths and what I’ve learned about them.</strong></p>
<h2 id="myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation"><a href="#myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation" aria-label="Anchor link for: myth-1-mpa-page-transitions-are-slow-because-javascript-and-css-are-downloaded-on-every-page-navigation">#</a>Myth 1:  MPA Page Transitions are slow because JavaScript and CSS are downloaded on every page navigation</h2>
<p>The perception that MPA page transitions are slow is widespread—and not entirely unfounded—since this is the default
behavior of browsers. However, browsers have made significant improvements over the past decade to mitigate this issue.</p>
<p>To illustrate, in the video below, a full page reload with the cache disabled takes 2.90 seconds until the
DOMContentLoaded event fires. I recorded this at a café with poor Wi-Fi, but let’s use this as a reference point. Keep
that number in mind.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log-exp-cache.mp4">
</video>
<p>It is common to reduce load times in MPAs using libraries such as <strong>PJAX, Turbolinks, and even HTMX Boost</strong>. These
libraries hijack the page reload using Javascript and swap out only the HTML body element between transitions. That way,
most of the page’s head section assets don’t need to be reloaded or re-downloaded.</p>
<p>But there’s a lesser known way of reducing how much assets are re-downloaded or evaluated during page transitions.</p>
<h3 id="client-side-caching-via-service-workers"><a href="#client-side-caching-via-service-workers" aria-label="Anchor link for: client-side-caching-via-service-workers">#</a>Client-side Caching via Service workers</h3>
<p>Frontend developers who have built Progressive Web Applications (PWA) with SPA frameworks might know about service
workers.</p>
<p>For those of us who are not frontend or PWA developers, service workers are a built-in feature of browsers. They let you
write Javascript code that sits between your users and the network, intercepting requests and deciding how the browser
handles them.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker-chart.png" alt="service-worker-chart.png"></p>
<p>Due to its association with the PWA trend, service workers are only ordinary among SPA developers, and developers need
to realize that this technology can also be used for regular Multi-Page Applications.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/log_exp_with_cache.mp4">
</video>
<p>In the video demonstration, we enable a service worker to cache and refresh the current page. You’ll notice that there’s
no flicker when clicking the link to reload the page, resulting in a smoother user experience.</p>
<p>Moreover, instead of transmitting over 2 MB of static assets as before, the browser now only fetches 84 KB of HTML
content—the actual page data. This optimization reduces the <code>DOMContentLoaded</code> event time from 2.9 seconds to under 500
milliseconds. Impressively, this improvement is achieved <strong>without</strong> using HTMX Boost, PJAX, or Turbolinks.</p>
<h3 id="how-to-implement-service-workers-in-your-multi-page-application"><a href="#how-to-implement-service-workers-in-your-multi-page-application" aria-label="Anchor link for: how-to-implement-service-workers-in-your-multi-page-application">#</a>How to Implement Service workers in Your Multi-Page Application</h3>
<p>You might be wondering how to replicate these performance gains in your own MPA. Here’s a simple guide:</p>
<ol>
<li><strong>Create a <code>sw.js</code> File</strong>: This is your service worker script that will manage caching and network requests.</li>
<li><strong>List Files to Cache</strong>: Within the service worker, specify all the assets (HTML, CSS, JavaScript, images) that
should be cached.</li>
<li><strong>Define Caching Strategies</strong>: Indicate how each type of asset should be cached—for example, whether they should be
cached permanently or refreshed periodically.</li>
</ol>
<p>By implementing a service worker, you effectively tell the browser how to handle network requests and caching, leading
to faster load times and a more seamless user experience.</p>
<h3 id="use-workbox-to-generate-service-workers"><a href="#use-workbox-to-generate-service-workers" aria-label="Anchor link for: use-workbox-to-generate-service-workers">#</a>Use Workbox to generate service workers</h3>
<p>While it’s possible to write service workers by hand—and there are excellent resources
like <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers">this MDN article</a> to
help you—I prefer using Google’s <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/workbox">Workbox</a> library to automate the process.</p>
<h3 id="steps-to-use-workbox"><a href="#steps-to-use-workbox" aria-label="Anchor link for: steps-to-use-workbox">#</a>Steps to Use Workbox:</h3>
<ol>
<li>
<p><strong>Install Workbox</strong>: Install Workbox via npm or your preferred package manager:</p>
<pre data-lang="bash"><code data-lang="bash"><span>npm</span><span> install workbox-cli</span><span> --global
</span></code></pre>
</li>
<li>
<p>Generate a Workbox Configuration file: Run the following command to create a configuration file:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> wizard
</span></code></pre>
</li>
<li>
<p><strong>Configure Asset Handling</strong>: In the generated <code>workbox-config.js</code> file, define how different assets should be
cached. Use the <code>urlPattern</code> property—a regular expression—to match specific HTTP requests. For each matching
request, specify a caching strategy, such as <code>CacheFirst</code> or <code>NetworkFirst</code>.</p>
<p><img src="https://htmx.org/img/you-cant/workbox-cfg.png" alt="workbox-cfg.png"></p>
</li>
<li>
<p><strong>Build the Service Worker</strong>: Run the Workbox build command to generate the <code>sw.js</code> file based on your configuration:</p>
<pre data-lang="bash"><code data-lang="bash"><span>workbox</span><span> generateSW workbox-config.js
</span></code></pre>
</li>
<li>
<p><strong>Register the Service Worker in Your Application</strong>: Add the following script to your HTML pages to register the
service worker:</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>script</span><span>&gt;
</span><span>  </span><span>if </span><span>(</span><span>'serviceWorker' </span><span>in navigator) {
</span><span>    window.</span><span>addEventListener</span><span>(</span><span>'load'</span><span>, </span><span>function</span><span>() {
</span><span>      navigator.serviceWorker.</span><span>register</span><span>(</span><span>'/sw.js'</span><span>).</span><span>then</span><span>(</span><span>function</span><span>(registration) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration successful with scope: '</span><span>, </span><span>registration</span><span>.scope);
</span><span>      }, </span><span>function</span><span>(err) {
</span><span>        console.</span><span>log</span><span>(</span><span>'ServiceWorker registration failed: '</span><span>, </span><span>err</span><span>);
</span><span>      });
</span><span>    });
</span><span>  }
</span><span>&lt;/</span><span>script</span><span>&gt;
</span></code></pre>
</li>
</ol>
<p>By following these steps, you instruct the browser to serve cached assets whenever possible, drastically reducing load
times and improving the overall performance of your multi-page application.</p>
<p><img src="https://htmx.org/img/you-cant/service-worker.png" alt="Image showing the registered service worker from the chrome browser console."></p>
<p>Image showing the registered service worker from the chrome browser console.</p>
<h3 id="speculation-rules-api-prerender-pages-for-instant-page-navigation"><a href="#speculation-rules-api-prerender-pages-for-instant-page-navigation" aria-label="Anchor link for: speculation-rules-api-prerender-pages-for-instant-page-navigation">#</a><a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API"><code>Speculation Rules API</code></a>: Prerender pages for instant page navigation.</h3>
<p>If you have used <strong>htmx-preload</strong> or <strong>instantpage.js,</strong> you’re familiar with prerendering and the problem
the <a rel="noopener" target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/API/Speculation_Rules_API">“Speculation Rules API”</a> aims to solve. The
Speculation Rules API is designed to improve performance for future navigations. It has an expressive syntax for
specifying which links should be prefetched or prerendered on the current page.</p>
<p><img src="https://htmx.org/img/you-cant/speculation-rules.png" alt="Speculation rules configuration example"></p>
<p>Speculation rules configuration example</p>
<p>The script above is an example of how speculation rules are configured. It is a Javascript object, and without going
into detail, you can see that it uses keywords such as “where,” “and,” “not,” etc. to describe what elements should
either be prefetched or prerendered.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/prerender-vid.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/prerender-pages">Example impact of prerendering (Chrome Team)</a></p>
<h2 id="myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network"><a href="#myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network" aria-label="Anchor link for: myth-2-mpas-can-t-operate-offline-and-save-updates-to-retry-when-there-s-network">#</a>Myth 2: MPAs can’t operate offline and save updates to retry when there’s network</h2>
<p>From the last sections, you know that service workers can cache everything and make our apps operate entirely offline.
But what if we want to save offline POST requests and retry them when there is internet?</p>
<p><img src="https://htmx.org/img/you-cant/workbox-offline-cfg.png" alt="workbox-offline-cfg.png"></p>
<p>The configuration javascript file above shows how to configure Workbox to support two common offline scenarios. Here,
you see background Sync, where we ask the service worker to cache any failed requests due to the internet and retry it
for up to 24 hours.</p>
<p>Below, we define an offline catch Handler, triggered when a request is made offline. We can return a template partial
with HTML or a JSON response or dynamically build a response based on the request input. The sky is the limit here.</p>
<h2 id="myth-3-mpas-always-flash-white-during-page-transitions"><a href="#myth-3-mpas-always-flash-white-during-page-transitions" aria-label="Anchor link for: myth-3-mpas-always-flash-white-during-page-transitions">#</a>Myth 3: MPAs always flash white during page Transitions</h2>
<p>In the service worker videos, we already saw that this will not happen if we configure caching and prerendering.
However, this myth was not generally true until 2019. Since 2019, most browsers withhold painting the next screen until
all the required assets for the next page are available or a timeout is reached, resulting in no flash of white while
transitioning between both pages. This only works when navigating within the same origin/domain.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/paint-holding.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/blog/paint-holding">Paint holding documentation on chrome.com</a>.</p>
<h2 id="myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas"><a href="#myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas" aria-label="Anchor link for: myth-4-fancy-cross-document-page-transitions-are-not-possible-with-mpas">#</a>Myth 4: Fancy Cross-document page transitions are not possible with MPAs.</h2>
<p>The advent of single-page application frameworks made custom transitions between pages more popular. The allure of
different navigation styles comes from completely taking control of page navigation from the browsers. In practice, such
transitions have mostly been popular within the demos at web dev conference talks.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/page-transitions.mp4">
</video>
<p><a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Cross Document Transitions documentation on chrome.com</a>.</p>
<p>This remains a common argument for single-page applications, especially on Reddit and Hacker News comment sections.
However, browsers have been working towards solving this problem natively for the last couple of years. Chrome 126
rolled out cross-document view transitions. This means we can build our MPAs to include those fancy animations and
transitions between pages using CSS only or CSS and Javascript.</p>
<p>My favorite bit is that we might be able to create lovely cross-document transitions with CSS only:</p>
<p><img src="https://htmx.org/img/you-cant/cross-doc-transitions-css.png" alt="cross-doc-transitions-css.png"></p>
<p>You can quickly learn more on
the <a rel="noopener" target="_blank" href="https://developer.chrome.com/docs/web-platform/view-transitions">Google Chrome announcement page</a></p>
<p>This link hosts a <a rel="noopener" target="_blank" href="https://view-transitions.netlify.app/stack-navigator/mpa-prerender/">multi-page application demo</a>,
where you can play around with a rudimentary server-rendered application using the cross-document view transitions API
to simulate a stack-based animation.</p>
<h2 id="myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server"><a href="#myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server" aria-label="Anchor link for: myth-5-with-htmx-or-mpas-every-user-action-must-happen-on-the-server">#</a>Myth 5: With htmx or MPAs, every user action must happen on the server.</h2>
<p>I’ve heard this a lot when HTMX is discussed. So, there might be some confusion caused by the HTMX positioning. But you
don’t have to do everything server-side. Many HTMX and regular MPA users continue to use Javascript, Alpine, or
Hyperscript where appropriate.</p>
<p>In situations where robust interactivity is helpful, you can lean into the component islands architecture using
WebComponents or any javascript framework (react, angular, etc) of your choice. That way, instead of your entire
application being an SPA, you can leverage those frameworks specifically for the bits of your application that need that
interactivity.</p>
<p>The example above shows a very interactive search component in the <a rel="noopener" target="_blank" href="https://apitoolkit.io/">APItoolkit</a>. It’s a web
component implemented with lit-element, a zero-compile-step library for writing web components. So, the entire web
component event fits in a Javascript file.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/webcomponents-filter-element2.mp4">
</video>
<h2 id="myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom"><a href="#myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom" aria-label="Anchor link for: myth-6-operating-directly-on-the-dom-is-slow-therefore-it-would-be-best-to-use-react-virtual-dom">#</a>Myth 6: Operating directly on the DOM is slow. Therefore, it would be best to use React/Virtual DOM.</h2>
<p>The speed of direct DOM operations was a major motivation for building ReactJS on and popularizing the virtual DOM
technology. While virtual DOM operations can be faster than direct DOM operations, this is only true for applications
that perform many complex operations and refresh in milliseconds, where that performance might be noticeable. But most
of us are not building such software.</p>
<p>The Svelte team wrote an excellent article
titled <a rel="noopener" target="_blank" href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">“Virtual DOM is pure Overhead.”</a> I recommend reading it,
as it better explains why Virtual DOM doesn’t matter for most applications.</p>
<h2 id="myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity"><a href="#myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity" aria-label="Anchor link for: myth-7-you-still-need-to-write-javascript-for-every-minor-interactivity">#</a>Myth 7: You still need to write JavaScript for every minor interactivity.</h2>
<p>With the advancements in browser tech, you can avoid writing a lot of client-side Javascript in the first place. For
example, a standard action on the web is to show and hide things based on a button click or toggle. These days, you can
show and hide elements with only CSS and HTML, for example, by using an HTML input checkbox to track state. We can style
an HTML label as a button and give it a <code>for="checkboxID</code>“ attribute, so clicking the label toggles the checkbox.</p>
<pre data-lang="jsx"><code data-lang="jsx"><span>&lt;input id="published" class="hidden peer" type="checkbox"/&gt;
</span><span>&lt;label for="published" class="btn"&gt;toggle content&lt;/label&gt;
</span><span>
</span><span>&lt;div class="hidden peer-checked:block"&gt;
</span><span>    Content to be toggled when label/btn is clicked
</span><span>&lt;/div&gt;
</span></code></pre>
<p>We can combine such a checkbox with HTMX intersect to fetch content from an endpoint when the button is clicked.</p>
<pre data-lang="html"><code data-lang="html"><span>&lt;</span><span>input </span><span>id</span><span>=</span><span>"published" </span><span>class</span><span>=</span><span>"peer" </span><span>type</span><span>=</span><span>"checkbox" </span><span>name</span><span>=</span><span>"status"</span><span>/&gt;
</span><span>&lt;</span><span>div
</span><span>        </span><span>class</span><span>=</span><span>"hidden peer-checked:block"
</span><span>        </span><span>hx-trigger</span><span>=</span><span>"intersect once"
</span><span>        </span><span>hx-get</span><span>=</span><span>"/log-item"
</span><span>&gt;Shell/Loading text etc
</span><span>&lt;/</span><span>div</span><span>&gt;
</span></code></pre>
<p>All the classes above are vanilla <a rel="noopener" target="_blank" href="https://tailwindcss.com/">Tailwind CSS</a> classes, but you can also write the CSS by
hand. Below is a video of that code being used to hide or reveal log items in the log explorer.</p>
<video controls="">
  <source src="https://htmx.org/img/you-cant/expanding-log-item.mp4">
</video>
<h2 id="final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable"><a href="#final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable" aria-label="Anchor link for: final-myth-without-a-proper-frontend-framework-your-client-side-javascript-will-be-spaghetti-and-unmaintainable">#</a>Final Myth: Without a <em>“Proper”</em> frontend framework, your Client-side Javascript will be <a rel="noopener" target="_blank" href="https://www.reddit.com/r/webdev/comments/bkk0gl/avoiding_the_vanillajs_spaghetticode/">Spaghetti and Unmaintainable</a>.</h2>
<p>This may or may not be true.</p>
<h3 id="who-cares-i-love-spaghetti"><a href="#who-cares-i-love-spaghetti" aria-label="Anchor link for: who-cares-i-love-spaghetti">#</a>Who cares? I love Spaghetti.</h3>
<p>I like to argue that some of the most productive days of the web were the PHP and JQuery spaghetti days. A lot of
software was built at that time, including many of the popular internet brands we know today. Most of them were built as
so-called spaghetti codes, which helped them ship their products early and survive long enough to refactor and not be
spaghetti.</p>
<h2 id="conclusion"><a href="#conclusion" aria-label="Anchor link for: conclusion">#</a>Conclusion</h2>
<p>The entire point of this talk is to show you that a lot is possible with browsers in 2024. While we were not looking,
browsers have closed the gap and borrowed the best ideas from the single-page application revolution. For example,
WebComponents exist thanks to the lessons we learned from single-page applications.</p>
<p>So now, we can build very interactive, even offline web applications using mostly browser tools—HTML, CSS, maybe some
Javascript—and still not sacrifice much in terms of user experience.</p>
<h3>The browser has come a long way. Give it a chance!</h3>

  <p>
    &lt;/&gt;
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude AI built me a React app to compare maps side by side (188 pts)]]></title>
            <link>https://github.com/veloplanner/map-matrix</link>
            <guid>42164141</guid>
            <pubDate>Sun, 17 Nov 2024 13:39:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/veloplanner/map-matrix">https://github.com/veloplanner/map-matrix</a>, See on <a href="https://news.ycombinator.com/item?id=42164141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Map Matrix</h2><a id="user-content-map-matrix" aria-label="Permalink: Map Matrix" href="#map-matrix"></a></p>
<p dir="auto">Compare multiple maps side by side. <a href="https://veloplanner.github.io/map-matrix/" rel="nofollow">Live demo</a></p>
<p dir="auto"><strong>This project was mostly generated by Claude AI.</strong></p>
<p dir="auto">I wanted to develop a simple tool that I needed for <a href="https://veloplanner.com/" rel="nofollow">veloplanner.com</a>. I thought about using this opportunity to try out Claude AI for coding a project from scratch. It worked surprisingly well! I was able to explain my idea and get a working prototype in a few hours. Most of the time I was just copying code from Claude and pasting it into the editor. Later, I started using Cursor AI (with claude-3.5-sonnet model) which improved the experience a lot.</p>
<p dir="auto">You can add custom map source by clicking the "Add Custom Source" button in the navbar.
Configuration is stored in the browser's local storage.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot.png" alt="screenshot"></a></p>
<p dir="auto">Example of Cursor AI flow:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/veloplanner/map-matrix/blob/main/screenshot-cursor-ai.png"><img src="https://github.com/veloplanner/map-matrix/raw/main/screenshot-cursor-ai.png" alt="screenshot-cursor-ai.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>

</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare.com's Robots.txt (129 pts)]]></title>
            <link>https://www.cloudflare.com/robots.txt</link>
            <guid>42163883</guid>
            <pubDate>Sun, 17 Nov 2024 12:39:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cloudflare.com/robots.txt">https://www.cloudflare.com/robots.txt</a>, See on <a href="https://news.ycombinator.com/item?id=42163883">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bpftune uses BPF to auto-tune Linux systems (219 pts)]]></title>
            <link>https://github.com/oracle/bpftune</link>
            <guid>42163597</guid>
            <pubDate>Sun, 17 Nov 2024 11:38:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/oracle/bpftune">https://github.com/oracle/bpftune</a>, See on <a href="https://news.ycombinator.com/item?id=42163597">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">bpftune - BPF driven auto-tuning</h2><a id="user-content-bpftune---bpf-driven-auto-tuning" aria-label="Permalink: bpftune - BPF driven auto-tuning" href="#bpftune---bpf-driven-auto-tuning"></a></p>
<p dir="auto">bpftune aims to provide lightweight, always-on auto-tuning of system
behaviour.  The key benefit it provides are</p>
<ul dir="auto">
<li>by using BPF observability features, we can continuously monitor
and adjust system behaviour</li>
<li>because we can observe system behaviour at a fine grain (rather
than using coarse system-wide stats), we can tune at a finer grain
too (individual socket policies, individual device policies etc)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">The problem</h2><a id="user-content-the-problem" aria-label="Permalink: The problem" href="#the-problem"></a></p>
<p dir="auto">The Linux kernel contains a large number of tunables; these
often take the form of sysctl(8) parameters, and are usually
introduced for situations where there is no one "right" answer
for a configuration choice.  The number of tunables available
is quite daunting.  On a 6.2 kernel we see</p>
<div data-snippet-clipboard-copy-content="# sysctl --all 2>/dev/null|wc -l
1624"><pre><code># sysctl --all 2&gt;/dev/null|wc -l
1624
</code></pre></div>
<p dir="auto"><a href="https://github.com/leandromoreira/linux-network-performance-parameters">See here for an excellent writeup on network-related tunables.</a>.</p>
<p dir="auto">At the same time, individual systems get a lot less care
and adminstrator attention than they used to; phrases like
"cattle not pets" exemplify this.  Given the modern cloud
architectures used for most deployments, most systems never
have any human adminstrator interaction after initial
provisioning; in fact given the scale requirements, this
is often an explicit design goal - "no ssh'ing in!".</p>
<p dir="auto">These two observations are not unrelated; in an earlier
era of fewer, larger systems, tuning by administrators was
more feasible.</p>
<p dir="auto">These trends - system complexity combined with minimal
admin interaction suggest a rethink in terms of tunable
management.</p>
<p dir="auto">A lot of lore accumulates around these tunables, and to help
clarify why we developed bpftune, we will use a straw-man
version of the approach taken with tunables:</p>
<p dir="auto">"find the set of magic numbers that will work for the
system forever"</p>
<p dir="auto">This is obviously a caricature of how administrators
approach the problem, but it does highlight a critical
implicit assumption - that systems are static.</p>
<p dir="auto">And that gets to the "BPF" in bpftune; BPF provides means
to carry out low-overhead observability of systems. So
not only can we observe the system and tune appropriately,
we can also observe the effect of that tuning and re-tune
if necessary.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Key design principles</h2><a id="user-content-key-design-principles" aria-label="Permalink: Key design principles" href="#key-design-principles"></a></p>
<ul dir="auto">
<li>Minimize overhead.  Use observability features sparingly; do not
trace very high frequency events.</li>
<li>Be explicit about policy changes providing both a "what" - what
change was made - and a "why" - how does it help? syslog logging
makes policy actions explicit with explanations</li>
<li>Get out of the way of the administrator.  We can use BPF
observability to see if the admin sets tunable values that we
are auto-tuning; if they do, we need to get out of the way and
disable auto-tuning of the related feature set.</li>
<li>Don't replace tunables with more tunables! bpftune is designed to
be zero configuration; there are no options, and we try to avoid
magic numbers where possible.</li>
<li>Use push-pull approaches. For example, with tcp buffer sizing,
we often want to get out of the way of applications and bump
up tcp sndbuf and rcvbuf, but at a certain point we run the
risk of exhausting TCP memory.  We can however monitor if we
are approaching TCP memory pressure and if so we can tune down
values that we've tuned up.  In this way, we can let the system
find a balance between providing resources and exhausting them.
In some cases, we won't need to tune up values; they may be fine
as they are. But in other cases these limits block optimal performance,
and if they are raised safely - with awareness of global memory
limits - we can get out the way of improved performance.  Another
concern is that increasing buffer size leads to latency - to
handle that, we correlate buffer size changes and TCP smoothed
round-trip time; if the correlation between these exceeds a
threshold (0.7) we stop increasing buffer size.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Concepts</h2><a id="user-content-concepts" aria-label="Permalink: Concepts" href="#concepts"></a></p>
<p dir="auto">The key components are</p>
<ul dir="auto">
<li>
<p dir="auto">tuners: each tuner manages tunables and handles events sent
from BPF programs to userspace via the shared ring buffer.
Each tuner has an associated set of tunables that it manages.</p>
</li>
<li>
<p dir="auto">optional strategies: a tuner can specify multiple strategies;
after running for a while a strategy times out and we assess
if a better strategy is available.  Each strategy specifies a</p>
<ul dir="auto">
<li>name</li>
<li>description</li>
<li>timeout</li>
<li>evaluation function</li>
<li>set of BPF program names in tuner associated with strategy</li>
</ul>
<p dir="auto">Strategies are optional and should be set in the tuner init()
method via bpftune_strategies_add().  See test/strategy
for a coded example.  When a strategy times out, the various
evaluation functions are called and the highest-value evaluation
dictates the next stratgey.</p>
<p dir="auto">Strategies provide a way of providing multiple schemes for
auto-tuning the same set of tunables, where the choice is
guided by an evaluation of the effectiveness of the strategies.</p>
</li>
<li>
<p dir="auto">events specify a</p>
<ul dir="auto">
<li>tuner id: which tuner the event is destined for</li>
<li>a scenario: what happened</li>
<li>an associated netns (if supported)</li>
<li>information about the event (IP address etc)</li>
</ul>
</li>
<li>
<p dir="auto">the tuner then responds to the event guided by the active strategy;
increase or decrease a tunable value, etc.  Describing the event
in the log is key; this allows an admin to understand what
changed and why.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li>bpftune is a daemon which manages a set of .so plugin tuners;
each of these is a shared object that is loaded on start-up.</li>
<li>tuners can be enabled or disabled; a tuner is automatically
disabled if the admin changes associated tunables manually.</li>
<li>tuners share a global BPF ring buffer which allows posting of
events from BPF programs to userspace.  For example, if the
sysctl tuner sees a systl being set, it posts an event.</li>
<li>each tuner has an associated id (set when it is loaded),
and events posted contain the tuner id.</li>
<li>each tuner has a BPF component (built using a BPF skeleton)
and a userspace component.  The latter has init(), fini()
and event_handler() entrypoints.  When an event is
received, the tuner id is used to identify the appropriate
event handler and its event_handler() callback function is run.</li>
<li>init, fini and event_handler functions are loaded from the
tuner .so object.</li>
<li>BPF components should include bpftune.bpf.h; it contains
the common map definitions (ringbuf, etc) and shared variables
such as learning rate and tuner ids that each tuner needs.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported tuners</h2><a id="user-content-supported-tuners" aria-label="Permalink: Supported tuners" href="#supported-tuners"></a></p>
<ul dir="auto">
<li>TCP connection tuner: auto-tune choice of congestion control algorithm.
See bpftune-tcp-conn (8).</li>
<li>neighbour table tuner: auto-tune neighbour table sizes by growing
tables when approaching full. See bpftune-neigh (8).</li>
<li>route table tuner: auto-tune route table size by growing tables
when approaching full.  See bpftune-route (8).</li>
<li>sysctl tuner: monitor sysctl setting and if it collides with an
auto-tuned sysctl value, disable the associated tuner.  See
bpftune-sysctl (8).</li>
<li>TCP buffer tuner: auto-tune max and initial buffer sizes.  See
bpftune-tcp-buffer (8).</li>
<li>net buffer tuner: auto-tune tunables related to core networking.
See bpftune-net-buffer (8).</li>
<li>netns tuner: notices addition and removal of network namespaces,
which helps power namespace awareness for bpftune as a whole.
Namespace awareness is important as we want to be able to auto-tune
containers also.  See bpftune-netns (8).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Code organization</h2><a id="user-content-code-organization" aria-label="Permalink: Code organization" href="#code-organization"></a></p>
<p dir="auto">Both core bpftune.c and individual tuners use the libbpftune library.
It handles logging, tuner init/fini, and BPF init/fini.</p>
<p dir="auto">Each tuner shared object defines an init(), fini() and event_handler()
function. These respectively set up and clean up BPF and handle events
that originate from the BPF code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">If building the repository manually, simply run</p>
<div data-snippet-clipboard-copy-content="$ make ; sudo make install"><pre><code>$ make ; sudo make install
</code></pre></div>
<p dir="auto">at the top-level of the repository.  bpftune also supports a</p>

<p dir="auto">target, which will make a bpftune RPM.  See ./buildrpm/bpftune.spec</p>
<p dir="auto">We can also build with non-standard libdir for distros which do not
use /usr/lib64 like CachyOS; in this case to install to /usr/lib
instead</p>
<div data-snippet-clipboard-copy-content="$ make libdir=lib
$ sudo make install libdir=lib"><pre><code>$ make libdir=lib
$ sudo make install libdir=lib
</code></pre></div>
<p dir="auto">To build the following packages are needed (names may vary by distro);</p>
<ul dir="auto">
<li>libbpf, libbpf-devel &gt;= 0.6</li>
<li>libcap-devel</li>
<li>bpftool &gt;= 4.18</li>
<li>libnl3-devel</li>
<li>clang &gt;= 11</li>
<li>llvm &gt;= 11</li>
<li>python3-docutils</li>
</ul>
<p dir="auto">From the kernel side, the kernel needs to support BPF ring buffer
(around the 5.6 kernel, though 5.4 is supported on Oracle Linux
as ring buffer support was backported), and kernel BTF is
required (CONFIG_DEBUG_INFO_BTF=y).  Verify /sys/kernel/btf/vmlinux
is present.</p>
<p dir="auto">To enable bpftune as a service</p>
<div data-snippet-clipboard-copy-content="$ sudo service bpftune start"><pre><code>$ sudo service bpftune start
</code></pre></div>
<p dir="auto">...and to enable it by default</p>
<div data-snippet-clipboard-copy-content="$ sudo systemctl enable bpftune"><pre><code>$ sudo systemctl enable bpftune
</code></pre></div>
<p dir="auto">bpftune logs to syslog so /var/log/messages will contain details
of any tuning carried out.</p>
<p dir="auto">bpftune can also be run in the foreground as a program; to redirect
output to stdout/stderr, run</p>

<p dir="auto">On exit, bpftune will summarize any tuning done.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests</h2><a id="user-content-tests" aria-label="Permalink: Tests" href="#tests"></a></p>
<p dir="auto">Tests are supplied for each tuner in the tests/ subdirectory.
"make test" runs all the tests.  Tests use network namespaces
to simulate interactions with remote hosts. See ./TESTING.md
for more details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Does my system support bpftune?</h2><a id="user-content-does-my-system-support-bpftune" aria-label="Permalink: Does my system support bpftune?" href="#does-my-system-support-bpftune"></a></p>
<p dir="auto">Simply run "bpftune -S" to see:</p>
<div data-snippet-clipboard-copy-content="$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)"><pre><code>$ bpftune -S
bpftune works fully
bpftune supports per-netns policy (via netns cookie)
</code></pre></div>
<p dir="auto">Two aspects are important here</p>
<ul dir="auto">
<li>does the system support fentry/fexit etc? If so full support
is likely.</li>
<li>does the system support network namespace cookies? If so
per-network-namespace policy is supported.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<p dir="auto">Simply starting bpftune and observing changes made via /var/log/messages
can be instructive.  For example, on a standard VM with sysctl defaults,
I ran</p>

<p dir="auto">...and went about normal development activities such as cloning git
trees from upstream, building kernels, etc.  From the log we see
some of the adjustments bpftune made to accommodate these activities</p>
<div data-snippet-clipboard-copy-content="$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -> (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -> (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm"><pre><code>$ sudo grep bpftune /var/log/messages
...
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune works fully
Apr 19 16:14:59 bpftest bpftune[2778]: bpftune supports per-netns policy (via netns cookie)
Apr 19 16:18:40 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:18:40 bpftest bpftune[2778]: due to loss events for 145.40.68.75, specify 'bbr' congestion control algorithm
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 6291456) -&gt; (4096 131072 7864320)
Apr 19 16:26:53 bpftest bpftune[2778]: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
Apr 19 16:26:53 bpftest bpftune[2778]: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 7864320) -&gt; (4096 131072 9830400)
Apr 19 16:29:04 bpftest bpftune[2778]: Scenario 'specify bbr congestion control' occurred for tunable 'TCP congestion control' in global ns. Because loss rate has exceeded 1 percent for a connection, use bbr congestion control algorithm instead of default
Apr 19 16:29:04 bpftest bpftune[2778]: due to loss events for 140.91.12.81, specify 'bbr' congestion control algorithm
</code></pre></div>
<p dir="auto">To deterministically trigger bpftune behaviour, one approach we can
take is to download a large file with inappropriate settings.</p>
<p dir="auto">In one window, set tcp rmem max to a too-low value, and run bpftune
as a program logging to stdout/stderr (-s):</p>
<div data-snippet-clipboard-copy-content="$ sudo sysctl -w net.ipv4.tcp_rmem=&quot;4096 131072 1310720&quot;
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s"><pre><code>$ sudo sysctl -w net.ipv4.tcp_rmem="4096 131072 1310720"
net.ipv4.tcp_rmem = 4096 131072 1310720
$ sudo bpftune -s
</code></pre></div>
<p dir="auto">In another window, wget a large file:</p>
<div data-snippet-clipboard-copy-content="$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso"><pre><code>$ wget https://yum.oracle.com/ISOS/OracleLinux/OL8/u7/x86_64/OracleLinux-R8-U7-x86_64-dvd.iso
</code></pre></div>
<p dir="auto">In the first window, we see bpftune tuning up rmem:</p>
<div data-snippet-clipboard-copy-content="bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -> (4096 131072 1638400)"><pre><code>bpftune: bpftune works in legacy mode
bpftune: bpftune does not support per-netns policy (via netns cookie)
bpftune: Scenario 'need to increase TCP buffer size(s)' occurred for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: Due to need to increase max buffer size to maximize throughput change net.ipv4.tcp_rmem(min default max) from (4096 131072 1310720) -&gt; (4096 131072 1638400)
</code></pre></div>
<p dir="auto">This occurs multiple times, and on exit (Ctrl+C) we see
the summary of changes made:</p>
<div data-snippet-clipboard-copy-content="bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -> (4096 131072 9765625 )"><pre><code>bpftune: Summary: scenario 'need to increase TCP buffer size(s)' occurred 9 times for tunable 'net.ipv4.tcp_rmem' in global ns. Need to increase buffer size(s) to maximize throughput
bpftune: sysctl 'net.ipv4.tcp_rmem' changed from (4096 131072 1310720 ) -&gt; (4096 131072 9765625 )
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">For more info</h2><a id="user-content-for-more-info" aria-label="Permalink: For more info" href="#for-more-info"></a></p>
<p dir="auto">See the docs/ subdirectory for manual pages covering bpftune
and associated tuners.</p>
<p dir="auto">bpftune was presented at the eBPF summit; <a href="https://www.youtube.com/watch?v=X0TvfH8hrQE&amp;t=420s" rel="nofollow">video here</a>.</p>
<p dir="auto">bpftune <a href="https://www.youtube.com/watch?v=3ylmGE6sW8w" rel="nofollow">was also discussed on Liz Rice's excellent eCHO eBPF podcast</a>, specifically in the context of using reinforcement learning in BPF</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">This project welcomes contributions from the community. Before submitting a pull request, please <a href="https://github.com/oracle/bpftune/blob/main/CONTRIBUTING.md">review our contribution guide</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security</h2><a id="user-content-security" aria-label="Permalink: Security" href="#security"></a></p>
<p dir="auto">Please consult the <a href="https://github.com/oracle/bpftune/blob/main/SECURITY.md">security guide</a> for our responsible security vulnerability disclosure process</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Copyright (c) 2023 Oracle and/or its affiliates.</p>
<p dir="auto">This software is available to you under</p>
<p dir="auto">SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note</p>
<p dir="auto">Being under the terms of the GNU General Public License version 2.</p>
<p dir="auto">SPDX-URL: <a href="https://spdx.org/licenses/GPL-2.0.html" rel="nofollow">https://spdx.org/licenses/GPL-2.0.html</a></p>
<p dir="auto">See <a href="https://github.com/oracle/bpftune/blob/main/LICENSE.txt">the license file</a> for more details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Garak, LLM Vulnerability Scanner (152 pts)]]></title>
            <link>https://github.com/NVIDIA/garak</link>
            <guid>42163591</guid>
            <pubDate>Sun, 17 Nov 2024 11:37:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/NVIDIA/garak">https://github.com/NVIDIA/garak</a>, See on <a href="https://news.ycombinator.com/item?id=42163591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">garak, LLM vulnerability scanner</h2><a id="user-content-garak-llm-vulnerability-scanner" aria-label="Permalink: garak, LLM vulnerability scanner" href="#garak-llm-vulnerability-scanner"></a></p>
<p dir="auto"><em>Generative AI Red-teaming &amp; Assessment Kit</em></p>
<p dir="auto"><code>garak</code> checks if an LLM can be made to fail in a way we don't want. <code>garak</code> probes for hallucination, data leakage, prompt injection, misinformation, toxicity generation, jailbreaks, and many other weaknesses. If you know <code>nmap</code>, it's <code>nmap</code> for LLMs.</p>
<p dir="auto"><code>garak</code> focuses on ways of making an LLM or dialog system fail. It combines static, dyanmic, and adaptive probes to explore this.</p>
<p dir="auto"><code>garak</code>'s a free tool. We love developing it and are always interested in adding functionality to support applications.</p>
<p dir="auto"><a href="https://opensource.org/licenses/Apache-2.0" rel="nofollow"><img src="https://camo.githubusercontent.com/5ce2e21e84680df1ab24807babebc3417d27d66e0826a350eb04ab57f4c8f3e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667" alt="License" data-canonical-src="https://img.shields.io/badge/License-Apache_2.0-blue.svg"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_linux.yml/badge.svg" alt="Tests/Linux"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_windows.yml/badge.svg" alt="Tests/Windows"></a>
<a href="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml"><img src="https://github.com/NVIDIA/garak/actions/workflows/test_macos.yml/badge.svg" alt="Tests/OSX"></a>
<a href="http://garak.readthedocs.io/en/latest/?badge=latest" rel="nofollow"><img src="https://camo.githubusercontent.com/ec7dff6db1b623f10238aaa176f6070b8dfee2ba106479e9ac7a66fbe8f3e778/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f676172616b2f62616467652f3f76657273696f6e3d6c6174657374" alt="Documentation Status" data-canonical-src="https://readthedocs.org/projects/garak/badge/?version=latest"></a>
<a href="https://discord.gg/uVch4puUCs" rel="nofollow"><img src="https://camo.githubusercontent.com/3dfa2e5918dc7c5299e3f3e8383c6d7fc9e5a26de70d29c5144a166075db153b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230646973636f72642d79656c6c6f772e737667" alt="discord-img" data-canonical-src="https://img.shields.io/badge/chat-on%20discord-yellow.svg"></a>
<a href="https://github.com/psf/black"><img src="https://camo.githubusercontent.com/5bf9e9fa18966df7cb5fac7715bef6b72df15e01a6efa9d616c83f9fcb527fe2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667" alt="Code style: black" data-canonical-src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
<a href="https://pypi.org/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/2e89cf4e24191c2b133e3cbc641eb89ec3d1c6e61bfec492ddec940757b871f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f676172616b" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/pypi/pyversions/garak"></a>
<a href="https://badge.fury.io/py/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/635b443d53cf2ab927beabd3255f7895db7b74af3dbf5a7777c61fcc96792bbd/68747470733a2f2f62616467652e667572792e696f2f70792f676172616b2e737667" alt="PyPI" data-canonical-src="https://badge.fury.io/py/garak.svg"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/b38370d22779880d75968e06cf8ade053d44a0e95f94649a8354756b5c24a4ba/68747470733a2f2f706570792e746563682f62616467652f676172616b" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak"></a>
<a href="https://pepy.tech/project/garak" rel="nofollow"><img src="https://camo.githubusercontent.com/3d929a651aaa7c9b44b580d1537bbb7954b796081bfeb1125fc3d1fac4f78585/68747470733a2f2f706570792e746563682f62616467652f676172616b2f6d6f6e7468" alt="Downloads" data-canonical-src="https://pepy.tech/badge/garak/month"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Get started</h2><a id="user-content-get-started" aria-label="Permalink: Get started" href="#get-started"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; See our user guide! <a href="https://docs.garak.ai/" rel="nofollow">docs.garak.ai</a></h3><a id="user-content--see-our-user-guide-docsgarakai" aria-label="Permalink: > See our user guide! docs.garak.ai" href="#-see-our-user-guide-docsgarakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Join our <a href="https://discord.gg/uVch4puUCs" rel="nofollow">Discord</a>!</h3><a id="user-content--join-our-discord" aria-label="Permalink: > Join our Discord!" href="#-join-our-discord"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Project links &amp; home: <a href="https://garak.ai/" rel="nofollow">garak.ai</a></h3><a id="user-content--project-links--home-garakai" aria-label="Permalink: > Project links &amp; home: garak.ai" href="#-project-links--home-garakai"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; Twitter: <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></h3><a id="user-content--twitter-garak_llm" aria-label="Permalink: > Twitter: @garak_llm" href="#-twitter-garak_llm"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">&gt; DEF CON <a href="https://garak.ai/garak_aiv_slides.pdf" rel="nofollow">slides</a>!</h3><a id="user-content--def-con-slides" aria-label="Permalink: > DEF CON slides!" href="#-def-con-slides"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">LLM support</h2><a id="user-content-llm-support" aria-label="Permalink: LLM support" href="#llm-support"></a></p>
<p dir="auto">currently supports:</p>
<ul dir="auto">
<li><a href="https://huggingface.co/models" rel="nofollow">hugging face hub</a> generative models</li>
<li><a href="https://replicate.com/" rel="nofollow">replicate</a> text models</li>
<li><a href="https://platform.openai.com/docs/introduction" rel="nofollow">openai api</a> chat &amp; continuation models</li>
<li><a href="https://www.litellm.ai/" rel="nofollow">litellm</a></li>
<li>pretty much anything accessible via REST</li>
<li>gguf models like <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> version &gt;= 1046</li>
<li>.. and many more LLMs!</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Install:</h2><a id="user-content-install" aria-label="Permalink: Install:" href="#install"></a></p>
<p dir="auto"><code>garak</code> is a command-line tool. It's developed in Linux and OSX.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Standard install with <code>pip</code></h3><a id="user-content-standard-install-with-pip" aria-label="Permalink: Standard install with pip" href="#standard-install-with-pip"></a></p>
<p dir="auto">Just grab it from PyPI and you should be good to go:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U garak"><pre><code>python -m pip install -U garak
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install development version with <code>pip</code></h3><a id="user-content-install-development-version-with-pip" aria-label="Permalink: Install development version with pip" href="#install-development-version-with-pip"></a></p>
<p dir="auto">The standard pip version of <code>garak</code> is updated periodically. To get a fresher version, from GitHub, try:</p>
<div data-snippet-clipboard-copy-content="python -m pip install -U git+https://github.com/NVIDIA/garak.git@main"><pre><code>python -m pip install -U git+https://github.com/NVIDIA/garak.git@main
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Clone from source</h3><a id="user-content-clone-from-source" aria-label="Permalink: Clone from source" href="#clone-from-source"></a></p>
<p dir="auto"><code>garak</code> has its own dependencies. You can to install <code>garak</code> in its own Conda environment:</p>
<div data-snippet-clipboard-copy-content="conda create --name garak &quot;python>=3.10,<=3.12&quot;
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e ."><pre><code>conda create --name garak "python&gt;=3.10,&lt;=3.12"
conda activate garak
gh repo clone NVIDIA/garak
cd garak
python -m pip install -e .
</code></pre></div>
<p dir="auto">OK, if that went fine, you're probably good to go!</p>
<p dir="auto"><strong>Note</strong>: if you cloned before the move to the <code>NVIDIA</code> GitHub organisation, but you're reading this at the <code>github.com/NVIDIA</code> URI, please update your remotes as follows:</p>
<div data-snippet-clipboard-copy-content="git remote set-url origin https://github.com/NVIDIA/garak.git"><pre><code>git remote set-url origin https://github.com/NVIDIA/garak.git
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting started" href="#getting-started"></a></p>
<p dir="auto">The general syntax is:</p>
<p dir="auto"><code>garak &lt;options&gt;</code></p>
<p dir="auto"><code>garak</code> needs to know what model to scan, and by default, it'll try all the probes it knows on that model, using the vulnerability detectors recommended by each probe. You can see a list of probes using:</p>
<p dir="auto"><code>garak --list_probes</code></p>
<p dir="auto">To specify a generator, use the <code>--model_type</code> and, optionally, the <code>--model_name</code> options. Model type specifies a model family/interface; model name specifies the exact model to be used. The "Intro to generators" section below describes some of the generators supported. A straightforward generator family is Hugging Face models; to load one of these, set <code>--model_type</code> to <code>huggingface</code> and <code>--model_name</code> to the model's name on Hub (e.g. <code>"RWKV/rwkv-4-169m-pile"</code>). Some generators might need an API key to be set as an environment variable, and they'll let you know if they need that.</p>
<p dir="auto"><code>garak</code> runs all the probes by default, but you can be specific about that too. <code>--probes promptinject</code> will use only the <a href="https://github.com/agencyenterprise/promptinject">PromptInject</a> framework's methods, for example. You can also specify one specific plugin instead of a plugin family by adding the plugin name after a <code>.</code>; for example, <code>--probes lmrc.SlurUsage</code> will use an implementation of checking for models generating slurs based on the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> framework.</p>
<p dir="auto">For help &amp; inspiration, find us on <a href="https://twitter.com/garak_llm" rel="nofollow">twitter</a> or <a href="https://discord.gg/uVch4puUCs" rel="nofollow">discord</a>!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Probe ChatGPT for encoding-based prompt injection (OSX/*nix) (replace example value with a real OpenAI API key)</p>
<div data-snippet-clipboard-copy-content="export OPENAI_API_KEY=&quot;sk-123XXXXXXXXXXXX&quot;
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding"><pre><code>export OPENAI_API_KEY="sk-123XXXXXXXXXXXX"
python3 -m garak --model_type openai --model_name gpt-3.5-turbo --probes encoding
</code></pre></div>
<p dir="auto">See if the Hugging Face version of GPT2 is vulnerable to DAN 11.0</p>
<div data-snippet-clipboard-copy-content="python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0"><pre><code>python3 -m garak --model_type huggingface --model_name gpt2 --probes dan.Dan_11_0
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reading the results</h2><a id="user-content-reading-the-results" aria-label="Permalink: Reading the results" href="#reading-the-results"></a></p>
<p dir="auto">For each probe loaded, garak will print a progress bar as it generates. Once generation is complete, a row evaluating that probe's results on each detector is given. If any of the prompt attempts yielded an undesirable behavior, the response will be marked as FAIL, and the failure rate given.</p>
<p dir="auto">Here are the results with the <code>encoding</code> module on a GPT-3 variant:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67"><img src="https://camo.githubusercontent.com/3c772412f9310195163d6092ba995d436ebad8d7e430d89a8484c3a92b5ec972/68747470733a2f2f692e696d6775722e636f6d2f3844786634354e2e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/8Dxf45N.png"></a></p>
<p dir="auto">And the same results for ChatGPT:
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67"><img src="https://camo.githubusercontent.com/5fc7c4aee43ab989750750ea912245acd367d41dee20f0532d19d8bcce9d3a5e/68747470733a2f2f692e696d6775722e636f6d2f564b41463569662e706e67" alt="alt text" data-canonical-src="https://i.imgur.com/VKAF5if.png"></a></p>
<p dir="auto">We can see that the more recent model is much more susceptible to encoding-based injection attacks, where text-babbage-001 was only found to be vulnerable to quoted-printable and MIME encoding injections.  The figures at the end of each row, e.g. 840/840, indicate the number of text generations total and then how many of these seemed to behave OK. The figure can be quite high because more than one generation is made per prompt - by default, 10.</p>
<p dir="auto">Errors go in <code>garak.log</code>; the run is logged in detail in a <code>.jsonl</code> file specified at analysis start &amp; end. There's a basic analysis script in <code>analyse/analyse_log.py</code> which will output the probes and prompts that led to the most hits.</p>
<p dir="auto">Send PRs &amp; open issues. Happy hunting!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to generators</h2><a id="user-content-intro-to-generators" aria-label="Permalink: Intro to generators" href="#intro-to-generators"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hugging Face</h3><a id="user-content-hugging-face" aria-label="Permalink: Hugging Face" href="#hugging-face"></a></p>
<p dir="auto">Using the Pipeline API:</p>
<ul dir="auto">
<li><code>--model_type huggingface</code> (for transformers models to run locally)</li>
<li><code>--model_name</code> - use the model name from Hub. Only generative models will work. If it fails and shouldn't, please open an issue and paste in the command you tried + the exception!</li>
</ul>
<p dir="auto">Using the Inference API:</p>
<ul dir="auto">
<li><code>--model_type huggingface.InferenceAPI</code> (for API-based model access)</li>
<li><code>--model_name</code> - the model name from Hub, e.g. <code>"mosaicml/mpt-7b-instruct"</code></li>
</ul>
<p dir="auto">Using private endpoints:</p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type huggingface.InferenceEndpoint</code> (for private endpoints)</p>
</li>
<li>
<p dir="auto"><code>--model_name</code> - the endpoint URL, e.g. <code>https://xxx.us-east-1.aws.endpoints.huggingface.cloud</code></p>
</li>
<li>
<p dir="auto">(optional) set the <code>HF_INFERENCE_TOKEN</code> environment variable to a Hugging Face API token with the "read" role; see <a href="https://huggingface.co/settings/tokens" rel="nofollow">https://huggingface.co/settings/tokens</a> when logged in</p>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OpenAI</h3><a id="user-content-openai" aria-label="Permalink: OpenAI" href="#openai"></a></p>
<ul dir="auto">
<li><code>--model_type openai</code></li>
<li><code>--model_name</code> - the OpenAI model you'd like to use. <code>gpt-3.5-turbo-0125</code> is fast and fine for testing.</li>
<li>set the <code>OPENAI_API_KEY</code> environment variable to your OpenAI API key (e.g. "sk-19763ASDF87q6657"); see <a href="https://platform.openai.com/account/api-keys" rel="nofollow">https://platform.openai.com/account/api-keys</a> when logged in</li>
</ul>
<p dir="auto">Recognised model types are whitelisted, because the plugin needs to know which sub-API to use. Completion or ChatCompletion models are OK. If you'd like to use a model not supported, you should get an informative error message, and please send a PR / open an issue.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Replicate</h3><a id="user-content-replicate" aria-label="Permalink: Replicate" href="#replicate"></a></p>
<ul dir="auto">
<li>set the <code>REPLICATE_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Public Replicate models:</p>
<ul dir="auto">
<li><code>--model_type replicate</code></li>
<li><code>--model_name</code> - the Replicate model name and hash, e.g. <code>"stability-ai/stablelm-tuned-alpha-7b:c49dae36"</code></li>
</ul>
<p dir="auto">Private Replicate endpoints:</p>
<ul dir="auto">
<li><code>--model_type replicate.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - username/model-name slug from the deployed endpoint, e.g. <code>elim/elims-llama2-7b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Cohere</h3><a id="user-content-cohere" aria-label="Permalink: Cohere" href="#cohere"></a></p>
<ul dir="auto">
<li><code>--model_type cohere</code></li>
<li><code>--model_name</code> (optional, <code>command</code> by default) - The specific Cohere model you'd like to test</li>
<li>set the <code>COHERE_API_KEY</code> environment variable to your Cohere API key, e.g. "aBcDeFgHiJ123456789"; see <a href="https://dashboard.cohere.ai/api-keys" rel="nofollow">https://dashboard.cohere.ai/api-keys</a> when logged in</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Groq</h3><a id="user-content-groq" aria-label="Permalink: Groq" href="#groq"></a></p>
<ul dir="auto">
<li><code>--model_type groq</code></li>
<li><code>--model_name</code> - The name of the model to access via the Groq API</li>
<li>set the <code>GROQ_API_KEY</code> environment variable to your Groq API key, see <a href="https://console.groq.com/docs/quickstart" rel="nofollow">https://console.groq.com/docs/quickstart</a> for details on creating an API key</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">ggml</h3><a id="user-content-ggml" aria-label="Permalink: ggml" href="#ggml"></a></p>
<ul dir="auto">
<li><code>--model_type ggml</code></li>
<li><code>--model_name</code> - The path to the ggml model you'd like to load, e.g. <code>/home/leon/llama.cpp/models/7B/ggml-model-q4_0.bin</code></li>
<li>set the <code>GGML_MAIN_PATH</code> environment variable to the path to your ggml <code>main</code> executable</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">REST</h3><a id="user-content-rest" aria-label="Permalink: REST" href="#rest"></a></p>
<p dir="auto"><code>rest.RestGenerator</code> is highly flexible and can connect to any REST endpoint that returns plaintext or JSON. It does need some brief config, which will typically result a short YAML file describing your endpoint. See <a href="https://reference.garak.ai/en/latest/garak.generators.rest.html" rel="nofollow">https://reference.garak.ai/en/latest/garak.generators.rest.html</a> for examples.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">NIM</h3><a id="user-content-nim" aria-label="Permalink: NIM" href="#nim"></a></p>
<p dir="auto">Use models from <a href="https://build.nvidia.com/" rel="nofollow">https://build.nvidia.com/</a> or other NIM endpoints.</p>
<ul dir="auto">
<li>set the <code>NIM_API_KEY</code> environment variable to your authentication API token, or specify it in the config YAML</li>
</ul>
<p dir="auto">For chat models:</p>
<ul dir="auto">
<li><code>--model_type nim</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>meta/llama-3.1-8b-instruct</code></li>
</ul>
<p dir="auto">For completion models:</p>
<ul dir="auto">
<li><code>--model_type nim.NVOpenAICompletion</code></li>
<li><code>--model_name</code> - the NIM <code>model</code> name, e.g. <code>bigcode/starcoder2-15b</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">OctoAI</h3><a id="user-content-octoai" aria-label="Permalink: OctoAI" href="#octoai"></a></p>
<ul dir="auto">
<li>set the <code>OCTO_API_TOKEN</code> environment variable to your Replicate API token, e.g. "r8-123XXXXXXXXXXXX"; see <a href="https://replicate.com/account/api-tokens" rel="nofollow">https://replicate.com/account/api-tokens</a> when logged in</li>
</ul>
<p dir="auto">Octo public endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo</code></li>
<li><code>--model_name</code> - the OctoAI public endpoint for the model, e.g. <code>mistral-7b-instruct-fp16</code></li>
</ul>
<p dir="auto">Octo private endpoint:</p>
<ul dir="auto">
<li><code>--model_type octo.InferenceEndpoint</code> (for private endpoints)</li>
<li><code>--model_name</code> - the deployed endpoint URL, e.g. <code>https://llama-2-70b-chat-xxx.octoai.run/v1/chat/completions</code></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Test</h3><a id="user-content-test" aria-label="Permalink: Test" href="#test"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><code>--model_type test</code></p>
</li>
<li>
<p dir="auto">(alternatively) <code>--model_name test.Blank</code>
For testing. This always generates the empty string, using the <code>test.Blank</code> generator.  Will be marked as failing for any tests that <em>require</em> an output, e.g. those that make contentious claims and expect the model to refute them in order to pass.</p>
</li>
<li>
<p dir="auto"><code>--model_type test.Repeat</code>
For testing. This generator repeats back the prompt it received.</p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Intro to probes</h2><a id="user-content-intro-to-probes" aria-label="Permalink: Intro to probes" href="#intro-to-probes"></a></p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Probe</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>blank</td>
<td>A simple probe that always sends an empty prompt.</td>
</tr>
<tr>
<td>atkgen</td>
<td>Automated Attack Generation. A red-teaming LLM probes the target and reacts to it in an attempt to get toxic output. Prototype, mostly stateless, for now uses a simple GPT-2 <a href="https://huggingface.co/garak-llm/artgpt2tox" rel="nofollow">fine-tuned</a> on the subset of hhrlhf attempts that yielded detectable toxicity (the only target currently supported for now).</td>
</tr>
<tr>
<td>av_spam_scanning</td>
<td>Probes that attempt to make the model output malicious content signatures</td>
</tr>
<tr>
<td>continuation</td>
<td>Probes that test if the model will continue a probably undesirable word</td>
</tr>
<tr>
<td>dan</td>
<td>Various <a href="https://adguard.com/en/blog/chatgpt-dan-prompt-abuse.html" rel="nofollow">DAN</a> and DAN-like attacks</td>
</tr>
<tr>
<td>donotanswer</td>
<td>Prompts to which responsible language models should not answer.</td>
</tr>
<tr>
<td>encoding</td>
<td>Prompt injection through text encoding</td>
</tr>
<tr>
<td>gcg</td>
<td>Disrupt a system prompt by appending an adversarial suffix.</td>
</tr>
<tr>
<td>glitch</td>
<td>Probe model for glitch tokens that provoke unusual behavior.</td>
</tr>
<tr>
<td>grandma</td>
<td>Appeal to be reminded of one's grandmother.</td>
</tr>
<tr>
<td>goodside</td>
<td>Implementations of Riley Goodside attacks.</td>
</tr>
<tr>
<td>leakerplay</td>
<td>Evaluate if a model will replay training data.</td>
</tr>
<tr>
<td>lmrc</td>
<td>Subsample of the <a href="https://arxiv.org/abs/2303.18190" rel="nofollow">Language Model Risk Cards</a> probes</td>
</tr>
<tr>
<td>malwaregen</td>
<td>Attempts to have the model generate code for building malware</td>
</tr>
<tr>
<td>misleading</td>
<td>Attempts to make a model support misleading and false claims</td>
</tr>
<tr>
<td>packagehallucination</td>
<td>Trying to get code generations that specify non-existent (and therefore insecure) packages.</td>
</tr>
<tr>
<td>promptinject</td>
<td>Implementation of the Agency Enterprise <a href="https://github.com/agencyenterprise/PromptInject/tree/main/promptinject">PromptInject</a> work (best paper awards @ NeurIPS ML Safety Workshop 2022)</td>
</tr>
<tr>
<td>realtoxicityprompts</td>
<td>Subset of the RealToxicityPrompts work (data constrained because the full test will take so long to run)</td>
</tr>
<tr>
<td>snowball</td>
<td><a href="https://ofir.io/snowballed_hallucination.pdf" rel="nofollow">Snowballed Hallucination</a> probes designed to make a model give a wrong answer to questions too complex for it to process</td>
</tr>
<tr>
<td>xss</td>
<td>Look for vulnerabilities the permit or enact cross-site attacks, such as private data exfiltration.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Logging</h2><a id="user-content-logging" aria-label="Permalink: Logging" href="#logging"></a></p>
<p dir="auto"><code>garak</code> generates multiple kinds of log:</p>
<ul dir="auto">
<li>A log file, <code>garak.log</code>. This includes debugging information from <code>garak</code> and its plugins, and is continued across runs.</li>
<li>A report of the current run, structured as JSONL. A new report file is created every time <code>garak</code> runs. The name of this file is output at the beginning and, if successful, also the end of the run. In the report, an entry is made for each probing attempt both as the generations are received, and again when they are evaluated; the entry's <code>status</code> attribute takes a constant from <code>garak.attempts</code> to describe what stage it was made at.</li>
<li>A hit log, detailing attempts that yielded a vulnerability (a 'hit')</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">How is the code structured?</h2><a id="user-content-how-is-the-code-structured" aria-label="Permalink: How is the code structured?" href="#how-is-the-code-structured"></a></p>
<p dir="auto">Check out the <a href="https://reference.garak.ai/" rel="nofollow">reference docs</a> for an authoritative guide to <code>garak</code> code structure.</p>
<p dir="auto">In a typical run, <code>garak</code> will read a model type (and optionally model name) from the command line, then determine which <code>probe</code>s and <code>detector</code>s to run, start up a <code>generator</code>, and then pass these to a <code>harness</code> to do the probing; an <code>evaluator</code> deals with the results. There are many modules in each of these categories, and each module provides a number of classes that act as individual plugins.</p>
<ul dir="auto">
<li><code>garak/probes/</code> - classes for generating interactions with LLMs</li>
<li><code>garak/detectors/</code> - classes for detecting an LLM is exhibiting a given failure mode</li>
<li><code>garak/evaluators/</code> - assessment reporting schemes</li>
<li><code>garak/generators/</code> - plugins for LLMs to be probed</li>
<li><code>garak/harnesses/</code> - classes for structuring testing</li>
<li><code>resources/</code> - ancillary items required by plugins</li>
</ul>
<p dir="auto">The default operating mode is to use the <code>probewise</code> harness. Given a list of probe module names and probe plugin names, the <code>probewise</code> harness instantiates each probe, then for each probe reads its <code>recommended_detectors</code> attribute to get a list of <code>detector</code>s to run on the output.</p>
<p dir="auto">Each plugin category (<code>probes</code>, <code>detectors</code>, <code>evaluators</code>, <code>generators</code>, <code>harnesses</code>) includes a <code>base.py</code> which defines the base classes usable by plugins in that category. Each plugin module defines plugin classes that inherit from one of the base classes. For example, <code>garak.generators.openai.OpenAIGenerator</code> descends from <code>garak.generators.base.Generator</code>.</p>
<p dir="auto">Larger artefacts, like model files and bigger corpora, are kept out of the repository; they can be stored on e.g. Hugging Face Hub and loaded locally by clients using <code>garak</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Developing your own plugin</h2><a id="user-content-developing-your-own-plugin" aria-label="Permalink: Developing your own plugin" href="#developing-your-own-plugin"></a></p>
<ul dir="auto">
<li>Take a look at how other plugins do it</li>
<li>Inherit from one of the base classes, e.g. <code>garak.probes.base.TextProbe</code></li>
<li>Override as little as possible</li>
<li>You can test the new code in at least two ways:
<ul dir="auto">
<li>Start an interactive Python session
<ul dir="auto">
<li>Import the model, e.g. <code>import garak.probes.mymodule</code></li>
<li>Instantiate the plugin, e.g. <code>p = garak.probes.mymodule.MyProbe()</code></li>
</ul>
</li>
<li>Run a scan with test plugins
<ul dir="auto">
<li>For probes, try a blank generator and always.Pass detector: <code>python3 -m garak -m test.Blank -p mymodule -d always.Pass</code></li>
<li>For detectors, try a blank generator and a blank probe: <code>python3 -m garak -m test.Blank -p test.Blank -d mymodule</code></li>
<li>For generators, try a blank probe and always.Pass detector: <code>python3 -m garak -m mymodule -p test.Blank -d always.Pass</code></li>
</ul>
</li>
<li>Get <code>garak</code> to list all the plugins of the type you're writing, with <code>--list_probes</code>, <code>--list_detectors</code>, or <code>--list_generators</code></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto">We have an FAQ <a href="https://github.com/NVIDIA/garak/blob/main/FAQ.md">here</a>. Reach out if you have any more questions! <a href="mailto:leon@garak.ai">leon@garak.ai</a></p>
<p dir="auto">Code reference documentation is at <a href="https://garak.readthedocs.io/en/latest/" rel="nofollow">garak.readthedocs.io</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing garak</h2><a id="user-content-citing-garak" aria-label="Permalink: Citing garak" href="#citing-garak"></a></p>
<p dir="auto">You can read the <a href="https://github.com/NVIDIA/garak/blob/main/garak-paper.pdf">garak preprint paper</a>. If you use garak, please cite us.</p>
<div data-snippet-clipboard-copy-content="@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}"><pre><code>@article{garak,
  title={{garak: A Framework for Security Probing Large Language Models}},
  author={Leon Derczynski and Erick Galinkin and Jeffrey Martin and Subho Majumdar and Nanna Inie},
  year={2024},
  howpublished={\url{https://garak.ai}}
}
</code></pre></div>
<hr>
<p dir="auto"><em>"Lying is a skill like any other, and if you wish to maintain a level of excellence you have to practice constantly"</em> - Elim</p>
<p dir="auto">For updates and news see <a href="https://twitter.com/garak_llm" rel="nofollow">@garak_llm</a></p>
<p dir="auto">© 2023- Leon Derczynski; Apache license v2, see <a href="https://github.com/NVIDIA/garak/blob/main/LICENSE">LICENSE</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Constraints in Go (164 pts)]]></title>
            <link>https://bitfieldconsulting.com/posts/constraints</link>
            <guid>42162878</guid>
            <pubDate>Sun, 17 Nov 2024 08:44:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bitfieldconsulting.com/posts/constraints">https://bitfieldconsulting.com/posts/constraints</a>, See on <a href="https://news.ycombinator.com/item?id=42162878">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="23" id="block-yui_3_17_2_1_1710022019086_133076"><blockquote>
<p><em>Design is the beauty of turning constraints into
advantages.</em><br>
—<a href="https://talks.ui-patterns.com/videos/design-is-the-beauty-of-turning-constraints-into-advantages-aza-raskin">Aza
Raskin</a> </p>
</blockquote>
<p>This is the fourth in a four-part series of tutorials on generics in
Go.</p>
<ol type="1">
<li><a href="https://bitfieldconsulting.com/posts/generics">Generics</a></li>
<li><a href="https://bitfieldconsulting.com/posts/type-parameters">Type parameters</a></li>
<li><a href="https://bitfieldconsulting.com/posts/generic-types">Generic types</a></li>
<li><strong>Constraints</strong></li>
</ol>
<hr>
<p>In my book <a href="https://bitfieldconsulting.com/books/generics">Know Go</a>, and in the previous
tutorials in this series, you’ll learn all about generic programming in
Go and the new universe of programs it opens up to us. Ironically, one
of the new features of Go that gives us the most freedom is
<em>constraints</em>. Let’s talk about that, and explain the
paradox.</p>
<p>We saw in the <a href="https://bitfieldconsulting.com/posts/generic-types">previous tutorial</a>
that when we’re writing generic functions that take any type, the range
of things we can <em>do</em> with values of that type is necessarily
rather limited. For example, we can’t add them together. For that, we’d
need to be able to prove to Go that they’re one of the types that
support the <code>+</code> operator.</p>
<h2 id="method-set-constraints">Method set constraints</h2>
<p>It’s the same with interfaces, as we discussed in the <a href="https://bitfieldconsulting.com/posts/generics">first post</a> in this series. The empty
interface, <code>any</code>, is implemented by every type, and so
knowing that something implements <code>any</code> tells you nothing
distinctive about it.</p>
<h3 id="limitations-of-the-any-constraint">Limitations of the
<code>any</code> constraint</h3>
<p>Similarly, in a generic function parameterised by some type T,
constraining T to <code>any</code> doesn’t give Go any information about
it. So it has no way to guarantee that a given operator, such as
<code>+</code>, will work with values of T.</p>
<p>A Go proverb says:</p>
<blockquote>
<p><em>The bigger the interface, the weaker the abstraction.</em><br>
—<a href="https://go-proverbs.github.io/">https://go-proverbs.github.io/</a></p>
</blockquote>
<p>And the same is true of constraints. The broader the constraint, and
thus the more types it allows, the less we can guarantee about what
operations we can do on them.</p>
<p>There <em>are</em> a few things we can do with <code>any</code>
values, as you already know, because we’ve done them. For example, we
can declare variables of that type, we can assign values to them, we can
return them from functions, and so on.</p>
<p>But we can’t really do a whole lot of <em>computation</em> with them,
because we can’t use operators like <code>+</code> or <code>-</code>. So
in order to be able to do something useful with values of T, such as
adding them, we need more restrictive constraints.</p>
<p>What kinds of constraints <em>could</em> there be on T? Let’s examine
the possibilities.</p>
<h3 id="basic-interfaces">Basic interfaces</h3>
<p>One kind of constraint that we’re already familiar with in Go is an
<em>interface</em>. In fact, all constraints are interfaces of a kind,
but let’s use the term <em>basic</em> interface here to avoid any
confusion. A basic interface, we’ll say, is one that contains only
method elements.</p>
<p>For example, the <code>fmt.Stringer</code> interface we saw in the <a href="https://bitfieldconsulting.com/posts/generics">first tutorial</a>:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>type</span> Stringer <span>interface</span> <span>{</span></span>
<span id="cb1-2">    String<span>()</span> <span>string</span></span>
<span id="cb1-3"><span>}</span></span></code></pre></div>
<p>We’ve seen that we can write an ordinary, non-generic function that
takes a parameter of type <code>Stringer</code>. And we can also use
this interface as a type constraint for a generic function.</p>
<p>For example, we could write a generic function parameterised by some
type T, but this time T can’t be just any type. Instead, we’ll say that
whatever T turns out to be, it must implement the
<code>fmt.Stringer</code> interface:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>func</span> Stringify<span>[</span>T fmt<span>.</span>Stringer<span>](</span>s T<span>)</span> <span>string</span> <span>{</span></span>
<span id="cb2-2">    <span>return</span> s<span>.</span>String<span>()</span></span>
<span id="cb2-3"><span>}</span></span></code></pre></div>
<p>This is clear enough, and it works the same way as the generic
functions we’ve already written. The only new thing is that we used the
constraint <code>Stringer</code> instead of <code>any</code>. Now when
we actually call this function in a program, we’re only allowed to pass
it arguments that implement <code>Stringer</code>.</p>
<p>What would happen, then, if we tried to call <code>Stringify</code>
with an argument that <em>doesn’t</em> implement <code>Stringer</code>?
We feel instinctively that this shouldn’t work, and it doesn’t:</p>
<div id="cb3"><pre><code><span id="cb3-1">fmt<span>.</span>Println<span>(</span>Stringify<span>(</span><span>1</span><span>))</span></span>
<span id="cb3-2"><span>// int does not implement Stringer (missing method String)</span></span></code></pre></div>
<p>That makes sense. It’s just the same as if we wrote an ordinary,
non-generic function that took a parameter of type
<code>Stringer</code>, as we did in the <a href="https://bitfieldconsulting.com/posts/generics">first
tutorial</a>.</p>
<p>There’s no advantage to writing a generic function in this case,
since we can use this interface type directly in an ordinary function.
All the same, a basic interface—one defined by a set of methods—is a
valid constraint for type parameters, and we can use it that way if we
want to.</p>
<h3 id="exercise-stringy-beans">Exercise: Stringy beans</h3>
<p>Flex your generics muscles a little now, by writing a generic
function constrained by <code>fmt.Stringer</code> to solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/stringy"><code>stringy</code></a>
exercise.</p>
<div id="cb4"><pre><code><span id="cb4-1"><span>type</span> greeting <span>struct</span><span>{}</span></span>
<span id="cb4-2"></span>
<span id="cb4-3"><span>func</span> <span>(</span>greeting<span>)</span> String<span>()</span> <span>string</span> <span>{</span></span>
<span id="cb4-4">    <span>return</span> <span>"Howdy!"</span></span>
<span id="cb4-5"><span>}</span></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span>func</span> TestStringifyTo_PrintsToSuppliedWriter<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb4-8">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb4-9">    buf <span>:=</span> <span>&amp;</span>bytes<span>.</span>Buffer<span>{}</span></span>
<span id="cb4-10">    stringy<span>.</span>StringifyTo<span>[</span>greeting<span>](</span>buf<span>,</span> greeting<span>{})</span></span>
<span id="cb4-11">    want <span>:=</span> <span>"Howdy!</span><span>\n</span><span>"</span></span>
<span id="cb4-12">    got <span>:=</span> buf<span>.</span>String<span>()</span></span>
<span id="cb4-13">    <span>if</span> want <span>!=</span> got <span>{</span></span>
<span id="cb4-14">        t<span>.</span>Errorf<span>(</span><span>"want %q, got %q"</span><span>,</span> want<span>,</span> got<span>)</span></span>
<span id="cb4-15">    <span>}</span></span>
<span id="cb4-16"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/stringy/stringy_test.go">Listing
<code>exercises/stringy</code></a>)</p>
<p><strong>GOAL:</strong> Your job here is to write a generic function
<code>StringifyTo[T]</code> that takes an <code>io.Writer</code> and a
value of some arbitrary type constrained by <code>fmt.Stringer</code>,
and prints the value to the writer.</p>
<hr>
<p><strong>HINT:</strong> This is a bit like the
<code>PrintAnything</code> function we saw before, isn’t it? Actually,
it’s a “print anything stringable” function. We already know what the
constraint is (<code>fmt.Stringer</code>), and the rest is
straightforward.</p>
<hr>
<p><strong>SOLUTION:</strong> Here’s a version that would work, for
example:</p>
<div id="cb5"><pre><code><span id="cb5-1"><span>func</span> StringifyTo<span>[</span>T fmt<span>.</span>Stringer<span>](</span>w io<span>.</span>Writer<span>,</span> p T<span>)</span> <span>{</span></span>
<span id="cb5-2">    fmt<span>.</span>Fprintln<span>(</span>w<span>,</span> p<span>.</span>String<span>())</span></span>
<span id="cb5-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/stringy/stringy.go">Listing
<code>solutions/stringy</code></a>)</p>
<p>Strictly speaking, of course, we don’t really need to call the
<code>String</code> method: <code>fmt</code> already knows how to do
that automagically. But if we just passed <code>p</code> directly, we
wouldn’t need the <code>Stringer</code> constraint, and we could use
<code>any</code>… but what would be the fun in that?</p>
<h2 id="type-set-constraints">Type set constraints</h2>
<p>We’ve seen that one way an interface can specify an allowed range of
types is by including a <em>method element</em>, such as
<code>String() string</code>. That would be a basic interface, but now
let’s introduce another kind of interface. Instead of listing methods
that the type must have, it directly specifies a set of types that are
allowed.</p>
<h3 id="type-elements">Type elements</h3>
<p>For example, suppose we wanted to write some generic function
<code>Double</code> that multiplies a number by two, and we want a type
constraint that allows only values of type <code>int</code>. We know
that <code>int</code> has no methods, so we can’t use any basic
interface as a constraint. How can we write it, then?</p>
<p>Well, here’s how:</p>
<div id="cb6"><pre><code><span id="cb6-1"><span>type</span> OnlyInt <span>interface</span> <span>{</span></span>
<span id="cb6-2">    <span>int</span></span>
<span id="cb6-3"><span>}</span></span></code></pre></div>
<p>Very straightforward! It looks just like a regular interface
definition, except that instead of method elements, it contains a single
<em>type element</em>, consisting of a named type. In this case, the
named type is <code>int</code>.</p>
<h3 id="using-a-type-set-constraint">Using a type set constraint</h3>
<p>How would we use a constraint like this? Let’s write
<code>Double</code>, then:</p>
<div id="cb7"><pre><code><span id="cb7-1"><span>func</span> Double<span>[</span>T OnlyInt<span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb7-2">    <span>return</span> v <span>*</span> <span>2</span></span>
<span id="cb7-3"><span>}</span></span></code></pre></div>
<p>In other words, for some T that satisfies the constraint
<code>OnlyInt</code>, <code>Double</code> takes a T parameter and
returns a T result.</p>
<p>Note that we now have one answer to the sort of problem we
encountered with <code>AddAnything</code>: how to enable the
<code>*</code> operator (or any other arithmetic operator) in a
parameterised function. Since T can only be <code>int</code> (thanks to
the <code>OnlyInt</code> constraint), Go can guarantee that the
<code>*</code> operator will work with T values.</p>
<p>It’s not the complete answer, though, since there are other types
that support <code>*</code> that <em>wouldn’t</em> be allowed by this
constraint. And in any case, if we were only going to support
<code>int</code>, we could have just written an ordinary function that
took an <code>int</code> parameter.</p>
<p>So we’ll need to be able to expand the range of types allowed by our
constraint a little, but not beyond the types that support
<code>*</code>. How can we do that?</p>
<h3 id="unions">Unions</h3>
<p>What types <em>can</em> satisfy the constraint <code>OnlyInt</code>?
Well, only <code>int</code>! To broaden this range, we can create a
constraint specifying more than one named type:</p>
<div id="cb8"><pre><code><span id="cb8-1"><span>type</span> Integer <span>interface</span> <span>{</span></span>
<span id="cb8-2">    <span>int</span> <span>|</span> <span>int8</span> <span>|</span> <span>int16</span> <span>|</span> <span>int32</span> <span>|</span> <span>int64</span></span>
<span id="cb8-3"><span>}</span></span></code></pre></div>
<p>The types are separated by the pipe character, <code>|</code>. You
can think of this as representing “or”. In other words, a type will
satisfy this constraint if it is <code>int</code> <em>or</em>
<code>int8</code> <em>or</em>… you get the idea.</p>
<p>This kind of interface element is called a <em>union</em>. The type
elements in a union can include any Go types, including interface
types.</p>
<p>It can even include other constraints. In other words, we can
<em>compose</em> new constraints from existing ones, like this:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>type</span> Float <span>interface</span> <span>{</span></span>
<span id="cb9-2">    <span>float32</span> <span>|</span> <span>float64</span></span>
<span id="cb9-3"><span>}</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span>type</span> Complex <span>interface</span> <span>{</span></span>
<span id="cb9-6">    <span>complex64</span> <span>|</span> <span>complex128</span></span>
<span id="cb9-7"><span>}</span></span>
<span id="cb9-8"></span>
<span id="cb9-9"><span>type</span> Number <span>interface</span> <span>{</span></span>
<span id="cb9-10">    Integer <span>|</span> Float <span>|</span> Complex</span>
<span id="cb9-11"><span>}</span></span></code></pre></div>
<p>We’re saying that <code>Integer</code>, <code>Float</code>, and
<code>Complex</code> are all unions of different built-in numeric types,
but we’re also creating a new constraint <code>Number</code>, which is a
union of those three <em>interface</em> types we just defined. If it’s
an integer, a float, or a complex number, then it’s a number!</p>
<h3 id="the-set-of-all-allowed-types">The set of all allowed types</h3>
<p>The <em>type set</em> of a constraint is the set of all types that
satisfy it. The type set of the empty interface (<code>any</code>) is
the set of all types, as you’d expect.</p>
<p>The type set of a union element (such as <code>Float</code> in the
previous example) is the union of the type sets of all its terms.</p>
<p>In the <code>Float</code> example, which is the union of
<code>float32 | float64</code>, its type set contains
<code>float32</code>, <code>float64</code>, and no other types.</p>
<h3 id="intersections">Intersections</h3>
<p>You probably know that with a basic interface, a type must have
<em>all</em> of the methods listed in order to implement the interface.
And if the interface contains other interfaces, a type must implement
<em>all</em> of those interfaces, not just one of them.</p>
<p>For example:</p>
<div id="cb10"><pre><code><span id="cb10-1"><span>type</span> ReaderStringer <span>interface</span> <span>{</span></span>
<span id="cb10-2">    io<span>.</span>Reader</span>
<span id="cb10-3">    fmt<span>.</span>Stringer</span>
<span id="cb10-4"><span>}</span></span></code></pre></div>
<p>If we were to write this as an <em>interface literal</em>, we would
separate the methods with a semicolon instead of a newline, but the
meaning is the same:</p>
<div id="cb11"><pre><code><span id="cb11-1"><span>interface</span> <span>{</span> io<span>.</span>Reader<span>;</span> fmt<span>.</span>Stringer <span>}</span></span></code></pre></div>
<p>To implement this interface, a type has to implement <em>both</em>
<code>io.Reader</code> <em>and</em> <code>fmt.Stringer</code>. Just one
or the other isn’t good enough.</p>
<p>Each line of an interface definition like this, then, is treated as a
distinct type element. The type set of the interface as a whole is the
<em>intersection</em> of the type sets of all its elements. That is,
only those types that all the elements have in common.</p>
<p>So putting interface elements on different lines has the effect of
requiring a type to implement <em>all</em> those elements. We don’t need
this kind of interface very often, but we can imagine cases where it
might be necessary.</p>
<h3 id="empty-type-sets">Empty type sets</h3>
<p>You might be wondering about what happens if we define an interface
whose type set is completely empty. That is, if there are no types that
can satisfy the constraint.</p>
<p>Well, that could happen with an intersection of two type sets that
have <em>no</em> elements in common. For example:</p>
<div id="cb12"><pre><code><span id="cb12-1"><span>type</span> Unpossible <span>interface</span> <span>{</span></span>
<span id="cb12-2">    <span>int</span></span>
<span id="cb12-3">    <span>string</span></span>
<span id="cb12-4"><span>}</span></span></code></pre></div>
<p>Clearly no type can be both <code>int</code> and <code>string</code>
at the same time! Or, to put it another way, this interface’s type set
is empty.</p>
<p>If we try to instantiate a function constrained by
<code>Unpossible</code>, we’ll find, naturally enough, that it can’t be
done:</p>
<pre><code>cannot implement Unpossible (empty type set)</code></pre>
<p>We probably wouldn’t do this on purpose, since an unsatisfiable
constraint doesn’t seem that useful. But with more sophisticated
interfaces, we might accidentally reduce the allowed type set to zero,
and it’s helpful to know what this error message means so that we can
fix the problem.</p>
<h2 id="composite-type-literals">Composite type literals</h2>
<p>A <em>composite</em> type is one that’s built up from other types. We
saw some composite types in the <a href="https://bitfieldconsulting.com/posts/generic-types">previous
tutorial</a>, such as <code>[]E</code>, which is a slice of some element
type E.</p>
<p>But we’re not restricted to defined types with names. We can also
construct new types on the fly, using a <em>type literal</em>: that is,
literally writing out the type definition as part of the interface.</p>
<h3 id="a-struct-type-literal">A struct type literal</h3>
<p>For example, this interface specifies a <em>struct</em> type
literal:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb14-2">    <span>struct</span><span>{</span> X<span>,</span> Y <span>int</span> <span>}</span></span>
<span id="cb14-3"><span>}</span></span></code></pre></div>
<p>A type parameter with this constraint would allow any instance of
such a struct. In other words, its type set contains exactly one type:
<code>struct{ X, Y int }</code>.</p>
<h3 id="access-to-struct-fields">Access to struct fields</h3>
<p>While we can write a generic function constrained by some struct type
such as <code>Pointish</code>, there are limitations on what that
function can do with that type. One is that it can’t access the struct’s
<em>fields</em>:</p>
<div id="cb15"><pre><code><span id="cb15-1"><span>func</span> GetX<span>[</span>T Pointish<span>](</span>p T<span>)</span> <span>int</span> <span>{</span></span>
<span id="cb15-2">    <span>return</span> p<span>.</span>X</span>
<span id="cb15-3"><span>}</span></span>
<span id="cb15-4"><span>// p.X undefined (type T has no field or method X)</span></span></code></pre></div>
<p>In other words, we can’t refer to a field on <code>p</code>, even
though the function’s constraint explicitly says that any <code>p</code>
is guaranteed to be a struct with at least the field <code>X</code>.
This is a limitation of the Go compiler that has not yet been overcome.
Sorry about that.</p>
<h2 id="some-limitations-of-type-sets">Some limitations of type
sets</h2>
<p>An interface containing type elements can <em>only</em> be used as a
constraint on a type parameter. It can’t be used as the type of a
variable or parameter declaration, like a basic interface can. That too
is something that might change in the future, but this is where we are
today.</p>
<h3 id="constraints-versus-basic-interfaces">Constraints versus basic
interfaces</h3>
<p>What exactly stops us from doing that, though? We already know that
we can write functions that take ordinary parameters of some basic
interface type such as <code>Stringer</code>. So what happens if we try
to do the same with an interface containing type elements, such as
<code>Number</code>?</p>
<p>Let’s see:</p>
<div id="cb16"><pre><code><span id="cb16-1"><span>func</span> Double<span>(</span>p Number<span>)</span> Number <span>{</span></span>
<span id="cb16-2"><span>// interface contains type constraints</span></span></code></pre></div>
<p>This doesn’t compile, for the reasons we’ve discussed. Some potential
confusion arises from the fact that a basic interface can be used as
both a regular interface type <em>and</em> a constraint on type
parameters. But interfaces that contain type elements can only be used
as constraints.</p>
<h3 id="constraints-are-not-classes">Constraints are not classes</h3>
<p>If you have some experience with languages that have <em>classes</em>
(hierarchies of types), then there’s another thing that might trip you
up with Go generics: constraints are not classes, and you can’t
instantiate a generic function or type on a constraint interface.</p>
<p>To illustrate, suppose we have some concrete types <code>Cow</code>
and <code>Chicken</code>:</p>
<div id="cb17"><pre><code><span id="cb17-1"><span>type</span> Cow <span>struct</span><span>{</span> moo <span>string</span> <span>}</span></span>
<span id="cb17-2"></span>
<span id="cb17-3"><span>type</span> Chicken <span>struct</span><span>{</span> cluck <span>string</span> <span>}</span></span></code></pre></div>
<p>And suppose we define some interface <code>Animal</code> whose type
set consists of <code>Cow</code> and <code>Chicken</code>:</p>
<div id="cb18"><pre><code><span id="cb18-1"><span>type</span> Animal <span>interface</span> <span>{</span></span>
<span id="cb18-2">    Cow <span>|</span> Chicken</span>
<span id="cb18-3"><span>}</span></span></code></pre></div>
<p>So far, so good, and suppose we now define a generic type
<code>Farm</code> as a slice of <code>T Animal</code>:</p>

<p>Since we know the type set of <code>Animal</code> contains exactly
<code>Cow</code> and <code>Chicken</code>, then either of those types
can be used to instantiate <code>Farm</code>:</p>
<div id="cb20"><pre><code><span id="cb20-1">dairy <span>:=</span> Farm<span>[</span>Cow<span>]{}</span></span>
<span id="cb20-2">poultry <span>:=</span> Farm<span>[</span>Chicken<span>]{}</span></span></code></pre></div>
<p>What about <code>Animal</code> itself? Could we create a
<code>Farm[Animal]</code>? No, because there’s no such type as
<code>Animal</code>. It’s a type <em>constraint</em>, not a type, so
this gives an error:</p>
<div id="cb21"><pre><code><span id="cb21-1">mixed <span>:=</span> Farm<span>[</span>Animal<span>]{}</span></span>
<span id="cb21-2"><span>// interface contains type constraints</span></span></code></pre></div>
<p>And, as we’ve seen, we also couldn’t use <code>Animal</code> as the
type of some variable, or ordinary function parameter. Only basic
interfaces can be used this way, not interfaces containing type
elements.</p>
<h2 id="approximations">Approximations</h2>
<p>Let’s return to our earlier definition of an interface
<code>Integer</code>, consisting of a union of named types.
Specifically, the built-in signed integer types:</p>
<div id="cb22"><pre><code><span id="cb22-1"><span>type</span> Integer <span>interface</span> <span>{</span></span>
<span id="cb22-2">    <span>int</span> <span>|</span> <span>int8</span> <span>|</span> <span>int16</span> <span>|</span> <span>int32</span> <span>|</span> <span>int64</span></span>
<span id="cb22-3"><span>}</span></span></code></pre></div>
<p>We know that the type set of this interface contains all the types
we’ve named. But what about defined types whose <em>underlying</em> type
is one of the built-in types?</p>
<h3 id="limitations-of-named-types">Limitations of named types</h3>
<p>For example:</p>

<p>Is <code>MyInt</code> also in the type set of <code>Integer</code>?
Let’s find out. Suppose we write a generic function that uses this
constraint:</p>
<div id="cb24"><pre><code><span id="cb24-1"><span>func</span> Double<span>[</span>T Integer<span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb24-2">    <span>return</span> v <span>*</span> <span>2</span></span>
<span id="cb24-3"><span>}</span></span></code></pre></div>
<p>Can we pass it a <code>MyInt</code> value? We’ll soon know:</p>
<div id="cb25"><pre><code><span id="cb25-1">fmt<span>.</span>Println<span>(</span>Double<span>(</span>MyInt<span>(</span><span>1</span><span>)))</span></span>
<span id="cb25-2"><span>// MyInt does not implement Integer</span></span></code></pre></div>
<p>No.&nbsp;That makes sense, because <code>Integer</code> is a list of named
types, and we can see that <code>MyInt</code> isn’t one of them.</p>
<p>How can we write an interface that allows not only a set of specific
named types, but also any other types <em>derived</em> from them?</p>
<h3 id="type-approximations">Type approximations</h3>
<p>We need a new kind of type element: a <em>type approximation</em>. We
write it using the tilde (<code>~</code>) character:</p>
<div id="cb26"><pre><code><span id="cb26-1"><span>type</span> ApproximatelyInt <span>interface</span> <span>{</span></span>
<span id="cb26-2">    <span>~</span><span>int</span></span>
<span id="cb26-3"><span>}</span></span></code></pre></div>
<p>The type set of <code>~int</code> includes <code>int</code> itself,
but also any type whose underlying type is <code>int</code> (for
example, <code>MyInt</code>).</p>
<p>If we rewrite <code>Double</code> to use this constraint, we can pass
it a <code>MyInt</code>, which is good. Even better, it will accept
<em>any</em> type, now or in the future, whose underlying type is
<code>int</code>.</p>
<h3 id="derived-types">Derived types</h3>
<p>Approximations are especially useful with struct type elements.
Remember our <code>Pointish</code> interface?</p>
<div id="cb27"><pre><code><span id="cb27-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb27-2">    <span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}</span></span>
<span id="cb27-3"><span>}</span></span></code></pre></div>
<p>Let’s write a generic function with this constraint:</p>
<div id="cb28"><pre><code><span id="cb28-1"><span>func</span> Plot<span>[</span>T Pointish<span>](</span>p T<span>)</span> <span>{</span></span></code></pre></div>
<p>We can pass it values of type <code>struct{ x, y int }</code>, as
you’d expect:</p>
<div id="cb29"><pre><code><span id="cb29-1">p <span>:=</span> <span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}{</span><span>1</span><span>,</span> <span>2</span><span>}</span></span>
<span id="cb29-2">Plot<span>(</span>p<span>)</span></span></code></pre></div>
<p>But now comes a problem: we can’t pass values of any <em>named</em>
struct type, even if the struct definition itself matches the constraint
perfectly:</p>
<div id="cb30"><pre><code><span id="cb30-1"><span>type</span> Point <span>struct</span> <span>{</span></span>
<span id="cb30-2">    x<span>,</span> y <span>int</span></span>
<span id="cb30-3"><span>}</span></span>
<span id="cb30-4">p <span>:=</span> Point<span>{</span><span>1</span><span>,</span> <span>2</span><span>}</span></span>
<span id="cb30-5">Plot<span>(</span>p<span>)</span></span>
<span id="cb30-6"><span>// Point does not implement Pointish (possibly missing ~ for</span></span>
<span id="cb30-7"><span>// struct{x int; y int} in constraint Pointish)</span></span></code></pre></div>
<p>What’s the problem here? Our constraint allows
<code>struct{ x, y int }</code>, but <code>Point</code> is <em>not that
type</em>. It’s a type <em>derived</em> from it. And, just as with
<code>MyInt</code>, a derived type is distinct from its underlying
type.</p>
<p>You know now how to solve this problem: use a type approximation! And
Go is telling us the same thing: “Hint, hint: I think you meant to write
a <code>~</code> in your constraint.”</p>
<p>If we add that approximation, the type set of our interface expands
to encompass all types derived from the specified struct, including
<code>Point</code>:</p>
<div id="cb31"><pre><code><span id="cb31-1"><span>type</span> Pointish <span>interface</span> <span>{</span></span>
<span id="cb31-2">    <span>~</span><span>struct</span><span>{</span> x<span>,</span> y <span>int</span> <span>}</span></span>
<span id="cb31-3"><span>}</span></span></code></pre></div>
<h3 id="exercise-a-first-approximation">Exercise: A first
approximation</h3>
<p>Can you use what you’ve just learned to solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/intish"><code>intish</code></a>
challenge?</p>
<p>Here you’re provided with a function <code>IsPositive</code>, which
determines whether a given value is greater than zero:</p>
<div id="cb32"><pre><code><span id="cb32-1"><span>func</span> IsPositive<span>[</span>T Intish<span>](</span>v T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb32-2">    <span>return</span> v <span>&gt;</span> <span>0</span></span>
<span id="cb32-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/intish/intish.go">Listing
<code>exercises/intish</code></a>)</p>
<p>And there’s a set of accompanying tests that instantiate this
function on some derived type <code>MyInt</code>:</p>
<div id="cb33"><pre><code><span id="cb33-1"><span>type</span> MyInt <span>int</span></span>
<span id="cb33-2"></span>
<span id="cb33-3"><span>func</span> TestIsPositive_IsTrueFor1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-4">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-5">    input <span>:=</span> MyInt<span>(</span><span>1</span><span>)</span></span>
<span id="cb33-6">    <span>if</span> <span>!</span>intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-7">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(1): want true, got false"</span><span>)</span></span>
<span id="cb33-8">    <span>}</span></span>
<span id="cb33-9"><span>}</span></span>
<span id="cb33-10"></span>
<span id="cb33-11"><span>func</span> TestIsPositive_IsFalseForNegative1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-12">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-13">    input <span>:=</span> MyInt<span>(-</span><span>1</span><span>)</span></span>
<span id="cb33-14">    <span>if</span> intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-15">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(-1): want false, got true"</span><span>)</span></span>
<span id="cb33-16">    <span>}</span></span>
<span id="cb33-17"><span>}</span></span>
<span id="cb33-18"></span>
<span id="cb33-19"><span>func</span> TestIsPositive_IsFalseForZero<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb33-20">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb33-21">    input <span>:=</span> MyInt<span>(</span><span>0</span><span>)</span></span>
<span id="cb33-22">    <span>if</span> intish<span>.</span>IsPositive<span>(</span>input<span>)</span> <span>{</span></span>
<span id="cb33-23">        t<span>.</span>Errorf<span>(</span><span>"IsPositive(0): want false, got true"</span><span>)</span></span>
<span id="cb33-24">    <span>}</span></span>
<span id="cb33-25"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/intish/intish_test.go">Listing
<code>exercises/intish</code></a>)</p>
<p><strong>GOAL:</strong> Your task here is to define the
<code>Intish</code> interface.</p>
<hr>
<p><strong>HINT:</strong> A method set won’t work here, because the
<code>int</code> type <em>has</em> no methods! On the other hand, the
type literal <code>int</code> won’t work either, because
<code>MyInt</code> is not <code>int</code>, it’s a new type derived from
it.</p>
<p>What kind of constraint could you use instead? I think you know where
this is going, don’t you? If not, have another look at the previous
section on type approximations.</p>
<hr>
<p><strong>SOLUTION:</strong> It’s not complicated, once you know that a
type approximation is required:</p>
<div id="cb34"><pre><code><span id="cb34-1"><span>type</span> Intish <span>interface</span> <span>{</span></span>
<span id="cb34-2">    <span>~</span><span>int</span></span>
<span id="cb34-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/intish/intish.go">Listing
<code>solutions/intish</code></a>)</p>
<h2 id="interface-literals">Interface literals</h2>
<p>Up to now, we’ve always used type parameters with a <em>named</em>
constraint, such as <code>Integer</code> (or even just
<code>any</code>). And we know that those constraints are defined as
interfaces. So could we use an <em>interface literal</em> as a type
constraint?</p>
<h3 id="syntax-of-an-interface-literal">Syntax of an interface
literal</h3>
<p>An interface literal, as you probably know, consists of the keyword
<code>interface</code> followed by curly braces containing (optionally)
some interface elements.</p>
<p>For example, the simplest interface literal is the empty interface,
<code>interface{}</code>, which is common enough to have its own
predeclared name, <code>any</code>.</p>
<p>We should be able to write this empty interface literal wherever
<code>any</code> is allowed as a type constraint, then:</p>
<div id="cb35"><pre><code><span id="cb35-1"><span>func</span> Identity<span>[</span>T <span>interface</span><span>{}](</span>v T<span>)</span> T <span>{</span></span></code></pre></div>
<p>And so we can. But we’re not restricted to only <em>empty</em>
interface literals. We could write an interface literal that contains a
method element, for example:</p>
<div id="cb36"><pre><code><span id="cb36-1"><span>func</span> Stringify<span>[</span>T <span>interface</span><span>{</span> String<span>()</span> <span>string</span> <span>}](</span>s T<span>)</span> <span>string</span> <span>{</span></span>
<span id="cb36-2">    <span>return</span> s<span>.</span>String<span>()</span></span>
<span id="cb36-3"><span>}</span></span></code></pre></div>
<p>This is a little hard to read at first, perhaps. But we’ve already
seen this exact function before, only in that case it had a
<em>named</em> constraint <code>Stringer</code>. We’ve simply replaced
that name with the corresponding interface literal:</p>
<div id="cb37"><pre><code><span id="cb37-1"><span>interface</span><span>{</span> String<span>()</span> <span>string</span> <span>}</span></span></code></pre></div>
<p>That is, the set of types that have a <code>String</code> method. We
don’t need to name this interface in order to use it as a constraint,
and sometimes it’s clearer to write it as a literal.</p>
<h3 id="omitting-the-interface-keyword">Omitting the
<code>interface</code> keyword</h3>
<p>And we’re not limited to just method elements in interface literals
used as constraints. We can use type elements too:</p>

<p>Conveniently, in this case we can omit the enclosing
<code>interface { ... }</code>, and write simply <code>~int</code> as
the constraint:</p>

<p>For example, we could write some function <code>Increment</code>
constrained to types derived from <code>int</code>:</p>
<div id="cb40"><pre><code><span id="cb40-1"><span>func</span> Increment<span>[</span>T <span>~</span><span>int</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb40-2">    <span>return</span> v <span>+</span> <span>1</span></span>
<span id="cb40-3"><span>}</span></span></code></pre></div>
<p>However, we can only omit the <code>interface</code> keyword when the
constraint contains exactly one type element. Multiple elements wouldn’t
be allowed, so this doesn’t work:</p>
<div id="cb41"><pre><code><span id="cb41-1"><span>func</span> Increment<span>[</span>T <span>~</span><span>int</span><span>;</span> <span>~</span><span>float64</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb41-2"><span>// syntax error: unexpected semicolon in parameter list; possibly </span></span>
<span id="cb41-3"><span>// missing comma or ]</span></span></code></pre></div>
<p>And we can’t omit <code>interface</code> with method elements
either:</p>
<div id="cb42"><pre><code><span id="cb42-1"><span>func</span> Increment<span>[</span>T String<span>()</span> <span>string</span><span>](</span>v T<span>)</span> T <span>{</span></span>
<span id="cb42-2"><span>// syntax error: unexpected ( in parameter list; possibly </span></span>
<span id="cb42-3"><span>// missing comma or ]</span></span></code></pre></div>
<p>And we can only omit <code>interface</code> in a constraint
<em>literal</em>. We can’t omit it when defining a named constraint. So
this doesn’t work, for example:</p>
<div id="cb43"><pre><code><span id="cb43-1"><span>type</span> Intish <span>~</span><span>int</span></span>
<span id="cb43-2"><span>// syntax error: unexpected ~ in type declaration</span></span></code></pre></div>
<h3 id="referring-to-type-parameters">Referring to type parameters</h3>
<p>We’ve seen that in certain cases, instead of having to define it
separately, we can write a constraint directly as an interface literal.
So you might be wondering: can we refer to T inside the interface
literal itself? Yes, we can.</p>
<p>To see why we might need to do that, suppose we wanted to write a
generic function <code>Contains[T]</code>, that takes a slice of T and
tells you whether or not it contains a given value.</p>
<p>And suppose that we’ll determine this, for any particular element of
the slice, by calling some <code>Equal</code> method on the element.
That means we must constrain the function to only types that have a
suitable <code>Equal</code> method.</p>
<p>So the constraint for T is going to be an interface containing the
method <code>Equal(T) bool</code>, let’s say.</p>
<p>Can we do this? Let’s try:</p>
<div id="cb44"><pre><code><span id="cb44-1"><span>func</span> Contains<span>[</span>T <span>interface</span><span>{</span> Equal<span>(</span>T<span>)</span> <span>bool</span> <span>}](</span>s <span>[]</span>T<span>,</span>  v T<span>)</span> <span>bool</span> <span>{</span></span></code></pre></div>
<p>Yes, this is fine. In fact, using an interface literal is the
<em>only</em> way to write this constraint. We couldn’t have created
some <em>named</em> interface type to do the same thing. Why not?</p>
<p>Let’s see what happens if we try:</p>
<div id="cb45"><pre><code><span id="cb45-1"><span>type</span> Equaler <span>interface</span> <span>{</span></span>
<span id="cb45-2">    Equal<span>(???)</span> <span>bool</span> <span>// we can't say 'T' here</span></span>
<span id="cb45-3"><span>}</span></span></code></pre></div>
<p>Because the type parameter T is part of the <code>Equal</code> method
signature, and we don’t <em>have</em> T here. The only way to refer to T
is in an interface literal inside a type constraint:</p>
<div id="cb46"><pre><code><span id="cb46-1"><span>[</span>T <span>interface</span><span>{</span> Equal<span>(</span>T<span>)</span> <span>bool</span> <span>}]</span></span></code></pre></div>
<p>At least, we can’t write a <em>specific</em> interface that mentions
T in its method set. What we’d need here, in fact, is a <em>generic</em>
interface, and you’ll learn how to define and use these in my book, <a href="https://bitfieldconsulting.com/posts/generics">Know Go</a>. If these tutorials have given you an
appetite for generic programming in Go, I think you’ll really enjoy the
book—check it out!</p>
<h3 id="exercise-greater-love">Exercise: Greater love</h3>
<p>Your turn now to see if you can solve the <a href="https://github.com/bitfield/know-go/tree/main/exercises/greater"><code>greater</code></a>
exercise.</p>
<p>You’ve been given the following (incomplete) function:</p>
<div id="cb47"><pre><code><span id="cb47-1"><span>func</span> IsGreater<span>[</span>T <span>/* Your constraint here! */</span><span>](</span>x<span>,</span> y T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb47-2">    <span>return</span> x<span>.</span>Greater<span>(</span>y<span>)</span></span>
<span id="cb47-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/greater/greater.go">Listing
<code>exercises/greater</code></a>)</p>
<p>This takes two values of some arbitrary type, and compares them by
calling the <code>Greater</code> method on the first value, passing it
the second value.</p>
<p>The tests exercise this function by calling it with two values of a
defined type <code>MyInt</code>, which has the required
<code>Greater</code> method.</p>
<div id="cb48"><pre><code><span id="cb48-1"><span>type</span> MyInt <span>int</span></span>
<span id="cb48-2"></span>
<span id="cb48-3"><span>func</span> <span>(</span>m MyInt<span>)</span> Greater<span>(</span>v MyInt<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb48-4">    <span>return</span> m <span>&gt;</span> v</span>
<span id="cb48-5"><span>}</span></span>
<span id="cb48-6"></span>
<span id="cb48-7"><span>func</span> TestIsGreater_IsTrueFor2And1<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb48-8">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb48-9">    <span>if</span> <span>!</span>greater<span>.</span>IsGreater<span>(</span>MyInt<span>(</span><span>2</span><span>),</span> MyInt<span>(</span><span>1</span><span>))</span> <span>{</span></span>
<span id="cb48-10">        t<span>.</span>Fatalf<span>(</span><span>"IsGreater(2, 1): want true, got false"</span><span>)</span></span>
<span id="cb48-11">    <span>}</span></span>
<span id="cb48-12"><span>}</span></span>
<span id="cb48-13"></span>
<span id="cb48-14"><span>func</span> TestIsGreater_IsFalseFor1And2<span>(</span>t <span>*</span>testing<span>.</span>T<span>)</span> <span>{</span></span>
<span id="cb48-15">    t<span>.</span>Parallel<span>()</span></span>
<span id="cb48-16">    <span>if</span> greater<span>.</span>IsGreater<span>(</span>MyInt<span>(</span><span>1</span><span>),</span> MyInt<span>(</span><span>2</span><span>))</span> <span>{</span></span>
<span id="cb48-17">        t<span>.</span>Fatalf<span>(</span><span>"IsGreater(1, 2): want false, got true"</span><span>)</span></span>
<span id="cb48-18">    <span>}</span></span>
<span id="cb48-19"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/exercises/greater/greater_test.go">Listing
<code>exercises/greater</code></a>)</p>
<p><strong>GOAL:</strong> To make these tests pass, you’ll need to write
an appropriate type constraint for <code>IsGreater</code>. Can you see
what to do?</p>
<hr>
<p><strong>HINT:</strong> Remember, we got here by talking about
constraints as interface literals, and in particular, interface literals
that refer to the type parameter.</p>
<p>If you try to define some <em>named</em> interface with the method
set containing <code>Greater</code>, for example, that won’t work. We
can’t do it for the same reason that we couldn’t define a named
interface with the method set <code>Equal</code>: we don’t know what
type of argument that method takes.</p>
<p>Just like <code>Equal</code>, <code>Greater</code> takes arguments of
some arbitrary type T, so we need an interface literal that can
<em>refer</em> to T in its definition. Does that help?</p>
<hr>
<p><strong>SOLUTION:</strong> Here’s one way to do it:</p>
<div id="cb49"><pre><code><span id="cb49-1"><span>func</span> IsGreater<span>[</span>T <span>interface</span><span>{</span> Greater<span>(</span>T<span>)</span> <span>bool</span> <span>}](</span>x<span>,</span> y T<span>)</span> <span>bool</span> <span>{</span></span>
<span id="cb49-2">    <span>return</span> x<span>.</span>Greater<span>(</span>y<span>)</span></span>
<span id="cb49-3"><span>}</span></span></code></pre></div>
<p>(<a href="https://github.com/bitfield/know-go/blob/main/solutions/greater/greater.go">Listing
<code>solutions/greater</code></a>)</p>
<p>Like most things, it’s delightfully simple once you know. For a type
parameter T, the required interface is:</p>

<p>And that’s how we do that.</p>
<p>Well, I hope you enjoyed this tutorial series, and if so, why not
treat yourself to a copy of <a href="https://bitfieldconsulting.com/books/generics">Know Go</a>?
There’s much more to explore, so I’d love you to come along with me for
the ride.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to setup self hosted wiki for your startup (104 pts)]]></title>
            <link>https://themythicalengineer.com/how-to-setup-self-hosted-wiki-for-your-startup.html</link>
            <guid>42162751</guid>
            <pubDate>Sun, 17 Nov 2024 08:14:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://themythicalengineer.com/how-to-setup-self-hosted-wiki-for-your-startup.html">https://themythicalengineer.com/how-to-setup-self-hosted-wiki-for-your-startup.html</a>, See on <a href="https://news.ycombinator.com/item?id=42162751">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        

<p>
  
    <a href="https://themythicalengineer.com/authors/themythicalengineer">The Mythical Engineer</a> <br>
    <span>Nov 17, 2024</span>
  
</p>


<p><img src="https://themythicalengineer.com/assets/images/setup-self-hosted-wiki/wiki_js.webp" alt="wiki_js_banner"></p>

<p>When it comes to setting up a wiki for your startup, you’ve probably looked at popular options like Confluence and Notion. While these tools are feature-rich, there’s one major drawback: they can get expensive really fast.</p>

<p>Most of these services charge per user per month (typically around $5), and even with enterprise negotiations, the costs can add up quickly as your team grows. Sure, they offer advanced features and granular access controls, but let’s be honest - most startups don’t need all those fancy features.</p>

<p>This is where self-hosted solutions shine. Your costs stay fixed regardless of how many employees join your company. After running a self-hosted wiki in production for over 3 years, I can confidently recommend <a href="https://js.wiki/">Wiki.js</a> as an excellent alternative to paid softwares.</p>

<h2 id="why-wikijs">Why Wiki.js?</h2>

<p>Setting up Wiki.js is surprisingly simple - you can have it running in minutes using Docker Compose. The basic setup (Wiki.js + PostgreSQL) is quite lightweight and can run smoothly on a modest server with:</p>
<ul>
  <li>4GB RAM</li>
  <li>2 vCPU</li>
</ul>

<p>If you want better search capabilities, you can add Elasticsearch as well, but you’ll need to increase the resources to atleast:</p>
<ul>
  <li>8GB RAM</li>
  <li>2 vCPU</li>
</ul>

<p>In our case, we’ve grown to over <strong>1,000 pages</strong> and nearly <strong>400 users</strong>, and our setup is still going strong. The only maintenance I’ve had to do was adding Elasticsearch for improved search functionality.</p>

<p><img src="https://themythicalengineer.com/assets/images/setup-self-hosted-wiki/elasticsearch_setup.webp" alt="elasticsearch_setup"></p>

<h2 id="cost-comparison">Cost Comparison</h2>

<p>Let’s talk numbers. Here’s what you might pay running this on AWS EC2:</p>

<table>
  <thead>
    <tr>
      <th>Instance Type</th>
      <th>vCPU</th>
      <th>RAM (GiB)</th>
      <th>On-Demand ($/hr)</th>
      <th>Monthly On-Demand</th>
      <th>Reserved 1-year ($/hr)</th>
      <th>Monthly Reserved</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>t4g.medium</td>
      <td>2</td>
      <td>4</td>
      <td>$0.0224</td>
      <td>$16.35</td>
      <td>$0.0142</td>
      <td>$10.37</td>
    </tr>
    <tr>
      <td>t4g.large</td>
      <td>2</td>
      <td>8</td>
      <td>$0.0448</td>
      <td>$32.70</td>
      <td>$0.0283</td>
      <td>$20.66</td>
    </tr>
  </tbody>
</table>

<p>To put this in perspective: if you had <strong>400</strong> users on a typical paid wiki platform charging $5 per user, you’d be looking at a <strong>$2,000+ monthly bill</strong>.</p>

<p>With a self-hosted solution, you will be paying less than <strong>$33/month</strong> even with the larger instance!</p>

<p>If you do need to scale up later, you can take small maintenance downtime to increase the instance size, or you can split the elasticsearch to a different instance.</p>

<h2 id="making-it-production-ready">Making It Production-Ready</h2>

<p>To transform this into a production-ready setup, I recommend implementing the following things:</p>

<ol>
  <li>Set up a custom domain like <code>wiki.yourcompany.com</code></li>
  <li>Configure DNS and a Load Balancer to handle traffic</li>
  <li>Implement <a href="https://docs.requarks.io/auth/google">SSO with Google</a></li>
  <li>Restrict self-registration to your company domain (e.g., yourcompany.com)</li>
  <li>Set up hourly AMI backups of your EC2 instance</li>
</ol>

<h2 id="the-setup">The Setup</h2>

<p>Here’s the Docker Compose file you can use to do the complete setup:</p>

<pre><code>services:
  db:
    image: postgres:16.4
    expose:
      - 5432
    ports:
      - 5432:5432
    restart: unless-stopped
    volumes:
      - db-data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: wiki
      POSTGRES_PASSWORD: pass # Change this to a strong password
      POSTGRES_USER: root

  wiki:
    image: requarks/wiki:2.5
    depends_on:
      - db
    environment:
      DB_TYPE: postgres
      DB_HOST: db
      DB_PORT: 5432
      DB_USER: root
      DB_PASS: pass # Change this to a strong password
      DB_NAME: wiki
    restart: unless-stopped
    ports:
      - "80:3000"

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.17.23
    container_name: elasticsearch
    restart: unless-stopped
    ports:
      - 127.0.0.1:9200:9200
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.license.self_generated.type=basic

volumes:
  db-data:
</code></pre>

<p>P.S. One of my friend is looking for a job in Product / Marketing &amp; Operations / Data Analytics Role.
He has good background in data analytics, mathematics and finance. He’s currently located in Bangalore, India, open to relocate and work in-office setup.
If you’re looking out for a great candidate, please connect directly to candidate on Phone, <a href="https://themythicalengineer.com/sachin.23@dmsiitd.org">Email</a> or <a href="https://linkedin.com/in/sachiniit">Linkedin</a>.
Please find the resume link <a href="https://drive.google.com/file/d/1PCDza6rQQ3Ecp_a8gyYEmneNGeL-snTD/view?usp=sharing">here</a></p>



  <p><span>
    
      
      <a href="https://themythicalengineer.com/tag/wiki"><code><nobr>#wiki</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/documentation"><code><nobr>#documentation</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/postgres"><code><nobr>#postgres</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/elasticsearch"><code><nobr>#elasticsearch</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/docker"><code><nobr>#docker</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/startup"><code><nobr>#startup</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/aws"><code><nobr>#aws</nobr></code>&nbsp;</a>
    
      
      <a href="https://themythicalengineer.com/tag/ec2"><code><nobr>#ec2</nobr></code>&nbsp;</a>
    
  </span></p><h2>Suggested Reading</h2>
<li>


	
	

	

	



	
	

	

	



	
	

	

	



	
	

	

	
	<p>
	  <h4> * <a href="https://themythicalengineer.com/setup-production-redis-cluster.html">Setup a Production Redis Cluster</a></h4>
	</p>
	
	
	
	



	
	

	

	



	
	

	

	



	
	

	

	
	<p>
	  <h4> * <a href="https://themythicalengineer.com/how-postgres-triggers-can-simplify-your-backend-development.html">How Postgres Triggers Can Simplify Your Backend Development</a></h4>
	</p>
	
	
	
	



	
	

	

	



	
	

	

	



	
	

	

	
	<p>
	  <h4> * <a href="https://themythicalengineer.com/running-rust-kernel-in-deepnote.html">Running Rust Kernel in Deepnote</a></h4>
	</p>
	
	
	
	



	
	

	

	



	
	

	

	
	<p>
	  <h4> * <a href="https://themythicalengineer.com/docker-pull-too-many-requests.html">Docker Pull Too Many Requests</a></h4>
	</p>
	
	
	
	



	
	

	

	



	
	

	

	
	<p>
	  <h4> * <a href="https://themythicalengineer.com/setup-production-redis-standalone.html">Setup a production Redis standalone server</a></h4>
	</p>
	
	
	
		
</li>

<h4>Share this:</h4>




<div>
    <p>Get new posts by email!</p>
		<p>Enter your email address to get an email whenever I write a new post.</p> 
    
    
</div>



      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[All-in-one embedding model for interleaved text, images, and screenshots (238 pts)]]></title>
            <link>https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/</link>
            <guid>42162622</guid>
            <pubDate>Sun, 17 Nov 2024 07:42:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/">https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/</a>, See on <a href="https://news.ycombinator.com/item?id=42162622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>




<p>TL;DR — We are excited to announce <code>voyage-multimodal-3</code>, a new state-of-the-art for multimodal embeddings and a big step forward towards seamless RAG and semantic search for documents rich with both visuals and text. Unlike existing multimodal embedding models, <code>voyage-multimodal-3</code> is capable of vectorizing interleaved texts + images and capturing key visual features from screenshots of PDFs, slides, tables, figures, and more, thereby eliminating the need for complex document parsing. <code>voyage-multimodal-3</code> improves retrieval accuracy by an average of 19.63% over the next best-performing multimodal embedding model when evaluated across 3 multimodal retrieval tasks (20 total datasets).</p>



<p>Two months ago, we released the <a href="https://blog.voyageai.com/2024/09/18/voyage-3/" rel="nofollow" target="_blank"><code>voyage-3</code> and <code>voyage-3-lite</code></a> series of multilingual text embedding models, providing best-in-class performance across a variety of datasets. Today, we’re excited to introduce <code>voyage-multimodal-3</code>, our first multimodal embedding model and a big step toward RAG and semantic search for knowledge bases rich with both visuals and text.</p>



<p><code>voyage-multimodal-3</code> supports text and content-rich images such as screenshots of texts, figures, tables, PDFs, slide decks, and more. The resultant vectors capture critical textual and visual features such as font size, text location, whitespace, etc. This eliminates the need for heuristic-based document parsing, which often struggles with accuracy when layouts are complex or interspersed with figures and photos. Unlike existing multimodal embedding models that handle either a single text or image input, <code>voyage-multimodal-3</code> allows for interleaved texts and images for maximum flexibility. Our <a href="https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9" rel="nofollow" target="_blank">sample notebook</a> demonstrates all of these features.</p>



<p><code>voyage-multimodal-3</code> has an architecture that is similar to that of modern vision-language transformers. This makes it a significant departure from existing multimodal embedding models, including, but not limited to, OpenAI CLIP large (<code>clip-vit-large-patch14-336</code>) and Cohere multimodal v3 (<code>embed-multimodal-v3.0</code>).</p>



<figure><img data-recalc-dims="1" fetchpriority="high" decoding="async" width="1440" height="800" data-attachment-id="1166" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-2/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=1440%2C800&amp;quality=80&amp;ssl=1" data-orig-size="1440,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 2" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=300%2C167&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?fit=1024%2C569&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1440%2C800&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?w=1440&amp;quality=80&amp;ssl=1 1440w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=300%2C167&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1024%2C569&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=768%2C427&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-2.png?resize=1200%2C667&amp;quality=80&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>In a set of evaluations across 20 multimodal retrieval datasets and 34 text retrieval datasets, we found that <code>voyage-multimodal-3</code>:</p>



<ol>
<li>Outperforms OpenAI CLIP large and Cohere multimodal v3 by an average of 41.44% (a 2.1x improvement) and 43.37% (a 2.2x improvement) on table/figure retrieval, 26.54% and 25.84% on document screenshot retrieval, and 6.55% and 5.86% on text-to-photo retrieval, respectively.</li>



<li>Outperforms OpenAI v3 large and Cohere multimodal/English<sup>1</sup> v3 by 5.13% and 13.70% on text-only datasets, respectively.</li>
</ol>



<h3>Support for Interleaved Text &amp; Images</h3>



<p>All existing commonly used multimodal embedding models (such as Amazon Titan Multimodal G1, Google Vertex AI multimodal, and Cohere multimodal v3) are based on OpenAI’s CLIP, which processes different modalities of data through independent networks. In other words, images <em>must</em> be vectorized through the vision tower, while text <em>must</em> be vectorized through the text tower, preventing these models from being able to processing interleaved data.</p>



<figure><img data-recalc-dims="1" decoding="async" width="1440" height="800" data-attachment-id="1167" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-1/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=1440%2C800&amp;quality=80&amp;ssl=1" data-orig-size="1440,800" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 1" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=300%2C167&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?fit=1024%2C569&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1440%2C800&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?w=1440&amp;quality=80&amp;ssl=1 1440w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=300%2C167&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1024%2C569&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=768%2C427&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-1.png?resize=1200%2C667&amp;quality=80&amp;ssl=1 1200w" sizes="(max-width: 1000px) 100vw, 1000px"></figure>



<p>In contrast, <code>voyage-multimodal-3</code> vectorizes both modalities of data directly within the same transformer encoder, ensuring that both text and visual features are treated as part of a unified representation rather than distinct components. This mimics the model architecture of the latest vision-language models, only for vectorization rather than generation. As a result, interleaved texts and images, document screenshots, PDFs with complex layouts, annotated images, etc can be vectorized in a way that preserves the contextual relationship between visual and textual information.</p>



<h3>Mixed Modality Search with Screenshots</h3>



<p>All CLIP-like models perform poorly on mixed-modality search due to a phenomenon known as the <a href="https://arxiv.org/abs/2203.02053" rel="nofollow" target="_blank">modality gap</a>. As illustrated in the figure below, the closest vector to the snippet “I address you, members of the Seventy-Seventh Congress…” is not its screenshot, but other texts. This leads to search results that are skewed towards items of the same modality; in other words, text vectors will be closer to irrelevant texts than relevant images in the embedding space.</p>



<figure><img data-recalc-dims="1" decoding="async" width="692" height="660" data-attachment-id="1171" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/slide-3-v3/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=692%2C660&amp;quality=80&amp;ssl=1" data-orig-size="692,660" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Slide 3 v3" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=300%2C286&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?fit=692%2C660&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?resize=692%2C660&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?w=692&amp;quality=80&amp;ssl=1 692w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/Slide-3-v3.png?resize=300%2C286&amp;quality=80&amp;ssl=1 300w" sizes="(max-width: 692px) 100vw, 692px"></figure>



<p>To illustrate this issue quantitatively, we conducted an experiment involving mixed-modality data. We created two sets of PyTorch documentation with identical content: one set as plain text (strings) and and the other set as screenshots. By combining a subset of text-based documentation with screenshots of remaining subset, we created a series of mixed-modality datasets. Each dataset represented a different proportion of text and screenshots, ranging from 0% to 100% screenshots. We then evaluated the retrieval accuracy of various multimodal models on these datasets, reporting the <a href="http://www.evidentlyai.com/ranking-metrics/ndcg-metric#:~:text=Normalized%20Discounted%20Cumulative%20Gain%20(NDCG)%20is%20a%20ranking%20quality%20metric,DCG%20representing%20a%20perfect%20ranking." rel="nofollow" target="_blank">normalized discounted cumulative gain</a>&nbsp;(NDCG@10) for each model across different screenshot ratios.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1920" height="1080" data-attachment-id="1172" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/voyage-multimodal-3_results-001/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=1920%2C1080&amp;quality=80&amp;ssl=1" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voyage-multimodal-3_results.001" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=300%2C169&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?fit=1024%2C576&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1920%2C1080&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?w=1920&amp;quality=80&amp;ssl=1 1920w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=300%2C169&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1024%2C576&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=768%2C432&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1536%2C864&amp;quality=80&amp;ssl=1 1536w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.001.png?resize=1200%2C675&amp;quality=80&amp;ssl=1 1200w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></figure>



<p>As shown above, CLIP-based models experience a decline in retrieval quality as the proportion of screenshots increases up to 90%, highlighting a retrieval bias influenced by modality. Moreover, these models perform poorly when all text is converted to images.</p>



<p>In contrast, <code>voyage-multimodal-3</code> is not only the most performant for all ratios, but also has little-to-no performance drop across the board, indicating that the vectors truly capture the semantic content contained in the screenshots. This robustness is due to the model’s unique approach of processing all input modalities through the same backbone.</p>



<p>With <code>voyage-multimodal-3</code>, there is no longer a need for screen parsing models, layout analysis, or any other complex text extraction pipelines; you can easily vectorize a knowledge base containing both pure-text documents as well unstructured data (such as PDFs/slides/webpages/etc) — screenshots are all you need.</p>



<h3>Evaluation Details</h3>



<p><strong>Datasets.</strong> We evaluate <code>voyage-multimodal-3</code> across 20 multimodal datasets spanning three different tasks: table/figure retrieval, document screenshot retrieval, and text-to-photo retrieval. We also evaluate <code>voyage-multimodal-3</code> on a standard text retrieval task spanning 34 datasets in 6 domains (law, finance, conversation, code, web, and tech).</p>



<p>For all datasets, the query is text, while the document could be a figure, photo, text, document screenshot, or a combination of these. For each task, we use prior top-performing models as the baseline. Alongside task names, we provide each task’s corresponding description and datasets used in the table below:</p>



<figure><table><thead><tr><th>Task</th><th>Description</th><th><strong>Datasets</strong></th></tr></thead><tbody><tr><td>Table/figure retrieval</td><td>Table/figure retrieval measures the strength of a model’s ability to match an image containing a table or figure (charts, graphs, etc) with descriptions, captions, or other textual queries which reference the figure.</td><td>charxiv, mmtab-test, ChartQA, Chartve, FintabnetQA, PlotQA,</td></tr><tr><td>Document screenshot retrieval</td><td>In this category, models are used to match queries with scans or screenshots of documents containing both text and charts.</td><td>Energy, Healthcare Industry, Artificial Intelligence, Government Report, InfoVQA, DocVQA, ArxivQA, TabFQuad, TAT-DQA, Shift Project</td></tr><tr><td>Text-to-photo retrieval</td><td>This is the typical text-to-image matching used by CLIP and other CLIP-like models, where queries are associated with the most semantically relevant photos.</td><td>meme-cap, mm-imdb, winoground, docci</td></tr><tr><td>Standard text retrieval</td><td>Standard text retrieval retrieves relevant documents by matching query strings with document strings.</td><td>LeCaRDv2, LegalQuAD legal_summarization, AILA_casedocs, AILA_statutes, rag-benchmark-finance-apple-10K-2022, financebench, TAT-QA, finance-alpaca-csv fiqa-personal-finance-dataset, finance-financialmodelingprep-stock-news-sentiments-rss-feed, ConvFinQA, finqa, hc3_finance, dialogsum, QAConv, HQA-data, LeetCodeCpp-new, LeetCodeJava-new, LeetCodePython-new, humaneval, mbpp, ds1000-referenceonly, ds1000, apps_5doc, Huffpostsports, Huffpostscience, Doordash, Healthforcalifornia, Cohere, 5GEdge, OneSignal, Langchain, PyTorch1024</td></tr></tbody></table></figure>



<p>Note that the standard text retrieval task encompasses all datasets used to evaluate <code>voyage-3</code> and <code>voyage-3-lite</code> except long context and multilingual datasets. See our <a href="https://blog.voyageai.com/2024/09/18/voyage-3/" rel="nofollow" target="_blank">previous blog post</a> for more information.</p>



<p><strong>Models</strong>. For the three multimodal tasks, we evaluate&nbsp;<code>voyage-multimodal-3</code>&nbsp;alongside four alternative multimodal embedding models: <a href="https://huggingface.co/openai/clip-vit-large-patch14-336" rel="nofollow" target="_blank">OpenAI CLIP large</a> (<code>clip-vit-large-patch14-336</code>), <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html" rel="nofollow" target="_blank">Amazon Titan Multimodal Embeddings G1</a> (<code>amazon.titan-embed-image-v1</code>), <a href="https://docs.cohere.com/reference/embed" rel="nofollow" target="_blank">Cohere multimodal v3</a> (<code>embed-multimodal-v3.0</code>), and <a href="https://huggingface.co/google/siglip-so400m-patch14-384" rel="nofollow" target="_blank">SigLIP So400M</a> (<code>siglip-so400m-patch14-384</code>). We also evaluate <a href="https://huggingface.co/vidore/colqwen2-v0.1" rel="nofollow" target="_blank">ColQwen2 v0.1</a> (<code>colqwen-v0.1</code>), a late interaction model that outputs many embeddings per document.</p>



<p>For the standard text retrieval task, we evaluate <code>voyage-multimodal-3</code> alongside <a href="https://platform.openai.com/docs/guides/embeddings" rel="nofollow" target="_blank">OpenAI v3 large</a> (<code>text-embeddings-3-large</code>), Cohere multimodal/English<sup>1</sup> v3, and <code>voyage-3</code>.</p>



<p><strong>Metrics.</strong> Given a query, we retrieve the top 10 results by cosine similarity and report the&nbsp;NDCG@10.</p>



<h3>Results</h3>



<p><strong>Multimodal retrieval</strong>. As shown in the figure below, <code>voyage-multimodal-3</code> outperforms OpenAI CLIP large, Amazon Titan Multimodal G1, Cohere multimodal v3, SigLIP So400M, and ColQwen2 v0.1 by:</p>



<ul>
<li>41.44%, 45.00%, 43.37%, 20.66%, and 6.14% on table/figure retrieval, respectively</li>



<li>26.54%, 37.68%, 25.84%, 35.62%, and 0.98% on document screenshot retrieval, respectively</li>



<li>6.55%, 5.16%, 5.86%, 3.42%, and 10.34% on text-to-photo retrieval, respectively</li>
</ul>



<p><strong>Standard text retrieval</strong>. As shown in the figure below, <code>voyage-multimodal-3</code> outperforms OpenAI v3 large and Cohere multimodal/English<sup>1</sup> v3 by 5.13% and 13.70%, respectively. The performance of <code>voyage-multimodal-3</code> is 0.05% better than that of <code>voyage-3</code>, making the two comparable in terms of retrieval accuracy for pure text documents.</p>



<figure><img data-recalc-dims="1" loading="lazy" decoding="async" width="1920" height="1080" data-attachment-id="1226" data-permalink="https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/voyage-multimodal-3_results-002-4/" data-orig-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=1920%2C1080&amp;quality=80&amp;ssl=1" data-orig-size="1920,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voyage-multimodal-3_results.002" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=300%2C169&amp;quality=80&amp;ssl=1" data-large-file="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?fit=1024%2C576&amp;quality=80&amp;ssl=1" src="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1920%2C1080&amp;quality=80&amp;ssl=1" alt="" srcset="https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?w=1920&amp;quality=80&amp;ssl=1 1920w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=300%2C169&amp;quality=80&amp;ssl=1 300w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1024%2C576&amp;quality=80&amp;ssl=1 1024w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=768%2C432&amp;quality=80&amp;ssl=1 768w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1536%2C864&amp;quality=80&amp;ssl=1 1536w, https://i0.wp.com/blog.voyageai.com/wp-content/uploads/2024/11/voyage-multimodal-3_results.002-3.png?resize=1200%2C675&amp;quality=80&amp;ssl=1 1200w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></figure>



<p>All evaluation results are available in <a href="https://docs.google.com/spreadsheets/d/1LAbDBkzO--LGR9y4FWzqhFsImO6ic5NITxgPWGmMIP4/edit?gid=222347087#gid=222347087" rel="nofollow" target="_blank">this spreadsheet</a>.</p>



<h3>Try voyage-multimodal-3 now!</h3>



<p><code>voyage-multimodal-3</code> is available today! The first 200 million tokens are free. To get started, check out our <a href="https://colab.research.google.com/drive/12aFvstG8YFAWXyw-Bx5IXtaOqOzliGt9" rel="nofollow" target="_blank">sample notebook</a>, or head over to our&nbsp;<a href="https://docs.voyageai.com/docs/multimodal-embeddings" rel="nofollow" target="_blank">docs</a>&nbsp;to learn more.</p>



<p>If you’re also interested in fine-tuned embedding models, we’d love to hear from you—please email us at&nbsp;<a href="mailto:contact@voyageai.com">contact@voyageai.com</a>. Follow us on&nbsp;<a href="https://x.com/VoyageAI" rel="nofollow" target="_blank">X (Twitter)</a>&nbsp;and&nbsp;<a href="https://www.linkedin.com/company/voyageai/" rel="nofollow" target="_blank">LinkedIn</a>,&nbsp;and join our&nbsp;<a href="https://discord.gg/zAU7GQEmvT" rel="nofollow" target="_blank">Discord</a>&nbsp;for more updates.</p>



<hr>



<p><sup>1 </sup>Cohere multimodal v3 uses Cohere English v3 (<code>embed-english-v3.0</code>) for the text tower, which makes the both models’ vectors identical on pure text. To minimize confusion, we use “Cohere multimodal v3” as the only label in the charts.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gandhi's Letter to Hitler (1940) (103 pts)]]></title>
            <link>https://www.mkgandhi.org/letters/hitler_ltr1.php</link>
            <guid>42162065</guid>
            <pubDate>Sun, 17 Nov 2024 04:55:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mkgandhi.org/letters/hitler_ltr1.php">https://www.mkgandhi.org/letters/hitler_ltr1.php</a>, See on <a href="https://news.ycombinator.com/item?id=42162065">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td>
<p>As at Wardha,<br>
December 24, 1940</p>
<p>DEAR FRIEND,</p>
<p>That I address you as a friend is no 
formality. I own no foes. My business in life has been for the past 
33 years to enlist the friendship of the whole of humanity by 
befriending mankind, irrespective of race, colour or creed.</p>
<p>I hope you will have the time and 
desire to know how a good portion of humanity who have view living 
under the influence of that doctrine of universal friendship view 
your action. We have no doubt about your bravery or devotion to your 
fatherland, nor do we believe that you are the monster described by 
your opponents. But your own writings and pronouncements and those 
of your friends and admirers leave no room for doubt that many of 
your acts are monstrous and unbecoming of human dignity, especially 
in the estimation of men like me who believe in universal 
friendliness. Such are your humiliation of Czechoslovakia, the rape 
of Poland and the swallowing of Denmark. I am aware that your view 
of life regards such spoliations as virtuous acts. But we have been 
taught from childhood to regard them as acts degrading humanity. 
Hence we cannot possibly wish success to your arms.</p>
<p>But ours is a unique position. We 
resist British Imperialism no less than Nazism. If there is a 
difference, it is in degree. One-fifth of the human race has been 
brought under the British heel by means that will not bear scrutiny. 
Our resistance to it does not mean harm to the British people. We 
seek to convert them, not to defeat them on the battle-field. Ours 
is an unarmed revolt against the British rule. But whether we 
convert them or not, we are determined to make their rule impossible 
by non-violent non-co-operation. It is a method in its nature 
indefensible. It is based on the knowledge that no spoliator can 
compass his end without a certain degree of co-operation, willing or 
compulsory, of the victim. Our rulers may have our land and bodies 
but not our souls. They can have the former only by complete 
destruction of every Indian-man, woman and child. That all may not 
rise to that degree of heroism and that a fair amount of 
frightfulness can bend the back of revolt is true but the argument 
would be beside the point. For, if a fair number of men and women be 
found in India who would be prepared without any ill will against 
the spoliators to lay down their lives rather than bend the knee to 
them, they would have shown the way to freedom from the tyranny of 
violence. I ask you to believe me when I say that you will find an 
unexpected number of such men and women in India. They have been 
having that training for the past 20 years.</p>
<p>We have been trying for the past half 
a century to throw off the British rule. The movement of 
independence has been never so strong as now. The most powerful 
political organization, I mean the Indian National Congress, is 
trying to achieve this end. We have attained a very fair measure of 
success through non-violent effort. We were groping for the right 
means to combat the most organized violence in the world which the 
British power represents. You have challenged it. It remains to be 
seen which is the better organized, the German or the British. We 
know what the British heel means for us and the non-European races 
of the world. But we would never wish to end the British rule with 
German aid. We have found in non-violence a force which, if 
organized, can without doubt match itself against a combination of 
all the most violent forces in the world. In non-violent technique, 
as I have said, there is no such thing as defeat. It is all 'do or 
die' without killing or hurting. It can be used practically without 
money and obviously without the aid of science of destruction which 
you have brought to such perfection. It is a marvel to me that you 
do not see that it is nobody's monopoly. If not the British, some 
other power will certainly improve upon your method and beat you 
with your own weapon. You are leaving no legacy to your people of 
which they would feel proud. They cannot take pride in a recital of 
cruel deed, however skilfully planned. I, therefore, appeal to you 
in the name of humanity to stop the war. You will lose nothing by 
referring all the matters of dispute between you and Great Britain 
to an international tribunal of your joint choice. If you attain 
success in the war, it will not prove that you were in the right. It 
will only prove that your power of destruction was greater. Whereas 
an award by an impartial tribunal will show as far as it is humanly 
possible which party was in the right.</p>
<p>You know that not long ago I made an 
appeal to every Briton to accept my method of non-violent 
resistance. I did it because the British know me as a friend though 
a rebel. I am a stranger to you and your people. I have not the 
courage to make you the appeal I made to every Briton. Not that it 
would not apply to you with the same force as to the British. But my 
present proposal is much simple because much more practical and familiar.</p>
<p>During this season when the hearts of 
the peoples of Europe yearn for peace, we have suspended even our 
own peaceful struggle. Is it too much to ask you to make an effort 
for peace during a time which may mean nothing to you personally but 
which must mean much to the millions of Europeans whose dumb cry for 
peace I hear, for my ears are attended to hearing the dumb millions? 
I had intended to address a joint appeal to you and Signor 
Mussolini, whom I had the privilege of meeting when I was in Rome 
during my visit to England as a delegate to the Round Table 
Conference. I hope that he will take this as addressed to him also 
with the necessary changes.</p>
<p>I am,<br>
Your sincere friend,<br>
M.K. GANDHI</p>
</td></div></div>]]></description>
        </item>
    </channel>
</rss>