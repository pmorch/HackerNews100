<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 25 Feb 2025 05:30:04 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Disclosure of personal information to DOGE “is irreparable harm,” judge rules (193 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</link>
            <guid>43167579</guid>
            <pubDate>Tue, 25 Feb 2025 02:59:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/">https://arstechnica.com/tech-policy/2025/02/judges-block-doge-access-to-personal-data-in-loss-for-trump-administration/</a>, See on <a href="https://news.ycombinator.com/item?id=43167579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<p>"The plaintiffs have made a clear showing that they are likely to suffer irreparable harm without injunctive relief," the order said. "DOGE affiliates have been granted access to systems of record that contain some of the plaintiffs' most sensitive data—Social Security numbers, dates of birth, home addresses, income and assets, citizenship status, and disability status—and their access to this trove of personal information is ongoing. There is no reason to believe their access to this information will end anytime soon because the government believes their access is appropriate."</p>
<p>The American Federation of Teachers, which represents 1.8 million teachers and nurses, was joined in the lawsuit by the International Association of Machinists and Aerospace Workers, International Federation of Professional and Technical Engineers, National Active and Retired Federal Employees Association, and National Federation of Federal Employees.</p>

<h2>No need to know</h2>
<p>The government insisted that the DOGE affiliates are employees of Education and OPM, and the judge assumed that is true for purposes of evaluating the motion for a restraining order. Even with that allowance, Boardman decided the data access is not permissible under the "need-to-know" exception to the law prohibiting unnecessary disclosure.</p>
<p>The Trump administration did not explain why "the DOGE affiliates at Education <em>need</em> such comprehensive, sweeping access to the plaintiffs' records to audit student loan programs for waste, fraud, and abuse or to conduct cost-estimate analyses," Boardman wrote, adding that "there appears to be no precedent with similar facts."</p>
<p>There are six DOGE affiliates working at Education. They include Adam Ramada, a United States DOGE Service employee, and five "DOGE-affiliated individuals" who have not been identified by name.</p>
<p>"It may be that, with additional time, the government can explain why granting such broad access to the plaintiffs' personal information is necessary for DOGE affiliates at Education to do their jobs, but for now, the record before the Court indicates they do not have a <em>need</em> for these records in the performance of their duties," Boardman wrote.</p>

          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek open source DeepEP – library for MoE training and Inference (109 pts)]]></title>
            <link>https://github.com/deepseek-ai/DeepEP</link>
            <guid>43167373</guid>
            <pubDate>Tue, 25 Feb 2025 02:27:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/deepseek-ai/DeepEP">https://github.com/deepseek-ai/DeepEP</a>, See on <a href="https://news.ycombinator.com/item?id=43167373">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">DeepEP</h2><a id="user-content-deepep" aria-label="Permalink: DeepEP" href="#deepep"></a></p>
<p dir="auto">DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also as known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.</p>
<p dir="auto">To align with the group-limited gating algorithm proposed in the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper, DeepEP offers a set of kernels optimized for asymmetric-domain bandwidth forwarding, such as forwarding data from NVLink domain to RDMA domain. These kernels deliver high throughput, making them suitable for both training and inference prefilling tasks. Additionally, they support SM (Streaming Multiprocessors) number control.</p>
<p dir="auto">For latency-sensitive inference decoding, DeepEP includes a set of low-latency kernels with pure RDMA to minimize delays. The library also introduces a hook-based communication-computation overlapping method that does not occupy any SM resource.</p>
<p dir="auto">Notice: the implementation in this library may have some slight differences from the <a href="https://github.com/deepseek-ai/DeepSeek-V3">DeepSeek-V3</a> paper.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Normal kernels with NVLink and RDMA forwarding</h3><a id="user-content-normal-kernels-with-nvlink-and-rdma-forwarding" aria-label="Permalink: Normal kernels with NVLink and RDMA forwarding" href="#normal-kernels-with-nvlink-and-rdma-forwarding"></a></p>
<p dir="auto">We test normal kernels on H800 (~160 GB/s NVLink maximum bandwidth), with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow the DeepSeek-V3/R1 pretraining setting (4096 tokens per batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Type</th>
<th>Dispatch #EP</th>
<th>Bottleneck bandwidth</th>
<th>Combine #EP</th>
<th>Bottleneck bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intranode</td>
<td>8</td>
<td>153 GB/s (NVLink)</td>
<td>8</td>
<td>158 GB/s (NVLink)</td>
</tr>
<tr>
<td>Internode</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
<td>16</td>
<td>43 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>32</td>
<td>44 GB/s (RDMA)</td>
<td>32</td>
<td>47 GB/s (RDMA)</td>
</tr>
<tr>
<td>Internode</td>
<td>64</td>
<td>46 GB/s (RDMA)</td>
<td>64</td>
<td>45 GB/s (RDMA)</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-latency kernels with pure RDMA</h3><a id="user-content-low-latency-kernels-with-pure-rdma" aria-label="Permalink: Low-latency kernels with pure RDMA" href="#low-latency-kernels-with-pure-rdma"></a></p>
<p dir="auto">We test low-latency kernels on H800 with each connected to a CX7 InfiniBand 400 Gb/s RDMA network card (~50 GB/s maximum bandwidth). And we follow a typical DeepSeek-V3/R1 production setting (128 tokens per batch, 7168 hidden, top-8 experts, FP8 dispatching and BF16 combining).</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Dispatch #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
<th>Combine #EP</th>
<th>Latency</th>
<th>RDMA bandwidth</th>
</tr>
</thead>
<tbody>
<tr>
<td>8</td>
<td>163 us</td>
<td>46 GB/s</td>
<td>8</td>
<td>318 us</td>
<td>46 GB/s</td>
</tr>
<tr>
<td>16</td>
<td>173 us</td>
<td>43 GB/s</td>
<td>16</td>
<td>329 us</td>
<td>44 GB/s</td>
</tr>
<tr>
<td>32</td>
<td>182 us</td>
<td>41 GB/s</td>
<td>32</td>
<td>350 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>64</td>
<td>186 us</td>
<td>40 GB/s</td>
<td>64</td>
<td>353 us</td>
<td>41 GB/s</td>
</tr>
<tr>
<td>128</td>
<td>192 us</td>
<td>39 GB/s</td>
<td>128</td>
<td>369 us</td>
<td>39 GB/s</td>
</tr>
<tr>
<td>256</td>
<td>194 us</td>
<td>39 GB/s</td>
<td>256</td>
<td>360 us</td>
<td>40 GB/s</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick start" href="#quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Hopper GPUs (may support more architectures or devices later)</li>
<li>Python 3.8 and above</li>
<li>CUDA 12.3 and above</li>
<li>PyTorch 2.1 and above</li>
<li>NVLink for intranode communication</li>
<li>RDMA network for internode communication</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Download and install NVSHMEM dependency</h3><a id="user-content-download-and-install-nvshmem-dependency" aria-label="Permalink: Download and install NVSHMEM dependency" href="#download-and-install-nvshmem-dependency"></a></p>
<p dir="auto">DeepEP also depends on our modified NVSHMEM. Please refer to our <a href="https://github.com/deepseek-ai/DeepEP/blob/main/third-party/README.md">NVSHMEM Installation Guide</a> for instructions.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Development</h3><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Build and make symbolic links for SO files
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
# You may modify the specific SO names according to your own platform
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

# Run test cases
# NOTES: you may modify the `init_dist` function in `tests/utils.py`
# according to your own cluster settings, and launch into multiple nodes 
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py"><pre><span><span>#</span> Build and make symbolic links for SO files</span>
NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py build
<span><span>#</span> You may modify the specific SO names according to your own platform</span>
ln -s build/lib.linux-x86_64-cpython-38/deep_ep_cpp.cpython-38-x86_64-linux-gnu.so

<span><span>#</span> Run test cases</span>
<span><span>#</span> NOTES: you may modify the `init_dist` function in `tests/utils.py`</span>
<span><span>#</span> according to your own cluster settings, and launch into multiple nodes </span>
python tests/test_intranode.py
python tests/test_internode.py
python tests/test_low_latency.py</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Installation</h3><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install"><pre>NVSHMEM_DIR=/path/to/installed/nvshmem python setup.py install</pre></div>
<p dir="auto">Then, import <code>deep_ep</code> in your Python project, and enjoy!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Network configurations</h2><a id="user-content-network-configurations" aria-label="Permalink: Network configurations" href="#network-configurations"></a></p>
<p dir="auto">DeepEP is fully tested with InfiniBand networks. However, it is theoretically compatible with RDMA over Converged Ethernet (RoCE) as well.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Traffic isolation</h3><a id="user-content-traffic-isolation" aria-label="Permalink: Traffic isolation" href="#traffic-isolation"></a></p>
<p dir="auto">Traffic isolation is supported by InfiniBand through Virtual Lanes (VL).</p>
<p dir="auto">To prevent interference between different types of traffic, we recommend segregating workloads across different virtual lanes as follows:</p>
<ul dir="auto">
<li>workloads using normal kernels</li>
<li>workloads using low-latency kernels</li>
<li>other workloads</li>
</ul>
<p dir="auto">For DeepEP, you can control the virtual lane assignment by setting the <code>NVSHMEM_IB_SL</code> environment variable.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adaptive routing</h3><a id="user-content-adaptive-routing" aria-label="Permalink: Adaptive routing" href="#adaptive-routing"></a></p>
<p dir="auto">Adaptive routing is an advanced routing feature provided by InfiniBand switches that can evenly distribute traffic across multiple paths. Currently, low-latency kernels support adaptive routing, while normal kernels do not (support may be added soon). <strong>Enabling adaptive routing for normal internode kernels may lead to deadlocks or data corruption issues</strong>.</p>
<p dir="auto">For low-latency kernels, enabling adaptive routing can completely eliminate network congestion caused by routing conflicts, but it also introduces additional latency. We recommend the following configuration for optimal performance:</p>
<ul dir="auto">
<li>enable adaptive routing in environments with heavy network loads</li>
<li>use static routing in environments with light network loads</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Congestion control</h3><a id="user-content-congestion-control" aria-label="Permalink: Congestion control" href="#congestion-control"></a></p>
<p dir="auto">Congestion control is disabled as we have not observed significant congestion in our production environment.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interfaces and examples</h2><a id="user-content-interfaces-and-examples" aria-label="Permalink: Interfaces and examples" href="#interfaces-and-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in model training or inference prefilling</h3><a id="user-content-example-use-in-model-training-or-inference-prefilling" aria-label="Permalink: Example use in model training or inference prefilling" href="#example-use-in-model-training-or-inference-prefilling"></a></p>
<p dir="auto">The normal kernels can be used in model training or the inference prefilling phase (without the backward part) as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import List, Tuple, Optional, Union

from deep_ep import Buffer, EventOverlap

# Communication buffer (will allocate at runtime)
_buffer: Optional[Buffer] = None

# Set the number of SMs to use
# NOTES: this is a static variable
Buffer.set_num_sms(24)


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, hidden_bytes: int) -> Buffer:
    global _buffer
    
    # NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests
    num_nvl_bytes, num_rdma_bytes = 0, 0
    for config in (Buffer.get_dispatch_config(group.size()), Buffer.get_combine_config(group.size())):
        num_nvl_bytes = max(config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes)
        num_rdma_bytes = max(config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes)

    # Allocate a buffer if not existed or not enough buffer size
    # NOTES: the adaptive routing configuration of the network **must be off**
    if _buffer is None or _buffer.group != group or _buffer.num_nvl_bytes < num_nvl_bytes or _buffer.num_rdma_bytes < num_rdma_bytes:
        _buffer = Buffer(group, num_nvl_bytes, num_rdma_bytes)
    return _buffer


def get_hidden_bytes(x: torch.Tensor) -> int:
    t = x[0] if isinstance(x, tuple) else x
    return t.size(1) * max(t.element_size(), 2)


def dispatch_forward(x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     topk_idx: torch.Tensor, topk_weights: torch.Tensor,
                     num_experts: int, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], torch.Tensor, torch.Tensor, List, Tuple, EventOverlap]:
    # NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency 
    # of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please
    # refer to the docs of `Buffer.dispatch`
    global _buffer

    # Calculate layout before actual dispatch
    num_tokens_per_rank, num_tokens_per_rdma_rank, num_tokens_per_expert, is_token_in_rank, previous_event = \
        _buffer.get_dispatch_layout(topk_idx, num_experts,
                                    previous_event=previous_event, async_finish=True,
                                    allocate_on_comm_stream=previous_event is not None)
    # Do MoE dispatch
    # NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph
    # For more advanced usages, please refer to the docs of the `dispatch` function
    recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event = \
        _buffer.dispatch(x, topk_idx=topk_idx, topk_weights=topk_weights,
                         num_tokens_per_rank=num_tokens_per_rank, num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,
                         is_token_in_rank=is_token_in_rank, num_tokens_per_expert=num_tokens_per_expert,
                         previous_event=previous_event, async_finish=True,
                         allocate_on_comm_stream=True)
    # For event management, please refer to the docs of the `EventOverlap` class
    return recv_x, recv_topk_idx, recv_topk_weights, num_recv_tokens_per_expert_list, handle, event


def dispatch_backward(grad_recv_x: torch.Tensor, grad_recv_topk_weights: torch.Tensor, handle: Tuple) -> \
        Tuple[torch.Tensor, torch.Tensor, EventOverlap]:
    global _buffer

    # The backward process of MoE dispatch is actually a combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_grad_x, combined_grad_recv_topk_weights, event = \
        _buffer.combine(grad_recv_x, handle, topk_weights=grad_recv_topk_weights, async_finish=True)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_grad_x, combined_grad_recv_topk_weights, event


def combine_forward(x: torch.Tensor, handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[torch.Tensor, EventOverlap]:
    global _buffer

    # Do MoE combine
    # For more advanced usages, please refer to the docs of the `combine` function
    combined_x, _, event = _buffer.combine(x, handle, async_finish=True, previous_event=previous_event,
                                           allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return combined_x, event


def combine_backward(grad_combined_x: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],
                     handle: Tuple, previous_event: Optional[EventOverlap] = None) -> \
        Tuple[Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]], EventOverlap]:
    global _buffer

    # The backward process of MoE combine is actually a dispatch
    # For more advanced usages, please refer to the docs of the `combine` function
    grad_x, _, _, _, _, event = _buffer.dispatch(grad_combined_x, handle=handle, async_finish=True,
                                                 previous_event=previous_event,
                                                 allocate_on_comm_stream=previous_event is not None)

    # For event management, please refer to the docs of the `EventOverlap` class
    return grad_x, event"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>List</span>, <span>Tuple</span>, <span>Optional</span>, <span>Union</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>, <span>EventOverlap</span>

<span># Communication buffer (will allocate at runtime)</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>

<span># Set the number of SMs to use</span>
<span># NOTES: this is a static variable</span>
<span>Buffer</span>.<span>set_num_sms</span>(<span>24</span>)


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>hidden_bytes</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span>global</span> <span>_buffer</span>
    
    <span># NOTES: you may also replace `get_*_config` with your auto-tuned results via all the tests</span>
    <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span> <span>=</span> <span>0</span>, <span>0</span>
    <span>for</span> <span>config</span> <span>in</span> (<span>Buffer</span>.<span>get_dispatch_config</span>(<span>group</span>.<span>size</span>()), <span>Buffer</span>.<span>get_combine_config</span>(<span>group</span>.<span>size</span>())):
        <span>num_nvl_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_nvl_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_nvl_bytes</span>)
        <span>num_rdma_bytes</span> <span>=</span> <span>max</span>(<span>config</span>.<span>get_rdma_buffer_size_hint</span>(<span>hidden_bytes</span>, <span>group</span>.<span>size</span>()), <span>num_rdma_bytes</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span># NOTES: the adaptive routing configuration of the network **must be off**</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>_buffer</span>.<span>num_nvl_bytes</span> <span>&lt;</span> <span>num_nvl_bytes</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>num_nvl_bytes</span>, <span>num_rdma_bytes</span>)
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>get_hidden_bytes</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>) <span>-&gt;</span> <span>int</span>:
    <span>t</span> <span>=</span> <span>x</span>[<span>0</span>] <span>if</span> <span>isinstance</span>(<span>x</span>, <span>tuple</span>) <span>else</span> <span>x</span>
    <span>return</span> <span>t</span>.<span>size</span>(<span>1</span>) <span>*</span> <span>max</span>(<span>t</span>.<span>element_size</span>(), <span>2</span>)


<span>def</span> <span>dispatch_forward</span>(<span>x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>,
                     <span>num_experts</span>: <span>int</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>List</span>, <span>Tuple</span>, <span>EventOverlap</span>]:
    <span># NOTES: an optional `previous_event` means a CUDA event captured that you want to make it as a dependency </span>
    <span># of the dispatch kernel, it may be useful with communication-computation overlap. For more information, please</span>
    <span># refer to the docs of `Buffer.dispatch`</span>
    <span>global</span> <span>_buffer</span>

    <span># Calculate layout before actual dispatch</span>
    <span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span>, <span>num_tokens_per_expert</span>, <span>is_token_in_rank</span>, <span>previous_event</span> <span>=</span> \
        <span>_buffer</span>.<span>get_dispatch_layout</span>(<span>topk_idx</span>, <span>num_experts</span>,
                                    <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                    <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)
    <span># Do MoE dispatch</span>
    <span># NOTES: the CPU will wait for GPU's signal to arrive, so this is not compatible with CUDA graph</span>
    <span># For more advanced usages, please refer to the docs of the `dispatch` function</span>
    <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>dispatch</span>(<span>x</span>, <span>topk_idx</span><span>=</span><span>topk_idx</span>, <span>topk_weights</span><span>=</span><span>topk_weights</span>,
                         <span>num_tokens_per_rank</span><span>=</span><span>num_tokens_per_rank</span>, <span>num_tokens_per_rdma_rank</span><span>=</span><span>num_tokens_per_rdma_rank</span>,
                         <span>is_token_in_rank</span><span>=</span><span>is_token_in_rank</span>, <span>num_tokens_per_expert</span><span>=</span><span>num_tokens_per_expert</span>,
                         <span>previous_event</span><span>=</span><span>previous_event</span>, <span>async_finish</span><span>=</span><span>True</span>,
                         <span>allocate_on_comm_stream</span><span>=</span><span>True</span>)
    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>recv_x</span>, <span>recv_topk_idx</span>, <span>recv_topk_weights</span>, <span>num_recv_tokens_per_expert_list</span>, <span>handle</span>, <span>event</span>


<span>def</span> <span>dispatch_backward</span>(<span>grad_recv_x</span>: <span>torch</span>.<span>Tensor</span>, <span>grad_recv_topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE dispatch is actually a combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span> <span>=</span> \
        <span>_buffer</span>.<span>combine</span>(<span>grad_recv_x</span>, <span>handle</span>, <span>topk_weights</span><span>=</span><span>grad_recv_topk_weights</span>, <span>async_finish</span><span>=</span><span>True</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_grad_x</span>, <span>combined_grad_recv_topk_weights</span>, <span>event</span>


<span>def</span> <span>combine_forward</span>(<span>x</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>combined_x</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>combine</span>(<span>x</span>, <span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>, <span>previous_event</span><span>=</span><span>previous_event</span>,
                                           <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>combined_x</span>, <span>event</span>


<span>def</span> <span>combine_backward</span>(<span>grad_combined_x</span>: <span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]],
                     <span>handle</span>: <span>Tuple</span>, <span>previous_event</span>: <span>Optional</span>[<span>EventOverlap</span>] <span>=</span> <span>None</span>) <span>-&gt;</span> \
        <span>Tuple</span>[<span>Union</span>[<span>torch</span>.<span>Tensor</span>, <span>Tuple</span>[<span>torch</span>.<span>Tensor</span>, <span>torch</span>.<span>Tensor</span>]], <span>EventOverlap</span>]:
    <span>global</span> <span>_buffer</span>

    <span># The backward process of MoE combine is actually a dispatch</span>
    <span># For more advanced usages, please refer to the docs of the `combine` function</span>
    <span>grad_x</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>_</span>, <span>event</span> <span>=</span> <span>_buffer</span>.<span>dispatch</span>(<span>grad_combined_x</span>, <span>handle</span><span>=</span><span>handle</span>, <span>async_finish</span><span>=</span><span>True</span>,
                                                 <span>previous_event</span><span>=</span><span>previous_event</span>,
                                                 <span>allocate_on_comm_stream</span><span>=</span><span>previous_event</span> <span><span>is</span> <span>not</span></span> <span>None</span>)

    <span># For event management, please refer to the docs of the `EventOverlap` class</span>
    <span>return</span> <span>grad_x</span>, <span>event</span></pre></div>
<p dir="auto">Moreover, inside the dispatch function, we may not know how many tokens to receive for the current rank. So an implicit CPU wait for GPU received count signal will be involved, as the following figure shows.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/normal.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/normal.png" alt="normal"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Example use in inference decoding</h3><a id="user-content-example-use-in-inference-decoding" aria-label="Permalink: Example use in inference decoding" href="#example-use-in-inference-decoding"></a></p>
<p dir="auto">The low latency kernels can be used in the inference decoding phase as the below example code shows.</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
import torch.distributed as dist
from typing import Tuple, Optional

from deep_ep import Buffer

# Communication buffer (will allocate at runtime)
# NOTES: there is no SM control API for the low-latency kernels
_buffer: Optional[Buffer] = None


# You may call this function at the framework initialization
def get_buffer(group: dist.ProcessGroup, num_max_dispatch_tokens_per_rank: int, hidden: int, num_experts: int) -> Buffer:
    # NOTES: the low-latency mode will consume much more space than the normal mode
    # So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256
    global _buffer
    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts)

    # Allocate a buffer if not existed or not enough buffer size
    if _buffer is None or _buffer.group != group or not _buffer.low_latency_mode or _buffer.num_rdma_bytes < num_rdma_bytes:
        # NOTES: for best performance, the QP number **must** be equal to the number of the local experts
        assert num_experts % group.size() == 0
        _buffer = Buffer(group, 0, num_rdma_bytes, low_latency_mode=True, num_qps_per_rank=num_experts // group.size())
    return _buffer


def low_latency_dispatch(hidden_states: torch.Tensor, topk_idx: torch.Tensor, num_max_dispatch_tokens_per_rank: int, num_experts: int):
    global _buffer

    # Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)
    recv_hidden_states, recv_expert_count, handle, event, hook = \
        _buffer.low_latency_dispatch(hidden_states, topk_idx, num_max_dispatch_tokens_per_rank, num_experts,
                                     async_finish=False, return_recv_hook=True)

    # NOTES: the actual tensor will not be received only if you call `hook()`,
    # it is useful for double-batch overlapping, but **without any SM occupation**
    # If you don't want to overlap, please set `return_recv_hook=False`
    # Later, you can use our GEMM library to do the computation with this specific format
    return recv_hidden_states, recv_expert_count, handle, event, hook


def low_latency_combine(hidden_states: torch.Tensor,
                        topk_idx: torch.Tensor, topk_weights: torch.Tensor, handle: Tuple):
    global _buffer

    # Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)
    combined_hidden_states, event_overlap, hook = \
        _buffer.low_latency_combine(hidden_states, topk_idx, topk_weights, handle,
                                    async_finish=False, return_recv_hook=True)

    # NOTES: the same behavior as described in the dispatch kernel
    return combined_hidden_states, event_overlap, hook"><pre><span>import</span> <span>torch</span>
<span>import</span> <span>torch</span>.<span>distributed</span> <span>as</span> <span>dist</span>
<span>from</span> <span>typing</span> <span>import</span> <span>Tuple</span>, <span>Optional</span>

<span>from</span> <span>deep_ep</span> <span>import</span> <span>Buffer</span>

<span># Communication buffer (will allocate at runtime)</span>
<span># NOTES: there is no SM control API for the low-latency kernels</span>
<span>_buffer</span>: <span>Optional</span>[<span>Buffer</span>] <span>=</span> <span>None</span>


<span># You may call this function at the framework initialization</span>
<span>def</span> <span>get_buffer</span>(<span>group</span>: <span>dist</span>.<span>ProcessGroup</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>hidden</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>) <span>-&gt;</span> <span>Buffer</span>:
    <span># NOTES: the low-latency mode will consume much more space than the normal mode</span>
    <span># So we recommend that `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256</span>
    <span>global</span> <span>_buffer</span>
    <span>num_rdma_bytes</span> <span>=</span> <span>Buffer</span>.<span>get_low_latency_rdma_size_hint</span>(<span>num_max_dispatch_tokens_per_rank</span>, <span>hidden</span>, <span>group</span>.<span>size</span>(), <span>num_experts</span>)

    <span># Allocate a buffer if not existed or not enough buffer size</span>
    <span>if</span> <span>_buffer</span> <span>is</span> <span>None</span> <span>or</span> <span>_buffer</span>.<span>group</span> <span>!=</span> <span>group</span> <span>or</span> <span>not</span> <span>_buffer</span>.<span>low_latency_mode</span> <span>or</span> <span>_buffer</span>.<span>num_rdma_bytes</span> <span>&lt;</span> <span>num_rdma_bytes</span>:
        <span># NOTES: for best performance, the QP number **must** be equal to the number of the local experts</span>
        <span>assert</span> <span>num_experts</span> <span>%</span> <span>group</span>.<span>size</span>() <span>==</span> <span>0</span>
        <span>_buffer</span> <span>=</span> <span>Buffer</span>(<span>group</span>, <span>0</span>, <span>num_rdma_bytes</span>, <span>low_latency_mode</span><span>=</span><span>True</span>, <span>num_qps_per_rank</span><span>=</span><span>num_experts</span> <span>//</span> <span>group</span>.<span>size</span>())
    <span>return</span> <span>_buffer</span>


<span>def</span> <span>low_latency_dispatch</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>num_max_dispatch_tokens_per_rank</span>: <span>int</span>, <span>num_experts</span>: <span>int</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE dispatch, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_dispatch</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>num_max_dispatch_tokens_per_rank</span>, <span>num_experts</span>,
                                     <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the actual tensor will not be received only if you call `hook()`,</span>
    <span># it is useful for double-batch overlapping, but **without any SM occupation**</span>
    <span># If you don't want to overlap, please set `return_recv_hook=False`</span>
    <span># Later, you can use our GEMM library to do the computation with this specific format</span>
    <span>return</span> <span>recv_hidden_states</span>, <span>recv_expert_count</span>, <span>handle</span>, <span>event</span>, <span>hook</span>


<span>def</span> <span>low_latency_combine</span>(<span>hidden_states</span>: <span>torch</span>.<span>Tensor</span>,
                        <span>topk_idx</span>: <span>torch</span>.<span>Tensor</span>, <span>topk_weights</span>: <span>torch</span>.<span>Tensor</span>, <span>handle</span>: <span>Tuple</span>):
    <span>global</span> <span>_buffer</span>

    <span># Do MoE combine, compatible with CUDA graph (but you may restore some buffer status once you replay)</span>
    <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span> <span>=</span> \
        <span>_buffer</span>.<span>low_latency_combine</span>(<span>hidden_states</span>, <span>topk_idx</span>, <span>topk_weights</span>, <span>handle</span>,
                                    <span>async_finish</span><span>=</span><span>False</span>, <span>return_recv_hook</span><span>=</span><span>True</span>)

    <span># NOTES: the same behavior as described in the dispatch kernel</span>
    <span>return</span> <span>combined_hidden_states</span>, <span>event_overlap</span>, <span>hook</span></pre></div>
<p dir="auto">For two micro-batch overlapping, you can refer to the following figure. With our receiving hook interface, the RDMA network traffics are happening in the background, without costing any GPU SMs from the computation part. But notice, the overlapped parts can be adjusted, i.e. the 4 parts of attention/dispatch/MoE/combine may not have the exact same execution time. You may adjust the stage settings according to your workload.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/deepseek-ai/DeepEP/blob/main/figures/low-latency.png"><img src="https://github.com/deepseek-ai/DeepEP/raw/main/figures/low-latency.png" alt="low-latency"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Notices</h2><a id="user-content-notices" aria-label="Permalink: Notices" href="#notices"></a></p>
<ul dir="auto">
<li>For extreme performance, we discover and use a behavior-out-of-doc PTX instruction: <code>ld.global.nc.L1::no_allocate.L2::256B</code>. This instruction will lead to an undefined behavior: accessing volatile GPU memory with non-coherent read-only PTX modifiers <code>.nc</code>. But the correctness is tested to be guaranteed with <code>.L1::no_allocate</code> on Hopper architectures, and performance will be much better. If you find kernels not working on some other platforms, you may add <code>DISABLE_AGGRESSIVE_PTX_INSTRS=1</code> to <code>setup.py</code> and disable this, or file an issue.</li>
<li>For better performance on your cluster, we recommend to run all the tests and use the best auto-tuned configuration. The default configurations are optimized on the DeepSeek's internal cluster.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This code repository is released under <a href="https://github.com/deepseek-ai/DeepEP/blob/main/LICENSE">the MIT License</a>, except for codes that reference NVSHMEM (including <code>csrc/kernels/ibgda_device.cuh</code> and <code>third-party/nvshmem.patch</code>), which are subject to <a href="https://docs.nvidia.com/nvshmem/api/sla.html" rel="nofollow">NVSHMEM SLA</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<p dir="auto">If you use this codebase, or otherwise found our work valuable, please cite:</p>
<div dir="auto" data-snippet-clipboard-copy-content="@misc{deepep2025,
      title={DeepEP: an efficient expert-parallel communication library},
      author={Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepEP}},
}"><pre><span>@misc</span>{<span>deepep2025</span>,
      <span>title</span>=<span><span>{</span>DeepEP: an efficient expert-parallel communication library<span>}</span></span>,
      <span>author</span>=<span><span>{</span>Chenggang Zhao and Shangyan Zhou and Liyue Zhang and Chengqi Deng and Zhean Xu and Yuxuan Liu and Kuai Yu and Jiashi Li and Liang Zhao<span>}</span></span>,
      <span>year</span>=<span><span>{</span>2025<span>}</span></span>,
      <span>publisher</span> = <span><span>{</span>GitHub<span>}</span></span>,
      <span>howpublished</span> = <span><span>{</span>\url{https://github.com/deepseek-ai/DeepEP}<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DigiCert: Threat of legal action to stifle Bugzilla discourse (132 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</link>
            <guid>43167087</guid>
            <pubDate>Tue, 25 Feb 2025 01:40:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">https://bugzilla.mozilla.org/show_bug.cgi?id=1950144</a>, See on <a href="https://news.ycombinator.com/item?id=43167087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1950144">Bug 1950144</a>
      </span>
      <span>
        <span>Opened <span title="2025-02-24 08:36 PST" data-time="1740414973">13 hours ago</span></span>
          <span>Updated <span title="2025-02-24 20:23 PST" data-time="1740457405">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>


<div id="module-categories">
        <p><span id="field-value-component">
      <div>
        <p><span id="component-name" tabindex="0" role="button" aria-haspopup="menu" aria-controls="component-info">CA Certificate Root Program
          
        </span></p>
      </div>
        </span>
    </p></div>






































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;136.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;128.7.0esr&quot;,&quot;FIREFOX_ESR115&quot;:&quot;115.20.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;137.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2025-02-03&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2025-02-04&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2025-01-30&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2025-01-31&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;136.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;135.0.1&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2025-03-03&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2025-03-04&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2025-02-27&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2025-02-28&quot;}">



<div id="c0" data-comment-id="17364853"><p>In <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322#c74" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322 comment 74</a> DigiCert wrote,</p>
<blockquote>
<p>“We have not used a legal team as a shield against accountability.”</p>
</blockquote>
<p>Contrary to this statement, I received a letter from DigiCert’s lawyers, Wilson Sonsini, regarding posts made by Sectigo’s Chief Compliance Officer in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1910322" title="RESOLVED FIXED - DigiCert: Random value in CNAME without underscore prefix">bug 1910322</a>.  The upshot of the letter was that DigiCert expected Sectigo to “ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization.”</p>
<p>I’m Brian Holland, General Counsel for Sectigo, and this is my first time posting on Bugzilla.  I’m posting because at Sectigo we believe that the WebPKI is best served by open, transparent, and honest debate about issues that impact our community.  Attempts to shut down these conversations, through lawyers or otherwise, are harmful to our collective core mission.</p>
<p>In its opening passages, this letter reads (emphasis mine),</p>
<blockquote>
<p>We ask for your prompt cooperation and assistance in taking corrective action and <strong>forcing Mr. Callan to cease his disparaging public statements. We hope your assistance in this matter will render unnecessary legal action by DigiCert against Sectigo.</strong></p>
</blockquote>
<p>After three pages of detail about specific Bugzilla posts and references to the Lanham Act, deceptive trade practices, corporate disparagement, and tortious interference, the letter (the full letter is included as an attachment to this bug) goes on to say (emphasis mine):</p>
<blockquote>
<p>At this point, we are bringing this situation to your attention on behalf of DigiCert because we are hopeful that Mr. Callan’s actions were the actions of one individual and were not part of an organized plan or institutional practice. We also hope that, upon receiving this information, Sectigo will recognize the impropriety of Mr. Callan’s statements and the substantial public, industry, and browser scrutiny and legal risk such statements would prompt if they were to continue. To that end, we expect that Sectigo will investigate this incident promptly and take the appropriate corrective actions, confirm that this situation was not part of an institutional practice, and <strong>ensure that Mr. Callan’s statements do not continue and will not be repeated by any other member of Sectigo’s organization. We hope we can resolve this situation as soon as possible before DigiCert is compelled to seek legal action.</strong></p>
</blockquote>
<p>On December 10, 2024 I sent this response in email to my contact at Wilson Sonsini:</p>
<blockquote>
<p>I have reviewed your letter and the Bugzilla thread referenced therein.  In that letter, you suggest that DigiCert has various legal claims against Sectigo and/or its COO [sic], Tim Callan, for what you call “false and misleading statements about DigiCert” made on the Bugzilla forum.  We strongly disagree.  The statements you point to are questions and/or statements of opinion that are not actionable statements of fact.  Moreover, those comments were made with the intent of facilitating discussion and debate about important questions of first impression for our industry.  They were made by Tim Callan in good faith, are fully protected by the First Amendment, and cannot, as a matter of law, form the basis for any of the causes of action mentioned in your letter.</p>
</blockquote>
<blockquote>
<p>As you are aware, the PKI community is a self-regulating group that, as set out in the bylaws of the Certificate Authority Browser Forum, works “closely together in defining the guidelines and means of implementation for best practices as a way of providing a heightened security for Internet transactions and creating a more intuitive method of displaying secure sites to Internet users.”  For the community to self-regulate, there needs to be open, uninhibited, and robust discussion and debate about best practices in the industry.  Any litigation threats that chill or stifle such debate undermine the self-regulatory system that has worked so well for the industry.</p>
</blockquote>
<blockquote>
<p>Certificate Authorities post incident reports on Bugzilla to “provide lessons learned and transparency about the steps the CA Owner takes to address the immediate issue and prevent future issues.” As the Common CA Database goes on to state “incident reports help the Web PKI ecosystem as a whole because they promote continuous improvement, information sharing, and highlight opportunities to define and adopt improved practices, policies, and controls” of all parties.</p>
</blockquote>
<blockquote>
<p>The TRO involved in this incident report, as one Bugzilla commenter noted, is “an unprecedented event in the WebPKI, and . . . if allowed to proliferate, it would potentially be used by subscribers en masse to do an end-run around important technical security controls.”</p>
</blockquote>
<blockquote>
<p>The PKI Community has never considered how it should respond to TROs and now needs to do so. Understanding the situation faced by your client and why it made certain decisions is important to improving the WebPKI ecosystem. This is why Mr. Callan, and many others, have been asking questions – some of which have been critical questions designed to achieve a consensus as to how best handle situations like this in the future.  In any such discussion, there will be differences of opinion, but open, uninhibited, robust, and transparent discussion is essential for the industry to learn how to best move forward.</p>
</blockquote>
<blockquote>
<p>I hope that your client will, on deeper reflection, realize that as a leader in the PKI Community, it should be driving, rather than stifling, discussion of this topic.  Your client’s threat of litigation is, in our view, both misguided and without merit.  We will strive to be respectful in our tone, but neither Mr. Callan nor Sectigo will be silenced or prevented from asking critical questions and/or engaging in critical discussion about issues of substantial concern to the public and the industry.</p>
</blockquote>
<p>We find the threat of legal action to stifle scrutiny and discussion of public CA practices to be deeply troubling and entirely at odds with the transparent, blameless post-mortem culture that the CCADB incident report guidelines expect CAs to embrace.  Even for a company like Sectigo, the threat of a lawsuit from a well-resourced organization like DigiCert is worrisome, regardless of our confidence that Mr. Callan’s speech was proper, legally protected, and in the best interest of the WebPKI.  Another party challenging DigiCert’s behavior, faced with this same threat, might choose simply to stop asking uncomfortable questions.</p>
<p>No CA should be allowed to intimidate its critics into silence. This would irreparably damage the integrity and quality of the WebPKI.</p>
<p>I am sharing this incident to bring attention to DigiCert’s actions and allow the community to evaluate this approach. What began as a discussion of the threat posed by certificate subscribers using the legal system to circumvent WebPKI security controls needs, in my opinion, to be broadened.</p>
</div><div id="a3699_1689"><p>Component: CA Certificate Compliance → CA Certificate Root Program</p></div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="1">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everyone at NSF overseeing the Platforms for Wireless Experimentation is gone (267 pts)]]></title>
            <link>https://discuss.systems/@ricci/114059690609284323</link>
            <guid>43166830</guid>
            <pubDate>Tue, 25 Feb 2025 00:59:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://discuss.systems/@ricci/114059690609284323">https://discuss.systems/@ricci/114059690609284323</a>, See on <a href="https://news.ycombinator.com/item?id=43166830">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It's still worth blogging in the age of AI (146 pts)]]></title>
            <link>https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</link>
            <guid>43166761</guid>
            <pubDate>Tue, 25 Feb 2025 00:46:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai">https://www.gilesthomas.com/2025/02/blogging-in-the-age-of-ai</a>, See on <a href="https://news.ycombinator.com/item?id=43166761">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            

            <div data-current-dropdown="" hx-on="click:
                    if (event.target.closest('.dropdown')) {
                        let targetId = event.target.closest('.dropdown').dataset.target;
                        this.dataset.currentDropdown = (this.dataset.currentDropdown === targetId) ? '' : targetId;
                        event.stopPropagation();
                    }">
                    
                        <p>
                            Archives <span></span>
                        </p>
                    
                    
                        <p>
                            Categories <span></span>
                        </p>
                    
                    <p>
                        Blogroll <span></span>
                    </p>
                </div>

            

    

    

    <p>My post about blogging as
<a href="https://www.gilesthomas.com/2025/02/20250223-til-deep-dive-posts">writing the tutorial that you wished you'd found</a>
really took off
<a href="https://news.ycombinator.com/item?id=43154666">on Hacker News</a>.  There were
a lot of excellent comments, but one thing kept coming up: what's the point
in blogging if people are using ChatGPT, Claude and DeepSeek to spoon-feed them
answers?  Who, apart from the AIs, will read what you write?</p>

<p>I was asking myself the same question when I started blogging semi-regularly again
last year, and this post is an attempt to summarise why I decided that it was worthwhile.
The TL;DR: <strong>blogging isn't just about being read -- it's about learning and thinking, and
having a durable proof that you can do both.</strong></p>


    
        <p>Let's start off by summarising the two big reasons to blog about what you've learned,
as you learn:</p>

<ul>
<li>It helps you make your newly-gained knowledge concrete.</li>
<li>It will help other people in the future -- they might be looking for the information
you blogged about, and find it on your blog.</li>
</ul>

<p>When we're thinking about AI, it's only the second one that matters; you'll learn better
by writing whether or not other people or LLMs read it.  But in terms of
helping other people, these days
you might publish your hard-earned learnings on <a href="https://www.gilesthomas.com/2021/03/fun-with-network-namespaces">Linux Network Namespaces</a>,
but when, the next day, someone wants to find out how to use them, they ask
ChatGPT, it does a search, finds your page, ingests it, and presents the results
as its own, perhaps mashed up with some scraps from elsewhere.  Sure, your site is probably
linked in the "references" section in the response, but frankly, no-one ever looks at that.
What's worse, within the next six months your site is likely to be sucked into the
AIs' next training run, and after that you won't even get a reference.</p>

<p>Now if the "solving other people's problems" aspect of blogging was purely altruistic,
that wouldn't matter a jot.  But of course it's not, there are a bunch of other reasons.
Three that come to mind:</p>

<ol>
<li>Making a name for yourself.</li>
<li>The sheer dopamine hit of knowing that other people like what you've done --
a higher-effort version of getting an upvote or like on social media.</li>
<li>Building a portfolio of writing you can point to.</li>
</ol>

<p>Let's take those in turn.</p>

<p>If you want to blog to make a name for yourself, then you're going to have a hard
time.  Here's an example: if you're not a regular reader of this blog, where do
I (as in, the author of this post) live?  What is my day job?  No cheating and
clicking on the "About" link above, please.</p>

<p>If you knew the answer, you're one of a rare few.  Yesterday there
were about 35,000 visits to this site thanks to that HN link, and fewer than 300 hits
on the "About" page.  This is normal!  If you write a blog post, then even if people
find it interesting, they'll come, read it, hopefully think that it was worth their
time, and then move on.  That is how it should be, there's no need for someone to
become fascinated with your life just because you said something useful once -- and
that's a good thing, no-one wants a stalker.</p>

<p>Even if you churn out banger posts again and again, as a pure blogger you're not going
to build up a "personal brand" that's worth much.</p>

<p>Think about the well-known bloggers you read: they’re famous because
they did something else
important.  They started a major open-source project, or a company, or invented something.
They give regular talks at conferences.  They write successful science fiction.
Or something else.</p>

<p>So, I don't think you can make a name for yourself by blogging alone, and if you
are blogging with that as your goal, I fear you're going to be disappointed.</p>

<p>The dopamine hit is definitely more of a thing.  When people comment on my
posts, I get a nice warm glow.  And when last night, just before I went to bed, I saw that
my previous post was #1 on the front page of HN, I took a screenshot and posted it
to my "Fellow Geeks" WhatsApp group with the caption "w00t!".</p>

<p>But those moments
are rare, and I don't really think AI will make them rarer.
Blogging can sometimes feel like you're
shouting into the void -- most posts get no engagement, and that has been true since
I started back in 2006.  You might have 500 loyal
readers, or none -- there's no way to tell.</p>

<p>I think that all I can say regarding that is to echo what serviceberry
<a href="https://news.ycombinator.com/item?id=43155587">said on HN</a> (bold mine):</p>

<blockquote>
  <p>The corollary is that if you find that post, <strong>say something</strong>. Drop the author a
  note, leave a comment. No one else does. For every YT celebrity, there are
  thousands of people posting good content on the internet and not knowing if
  it's being seen or appreciated by anyone.</p>
</blockquote>

<p>...and maybe suggest that we all occasionally check the references in our helpful AI-generated
responses and drop a line to the authors to say "thanks"!</p>

<p>But let's finish with the last one, which is more positive.  I said that you will
be vanishingly unlikely to make a name for yourself with blogging on its own.  But that
doesn't mean it's pointless from a career perspective.  You're building up a portfolio
of writing about topics that interest you.  Imagine you're in a job interview and
are asked about X.  You reply with the details you know, and add
"but I blogged about that in detail a while back, shall I send you a link later?"
Or if you're aiming to close a contract with a potential consulting client in a
particular area -- wouldn't it be useful to send them a list of links showing your
thoughts on aspects of exactly that topic?</p>

<p>Your GitHub profile shows your contributions to open source and lets people know
how well you can code.  But your blog shows your contributions to knowledge, and
shows how well you can think.  That's valuable!</p>

<p>It's time to wrap this up.  Blogging is valuable because it helps you learn, because it
helps others solve problems, because you get a rare buzz when you realise that yes,
people are reading this stuff, and because you're building a portfolio of writing to show
your skills.  The only one of those that I believe AI might harm is the buzz of
engagement, and that's so rare for most blogs that I don't think it's worth worrying about.</p>

<p>And after all -- if the AI doom scenario does come true, at least as someone whose
thoughts have been regularly published on the Internet, you'll be part of the
paperclip maximisers' training set, so they'll remember you in a sense.  So
there's that.</p>

    

    
        
    

    



            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clean Code vs. A Philosophy Of Software Design (124 pts)]]></title>
            <link>https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</link>
            <guid>43166362</guid>
            <pubDate>Mon, 24 Feb 2025 23:52:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md">https://github.com/johnousterhout/aposd-vs-clean-code/blob/main/README.md</a>, See on <a href="https://news.ycombinator.com/item?id=43166362">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true" aria-labelledby="file-name-id-wide file-name-id-mobile"><article itemprop="text"><p dir="auto"><em>(This document is the result of a series of discussions, some online and
some in person, held between Robert "Uncle Bob" Martin and John Ousterhout between
September, 2024 and February, 2025. If you would like to comment on anything
in this discussion, we recommend that you do so on the <a href="https://groups.google.com/g/software-design-book" rel="nofollow">Google group
associated with APOSD</a>)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introductions</h2><a id="user-content-introductions" aria-label="Permalink: Introductions" href="#introductions"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Hi (Uncle) Bob! You and I have each written books on software design.
We agree on some things, but there are some pretty big differences of
opinion between my recent book <em>A Philosophy of Software Design</em>
(hereafter "APOSD") and your classic book <em>Clean Code</em>. Thanks for
agreeing to discuss those differences here.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My pleasure John.  Before we begin let me say that I've carefully read through your book and I found it very enjoyable, and full of valuable insights.  There are some things I disagree with you on, such as TDD, and Abstraction-First incrementalism, but overall I enjoyed it a lot.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I'd like to discuss three topics with you: method length, comments,
and test-driven development. But before getting into these,
let's start by comparing overall philosophies. When you hear about a
new idea related to software design, how do you decide whether or not
to endorse that idea?</p>
<p dir="auto">I'll go first. For me, the fundamental goal of software design is
to make it easy to understand and modify the system. I use the term
"complexity" to refer to things that make it hard to understand and
modify a system. The most important contributors
to complexity relate to information:</p>
<ul dir="auto">
<li>How much information must a developer have in their head in order to carry out a task?</li>
<li>How accessible and obvious is the information that the developer needs?</li>
</ul>
<p dir="auto">The more information a developer needs to have, the harder it will be
for them to work on the system. Things get even worse if the required
information isn't obvious. The worst case is when there is a crucial
piece of information hidden in some far-away piece of code
that the developer has never heard of.</p>
<p dir="auto">When I'm evaluating an idea related to software design, I ask whether
it will reduce complexity. This usually means either reducing the amount
of information a developer has to know, or making the required information
more obvious.</p>
<p dir="auto">Now over to you: are there general principles that you use when deciding
which ideas to endorse?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree with your approach. A discipline or technique should make the job of programmers easier. I would add that the programmer we want to help most is not the author.  The programmer whose job we want to make easier is the programmer who must read and understand the code written by others (or by themself a week later).  Programmers spend far more hours reading code than writing code, so the activity we want to ease is that of reading.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Method Length</h2><a id="user-content-method-length" aria-label="Permalink: Method Length" href="#method-length"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our first area of disagreement is method length.
On page 34 of <em>Clean Code</em> you say "The first rule of functions is that
they should be small. The second rule of functions is that
<em>they should be smaller than that</em>." Later on, you say "Functions
should hardly ever be 20 lines long" and suggest that functions
should be "just two, three, or four lines long". On page 35, you
say "Blocks within <code>if</code> statements, <code>else</code> statements, <code>while</code> statements,
and so on should be one line long. Probably that line should be a function
call." I couldn't find anything in <em>Clean Code</em> to suggest that a function
could ever be too short.</p>
<p dir="auto">I agree that dividing up code into relatively small units ("modular design")
is one of the most important ways to reduce the amount of information a
programmer has to keep in their mind at once. The idea, of course, is to take a
complex chunk of functionality and encapsulate it in a separate method
with a simple interface. Developers can then harness the functionality
of the method (or read code that invokes the method) without learning
the details of how the method is implemented; they only need to learn its
interface. The best methods are those that provide a lot of functionality
but have a very simple interface: they replace a large cognitive load
(reading the detailed implementation) with a much smaller
cognitive load (learning the interface). I call these methods "deep".</p>
<p dir="auto">However, like most ideas in software design, decomposition can be taken too far.
As methods get smaller and smaller there is less and less
benefit to further subdivision.
The amount of functionality hidden behind each interface
drops, while the interfaces often become more complex.
I call these interfaces "shallow": they don't help much in terms of
reducing what the programmer needs to know. Eventually, the point is
reached where someone using the method needs
to understand every aspect of its implementation. Such methods
are usually pointless.</p>
<p dir="auto">Another problem with decomposing too far is that it tends to
result in <em>entanglement</em>. Two methods
are entangled (or "conjoined" in APOSD terminology) if, in order to
understand how one of them works internally, you also need to read the
code of the other. If you've ever found yourself flipping back and forth
between the implementations of two methods as you read code, that's a
red flag that the methods might be entangled. Entangled methods
are hard to read because the information you need to have in your head
at once isn't all in the same place. Entangled methods can usually
be improved by combining them so that all the code is in one place.</p>
<p dir="auto">The advice in <em>Clean Code</em> on method length is so extreme that it encourages
programmers to create teeny-tiny methods that suffer from both shallow
interfaces and entanglement.  Setting arbitrary numerical limits such
as 2-4 lines in a method and a single line in the body of an
<code>if</code> or <code>while</code> statement exacerbates this problem.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">While I do strongly recommend very short functions, I don't think it's fair to say that the book sets arbitrary numerical limits. The 2-4 line functions that you referred to on page 34 were part of the <em>Sparkle</em> applet that Kent Beck and I wrote together in 1999 as an exercise for learning TDD. I thought it was remarkable that most of the functions in that applet were 2-4 lines long because it was a Swing program; and Swing programs tend to have very long methods.</p>
<p dir="auto">As for setting limits, on page 13 I make clear that although the recommendations in the book have worked well for me and the other authors, they might not work for everyone.  I claimed no final authority, nor even any absolute "rightness". They are offered for consideration.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think these problems will be easiest to understand if we look at
specific code examples. But before we do that, let me ask you, Bob:
do you believe that it's possible for code to be over-decomposed, or
is smaller always better? And, if you believe that over-decomposition
is possible, how do you recognize when it has occurred?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It is certainly possible to over-decompose code.  Here's an example:</p>
<div data-snippet-clipboard-copy-content="void doSomething() {doTheThing()} // over-decomposed."><pre><code>void doSomething() {doTheThing()} // over-decomposed.
</code></pre></div>
<p dir="auto">The strategy that I use for deciding how far to take decomposition is the old rule that a method should do "<em>One Thing</em>".  If I can <em>meaningfully</em> extract one method from another, then the original method did more than one thing.  "Meaningfully" means that the extracted functionality can be given a descriptive name; and that it does less than the original method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Unfortunately the One Thing approach will lead to over-decompositon:</p>
<ol dir="auto">
<li>
<p dir="auto">The term "one thing" is vague and easy to abuse. For example, if a method has two lines of code, isn't it doing two things?</p>
</li>
<li>
<p dir="auto">You haven't provided any useful guardrails to prevent over-decomposition. The example you gave is too extreme to be useful, and the "can it be named" qualification doesn't help: anything can be named.</p>
</li>
<li>
<p dir="auto">The One Thing approach is simply wrong in many cases. If two things are closely related, it might well make sense to implement them in a single method. For example, any thread-safe method will first have to acquire a lock, then carry out its function. These are two "things", but they belong in the same method.</p>
</li>
</ol>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let me tackle the last thing first.  You suggested that locking the thread, and preforming a critical section should be together in the same method.  However, I would be tempted to separate the locking from the critical section.</p>
<div data-snippet-clipboard-copy-content="void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}"><pre><code>void concurrentOperation() {
	lock()
	criticalSection();
	unlock()
}
</code></pre></div>
<p dir="auto">This decouples the critical section from the lock and allows it to be called at times when locking isn't necessary (e.g. in single thread mode) or when the a lock has already been set by someone else.</p>
<p dir="auto">Now, on to the "ease of abuse" argument.  I don't consider that to be a significant concern. <code>If</code> statements are easy to abuse.  <code>Switch</code> statements are easy to abuse.  Assignment statements are easy to abuse.  The fact that something is easy to abuse does not mean that it should be avoided or suppressed.  It simply means people should take appropriate care. There will always be this thing called: <em>judgment</em>.</p>
<p dir="auto">So when faced with this snippet of code in a larger method:</p>
<div data-snippet-clipboard-copy-content="...
amountOwed=0;
totalPoints=0;
..."><pre><code>...
amountOwed=0;
totalPoints=0;
...
</code></pre></div>
<p dir="auto">It would be poor judgement to extract them as follows, because the extraction is not meaningful.  The implementation is not more deeply detailed than the interface.</p>
<div data-snippet-clipboard-copy-content="void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}"><pre><code>void clearAmountOwed() {
  amountOwed=0;
}

void clearTotalPoints() {
  totalPoints=0;
}
</code></pre></div>
<p dir="auto">However it may be good judgement to extract them as follows because the interface is abstract, and the implemention has deeper detail.</p>
<div data-snippet-clipboard-copy-content="void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}"><pre><code>void clearTotals() {
	amountOwed=0;
	totalPoints=0;
}
</code></pre></div>
<p dir="auto">The latter has a nice descriptive name that is abstract enough to be meaningful without being redundant.  And the two lines together are strongly related so as to qualify for doing <em>one thing</em>: initialization.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Of course anything can be abused. But the best approaches to design
encourage people to do things the right way and discourage abuse.
Unfortunately, the One Thing Rule encourages abuse for the reasons I
gave above.</p>
<p dir="auto">And of course software designers will need to use judgment: it isn't
possible to provide precise recipes for software design.
But good judgment requires principles and guidance. The
<em>Clean Code</em> arguments about decomposition, including the One Thing
Rule, are one-sided. They give strong, concrete, quantitative
advice about when to chop things up, with virtually no guidance for
how to tell you've gone too far. All I could find is a 2-sentence
example on page 36 about Listing 3-3 (which is pretty trivial),
buried in the middle of exhortations to "chop, chop, chop".</p>
<p dir="auto">One of the reasons I use the deep/shallow characterization is that it
captures both sides of the tradeoff; it will tell you when a decomposition
is good and also when decomposition makes things worse.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You make a good point that I don't talk much, in the book, about how to make the judgement call.  Back in 2008 my concern was breaking the habit of the very large functions that were common in those early days of the web.  I have been more balanced in the 2d ed.</p>
<p dir="auto">Still, if I must err, I'd rather err on the side of decomposition.  There is value in considering, and visualizing decompositions.  They can always be inlined if we judge them to have gone too far.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Coming back to your <code>clearTotals</code> example:</p>
<ul dir="auto">
<li>The <code>clearTotals</code> method seems to contradict the One Thing Rule: the
variables <code>amountOwed</code> and <code>totalPoints</code> don't seem particularly related, so
initializing them both is doing two things, no? You say that both
statements are performing initialization, which makes it just one thing
(initialization). Does that mean it would also be okay to have a single
method that initializes two completely independent objects with nothing in
common? I suspect not. It feels like you are struggling to create a clean
framework for applying the One Thing Rule; that makes me think it isn't
a good rule.</li>
<li>Without seeing more context I'm skeptical that the <code>clearTotals</code>
method makes sense.</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I hope you agree that between these two examples, the former is a bit better.</p>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  clearTotals();
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<hr>
<div data-snippet-clipboard-copy-content="public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}"><pre><code>public String makeStatement() {
  amountOwed=0;
  totalPoints=0;
  return makeHeader() + makeRentalDetails() + makeFooter();
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Well, actually, no. The second example is completely clear and obvious:
I don't see anything to be gained by splitting it up.</p>
<p dir="auto"><strong>SPOCK (a.k.a UB):</strong></p>
<p dir="auto">Fascinating.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I think it will be easier to clarify our differences if we consider
a nontrivial code example. Let's look at the <code>PrimeGenerator</code> class from
<em>Clean Code</em>, which is Listing 10-8 on pages 145-146. This Java class
generates the first N prime numbers:</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList<Integer> multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList<Integer>();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex < primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple < candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator {
  private static int[] primes;
  private static ArrayList&lt;Integer&gt; multiplesOfPrimeFactors;

  protected static int[] generate(int n) {
    primes = new int[n];
    multiplesOfPrimeFactors = new ArrayList&lt;Integer&gt;();
    set2AsFirstPrime();
    checkOddNumbersForSubsequentPrimes();
    return primes;
  }

  private static void set2AsFirstPrime() {
    primes[0] = 2;
    multiplesOfPrimeFactors.add(2);
  }

  private static void checkOddNumbersForSubsequentPrimes() {
    int primeIndex = 1;
    for (int candidate = 3;
         primeIndex &lt; primes.length;
         candidate += 2) {
      if (isPrime(candidate))
        primes[primeIndex++] = candidate;
    }
  }

  private static boolean isPrime(int candidate) {
    if (isLeastRelevantMultipleOfLargerPrimeFactor(candidate)) {
      multiplesOfPrimeFactors.add(candidate);
      return false;
    }
    return isNotMultipleOfAnyPreviousPrimeFactor(candidate);
  }

  private static boolean
  isLeastRelevantMultipleOfLargerPrimeFactor(int candidate) {
    int nextLargerPrimeFactor = primes[multiplesOfPrimeFactors.size()];
    int leastRelevantMultiple = nextLargerPrimeFactor * nextLargerPrimeFactor;
    return candidate == leastRelevantMultiple;
  }

  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }

  private static boolean
  isMultipleOfNthPrimeFactor(int candidate, int n) {
    return candidate ==
      smallestOddNthMultipleNotLessThanCandidate(candidate, n);
  }

  private static int
  smallestOddNthMultipleNotLessThanCandidate(int candidate, int n) {
    int multiple = multiplesOfPrimeFactors.get(n);
    while (multiple &lt; candidate)
      multiple += 2 * primes[n];
    multiplesOfPrimeFactors.set(n, multiple);
    return multiple;
  }
}
</code></pre></div>
<p dir="auto">Before we dive into this code, I'd encourage everyone reading
this article to take time to read over the code and draw your own conclusions
about it. Did you find the code easy to understand? If so, why? If not, what
makes it complex?</p>
<p dir="auto">Also, Bob, can you confirm that you stand by this code (i.e. the code
properly exemplifies the design philosophy of <em>Clean Code</em> and this
is the way you believe the code should appear if it were used in
production)?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Ah, yes.  The <code>PrimeGenerator</code>.  This code comes from the 1982 paper on <a href="https://www.cs.tufts.edu/~nr/cs257/archive/literate-programming/01-knuth-lp.pdf" rel="nofollow"><em>Literate Programming</em></a> written by Donald Knuth.  The program was originally written in Pascal, and was automatically generated by Knuth's WEB system into a single very large method which I translated into Java.</p>
<p dir="auto">Of course this code was never meant for production.  Both Knuth and I used it as a pedagogical example.  In <em>Clean Code</em> it appears in a chapter named <em>Classes</em>.  The lesson of the chapter is that a very large method will often contain many different sections of code that are better decomposed into independent classes.</p>
<p dir="auto">In the chapter I extracted three classes from that function: <code>PrimePrinter</code>, <code>RowColumnPagePrinter</code> and <code>PrimeGenerator</code>.</p>
<p dir="auto">One of those extracted classes was the <code>PrimeGenerator</code>. It had the following code (which I did not publish in the book.)  The variable names and the overall structure are Knuth's.</p>
<div data-snippet-clipboard-copy-content="public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList<Integer> mult = new ArrayList<Integer>();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k < p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi < ord; mi++) {
          int m = mult.get(mi);
          while (m < j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}"><pre><code>public class PrimeGenerator {
  protected static int[] generate(int n) {
    int[] p = new int[n];
    ArrayList&lt;Integer&gt; mult = new ArrayList&lt;Integer&gt;();
    p[0] = 2;
    mult.add(2);
    int k = 1;
    for (int j = 3; k &lt; p.length; j += 2) {
      boolean jprime = false;
      int ord = mult.size();
      int square = p[ord] * p[ord];
      if (j == square) {
        mult.add(j);
      } else {
        jprime=true;
        for (int mi = 1; mi &lt; ord; mi++) {
          int m = mult.get(mi);
          while (m &lt; j)
            m += 2 * p[mi];
          mult.set(mi, m);
          if (j == m) {
            jprime = false;
            break;
          }
        }
      }
      if (jprime)
        p[k++] = j;
    }
    return p;
  }
}
</code></pre></div>
<p dir="auto">Even though I was done with the lesson of the chapter, I didn't want to leave that method looking so outdated.  So I cleaned it up a bit as an afterthought.  My goal was not to describe how to generate prime numbers.  I wanted my readers to see how large methods, that violate the Single Responsibility Principle, can be broken down into a few smaller well-named classes containing a few smaller well-named methods.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Thanks for the background. Even though the details of that code weren't
the main point of the chapter, presumably the code represents what you think
is the "right" and "cleanest" way to do things, given the algorithm at hand.
And that's where I disagree.</p>
<p dir="auto">There are many design problems with <code>PrimeGenerator</code>, but for now I'll
focus on method length. The code is chopped up so much (8 teeny-tiny methods)
that it's difficult to read. For starters, consider the
<code>isNotMultipleOfAnyPreviousPrimeFactor</code> method. This method invokes
<code>isMultipleOfNthPrimeFactor</code>, which invokes
<code>smallestOddNthMultipleNotLessThanCandidate</code>. These methods are shallow
and entangled:
in order to understand
<code>isNot...</code> you have to read the other two
methods and load all of that code into your mind at once. For example,
<code>isNot...</code> has side effects (it modifies <code>multiplesOfPrimeFactors</code>) but
you can't see that unless you read all three methods.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you have a point.  Eighteen years ago, when I was in the throes of this refactoring, the names and structure made perfect sense to me.  They make sense to me now, too -- but that's because I once again understand the algorithm.  When I returned to the algorithm for the first time a few days ago, I  struggled with the names and structure.  Once I understood the algorithm the names and structure made perfect sense.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Those names are problematic even for someone who understands the algorithm;
we'll talk about them a bit later, when discussing comments. And, if code
no longer makes sense to the writer when the writer returns to the code later,
that means the code is problematic. The fact that code can eventually
be understood (with great pain and suffering) does not excuse its entanglement.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Would that we had such a crystal ball that we could help our future selves avoid such "<em>great pain and suffering</em>".  ;-)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There is no need for a crystal ball. The problems with <code>PrimeGenerator</code> are
pretty obvious, such as the entanglement and interface complexity; maybe you
were surprised that it is hard to understand, but I am not. Said another
way, if you are unable to predict whether your code will be easy to
understand, there are problems with your design methodology.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  I will say, however, that I had equal "<em>pain and suffering</em>" interpreting your rewrite (below).  So, apparently, neither of our methodologies were sufficient to rescue our readers from such struggles.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Going back to my introductory remarks about complexity, splitting up
<code>isNot...</code> into three methods doesn't reduce the amount of information
you have to keep in your mind. It just spreads it out, so it isn't as
obvious that you need to read all three methods together. And, it's harder
to see the overall structure of the code because it's split up: readers have
to flip back and forth between the methods, effectively reconstructing a
monolithic version in their minds. Because the pieces are all related,
this code will be easiest to understand if it's all together in one place.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I disagree.  Here is <code>isNotMultipleOfAnyPreviousPrimeFactor</code>.</p>
<div data-snippet-clipboard-copy-content="  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n < multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }"><pre><code>  private static boolean
  isNotMultipleOfAnyPreviousPrimeFactor(int candidate) {
    for (int n = 1; n &lt; multiplesOfPrimeFactors.size(); n++) {
      if (isMultipleOfNthPrimeFactor(candidate, n))
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto">If you trust the <code>isMultipleOfNthPrimeFactor</code> method, then this method stands alone quite nicely.  I mean we loop through all n previous primes and see if the candidate is a multiple.  That's pretty straight forward.</p>
<p dir="auto">Now it would be fair to ask the question how we determine whether the candidate is a multiple, and in that case you'd want to inspect the <code>isMultiple...</code> method.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This code does appear to be simple and obvious.
Unfortunately, this appearance is deceiving.
If a reader trusts the name <code>isMultipleOfNthPrimeFactor</code> (which suggests
a predicate with no side effects) and doesn't bother to read its code, they
will not realize that it has side effects, and that the side effects
create a constraint on the <code>candidate</code> argument to <code>isNot...</code>
(it must be monotonically non-decreasing from invocation
to invocation). To understand these behaviors, you have to
read both <code>isMultiple...</code> and <code>smallestOdd...</code>. The current decomposition
hides this important information from the reader.</p>
<p dir="auto">If there is one thing more likely to result in bugs than not understanding code,
it's thinking you understand it when you don't.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That's a valid concern.  However, it is tempered by the fact that the functions are presented in the order they are called.  Thus we can expect that the reader has already seen the main loop and understands that <code>candidate</code> increases by two each iteration.</p>
<p dir="auto">The side effect buried down in <code>smallestOddNth...</code> is a bit more problematic. Now that you've pointed it out I don't like it much.  Still, that side effect should not confound the basic understanding of <code>isNot...</code>.</p>
<p dir="auto">In general, if you trust the names of the methods being called then understanding the caller does not require understanding the callee.  For example:</p>
<div data-snippet-clipboard-copy-content="for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();"><pre><code>for (Employee e : employees)
  if (e.shouldPayToday())
	  e.pay();
</code></pre></div>
<p dir="auto">This would not be made more understandable if we replaced those two method calls with the their implementations.  Such a replacement would simply obscure the intent.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This example works because the called methods are relatively independent of
the parent. Unfortunately that is not the case for <code>isNot...</code>.</p>
<p dir="auto">In fact, <code>isNot...</code> is not only entangled with the methods it calls, it's also
entangled with its callers. <code>isNot...</code> only works if it is invoked in
a loop where <code>candidate</code> increases monotonically. To convince yourself
that it works, you have to find the code that invokes <code>isNot...</code> and
make sure that <code>candidate</code> never decreases from one call to the next.
Separating <code>isNot...</code> from the loop that invokes it makes it harder
for readers to convince themselves that it works.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Which, as I said before, is why the methods are ordered the way they are.  I expect that by the time you get to <code>isNot...</code> you've already read <code>checkOddNumbersForSubsequentPrimes</code> and know that <code>candidate</code> increases by twos.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's discuss this briefly, because it's another area where I
disagree with <em>Clean Code</em>. If methods are entangled, there is no
clever ordering of the method definitions that will fix the problem.</p>
<p dir="auto">In this particular situation two other methods intervene between the
loop in <code>checkOdd...</code> and <code>isNot...</code>, so readers will have forgotten
the loop context before they get to <code>isNot...</code>. Furthermore, the actual
code that creates a dependency on the loop isn't in <code>isNot...</code>: it's in
<code>smallestOdd...</code>, which is even farther away from <code>checkOdd...</code>.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I sincerely doubt anyone is going to forget that <code>candidate</code> is being increased by twos.  It's a pretty obvious way to avoid waste.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In my opening remarks I talked about how it's important to reduce the
amount of information people have to keep in their minds at once.
In this situation, readers have to remember that loop while they read
four intervening methods that are mostly unrelated to the loop. You apparently think
this will be easy and natural (I disagree). But it's even worse than
that. There is no indication which parts of <code>checkOdd...</code> will be important
later on, so the only safe approach is to remember <em>everything</em>, from <em>every</em>
method, until you have encountered every other method that could possibly
descend from it. And, to make the connection between the pieces, readers
must also reconstruct the call graph to notice that, even through
4 layers of method call, the code in <code>smallestOdd...</code> places constraints
on the loop in <code>checkOdd...</code>. This is an unreasonable cognitive burden to
place on readers.</p>
<p dir="auto">If two pieces of code are tightly related, the solution is to bring
them together. Separating the pieces, even in physically adjacent methods,
makes the code harder to understand.</p>
<p dir="auto">To me, all of the methods in <code>PrimeGenerator</code> are entangled: in order to
understand the class I had to load all of them into my mind
at once. I was constantly flipping back and forth between the methods
as I read the code. This is a red flag indicating
that the code has been over-decomposed.</p>
<p dir="auto">Bob, can you help me understand why you divided the code into such
tiny methods?
Is there some benefit to having so many methods that I have missed?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think you and I are just going to disagree on this.  In general I believe in the principle of small well-named methods and the separation of concerns. Generally speaking if you can break a large method into several well-named smaller methods with different concerns, and by doing so expose their interfaces, and the high level functional decomposition, then that's a good thing.</p>
<ul dir="auto">
<li>Looping over the odd numbers is one concern.</li>
<li>Determining primality is another.</li>
<li>Marking off the multiples of primes is yet another.</li>
</ul>
<p dir="auto">It seems to me that separating and naming those concerns helps to expose the way the algorithm works -- even at the expense of some entaglement.</p>
<p dir="auto">In your solution, which we are soon to see below, you break the algorithm up in a similar way.  However, instead of separating the concerns into functions, you separate them into sections with comments above them.</p>
<p dir="auto">You mentioned that in my solution readers will have to keep the loop context in mind while reading the other functions.  I suggest that in your solution, readers will have to keep the loop context in mind while reading your explanatory comments.  They may have to "flip back and forth" between the sections in order to establish their understanding.</p>
<p dir="auto">Now perhaps you are concerned that in my solution the "flipping" is a longer distance (in lines) than in yours.  I'm not sure that's a significant point since they all fit on the same screen (at least they do on my screen) and the landmarks are pretty obvious.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Method Length Summary</h3><a id="user-content-method-length-summary" aria-label="Permalink: Method Length Summary" href="#method-length-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like it's time to wrap up this section. Is this a reasonable
summary of where we agree and disagree?</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that modular design is a good thing.</p>
</li>
<li>
<p dir="auto">We agree that it is possible to over-decompose, and that <em>Clean Code 1st ed.</em>
doesn't provide much guidance on how to recognize over-decomposition.</p>
</li>
<li>
<p dir="auto">We disagree on how far to decompose: you recommend decomposing
code into much smaller units than I do. You believe that
the additional decomposition you recommend makes code easier to
understand; I believe that it goes too far and actually makes code
more difficult to understand.</p>
</li>
<li>
<p dir="auto">You believe that the One Thing Rule, applied with judgment, will
lead to appropriate decompositions. I believe it lacks guardrails
and will lead to over-decomposition.</p>
</li>
<li>
<p dir="auto">We agree that the internal decomposition of <code>PrimeGenerator</code> into
methods is problematic. You point out that your main goal in writing
<code>PrimeGenerator</code> was to show how to decompose into classes, not
so much how to decompose a class internally into methods.</p>
</li>
<li>
<p dir="auto">Entanglement between methods in a class doesn't bother you
as much as it bothers me. You believe that the benefits of decomposing
methods can compensate for problems caused by entanglement.
I believe they can't: when decomposed methods are entangled,
they are harder to read than if they were not decomposed, and this
defeats the whole purpose of decomposition.</p>
</li>
<li>
<p dir="auto">You believe that ordering the methods in a class can help to
compensate for entanglement between methods; I don't.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think this is a fair assessment of our agreements and disagreements.  We both value decomposition,
and we both avoid entanglement; but we disagree on the relative weighting of those two values.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Comments</h2><a id="user-content-comments" aria-label="Permalink: Comments" href="#comments"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to the second area of disagreement: comments. In my opinion,
the <em>Clean Code</em> approach to commenting results in code with
inadequate documentation, which increases the cost of software development.
I'm sure you disagree, so let's discuss.</p>
<p dir="auto">Here is what <em>Clean Code</em> says about comments (page 54):</p>
<blockquote>
<p dir="auto">The proper use of comments is to compensate for our failure to express
ourselves in code. Note that I use the word failure. I meant it.
Comments are always failures. We must have them because we cannot always
figure out how to express ourselves without them, but their use is not
a cause for celebration... Every time you write a comment, you should
grimace and feel the failure of your ability of expression.</p>
</blockquote>
<p dir="auto">I have to be honest: I was horrified when I first read this text, and it
still makes me cringe. This stigmatizes writing comments. Junior developers
will think "if I write comments, people may think I've failed, so the
safest thing is to write no comments."</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">That chapter begins with these words:</p>
<blockquote>
<p dir="auto"><em>Nothing can be quite so helpful as a well placed comment.</em></p>
</blockquote>
<p dir="auto">It goes on to say that comments are a <em>necessary</em> evil.</p>
<p dir="auto">The only way a reader could infer that they should write no comments is if they hadn't actually read the chapter.  The chapter walks through a series of comments, some bad, some good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto"><em>Clean Code</em> focuses a lot more on the "evil" aspects of comments than the
"necessary" aspects. The sentence you quoted above is followed by two
sentences criticizing comments. Chapter 4 spends 4 pages talking about good
comments, followed by 15 pages talking about bad comments. There are snubs
like "the only truly good comment is the comment you found a way
not to write". And "Comments are always failures" is so catchy
that it's the one thing readers are most likely to remember from the
chapter.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">The difference in page count is because there are just a few ways to write good comments, and so many more ways to write bad ones.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree; this illustrates your bias against comments. If you look at
Chapter 13 of APOSD, it finds a lot more
constructive ways to use comments than <em>Clean Code</em>. And if you compare
the tone of Chapter 13 of APOSD with Chapter 4 of <em>Clean Code</em>, the hostility
of <em>Clean Code</em> towards comments becomes pretty clear.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll leave you to balance that last comment with the initial statement, and the final example, in the <em>Comments</em> chapter. They do not communicate "hostility".</p>
<p dir="auto">I'm not hostile to comments in general.  I <em>am</em> very hostile to gratuitous comments.</p>
<p dir="auto">You and I likely both survived through a time when comments were absolutely necessary.  In the '70s and '80s I was an assembly language programmer.  I also wrote a bit of FORTRAN. Programs in those languages that had no comments were impenetrable.</p>
<p dir="auto">As a result it became conventional wisdom to write comments by default.  And, indeed, computer science students were taught to write comments uncritically.  Comments became <em>pure good</em>.</p>
<p dir="auto">In <em>Clean Code</em> I decided to fight that mindset.  Comments can be <em>really bad</em> as well as good.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that comments are less necessary today than they were
40 years ago.</p>
<p dir="auto">Comments are crucially important and add enormous value to software.
The problem is that there is a lot of important information that simply
cannot be expressed in code. By adding comments to fill in this missing
information, developers can make code dramatically easier to read.
This is not a "failure of their ability to express themselves", as you
put it.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's very true that there is important information that is not, or cannot be, expresssed in code.  That's a failure.  A failure of our languages, or of our ability to use them to express ourselves.  In every case a comment is a failure of our ability to use our languages to express our intent.</p>
<p dir="auto">And we fail at that very frequently, and so comments are a necessary evil -- or, if you prefer, <em>an unfortunate necessity</em>.  If we had the perfect programming language (TM) we would never write another comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't agree that a perfect programming language would
eliminate the need for comments. Comments and code serve very different
purposes, so it's not obvious to me that we should use the same
language for both. In my experience, English works quite well
as a language for comments.
Why do you feel that information about a program should
be expressed entirely in code, rather than using a combination of code
and English?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I bemoan the fact that we must sometimes use a human language instead of a programming language.  Human languages are imprecise and full of ambiguities. Using a human language to describe something as precise as a program is very hard, and fraught with many opportunities for error and inadvertent misinformation.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that English isn't always as precise as code, but it can still be
used in precise ways and comments typically don't need the same
degree of precision as code.
Comments often contain qualitative information such
as <em>why</em> something is being done, or the overall idea of something.
English works better for these than code because it is a more
expressive language.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I have no argument with that statement.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Are you concerned that comments will be incorrect or
misleading and that this will slow down software development?
I often hear people complain about stale comments (usually as an excuse
for writing no comments at all) but
I have not found them be a significant problem
over my career. Incorrect comments do happen, but I don't encounter them
very often and when I do, they rarely cost me much time. In contrast, I waste
<em>enormous</em> amounts of time because of inadequate documentation; it's not
unusual for me to spend 50-80% of my development time wading through
code to figure out things that would be obvious if the code was properly
commented.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">You and I have had some very different experiences.</p>
<p dir="auto">I have certainly been helped by well placed comments.  I have also, just as certainly, (and within this very document) been distracted and confused by a comment that was incorrect, misplaced, gratuitous, or otherwise just plain bad.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I invite everyone reading this article to ask yourself the following questions:</p>
<ul dir="auto">
<li>How much does your software development speed suffer because of
incorrect comments?</li>
<li>How much does your software development speed suffer because of
missing comments?</li>
</ul>
<p dir="auto">For me the cost of missing comments is easily 10-100x the cost of incorrect
comments. That is why I cringe when I see things in <em>Clean Code</em> that
discourage people from writing comments.</p>
<p dir="auto">Let's consider the <code>PrimeGenerator</code> class. There is not a single comment
in that code; does this seem appropriate to you?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it was appropriate for the purpose for which I wrote it. It was an adjunct to the lesson that very large methods can be broken down into smaller classes containing smaller methods. Adding lots of explanatory comments would have detracted from that point.</p>
<p dir="auto">In general, however, the commenting style I used in Listing 4-8 is more appropriate.  That listing, at the very end of the <em>Comments</em> chapter, describes yet another <code>PrimeGenertor</code> with a slightly different algorithm, and a better set of comments.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I disagree that adding comments would have distracted from your point,
and I think Listing 4-8 is also woefully undercommented.
But let's not argue about either of those issues. Instead, let's discuss
what comments the PrimeGenerator code <em>should</em> have if it were used in production.
I will make some suggestions and you can agree or disagree.</p>
<p dir="auto">For starters, let's discuss your use of megasyllabic names like
<code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.  My understanding is that
you advocate using names like this instead of using shorter names
augmented with descriptive comments: you're effectively moving the
comments into code. To me, this approach is problematic:</p>
<ul dir="auto">
<li>Long names are awkward. Developers effectively have to retype
the documentation for a method every time they invoke it, and the long
names waste horizontal space and trigger line wraps in the code. The names are
also awkward to read: my mind wants to parse every syllable every time
I read it, which slows me down. Notice that both you and I resorted to
abbreviating names in this discussion: that's an indication that
the long names are awkward and unhepful.</li>
<li>The names are hard to parse and don't convey information as effectively
as a comment.
When students read <code>PrimeGenerator</code> one of the first things they
complain about is the long names (students can't make sense of them).
For example, the name above is
vague and cryptic: what does "least relevant" mean, and what is a
"larger prime factor"? Even with a complete understanding of the code in
the method, it's hard for me to make sense of the name.  If this name
is going to eliminate the need for a comment, it needs to be even longer.</li>
</ul>
<p dir="auto">In my opinion, the traditional approach of using shorter names with
descriptive comments is more convenient and conveys the required information
more effectively. What advantage is there in the approach you advocate?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">"<em>Megasyllabic</em>": Great word!</p>
<p dir="auto">I like my method names to be sentence fragments that fit nicely with keywords and assignment statements.  It makes the code a bit more natural to read.</p>
<div data-snippet-clipboard-copy-content="if (isTooHot)
  cooler.turnOn();"><pre><code>if (isTooHot)
  cooler.turnOn();
</code></pre></div>
<p dir="auto">I also follow a simple rule about the length of names.  The larger the scope of a method, the shorter its name should be and vice-versa -- the shorter the scope the longer the  name.  The private methods I extracted in this case live in very small scopes, and so have longish names.  Methods like this are typically called from only one place, so there is no burden on the programmer to remember a long name for another call.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Names like <code>isTooHot</code> are totally fine by me.
My concern is about names like <code>isLeastRelevantMultipleOfLargerPrimeFactor</code>.</p>
<p dir="auto">It's interesting that as methods get smaller and narrower, you recommend
longer names.
What this says to me is that the interfaces for those functions are
more complex, so it takes more words to describe them. This provides
supporting evidence for
my assertion a while back that the more you split up a method,
the shallower the resulting methods will be.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's not the functions that get smaller, it's the scope that gets smaller.  A private function has a smaller scope than the public function that calls it.  A function called by that private function has an even smaller scope.  As we descend in scope, we also descend in situational detail.  Describing such detail often requires a long name, or a long comment.  I prefer to use a name.</p>
<p dir="auto">As for long names being hard to parse, that's a matter of practice.  Code is full of things that take practice to get used to.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't accept this. Code may be full of things that take practice to get used
to, but that doesn't excuse it.
Approaches that require more practice are worse than
those that require less.
If it's going to take a lot of work to get comfortable with the long names
then there had better be some compensating benefit; so far I'm not seeing any.
And I don't see any reason to believe that practice will make those names
easier to digest.</p>
<p dir="auto">In addition, your comment above violates one of my fundamental rules, which
is "complexity is in the eye of the reader". If you write code that someone
else thinks is complicated, then you must accept that the code is probably
complicated (unless you think the reader is completely incompetent). It
is not OK to make excuses or suggest that it is really the reader's problem
("you just don't have enough practice"). I'm going to have to live by this
same rule a bit later in our discussion.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  As for the meaning of "leastRelevant", that's a much larger problem that you and I will encounter shortly.  It has to do with the intimacy that the author has with the solution, and the reader's lack of that intimacy.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You still haven't answererd my question: why is it better to use super-long names
rather than shorter names augmented with descriptive comments?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">It's a matter of preference for me.  I prefer long names to comments.  I don't trust comments to be maintained, nor do I trust that they will be read.  Have you ever noticed that many IDEs paint comments in light grey so that they can be easily ignored?  It's harder to ignore a name than a comment.</p>
<p dir="auto">(BTW, I have my IDE paint comments in bright fire-engine red)</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't see why a monster name is more likely to be "maintained" than
a comment, and I don't agree that IDEs encourage people to ignore
comments (this is your bias coming out again). My current IDE (VSCode)
doesn't use a lighter color for comments.
My previous one (NetBeans) did, but the color scheme didn't hide the comments; it
distinguished them from the code in a way that made both code and comments
easier to read.</p>
<p dir="auto">Now that we've discussed the specific issue of comments vs. long method
names, let's talk about comments in general. I think there are two major reasons
why comments are needed. The first reason for comments is abstraction.
Simply put, without comments there is no way to have abstraction or modularity.</p>
<p dir="auto">Abstraction is one of the most important components of good software design.
I define an abstraction as "a simplified way of thinking about something
that omits unimportant details." The most obvious example of an abstraction
is a method. It should be possible to use a method without reading its code.
The way we achieve this is by writing a header comment that describes
the method's <em>interface</em> (all the information someone needs in order
to invoke the method). If the method is well designed, the interface will be
much simpler than the code of the method (it omits implementation details),
so the comments reduce the amount of information people must have in
their heads.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Long ago, in a 1995 book, I defined abstraction as:</p>
<blockquote>
<p dir="auto"><em>The amplification of the essential and the elimination of the irrelevant.</em></p>
</blockquote>
<p dir="auto">I certainly agree that abstraction is of importance to good software design.  I also agree that well placed comments can enhance the ability of readers to understand the abstractions we are attempting to employ.  I disagree that comments are the <em>only</em>, or even the <em>best</em>, way to understand those abstractions.  But sometimes they are the only option.</p>
<p dir="auto">But consider:</p>
<div data-snippet-clipboard-copy-content="addSongToLibrary(String title, String[] authors, int durationInSeconds);"><pre><code>addSongToLibrary(String title, String[] authors, int durationInSeconds);
</code></pre></div>
<p dir="auto">This seems like a very nice abstraction to me, and I cannot imagine how a comment might improve it.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Our definitions of abstraction are very similar; that's good to see.
However, the <code>addSongToLibrary</code> declaration is not (yet) a good abstraction
because it omits information
that is essential. In order to use <code>addSongToLibrary</code>, developers
need answers to the following questions:</p>
<ul dir="auto">
<li>Is there any expected format for an author string, such as "LastName, FirstName"?</li>
<li>Are the authors expected to be in alphabetical order? If not, is the order
significant in some other way?</li>
<li>What happens if there is already a song in the library with the given title
but different authors? Is it replaced with the new one, or will the library
keep multiple songs with the same title?</li>
<li>How is the library stored (e.g. is it entirely in memory? saved on disk?)?
If this information is documented somewhere else, such as the
overall class documentation, then it need not be repeated here.</li>
</ul>
<p dir="auto">Thus <code>addSongToLibrary</code> needs quite a few comments.
Sometimes the signature of a method (names and types of the method, its
arguments, and its return value) contains all the information
needed to use it, but this is pretty rare. Just skim through the documentation
for your favorite library package: in how many cases could you understand how
to use a method with only its signature?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Yes, there are times when the signature of a method is an incomplete abstraction and a comment
is required.  This is especially true when the interface is part of a public API, or an API intended
for use by a separate team of developers.  Within a single development team, however, long descriptive
comments on interfaces are often more of an impediment than a help.  The team has intimate knowledge of the
internals of the system, and will generally be able to understand an interface simply from its
signature.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In one of our in-person discussions you argued that interface comments
are unnecessary because when a group of developers is working on a body
of code they can collectively keep the entire code "loaded" in their
minds, so comments are unnecessary: if you have a question, just ask the
person who is familiar with that code. This creates a huge cognitive load
to keep all that code mentally loaded, and it's hard for me to imagine
that it would actually work. Maybe your memory is better than mine, but I
find that I quickly forget code that I wrote just a few weeks ago. In
a project of any size, I think your approach would result in developers
spending large amounts of time reading code to re-derive the interfaces,
and probably making mistakes along the way. Spending a few minutes to
document the interfaces would save time, reduce cognitive load, and
reduce bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think that certain interfaces need comments, even if they are private to the team.  But I think it is more often the case that the team is familiar enough with the system that well named methods and arguments are sufficient.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's consider a specific example from <code>PrimeGenerator</code>: the <code>isMultipleOfNthPrimeFactor</code>
method. When someone reading the code encounters the call to <code>isMultiple...</code>
in <code>isNot...</code> they need to understand enough about how <code>isMultiple...</code> works
in order to see how it fits into the code of <code>isNot...</code>.
The method name does not fully document the interface, so if there
is no header comment then readers will have to read the code of <code>isMultiple</code>.
This will force readers to load more information into their
heads, which makes it harder to work in the code.</p>
<p dir="auto">Here is my first attempt at a header comment for <code>isMultiple</code>:</p>
<div data-snippet-clipboard-copy-content="    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      <= multiplesOfPrimeFactors.size().
     */"><pre><code>    /**
     * Returns true if candidate is a multiple of primes[n], false otherwise.
     * May modify multiplesOfPrimeFactors[n].
     * @param candidate
     *      Number being tested for primality; must be at least as
     *      large as any value passed to this method in the past.
     * @param n
     *      Selects a prime number to test against; must be
     *      &lt;= multiplesOfPrimeFactors.size().
     */
</code></pre></div>
<p dir="auto">What do you think of this?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think it's accurate.  I wouldn't delete it if I encountered it.  I don't think it should be a javadoc.</p>
<p dir="auto">The first sentence is redundant with the name <code>isMultipleOfNthPrimeFactor</code> and so could be deleted.  The warning of the side effect is useful.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I agree that the first sentence is largely redundant with the name,
and I debated with myself about whether to keep it. I decided to keep it
because I think it is a bit more precise than the name; it's also easier
to read. You propose to eliminate the redundancy between the comment and
the method name by dropping the comment; I would eliminate the redundancy by
shortening the method name.</p>
<p dir="auto">By the way, you complained earlier about comments being less precise than
code, but in this case the comment is <em>more</em> precise (the method
name can't include text like <code>primes[n]</code>).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Fair enough.  There are times when precision is better expressed in a comment.</p>
<p dir="auto">Continuing with my critique of your comment above: The name <code>candidate</code> is synonymous with "Number being tested for primality".</p>
<p dir="auto">In the end, however, all the words in a comment are just going to have to sit in my brain
until I understand why they are there.  I'm also going to have to worry if
they are accurate.  So I'm going to have to read the code to understand and
validate the comment.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Whoah. That loud sound you just heard was my jaw hitting the floor.
Help me understand this a bit better: approximately what
fraction of comments that you encounter in practice are you willing to
trust without reading the code to verify them?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I look at every comment as potential misinformation.  At best they are a way to crosscheck the author's intent against the code. The amount of credence I give to a comment depends a lot on how easy they make that crosscheck.  When I read a comment that does not cause me to crosscheck, then I consider it to be of no value.  When I see a comment that causes me to crosscheck, and when that crosscheck turns out to be valuable, then that's a really good comment.</p>
<p dir="auto">Another way to say this is that the best comments tell me something surprising and verifiable about the code.  The worst are those that waste my time telling me something obvious, or incorrect.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">It sounds like your answer is 0%: you don't trust any comment unless it has
been verified against the code. This makes no sense to me. As I said above, the vast
majority of comments are correct. It's not hard to write comments; the students
in my software design class are doing this pretty well within a few weeks.
It's also not hard to keep comments up to date as code evolves. Your refusal
to trust comments is another sign of your irrational bias against comments.</p>
<p dir="auto">Refusing to trust comments incurs a very high cost. In order to understand
how to invoke a method, you will have to read all of the code of that method;
if the method invokes other methods, you will
also have to read them, and the methods they invoke, recursively. This is
an enormous amount of work in comparison to reading (and trusting) a
simple interface comment like the one I wrote above.</p>
<p dir="auto">If you choose not to write an interface comment for methods, then you
leave the interface of that method undefined. Even if someone reads the
code of the method, they won't be able to tell which parts of the
implementation are expected to remain the same and which parts may
change (there is no way to specify this "contract" in code). This will
result in misunderstanding and more bugs.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Well, I guess I've just been burned more than you have.  I've gone down too many false comment induced rabbit holes, and wasted too much time on worthless word salads.</p>
<p dir="auto">Of course my trust in comments is not a binary thing.  I read them if they are there; but
I don't implicitly trust then.  The more gratuitous I feel the author was, or the less adept at english the author is, the less I trust the comments.</p>
<p dir="auto">As I said above, our IDEs tend to paint comments in an ignorable color.  I have my IDE paint comments in bright fire engine red because when I write a comment I intend for it to be read.</p>
<p dir="auto">By the same token I use long names as a subsitute for comments because I intend for those long names to be read; and it is very hard for a programmer to ignore names.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned earlier that there are two general reasons why comments are
needed. So far we've been discussing the first reason (abstraction).
The second general reason for comments is for important information
that is not obvious from the code. The algorithm in <code>PrimeGenerator</code>
is very non-obvious, so quite a few comments are needed to help readers
understand what is going on and why. Most of the algorithm's complexity
arises because it is designed to compute primes efficiently:</p>
<ul dir="auto">
<li>
<p dir="auto">The algorithm goes out of its way to avoid divisions, which were quite
expensive when Knuth wrote his original version (they aren't that expensive
nowadays).</p>
</li>
<li>
<p dir="auto">The first multiple for each new prime number is computed by squaring the
prime, rather than multiplying it by 3. This is mysterious: why is it safe
to skip the intervening odd multiples? Furthermore, it might seem that this
optimization only has a small impact on performance, but in fact it makes an
<em>enormous</em> difference (orders of magnitude). Using the square has the
side-effect that when
testing a candidate, only primes up to the square root of the
candidate are tested. If 3x were used as the initial multiple, primes
within a factor of 3 of the candidate would be tested; that's a <em>lot</em>
more tests.
This implication of using the square is so non-obvious that I only realized
it while preparing material for this discussion; it never occurred to me in
the many times I have discussed the code with students.</p>
</li>
</ul>
<p dir="auto">Neither of these issues is obvious from the code; without
comments, readers are left to figure them out on their own. The students
in my class are generally unable to figure out either of them in the
30 minutes I give them, but I think that comments would have
allowed them to understand in a few minutes. Going back to my
introductory remarks, this is an example where information is important,
so it needs to be made available.</p>
<p dir="auto">Do you agree that there should be comments to explain each of these
two issues?</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I agree that the algorithm is subtle.  Setting the first prime multiple as the square of the prime was deeply mysterious at first.  I had to go on an hour long bike ride to understand it.</p>
<p dir="auto">Would a comment help?  Perhaps.  However, my guess is that no one who has been reading our conversation has been helped by it, because you and I are now too intimate with the solution.  You and I can talk about that solution using words that fit into that intimacy; but our readers likely do not yet enjoy that fit.</p>
<p dir="auto">One solution is to paint a picture -- being worth a thousand words.  Here's my attempt.</p>
<div data-snippet-clipboard-copy-content="                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"><pre><code>                                                                X
                                                    1111111111111111111111111
       1111122222333334444455555666667777788888999990000011111222223333344444
   35791357913579135791357913579135791357913579135791357913579135791357913579
   !!! !! !! !  !!  ! !! !  !  !!  ! !!  ! !  !   ! !! !! !
 3 |||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-||-
 5 |||||||||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-||||-
 7 |||||||||||||||||||||||-||||||-||||||-||||||-||||||-||||||-||||||-
11 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||-||||||||||-
13 ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
...
113||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
</code></pre></div>
<p dir="auto">I expect that our readers will have to stare at this for some time, and also look at the code.  But then there will be a <em>click</em> in their brains and they'll say "Ohhh!  Yes!  I see it now!"</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I found this diagram very hard to understand.
It begs for supplemental English text to explain the ideas being
presented. Even the syntax is non-obvious: what does
<code>1111111111111111111111111</code> mean?</p>
<p dir="auto">Maybe we have a fundamental difference of philosophy here. I get the sense
that you are happy to give readers a few clues and leave it to them to put
the clues together. Perhaps you don't mind if people have to stare at something
for a while to figure it out? I don't agree with this approach: it results
in wasted time, misunderstandings, and bugs.
I think software should be totally <em>obvious</em>, where readers don't need to
be clever or "stare at this for some time" to figure things out.
Suffering followed by catharsis is great for Greek tragedies, but not
for reading code. Every question
a reader might have should be naturally answered, either in the code or
in comments. Key ideas and important conclusions should be stated explicitly,
not left for the reader to deduce. Ideally, even if a reader is in a hurry
and doesn't read the code very carefully, their first guesses about how
things work (and why) should be correct. To me, that's clean code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I don't disagree with your sentiment.  Good clean code should be as easy as possible to understand.  I want to give my readers as many clues as possible so that the code is intuitive to read.</p>
<p dir="auto">That's the goal.  As we are about to see, that can be a tough goal to achieve.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">In that case, do you still stand by the "picture" you painted above? It doesn't
seem consistent with what you just said. And if you really wanted to give
your readers as many clues as possible, you'd include a lot more comments.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I stand by the picture as far as it's accuracy is concerned.  And I think it
makes a good crosscheck.  I have no illusions that it is easy to understand.</p>
<p dir="auto">This algorithm is challenging and will require work to comprehend.  I finally
understood it when I drew this picture in my mind while on that bike ride.  When I got home I drew it for real and presented it in hopes that it might help
someone willing to do the work to understand it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Comments Summary</h3><a id="user-content-comments-summary" aria-label="Permalink: Comments Summary" href="#comments-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's wrap up this section of the discussion. Here is my summary of
where we agree and disgree.</p>
<ul dir="auto">
<li>
<p dir="auto">Our overall views of comments are fundamentally different. I see more
value in comments than you do, and I believe that they play a fundamental
and irreplaceable role in system design. You agree that there are places
where comments are necessary, but that comments don't always make it
easier to understand code, so you see far fewer places where comments are
needed.</p>
</li>
<li>
<p dir="auto">I would probably write 5-10x more lines of comments for a given piece of
code than you would.</p>
</li>
<li>
<p dir="auto">I believe that missing comments are a much greater cause of lost
productivity than erroneous or unhelpful comments;
you believe that comments are a net negative, as generally practiced:
bad comments cost more time than good comments save.</p>
</li>
<li>
<p dir="auto">You view it as problematic that comments are written in English
rather than a programming language. I don't see this as particularly
problematic and think that in many cases English works better.</p>
</li>
<li>
<p dir="auto">You recommend that developers should take information that I would
represent as comments and recast it into code if at all possible. One
example of this is super-long method names. I believe that super-long names
are awkward and hard to understand, and that it would be better to use
shorter names supplemented with comments.</p>
</li>
<li>
<p dir="auto">I believe that it is not possible to define interfaces and create
abstractions without a lot of comments. You agree for public APIs, but see little need to comment
interfaces that are internal to the team.</p>
</li>
<li>
<p dir="auto">You are unwilling to trust comments until you have read code to
verify them. I generally trust comments; by doing so, I don't need to read
as much code as you do. You think this exposes me to too much risk.</p>
</li>
<li>
<p dir="auto">We agree that implementation code only needs comments when the code is
nonobvious. Although neither of us argues for a large number of implementation
comments, I'm more likely to see value in them than you do.</p>
</li>
</ul>
<p dir="auto">Overall, we struggled to find areas of agreement on this topic.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair assessment of our individual positions; which I assume are based on our
different individual experiences.  Over the years I have found the vast majority
of comments, as generally practiced in the industry, to be unhelpful. You seem to have found more
help in the comments you have encountered.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">John's Rewrite of PrimeGenerator</h2><a id="user-content-johns-rewrite-of-primegenerator" aria-label="Permalink: John's Rewrite of PrimeGenerator" href="#johns-rewrite-of-primegenerator"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I mentioned that I ask the students in my software design class to rewrite
<code>PrimeGenerator</code> to fix all of its design problems. Here is my rewrite
(note: this was written before we began our discussion; given what I
have learned during the discussion, I would now change several of the
comments, but I have left this in its original form):</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound < n; candidate += 2) {
            if (candidate >= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i <= lastMultiple; i++) {
                while (multiples[i] < candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}"><pre><code>package literatePrimes;

import java.util.ArrayList;

public class PrimeGenerator2 {

    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
        int[] primes = new int[n];

        // Used to test efficiently (without division) whether a candidate
        // is a multiple of a previously-encountered prime number. Each entry
        // here contains an odd multiple of the corresponding entry in
        // primes. Entries increase monotonically.
        int[] multiples = new int[n];

        // Index of the last value in multiples that we need to consider
        // when testing candidates (all elements after this are greater
        // than our current candidate, so they don't need to be considered).
        int lastMultiple = 0;

        // Number of valid entries in primes.
        int primesFound = 1;

        primes[0] = 2;
        multiples[0] = 4;

        // Each iteration through this loop considers one candidate; skip
        // the even numbers, since they can't be prime.
        candidates: for (int candidate = 3; primesFound &lt; n; candidate += 2) {
            if (candidate &gt;= multiples[lastMultiple]) {
                lastMultiple++;
            }

            // Each iteration of this loop tests the candidate against one
            // potential prime factor. Skip the first factor (2) since we
            // only consider odd candidates.
            for (int i = 1; i &lt;= lastMultiple; i++) {
                while (multiples[i] &lt; candidate) {
                    multiples[i] += 2*primes[i];
                }
                if (multiples[i] == candidate) {
                    continue candidates;
                }
            }
            primes[primesFound] = candidate;

            // Start with the prime's square here, rather than 3x the prime.
            // This saves time and is safe because all of the intervening
            // multiples will be detected by smaller prime numbers. As an
            // example, consider the prime 7: the value in multiples will
            // start at 49; 21 will be ruled out as a multiple of 3, and
            // 35 will be ruled out as a multiple of 5, so 49 is the first
            // multiple that won't be ruled out by a smaller prime.
            multiples[primesFound] = candidate*candidate;
            primesFound++;
        }
        return primes;
    }
}
</code></pre></div>
<p dir="auto">Everyone can read this and decide for themselves whether they think
it is easier to understand than the original. I'd like to mention a
couple of overall things:</p>
<ul dir="auto">
<li>There is only one method. I didn't subdivide it because I felt the method already divides naturally into pieces that are distinct and understandable. It didn't seem to me that pulling out methods would improve readability significantly. When students rewrite the code, they typically have 2 or 3 methods, and those are usually OK too.</li>
<li>There are a <em>lot</em> of comments. It's extremely rare for me to write code with this density of comments. Most methods I write have no comments in the body, just a header comment describing the interface. But this code is subtle and tricky, so it needs a lot of comments to make the subtleties clear to readers. The long length of some of the comments is a red flag indicating that I struggled to find a clear and simple explanation for the code. Even with all the additional explanatory material this version is a bit shorter than the original (65 lines vs. 70).</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I presume this is a complete rewrite.  My guess is that you worked to understand the algorithm from <em>Clean Code</em> and then wrote this from scratch.  If that's so, then fair enough.</p>
<p dir="auto">In <em>Clean Code</em> I <em>refactored</em> Knuth's algorithm in order to give it a little structure.  That's not the same as a complete rewrite.</p>
<p dir="auto">Having said that, your version is much better than either Knuth's or mine.</p>
<p dir="auto">I wrote that chapter 18 years ago, so it's been a long time since I saw and understood this algorithm.  When I first saw your challenge I thought: "Oh, I can figure out my own code!"  But, no.  I could see all the moving parts, but I could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">So then I looked at your code.  I had the same problem.  I could see all the moving parts, all with comments, but I still could not figure out why those moving parts generated a list of prime numbers.</p>
<p dir="auto">Figuring that out required a lot of staring at the ceiling, closing my eyes, visualizing, and riding my bike.</p>
<p dir="auto">Among the problems I had were the comments you wrote.  Let's take them one at a time.</p>
<div data-snippet-clipboard-copy-content="    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {"><pre><code>    /**
     * Computes the first prime numbers; the return value contains the
     * computed primes, in increasing order of size.
     * @param n
     *      How many prime numbers to compute.
     */
    public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">It seems to me that this would be better as:</p>
<div data-snippet-clipboard-copy-content="public static int[] generateNPrimeNumbers(int n) {"><pre><code>public static int[] generateNPrimeNumbers(int n) {
</code></pre></div>
<p dir="auto">or if you must:</p>
<div data-snippet-clipboard-copy-content="//Return the first n prime numbers
public static int[] generate(int n) {"><pre><code>//Return the first n prime numbers
public static int[] generate(int n) {
</code></pre></div>
<p dir="auto">I'm not opposed to Javadocs as a rule; but I write them only when absolutely necessary. I also have an aversion for descriptions and <code>@param</code> statements that are perfectly obvious from the method signature.</p>
<p dir="auto">The next comment cost me a good 20 minutes of puzzling things out.</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">First of all I'm not sure why the "division" statement is necessary.  I'm old school so I expect that everyone knows to avoid division in inner loops if it can be avoided.  But maybe I'm wrong about that...</p>
<p dir="auto">Also, the <em>Sieve of Eratosthenes</em> does not do division, and is a lot easier to understand <em>and explain</em> than this algorithm.  So why this particular algorithm?  I think Knuth was trying to save <em>memory</em> -- and in 1982 saving memory was important.  This algorithm uses a lot less memory than the sieve.</p>
<p dir="auto">Then came the phrase: <code>Each entry here contains an odd multiple...</code>.  I looked at that, and then at the code, and I saw: <code>multiples[0] = 4;</code>.</p>
<p dir="auto">"That's not odd" I said to myself.  "So maybe he meant even."</p>
<p dir="auto">So then I looked down and saw: <code>multiples[i] += 2*primes[i];</code></p>
<p dir="auto">"That's adding an even number!" I said to myself.  "I'm pretty sure he meant to say 'even' instead of 'odd'."</p>
<p dir="auto">I hadn't yet worked out what the <code>multiples</code> array was.  So I thought it was perfectly reasonable that it would have even numbers in it, and that your comment was simply an understandable word transposition.  After all, there's no compiler for comments so they suffer from the kinds of mistakes that humans often make with words.</p>
<p dir="auto">It was only when I got to <code>multiples[primesFound] = candidate*candidate;</code> that I started to question things.  If the <code>candidate</code> is prime, shouldn't <code>prime*prime</code> be odd in every case beyond 2?  I had to do the math in my head to prove that.  (2n+1)(2n+1) = 4n^2+4n+1 ... Yeah, that's odd.</p>
<p dir="auto">OK, so the <code>multiples</code> array is full of odd multiples, except for the first element, since it will be muliples of 2.</p>
<p dir="auto">So perhaps that comment should be:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">Or perhaps we should change the name of the array to something like <code>primeMultiples</code> and drop the comment altogether.</p>
<p dir="auto">Moving on to the next comment:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">That doesn't make a lot of sense.  The code it's talking about is:</p>
<div data-snippet-clipboard-copy-content="for (int i = 1; i <= lastMultiple; i++) {
    while (multiples[i] < candidate) {"><pre><code>for (int i = 1; i &lt;= lastMultiple; i++) {
    while (multiples[i] &lt; candidate) {
</code></pre></div>
<p dir="auto">The <code>multiples</code> array, as we have now learned, is an array of <em>multiples</em> of prime numbers.  This loop is not testing the candidate against prime <em>factors</em>, it's testing it against the current prime <em>multiples</em>.</p>
<p dir="auto">Fortunately for me the third of fourth time I read this comment I realized that you really meant to use the word "multiples".  But the only way for me to know that was to understand the algorithm.  And when I understand the algorithm, why do I need the comment?</p>
<p dir="auto">That left me with one final question.  What the deuce was the reason behind:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate*candidate;"><pre><code>multiples[primesFound] = candidate*candidate;
</code></pre></div>
<p dir="auto">Why the square?  That makes no sense.  So I changed it to:</p>
<div data-snippet-clipboard-copy-content="multiples[primesFound] = candidate;"><pre><code>multiples[primesFound] = candidate;
</code></pre></div>
<p dir="auto">And it worked just fine.  So this must be an optimization of some kind.</p>
<p dir="auto">Your comment to explain this is:</p>
<div data-snippet-clipboard-copy-content="// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime."><pre><code>// Start with the prime's square here, rather than 3x the prime.
// This saves time and is safe because all of the intervening
// multiples will be detected by smaller prime numbers. As an
// example, consider the prime 7: the value in multiples will
// start at 49; 21 will be ruled out as a multiple of 3, and
// 35 will be ruled out as a multiple of 5, so 49 is the first
// multiple that won't be ruled out by a smaller prime.
</code></pre></div>
<p dir="auto">The first few times I read this it made no sense to me at all.  It was just a jumble of numbers.</p>
<p dir="auto">I stared at the ceiling, and closed my eyes to visualize. I couldn't see it.  So I went on a long contemplative bike ride during which I realized that the prime multiples of 2 will at one point contain 2*3 and then 2*5.  So the <code>multiples</code> array will at some point contain multiples of primes <em>larger</em> than the prime they represent.  <em>And it clicked!</em></p>
<p dir="auto">Suddenly it all made sense. I realized that the <code>multiples</code> array was the equivalent of the array of booleans we use in the <em>Sieve of Eratosthenes</em> -- but with a really interesting twist.  If you were to do the sieve on a whiteboard, you <em>could</em> erase every number less than the candidate, and only cross out the numbers that were the next multiples of all the previous primes.</p>
<p dir="auto">That explanation makes perfect sense to me -- now, but I'd be willing to bet that those who are reading it are puzzling over it.  The idea is just hard to explain.</p>
<p dir="auto">Finally I went back to your comment and could see what you were saying.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">A Tale of Two Programmers</h3><a id="user-content-a-tale-of-two-programmers" aria-label="Permalink: A Tale of Two Programmers" href="#a-tale-of-two-programmers"></a></p>
<p dir="auto">The bottom line here is that you and I both fell into the same trap.  I refactored that old algorithm 18 years ago, and I thought all those method and variable names would make my intent clear -- <em>because I understood that algorithm</em>.</p>
<p dir="auto">You wrote that code awhile back and decorated it with comments that you thought would explain your intent -- <em>because you understood that algorithm</em>.</p>
<p dir="auto">But my names didn't help me 18 years later.  They didn't help you, or your students either.  And your comments didn't help me.</p>
<p dir="auto">We were inside the box trying to communicate to those who stood outside and could not see what we saw.</p>
<p dir="auto">The bottom line is that it is very difficult to explain something to someone who is not intimate with the details you are trying to explain. Often our explanations make sense only after the reader has worked out the details for themself.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">There's a lot of stuff in your discussion above, but I think it all boils down
to one thing: you don't like the comments that I wrote. As I mentioned earlier,
complexity is in the eye of the reader: if you say that my comments were
confusing or didn't help you to understand the code, then I have to take that
seriously.</p>
<p dir="auto">At the same time, you have made it clear that you don't see much value in
comments in general. Your preference is to have essentially no
comments for this code (or any code). You argue above that there is simply nothing that
comments can do to make the code easier to understand; the only way to
understand the code is to read the code. That is a cop-out.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Sorry to interrupt you; but I think you are overstating my position.  I certainly never said that comments can never be helpful.  Sometimes, of course, they are.  What I said was that I only trust them if the code validates them.  Sometimes a comment will make that validation a lot easier.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You keep saying that you sometimes find use for comments, but the reality
is that "sometimes" almost never occurs in your code. We'll see this when
we look at your revision of my code.</p>
<p dir="auto">Now back to my point. In order to
write our various versions of the code, you and I had to accumulate a lot of
knowledge about the algorithm, such as why it's OK for the first multiple
of a prime to be its square. Unfortunately, not all of that knowledge can
be represented in the code. It is our professional responsibility to do
the best we can to convey
that knowledge in comments, so that readers do not
have to reconstruct it over and over. Even if the resulting comments are
imperfect, they will make the code easier to understand.</p>
<p dir="auto">If a situation like this occurred in real life I would work with
you and others to improve my comments. For example, I would ask you
questions to get a better sense of
why the "squared prime" comment didn't seem to help you:</p>
<ul dir="auto">
<li>Are there things in the comment that are misleading or confusing?</li>
<li>Is there some important piece of information you acquired on your
bike ride that suddenly made things clear?</li>
</ul>
<p dir="auto">I would also show the comment to a few other people to get their takes
on it. Then I would rework the comment to improve it.</p>
<p dir="auto">Given your fundamental disbelief in comments, I think it's likely that
you would still see no value in the comment, even after my reworking.
In this case I would show the comment to other people, particularly those
who have a more positive view of comments in general, and get
their input. As long as the comment is not misleading and at least a few
people found it helpful, I would retain it.</p>
<p dir="auto">Now let me me discuss two few specific comments that you objected to. The
first comment was the one for the <code>multiples</code> variable:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// here contains an odd multiple of the corresponding entry in
// primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">There is a bug in this comment that you exposed (the first entry is not odd);
good catch! You then argued that most of the information in the comment
is unnecessary and proposed this as an alternative:</p>
<div data-snippet-clipboard-copy-content=" // multiples of corresponding prime."><pre><code> // multiples of corresponding prime.
</code></pre></div>
<p dir="auto">You have left out too much useful information here. For example, I don't think
it is safe to assume that readers will figure out that the motivation is
avoiding divisions. It's always better to state these assumptions and
motivations clearly so that there will be no confusion. And I think it's
helpful for readers to know that these entries never decrease.
I would simply fix the bug, leaving all of the information intact:</p>
<div data-snippet-clipboard-copy-content="// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically."><pre><code>// Used to test efficiently (without division) whether a candidate
// is a multiple of a previously-encountered prime number. Each entry
// (except the first, which is never used) contains an odd multiple of
// the corresponding entry in primes. Entries increase monotonically.
</code></pre></div>
<p dir="auto">The second comment was this one, for the <code>for</code> loop:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates."><pre><code>// Each iteration of this loop tests the candidate against one
// potential prime factor. Skip the first factor (2) since we
// only consider odd candidates.
</code></pre></div>
<p dir="auto">You objected to this comment because the code of the loop doesn't actually
test the candidate against the prime factor; it tests it against a multiple.
When I write implementation comments like this, my goal is not to restate
the code; comments like that don't usually provide much value. The goal here was
to say <em>what</em> the code is doing in a logical sense, not <em>how</em> it does it.
In that sense, the comment is correct.</p>
<p dir="auto">However, if a comment causes confusion in the reader, then it is not a
good comment. Thus I would rewrite this comment to make it clear that
it describes the abstract function of the code, not its
precise behavior:</p>
<div data-snippet-clipboard-copy-content="// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates."><pre><code>// Each iteration of this loop considers one existing prime, ruling
// out the candidate if it is a multiple of that prime. Skip the
// first prime (2) since we only consider odd candidates.
</code></pre></div>
<p dir="auto">To conclude, I agree with your assertion "it is very difficult to explain
something to someone who is not intimate with the details you are trying
to explain." And yet, it is our responsibility as programmers to do exactly
that.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'm glad we agree.  We also agree about getting others to review the code and make recommendations on the code <em>and</em> the comments.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Bob's Rewrite of PrimeGenerator2</h2><a id="user-content-bobs-rewrite-of-primegenerator2" aria-label="Permalink: Bob's Rewrite of PrimeGenerator2" href="#bobs-rewrite-of-primegenerator2"></a></p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">When I saw your solution, and after I gained a good understanding of it.  I refactored it just a bit.  I loaded it into my IDE, wrote some simple tests, and extracted a few simple methods.</p>
<p dir="auto">I also got rid of that <em>awful</em> labeled <code>continue</code> statement.  And I added 3 to the primes list so that I could mark the first element as <em>irrelevant</em> and give it a value of -1.  (I think I was still reeling from the even/odd confusion.)</p>
<p dir="auto">I like this because the implementation of the <code>generateFirstNPrimes</code> method describes the moving parts in a way that hints at what is going on.  It's easy to read that implementation and get a glimpse of the mechanism.  I'm not at all sure that the comment helps.</p>
<p dir="auto">I think it is just the reality of this algorithm that the effort required to properly explain it, and the effort required for anyone else to read and understand that explanation is roughly equivalent to the effort needed to read the code and go on a bike ride.</p>
<div data-snippet-clipboard-copy-content="package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound < n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate >= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i <= lastRelevantMultiple; i++)
            while (primeMultiples[i] < candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i <= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}"><pre><code>package literatePrimes;

public class PrimeGenerator3 {
    private static int[] primes;
    private static int[] primeMultiples;
    private static int lastRelevantMultiple;
    private static int primesFound;
    private static int candidate;

    // Lovely little algorithm that finds primes by predicting
    // the next composite number and skipping over it. That prediction
    // consists of a set of prime multiples that are continuously
    // increased to keep pace with the candidate.

    public static int[] generateFirstNPrimes(int n) {
        initializeTheGenerator(n);

        for (candidate = 5; primesFound &lt; n; candidate += 2) {
            increaseEachPrimeMultipleToOrBeyondCandidate();
            if (candidateIsNotOneOfThePrimeMultiples()) {
                registerTheCandidateAsPrime();
            }
        }
        return primes;
    }

    private static void initializeTheGenerator(int n) {
        primes = new int[n];
        primeMultiples = new int[n];
        lastRelevantMultiple = 1;

        // prime the pump. (Sorry, couldn't resist.)
        primesFound = 2;
        primes[0] = 2;
        primes[1] = 3;

        primeMultiples[0] = -1;// irrelevant
        primeMultiples[1] = 9;
    }

    private static void increaseEachPrimeMultipleToOrBeyondCandidate() {
        if (candidate &gt;= primeMultiples[lastRelevantMultiple])
            lastRelevantMultiple++;

        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            while (primeMultiples[i] &lt; candidate)
                primeMultiples[i] += 2 * primes[i];
    }

    private static boolean candidateIsNotOneOfThePrimeMultiples() {
        for (int i = 1; i &lt;= lastRelevantMultiple; i++)
            if (primeMultiples[i] == candidate)
                return false;
        return true;
    }

    private static void registerTheCandidateAsPrime() {
        primes[primesFound] = candidate;
        primeMultiples[primesFound] = candidate * candidate;
        primesFound++;
    }
}
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">This version is a considerable improvement over the version in <em>Clean Code</em>.
Reducing the number of methods made the code easier to read and resulted
in cleaner interfaces. If it were properly commented, I think this version
would be about as easy to read as my version (the additional methods you
created didn't particularly help, but they didn't hurt either). I suspect
that if we polled readers, some would like your version better and some
would prefer mine.</p>
<p dir="auto">Unfortunately, this revision of the code creates a serious performance
regression: I measured a factor of 3-4x slowdown compared to either
of the earlier revisions. The problem is that you changed the processing of a
particular candidate from a single loop to two loops (the <code>increaseEach...</code> and
<code>candidateIsNot...</code> methods). In the loop from earlier revisions, and in
the <code>candidateIsNot</code>
method, the loop aborts once the candidate is disqualified (and
most candidates are quickly eliminated). However,
<code>increaseEach...</code> must examine every entry in <code>primeMultiples</code>.
This results in 5-10x as many loop iterations and a 3-4x overall slowdown.</p>
<p dir="auto">Given that the whole reason for the current algorithm (and its complexity)
is to maximize performance, this slowdown is unacceptable. The two
methods must be combined.</p>
<p dir="auto">I think what happened here is that you were so focused on something
that isn't actually all that important (creating the tiniest possible methods)
that you dropped the ball on other issues that really are important.
We have now seen this twice. In the original version of <code>PrimeGenerator</code>
you were so determined to make tiny methods that you didn't notice that the
code was becoming incomprehensible. In this version you were so eager to
chop up my single method that you didn't notice that you were blowing up the
performance.</p>
<p dir="auto">I don't think this was just an unfortunate combination of oversights.
One of the most important things
in software design is to identify what is important and focus on that;
if you focus on things that are unimportant, you're likely to mess up the
things that are important.</p>
<p dir="auto">The code in your revision is still under-commented. You believe
that there is no meaningful way for comments to assist the reader in
understanding the code. I think this stems from your general disbelief in
the value of comments; you are quick to throw in the towel.
This algorithm is unusually difficult to explain,
but I still believe that comments can help. For example, I believe you
must make some attempt to help readers understand why the first multiple
for a prime is the square of the prime. You have taken a lot of time to
develop your understanding of this; surely there must be some way to convey
that understanding to others? If you had included that information in
your original version of the code you could have saved yourself that long
bike ride.
Giving up on this is an abdication of professional responsibility.</p>
<p dir="auto">The few comments that you included in your revision are of little value.
The first comment is too cryptic to provide much help: I can't
make any sense of the phrase "predicting the next composite number and
skipping over it" even though I completely understand the code it purports
to explain. One of the comments is just a joke; I was surprised to see
this, given your opposition to extraneous comments.</p>
<p dir="auto">Clearly you and I live in different universes when it comes to comments.</p>
<p dir="auto">Finally, I don't understand why you are offended by the labeled <code>continue</code>
statement in my code. This is a clean and elegant solution to the problem
of escaping from nested loops. I wish more languages
had this feature; the alternative is awkward code where you set a variable,
then exit one level of loop, then check the variable and exit the next
level.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Good catch!  I would have caught that too had I thought to profile the solution.  You are right that separating the two loops added some unecessary iteration.  I found a nice way to solve that problem without using the horrible <code>continue</code>.  My updated version is now faster than yours!  A million primes in 440ms as opposed to yours which takes 561ms.  ;-) Below are just the changes.</p>
<div data-snippet-clipboard-copy-content="  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound < n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate >= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i <= lastRelevantMultiple; i++) {
      while (primeMultiples[i] < candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }"><pre><code>  public static int[] generateFirstNPrimes(int n) {
    initializeTheGenerator(n);

    for (candidate = 5; primesFound &lt; n; candidate += 2)
      if (candidateIsPrime())
        registerTheCandidateAsPrime();

    return primes;
  }

  private static boolean candidateIsPrime() {
    if (candidate &gt;= primeMultiples[lastRelevantMultiple])
      lastRelevantMultiple++;

    for (int i = 1; i &lt;= lastRelevantMultiple; i++) {
      while (primeMultiples[i] &lt; candidate)
        primeMultiples[i] += 2 * primes[i];
      if (primeMultiples[i] == candidate)
        return false;
    }
    return true;
  }
</code></pre></div>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Yep, that fixes the problem. I note that you are now down to 4 methods,
from 8 in the <em>Clean Code</em> version.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Test-Driven Development</h2><a id="user-content-test-driven-development" aria-label="Permalink: Test-Driven Development" href="#test-driven-development"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Let's move on to our third area of disagreement, which is Test-Driven
Development. I am a huge fan of unit testing. I believe that unit tests are
an indispensable part of the software development process and pay for
themselves over and over. I think we agree on this.</p>
<p dir="auto">However, I am not fan of Test-Driven Development (TDD), which dictates
that tests must be written before code and that code must be written
and tested in tiny increments. This approach has serious problems
without any compensating advantages that I have been able to identify.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">As I said at the start I have carefully read <em>A Philosophy of Software Design</em>. I found it to be full of worthwhile insights, and I strongly agree with most of the points you make.</p>
<p dir="auto">So I was surprised to find, on page 157, that you wrote a very short, dismissive, pejorative, and inaccurate section on <em>Test Driven Development</em>.  Sorry for all the adjectives, but I think that's a fair characterization.  So my goal, here, is to correct the misconceptions that led you to write the following:</p>
<blockquote>
<p dir="auto">"Test-driven development is an approach to software development where programmers write unit tests before they write code.  When creating a new class, the develper first writes unit tests for the class, based on its expected behavior.  None of these tests pass, since there is no code for the class.  Then the developer works through the tests one at a time, writing enough code for that test to pass.  When all of the tests pass, the class is finished."</p>
</blockquote>
<p dir="auto">This is just wrong.  TDD is quite considerably different from what you describe.  I describe it using three laws.</p>
<ol dir="auto">
<li>
<p dir="auto">You are not allowed to write any production code until you have first written a unit test that fails because that code does not exist.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more of a unit test than is sufficient to fail, and failing to compile is failing.</p>
</li>
<li>
<p dir="auto">You are not allowed to write more production code than is sufficient to make the currently failing test pass.</p>
</li>
</ol>
<p dir="auto">A little thought will convince you that these three laws will lock you into a cycle that is just a few seconds long.  You'll write a line or two of a test that will fail, you'll write a line or two of production code that will pass, around and around every few seconds.</p>
<p dir="auto">A second layer of TDD is the Red-Green-Refactor loop.  This loop is several minutes long.  It is comprised of a few cycles of the three laws, followed by a period of reflection and refactoring.  During that reflection we pull back from the intimacy of the quick cycle and look at the design of the code we've just written.  Is it clean?  Is it well structured?  Is there a better approach?  Does it match the design we are pursuing?  If not, should it?</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Oops! I plead "guilty as charged" to inaccurately describing TDD.
I will fix this in the next revision of APOSD. That said, your definition
of TDD does not change my concerns.</p>
<p dir="auto">Let's discuss the potential advantages and disadvantages
of TDD; then readers can decide for themselves whether they think TDD is a
good idea overall.</p>
<p dir="auto">Before we start that discussion, let me clarify the approach I prefer as an
alternative to TDD. In your online videos you describe the alternative to
TDD as one where a developer writes the code, gets it fully working
(presumably with manual tests), then goes back and writes the unit tests.
You argue that this approach would be terrible: developers
lose interest once they think code is working, so they wouldn't actually
write the tests. I agree with you completely. However, this isn't the only
alternative to TDD.</p>
<p dir="auto">The approach I prefer is one where the developer works in somewhat
larger units than in TDD, perhaps a few methods or a class. The developer
first writes some code (anwywhere from a few tens of lines to a few hundred
lines), then writes unit tests for that code. As with TDD, the
code isn't considered to be "working" until it has comprehensive unit
tests.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">How about if we call this technique "bundling" for purposes of this
document?  This is the term I use in <em>Clean Code 2d ed.</em></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Fine by me.</p>
<p dir="auto">The reason for working in larger units is to encourage design
thinking, so that a developer can think about a collection of related
tasks and do a bit of planning to come up with a good overall design
where the pieces fit together well.
Of course the initial design ideas will have flaws and refactoring
will still be necessary, but the goal is to center the development
process around design, not tests.</p>
<p dir="auto">To start our discussion, can you make a list of the advantages you
think that TDD provides over the approach I just described?</p>
<p dir="auto"><strong>UB:</strong>
The advantages I usually attribute to TDD are:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging.  After all, if you just saw everything working a minute or two ago, there's not much to debug.</p>
</li>
<li>
<p dir="auto">A stream of reliable low level documentation, in the form of very small and isolated unit tests.  Those tests describe the low level structure and operation of every facet of the system.  If you want to know how to do something in the system, there are tests that will show you how.</p>
</li>
<li>
<p dir="auto">A less coupled design which results from the fact that every small part of the system must be designed to be testable, and testability requires decoupling.</p>
</li>
<li>
<p dir="auto">A suite of tests that you trust with your life, and therefore supports fearless refactoring.</p>
</li>
</ul>
<p dir="auto">However, you asked me which of these advantages TDD might have over <em>your</em> preferred method.  That depends on how big you make those larger units you described.  The important thing to me is to keep the cycle time short, and to prevent entanglements that block testability.</p>
<p dir="auto">It seems to me that working in small units, and then immediately writing after the fact tests, can give you all the above advantages, so long as you are very careful to test every aspect of the code you just wrote.  I think a disciplined programmer could effectively work that way.  Indeed, I think such a programmer would produce code that I could not distinguish from code written by another programmer following TDD.</p>
<p dir="auto">Above you suggested that bundling is to encourage design.  I think encouraging design is a very good thing.  My question for you is: Why do you think that TDD does not encourage design?  My own experience is that design comes from strategic thought, which is independent of the tactical behavior of either TDD or Bundling.  Design is taking one step back from the code and envisioning structures that address a larger set of constraints and needs.</p>
<p dir="auto">Once you have that vision in your head it seems to me bundling and TDD will yield similar results.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, let me address the four advantages you listed for TDD:</p>
<ul dir="auto">
<li>
<p dir="auto">Very little need for debugging? I think any form of unit testing can
reduce debugging work, but not for the reason you
suggested. The benefit comes because unit tests expose bugs earlier
and in an environment where they are easier to track down. A
relatively simple bug to fix in development can be very painful to
track down in production. I'm not convinced by your argument that
there's less debugging because "you just saw everything working a
minute ago": it's easy to make a tiny change that exposes a really
gnarly bug that has existed for a long time but hasn't yet been
triggered. Hard-to-debug problems arise from the accumulated complexity
of the system, not from the size of the code increments.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> True.  However, when the cycles are very short then the cause
of even the gnarliest of bugs have the best chance of being tracked down.
The shorter the cycles, the better the chances.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> This is only true up to a point. I think you believe
that making units smaller and smaller continues to provide benefits,
with almost no limit to how small they can get. I think that there
is a point of diminishing returns, where making things even smaller
no longer helps and actually starts to hurt. We saw this disagreement
over method length, and I think we're seeing it again here.</p>
</blockquote>
</li>
<li>
<p dir="auto">Low level documentation? I disagree: unit tests are a poor form
of documentation. Comments are a much more
effective form of documentation, and you can put them right next to the
relevant code. Trying to learn a method's
interface by reading a bunch of unit tests seems much more difficult
than just reading a couple of sentences of English text.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Nowadays it's very easy to find the tests for
a function by using the "where-used" feature of the IDE.  As for comments
being better, if that were true then no one would publish example code.</p>
</blockquote>
</li>
<li>
<p dir="auto">A less coupled design? Possibly, but I haven't experienced this myself.
It's not clear to me that designing for testability will produce the
best design.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Generally the decoupling arises because the test requires a mock
of some kind.  Mocks tend to force abstractions that might otherwise not exist.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong> In my experience, mocking virtually never changes interfaces;
it just provides replacements for existing (typically immovable)
interfaces.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>UB:</strong> Our experiences differ.</p>
</blockquote>
</li>
<li>
<p dir="auto">Enabling fearless refactoring? BINGO! This is the where almost all of the
benefits from unit testing come from, and it is a really really big deal.</p>
<blockquote>
<p dir="auto"><strong>UB:</strong> Agreed.</p>
</blockquote>
</li>
</ul>
<p dir="auto">I agree with your conclusion that TDD and bundling are about the
same in terms of providing these benefits.</p>
<p dir="auto">Now let me explain why I think TDD is likely to result in bad designs.
The fundamental problem with TDD is that it forces developers to work
too tactically, in
units of development that are too small; it discourages design
thinking.  With TDD the basic unit of
development is one test: first the test is written, then the code to
make that test pass. However, the natural units for design are larger
than this: a class or method, for example. These units
correspond to multiple test cases. If a developer thinks only about
the next test, they are only considering part of a design problem at
any given time. It's hard to design something well if you don't think
about the whole design problem at once. TDD explicitly
prohibits developers from writing more code than is needed to pass
the current test; this discourages the kind of strategic thinking needed
for good design.</p>
<p dir="auto">TDD does not provide adequate guidance to encourage design. You mentioned
the Red-Green-Refactor loop, which recommends refactoring after each step,
but there's almost no guidance for refactoring. How should developers
decide when and what to refactor? This seems to be left purely to their
own judgment. For example, if I am writing a method that requires
multiple iterations of the TDD loop, should I refactor after every iteration
(which sounds pretty tedious) or wait until after several iterations so that
I can look at a bigger chunk of code when refactoring and hence be more
strategic? Without guidance it will be tempting for developers to keep
putting off refactoring.</p>
<p dir="auto">TDD is similar to the One Thing Rule we discsused earlier in that it is
biased: it provides very strong and clear instructions pushing developers
in one direction (in this case, acting tactically) with only vague
guidance in the other direction (designing more strategically). As a result,
developers are likely to err on the side of being too tactical.</p>
<p dir="auto">TDD guarantees that developers will initially write bad code. If you start
writing code without thinking about the whole design problem, the first code
you write will almost certainly be wrong. Design only
happens after a bunch of bad code has accumulated.
I watched your video on TDD, and
you repeatedly wrote the wrong code, then fixed it later. If the developer
refactors conscientiously (as you did) they can still end up with good
code, but this works against human nature. With TDD, that bad code will
actually work (there are tests to prove it!) and it's human nature not
to want to change something that
works. If the code I'm developing is nontrivial, I will probably have to
accumulate a lot of bad code with TDD before I have enough code in front
of me to understand what the design should have been.
It will be very difficult for me to force myself to throw away
all that work.</p>
<p dir="auto">It's easy for a developer to believe they are doing TDD correctly while
working entirely tactically, layering on hack after hack with an
occasional minor refactor, without ever thinking about the overall design.</p>
<p dir="auto">I believe that the bundling approach is superior to TDD because it focuses
the development process around design: design first, then code, then write
unit tests. Of course, refactoring will still be
required: it's almost never possible to get the design right the first time.
But starting with design will reduce the amount of bad code you write and
get you to a good design sooner. It is possible to produce equally good
designs with TDD; it's just harder and requires a lot more discipline.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I'll address your points one at a time.</p>
<ul dir="auto">
<li>
<p dir="auto">I haven't found that the scale of TDD is so tactical that it discourages thinking.  Every programmer, regardless of their testing discipline, writes code one line at a time.  That's immensely tactical and yet does not discourage design.  So why would one test at a time discourage it?</p>
</li>
<li>
<p dir="auto">The literature on TDD strongly discourages delaying refactoring.  While thinking about design is strongly encouraged.  Both are integral parts of the discipline..</p>
</li>
<li>
<p dir="auto">We all write bad code at the start.  The discipline of TDD gives us the opportunity, and the safety, to continuously clean it.  Design insights arise from those kinds of cleaning activities.  The discipline of refactoring allows bad designs to be transformed, one step at a time, into better designs.</p>
</li>
<li>
<p dir="auto">It's not clear to me why the act of writing tests late is a better design choice.  There's nothing in TDD that prevents me from thinking through a design long before I write the very first tested code.</p>
</li>
</ul>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You say there is nothing about TDD that stops developers from thinking ahead
about design. This is only partly true. Under TDD I can think ahead, but I
can't actually write my ideas down in the form of code, since that would
violate TDD Rule 1. This is a significant discouragement.</p>
<p dir="auto">You claim that "thinking about design is strongly encouraged" in TDD,
but I haven't seen this in your discussions of TDD. I watched your
video example of using TDD
for computing bowling scores, and design is never even mentioned after the
first minute or two (ironically, one of the conclusions of this
example is that the brief initial design turned out to be
useless). There is no suggestion of thinking ahead in the video;
it's all about cleaning up messes after the fact.
In all of the TDD materials you have shown me, I have not seen any
warnings about the dangers of becoming so tactical with TDD that
design never occurs (perhaps you don't even view this as a serious risk?).</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I usually use an abbeviated form of UML to capture my early design decisions.  I have no objection to capturing them in pseudo-code, or even real code.  However, I would not commit any such pre-written code.  I would likely hold it in a text file, and consult it while following the TDD cycle.  I might feel safe enough to copy and paste from the text file into my IDE in order to make a failing test pass.</p>
<p dir="auto">The Bowling game is an example of how wildly our initial design decisions can  deviate from our eventual solutions.  It's true that introductory videos often do not expose the depth of a discipline.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">As I was watching your TDD video for the second time, you said something
that jumped out at me:</p>
<blockquote>
<p dir="auto">Humans consider things that come first to be important and things that
come at the end to be less important and somehow optional; that's
why they are at the end, so we can leave them out if we have to.</p>
</blockquote>
<p dir="auto">This captures perfectly my concern about TDD. TDD insists that tests must
come first, and design, if it happens at all, comes at the end, after
code is working. I believe that good design is the most important
thing, so it must be the top priority. I don't consider tests optional,
but delaying them is safer than delaying design. Writing tests isn't particularly
difficult; the most important thing is having the discipline to do it.
Getting a good design is really hard, even if you are very disciplined;
that's why it needs to be the center of attention.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">TDD is a coding discipline.  Of course design comes before coding -- I don't know anyone who thinks otherwise.  Even the Bowling Game video made that point. But, as we saw in the Bowling Game video, sometimes the code will take you in a very different direction.</p>
<p dir="auto">That difference does't imply that the design shouldn't have been done.  It just implies that designs are speculative and may not aways survive reality.</p>
<p dir="auto">As Eisenhower once said:</p>
<blockquote>
<p dir="auto">“In preparing for battle I have always found that plans are useless, but planning is indispensable.”</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask why writing tests later is a better design choice. It isn't.
The benefit of the bundled approach doesn't come from writing tests later;
it comes from doing design sooner. Writing tests (a bit) later is a
consequence of this choice. The tests are still written pretty early-on
with the bundled approach, so I don't think the delay causes significant
problems.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">I think we simply disagree that TDD discourages design.  The practice of TDD does not discourage me from design; because I value design.  I would suggest that those who do not value design will not design, no matter what discipline they practice.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You claim that the problems I worry about with TDD simply don't happen in
practice. Unfortunately I have heard contrary claims from senior
developers that I trust. They complain about horrible code produced by
TDD-based teams, and they believe that the problems were caused by TDD.
Of course horrible code can be produced with any design approach.
And maybe those teams didn't implement TDD properly, or maybe those
cases were outliers.
But the problems reported to me line up exactly with what I would
expect to happen, given the tactical nature of TDD.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">My experience differs. I've worked on many projects where TDD has been used
effectively and profitably.  I'm sure the senior developers that you trust are telling you the truth about their experience.  Having never seen TDD lead to such bad outcomes myself, I sincerely doubt that the blame can be traced to TDD.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">You ask me to trust your extensive experience with
TDD, and I admit that I have no personal experience with TDD.
On the other hand, I have a lot of experience with tactical programming,
and I know that it rarely ends well.
TDD is one of the most extreme forms of tactical programming I've
encountered.
In general, if "making it work" is the #1 prority, instead of
"develop a clean design", code turns to spaghetti.
I don't see enough safeguards in your approach to TDD
to prevent the disaster scenarios; I don't even see a clear
recognition of the risk.</p>
<p dir="auto">Overall, TDD is in a bad place on the risk-reward spectrum. In comparison
to the bundling approch, the downside risks for poor code quality in TDD
are huge, and I don't see enough upside reward (if any) to compensate.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">All I can say to that is that your opinion is based on a number of false impressions and speculations, and not upon direct experience.</p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Now let me ask you a couple of questions.</p>
<p dir="auto">First, at a microscopic level, why on earth does TDD prohibit developers
from writing more code than needed to pass the current test? How does
enforcing myopia make systems better?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
The goal of the discipline is to make sure that everything is tested.
One good way to do that is to refuse to write any code unless it is to make a failing test pass.  Also, working in such short cycles provides insights into
the way the code is working.  Those insights often lead to better design decisions.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I agree that seeing code (partially) working can provide insights. But
surely that benefit can be had without such a severe restriction on
how developers think?</p>
</blockquote>
<p dir="auto">Second, at a broader level, do you think TDD is likely to produce better
designs than approaches that are more design-centric, such as the bundling
approach I described? If so, can you explain why?</p>
<blockquote>
<p dir="auto"><strong>UB:</strong>
My guess is that someone adept at bundling, and someone adept at TDD would produce very similar designs, with very similar test coverage.  I would also venture to guess that the TDDer would be somewhat more productive than the bundler if for no reason other than that the TDDer finds and fixes problems earlier than the bundler.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>JOHN:</strong>
I think that the bundling approach will result in a better design because
it actually focuses on design, rather than focusing on tests and hoping
that a good design will magically emerge. I think it's really hard to argue
that the best way to achieve one thing is to focus your attention on
something else. And the bundling approach will
make progress faster because the early thinking about design will reduce the
amount of bad code you end up having to throw away under TDD. Overall, I'd
argue that the best-case outcomes for the two approaches will
be about the same, but average and (especially) worst-case outcomes will
be far worse for TDD.</p>
</blockquote>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">I don't think we're going to resolve our disagreements on TDD.
To do that, we'd need empirical data about the frequency of good and bad
outcomes from TDD. Unfortunately I'm not aware of any such data.
Thus, readers will have to decide for themselves whether the potential
benefits of TDD outweigh the risks.</p>
<p dir="auto">For anyone who chooses to use TDD, I urge you to do so with extreme
caution. Your primary goal must not be just working code, but rather a
clean design that will allow you to develop quickly in the future.
TDD will not lead you naturally to the best design, so you will need
to do significant and continuous refactoring to avoid spaghetti code.
Ask yourself repeatedly "suppose that I knew everything I know now when
I first started on this project; would I have chosen the current
structure for the code?" When the answer is no (which will happen
frequently) stop and refactor. Recognize that TDD will cause you to
write more bad code than you may be used to, so
you must be prepared to throw out and rewrite more than you are used to.
Take time to plan ahead and think about the overall design, rather than
just making the next test work.
If you do all of these things diligently, I think it is possible to
mitigate the risks of TDD and produce well-designed code.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">Let's just say that I agree with all that advice, but disagree with your assertion that TDD might be the cause of bad code.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">TDD Summary</h3><a id="user-content-tdd-summary" aria-label="Permalink: TDD Summary" href="#tdd-summary"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">Here is my attempt to summarize our thoughts on Test-Driven Development:</p>
<ul dir="auto">
<li>
<p dir="auto">We agree that unit tests are an essential element in software development.
They allow developers to make significant changes to a system without fear
of breaking something.</p>
</li>
<li>
<p dir="auto">We agree that it is possible use TDD to produce systems with good designs.</p>
</li>
<li>
<p dir="auto">I believe that TDD discourages good design and can easily lead to very bad
code. You do not believe that TDD discourages good
design and don't see much of a risk of bad code.</p>
</li>
<li>
<p dir="auto">I believe that there are better approaches than TDD for producing good
unit test suites, such as the "bundling" approach discussed above. You agree
that bundling can produce outcomes just as good as TDD but think it may lead to
somewhat less test coverage.</p>
</li>
<li>
<p dir="auto">I believe that TDD and bundling have similar best-case outcomes, but that
the average and worst-case outcomes will be much worse for TDD. You disagree
and believe that, if anything, TDD may produce marginally better outcomes
than bundling. You also think that preference and personality are larger factors in
making the choice between the two.</p>
</li>
</ul>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">This is a fair summary of our discussion.  We seem to disagree over the best application
of discipline.  I prefer a disciplined approach to keep the code covered by tests
written first in very short cycles.  You prefer a disciplined approach of writing relatively longer
bundles of code and then writing tests for those bundles.  We disagree on the risks and rewards of
these two disciplines.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Closing Remarks</h2><a id="user-content-closing-remarks" aria-label="Permalink: Closing Remarks" href="#closing-remarks"></a></p>
<p dir="auto"><strong>JOHN:</strong></p>
<p dir="auto">First, I'd like to thank you for tolerating (and responding to) the arguments
I have made about some of the key ideas in <em>Clean Code</em>. I hope this
discussion will provide food for thought for readers.</p>
<p dir="auto">We have covered a lot of topics and subtopics in this discussion, but
I think that most of my concerns result from two general errors made
by <em>Clean Code</em>: failure to focus on what is important, and failure to
balance design tradeoffs.</p>
<p dir="auto">In software design (and probably in any design environment) it is essential
to identify the things that really matter and focus on those. If you
focus your attention on things that are unimportant you are
unlikely to achieve the things that really are important.
Unfortunately, <em>Clean Code</em> repeatedly focuses on things that don't really
matter, such as:</p>
<ul dir="auto">
<li>Dividing ten-line methods into five-line methods and dividing five-line methods
into two- or three-line methods.</li>
<li>Eliminating the use of comments written in English.</li>
<li>Writing tests before code and making the basic unit of development a
test rather than an abstraction.</li>
</ul>
<p dir="auto">None of these provides significant value, and we have seen how they
distract from producing the best possible designs.</p>
<p dir="auto">Conversely, <em>Clean Code</em> fundamentally undervalues comments, which are
essential and irreplaceable. This
comes at a huge cost. Without interface comments the specifications for
interfaces are incomplete. This is guaranteed to result in confusion and bugs.
Without implementation comments, readers are forced to rederive knowledge
and intentions that were in the mind of the original developer. This wastes
time and leads to more bugs.</p>
<p dir="auto">In my opening remarks I said that systems become complex when important
information is not accessible and obvious to developers. By refusing to
write comments, you are hiding important information that you have and
that others need.</p>
<p dir="auto">The second general error in <em>Clean Code</em> has to do with balance. Design
represents a balance between competing concerns. Almost any design idea
becomes a bad thing if taken to the extreme. However, <em>Clean Code</em>
repeatedly gives very strong advice in one direction without correspondingly
strong advice in the other direction or any meaningful guidance about how
to recognize when you have gone too far. For example, making methods
shorter is often a good thing, but the <em>Clean Code</em> position is so one-sided
and extreme that readers are likely to chop things up too much. We saw
in the <code>PrimeGenerator</code> example how this resulted in code that was
nearly incomprehensible. Similarly, the <em>Clean Code</em> position on TDD is
one-sided, failing to
recognize any possible weakness and encouraging readers to take this to
a tactical extreme where design is completely squeezed out of the development
process.</p>
<p dir="auto"><strong>UB:</strong></p>
<p dir="auto">John, I'd like to thank you for participating in this project.  This was a lot of fun for me.  I love disagreement and debate with smart people.  I also think that we share far more values than separate us.</p>
<p dir="auto">For my part I'll just say that I have given due consideration to the points you've made, and while I disagree with your conclusions above, I have integrated several of your better ideas, as well as this entire document, into the second edition of <em>Clean Code</em>.</p>
<p dir="auto">Thanks again, and give my best to your students.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["The closer to the train station, the worse the kebab" – a "study" (237 pts)]]></title>
            <link>https://www.jmspae.se/write-ups/kebabs-train-stations/</link>
            <guid>43165112</guid>
            <pubDate>Mon, 24 Feb 2025 21:25:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jmspae.se/write-ups/kebabs-train-stations/">https://www.jmspae.se/write-ups/kebabs-train-stations/</a>, See on <a href="https://news.ycombinator.com/item?id=43165112">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				

<p><strong>2025-02-14</strong></p>

    <ul>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#introduction">Introduction</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#method">Method</a>
            
                <ul>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#network-data">Network Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#restaurant-data">Restaurant Data</a>
                        </li>
                    
                        <li>
                            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#routing-and-distance">Routing and Distance</a>
                        </li>
                    
                </ul>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#results">Results</a>
            
        </li>
    
        <li>
            <a href="https://jmspae.se/write-ups/kebabs-train-stations/#discussion">Discussion</a>
            
        </li>
    
    </ul>

<p><em>This write-up was originally posted <a href="https://www.reddit.com/r/gis/comments/1iph0yy/the_closer_to_the_railway_station_the_less_tasty/">on reddit</a>, though I've cleaned things up specifically for this post. Due to reasons discussed towards the end of this post, I'm not entirely happy with the results and intend to take another shot at it in the near future.</em></p>
<h2 id="introduction">Introduction<a href="#introduction" aria-label="Anchor link for: introduction">🔗</a></h2>
<p>I came across <a href="https://www.reddit.com/r/gis/comments/1iopp56/anyone_motivated_to_prove_that_the_closer_from/">this post</a> sharing a hypothesis from a French subreddit;</p>
<blockquote>
<p>The closer to the train station, the worse the kebab.</p>
</blockquote>
<p>The original French post gained a decent amount of traction compared to the subreddit's relatively small size, indicating a certain amount of agreement among its members. There were some detractors in the comments, however, sharing experiences which ran contrary to the stated hypothesis.</p>
<p>Thus, I figured I had nothing better to do, being a burned-out, unemployed drop-out with a newly-obtained autism diagnosis, so I figured I'd sacrifice my time for a worthy cause and perform this informal <em>"study"</em>. I'll be expecting my Nobel peace prize in the postbox and several job offers in my DMs within the next 3 working days.</p>
<h2 id="method">Method<a href="#method" aria-label="Anchor link for: method">🔗</a></h2>
<p>I assumed the best study area to be Paris, France since;</p>
<ol>
<li>The original post was French</li>
</ol>
<p>I haven't personally heard of this hypothesis in my home country (Sweden, also home to many a kebab-serving restaurant) so I figured I'd assume this to be a French phenomenon for the purpose of this informal "Study".</p>
<ol start="2">
<li>Density</li>
</ol>
<p>The inner city is <em><strong>dense</strong></em> with dozens of train/metro stations and god knows how many kebab shops. I knew early on that this would make my life pretty miserable, but at least it'd provide plenty of sample data.</p>
<h2 id="network-data">Network Data<a href="#network-data" aria-label="Anchor link for: network-data">🔗</a></h2>
<p>I used OSMnx to download and save a navigation network. Given the public transit-centric nature of the French subreddit, I though it'd make sense to stick to walking distance (eg. footpaths, side-walks) thus i set the OSMnx <code>network_type</code> to <code>"walk"</code>. Given the location (and that OSMnx used this CRS automatically when none was provided), all data was projected to EPSG:32631 (UTM zone 31N).</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>osmnx </span><span>as </span><span>ox
</span><span>from </span><span>geopandas </span><span>import </span><span>GeoDataFrame
</span><span>
</span><span>#EPSG
</span><span>PROJECTION </span><span>= </span><span>32631
</span><span>
</span><span>graph </span><span>= </span><span>ox.</span><span>graph_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>network_type</span><span>=</span><span>"walk"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>ox.</span><span>save_graphml</span><span>(graph, </span><span>filepath</span><span>=</span><span>"network.graphml"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-1.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-1.c53f23b9b1fd351c.jpg"></a>

<em>Figure 1: The study area and network</em></p>
<p>Next up is the various train/metro stations. Given the nature of the original French sub, I figured it'd make sense to include both the long-distance central stations along with the countless metro stations. This was also rather trivial with OSMnx, filtering by <code>railway=subway_entrance</code> or <code>railway=train_station_entrance.</code></p>
<pre data-lang="py"><code data-lang="py"><span>stations: GeoDataFrame </span><span>= </span><span>ox.</span><span>features_from_place</span><span>(</span><span>'Paris, FR'</span><span>, </span><span>tags </span><span>= </span><span>{
</span><span>    </span><span>"railway"</span><span>: [</span><span>"subway_entrance"</span><span>, </span><span>"train_station_entrance"</span><span>]
</span><span>})
</span><span>
</span><span># Filter results to points
</span><span>station_nodes: GeoDataFrame </span><span>= </span><span>stations.loc[stations.geom_type</span><span>==</span><span>"Point"</span><span>]
</span><span>station_nodes </span><span>= </span><span>station_nodes.</span><span>to_crs</span><span>(</span><span>epsg</span><span>=</span><span>PROJECTION</span><span>)
</span><span>
</span><span>station_nodes.</span><span>to_file</span><span>(</span><span>"train_station_entrances.gpkg"</span><span>)
</span></code></pre>
<p>I saved outputs religiously so I could easily inspect them in QGIS. I did attempt to get python notebooks working with my NeoVIM setup, but it was all for naught.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-2.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-2.51f0e47dcd63c0a8.jpg"></a>

<em>Figure 2: Rail/metro entrances... Please ignore the airport iconography.</em></p>
<p>... And there we have the first half of the data, now for the restaurants.</p>
<h2 id="restaurant-data">Restaurant Data<a href="#restaurant-data" aria-label="Anchor link for: restaurant-data">🔗</a></h2>
<p>The Google Places API (and their respective reviews) seemed like a reasonable choice. Google reviews are naturally far from perfect and subject to their fair share of botting and the like, but it's the best I could think of at the time. There are alternatives such as Yelp, but their API is horrifically expensive for poor old me, and I was not in the mood to build a web scraper (it has the same soul-sucking effect on me as prompting an LLM). The $200 of free credit was also enticing.</p>
<p>However, as I started exploring the API... I realised that the Places API doesn't seem to have any way to search within a polygon, only within a point radius. Thank you, Mr. publicly owned mega-corporation. How Fun.</p>
<p>It also didn't help that autocomplete for the <code>googlemaps</code> library wasn't working. Python's a fine language, but its tooling does like to test my patience a little too often. And whilst I'm still complaining... The Google Cloud dashboard is likely the slowest "website" I've ever had the displeasure of interacting with.</p>
<p>So... This meant I'd have to perform some sort of grid search of the whole of Paris, crossing my fingers that I wouldn't bust my free usage. This, along with a couple interesting questions;</p>
<ol>
<li>What is... <em>A kebab?</em></li>
</ol>
<p>When I search for "kebab" (no further context necessary)... How does Google decide what restaurant serves kebab?</p>
<p>After some perusing, it didn't seem to be as deep as I thought. Plenty of restaurants simply had "kebab" in the name, some were designated as "Mediterranean" (Kebab has its origins in Turkey, Persia, middle east in general) and others had a fair few reviews simply mentioning "kebab." Good enough for me.</p>
<ol start="2">
<li>Trouble in query-land</li>
</ol>
<p>It turns out that when you query for places within a given radius, it's only a "bias." It's not a hard cut-off that'll help narrow-down our data harvesting and reduce unnecessary requests. It was becoming increasingly clear that google isn't really a fan of people doing this.</p>
<p>Now with all of that preamble out of the way, I needed to prepare my search.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-3.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-3.605c84ffddad8323.jpg"></a>

<em>Figure 3. Original admin boundaries</em></p>
<p>Paris' administrative boundary contains a couple of large green spaces. To the west, a park and to the east, some sort of sports institute.</p>
<p>After perusing these rather large spaces in Google maps, they seemed to contain a distinct lack of kebab-serving establishments. Thus, they were a burden on our API budget and needed to go.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-4.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-4.b360881962350eea.jpg"></a>

<em>Figure 4. Adjusted admin boundaries w/ network</em></p>
<p>I figured keeping the network and stations wouldn't do any harm, so they went unmodified.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-5.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-5.eedb86cb4c85e12c.jpg"></a>

<em>Figure 5. Sampling points, later projected to WGS84 for harvesting purposes</em></p>
<p>To maximise data-harvesting, I decided to go with a hex layout with a vertical spacing of 1 km. This should give us a search radius of 500m * √3 ~= 866 meters. Plenty of overlap, sure, but we shouldn't be getting any holes anywhere. I'm not sure why I was spending this much time ensuring "data integrity" when that might just have flown the window courtesy of Google, but it's the illusion of control that counts.</p>
<p>This give us 99 sample points which... Should be enough?</p>
<p>Regardless, here's how my 3AM python turned out:</p>
<pre data-lang="py"><code data-lang="py"><span># Already projected to WGS84
</span><span>sample_points: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"samples.gpkg"</span><span>)
</span><span>gmaps: googlemaps.Client </span><span>= </span><span>googlemaps.</span><span>Client</span><span>(</span><span>key</span><span>=</span><span>'get-your-own'</span><span>)
</span><span>
</span><span>output </span><span>= </span><span>{}
</span><span>
</span><span>for </span><span>point </span><span>in </span><span>sample_points.geometry:
</span><span>    lat, lon </span><span>= </span><span>point.y, point.x
</span><span>
</span><span>    next_page_token </span><span>= </span><span>None
</span><span>    num_fetches </span><span>= </span><span>3
</span><span>
</span><span>    </span><span>while </span><span>num_fetches </span><span>&gt; </span><span>0</span><span>:
</span><span>        result </span><span>= </span><span>{}
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>"kebab"</span><span>,
</span><span>                </span><span>location</span><span>=</span><span>(lat, lon),
</span><span>                </span><span>radius</span><span>=</span><span>866</span><span>,
</span><span>            )
</span><span>        </span><span>else</span><span>:
</span><span>            result </span><span>= </span><span>gmaps.</span><span>places</span><span>(
</span><span>                </span><span>page_token</span><span>=</span><span>next_page_token
</span><span>            )
</span><span>
</span><span>        next_page_token </span><span>= </span><span>result.</span><span>get</span><span>(</span><span>"next_page_token"</span><span>)
</span><span>        </span><span>print</span><span>(result[</span><span>"status"</span><span>], next_page_token)
</span><span>
</span><span>        </span><span>for </span><span>p </span><span>in </span><span>result[</span><span>"results"</span><span>]:
</span><span>            output[p[</span><span>"place_id"</span><span>]] </span><span>= </span><span>p
</span><span>
</span><span>        </span><span>if </span><span>next_page_token </span><span>== </span><span>None</span><span>:
</span><span>            </span><span>break
</span><span>
</span><span>        num_fetches </span><span>-= </span><span>1
</span><span>
</span><span>        </span><span>sleep</span><span>(</span><span>2</span><span>)
</span><span>
</span><span>json_out </span><span>= </span><span>json.</span><span>dumps</span><span>(output)
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"w"</span><span>) </span><span>as </span><span>file:
</span><span>    file.</span><span>write</span><span>(json_out)
</span></code></pre>
<p>This worked quite well. Initially I skipped paging, resulting in 322 results. However, I noticed that a few establishments were missing in the results compared to my explorations in Google Maps.</p>
<p>After implementing paging and re-running, this gave us a grand total of 400 kebab-serving establishments. I was likely over-zealous with the paging considering how few additional results were retrieved. That, and that the API doesn't cap the search radius (again, it's only a bias) likely led to a fair few redundant API calls.</p>
<p>The raw Google Places API-output also needed to be clipped to the study area, projected to the local UTM zone as well as converted to a geospatial format;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>pandas </span><span>as </span><span>pd
</span><span>
</span><span>with </span><span>open</span><span>(</span><span>"output.json"</span><span>, </span><span>"r"</span><span>) </span><span>as </span><span>file:
</span><span>    data </span><span>= </span><span>json.</span><span>load</span><span>(file)
</span><span>    file.</span><span>close</span><span>()
</span><span>    
</span><span>    </span><span>for </span><span>id </span><span>in </span><span>data:
</span><span>        place </span><span>= </span><span>data[</span><span>id</span><span>]
</span><span>        point </span><span>= </span><span>place[</span><span>"geometry"</span><span>][</span><span>"location"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lng"</span><span>] </span><span>= </span><span>point[</span><span>"lng"</span><span>]
</span><span>        data[</span><span>id</span><span>][</span><span>"lat"</span><span>] </span><span>= </span><span>point[</span><span>"lat"</span><span>]
</span><span>        </span><span>del </span><span>data[</span><span>id</span><span>][</span><span>"geometry"</span><span>]
</span><span>
</span><span>    data </span><span>= </span><span>pd.DataFrame.</span><span>from_dict</span><span>(data).T
</span><span>    data.rating </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.rating)
</span><span>    data.user_ratings_total </span><span>= </span><span>pd.</span><span>to_numeric</span><span>(data.user_ratings_total)
</span><span>    data </span><span>= </span><span>data[data[</span><span>"user_ratings_total"</span><span>] </span><span>&gt; </span><span>0</span><span>]
</span><span>
</span><span>    </span><span># Cleanup was added after the screenshot below was taken
</span><span>    data </span><span>= </span><span>data.</span><span>drop</span><span>(</span><span>columns</span><span>=</span><span>[
</span><span>        </span><span>"icon"</span><span>,
</span><span>        </span><span>"icon_background_color"</span><span>,
</span><span>        </span><span>"icon_mask_base_uri"</span><span>,
</span><span>        </span><span>"plus_code"</span><span>,
</span><span>        </span><span>"reference"</span><span>,
</span><span>        </span><span>"photos"</span><span>,
</span><span>        </span><span>"opening_hours"
</span><span>    ])
</span><span>
</span><span>    gdata </span><span>= </span><span>GeoDataFrame</span><span>(
</span><span>        data, </span><span>geometry</span><span>=</span><span>geopandas.</span><span>points_from_xy</span><span>(data.lng, data.lat),
</span><span>        </span><span>crs</span><span>=</span><span>4326
</span><span>    )
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>to_crs</span><span>(</span><span>PROJECTION</span><span>)
</span><span>
</span><span>    </span><span># Modified boundaries from Figure 4.
</span><span>    paris </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"mod_bounary.gpkg"</span><span>);
</span><span>
</span><span>    gdata: GeoDataFrame </span><span>= </span><span>gdata.</span><span>clip</span><span>(paris)
</span><span>
</span><span>    gdata.</span><span>to_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span></code></pre>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-6.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-6.1eab106a30493ee2.jpg"></a>

<em>Figure 6. We're in f###ing business</em></p>
<h2 id="routing-and-distance">Routing and Distance<a href="#routing-and-distance" aria-label="Anchor link for: routing-and-distance">🔗</a></h2>
<p>Finally, the fun part. I need to get the distance to the nearest station entrance for each establishment.</p>
<p>I could've absolutely just routed to every single entrance for every single restaurant to get the nearest... But that would've taken several decades. I needed to build some sort of spatial index and route to the nearest ~3 or something along those lines. Since Paris is so dense with plenty of routing options, I figured I wouldn't need to perform too many routing operations.</p>
<p>After some googling and dredging through API docs, however, it seemed GeoPandas was nice enough to do that for us with <code>sindex</code>. Although it didn't have the same "return nearest N" like my beloved r-tree rust library I was all too used to, it did allow me to search within a certain radius (1 km was large enough) and go from there. The query results weren't sorted, so I had to sort the indexes by distance and cut it down to size.</p>
<p>The network analysis was relatively straight-forward thanks to NetworkX, and after a couple of hours I managed to cobble together the following;</p>
<pre data-lang="py"><code data-lang="py"><span>import </span><span>networkx </span><span>as </span><span>nx
</span><span>import </span><span>shapely </span><span>as </span><span>shp
</span><span>
</span><span>establishments: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"establishments.gpkg"</span><span>)
</span><span>entrances: GeoDataFrame </span><span>= </span><span>GeoDataFrame.</span><span>from_file</span><span>(</span><span>"entrances.gpkg"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>load_graphml</span><span>(</span><span>"network.graphml"</span><span>)
</span><span>graph </span><span>= </span><span>ox.</span><span>project_graph</span><span>(graph, </span><span>to_crs </span><span>= </span><span>PROJECTION</span><span>)
</span><span>
</span><span># Ensure the same CRS
</span><span>if </span><span>(establishments.crs </span><span>!= </span><span>entrances.crs </span><span>!= </span><span>PROJECTION</span><span>):
</span><span>    </span><span>exit</span><span>(</span><span>100</span><span>)
</span><span>
</span><span># Helper function to get the distance between a graph node and establishment geometry
</span><span>def </span><span>node_geom_dist</span><span>(</span><span>node_id</span><span>: int, </span><span>geom</span><span>: shp.Point):
</span><span>    node </span><span>= </span><span>graph.nodes[node_id]
</span><span>    </span><span>return </span><span>math.</span><span>sqrt</span><span>((geom.x </span><span>- </span><span>node[</span><span>'x'</span><span>]) </span><span>** </span><span>2 </span><span>+ </span><span>(geom.y </span><span>- </span><span>node[</span><span>'y'</span><span>]) </span><span>** </span><span>2</span><span>)
</span><span>
</span><span>distances: list[float] </span><span>= </span><span>[]
</span><span>
</span><span>for </span><span>(</span><span>id</span><span>, establishment) </span><span>in </span><span>establishments.</span><span>iterrows</span><span>():
</span><span>    establishment_geom: shp.Point </span><span>= </span><span>establishment.geometry
</span><span>    establishment_node: int </span><span>= </span><span>ox.</span><span>nearest_nodes</span><span>(graph, establishment_geom.x, establishment_geom.y)
</span><span>    establishment_dist_to_node: float </span><span>= </span><span>node_geom_dist</span><span>(establishment_node, establishment_geom)
</span><span>    
</span><span>    </span><span># Spatial index for rail entrances
</span><span>    index: shp.STRtree </span><span>= </span><span>entrances.sindex
</span><span>    nearest_q </span><span>= </span><span>index.</span><span>query</span><span>(establishment_geom, </span><span>predicate</span><span>=</span><span>"dwithin"</span><span>, </span><span>distance </span><span>= </span><span>1000</span><span>)
</span><span>    nearest_entrances: list[tuple[int, float]] </span><span>= </span><span>[]
</span><span>
</span><span>    </span><span>for </span><span>i </span><span>in </span><span>nearest_q:
</span><span>        ent </span><span>= </span><span>entrances.iloc[i]
</span><span>        ent_geom: shp.Point </span><span>= </span><span>ent.geometry
</span><span>
</span><span>        dist </span><span>= </span><span>ent_geom.</span><span>distance</span><span>(establishment.geometry)
</span><span>        
</span><span>        nearest_entrances.</span><span>append</span><span>((i, dist))
</span><span>     
</span><span>    nearest_entrances </span><span>= </span><span>sorted</span><span>(nearest_entrances, </span><span>key </span><span>= lambda </span><span>e</span><span>: e[</span><span>1</span><span>])[:</span><span>3</span><span>]
</span><span>    entrance_geom: list[shp.Point] </span><span>= </span><span>[entrances.iloc[i].geometry </span><span>for </span><span>(i, </span><span>_</span><span>) </span><span>in </span><span>nearest_entrances]
</span><span>    entrance_nodes: list[int] </span><span>= </span><span>[ox.</span><span>nearest_nodes</span><span>(graph, point.x, point.y) </span><span>for </span><span>point </span><span>in </span><span>entrance_geom]
</span><span>    entrance_geom_dist_to_node: list[float] </span><span>= </span><span>[</span><span>node_geom_dist</span><span>(entrance_nodes[i], entrance_geom[i]) </span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>len</span><span>(nearest_entrances))]
</span><span>
</span><span>    result_paths </span><span>= </span><span>[nx.</span><span>shortest_path</span><span>(graph, establishment_node, dest_node, </span><span>weight</span><span>=</span><span>"length"</span><span>) </span><span>for </span><span>dest_node </span><span>in </span><span>entrance_nodes]
</span><span>    result_lengths: list[float] </span><span>= </span><span>[nx.</span><span>path_weight</span><span>(graph, path, </span><span>"length"</span><span>) </span><span>+ </span><span>entrance_geom_dist_to_node[i] </span><span>+ </span><span>establishment_dist_to_node </span><span>for </span><span>(i, path) </span><span>in </span><span>enumerate</span><span>(result_paths)]
</span><span>
</span><span>    distances.</span><span>append</span><span>(</span><span>min</span><span>(result_lengths))
</span><span>
</span><span>establishments[</span><span>"distance"</span><span>] </span><span>= </span><span>distances 
</span><span>establishments.</span><span>to_file</span><span>(</span><span>"establishment_results.gpkg"</span><span>)
</span></code></pre>
<p>Not exactly my finest work. The sheer amount of list comprehension is perhaps a little terrifying, but it works.</p>
<p>After some prodding around in QGIS with the resulting data and networks (and many print() statements), I was confident in the accuracy of the results.</p>
<h2 id="results">Results<a href="#results" aria-label="Anchor link for: results">🔗</a></h2>
<p>Now with all of this data, it is time to settle the question of whether or not the kebabs are less tasty the closer they are to a train/metro station...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-7.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-7.1ff7f2b316a90f62.jpg"></a>

<em>Figure 7. Hmmmmm....</em></p>
<p>With a mighty Pearson's correlation of 0.091, the data indicates that this could be true! If you ignore the fact that the correlation is so weak that calling it 'statistically insignificant' would be quite generous.</p>
<p>Outliers can have an outsized impact on a Pearson's correlation, so after ridding the dataset of some outliers via IQR fencing...</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-8.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-8.af2b5c5cacb7b32d.jpg"></a>

<em>Figure 8. Removed outliers</em></p>
<p>... This increased the coefficient to a whopping 0.098.</p>
<p>This was a bit of a bummer (though hardly surprising) and figuring I had nothing to lose from messing around a little, I tried filtering out metro stations in case my original assumption of the metro being included in the original hypothesis was incorrect.</p>
<p>
<a href="https://jmspae.se/write-ups/kebabs-train-stations/fig-9.webp" target="_blank"><img src="https://jmspae.se/processed_images/fig-9.3427bdea5242e0b1.jpg"></a>

*Figure 9. Not much better, eh? Correction: "... Nearest train station entrance."</p>
<p>With an even worse coefficient of 0.001, I think It's time to hang up the towel.</p>
<p>Whilst there are some minor indications that the hypothesis <em>could</em> be correct (eg. Many of the absolute worst restaurants being some of the closest) the correlation is simply too weak.</p>
<h2 id="discussion">Discussion<a href="#discussion" aria-label="Anchor link for: discussion">🔗</a></h2>
<p><em><strong>- Are Google reviews an objective measurement of how tasty the kebabs are?</strong></em></p>
<p>Absolutely the f### not. This was a rather subjective observation from the very beginning and Google reviews aren't exactly a good measure of "is the food good?" There are many aspects of the dining experience that could hypothetically impact a review score. The staff, cleanliness, the surrounding environment, etc. Not to mention online skulduggery and review manipulation.</p>
<p><em><strong>- Can tourism have an impact?</strong></em></p>
<p>It absolutely could. I don't want to make any definitive assumptions, but I can absolutely imagine the local regulars being harsher than the massive tourist population, or even vice-versa.</p>
<p><em><strong>- Were the Google results accurate?</strong></em></p>
<p>To an extent, yes. From what I could gather, every location from the query seemed to serve kebab in some form. There were a few weird outliers and nuances, such as Pizza Hut which likely only serves kebab pizza rather than the multitude of different forms in which kebab could possibly be consumed.</p>
<p><em><strong>- Why not restaurants in general?</strong></em></p>
<p>Because the initial hypothesis was too comically hyper-specific for me to give up on.</p>
<p><em><strong>- What about review count?</strong></em></p>
<p>This could very well have an effect, though I was not entirely certain how to properly implement this metric into the analysis at the time.</p>
<p><em><strong>- Gib Data</strong></em></p>
<p>I'm not quite comfortable in doing so, mostly due to potential breaches of Google's TOS. I don't think they would care about me harvesting some 400 POIs for this little experiment, I'm not quite willing to gamble sharing the data with others.</p>
<p>Besides, I gave you the code. Go burn some of your own credits.</p>
<p><em><strong>- Are you Ok?</strong></em></p>
<p>... I guess? Are you?</p>
<p>In conclusion, this was actually quite fun. I wrote this as the project went on (otherwise I would likely never have found the motivation) and I would encourage others to do other silly explorations like this, even if the results end up slightly depressing.</p>
<p>... <em>However</em>, after some additional discussion, I decided I wasn't quite done.</p>
<p>As stated earlier, there were a few detracting comments on the original French post. Interestingly, many of the provided examples of good kebab restaurants next to train stations just so happened to be in Paris.</p>
<p>The user who originally posted the French post for the sub in English provided some <a href="https://imgur.com/gallery/kebab-railway-stations-wuYG9D2">examples</a> which seem to strengthen the hypothesis. It could very well be that whatever conditions affect Paris restaurants (whether it be higher rent, wages, tourism, population density...) had a larger impact than I initially suspected.</p>
<p><em><strong>Stay tuned for part 2... Whenever I get around to doing it!</strong></em></p>


			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Claude 3.7 Sonnet and Claude Code (1461 pts)]]></title>
            <link>https://www.anthropic.com/news/claude-3-7-sonnet</link>
            <guid>43163011</guid>
            <pubDate>Mon, 24 Feb 2025 18:28:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.anthropic.com/news/claude-3-7-sonnet">https://www.anthropic.com/news/claude-3-7-sonnet</a>, See on <a href="https://news.ycombinator.com/item?id=43163011">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today, we’re announcing Claude 3.7 Sonnet<sup>1</sup>, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made <a href="https://youtu.be/t3nnDXa81Hs">visible to the user</a>. API users also have fine-grained control over <em>how long</em> the model can think for.</p><p>Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. Along with the model, we’re also introducing a command line tool for agentic coding, Claude Code. Claude Code is available as a limited research preview, and enables developers to delegate substantial engineering tasks to Claude directly from their terminal.</p><div><figure><img alt="Screen showing Claude Code onboarding" loading="eager" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F4a4df6b6629f9814aec4eb9323028130f43a8d70-1920x1080.png&amp;w=3840&amp;q=75"></figure></div><p>Claude 3.7 Sonnet is now available on all <a href="https://claude.ai/new">Claude</a> plans—including Free, Pro, Team, and Enterprise—as well as the <a href="https://docs.anthropic.com/en/docs/about-claude/models">Anthropic API</a>, <a href="https://aws.amazon.com/bedrock/claude/">Amazon Bedrock</a>, and <a href="https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude">Google Cloud’s Vertex AI</a>. Extended thinking mode is available on all surfaces except the free Claude tier.</p><p>In both standard and extended thinking modes, Claude 3.7 Sonnet has the same price as its predecessors: $3 per million input tokens and $15 per million output tokens—which includes thinking tokens.</p><h2 id="claude-37-sonnet-frontier-reasoning-made-practical">Claude 3.7 Sonnet: Frontier reasoning made practical</h2><p>We’ve developed Claude 3.7 Sonnet with a different philosophy from other reasoning models on the market. Just as humans use a single brain for both quick responses and deep reflection, we believe reasoning should be an integrated capability of frontier models rather than a separate model entirely. This unified approach also creates a more seamless experience for users.</p><p>Claude 3.7 Sonnet embodies this philosophy in several ways. First, Claude 3.7 Sonnet is both an ordinary LLM and a reasoning model in one: you can pick when you want the model to answer normally and when you want it to <a href="https://www.anthropic.com/research/visible-extended-thinking">think longer before answering</a>. In the standard mode, Claude 3.7 Sonnet represents an upgraded version of Claude 3.5 Sonnet. In <a href="https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking">extended thinking mode</a>, it self-reflects before answering, which improves its performance on math, physics, instruction-following, coding, and many other tasks. We generally find that prompting for the model works similarly in both modes.</p><p>Second, when using Claude 3.7 Sonnet through the API, users can also control the <em>budget </em>for thinking: you can tell Claude to think for no more than N tokens, for any value of N up to its output limit of 128K tokens. This allows you to trade off speed (and cost) for quality of answer.</p><p>Third, in developing our reasoning models, we’ve optimized somewhat less for math and computer science competition problems, and instead shifted focus towards real-world tasks that better reflect how businesses actually use LLMs.</p><p><a href="https://www.anthropic.com/claude/sonnet">Early testing</a> demonstrated Claude’s leadership in coding capabilities across the board: Cursor noted Claude is once again best-in-class for real-world coding tasks, with significant improvements in areas ranging from handling complex codebases to advanced tool use. Cognition found it far better than any other model at planning code changes and handling full-stack updates. Vercel highlighted Claude’s exceptional precision for complex agent workflows, while Replit has successfully deployed Claude to build sophisticated web apps and dashboards from scratch, where other models stall. In Canva’s evaluations, Claude consistently produced production-ready code with superior design taste and drastically reduced errors.</p><div><figure><img alt="Bar chart showing Claude 3.7 Sonnet as state-of-the-art for SWE-bench Verified" loading="lazy" width="1920" height="1145" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F08bba4487fb5ac1ba52540ee656d7e4da10ca1be-1920x1145.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet achieves state-of-the-art performance on SWE-bench Verified, which evaluates AI models’ ability to solve real-world software issues. See the appendix for more information on scaffolding.</figcaption></figure></div><div><figure><img alt="Bar chart showing Claude 3.7 Sonnet as state-of-the-art for TAU-bench" loading="lazy" width="1920" height="1114" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F787e59d548c230afd7efaed1bda1fb7f7ca207b8-1920x1114.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet achieves state-of-the-art performance on TAU-bench, a framework that tests AI agents on complex real-world tasks with user and tool interactions. See the appendix for more information on scaffolding.</figcaption></figure></div><div><figure><img alt="Benchmark table comparing frontier reasoning models" loading="lazy" width="2600" height="2360" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png&amp;w=3840&amp;q=75 1x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F654cf6680d32858dfba9af644f8c4a5b04425af1-2600x2360.png&amp;w=3840&amp;q=75"><figcaption>Claude 3.7 Sonnet excels across instruction-following, general reasoning, multimodal capabilities, and agentic coding, with extended thinking providing a notable boost in math and science. Beyond traditional benchmarks, it even outperformed all previous models in our <a href="https://www.anthropic.com/research/visible-extended-thinking">Pokémon gameplay tests</a>.</figcaption></figure></div><h2 id="claude-code">Claude Code</h2><p>Since June 2024, Sonnet has been the preferred model for developers worldwide. Today, we're empowering developers further by introducing Claude Code—our first agentic coding tool—in a limited research preview.</p><p>Claude Code is an active collaborator that can search and read code, edit files, write and run tests, commit and push code to GitHub, and use command line tools—keeping you in the loop at every step.</p><p>Claude Code is an early product but has already become indispensable for our team, especially for test-driven development, debugging complex issues, and large-scale refactoring. In early testing, Claude Code completed tasks in a single pass that would normally take 45+ minutes of manual work, reducing development time and overhead.</p><p>In the coming weeks, we plan to continually improve it based on our usage: enhancing tool call reliability, adding support for long-running commands, improved in-app rendering, and expanding Claude's own understanding of its capabilities.</p><p>Our goal with Claude Code is to better understand how developers use Claude for coding to inform future model improvements. By <a href="https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/overview">joining this preview</a>, you’ll get access to the same powerful tools we use to build and improve Claude, and your feedback will directly shape its future.</p><h2 id="working-with-claude-on-your-codebase">Working with Claude on your codebase</h2><p>We’ve also improved the coding experience on Claude.ai. Our GitHub integration is now available on all Claude plans—enabling developers to connect their code repositories directly to Claude.</p><p>Claude 3.7 Sonnet is our best coding model to date. With a deeper understanding of your personal, work, and open source projects, it becomes a more powerful partner for fixing bugs, developing features, and building documentation across your most important GitHub projects.</p><h2 id="building-responsibly">Building responsibly</h2><p>We’ve conducted extensive testing and evaluation of Claude 3.7 Sonnet, working with external experts to ensure it meets our standards for security, safety, and reliability. Claude 3.7 Sonnet also makes more nuanced distinctions between harmful and benign requests, reducing <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">unnecessary refusals by 45%</a> compared to its predecessor.</p><p>The <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">system card</a> for this release covers new safety results in several categories, providing a detailed breakdown of our Responsible Scaling Policy evaluations that other AI labs and researchers can apply to their work. The card also addresses emerging risks that come with computer use, particularly prompt injection attacks, and explains how we evaluate these vulnerabilities and train Claude to resist and mitigate them. Additionally, it examines potential safety benefits from reasoning models: the ability to understand how models make decisions, and whether model reasoning is genuinely trustworthy and reliable. Read the full <a href="https://www.anthropic.com/claude-3-7-sonnet-system-card">system card </a>to learn more.</p><h2 id="looking-ahead">Looking ahead</h2><p>Claude 3.7 Sonnet and Claude Code mark an important step towards AI systems that can truly augment human capabilities. With their ability to reason deeply, work autonomously, and collaborate effectively, they bring us closer to a future where AI enriches and expands what <a href="https://darioamodei.com/machines-of-loving-grace">humans can achieve</a>.</p><div><figure><img alt="Milestone timeline showing Claude progressing from assistant to pioneer" loading="lazy" width="1920" height="1080" decoding="async" data-nimg="1" srcset="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=1920&amp;q=75 1x, https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=3840&amp;q=75 2x" src="https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde9831ea84e3663fe4598589d71eaa531f9912-1920x1080.png&amp;w=3840&amp;q=75"></figure></div><p>We're excited for you to explore these new capabilities and to see what you’ll create with them. As always, we welcome your <a href="mailto: feedback@anthropic.com">feedback</a> as we continue to improve and evolve our models.</p></div></article><div><h4>Appendix</h4><p><sup>1 </sup>Lesson learned on <a href="https://www.anthropic.com/news/3-5-models-and-computer-use">naming</a>.</p><h3>Eval data sources</h3><ul><li><a href="https://x.ai/blog/grok-3">Grok</a></li><li><a href="https://developers.googleblog.com/en/gemini-2-family-expands/">Gemini 2 Pro</a></li><li><a href="https://openai.com/index/openai-o3-mini/">o1 and o3-mini</a></li><li><a href="https://cdn.openai.com/o1-system-card-20241205.pdf">Supplementary o1</a></li><li><a href="https://web.archive.org/web/20250203044057/https://openai.com/index/o1-and-new-tools-for-developers/">o1 TAU-bench</a></li><li><a href="https://cdn.openai.com/o3-mini-system-card-feb10.pdf">Supplementary o3-mini</a></li><li><a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">Deepseek R1</a></li></ul><h3>TAU-bench</h3><p><strong>Information about the scaffolding</strong></p><p>Scores were achieved with a prompt addendum to the Airline Agent Policy instructing Claude to better utilize a “planning” tool, where the model is encouraged to write down its thoughts as it solves the problem distinct from our usual thinking mode, during the multi-turn trajectories to best leverage its reasoning abilities. To accommodate the additional steps Claude incurs by utilizing more thinking, the maximum number of steps (counted by model completions) was increased from 30 to 100 (most trajectories completed under 30 steps with only one trajectory reaching above 50 steps).</p><p>Additionally, the TAU-bench score for Claude 3.5 Sonnet (new) differs from what we originally reported on release because of small dataset improvements introduced since then. We re-ran on the updated dataset for more accurate comparison with Claude 3.7 Sonnet.</p><h3>SWE-bench Verified</h3><p><strong>Information about the scaffolding</strong></p><p>There are many approaches to solving open ended agentic tasks like SWE-bench. Some approaches offload much of the complexity of deciding which files to investigate or edit and which tests to run to more traditional software, leaving the core language model to generate code in predefined places, or select from a more limited set of actions. Agentless (<a href="https://arxiv.org/abs/2407.01489">Xia et al., 2024</a>) is a popular framework used in the evaluation of Deepseek’s R1 and other models which augments an agent with prompt- and embedding-based file retrieval mechanisms, patch localization, and best-of-40 rejection sampling against regression tests. Other scaffolds (e.g. <a href="https://aide.dev/blog/sota-bitter-lesson">Aide</a>) further supplement models with additional test-time compute in the form of retries, best-of-N, or Monte Carlo Tree Search (MCTS).</p><p>For Claude 3.7 Sonnet and Claude 3.5 Sonnet (new), we use a much simpler approach with minimal scaffolding, where the model decides which commands to run and files to edit in a single session. Our main “no extended thinking” pass@1 result simply equips the model with the <a href="https://www.anthropic.com/research/swe-bench-sonnet">two tools described here</a>—a bash tool, and a file editing tool that operates via string replacements—as well as the “planning tool” mentioned above in our TAU-bench results. Due to infrastructure limitations, only 489/500 problems are actually solvable on our internal infrastructure (i.e., the golden solution passes the tests). For our vanilla pass@1 score we are counting the 11 unsolvable problems as failures to maintain parity with the <a href="https://www.swebench.com/#verified">official leaderboard</a>. For transparency, we separately release the test cases that did not work on our infrastructure.</p><p>For our “high compute” number we adopt additional complexity and parallel test-time compute as follows:</p><ul><li>We sample multiple parallel attempts with the scaffold above</li><li>We discard patches that break the visible regression tests in the repository, similar to the rejection sampling approach adopted by Agentless; note no hidden test information is used.</li><li>We then rank the remaining attempts with a scoring model similar to our results on GPQA and AIME described in our <a href="https://www.anthropic.com/news/visible-extended-thinking">research post</a> and choose the best one for the submission.</li></ul><p>This results in a score of 70.3% on the subset of n=489 verified tasks which work on our infrastructure. Without this scaffold, Claude 3.7 Sonnet achieves 63.7% on SWE-bench Verified using this same subset. The excluded 11 test cases that were incompatible with our internal infrastructure are:</p><ul><li>scikit-learn__scikit-learn-14710</li><li>django__django-10097</li><li>psf__requests-2317</li><li>sphinx-doc__sphinx-10435</li><li>sphinx-doc__sphinx-7985</li><li>sphinx-doc__sphinx-8475</li><li>matplotlib__matplotlib-20488</li><li>astropy__astropy-8707</li><li>astropy__astropy-8872</li><li>sphinx-doc__sphinx-8595</li><li>sphinx-doc__sphinx-9711</li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The best way to use text embeddings portably is with Parquet and Polars (155 pts)]]></title>
            <link>https://minimaxir.com/2025/02/embeddings-parquet/</link>
            <guid>43162995</guid>
            <pubDate>Mon, 24 Feb 2025 18:27:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://minimaxir.com/2025/02/embeddings-parquet/">https://minimaxir.com/2025/02/embeddings-parquet/</a>, See on <a href="https://news.ycombinator.com/item?id=43162995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/">Text embeddings</a>, particularly modern embeddings generated from large language models, are one of the most useful applications coming from the generative AI boom. Embeddings are a list of numbers which represent an object: in the case of text embeddings, they can represent words, sentences, and full paragraphs and documents, and they do so with a surprising amount of distinctiveness.</p><p>Recently, I created text embeddings representing every distinct <a href="https://magic.wizards.com/en">Magic: the Gathering</a> card released as of the February 2025 Aetherdrift expansion: 32,254 in total. With these embeddings, I can find the mathematical similarity between cards through the encoded representation of their card design, including all mechanical attributes such as the card name, card cost, card text, and even card rarity.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/wog_hu17410071956209960561.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/wog_hu17201351259632573234.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/wog.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/wog.webp" alt="The iconic Magic card Wrath of God, along with its top four most similar cards identified using their respective embeddings. The similar cards are valid matches, with similar card text and card types."><figcaption><p>The iconic Magic card <a href="https://gatherer.wizards.com/pages/card/Details.aspx?multiverseid=129808">Wrath of God</a>, along with its top four most similar cards identified using their respective embeddings. The similar cards are valid matches, with similar card text and card types.</p></figcaption></figure><p>Additionally, I can create a fun 2D <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a> projection of all those cards, which also identifies interesting patterns:</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu1954379011677155299.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu11781744338083152452.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap_hu15618369906233811734.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap.webp 1200w" src="https://minimaxir.com/2025/02/embeddings-parquet/mtg_umap.webp" alt="The UMAP dimensionality reduction process also implicitly clusters the Magic cards to logical clusters, such as by card color(s) and card type."><figcaption><p>The UMAP dimensionality reduction process also implicitly clusters the Magic cards to logical clusters, such as by card color(s) and card type.</p></figcaption></figure><p>I generated these Magic card embeddings for <em>something special</em> besides a pretty data visualization, but if you are curious how I generated them, they were made using the new-but-underrated <a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">gte-modernbert-base</a> embedding model and the process is detailed <a href="https://github.com/minimaxir/mtg-embeddings">in this GitHub repository</a>. The embeddings themselves (including the coordinate values to reproduce the 2D UMAP visualization) are available as a <a href="https://huggingface.co/datasets/minimaxir/mtg-embeddings">Hugging Face dataset</a>.</p><p>Most tutorials involving embedding generation omit the obvious question: what do you <em>do</em> with the text embeddings after you generate them? The common solution is to use a <a href="https://en.wikipedia.org/wiki/Vector_database">vector database</a>, such as <a href="https://github.com/facebookresearch/faiss">faiss</a> or <a href="https://qdrant.tech/">qdrant</a>, or even a cloud-hosted service such as <a href="https://www.pinecone.io/">Pinecone</a>. But those aren’t easy to use: faiss has <a href="https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index">confusing configuration options</a>, qdrant requires <a href="https://github.com/qdrant/qdrant?tab=readme-ov-file#client-server">using a Docker container</a> to host the storage server, and Pinecone can get <a href="https://www.pinecone.io/pricing/">very expensive</a> very quickly, and its free Starter tier is limited.</p><p>What many don’t know about text embeddings is that you don’t <em>need</em> a vector database to calculate nearest-neighbor similarity if your data isn’t too large. Using <a href="https://numpy.org/doc/stable/index.html">numpy</a> and my Magic card embeddings, a 2D matrix of 32,254 <code>float32</code> embeddings at a dimensionality of 768D (common for “smaller” LLM embedding models) occupies <strong>94.49 MB</strong> of system memory, which is relatively low for modern personal computers and can fit within free usage tiers of cloud VMs. If both the query vector and the embeddings themselves are unit normalized (many embedding generators normalize by default), then the matrix dot product between the query and embeddings results in a cosine similarity between <code>[-1, 1]</code>, where the higher score is better/more similar. Since dot products are such a fundamental aspect of linear algebra, numpy’s implementation is extremely fast: with the help of additional numpy <a href="https://numpy.org/doc/stable/reference/generated/numpy.argpartition.html">sorting</a> <a href="https://numpy.org/doc/2.1/reference/generated/numpy.argsort.html">shenanigans</a>, on my M3 Pro MacBook Pro it takes just <strong>1.08 ms</strong> on average to calculate all 32,254 dot products, find the top 3 most similar embeddings, and return their corresponding <code>idx</code> of the matrix and and cosine similarity <code>score</code>.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>def</span> <span>fast_dot_product</span><span>(</span><span>query</span><span>,</span> <span>matrix</span><span>,</span> <span>k</span><span>=</span><span>3</span><span>):</span>
</span></span><span><span>    <span>dot_products</span> <span>=</span> <span>query</span> <span>@</span> <span>matrix</span><span>.</span><span>T</span>
</span></span><span><span>
</span></span><span><span>    <span>idx</span> <span>=</span> <span>np</span><span>.</span><span>argpartition</span><span>(</span><span>dot_products</span><span>,</span> <span>-</span><span>k</span><span>)[</span><span>-</span><span>k</span><span>:]</span>
</span></span><span><span>    <span>idx</span> <span>=</span> <span>idx</span><span>[</span><span>np</span><span>.</span><span>argsort</span><span>(</span><span>dot_products</span><span>[</span><span>idx</span><span>])[::</span><span>-</span><span>1</span><span>]]</span>
</span></span><span><span>
</span></span><span><span>    <span>score</span> <span>=</span> <span>dot_products</span><span>[</span><span>idx</span><span>]</span>
</span></span><span><span>
</span></span><span><span>    <span>return</span> <span>idx</span><span>,</span> <span>score</span>
</span></span></code></pre></div><p>In most implementations of vector databases, once you insert the embeddings, they’re stuck there in a proprietary serialization format and you are locked into that library and service. If you’re just building a personal pet project or sanity-checking embeddings to make sure the results are good, that’s a huge amount of friction. For example, when I want to experiment with embeddings, I generate them on a cloud server with a GPU since LLM-based embeddings models are often slow to generate without one, and then download them locally to my personal computer. What is the best way to handle embeddings portably such that they can easily be moved between machines and also in a non-proprietary format?</p><p>The answer, after much personal trial-and-error, is Parquet files, which still has a surprising amount of nuance. But before we talk about why Parquet files are good, let’s talk about how <em>not</em> to store embeddings.</p><h2 id="the-worst-ways-to-store-embeddings">The Worst Ways to Store Embeddings</h2><p>The incorrect-but-unfortunately-common way to store embeddings is in a text format such as a CSV file. Text data is substantially larger than <code>float32</code> data: for example, a decimal number with full precision (e.g. <code>2.145829051733016968e-02</code>) as a <code>float32</code> is 32 bits/4 bytes, while as a text representation (in this case 24 ASCII <code>char</code>s) it’s 24 bytes, <strong>6x larger</strong>. When the CSV is saved and loaded, the data has to be serialized between a numpy and a string representation of the array, which adds significant overhead. Despite that, in <a href="https://github.com/openai/openai-cookbook/blob/a3e98ea4dcf866b5e7a3cb7d63dccaa68c7d63aa/examples/Embedding_Wikipedia_articles_for_search.ipynb">one of OpenAI’s official tutorials</a> for their embeddings models, they save the embeddings as a CSV using <a href="https://pandas.pydata.org/">pandas</a> with the admitted caveat of “Because this example only uses a few thousand strings, we’ll store them in a CSV file. (For larger datasets, use a vector database, which will be more performant.)”. In the case of the Magic card embeddings, pandas-to-CSV performs the <em>worst</em> out of any encoding options: more on why later.</p><p>Numpy has native methods to <a href="https://numpy.org/doc/stable/reference/generated/numpy.savetxt.html">save</a> and <a href="https://numpy.org/doc/stable/reference/generated/numpy.loadtxt.html">load</a> embeddings as a <code>.txt</code> that’s straightforward:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>np</span><span>.</span><span>savetxt</span><span>(</span><span>"embeddings_txt.txt"</span><span>,</span> <span>embeddings</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_r</span> <span>=</span> <span>np</span><span>.</span><span>loadtxt</span><span>(</span><span>"embeddings_txt.txt"</span><span>,</span> <span>dtype</span><span>=</span><span>np</span><span>.</span><span>float32</span><span>,</span> <span>delimiter</span><span>=</span><span>" "</span><span>)</span>
</span></span></code></pre></div><p>The resulting file not only takes a few seconds to save and load, but it’s also massive: <strong>631.5 MB</strong>!</p><p>As an aside, HTTP APIs such as OpenAI’s <a href="https://platform.openai.com/docs/guides/embeddings">Embeddings API</a> do transmit the embeddings over text which adds needless latency and bandwidth overhead. I wish more embedding providers offered <a href="https://grpc.io/">gRPC</a> APIs which allow transfer of binary <code>float32</code> data instead to gain a performance increase: Pinecone’s <a href="https://docs.pinecone.io/reference/python-sdk">Python SDK</a>, for example, does just that.</p><p>The second incorrect method to save a matrix of embeddings to disk is to save it as a Python <a href="https://docs.python.org/3/library/pickle.html">pickle</a> object, which stores its representation in memory on disk with a few lines of code from the native <code>pickle</code> library. Pickling is unfortunately common in the machine learning industry since many ML frameworks such as <a href="https://scikit-learn.org/stable/">scikit-learn</a> don’t have easy ways to serialize encoders and models. But it comes with two major caveats: pickled files are a massive security risk as they can execute arbitrary code, and the pickled file may not be guaranteed to be able to be opened on other machines or Python versions. It’s 2025, just stop pickling if you can.</p><p>In the case of the Magic card embeddings, it does indeed work with instant save/loads, and the file size on disk is <strong>94.49 MB</strong>: the same as its memory consumption and about 1/6th of the text size as expected:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>with</span> <span>open</span><span>(</span><span>"embeddings_matrix.pkl"</span><span>,</span> <span>"wb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
</span></span><span><span>    <span>pickle</span><span>.</span><span>dump</span><span>(</span><span>embeddings</span><span>,</span> <span>f</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>with</span> <span>open</span><span>(</span><span>"embeddings_matrix.pkl"</span><span>,</span> <span>"rb"</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
</span></span><span><span>    <span>embeddings_r</span> <span>=</span> <span>pickle</span><span>.</span><span>load</span><span>(</span><span>f</span><span>)</span>
</span></span></code></pre></div><p>But there are still better and easier approaches.</p><h2 id="the-intended-but-not-great-way-to-store-embeddings">The Intended-But-Not-Great Way to Store Embeddings</h2><p>Numpy itself has a canonical way to <a href="https://numpy.org/doc/2.1/reference/generated/numpy.save.html">save</a> and <a href="https://numpy.org/doc/2.1/reference/generated/numpy.load.html">load</a> matrixes — which annoyingly saves as a pickle by default for compatability reasons, but that can fortunately be disabled by setting <code>allow_pickle=False</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>np</span><span>.</span><span>save</span><span>(</span><span>"embeddings_matrix.npy"</span><span>,</span> <span>embeddings</span><span>,</span> <span>allow_pickle</span><span>=</span><span>False</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_r</span> <span>=</span> <span>np</span><span>.</span><span>load</span><span>(</span><span>"embeddings_matrix.npy"</span><span>,</span> <span>allow_pickle</span><span>=</span><span>False</span><span>)</span>
</span></span></code></pre></div><p>File size and I/O speed are the same as with the <code>pickle</code> approach.</p><p>This works — and it’s something I had used for awhile — but in the process it exposes another problem: how do we map metadata (the Magic cards in this case) to embeddings? Currently, we use the <code>idx</code> of the most-similar matches to perform an efficient batched lookup to the source data. In this case, the number of rows matches the number of cards exactly, but what happens if the embeddings matrix needs to be changed, such as to add or remove cards and their embeddings? What happens if you want to add a dataset filter? It becomes a mess that inevitably causes technical debt.</p><p>The solution to this is to colocate metadata such as card names, card text, and attributes with their embeddings: that way, if they are later added, removed, or sorted, the results will remain the same. Modern vector databases such as qdrant and Pinecone do just that, with the ability to filter and sort on the metadata at the same time you query the most similar vectors. This is a bad idea to do in numpy itself, as it’s more optimized for numbers and not other data types such as strings, which have <a href="https://numpy.org/devdocs/user/basics.strings.html">limited operations available</a>.</p><p>The solution is to look at another file format that can store metadata and embeddings simultaneously, and the answer to that is Parquet files. But there’s a rabbit hole as to what’s the <em>best</em> way to interact with them.</p><h2 id="what-are-parquet-files">What are Parquet files?</h2><p>Parquet, developed by the open-source <a href="https://parquet.apache.org/">Apache Parquet</a> project, is a file format for handling columnar data, but despite being <a href="https://blog.x.com/engineering/en_us/a/2013/announcing-parquet-10-columnar-storage-for-hadoop">first released in 2013</a> it hasn’t taken off in the data science community until very recently. <sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> The most relevant feature of Parquet is that the resulting files are typed for each column, and that this typing includes nested lists, such as an embedding which is just a list of <code>float32</code> values. As a bonus, the columnar format allows downstream libraries to save/load them selectively and very quickly, far faster than CSVs and with rare parsing errors. The file format also allows for efficient compression and decompression, but that’s less effective with embeddings as there’s little redundant data.</p><p>For Parquet file I/O, the standard approach is to use the <a href="https://arrow.apache.org/">Apache Arrow</a> protocol that is columnar in-memory, which complements the Parquet storage medium on disk. But how do you use Arrow?</p><h2 id="how-do-you-use-parquet-files-in-python-for-embeddings">How do you use Parquet files in Python for embeddings?</h2><p>Ideally, we need a library that can handle nested data easily and can interoperate with numpy for serializing to a matrix and can run fast dot products.</p><p>The official Arrow library that <a href="https://arrow.apache.org/docs/python/index.html">interacts with Parquet natively</a> in Python is <a href="https://arrow.apache.org/docs/python/index.html">pyarrow</a>. Here, I have an example Parquet file generated with [SPOILERS] that contains both the card metadata and an <code>embedding</code> column, with the embedding for each row corresponding to that card.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pa</span><span>.</span><span>parquet</span><span>.</span><span>read_table</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>)</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu17998700735124782486.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu3640072816198911328.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/parquet_hu8958370197007221068.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/parquet.png 1352w" src="https://minimaxir.com/2025/02/embeddings-parquet/parquet.png" alt="Pyarrow’s table schema from the input Parquet file of Magic card embeddings. Note the embedding column at the bottom is a list of 768 floats."><figcaption><p>Pyarrow’s table schema from the input Parquet file of Magic card embeddings. Note the <code>embedding</code> column at the bottom is a list of 768 floats.</p></figcaption></figure><p>But pyarrow is not a DataFrame library, and despite the data being in a Table, it’s hard to slice and access: the documentation suggests that you export to pandas if you need more advanced manipulation.</p><p>Other more traditional data science libraries can leverage pyarrow directly. The most popular one is, of course, pandas itself which can <a href="https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html">read/write Parquet</a> doing just that. There are many, many resources for using pandas well, so it’s often the first choice among data science practioners.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pd</span><span>.</span><span>read_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>"name"</span><span>,</span> <span>"embedding"</span><span>])</span>
</span></span><span><span><span>df</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu6407179862966887367.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu14762325519826550103.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed_hu12287036768330367704.webp 1024w,https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed.png 1224w" src="https://minimaxir.com/2025/02/embeddings-parquet/pandas_embed.png" alt="Pandas HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook."><figcaption><p>Pandas HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook.</p></figcaption></figure><p>There’s one major weakness for the use case of embeddings: pandas is very bad at nested data. From the image above you’ll see that the <code>embedding</code> column <em>appears</em> to be a list of numbers, but it’s actually a list of numpy <code>object</code>s, which is a very inefficent datatype and why I suspect writing it to a CSV is very slow. Simply converting it to numpy with <code>df["embedding"].to_numpy()</code> results in a 1D array, which is definitely wrong, and trying to cast it to <code>float32</code> doesn’t work. I found that the best way to extract the embeddings matrix from a pandas <code>embedding</code> column is to <a href="https://numpy.org/doc/2.1/reference/generated/numpy.vstack.html">np.vstack()</a> the embeddings, e.g. <code>np.vstack(df["embedding"].to_numpy())</code>, which does result in a <code>(32254, 768)</code> <code>float32</code> matrix as expected. That adds a lot of compute and memory overhead in addition to unnecessary numpy array copies. Finally, after computing the dot products between a candidate query and the embedding matrix, row metadata with the most similar values can then be retrieved using <code>df.loc[idx]</code>. <sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p><p>However, there is another, more recent tabular data library that not only is faster than pandas, it has proper support for nested data. That library is polars.</p><h2 id="the-power-of-polars">The Power of polars</h2><p><a href="https://pola.rs/">Polars</a> is a relatively new Python library which is primarily written in <a href="https://www.rust-lang.org/">Rust</a> and <a href="https://docs.pola.rs/#key-features">supports Arrow</a>, which gives it a <a href="https://duckdblabs.github.io/db-benchmark/">massive performance increase</a> over pandas and many other DataFrame libraries. In the case of Magic cards, 32k rows isn’t nearly “big data” and the gains of using a high-performance library are lesser, but there are some unexpected features that coincidentally work <em>perfectly</em> for the embeddings use case.</p><p>As with pandas, you read a parquet file with a <code>read_parquet()</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pl</span><span>.</span><span>read_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>,</span> <span>columns</span><span>=</span><span>[</span><span>"name"</span><span>,</span> <span>"embedding"</span><span>])</span>
</span></span><span><span><span>df</span>
</span></span></code></pre></div><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/polars_embed_hu6230264701954762810.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/polars_embed_hu6820488175446530372.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/polars_embed.png 957w" src="https://minimaxir.com/2025/02/embeddings-parquet/polars_embed.png" alt="Polars HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook."><figcaption><p>Polars HTML table output of the Magic card DataFrame when printed in a Jupyter Notebook.</p></figcaption></figure><p>There’s a notable difference in the table output compared to <code>pandas</code>: it also reports the data type of its columns, and more importantly, it shows that the <code>embedding</code> column consists of arrays, all <code>float32</code>s, and all length 768. That’s a great start!</p><p>polars also has a to_numpy() function. Unlike pandas, if you call <code>to_numpy()</code> on a column as a Series, e.g. <code>df['embedding'].to_numpy()</code>, the returned object is a numpy 2D matrix: no <code>np.vstack()</code> needed. If you look at the <a href="https://docs.pola.rs/api/python/stable/reference/series/api/polars.Series.to_numpy.html">documentation</a> for the function, there’s a curious feature:</p><blockquote><p>This operation copies data only when necessary. The conversion is zero copy when all of the following hold: […]</p></blockquote><p>Zero copy! And in the case of columnar-stored embeddings, the conditions will always hold, but you can set <code>allow_copy=False</code> to throw an error just in case.</p><p>Inversely, if you want to add a 2D embeddings matrix to an existing DataFrame and colocate each embedding’s corresponding metadata, such as after you batch-generate thousands of embeddings and want to save and download the resulting Parquet, it’s just as easy as adding a column to the DataFrame.</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df</span> <span>=</span> <span>pl</span><span>.</span><span>with_columns</span><span>(</span><span>embedding</span><span>=</span><span>embeddings</span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>df</span><span>.</span><span>write_parquet</span><span>(</span><span>"mtg-embeddings.parquet"</span><span>)</span>
</span></span></code></pre></div><p>Now, let’s put the speed to the test using all the Magic card metadata. What if we perform embedding similarity on a Magic card, but beforehand dynamically filter the dataset according to user parameters (therefore filtering the candidate embeddings at the same time since they are colocated) and perform the similarity calculations quickly as usual? Let’s try with <a href="https://gatherer.wizards.com/pages/card/details.aspx?multiverseid=87908">Lightning Helix</a>, a card whose effects are self-explanatory even to those who don’t play Magic.</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/helix_1_hu9495365185621367508.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/helix_1_hu243742327427369351.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/helix_1.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/helix_1.webp" alt="The most similar cards to Lightning Helix do have similar effects, although “Lightning” cards dealing damage is a common trope in Magic. Warleader’s Helix is a direct reference to Lightning Helix."><figcaption><p>The most similar cards to Lightning Helix do have similar effects, although “Lightning” cards dealing damage is a common trope in Magic. <a href="https://gatherer.wizards.com/pages/card/Details.aspx?multiverseid=456806">Warleader’s Helix</a> is a direct reference to Lightning Helix.</p></figcaption></figure><p>Now we can also find similar cards to Lightning Helix but with filters. In this case, let’s look for a Sorcery (which are analogous to Instants but tend to be stronger since they have play limitations) and has Black as one of its colors. This limits the candidates to ~3% of the original dataset. The resulting code would look like this, given a <code>query_embed</code>:</p><div><pre tabindex="0"><code data-lang="py3"><span><span><span>df_filter</span> <span>=</span> <span>df</span><span>.</span><span>filter</span><span>(</span>
</span></span><span><span>    <span>pl</span><span>.</span><span>col</span><span>(</span><span>"type"</span><span>)</span><span>.</span><span>str</span><span>.</span><span>contains</span><span>(</span><span>"Sorcery"</span><span>),</span>
</span></span><span><span>    <span>pl</span><span>.</span><span>col</span><span>(</span><span>"manaCost"</span><span>)</span><span>.</span><span>str</span><span>.</span><span>contains</span><span>(</span><span>"B"</span><span>),</span>
</span></span><span><span><span>)</span>
</span></span><span><span>
</span></span><span><span><span>embeddings_filter</span> <span>=</span> <span>df_filter</span><span>[</span><span>"embedding"</span><span>]</span><span>.</span><span>to_numpy</span><span>(</span><span>allow_copy</span><span>=</span><span>False</span><span>)</span>
</span></span><span><span><span>idx</span><span>,</span> <span>_</span> <span>=</span> <span>fast_dot_product</span><span>(</span><span>query_embed</span><span>,</span> <span>embeddings_filter</span><span>,</span> <span>k</span><span>=</span><span>4</span><span>)</span>
</span></span><span><span><span>related_cards</span> <span>=</span> <span>df_filter</span><span>[</span><span>idx</span><span>]</span>
</span></span></code></pre></div><p>As an aside, in polars you can call row subsets of a DataFrame with <code>df[idx]</code>, which makes it infinitely better than pandas and its <code>df.iloc[idx]</code>.</p><p>The resulting similar cards:</p><figure><img loading="lazy" srcset="https://minimaxir.com/2025/02/embeddings-parquet/helix_2_hu8536479567478311954.webp 320w,https://minimaxir.com/2025/02/embeddings-parquet/helix_2_hu10382916257055575322.webp 768w,https://minimaxir.com/2025/02/embeddings-parquet/helix_2.webp 976w" src="https://minimaxir.com/2025/02/embeddings-parquet/helix_2.webp" alt="In this case, the similarity focuses on card text similarity, and these cards have near identical text. Smiting Helix is also a direct reference to Lightning Helix."><figcaption><p>In this case, the similarity focuses on card text similarity, and these cards have near identical text. <a href="https://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=464058">Smiting Helix</a> is also a direct reference to Lightning Helix.</p></figcaption></figure><p>Speed-wise, the code runs at about <strong>1.48ms</strong> on average, or about 37% slower than calculating all dot products, so the filtering does still have some overhead, which is not surprising as that the filtered dataframe does copy the embeddings. Overall, it’s still more than fast enough for a hobby project.</p><p>I’ve created an <a href="https://colab.research.google.com/drive/19C_9sBC0Py2PlXYihl2ed378oGyroONZ?usp=sharing">interactive Colab Notebook</a> where you can generate similarities for any Magic card, and apply any filters you want!</p><h2 id="scaling-to-vector-databases">Scaling to Vector Databases</h2><p>Again, all of this assumes that you are using the embeddings for smaller/noncommercial projects. If you scale to hundreds of thousands of embeddings, the parquet and dot product approach for finding similarity should still be fine, but if it’s a business critical application, the marginal costs of querying a vector database are likely lower than the marginal revenue from a snappy similarity lookup. Deciding how to make these tradeoffs is the fun part of MLOps!</p><p>In the case that the amount of vectors is too large to fit into memory but you don’t want to go all-in on vector databases, another option that may be worth considering is using an old-fashioned database that can now support vector embeddings. Notably, <a href="https://www.sqlite.org/">SQLite</a> databases are just a single portable file, however interacting with them has more technical overhead and considerations than the <code>read_parquet()</code> and <code>write_parquet()</code> of polars. One notable implementation of vector databases in SQLite is the <a href="https://alexgarcia.xyz/sqlite-vec/">sqlite-vec extension</a>, which also allows for simultaneous filtering and similarity calculations.</p><p>The next time you’re working with embeddings, consider whether you really need a vector database. For many applications, the combination of Parquet files and polars provides everything you need: efficient storage, fast similarity search, and easy metadata filtering. Sometimes the simplest solution is the best one.</p><p><em>The code used to process the Magic card data, create the embeddings, and plot the UMAP 2D projection, is all available <a href="https://github.com/minimaxir/mtg-embeddings">in this GitHub repository</a>.</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Co-Scientist AI fed previous paper with the answer in it (153 pts)]]></title>
            <link>https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/</link>
            <guid>43162582</guid>
            <pubDate>Mon, 24 Feb 2025 17:52:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/">https://pivot-to-ai.com/2025/02/22/google-co-scientist-ai-cracks-superbug-problem-in-two-days-because-it-had-been-fed-the-teams-previous-paper-with-the-answer-in-it/</a>, See on <a href="https://news.ycombinator.com/item?id=43162582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>The hype cycle for Google’s fabulous new AI Co-Scientist tool, based on the Gemini LLM, includes a BBC headline about how José Penadés’ team at Imperial College asked the tool about a problem he’d been working on for years — and it solved it in less than 48 hours! [<a href="https://www.bbc.co.uk/news/articles/clyz6e9edy3o"><i>BBC</i></a><i>; </i><a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/"><i>Google</i></a>]</p>
<p>Penadés works on the evolution of drug-resistant bacteria. Co-Scientist suggested the bacteria might be hijacking fragments of DNA from bacteriophages. The team said that if they’d had this hypothesis at the start, it would have saved years of work.</p>
<p>Sounds almost too good to be true! Because it is. It turns out Co-Scientist had been fed a 2023 paper by Penadés’ team that included a version of the hypothesis. The BBC coverage failed to mention this bit. [<a href="https://www.newscientist.com/article/2469072-can-googles-new-research-assistant-ai-give-scientists-superpowers/"><i>New Scientist</i></a><i>, </i><a href="https://archive.is/etd5F"><i>archive</i></a>]</p>
<p>Google’s other claimed successes for Co-Scientist follow this pattern. The system proposed new drugs for liver fibrosis — but the proposed drugs had previously been studied for this use case.</p>
<p>In 2023, Google loudly publicised how DeepMind had synthesized 43 “new materials” — but studies in 2024 showed that none of the materials was actually new, and that only 3 of 58 syntheses were even successful. [<a href="https://journals.aps.org/prxenergy/abstract/10.1103/PRXEnergy.3.011002"><i>APS</i></a><i>; </i><a href="https://chemrxiv.org/engage/api-gateway/chemrxiv/assets/orp/resource/item/65957d349138d231611ad8f7/original/challenges-in-high-throughput-inorganic-material-prediction-and-autonomous-synthesis.pdf"><i>ChemrXiv</i></a>]</p>
<p>“Everything was already published, but in different bits,” said Penadés about Co-Scientist. “The system was able to put everything together.”</p>
<p>Sure. LLM-based madlibs can work as a suggestion tool.&nbsp; But the headline claim is not so convincing on AI scientific creativity.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Student refines 100-year-old math problem, expanding wind energy possibilities (118 pts)]]></title>
            <link>https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities</link>
            <guid>43162544</guid>
            <pubDate>Mon, 24 Feb 2025 17:49:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities">https://www.psu.edu/news/engineering/story/student-refines-100-year-old-math-problem-expanding-wind-energy-possibilities</a>, See on <a href="https://news.ycombinator.com/item?id=43162544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemscope="" itemtype="http://schema.org/ImageObject" data-testid="article-hero-image"><figcaption id="caption" data-testid="caption"><div><p><span>Divya Tyagi, right, a Penn State engineering graduate student, shows her work on a century-old math problem to Sven Schmitz, a College of Engineering faculty member and Tyagi's adviser.</span>&nbsp;<span data-testid="credit">Credit: <!-- -->Kevin Sliman<!-- -->. <!-- -->All Rights Reserved<!-- -->.</span></p></div></figcaption></div><div id="text-content-container"><p>UNIVERSITY PARK, Pa. — A Penn State engineering student refined a century-old math problem into a simpler, more elegant form, making it easier to use and explore. <a href="https://www.linkedin.com/in/dft5201/">Divya Tyagi’s</a> work expands research in aerodynamics, unlocking new possibilities in wind turbine design that Hermann Glauert, a British aerodynamicist and the original author, did not consider.&nbsp;&nbsp;&nbsp;</p>
<p>Tyagi, a graduate student pursuing her master’s degree in aerospace engineering, completed this work as a Penn State undergraduate for her Schreyer Honors College thesis. Her research was published in&nbsp;<a href="https://wes.copernicus.org/articles/10/451/2025/">Wind Energy Science</a>.</p>
<p>“I created an addendum to Glauert’s problem which determines the optimal aerodynamic performance of a wind turbine by solving for the ideal flow conditions for a turbine in order to maximize its power output,” said Tyagi, who earned her bachelor’s degree in aerospace engineering.&nbsp;</p>
<p>Her adviser, <a href="https://www.aero.psu.edu/department/directory-detail-g.aspx?q=SUS52">Sven Schmitz</a>, the Boeing/A.D. Welliver Professor in the Department of Aerospace Engineering and co-author on the paper, said Glauert’s original work focused exclusively on the maximum attainable power coefficient, which measures how efficiently a turbine converts wind energy into electricity. However, Glauert did not account for the total force and moment coefficients acting on the rotor — the spinning unit with attached blades — or how turbine blades bend under wind pressure.&nbsp;&nbsp;&nbsp;</p>
<p>“If you have your arms spread out and someone presses on your palm, you have to resist that movement,” said Schmitz, a faculty member in the <a href="https://iee.psu.edu/">Institute of Energy and the Environment</a>. “We call that the downwind thrust force and the root bending moment, and wind turbines must withstand that, too. You need to understand how large the total load is, which Glauert did not do.”&nbsp;&nbsp;</p>
<p>Schmitz said the simplicity of Tyagi’s addendum based on calculus of variations, a mathematical method used for constrained optimization problems, will allow people to explore new facets of wind turbine design.&nbsp;&nbsp;</p>
<p>“The real impact will be on the next generation of wind turbines using the new knowledge that has been unveiled,” Schmitz said. “As for Divya’s elegant solution, I think it will find its way into the classrooms, across the country and around the world.”&nbsp;</p>
<p>Tyagi said she sees her work as a step toward improving wind energy production and reducing costs.&nbsp;&nbsp;&nbsp;</p>
<p>“Improving the power coefficient of a large wind turbine by just 1% has significant impacts on the energy production of a turbine, and that translates towards the other coefficients that we derived relations for,” she said. "A 1% improvement in power coefficient could notably increase a turbine’s energy output, potentially powering an entire neighborhood."&nbsp;</p>
<p>During her senior year, Tyagi won the Anthony E. Wolk Award for her thesis on the addendum to Glauert’s work. The Wolk Award is presented to a senior in aerospace engineering who has developed the best thesis among aerospace engineering students.&nbsp;&nbsp;</p>
<p>Now pursuing her master’s degree, Tyagi is studying computational fluid dynamics simulations, analyzing airflow around a helicopter rotor.&nbsp;&nbsp;&nbsp;</p>
<p>“The goal is to integrate that with the complex flow around a ship to see how the ship airwake interacts with a helicopter trying to land on its deck,” she said.&nbsp;&nbsp;&nbsp;</p>
<p>Her U.S. Navy-supported research aims to improve flight simulation and pilot safety by better understanding these dynamic interactions.&nbsp;&nbsp;&nbsp;</p>
<p>Reflecting on her undergraduate research, Tyagi said proving her solution on paper was challenging.&nbsp;</p>
<p>“I would spend about 10 to 15 hours a week between the problem, writing the thesis and on research. It took a long time because it was so math intensive,” she said. “But I feel really proud now, seeing all the work I’ve done.”&nbsp;&nbsp;&nbsp;</p>
<p>Schmitz, who has contemplated Glauert’s problem for decades, credited Tyagi’s persistence in tackling it.&nbsp;&nbsp;&nbsp;</p>
<p>“When I thought about the Glauert problem, I thought steps were missing and it was very complicated,” Schmitz said. “There had to be an easier way to do it. That’s when Divya came in. She was the fourth student I challenged with looking at it, and she was the only one who took it on. Her work is truly impressive.”</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a site to tell the time in corporate (231 pts)]]></title>
            <link>https://corporate.watch</link>
            <guid>43162340</guid>
            <pubDate>Mon, 24 Feb 2025 17:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://corporate.watch">https://corporate.watch</a>, See on <a href="https://news.ycombinator.com/item?id=43162340">Hacker News</a></p>
<div id="readability-page-1" class="page">
    <h2>We are <span color="magenta">7 weeks</span> into <span color="magenta">Q1, 2025</span> (<span color="magenta">week 7 of 13</span>)</h2>
    <h2>The quarter started <span color="magenta">Wednesday, 01 January</span> and will end <span color="magenta">
            Monday, 31 March</span> (each quarter is <span color="magenta">~13</span> weeks)</h2>
    <h2>There are <span color="magenta">35</span> calendar days until the end of the quarter (<span color="magenta">39.33%</span> to go)</h2>
    <h4>Corporate coordinates generated <span color="darkgreen">Mon, 24 Feb 2025 21:30:01 +0000</span> (<span color="darkgreen">
            2025-02-24T21:30:01.584254767+00:00</span>)</h4>

    <hr>
    <center>
        <h4>
            <i>Wouldn't it be better if there was a tool that removed the need for this though?</i>
        </h4>
        <h4>
            <b>Take a look at <a href="https://objectivetrackr.com/?utm_source=corporatewatch&amp;utm_medium=web&amp;utm_campaign=corporatewatchlanding">objectivetrackr.com</a>!</b>
        </h4>
    </center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Right to Repair laws have now been introduced in all 50 us states (434 pts)]]></title>
            <link>https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states</link>
            <guid>43161777</guid>
            <pubDate>Mon, 24 Feb 2025 16:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states">https://www.ifixit.com/News/108371/right-to-repair-laws-have-now-been-introduced-in-all-50-us-states</a>, See on <a href="https://news.ycombinator.com/item?id=43161777">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
<p>With the introduction of <a href="https://www.midwestfarmreport.com/2025/02/21/sen-pfaff-introduces-right-to-repair-legislation/">a bill in Wisconsin</a>, Right to Repair legislation has now been introduced in <a href="https://pirg.org/media-center/release-all-50-states-now-have-filed-right-to-repair-legislation-over-last-8-years/">every single US state</a>.&nbsp;</p>



<p>We’ve been fighting for the simple right to fix everything we own for the last eleven years—and we’ve been joined in that fight by more and more advocates, tinkerers, farmers, students, and lawmakers. Today, that movement has touched every corner of the country. Lawmakers in every state in the union have <a href="https://www.repair.org/legislation">filed legislation</a> demanding access to the parts, tools, and documentation we need for repair. This year alone, legislation is active in 24 states.&nbsp;</p>



<figure><img fetchpriority="high" decoding="async" width="1220" height="966" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation.png 1220w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014255/all-50-states-have-introduced-right-to-repair-legislation-1137x900.png 1137w" sizes="(max-width: 1220px) 100vw, 1220px"></figure>



<h2>One in Five Americans Is Protected by Right to Repair Legislation</h2>



<p>Some of those laws have passed: Five states (<a href="https://www.ifixit.com/News/70515/new-york-passes-historic-right-to-repair-bill">New York</a>, <a href="https://www.ifixit.com/News/84491/california-right-to-repair-signed-into-law">California</a>, <a href="https://www.ifixit.com/News/75965/minnesotas-new-right-to-repair-law-will-give-the-whole-world-repair-manuals">Minnesota</a>, <a href="https://www.ifixit.com/News/92144/oregon-just-struck-a-blow-to-parts-pairing-and-won-a-decade-of-repair-support">Oregon</a>, and <a href="https://www.ifixit.com/News/96296/colorado-adds-electronics-to-right-to-repair-protections">Colorado</a>) have passed electronics Right to Repair legislation. One in five Americans lives in a state that has passed Right to Repair—and the remaining states are working hard to restore repair competition.</p>



<p>“Here, there and everywhere—people just want to fix their stuff,” said PIRG’s Senior Right to&nbsp;Repair Campaign Director Nathan Proctor. “Americans are fed up with all the ways in which&nbsp;manufacturers of everything from toasters to tractors frustrate or block repairs, and lawmakers&nbsp;are hearing that frustration and taking action.”</p>



<figure><img decoding="async" width="850" height="850" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image.png 850w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014620/image-150x150.png 150w" sizes="(max-width: 850px) 100vw, 850px"><figcaption>Some scenes from iFixit’s eleven years of advocacy: We called on Governor Kathy Hochul to sign the <a href="https://www.ifixit.com/News/70515/new-york-passes-historic-right-to-repair-bill">first bill in New York</a> and brought a tractor to the <a href="https://www.ifixit.com/News/73291/colorado-approves-first-ever-agricultural-right-to-repair-bill">Colorado</a> statehouse.</figcaption></figure>



<h2>iFixit’s Eleven Years of Advocacy</h2>



<p>We’ve been boots-on-the-ground fighting for Right to Repair since <a href="https://www.repair.org/history">the very beginning</a>, working to develop and testifying on behalf of the first electronics bill, introduced in South Dakota in 2014. Since then, we’ve worked closely with our US advocacy partners, Repair.org and PIRG, to advance legislation.&nbsp;</p>



<p>“Now that Wisconsin filed their first Right to Repair legislation, we’ve completed the sweep of&nbsp;getting bills filed in all 50 states. Our legislative map no longer has any blanks,” said Gay&nbsp;Gordon-Byrne, Executive Director at Repair.org. “This proves that Right to Repair is needed&nbsp;everywhere—and we are well on our way towards making that happen.”</p>



<p><a href="https://www.ifixit.com/News/3970/2012-the-year-of-the-fixer">When we first got involved</a> in Right to Repair, it felt like an uphill battle. Manufacturers told legislators there was no problem—so we brought in repair professionals, did surveys, and shared our repairability expertise to prove that things were really becoming increasingly hard to fix. Manufacturers told legislators that sharing repair information would make it impossible for them to protect their trade secrets and would be dangerous to customers—so we brought in experts, shared data, and proved that repair information is not protected and changing batteries isn’t dangerous.</p>



<p>Over time, more and more legislators joined the fight. The message is simple: If you bought it, you should be able to fix it. And soon, companies started to work with us instead of against us. Now, Google is a major supporter of Right to Repair legislation in the US, and even Apple has come on board to support some laws.</p>



<p>“This is more than a legislative landmark—it’s a tipping point. We’ve gone from a handful of&nbsp;passionate advocates to a nationwide call for repair autonomy,” said Kyle Wiens, CEO of&nbsp;iFixit. “People are fed up with disposable products and locked-down devices. Repair is the&nbsp;future, and this moment proves it.”</p>



<figure><img decoding="async" width="1926" height="1084" src="https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1.png" alt="" srcset="https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1.png 1926w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1-1536x864.png 1536w, https://valkyrie.cdn.ifixit.com/media/2025/02/24014856/image-1-1599x900.png 1599w" sizes="(max-width: 1926px) 100vw, 1926px"><figcaption><a href="https://www.ifixit.com/News/78204/congress-asks-ifixit-if-the-right-to-repair-exists">Kyle testifying before Congress</a> in a Right to Repair hearing in 2023.</figcaption></figure>



<h2>We’re Not Done Fighting</h2>



<p>Having introduced bills in all 50 states is a massive milestone. It means more pressure on lawmakers, more attention from manufacturers, and more opportunities for all of us to demand repair-friendly products. But we’re not done yet. We’ll keep pushing for stronger laws, better standards, and a future where repair autonomy is a given, not a privilege.</p>



<p>Thank you for being part of this journey. Let’s keep fixing what matters.</p>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: SubImage (YC W25) – See your infra from an attacker's perspective (107 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=43161332</link>
            <guid>43161332</guid>
            <pubDate>Mon, 24 Feb 2025 16:22:10 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=43161332">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td colspan="2"></td><td><div><p>Hi HN! I’m Alex, and along with my co-founder Kunaal, we are thrilled to introduce SubImage (<a href="https://subimage.io/">https://subimage.io</a>): a tool that lets your security team fix issues before they’re found by attackers. Teams use SubImage to map their infrastructure and emulate adversary behavior. Here’s a video of how I would use it to hack our own company: <a href="https://www.youtube.com/watch?v=P_meu4_aIVA" rel="nofollow">https://www.youtube.com/watch?v=P_meu4_aIVA</a>.</p><p>SubImage is our hosted offering built on top of Cartography (<a href="https://github.com/cartography-cncf/cartography">https://github.com/cartography-cncf/cartography</a>), the open source security graph that we created at Lyft in 2019, originally shared on HN here: <a href="https://news.ycombinator.com/item?id=19517977">https://news.ycombinator.com/item?id=19517977</a>. You can think of us as an open-core Wiz alternative.</p><p>In 2016, I worked on Microsoft’s Azure Red Team, where we built an infra mapping service to find the shortest paths to exploit our targets. We were so effective that the Blue Team wanted it too. In 2019, I joined Lyft, where we applied the same ideas to AWS and beyond, helping build and open-source Cartography. Over the past six years, it’s been incredible to grow the community and see over 70 companies (that I know of) use it.</p><p>Kunaal and I first worked closely together in 2020 when we helped bootstrap Lyft’s vulnerability management program and used Cartography as its backbone: <a href="https://eng.lyft.com/vulnerability-management-at-lyft-enforcing-the-cascade-part-1-234d1561b994" rel="nofollow">https://eng.lyft.com/vulnerability-management-at-lyft-enforc...</a>. This is actually where the name SubImage comes from: Lyft services are made up of one or more “SubImages”, and modeling this properly was such a memorable engineering challenge that we decided to name our company after it.</p><p>Cartography pulls metadata from multiple sources -- SaaS, cloud service providers, a company’s internal services -- and writes it to a graph database. This simple technique is incredibly powerful in modeling otherwise unseen misconfigurations and attack paths in areas like access permissions, networking, and software vulnerabilities.</p><p>SubImage picks up where Cartography leaves off: it’s a fully-hosted solution that provides specific recommendations for the problems it finds. The fix-action depends on company size: small teams might run AWS CLI commands, while larger orgs require automated infrastructure-as-code pull requests.</p><p>Here’s a video demo showing how we can use SubImage to understand and take action if our Stripe API key is unexpectedly used: <a href="https://www.youtube.com/watch?v=RBCr35hb5Hk" rel="nofollow">https://www.youtube.com/watch?v=RBCr35hb5Hk</a>.</p><p>SubImage also provides a natural language interface to quickly answer questions about our infra: <a href="https://imgur.com/a/subimage-natural-language-interface-query-graph-QL2ico5" rel="nofollow">https://imgur.com/a/subimage-natural-language-interface-quer...</a>.</p><p>Security is a competitive space, but we have a few differentiators:</p><p>First, we allow a very deep level of customization where the security team can enrich their graph with their own internal data, not just data from the major cloud providers. If it can be expressed as structured JSON, you can graph it; here’s a demo: <a href="https://www.youtube.com/watch?v=rvwDJoZaO_w" rel="nofollow">https://www.youtube.com/watch?v=rvwDJoZaO_w</a>. This flexibility is needed to answer questions like: Which storage buckets contain PII? Who owns them? Who’s on-call for <a href="https://example.com/api/payment" rel="nofollow">https://example.com/api/payment</a>? Which company director owns the most risk?</p><p>Since it’s built on Cartography, teams can also just write custom plugins in Python if they’d like: <a href="https://cartography-cncf.github.io/cartography/dev/writing-intel-modules.html" rel="nofollow">https://cartography-cncf.github.io/cartography/dev/writing-i...</a>.</p><p>Second, our core principle is actionability. Security teams drown in alerts. SubImage traces paths from critical assets to the most exploitable misconfigurations, helping teams cut through the noise and prioritize real threats.</p><p>Finally, we’re built on open source. We created Cartography and as it improves, so does SubImage. Cartography is a CNCF project (<a href="https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7" rel="nofollow">https://eng.lyft.com/cartography-joins-the-cncf-6f6b7be099a7</a>), which means that it is full open source and will remain so.</p><p>Going forward, we’re maintaining Cartography while launching SubImage as a fully managed offering. Our roadmap includes Access Management (prune excessive permissions and enforce security invariants, Change Tracking (detect and alert on infra changes that introduce risk), and Cloud &amp; SaaS Misconfigurations (expand visibility, including vulnerability management).</p><p>Thanks for reading! If this sounds interesting, try out <a href="https://github.com/cartography-cncf/cartography">https://github.com/cartography-cncf/cartography</a>.</p><p>It’s an honor to share SubImage with HN, especially having followed projects here for over a decade. We’d love to hear your questions, feedback, and the challenges you face in security and infra!</p></div></td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Larry Ellison's half-billion-dollar quest to change farming (116 pts)]]></title>
            <link>https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f</link>
            <guid>43161188</guid>
            <pubDate>Mon, 24 Feb 2025 16:11:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f">https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f</a>, See on <a href="https://news.ycombinator.com/item?id=43161188">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/larry-ellison-hawaii-greenhouse-farm-food-2d260e1f: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Breaking into apartment buildings in five minutes on my phone (336 pts)]]></title>
            <link>https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/</link>
            <guid>43160884</guid>
            <pubDate>Mon, 24 Feb 2025 15:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/">https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/</a>, See on <a href="https://news.ycombinator.com/item?id=43160884">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="title-block-header">
        <p>
          What a place to use default credentials
        </p>
      </div><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<section id="background">
<h3 data-anchor-id="background">Background</h3>
<p>A few months ago I was on my way to catch the <a href="https://www.instagram.com/seabusmemes/?hl=en">SeaBus</a> when I walked by an apartment building with an interesting looking access control panel. I wrote down the “MESH by Viscount” brand name and made a note to look into it when I had a chance. I ended up just missing my ferry (the 30 minute Sunday headways are brutal), so I decided to see if I could find anything promising on my phone while waiting at Waterfront for the next boat.</p>
</section>
<section id="part-0-recon">
<h3 data-anchor-id="part-0-recon">Part 0: Recon</h3>
<p>Googling the name of the system brings up a sales page advertising “TCP/IP capability to remotely program and maintain the system.” That sounds promising, so let’s try to find a manual. <code>"mesh by viscount" filetype:pdf</code> gets us an <a href="https://files.identiv.com/products/telephone-entry/common/Enterphone_MESH_Installation_Guide.pdf">installation guide</a>. Page 4 explains how to log in to the system’s web UI:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/manual.png"></p>
</figure>
</div>
<p>Default credentials that “should” be changed, with no requirement or explanation of how to do so. Surely no building managers ever leave the defaults, right? And even if they did, they’d surely have no reason to expose this thing to the Internet, right?</p>
<p>The screenshot from the manual tells us the web UI login page’s title is “FREEDOM Administration Login”, which gives us something to search for.</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/google.png"></p>
</figure>
</div>
<p>Oh no.</p>
</section>
<section id="part-1-pii-galore">
<h3 data-anchor-id="part-1-pii-galore">Part 1: PII galore</h3>
<p>Exposing the panel to the Internet is dumb, but fortunately none of these systems were accessible using the def– just kidding. The <em>very first result</em> happily lets me in with the <code>freedom:viscount</code> login. The first interesting thing here is the Users section:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/users_censored.png"></p>
</figure>
</div>
<p>This maps residents’ full names to their unit numbers. The building address is also used as the Site title. That’s already not great, but it’s worse in conjunction with the Events section:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/events_censored.png"></p>
</figure>
</div>
<p>This is a multi-year log of every time a fob associated with a certain suite number accessed an entrance or an elevator. So we can now easily determine that, say, Jon Snow of Unit 999, 123 Bear St Vancouver BC comes home every day at 6pm.</p>
<p>For good measure, there’s also a Users section which exposes every resident’s phone number.</p>
</section>
<section id="part-2-breaking-in">
<h3 data-anchor-id="part-2-breaking-in">Part 2: Breaking in</h3>
<p>The PII leaks are pretty wild, but the most interesting thing we have access to is the Controlled Areas section. In here I can apparently register new access fobs, disable existing ones, and change the floors they’re authorized for. The system for this is somewhat convoluted. Fortunately I don’t need to understand it at all, because I can just unlock any entrance I want through an override function:</p>
<div>
<figure>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/override.png"></p>
</figure>
</div>
<p>So I can break into this building in about 5 minutes without attracting any attention whatsoever. Neat.</p>
</section>
<section id="part-3-how-widespread-is-this">
<h3 data-anchor-id="part-3-how-widespread-is-this">Part 3: How widespread is this?</h3>
<p>Maybe I just got lucky that the default credentials worked on the first result and this is actually really rare. Let’s get back to a desktop and scan more properly with ZoomEye:</p>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/zoomeye.png"></p>
<p>That’s not a good sign. ZoomEye kindly offers to let me download a CSV of the results for 700 ZoomPoints. I have no idea what a ZoomPoint is nor how I ended up with 2000 of them, but this seems as good a use as any. With all the hosts in hand, let’s put together a quick Nuclei template:</p>
<pre><code>id: mesh-default-login
info:
  name: MESH By Viscount
  author: Eric Daigle
  severity: high
  description: |
    MESH By Viscount default credentials were discovered.
http:
  - method: POST
  redirects: false
  path:
    - "{{BaseURL}}/mesh/servlet/mesh.webadmin.MESHAdminServlet?requestedAction=login"
  headers:
    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
    Accept-Language: fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7
    Cache-Control: max-age=0
    Content-Type: application/x-www-form-urlencoded
    Cookie: MESHWebAdminLanguage=en; MESHWebAdminRefreshInterval=0;
    MESHWebAdminPageSize=100;
    Connection: keep-alive
  body: "formLoginName=freedom&amp;formLoginPassword=viscount&amp;formLanguage=en&amp;formLogRefreshInterval=0&amp;formPageSize=100"
  matchers:
    - type: word
      part: body
      words:
        - 'Login Failed. Invalid username or password.'
      negative: true</code></pre>
<p>The login behaviour is poorly coded (shocking, I know): it returns 200 whether or not the login was successful. To get around this we use a negative matcher that returns true as long as the “Login Failed” string is not present. The web UI also returns a 301 if the default landing page on successful login has been changed, which we handle as well. Time to send it:</p>
<p><img src="https://www.ericdaigle.ca/posts/breaking-into-dozens-of-apartments-in-five-minutes/nuclei.png"></p>
<p>In total, Nuclei finds 89 hits, so about 14% of the apartment buildings using this system that have ever exposed it to ZoomEye are vulnerable. But most of those 659 hits were old - of the buildings using this system that have exposed it to ZoomEye in the past year, 43% are vulnerable and have essentially no access control. The large majority (71) of the exposed systems are in Canada, not surprising since 582 out of the 742 ZoomEye hits were Canadian (Nuclei scans fewer targets due to some duplicates).</p>
<p>I’m so glad we have modern IoT technology to keep us safe! It’s crazy to think people used to trust analog locks with physical keys.</p>
</section>
<section id="timeline">
<h3 data-anchor-id="timeline">Timeline</h3>
<ul>
<li>2024-12-20: vulnerability discovered</li>
<li>2024-12-27: Current vendor of MESH identified as Hirsch (subsidiary of Vitaprotech Group) and contacted</li>
<li>2025-01-09: CEO of Identiv, former vendor of MESH, contacted</li>
<li>2025-01-11: Hirsch product security responds requesting details and are asked if they intend to alert clients</li>
<li>2025-01-29: Hirsch replies stating that these vulnerable systems are not following manufacturers’ recommendations to change the default password</li>
<li>2025-01-30: Hirsch asked for an update as to whether clients running vulnerable systems have been alerted (no response as of publication)</li>
<li>2025-02-14: CVE-2025-26793 assigned</li>
<li>2025-02-15: publication</li>
</ul>
</section>
<section id="support">
<h3 data-anchor-id="support">Support</h3>
<p>If you’ve made it this far, consider supporting my work with a small donation on <a href="https://ko-fi.com/edaigle">ko-fi</a>! This site is ad-free, and social-media-free and uses open-source privacy-respecting <a href="https://matomo.org/privacy/">analytics</a>.</p>


</section>

</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Stochastic Calculus (348 pts)]]></title>
            <link>https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/</link>
            <guid>43160779</guid>
            <pubDate>Mon, 24 Feb 2025 15:40:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/">https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/</a>, See on <a href="https://news.ycombinator.com/item?id=43160779">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="introduction-to-stochastic-calculus"><strong>Introduction to Stochastic Calculus</strong></h2><p>Notation and code for generating visuals are presented in the <a href="#appendix">Appendix</a>.</p><h3 id="0-introduction"><span><strong>0. Introduction</strong></span><a href="#0-introduction"><i></i></a></h3><p>This document is a brief introduction to stochastic calculus. Like, an actual introduction. Not the textbook “introductions” which immediately blast you with graduate-level probability theory axioms and definitions.</p><p>The goal of this blog post is more to focus on the physical intuition and derivation of Brownian motion, which is the foundation of stochastic calculus. I will avoid very technical formalisms such as probability spaces, measure theory, filtrations, etc. in favor of a more informal approach by considering only well-behaved cases. I also try to avoid introducing too many new concepts and vocabulary.</p><p>I hope that a wider audience can feel inspired as to how stochastic calculus emerges naturally from the physical world. Then, hopefully, more people can appreciate the beauty and meaning of the mathematics behind it, and decide to dig deeper into the subject.</p><h4 id="applications"><span>Applications</span><a href="#applications"><i></i></a></h4><p>Brownian motion and Itô calculare a notable example of fairly high-level mathematics that are applied to model the real world. Stock prices jiggle erratically, molecules bounce in fluids, and noise partially corrupts signals. Stochastic calculus gives us tools to predict, optimize, and understand these messy systems in a simpified model.</p><ul><li><strong>Physics</strong>: Einstein used Brownian motion to prove atoms exist—its jittering matched molecular collisions.</li><li><strong>Finance</strong>: Option pricing (e.g., the famous Black-Scholes equation) relies on stochastic differential equations like \(dS = \mu S dt + \sigma S dW\).</li><li><strong>Biology</strong>: Random walks model how species spread or neurons fire.</li></ul><p>This is just the tip of the iceberg. More and more applications are emerging, notably in machine learning, as <a href="https://arxiv.org/abs/2011.13456">Song et al. (2021)</a> have shown in their great paper “Score-Based Generative Modeling through Stochastic Differential Equations”.</p><p>They precisely use a stochastic differential equation using Itô calculus to model the evolution of noise over time, which they can then reverse in time to generate new samples. This framework generalizes previous ones and improves performance, allowing for new paths of innovation to be explored.</p><h3 id="1-motivation"><span><strong>1. Motivation</strong></span><a href="#1-motivation"><i></i></a></h3><p>Pascal’s triangle gives the number of paths that go either left or right at each step, up to a certain point:</p><p>\[\begin{array}{cccccc} &amp; &amp; &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; 1 &amp; &amp; 1 &amp; &amp; \\ &amp; 1 &amp; &amp; 2 &amp; &amp; 1 &amp; \\ 1 &amp; &amp; 3 &amp; &amp; 3 &amp; &amp; 1 \end{array}\]</p><p>Using 0-indexing, the number of ways to reach the \(k\)-th spot in the \(n\)-th row is \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\). For example, in row 3, there are \(\binom{3}{2} = 3\) ways to hit position 2.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/binom_3_2_paths_pascal.svg"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/binom_3_2_paths_pascal.svg" alt="Pascal's Triangle Paths for 3 choose 2" loading="lazy"></a> <em><a href="#b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle">Code</a> 2D image: All 3 paths for the 2nd position in the 3rd row of Pascal’s triangle</em></p><p>Why care? This setup powers the binomial distribution, which models repeated trials with two outcomes—win or lose, heads or tails. Think of:</p><ul><li>A basketball player shooting free throws with probability \(p\) of success and \(q = 1 - p\) of failure.</li><li>A gambler betting on dice rolls.</li></ul><p>Pascal’s triangle tells us there are \(\binom{n}{k}\) ways to get \(k\) wins in \(n\) trials. If the trials are <strong>independent</strong>, we can use the multiplication rule for probabilities:</p><blockquote><p>Note that the independence assumption is <strong>strong</strong>. Real life isn’t always so clean—winning streaks in games often tie to mentality or momentum, not just chance. Keep in mind that this model can and will be inaccurate, especially visibile for very long streaks in phenomena like stock prices or sports. However, in more common scenarios, it usually approximates reality well.</p></blockquote><p>\[P(A \text{ and } B \text{ and } C \dots) = P(A) P(B) P(C) \dots\]</p><p>For one sequence with \(k\) wins (probability \(p\) each) and \(n - k\) losses (probability \(q\) each), the probability is \(p^k q^{n-k}\). Multiply by the number of ways to arrange those wins, and we get:</p><p>\[P(k \text{ wins in } n \text{ trials}) = \binom{n}{k} p^k q^{n-k}\]</p><p>This is the binomial distribution—great for discrete setups. Now, let’s zoom out. The real world often involves <strong>continuous</strong> processes, like:</p><ul><li>The motion of a falling object,</li><li>Gas diffusing through a room,</li><li>Stock prices jumping around,</li><li>Molecules colliding in a liquid.</li></ul><p>For these, the binomial model gets messy as trials pile up. Calculus, with its focus on continuous change, feels more natural. In the continuous case:</p><blockquote><p>Points and sums (discrete tools) lead to infinities. We need <strong>intervals</strong> and <strong>integrals</strong> instead.</p></blockquote><h3 id="2-from-discrete-steps-to-continuous-limits"><span><strong>2. From Discrete Steps to Continuous Limits</strong></span><a href="#2-from-discrete-steps-to-continuous-limits"><i></i></a></h3><p>It’s actually known what happens to the binomial distribution as it becomes continuous. But what does that conversion mean mathematically? Let’s dig in with examples and then formalize it.</p><p>In calculus, going from discrete to continuous means shrinking step sizes and cranking up the number of steps. For an interval \([a, b]\), we:</p><ol><li>Split it into \(n\) chunks of size \(h = \frac{b - a}{n}\),</li><li>Sum up contributions (like a Riemann sum),</li><li>Let \(n \to \infty\) and \(h \to 0\), landing on an integral.</li></ol><p>Can we adapt this to the binomial distribution? Let’s try.</p><p>Picture the \(n\)-th row of Pascal’s triangle as a random walk: at each of \(n\) steps, we move \(+1\) (a win) or \(-1\) (a loss).</p><p>We’ll set the probabability of winning as \(p = 0.5\) as a first example since it’s symmetric, making each direction equally likely and simpler to work with.</p><p>The number of ways to get \(k\) wins (and \(n - k\) losses) is \(\binom{n}{k}\). Let’s try to plot this for a different values \(n\) over \(k\). (The code can be found in the <a href="#b1-python-code-for-binomial-plots">Appendix</a>.)</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/random_walk_combined.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/random_walk_combined.png" alt="Plots for n=5,10,25,50,100" loading="lazy"></a> <em><a href="#b1-python-code-for-binomial-plots">Code</a> 2D image: Binomial distribution plots for n=5,10,25,50,100</em></p><p>That looks awfully familiar, doesn’t it? It’s a bell curve, so naturally, we might guess that the limit is a <strong>normal distribution</strong> (aka Gaussian distribution).</p><p>Where does such a normal distribution arise from? The answer lies in the <strong>Central Limit Theorem</strong>, which states that the sum of a large number of independent random variables will be approximately normally distributed. So where’s the sum happening here? Let’s proceed to formalizing our intuition.</p><p>To accomplish this, let’s define a random variable for a single step as:</p><p>\[X(t) = \begin{cases} 1 &amp; \text{with probability } \frac{1}{2} \\ -1 &amp; \text{with probability } \frac{1}{2} \\ \end{cases}\]</p><p>Here, \(X(t)\) will encode our displacement at the \(t\)-th step where \(t \in \{1,\dots,n\}\) is an indexing parameter. As before, we assume that \(X(t_1)\) is independent of \(X(t_2)\) for \(t_1 \ne t_2\). At each step \(t\), \(X(t)\) has mean \(0\) and variance \(1\).</p><p>Then, the overall displacement \(S(n)\) is:</p><p>\[S(n) = X(1) + X(2) + \dots + X(n) = \sum_{t=1}^n X(t)\]</p><p>So there it is! The central limit theorem states more precisely that given \(n\) independent and identically distributed random variables \(X_1, X_2, \dots, X_n\) with mean \(\mu\) and variance \(\sigma^2\), we have:</p><p>\[X_1 + \dots + X_n \sim N(n\mu, n\sigma^2) \text{ as } n \to \infty\]</p><p>This is precisely what need. As we take \(n \to \infty\), we have that</p><p>\[S(n) \sim N(0, n)\]</p><p>such that</p><p>\[\lim_{n \to \infty} \frac{1}{\sqrt{n}} \cdot S(n) = N(0, 1)\]</p><p>which is our desired limit. We have shown that a “continuous binomial distribution” is in fact a normal distribution.</p><p>Here are some very nice 3D animations of sample paths with the distribution evolving over the number of steps:</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial.gif" alt="Discrete Random Walk, 15 steps" loading="lazy"></a> <em><a href="#c1-3d-plot-of-discrete-random-walks">Code</a> 3D animation: Discrete Random Walk, 15 steps</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial_normalizing.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_binomial_normalizing.gif" alt="Discrete Random Walk, 100 steps" loading="lazy"></a> <em><a href="#c1-3d-plot-of-discrete-random-walks">Code</a> 3D Animation: Discrete Random Walk, 100 steps over 5 seconds</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_random_walk.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/discrete_random_walk.gif" alt="Normal Distribution Approximation by Random Walks" loading="lazy"></a> <em><a href="#c4-python-code-for-normal-distribution-approximation-by-random-walks">Code</a> 2D animation: Normal distribution approximation by discrete random walks</em></p><h3 id="3-defining-brownian-motion-wiener-process"><span><strong>3. Defining Brownian motion (Wiener process)</strong></span><a href="#3-defining-brownian-motion-wiener-process"><i></i></a></h3><p>Let’s consider a scenario faced by Scottish botanist <strong>Robert Brown</strong> in the 1820s. Imagine a small particle, like dust or pollen, floating on a body of water.</p><p>Brown realized that its movement was surprisingly erratic. It seemed like the small-scale nature of the setup resulted in such sensitivity to fluctuations, so much is that the real movement from external forces would completely overtake the previous one. Hence, in a simplified mathematical model we scale consider the events at different times as <em>independent</em>.</p><p>In addition, there is positional symmetry: the average position of the particle at time \(t\) seemed float approximately around the origin.</p><p>Motivated by these observations as well as our previous intuition on continuous random walks, let’s first think about a simplified model for 1-dimensional discrete case. We’ll list some properties that a continuous random walk should have.</p><ol><li><strong>Starting Point</strong>: As a mathematical convenience, we position our coordinate system to set the starting point of the walk to be zero.</li><li><strong>Positional Symmetry</strong>: The walk has no directional bias. For each step, the expected displacement is zero, such that the overall expected displacement is also zero.</li><li><strong>Independence</strong>: Steps at different times are independent. The displacement between two different intervals of time is independent.</li><li><strong>Continuity</strong>: The walk is continuous, with no jumps or gaps.</li><li><strong>Normality</strong>: As we established by taking discrete random walks in the continuous limit, the distribution of positions at any given time should be normal.</li></ol><p>So let’s write this mathematically. Such a random variable is usually denoted either by \(B_t\) for “Bronian motion”, which is the physical phenomenon, or \(W_t\) for “Wiener process”, in honor of the mathematician <strong>Norbert Wiener</strong> who developed a lot of its early theory.</p><p>I will use \(W(t)\) to emphasize its dependence on \(t\).</p><p>Let \(W(t)\) be the position of the Brownian motion at time \(t\), and let \(\Delta W(t_1,t_2)\) be the displacement of the Brownian motion from time \(t_1\) to time \(t_2\).</p><blockquote><p>Note that, unlike the discrete case, we cannot consider a single increment and have a single index \(t\) for displacements as we did with \(X(t)\). As mentioned, the continuous case requires considering intervals instead of single steps.</p></blockquote><p>Then, we write some properties of Brownian motion:</p><ol><li>\(W(0)=0\) almost surely</li><li>\(W(t)\sim N(0,t)\)<ul><li>With the first condition, this is often written equivalently as \(\Delta W(s,t)\sim N(0,t-s)\) for all \(s \ne t\)</li></ul></li><li>\(\Delta W(t_1,t_2)\) is independent of \(\Delta W(t_2,t_3)\) for arbitrary distinct \(t_1 &lt; t_2 \le t_3\)</li></ol><p>We can straightforwardly use these conditions are enough to find</p><ol><li>\(E[W(t)]=0\) for all \(t\)</li><li>\(Var(W(t))=t\) for all \(t\)</li></ol><p>This is analogous to the discrete case.</p><p>But it also turns out that these conditions are sufficient to prove continuity, although it’s more involved:</p><ol><li>The sample path \(t \mapsto W(t)\) is almost surely uniformly Hölder continuous for each exponent \(\gamma &lt; \frac{1}{2}\), but is nowhere Hölder continuous for \(\gamma &gt;= \frac{1}{2}\). <a href="https://math.nyu.edu/~bourgade/SA2010/StochasticAnalysis.pdf#page30">p.30,33 of source</a><ul><li>In particular, a sample path \(t \mapsto W(t)\) is almost surely nowhere differentiable.</li></ul></li></ol><p>So, \(W(t)\) is our mathematical model for Brownian motion: a continuous, random, zero-mean process with variance proportional to time. It’s wild—it’s globally somewhat predictable yet locally completely unpredictable. A plot of W(t) looks like a jagged mess, but it’s got structure under the hood. (You can generate one yourself with the code in <a href="#b2-python-code-for-brownian-motion-plot">Appendix</a>.)</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_brownian_motion.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_brownian_motion.png" alt="Sample Brownian Motion Path" loading="lazy"></a> <em><a href="#b2-python-code-for-brownian-motion-plot">Code</a> 2D image: Sample Brownian motion path</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/continuous_brownian_3d_smooth.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/continuous_brownian_3d_smooth.gif" alt="3D Animation Continuous Brownian Motion" loading="lazy"></a> <em><a href="#c2-3d-animation-of-brownian-motion">Code</a> 3D animation: Brownian motion with evolving distribution</em></p><p>Now, let’s take this beast and do something useful with it.</p><hr><h3 id="4-itô-calculus"><span><strong>4. Itô Calculus</strong></span><a href="#4-itô-calculus"><i></i></a></h3><p>Brownian motion \(W(t)\) is continuous but so irregular that it’s nowhere differentiable. To see why, consider the rate of change over a small interval \(dt\):</p><p>\[\lim_{dt \to 0} \frac{W(t + dt) - W(t)}{dt} = \lim_{dt \to 0} \frac{\Delta W(t, t + dt)}{dt}\]</p><p>Since \(\Delta W(t, t + dt) \sim N(0, dt) = \sqrt{dt} \, N(0, 1)\):</p><p>\[\frac{\Delta W(t, t + dt)}{dt} = \frac{\sqrt{dt} \, N(0, 1)}{dt} = \frac{1}{\sqrt{dt}} N(0, 1)\]</p><p>As \(dt \to 0\), \(\frac{1}{\sqrt{dt}}\) grows without bound, and the expression becomes dominated by random fluctuations—it doesn’t converge to a finite derivative. This rules out standard calculus for handling Brownian motion, but we still need a way to work with processes driven by it, like stock prices or particle diffusion.</p><p>In the 1940s, Kiyosi Itô developed a framework to address this: <strong>Itô calculus</strong>. Rather than forcing Brownian motion into the rules of regular calculus, Itô built a new system tailored to its random nature, forming the foundation of stochastic calculus.</p><h4 id="the-increment-dw-and-its-properties"><span><strong>The Increment \(dW\) and Its Properties</strong></span><a href="#the-increment-dw-and-its-properties"><i></i></a></h4><p>Define the small change in Brownian motion over an interval \(dt\):</p><p>\[dW := W(t + dt) - W(t) = \Delta W(t, t + dt)\]</p><p>From Section 3, \(W(t + dt) - W(t) \sim N(0, dt)\), so:</p><p>\[dW = \sqrt{dt} \, N(0, 1)\]</p><p>Unlike the deterministic \(dx\) in regular calculus, \(dW\) is random—its magnitude scales with \(\sqrt{dt}\), and its sign depends on a standard normal distribution \(N(0, 1)\). It’s a small but erratic step, with:</p><ul><li>\(E[dW] = 0\),</li><li>\(Var(dW) = E[(dW)^2] = dt\).</li></ul><p>Now consider \((dW)^2\). Its expected value is \(dt\), but what about its variability? The variance is \(Var[(dW)^2] = 2 dt^2\), which becomes negligible as \(dt \to 0\). This stability allows us to treat \((dW)^2 \approx dt\) in Itô calculus (formally, in the mean-square sense—see the <a href="#a1-notation">Appendix</a> for details). In contrast to regular calculus, where \((dx)^2\) is too small to matter, \((dW)^2\) is on the same scale as \(dt\), which changes how we handle calculations.</p><h4 id="the-itô-integral-integrating-against-randomness"><span><strong>The Itô Integral: Integrating Against Randomness</strong></span><a href="#the-itô-integral-integrating-against-randomness"><i></i></a></h4><p>In regular calculus, \(\int_a^b f(x) \, dx\) approximates the area under a curve by summing rectangles, \(\sum f(x_i) \Delta x\), and taking the limit as \(\Delta x \to 0\). For Brownian motion, we want something like \(\int_0^t f(s) \, dW(s)\), where \(dW(s)\) replaces \(dx\). Here, the steps are random: \(\Delta W(s_i, s_{i+1}) \sim \sqrt{\Delta s} \, N(0, 1)\). We approximate:</p><p>\[\int_0^t f(s) \, dW(s) \approx \sum_{i=0}^{n-1} f(s_i) [\Delta W(s_i, s_{i+1})]\]</p><p>over a partition \(s_0, s_1, \dots, s_n\) of \([0, t]\), then let \(n \to \infty\). Unlike a deterministic integral, the result is a random variable, reflecting \(W(t)’s\) randomness. Using \(f(s_i)\) from the left endpoint keeps the integral “non-anticipating”—we only use information up to time \(s_i\), which aligns with the forward-evolving nature of stochastic processes.</p><h4 id="itôs-lemma-a-chain-rule-for-randomness"><span><strong>Itô’s Lemma: A Chain Rule for Randomness</strong></span><a href="#itôs-lemma-a-chain-rule-for-randomness"><i></i></a></h4><p>For a function \(f(t, W(t))\), regular calculus gives:</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW\]</p><p>But Brownian motion’s roughness requires a second-order term. Taylor-expand \(f(t, W(t))\):</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW + \frac{1}{2} \frac{\partial^2 f}{\partial W^2} (dW)^2 + \text{smaller terms}\]</p><p>As \(dt \to 0\):</p><ul><li>\(dt^2\) and \(dt \, dW\) vanish,</li><li>\((dW)^2 \approx dt\) stays significant.</li></ul><p>This leaves:</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial W} dW + \frac{1}{2} \frac{\partial^2 f}{\partial W^2} dt\]</p><p>This is <strong>Itô’s Lemma</strong>. The extra \(\frac{1}{2} \frac{\partial^2 f}{\partial W^2} dt\) arises because \((dW)^2\) contributes at the \(dt\) scale, capturing the effect of Brownian motion’s curvature.</p><p>Since we have the algebraic heuristic \(dW^2 = dt\), we could in some define everything in terms of powers \(dW\) to expand things algebraically and implicitly compute derivative rules.</p><p>This is precisely the idea behind my blog post on <a href="https://jiha-kim.github.io/posts/automatic-stochastic-differentiation/index.html">Automatic Stochastic Differentiation</a>, where we use \(\mathbb{R}[\epsilon]/\epsilon^3\) in a similar fashion to dual numbers \(\mathbb{R}[\epsilon]/\epsilon^2\) for automatic differentiation in deterministic calculus.</p><p>If you haven’t already, I highly recommend checking it out.</p><h4 id="example-fw--w2"><span><strong>Example: \(f(W) = W^2\)</strong></span><a href="#example-fw--w2"><i></i></a></h4><p>Take \(f(t, W(t)) = W^2\):</p><ul><li>\(\frac{\partial f}{\partial t} = 0\),</li><li>\(\frac{\partial f}{\partial W} = 2W\),</li><li>\(\frac{\partial^2 f}{\partial W^2} = 2\).</li></ul><p>Then:</p><p>\[d(W^2) = 0 \cdot dt + 2W \, dW + \frac{1}{2} \cdot 2 \cdot dt = 2W \, dW + dt\]</p><p>Integrate from 0 to \(t\) (with \(W(0) = 0\)):</p><p>\[W(t)^2 = \int_0^t 2W(s) \, dW(s) + t\]</p><p>The \(t\) term matches \(E[W(t)^2] = t\), and the integral is a random component with mean 0, consistent with Brownian motion’s properties.</p><hr><h3 id="5-stochastic-differential-equations"><span><strong>5. Stochastic Differential Equations</strong></span><a href="#5-stochastic-differential-equations"><i></i></a></h3><p>Itô calculus gives us tools—integrals and a chain rule—to handle Brownian motion. Now we can model systems where randomness and trends coexist, using <strong>stochastic differential equations (SDEs)</strong>. Unlike regular differential equations (e.g., \(\frac{dx}{dt} = -kx\)) that describe smooth dynamics, SDEs blend deterministic behavior with stochastic noise, fitting phenomena like stock prices or diffusing particles.</p><h4 id="defining-an-sde"><span><strong>Defining an SDE</strong></span><a href="#defining-an-sde"><i></i></a></h4><p>Consider a process influenced by both a predictable trend and random fluctuations:</p><p>\[dX(t) = a(t, X(t)) \, dt + b(t, X(t)) \, dW(t)\]</p><ul><li>\(X(t)\): The evolving quantity (e.g., position or price).</li><li>\(a(t, X(t)) \, dt\): The “drift”—the systematic part, scaled by \(dt\).</li><li>\(b(t, X(t)) \, dW(t)\): The “diffusion”—random perturbations from Brownian motion.</li></ul><p>Here, \(a\) and \(b\) are functions of time and state, and \(dW(t) = \sqrt{dt} \, N(0, 1)\) brings the noise. Solutions to SDEs aren’t fixed curves but random paths, each run producing a different trajectory with statistical patterns we can study.</p><h4 id="itôs-lemma-revisited"><span><strong>Itô’s Lemma Revisited</strong></span><a href="#itôs-lemma-revisited"><i></i></a></h4><p>Itô’s lemma actually applies to a function \(f(t, X(t))\) and its stochastic derivative \(df(t, X(t))\) for a general \(dX(t) = b(t,X(t))dt+\sigma(t,X(t))dW\), and this is done through the linearity of the Itô differential (as seen using the \(\mathbb{R}[\epsilon]/\epsilon^3\) formulation).</p><p>Considering that \(dX=O(dW)\), we consider terms up to \(dX^2=O(dW^2)\):</p><p>\[\begin{aligned} df &amp;= f_t \, dt + f_X \, dX + \frac{1}{2}f_{XX} dX^2 \\ &amp;= f_t \, dt + f_X \, (b \, dt+\sigma \, dW) + \frac{1}{2}f_{XX} (b \, dt+\sigma \, dW)^2 \\ &amp;= (f_t + bf_X+\frac{1}{2}\sigma^2 f_{XX}) \, dt + \sigma f_X \, dW \end{aligned}\]</p><p>which is the general form typically presented.</p><h4 id="drift-and-diffusion"><span><strong>Drift and Diffusion</strong></span><a href="#drift-and-diffusion"><i></i></a></h4><p>The drift \(a(t, X)\) sets the average direction, like a current pushing a particle. The diffusion \(b(t, X)\) determines the random jitter’s strength. If \(b = 0\), we get a standard ODE; if \(a = 0\), it’s just scaled Brownian motion. Together, they model systems with both structure and uncertainty.</p><p>Take a simple case:</p><p>\[dX(t) = \mu \, dt + \sigma \, dW(t)\]</p><ul><li>\(\mu\): Constant drift.</li><li>\(\sigma\): Constant noise amplitude.</li></ul><p>Starting at \(X(0) = 0\), integrate:</p><p>\[X(t) = \int_0^t \mu \, ds + \int_0^t \sigma \, dW(s) = \mu t + \sigma W(t)\]</p><p>Since \(W(t) \sim N(0, t)\), we have \(X(t) \sim N(\mu t, \sigma^2 t)\)—a process drifting linearly with noise spreading over time. It’s a basic model for things like a stock with steady growth and volatility.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_SDE.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/sample_SDE.png" alt="Sample SDE Path" loading="lazy"></a> <em><a href="#b3-python-code-for-basic-sde-simulation">Code</a> 2D image: Sample SDE path with mu=1.0, sigma=0.5</em></p><h4 id="geometric-brownian-motion"><span><strong>Geometric Brownian Motion</strong></span><a href="#geometric-brownian-motion"><i></i></a></h4><p>For systems where changes scale with size—like stock prices or certain physical processes—consider <strong>geometric Brownian motion (GBM)</strong>:</p><p>\[dS(t) = \mu S(t) \, dt + \sigma S(t) \, dW(t)\]</p><ul><li>\(S(t)\): The state (e.g., stock price).</li><li>\(\mu S(t)\): Proportional drift.</li><li>\(\sigma S(t)\): Proportional noise.</li></ul><p>The percentage change \(\frac{dS}{S} = \mu \, dt + \sigma \, dW\) has a trend and randomness. To solve, let \(f = \ln S\):</p><ul><li>\(\frac{\partial f}{\partial t} = 0\),</li><li>\(\frac{\partial f}{\partial S} = \frac{1}{S}\),</li><li>\(\frac{\partial^2 f}{\partial S^2} = -\frac{1}{S^2}\).</li></ul><p>Using Itô’s lemma:</p><p>\[d(\ln S) = \frac{1}{S} (\mu S \, dt + \sigma S \, dW) + \frac{1}{2} \left( -\frac{1}{S^2} \right) (\sigma^2 S^2 dt)\] \[= \left( \mu - \frac{1}{2} \sigma^2 \right) dt + \sigma \, dW\]</p><p>Integrate from \(0\) to \(t\):</p><p>\[\ln S(t) - \ln S(0) = \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W(t)\] \[S(t) = S(0) \exp\left( \left( \mu - \frac{1}{2} \sigma^2 \right) t + \sigma W(t) \right)\]</p><p>The drift is adjusted by \(-\frac{1}{2} \sigma^2\) due to the second-order effect of noise, and \(\sigma W(t)\) adds random fluctuations. This form underlies the Black-Scholes model in finance.</p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/gbm_path.png"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/gbm_path.png" alt="Sample Geometric Brownian Motion Path" loading="lazy"></a> <em><a href="#b4-python-code-for-geometric-brownian-motion-simulation">Code</a> 2D image: A sample path of a geometric Brownian motion with parameters μ = 0.15 and σ = 0.2</em></p><p><a href="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/geometric_brownian_drifted_3d.gif"><img src="https://jiha-kim.github.io/posts/introduction-to-stochastic-calculus/geometric_brownian_drifted_3d.gif" alt="Geometric Brownian Motion drifting over time" loading="lazy"></a> <em><a href="#c3-3d-animation-of-geometric-brownian-motion">Code</a> 3D animation: Geometric Brownian Motion drifting over time</em></p><h4 id="beyond-analytics"><span><strong>Beyond Analytics</strong></span><a href="#beyond-analytics"><i></i></a></h4><p>Analytical solutions like GBM’s are exceptions. Most SDEs require numerical simulation (e.g., stepping \(X(t + \Delta t) = X(t) + \mu \Delta t + \sigma \sqrt{\Delta t} \, N(0, 1)\)) or statistical analysis via equations like Fokker-Planck. See the <a href="#b3-python-code-for-basic-sde-simulation">appendix</a> for simulation code.</p><hr><h3 id="6-stratonovich-calculus"><span><strong>6. Stratonovich Calculus</strong></span><a href="#6-stratonovich-calculus"><i></i></a></h3><p>Recall Itô’s lemma:</p><p>\[df = \left(\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial X^2}\right) dt + \frac{\partial f}{\partial X} dX\]</p><p>That second derivative term is pretty annoying to deal with in calculations. Is there a way we can simplify it to the familiar chain rule in regular calculus?</p><p>\[df = \frac{\partial f}{\partial t} dt + \frac{\partial f}{\partial X} dX\]</p><p>The answer is yes, and it’s called <strong>Stratonovich calculus</strong>. Let’s explore a bit. First, the deterministic part clearly satisfies the regular chain rule, since we can directly apply it using linearity. The trouble arises in the stochastic part, which we need to analyze. This means we only need to consider a function \(f(X(t))\).</p><p>Remember, for the Itô form, we chose to define the integral by choosing the left endpoint of each interval. In other words, it is this stochastic part that will vary. To delete this second order term, we need to somehow absorb it into the stochastic part by defining some Stratonovich differential, typically denoted by \(\circ dW\).</p><p>Going back to our Riemann sum definitions, our degrees of freedom lie in the choice of the evaluation point for each interval:</p><p>\[\int_{0}^{T} f(X(t)) \diamond dW = \lim_{n \to \infty} \sum_{i=0}^{n-1} f(X(t_i) + \lambda \Delta X(t_i,t_{i+1})) \Delta W(t_i, t_{i+1})\]</p><p>where \(\lambda \in [0,1]\) is a constant that linearly interpolates between the left and right endpoints of each interval giving a corresponding differential \(\diamond dW\), and \(\Delta X(t_i,t_{i+1}):=X(t_{i+1})-X(t_i)\).</p><p>In the deterministic case, since we always have \(O(dX^2) \to 0\), it doesn’t matter where we choose the evaluation point. However, in the stochastic case, remember that \(O(dW^2) \to O(dt)\), so we need a more careful choice of evaluation point.</p><p>Mathematically, our goal is to define a new stochastic integral that preserves the standard chain rule:</p><p>\[df = f_X \circ dX\]</p><p>In the limiting discrete form, let’s try setting every term equal to each other:</p><p>\[f(X+\Delta X) - f(X) = f_X(X+\lambda \Delta X) \Delta X\]</p><p>In other words, our newly defined differential should result in the derivative being a linear approximation of the original function instead of quadratic:</p><p>\[\frac{f(X+\Delta X)-f(X)}{\Delta X} = f_X(X+\lambda \Delta X)\]</p><p>But watch what happens as we take the Taylor expansion on both sides about \(X\) (recalling that \(o(\Delta X^2)\to 0\)):</p><p>\[f_X + \frac{1}{2}f_{XX}\Delta X = f_X + \lambda f_{XX}\Delta X\]</p><p>Comparing coefficients, we wish to set \(\lambda = 1/2\) to preserve the chain rule. So Stratonovich integrals are defined by the midpoint evaluation rule:</p><p>\[\begin{aligned} \int_{0}^{T} f(X(t)) \circ dW &amp;= \lim_{n \to \infty} \sum_{i=0}^{n-1} f(X(t_i) + \frac{1}{2} \Delta X(t_i,t_{i+1})) \Delta W(t_i, t_{i+1}) \\ &amp;= \lim_{n \to \infty} \sum_{i=0}^{n-1} f\left(\frac{X(t_i)+X(t_{i+1})}{2}\right) \Delta W(t_i, t_{i+1}) \\ \end{aligned}\]</p><h4 id="conversion-formula-between-itô-and-stratonovich"><span>Conversion Formula between Itô and Stratonovich</span><a href="#conversion-formula-between-itô-and-stratonovich"><i></i></a></h4><p>There is a formula to convert the Stratonovich differential into a corresponding Itô SDE that depends on the Itô differential as well as the volatility function \(\sigma\).</p><p>Recall that Itô’s lemma states that for \(dX = a dt + b dW\):</p><p>\[df = f_t dt + f_X dX + \frac{1}{2}f_{XX} dX^2 = (af_t + \frac{1}{2} b^2 f_{XX}) dt + bf_X dW\]</p><p>In parallel, we defined Stratonovich’s chain rule to satisfy for \(dX = \tilde a dt + \tilde b \circ dW\):</p><p>\[df = f_t dt + f_X \circ dX = (f_t + \tilde a f_X) dt + \tilde b f_X \circ dW\]</p><p>Hence, between Itô and Stratonovich SDEs, we have in both cases that the differential is scaled by the volatility function of \(X\) and \(f_X\), but the drift function changes. Let’s find a conversion formula between the two.</p><p>Suppose we have:</p><p>\[dX = a dt + b dW = \tilde a dt + b \circ dW\]</p><p>Then, our objective is to find \(\tilde a\) in terms of \(a\).</p><p>Recall from the integral definition that \(b(X) \circ dW = b(X+\frac{1}{2}dX) dW\). If we Taylor expand around \(X\), we have:</p><p>\[b(X+\frac{1}{2}dX) dW = b(X)dW + b_X(X)\frac{1}{2}dX dW + o(dt)\]</p><p>Now, if we plug in \(dX=a dt + b dW\), the first term vanishes, leaving \(b_X b \frac{1}{2}dW^2 \sim \frac{1}{2}b_X b dt\) (where the arguments \(X\) are left implicit).</p><p>Hence:</p><p>\[a = \tilde a + \frac{1}{2} b_X b.\]</p><h4 id="applications-of-stratonovich-calculus"><span><strong>Applications of Stratonovich Calculus</strong></span><a href="#applications-of-stratonovich-calculus"><i></i></a></h4><p>Stratonovich calculus, with its midpoint evaluation rule, adjusts how we handle stochastic integrals compared to Itô’s left-endpoint approach. This shift makes it valuable in certain fields where its properties align with physical systems or simplify calculations. Below are some practical applications, each with a concrete mathematical example.</p><ul><li><p><strong>Physics with Multiplicative Noise</strong>: In physical systems, noise often scales with the state—like a particle in a fluid where random kicks depend on its position. Consider a damped oscillator with position \(X(t)\) under state-dependent noise:</p>\[dX = -k X \, dt + \sigma X \circ dW\]<p>Here, \(k &gt; 0\) is the damping constant, \(\sigma\) is the noise strength, and \(\circ dW\) denotes the Stratonovich differential. Using Stratonovich’s chain rule, for \(f(X) = \ln X\):</p>\[d(\ln X) = \frac{1}{X} (-k X \, dt + \sigma X \circ dW) = -k \, dt + \sigma \circ dW\]<p>This integrates to \(X(t) = X(0) e^{-kt + \sigma W(t)}\), matching the expected exponential decay with noise. Stratonovich fits here because it preserves symmetries in continuous physical processes, unlike Itô, which adds a \(\frac{1}{2} \sigma^2 X \, dt\) drift term.</p></li><li><p><strong>Wong-Zakai Theorem and Smooth Noise</strong>: Real-world noise isn’t perfectly white (uncorrelated like \(dW\))—it’s often smoother. The Wong-Zakai theorem shows that approximating smooth noise (e.g., \(\eta(t)\) with correlation time \(\epsilon\)) as \(\epsilon \to 0\) yields a Stratonovich SDE. Take a simple system:</p>\[\dot{x} = a x + b x \eta(t)\]<p>As \(\eta(t) \to\) white noise, this becomes \(dX = a X \, dt + b X \circ dW\). In Stratonovich form, the solution is \(X(t) = X(0) e^{a t + b W(t)}\). This is useful in engineering, like modeling voltage in a circuit with thermal fluctuations, where noise has slight smoothness.</p></li><li><p><strong>Stochastic Control</strong>: In control problems, Stratonovich can simplify dynamics under feedback. Consider a system with control input \(u(t)\) and noise:</p>\[dX = (a X + u) \, dt + \sigma X \circ dW\]<p>For \(f(X) = X^2\), the Stratonovich rule gives:</p>\[d(X^2) = 2X (a X + u) \, dt + 2X \cdot \sigma X \circ dW = (2a X^2 + 2X u) \, dt + 2\sigma X^2 \circ dW\]<p>The lack of a second-derivative term (unlike Itô’s \(+ \sigma^2 X^2 dt\)) aligns with classical control intuition, making it easier to design \(u(t)\) for, say, stabilizing a noisy pendulum or a drone in wind.</p></li><li><p><strong>Biological Diffusion</strong>: In biology, noise can depend on spatial gradients, like protein diffusion across a cell. Model this as:</p>\[dX = \mu \, dt + \sigma(X) \circ dW, \quad \sigma(X) = \sqrt{2D (1 + k X^2)}\]<p>where \(D\) is diffusivity and \(k\) adjusts noise with position. Stratonovich ensures the diffusion term reflects physical conservation laws, matching experimental data in systems like bacterial motility better than Itô, which alters the drift.</p></li><li><p><strong>Numerical Stability</strong>: For simulations, Stratonovich pairs well with midpoint methods. Take \(dX = -a X \, dt + \sigma \circ dW\). A Stratonovich discretization might use:</p>\[X_{n+1} = X_n - a \left(\frac{X_n + X_{n+1}}{2}\right) \Delta t + \sigma \Delta W_n\]<p>This implicit scheme leverages the midpoint rule, reducing numerical artifacts in models like chemical kinetics compared to Itô’s explicit steps.</p></li></ul><p>The choice between Stratonovich and Itô depends on context. Stratonovich suits systems where noise is tied to physical continuity or symmetry, while Itô dominates in finance for its non-anticipating properties. The conversion \(a = \tilde{a} + \frac{1}{2} b b_X\) lets you switch forms as needed.</p><h2 id="appendix"><span>Appendix</span><a href="#appendix"><i></i></a></h2><h3 id="a0-further-reading"><span>A.0. Further Reading</span><a href="#a0-further-reading"><i></i></a></h3><ul><li><a href="https://www.chrisrackauckas.com/assets/Papers/ChrisRackauckas-IntuitiveSDEs.pdf">An Intuitive Introduction For Understanding and Solving Stochastic Differential Equations - Chris Rackauckas (2017)</a></li><li><a href="https://math.nyu.edu/~bourgade/SA2010/StochasticAnalysis.pdf">Stochastic analysis - Paul Bourgade (2010)</a></li><li><a href="https://www.cmor-faculty.rice.edu/~cox/stoch/SDE.course.pdf">AN INTRODUCTION TO STOCHASTIC DIFFERENTIAL EQUATIONS VERSION 1.2 - Lawrence C. Evans (2013)</a></li><li>Stochastic differential equations An introduction with applications - Bernt K. Øksendal (2003)</li><li><a href="https://en.wikipedia.org/wiki/Stochastic_calculus">Wikipedia: Stochastic calculus</a></li><li><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">Wikipedia: Stochastic differential equation</a></li></ul><h3 id="a1-notation"><span>A.1. Notation</span><a href="#a1-notation"><i></i></a></h3><p>Here is a list of notation used in this document:</p><ul><li>\(\binom{n}{k}=\frac{n!}{k!(n-k)!}\) is the binomial coefficient</li><li>\(X: \Omega \to \mathbb{R}\) is a random variable from a sample space \(\Omega\) to a real number</li><li>\(P(A)\) is the probability of event \(A\)</li><li>\(E[X]=\int_{\omega \in \Omega} X(\omega) dP(\omega)\) is the expected value of \(X\)</li><li>\(N(\mu, \sigma^2)\) is a normal distribution with mean \(\mu\) and variance \(\sigma^2\)</li><li>\(W(t)\) is the position of a Brownian motion at time \(t\)</li><li>\(\Delta W(t_1,t_2)\) is the displacement of a Brownian motion from time \(t_1\) to time \(t_2\)</li><li>\(dt\) is an infinitesimal time increment</li><li>\(dW := \Delta W(t,t+dt)\) is an infinitesimal increment of Brownian motion over time</li><li>\((dW)^2 \sim dt\) denotes that \((dW^2) = dt + o(dt)\) where \(\lim_{t \to 0} \frac{o(dt)}{dt} = 0\), such that \((dW)^2\) is asymptotically equal to \(dt\) in the mean-square limit:</li></ul><p>\(\lim_{dt \to 0} \frac{E[(dW)^2-dt]^2}{dt}=0\)</p><ul><li>\(f_t:=\frac{\partial f}{\partial t}\) is the partial derivative of \(f\) with respect to \(t\)</li><li>\(f_xx:=\frac{\partial^2 f}{\partial x^2}\) is the second order partial derivative of \(f\) with respect to \(x\)</li></ul><h3 id="b1-python-code-for-binomial-plots"><span>B.1. Python code for binomial plots</span><a href="#b1-python-code-for-binomial-plots"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>binom</span>

<span>n_values</span> <span>=</span> <span>[</span><span>5</span><span>,</span> <span>10</span><span>,</span> <span>25</span><span>,</span> <span>50</span><span>,</span> <span>100</span><span>]</span>
<span>p</span> <span>=</span> <span>0.5</span>

<span># Individual plots
</span><span>for</span> <span>n</span> <span>in</span> <span>n_values</span><span>:</span>
    <span>k</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>n</span> <span>+</span> <span>1</span><span>)</span>
    <span>positions</span> <span>=</span> <span>2</span> <span>*</span> <span>k</span> <span>-</span> <span>n</span>
    <span>probs</span> <span>=</span> <span>binom</span><span>.</span><span>pmf</span><span>(</span><span>k</span><span>,</span> <span>n</span><span>,</span> <span>p</span><span>)</span>
    
    <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>6</span><span>,</span> <span>4</span><span>))</span>
    <span>plt</span><span>.</span><span>bar</span><span>(</span><span>positions</span><span>,</span> <span>probs</span><span>,</span> <span>width</span><span>=</span><span>1.0</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>skyblue</span><span>'</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>black</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>f</span><span>'</span><span>n = </span><span>{</span><span>n</span><span>}</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Position (# wins - # losses)</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>max</span><span>(</span><span>probs</span><span>)</span> <span>*</span> <span>1.2</span><span>)</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>f</span><span>'</span><span>random_walk_n_</span><span>{</span><span>n</span><span>}</span><span>.png</span><span>'</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>'</span><span>tight</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>

<span># Combined plot
</span><span>fig</span><span>,</span> <span>axes</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>(</span><span>5</span><span>,</span> <span>1</span><span>,</span> <span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>12</span><span>),</span> <span>sharex</span><span>=</span><span>True</span><span>)</span>
<span>for</span> <span>i</span><span>,</span> <span>n</span> <span>in</span> <span>enumerate</span><span>(</span><span>n_values</span><span>):</span>
    <span>k</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>n</span> <span>+</span> <span>1</span><span>)</span>
    <span>positions</span> <span>=</span> <span>2</span> <span>*</span> <span>k</span> <span>-</span> <span>n</span>
    <span>probs</span> <span>=</span> <span>binom</span><span>.</span><span>pmf</span><span>(</span><span>k</span><span>,</span> <span>n</span><span>,</span> <span>p</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>bar</span><span>(</span><span>positions</span><span>,</span> <span>probs</span><span>,</span> <span>width</span><span>=</span><span>1.0</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>skyblue</span><span>'</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>black</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>n = </span><span>{</span><span>n</span><span>}</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_ylabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>axes</span><span>[</span><span>i</span><span>].</span><span>set_ylim</span><span>(</span><span>0</span><span>,</span> <span>max</span><span>(</span><span>probs</span><span>)</span> <span>*</span> <span>1.2</span><span>)</span>
<span>axes</span><span>[</span><span>-</span><span>1</span><span>].</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (# wins - # losses)</span><span>'</span><span>)</span>
<span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
<span>plt</span><span>.</span><span>savefig</span><span>(</span><span>'</span><span>random_walk_combined.png</span><span>'</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>'</span><span>tight</span><span>'</span><span>)</span>
<span>plt</span><span>.</span><span>close</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b2-python-code-for-brownian-motion-plot"><span><strong>B2. Python Code for Brownian Motion Plot</strong></span><a href="#b2-python-code-for-brownian-motion-plot"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate Brownian motion
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>
<span>t</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>1000</span><span>)</span>  <span># Time from 0 to 1
</span><span>dt</span> <span>=</span> <span>t</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t</span><span>[</span><span>0</span><span>]</span>
<span>dW</span> <span>=</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t</span><span>)</span><span>-</span><span>1</span><span>)</span>  <span># Increments
</span><span>W</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>([[</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>dW</span><span>)])</span>  <span># Cumulative sum starts at 0
</span>
<span># Plot
</span><span>plt</span><span>.</span><span>plot</span><span>(</span><span>t</span><span>,</span> <span>W</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Brownian Motion Path</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>W(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b3-python-code-for-basic-sde-simulation"><span><strong>B3. Python Code for Basic SDE Simulation</strong></span><a href="#b3-python-code-for-basic-sde-simulation"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate simple SDE: dX = mu dt + sigma dW
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>
<span>T</span> <span>=</span> <span>1.0</span>
<span>N</span> <span>=</span> <span>1000</span>
<span>dt</span> <span>=</span> <span>T</span> <span>/</span> <span>N</span>
<span>t</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>T</span><span>,</span> <span>N</span><span>+</span><span>1</span><span>)</span>
<span>mu</span><span>,</span> <span>sigma</span> <span>=</span> <span>1.0</span><span>,</span> <span>0.5</span>
<span>X</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>N</span><span>+</span><span>1</span><span>)</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>N</span><span>):</span>
    <span>dW</span> <span>=</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>)</span>
    <span>X</span><span>[</span><span>i</span><span>+</span><span>1</span><span>]</span> <span>=</span> <span>X</span><span>[</span><span>i</span><span>]</span> <span>+</span> <span>mu</span> <span>*</span> <span>dt</span> <span>+</span> <span>sigma</span> <span>*</span> <span>dW</span>

<span>plt</span><span>.</span><span>plot</span><span>(</span><span>t</span><span>,</span> <span>X</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>"</span><span>μ=</span><span>{</span><span>mu</span><span>}</span><span>, σ=</span><span>{</span><span>sigma</span><span>}</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Path of dX = μ dt + σ dW</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>X(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>legend</span><span>()</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b4-python-code-for-geometric-brownian-motion-simulation"><span><strong>B4. Python Code for Geometric Brownian Motion Simulation</strong></span><a href="#b4-python-code-for-geometric-brownian-motion-simulation"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>

<span># Simulate simple SDE: dX = mu dt + sigma dW
</span><span>np</span><span>.</span><span>random</span><span>.</span><span>seed</span><span>(</span><span>42</span><span>)</span>

<span># Simulate Geometric Brownian Motion (exact solution)
</span><span>T_gbm</span> <span>=</span> <span>10.0</span>  <span># Longer time to show exponential nature
</span><span>N_gbm</span> <span>=</span> <span>1000</span>
<span>t_gbm</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0</span><span>,</span> <span>T_gbm</span><span>,</span> <span>N_gbm</span><span>+</span><span>1</span><span>)</span>
<span>S0</span> <span>=</span> <span>100.0</span>  <span># Initial stock price
</span><span>mu</span><span>,</span> <span>sigma</span> <span>=</span> <span>0.15</span><span>,</span> <span>0.2</span>  <span># Slightly larger for visibility
</span><span>S</span> <span>=</span> <span>S0</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>((</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>t_gbm</span> <span>+</span> <span>sigma</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t_gbm</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>N_gbm</span><span>+</span><span>1</span><span>))</span>

<span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>8</span><span>,</span> <span>4</span><span>))</span>
<span>plt</span><span>.</span><span>plot</span><span>(</span><span>t_gbm</span><span>,</span> <span>S</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>"</span><span>μ=</span><span>{</span><span>mu</span><span>}</span><span>, σ=</span><span>{</span><span>sigma</span><span>}</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"</span><span>Sample Path: Geometric Brownian Motion</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>"</span><span>Time t</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>"</span><span>S(t)</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>legend</span><span>()</span>
<span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
<span>plt</span><span>.</span><span>savefig</span><span>(</span><span>"</span><span>gbm_path.png</span><span>"</span><span>,</span> <span>dpi</span><span>=</span><span>300</span><span>,</span> <span>bbox_inches</span><span>=</span><span>"</span><span>tight</span><span>"</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle"><span>B5. LaTeX Code for Tikz Diagram of Paths in Pascal’s Triangle</span><a href="#b5-latex-code-for-tikz-diagram-of-paths-in-pascals-triangle"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre></td><td><pre><span>\documentclass</span><span>{</span>standalone<span>}</span>
<span>\usepackage</span><span>{</span>tikz<span>}</span>
<span>\begin{document}</span>

<span>\begin{tikzpicture}</span>[scale=0.8]
    <span>% Add a white background rectangle</span>
  <span>\fill</span><span>[white]</span> (-12, 1) rectangle (10, -5);
  
  <span>% Row labels (only once, to the left of the first diagram)</span>
  <span>\node</span><span>[align=right]</span> at (-11, 0) <span>{</span>Row 0<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -1) <span>{</span>Row 1<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -2) <span>{</span>Row 2<span>}</span>;
  <span>\node</span><span>[align=right]</span> at (-11, -3) <span>{</span>Row 3<span>}</span>;

  <span>% Diagram 1: Path RRL</span>
  <span>\node</span> at (-6, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (-7, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (-5, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-8, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (-6, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (-4, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-9, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (-7, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (-5, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (-3, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, red, thick]</span> (-6, 0) -- (-5, -1) -- (-4, -2) -- (-5, -3); <span>% RRL</span>
  <span>\node</span> at (-6, -4) <span>{</span>Right-Right-Left<span>}</span>;

  <span>% Diagram 2: Path RLR</span>
  <span>\node</span> at (0, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (-1, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (1, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-2, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (0, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (2, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (-3, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (-1, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (1, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (3, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, blue, thick]</span> (0, 0) -- (1, -1) -- (0, -2) -- (1, -3); <span>% RLR</span>
  <span>\node</span> at (0, -4) <span>{</span>Right-Left-Right<span>}</span>;

  <span>% Diagram 3: Path LRR</span>
  <span>\node</span> at (6, 0) <span>{</span>1<span>}</span>; <span>% Row 0</span>
  <span>\node</span> at (5, -1) <span>{</span>1<span>}</span>; <span>% Row 1</span>
  <span>\node</span> at (7, -1) <span>{</span>1<span>}</span>;
  <span>\node</span> at (4, -2) <span>{</span>1<span>}</span>; <span>% Row 2</span>
  <span>\node</span> at (6, -2) <span>{</span>2<span>}</span>;
  <span>\node</span> at (8, -2) <span>{</span>1<span>}</span>;
  <span>\node</span> at (3, -3) <span>{</span>1<span>}</span>; <span>% Row 3</span>
  <span>\node</span> at (5, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (7, -3) <span>{</span>3<span>}</span>;
  <span>\node</span> at (9, -3) <span>{</span>1<span>}</span>;
  <span>\draw</span><span>[-&gt;, green, thick]</span> (6, 0) -- (5, -1) -- (6, -2) -- (7, -3); <span>% LRR</span>
  <span>\node</span> at (6, -4) <span>{</span>Left-Right-Right<span>}</span>;
<span>\end{tikzpicture}</span>

<span>\end{document}</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="3d-visualizations"><span>3D Visualizations</span><a href="#3d-visualizations"><i></i></a></h3><h4 id="c1-3d-plot-of-discrete-random-walks"><span>C1. 3D Plot of Discrete Random Walks</span><a href="#c1-3d-plot-of-discrete-random-walks"><i></i></a></h4><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># using modern imageio v3 API
</span><span>import</span> <span>os</span>
<span>from</span> <span>scipy.special</span> <span>import</span> <span>comb</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>

<span># Create a directory for frames
</span><span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span>##############################################
# Part 1: Discrete Binomial Random Walk (N = 15)
##############################################
</span>
<span>N</span> <span>=</span> <span>15</span>  <span># total number of steps (kept small for clear discreteness)
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>  <span># number of sample paths to overlay
</span>
<span># Simulate a few discrete random walk sample paths
</span><span>sample_paths</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>steps</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>N</span><span>)</span>
    <span>path</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>steps</span><span>)))</span>
    <span>sample_paths</span><span>.</span><span>append</span><span>(</span><span>path</span><span>)</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>sample_paths</span><span>)</span>  <span># shape: (num_sample_paths, N+1)
</span>
<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>t_step</span> <span>in</span> <span>range</span><span>(</span><span>N</span> <span>+</span> <span>1</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span># For each discrete time slice up to the current time, plot the PMF
</span>    <span>for</span> <span>t</span> <span>in</span> <span>range</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>):</span>
        <span># For a random walk starting at 0, possible positions are -t, -t+2, ..., t
</span>        <span>x_values</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span> <span>+</span> <span>1</span><span>,</span> <span>2</span><span>)</span>
        <span>if</span> <span>t</span> <span>==</span> <span>0</span><span>:</span>
            <span>p_values</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.0</span><span>])</span>
        <span>else</span><span>:</span>
            <span># k = (x + t)/2 gives the number of +1 steps
</span>            <span>k</span> <span>=</span> <span>(</span><span>x_values</span> <span>+</span> <span>t</span><span>)</span> <span>//</span> <span>2</span>  
            <span>p_values</span> <span>=</span> <span>comb</span><span>(</span><span>t</span><span>,</span> <span>k</span><span>)</span> <span>*</span> <span>(</span><span>0.5</span> <span>**</span> <span>t</span><span>)</span>
        <span># Plot the discrete PMF as blue markers (and connect them with a line)
</span>        <span>ax</span><span>.</span><span>scatter</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>s</span><span>=</span><span>50</span><span>)</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
    
    <span># Overlay the sample random walk paths (projected at z=0)
</span>    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>t_step</span> <span>+</span> <span>1</span><span>],</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span>
                <span>'</span><span>r-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>5</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Sample Path</span><span>'</span> <span>if</span> <span>t_step</span> <span>==</span> <span>0</span> <span>else</span> <span>""</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (steps)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Discrete Binomial Random Walk: Step </span><span>{</span><span>t_step</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlim</span><span>(</span><span>0</span><span>,</span> <span>1.0</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_binomial_</span><span>{</span><span>t_step</span><span>:</span><span>02</span><span>d</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)
</span><span>durations</span> <span>=</span> <span>[</span><span>0.25</span><span>]</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>frames</span><span>)</span> <span>-</span> <span>1</span><span>)</span> <span>+</span> <span>[</span><span>2.0</span><span>]</span>

<span># Write the GIF with variable durations and infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_binomial.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>durations</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>

<span>##############################################
# Part 2: Discrete Random Walk Normalizing (N = 50)
##############################################
</span>
<span>N</span> <span>=</span> <span>50</span>  <span># total number of steps (increased to show gradual convergence)
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>  <span># number of sample paths to overlay
</span>
<span># Simulate a few discrete random walk sample paths
</span><span>sample_paths</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>steps</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>N</span><span>)</span>
    <span>path</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>steps</span><span>)))</span>
    <span>sample_paths</span><span>.</span><span>append</span><span>(</span><span>path</span><span>)</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>(</span><span>sample_paths</span><span>)</span>  <span># shape: (num_sample_paths, N+1)
</span>
<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>t_step</span> <span>in</span> <span>range</span><span>(</span><span>N</span> <span>+</span> <span>1</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span># Plot the PMFs for all time slices from 0 to the current step
</span>    <span>for</span> <span>t</span> <span>in</span> <span>range</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>):</span>
        <span># For a random walk starting at 0, possible positions are -t, -t+2, ..., t
</span>        <span>x_values</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span> <span>+</span> <span>1</span><span>,</span> <span>2</span><span>)</span>
        <span>if</span> <span>t</span> <span>==</span> <span>0</span><span>:</span>
            <span>p_values</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1.0</span><span>])</span>
        <span>else</span><span>:</span>
            <span># For each x, number of +1 steps is (x+t)/2
</span>            <span>k</span> <span>=</span> <span>(</span><span>x_values</span> <span>+</span> <span>t</span><span>)</span> <span>//</span> <span>2</span>
            <span>p_values</span> <span>=</span> <span>comb</span><span>(</span><span>t</span><span>,</span> <span>k</span><span>)</span> <span>*</span> <span>(</span><span>0.5</span> <span>**</span> <span>t</span><span>)</span>
        
        <span># Plot the discrete PMF as blue markers and lines
</span>        <span>ax</span><span>.</span><span>scatter</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>s</span><span>=</span><span>50</span><span>)</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_values</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_values</span><span>),</span> <span>p_values</span><span>,</span> <span>color</span><span>=</span><span>'</span><span>blue</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
        
        <span># For the current time slice, overlay the normal approximation in red
</span>        <span>if</span> <span>t</span> <span>==</span> <span>t_step</span> <span>and</span> <span>t</span> <span>&gt;</span> <span>0</span><span>:</span>
            <span>x_cont</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>t</span><span>,</span> <span>t</span><span>,</span> <span>200</span><span>)</span>
            <span>normal_pdf</span> <span>=</span> <span>norm</span><span>.</span><span>pdf</span><span>(</span><span>x_cont</span><span>,</span> <span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t</span><span>))</span>
            <span>ax</span><span>.</span><span>plot</span><span>(</span><span>x_cont</span><span>,</span> <span>[</span><span>t</span><span>]</span><span>*</span><span>len</span><span>(</span><span>x_cont</span><span>),</span> <span>normal_pdf</span><span>,</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>linewidth</span><span>=</span><span>2</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Normal Approx.</span><span>'</span><span>)</span>
    
    <span># Overlay the sample random walk paths (projected along the z=0 plane)
</span>    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>t_step</span> <span>+</span> <span>1</span><span>],</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>t_step</span> <span>+</span> <span>1</span><span>),</span>
                <span>'</span><span>g-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>5</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>Sample Path</span><span>'</span> <span>if</span> <span>t_step</span> <span>==</span> <span>0</span> <span>else</span> <span>""</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (steps)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Probability</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Discrete Binomial Random Walk at Step </span><span>{</span><span>t_step</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlim</span><span>(</span><span>0</span><span>,</span> <span>1.0</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_binomial_</span><span>{</span><span>t_step</span><span>:</span><span>02</span><span>d</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Compute per-frame durations: 0.25 sec for all frames except the last one (2 sec)
</span><span>durations</span> <span>=</span> <span>[</span><span>0.25</span><span>]</span> <span>*</span> <span>(</span><span>len</span><span>(</span><span>frames</span><span>)</span> <span>-</span> <span>1</span><span>)</span> <span>+</span> <span>[</span><span>2.0</span><span>]</span>

<span># Write the GIF with variable durations and infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_binomial_normalizing.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>durations</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c2-3d-animation-of-brownian-motion"><span>C2. 3D Animation of Brownian Motion</span><a href="#c2-3d-animation-of-brownian-motion"><i></i></a></h3><p>Normal distribution sweeping and evolving across time according Brownian motion</p><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>
<span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># using modern API
</span><span>import</span> <span>os</span>

<span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># Parameters for continuous Brownian motion
</span><span>num_frames</span> <span>=</span> <span>100</span>  <span># more frames for smoother animation
</span><span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>num_frames</span><span>)</span>
<span>x</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>200</span><span>)</span>  <span># increased resolution
</span>
<span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>dt_cont</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt_cont</span><span>),</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>)</span>
    <span>sample_paths</span><span>[</span><span>i</span><span>,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span>mask</span> <span>=</span> <span>t_values</span> <span>&lt;=</span> <span>t</span>
    <span>T_sub</span><span>,</span> <span>X_sub</span> <span>=</span> <span>np</span><span>.</span><span>meshgrid</span><span>(</span><span>t_values</span><span>[</span><span>mask</span><span>],</span> <span>x</span><span>)</span>
    <span>P_sub</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>T_sub</span><span>))</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>X_sub</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>T_sub</span><span>))</span>
    <span>ax</span><span>.</span><span>plot_surface</span><span>(</span><span>X_sub</span><span>,</span> <span>T_sub</span><span>,</span> <span>P_sub</span><span>,</span> <span>cmap</span><span>=</span><span>'</span><span>viridis</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.7</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>none</span><span>'</span><span>)</span>
    
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>i</span><span>+</span><span>1</span><span>),</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>marker</span><span>=</span><span>'</span><span>o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Position (x)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time (t)</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Continuous Brownian Motion at t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/continuous_3d_smooth_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>continuous_brownian_3d_smooth.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c3-3d-animation-of-geometric-brownian-motion"><span>C3. 3D Animation of Geometric Brownian Motion</span><a href="#c3-3d-animation-of-geometric-brownian-motion"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>mpl_toolkits.mplot3d</span> <span>import</span> <span>Axes3D</span>  <span># for 3D plotting
</span><span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># modern API
</span><span>import</span> <span>os</span>

<span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># Parameters for geometric Brownian motion (GBM)
</span><span>S0</span> <span>=</span> <span>1.0</span>    <span># initial stock price
</span><span>mu</span> <span>=</span> <span>0.2</span>    <span># drift rate (increased for noticeable drift)
</span><span>sigma</span> <span>=</span> <span>0.2</span> <span># volatility
</span>
<span>num_frames</span> <span>=</span> <span>100</span>
<span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>num_frames</span><span>)</span>  <span># avoid t=0 to prevent singularity in density
</span><span>S_range</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.01</span><span>,</span> <span>5</span><span>,</span> <span>200</span><span>)</span>         <span># price range
</span>
<span># Simulate GBM sample paths
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>dt</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>
<span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_sample_paths</span><span>):</span>
    <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>),</span> <span>size</span><span>=</span><span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>)</span>
    <span>W</span> <span>=</span> <span>np</span><span>.</span><span>concatenate</span><span>(([</span><span>0</span><span>],</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)))</span>
    <span>sample_paths</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>S0</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>((</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>t_values</span> <span>+</span> <span>sigma</span> <span>*</span> <span>W</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>fig</span> <span>=</span> <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>7</span><span>))</span>
    <span>ax</span> <span>=</span> <span>fig</span><span>.</span><span>add_subplot</span><span>(</span><span>111</span><span>,</span> <span>projection</span><span>=</span><span>'</span><span>3d</span><span>'</span><span>)</span>
    
    <span>mask</span> <span>=</span> <span>t_values</span> <span>&lt;=</span> <span>t</span>
    <span>T_sub</span><span>,</span> <span>S_sub</span> <span>=</span> <span>np</span><span>.</span><span>meshgrid</span><span>(</span><span>t_values</span><span>[</span><span>mask</span><span>],</span> <span>S_range</span><span>)</span>
    <span>P_sub</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>(</span><span>S_sub</span> <span>*</span> <span>sigma</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>T_sub</span><span>)))</span> <span>*</span> \
            <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span> <span>(</span><span>np</span><span>.</span><span>log</span><span>(</span><span>S_sub</span> <span>/</span> <span>S0</span><span>)</span> <span>-</span> <span>(</span><span>mu</span> <span>-</span> <span>0.5</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span><span>)</span> <span>*</span> <span>T_sub</span><span>)</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>sigma</span><span>**</span><span>2</span> <span>*</span> <span>T_sub</span><span>))</span>
    <span>ax</span><span>.</span><span>plot_surface</span><span>(</span><span>S_sub</span><span>,</span> <span>T_sub</span><span>,</span> <span>P_sub</span><span>,</span> <span>cmap</span><span>=</span><span>'</span><span>viridis</span><span>'</span><span>,</span> <span>alpha</span><span>=</span><span>0.7</span><span>,</span> <span>edgecolor</span><span>=</span><span>'</span><span>none</span><span>'</span><span>)</span>
    
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>ax</span><span>.</span><span>plot</span><span>(</span><span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>i</span><span>+</span><span>1</span><span>),</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>marker</span><span>=</span><span>'</span><span>o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    
    <span>ax</span><span>.</span><span>set_xlabel</span><span>(</span><span>'</span><span>Stock Price S</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_ylabel</span><span>(</span><span>'</span><span>Time t</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_zlabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>set_title</span><span>(</span><span>f</span><span>'</span><span>Geometric Brownian Motion at t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>ax</span><span>.</span><span>view_init</span><span>(</span><span>elev</span><span>=</span><span>30</span><span>,</span> <span>azim</span><span>=-</span><span>60</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/geometric_brownian_drifted_3d_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>geometric_brownian_drifted_3d.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div><h3 id="c4-python-code-for-normal-distribution-approximation-by-random-walks"><span>C4. Python Code for Normal Distribution Approximation by Random Walks</span><a href="#c4-python-code-for-normal-distribution-approximation-by-random-walks"><i></i></a></h3><div><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
</pre></td><td><pre><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>norm</span>
<span>import</span> <span>imageio.v3</span> <span>as</span> <span>imageio</span>  <span># modern ImageIO v3 API
</span><span>import</span> <span>os</span>
<span>from</span> <span>scipy.special</span> <span>import</span> <span>comb</span>

<span># Create a directory for frames
</span><span>os</span><span>.</span><span>makedirs</span><span>(</span><span>'</span><span>gif_frames</span><span>'</span><span>,</span> <span>exist_ok</span><span>=</span><span>True</span><span>)</span>

<span># 1. Continuous Brownian Motion with Sample Paths
</span>
<span># Define time values and x range for density
</span><span>t_values</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>0.1</span><span>,</span> <span>5</span><span>,</span> <span>50</span><span>)</span>  <span># Times from 0.1 to 5
</span><span>x</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>100</span><span>)</span>          <span># Range of x values
</span>
<span># Simulate a few sample Brownian motion paths
</span><span>num_sample_paths</span> <span>=</span> <span>5</span>
<span>dt_cont</span> <span>=</span> <span>t_values</span><span>[</span><span>1</span><span>]</span> <span>-</span> <span>t_values</span><span>[</span><span>0</span><span>]</span>  <span># constant time step (~0.1)
</span><span>sample_paths</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)))</span>
<span>sample_paths</span><span>[:,</span> <span>0</span><span>]</span> <span>=</span> <span>0</span>
<span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>normal</span><span>(</span><span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt_cont</span><span>),</span> <span>size</span><span>=</span><span>(</span><span>num_sample_paths</span><span>,</span> <span>len</span><span>(</span><span>t_values</span><span>)</span><span>-</span><span>1</span><span>))</span>
<span>sample_paths</span><span>[:,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>t_values</span><span>):</span>
    <span>p</span> <span>=</span> <span>(</span><span>1</span> <span>/</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>2</span> <span>*</span> <span>np</span><span>.</span><span>pi</span> <span>*</span> <span>t</span><span>))</span> <span>*</span> <span>np</span><span>.</span><span>exp</span><span>(</span><span>-</span><span>x</span><span>**</span><span>2</span> <span>/</span> <span>(</span><span>2</span> <span>*</span> <span>t</span><span>))</span>
    
    <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>12</span><span>,</span> <span>4</span><span>))</span>
    <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
    <span>plt</span><span>.</span><span>plot</span><span>(</span><span>x</span><span>,</span> <span>p</span><span>,</span> <span>'</span><span>b-</span><span>'</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>'</span><span>t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Brownian Motion Distribution</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>x</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Density p(x,t)</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>0.8</span><span>)</span>
    <span>plt</span><span>.</span><span>legend</span><span>()</span>
    <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
    
    <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>)</span>
    <span>for</span> <span>sp</span> <span>in</span> <span>sample_paths</span><span>:</span>
        <span>plt</span><span>.</span><span>plot</span><span>(</span><span>t_values</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>sp</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>'</span><span>-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
    <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Sample Brownian Paths</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Time</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
    <span>plt</span><span>.</span><span>xlim</span><span>(</span><span>0</span><span>,</span> <span>5</span><span>)</span>
    <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
    
    <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/continuous_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
    <span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
    <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
    <span>plt</span><span>.</span><span>close</span><span>()</span>
    <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Save the continuous Brownian motion GIF
# (duration in seconds per frame; adjust as desired)
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>continuous_brownian.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>)</span>

<span># 2. Discrete Random Walk with Sample Paths
</span>
<span>def</span> <span>simulate_random_walk</span><span>(</span><span>dt</span><span>,</span> <span>T</span><span>,</span> <span>num_paths</span><span>):</span>
    <span>"""</span><span>Simulate random walk paths with step size sqrt(dt).</span><span>"""</span>
    <span>n_steps</span> <span>=</span> <span>int</span><span>(</span><span>T</span> <span>/</span> <span>dt</span><span>)</span>
    <span>positions</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>num_paths</span><span>,</span> <span>n_steps</span> <span>+</span> <span>1</span><span>))</span>
    <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>num_paths</span><span>):</span>
        <span>increments</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>choice</span><span>([</span><span>-</span><span>1</span><span>,</span> <span>1</span><span>],</span> <span>size</span><span>=</span><span>n_steps</span><span>)</span> <span>*</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>dt</span><span>)</span>
        <span>positions</span><span>[</span><span>i</span><span>,</span> <span>1</span><span>:]</span> <span>=</span> <span>np</span><span>.</span><span>cumsum</span><span>(</span><span>increments</span><span>)</span>
    <span>return</span> <span>positions</span>

<span>dt</span> <span>=</span> <span>0.01</span>  <span># Step size
</span><span>T</span> <span>=</span> <span>5.0</span>    <span># Total time
</span><span>num_paths</span> <span>=</span> <span>10000</span>  <span># For histogram
</span><span>times</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>T</span> <span>+</span> <span>dt</span><span>,</span> <span>dt</span><span>)</span>
<span>positions</span> <span>=</span> <span>simulate_random_walk</span><span>(</span><span>dt</span><span>,</span> <span>T</span><span>,</span> <span>num_paths</span><span>)</span>
<span>sample_indices</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>5</span><span>)</span>

<span>frames</span> <span>=</span> <span>[]</span>
<span>for</span> <span>i</span><span>,</span> <span>t</span> <span>in</span> <span>enumerate</span><span>(</span><span>times</span><span>):</span>
    <span>if</span> <span>i</span> <span>%</span> <span>10</span> <span>==</span> <span>0</span><span>:</span>  <span># Use every 10th frame for the GIF
</span>        <span>current_positions</span> <span>=</span> <span>positions</span><span>[:,</span> <span>i</span><span>]</span>
        <span>x_vals</span> <span>=</span> <span>np</span><span>.</span><span>linspace</span><span>(</span><span>-</span><span>5</span><span>,</span> <span>5</span><span>,</span> <span>100</span><span>)</span>
        <span>p_theoretical</span> <span>=</span> <span>norm</span><span>.</span><span>pdf</span><span>(</span><span>x_vals</span><span>,</span> <span>0</span><span>,</span> <span>np</span><span>.</span><span>sqrt</span><span>(</span><span>t</span><span>)</span> <span>if</span> <span>t</span> <span>&gt;</span> <span>0</span> <span>else</span> <span>1e-5</span><span>)</span>
        
        <span>plt</span><span>.</span><span>figure</span><span>(</span><span>figsize</span><span>=</span><span>(</span><span>12</span><span>,</span> <span>4</span><span>))</span>
        <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
        <span>plt</span><span>.</span><span>hist</span><span>(</span><span>current_positions</span><span>,</span> <span>bins</span><span>=</span><span>50</span><span>,</span> <span>density</span><span>=</span><span>True</span><span>,</span> <span>alpha</span><span>=</span><span>0.6</span><span>,</span> <span>label</span><span>=</span><span>f</span><span>'</span><span>t = </span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>plot</span><span>(</span><span>x_vals</span><span>,</span> <span>p_theoretical</span><span>,</span> <span>'</span><span>r-</span><span>'</span><span>,</span> <span>label</span><span>=</span><span>'</span><span>N(0,t)</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Discrete Random Walk Distribution</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Density</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylim</span><span>(</span><span>0</span><span>,</span> <span>0.8</span><span>)</span>
        <span>plt</span><span>.</span><span>legend</span><span>()</span>
        <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
        
        <span>plt</span><span>.</span><span>subplot</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>2</span><span>)</span>
        <span>for</span> <span>idx</span> <span>in</span> <span>sample_indices</span><span>:</span>
            <span>plt</span><span>.</span><span>plot</span><span>(</span><span>times</span><span>[:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>positions</span><span>[</span><span>idx</span><span>,</span> <span>:</span><span>i</span><span>+</span><span>1</span><span>],</span> <span>'</span><span>-o</span><span>'</span><span>,</span> <span>markersize</span><span>=</span><span>3</span><span>)</span>
        <span>plt</span><span>.</span><span>title</span><span>(</span><span>'</span><span>Sample Random Walk Paths</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlabel</span><span>(</span><span>'</span><span>Time</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>ylabel</span><span>(</span><span>'</span><span>Position</span><span>'</span><span>)</span>
        <span>plt</span><span>.</span><span>xlim</span><span>(</span><span>0</span><span>,</span> <span>T</span><span>)</span>
        <span>plt</span><span>.</span><span>grid</span><span>(</span><span>True</span><span>)</span>
        
        <span>frame_path</span> <span>=</span> <span>f</span><span>'</span><span>gif_frames/discrete_t_</span><span>{</span><span>t</span><span>:</span><span>.</span><span>2</span><span>f</span><span>}</span><span>.png</span><span>'</span>
        <span>plt</span><span>.</span><span>tight_layout</span><span>()</span>
        <span>plt</span><span>.</span><span>savefig</span><span>(</span><span>frame_path</span><span>)</span>
        <span>plt</span><span>.</span><span>close</span><span>()</span>
        <span>frames</span><span>.</span><span>append</span><span>(</span><span>imageio</span><span>.</span><span>imread</span><span>(</span><span>frame_path</span><span>))</span>

<span># Save the discrete random walk GIF with infinite looping
</span><span>imageio</span><span>.</span><span>imwrite</span><span>(</span><span>'</span><span>discrete_random_walk.gif</span><span>'</span><span>,</span> <span>frames</span><span>,</span> <span>duration</span><span>=</span><span>0.1</span><span>,</span> <span>loop</span><span>=</span><span>0</span><span>)</span>
</pre></td></tr></tbody></table></code></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MongoDB acquires Voyage AI (104 pts)]]></title>
            <link>https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations</link>
            <guid>43160731</guid>
            <pubDate>Mon, 24 Feb 2025 15:37:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations">https://investors.mongodb.com/news-releases/news-release-details/mongodb-announces-acquisition-voyage-ai-enable-organizations</a>, See on <a href="https://news.ycombinator.com/item?id=43160731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><b><i><org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> to integrate Voyage AI's industry-leading embedding and reranking models, delivering highly accurate and relevant information retrieval to power sophisticated AI use cases</i></b></p>
<p>,  /PRNewswire/ -- <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB, Inc.</org> (NASDAQ: MDB), the leading database for modern applications, today announced it has acquired Voyage AI, a pioneer in state-of-the-art embedding and reranking models that power next-generation AI applications. Integrating Voyage AI's technology with <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> will enable organizations to easily build trustworthy, AI-powered applications by offering highly accurate and relevant information retrieval deeply integrated with operational data.</p>

    <p>
                        <a href="https://mma.prnewswire.com/media/384058/MongoDB_Logo.html" target="_blank" rel="nofollow">
&nbsp;<img src="https://mma.prnewswire.com/media/384058/MongoDB_Logo.jpg" title="MongoDB" alt="MongoDB">
&nbsp;</a>
                </p>
<p>AI-powered applications can address a broad range of complex use cases that traditional software cannot; however, because AI models are probabilistic, they can hallucinate––when a model generates false or misleading information. Inaccurate or low-quality results can create serious risks––especially in cases where the accuracy of information is critical, such as a hospital performing cancer screenings, a financial firm making autonomous investment decisions, or a law firm offering legal advice. Consequently, the risk of hallucinations has limited the use of AI applications for mission-critical use cases. These hallucinations typically occur when the AI model lacks sufficient understanding or context of data within an enterprise.</p>
<p>To address this challenge, companies need high-quality retrieval—a critical AI capability that ensures the most relevant information is extracted from their data with precision. Voyage AI's advanced embedding and reranking models enable applications to extract meaning from highly specialized and domain-specific text and unstructured data—ranging from legal and financial documents to images, code, and enterprise knowledge bases. Their models are trusted by leading AI innovators like Anthropic, LangChain, Harvey, and Replit. Notably, Voyage AI's embedding models are the highest-rated zero-shot models in the Hugging Face community. Voyage AI is a leader in AI-powered search and retrieval, backed by a team of world-class AI researchers with roots at <org>Stanford</org>, <org>MIT</org>, <org value="ACORN:6001201275" idsrc="xmltag.org">UC Berkeley</org>, and Princeton. Their expertise in cutting-edge embedding models and retrieval architectures will enhance <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> AI capabilities to solve the most challenging problems with building and scaling AI applications.</p>
<p>"AI has the promise to transform every business, but adoption is held back by the risk of hallucinations," said <person>Dev Ittycheria</person>, CEO of <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org>. "By bringing the power of advanced AI-powered search and retrieval to our highly flexible database, the combination of <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> and Voyage AI enables enterprises to easily build trustworthy AI-powered applications that drive meaningful business impact. With this acquisition, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> is redefining what's required of the database for the AI era."</p>
<p>"For AI applications to reach their full potential, businesses must trust their outputs, so retrieval needs to be deeply integrated with operational data to be accurate and relevant," said Tengyu Ma, Founder of Voyage AI. "Joining MongoDB enables us to bring our cutting-edge AI retrieval technology to a broader audience and integrate it seamlessly into mission-critical applications. By combining our expertise in embeddings and reranking with <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> best-in-class database, we can help organizations build AI applications that deliver more accurate and reliable results at scale, empowering them to confidently apply AI to high-stakes use cases."</p>
<p>Voyage AI's embedding and reranking models will remain available through <u><a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=744614746&amp;u=http%3A%2F%2Fvoyage.ai%2F&amp;a=voyage.ai" target="_blank" rel="nofollow">voyage.ai</a></u>, <location>AWS Marketplace</location>, and <location>Azure Marketplace</location>, with further <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> integrations launching later this year.</p>
<p>For a deeper look at the technology behind Voyage AI and what this means for AI-powered applications, see the <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> blog <a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=3611869988&amp;u=https%3A%2F%2Fwww.mongodb.com%2Fblog%2Fpost%2Fredefining-database-ai-why-mongodb-acquired-voyage-ai&amp;a=here" target="_blank" rel="nofollow">here</a>.</p>
<p><b>About <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org></b></p>
<p>Headquartered in <location value="LU/us.ny.nyc" idsrc="xmltag.org">New York</location>, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> mission is to empower innovators to create, transform, and disrupt industries with software and data. <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> unified, intelligent data platform was built to power the next generation of applications, and <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> is the most widely available, globally distributed database on the market. With integrated capabilities for operational data, search, real-time analytics, and AI-powered retrieval, <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB</org> helps organizations everywhere move faster, innovate more efficiently, and simplify complex architectures. Millions of developers and more than 50,000 customers across almost every industry—including 70% of the Fortune 100—rely on&nbsp;MongoDB for their most important applications. To learn more, visit <a href="https://c212.net/c/link/?t=0&amp;l=en&amp;o=4368243-1&amp;h=952736343&amp;u=https%3A%2F%2Fwww.mongodb.com%2F&amp;a=mongodb.com" target="_blank" rel="nofollow">mongodb.com</a>.</p>
<p><b>Forward-looking Statements</b></p>
<p>This press release includes certain "forward-looking statements" within the meaning of Section 27A of the Securities Act of 1933, as amended, or the Securities Act, and Section 21E of the Securities Exchange Act of 1934, as amended, including statements concerning <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB's</org> acquisition of Voyage AI. These forward-looking statements include, but are not limited to, plans, objectives, expectations and intentions and other statements contained in this press release that are not historical facts and statements identified by words such as "anticipate," "believe," "continue," "could," "estimate," "expect," "intend," "may," "plan," "project," "will," "would" or the negative or plural of these words or similar expressions or variations. These forward-looking statements reflect our current views about our plans, intentions, expectations, strategies and prospects, which are based on the information currently available to us and on assumptions we have made. Although we believe that our plans, intentions, expectations, strategies and prospects as reflected in or suggested by those forward-looking statements are reasonable, we can give no assurance that the plans, intentions, expectations or strategies will be attained or achieved. Furthermore, actual results may differ materially from those described in the forward-looking statements and are subject to a variety of assumptions, uncertainties, risks and factors that are beyond our control including, without limitation: our customers renewing their subscriptions with us and expanding their usage of software and related services; global political changes; the effects of the ongoing military conflicts between <location value="LC/ru" idsrc="xmltag.org">Russia</location> and <location value="LC/ua" idsrc="xmltag.org">Ukraine</location> and <location value="LC/il" idsrc="xmltag.org">Israel</location> and <org value="ACORN:5020096112" idsrc="xmltag.org">Hamas</org> on our business and future operating results; economic downturns and/or the effects of rising interest rates, inflation and volatility in the global economy and financial markets on our business and future operating results; our potential failure to meet publicly announced guidance or other expectations about our business and future operating results; our limited operating history; our history of losses; failure of our platform to satisfy customer demands; the effects of increased competition; our investments in new products and our ability to introduce new features, services or enhancements; social, ethical and security issues relating to the use of new and evolving technologies, such as artificial intelligence, in our offerings or partnerships; our ability to effectively expand our sales and marketing organization; our ability to continue to build and maintain credibility with the developer community; our ability to add new customers or increase sales to our existing customers; our ability to maintain, protect, enforce and enhance our intellectual property; the effects of social, ethical and regulatory issues relating to the use of new and evolving technologies, such as artificial intelligence, in our offerings or partnerships; the growth and expansion of the market for database products and our ability to penetrate that market; our ability to integrate acquired businesses and technologies successfully or achieve the expected benefits of such acquisitions, including the acquisition of Voyage AI; the risk of any unexpected costs or expenses resulting from the acquisition of Voyage AI; the risk of any litigation relating to such acquisition; the risk that such acquisition and the announcement of it could have an adverse effect on our operating results and business generally; our ability to maintain the security of our software and adequately address privacy concerns; our ability to manage our growth effectively and successfully recruit and retain additional highly-qualified personnel; and the price volatility of our common stock. These and other risks and uncertainties are more fully described in our filings with the <org>Securities and Exchange Commission</org> ("<org>SEC</org>"), including under the caption "Risk Factors" in our Quarterly Report on Form 10-Q for the quarter ended <chron>October 31, 2024</chron>, filed with the <org>SEC</org> on <chron>December 10, 2024</chron>, and other filings and reports that we may file from time to time with the <org>SEC</org>. Except as required by law, we undertake no duty or obligation to update any forward-looking statements contained in this release as a result of new information, future events, changes in expectations or otherwise.</p>
<p><b>Press Contact:<br></b><u><a href="mailto:press@mongodb.com" target="_blank" rel="nofollow">press@mongodb.com</a></u></p>






<p id="PURL"><img title="Cision" width="12" height="12" alt="Cision" src="https://c212.net/c/img/favicon.png?sn=NY25220&amp;sd=2025-02-24" loading="lazy"> View original content to download multimedia:<a id="PRNURL" rel="nofollow" href="https://www.prnewswire.com/news-releases/mongodb-announces-acquisition-of-voyage-ai-to-enable-organizations-to-build-trustworthy-ai-applications-302382979.html" target="_blank">https://www.prnewswire.com/news-releases/mongodb-announces-acquisition-of-voyage-ai-to-enable-organizations-to-build-trustworthy-ai-applications-302382979.html</a></p>
<p>SOURCE  <org value="NASDAQ-NMS:MDB" idsrc="xmltag.org">MongoDB, Inc.</org></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Laravel Cloud (158 pts)]]></title>
            <link>https://app.laravel.cloud/</link>
            <guid>43160612</guid>
            <pubDate>Mon, 24 Feb 2025 15:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://app.laravel.cloud/">https://app.laravel.cloud/</a>, See on <a href="https://news.ycombinator.com/item?id=43160612">Hacker News</a></p>
<div id="readability-page-1" class="page"><div x-data="{ open: false }">
        

        

        <section x-ref="darkHero">
            

            <div>
                            <h2>
                                <span>
                                    The fastest way to deploy and
                                </span>
                                <br>
                                <span>
                                    scale Laravel applications
                                </span>
                            </h2>

                            <p>
                                <span>
                                    Deploy your Laravel applications without managing servers.
                                </span>
                                <br>
                                <span>
                                    One-click autoscaling, databases, caching, storage, and security.
                                </span>
                            </p>

                            
                        </div>

            <div>
                <p>Trusted by teams who want to ship fast</p>

                
            </div>
        </section>

        <div>
                <div>
                    <h2>
                        Speed, simplicity, and scalability without the headaches
                    </h2>

                    <p>
                        Built specifically for Laravel applications, Cloud eliminates configuration hassles and deployment complexity. So you can
                        focus on building, not configuring.
                    </p>
                </div>

                <div>
            <p>
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M14.75 10C14.75 11.5188 13.5188 12.75 12 12.75C10.4812 12.75 9.25 11.5188 9.25 10C9.25 8.48122 10.4812 7.25 12 7.25C13.5188 7.25 14.75 8.48122 14.75 10ZM19.25 10C19.25 16.0755 12 21.3929 12 21.3929C12 21.3929 4.75 16.0755 4.75 10C4.75 5.99594 7.99594 2.75 12 2.75C16.0041 2.75 19.25 5.99594 19.25 10Z"></path>

            </svg>


                    US East (Ohio)
                </p>

            

            

            

            <div>
                    <p>
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M3.75 6.95C3.75 5.82989 3.75 5.26984 3.96799 4.84202C4.15973 4.46569 4.46569 4.15973 4.84202 3.96799C5.26984 3.75 5.82989 3.75 6.95 3.75H17.05C18.1701 3.75 18.7302 3.75 19.158 3.96799C19.5343 4.15973 19.8403 4.46569 20.032 4.84202C20.25 5.26984 20.25 5.82989 20.25 6.95V17.05C20.25 18.1701 20.25 18.7302 20.032 19.158C19.8403 19.5343 19.5343 19.8403 19.158 20.032C18.7302 20.25 18.1701 20.25 17.05 20.25H6.95C5.82989 20.25 5.26984 20.25 4.84202 20.032C4.46569 19.8403 4.15973 19.5343 3.96799 19.158C3.75 18.7302 3.75 18.1701 3.75 17.05V6.95ZM9.75 3.75V20.25M3.75 9.75H20.25"></path>

            </svg>


                        Type
                    </p>
                    <p>Serverless Postrgres 17</p>
                </div>

            

            
        </div>

                <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M17.75 19.25L22.5 19.25C22.2294 15.6058 20.3551 12.75 17 12.75C16.5539 12.75 16.134 12.8005 15.7406 12.8966M11.25 7C11.25 8.79493 9.79492 10.25 8 10.25C6.20507 10.25 4.75 8.79493 4.75 7C4.75 5.20507 6.20507 3.75 8 3.75C9.79493 3.75 11.25 5.20508 11.25 7ZM19.75 7.5C19.75 9.01878 18.5188 10.25 17 10.25C15.4812 10.25 14.25 9.01878 14.25 7.5C14.25 5.98122 15.4812 4.75 17 4.75C18.5188 4.75 19.75 5.98122 19.75 7.5ZM1.75 20.25C2.05745 16.0451 4.18738 12.75 8 12.75C11.8126 12.75 13.9425 16.0451 14.25 20.25L1.75 20.25Z"></path>

            </svg>


                                    For individuals and teams
                                </h3>

                                <p>
                                    Great for indie devs, great for businesses. Invite your team members to collaborate on your app, and manage their
                                    permissions with ease.
                                </p>
                            </div>

                <div x-data="{
        active: 'logs',
        get activeButton() {
            return document.getElementById(`${this.active}-button`)
        },
        get indicatorTop() {
            return (
                this.activeButton.getBoundingClientRect()?.top -
                this.$refs.container.getBoundingClientRect()?.top
            )
        },
        get indicatorHeight() {
            return this.activeButton.getBoundingClientRect()?.height
        },
        get indicatorLeft() {
            return (
                this.activeButton.getBoundingClientRect()?.left -
                this.$refs.container.getBoundingClientRect()?.left +
                this.$refs.container.scrollLeft
            )
        },
        get indicatorWidth() {
            return this.activeButton.getBoundingClientRect()?.width
        },
    }">
    <div>
            <h2>Everything you need, included</h2>

            <p>Monitor and debug effortlessly with streamlined logs, commands, metrics, and resource navigation.</p>
        </div>

    <div>
            <div :class="active === 'logs' ? '!opacity-100' : '!opacity-0 pointer-events-none'">
            <div>
                    <p>
                        2025-02-24
                        <span>15:01:32.471</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>...............</span>
                        <span>26.07ms</span>
                        DONE
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:02:42.134</span>
                        UTC
                    </p>
                    <p>
                        <span>INFO: Sending daily digest</span>
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:23.421</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.948</span>
                        UTC
                    </p>

                    <div>
                        <p>
                            <span>Error: failed to upload file to the server</span>
                        </p>

                        <div>
                            <p>
                                class:
                                <span>App\Exceptions\FileUploadException</span>
                            </p>
                            <p>
                                file:
                                <span>/var/www/html/routes/web.php: 22</span>
                            </p>

                            <p>
                                Show stack trace
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M8 10L12 14L16 10"></path>

            </svg>


                            </p>
                        </div>
                    </div>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:04:53.634</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.052</span>
                        UTC
                    </p>
                    <div>
                        <p>
                            <span>INFO: [php artisan queue:work database --quiet] starting service</span>
                        </p>

                        <div>
                            <p>
                                command:
                                <span>php artisan queue:work database --quiet</span>
                            </p>
                            <p>
                                source:
                                <span>cloud-init</span>
                            </p>
                            <p>
                                msg:
                                <span>starting service</span>
                            </p>
                            <p>
                                instance:
                                <span>3</span>
                            </p>
                            <p>
                                time:
                                <span>2025-02-24T15:03:54.948028493Z</span>
                            </p>
                            <p>
                                level:
                                <span>INFO</span>
                            </p>
                        </div>
                    </div>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:04:31.532</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.948</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.948</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.948</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>

            <div>
                    <p>
                        2025-02-24
                        <span>15:03:54.948</span>
                        UTC
                    </p>
                    <p>
                        <span>App\Jobs\UploadFile</span>
                        <span>....................</span>
                        RUNNING
                    </p>
                </div>
        </div>
            <div :class="active === 'commands' ? '!opacity-100' : '!opacity-0 pointer-events-none'" x-cloak="">
            <div>
                        <p>
                            php artisan
                            <span>inspire</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/taylor.png" alt="Taylor Otwell">
                            <span>Taylor Otwell</span>
                            <span>·</span>
                            <span>Just now</span>
                        </p>
                    </div>

            <div>
                        <p>
                            php artisan
                            <span>config:show database</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/joe.png" alt="Joe Dixon">
                            <span>Joe Dixon</span>
                            <span>·</span>
                            <span>4 minutes ago</span>
                        </p>
                    </div>

            <div>
                        <p>
                            php artisan
                            <span>schedule:list</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/nuno.png" alt="Nuno Maduro">
                            <span>Nuno Maduro</span>
                            <span>·</span>
                            <span>8 minutes ago</span>
                        </p>
                    </div>

            <div>
                        <p>
                            php artisan
                            <span>migrate:status</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/taylor.png" alt="Taylor Otwell">
                            <span>Taylor Otwell</span>
                            <span>·</span>
                            <span>1 hour ago</span>
                        </p>
                    </div>

            <div>
                        <p>
                            php artisan
                            <span>about</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/taylor.png" alt="Taylor Otwell">
                            <span>Taylor Otwell</span>
                            <span>·</span>
                            <span>2 hours ago</span>
                        </p>
                    </div>

            <div>
                        <p>
                            php artisan
                            <span>inspire</span>
                        </p>

                        <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/team/taylor.png" alt="Taylor Otwell">
                            <span>Taylor Otwell</span>
                            <span>·</span>
                            <span>4 hours ago</span>
                        </p>
                    </div>
        </div>
            <div :class="active === 'metrics' ? '!opacity-100' : '!opacity-0 pointer-events-none'" x-cloak="">
        <div>
                <p>06:00 PM</p>
                <p>09:00 PM</p>
                <p>12:00 AM</p>
                <p>03:00 AM</p>
                <p>06:00 AM</p>
                <p>09:00 AM</p>
                <p>12:00 PM</p>
                <p>03:00 PM</p>
            </div>

        <div>
                <p>06:00 PM</p>
                <p>09:00 PM</p>
                <p>12:00 AM</p>
                <p>03:00 AM</p>
                <p>06:00 AM</p>
                <p>09:00 AM</p>
                <p>12:00 PM</p>
                <p>03:00 PM</p>
            </div>
    </div>
            
        </div>
</div>

                <div>
                    <div>
                        <h3>
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12 19.75L12 12.25M12 12.25L14.5 14.75M12 12.25L9.5 14.75M15 19.75L18.125 19.75C20.4032 19.75 22.25 17.9032 22.25 15.625C22.25 13.3468 20.4032 11.5 18.125 11.5C18.0814 11.5 18.038 11.5007 17.9948 11.502C17.9983 11.4184 18 11.3344 18 11.25C18 7.93629 15.3137 5.25 12 5.25C9.3716 5.25 7.13793 6.94009 6.32647 9.2928C3.74507 9.62339 1.75 11.8287 1.75 14.5C1.75 17.3995 4.1005 19.75 7 19.75L9 19.75"></path>

            </svg>


                            Push to deploy
                        </h3>

                        <p>You can deploy manually, or automatically on every `git push`.</p>
                    </div>

                    <div>
                        <h3>
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12 21.75C6.89137 21.75 2.75 17.6086 2.75 12.5C2.75 7.39137 6.89137 3.25 12 3.25M16.625 4.48749C17.4767 4.98018 18.2426 5.60477 18.8947 6.33333M16.625 20.5125C17.4767 20.0198 18.2426 19.3952 18.8947 18.6667M21.0751 10.7005C21.1898 11.2826 21.25 11.8843 21.25 12.5C21.25 13.1157 21.1898 13.7174 21.0751 14.2995M12.75 9L16.25 12.5L12.75 16M15.25 12.5H7.75"></path>

            </svg>


                            Deploy hooks
                        </h3>

                        <p>Connect your app to any external service of your choice.</p>
                    </div>

                    <div>
                        <h3>
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M7.75 5.25L7.75 3.25M16.25 5.25L16.25 3.25M9.25 13.275L10.9 14.925L14.75 11.075M3.75 5.25L20.25 5.25L20.25 20.75L3.75 20.75L3.75 5.25Z"></path>

            </svg>


                            Task scheduler
                        </h3>

                        <p>Need a job to run every hour, or every day? Just toggle it on.</p>
                    </div>

                    <div>
                        <h3>
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M1.29022 10.7389L2.982 6.28677C3.4272 5.17379 4.22857 4.28337 5.34159 3.79364L9.70459 1.87926C10.7731 1.38954 11.9751 1.38954 13.0882 1.79022L17.5847 3.482C18.6532 3.88268 19.5436 4.72857 20.0334 5.79707L21.9923 10.1601C22.482 11.2285 22.482 12.4306 22.0813 13.5436L20.345 18.0402C19.9443 19.1087 19.0984 19.9991 18.0299 20.4888L13.6669 22.4477C12.5984 22.9375 11.3964 22.9375 10.2834 22.5368L5.78678 20.8005C4.7183 20.3998 3.82789 19.5539 3.33816 18.4854L1.37926 14.1224C0.889535 13.0539 0.889535 11.8518 1.29022 10.7389ZM16.8782 4.73047L4.23438 6.9565L6.46041 19.6003L19.1042 17.3743L16.8782 4.73047Z" fill="currentColor" stroke="none"></path>

            </svg>


                            Octane
                        </h3>

                        <p>Run your app on Octane, Laravel's high‑performance application server.</p>
                    </div>
                </div>
            </div>

        <div>
                <div>
                    <div>
                        <h2>
                            Security and speed, automatically configured
                        </h2>

                        <p>
                            Cloud provides enterprise-grade security and performance out of the box. No need to configure firewalls, load balancers,
                            or caching layers - it's all done for you.
                        </p>
                    </div>

                    <div>
                        <div>
                            <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M9.39691 5.5C7.62463 6.2104 6.2104 7.62463 5.5 9.39691M14.6031 5.5C16.3754 6.2104 17.7896 7.62463 18.5 9.39691M18.5 14.6031C17.7896 16.3754 16.3754 17.7896 14.6031 18.5M5.5 14.6031C6.2104 16.3754 7.62463 17.7896 9.39691 18.5M9.75 16.75H14.25V21.25H9.75V16.75ZM9.75 2.75H14.25V7.25H9.75V2.75ZM21.25 9.75V14.25H16.75V9.75H21.25ZM7.25 9.75V14.25H2.75V9.75H7.25Z"></path>

            </svg>


                                    Edge network
                                </h3>

                                <p>
                                    Content caching at the edge ensure your application stays fast and reliable for users worldwide.
                                </p>
                            </div>

                            <div>
    <p><img src="https://dxr3k2zm7n01i.cloudfront.net/2fb799b7-7081-4d7d-a16d-7ed758dd4344/images/marketing/edge-network.png" alt="Edge network">
    </p>
</div>
                        </div>

                        <div>
                            <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12 21.25C17.1086 21.25 21.25 17.1086 21.25 12C21.25 6.89137 17.1086 2.75 12 2.75M12 21.25C6.89137 21.25 2.75 17.1086 2.75 12C2.75 6.89137 6.89137 2.75 12 2.75M12 21.25C9.65279 21.25 7.75 17.1086 7.75 12C7.75 6.89137 9.65279 2.75 12 2.75M12 21.25C14.3472 21.25 16.25 17.1086 16.25 12C16.25 6.89137 14.3472 2.75 12 2.75M21 12H3"></path>

            </svg>


                                    Custom domains
                                </h3>

                                <p>
                                    Use your own domain name with your app, and get a free SSL certificate automatically. No need to configure
                                    TSL/SSL.
                                </p>
                            </div>

                            <div>
            <div>
                <p>
                    beep-staging-8g3pxa
                    <span>.laravel.cloud</span>
                </p>

                <div>
                    <p>
                        <svg viewBox="0 0 14 14" xmlns="http://www.w3.org/2000/svg">
                            <path fill-rule="evenodd" d="M7 14C10.866 14 14 10.866 14 7C14 3.13401 10.866 0 7 0C3.13401 0 0 3.13401 0 7C0 10.866 3.13401 14 7 14ZM10.1865 4.1874C9.82121 3.8975 9.28276 3.94972 8.98386 4.30405L5.9191 7.93721L4.95897 7.00596C4.62521 6.68224 4.08408 6.68224 3.75032 7.00596C3.41656 7.32969 3.41656 7.85454 3.75032 8.17827L5.37822 9.75721C5.54896 9.92281 5.78396 10.0106 6.02512 9.99897C6.26629 9.9873 6.49111 9.87723 6.64401 9.69597L10.3068 5.35389C10.6057 4.99956 10.5518 4.47731 10.1865 4.1874Z"></path>
                        </svg>
                        Connected
                    </p>

                    <p>Redirect from www.</p>

                    
                </div>
            </div>

            

            <div>
                <p>beep.today</p>

                <div>
                    <p>
                        <svg viewBox="0 0 14 14" xmlns="http://www.w3.org/2000/svg">
                            <path fill-rule="evenodd" d="M7 14C10.866 14 14 10.866 14 7C14 3.13401 10.866 0 7 0C3.13401 0 0 3.13401 0 7C0 10.866 3.13401 14 7 14ZM10.1865 4.1874C9.82121 3.8975 9.28276 3.94972 8.98386 4.30405L5.9191 7.93721L4.95897 7.00596C4.62521 6.68224 4.08408 6.68224 3.75032 7.00596C3.41656 7.32969 3.41656 7.85454 3.75032 8.17827L5.37822 9.75721C5.54896 9.92281 5.78396 10.0106 6.02512 9.99897C6.26629 9.9873 6.49111 9.87723 6.64401 9.69597L10.3068 5.35389C10.6057 4.99956 10.5518 4.47731 10.1865 4.1874Z"></path>
                        </svg>
                        Connected
                    </p>

                    <p>Redirect from www.</p>

                    
                </div>
            </div>

            <div>
                <p>*.beep.today</p>

                <div>
                    <p>
                        <svg viewBox="0 0 14 14" xmlns="http://www.w3.org/2000/svg">
                            <path fill-rule="evenodd" d="M7 14C10.866 14 14 10.866 14 7C14 3.13401 10.866 0 7 0C3.13401 0 0 3.13401 0 7C0 10.866 3.13401 14 7 14ZM10.1865 4.1874C9.82121 3.8975 9.28276 3.94972 8.98386 4.30405L5.9191 7.93721L4.95897 7.00596C4.62521 6.68224 4.08408 6.68224 3.75032 7.00596C3.41656 7.32969 3.41656 7.85454 3.75032 8.17827L5.37822 9.75721C5.54896 9.92281 5.78396 10.0106 6.02512 9.99897C6.26629 9.9873 6.49111 9.87723 6.64401 9.69597L10.3068 5.35389C10.6057 4.99956 10.5518 4.47731 10.1865 4.1874Z"></path>
                        </svg>
                        Connected
                    </p>

                    <p>Redirect from www.</p>

                    
                </div>
            </div>

            <div>
                <p>getbeep.com</p>

                <div>
                    <p>
                        <svg viewBox="0 0 14 14" xmlns="http://www.w3.org/2000/svg">
                            <path fill-rule="evenodd" d="M7 14C10.866 14 14 10.866 14 7C14 3.13401 10.866 0 7 0C3.13401 0 0 3.13401 0 7C0 10.866 3.13401 14 7 14ZM10.1865 4.1874C9.82121 3.8975 9.28276 3.94972 8.98386 4.30405L5.9191 7.93721L4.95897 7.00596C4.62521 6.68224 4.08408 6.68224 3.75032 7.00596C3.41656 7.32969 3.41656 7.85454 3.75032 8.17827L5.37822 9.75721C5.54896 9.92281 5.78396 10.0106 6.02512 9.99897C6.26629 9.9873 6.49111 9.87723 6.64401 9.69597L10.3068 5.35389C10.6057 4.99956 10.5518 4.47731 10.1865 4.1874Z"></path>
                        </svg>
                        Connected
                    </p>

                    <p>Redirect from www.</p>

                    
                </div>
            </div>
        </div>
                        </div>
                    </div>

                    <div>
                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M9.25 11.5L11 13.25L14.75 9.5M12 2.75L20.25 5.5V11.9123C20.25 16.8848 16 19.25 12 21.4079C8 19.25 3.75 16.8848 3.75 11.9123V5.5L12 2.75Z"></path>

            </svg>


                                DDoS protection
                            </h3>

                            <p>Attacks are automatically detected and mitigated.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M20.25 8.75H13.25V1.75L3.75 15.0473H10.75V22.25L20.25 8.75Z"></path>

            </svg>


                                Content caching
                            </h3>

                            <p>Static assets are cached at the edge, so your app stays fast.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M7.75 15.75L7.75 22.75L12 20.75L16.25 22.75L16.25 15.75M19.25 9.5C19.25 13.5041 16.0041 16.75 12 16.75C7.99593 16.75 4.75 13.5041 4.75 9.5C4.75 5.49594 7.99594 2.25 12 2.25C16.0041 2.25 19.25 5.49594 19.25 9.5Z"></path>

            </svg>


                                SSL and TLS certificates
                            </h3>

                            <p>Cloud automatically provisions and renews SSL certificates for your app.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M3.75 9.25V3.75M3.75 3.75H9.25M3.75 3.75L12 12M14.75 3.75H20.25M20.25 3.75V9.25M20.25 3.75L12 12M12 12V20.25"></path>

            </svg>


                                Load balancing
                            </h3>

                            <p>Cloud automatically balances traffic as your apps scale.</p>
                        </div>
                    </div>
                </div>

                <div>
                    <div>
                        <h2>Scale up and down in your sleep</h2>

                        <p>
                            Cloud scales your app automatically, so you can handle any amount of traffic without breaking a sweat, or the bank.
                        </p>
                    </div>

                    <div>
                        <div>
                            <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12.25 19.25L12.25 13.75C12.25 12.6454 11.3546 11.75 10.25 11.75L2.75 11.75M12.25 19.25L4.75 19.25C3.6454 19.25 2.75 18.3546 2.75 17.25L2.75 11.75M12.25 19.25L19.25 19.25C20.3546 19.25 21.25 18.3546 21.25 17.25L21.25 6.75C21.25 5.6454 20.3546 4.75 19.25 4.75L4.75 4.75C3.6454 4.75 2.75 5.6454 2.75 6.75L2.75 11.75"></path>

            </svg>


                                    Autoscaling
                                </h3>

                                <p>
                                    Configure your compute clusters and Cloud automatically scales horizontally within your predefined limits. You
                                    only pay for what you use.
                                </p>
                            </div>

                            <div>
        <div>
            <div>
                    <p>Unlimited</p>
                    <p>Automatically scale replicas to handle any level of demand.</p>
                </div>

            <p><span>Recommended</span>
        </p></div>

        <div>
                        <p>Custom scale range</p>
                        <p>Set minimum and maximum replica counts.</p>
                    </div>

        <div>
                    <p>No autoscaling</p>
                    <p>Run on a single replica without autoscaling.</p>
                </div>
    </div>
                        </div>

                        <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M15.5 3.4351C14.4204 2.99344 13.2386 2.75 12 2.75C6.89137 2.75 2.75 6.89137 2.75 12C2.75 17.1086 6.89137 21.25 12 21.25C17.1086 21.25 21.25 17.1086 21.25 12C21.25 11.1337 21.1309 10.2952 20.9082 9.5M12 7.75V12L14.5 14.5M18.75 1.75H22.25L18.75 6.25H22.25"></path>

            </svg>


                                    Hibernation
                                </h3>

                                <p>
                                    Hibernation kicks in to save costs when requests are low. Your app will wake up automatically when traffic
                                    returns.
                                </p>
                            </div>
                    </div>

                    <div>
                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12 6.75V8.5M12 8.5C10.067 8.5 8.5 10.067 8.5 12C8.5 13.933 10.067 15.5 12 15.5M12 8.5C13.1155 8.5 14.1091 9.02183 14.75 9.8347M12 15.5V17.25M12 15.5C13.1155 15.5 14.1091 14.9782 14.75 14.1653M21.25 12C21.25 17.1086 17.1086 21.25 12 21.25C6.89137 21.25 2.75 17.1086 2.75 12C2.75 6.89137 6.89137 2.75 12 2.75C17.1086 2.75 21.25 6.89137 21.25 12Z"></path>

            </svg>


                                Cost optimized options
                            </h3>

                            <p>Keep your costs low with hibernating apps and databases and lightweight compute sizes.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M10 10.5L5.5 6M12 5.75L12 3.25C17.1086 3.25 21.25 7.39137 21.25 12.5C21.25 17.6086 17.1086 21.75 12 21.75C6.89136 21.75 2.75 17.6086 2.75 12.5C2.75 11.0672 3.07574 9.71057 3.65722 8.5M14.75 12.5C14.75 14.0188 13.5188 15.25 12 15.25C10.4812 15.25 9.25 14.0188 9.25 12.5C9.25 10.9812 10.4812 9.75 12 9.75C13.5188 9.75 14.75 10.9812 14.75 12.5Z"></path>

            </svg>


                                Performance optimized options
                            </h3>

                            <p>Need more power? Upgrade to larger compute sizes designed for heavy utilization.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M14.2452 3.52438L13.0031 8.49995L9.0081 9.50495L8.0031 11.4999L3.7503 8.31174M14.2452 3.52438C13.5264 3.34514 12.7743 3.25 12 3.25C8.39887 3.25 5.27837 5.30785 3.7503 8.31174M14.2452 3.52438C18.2689 4.52764 21.25 8.16569 21.25 12.5C21.25 17.6086 17.1086 21.75 12 21.75C6.89136 21.75 2.75 17.6086 2.75 12.5C2.75 10.9925 3.11062 9.56923 3.7503 8.31174M15.0029 17.5L16.0029 15.5L13.0549 13.519L11.1119 13.395L10.0029 14.5L12.0029 17.5L15.0029 17.5Z"></path>

            </svg>


                                Multiple regions
                            </h3>

                            <p>Choose a region close to your users to reduce latency and improve reliability.</p>
                        </div>
                    </div>
                </div>

                <div>
                    <div>
                        <h2>
                            Connect it all with one-click resources
                        </h2>

                        <p>
                            Essential services, zero complexity. Launch production-ready databases, caches, and object storage with a single click.
                        </p>
                    </div>

                    <div>
                        <div>
                            <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M21.25 8.25V4.75H2.75V19.25H10.25M22.25 16C19.5 16 18 17.5 18 20.25C18 17.5 16.5 16 13.75 16C16.5 16 18 14.5 18 11.75C18 14.5 19.5 16 22.25 16Z"></path>
            <path d="M6 8H6.01M9 8H9.01M12 8H12.01" stroke-width="2"></path>

            </svg>


                                    Fully managed resources in seconds
                                </h3>

                                <p>
                                    Cloud manages your databases, caches, and object storage for you. No need to worry about backups, scaling, or
                                    maintenance.
                                </p>
                            </div>

                            <div>
                                <div>
                    <p>Add object storage</p>
                    <p>Store images, videos, and more</p>
                </div>

<div>
        <div>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M3.75 6.95C3.75 5.82989 3.75 5.26984 3.96799 4.84202C4.15973 4.46569 4.46569 4.15973 4.84202 3.96799C5.26984 3.75 5.82989 3.75 6.95 3.75H17.05C18.1701 3.75 18.7302 3.75 19.158 3.96799C19.5343 4.15973 19.8403 4.46569 20.032 4.84202C20.25 5.26984 20.25 5.82989 20.25 6.95V17.05C20.25 18.1701 20.25 18.7302 20.032 19.158C19.8403 19.5343 19.5343 19.8403 19.158 20.032C18.7302 20.25 18.1701 20.25 17.05 20.25H6.95C5.82989 20.25 5.26984 20.25 4.84202 20.032C4.46569 19.8403 4.15973 19.5343 3.96799 19.158C3.75 18.7302 3.75 18.1701 3.75 17.05V6.95Z"></path>
            <path d="M14.2266 7.16291C14.524 6.27579 13.4007 5.57824 12.7514 6.28729L7.73231 11.7682C7.21953 12.3282 7.60932 13.2428 8.37725 13.2428H10.7922C10.8743 13.2428 10.9396 13.3251 10.9121 13.412L9.82717 16.8522C9.54697 17.7407 10.6725 18.418 11.3123 17.7072L16.2732 12.1964C16.7789 11.6347 16.3876 10.7274 15.6232 10.7274H13.2083C13.1253 10.7274 13.0598 10.6433 13.0891 10.5559L14.2266 7.16291Z" fill="currentColor" stroke="none"></path>

            </svg>


                <div>
                    <p>Add cache</p>
                    <p>Compatible with the Redis™ API</p>
                </div>
            </div>

        <div>
                    <div>
                        <p>
                            beep_cache
                            <span>
                                Laravel KV Store
                            </span>
                        </p>
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M10 8L14 12L10 16"></path>

            </svg>


                    </div>

                    <div>
                        <p>
                            latterly_cache
                            <span>
                                Laravel KV Store
                            </span>
                        </p>
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M10 8L14 12L10 16"></path>

            </svg>


                    </div>
                </div>
    </div>
                            </div>
                        </div>

                        <div>
                            <div>
                                <h3>
                                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M13.25 5.25L2.75 5.25L2.75 19.75L13.25 19.75M16.2454 5.25L21.2454 5.25L21.2454 19.75L16.2454 19.75M16.2454 5.25L16.2454 3M16.2454 5.25L16.2454 19.75M16.2454 19.75L16.2454 22M8.5 10L11 12.5L8.5 15"></path>

            </svg>


                                    Injected environment variables
                                </h3>

                                <p>
                                    Cloud automatically injects the right Laravel environment variables and handles all the details for you.
                                </p>
                            </div>

                            <div>
        <div>
            <p>Environment variables</p>
            <p>
                Add your environment variables in the format&nbsp;
                <code>KEY=value</code>
                , each on a new line.
            </p>
        </div>

        <p>
            <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
                <path fill-rule="evenodd" d="M6.42152 2.17175C7.13215 0.976605 8.86252 0.976602 9.57315 2.17175L14.5625 10.5629C15.2891 11.7849 14.4084 13.3332 12.9867 13.3332H3.00802C1.58624 13.3332 0.705565 11.7849 1.4322 10.5629L6.42152 2.17175ZM7.9974 5.33317C8.27354 5.33317 8.4974 5.55703 8.4974 5.83317V8.49984C8.4974 8.77598 8.27354 8.99984 7.9974 8.99984C7.72125 8.99984 7.4974 8.77598 7.4974 8.49984V5.83317C7.4974 5.55703 7.72125 5.33317 7.9974 5.33317ZM8.66406 10.3332C8.66406 10.7014 8.36559 10.9998 7.9974 10.9998C7.62921 10.9998 7.33073 10.7014 7.33073 10.3332C7.33073 9.96498 7.62921 9.6665 7.9974 9.6665C8.36559 9.6665 8.66406 9.96498 8.66406 10.3332Z"></path>
            </svg>
            You are overwriting 1 injected variable.
        </p>

        <div>
                <p>
                    <span>APP_ENV</span>
                    =production
                </p>

                <p>
                    <span>APP_KEY</span>
                    =base64:xIWs6t29ShaD+H7pD56L1E1VRgHnfK2BVzbvuw+EPws=
                </p>

                <p>
                    <span>APP_DEBUG</span>
                    =true
                </p>
            </div>
    </div>
                        </div>
                    </div>

                    <div>
                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M23 23.5C23 23.5 20.1823 21.1327 18.303 19.7122C18.774 19.2389 22.5314 19.2387 22.5314 19.2387C21.5918 18.1339 18.7728 15.9242 16.4224 15.4505C15.484 11.6627 11.6301 3.13948 5.6163 3.13948C4.67664 2.35032 2.51542 0.961383 1.38783 1.71898C0.260237 2.47658 1.85766 4.24445 2.79732 5.03368C2.95434 6.29635 3.64426 9.01108 5.14772 9.76868C4.36467 12.294 3.45633 17.3447 6.08738 17.3447C6.4006 16.8712 7.02704 15.6401 7.02704 14.5037C7.81009 16.2397 9.75205 19.7118 11.2555 19.712"></path>

            </svg>


                                MySQL
                            </h3>

                            <p>Launch a MySQL database with a single click.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M12 22.5H13.5C13.5 17 14 14 18 10.5C18 7.66667 16.8 2 12 2M15.5 3.5L20 5L22 12C21.6667 13.1667 19.8 15.3 15 14.5C17 17.5 19.5 17 20 17M12 22.5H10.5C10.5 17 10 14 6 10.5C6 7.66667 7.2 2 12 2M8.5 3.5L4 5L2 12C2.33333 13.1667 4.2 15.3 9 14.5C7 17.5 4.5 17 4 17"></path>

            </svg>


                                Serverless Postgres
                            </h3>

                            <p>Unleash the power of Postgres without managing servers.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M3.75 6.95C3.75 5.82989 3.75 5.26984 3.96799 4.84202C4.15973 4.46569 4.46569 4.15973 4.84202 3.96799C5.26984 3.75 5.82989 3.75 6.95 3.75H17.05C18.1701 3.75 18.7302 3.75 19.158 3.96799C19.5343 4.15973 19.8403 4.46569 20.032 4.84202C20.25 5.26984 20.25 5.82989 20.25 6.95V17.05C20.25 18.1701 20.25 18.7302 20.032 19.158C19.8403 19.5343 19.5343 19.8403 19.158 20.032C18.7302 20.25 18.1701 20.25 17.05 20.25H6.95C5.82989 20.25 5.26984 20.25 4.84202 20.032C4.46569 19.8403 4.15973 19.5343 3.96799 19.158C3.75 18.7302 3.75 18.1701 3.75 17.05V6.95Z"></path>
            <path d="M14.2266 7.16291C14.524 6.27579 13.4007 5.57824 12.7514 6.28729L7.73231 11.7682C7.21953 12.3282 7.60932 13.2428 8.37725 13.2428H10.7922C10.8743 13.2428 10.9396 13.3251 10.9121 13.412L9.82717 16.8522C9.54697 17.7407 10.6725 18.418 11.3123 17.7072L16.2732 12.1964C16.7789 11.6347 16.3876 10.7274 15.6232 10.7274H13.2083C13.1253 10.7274 13.0598 10.6433 13.0891 10.5559L14.2266 7.16291Z" fill="currentColor" stroke="none"></path>

            </svg>


                                Key-value stores
                            </h3>

                            <p>Redis API-compatible key-value store for caching and session storage.</p>
                        </div>

                        <div>
                            <h3>
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                <path d="M20.25 7.75V17.05C20.25 18.1701 20.25 18.7302 20.032 19.158C19.8403 19.5343 19.5343 19.8403 19.158 20.032C18.7302 20.25 18.1701 20.25 17.05 20.25H6.95C5.8299 20.25 5.26984 20.25 4.84202 20.032C4.46569 19.8403 4.15973 19.5343 3.96799 19.158C3.75 18.7302 3.75 18.1701 3.75 17.05V7.75M2.75 3.75H21.25V7.75H2.75V3.75ZM10 11.75H14"></path>

            </svg>


                                Object storage
                            </h3>

                            <p>S3-compatible object storage for your user-uploaded assets.</p>
                        </div>
                    </div>
                </div>
            </div>

        <div>
                    <h2>Trusted by developers, startups, and enterprises</h2>

                    <p>Join thousands of developers and companies around the world</p>
                </div>

        <div>
        <h2>Frequently asked questions</h2>

        <div x-data="{ open: null }">
            
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="What databases does Laravel Cloud support?">
        <p>
                    Laravel Cloud offers serverless Postgres databases that auto-scale with your needs. MySQL support is coming soon.
                    You can also bring your own existing database by configuring your connection credentials. <a href="https://app.laravel.cloud/docs/resources/databases" target="_blank">Read more in our docs</a>.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="What regions are you currently in?">
        <p>Laravel Cloud is available in US East (Ohio), EU West (London), EU Central (Frankfurt), and Asia Pacific (Singapore). We're actively expanding our regional availability.</p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="Is there a free tier?">
        <p>
                    Our Sandbox plan has no monthly fee - you only pay for what you use. Thanks to auto-hibernation, your application automatically
                    scales down to zero when not in use, meaning you don't pay for compute time during idle periods. Each plan includes generous bandwidth
                    allowances. View detailed pricing <a href="https://app.laravel.cloud/docs/pricing" target="_blank">here</a>.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="Is Laravel Cloud serverless?">
        <p>
                    No. Laravel Cloud runs on dedicated AWS EC2 servers, providing predictable performance and costs.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="How does autoscaling work on Laravel Cloud?">
        <p>
                    You set your maximum number of application replicas, and Cloud automatically scales within those limits based on CPU usage.
                    Unlike serverless platforms, you have complete control over your scaling limits, and you only pay for replicas when
                    they're actually running - giving you both flexibility and cost predictability.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="How is Cloud different from Forge?">
        <p>
                    Cloud is a fully-managed platform that handles infrastructure, scaling, and maintenance for you, while Forge gives you direct
                    server control and management tools. See our <a href="https://app.laravel.cloud/cloud-vs-forge">comparison guide</a> for more information.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="How is Cloud different from Vapor?">
        <p>
                    Cloud provides traditional server-based hosting with easy scaling, while Vapor offers serverless Laravel hosting on AWS Lambda and requires AWS Management Console experience.
                    See our <a href="https://app.laravel.cloud/cloud-vs-vapor">comparison guide</a> for more information.
                </p>
    </div>
            <div x-show="expanded" x-collapse.duration.100ms="" x-cloak="" x-data="{
        id: $id('faq-question'),
        get expanded() {
            return this.open === this.id
        },
        set expanded(value) {
            this.open = value ? this.id : null
        },
    }" role="region" question="Can I set spending caps?">
        <p>
                    While Laravel Cloud doesn't have explicit spending caps, your costs are predictable because you have the option to set compute and resource limits upfront.
                    Spending alerts are also coming soon, which will allow you to get notified when your total spend reaches a threshold you set.
                </p>
    </div>
            
        </div>
    </div>

        <div>
            <h2>Ready to ship?</h2>
            <p>Let’s build the incredible together, with Laravel</p>
            
        </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARPA is funding cheap community-owned gigabit fiber to neglected neighborhoods (149 pts)]]></title>
            <link>https://www.techdirt.com/2025/02/24/arpa-is-quietly-funding-cheap-50-65-a-month-community-owned-gigabit-fiber-access-to-long-neglected-neighborhoods/</link>
            <guid>43160196</guid>
            <pubDate>Mon, 24 Feb 2025 14:48:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2025/02/24/arpa-is-quietly-funding-cheap-50-65-a-month-community-owned-gigabit-fiber-access-to-long-neglected-neighborhoods/">https://www.techdirt.com/2025/02/24/arpa-is-quietly-funding-cheap-50-65-a-month-community-owned-gigabit-fiber-access-to-long-neglected-neighborhoods/</a>, See on <a href="https://news.ycombinator.com/item?id=43160196">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-468558">

				


				


				<h3>from the <i>build-it-and-they-will-come</i> dept</h3>
				


				<p>The 2021 American Rescue Plan Act (ARPA) continues to quietly help fund a number of extremely popular community-owned, open access fiber deployments that are challenging entrenched U.S. monopoly power, and driving super cheap, community-owned and operated fiber networks into long neglected towns.</p>
<p>New York State, for example, just leveraged ARPA funds to <a href="https://www.governor.ny.gov/news/governor-hochul-announces-26-million-connectall-investment-expand-affordable-broadband-access">give a $26 million grant to Oswego County</a>. Oswego County is going to use that money to build an open access fiber network. That means multiple ISPs can come in and compete over shared infrastructure owned by the county. Our <a href="https://copia.is/library/just-a-click-away/">Copia report</a> showcased how this model can help disrupt monopoly power and lower broadband costs for users.</p>
<p>The anchor tenant on Oswego County’s new network, Empire Access, will provide locals with 500 Megabit per second (Mbps) service for $50 a month; symmetrical 1 gigabit per second (Gbps) service for $65 a month; and symmetrical 2 Gbps service for $100 a month.</p>
<p>That’s not great news for regional New York State monopolies Charter and Verizon, who’ve grown fat and comfortable charging much higher prices for much slower access. The lack of real competition between the two giants for decades has resulted in high prices, slow speeds, spotty coverage, inconsistent upgrades, repair delays, and substandard customer service.</p>
<p>Charter, you might recall, was <a href="https://www.techdirt.com/2019/04/29/charter-spectrum-wont-get-kicked-out-new-york-state-after-isp-promises-to-suck-less/">almost kicked out the state</a> for lying to regulators about its merger with Time Warner Cable. Verizon similarly has long been under fire for <a href="https://www.techdirt.com/2015/06/16/verizon-says-claims-abandoning-dsl-customers-pure-nonsense-as-company-clearly-busy-abandoning-dsl-customers/">cheaping out on uniform fiber upgrades</a> despite untold millions in taxpayer subsidies. </p>
<p>Meanwhile in Minnesota, Carver County officials say they’ve also been leveraging ARPA funds to deploy affordable gigabit fiber to <a href="https://communitynets.org/content/carver-county-minnesotas-carverlink-closes-100-gigabit-fiber-coverage">every county resident</a>. Their model is slightly different: The city has used grant money to build dark fiber, which they then lease to a company called MetroNet as part of a public-private partnership. MetroNet is offering locals gigabit fiber for prices way less than regional monopolies:</p>
<blockquote>
<p><em>“Metronet currently&nbsp;<a href="https://gometronet.com/internet/">offers four tiers of service</a>&nbsp;with varying promotions, which currently include symmetrical 150 megabit per second (Mbps) fiber for $35 a month; symmetrical 500 Mbps for $45 a month; symmetrical <strong>1 gigabit per second (Gbps) for $50 a month</strong>; symmetrical 2 Gbps for $70 a month; and symmetrical 5 Gbps for $110 a month.”</em></p>
</blockquote>
<p>Again, this kind of stuff doesn’t get much attention from a press that declares infrastructure <strong>too boring</strong> to cover. But this kind of stuff is quietly transformative all the same. It’s also not clear to me why Senate Democrats aren’t competently messaging the impact ARPA funds are having on affordable broadband. Or local community centers, local road improvements, or affordable housing. </p>
<p>Many states try to “address the digital divide” by throwing <a href="https://www.techdirt.com/2024/05/20/pennsylvania-once-again-shows-what-broadband-corruption-looks-like-doles-out-millions-in-dodgy-non-transparent-grants-to-comcast-verizon-in-favored-political-districts/">more and more money into the laps of giant regional telecom monopolies</a> with a long history of subsidy abuse. Many other states are trying to “fix broadband access” by <a href="https://www.techdirt.com/2024/12/09/trump-2-0-to-slather-elon-musks-starlink-with-billions-in-taxpayer-subsidies-it-doesnt-deserve/">throwing money at Elon Musk’s Starlink</a>, ignoring the LEO satellite platform’s capacity constraints, high prices, erratic leadership, and <a href="https://www.space.com/megaconstellations-threat-to-ozone-layer-recovery">problematic environmental impact</a>.</p>
<p>But some states (most notably Vermont, Maine, California, and New York) are trying a different tack: they’re investing heavily in community-owned open access infrastructure, and treating broadband more like an essential utility (where maximizing shareholder profits isn’t the top priority). They’re leveraging an historic infusion of federal funds to put local communities in charge of their own connectivity fate.</p>
<p>Entrenched telecom monopolies, which have worked tirelessly over decades to dismantle broadband competition and state and federal oversight, have worked tirelessly to demonize and undermine community broadband access. But in a decade it should be interesting to see what the data says about the differing approaches. </p>
<p>Keep in mind that states are also poised to receive more than $42.5 <strong>billion</strong> in additional broadband grants courtesy of the 2021 infrastructure bill. That program has significantly more restrictions than ARPA, and there’s every indication that the Trump administration will do its best to <a href="https://www.techdirt.com/2025/02/06/trumps-ntia-pick-prepares-to-redirect-42-5-billion-in-infrastructure-bill-broadband-grants-to-trump-cronies/">redirect as much of that money as possible</a> away from community owned endeavors and toward companies that kiss Trump’s ass. </p>

				
<p>

	Filed Under: <a href="https://www.techdirt.com/tag/arpa/" rel="tag">arpa</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/fiber/" rel="tag">fiber</a>, <a href="https://www.techdirt.com/tag/gigabit/" rel="tag">gigabit</a>, <a href="https://www.techdirt.com/tag/grants/" rel="tag">grants</a>, <a href="https://www.techdirt.com/tag/high-speed-internet/" rel="tag">high speed internet</a>, <a href="https://www.techdirt.com/tag/monopoly/" rel="tag">monopoly</a>, <a href="https://www.techdirt.com/tag/municipal-broadband/" rel="tag">municipal broadband</a>, <a href="https://www.techdirt.com/tag/open-access/" rel="tag">open access</a>
	<br>

	Companies: <a href="https://www.techdirt.com/company/empire-access/" rel="category tag">empire access</a>, <a href="https://www.techdirt.com/company/metronet/" rel="category tag">metronet</a>
</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Impossible-to-Hack' Security Turns Out to Be No Security at All (151 pts)]]></title>
            <link>https://jltee.substack.com/p/new-zealand-companys-impossible-to-hack-security</link>
            <guid>43159544</guid>
            <pubDate>Mon, 24 Feb 2025 13:51:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jltee.substack.com/p/new-zealand-companys-impossible-to-hack-security">https://jltee.substack.com/p/new-zealand-companys-impossible-to-hack-security</a>, See on <a href="https://news.ycombinator.com/item?id=43159544">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div><p>“Beautifully Designed, Easy to Use</p><p><span>Comprehensive Software Solution for implementation &amp; maintenance of ISO Standards and other Compliance Requirements such as H&amp;S, Quality, Environmental, Food Safety, Information Security and many more.” according to their </span><a href="https://teammateapp.com/" rel="nofollow ugc noopener">website</a><span>.</span></p></div><p><span>For those curious about what the company offers, you can check </span><a href="https://teammateapp.com/features" rel="nofollow ugc noopener">https://teammateapp.com/features</a><span>, a special mention to the feature below:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png" width="342" height="101" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:101,&quot;width&quot;:342,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:32298,&quot;alt&quot;:&quot;One of the features offered by Teammate App listed on their website that reads: Risk Management, Manage your safety data from a single, secure, and accessible system.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png&quot;,&quot;isProcessing&quot;:false}" alt="One of the features offered by Teammate App listed on their website that reads: Risk Management, Manage your safety data from a single, secure, and accessible system." title="One of the features offered by Teammate App listed on their website that reads: Risk Management, Manage your safety data from a single, secure, and accessible system." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F030901ee-a119-40d5-87e4-eb17b2a1cfa3_342x101.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption>Teammate App offer for Risk Management that includes data being secure.</figcaption></figure></div><p><span>Another good page to understand the claimed security of Teammate App can be read here: </span><a href="https://teammateapp.com/security-policy" rel="nofollow ugc noopener">https://teammateapp.com/security-policy</a></p><p>On February 11th 2025 I was looking at some servers running exposed databases publicly and noticed a server with almost 200 tables exposed that contained some interesting names.</p><p><span>After a quick look I wasn’t exactly sure who was responsible for the server I was looking at and I was busy with other things so I flagged it to check later and a couple days later I took a better look and through DNS records and data on the exposed tables I confirmed it belonged to </span><a href="https://teammateapp.com/" rel="nofollow ugc noopener">https://teammateapp.com</a><span> and it was one of their live databases.</span></p><p>I checked my logs to see when was the first time I had this flagged as exposed and the first result was from December 3rd 2024.</p><p><span>On February 15th I sent an email to multiple emails from the CEO that were exposed on the database, his personal Gmail and a company email for </span><a href="https://kaizenconsulting.co.nz/" rel="nofollow ugc noopener">https://kaizenconsulting.co.nz</a><span> which according to public records he also owns and had a lot of data exposed here. I also added a couple of Teammate App emails listed on their website.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png" width="1053" height="861" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:861,&quot;width&quot;:1053,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:191517,&quot;alt&quot;:&quot;Redacted screenshot of the first email sent to Teammate App alerting them of their security issue, the email mentions among other things mentioned below, some information I saw exposed on the server.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png&quot;,&quot;isProcessing&quot;:false}" alt="Redacted screenshot of the first email sent to Teammate App alerting them of their security issue, the email mentions among other things mentioned below, some information I saw exposed on the server." title="Redacted screenshot of the first email sent to Teammate App alerting them of their security issue, the email mentions among other things mentioned below, some information I saw exposed on the server." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa73147c3-7b90-4a7a-b730-9e4104bc2666_1053x861.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Alert sent to Teammate App on February 15th with information regarding the exposed server.</figcaption></figure></div><p>The first phrase on that email states the following: </p><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre><span>  </span><em>  " First of all, please do not ignore this email, this is not a scam attempt nor am I trying to sell anything, I am just alerting and looking for help closing down a security issue related to https://teammateapp.com/ "</em></pre></div><p>I have been adding this phrase to the start of my emails recently because companies assume my emails are either scam/phishing attempts or I’m some cybersecurity vendor or whatever trying to sell them some service or product and often ignore my alerts because of it.</p><p>The email was read by someone, I assume the CEO, and less than an hour after it was sent, I could not connect to the exposed server anymore. I did not get any reply back so a few days later I sent a follow up email.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png" width="1034" height="724" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1034,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:136628,&quot;alt&quot;:&quot;Screenshot of the follow up email sent to Teammate App, the contents of the email are explained on the post.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png&quot;,&quot;isProcessing&quot;:false}" alt="Screenshot of the follow up email sent to Teammate App, the contents of the email are explained on the post." title="Screenshot of the follow up email sent to Teammate App, the contents of the email are explained on the post." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F31414391-de2b-49a2-aed1-bab0223de3fc_1034x724.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Follow up email sent asking about their intent regarding notifications to regulators and their clients.</figcaption></figure></div><p>On this email I asked the usual questions I do on my follow ups such as intent regarding notifications, if the company needed me to delay my publication to give them more time to notify anyone and if they wanted to provide an official comment to add to this publication. </p><p>A couple days later I got a reply that is both highly inappropriate and laughable at the same time.</p><p>Teammate App CEO, Sean Banayan, who has the reading comprehension and IT knowledge of a toddler, decided to reply the following:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png" width="1194" height="997" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:997,&quot;width&quot;:1194,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:241867,&quot;alt&quot;:&quot;Email response transcript: Hi Jaye,  This had no impact on anything or anyone and all anyone could see was basic information of *REDACTED* database size etc.  There were few more security layers which would have made any data breach impossible anyway.  Not sure what's your business and what the heck this Proton actually does, but if you don't stop harassing us, I'll get in touch with them to stop you.  Whatever you're selling, we're not interested in purchasing it.  Get it??&quot;,&quot;title&quot;:&quot;Email response transcript: Hi Jaye,  This had no impact on anything or anyone and all anyone could see was basic information of *REDACTED* database size etc.  There were few more security layers which would have made any data breach impossible anyway.  Not sure what's your business and what the heck this Proton actually does, but if you don't stop harassing us, I'll get in touch with them to stop you.  Whatever you're selling, we're not interested in purchasing it.  Get it??&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png&quot;,&quot;isProcessing&quot;:false}" alt="Email response transcript: Hi Jaye,  This had no impact on anything or anyone and all anyone could see was basic information of *REDACTED* database size etc.  There were few more security layers which would have made any data breach impossible anyway.  Not sure what's your business and what the heck this Proton actually does, but if you don't stop harassing us, I'll get in touch with them to stop you.  Whatever you're selling, we're not interested in purchasing it.  Get it??" title="Email response transcript: Hi Jaye,  This had no impact on anything or anyone and all anyone could see was basic information of *REDACTED* database size etc.  There were few more security layers which would have made any data breach impossible anyway.  Not sure what's your business and what the heck this Proton actually does, but if you don't stop harassing us, I'll get in touch with them to stop you.  Whatever you're selling, we're not interested in purchasing it.  Get it??" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe74440fb-1997-4c82-a179-a79f12b4a349_1194x997.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Response from Teammate APP CEO to my follow up email regarding notifications.</figcaption></figure></div><p>Apparently alerting him about a severe security issue with his App and sending a follow up email to try to avoid publishing anything before the company had time to do their own notifications, if that was their intent, means I’m harassing the company.</p><p>Sean was not interested in anything I was selling either (I don’t sell anything and state exactly that on my first contact) and even threatened to stop me if I didn’t stop with the harassment.</p><p>He was also kind enough to lie and claim “There were few more security layers which would have made any data breach impossible anyway.” and only basic information such as database sizes was exposed.</p><p>The email ended with a “Get it??” and in light of his demand that I allegedly stop harassing him and his obvious cluelessness about the security of his own data, I did not reply to that email. The remainder of this post will demonstrate how wildly inaccurate his claims were.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif" width="480" height="270" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:270,&quot;width&quot;:480,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:5007567,&quot;alt&quot;:&quot;A gif that shows the phrases \&quot;It's impossible\&quot; followed by \&quot;It's possible,\&quot; and then \&quot;but it's impossible to ME!\&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif&quot;,&quot;isProcessing&quot;:false}" alt="A gif that shows the phrases &quot;It's impossible&quot; followed by &quot;It's possible,&quot; and then &quot;but it's impossible to ME!&quot;" title="A gif that shows the phrases &quot;It's impossible&quot; followed by &quot;It's possible,&quot; and then &quot;but it's impossible to ME!&quot;" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ed830b7-34c3-4fe9-b553-d7744dd1c10e_480x270.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The next part might come as a shock, to no one, but companies just tell whatever bullshit serves them best, not only to me but to their clients as well, this is a common occurrence.</p><p>Let’s see what “exposed basic information such as database sizes” really means.</p><p>Top 30 exposed tables in terms of record counts:</p><blockquote><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>   715,090 formfieldhistories
   351,007 auditquestioncomments
   348,542 dailymaillogs
   139,419 formfields
   106,454 employeeskills
   100,915 auditformsectionquestions
     98,843 formsubmissions
     90,901 documenthistories
     90,571 tasks
     81,259 documents
     80,680 auditscores
     50,597 formfieldauditlogs
     48,510 risks
     48,079 activityloggers
     47,409 audits
     36,416 auditformsectionscores
     27,245 hazardous
     25,056 formtemplateauditlogs
     23,766 plants
     23,518 formtemplatehistories
     23,279 employees
     20,179 documentacknowledgements
     20,131 employees_16_1,012024
     17,202 auditformsections
     16,719 reassignownerships
     16,641 suppliers
     16,506 vismatelogtimes
     16,474 users
     15,413 employeeassisgnonlinetrainingforms
     15,126 personnels</pre></div></blockquote><p><span>The full table list and sizes can be seen here: </span><a href="https://pastebin.com/8q7CNYBi" rel="nofollow ugc noopener">https://pastebin.com/8q7CNYBi</a></p><p>The database contained a total of 2,963,124 records of exposed data, using around 3.8GB of storage. </p><p>If what Sean wrote to me was the truth, the post would be about done here. In reality it wouldn’t even exist, why would I waste my time reporting exposed database sizes and table counts?</p><p>So let’s analyze some of the tables exposed and the data in them.</p><p>This was the biggest table in terms of records on the DB and didn’t contain any relevant PII that I was aware, it was the updates made to the fields of the various forms companies have on the app. I saw a few links to actual filed forms on some tables and I could check them with no authentication, but I did not look much into it though.</p><p>This table contained usernames, emails, auth tokens and passwords. Around 9,000 users had bcrypt hashed passwords and around 6,000 had auth tokens set.</p><p>This also contained multiple foreign keys to other tables on the database with more information on the users such as companyId, employeeId, supplierId, etc.</p><p>The top 20 email domains exposed on the user table and their counts:</p><blockquote><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>   651 awanuilabs.co.nz
   542 labtests.co.nz
   527 sclabs.co.nz
   368 wellingtonscl.co.nz
   292 jasmax.com
   246 gmail.com
   137 kaizenconsulting.co.nz
   132 medlabsouth.co.nz
   129 gribbles.co.nz
   126 awanuigroup.co.nz
   114 snell.co.nz
   103 culham.co.nz
   100 nz.rlb.com
     98 wadegroup.co.nz
     74 tigerturf.com
     66 cmp.net.nz
     64 zeagoldnutrition.co.nz
     62 taranakipath.co.nz
     62 norpath.co.nz
     58 teammateapp.com</pre></div></blockquote><p><span>A full count of the email domains can be seen here: </span><a href="https://pastebin.com/6L4hb2wL" rel="nofollow ugc noopener">https://pastebin.com/6L4hb2wL</a></p><p>This contained fields such as first and last name, company and workplace foreign keys, email, phone and mobile, date of birth and a field with additional information such as medical recommendations. There were multiple other tables related to employee data such as “employeesppes” which contained PPE (Personal protective equipment) information, mostly uniform sizes.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png" width="1064" height="663" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:663,&quot;width&quot;:1064,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:218081,&quot;alt&quot;:&quot;Redacted example of the employees table with details on injuries and medical issues for an employee of kawekahealth.nz&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png&quot;,&quot;isProcessing&quot;:false}" alt="Redacted example of the employees table with details on injuries and medical issues for an employee of kawekahealth.nz" title="Redacted example of the employees table with details on injuries and medical issues for an employee of kawekahealth.nz" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1573978e-a653-43f3-91d0-c7bfce7762fb_1064x663.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Example of an entry on the employees table.</figcaption></figure></div><p>Top 20 email domains on the employees table:</p><blockquote><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>  3281 gmail.com
    642 awanuilabs.co.nz
    616 mediaworks.co.nz
    607 labtests.co.nz
    596 sclabs.co.nz
    454 hotmail.com
    398 wellingtonscl.co.nz
    312 jasmax.com
    298 yahoo.com
    209 morganprojects.co.nz
    206 xtra.co.nz
    174 vnpf.com.vu
    173 culham.co.nz
    151 kawekahealth.nz
    146 medlabsouth.co.nz
    144 awanuigroup.co.nz
    138 tranzit.co.nz
    138 page-macrae.co.nz
    135 gribbles.co.nz
    123 outlook.com</pre></div></blockquote><p><span>A full count of the email domains can be seen here: </span><a href="https://pastebin.com/5vgz1JKq" rel="nofollow ugc noopener">https://pastebin.com/5vgz1JKq</a></p><p>This table if looked at briefly, probably wouldn’t mean much. There are no actual documents exposed on it, it contains partial paths, filenames, notes and information on who the file belongs to etc through multiple foreign keys to other tables.</p><p>Checking the companyId foreign key to check the companies with most records on the table we get the top 10 being:</p><blockquote></blockquote><p>The counts are only for a single ID, if a company had multiple IDs the file count would be higher all put together, I did not look for such cases.</p><p>I was told about multiple security layers who made a data breach impossible, so of course I had to dig through this table until I found a way to test if the files were actually secure and I guess some layers were currently malfunctioning, as expected, I could actually download the files without any authentication. </p><p><span>The download link still redirected me to </span><a href="https://my.teammateapp.com/login" rel="nofollow ugc noopener">https://my.teammateapp.com/login</a><span> but a request still went through for the file if it still exists in storage. From a small sample, around 75% of the files still exist and can still be accessed without any authentication or login.</span></p><p>The links still work as of publishing this, people would still need to know exactly how to get the working link but the company exposed that publicly for 2 months, who knows how many people got access to that information.</p><p>The sample contained files such as: </p><pre><code>email communications(.msg files), incident reports, site induction questionnares, drivers licences, customer complaint reports, job safety analysis sheets, service reports, workplace safety checklists, job hazard analysis, tool box talk reports, field information reports, consumer feedback spreadsheets.</code></pre><figure data-drag-handle="true" data-component-name="ImageGallery"><div><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F055d16b1-8574-4005-8120-3a591d9a17b3_506x807.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F055d16b1-8574-4005-8120-3a591d9a17b3_506x807.png 474w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F055d16b1-8574-4005-8120-3a591d9a17b3_506x807.png" sizes="100vw" alt="Examples of the files available for download without authentication. Examples show a drivers licence, a Workplace safety checklist from Kaweka Hospital and a consumer feedback spreadsheet from G&amp;H CardioVascular." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F055d16b1-8574-4005-8120-3a591d9a17b3_506x807.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F055d16b1-8574-4005-8120-3a591d9a17b3_506x807.png 474w" width="474"></picture><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff889057c-fe43-4aa2-a9e7-393d71bcd463_931x903.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff889057c-fe43-4aa2-a9e7-393d71bcd463_931x903.png 474w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff889057c-fe43-4aa2-a9e7-393d71bcd463_931x903.png" sizes="100vw" alt="Examples of the files available for download without authentication. Examples show a drivers licence, a Workplace safety checklist from Kaweka Hospital and a consumer feedback spreadsheet from G&amp;H CardioVascular." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff889057c-fe43-4aa2-a9e7-393d71bcd463_931x903.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff889057c-fe43-4aa2-a9e7-393d71bcd463_931x903.png 474w" width="474"></picture><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7566ffc1-134d-4f1d-9d0f-32d41d899a1b_1275x352.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7566ffc1-134d-4f1d-9d0f-32d41d899a1b_1275x352.png 474w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7566ffc1-134d-4f1d-9d0f-32d41d899a1b_1275x352.png" sizes="100vw" alt="Examples of the files available for download without authentication. Examples show a drivers licence, a Workplace safety checklist from Kaweka Hospital and a consumer feedback spreadsheet from G&amp;H CardioVascular." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7566ffc1-134d-4f1d-9d0f-32d41d899a1b_1275x352.png 424w, https://substackcdn.com/image/fetch/w_474,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7566ffc1-134d-4f1d-9d0f-32d41d899a1b_1275x352.png 474w" width="474"></picture></div><figcaption>Examples of exposed files from companies such as Kaweka Hospital and G&amp;H CardioVascular.</figcaption></div></figure><p>There were multiple more file types but I have no interest in downloading around 60,000 files just to prove a point. </p><p>The server IP was scanned by multiple websites that scan for open ports, I noticed it on at least 2 different websites. </p><p>Was anyone looking at the logs to see random IPs connecting and querying for “basic information” for over 2 months? What else were people doing that this supposed security layers missed? I’m sure the company has an answer for that.</p><p>There is likely more data exposed here, this post only reveals a small sample of what was exposed but I can’t dedicate all my free time to analyze it and I’m not interested in doing an in depth security audit on the exposed data to a company who told me not to harass them. </p><p>This post serves to refute the claims of ‘Impossible to hack security’ made by Sean and if you’re a client or employee that uses the App, you might be wondering what else was exposed here. I would tell you to contact Sean or the company for clarification, but be wary you might be “harassing” them if you ask any questions.</p><p>I bet it felt really good and mighty sending that email shitting on me as if I’m some random idiot begging people to buy something, but some advice for you, next time maybe use google to look up what ProtonMail is before claiming you’re gonna report me to my “boss” Proton, but thanks for the laughs on that one. </p><p><span>Also maybe read what I wrote on my email where I mention I’m not selling anything to you, in fact I alerted you of some gross incompetence, free of charge and likely avoided your data from being wiped by some russian running an automated script that wipes everything it connects to, </span><a href="https://infosec.exchange/@JayeLTee/113075636340119294" rel="nofollow ugc noopener">I’ve seen it happen live before on the same service this data was exposed</a><span>. </span></p><p>You might read this post and think that now you are for sure reporting me to my “boss” but you’ll be disappointed to know that I do not work for anyone, so you can’t go and harass my “boss” with stupid claims.</p><p>Those database sizes sure did contain a lot more than just the size, oops.</p><p>And one last thing Sean:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif" data-component-name="Image2ToDOM" rel="nofollow ugc noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif" width="498" height="272" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:272,&quot;width&quot;:498,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2934205,&quot;alt&quot;:&quot;Gif with the phrase \&quot;Did you get it?\&quot; circling around Jonah Hill.&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://jltee.substack.com/i/157546787?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif&quot;,&quot;isProcessing&quot;:false}" alt="Gif with the phrase &quot;Did you get it?&quot; circling around Jonah Hill." title="Gif with the phrase &quot;Did you get it?&quot; circling around Jonah Hill." srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0ab5d1f6-76f4-4606-a97f-da5491a511c6_498x272.gif 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><div data-component-name="PreformattedTextBlockToDOM"><p><label contenteditable="false">Text within this block will maintain its original spacing when published</label></p><pre>
If you’re interested in more incidents I dealt with, you can check all my public finds indexed by country on the post below:</pre></div><div data-component-name="DigestPostEmbed"><a href="https://jltee.substack.com/p/the-hub-of-stupi-misconfigs-index" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d7927a-beaa-40c0-b996-b78818ab0401_1080x1080.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10d7927a-beaa-40c0-b996-b78818ab0401_1080x1080.jpeg" sizes="100vw" alt="The Hub of Stupi... *misconfigs Index" width="140" height="140"></picture></div></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I built an app to stop me doomscrolling by touching grass (1008 pts)]]></title>
            <link>https://touchgrass.now/</link>
            <guid>43158660</guid>
            <pubDate>Mon, 24 Feb 2025 12:15:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://touchgrass.now/">https://touchgrass.now/</a>, See on <a href="https://news.ycombinator.com/item?id=43158660">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Wrongs of Thomas More (106 pts)]]></title>
            <link>https://nealstephenson.substack.com/p/the-wrongs-of-thomas-more-wrong-5</link>
            <guid>43158442</guid>
            <pubDate>Mon, 24 Feb 2025 11:46:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nealstephenson.substack.com/p/the-wrongs-of-thomas-more-wrong-5">https://nealstephenson.substack.com/p/the-wrongs-of-thomas-more-wrong-5</a>, See on <a href="https://news.ycombinator.com/item?id=43158442">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png" width="1063" height="990" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:990,&quot;width&quot;:1063,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1138919,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F943a0406-c694-4562-91e2-4fece746d0f4_1063x990.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>In </span><a href="https://open.substack.com/pub/nealstephenson/p/the-oed-on-wrong-wrong-4?r=7y05q&amp;utm_campaign=post&amp;utm_medium=web" rel="">my previous post</a><span> I talked about spelunking through the Oxford English Dictionary’s definition of “wrong” to see how the usage of that word had developed down through the ages.</span></p><p>Embedded in that definition was a citation that caught my eye. But first I need to point out that “wrong” has many shades of meaning. The particular one to which the following quote applies is: “Not in consonance with facts or truth; incorrect, false, mistaken.” And one of the oldest, and certainly pithiest, examples of this usage is cited as follows:</p><p><strong>1528 MORE Dyaloge III. Wks 210/1</strong><span> </span><em>Our hart euer thinketh the judgement wrong, that wringeth us to the worse.</em></p><p>Now, that one's a beauty because it has one foot in the more ancient meaning of the word, and one in the modern. “Wringeth us to the worse” goes to the older, bending or twisting sense of the word, and means turning or wrenching us off course into a less desirable outcome. “The judgement wrong” refers to an error, a bad call. How do we discern between a right and wrong judgment? Our heart does it (the author, writing in 1528, doesn't draw modern distinctions between the heart and the brain). Evaluating a particular judgment, our heart thinks that it's wrong if its result is that our fate is turned or wrung in a bad direction.</p><p>The author is clearly engaging in wordplay here; he knows the etymology of this word. He’s amusing himself, and perhaps his more erudite readers, with the neat turn of phrase. Thanks to the OED, we less erudite moderns can get the joke too.</p><p>I was so curious about the context of this passage that I began tracking it down in the expectation that it might make for an interesting footnote. Instead I fell into a substantial rabbit hole.</p><p>In the OED, citations are, of necessity, very terse. Otherwise it would be even longer than 20 volumes. So all I knew at first was that the author was someone named More—a reasonably common English name.</p><p>The OED's Bibliography listed the full citation:</p><p>A Dyaloge wherin be treatyd dyvers maters as of the veneration and worshyp of ymagys etc. ( = A dialoge concerning heresyes) 1528 Sir Thomas More</p><p>So it was none other than the original Man for All Seasons, the author of Utopia, High Steward of both Oxford and Cambridge Universities, Lord Chancellor, scourge of heretics, [much later] both a Catholic saint and a Hero of the Russian Revolution, advisor to King Henry VIII, and—in a turnaround reminiscent of the First Trump Administration—publicly beheaded on Tower Hill when he refused to go along with the King's irregular marriage to Anne Boleyn.</p><p>“A Dyaloge Wherin be Treatyd Dyvers Maters” was published in London in 1529 (not 1528 as the OED has it). This is an astonishingly obscure book. Its existence is not mentioned in either More's reasonably thorough Wikipedia entry or his even longer Encyclopedia Britannica biography. I was not able to find an electronic copy on the Internet, which is surprising given the author's prominence.</p><p><span>I was, however, able to purchase, for a cool $150, a physical copy. This is not a modern transcription but a facsimile of the copy in the Bodleian Library in Oxford. It's printed in the blackletter typeface that was common at the time. Obviously, the spelling is half a millennium out of date. Moreover, it has a number of typographical pecularities that make it heavy sledding for the modern reader. The exact page number isn't listed in the OED—this book doesn't even </span><em>have</em><span> page numbers—so I had to read through most of it to find the actual quote. And having found it, I wanted to get a better understanding of the context in which More was saying it.</span></p><p>I expected that this would be a simple matter of translating the preceding couple of paragraphs into modern English. I was badly wrong. The context turns out to be pretty vast.</p><p><span>The overall conceit of </span><em>Dyaloge</em><span>—which is internally divided into four Books—is that More is engaging in a debate with an anonymous correspondent. Or, to be precise with a friend of the correspondent—referred to as “your friend”—who is arguing with More about (according to the title page) “the veneration and worship of images and relics, praying to saints, going on pilgrimage…With many other things touching the pestilent sect of Luther and Tyndale, by the one bygone in Saxony and by the other labored to be brought in to England.”</span></p><p>The book is framed as an extended point/counterpoint exchange between More and “your friend.” We never hear directly from “your friend,” but More summarizes each of the friend's points before serving up a withering counter-argument. It all seems a little like the common social media joke “asking for a friend.” More seems to be offering plausible deniability to whomever he's arguing with.</p><p>Who is “your friend?” Mention is made of a messenger who is going back and forth between wherever More is and “the university,” so we can guess that the person More is arguing with is at Oxford or Cambridge.</p><p><span>It's tempting to identify this group with a circle at Cambridge mentioned in </span><a href="https://en.wikipedia.org/wiki/English_Reformation" rel="">the Wikipedia article about the English Reformation</a><span>, from which I quote: “a group of reform-minded university students that met at the White Horse tavern from the mid-1520s, known by the moniker ‘Little Germany’.”</span></p><p>In this case “Germany” would be a reference to Martin Luther, he of the “pestilent sect” called out on the title page. Luther had authored his 95 Theses only about ten years earlier, in 1517, and had been excommunicated in 1521.</p><p>The Little Germany group at Cambridge included several future Protestant martyrs.</p><p><span>The </span><em>Dyaloge </em><span>creates serious challenges for anyone who is inclined to take an apologetic or sympathetic position vis-a-vis Saint Thomas More, humanist and intellectual. There's a reason it's obscure: it makes Thomas More look like a terrible human being. Anyone who wants to make More look good would want to bury this book. The best you can say about him is that he has a way with words, and he's prolific. But these dialogues most consist of him explaining, with forced patience, to an obvious straw man, that the Roman Catholic Church is infallible, that judges—civil or ecclesiastical—are wise and incorruptible, and that anyone who persistently argues to the contrary is either childish, or so deeply steeped in heresy that burning them alive in public is a perfectly reasonable corrective measure.</span></p><p>Mention is made (approvingly) of “the burnyng of the new testament” (presumably Tyndale's) and the forbidding of the reading of Luther's books.</p><p>“And finally touching the burning of heretics, there were some that thought the clergy therein far out of right order of charity,” More says.</p><p>He isn't being arch or funny there, he seems genuinely bemused that the college kids consider setting humans on fire to be an uncharitable way for clergymen to behave.</p><p>The dialog ranges far and wide, but in the first part of Book 3 they keep circling back to an individual, who is never named, but is referred to as “the man we now talk of,” “the person abjured,” “the man we speak of which was abjured,” and so on. He was called “a good man and very devout.” But, in some way never explained, “superstitious fear and scrupulosity...drove him to the delight of such liberty as brought him to the contempt of the good devout things used commonly in Christ's church.”</p><p>It is all maddeningly vague. Reading between the lines, we can infer that the man in question was a priest gone wrong. He both spoke and wrote certain opinions that were heretical. He belonged to “that sect,” which is never mentioned by name, but is “false.”</p><p>“For he forthwith forsook them and ever before his judges he confessed from the beginning that the matters were plain false heresies and the holders therewith hereticks, saying for himself that he never preached them.”</p><p>In other words, this man broke down under interrogation—presumably including torture—and claimed that he never actually believed in these heresies.</p><p>But according to More, this man “used among some of that sect to say, ‘let us preach and set forth our way. And if we be accused, let us say we said not so, and yet some of them shall we win away the while.’”</p><p>In other words, it was all part of the heretics' pre-arranged plan that if they got caught and interrogated by the authorities, they would claim that they had never preached any heretical doctrine. But in the meantime they might have converted a few other persons to their heretical views.</p><p>“I assure you,” More continues, “to my mind his manner in this matter before his judges was as consonant as could be to that intent and purpose.” He's talking about that pre-arranged plan of claiming innocence when caught and interrogated.</p><p>So, More is telling us that he was personally present at the man's trial; that he witnessed the man pleading innocence; but he saw through the deception.</p><p>Based on circumstantial evidence, it would seem that the man in question was one Thomas Hitton. And by “circumstantial evidence” I refer to the fact that Thomas Hitton was the only heretic burned in the year 1529. Early that year—the year that More's book was published, and the year More become Henry VIII's Lord Chancellor—Hitton had been arrested at Gravesend. Before then he had been in exile along with other English Protestants in the Low Countries. He had crossed over to England on a clandestine mission related to smuggling unauthorized translations of the New Testament. After a series of interrogations he was burned at the stake.</p><p>No other Protestant “heretic” that I can find in the historical record matches this chronology so well, and so I can only assume that More is writing this Dialog in an effort to sway the opinions of heresy-curious academics such as the White Horse Tavern “Little Germany” crowd. Or, barring that, to publicly set forth his view on why they are in the wrong.</p><p>So, let's sum up the situation as far as I can reconstruct it.</p><p>More is a devout, sincere Catholic who, earlier in his life, came this close to becoming a Carthusian monk, but instead went into the legal profession. As a matter of fact, he's so sincere that six years from now he's going to have his head chopped off because he won't support an un-Catholic royal marriage.</p><p>At the same time, he's friends with people like Erasmus, who wants to reform the Catholic Church from within. And apparently he's also in communication with the White Horse Tavern crowd at Cambridge, or people like them. He thinks they are wrong, but persuadable.</p><p><span>In 1528 or 9, when he's writing this </span><em>Dyaloge</em><span>, he's on the threshold of being Lord Chancellor. Or it may have happened already; it all depends on specific dates of when he wrote the </span><em>Dyaloge</em><span>. Over the course of 1529, Cardinal Wolsey, his predecessor, was getting into trouble with Henry VIII and at some point was clearly on his way out. So even if More, in the </span><em>Dyaloge</em><span>, isn't yet writing in his official capacity as Lord Chancellor, he's almost certainly writing as one who's in the running to get the job soon.</span></p><p>By his own admission in these pages, he has been present at the interrogation, by a Church court, of the heretic Thomas Hitton. This was part of a legal proceeding that led to Hitton's being burned at the stake.</p><p><span>More is taking time out from what is probably a busy schedule to write a lengthy and abstruse </span><em>Dyaloge </em><span>to a quasi-imaginary friend at Cambridge or Oxford who seems to be exhibiting sympathy for the budding English Reformation and who takes a dim view of Churchmen burning heretics.</span></p><p><span>Who is More writing this for? Definitely not a novelist 500 years in the future who is curious about the etymology of “wrong.” This is for a small and select audience, many of whom More probably knows by name. The population of England and Wales at this point is maybe three and a half million: a good-sized city in the United States today. Of those, only a small percentage can read. Those people—the literate nobles, clergy, and city dwellers—are More's only possible audience for this </span><em>Dyaloge</em><span>. The whole vibe of the document is precious, coy, cliquish. Clearly, everyone who reads it knows exactly who “the man we now talk of” is. It's not even necessary to mention his name; to do so would just be tacky. The person More's arguing with is addressed through multiple layers of indirection. More doesn't talk to his counterpart straight out. Instead there's an unidentified “messenger” who goes back and forth between More and his contact at “the university” where the contact has been in touch with “your friend.” Again, it all reeks of some kind of controversy within an in-group of people who all know each other but don't want to name names.</span></p><p>More is staking out a position here. If he's not already Lord Chancellor, he suspects he’s likely to be soon. The Protestant Reformation is creating trouble all over the place. He knows which side he's on. The King of England is a Trumpian figure. More works for him as an attack dog going after the likes of Martin Luther. He's a John Bolton type: a sincere true believer who gets recruited because he's passionate about what he believes. But, precisely because he's more sincere than his boss, it's not going to end well.</p><p>Now a priest-turned-heretic has been arrested, probably tortured—possibly while More was nearby—and convicted, leading to the first burning at the stake of an English Reformation heretic. Several more such burnings will come soon, all of them under More's jurisdiction as Lord Chancellor. More has to stake out a position, and he has to do so “publicly” where the “public” in this case is a few thousand literate Englishmen who actually care about such things.</p><p>This all gets us to the threshold of Chapter 3, which, on the surface, is mostly about judicial proceedings. A little prologue—what we would now call a TL;DR—says “The author [More] shows that men ought not to be light in mistrusting of any judgment given in the court (i.e. they should not be quick to assume that a court's judgments are false). And that much less ought any man to be bold in the reproving of a common law. And he shows also the cause why that the law admitteth more slight witness in heinous criminal causes than in slighter matters of covenants or contracts.”</p><p>The chapter has to be read on two levels. It's about judges, witnesses, evidence, the written law, and how courts and justice ought to work in general. But it's also specifically about the case of the doomed heretic I'm thinking is Thomas Hitton.</p><p>In this chapter “your friend” is expressing doubt that “the man we now talk of”—whom he apparently holds in high personal regard—was righteously convicted. “Your friend” complains that the convicted man is virtuous and decent.</p><p>More replies that even if “your friend” has a high opinion of the convicted man's virtue, that doesn't give just cause to doubt the honesty of the judge. More goes on to point out that judges are chosen for their impartiality and that they don't have a financial stake in the outcome of cases and so it's unfair to doubt their judgments unless you have “plain and sure information” that the judgment is wrong.</p><p>Which is fair enough, as far as it goes; character witnesses don't pull a lot of weight, in court, if the evidence points toward guilt.</p><p>Here’s a scan of the section in question with the relevant quote highlighted. Following that is my attempt to transliterate it into modern spelling.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png" width="800" height="755" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:755,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:997224,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83d453ef-4a74-4564-85f3-6e85bfb859a9_800x755.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>“And yet is it as Aristotle saith well done indeed to make the laws so sufficient that as few things as may shall remain and be left to the discretion of the judge [illegible] that the common laws be constantly made by many more that are the particular judges and also many such as are as wise as judges. And over that the laws be to the judges a sure and substantial shield to defend and keep them from the hatred and obloquy that else would follow their sentence on the one side or the other, were their judgement never so wise. For men be so partial always to themselves that </span><em>our heart ever thinketh the judgement wrong that wryngeth us to the worse</em><span>. For be it never so right, all reckon we wrong whereof we feel harm. But yet of all things specially the law should best content us for that it is farthest out of all cause of such suspicion. For whereas a judge medleth with a matter present and persons who he sees and knows whereby there may perchance favor, hatred, hope or dread, pity, cruelty, mede, request, or some other affection incline him to mis-order himself in the matter, the laws always be made for the punishment of things only that are yet to come, and who shall fall in that peril the makers cannot tell: haply their foes, haply their friends, (and as men’s manners be mutable) peradventure themselves. For which cause the makers of the law made by the people in causes criminal can be but indifferent.”</span></p><p><span>It's easy to read these words as the abstract, philosophical musings of a bien-pensant humanist—the proto-Communist author of Utopia who later became a Catholic saint. But in the same year that the above words were printed, their author become Henry VIII’s Lord Chancellor—the highest judicial post in England and Wales. During Thomas More’s chancellorship, six Protestants were burned at the stake for the crime of heresy. And this is not just coincidental. More by this point was an inveterate heretic-fighter who had personally assisted Henry VIII in writing a polemic aimed at Martin Luther. Luther’s response had then triggered what we would now call a flame war, with More writing amazingly scatological, abusive and threatening attacks on Luther under an assumed name, just like a modern-day Redditor. Earlier in “</span><em>A Dyaloge</em><span>,” More talks at length about his Protestant-fighting efforts. So, his words in the above passage have to be read both as a general, abstract statement about the importance of a clear and specific legal code to the maintenance of a fair and trusted judiciary, and a way of preparing himself to hear the screams of heretics dying at the stake as punishment for their thought crimes. More believes in absolute right and wrong strongly enough to see people burned alive for it. But being who he is, he needs a theory of right and wrong that’s robust enough to back that up.</span></p><p><span>To reference a common Internet meme: that escalated quickly. All I wanted was to track down a particularly delicious play on “wrong” and “wring” cited in the OED, and in no time I found myself embroiled in a minor detective story that completely destroyed any illusions I might ever have entertained about the character of Thomas More. He was a monster. To be clear, I am not one of those who likes to hold historical persons to modern standards. But by his own admission in </span><em>Dyaloge</em><span>, More was in communication with peers at Oxford or Cambridge who were expressing (to put it mildly) reservations about the practice of burning thought criminals alive, and questioning whether the Church was infallible. He could have simply agreed with them. Instead he hardened his heart and made a career of setting people on fire.</span></p><p>Thomas More was as intellectually gifted as anyone who ever lived. It's not like he was worried that the conversation was getting over his head. There was no argument he wasn't smart enough to follow, no debater who could outscore him on points. But in 1529 he's been shouldering serious real-world responsibilities for a while, and he's getting ready to become the chief of the British judiciary. I think he has made an internal decision: these religious debates are fine for academics, but I've got a job to do, decisions to make, actions to take, and so it all has to be reduced to simple terms that we must all abide by.</p><p><span>Why does he make the decision that he does, though? The picture that comes through clearly in </span><em>Dyaloge </em><span>is of a brilliant and capable man who has made a choice to debase himself at the altar of the Roman Catholic Church. We know that he was in the habit of wearing a hair shirt. This is a term that nowadays is only used in a metaphorical sense. Thomas More was doing it for real. He could afford comfortable clothes made of good fabric. But underneath those, against his skin, he chose to wear rough, itchy fabric as a form of self-punishment and penitence.</span></p><p><span>I'll leave the armchair psychoanalysis as an exercise for the reader. Some caution is warranted. But clearly it was important for Thomas More to subordinate himself totally to his religious beliefs. The brain that was capable of learning so many things and dealing with so many ideas had to be bridled, humbled, and harnessed to an overwhelmingly powerful institution with a clear and strong hierarchy that led ultimately to the Pope and thence to God. This is precisely the argument that More is making in the pages of the Dyaloge. His opponent, “your friend,” is trying to make the argument that scripture alone is enough to lead us to salvation. More counters that both scripture and the Church—the one Church—are necessary. He hates the idea that random individuals can read scripture—translated by the likes of Luther and Tyndale into their own languages—and find true religion. It looks like anarchy to him. Submission to authority must be part of the picture. Any intelligent person will counter “what if the authority is wrong?” and so More is forced to support the position that the Church </span><em>can't </em><span>be wrong.</span></p><p>No reasonable human, then or now, believes that there's any institution, made up of fallible humans, that's never wrong. When More comes out in support of that position, he's putting on an intellectual hair shirt. That's his act of self-degradation. All of his learning and brilliance are being publicly humiliated and subordinated to an authority. It's the very publicness of that subordination, the blatantness of it, that is at the basis of authoritarianism, then and now. When otherwise well-informed and intelligent persons come out in favor of a Hitler, a Mussolini, a Trump, or any other authoritarian figure, they're not really claiming that they believe everything the boss says. No one could believe that. They're making a public gesture of submissiveness. And the more outrageous the leader's lies, the greater the humiliation, the more profound the submission. If you are psychologically predisposed to be submissive, then there is pleasure in the submitting; and once it's done, it gives you license to burn your enemies with a clear conscience.</p><p><span>Four centuries after his death, within a span of 27 years, More was made a Roman Catholic saint </span><em>and </em><span>a Hero of the Russian Revolution. In 1918, in the Soviet Union, his name was carved on a monument along with those of other founders of the Communist movement. Catholic sainthood came in 1935. In most ways the Soviet Union and the Roman Catholic church could hardly be more different. They were, of course, blood enemies. But both are among the most centralized, top-heavy, and authoritarian institutions that humans have ever created.</span></p><p>This series of posts is about being wrong—about taking the wrong turn at some point or other, leading to wrong conclusions and wrong actions. It's about errors in thinking. But the whole point of becoming an authoritarian is that you are releasing yourself from the obligation to think—from the stress of it, the work, the fear that you might be wrong.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I scrape Steam data every month and it's yours to download for free (146 pts)]]></title>
            <link>https://www.gginsights.io</link>
            <guid>43158425</guid>
            <pubDate>Mon, 24 Feb 2025 11:43:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gginsights.io">https://www.gginsights.io</a>, See on <a href="https://news.ycombinator.com/item?id=43158425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img alt="image" loading="lazy" width="32" height="32" decoding="async" data-nimg="1" src="https://www.gginsights.io/logo.svg"></p><h3>Save over 120 hours of scraping</h3><p>Every month we scrape Steam data for you. So don't worry about having to create scripts, massage the data and wait for the results. We will do it for you. Rather, jump right into analyzing the data.</p></div><div><p><img alt="image" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" src="https://www.gginsights.io/images/experiences/img1.png"></p><p><strong>Leverage the power of AI</strong> to help answer your questions about the Steam market and become a data expert, transforming data into actionable insights.</p></div><div><p><img alt="image" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" src="https://www.gginsights.io/images/experiences/img2.png"></p><p>Need numbers to add to a pitch deck? Help your <strong>potential publisher understand the value</strong>. It will also help you speak with confidence knowing you have the data to back you up.</p></div><div><p><img alt="image" loading="lazy" width="60" height="50" decoding="async" data-nimg="1" src="https://www.gginsights.io/images/experiences/img3.png"></p><p><strong>Be strategic</strong>. Don't regret not analyzing the market holistically before you start working on your game. Stop wasting time on assumptions, rather, take into account the competition, the market size, the value of each tag, etc.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Ketamine Neurotoxic? (164 pts)]]></title>
            <link>https://desmolysium.com/ketamineneurotoxic/</link>
            <guid>43158292</guid>
            <pubDate>Mon, 24 Feb 2025 11:26:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://desmolysium.com/ketamineneurotoxic/">https://desmolysium.com/ketamineneurotoxic/</a>, See on <a href="https://news.ycombinator.com/item?id=43158292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="1109504" data-element_type="widget" data-widget_type="theme-post-content.default">
					
<p>There are dozens of animal studies that show unequivocally that ketamine is neurotoxic. There is a common myth that ketamine is “neurogenetic”. This is mostly false. It increases neuroplasticity temporarily (days to weeks), which though is very different from neurogenesis (the birth of new neurons) – discussed in more detail in the next section.</p>



<p>At recreational doses, ketamine is addictive, destroys the bladder, and is toxic to the central nervous system. Microscopic lesions (including but not limited to so-called “Olney’s lesions”) in both grey and white matter are evident already after three months of high-dose ketamine use.</p>



<p>By year three of heavy use, MRI scans of ketamine users often reveal a brain that looks like a mix between multiple sclerosis (white matter lesions) and Alzheimer’s disease (grey matter atrophy).</p>



<figure><img fetchpriority="high" decoding="async" width="656" height="440" src="https://desmolysium.com/wp-content/uploads/2023/07/image-492.png" alt="" srcset="https://desmolysium.com/wp-content/uploads/2023/07/image-492.png 656w, https://desmolysium.com/wp-content/uploads/2023/07/image-492-300x201.png 300w" sizes="(max-width: 656px) 100vw, 656px"></figure>



<figure><img decoding="async" width="1024" height="246" src="https://desmolysium.com/wp-content/uploads/2023/10/keta-1024x246.jpg" alt="" srcset="https://desmolysium.com/wp-content/uploads/2023/10/keta-1024x246.jpg 1024w, https://desmolysium.com/wp-content/uploads/2023/10/keta-300x72.jpg 300w, https://desmolysium.com/wp-content/uploads/2023/10/keta-768x185.jpg 768w, https://desmolysium.com/wp-content/uploads/2023/10/keta-1536x370.jpg 1536w, https://desmolysium.com/wp-content/uploads/2023/10/keta.jpg 2020w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Ketamine <em>abuse</em> is well known to cause widespread cognitive and neurological impairment. However, the average ketamine “user” consumes 500-1000mg of ketamine per day, which is a lot more than therapeutic use (0.5mg/kg). One glass of wine per week is surely different from a gallon of vodka per day.</p>



<p>Nonetheless, it’s widely recognized that individuals who occasionally use ketamine find they need significantly higher doses than when they first started, even after prolonged periods of abstinence.</p>



<p>For instance, after a few months of using therapeutic doses of ketamine (around 0.5mg/kg per week), I noticed that its impact was much less pronounced compared to my initial experiences with the substance. Even extended breaks did not bring back my virgin response – <a href="https://desmolysium.com/recreational/#mdma" data-type="post" target="_blank" rel="noreferrer noopener">in an analogous way that MDMA users never recapture the magic of their first few trips</a>.</p>



<p>This form of “permatolerance” may point to neurotoxicity – in the same (but mechanistically different) way it does with MDMA.</p>



<p>Researchers are still unsure whether it is safe to use ketamine at low therapeutic doses at reasonable frequencies for a couple of weeks to a couple of months. However, in the case of otherwise untreatable depression, the risks and side effects need to be weighed against the risks and side effects of ongoing depression. Furthermore, depression itself is quite damaging to brain health itself.</p>



<p>I looked into this matter quite extensively myself and I also talked to a handful of university psychiatrists who have been using ketamine on their patients for years – long before Spravato was available. Every single one of them believes that ketamine is quite neurotoxic but in contrast to otherwise untreatable depression the lesser evil.</p>



<p>There is one study that looked at ketamine administration long-term and did not find evidence of neurocognitive decline. However, they were using clinical tests, which are not very sensitive and to reach statistical significance quite a bit of degeneration has had to happen. Furthermore, these were depressive subjects and if you treat depression, neurocognition usually increases. So, if ketamine really treated their depression (which it reportedly did), should there not be an <em>increase </em>in cognition between baseline and timepoint X? Possibly, neurocognitive domains did improve but the gains were erased by gradual neurotoxicity.</p>



<p>I discuss ketamine, and my personal experience with it, in more detail <a data-type="post" data-id="3364" href="https://desmolysium.com/ketamine/" target="_blank" rel="noreferrer noopener"><strong>here</strong></a>.</p>



<div><p><strong><em>Subscribe to the Desmolysium newsletter and get access to three exclusive articles!</em></strong></p></div>



<h2>But isn’t ketamine neurogenetic?!</h2>



<p>No, it is not. It increases <em>neuroplasticity</em>, which, for practical purposes is limited to the generation of dendritic spines (spinogenesis) and synapses (synaptogenesis). Spinogenesis and synaptogenesis are very different from <em>neurogenesis</em> (the birth of new neurons). Neurogenesis is limited to two sites in the human brain and a local creation of neurons in these sites cannot make up for the death of functionally integrated and possibly extensively connected neurons elsewhere.</p>



<p>Synaptogenesis and spinogenesis are short-lived (weeks) but dead neurons will never undie.</p>



<p>Given that Spravato (esketamine nasal spray) has been widely available and so-called “ketamine-clinics” proliferated, we are possibly going to have an answer to the question regarding whether and the extent of ketamine neurotoxicity soon. In the meantime, it is probably prudent to err on the side of caution.</p>



<p>My gut feeling is that people who are doing ketamine at even therapeutic doses regularly are more likely than not to give themselves non-negligible amounts of irreversible brain damage.</p>



<p>Unfortunately, ketamine has become a frequent drug of abuse at doses far higher than therapeutic doses. Similar to MDMA (although by a completely different mechanism), the neurotoxicity caused by high doses is irreversible. It is also quite worrying to see ketamine clinics mushrooming everywhere, with some patients reportedly going in twice per week and being given infusions of 100mg or more. As so often, hype and industry are about to destroy something that was originally quite useful in select cases.</p>



<p>For a related article: <a href="https://desmolysium.com/neurotoxicityofamphetamine/" target="_blank" rel="noreferrer noopener">“Adderall/Vyvanse Changed My Life”…But Will It Wreck Your Brain?</a></p>



<h4>Sources &amp; further info</h4>



<ul>
<li><strong>Scientific article:</strong> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8461018/">Ketamine: Neuroprotective or Neurotoxic?</a></li>



<li><strong>Scientific study:</strong> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713393/">Brain damages in ketamine addicts as revealed by magnetic resonance imaging</a></li>



<li><strong><span>Scientific article:</span></strong><a href="https://www.tandfonline.com/doi/abs/10.1080/14740338.2022.2071867?journalCode=ieds20"> A review of potential neuropathological changes associated with ketamine</a></li>
</ul>







<h4><strong>Disclaimer</strong></h4>



<p><em>The content available on this website is based on the author’s individual research, opinions, and personal experiences. It is intended solely for informational and entertainment purposes and does not constitute medical advice. The author does not endorse the use of supplements, pharmaceutical drugs, or hormones without the direct oversight of a qualified physician. People should never disregard professional medical advice or delay in seeking it because of something they have read on the internet.</em></p>



<p><strong><em>The above is only a fraction of the article. This article is currently undergoing final revisions and is expected to be published within the next few weeks to months. To receive a notification upon its release, sign up for my newsletter.</em></strong></p>



<div><p><strong><em>Subscribe to the Desmolysium newsletter and get access to three exclusive articles!</em></strong></p></div>




				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Says It Will Add 20k Jobs, Spend $500B, Produce AI Servers in US (542 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-02-24/apple-says-it-will-add-20-000-jobs-spend-500-billion-produce-ai-servers-in-us</link>
            <guid>43158168</guid>
            <pubDate>Mon, 24 Feb 2025 11:05:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-02-24/apple-says-it-will-add-20-000-jobs-spend-500-billion-produce-ai-servers-in-us">https://www.bloomberg.com/news/articles/2025-02-24/apple-says-it-will-add-20-000-jobs-spend-500-billion-produce-ai-servers-in-us</a>, See on <a href="https://news.ycombinator.com/item?id=43158168">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-02-24/apple-says-it-will-add-20-000-jobs-spend-500-billion-produce-ai-servers-in-us: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[What's New in Emacs 30.1? (144 pts)]]></title>
            <link>https://www.masteringemacs.org/article/whats-new-in-emacs-301</link>
            <guid>43158164</guid>
            <pubDate>Mon, 24 Feb 2025 11:05:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.masteringemacs.org/article/whats-new-in-emacs-301">https://www.masteringemacs.org/article/whats-new-in-emacs-301</a>, See on <a href="https://news.ycombinator.com/item?id=43158164">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="primary-article">
<p>Emacs 30.1 is upon us, and it’s time for another bumper release full of new features and improvements.</p>
<p>As always, I have taken the time to go through most of the changes in Emacs 30.1 and annotated them to give perspective and clarity.</p>
<p>Here is but a few of the highlights in 30.1:</p>
<ul>
<li><p><code>completion-preview-mode</code> is a tantalizing replacement for company mode and corfu as it uses a buffer and window system (not unlike the <code>*Completions*</code> window when you minibuffer complete with <code>TAB</code>), and with an <em>excellent</em> user experience. It even has the same schnazzy predictive typing that the old auto complete package did!</p>
<p>You also get to leverage the full force of Emacs’s expansive set of minibuffer completion styles, orders and more.</p>
<p>This is my favorite feature in Emacs 30.1. The user experience is superb; someone really cared for this, and it shows.</p></li>
<li><p>The Org URI protocol should now register automatically, meaning you can send data from a browser bookmarklet straight into org capture in your running Emacs instance.</p>
<p>The org protocol feature’s not new to Emacs 30.1, but automatically configuring your Emacs on Linux so it works properly <em>is</em>.</p></li>
<li><p>Which-key is now built in, which is good news for people who prefer that method of help (as opposed to typing <code>C-h</code> in a key chord).</p></li>
<li><p><code>libjansson</code> is dead; long live Emacs’s builtin JSON parser. The new parser is much faster.</p></li>
<li><p>The tree-sitter-specific <code>*-sexp</code> commands (such as <code>forward-sexp</code>) now work well.</p></li>
<li><p>Native compilation is on by default, provided your system has the required dependencies. A massive vote of confidence for a great feature.</p></li>
<li><p>Full support for Emacs on Android, and a whole host of touch screen-related improvements to Emacs as a result.</p></li>
<li><p>You can now request a diff of your multi-file replace regexp (see <code>C-h a replace diff</code>)</p></li>
<li><p>More Tramp (Emacs’s remote file editing system) connections, including support for Kubernetes pods.</p></li>
</ul>
<p>I will, as per usual, begin to update <a href="https://www.masteringemacs.org/book">my book, Mastering Emacs</a>, so it includes these latest changes.</p>
<p>Until then, and for the next couple of days, you can enjoy a 30% discount on my book. Enjoy!</p>
<h2 id="installation-changes-in-emacs-30.1">Installation Changes in Emacs 30.1</h2>
<pre><code>Native compilation is now enabled by default.
'configure' will enable the Emacs Lisp native compiler, so long as
libgccjit is present and functional on the system.  To disable native
compilation, configure Emacs with the option:

    ./configure --with-native-compilation=no</code></pre>
<p>That is good news. I’m hoping by declaring that native compilation, a must-have feature that <a href="https://www.masteringemacs.org/article/speed-up-emacs-libjansson-native-elisp-compilation">compiles elisp into native code</a> and greatly speeds up your Emacs, it encourage more distros to adopt the setting, if they have not already. If you’re the type to compile your own Emacs, you may want to read me article as it does require a few mandatory components that most Linux distributions do not ship with.</p>
<pre><code>Emacs has been ported to the Android operating system.
This requires Emacs to be compiled on another computer.  The Android
NDK, SDK, and a suitable Java compiler must also be installed.

See the file "java/INSTALL" for more details.</code></pre>
<p>I’ve been following this development for quite a while, and it’s wonderful to see that you can run a full-blown Emacs instance on Android <em>and</em> with the support of the Emacs developers behind it. How exciting. I’ll have to find the time to play around with it.</p>
<pre><code>Native JSON support is now always available; libjansson is no longer used.
No external library is required.  The '--with-json' configure option has
been removed.  'json-available-p' now always returns non-nil and is only
kept for compatibility.</code></pre>
<p>JSON is a massive bottleneck when you’re using LSP servers, as LSPs are chatty and JSON is how they communicate. I wrote about the introduction of <a href="https://www.masteringemacs.org/article/speed-up-emacs-libjansson-native-elisp-compilation">libjansson</a> when it was first introduced as it made a noticeable difference in performance back when it made an optional dependency in Emacs. It turns out that libjansson is not that quick. One day, a new contributor, Géza Herman, showed up with a JSON parser that was not only faster than libjansson, but more complete in its implementation.</p>
<p>As a result libjansson is not only deprecated but removed entirely. Excellent!</p>
<pre><code>Emacs now defaults to the ossaudio library for sound on NetBSD and OpenBSD.
Previously, configure used ALSA libraries if installed on the system
when configured '--with-sound=yes' (which is the default), with fallback
to libossaudio.  The libossaudio library included with the base system
is now used even if ALSA is found to avoid relying on external packages
and to resolve potential incompatibilities between GNU/Linux and *BSD
versions of ALSA.  Use '--with-sound=alsa' to build with ALSA on these
operating systems instead.</code></pre>
<pre><code>New configuration option '--disable-gc-mark-trace'.
This disables the GC mark trace buffer for about 5% better garbage
collection performance.  Doing so may make it more difficult for Emacs
developers to help finding GC-related bugs that you run into, which is
why the mark trace buffer is enabled by default.</code></pre>
<p>If you’re the type of person to compile your own Emacs, you might as well give this one a whirl. GC bugs do happen, but I’d say they’re rare enough that you can turn it off and claim your 5% speed-up voucher.</p>
<h2 id="startup-changes-in-emacs-30.1">Startup Changes in Emacs 30.1</h2>
<pre><code>On GNU/Linux, Emacs is now the default application for 'org-protocol'.
Org mode provides a way to quickly capture bookmarks, notes, and links
using 'emacsclient':

    emacsclient "org-protocol://store-link?url=URL&amp;title=TITLE"

Previously, users had to manually configure their GNU/Linux desktop
environment to open 'org-protocol' links in Emacs.  These links should
now open in Emacs automatically, as the "emacsclient.desktop" file now
arranges for Emacs to be the default application for the 'org-protocol'
URI scheme.  See the Org mode manual, Info node "(org) Protocols" for
more details.</code></pre>
<p>OK, no lie, this is a really cool feature that just got easier to use.</p>
<p>Org can register (on Windows, Linux, etc.) itself as a URI protocol controller. When you open a <code>https</code> hyperlink, your OS knows it has to use your preferred browser; ditto for e-mail. Here it’s called <code>org-protocol</code>, and with this new change (or the slightly more manual steps outlined <a href="https://orgmode.org/worg/org-contrib/org-protocol.html">in the org mode manual</a>) you can request that <code>org-protocol://....</code> contents are sent to your Emacs for processing.</p>
<p>That means you can send information to Org from other tools — such as the commandline or, more tantalizingly, from your web browser or even e-mails, provided your e-mail client follows hyperlinks and supports custom protocols.</p>
<p>Provided you’ve done the installation steps (I recommend you check out the <a href="https://orgmode.org/worg/org-contrib/org-protocol.html">the link</a> if the example below does not work) you can use a snippet like this in a Bookmarklet in your browser – or straight into the console for testing:</p>
<pre><code>javascript:location.href = 'org-protocol://capture?' +
    new URLSearchParams({
          template: "m",
          url: location.href,
          title: document.title,
          body: window.getSelection()})</code></pre>
<p>Replacing <code>template: "m"</code> with the <code>M-x org-capture</code> template key of your choice. The document’s title, URL and text selection is then sent to your org capture.</p>
<p>Pretty darn cool! If you’re an org mode user – I know you are! – you should get on top of this right away.</p>
<p>You can even register your own protocols (<code>org-protocol-protocol-alist</code>) and build your own integrations, and it’s not like you have to feed it into org!</p>
<pre><code>New variable lets Lisp code read emacsclient arguments.
When '--eval' is passed to emacsclient and Emacs is evaluating each
argument, the new variable 'server-eval-args-left' is set to those
arguments not yet evaluated.  It can be used by Lisp code to 'pop'
arguments and process them by the function called in the '--eval'
expression, which is useful when those arguments contain arbitrary
characters that otherwise might require elaborate and error-prone
escaping (to protect them from the shell).</code></pre>
<p>Do not sleep on Emacs’s client-server architecture, or the headless evaluation feature it has. My article on <a href="https://www.masteringemacs.org/article/fuzzy-finding-emacs-instead-of-fzf">Fuzzy Finding with Emacs Instead of fzf</a> is a great example of how two dozen lines of elisp and another baker’s dozen of bash can clone the keystone feature of <code>fzf</code>.</p>
<h2 id="incompatible-changes-in-emacs-30.1">Incompatible Changes in Emacs 30.1</h2>
<pre><code>Tree-Sitter modes are now declared as submodes of the non-TS modes.
In order to help the use of those Tree-Sitter modes, they are now
declared to have the corresponding non-Tree-Sitter mode as an
additional parent.
This way, things like ".dir-locals.el" settings, and YASnippet
collections of snippets automatically apply to the new Tree-Sitter modes.

Note that those modes still do not inherit from the non-TS mode, so
configuration settings installed via mode hooks are not affected.

Loading a Tree-Sitter mode (such as by using 'M-x load-library' or with
'M-x load-file') by default causes the corresponding non-Tree-Sitter
mode be remapped to the Tree-Sitter mode.  This remapping affects
visiting files for which 'auto-mode-alist' specifies a non-Tree-Sitter
mode, and also affects mode-specification cookies on the first line of a
file and mode specifications in file- and directory-local variables.  To
revert to using a non-Tree-Sitter mode, reload the corresponding mode
file anew.  To prevent file loading from turning on Tree-Sitter mode
when 'auto-mode-alist' or the file/directory-local variables specify a
non-Tree-Sitter mode, customize the user option 'major-mode-remap-alist'
to specify that a non-Tree-Sitter mode is "remapped" to itself.  For
example:

    (add-to-list 'major-mode-remap-alist '(c-mode))

specifies that C Mode should not be remapped to 'c-ts-mode' even if and
when 'c-ts-mode' is loaded.  Conversely,

    (add-to-list 'major-mode-remap-alist '(c-mode . c-ts-mode))

tells Emacs to always invoke 'c-ts-mode' whenever 'c-mode' is
requested, either by 'auto-mode-alist' or by file/directory-local
variables.

We recommend using 'major-mode-remap-alist' to express your preferences
for using Tree-Sitter or non-Tree-Sitter modes for files for which both
variants of major modes are available, because that variable overrides
the remapping Emacs might decide to perform as result of loading Lisp
files and features.</code></pre>
<p>I mean, I’m glad they’re slowly making these changes, as the tree-sitter modes are threadbare. Aside from better (usually) syntax highlighting and indentation, they rarely offer much.</p>
<p>But this is yet another thing to explain to people that, yes, now your dir-locals will pick up on the TS mode you’re using, but all the other features present in the old major modes probably don’t work.</p>
<p>In my opinion, tree-sitter major modes should be deprecated and the syntax highlighting and indentation relegated to optional minor modes you can enable in the existing major modes (and obviously brand new ones, if no such major modes exist) and all other “tree-sitter-portable” custom commands are gated behind a feature flag that require the presence of TS and the right grammar file to work, with perhaps a fallback for things that can be implemented – or is already – with non-TS code.</p>
<pre><code>Mouse wheel events should now always be 'wheel-up/down/left/right'.
At those places where the old 'mouse-4/5/6/7' events could still occur
(i.e., X11 input in the absence of XInput2, and 'xterm-mouse-mode'),
we remap them to the corresponding 'wheel-up/down/left/right' event,
according to the new user option 'mouse-wheel-buttons'.
The old variables 'mouse-wheel-up-event', 'mouse-wheel-down-event',
'mouse-wheel-left-event', and 'mouse-wheel-right-event' are thereby
obsolete.</code></pre>
<p>I’ve never in my life been able to remember what the numbers map to, so I this is a useful aid to me and, no doubt, anybody else looking to remap or just figure out what a mouse event is.</p>
<pre><code>'completion-auto-help' now affects 'icomplete-in-buffer'.
Previously, 'completion-auto-help' mostly affected only minibuffer
completion.  Now, if 'completion-auto-help' has the value 'lazy', then
Icomplete's in-buffer display of possible completions will only appear
after the 'completion-at-point' command has been invoked twice, and if
'completion-auto-help' is nil, then Icomplete's in-buffer display is
completely suppressed.  Thus, if you use 'icomplete-in-buffer', ensure
'completion-auto-help' is not customized to 'lazy' or nil.</code></pre>
<pre><code>The "*Completions*" buffer now always accompanies 'icomplete-in-buffer'.
Previously, it was not consistent whether the "*Completions*" buffer would
appear when using 'icomplete-in-buffer'.  Now the "*Completions*" buffer
and Icomplete's in-buffer display of possible completions always
appear together.  If you would prefer to see only Icomplete's
in-buffer display, and not the "*Completions*" buffer, you can add this
to your init file:

    (advice-add 'completion-at-point :after #'minibuffer-hide-completions)</code></pre>
<p>This governs the <code>*Completions*</code> window that may (or may not, as the case may be) pop up to offer assistance if when you are completing. If you use the default completion mechanism in Emacs, you may want to consider looking into tweaking this, if you find the current <code>*Completions*</code> behavior is not to your liking.</p>
<pre><code>The default process filter was rewritten in native code.
The round-trip through the Lisp function
'internal-default-process-filter' is skipped when the process filter is
the default one.  It is reimplemented in native code, reducing GC churn.
To undo this change, set 'fast-read-process-output' to nil.</code></pre>
<p>Anything that speeds up a known and serious IO bottleneck in Emacs is welcome news to me. The fact they named the variable <code>fast-read-process-output</code> says it all.</p>
<pre><code>Network Security Manager now warns about 3DES by default.
This cypher is no longer recommended owing to a major vulnerability
disclosed in 2016, and its small 112 bit key size.  Emacs now warns
about its use also when 'network-security-level' is set to 'medium'
(the default).  See 'network-security-protocol-checks'.</code></pre>
<pre><code>Network Security Manager now warns about &lt;2048 bits in DH key exchange.
Emacs used to warn for ephemeral Diffie-Hellman (DHE) key exchanges with
prime numbers smaller than 1024 bits.  Since more servers now support
it, this number has been bumped to 2048 bits.</code></pre>
<pre><code>URL now never sends user email addresses in HTTP requests.
Emacs never sent email addresses by default, but it used to be
possible to customize 'url-privacy-level' so that the user's email
address was sent along in HTTP requests.  This feature has now been
removed, as it was considered more dangerous than useful.  RFC 9110
(§ 10.1.2) also recommends against it.  The user option
'url-personal-mail-address' is now also obsolete.

To send an email address in the header of individual HTTP requests,
see the variable 'url-request-extra-headers'.</code></pre>
<p>I am not sure why there was ever the option of sending email addressed alongside HTTP requests. I don’t recall that ever being a thing in the 90s either, but perhaps that is just my memory failing me.</p>
<pre><code>'pixel-scroll-precision-mode' sets 'make-cursor-line-fully-visible'.
'pixel-scroll-precision-mode' sets 'make-cursor-line-fully-visible' to a
nil value globally, since the usual requirement of the Emacs display to
make the cursor line fully visible contradicts the smooth scrolling
expectations.</code></pre>
<p>I recommend you try <a href="https://github.com/jdtsmith/ultra-scroll">ultra-scroll</a> if you’re serious about wanting smooth scrolling in Emacs.</p>
<h2 id="changes-in-emacs-30.1">Changes in Emacs 30.1</h2>
<pre><code>Fix shell injection vulnerability in man.el (CVE-2025-1244).
We urge all users to upgrade immediately.</code></pre>
<pre><code>New user option 'trusted-content' to allow potentially dangerous features.
This option lists those files and directories whose content Emacs should
consider as sufficiently trusted to run any part of the code contained
therein even without any explicit user request.

For example, Flymake's backend for Emacs Lisp consults this option
and disables itself with an "untrusted content" warning if the file
is not listed.

Emacs Lisp authors should note that a major or minor mode must never set
this option to the ':all' value.

This option is used to fix CVE-2024-53920.  See below for details.</code></pre>
<p>It shouldn’t come as a surprise to anybody that Emacs, a dynamic computing environment, can be coaxed into running stuff it shouldn’t. Ideally that’d never happen, but given the nature of what Emacs does – and how integrated it is in our lives – it is no surprise that on occasion its power is turned against it. What surprises me is that these issues don’t happen more frequently.</p>
<pre><code>Emacs now supports Unicode Standard version 15.1.</code></pre>
<pre><code>Emacs now comes with Org v9.7.
See the file "etc/ORG-NEWS" for user-visible changes in Org.</code></pre>
<pre><code>Improved support for touchscreen devices.
On systems that understand them (at present X, Android, PGTK, and
MS-Windows), many touch screen gestures are now implemented and
translated into mouse or gesture events, and support for tapping tool
bar buttons and opening menus has been added.  Countless packages, such
as Dired and Custom, have been adjusted to better understand touch
screen input.</code></pre>
<p>This obviously goes hand in hand with the Android changes. The myth that Emacs is a keyboard only system is obviously false, and it has been since Lucid/XEmacs improved mouse support in the 1990s. Now that touch screen support is improved, I’m curious to see if we’ll see the emergence of org packages/email clients on Android that lean into the new touch support.</p>
<pre><code>Support for styled underline face attributes.
These are implemented as new values of the 'style' attribute in a face
underline specification, 'double-line', 'dots', and 'dashes', and are
available on GUI systems.  If your terminal's termcap or terminfo
database entry defines the 'Su' or 'Smulx' capability, Emacs will also
emit the prescribed escape sequence to render faces with such styles on
TTY frames.</code></pre>
<pre><code>Support for underline colors on TTY frames.
Colors specified in the underline face will now also be displayed on TTY
frames on terminals that support the 'Su' or 'Smulx' capabilities.</code></pre>
<p>This is addition to things like squiggly underlines that we’ve had for a long time now. I’m quite surprised that terminals can render anything other than a opaque line, though; that is news to me.</p>
<pre><code>Modeline elements can now be right-aligned.
Anything following the symbol 'mode-line-format-right-align' in
'mode-line-format' will be right-aligned.  Exactly where it is
right-aligned to is controlled by the new user option
'mode-line-right-align-edge'.</code></pre>
<p>Can’t say I’ve ever had the desire to heavily mod my mode line, but I know that a lot of people care greatly about its aethestics. I’m sure this is of benefit to people who use right-to-left languages also; perhaps they are the primary users?</p>
<pre><code>X selection requests are now handled much faster and asynchronously.
This means it should be less necessary to disable the likes of
'select-active-regions' when Emacs is running over a slow network
connection.</code></pre>
<pre><code>Emacs now updates invisible frames that are made visible by a compositor.
If an invisible or an iconified frame is shown to the user by the
compositing manager, Emacs will now redisplay such a frame even though
'frame-visible-p' returns nil or 'icon' for it.  This can happen, for
example, as part of preview for iconified frames.</code></pre>
<p>I guess this refers to the ‘window peek’ feature in Windows’ taskbar, and that it’s really more of a bugfix / tweak more than an actual feature.</p>
<pre><code>Most file notification backends detect unmounting of a watched filesystem.
The only exception is w32notify.</code></pre>
<p>Emacs will watch file changes using a range of mechanisms depending on the platform of choice.</p>
<pre><code>The ':map' property of images is now recomputed when image is transformed.
Images with clickable maps now work as expected after you run commands
such as 'image-increase-size', 'image-decrease-size', 'image-rotate',
'image-flip-horizontally', and 'image-flip-vertically'.
Set the new user option 'image-recompute-map-p' to nil to prevent Emacs
from recomputing image maps.</code></pre>
<h2 id="minibuffer-and-completions">Minibuffer and Completions</h2>
<pre><code>New commands 'previous-line-completion' and 'next-line-completion'.
Bound to '&lt;up&gt;' and '&lt;down&gt;' arrow keys, respectively, they navigate
the "*Completions*" buffer vertically by lines, wrapping at the
top/bottom when 'completion-auto-wrap' is non-nil.</code></pre>
<pre><code>New user option 'minibuffer-visible-completions'.
When customized to non-nil, you can use arrow keys in the minibuffer
to navigate the completions displayed in the "*Completions*" window.
Typing 'RET' selects the highlighted candidate.  'C-g' hides the
completions window.  When the completions window is not visible,
then all these keys have their usual meaning in the minibuffer.
This option is supported for in-buffer completion as well.</code></pre>
<pre><code>Selected completion candidates are deselected on typing.
When you type at the minibuffer prompt, the current completion
candidate will be un-highlighted, and point in the "*Completions*" window
will be moved off that candidate.  'minibuffer-choose-completion'
('M-RET') will still choose a previously-selected completion
candidate, but the new command 'minibuffer-choose-completion-or-exit'
(bound to 'RET' by 'minibuffer-visible-completions') will exit with
the minibuffer contents instead.  This deselection behavior can be
controlled with the new user option 'completion-auto-deselect', which
is t by default.</code></pre>
<p>There’s a longer-running theme to try and make the <code>*Completions*</code> window useful in contexts outside the minibuffer’s completion mechanism. This mechanism is rather confusingly named <code>completion-in-region</code>, even though it’s not just about acting on a region. If you use popup completion tools like company and corfu, then you should consider experimenting with this improved completion system, as it’s a “native” buffer-and-window-based approach to the same.</p>
<pre><code>New value 'historical' for user option 'completions-sort'.
When 'completions-sort' is set to 'historical', completion candidates
will be first sorted alphabetically, and then re-sorted by their order
in the minibuffer history, with more recent candidates appearing first.</code></pre>
<p>There are popular third-party packages that do much of this already, so it’s nice to see that a similar feature has made it into core. My article on <a href="https://www.masteringemacs.org/article/understanding-minibuffer-completion">Understanding Minibuffer Completion</a> goes into more detail on how to configure minibuffer completion in Emacs.</p>
<pre><code>'completion-category-overrides' supports more metadata.
The new supported completion properties are 'cycle-sort-function',
'display-sort-function', 'annotation-function', 'affixation-function',
and 'group-function'.  You can now customize them for any category in
'completion-category-overrides' that will override the properties
defined in completion metadata.</code></pre>
<pre><code>'completion-extra-properties' supports more metadata.
The new supported completion properties are 'category',
'group-function', 'display-sort-function', and 'cycle-sort-function'.</code></pre>
<p>See <a href="https://www.masteringemacs.org/article/understanding-minibuffer-completion">my article</a> for more information on customizing Emacs’s minibuffer.</p>
<h2 id="windows">Windows</h2>
<pre><code>New command 'toggle-window-dedicated'.
This makes it easy to interactively mark a specific window as
dedicated, so it won't be reused by 'display-buffer'.  This can be
useful for complicated window setups.  It is bound to 'C-x w d'
globally.</code></pre>
<pre><code>"d" in the mode line now indicates that the window is dedicated.
Windows have always been able to be dedicated to a specific buffer;
see 'window-dedicated-p'.  Now the mode line indicates the dedicated
status of a window, with "d" appearing in the mode line if a window is
dedicated and "D" if the window is strongly dedicated.  This indicator
appears before the buffer name, and after the buffer modification and
remote buffer indicators (usually "---" together).</code></pre>
<p>Coaxing Emacs into putting your windows where you want them to go is a common source of frustrating. I wrote <a href="https://www.masteringemacs.org/article/demystifying-emacs-window-manager">Demystifying Emacs’s Window Manager</a> in part to help people wrangle the windows into place. Dedicated windows are a blunt (though useful) tool that tells Emacs not to use a window for anything other than what is in it already. This feature mirrors what you find in a lot of other IDEs and editors where most windows have a singular purpose, unlike Emacs’.</p>
<pre><code>New action alist entry 'some-window' for 'display-buffer'.
It specifies which window 'display-buffer-use-some-window' should prefer.
For example, when 'display-buffer-base-action' is customized to
'(nil . ((some-window . mru)))', then a buffer will be displayed
in the same most recently used window from consecutive calls of
'display-buffer' (in a configuration with more than two windows).</code></pre>
<p>This is pretty advanced stuff, but if you’re unhappy with the default methods employed by Emacs (see <code>display-buffer-fallback-action</code>) you can now tell it to more aggressively reuse windows using the “most recently used” order of windows.</p>
<pre><code>New action alist entry 'category' for 'display-buffer'.
If the caller of 'display-buffer' passes '(category . symbol)'
in its 'action' argument, you can match the displayed buffer
by adding '(category . symbol)' to the condition part of
'display-buffer-alist' entries.</code></pre>
<p>I do not think you’re likely to encounter code that does this just yet, but I suppose this is one way of disambiguating windows that may share the same buffer name or major mode, but differ in some crucial way that only a custom cons cell <code>category</code> can sort out.</p>
<p>Pretty niche. You’re probably not going to use this <em>really</em> into <code>display-buffer-alist</code>.</p>
<pre><code>New action alist entry 'post-command-select-window' for 'display-buffer'.
It specifies whether the window of the displayed buffer should be
selected or deselected at the end of executing the current command.</code></pre>
<pre><code>New variable 'window-restore-killed-buffer-windows'.
It specifies how 'set-window-configuration' and 'window-state-put'
should proceed with windows whose buffer was killed after the
corresponding configuration or state was recorded.</code></pre>
<p>Window configurations are, well, configurations of windows. They are snapshots of window layouts: <code>C-x r w</code> puts your current window configuration into a register, and that is one such use; the other is tab bars in <code>tab-bar-mode</code>. Both set and restore window configurations when you change tabs or explicitly change them with a register. This variable’s really quite complex, so if you have strong opinions on this (you probably do not) then you really should read the lengthy docstring for this variable.</p>
<pre><code>New variable 'window-point-context-set-function'.
It can be used to set a context for window point in all windows by
'window-point-context-set' before calling 'current-window-configuration'
and 'window-state-get'.  Then later another new variable
'window-point-context-use-function' can be used by
'window-point-context-use' after 'set-window-configuration' and
'window-state-put' to restore positions of window points
according to the context stored in a window parameter.</code></pre>
<p>Emacs tracks the position of point by <em>window</em> and not by buffer, as you may think. Split your current buffer and then move the point in one window and switch to the other: observe that your point is remembered between each window. Very handy, that, and this (alongside the window configuration stuff I mentioned before) is what this is about. You… probably don’t care.</p>
<pre><code>New functions 'set-window-cursor-type' and 'window-cursor-type'.
'set-window-cursor-type' sets a per-window cursor type, and
'window-cursor-type' queries this setting for a given window.  Windows
are always created with a 'window-cursor-type' of t, which means to
consult the variable 'cursor-type' as before.</code></pre>
<p>You can now change the cursor type for each window. Neat. The variable <code>cursor-type</code> used to just override it for the whole Emacs session.</p>
<pre><code>The user option 'display-comint-buffer-action' is now obsolete.
You can use a '(category . comint)' condition in 'display-buffer-alist'
to match buffers displayed by comint-related commands.  Another
user option 'display-tex-shell-buffer-action' is obsolete too
for which you can use '(category . tex-shell)'.</code></pre>
<p>I suppose this is what the change above is really about: to remove bespoke code and generalize it into the new category system.</p>

<pre><code>Tool bars can now be placed on the bottom on more systems.
The 'tool-bar-position' frame parameter can be set to 'bottom' on all
window systems other than macOS and GNUstep (Nextstep).</code></pre>
<pre><code>New global minor mode 'modifier-bar-mode'.
When this minor mode is enabled, the tool bar displays buttons
representing modifier keys.  Clicking on these buttons applies the
corresponding modifiers to the next input event.</code></pre>
<p>Useful on portable devices (like Android) that may not have certain modifier keys on its native keyboard.</p>
<pre><code>New user option 'tool-bar-always-show-default'.
When non-nil, the tool bar at the top of a frame does not show buffer
local customization of the tool bar.  The default value is nil.</code></pre>
<h2 id="tab-bars-and-tab-lines">Tab Bars and Tab Lines</h2>
<p>Tab bars are window configurations; tab lines are basically tabs like they appear in your browser.</p>
<pre><code>New user option 'tab-bar-select-restore-context'.
It uses 'window-point-context-set' to save contexts where
window points were located before switching away from the tab,
and 'window-point-context-use' to restore positions of window
points after switching back to that tab.</code></pre>
<p>If you have multiple tabs featuring the same buffer, you can now preserve the point location between tabs.</p>
<pre><code>New user option 'tab-bar-select-restore-windows'.
It defines what to do with windows whose buffer was killed since the tab
was last selected.  By default it displays a placeholder buffer
with the name " *Old buffer &lt;name&gt;*" that provides information about
the name of the killed buffer that was displayed in that window.</code></pre>
<pre><code>New user option 'tab-bar-tab-name-format-functions'.
It can be used to add, remove and reorder functions that change the
appearance of every tab on the tab bar.</code></pre>
<pre><code>New hook 'tab-bar-tab-post-select-functions'.</code></pre>
<pre><code>New keymap 'tab-bar-mode-map'.
By default it contains a keybinding 'C-TAB' to switch tabs, but only
when 'C-TAB' is not bound globally.  You can unbind it if it conflicts
with 'C-TAB' in other modes.</code></pre>
<pre><code>New keymap 'tab-line-mode-map'.
By default it contains keybindings for switching tabs: 'C-x &lt;left&gt;',
'C-x &lt;right&gt;', 'C-x C-&lt;left&gt;', 'C-x C-&lt;right&gt;'.  You can unbind them if
you want to use these keys for the commands 'previous-buffer' and
'next-buffer'.</code></pre>
<p>A better way of configuring tab bar/line modes’ keys. You can opt for more complex keys, and even use keyboard repeat maps if you use <code>C-&lt;TAB&gt;</code> for other things.</p>
<pre><code>Default list of tab-line tabs is changed to support a fixed order.
This means that 'tab-line-tabs-fixed-window-buffers', the new default
tabs function, is like the previous 'tab-line-tabs-window-buffers' where
both of them show only buffers that were previously displayed in the
window.  But the difference is that the new function always keeps the
original order of buffers on the tab line, even after switching between
these buffers.  You can drag the tabs and release at a new position
to manually reorder the buffers on the tab line.</code></pre>
<pre><code>New user option 'tab-line-tabs-buffer-group-function'.
It provides two choices to group tab buffers by major mode and by
project name.</code></pre>
<pre><code>Buffers on tab-line group tabs are now sorted alphabetically.
This will keep the fixed order of tabs, even after switching between
them.</code></pre>
<h2 id="help">Help</h2>
<pre><code>New command 'help-find-source'.
Switch to a buffer visiting the source of what is being described in
"*Help*".  It is bound to 'C-h 4 s' globally.</code></pre>
<p>Other notable commands include <code>M-x find-variable</code>, <code>M-x find-function</code> and <code>M-x find-function-on-key</code> to jump to their source definitions. I use ’em all the time.</p>
<pre><code>New user option 'describe-bindings-outline-rules'.
This user option controls outline visibility in the output buffer of
'describe-bindings' when 'describe-bindings-outline' is non-nil.</code></pre>
<p>All this does is determine whether <code>outline-minor-mode</code> is enabled and configured to make headings collapsible.</p>
<pre><code>'describe-function' shows the function's inferred type when available.
For native compiled Lisp functions, 'describe-function' prints (after
the signature) the automatically inferred function type as well.  If the
function's type was explicitly declared (via the 'declare' form's
'ftype' property), 'describe-function' shows the declared type.  This is
controlled by the new user option 'help-display-function-type', which is
by default t; customize to nil to disable function type display.</code></pre>
<p>This is really only likely to be of interest and use to elisp hackers who know their way around Emacs’s internals. The declared type for <code>+</code> is <code>Declared type: (function (&amp;rest (or number marker)) number)</code> for example.</p>
<pre><code>'describe-function' now shows the type of the function object.
The text used to say things like "car is a built-in function" whereas it
now says "car is a primitive-function" where "primitive-function" is the
name of the symbol returned by 'cl-type-of'.  You can click on those
words to get information about that type.</code></pre>
<p>It’s been a long-running saga to get some sort of type taxonomy into Emacs, and it’s slowly happening now. I am unsure how useful this is to your average elisp hacker; this is more likely to be of use for byte and native compilation. See also the entry further down on declaratively specifying the arguments and return type of a function.</p>
<pre><code>'C-h m' ('describe-mode') uses outlining by default.
Set 'describe-mode-outline' to nil to get back the old behavior.</code></pre>
<p>See above.</p>
<pre><code>'C-h k' ('describe-key') shows Unicode name.
For keybindings which produce single characters via translation or input
methods, 'C-h k' now shows the Unicode name of the produced character in
addition to the character itself, e.g.

'C-h k C-x 8 E' =&gt;

    € 'EURO SIGN' (translated from C-x 8 E)</code></pre>
<p>A useful change, even though I doubt I’ll remember that I can do this. For inputtable characters I normally type them then use <code>C-u C-x =</code> to see its unicode name along with a host of other useful information.</p>
<pre><code>'C-h b' ('describe-bindings') shows Unicode names.
For keybindings which produce single characters via translation (such as
those using the 'C-x 8' or 'A-' prefix, or 'dead-acute', 'dead-grave',
etc), the Unicode names will now be shown in addition to the character
itself, i.e.

    A-!         ¡   INVERTED EXCLAMATION MARK
    A-$         ¤   CURRENCY SIGN

and so on.</code></pre>
<p>This I am more likely to find a use for. <code>C-h b</code> is a must-know command, and now it shows the unicode name for self-inserted characters.</p>
<pre><code>Multi-character key echo now ends with a suggestion to use Help.
Customize 'echo-keystrokes-help' to nil to prevent that.</code></pre>
<h2 id="customize">Customize</h2>
<pre><code>New command 'customize-dirlocals'.
This command pops up a buffer to edit the settings in ".dir-locals.el".</code></pre>
<p>Wow. This change is great. Directory-local variables are elisp variables declared in one of several files (such as <code>.dir-locals.el</code>) that you can use to effect directory-specific changes to Emacs buffers. I really love dirlocals; however, they’re a pain to add and edit. <code>M-x add-dir-local-variable</code> is a newbie trap, and it’s easy to type the wrong thing and cause all sorts of issues (or worse: that nothing at all happens.)</p>
<p>A nice customize UI will go a long way towards encouraging more people to use it.</p>
<pre><code>New command 'customize-toggle-option'.
This command can toggle boolean options for the duration of a session.</code></pre>
<p>One cool aspect of Customize is that you can set settings for the duration only, and then later on return to finalize them and save them to a file. All this does is add that functionality to booleans.</p>
<pre><code>New prefix argument for modifying directory-local variables.
The commands 'add-dir-local-variable', 'delete-dir-local-variable' and
'copy-file-locals-to-dir-locals' now take an optional prefix argument,
to enter the file name where you want to modify directory-local
variables.</code></pre>
<pre><code>New user option 'safe-local-variable-directories'.
This user option names directories in which Emacs will treat all
directory-local variables as safe.</code></pre>
<p>Very useful. Emacs is squeamish about dirlocals as it maintains a list of safe variables and anything else is considered unsafe, and you’re asked to approve every setting. Whitelisting certain directories is a great compromise for people who are OK with the risks that follow from such a feature.</p>
<h2 id="cl-print">CL Print</h2>
<p>This is for elisp programmers only, and governs how Elisp forms are printed on the screen for human consumption.</p>
<pre><code>There is a new chapter in the CL manual documenting cl-print.el.
See the Info node "(cl) Printing".</code></pre>
<pre><code>You can expand the "..." truncation everywhere.
The code that allowed "..." to be expanded in the "*Backtrace*" buffer
should now work anywhere the data is generated by 'cl-print'.</code></pre>
<pre><code>The 'backtrace-ellipsis' button is replaced by 'cl-print-ellipsis'.</code></pre>
<pre><code>hash-tables' contents can be expanded via the ellipsis.</code></pre>
<pre><code>Modes can control the expansion via 'cl-print-expand-ellipsis-function'.</code></pre>
<pre><code>New setting 'raw' for 'cl-print-compiled'.
This setting causes byte-compiled functions to be printed in full by
'prin1'.  A button on this output can be activated to disassemble the
function.</code></pre>
<h2 id="miscellaneous">Miscellaneous</h2>
<pre><code>New command 'kill-matching-buffers-no-ask'.
This works like 'kill-matching-buffers', but without asking for
confirmation.</code></pre>
<p><code>M-x kill-matching-buffers</code> kills buffers by regexp; and now you can have it do so without asking your permission.</p>
<pre><code>'recover-file' can show diffs between auto save file and current file.
When answering the prompt with "diff" or "=", it now shows the diffs
between the auto save file and the current file.</code></pre>
<p>Ah, very handy. You can already diff files you want to save with <code>C-x s</code> and then <code>d</code>.</p>
<pre><code>'read-passwd' can toggle the visibility of passwords.
Use 'TAB' in the minibuffer to show or hide the password.
Alternatively, click the new show-password icon on the mode-line with
'mouse-1' to toggle the visibility of the password.</code></pre>
<p>TAB, eh? I’m most assuredly going to accidentally reveal a password that way. You can edit <code>read-passwd-map</code> to remove the TAB if you want, though.</p>
<pre><code>'advice-remove' is now an interactive command.
When called interactively, 'advice-remove' now prompts for an advised
function to the advice to remove.</code></pre>
<p>Good. You can do this with <code>M-x remove-hook</code> also. Useful if you foolishly cram lambdas into hooks or advice… and are now struggling to remove them. But you’d never make that mistake, right? :)</p>
<pre><code>New user option 'uniquify-dirname-transform'.
This can be used to customize how buffer names are uniquified, by
making arbitrary transforms on the buffer's directory name (whose
components are used to uniquify buffer names when they clash).  You
can use this to distinguish between buffers visiting files with the
same base name that belong to different projects by using the provided
transform function 'project-uniquify-dirname-transform'.</code></pre>
<p>Uniquify renames buffers (but not the underlying files, if any) to sensible names instead of <code>foo&lt;1&gt;</code>, <code>foo&lt;2&gt;</code>, etc. It’s enabled by default, nowadays, but if you use the projects feature in Emacs, and if you frequently open the same file name in multiple projects, give this setting a try.</p>
<pre><code>New user option 'remote-file-name-inhibit-delete-by-moving-to-trash'.
When non-nil, this option suppresses moving remote files to the local
trash when deleting.  Default is nil.</code></pre>
<pre><code>New user option 'remote-file-name-inhibit-auto-save'.
If this user option is non-nil, 'auto-save-mode' will not auto-save
remote buffers.  The default is nil.</code></pre>
<pre><code>New user option 'remote-file-name-access-timeout'.
If a positive number, this option limits the call of 'access-file'
for remote files to that number of seconds.  Default is nil.</code></pre>
<p>These are Tramp features, part of Emacs’s remote file editing system.</p>
<pre><code>New user option 'yes-or-no-prompt'.
This allows the user to customize the prompt that is appended by
'yes-or-no-p' when asking questions.  The default value is
"(yes or no) ".</code></pre>
<pre><code>New user option 'menu-bar-close-window'.
When non-nil, selecting "Close" from the "File" menu or clicking
"Close" in the tool bar will result in the current window being
deleted, if possible.  The default is nil, and these gestures kill the
buffer shown in the current window, but don't delete the window.</code></pre>
<pre><code>New face 'display-time-date-and-time'.
This is used for displaying the time and date components of
'display-time-mode'.</code></pre>
<pre><code>New face 'appt-notification' for 'appt-display-mode-line'.
It can be used to customize the look of the appointment notification
displayed on the mode line when 'appt-display-mode-line' is non-nil.</code></pre>
<pre><code>New icon images for general use.
Several symbolic icons have been added to "etc/images/symbols",
including plus, minus, check-mark, star, etc.</code></pre>
<pre><code>Emacs now recognizes shebang lines that pass '-S'/'--split-string' to 'env'.
When visiting a script that invokes 'env -S INTERPRETER ARGS...' in
its shebang line, Emacs will now skip over 'env -S' and deduce the
major mode based on the interpreter after 'env -S'.</code></pre>
<p>Emacs uses a wide range of tools to infer the major mode to use. Filename is not the only way: Emacs will scan the file to try and determine the major mode if there is no obvious choice that matches its filename. Looking at <code>/usr/bin/env</code> is one such method.</p>
<pre><code>'insert-directory-program' is now a user option.
On *BSD and macOS systems, this user option now defaults to the "gls"
executable, if it exists.  This should remove the need to change its
value when installing GNU coreutils using something like ports or
Homebrew.</code></pre>
<pre><code>'write-region-inhibit-fsync' now defaults to t in interactive mode.
This is the default in batch mode since Emacs 24.</code></pre>
<pre><code>The default value of 'read-process-output-max' was increased to 65536.</code></pre>
<p>You can probably experiment with setting this even higher to batch up output from commands. Your mileage may vary, though. Mine is set to <code>1048576</code>.</p>
<pre><code>'url-gateway-broken-resolution' is now obsolete.
This option was intended for use on SunOS 4.x and Ultrix systems,
neither of which have been supported by Emacs since version 23.1.
The user option 'url-gateway-nslookup-program' and the command
'url-gateway-nslookup-host' are consequently also obsolete.

</code></pre>
<h2 id="editing-changes-in-emacs-30.1">Editing Changes in Emacs 30.1</h2>
<pre><code>New minor mode 'visual-wrap-prefix-mode'.
When enabled, continuation lines displayed for a wrapped long line
will receive a 'wrap-prefix' automatically computed from the line's
surrounding context, such that continuation lines are indented on
display as if they were filled with 'M-q' or similar.  Unlike 'M-q',
the indentation only happens on display, and doesn't change the buffer
text in any way.  The global minor mode
'global-visual-wrap-prefix-mode' enables this minor mode in all
buffers.

(This minor mode is the 'adaptive-wrap' ELPA package renamed and
lightly edited for inclusion in Emacs.)</code></pre>
<pre><code>New global minor mode 'kill-ring-deindent-mode'.
When enabled, text being saved to the kill ring will be de-indented by
the column number at its start.  For example, saving the entire
function call within an indented block:

foo ()
{
  long_function_with_several_arguments (argument_1_compute (),
                                        argument_2_compute (),
                                        argument_3_compute ());
}

will save this to the kill ring:

long_function_with_several_arguments (argument_1_compute (),
                                      argument_2_compute (),
                                      argument_3_compute ())

This omits the two columns of extra indentation that would otherwise be
copied from the second and third lines and saved to the kill ring.</code></pre>
<p>This is worth enabling if inconsistent kill-yank indentation annoys you. Doing this correctly, especially in whitespace sensitive languages, is really important in structured editing tools such as my package, <a href="https://www.masteringemacs.org/article/combobulate-structured-movement-editing-treesitter">Combobulate</a>.</p>
<pre><code>New command 'replace-regexp-as-diff'.
It reads a regexp to search for and a string to replace with, then
displays a buffer with replacements as diffs.  After reviewing the
changes in the output buffer you can apply the replacements as
a patch to the current file buffer.  There are also new commands
'multi-file-replace-regexp-as-diff' that shows as diffs replacements
in a list of specified files, and 'dired-do-replace-regexp-as-diff'
that shows as diffs replacements in the marked files in Dired.</code></pre>
<p>This is a very welcome addition for bulk text replacing. It’s a nervewracking thing to accept/reject changes and hope you didn’t screw it up. Having a dedicated diff window that works across multiple files is a superb addition to Emacs.</p>
<pre><code>New mode of prompting for register names and showing preview.
The new user option 'register-use-preview' can be customized to the
value t or 'insist' to request a different user interface of prompting for
register names and previewing the registers: Emacs will require
confirmation for overwriting the value of a register, and will show
the preview of registers without delay.  You can also customize this
new option to disable the preview completely.

The default value of 'register-use-preview' ('traditional') preserves the
behavior of Emacs 29 and before.  See the Info node "(emacs) Registers"
for more details about the new UI and its variants.</code></pre>
<p>I love registers, and I use them a lot. I use them because they require no forethought on my part to use: set and recall; it’s that simple. No fuss. If you want added distractions like confirmation prompts if you override a register? Go ahead and change the new option. If you like registers the way they are, then you don’t have to do anything.</p>
<pre><code>New advanced macro counter commands.
New commands have been added to implement advanced macro counter
functions.

The commands 'C-x C-k C-r l' and 'C-x C-k C-r s' load and save the
macro counter from and to a number register, respectively.

The commands 'C-x C-k C-r a =', 'C-x C-k C-r a &lt;', and 'C-x C-k C-r a &gt;'
compare the macro counter with the contents of a number register and
increment the counter by an optional prefix if the comparison succeeds.

The commands 'C-x C-k C-q =', 'C-x C-k C-q &lt;', and 'C-x C-k C-q &gt;'
compare the macro counter with an optional prefix and terminate the
macro if the comparison succeeds.</code></pre>
<p>Advanced indeed. Registers are capable of incrementing and decrementing a number already, and kmacro (Emacs’s keyboard macro facility) has its own internal counter (see <a href="https://www.masteringemacs.org/article/keyboard-macros-are-misunderstood">Keyboard Macros are Misunderstood</a>) also. Now you can switch values between them and also do simple comparisons. Combine it with the ability to physically move up or down a line; increment and decrement; and conditional termination, and I’m pretty sure kmacro close to Turing-complete.</p>
<pre><code>New mode 'kmacro-menu-mode' and new command 'list-keyboard-macros'.
The new command 'list-keyboard-macros' is the keyboard-macro version
of commands like 'list-buffers' and 'list-processes', creating a listing
of the currently existing keyboards macros using the new mode
'kmacro-menu-mode'.  It allows rearranging the macros in the ring,
duplicating them, deleting them, and editing their counters, formats,
and keys.</code></pre>
<p>Excellent. There’s already a kmacro ring for macros known to Emacs, but it’s a bit awkward. Now you can list the macros.</p>
<pre><code>On X, Emacs now supports input methods which perform "string conversion".
This means an input method can now ask Emacs to delete text
surrounding point and replace it with something else, as well as query
Emacs for surrounding text.  If your input method allows you to "undo"
mistaken compositions, this will now work as well.</code></pre>
<pre><code>New user option 'duplicate-region-final-position'.
It controls the placement of point and the region after duplicating a
region with 'duplicate-dwim'.</code></pre>
<pre><code>New user option 'mouse-prefer-closest-glyph'.
When enabled, clicking or dragging with the mouse will put the point
or start the drag in front of the buffer position corresponding to the
glyph with the closest X coordinate to the click or start of the drag.
In other words, if the mouse pointer is in the right half of a glyph,
point will be put after the buffer position corresponding to that glyph,
whereas if the mouse pointer is in the left half of a glyph, point
will be put in front the buffer position corresponding to that glyph.
By default this is disabled.</code></pre>
<pre><code>New pre-defined values for 'electric-quote-chars'.
The available customization options for 'electric-quote-chars' have been
updated with common pairs of quotation characters, including "‘", "’",
"“", "”", "«", "»", "‹", "›", "‚", "„", "「", "」", "『", and "』".
The default is unchanged.</code></pre>
<p>This is for <code>M-x electric-pair-mode</code>, which automatically pairs an opening character with its constituent closing character.</p>
<pre><code>'M-TAB' now invokes 'completion-at-point' in Text mode.
By default, Text mode no longer binds 'M-TAB' to 'ispell-complete-word'.
Instead, this mode arranges for 'completion-at-point', globally bound to
'M-TAB', to perform word completion as well.  You can have Text mode
binding 'M-TAB' to 'ispell-complete-word' as it did in previous Emacs
versions, or disable Ispell word completion in Text mode altogether, by
customizing the new user option 'text-mode-ispell-word-completion'.</code></pre>
<p><code>M-&lt;TAB&gt;</code> is also Alt+Tab. The key binding <code>C-M-i</code> is functionally equivalent.</p>
<h2 id="internationalization">Internationalization</h2>
<pre><code>Mode-line mnemonics for some coding-systems have changed.
The mode-line mnemonic for 'utf-7' is now the lowercase 'u', to be
consistent with the other encodings of this family.

The mode-line mnemonic for 'koi8-u' is now 'У', U+0423 CYRILLIC
CAPITAL LETTER U, to distinguish between this encoding and the
UTF-8/UTF-16 family.

If your terminal cannot display 'У', or if you want to get the old
behavior back for any other reason, you can do that using the
'coding-system-put' function.  For example, the following restores the
previous behavior of showing 'U' in the mode line for 'koi8-u':

    (coding-system-put 'koi8-u :mnemonic ?U)</code></pre>
<p>The mode line adopts a different character for the coding system in the active buffer. You can find it in the bottom left.</p>
<pre><code>'vietnamese-tcvn' is now a coding system alias for 'vietnamese-vscii'.
VSCII-1 and TCVN-5712 are different names for the same character
encoding.  Therefore, the duplicate coding system definition has been
dropped in favor of an alias.

The mode-line mnemonic for 'vietnamese-vscii' and its aliases is the
lowercase letter "v".</code></pre>
<pre><code>Users in CJK locales can control width of some non-CJK characters.
Some characters are considered by Unicode as "ambiguous" with respect
to their display width: either "full-width" (i.e., taking 2 columns on
display) or "narrow" (taking 1 column).  The actual width depends on
the fonts used for these characters by Emacs or (for text-mode frames)
by the terminal emulator.  Traditionally, font sets in CJK locales
were set up so as to display these characters as full-width, and thus
Emacs modified the char-width table in those locales to follow suit.
Lately, the tendency is to display these characters as narrow.  The
new user option 'cjk-ambiguous-chars-are-wide' allows users to control
whether Emacs considers these characters as full-width (the default)
or narrow (if the variable is customized to the nil value).

This setting affects the results of 'string-width' and similar
functions in CJK locales.</code></pre>
<pre><code>New input methods for the Urdu, Pashto, and Sindhi languages.
These languages are spoken in Pakistan and Afghanistan.</code></pre>
<pre><code>New input method "english-colemak".
This input method supports the Colemak keyboard layout.</code></pre>
<p>Input methods are really neat. You can switch to a ‘virtual keyboard’ that behaves like another language, letting you enter diacritics and complex unicode characters using your current keyboard’s layout. You are not limited to actual languages, either: there are input methods to help you enter LaTeX for example.</p>
<p>You can toggle the input method with <code>C-\</code> and with a prefix argument to set the input method type. I wrote about them in <a href="https://www.masteringemacs.org/article/diacritics-in-emacs">Olé! Diacritics in Emacs</a>. I even wrote my own input method for <a href="https://www.masteringemacs.org/article/inserting-emoji-input-methods">Inserting Emoji</a> just to highlight how powerful and simple the system is.</p>
<pre><code>Additional 'C-x 8' key translations for "æ" and "Æ".
These characters can now be input with 'C-x 8 a e' and 'C-x 8 A E',
respectively, in addition to the existing translations 'C-x 8 / e' and
'C-x 8 / E'.</code></pre>
<p>Wonderful news for people who prefer the original spelling of encyclopedia, and Danes, I suppose.</p>
<pre><code>New 'C-x 8' key translations for "low" quotes "„", and "‚".
These can now be entered with 'C-x , "' and 'C-x , ''.</code></pre>
<pre><code>New German language 'C-x 8' key translations for quotation marks.
The characters "„", "“", and "”" can now be entered with 'C-x 8 v',
'C-x 8 b' and 'C-x 8 n'.  The single versions "‚", "‘", and "’" can now
be entered with 'C-x 8 V', 'C-x 8 B' and 'C-x 8 N'.  These characters
are used for the official German quoting style.  Using them requires
activating German language support via 'iso-transl-set-language'.</code></pre>
<pre><code>"latin-prefix" and "latin-postfix" quotation marks additions.
These input methods can now produce single, double and "low" left and
right quotation marks:

    "‘", "’", "“", "”", "„", and "‚"

by using "[", "]", and "," for "left", "right", and "low" respectively
to modify "'" and """.</code></pre>
<pre><code>"latin-prefix" and "latin-postfix" guillemets support.
These input methods can now produce single guillemets "‹" and "›".  For
"latin-prefix" use "~~&lt;" and "~~&gt;", for "latin-postfix" use "&lt;~" and
"&gt;~".  Double guillemets ("«" and "»") were already supported.</code></pre>
<pre><code>New French language 'C-x 8' key translations for "‹" and "›".
These characters can now be entered using 'C-x 8 ~ &lt;' and 'C-x 8 ~ &gt;',
respectively, after activating French language support via
'iso-transl-set-language'.  Double guillemets were already supported via
'C-x 8 &lt;' and 'C-x 8 &gt;'</code></pre>
<pre><code>Additional 'C-x 8' key translation for Euro "€" currency symbol.
This can now be entered using 'C-x 8 E' in addition to the existing
'C-x 8 * E' translation.</code></pre>
<h2 id="changes-in-specialized-modes-and-packages-in-emacs-30.1">Changes in Specialized Modes and Packages in Emacs 30.1</h2>
<h3 id="outline-mode">Outline mode</h3>
<p>Outline mode is a cut-down outliner for collapsing and expanding headings.</p>
<pre><code>New commands to show/hide outlines by regexp.
'C-c / h' ('outline-hide-by-heading-regexp') asks for a regexp and then
hides the body lines of all outlines whose heading lines match the
regexp.  'C-c / s' ('outline-show-by-heading-regexp') does the inverse:
it shows the bodies of outlines that matched a regexp.</code></pre>
<pre><code>'outline-minor-mode' is supported in tree-sitter major modes.
It can be used in all tree-sitter major modes that set either the
variable 'treesit-simple-imenu-settings' or 'treesit-outline-predicate'.</code></pre>
<p>Excellent news, and likely to be far more robust as it uses tree-sitter’s nodes to determine what to hide and show.</p>
<h3 id="info">Info</h3>
<p>Info mode is Emacs’s implementation of the TexInfo hypertext viewer.</p>
<pre><code>New user option 'Info-url-alist'.
This user option associates manual names with URLs.  It affects the
'Info-goto-node-web' command.  By default, associations for all
Emacs-included manuals are set.  Further associations can be added for
arbitrary Info manuals.</code></pre>
<pre><code>Emacs can now display Info manuals compressed with 'lzip'.
This requires the 'lzip' program to be installed on your system.</code></pre>
<h3 id="gud-grand-unified-debugger">GUD (Grand Unified Debugger)</h3>
<p>GUD is a user interface for debuggers such as GDB.</p>
<pre><code>New user option 'gud-highlight-current-line'.
When enabled, GUD will visually emphasize the line being executed upon
pauses in the debuggee's execution, such as those occasioned by
breakpoints being hit.</code></pre>
<pre><code>New command 'lldb'.
Run the LLDB debugger, analogous to the 'gud-gdb' command.</code></pre>
<pre><code>Variable order and truncation can now be configured in 'gdb-many-windows'.
The new user option 'gdb-locals-table-row-config' allows users to
configure the order and max length of various properties in the local
variables buffer when using 'gdb-many-windows'.

By default, this user option is set to write the properties in the order:
'name', 'type' and 'value', where the 'name' and 'type' are truncated to 20
characters, and the 'value' is truncated according to the value of
'gdb-locals-value-limit'.

If you want to get back the old behavior, set the user option to the value

    (setopt gdb-locals-table-row-config
            `((type . 0) (name . 0) (value . ,gdb-locals-value-limit)))</code></pre>
<pre><code>New user option 'gdb-display-io-buffer'.
If this is nil, command 'gdb' will neither create nor display a separate
buffer for the I/O of the program being debugged, but will instead
redirect the program's interaction to the GDB execution buffer.  The
default is t, to preserve previous behavior.</code></pre>
<h3 id="grep">Grep</h3>
<p>Grep (or rather the interface for talking to and displaying the results from <code>grep</code>) is a powerful feature in Emacs with a wide range of commands.</p>
<pre><code>New user option 'grep-use-headings'.
When non-nil, the output of Grep is split into sections, one for each
file, instead of having file names prefixed to each line.  It is
equivalent to the '--heading' option of some tools such as 'git grep'
and 'rg'.  The headings are displayed using the new 'grep-heading' face.
The default is nil.</code></pre>
<p>Good news if you prefer the heading system used by tools such as <code>rg</code>. I have not tried it yet, but I wonder how easy it is to bulk edit the output of a grep command when it is in header mode.</p>
<h3 id="compilation-mode">Compilation mode</h3>
<p>Compilation mode (via <code>M-x compile</code> usually) is Emacs’s interface for talking to not just compilers but any tool that emits structured output.</p>
<pre><code>The 'omake' matching rule is now disabled by default.
This is because it partly acts by modifying other rules which may
occasionally be surprising.  It can be re-enabled by adding 'omake' to
'compilation-error-regexp-alist'.</code></pre>
<pre><code>Lua errors and stack traces are now recognized.
Compilation mode now recognizes Lua language errors and stack traces.
Every Lua error is recognized as a compilation error, and every Lua
stack frame is recognized as a compilation info.</code></pre>
<p>Emacs already has a huge range of compilation rules, so I’m surprised it didn’t have it for Lua, given how popular it is.</p>
<h3 id="project">Project</h3>
<p>Project is one of many, many file project solutions built into Emacs, and arguably the best one to use. See <code>C-x p C-h</code>.</p>
<pre><code>New user option 'project-mode-line'.
When non-nil, display the name of the current project on the mode
line.  Clicking 'mouse-1' on the project name pops up the project
menu.  The default value is nil.</code></pre>
<p>If you’re not strapped for space, why not enable this?</p>
<pre><code>New user option 'project-file-history-behavior'.
Customizing it to 'relativize' makes commands like 'project-find-file'
and 'project-find-dir' display previous history entries relative to
the current project.</code></pre>
<pre><code>New user option 'project-key-prompt-style'.
The look of the key prompt in the project switcher has been changed
slightly.  To get the previous one, set this option to 'brackets'.</code></pre>
<pre><code>Function 'project-try-vc' tries harder to find the responsible VCS.
When 'project-vc-extra-root-markers' is non-nil, and causes a
subdirectory project to be detected which is not a VCS root, Project now
additionally traverses the parent directories until a VCS root is found
(if any), so that the ignore rules for that repository are used, and
the file listing's performance is still optimized.</code></pre>
<p>Root markers are used to find the boundaries of a project (for example the <code>.git</code> directory, or maybe a file such as a Makefile), and that works well enough for simple use cases. You can tell Emacs to look for other things in addition to its builtin rules, and this is where the contract of a root marker breaks down a little, particularly if you have a repo and another non-repo marker inside of that.</p>
<pre><code>New commands 'project-any-command' and 'project-prefix-or-any-command'.
The former is now bound to 'C-x p o' by default.
The latter is designed primarily for use as a value of
'project-switch-commands'.  If instead of a short menu you prefer to
have access to all keys defined inside 'project-prefix-map', as well
as global bindings (to run other commands inside the project root),
you can add this to your init script:

    (setopt project-switch-commands #'project-prefix-or-any-command)</code></pre>
<p>This is effectively a way of executing a key bindings from the root directory of the current buffer’s root project.</p>
<pre><code>New variable 'project-files-relative-names'.
If it is non-nil, 'project-files' can return file names relative to the
project root.  Project backends can use this to improve the performance
of their 'project-files' implementation.</code></pre>
<h3 id="vc">VC</h3>
<p>VC is Emacs’s excellent generic version control interface. If you do not use <a href="https://www.masteringemacs.org/article/introduction-magit-emacs-mode-git">Magit, an Emacs mode for Git</a>, or if you use a source control that is not git, then I highly recommend you sit down and learn how to use VC. It’s really excellent.</p>
<pre><code>Log-Edit buffers now display a tool bar.
This tool bar contains items for committing log entries and editing or
generating log entries, among other editing operations.</code></pre>
<p>I’ve long advocated using the menu bar because telling beginners to turn it off is <a href="https://www.masteringemacs.org/article/bad-emacs-advice">Bad Emacs Advice</a>. The toolbar is harder to justify turning on as it is a little bit threadbare. Having said that, making it even more contextual is still a very positive thing indeed.</p>
<pre><code>New user option 'vc-git-shortlog-switches'.
This is a string or a list of strings that specifies the Git log
switches for shortlogs, such as the one produced by 'C-x v L'.
'vc-git-log-switches' is no longer used for shortlogs.</code></pre>
<pre><code>New value 'no-backend' for user option 'vc-display-status'.
With this value only the revision number is displayed on the mode-line.</code></pre>
<pre><code>Obsolete command 'vc-switch-backend' re-added as 'vc-change-backend'.
The command was previously obsoleted and unbound in Emacs 28.</code></pre>
<p>It is rare indeed for a command to get obsoleted and the unobsoleted.</p>
<pre><code>Support for viewing VC change history across renames.
When a fileset's VC change history ends at a rename, 'C-x v l' now
prints the old name(s) and shows a button which jumps to the history of
the files under the old names.  This feature is supported for Git and
Hg.  Naturally, 'vc-git-print-log-follow' should be nil for this to work
(or '--follow' should not be in 'vc-hg-log-switches', in Hg's case).
Unlike when the '--follow' switch is used, commands to see the diff of
the old revision ('d'), to check out an old file version ('f') or to
annotate it ('a'), also work on revisions which precede renames.</code></pre>
<pre><code>'vc-annotate' now abbreviates the Git revision in the buffer name.
When using the Git backend, 'vc-annotate' will use an abbreviated
revision identifier in its buffer name.  To restore the previous
behavior, set user option 'vc-annotate-use-short-revision' to nil.</code></pre>
<pre><code>New user option 'vc-git-file-name-changes-switches'.
It allows tweaking the thresholds for rename and copy detection.</code></pre>
<pre><code>VC Directory buffers now display the upstream branch in Git repositories.
The "upstream branch" is the branch from which 'vc-pull' fetches changes
by default.  In Git terms, the upstream branch of branch B is determined
by configuration variables 'branch.B.remote' and 'branch.B.merge'.

When these configuration variables are set for the current branch, the
VC Directory buffer will show the corresponding upstream branch under
the "Tracking" header.</code></pre>
<h3 id="diff-mode">Diff mode</h3>
<p>Diff mode is one of several modes for interacting with and browsing <code>.diff</code> files.</p>
<pre><code>New user option 'diff-refine-nonmodified'.
When this is non-nil, 'diff-refine-hunk' will highlight lines that were
added or removed in their entirety (as opposed to modified lines, where
some parts of the line were modified), using the same faces as for
highlighting the words added and removed within modified lines.  The
default value is nil.</code></pre>
<pre><code>'diff-ignore-whitespace-hunk' can now be applied to all hunks.
When called with a non-nil prefix argument,
'diff-ignore-whitespace-hunk' now iterates over all the hunks in the
current diff, regenerating them without whitespace changes.</code></pre>
<p>Very useful in Lisp, in my opinion, where a change of indentation can easily happen when you change something else. I have git set to ignore whitespace by default to avoid indentation changes gumming up the staging area.</p>
<pre><code>New user option 'diff-ignore-whitespace-switches'.
This allows changing which type of whitespace changes are ignored when
regenerating hunks with 'diff-ignore-whitespace-hunk'.  Defaults to
the previously hard-coded "-b".</code></pre>
<pre><code>New command 'diff-apply-buffer' bound to 'C-c RET a'.
It applies the diff in the entire diff buffer and
saves all modified file buffers.</code></pre>
<p>Good all-round command and saves having to do multiple keystrokes to achieve the same effect.</p>
<h3 id="dired">Dired</h3>
<p>Dired is Emacs’s all-singing, all-dancing directory and file editor, bound to <code>C-x d</code> and <code>M-x dired</code>.</p>
<pre><code>New user option 'dired-movement-style'.
When non-nil, make 'dired-next-line', 'dired-previous-line',
'dired-next-dirline', 'dired-prev-dirline' skip empty lines.
It also controls how to move point when encountering a boundary
(e.g., if every line is visible, invoking 'dired-next-line' at
the last line will move to the first line).  The default is nil.</code></pre>
<p>Oh, this is much-needed. Here’s how it works: you can insert the contents of a directory into an existing dired buffer (find a directory and tap <code>i</code>) and act on the files across the directory boundaries as though they were one directory – very powerful, and it means you can use <a href="https://www.masteringemacs.org/article/dired-shell-commands-find-xargs-replacement">dired to replace find &amp; xargs</a> – but the <code>p</code> and <code>n</code> commands to move between files does not jump between these virtual directories, and so you’re stuck having to move point between a bunch of dead space. Until now, that is. I recommend you set it to <code>bounded</code> or <code>cycle</code>.</p>
<pre><code>New user option 'dired-filename-display-length'.
It is an integer representing the maximum display length of file names.
The middle part of a file name whose length exceeds the restriction is
hidden and an ellipsis is displayed instead.  A value of 'window'
means using the right edge of window as the display restriction.  The
default is nil.</code></pre>
<p>If you squeeze dired into a side bar window, you can now truncate filenames to make them fit to the size of the window, which is a good quality of life improvement.</p>
<pre><code>New user option 'shell-command-guess-functions'.
It defines how to populate a list of commands available
for 'M-!', 'M-&amp;', '!', '&amp;' and the context menu "Open With"
based on marked files in Dired.  Possible backends are
'dired-guess-default', MIME types, XDG configuration
and a universal command such as "open" or "start"
that delegates to the OS.</code></pre>
<p>Emacs has several ways of guessing the right command to run on a filename, and they are annoyingly different and only available in some parts of Emacs and not others. This is a clear attempt to clean up this mess and offer a unified way of picking the right command to run.</p>
<pre><code>New command 'dired-do-open'.
This command is bound to 'E' (mnemonics "External").  Also it can be
used by clicking "Open" in the context menu; it "opens" the marked or
clicked on files according to the OS conventions.  For example, on
systems supporting XDG, this runs 'xdg-open' on the files.</code></pre>
<p>Excellent addition. I used to do <code>!</code> to do this, but now it has its own key.</p>
<pre><code>New variable 'dired-guess-shell-alist-optional'.
It contains commands for external viewers and players for various media
formats, moved to this list from 'dired-guess-shell-alist-default'.</code></pre>
<p>Worth a look if you want Emacs to open your media files in other tools than its defaults.</p>
<pre><code>The default value of 'dired-omit-size-limit' was increased.
After performance improvements to omitting in large directories, the new
default value is 300k, up from 100k.  This means 'dired-omit-mode' will
omit files in directories whose directory listing is up to 300 kilobytes
in size.</code></pre>
<p>Omit mode hides useless files based on regexp filters. Great way to hide backup files, compiler detritus and more. You may have to <code>(require 'dired-x)</code> first.</p>
<pre><code>'dired-listing-switches' handles connection-local values if exist.
This allows you to customize different switches for different remote machines.</code></pre>
<p>This relates to Tramp, Emacs’s remote file editing protocol. Because Tramp sends commands to the remote host, you can now customize how dired should query for files on the remote file system.</p>
<h3 id="ediff">Ediff</h3>
<p>Ediff is another diff tool in Emacs, albeit a far more advanced one. It has a wide range of entrypoints: <code>C-h a ediff</code>.</p>
<pre><code>New user option 'ediff-floating-control-frame'.
If non-nil, try making the control frame be floating rather than tiled.

Many X tiling window managers make the Ediff control frame a tiled
window equal in size to the main Emacs frame, which works poorly.
This option is useful to set if you use such a window manager.</code></pre>
<p>Ediff’s achilles’ heel is its awkward frame that it uses as a control interface; but now it at least grants succor to people who use tiling window managers.</p>
<h3 id="buffer-selection">Buffer Selection</h3>
<p>Buffer selection, not to be confused with <code>C-x b</code>, is one of, ah, several buffer switching tools in Emacs. Try it with <code>M-x bs-show</code>.</p>
<pre><code>New user option 'bs-default-action-list'.
You can now configure how to display the "*buffer-selection*" buffer
using this new option.  (Or set 'display-buffer-alist' directly.)</code></pre>
<h3 id="eshell">Eshell</h3>
<p>Eshell is an Emacs shell written entirely in Emacs. It’s worth <a href="https://www.masteringemacs.org/article/complete-guide-mastering-eshell">Mastering Eshell</a> if you want an all-Emacs command line experience.</p>
<pre><code>You can now run Eshell scripts in batch mode.
By adding the following interpreter directive to an Eshell script, you
can make it executable like other shell scripts:

    #!/usr/bin/env -S emacs --batch -f eshell-batch-file</code></pre>
<p>If you thought someone using korn or c shell wasn’t bad enough, now you can frustrate your coworkers by writing your shell scripts in Eshells’ own zsh-like language.</p>
<pre><code>New builtin Eshell command 'compile'.
This command runs another command, sending its output to a compilation
buffer when the command would output interactively.  This can be useful
when defining aliases so that they produce a compilation buffer when
appropriate, but still allow piping the output elsewhere if desired.
For more information, see the "(eshell) Built-ins" node in the Eshell
manual.</code></pre>
<p>Eshell has a huge range of commands that it exposes to its shell, only to intercept them and pass the input and output to a dedicated Emacs function. See <code>C-u C-h a eshell/</code>.</p>
<pre><code>Eshell's 'env' command now supports running commands.
Like in many other shells, Eshell's 'env' command now lets you run a
command passed as arguments to 'env'.  If you pass any initial
arguments of the form 'VAR=VALUE', 'env' will first set 'VAR' to
'VALUE' before running the command.</code></pre>
<pre><code>Eshell's 'umask' command now supports setting the mask symbolically.
Now, you can pass an argument like "u+w,o-r" to Eshell's 'umask'
command, which will give write permission for owners of newly-created
files and deny read permission for users who are not members of the
file's group.  See the Info node "(coreutils) File permissions" for
more information on this notation.</code></pre>
<pre><code>Performance improvements for interactive output in Eshell.
Interactive output in Eshell should now be significantly faster,
especially for commands that can print large amounts of output
(e.g. "cat").  For external commands, Eshell saves time by only looking
for password prompts in the last 256 characters of each block of output.
To restore the previous behavior when checking for password prompts, set
'eshell-password-prompt-max-length' to 'most-positive-fixnum'.</code></pre>
<pre><code>Eshell built-in commands can now display progress.
Eshell built-in commands like "cat" and "ls" now update the display
periodically while running to show their progress.</code></pre>
<pre><code>New special reference type '#&lt;marker POSITION BUFFER&gt;'.
This special reference type returns a marker at 'POSITION' in
'BUFFER'.  You can insert it by typing or using the new interactive
command 'eshell-insert-marker'.  You can also insert special
references of any type using the new interactive command
'eshell-insert-special-reference'.  See the "(eshell) Arguments" node
in the Eshell manual for more details.</code></pre>
<pre><code>New splice operator for Eshell dollar expansions.
Dollar expansions in Eshell now let you splice the elements of the
expansion in-place using '$@expr'.  This makes it easier to fill lists
of arguments into a command, such as when defining aliases.  For more
information, see the "(eshell) Dollars Expansion" node in the Eshell
manual.</code></pre>
<p>Eshell’s expansion system is inspired by zsh’s, though they’re not at all interchangeable, as Eshell effectively passes things to elisp functions internally, arguments and all, so being able to split, splice and otherwise manipulate how entries are fed to the underlying commands is very important.</p>
<pre><code>You can now splice Eshell globs in-place into argument lists.
By setting 'eshell-glob-splice-results' to a non-nil value, Eshell
will expand glob results in-place as if you had typed each matching
file name individually.  For more information, see the "(eshell)
Globbing" node in the Eshell manual.</code></pre>
<pre><code>Eshell now supports negative numbers and ranges for indices.
Now, you can retrieve the last element of a list with '$my-list[-1]'
or get a sublist of elements 2 through 4 with '$my-list[2..5]'.  For
more information, see the "(eshell) Dollars Expansion" node in the
Eshell manual.</code></pre>
<pre><code>Eshell commands can now be explicitly-remote (or local).
By prefixing a command name in Eshell with a remote identifier, like
"/ssh:user@remote:whoami", you can now run commands on a particular
host no matter your current directory.  Likewise, you can run a
command on your local system no matter your current directory via
"/local:whoami".  For more information, see the "(eshell) Remote Access"
node in the Eshell manual.</code></pre>
<pre><code>Eshell's '$UID' and '$GID' variables are now connection-aware.
Now, when expanding '$UID' or '$GID' in a remote directory, the value
is the user or group ID associated with the remote connection.</code></pre>
<p>One cool feature of Eshell is that you can <code>cd</code> into a remote system using Tramp – for example <code>cd /ssh:homer@powerplant:/etc/</code> – and then operate on the remote machine through Eshell and Tramp as though it were ‘local’. Really powerful, but sometimes there are cracks in the machinery, such as the case of the user and group id environment variables not being correct.</p>
<pre><code>Eshell now uses 'field' properties in its output.
In particular, this means that pressing the '&lt;home&gt;' key moves the
point to the beginning of your input, not the beginning of the whole
line.  If you want to go back to the old behavior, add something like
this to your configuration:

    (keymap-set eshell-mode-map "&lt;home&gt;" #'eshell-bol-ignoring-prompt)

This also means you no longer need to adjust 'eshell-prompt-regexp'
when customizing your Eshell prompt.</code></pre>
<p>I long ago abandoned using <code>C-a</code> and friends to move the beginning of line, preferring <code>M-m</code> instead (as it skips forward past whitespace after moving to the beginning of the line) but this command obviously does not respect the prompt. So if you use <code>M-m</code>, just rebind it to the same key as <code>C-a</code>.</p>
<pre><code>You can now properly unload Eshell.
Calling '(unload-feature 'eshell)' no longer signals an error, and now
correctly unloads Eshell and all of its modules.</code></pre>
<pre><code>'eshell-read-aliases-list' is now an interactive command.
After manually editing 'eshell-aliases-file', you can use this command
to load the edited aliases.</code></pre>
<p>Aliases here referring to the same aliasing you can do in other shells.</p>
<pre><code>'rgrep' is now a builtin Eshell command.
Running 'rgrep' in Eshell now uses the Emacs grep facility instead of
calling external rgrep.</code></pre>
<p><code>M-x rgrep</code> is a command I use all the time when I want more specificity around searching than what a tool like <code>rg</code> gives me. <code>grep</code> already works great in Eshell, so it’s nice to see that <code>rgrep</code> does so too.</p>
<pre><code>If a command exits abnormally, the Eshell prompt now shows its exit code.</code></pre>
<p>I added this to my bash prompt decades ago, as I find it invaluable when I want to compare exit codes in scripts.</p>
<pre><code>New user option 'eshell-history-append'.
If non-nil, each Eshell session will save history by appending new
entries of that session to the history file rather than overwriting
the file with the whole history of the session.  The default is nil.</code></pre>
<h3 id="pcomplete">Pcomplete</h3>
<p>Pcomplete is the completer tool originally built for Eshell, but now has uses elsewhere. It’s a pretty nifty little completion engine, as I demonstrate in <a href="https://www.masteringemacs.org/article/pcomplete-context-sensitive-completion-emacs">PComplete: Context-Sensitive Completion in Emacs</a></p>
<pre><code>New user option 'pcomplete-remote-file-ignore'.
When this option is non-nil, remote file names are not completed by
Pcomplete.  Packages, like 'shell-mode', could set this in order to
suppress remote file name completion at all.</code></pre>
<pre><code>Completion for the 'doas' command has been added.
Command completion for 'doas' in Eshell and Shell mode will now work.</code></pre>
<p>The mooted successor to <code>sudo</code>.</p>
<h3 id="shell-mode">Shell mode</h3>
<p>This is the shell I actually use the most day to day. Delightfully dumb and it behaves exactly like a souped-up Emacs buffer. See <a href="https://www.masteringemacs.org/article/running-shells-in-emacs-overview">Running Shells and Terminal Emulators in Emacs</a> if you want to know what all the differences are.</p>
<pre><code>New user option 'shell-get-old-input-include-continuation-lines'.
When this user option is non-nil, 'shell-get-old-input' ('C-RET')
includes multiple shell "\" continuation lines from command output.
Default is nil.</code></pre>
<pre><code>New user option 'shell-history-file-name'.
When this user option is set to t, 'shell-mode' does not read the shell
history file.  Setting this user option to a string specifies the name
of the shell history file to be read.  A nil value triggers reading the
environment variable 'HISTFILE'.

In a 'shell' buffer, this user option is connection-local.</code></pre>
<p>This is a nice change as Emacs will of course keep its own history (see <a href="https://www.masteringemacs.org/article/shell-comint-secrets-history-commands">Shell &amp; Comint Secrets: History commands</a>) but that history does not persist under normal circumstances. If you’re using a non-default history file for your shell and you want Emacs to respect it, now you can.</p>
<pre><code>Performance improvements for interactive output.
Interactive output in Shell mode now scans more selectively for password
prompts by only examining the last 256 characters of each block of
output, reducing the time spent when printing large amounts of output.
To restore the old behavior, set 'comint-password-prompt-max-length' to
'most-positive-fixnum'.</code></pre>
<p>Anything that speeds up comint is a good thing in my book.</p>
<h3 id="prog-mode">Prog mode</h3>
<p>Prog mode is a generic major mode that programming modes should try to inherit from. It is not intended for normal users.</p>
<pre><code>New command 'prog-fill-reindent-defun'.
This command either fills a single paragraph in a defun, such as a
docstring, or a comment, or (re)indents the surrounding defun if point
is not in a comment or a string.  By default, it is bound to 'M-q' in
'prog-mode' and all its descendants.</code></pre>

<p>Imenu is a lookup feature for structured elements in the current buffer, such as function and class names in code, and headings and sub-headings in text.</p>
<pre><code>New user option 'imenu-flatten'.
It controls whether to flatten the list of sections in an imenu, and
how to display the sections in the flattened list.</code></pre>
<p>If you use a completion mechanism that makes accessing nested elements hard in imenu, you can now request that it flattens the hierarchy.</p>
<pre><code>The sort order of Imenu completions can now be customized.
You can customize the user option 'completion-category-overrides'
and set 'display-sort-function' for the category 'imenu'.</code></pre>
<h3 id="which-function-mode">Which Function mode</h3>
<p>Which function shows the current function in the mode line by default. It works anywhere <code>M-x imenu</code> works.</p>
<pre><code>Which Function mode can now display function names on the header line.
The new user option 'which-func-display' allows choosing where the
function name is displayed.  The default is 'mode' to display in the
mode line.  'header' will display in the header line;
'mode-and-header' displays in both the header line and mode line.</code></pre>
<p>The mode line is crowded and often truncated if you split your windows. Having another place to put this information is useful.</p>
<pre><code>New user option 'which-func-update-delay'.
This replaces the user option 'idle-update-delay', which was previously
used to control the delay before 'which-function-mode' updated its
display.  The user option 'idle-update-delay', which was only used by
Which Function mode, is now obsolete.</code></pre>
<h3 id="tramp">Tramp</h3>
<p>Tramp is Emacs’s remote file editing facility.</p>
<pre><code>Tramp methods can be optional.
An optional connection method is not enabled by default.  The user must
enable it explicitly by the 'tramp-enable-method' command.  The existing
methods "fcp", "krlogin", " ksu" and "nc" are optional now.</code></pre>
<pre><code>New optional connection method "androidsu".
This provides access to system files with elevated privileges granted by
the idiosyncratic 'su' implementations and system utilities customary on
Android.</code></pre>
<p>Tramp supports a dizzying array of “connection” methods, including things that aren’t “connections” in the traditional sense, such as <code>sudo</code> and now <code>androidsu</code>.</p>
<pre><code>New optional connection method "run0".
This connection method is similar to "sudo", but it uses the 'systemd'
framework internally.</code></pre>
<pre><code>New connection methods "dockercp" and "podmancp".
These are the external methods counterparts of "docker" and "podman".</code></pre>
<p>Tramp added native docker support a version or two ago, which is super handy for editing or viewing stuff inside a container from the comfort of your own Emacs.</p>
<pre><code>New optional connection methods for containers.
There are new optional connection methods "toolbox", "distrobox",
"flatpak", "apptainer" and "nspawn".  They allow accessing system
containers provided by Toolbox or Distrobox, sandboxes provided by
Flatpak, instances managed by Apptainer, or accessing systemd-based
light-weight containers..</code></pre>
<p>Good to see more support for namespace-based container solutions other than docker.</p>
<pre><code>Connection method "kubernetes" supports now optional container name.
The host name for Kubernetes connections can be of kind [CONTAINER.]POD,
in order to specify a dedicated container.  If there is just the pod
name, the first container in the pod is taken.  The new user options
'tramp-kubernetes-context' and 'tramp-kubernetes-namespace' allow
accessing pods with different context or namespace but the default one.</code></pre>
<p>Tramp can chain connections, so you can connect to a bastion host first before you connect to kubernetes pods.</p>
<pre><code>Rename 'tramp-use-ssh-controlmaster-options' to 'tramp-use-connection-share'.
The old name still exists as obsolete variable alias.  This user
option controls now connection sharing for both ssh-based and
plink-based methods.  It allows the values t, nil, and 'suppress'.
The latter suppresses also "ControlMaster" settings in the user's
"~/.ssh/config" file, or connection share configuration in PuTTY
sessions, respectively.</code></pre>
<p>You absolutely must enable connection sharing (controlmaster) if you use ssh. It makes a huge difference in performance.</p>
<pre><code>New command 'tramp-cleanup-some-buffers'.
It kills only a subset of opened remote buffers, subject to the user
option 'tramp-cleanup-some-buffers-hook'.</code></pre>
<pre><code>New command 'inhibit-remote-files'.
This command disables the handling of file names with the special
remote file name syntax.  It should be applied only when remote files
won't be used in this Emacs instance.  It provides a slightly improved
performance of file name handling in Emacs.</code></pre>
<pre><code>New macro 'without-remote-files'.
This macro could wrap code which handles local files only.  Due to the
temporary deactivation of remote files, it results in a slightly
improved performance of file name handling in Emacs.</code></pre>
<pre><code>New user option 'tramp-completion-multi-hop-methods'.
It contains a list of connection methods for which completion should
be attempted at the end of a multi-hop chain.  This allows completion
candidates to include a list of, for example, containers running on a
remote docker host.</code></pre>
<p>Oh, cool! One common criticism I had of multi-hop/chaining connections is that you really need to know what you’re looking for when you are writing out these multi-hop connections, as the completion support is non-existent. Good to see that Tramp will now attempt to complete via the multi-hop links you have already written. Neat!</p>
<pre><code>New command 'tramp-revert-buffer-with-sudo'.
It reverts the current buffer to visit with "sudo" permissions.  The
buffer must either visit a file, or it must run 'dired-mode'.  Another
method but "sudo" can be configured with user option
'tramp-file-name-with-method'.</code></pre>
<p>I’ve had a snippet of code called <code>sudo</code> that does exactly this for what must be a decade plus now. Now it’s built in, albeit with a harder-to-remember name.</p>
<pre><code>Direct asynchronous processes are indicated by a connection-local variable.
If direct asynchronous processes shall be used, set the connection-local
variable 'tramp-direct-async-process' to a non-nil value.  In previous
Emacs versions this was indicated by the connection property
"direct-async-process".  That connection property (though not connection
properties and 'tramp-connection-properties' in general) is now
deprecated.  See the Tramp manual "(tramp) Improving performance of
asynchronous remote processes".</code></pre>
<p>If you have performance issues with Tramp, you should try this out.</p>
<pre><code>Direct asynchronous processes use 'tramp-remote-path'.
When a direct asynchronous process is invoked, it uses 'tramp-remote-path'
for setting the remote 'PATH' environment variable.</code></pre>
<h3 id="shr">SHR</h3>
<p>SHR is Emacs’s HTML rendering engine that powers EWW, the Emacs Web Wowser.</p>
<pre><code>New user option 'shr-fill-text'.
When 'shr-fill-text' is non-nil (the default), SHR will fill text
according to the width of the window.  If you customize it to nil, SHR
will leave the text as-is; in that case, EWW will automatically enable
'visual-line-mode' when displaying a page so that long lines are
visually wrapped at word boundaries.</code></pre>
<h3 id="eww">EWW</h3>
<p><code>M-x eww</code> is Emacs’s builtin web browser.</p>
<pre><code>New mouse bindings in EWW buffers.
Certain form elements that were displayed as buttons, yet could only be
activated by keyboard input, are now operable using 'mouse-2'.  With
"Submit" buttons, this triggers submission of the form, while clicks on
other classes of buttons either toggle their values or prompt for user
input, as the case may be.</code></pre>
<pre><code>EWW text input fields and areas are now fields.
In consequence, movement commands and OS input method features now
recognize and confine their activities to the text input field around
point.  See also the Info node "(elisp) Fields".</code></pre>
<p>Fields here meaning the widget system that Emacs has built in.</p>
<pre><code>'eww-open-file' can now display the file in a new buffer.
By default, the command reuses the "*eww*" buffer, but if called with
the new argument NEW-BUFFER non-nil, it will use a new buffer instead.
Interactively, invoke 'eww-open-file' with a prefix argument to
activate this behavior.</code></pre>
<pre><code>'eww' URL or keyword prompt now has tab completion.
The interactive minibuffer prompt when invoking 'eww' now has support
for tab completion.</code></pre>
<pre><code>'eww' URL and keyword prompt now completes suggested URIs and bookmarks.
The interactive minibuffer prompt when invoking 'eww' now provides
completions from 'eww-suggest-uris'.  'eww-suggest-uris' now includes
bookmark URIs.</code></pre>
<pre><code>New command 'eww-copy-alternate-url'.
It copies an alternate link on the page currently visited in EWW into
the kill ring.  Alternate links are optional metadata that HTML pages
use for linking to their alternative representations, such as translated
versions or associated RSS feeds.  It is bound to 'A' by default.</code></pre>
<pre><code>'eww-open-in-new-buffer' supports the prefix argument.
When invoked with the prefix argument ('C-u'),
'eww-open-in-new-buffer' will not make the new buffer the current one.
This is useful for continuing reading the URL in the current buffer
when the new URL is fetched.</code></pre>
<pre><code>History navigation in EWW now behaves as in other browsers.
Previously, when navigating back and forward through page history, EWW
would add a duplicate entry to the end of the history list each time.
This made it impossible to navigate to the "end" of the history list.
Now, navigating through history in EWW simply changes your position in
the history list, allowing you to reach the end as expected.  In
addition, when browsing to a new page from a "historical" one (i.e., a
page loaded by navigating back through history), EWW deletes the history
entries newer than the current page.  To change the behavior when
browsing from "historical" pages, you can customize
'eww-before-browse-history-function'.</code></pre>
<pre><code>'eww-readable' now toggles display of the readable parts of a web page.
When called interactively, 'eww-readable' toggles whether to display
only the readable parts of a page or the full page.  With a positive
prefix argument, it always displays the readable parts, and with a zero
or negative prefix, it always displays the full page.</code></pre>
<p>This feature has been around for ages, and it’s great if you want to remove all the chaff from a page. Note that the only real change here is that it toggles.</p>
<pre><code>New user option 'eww-readable-urls'.
This is a list of regular expressions matching the URLs where EWW should
display only the readable parts by default.  For more details, see
"(eww) Basics" in the EWW manual.</code></pre>
<p>Great stuff if you regularly visit pages that require toggling on readable mode.</p>
<pre><code>New user option 'eww-readable-adds-to-history'.
When non-nil (the default), calling 'eww-readable' adds a new entry to
the EWW page history.</code></pre>
<h3 id="go-ts-mode">Go-ts mode</h3>
<p>This is the tree-sitter version for Go.</p>
<pre><code>New command 'go-ts-mode-docstring'.
This command adds a docstring comment to the current defun.  If a
comment already exists, point is only moved to the comment.  It is
bound to 'C-c C-d' in 'go-ts-mode'.</code></pre>
<h3 id="man-mode">Man mode</h3>
<p>Man referring here to the manual page tool <code>man</code>.</p>
<pre><code>New user option 'Man-prefer-synchronous-call'.
When this is non-nil, run the 'man' command synchronously rather than
asynchronously (which is the default behavior).</code></pre>
<pre><code>New user option 'Man-support-remote-systems'.
This option controls whether the man page is formatted on the remote
system when the current buffer's default-directory is remote.  You can
invoke the 'man' command with a prefix argument to countermand the
value of this option for the current invocation of 'man'.</code></pre>
<h3 id="docview">DocView</h3>
<p>DocView is a generic tool for converting complex documents into images for display in Emacs (pdf, docx, etc.)</p>
<pre><code>New user option 'doc-view-mpdf-use-svg'.
If non-nil, DocView uses SVG images to display PDF documents.  The
default is non-nil if your system supports display of SVG images.</code></pre>
<p>Emacs’s SVG engine is quite good nowadays, and it’s definitely better than a raster image of a PDF.</p>
<pre><code>New face 'doc-view-svg-face'.
This replaces 'doc-view-svg-foreground' and 'doc-view-svg-background'.
By default, this face has black foreground on white background and
inherits from the default face.  When unsetting the foreground and
background values, the display in DocView is styled according to the
current theme.  However, this, or any non-standard values, can result in
poor contrast for documents which aren't simply black text on white
background.</code></pre>
<p>If your theme clashes with the colors in SVG files, you can now change them to better suit your theme.</p>
<pre><code>DocView buffers now display a new tool bar.
This tool bar contains options for searching and navigating within the
document, replacing the incompatible items for incremental search and
editing within the default tool bar displayed in the past.</code></pre>
<h3 id="shortdoc">Shortdoc</h3>
<p>Shortdoc is <a href="https://www.masteringemacs.org/article/emacs-builtin-elisp-cheat-sheet">Emacs’s Builtin Elisp Cheat Sheet</a>.</p>
<pre><code>New function 'shortdoc-function-examples'.
This function returns examples of use of a given Emacs Lisp function
from the available shortdoc information.</code></pre>
<pre><code>New function 'shortdoc-help-fns-examples-function'.
This function inserts into the current buffer examples of use of a
given Emacs Lisp function, which it gleans from the shortdoc
information.  If you want 'describe-function' ('C-h f') to insert
examples of using the function into regular "*Help*" buffers, add the
following to your init file:

    (add-hook 'help-fns-describe-function-functions
              #'shortdoc-help-fns-examples-function)</code></pre>
<h3 id="package">Package</h3>
<p>Package is the package manager and installer in Emacs.</p>
<pre><code>New user option 'package-vc-register-as-project'.
When non-nil, 'package-vc-install' and 'package-vc-checkout' will
automatically register every package they install as a project, that you
can quickly select using 'project-switch-project' ('C-x p p').  Default
is t.</code></pre>
<p>Oh that is very nice. I will definitely enable this now that <code>package</code> can install from source control using <code>vc</code>.</p>
<pre><code>New user option 'package-vc-allow-build-commands'.
Controls for which packages Emacs runs extra build commands when
installing directly from the package VCS repository.</code></pre>
<pre><code>New command 'package-vc-log-incoming'.
This commands displays incoming changes for a VC package without
modifying the current checkout.</code></pre>
<pre><code>New command to start an inferior Emacs loading only specific packages.
The new command 'package-isolate' will start a new Emacs process, as
a sub-process of Emacs where you invoke the command, in a way that
causes the new process to load only some of the installed packages.
The command prompts for the packages to activate in this
sub-process, and is intended for testing Emacs and/or the packages
in a clean environment.</code></pre>
<p>That’s a really cool addition, and I look forward to using it with my own packages. I normally maintain a separate Emacs installation just for this purpose, but this approach might save me the hassle of having to maintain that.</p>
<h3 id="flymake">Flymake</h3>
<p>Flymake is Emacs’s linter/on-the-fly-compiler interface.</p>
<pre><code>New user option 'flymake-indicator-type'.
This controls which error indicator type Flymake should use in the
current buffer.  Depending on your preference, this can either use
fringes or margins for indicating errors, the default is 'margins'.</code></pre>
<pre><code>New user option 'flymake-margin-indicators-string'.
It controls, for each error type, the string and its face to display as
the margin indicator.</code></pre>
<pre><code>New user option 'flymake-autoresize-margins'.
If non-nil (the default), Flymake will resize the margins when
'flymake-mode' is turned on or off.
Only relevant if 'flymake-indicator-type' is set to 'margins'.</code></pre>
<pre><code>New user option 'flymake-margin-indicator-position'.
It controls whether to use margins for margin indicators, and which
margin (left or right) to use.  Default is to use the left margin.</code></pre>
<pre><code>New user option 'flymake-show-diagnostics-at-end-of-line'.
When non-nil, Flymake shows summarized descriptions of diagnostics at
the end of the line.  Depending on your preference, this can either be
distracting and easily confused with actual code, or a significant
early aid that relieves you from moving the buffer or reaching for the
mouse to consult an error message.  Default is nil.</code></pre>
<p>Worth enabling to see if it is something that helps or hinders. You should consider setting it in a dir locals variable or mode hook so it only applies to some buffers.</p>
<h3 id="flyspell">Flyspell</h3>
<p>Emacs’s on the fly spell checker.</p>
<pre><code>New user option 'flyspell-check-changes'.
When non-nil, Flyspell mode spell-checks only words that you edited; it
does not check unedited words just because you move point across them.
Default is nil.</code></pre>
<h3 id="js-mode">JS mode</h3>
<p>One of a dozen modes that edit Javascript.</p>
<pre><code>The binding 'M-.' has been removed from the major mode keymaps in
'js-mode' and 'js-ts-mode', having it default to the global binding
which calls 'xref-find-definitions'.  If the previous one worked
better for you, use 'define-key' in your init script to bind
'js-find-symbol' to that combination again.</code></pre>
<p>This is just standardizing on the xref interface and its default global keys.</p>
<p>:</p>
<pre><code>'js-json-mode' does not derive from 'js-mode' any more so as not
to confuse tools like Eglot or YASnippet into thinking that those
buffers contain Javascript code.</code></pre>
<h3 id="python-mode">Python mode</h3>
<p>The major mode for editing Python.</p>
<pre><code>New user option 'python-indent-block-paren-deeper'.
If non-nil, increase the indentation of the lines inside parens in a
header of a block when they are indented to the same level as the body
of the block, producing:

    if (some_expression
            and another_expression):
        do_something()

instead of:

    if (some_expression
        and another_expression):
        do_something()

Default is nil.</code></pre>
<p>I long ago stopped worrying about this stuff when code formatting tools became prevalent. Why get angry at your coworkers’ bad code formatting habits when you can direct your anger at a faceless code formatting tool instead?</p>
<pre><code>New user option 'python-interpreter-args'.
This allows the user to specify command line arguments to the non
interactive Python interpreter specified by 'python-interpreter'.</code></pre>
<pre><code>New function 'python-shell-send-block'.
It sends the python block delimited by 'python-nav-beginning-of-block'
and 'python-nav-end-of-block' to the inferior Python process.</code></pre>
<h3 id="inferior-python-mode">Inferior Python mode</h3>
<p>Inferior Python mode refers to the comint version that talks to the external python interpreter.</p>
<pre><code>Default value of 'python-shell-compilation-regexp-alist' is changed.
Support for Python's ExceptionGroup has been added, so in the Python
shell, the line indicating the source of an error in the error messages
from ExceptionGroup will be recognized as well.</code></pre>
<h3 id="eldoc">Eldoc</h3>
<p>Eldoc is a generic documentation popup system.</p>
<pre><code>'eldoc' no longer truncates to a single line by default.
Previously, the entire docstring was not available to eldoc, which made
'eldoc-echo-area-use-multiline-p' ineffective.  The old behavior may be
kept by customizing 'eldoc-echo-area-use-multiline-p'.</code></pre>
<h3 id="scheme-mode">Scheme mode</h3>
<pre><code>Scheme mode now handles the regular expression literal '#/regexp/' that
 is available in some Scheme implementations.
 Also, it should now handle nested sexp-comments.</code></pre>
<h3 id="use-package">Use package</h3>
<p><a href="https://www.masteringemacs.org/article/spotlight-use-package-a-declarative-configuration-tool">use-package is a declarative configuration tool</a>.</p>
<pre><code>New ':vc' keyword.
This keyword enables the user to install packages using package-vc.el.</code></pre>
<p>This is a great addition and it rounds off the ability to manually install packages with VC that arrived in Emacs 29. You can replace a lot of the third-party package managers now if your needs are simple.</p>
<pre><code>New user option 'use-package-vc-prefer-newest'.
If non-nil, always install the newest commit of a package when using the
':vc' keyword rather than its stable release.  Default is nil.</code></pre>
<h3 id="gnus">Gnus</h3>
<p>Gnus is an expansive mail and news agent in Emacs.</p>
<pre><code>New backend 'nnfeed'.
This allows backend developers to easily create new backends for web
feeds, as inheriting backends of 'nnfeed'.</code></pre>
<pre><code>New backend 'nnatom'.
This allow users to add Atom Syndication Format feeds to Gnus as
servers.</code></pre>
<pre><code>The 'nnweb-type' option 'gmane' has been removed.
The gmane.org website is, sadly, down since a number of years with no
prospect of it coming back.  Therefore, it is no longer valid to set
the server variable 'nnweb-type' to 'gmane'.</code></pre>
<pre><code>New user option 'gnus-mode-line-logo'.
This allows the user to either disable the display of any logo or
specify which logo will be displayed as part of the
buffer-identification in the mode-line of Gnus buffers.</code></pre>
<pre><code>'gnus-summary-limit-to-age' now counts days since midnight.
"Less than 1 day" now means "since last midnight", rather than "less
than 24 hours old".</code></pre>
<h3 id="rmail">Rmail</h3>
<p>Rmail is one of many ways of sending and reading email in Emacs.</p>
<pre><code>New commands for reading mailing lists.
The new Rmail commands 'rmail-mailing-list-post',
'rmail-mailing-list-unsubscribe', 'rmail-mailing-list-help', and
'rmail-mailing-list-archive' allow, respectively, posting to,
unsubscribing from, requesting help about, and browsing the archives
of, the mailing list from which the current email message was
delivered.</code></pre>
<h3 id="dictionary">Dictionary</h3>
<p>Dictionary is a lookup system for <a href="https://www.masteringemacs.org/article/wordsmithing-in-emacs">wordsmiths</a>.</p>
<pre><code>New user option 'dictionary-search-interface'.
Controls how the 'dictionary-search' command prompts for and displays
dictionary definitions.  Customize this user option to 'help' to have
'dictionary-search' display definitions in a "*Help*" buffer and
provide dictionary-based minibuffer completion for word selection.
Default is nil, which means to use a "*Dictionary*" buffer.</code></pre>
<pre><code>New user option 'dictionary-read-word-prompt'.
This allows the user to customize the prompt that is used by
'dictionary-search' when asking for a word to search in the
dictionaries.</code></pre>
<pre><code>New user option 'dictionary-display-definition-function'.
This allows the user to customize the way in which 'dictionary-search'
displays word definitions.  If non-nil, this user option should be set
to a function that displays a word definition obtained from a
dictionary server.  The new function
'dictionary-display-definition-in-help-buffer' can be used to display
the definition in a "*Help*" buffer, instead of the default
"*Dictionary*" buffer.</code></pre>
<pre><code>New user option 'dictionary-read-word-function'.
This allows the user to customize the way in which 'dictionary-search'
prompts for a word to search in the dictionary.  This user option
should be set to a function that lets the user select a word and
returns it as a string.  The new function
'dictionary-completing-read-word' can be used to prompt with
completion based on dictionary matches.</code></pre>
<pre><code>New user option 'dictionary-read-dictionary-function'.
This allows the user to customize the way in which 'dictionary-search'
prompts for a dictionary to search in.  This user option should be set
to a function that lets the user select a dictionary and returns its
name as a string.  The new function
'dictionary-completing-read-dictionary' can be used to prompt with
completion based on dictionaries that the server supports.</code></pre>
<pre><code>The default value of 'dictionary-tooltip-dictionary' has changed.
The new default value is t, which means use the same dictionary as the
value of 'dictionary-default-dictionary'.  The previous default value
was nil, which effectively disabled 'dictionary-tooltip-mode', even if
the mode was turned on.</code></pre>
<h3 id="pp">Pp</h3>
<p>Pp is the pretty printing functionality for elisp.</p>
<pre><code>New 'pp-default-function' user option replaces 'pp-use-max-width'.
Its default value is 'pp-fill', a new default pretty-printing function,
which tries to obey 'fill-column'.</code></pre>
<pre><code>'pp-to-string' takes an additional PP-FUNCTION argument.
This argument specifies the prettifying algorithm to use.</code></pre>
<pre><code>'pp' and 'pp-to-string' now always include a terminating newline.
In the past they included a terminating newline in most cases but not all.</code></pre>
<h3 id="emacs-lisp-mode">Emacs Lisp mode</h3>
<p>The major mode used to edit elisp code.</p>
<pre><code>'elisp-flymake-byte-compile' is disabled for untrusted files.
For security reasons, this backend can be used only in those files
specified as trusted according to 'trusted-content' and emits an
"untrusted content" warning otherwise.
This fixes CVE-2024-53920.</code></pre>
<pre><code>',@' now has 'prefix' syntax.
Previously, the '@' character, which normally has 'symbol' syntax,
would combine with a following Lisp symbol and interfere with symbol
searching.</code></pre>
<pre><code>'emacs-lisp-docstring-fill-column' now defaults to 72.
It was previously 65.  The new default formats documentation strings to
fit on fewer lines without negatively impacting readability.</code></pre>
<h3 id="cperl-mode">CPerl mode</h3>
<p>One of two Perl modes in Emacs.</p>
<pre><code>Subroutine signatures are now supported.
CPerl mode fontifies subroutine signatures like variable declarations
which makes them visually distinct from subroutine prototypes.</code></pre>
<pre><code>Syntax of Perl up to version 5.40 is supported.
CPerl mode supports the new keywords for exception handling and the
object oriented syntax which were added in Perl 5.36, 5.38 and 5.40.</code></pre>
<pre><code>New user option 'cperl-fontify-trailer'.
This user option takes the values 'perl-code' or 'comment' and treats
text after an "__END__" or "__DATA__" token accordingly.  The default
value of 'perl-code' is useful for trailing POD and for AutoSplit
modules, the value 'comment' makes CPerl mode treat trailers as
comment, like Perl mode does.</code></pre>
<pre><code>New command 'cperl-file-style'.
This command sets the indentation style for the current buffer.  To
change the default style, either use the user option with the same name
or use the command 'cperl-set-style'.</code></pre>
<pre><code>New minor mode 'cperl-extra-paired-delimiters-mode'.
Perl 5.36 and newer allows using more than 200 non-ASCII paired
delimiters for quote-like constructs, e.g. "q«text»".  Use this minor
mode in buffers where this feature is activated.</code></pre>
<pre><code>Commands using the Perl Info manual are obsolete.
The Perl documentation in Info format is no longer distributed with
Perl or on CPAN since more than 10 years.  Perl documentation can be
read with 'cperl-perldoc' instead.</code></pre>
<pre><code>Highlighting trailing whitespace has been removed.
The user option 'cperl-invalid-face' is now obsolete, and does
nothing.  See the user option 'show-trailing-whitespace' instead.</code></pre>
<h3 id="emacs-sessions-desktop">Emacs Sessions (Desktop)</h3>
<p>Desktop saves and restores your Emacs buffers and files between sessions.</p>
<pre><code>Restoring buffers visiting remote files can now time out.
When a buffer is restored which visits a remote file, the restoration
of the session could hang if the remote host is off-line or slow to
respond.  Setting the user option 'remote-file-name-access-timeout' to
a positive number will abandon the attempt to restore such buffers
after a timeout of that many seconds, thus allowing the rest of
desktop restoration to continue.</code></pre>
<p>I have absolutely been caught out by this before, so that is a welcome change indeed.</p>
<h3 id="recentf">Recentf</h3>
<p>Recentf remembers files you have visited and lets you re-visit them quickly.</p>
<pre><code>Checking recent remote files can now time out.
Similarly to buffer restoration by Desktop, 'recentf-mode' checking
of the accessibility of remote files can now time out if
'remote-file-name-access-timeout' is set to a positive number.</code></pre>
<h3 id="image-dired">Image Dired</h3>
<p>One of several ways of displaying images in Emacs.</p>
<pre><code>New user option 'image-dired-thumb-naming'.
You can now configure how thumbnails are named using this option.</code></pre>
<h3 id="ert">ERT</h3>
<p>This is a test runner framework for Emacs, and a rather useful one at that.</p>
<pre><code>New macro 'skip-when' to skip 'ert-deftest' tests.
This can help to avoid some awkward skip conditions.  For example
'(skip-unless (not noninteractive))' can be changed to the easier
to read '(skip-when noninteractive)'.</code></pre>
<pre><code>Syntax highlighting unit testing support.
An ERT extension ('ert-font-lock') now provides support for face
assignment unit testing.  For more information, see the "(ert) Syntax
Highlighting Tests" node in the ERT manual.</code></pre>
<h3 id="socks">Socks</h3>
<p>SOCKS is a proxy-like protocol that, quite frankly, I don’t think I’ve used in decades.</p>
<pre><code>Socks supports version 4a.
The 'socks-server' user option accepts '4a' as a value for its version
field.</code></pre>
<h3 id="edmacro">Edmacro</h3>
<p>This is the major mode for interactively editing keyboard macros.</p>
<pre><code>New command 'edmacro-set-macro-to-region-lines'.
Bound to 'C-c C-r', this command replaces the macro text with the
lines of the region.  If needed, the region is extended to include
whole lines.  If the region ends at the beginning of a line, that last
line is excluded.</code></pre>
<pre><code>New user option 'edmacro-reverse-macro-lines'.
When this is non-nil, the lines of key sequences are displayed with
the most recent line first.  This is can be useful when working with
macros with many lines, such as from 'kmacro-edit-lossage'.</code></pre>
<h3 id="calc">Calc</h3>
<p>This is Emacs’s advanced symbolic computer algebra system.</p>
<pre><code>Calc parses fractions written using U+2044 FRACTION SLASH.
Fractions of the form "123⁄456" are handled as if written "123:456".
Note in particular the difference in behavior from U+2215 DIVISION SLASH
and U+002F SOLIDUS, which result in division rather than a rational
fraction.  In addition, precomposed fraction characters, such as ½
(U+00BD VULGAR FRACTION ONE HALF), are also recognized as rational
fractions.  (They have been recognized since 2004, but it looks like it
was never mentioned in the NEWS, or even the Calc manual.)</code></pre>
<h3 id="ielm">IELM</h3>
<p>IELM is the interactive shell for <a href="https://www.masteringemacs.org/article/evaluating-elisp-emacs">Evaluating Elisp in Emacs</a>.</p>
<pre><code>IELM now remembers input history between sessions.
The new user option 'ielm-history-file-name' is the name of the file
where IELM input history will be saved.  Customize it to nil to revert
to the old behavior of not remembering input history between sessions.</code></pre>
<h3 id="easypg">EasyPG</h3>
<p>EasyPG is a wrapper for GnuPG and friends. See <a href="https://www.masteringemacs.org/article/keeping-secrets-in-emacs-gnupg-auth-sources">Keeping Secrets in Emacs with GnuPG and Auth Sources</a>.</p>
<pre><code>New user option 'epa-keys-select-method'.
This allows the user to customize the key selection method, which can be
either by using a pop-up buffer or from the minibuffer.  The pop-up
buffer method is the default, which preserves previous behavior.</code></pre>
<h3 id="widget">Widget</h3>
<p>This is the internal library used by Emacs for buttons, edit fields, etc. in places like the Customize interface.</p>
<pre><code>New face 'widget-unselected'.
Customize this face to a non-default value to visually distinguish the
labels of unselected active radio-button or checkbox widgets from the
labels of unselected inactive widgets (the default value inherits from
the 'widget-inactive' face).</code></pre>
<pre><code>New user option 'widget-skip-inactive'.
If non-nil, moving point forward or backward between widgets by typing
'TAB' or 'S-TAB' skips over inactive widgets.  The default value is nil.</code></pre>
<h3 id="ruby-mode">Ruby mode</h3>
<p>Programming mode for Ruby.</p>
<pre><code>New user option 'ruby-rubocop-use-bundler'.
By default it retains the previous behavior: read the contents of
Gemfile and act accordingly.  But you can also set it to t or nil to
skip checking the Gemfile.</code></pre>
<pre><code>New user option 'ruby-bracketed-args-indent'.
When it is set to nil, multiple consecutive open braces/brackets/parens
result in only one additional indentation level.  Default is t.</code></pre>
<h3 id="thingatpt">Thingatpt</h3>
<p>A framework for plucking the thing at point: words, functions, emails, urls, you name it. A useful thing to learn about and use, as it has a wide range of applications.</p>
<pre><code>New variables for providing custom thingatpt implementations.
The new variables 'bounds-of-thing-at-point-provider-alist' and
'forward-thing-provider-alist' now allow defining custom implementations
of 'bounds-of-thing-at-point' and 'forward-thing', respectively.</code></pre>
<pre><code>New helper functions for text property-based thingatpt providers.
The new helper functions 'thing-at-point-for-char-property',
'bounds-of-thing-at-point-for-char-property', and
'forward-thing-for-char-property' can help to implement custom thingatpt
providers for "things" that are defined by text properties.</code></pre>
<pre><code>'bug-reference-mode' now supports 'thing-at-point'.
Now, calling '(thing-at-point 'url)' when point is on a bug reference
will return the URL for that bug.</code></pre>
<p>Bug reference mode is a small minor mode that makes ‘bug references’ (such as #2823) clickable.</p>

<p>Yet another buffer selection and display system in Emacs.</p>
<pre><code>New user option 'Buffer-menu-group-by'.
It controls how buffers are divided into groups that are displayed with
headings using Outline minor mode.  Using commands that mark buffers
on the outline heading line will mark all buffers in the outline.  By
default, no grouping is performed.</code></pre>
<pre><code>New command 'Buffer-menu-toggle-internal'.
This command toggles the display of internal buffers in Buffer Menu mode;
that is, buffers not visiting a file and whose names start with a space.
Previously, such buffers were never shown.  This command is bound to 'I'
in Buffer Menu mode.</code></pre>
<h3 id="miscellaneous-1">Miscellaneous</h3>
<pre><code>New user option 'rcirc-log-time-format'.
This allows for rcirc logs to use a custom timestamp format, which the
chat buffers use by default.</code></pre>
<p>RCIRC is one of two IRC clients in Emacs, the other being ERC.</p>
<pre><code>'ffap-lax-url' now defaults to nil.
Previously, it was set to t, but this broke remote file name detection.</code></pre>
<p>FFAP is Find File At Point is a system for plucking the file at point and opening it.</p>
<pre><code>More control on automatic update of Proced buffers.
The user option 'proced-auto-update-flag' can now be set to an
additional value 'visible', which controls automatic updates of Proced
buffers that are displayed in some window.</code></pre>
<p>Proced’s basically Emacs’s answer to <code>top</code>. See <a href="https://www.masteringemacs.org/article/displaying-interacting-processes-proced">Displaying and Interacting with processes using Proced</a>.</p>
<pre><code>nXML Mode now comes with schemas for Mono/.NET development.
The following new XML schemas are now supported:
- MSBuild project files
- Dotnet package properties files
- Dotnet resource extension files
- Dotnet Application config files
- Nuget config file
- Nuget package specification file
- Nuget packages config file</code></pre>
<pre><code>color.el now supports the Oklab color representation.</code></pre>
<p>The color file has a large range of advanced color conversion routines. Worth a look if that is something you interact with a lot.</p>
<pre><code>New user option 'xwidget-webkit-disable-javascript'.
This allows disabling JavaScript in xwidget Webkit sessions.</code></pre>
<p>XWidget is a half-abandoned, half-working attempt at bringing the Webkit browser to Emacs.</p>
<pre><code>'ls-lisp--insert-directory' supports more long options of 'ls'.
'ls-lisp--insert-directory', the ls-lisp implementation of
'insert-directory', now supports the '--time=TIME' and '--sort=time'
options of GNU 'ls'.</code></pre>
<p>How does dired work on platforms (such as MS-DOS or Windows) where there is no <code>ls</code> program? The answer is simple: Emacs has an elisp emulation layer that talks to the underlying system directly.</p>
<pre><code>'M-x ping' can now give additional flags to the 'ping' program.
Typing 'C-u M-x ping' prompts first for the host, and then for the flags
to give to the 'ping' command.</code></pre>
<p>There is a host of network utility wrappers in Emacs. See <a href="https://www.masteringemacs.org/article/network-utilities-emacs">Using the commandline network utilities from Emacs</a>.</p>
<pre><code>Webjump now assumes URIs are HTTPS instead of HTTP.
For links in 'webjump-sites' without an explicit URI scheme, it was
previously assumed that they should be prefixed with "http://".  Such
URIs are now prefixed with "https://" instead.</code></pre>
<p>Another obscure feature in Emacs. Here it’s a flat list of common URLs and a command to select and then open one of them. The fact that “Yahoo” is still in there says it all.</p>
<pre><code>Added prefixes in titdic-cnv library.
Most of the variables and functions in the file have been renamed to
make sure they all use a 'tit-' namespace prefix.</code></pre>
<p>Obscure library for working with Quail, a format used in multilingual systems.</p>
<pre><code>'xref-revert-buffer' is now an alias of 'revert-buffer'.
The Xref buffer now sets up 'revert-buffer-function' such that
'revert-buffer' behaves like 'xref-revert-buffer' did in previous Emacs
versions, and the latter is now an alias of the former.</code></pre>
<p>Xref is Emacs’s cross-referencing system, and all this change does is make it so <code>M-x revert-buffer</code> (which I have a version of that reverts the current buffer bound to <code>F6</code>) reverts properly in xref buffers also.</p>
<pre><code>The Makefile browser is now obsolete.
The command 'makefile-switch-to-browser' command is now obsolete,
together with related commands used in the "*Macros and Targets*"
buffer.  We recommend using an alternative like 'imenu' instead.</code></pre>
<pre><code>'jsonrpc-default-request-timeout' is now a defcustom.</code></pre>
<h2 id="new-modes-and-packages-in-emacs-30.1">New Modes and Packages in Emacs 30.1</h2>
<h3 id="new-major-modes-based-on-the-tree-sitter-library">New major modes based on the tree-sitter library</h3>
<pre><code>New major mode 'elixir-ts-mode'.
A major mode based on the tree-sitter library for editing Elixir files.</code></pre>
<pre><code>New major mode 'heex-ts-mode'.
A major mode based on the tree-sitter library for editing HEEx files.</code></pre>
<pre><code>New major mode 'html-ts-mode'.
An optional major mode based on the tree-sitter library for editing
HTML files.</code></pre>
<pre><code>New major mode 'lua-ts-mode'.
A major mode based on the tree-sitter library for editing Lua files.</code></pre>
<pre><code>New major mode 'php-ts-mode'.
A major mode based on the tree-sitter library for editing PHP files.</code></pre>
<p>Good to see more tree-sitter major modes.</p>
<pre><code>New package EditorConfig.
This package provides support for the EditorConfig standard,
an editor-neutral way to provide directory local (project-wide) settings.
It is enabled via a new global minor mode 'editorconfig-mode'
which makes Emacs obey the '.editorconfig' files.
There is also a new major mode 'editorconfig-conf-mode'
to edit those configuration files.</code></pre>
<p>Long overdue, even if I find the capabilities of editorconfig rather basic.</p>
<pre><code>New global minor mode 'etags-regen-mode'.
This minor mode generates the tags table automatically based on the
current project configuration, and later updates it as you edit the
files and save the changes.</code></pre>
<pre><code>New package 'which-key'.
The 'which-key' package from GNU ELPA is now included in Emacs.  It
implements the global minor mode 'which-key-mode' that displays a table
of key bindings upon entering a partial key chord and waiting for a
moment.  For example, after enabling the minor mode, if you enter 'C-x'
and wait for one second, the minibuffer will expand with all available
key bindings that follow 'C-x' (or as many as space allows).</code></pre>
<p>This is a long-awaited inclusion for some people. Which-key pops up a list of key bindings if you partially type a key and a wait a little while.</p>
<pre><code>New minor mode 'completion-preview-mode'.
This minor mode shows you symbol completion suggestions as you type,
using an inline preview.  New user options in the 'completion-preview'
customization group control exactly when Emacs displays this preview.
'completion-preview-mode' is buffer-local, to enable it globally use
'global-completion-preview-mode'.</code></pre>
<p>Buried in the foot of the NEWS file is one of the best new additions in Emacs. A window-based answer to Company / Corfu, with a host of really nice ergonomic features. I really, really recommend you disable your in-line completer (corfu, company, auto-complete, whatever) and give this one a try for a little while.</p>
<pre><code>New package Window-Tool-Bar.
This provides a new minor mode, 'window-tool-bar-mode'.  When this minor
mode is enabled, a tool bar is displayed at the top of a window.  To
conserve space, no tool bar is shown if 'tool-bar-map' is nil.  The
global minor mode 'global-window-tool-bar-mode' enables this minor mode
in all buffers.</code></pre>
<p>A compromise for people who want the toolbar but only when it has something interesting to offer.</p>
<pre><code>New library Track-Changes.
This library is a layer of abstraction above 'before-change-functions'
and 'after-change-functions' which provides a superset of
the functionality of 'after-change-functions':
- It provides the actual previous text rather than only its length.
- It takes care of accumulating and bundling changes until a time when
  its client finds it convenient to react to them.
- It detects most cases where some changes were not properly
  reported (calls to 'before/after-change-functions' that are
  incorrectly paired, missing, etc...) and reports them adequately.</code></pre>
<p>Building this sort of thing is surprisingly tricky, as the text above briefly alludes to. It does not seem to be used in many places yet, but I foresee that it will play an important role in tools that want to provide in-buffer tracking of changes without the tears of having to build all this infrastructure from scratch. I especially like the feature that it collects the changes until such a time that the client deems it needs them; very useful, and hard to do well with nothing more than a set of before/after change hooks.</p>
<pre><code>New global minor mode 'minibuffer-regexp-mode'.
This is a minor mode for editing regular expressions in the minibuffer,
for example in 'query-replace-regexp'.  It correctly highlights parens
via 'show-paren-mode' and 'blink-matching-paren' in a user-friendly way,
avoids reporting alleged paren mismatches and makes sexp navigation more
intuitive.  It is enabled by default, 'minibuffer-regexp-prompts' can be
used to tune when it takes effect.</code></pre>
<p>Excellent feature that is enabled by default, so no need to do anything.</p>
<pre><code>The highly accessible Modus themes collection has eight items.
The 'modus-operandi' and 'modus-vivendi' are the main themes that have
been part of Emacs since version 28.  The former is light, the latter
dark.  In addition to these, we now have 'modus-operandi-tinted' and
'modus-vivendi-tinted' for easier legibility, as well as
'modus-operandi-deuteranopia', 'modus-vivendi-deuteranopia',
'modus-operandi-tritanopia', and 'modus-vivendi-tritanopia' to cover
the needs of users with red-green or blue-yellow color deficiency.
The Info manual "(modus-themes) Top" describes the details and
showcases all their user options.</code></pre>
<p>A popular set of themes beloved by many.</p>
<h3 id="new-library-peg">New library PEG</h3>
<pre><code>Emacs now includes a library for writing Parsing Expression
Grammars (PEG), an approach to text parsing that provides more structure
than regular expressions, but less complexity than context-free
grammars.  The Info manual "(elisp) Parsing Expression Grammars" has
documentation and examples.</code></pre>
<p>Interesting inclusion and I am curious to see it in use. Emacs already has parsing tools already (Semantic Bovinator being one such example) so I guess this will find a home in Org mode as it has a range of custom query languages.</p>
<pre><code>New major mode 'shell-command-mode'.
This mode is used by default for the output of asynchronous 'shell-command'.
To revert to the previous behavior, set the (also new) variable
'async-shell-command-mode' to 'shell-mode'.  Any hooks or mode-specific
variables used should be adapted appropriately.</code></pre>
<pre><code>New package Compat.
Emacs now comes with a stub implementation of the
forwards-compatibility Compat package from GNU ELPA.  This allows
built-in packages to use the library more effectively, and helps
preventing the installation of Compat if unnecessary.</code></pre>
<p>Compat provides backwards and forwards compatible changes for package authors so they can target older versions of Emacs, and keep older packages working in newer ones, too.</p>
<h3 id="incompatible-lisp-changes-in-emacs-30.1">Incompatible Lisp Changes in Emacs 30.1</h3>
<pre><code>Evaluating a 'lambda' returns an object of type 'interpreted-function'.
Instead of representing interpreted functions as lists that start with
either 'lambda' or 'closure', Emacs now represents them as objects
of their own 'interpreted-function' type, which is very similar
to 'byte-code-function' objects (the argument list, docstring, and
interactive forms are placed in the same slots).
Lists that start with 'lambda' are now used only for non-evaluated
functions (in other words, for source code), but for backward compatibility
reasons, 'functionp' still recognizes them as functions and you can
still call them as before.
Thus code that attempts to "dig" into the internal structure of an
interpreted function's object with the likes of 'car' or 'cdr' will
no longer work and will need to use 'aref' instead to extract its
various subparts (when 'interactive-form', 'documentation', and
'help-function-arglist' aren't adequate).</code></pre>
<p>The heart of this is that in Lisp, code and data are the same, and when Emacs emits a readable form of a lambda or closure you just created, it does so in a way that looks like a list. In older Emacsen, you could write <code>(car (lambda))</code> and it’d spit out <code>closure</code> (provided <code>lexical-binding</code> is enabled), and now you cannot do this; it’s not a list any more.</p>
<p>It’s very unlikely this will affect you in any way.</p>
<pre><code>The escape sequence '\x' not followed by hex digits is now an error.
Previously, '\x' without at least one hex digit denoted character code
zero (NUL) but as this was neither intended nor documented or even
known by anyone, it is now treated as an error by the Lisp reader.</code></pre>
<pre><code>'subr-native-elisp-p' is renamed to 'native-comp-function-p'.
The previous name still exists but is marked as obsolete.</code></pre>
<pre><code>'define-globalized-minor-mode' requires that modes use 'run-mode-hooks'.
Minor modes defined with 'define-globalized-minor-mode', such as
'global-font-lock-mode', will not be enabled any more in those buffers
whose major modes fail to use 'run-mode-hooks'.  Major modes defined
with 'define-derived-mode' are not affected.  'run-mode-hooks' has been the
recommended way to run major mode hooks since Emacs 22.</code></pre>
<pre><code>'buffer-match-p' and 'match-buffers' take '&amp;rest ARGS'.
They used to take a single '&amp;optional ARG' and were documented to use
an unreliable hack to try and support condition predicates that
don't accept this optional ARG.
The new semantics makes no such accommodation, but the code still
supports it (with a warning) for backward compatibility.</code></pre>
<pre><code>'post-gc-hook' runs after updating 'gcs-done' and 'gc-elapsed'.</code></pre>
<pre><code>Connection-local variables are applied in buffers visiting remote files.
This overrides possible directory-local or file-local variables with
the same name.</code></pre>
<pre><code>'copy-tree' now copies records when its optional 2nd argument is non-nil.</code></pre>
<pre><code>Regexp zero-width assertions followed by operators are better defined.
Previously, regexps such as "xy\\B*" would have ill-defined behavior.
Now any operator following a zero-width assertion applies to that
assertion only (which is useless).  For historical compatibility, an
operator character following '^' or '\`' becomes literal, but we
advise against relying on this.</code></pre>
<pre><code>Infinities and NaNs no longer act as symbols on non-IEEE platforms.
On old platforms like the VAX that do not support IEEE floating-point,
tokens like '0.0e+NaN' and '1.0e+INF' are no longer read as symbols.
Instead, the Lisp reader approximates an infinity with the nearest
finite value, and a NaN with some other non-numeric object that
provokes an error if used numerically.</code></pre>
<pre><code>Conversion of strings to and from byte-arrays works with multibyte strings.
The functions 'dbus-string-to-byte-array' and
'dbus-byte-array-to-string' now accept and return multibyte Lisp
strings, encoding to UTF-8 and decoding from UTF-8 internally.  This
means that the argument to 'dbus-byte-array-to-string' must be a valid
UTF-8 byte sequence, and the optional parameter MULTIBYTE of
'dbus-byte-array-to-string' is now obsolete and unused.  The argument of
'dbus-string-to-byte-array' should be a regular Lisp string, not a
unibyte string.</code></pre>
<pre><code>'minibuffer-allow-text-properties' now can be set buffer-local.
'read-from-minibuffer' and functions that use it can take the
buffer-local value from the minibuffer.</code></pre>
<pre><code>'minibuffer-allow-text-properties' now also affects completions.
When it has a non-nil value, then completion functions like
'completing-read' don't discard text properties from the returned
completion candidate.</code></pre>
<pre><code>X color support compatibility aliases are now obsolete.
The compatibility aliases 'x-defined-colors', 'x-color-defined-p',
'x-color-values', and 'x-display-color-p' are now obsolete.</code></pre>
<pre><code>'easy-mmode-define-{minor,global}-mode' aliases are now obsolete.
Use 'define-minor-mode' and 'define-globalized-minor-mode' instead.</code></pre>
<pre><code>The 'millisec' argument of 'sleep-for' is now obsolete.
Use a float value for the first argument instead.</code></pre>
<pre><code>User options 'eshell-NAME-unload-hook' are now obsolete.
These hooks were named incorrectly, and so they never actually ran
when unloading the corresponding feature.  Instead, you should use
hooks named after the feature name, like 'esh-mode-unload-hook'.</code></pre>
<pre><code>User options 'eshell-process-wait-{seconds,milliseconds}' are now obsolete.
Instead, use 'eshell-process-wait-time', which supports floating-point
values.</code></pre>
<pre><code>User option 'tramp-completion-reread-directory-timeout' has been removed.
This user option was obsoleted in Emacs 27, use
'remote-file-name-inhibit-cache' instead.</code></pre>
<pre><code>The obsolete calling convention of 'sit-for' has been removed.
That convention was: '(sit-for SECONDS MILLISEC &amp;optional NODISP)'.</code></pre>
<pre><code>'defadvice' is marked as obsolete.
See the "(elisp) Porting Old Advice" Info node for help converting
them to use 'advice-add' or 'define-advice' instead.</code></pre>
<p>Emacs added a new advice system many versions ago; it’s better, and I should get around to removing the old advice forms…</p>
<pre><code>'cl-old-struct-compat-mode' is marked as obsolete.
You may need to recompile your code if it was compiled with Emacs &lt; 24.3.</code></pre>
<pre><code>Old derived.el functions removed.
The following functions have been deleted because they were only used
by code compiled with Emacs &lt; 21:
'derived-mode-init-mode-variables', 'derived-mode-merge-abbrev-tables',
'derived-mode-merge-keymaps', 'derived-mode-merge-syntax-tables',
'derived-mode-run-hooks', 'derived-mode-set-abbrev-table',
'derived-mode-set-keymap', 'derived-mode-set-syntax-table',
'derived-mode-setup-function-name'.

</code></pre>
<h3 id="lisp-changes-in-emacs-30.1">Lisp Changes in Emacs 30.1</h3>
<pre><code>The 'wheel-up/down/left/right' events are now bound unconditionally.
The 'mouse-wheel-up/down/left/right-event' variables are thus used only
to specify the 'mouse-4/5/6/7' events that might still happen to be
generated by some old packages (or if 'mouse-wheel-buttons' has been set
to nil).</code></pre>
<pre><code>Xterm Mouse mode now emits 'wheel-up/down/right/left' events.
This is instead of 'mouse-4/5/6/7' events for the mouse wheel.  It uses
the new variable 'mouse-wheel-buttons' to decide which button maps to
which wheel event (if any).</code></pre>
<pre><code>In batch mode, tracing now sends the trace to stdout.</code></pre>
<pre><code>New hook 'hack-dir-local-get-variables-functions'.
This can be used to provide support for other directory-local settings
beside ".dir-locals.el".</code></pre>
<pre><code>'auto-coding-functions' can know the name of the file.
The functions on this hook can now find the name of the file to
which the text belongs by consulting the variable 'auto-coding-file-name'.</code></pre>
<pre><code>New user option 'compilation-safety' to control safety of native code.
It is now possible to control how safe is the code generated by native
compilation, by customizing this user option.  It is also possible to
control this at function granularity by using the new 'safety' parameter
in the function's 'declare' form.</code></pre>
<pre><code>New types 'closure' and 'interpreted-function'.
'interpreted-function' is the new type used for interpreted functions,
and 'closure' is the common parent type of 'interpreted-function'
and 'byte-code-function'.

Those new types come with the associated new predicates 'closurep' and
'interpreted-function-p' as well as a new constructor
'make-interpreted-closure'.</code></pre>
<pre><code>New function 'help-fns-function-name'.
For named functions, it just returns the name and otherwise
it returns a short "unique" string that identifies the function.
In either case, the string is propertized so clicking on it gives
further details.</code></pre>
<pre><code>New function 'char-to-name'.
This is a convenience function to return the Unicode name of a char (if
it has one).</code></pre>
<pre><code>New function 'cl-type-of'.
This function is like 'type-of' except that it sometimes returns
a more precise type.  For example, for nil and t it returns 'null'
and 'boolean' respectively, instead of just 'symbol'.</code></pre>
<pre><code>New functions 'primitive-function-p' and 'cl-functionp'.
'primitive-function-p' is like 'subr-primitive-p' except that it returns
t only if the argument is a function rather than a special-form,
and 'cl-functionp' is like 'functionp' except it returns nil
for lists and symbols.</code></pre>
<pre><code>Built-in types now have corresponding classes.
At the Lisp level, this means that things like '(cl-find-class 'integer)'
will now return a class object, and at the UI level it means that
things like 'C-h o integer RET' will show some information about that type.</code></pre>
<pre><code>New variable 'major-mode-remap-defaults' and function 'major-mode-remap'.
The first is like Emacs-29's 'major-mode-remap-alist' but to be set by
packages (instead of users).  The second looks up those two variables.</code></pre>
<p>Yet more complexity around how to remap major modes to accommodate tree-sitter major modes.</p>
<pre><code>Pcase's functions (in 'pred' and 'app') can specify the argument position.
For example, instead of '(pred (&lt; 5))' you can write '(pred (&gt; _ 5))'.</code></pre>
<p>Ah good. I always found it odd that it would bind to the left-most argument, but now I can specify which one I want.</p>
<pre><code>'define-advice' now sets the new advice's 'name' property to NAME.
Named advices defined with 'define-advice' can now be removed with
'(advice-remove SYMBOL NAME)' in addition to '(advice-remove SYMBOL
SYMBOL@NAME)'.</code></pre>
<pre><code>New function 'require-with-check' to detect new versions shadowing.
This is like 'require', but it checks whether the argument 'feature'
is already loaded, in which case it either signals an error or
forcibly reloads the file that defines the feature.</code></pre>
<pre><code>New variable 'lisp-eval-depth-reserve'.
It puts a limit to the amount by which Emacs can temporarily increase
'max-lisp-eval-depth' when handling signals.</code></pre>
<pre><code>New special form 'handler-bind'.
It provides a functionality similar to 'condition-case' except it runs
the handler code without unwinding the stack, such that we can record
the backtrace and other dynamic state at the point of the error.  See
the Info node "(elisp) Handling Errors".</code></pre>
<pre><code>New text properties add tooltips on fringes.
It is now possible to provide tooltips on fringes by adding special text
properties 'left-fringe-help' and 'right-fringe-help'.  See the "(elisp)
Special Properties" Info node in the Emacs Lisp Reference Manual for
more details.</code></pre>
<pre><code>New 'display-buffer' action alist entry 'pop-up-frames'.
This has the same effect as the variable of the same name and takes
precedence over the variable when present.</code></pre>
<pre><code>New function 'merge-ordered-lists'.
Mostly used internally to do a kind of topological sort of
inheritance hierarchies.</code></pre>
<pre><code>'drop' is now an alias for the function 'nthcdr'.</code></pre>
<pre><code>New polymorphic comparison function 'value&lt;'.
This function returns non-nil if the first argument is less than the
second.  It works for any two values of the same type with reasonable
ordering for numbers, strings, symbols, bool-vectors, markers, buffers
and processes.  Conses, lists, vectors and records are ordered
lexicographically.
It is intended as a convenient ordering predicate for sorting, and is
likely to be faster than hand-written Lisp functions.</code></pre>
<p>Ah that is very useful actually. Now to remember that it exists…</p>
<pre><code>New 'sort' arguments and features.
The 'sort' function can now be called using the signature

    (sort SEQ &amp;rest KEYWORD-ARGUMENTS)

where arguments after the first are keyword/value pairs, all optional:
':key' specifies a function that produces the sorting key from an element,
':lessp' specifies the ordering predicate, defaulting to 'value&lt;',
':reverse' is used to reverse the sorting order,
':in-place' is used for in-place sorting, as the default is now to
sort a copy of the input.

The new signature is less error-prone and reduces the need to write
ordering predicates by hand.  We recommend that you use the ':key'
argument instead of ':lessp' unless a suitable ordering predicate is
already available.  This can also be used for multi-key sorting:

    (sort seq :key (lambda (x) (list (age x) (size x) (cost x))))

sorts by the return value of 'age', then by 'size', then by 'cost'.

The old signature, '(sort SEQ PREDICATE)', can still be used and sorts
its input in-place as before.</code></pre>
<pre><code>New API for 'derived-mode-p' and control of the graph of major modes</code></pre>
<pre><code>'derived-mode-p' now takes the list of modes as a single argument.
The same holds for 'provided-mode-derived-p'.
The old calling convention where multiple modes are passed as
separate arguments is deprecated.</code></pre>
<pre><code>New functions to access the graph of major modes.
While 'define-derived-mode' still only supports single inheritance,
modes can declare additional parents (for tests like 'derived-mode-p')
with 'derived-mode-add-parents'.
Accessing the 'derived-mode-parent' property directly is now
deprecated in favor of the new functions 'derived-mode-set-parent'
and 'derived-mode-all-parents'.</code></pre>
<p>This complexity is yet again borne out of tree-sitter’s introduction to Emacs. I do wonder if all this complexity (remember diamond pattern inheritance in OO?) will come back to bite us in a few years… I hope not.</p>
<pre><code>Drag-and-drop functions can now be called once for compound drops.
It is now possible for drag-and-drop handler functions to respond to
drops incorporating more than one URL.  Functions capable of this must
set their 'dnd-multiple-handler' symbol properties to a non-nil value.
See the Info node "(elisp) Drag and Drop".

The function 'dnd-handle-one-url' has been made obsolete, since it
cannot take these new handlers into account.</code></pre>
<pre><code>'notifications-notify' can use Icon Naming Specification for ':app-icon'.
You can use a symbol as the value for ':app-icon' to provide icon name
without specifying a file, like this:

    (notifications-notify
      :title "I am playing music" :app-icon 'multimedia-player)</code></pre>
<pre><code>New function 're-disassemble' to see the innards of a regexp.
If you built Emacs with '--enable-checking', you can use this to help
debug either your regexp performance problems or the regexp engine.</code></pre>
<pre><code>XLFDs are no longer restricted to 255 characters.
'font-xlfd-name' now returns an XLFD even if it is greater than 255
characters in length, provided that the LONG_XLFDs argument is true.
Other features in Emacs which employ XLFDs have been modified to
produce and understand XLFDs larger than 255 characters.</code></pre>
<pre><code>New macro 'static-if' for conditional evaluation of code.
This macro hides a form from the evaluator or byte-compiler based on a
compile-time condition.  This is handy for avoiding byte-compilation
warnings about code that will never actually run under some conditions.</code></pre>
<pre><code>Desktop notifications are now supported on the Haiku operating system.
The new function 'haiku-notifications-notify' provides a subset of the
capabilities of the 'notifications-notify' function in a manner
analogous to 'w32-notification-notify'.</code></pre>
<pre><code>New Haiku specific variable 'haiku-pass-control-tab-to-system'.
This sets whether Emacs should pass 'C-TAB' on to the system instead of
handling it, fixing a problem where window switching would not activate
if an Emacs frame had focus on the Haiku operating system.  Default
value is t.</code></pre>
<pre><code>New value 'if-regular' for the REPLACE argument to 'insert-file-contents'.
It results in 'insert-file-contents' erasing the buffer instead of
preserving markers if the file being inserted is not a regular file,
rather than signaling an error.</code></pre>
<pre><code>New variable 'current-key-remap-sequence'.
It is bound to the key sequence that caused a call to a function bound
within 'function-key-map' or 'input-decode-map' around those calls.</code></pre>
<pre><code>The function 'key-translate' can now remove translations.
If the second argument TO is nil, the existing key translation is
removed.</code></pre>
<pre><code>New variables describing the names of built in programs.
The new variables 'ctags-program-name', 'ebrowse-program-name',
'etags-program-name', 'hexl-program-name', 'emacsclient-program-name'
'movemail-program-name', and 'rcs2log-program-name' should be used
instead of "ctags", "ebrowse", "etags", "hexl", "emacsclient", and
"rcs2log", when starting one of these built in programs in a subprocess.</code></pre>
<pre><code>New variable 'case-symbols-as-words' affects case operations for symbols.
If non-nil, then case operations such as 'upcase-initials' or
'replace-match' (with nil FIXEDCASE) will treat the entire symbol name
as a single word.  This is useful for programming languages and styles
where only the first letter of a symbol's name is ever capitalized.
The default value of this variable is nil.</code></pre>
<pre><code>Bytecode is now always loaded eagerly.
Bytecode compiled with older Emacs versions for lazy loading using
'byte-compile-dynamic' is now loaded all at once.
As a consequence, 'fetch-bytecode' has no use, does nothing, and is
now obsolete.  The variable 'byte-compile-dynamic' has no effect any
more; compilation will always yield bytecode for eager loading.</code></pre>
<pre><code>Returned strings from functions and macros are never docstrings.
Functions and macros whose bodies consist of a single string literal now
only return that string, and will not use it as a docstring.  Example:

    (defun sing-a-song ()
      "Sing a song.")

The above function returns the string "Sing a song." and has no
docstring.  Previously, that string was used as both the docstring and
return value, which was never what the programmer wanted.  If you want
the string to be a docstring, add an explicit return value.

This change applies to 'defun', 'defsubst', 'defmacro' and 'lambda'
forms; other defining forms such as 'cl-defun' already worked this way.</code></pre>
<h3 id="new-or-changed-byte-compilation-warnings">New or changed byte-compilation warnings</h3>
<pre><code>Warn about missing 'lexical-binding' directive.
The compiler now warns if an Elisp file lacks the standard
'-*- lexical-binding: ... -*-' cookie on the first line.
This line typically looks something like

    ;;; My little pony mode  -*- lexical-binding: t -*-

It is needed to inform the compiler about which dialect of ELisp
your code is using: the modern dialect with lexical binding or
the old dialect with only dynamic binding.

Lexical binding avoids some name conflicts and allows the compiler to
detect more mistakes and generate more efficient code, so it is
recommended.  For how to adapt your code to lexical binding, see the
manual section "(elisp) Converting to Lexical Binding".

If your code cannot be converted to lexical binding, you can insert
the line

    ;;; -*- lexical-binding: nil -*-

first in the file to declare that it uses the old dialect.</code></pre>
<pre><code>Warn about empty bodies for more special forms and macros.
The compiler now warns about an empty body argument to 'when',
'unless', 'ignore-error' and 'with-suppressed-warnings' in addition to
the existing warnings for 'let' and 'let*'.  Example:

    (when (&gt; x 2))

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'empty-body'.</code></pre>
<pre><code>Warn about quoted error names in 'condition-case' and 'ignore-error'.
The compiler now warns about quoted condition (error) names
in 'condition-case' and 'ignore-error'.  Example:

    (condition-case nil
        (/ x y)
      ('arith-error "division by zero"))

Quoting them adds the error name 'quote' to those handled or ignored
respectively, which was probably not intended.</code></pre>
<pre><code>Warn about comparison with literal constants without defined identity.
The compiler now warns about comparisons by identity with a literal
string, cons, vector, record, function, large integer or float as this
may not match any value at all.  Example:

    (eq x "hello")

Only literals for symbols and small integers (fixnums), including
characters, are guaranteed to have a consistent (unique) identity.
This warning applies to 'eq', 'eql', 'memq', 'memql', 'assq', 'rassq',
'remq' and 'delq'.

To compare by (structural) value, use 'equal', 'member', 'assoc',
'rassoc', 'remove' or 'delete' instead.  Floats and bignums can also
be compared using 'eql', '=' and 'memql'.  Function literals cannot be
compared reliably at all.

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'suspicious'.</code></pre>
<pre><code>Warn about 'condition-case' without handlers.
The compiler now warns when the 'condition-case' form is used without
any actual handlers, as in

    (condition-case nil (read buffer))

because it has no effect other than the execution of the body form.
In particular, no errors are caught or suppressed.  If the intention
was to catch all errors, add an explicit handler for 'error', or use
'ignore-error' or 'ignore-errors'.

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'suspicious'.</code></pre>
<pre><code>Warn about 'unwind-protect' without unwind forms.
The compiler now warns when the 'unwind-protect' form is used without
any unwind forms, as in

    (unwind-protect (read buffer))

because the behavior is identical to that of the argument; there is
no protection of any kind.  Perhaps the intended unwind forms have
been misplaced or forgotten, or the use of 'unwind-protect' could be
simplified away.

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'suspicious'.</code></pre>
<pre><code>Warn about useless trailing 'cond' clauses.
The compiler now warns when a 'cond' form contains clauses following a
default (unconditional) clause.  Example:

    (cond ((= x 0) (say "none"))
          (t (say "some"))
          (say "goodbye"))

Such a clause will never be executed, and is likely to be a mistake,
perhaps due to misplaced parens.

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'suspicious'.</code></pre>
<pre><code>Warn about mutation of constant values.
The compiler now warns about code that modifies program constants in
some obvious cases.  Examples:

    (setcar '(1 2) 7)
    (aset [3 4] 0 8)
    (aset "abc" 1 ?d)

Such code may have unpredictable behavior because the constants are part
of the program and are not data structures generated afresh during
execution; the compiler does not expect them to change.

To avoid the warning, operate on an object created by the program
(maybe a copy of the constant), or use a non-destructive operation
instead.

This warning can be suppressed using 'with-suppressed-warnings' with
the warning name 'mutate-constant'.</code></pre>
<pre><code>Warn about more ignored function return values.
The compiler now warns when the return value from certain functions is
implicitly ignored.  Example:

    (progn (nreverse my-list) my-list)

will elicit a warning because it is usually pointless to call
'nreverse' on a list without using the returned value.

To silence the warning, make use of the value in some way, such as
assigning it to a variable.  You can also wrap the function call in
'(ignore ...)', or use 'with-suppressed-warnings' with the warning
name 'ignored-return-value'.

The warning will only be issued for calls to functions declared
'important-return-value' or 'side-effect-free' (but not 'error-free').</code></pre>
<pre><code>Warn about docstrings that contain control characters.
The compiler now warns about docstrings with control characters other
than newline and tab.  This is often a result of improper escaping.
Example:

    (defun my-fun ()
      "Uses c:\remote\dir\files and the key \C-x."
      ...)

where the docstring contains the four control characters 'CR', 'DEL',
'FF' and 'C-x'.

The warning name is 'docstrings-control-chars'.</code></pre>
<pre><code>The warning about wide docstrings can now be disabled separately.
Its warning name is 'docstrings-wide'.</code></pre>
<pre><code>'fset', 'defalias' and 'defvaralias' now signal an error for cyclic aliases.
Previously, 'fset', 'defalias' and 'defvaralias' could be made to
build circular function and variable indirection chains as in

    (defalias 'able 'baker)
    (defalias 'baker 'able)

but trying to use them would sometimes make Emacs hang.  Now, an attempt
to create such a loop results in an error.

Since circular alias chains now cannot occur, 'function-alias-p',
'indirect-function' and 'indirect-variable' will never signal an error.
Their 'noerror' arguments have no effect and are therefore obsolete.</code></pre>
<h3 id="touch-screen-support">Touch Screen support</h3>
<pre><code>'x-popup-menu' now understands touch screen events.
When a 'touchscreen-begin' or 'touchscreen-end' event is passed as the
POSITION argument, it will behave as if that event was a mouse event.</code></pre>
<pre><code>New functions for handling touch screen events.
The new functions 'touch-screen-track-tap' and 'touch-screen-track-drag'
handle tracking common touch screen gestures from within a command.</code></pre>
<pre><code>New parameter to 'touchscreen-end' events.
CANCEL non-nil establishes that the touch sequence has been intercepted
by programs such as window managers and should be ignored with Emacs.</code></pre>
<pre><code>New variable 'inhibit-auto-fill' to temporarily prevent auto-fill.</code></pre>
<pre><code>New variable 'secondary-tool-bar-map'.
If non-nil, this variable contains a keymap of menu items that are
displayed along tool bar items defined by 'tool-bar-map'.  These items
are displayed below the tool bar if the value of 'tool-bar-position' is
'top', and above it if the value is 'bottom'.  This is used by
'modifier-bar-mode'.</code></pre>
<pre><code>New variable 'completion-lazy-hilit'.
Lisp programs that present completion candidates may bind this
variable non-nil around calls to functions such as
'completion-all-completions'.  This tells the underlying completion
styles to skip eager fontification of completion candidates, which
improves performance.  Such a Lisp program can then use the
'completion-lazy-hilit' function to fontify candidates just in time.</code></pre>
<pre><code>New primitive 'buffer-last-name'.
It returns the name of a buffer before the last time it was renamed or
killed.</code></pre>
<pre><code>New primitive 'marker-last-position'.
It returns the last position of a marker in its buffer even if that
buffer has been killed.  ('marker-position' would return nil in that
case.)</code></pre>
<h3 id="functions-and-variables-to-transpose-sexps">Functions and variables to transpose sexps</h3>
<p>Juri Linkov did an awful lot of work to try and salvage the rather broken tree-sitter “sexp” commands and I think he’s done a stellar job, considering how hard it is to do <a href="https://www.masteringemacs.org/article/combobulate-structured-movement-editing-treesitter">Structured Movement and Editing with Tree-Sitter</a> in a way that works the way users expect it work.</p>
<pre><code>New helper variable 'transpose-sexps-function'.
Lisp programs can now set this variable to customize the behavior of the
'transpose-sexps' command.</code></pre>
<pre><code>New function 'transpose-sexps-default-function'.
The previous implementation of 'transpose-sexps' was moved into its own
function, to be used in 'transpose-sexps-function'.</code></pre>
<pre><code>New function 'treesit-transpose-sexps'.
Tree-sitter now unconditionally sets 'transpose-sexps-function' for all
tree-sitter enabled modes to this function.</code></pre>
<h3 id="functions-and-variables-to-move-by-program-statements">Functions and variables to move by program statements</h3>
<pre><code>New variable 'forward-sentence-function'.
Major modes can now set this variable to customize the behavior of the
'forward-sentence' command.</code></pre>
<pre><code>New function 'forward-sentence-default-function'.
The previous implementation of 'forward-sentence' is moved into its
own function, to be bound by 'forward-sentence-function'.</code></pre>
<pre><code>New function 'treesit-forward-sentence'.
All tree-sitter enabled modes that define 'sentence' in
'treesit-thing-settings' now set 'forward-sentence-function' to call
'treesit-forward-sentence'.</code></pre>
<h3 id="functions-and-variables-to-move-by-program-sexps">Functions and variables to move by program sexps</h3>
<pre><code>New function 'treesit-forward-sexp'.
Tree-sitter conditionally sets 'forward-sexp-function' for major modes
that have defined 'sexp' in 'treesit-thing-settings' to enable
sexp-related motion commands.</code></pre>
<pre><code>New user option 'native-comp-async-warnings-errors-kind'.
It allows control of what kinds of warnings and errors from asynchronous
native compilation are reported to the parent Emacs process.  The
default is to report all errors and only important warnings.  If you
were used to customizing 'native-comp-async-report-warnings-errors' to
nil or 'silent', we suggest that you now leave it at its default value,
and see if you get only warnings that matter.</code></pre>
<pre><code>New 'ftype' function declaration.
The declaration '(ftype TYPE)' specifies the type of a function.
Example:

    (defun hello (x y)
      (declare (ftype (function (integer boolean) string)))
      ...)

specifies that the function takes two arguments, an integer and a
boolean, and returns a string.  If the compilation happens with
'compilation-safety' set to zero, this information can be used by the
native compiler to produce better code, but specifying an incorrect type
may lead to Emacs crashing.  See the Info node "(elisp) Declare Form"
for further information.</code></pre>
<p>You can now declare the type of functions in Emacs, presumably to help the native compiler make informed choices about the input and return value. We are moving inexorably closer to Common Lisp…</p>
<pre><code>New 'important-return-value' function declaration and property.
The declaration '(important-return-value t)' sets the
'important-return-value' property which indicates that the function
return value should probably not be thrown away implicitly.</code></pre>
<pre><code>New functions 'file-user-uid' and 'file-group-gid'.
These functions are like 'user-uid' and 'group-gid', respectively, but
are aware of file name handlers, so they will return the remote UID or
GID for remote files (or -1 if the connection has no associated user).</code></pre>
<pre><code>'treesit-font-lock-rules' now accepts additional global keywords.
When supplied with ':default-language LANGUAGE', rules after it will
default to use 'LANGUAGE'.</code></pre>
<p>A welcome bit of syntactic sugar.</p>
<pre><code>New optional argument to 'modify-dir-local-variable'.
An optional 5th argument FILE has been added to
'modify-dir-local-variable'.  It can be used to specify which file to
modify instead of the default ".dir-locals.el".</code></pre>
<h3 id="connection-local-variables">Connection local variables</h3>
<p>Connection local variables are variables local to Tramp-specific connections.</p>
<pre><code>New macros 'connection-local-p' and 'connection-local-value'.
The former macro returns non-nil if a variable has a connection-local
binding.  The latter macro returns the connection-local value of a
variable if any, or its current value.</code></pre>
<h3 id="hash-tables">Hash tables</h3>
<pre><code>':rehash-size' and ':rehash-threshold' args no longer have any effect.
These keyword arguments are now ignored by 'make-hash-table'.  Emacs
manages the memory for all hash table objects in the same way.
The functions 'hash-table-rehash-size' and 'hash-table-rehash-threshold'
remain for compatibility but now always return the old default values.</code></pre>
<pre><code>The printed representation has been shrunk and simplified.
The 'test' parameter is omitted if it is 'eql' (the default), as is
'data' if empty.  'rehash-size', 'rehash-threshold' and 'size' are
always omitted, and ignored if present when the object is read back in.</code></pre>
<h3 id="obarrays">Obarrays</h3>
<pre><code>New obarray type.
Obarrays are now represented by an opaque type instead of using vectors.
They are created by 'obarray-make' and manage their internal storage
automatically, which means that the size parameter to 'obarray-make' can
safely be omitted.  That is, they do not become slower as they fill up.

The old vector representation is still accepted by functions operating
on obarrays, but 'obarrayp' only returns t for obarray objects.
'type-of' now returns 'obarray' for obarray objects.

Old code which (incorrectly) created "obarrays" as Lisp vectors filled
with something other than 0, as in '(make-vector N nil)', will no longer
work, and should be rewritten to use 'obarray-make'.  Alternatively, you
can fill the vector with 0.</code></pre>
<pre><code>New function 'obarray-clear' removes all symbols from an obarray.</code></pre>
<pre><code>'obarray-size' and 'obarray-default-size' are now obsolete.
They pertained to the internal storage size which is now irrelevant.</code></pre>
<pre><code>'treesit-install-language-grammar' can handle local directory instead of URL.
It is now possible to pass a directory of a local repository as URL
inside 'treesit-language-source-alist', so that calling
'treesit-install-language-grammar' will avoid cloning the repository.
It may be useful, for example, for the purposes of bisecting a
treesitter grammar.</code></pre>
<pre><code>New buffer-local variable 'tabulated-list-groups'.
It controls display and separate sorting of groups of entries.  By
default no grouping or sorting is done.</code></pre>
<pre><code>New variable 'revert-buffer-restore-functions'.
It helps to preserve various states after reverting the buffer.</code></pre>
<pre><code>New text property 'context-menu-functions'.
Like the variable with the same name, it adds menus from the list that
is the value of the property to context menus shown when clicking on the
text which as this property.</code></pre>
<pre><code>Detecting the end of an iteration of a keyboard macro.
'read-event', 'read-char', and 'read-char-exclusive' no longer return -1
when called at the end of an iteration of the execution of a keyboard
macro.  Instead, they will transparently continue reading available input
(e.g., from the keyboard).  If you need to detect the end of a macro
iteration, check the following condition before calling one of the
aforementioned functions:

    (and (arrayp executing-kbd-macro)
         (&gt;= executing-kbd-macro-index (length executing-kbd-macro)))</code></pre>
<pre><code>'vtable-update-object' updates an existing object with just two arguments.
It is now possible to update the representation of an object in a vtable
by calling 'vtable-update-object' with just the vtable and the object as
arguments.  (Previously, the OLD-OBJECT argument was required which, in
this case, would mean repeating the object in the argument list.)  When
replacing an object with a different one, passing both the new and old
objects is still necessary.</code></pre>
<pre><code>'vtable-insert-object' can insert "before" or at an index.
The signature of 'vtable-insert-object' has changed and is now:

    (vtable-insert-object TABLE OBJECT &amp;optional LOCATION BEFORE)

LOCATION corresponds to the old AFTER-OBJECT argument; if BEFORE is
non-nil, the new object is inserted before the LOCATION object, making
it possible to insert a new object at the top of the table.  (Before,
this was not possible.)  In addition, LOCATION can be an integer, a
(zero-based) index into the table at which the new object is inserted
(BEFORE is ignored in this case).</code></pre>
<pre><code>New function 'sqlite-execute-batch'.
This function lets the user execute multiple SQL statements in one go.
It is useful, for example, when a Lisp program needs to evaluate an
entire SQL file.</code></pre>
<h3 id="json">JSON</h3>
<pre><code>'json-serialize' now always returns a unibyte string.
This is appropriate since it is an encoding operation.  In the unlikely
event that a multibyte string is needed, the result can be decoded using

    (decode-coding-string RESULT 'utf-8)</code></pre>
<pre><code>The parser keeps duplicated object keys in alist and plist output.
A JSON object such as '{"a":1,"a":2}' will now be translated into the
Lisp values '((a . 1) (a . 2))' or '(:a 1 :a 2)' if alist or plist
object types are requested.</code></pre>
<pre><code>The parser sometimes signals different types of errors.
It will now signal 'json-utf8-decode-error' for inputs that are not
correctly UTF-8 encoded.</code></pre>
<pre><code>The parser and encoder now accept arbitrarily large integers.
Previously, they were limited to the range of signed 64-bit integers.</code></pre>
<pre><code>New tree-sitter functions and variables for defining and using "things"</code></pre>
<pre><code>New variable 'treesit-thing-settings'.
It allows modes to define "things" like 'defun', 'text', 'sexp', and
'sentence' for navigation commands and tree-traversal functions.</code></pre>
<pre><code>New functions for navigating "things".
There are new navigation functions 'treesit-thing-prev',
'treesit-thing-next', 'treesit-navigate-thing',
'treesit-beginning-of-thing', and 'treesit-end-of-thing'.</code></pre>
<pre><code>New functions 'treesit-thing-at', 'treesit-thing-at-point'.</code></pre>
<p>The tree-sitter-enabled equivalent of <code>thing-at-point</code>. This really should be merged into the existing thing at point system; if TS is available, then its settings should take precedence.</p>
<pre><code>Tree-traversing functions.
The functions 'treesit-search-subtree', 'treesit-search-forward',
'treesit-search-forward-goto', and 'treesit-induce-sparse-tree' now
accept more kinds of predicates.  Lisp programs can now use thing
symbols (defined in 'treesit-thing-settings') and any thing definitions
for the predicate argument.</code></pre>
<h3 id="other-tree-sitter-function-and-variable-changes">Other tree-sitter function and variable changes</h3>
<pre><code>'treesit-parser-list' now takes additional optional arguments.
The additional arguments are LANGUAGE and TAG.  If LANGUAGE is given,
only return parsers for that language.  If TAG is given, only return
parsers with that tag.  Note that passing nil as tag doesn't mean return
all parsers, but rather "all parsers with no tags".</code></pre>
<pre><code>New variable 'treesit-primary-parser'.
This variable should be set by multi-langauge major modes before calling
'treesit-major-mode-setup', in order for tree-sitter integration
functionalities to operate correctly.</code></pre>
<p>I’m not convinced having the notion of a primary parser is the right approach for multi-language support in a buffer. The notion of primacy is not going to resolve problems where multiple languages that do not know of each other have to coexist in the same buffer; those languages by definition do not have a primary language, and trying to coax Emacs and thus tree-sitter into thinking there is such a thing is flawed.</p>
<h3 id="changes-in-emacs-30.1-on-non-free-operating-systems">Changes in Emacs 30.1 on Non-Free Operating Systems</h3>
<h3 id="ms-windows">MS-Windows</h3>
<pre><code>You can now opt out of following MS-Windows' Dark mode.
By default, Emacs on MS-Windows follows the system's Dark mode for its
title bars' and scroll bars' appearance.  If the new user option
'w32-follow-system-dark-mode' is customized to the nil value, Emacs
will disregard the system's Dark mode and will always use the default
Light mode.</code></pre>
<pre><code>You can now use Image-Dired even if the 'convert' program is not installed.
If you don't have GraphicsMagick or ImageMagick installed, and thus the
'gm convert'/'convert' program is not available, Emacs on MS-Windows
will now use its own function 'w32image-create-thumbnail' to create
thumbnail images and show them in the thumbnail buffer.  Unlike with
using 'convert', this fallback method is synchronous, so Emacs will wait
until all the thumbnails are created and displayed, before showing them.</code></pre>
<pre><code>Emacs on MS-Windows now supports the ':stipple' face attribute.</code></pre>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building a BitTorrent client from the ground up in Go (180 pts)]]></title>
            <link>https://blog.jse.li/posts/torrent/</link>
            <guid>43157980</guid>
            <pubDate>Mon, 24 Feb 2025 10:34:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jse.li/posts/torrent/">https://blog.jse.li/posts/torrent/</a>, See on <a href="https://news.ycombinator.com/item?id=43157980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>
    

    
    <p><strong>tl;dr:</strong> What is the complete path between visiting thepiratebay and sublimating an mp3 file from thin air? In this post, we’ll implement enough of the BitTorrent protocol to download Debian. Look at the <a href="https://github.com/veggiedefender/torrent-client/">Source code</a> or skip to the <a href="https://blog.jse.li/posts/torrent#putting-it-all-together">last bit</a>.
    </p>
    

    <section>
      
<p>This post is also available in <a href="https://web.archive.org/web/20200616163517if_/https://4gophers.ru/articles/bittorrent/#.Xuj0zivLerw">Russian</a>, <a href="https://markruler.github.io/posts/go/building-bittorrent-client/">Korean</a>, and <a href="https://blog.mynameisdhr.com/YongGOCongLingJianLiBitTorrentKeHuDuan/">Chinese</a>.</p>
<p>BitTorrent is a protocol for downloading and distributing files across the Internet. In contrast with the traditional client/server relationship, in which downloaders connect to a central server (for example: watching a movie on Netflix, or loading the web page you’re reading now), participants in the BitTorrent network, called <strong>peers</strong>, download pieces of files from <em>each other</em>—this is what makes it a <strong>peer-to-peer</strong> protocol. We’ll investigate how this works, and build our own client that can find peers and exchange data between them.</p>
<img src="https://blog.jse.li/torrent/client-server-p2p.png" alt="diagram showing the difference between client/server (all clients connecting to one server) and peer-to-peer (peers connecting to each other) relationships">
<p>The protocol evolved organically over the past 20 years, and various people and organizations added extensions for features like encryption, private torrents, and new ways of finding peers. We’ll be implementing the <a href="https://www.bittorrent.org/beps/bep_0003.html">original spec</a> from 2001 to keep this a weekend-sized project.</p>
<p>I’ll be using a <a href="https://cdimage.debian.org/debian-cd/current/amd64/bt-cd/#indexlist">Debian ISO</a> file as my guinea pig because it’s big, but not huge, at 350MB. As a popular Linux distribution, there will be lots of fast and cooperative peers for us to connect to. And we’ll avoid the legal and ethical issues related to downloading pirated content.</p>

<div><p>Consider donating to a local <a href="https://www.communityjusticeexchange.org/nbfn-directory">community bail fund</a>.
</p><p>
Your money will pay for legal aid and bail for protestors who have been arrested for standing up to police brutality, institutional racism, and the murder of Black men and women like George Floyd, Breonna Taylor, Ahmaud Arbery, and Nina Pop.
</p><p>
In the tech community, we talk a lot about inclusivity and diversity. Now is the time to take concrete action.
</p><p>
<a href="https://www.communityjusticeexchange.org/nbfn-directory">https://www.communityjusticeexchange.org/nbfn-directory</a></p></div>

<h2 id="finding-peers">Finding peers</h2>
<p>Here’s a problem: we want to download a file with BitTorrent, but it’s a peer-to-peer protocol and we have no idea where to find peers to download it from. This is a lot like moving to a new city and trying to make friends—maybe we’ll hit up a local pub or a meetup group! Centralized locations like these are the big idea behind <strong>trackers</strong>, which are central servers that introduce peers to each other. They’re just web servers running over HTTP<span>*<span>Some trackers use a <a href="http://bittorrent.org/beps/bep_0015.html">UDP</a> binary protocol to save bandwidth</span></span>
, and you can find Debian’s at <a href="http://bttracker.debian.org:6969/">http://bttracker.debian.org:6969/</a></p>
<img src="https://blog.jse.li/torrent/trackers.png" alt="illustration of a desktop computer and laptop sitting at a pub">
<p>Of course, these central servers are liable to get raided by the feds if they facilitate peers exchanging illegal content. You may remember reading about trackers like TorrentSpy, Popcorn Time, and KickassTorrents getting seized and shut down. New methods cut out the middleman by making even <strong>peer discovery</strong> a distributed process. We won’t be implementing them, but if you’re interested, some terms you can research are <strong>DHT</strong>, <strong>PEX</strong>, and <strong>magnet links</strong>.</p>
<h2 id="parsing-a-torrent-file">Parsing a .torrent file</h2>
<p>A .torrent file describes the contents of a torrentable file and information for connecting to a tracker. It’s all we need in order to kickstart the process of downloading a torrent. Debian’s .torrent file looks like this:</p>
<pre><code>d8:announce41:http://bttracker.debian.org:6969/announce7:comment35:"Debian CD from cdimage.debian.org"13:creation datei1573903810e9:httpseedsl145:https://cdimage.debian.org/cdimage/release/10.2.0//srv/cdbuilder.debian.org/dst/deb-cd/weekly-builds/amd64/iso-cd/debian-10.2.0-amd64-netinst.iso145:https://cdimage.debian.org/cdimage/archive/10.2.0//srv/cdbuilder.debian.org/dst/deb-cd/weekly-builds/amd64/iso-cd/debian-10.2.0-amd64-netinst.isoe4:infod6:lengthi351272960e4:name31:debian-10.2.0-amd64-netinst.iso12:piece lengthi262144e6:pieces26800:�����PS�^�� (binary blob of the hashes of each piece)ee
</code></pre><p>That mess is encoded in a format called <strong>Bencode</strong> (pronounced <em>bee-encode</em>), and we’ll need to decode it.</p>
<p>Bencode can encode roughly the same types of structures as JSON—strings, integers, lists, and dictionaries. Bencoded data is not as human-readable/writable as JSON, but it can efficiently handle binary data and it’s really simple to parse from a stream. Strings come with a length prefix, and look like <code>4:spam</code>. Integers go between <em>start</em> and <em>end</em> markers, so <code>7</code> would encode to <code>i7e</code>. Lists and dictionaries work in a similar way: <code>l4:spami7ee</code> represents <code>['spam', 7]</code>, while <code>d4:spami7ee</code> means <code>{spam: 7}</code>.</p>
<p>In a prettier format, our .torrent file looks like this:</p>
<div>
  <pre><code><span>d</span>
  <span>8</span>:<span>announce</span>
    <span>41</span>:<span>http://bttracker.debian.org:6969/announce</span>
  <span>7</span>:<span>comment</span>
    <span>35</span>:<span>"Debian CD from cdimage.debian.org"</span>
  <span>13</span>:<span>creation date</span>
    <span>i</span><span>1573903810</span><span>e</span>
  <span>4</span>:<span>info</span>
    <span>d</span>
      <span>6</span>:<span>length</span>
        <span>i</span><span>351272960</span><span>e</span>
      <span>4</span>:<span>name</span>
        <span>31</span>:<span>debian-10.2.0-amd64-netinst.iso</span>
      <span>12</span>:<span>piece length</span>
        <span>i</span><span>262144</span><span>e</span>
      <span>6</span>:<span>pieces</span>
        <span>26800</span>:<span>�����PS�^�� (binary blob of the hashes of each piece)</span></code>
    <span>e</span>
<span>e</span></pre>
</div>
<p>In this file, we can spot the URL of the tracker, the creation date (as a Unix timestamp), the name and size of the file, and a big binary blob containing the SHA-1 hashes of each <strong>piece</strong>, which are equally-sized parts of the file we want to download. The exact size of a piece varies between torrents, but they are usually somewhere between 256KB and 1MB. This means that a large file might be made up of <em>thousands</em> of pieces. We’ll download these pieces from our peers, check them against the hashes from our torrent file, assemble them together, and boom, we’ve got a file!</p>
<img src="https://blog.jse.li/torrent/pieces.png" alt="illustration of a file being cut with scissors into multiple pieces, starting with piece 0">
<p>This mechanism allows us to verify the integrity of each piece as we go. It makes BitTorrent resistant to accidental corruption or intentional <strong>torrent poisoning</strong>. Unless an attacker is capable of breaking SHA-1 with a preimage attack, we will get exactly the content we asked for.</p>
<p>It would be really fun to write a bencode parser, but parsing isn’t our focus today. But I found Fredrik Lundh’s <a href="https://web.archive.org/web/20200105114449/https://effbot.org/zone/bencode.htm">50 line parser</a> to be especially illuminating. For this project, I used <a href="https://github.com/jackpal/bencode-go">github.com/jackpal/bencode-go</a>:</p>
<div>
<div><pre><code data-lang="golang"><span>import</span> <span>(</span>
    <span>"github.com/jackpal/bencode-go"</span>
<span>)</span>

<span>type</span> <span>bencodeInfo</span> <span>struct</span> <span>{</span>
    <span>Pieces</span>      <span>string</span> <span>`bencode:"pieces"`</span>
    <span>PieceLength</span> <span>int</span>    <span>`bencode:"piece length"`</span>
    <span>Length</span>      <span>int</span>    <span>`bencode:"length"`</span>
    <span>Name</span>        <span>string</span> <span>`bencode:"name"`</span>
<span>}</span>

<span>type</span> <span>bencodeTorrent</span> <span>struct</span> <span>{</span>
    <span>Announce</span> <span>string</span>      <span>`bencode:"announce"`</span>
    <span>Info</span>     <span>bencodeInfo</span> <span>`bencode:"info"`</span>
<span>}</span>

<span>// Open parses a torrent file
</span><span></span><span>func</span> <span>Open</span><span>(</span><span>r</span> <span>io</span><span>.</span><span>Reader</span><span>)</span> <span>(</span><span>*</span><span>bencodeTorrent</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>bto</span> <span>:=</span> <span>bencodeTorrent</span><span>{}</span>
    <span>err</span> <span>:=</span> <span>bencode</span><span>.</span><span>Unmarshal</span><span>(</span><span>r</span><span>,</span> <span>&amp;</span><span>bto</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>return</span> <span>nil</span><span>,</span> <span>err</span>
    <span>}</span>
    <span>return</span> <span>&amp;</span><span>bto</span><span>,</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/torrentfile/torrentfile.go">view in context</a></p>
</div>
<p>Because I like to keep my structures relatively flat, and I like to keep my application structs separate from my serialization structs, I exported a different, flatter struct named <code>TorrentFile</code> and wrote a few helper functions to convert between the two.</p>
<p>Notably, I split <code>pieces</code> (previously a string) into a slice of hashes (each <code>[20]byte</code>) so that I can easily access individual hashes later. I also computed the SHA-1 hash of the entire bencoded <code>info</code> dict (the one which contained the name, size, and piece hashes). We know this as the <strong>infohash</strong> and it uniquely identifies files when we talk to trackers and peers. More on this later.</p>
<img src="https://blog.jse.li/torrent/info-hash.png" alt="a name tag saying 'Hello my name is 86d4c80024a469be4c50bc5a102cf71780310074'">
<div>
<div><pre><code data-lang="golang"><span>type</span> <span>TorrentFile</span> <span>struct</span> <span>{</span>
    <span>Announce</span>    <span>string</span>
    <span><span>InfoHash</span>    <span>[</span><span>20</span><span>]</span><span>byte</span></span>
    <span><span>PieceHashes</span> <span>[][</span><span>20</span><span>]</span><span>byte</span></span>
    <span>PieceLength</span> <span>int</span>
    <span>Length</span>      <span>int</span>
    <span>Name</span>        <span>string</span>
<span>}</span>
</code><p><code data-lang="golang"><span>func</span> <span>(</span><span>bto</span> <span><em></em></span><em><span>bencodeTorrent</span><span>)</span> <span>toTorrentFile</span><span>()</span> <span>(</span><span></span></em><span>TorrentFile</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
<span>// …
</span><span></span><span>}</span></code></p></pre></div>
<p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/torrentfile/torrentfile.go#L120-L138">view in context</a></p>
</div>
<h2 id="retrieving-peers-from-the-tracker">Retrieving peers from the tracker</h2>
<p>Now that we have information about the file and its tracker, let’s talk to the tracker to <strong>announce</strong> our presence as a peer and to retrieve a list of other peers. We just need to make a GET request to the <code>announce</code> URL supplied in the .torrent file, with a few query parameters:</p>
<div>
<div><pre><code data-lang="golang"><span>func</span> <span>(</span><span>t</span> <span>*</span><span>TorrentFile</span><span>)</span> <span>buildTrackerURL</span><span>(</span><span>peerID</span> <span>[</span><span>20</span><span>]</span><span>byte</span><span>,</span> <span>port</span> <span>uint16</span><span>)</span> <span>(</span><span>string</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>base</span><span>,</span> <span>err</span> <span>:=</span> <span>url</span><span>.</span><span>Parse</span><span>(</span><span>t</span><span>.</span><span>Announce</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>return</span> <span>""</span><span>,</span> <span>err</span>
    <span>}</span>
    <span>params</span> <span>:=</span> <span>url</span><span>.</span><span>Values</span><span>{</span>
        <span>"info_hash"</span><span>:</span>  <span>[]</span><span>string</span><span>{</span><span>string</span><span>(</span><span>t</span><span>.</span><span>InfoHash</span><span>[:])},</span>
        <span>"peer_id"</span><span>:</span>    <span>[]</span><span>string</span><span>{</span><span>string</span><span>(</span><span>peerID</span><span>[:])},</span>
        <span>"port"</span><span>:</span>       <span>[]</span><span>string</span><span>{</span><span>strconv</span><span>.</span><span>Itoa</span><span>(</span><span>int</span><span>(</span><span>Port</span><span>))},</span>
        <span>"uploaded"</span><span>:</span>   <span>[]</span><span>string</span><span>{</span><span>"0"</span><span>},</span>
        <span>"downloaded"</span><span>:</span> <span>[]</span><span>string</span><span>{</span><span>"0"</span><span>},</span>
        <span>"compact"</span><span>:</span>    <span>[]</span><span>string</span><span>{</span><span>"1"</span><span>},</span>
        <span>"left"</span><span>:</span>       <span>[]</span><span>string</span><span>{</span><span>strconv</span><span>.</span><span>Itoa</span><span>(</span><span>t</span><span>.</span><span>Length</span><span>)},</span>
    <span>}</span>
    <span>base</span><span>.</span><span>RawQuery</span> <span>=</span> <span>params</span><span>.</span><span>Encode</span><span>()</span>
    <span>return</span> <span>base</span><span>.</span><span>String</span><span>(),</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/torrentfile/tracker.go#L19-L35">view in context</a></p>
</div>
<p>The important ones:</p>
<ul>
<li><strong>info_hash</strong>: Identifies the <em>file</em> we’re trying to download. It’s the infohash we calculated earlier from the bencoded <code>info</code> dict. The tracker will use this to figure out which peers to show us.</li>
<li><strong>peer_id</strong>: A 20 byte name to identify <em>ourselves</em> to trackers and peers. We’ll just generate 20 random bytes for this. Real BitTorrent clients have IDs like <code>-TR2940-k8hj0wgej6ch</code> which identify the client software and version—in this case, TR2940 stands for Transmission client 2.94.</li>
</ul>
<img src="https://blog.jse.li/torrent/info-hash-peer-id.png" alt="a file with a name tag saying 'info_hash' and a person with a name tag 'peer_id'">
<h2 id="parsing-the-tracker-response">Parsing the tracker response</h2>
<p>We get back a bencoded response:</p>
<div>
  <pre><code><span>d</span>
  <span>8</span>:<span>interval</span>
    <span>i</span><span>900</span><span>e</span>
  <span>5</span>:<span>peers</span>
    <span>252</span>:<span>(another long binary blob)</span>
<span>e</span></code></pre>
</div>
<p><code>Interval</code> tells us how often we’re supposed to connect to the tracker again to refresh our list of peers. A value of 900 means we should reconnect every 15 minutes (900 seconds).</p>
<p><code>Peers</code> is another long binary blob containing the IP addresses of each peer. It’s made out of <strong>groups of six bytes</strong>. The first four bytes in each group represent the peer’s IP address—each byte represents a number in the IP. The last two bytes represent the port, as a big-endian <code>uint16</code>. <strong>Big-endian</strong>, or <strong>network order</strong>, means that we can interpret a group of bytes as an integer by just squishing them together left to right. For example, the bytes <code>0x1A</code>, <code>0xE1</code> make <code>0x1AE1</code>, or 6881 in decimal.<span>*<span>Interpreting the same bytes in <strong>little-endian</strong> order would make 0xE11A = 57626</span></span></p>
<img src="https://blog.jse.li/torrent/address.png" alt="diagram showing how 192, 0, 2, 123, 0x1A, 0xE1 can be interpreted as 192.0.1.123:6881">
<div>
<div><pre><code data-lang="golang"><span>// Peer encodes connection information for a peer
</span><span></span><span>type</span> <span>Peer</span> <span>struct</span> <span>{</span>
    <span>IP</span>   <span>net</span><span>.</span><span>IP</span>
    <span>Port</span> <span>uint16</span>
<span>}</span>

<span>// Unmarshal parses peer IP addresses and ports from a buffer
</span><span></span><span>func</span> <span>Unmarshal</span><span>(</span><span>peersBin</span> <span>[]</span><span>byte</span><span>)</span> <span>([]</span><span>Peer</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>const</span> <span>peerSize</span> <span>=</span> <span>6</span> <span>// 4 for IP, 2 for port
</span><span></span>    <span>numPeers</span> <span>:=</span> <span>len</span><span>(</span><span>peersBin</span><span>)</span> <span>/</span> <span>peerSize</span>
    <span>if</span> <span>len</span><span>(</span><span>peersBin</span><span>)</span><span>%</span><span>peerSize</span> <span>!=</span> <span>0</span> <span>{</span>
        <span>err</span> <span>:=</span> <span>fmt</span><span>.</span><span>Errorf</span><span>(</span><span>"Received malformed peers"</span><span>)</span>
        <span>return</span> <span>nil</span><span>,</span> <span>err</span>
    <span>}</span>
    <span>peers</span> <span>:=</span> <span>make</span><span>([]</span><span>Peer</span><span>,</span> <span>numPeers</span><span>)</span>
    <span>for</span> <span>i</span> <span>:=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>numPeers</span><span>;</span> <span>i</span><span>++</span> <span>{</span>
        <span>offset</span> <span>:=</span> <span>i</span> <span>*</span> <span>peerSize</span>
        <span>peers</span><span>[</span><span>i</span><span>].</span><span>IP</span> <span>=</span> <span>net</span><span>.</span><span>IP</span><span>(</span><span>peersBin</span><span>[</span><span>offset</span> <span>:</span> <span>offset</span><span>+</span><span>4</span><span>])</span>
        <span>peers</span><span>[</span><span>i</span><span>].</span><span>Port</span> <span>=</span> <span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint16</span><span>(</span><span>peersBin</span><span>[</span><span>offset</span><span>+</span><span>4</span> <span>:</span> <span>offset</span><span>+</span><span>6</span><span>])</span>
    <span>}</span>
    <span>return</span> <span>peers</span><span>,</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/peers/peers.go">view in context</a></p>
</div>
<h2 id="downloading-from-peers">Downloading from peers</h2>
<p>Now that we have a list of peers, it’s time to connect with them and start downloading pieces! We can break down the process into a few steps. For each peer, we want to:</p>
<ol>
<li>Start a TCP connection with the peer. This is like starting a phone call.</li>
<li>Complete a two-way BitTorrent <strong>handshake</strong>. <em>“Hello?” “Hello."</em></li>
<li>Exchange <strong>messages</strong> to download <strong>pieces</strong>. <em>“I’d like piece #231 please."</em></li>
</ol>
<h2 id="start-a-tcp-connection">Start a TCP connection</h2>
<div>
<div><pre><code data-lang="golang"><span>conn</span><span>,</span> <span>err</span> <span>:=</span> <span>net</span><span>.</span><span>DialTimeout</span><span>(</span><span>"tcp"</span><span>,</span> <span>peer</span><span>.</span><span>String</span><span>(),</span> <span>3</span><span>*</span><span>time</span><span>.</span><span>Second</span><span>)</span>
<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
    <span>return</span> <span>nil</span><span>,</span> <span>err</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/client/client.go#L65-L69">view in context</a></p>
</div>
<p>I set a timeout so that I don’t waste too much time on peers that aren’t going to let me connect. For the most part, it’s a pretty standard TCP connection.</p>
<h2 id="complete-the-handshake">Complete the handshake</h2>
<p>We’ve just set up a connection with a peer, but we want do a handshake to validate our assumptions that the peer</p>
<ul>
<li>can communicate using the BitTorrent protocol</li>
<li>is able to understand and respond to our messages</li>
<li>has the file that we want, or at least knows what we’re talking about</li>
</ul>
<img src="https://blog.jse.li/torrent/handshake.png" alt="Two computers communicating. One asks 'do you speak BitTorrent and have this file?' and the other replies 'I speak BitTorrent and have that file'">
<p>My father told me that the secret to a good handshake is a firm grip and eye contact. The secret to a good BitTorrent handshake is that it’s made up of five parts:</p>
<ol>
<li>The length of the protocol identifier, which is always 19 (0x13 in hex)</li>
<li>The protocol identifier, called the <strong>pstr</strong> which is always <code>BitTorrent protocol</code></li>
<li>Eight <strong>reserved bytes</strong>, all set to 0. We’d flip some of them to 1 to indicate that we support certain <a href="http://www.bittorrent.org/beps/bep_0010.html">extensions</a>. But we don’t, so we’ll keep them at 0.</li>
<li>The <strong>infohash</strong> that we calculated earlier to identify which file we want</li>
<li>The <strong>Peer ID</strong> that we made up to identify ourselves</li>
</ol>
<p>Put together, a handshake string might look like this:</p>
<div><pre><code><span>\x13</span><span>BitTorrent protocol</span><span>\x00\x00\x00\x00\x00\x00\x00\x00</span><span>\x86\xd4\xc8\x00\x24\xa4\x69\xbe\x4c\x50\xbc\x5a\x10\x2c\xf7\x17\x80\x31\x00\x74</span><span>-TR2940-k8hj0wgej6ch</span></code></pre></div>
<p>After we send a handshake to our peer, we should receive a handshake back in the same format. The infohash we get back should match the one we sent so that we know that we’re talking about the same file. If everything goes as planned, we’re good to go. If not, we can sever the connection because there’s something wrong. <em>“Hello?” “这是谁？ 你想要什么？” “Okay, wow, wrong number."</em></p>
<p>In our code, let’s make a struct to represent a handshake, and write a few methods for serializing and reading them:</p>
<div>
<div><pre><code data-lang="golang"><span>// A Handshake is a special message that a peer uses to identify itself
</span><span></span><span>type</span> <span>Handshake</span> <span>struct</span> <span>{</span>
    <span>Pstr</span>     <span>string</span>
    <span>InfoHash</span> <span>[</span><span>20</span><span>]</span><span>byte</span>
    <span>PeerID</span>   <span>[</span><span>20</span><span>]</span><span>byte</span>
<span>}</span>

<span>// Serialize serializes the handshake to a buffer
</span><span></span><span>func</span> <span>(</span><span>h</span> <span>*</span><span>Handshake</span><span>)</span> <span>Serialize</span><span>()</span> <span>[]</span><span>byte</span> <span>{</span>
    <span>buf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>len</span><span>(</span><span>h</span><span>.</span><span>Pstr</span><span>)</span><span>+</span><span>49</span><span>)</span>
    <span>buf</span><span>[</span><span>0</span><span>]</span> <span>=</span> <span>byte</span><span>(</span><span>len</span><span>(</span><span>h</span><span>.</span><span>Pstr</span><span>))</span>
    <span>curr</span> <span>:=</span> <span>1</span>
    <span>curr</span> <span>+=</span> <span>copy</span><span>(</span><span>buf</span><span>[</span><span>curr</span><span>:],</span> <span>h</span><span>.</span><span>Pstr</span><span>)</span>
    <span>curr</span> <span>+=</span> <span>copy</span><span>(</span><span>buf</span><span>[</span><span>curr</span><span>:],</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>8</span><span>))</span> <span>// 8 reserved bytes
</span><span></span>    <span>curr</span> <span>+=</span> <span>copy</span><span>(</span><span>buf</span><span>[</span><span>curr</span><span>:],</span> <span>h</span><span>.</span><span>InfoHash</span><span>[:])</span>
    <span>curr</span> <span>+=</span> <span>copy</span><span>(</span><span>buf</span><span>[</span><span>curr</span><span>:],</span> <span>h</span><span>.</span><span>PeerID</span><span>[:])</span>
    <span>return</span> <span>buf</span>
<span>}</span>

<span>// Read parses a handshake from a stream
</span><span></span><span>func</span> <span>Read</span><span>(</span><span>r</span> <span>io</span><span>.</span><span>Reader</span><span>)</span> <span>(</span><span>*</span><span>Handshake</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>// Do Serialize(), but backwards
</span><span></span>    <span>// ...
</span><span></span><span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/a83013d250dd9b4268cceace28e4cd82b07f2cbd/handshake/handshake.go">view in context</a></p>
</div>
<h2 id="send-and-receive-messages">Send and receive messages</h2>
<p>Once we’ve completed the initial handshake, we can send and receive <strong>messages</strong>. Well, not quite—if the other peer isn’t ready to accept messages, we can’t send any until they tell us they’re ready. In this state, we’re considered <strong>choked</strong> by the other peer. They’ll send us an <strong>unchoke</strong> message to let us know that we can begin asking them for data. By default, we assume that we’re choked until proven otherwise.</p>
<p>Once we’ve been unchoked, we can then begin sending <strong>requests</strong> for pieces, and they can send us messages back containing pieces.</p>
<img src="https://blog.jse.li/torrent/choke.png" alt="A cartoon in which person 1 says 'hello I would like piece number—' and person 2 grabs him by the neck and says '00 00 00 01 00 (choke)'">
<h3 id="interpreting-messages">Interpreting messages</h3>
<p>A message has a length, an <strong>ID</strong> and a <strong>payload</strong>. On the wire, it looks like:</p>
<img src="https://blog.jse.li/torrent/message.png" alt="A message with 4 byte for the length, 1 byte for ID, and an optional payload">
<p>A message starts with a length indicator which tells us how many bytes long the message will be. It’s a 32-bit integer, meaning it’s made out of four bytes smooshed together in big-endian order. The next byte, the <strong>ID</strong>, tells us which type of message we’re receiving—for example, a <code>2</code> byte means “interested.” Finally, the optional <strong>payload</strong> fills out the remaining length of the message.</p>
<div>
<div><pre><code data-lang="golang"><span>type</span> <span>messageID</span> <span>uint8</span>

<span>const</span> <span>(</span>
    <span>MsgChoke</span>         <span>messageID</span> <span>=</span> <span>0</span>
    <span>MsgUnchoke</span>       <span>messageID</span> <span>=</span> <span>1</span>
    <span>MsgInterested</span>    <span>messageID</span> <span>=</span> <span>2</span>
    <span>MsgNotInterested</span> <span>messageID</span> <span>=</span> <span>3</span>
    <span>MsgHave</span>          <span>messageID</span> <span>=</span> <span>4</span>
    <span>MsgBitfield</span>      <span>messageID</span> <span>=</span> <span>5</span>
    <span>MsgRequest</span>       <span>messageID</span> <span>=</span> <span>6</span>
    <span>MsgPiece</span>         <span>messageID</span> <span>=</span> <span>7</span>
    <span>MsgCancel</span>        <span>messageID</span> <span>=</span> <span>8</span>
<span>)</span>

<span>// Message stores ID and payload of a message
</span><span></span><span>type</span> <span>Message</span> <span>struct</span> <span>{</span>
    <span>ID</span>      <span>messageID</span>
    <span>Payload</span> <span>[]</span><span>byte</span>
<span>}</span>

<span>// Serialize serializes a message into a buffer of the form
</span><span>// &lt;length prefix&gt;&lt;message ID&gt;&lt;payload&gt;
</span><span>// Interprets `nil` as a keep-alive message
</span><span></span><span>func</span> <span>(</span><span>m</span> <span>*</span><span>Message</span><span>)</span> <span>Serialize</span><span>()</span> <span>[]</span><span>byte</span> <span>{</span>
    <span>if</span> <span>m</span> <span>==</span> <span>nil</span> <span>{</span>
        <span>return</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>4</span><span>)</span>
    <span>}</span>
    <span>length</span> <span>:=</span> <span>uint32</span><span>(</span><span>len</span><span>(</span><span>m</span><span>.</span><span>Payload</span><span>)</span> <span>+</span> <span>1</span><span>)</span> <span>// +1 for id
</span><span></span>    <span>buf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>4</span><span>+</span><span>length</span><span>)</span>
    <span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>PutUint32</span><span>(</span><span>buf</span><span>[</span><span>0</span><span>:</span><span>4</span><span>],</span> <span>length</span><span>)</span>
    <span>buf</span><span>[</span><span>4</span><span>]</span> <span>=</span> <span>byte</span><span>(</span><span>m</span><span>.</span><span>ID</span><span>)</span>
    <span>copy</span><span>(</span><span>buf</span><span>[</span><span>5</span><span>:],</span> <span>m</span><span>.</span><span>Payload</span><span>)</span>
    <span>return</span> <span>buf</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/message/message.go#L90-L103">view in context</a></p>
</div>
<p>To read a message from a stream, we just follow the format of a message. We read four bytes and interpret them as a <code>uint32</code> to get the <strong>length</strong> of the message. Then, we read that number of bytes to get the <strong>ID</strong> (the first byte) and the <strong>payload</strong> (the remaining bytes).</p>
<div>
<div><pre><code data-lang="golang"><span>// Read parses a message from a stream. Returns `nil` on keep-alive message
</span><span></span><span>func</span> <span>Read</span><span>(</span><span>r</span> <span>io</span><span>.</span><span>Reader</span><span>)</span> <span>(</span><span>*</span><span>Message</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>lengthBuf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>4</span><span>)</span>
    <span>_</span><span>,</span> <span>err</span> <span>:=</span> <span>io</span><span>.</span><span>ReadFull</span><span>(</span><span>r</span><span>,</span> <span>lengthBuf</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>return</span> <span>nil</span><span>,</span> <span>err</span>
    <span>}</span>
    <span>length</span> <span>:=</span> <span>binary</span><span>.</span><span>BigEndian</span><span>.</span><span>Uint32</span><span>(</span><span>lengthBuf</span><span>)</span>

    <span>// keep-alive message
</span><span></span>    <span>if</span> <span>length</span> <span>==</span> <span>0</span> <span>{</span>
        <span>return</span> <span>nil</span><span>,</span> <span>nil</span>
    <span>}</span>

    <span>messageBuf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>length</span><span>)</span>
    <span>_</span><span>,</span> <span>err</span> <span>=</span> <span>io</span><span>.</span><span>ReadFull</span><span>(</span><span>r</span><span>,</span> <span>messageBuf</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>return</span> <span>nil</span><span>,</span> <span>err</span>
    <span>}</span>

    <span>m</span> <span>:=</span> <span>Message</span><span>{</span>
        <span>ID</span><span>:</span>      <span>messageID</span><span>(</span><span>messageBuf</span><span>[</span><span>0</span><span>]),</span>
        <span>Payload</span><span>:</span> <span>messageBuf</span><span>[</span><span>1</span><span>:],</span>
    <span>}</span>

    <span>return</span> <span>&amp;</span><span>m</span><span>,</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/message/message.go#L105-L131">view in context</a></p>
</div>
<h3 id="bitfields">Bitfields</h3>
<p>One of the most interesting types of message is the <strong>bitfield</strong>, which is a data structure that peers use to efficiently encode which pieces they are able to send us. A bitfield looks like a byte array, and to check which pieces they have, we just need to look at the positions of the <em>bits</em> set to 1. You can think of it like the digital equivalent of a coffee shop loyalty card. We start with a blank card of all <code>0</code>, and flip bits to <code>1</code> to mark their positions as “stamped.”</p>
<img src="https://blog.jse.li/torrent/bitfield.png" alt="a coffee shop loyalty card with eight slots, with stamps on the first four slots and a stamp on the second to last slot, represented as 11110010">
<p>By working with <em>bits</em> instead of <em>bytes</em>, this data structure is super compact. We can stuff information about eight pieces in the space of a single byte—the size of a <code>bool</code>. The tradeoff is that accessing values becomes a little more tricky. The smallest unit of memory that computers can address are bytes, so to get to our bits, we have to do some bitwise manipulation:</p>
<div>
<div><pre><code data-lang="golang"><span>// A Bitfield represents the pieces that a peer has
</span><span></span><span>type</span> <span>Bitfield</span> <span>[]</span><span>byte</span>

<span>// HasPiece tells if a bitfield has a particular index set
</span><span></span><span>func</span> <span>(</span><span>bf</span> <span>Bitfield</span><span>)</span> <span>HasPiece</span><span>(</span><span>index</span> <span>int</span><span>)</span> <span>bool</span> <span>{</span>
    <span>byteIndex</span> <span>:=</span> <span>index</span> <span>/</span> <span>8</span>
    <span>offset</span> <span>:=</span> <span>index</span> <span>%</span> <span>8</span>
    <span>return</span> <span>bf</span><span>[</span><span>byteIndex</span><span>]</span><span>&gt;&gt;</span><span>(</span><span>7</span><span>-</span><span>offset</span><span>)</span><span>&amp;</span><span>1</span> <span>!=</span> <span>0</span>
<span>}</span>

<span>// SetPiece sets a bit in the bitfield
</span><span></span><span>func</span> <span>(</span><span>bf</span> <span>Bitfield</span><span>)</span> <span>SetPiece</span><span>(</span><span>index</span> <span>int</span><span>)</span> <span>{</span>
    <span>byteIndex</span> <span>:=</span> <span>index</span> <span>/</span> <span>8</span>
    <span>offset</span> <span>:=</span> <span>index</span> <span>%</span> <span>8</span>
    <span>bf</span><span>[</span><span>byteIndex</span><span>]</span> <span>|=</span> <span>1</span> <span>&lt;&lt;</span> <span>(</span><span>7</span> <span>-</span> <span>offset</span><span>)</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/bitfield/bitfield.go">view in context</a></p>
</div>
<h2 id="putting-it-all-together">Putting it all together</h2>
<p>We now have all the tools we need to download a torrent: we have a list of peers obtained from the tracker, and we can communicate with them by dialing a TCP connection, initiating a handshake, and sending and receiving messages. Our last big problems are handling the <strong>concurrency</strong> involved in talking to multiple peers at once, and managing the <strong>state</strong> of our peers as we interact with them. These are both classically Hard problems.</p>
<h3 id="managing-concurrency-channels-as-queues">Managing concurrency: channels as queues</h3>
<p>In Go, we <a href="https://blog.golang.org/share-memory-by-communicating">share memory by communicating</a>, and we can think of a Go channel as a cheap thread-safe queue.</p>
<p>We’ll set up two channels to synchronize our concurrent workers: one for dishing out work (pieces to download) between peers, and another for collecting downloaded pieces. As downloaded pieces come in through the results channel, we can copy them into a buffer to start assembling our complete file.</p>
<div>
<div><pre><code data-lang="golang"><span>// Init queues for workers to retrieve work and send results
</span><span></span><span>workQueue</span> <span>:=</span> <span>make</span><span>(</span><span>chan</span> <span>*</span><span>pieceWork</span><span>,</span> <span>len</span><span>(</span><span>t</span><span>.</span><span>PieceHashes</span><span>))</span>
<span>results</span> <span>:=</span> <span>make</span><span>(</span><span>chan</span> <span>*</span><span>pieceResult</span><span>)</span>
<span>for</span> <span>index</span><span>,</span> <span>hash</span> <span>:=</span> <span>range</span> <span>t</span><span>.</span><span>PieceHashes</span> <span>{</span>
    <span>length</span> <span>:=</span> <span>t</span><span>.</span><span>calculatePieceSize</span><span>(</span><span>index</span><span>)</span>
    <span>workQueue</span> <span>&lt;-</span> <span>&amp;</span><span>pieceWork</span><span>{</span><span>index</span><span>,</span> <span>hash</span><span>,</span> <span>length</span><span>}</span>
<span>}</span>

<span>// Start workers
</span><span></span><span>for</span> <span>_</span><span>,</span> <span>peer</span> <span>:=</span> <span>range</span> <span>t</span><span>.</span><span>Peers</span> <span>{</span>
    <span>go</span> <span>t</span><span>.</span><span>startDownloadWorker</span><span>(</span><span>peer</span><span>,</span> <span>workQueue</span><span>,</span> <span>results</span><span>)</span>
<span>}</span>

<span>// Collect results into a buffer until full
</span><span></span><span>buf</span> <span>:=</span> <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>t</span><span>.</span><span>Length</span><span>)</span>
<span>donePieces</span> <span>:=</span> <span>0</span>
<span>for</span> <span>donePieces</span> <span>&lt;</span> <span>len</span><span>(</span><span>t</span><span>.</span><span>PieceHashes</span><span>)</span> <span>{</span>
    <span>res</span> <span>:=</span> <span>&lt;-</span><span>results</span>
    <span>begin</span><span>,</span> <span>end</span> <span>:=</span> <span>t</span><span>.</span><span>calculateBoundsForPiece</span><span>(</span><span>res</span><span>.</span><span>index</span><span>)</span>
    <span>copy</span><span>(</span><span>buf</span><span>[</span><span>begin</span><span>:</span><span>end</span><span>],</span> <span>res</span><span>.</span><span>buf</span><span>)</span>
    <span>donePieces</span><span>++</span>
<span>}</span>
<span>close</span><span>(</span><span>workQueue</span><span>)</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/p2p/p2p.go#L188-L214">view in context</a></p>
</div>
<p>We’ll spawn a worker goroutine for each peer we’ve received from the tracker. It’ll connect and handshake with the peer, and then start retrieving work from the <code>workQueue</code>, attempting to download it, and sending downloaded pieces back through the <code>results</code> channel.</p>
<img src="https://blog.jse.li/torrent/download.png" alt="a flow chart of the download strategy">
<div>
<div><pre><code data-lang="golang"><span>func</span> <span>(</span><span>t</span> <span>*</span><span>Torrent</span><span>)</span> <span>startDownloadWorker</span><span>(</span><span>peer</span> <span>peers</span><span>.</span><span>Peer</span><span>,</span> <span>workQueue</span> <span>chan</span> <span>*</span><span>pieceWork</span><span>,</span> <span>results</span> <span>chan</span> <span>*</span><span>pieceResult</span><span>)</span> <span>{</span>
    <span>c</span><span>,</span> <span>err</span> <span>:=</span> <span>client</span><span>.</span><span>New</span><span>(</span><span>peer</span><span>,</span> <span>t</span><span>.</span><span>PeerID</span><span>,</span> <span>t</span><span>.</span><span>InfoHash</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>log</span><span>.</span><span>Printf</span><span>(</span><span>"Could not handshake with %s. Disconnecting\n"</span><span>,</span> <span>peer</span><span>.</span><span>IP</span><span>)</span>
        <span>return</span>
    <span>}</span>
    <span>defer</span> <span>c</span><span>.</span><span>Conn</span><span>.</span><span>Close</span><span>()</span>
    <span>log</span><span>.</span><span>Printf</span><span>(</span><span>"Completed handshake with %s\n"</span><span>,</span> <span>peer</span><span>.</span><span>IP</span><span>)</span>

    <span>c</span><span>.</span><span>SendUnchoke</span><span>()</span>
    <span>c</span><span>.</span><span>SendInterested</span><span>()</span>

    <span>for</span> <span>pw</span> <span>:=</span> <span>range</span> <span>workQueue</span> <span>{</span>
        <span>if</span> <span>!</span><span>c</span><span>.</span><span>Bitfield</span><span>.</span><span>HasPiece</span><span>(</span><span>pw</span><span>.</span><span>index</span><span>)</span> <span>{</span>
            <span>workQueue</span> <span>&lt;-</span> <span>pw</span> <span>// Put piece back on the queue
</span><span></span>            <span>continue</span>
        <span>}</span>

        <span>// Download the piece
</span><span></span>        <span>buf</span><span>,</span> <span>err</span> <span>:=</span> <span>attemptDownloadPiece</span><span>(</span><span>c</span><span>,</span> <span>pw</span><span>)</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>log</span><span>.</span><span>Println</span><span>(</span><span>"Exiting"</span><span>,</span> <span>err</span><span>)</span>
            <span>workQueue</span> <span>&lt;-</span> <span>pw</span> <span>// Put piece back on the queue
</span><span></span>            <span>return</span>
        <span>}</span>

        <span>err</span> <span>=</span> <span>checkIntegrity</span><span>(</span><span>pw</span><span>,</span> <span>buf</span><span>)</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>log</span><span>.</span><span>Printf</span><span>(</span><span>"Piece #%d failed integrity check\n"</span><span>,</span> <span>pw</span><span>.</span><span>index</span><span>)</span>
            <span>workQueue</span> <span>&lt;-</span> <span>pw</span> <span>// Put piece back on the queue
</span><span></span>            <span>continue</span>
        <span>}</span>

        <span>c</span><span>.</span><span>SendHave</span><span>(</span><span>pw</span><span>.</span><span>index</span><span>)</span>
        <span>results</span> <span>&lt;-</span> <span>&amp;</span><span>pieceResult</span><span>{</span><span>pw</span><span>.</span><span>index</span><span>,</span> <span>buf</span><span>}</span>
    <span>}</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/p2p/p2p.go#L133-L169">view in context</a></p>
</div>
<h3 id="managing-state">Managing state</h3>
<p>We’ll keep track of each peer in a struct, and modify that struct as we read messages. It’ll include data like how much we’ve downloaded from the peer, how much we’ve requested from them, and whether we’re choked. If we wanted to scale this further, we could formalize this as a finite state machine. But a struct and a switch are good enough for now.</p>
<div>
<div><pre><code data-lang="golang"><span>type</span> <span>pieceProgress</span> <span>struct</span> <span>{</span>
    <span>index</span>      <span>int</span>
    <span>client</span>     <span>*</span><span>client</span><span>.</span><span>Client</span>
    <span>buf</span>        <span>[]</span><span>byte</span>
    <span>downloaded</span> <span>int</span>
    <span>requested</span>  <span>int</span>
    <span>backlog</span>    <span>int</span>
<span>}</span>

<span>func</span> <span>(</span><span>state</span> <span>*</span><span>pieceProgress</span><span>)</span> <span>readMessage</span><span>()</span> <span>error</span> <span>{</span>
    <span>msg</span><span>,</span> <span>err</span> <span>:=</span> <span>state</span><span>.</span><span>client</span><span>.</span><span>Read</span><span>()</span> <span>// this call blocks
</span><span></span>    <span>switch</span> <span>msg</span><span>.</span><span>ID</span> <span>{</span>
    <span>case</span> <span>message</span><span>.</span><span>MsgUnchoke</span><span>:</span>
        <span>state</span><span>.</span><span>client</span><span>.</span><span>Choked</span> <span>=</span> <span>false</span>
    <span>case</span> <span>message</span><span>.</span><span>MsgChoke</span><span>:</span>
        <span>state</span><span>.</span><span>client</span><span>.</span><span>Choked</span> <span>=</span> <span>true</span>
    <span>case</span> <span>message</span><span>.</span><span>MsgHave</span><span>:</span>
        <span>index</span><span>,</span> <span>err</span> <span>:=</span> <span>message</span><span>.</span><span>ParseHave</span><span>(</span><span>msg</span><span>)</span>
        <span>state</span><span>.</span><span>client</span><span>.</span><span>Bitfield</span><span>.</span><span>SetPiece</span><span>(</span><span>index</span><span>)</span>
    <span>case</span> <span>message</span><span>.</span><span>MsgPiece</span><span>:</span>
        <span>n</span><span>,</span> <span>err</span> <span>:=</span> <span>message</span><span>.</span><span>ParsePiece</span><span>(</span><span>state</span><span>.</span><span>index</span><span>,</span> <span>state</span><span>.</span><span>buf</span><span>,</span> <span>msg</span><span>)</span>
        <span>state</span><span>.</span><span>downloaded</span> <span>+=</span> <span>n</span>
        <span>state</span><span>.</span><span>backlog</span><span>--</span>
    <span>}</span>
    <span>return</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/p2p/p2p.go#L53-L83">view in context</a></p>
</div>
<h3 id="time-to-make-requests">Time to make requests!</h3>
<p>Files, pieces, and piece hashes aren’t the full story—we can go further by breaking down pieces into <strong>blocks</strong>. A block is a part of a piece, and we can fully define a block by the <strong>index</strong> of the piece it’s part of, its byte <strong>offset</strong> within the piece, and its <strong>length</strong>. When we make requests for data from peers, we are actually requesting <em>blocks</em>. A block is usually 16KB large, meaning that a single 256 KB piece might actually require 16 requests.</p>
<p>A peer is supposed to sever the connection if they receive a request for a block larger than 16KB. However, based on my experience, they’re often perfectly happy to satisfy requests up to 128KB. I only got moderate gains in overall speed with larger block sizes, so it’s probably better to stick with the spec.</p>
<h3 id="pipelining">Pipelining</h3>
<p>Network round-trips are expensive, and requesting each block one by one will absolutely tank the performance of our download. Therefore, it’s important to <strong>pipeline</strong> our requests such that we keep up a constant pressure of some number of unfulfilled requests. This can increase the throughput of our connection by an order of magnitude.</p>
<img src="https://blog.jse.li/torrent/pipelining.png" alt="Two email threads simulating peer connections. The thread on the left shows a request followed by a reply, repeated three times. The thread on the left sends three requests, and receives three replies in quick succession.">
<p>Classically, BitTorrent clients kept a queue of five pipelined requests, and that’s the value I’ll be using. I found that increasing it can up to double the speed of a download. Newer clients use an <a href="https://luminarys.com/posts/writing-a-bittorrent-client.html">adaptive</a> queue size to better accommodate modern network speeds and conditions. This is definitely a parameter worth tweaking, and it’s pretty low hanging fruit for future performance optimization.</p>
<div>
<div><pre><code data-lang="golang"><span>// MaxBlockSize is the largest number of bytes a request can ask for
</span><span></span><span>const</span> <span>MaxBlockSize</span> <span>=</span> <span>16384</span>

<span>// MaxBacklog is the number of unfulfilled requests a client can have in its pipeline
</span><span></span><span>const</span> <span>MaxBacklog</span> <span>=</span> <span>5</span>

<span>func</span> <span>attemptDownloadPiece</span><span>(</span><span>c</span> <span>*</span><span>client</span><span>.</span><span>Client</span><span>,</span> <span>pw</span> <span>*</span><span>pieceWork</span><span>)</span> <span>([]</span><span>byte</span><span>,</span> <span>error</span><span>)</span> <span>{</span>
    <span>state</span> <span>:=</span> <span>pieceProgress</span><span>{</span>
        <span>index</span><span>:</span>  <span>pw</span><span>.</span><span>index</span><span>,</span>
        <span>client</span><span>:</span> <span>c</span><span>,</span>
        <span>buf</span><span>:</span>    <span>make</span><span>([]</span><span>byte</span><span>,</span> <span>pw</span><span>.</span><span>length</span><span>),</span>
    <span>}</span>

    <span>// Setting a deadline helps get unresponsive peers unstuck.
</span><span></span>    <span>// 30 seconds is more than enough time to download a 262 KB piece
</span><span></span>    <span>c</span><span>.</span><span>Conn</span><span>.</span><span>SetDeadline</span><span>(</span><span>time</span><span>.</span><span>Now</span><span>().</span><span>Add</span><span>(</span><span>30</span> <span>*</span> <span>time</span><span>.</span><span>Second</span><span>))</span>
    <span>defer</span> <span>c</span><span>.</span><span>Conn</span><span>.</span><span>SetDeadline</span><span>(</span><span>time</span><span>.</span><span>Time</span><span>{})</span> <span>// Disable the deadline
</span><span></span>
    <span>for</span> <span>state</span><span>.</span><span>downloaded</span> <span>&lt;</span> <span>pw</span><span>.</span><span>length</span> <span>{</span>
        <span>// If unchoked, send requests until we have enough unfulfilled requests
</span><span></span>        <span>if</span> <span>!</span><span>state</span><span>.</span><span>client</span><span>.</span><span>Choked</span> <span>{</span>
            <span>for</span> <span>state</span><span>.</span><span>backlog</span> <span>&lt;</span> <span>MaxBacklog</span> <span>&amp;&amp;</span> <span>state</span><span>.</span><span>requested</span> <span>&lt;</span> <span>pw</span><span>.</span><span>length</span> <span>{</span>
                <span>blockSize</span> <span>:=</span> <span>MaxBlockSize</span>
                <span>// Last block might be shorter than the typical block
</span><span></span>                <span>if</span> <span>pw</span><span>.</span><span>length</span><span>-</span><span>state</span><span>.</span><span>requested</span> <span>&lt;</span> <span>blockSize</span> <span>{</span>
                    <span>blockSize</span> <span>=</span> <span>pw</span><span>.</span><span>length</span> <span>-</span> <span>state</span><span>.</span><span>requested</span>
                <span>}</span>

                <span>err</span> <span>:=</span> <span>c</span><span>.</span><span>SendRequest</span><span>(</span><span>pw</span><span>.</span><span>index</span><span>,</span> <span>state</span><span>.</span><span>requested</span><span>,</span> <span>blockSize</span><span>)</span>
                <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
                    <span>return</span> <span>nil</span><span>,</span> <span>err</span>
                <span>}</span>
                <span>state</span><span>.</span><span>backlog</span><span>++</span>
                <span>state</span><span>.</span><span>requested</span> <span>+=</span> <span>blockSize</span>
            <span>}</span>
        <span>}</span>

        <span>err</span> <span>:=</span> <span>state</span><span>.</span><span>readMessage</span><span>()</span>
        <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
            <span>return</span> <span>nil</span><span>,</span> <span>err</span>
        <span>}</span>
    <span>}</span>

    <span>return</span> <span>state</span><span>.</span><span>buf</span><span>,</span> <span>nil</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/p2p/p2p.go#L85-L123">view in context</a></p>
</div>
<h3 id="maingo">main.go</h3>
<p>This is a short one. We’re almost there.</p>
<div>
<div><pre><code data-lang="golang"><span>package</span> <span>main</span>

<span>import</span> <span>(</span>
    <span>"log"</span>
    <span>"os"</span>

    <span>"github.com/veggiedefender/torrent-client/torrentfile"</span>
<span>)</span>

<span>func</span> <span>main</span><span>()</span> <span>{</span>
    <span>inPath</span> <span>:=</span> <span>os</span><span>.</span><span>Args</span><span>[</span><span>1</span><span>]</span>
    <span>outPath</span> <span>:=</span> <span>os</span><span>.</span><span>Args</span><span>[</span><span>2</span><span>]</span>

    <span>tf</span><span>,</span> <span>err</span> <span>:=</span> <span>torrentfile</span><span>.</span><span>Open</span><span>(</span><span>inPath</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>log</span><span>.</span><span>Fatal</span><span>(</span><span>err</span><span>)</span>
    <span>}</span>

    <span>err</span> <span>=</span> <span>tf</span><span>.</span><span>DownloadToFile</span><span>(</span><span>outPath</span><span>)</span>
    <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> <span>{</span>
        <span>log</span><span>.</span><span>Fatal</span><span>(</span><span>err</span><span>)</span>
    <span>}</span>
<span>}</span>
</code></pre></div><p><a target="_blank" rel="noopener" href="https://github.com/veggiedefender/torrent-client/blob/2bde944888e1195e81cc5d5b686f6ec3a9f08c25/main.go">view in context</a></p>
</div>

<h2 id="this-isnt-the-full-story">This isn’t the full story</h2>
<p>For brevity, I included only a few of the important snippets of code. Notably, I left out all the glue code, parsing, unit tests, and the boring parts that build character. View my <a href="https://github.com/veggiedefender/torrent-client">full implementation</a> if you’re interested.</p>

<div><p>This is an ad.
</p><p>
If you’d like to try building a BitTorrent client yourself, but could use some structure and personalized guidance, consider using <a href="https://app.codecrafters.io/join?via=veggiedefender">CodeCrafters</a>.
</p><p>
They make programming challenges for experienced engineers (e.g Build your own Git, Docker, Redis etc). In particular, they offer a BitTorrent challenge which walks you through everything described in this post — parsing bencode, discovering peers, and downloading pieces — with test harnesses and scaffolding code for a bunch of languages.
</p><p>
It’s a paid service, but you can get a discount with my referral link: <a href="https://app.codecrafters.io/join?via=veggiedefender">https://app.codecrafters.io/join?via=veggiedefender</a></p></div>


    </section>
  </article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Cancels Leases for AI Data Centers, Analyst Says (336 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2025-02-24/microsoft-cancels-leases-for-ai-data-centers-analyst-says</link>
            <guid>43157831</guid>
            <pubDate>Mon, 24 Feb 2025 10:12:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2025-02-24/microsoft-cancels-leases-for-ai-data-centers-analyst-says">https://www.bloomberg.com/news/articles/2025-02-24/microsoft-cancels-leases-for-ai-data-centers-analyst-says</a>, See on <a href="https://news.ycombinator.com/item?id=43157831">Hacker News</a></p>
Couldn't get https://www.bloomberg.com/news/articles/2025-02-24/microsoft-cancels-leases-for-ai-data-centers-analyst-says: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Cloudflare takes legal action over LaLiga's "disproportionate blocking efforts" (162 pts)]]></title>
            <link>https://www.broadbandtvnews.com/2025/02/19/cloudflare-takes-legal-action-over-laligas-disproportionate-blocking-efforts/</link>
            <guid>43157000</guid>
            <pubDate>Mon, 24 Feb 2025 08:19:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.broadbandtvnews.com/2025/02/19/cloudflare-takes-legal-action-over-laligas-disproportionate-blocking-efforts/">https://www.broadbandtvnews.com/2025/02/19/cloudflare-takes-legal-action-over-laligas-disproportionate-blocking-efforts/</a>, See on <a href="https://news.ycombinator.com/item?id=43157000">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Cloudflare takes legal action over LaLiga’s “disproportionate blocking efforts”" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text"><figure><img decoding="async" width="900" height="607" data-attachment-id="226912" data-permalink="https://www.broadbandtvnews.com/2025/02/19/cloudflare-takes-legal-action-over-laligas-disproportionate-blocking-efforts/laliga-yellow/" data-orig-file="https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow.jpg" data-orig-size="1200,809" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LaLiga Yellow" data-image-description="" data-image-caption="" data-medium-file="https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-300x202.jpg" data-large-file="https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-900x607.jpg" src="https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-900x607.jpg" alt="" srcset="https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-900x607.jpg 900w, https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-300x202.jpg 300w, https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow-768x518.jpg 768w, https://cdn.broadbandtvnews.com/wp-content/uploads/2025/02/19125845/LaLiga-Yellow.jpg 1200w" sizes="(max-width: 900px) 100vw, 900px"></figure><p>Cloudflare has launched a legal action against LaLiga over IP blocking action that it says blocked millions of users from accessing unrelated websites.</p> <p>As reported in Broadband TV News, websites DazcFutbolios and RBTV77 were last weekend blocked in a joint action by the Spanish football league and local ISPs after illegally distributing LaLiga matches.</p><p>LaLiga said the two pirates were using technology provided by Cloudflare to conceal their identity and evade security controls. Together, the two platforms had more than 400,000 unique monthly users in Spain.</p><p>Cloudflare called the effort misguided and said LaLiga had left it with no other option than to pursue the legal action. “LaLiga secured this blocking order without notifying cloud providers, while concealing from the court the predictable harm to third parties and the public good. LaLiga’s actions pose a clear threat to the open Internet. Cloudflare has now filed a legal action to challenge the order and establish that LaLiga’s disproportionate blocking efforts are unlawful.&nbsp;</p><p>“Instead of addressing Spanish users’ concerns about excessive content blocking, LaLiga has attempted to deflect with baseless claims against Cloudflare while doubling down on its unlawful blocking practices.&nbsp;Cloudflare hopes this legal action helps prevent future indiscriminate blocking measures, and makes it clear that rightsholders cannot prioritise&nbsp;their commercial interests over the fundamental right of millions of consumers to access the open Internet.”</p><p>Like the majority of cloud providers, Cloudflare uses shared IP addresses to manage its network, meaning that thousands of domains can be accessed with a single IP address.</p><p>LaLiga claims more than 50% of pirate IPs illegally distributing its content are protected by Cloudflare.&nbsp;</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We don't need startups, we need Digital-Mittelstand (132 pts)]]></title>
            <link>https://mertbulan.com/2025/02/24/we-dont-need-startups-we-need-digital-mittelstand/</link>
            <guid>43156785</guid>
            <pubDate>Mon, 24 Feb 2025 07:40:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mertbulan.com/2025/02/24/we-dont-need-startups-we-need-digital-mittelstand/">https://mertbulan.com/2025/02/24/we-dont-need-startups-we-need-digital-mittelstand/</a>, See on <a href="https://news.ycombinator.com/item?id=43156785">Hacker News</a></p>
Couldn't get https://mertbulan.com/2025/02/24/we-dont-need-startups-we-need-digital-mittelstand/: Error: timeout of 10000ms exceeded]]></description>
        </item>
    </channel>
</rss>