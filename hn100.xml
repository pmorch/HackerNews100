<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Jun 2025 14:30:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Being fat is a trap (125 pts)]]></title>
            <link>https://federicopereiro.com/fat-trap/</link>
            <guid>44200199</guid>
            <pubDate>Fri, 06 Jun 2025 12:32:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://federicopereiro.com/fat-trap/">https://federicopereiro.com/fat-trap/</a>, See on <a href="https://news.ycombinator.com/item?id=44200199">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

			
<p>This has been a difficult article to write. Partly because this is a painful topic for many, and partly because I have to be completely open and vulnerable to write about it. I only write this in the hope it can help others in similar situations. Nothing in this article is meant to judge you. If someone judges you for being or feeling fat, they should go f…figure out themselves. Judging people inside the fat trap just intensifies their misery and reduces the odds they can get out of it.</p>



<p>Let’s start with my story. I have been overweight (occasionally bordering on obese) from about age 8 to 23. At 23 I found out I was pre-diabetic and went on a strict three month diet that put me back on a healthy weight. I’ve maintained a healthy weight since then till the present (I’m 40 now). However, I’ve spent most of that time focused (sometimes obsessed) with not becoming fat again and generally struggling with food. It is only in the past couple of years that I am finally starting to trust myself with food and giving up the feelings of worry and insecurity about how my body looks and feels.</p>



<p>The notion of two traps, a physical trap and a mental trap, comes from Allen Carr’s fantastic <a href="https://en.wikipedia.org/wiki/The_Easy_Way_to_Stop_Smoking">Easy Way To Stop Smoking</a>. He states, quite convincingly, that there is a physical aspect to smoking (nicotine addiction) and a mental aspect to smoking (feeling that you need it).</p>



<p>I believe that the fat trap is also physical and mental. Let’s start with the physical, which is the easiest to understand. There is a range of body fat percentage that is healthy. If you are outside (probably above) this percentage, you are damaging your health. The science is unequivocal on this. What the range is is a bit harder to ascertain, but, from what I’ve read, it is about 10-20% for men and 15-25% for women.</p>



<p>The physical trap of being fat is being physically laden with just too much fat. Besides the overall damage to your health, it is less fun to be in an overweight body: you have less energy; you have more aches; it is harder to move, your sleep is worse. As painful and sad this is, there is no denying it.</p>



<p>Now, I believe the body positivity movement is a great step forward. Body positivity is about accepting others’ bodies, as well as your own, without regard to size, shape and gender. For those inside the fat trap, this brings tremendous relief. Being judged for being fat, or for being obsessed about fat, is almost always extremely counterproductive. It is harmful. If you are a non-fat person that goes around judging fat people, it might astonish you to find out that <strong>most fat people are painfully and constantly aware that they are fat</strong>, as well as the fact that that’s bad for them and they should make a change. There is nothing to be gained and everything to lose by judging someone for being fat. Eating disorders have extremely high mortality rates among psychiatric disorders: <em>if you’re making someone feel bad about their weight, you may well be increasing the likelihood they will die sooner.</em></p>



<p>And while self-acceptance is essential to get out of the mental fat trap, I don’t think you should accept that you are going to be overweight or even obese for the rest of your life. I might be wrong, but I believe you have a choice. It might be a very difficult choice, and it may take years, but you have the choice to step out of the fat trap. Both of them.</p>



<p>Getting out of the physical fat trap, strictly speaking, is simple:</p>



<ul>
<li>Develop basic sleep habits so that you get decent sleep.</li>



<li>Exercise daily or almost daily.</li>



<li>Walk 7500 steps a day.</li>



<li>Reduce refined carbs, unhealthy fats and alcohol from your diet. Focus on getting enough vegetables, fruits, complex carbs and healthy fats.</li>



<li>In some cases, you’ll also need medical advice, particularly if you have hormonal issues.</li>
</ul>



<p>If you stick to the above, you’ll be out of the physical fat trap in a few months — if you are morbidly obese, it might take a couple of years. But it doesn’t really matter how long it will take. What matters is that you start the journey.</p>



<p>Simple, however, is not easy. And what really makes it hard to stick with a healthy lifestyle is the mental fat trap.</p>



<p>The mental fat trap is the most difficult one to escape. For me, it manifests in the following thoughts:</p>



<ul>
<li>I want food but I can’t have it.</li>



<li>If I have what I want, I will be fat again.</li>



<li>I cannot trust myself with food.</li>



<li>I don’t look good enough to feel good about myself.</li>



<li>When I manage to achieve X target weight, <em>then</em> I will feel good.</li>



<li>Counting the days or weeks until you reach “your goal”.</li>



<li>I’m too old not to be fat.</li>



<li>I don’t have the genes not to be fat.</li>



<li>I don’t have the genes to eat healthily.</li>



<li>Let’s read about this diet.</li>
</ul>



<p>These thoughts are the walls of the mental fat trap. It is an obsession with the problem of being or feeling fat, even if you’re objectively not overweight. A typical pattern (at least for me) is to fixate on “when will I get there”. The answer is: you are there now. It is only through daily acceptance and work that you can be free. This point comes straight from Carr’s insight: that the mental trap is a waiting-for-nothing. When you focus on the present, the trap dissolves.</p>



<p>If getting out of the physical trap is to switch to a healthy lifestyle and stick with it, getting out of the mental trap is to leave behind the thoughts that keep you obsessed with food and dieting. Getting out is learning to identify as someone that no longer has issues with food, and that is healing every day from the wounds of being in the fat trap.</p>



<p>How to get out of the mental fat trap? I know three ways only.</p>



<ul>
<li>Therapy.</li>



<li>Meditation.</li>



<li>Read books on the subject.</li>
</ul>



<p>Therapy is going to reveal painful, uncomfortable truths that you’ve been avoiding — or rather, confronting indirectly through your obsession with food. Therapy can also give you tools to replace your harmful thoughts with better alternatives.</p>



<p>Meditation will give you space to detach from your thoughts, so that you can identify them and be able to start letting some of them go.</p>



<p>Reading is probably the easiest starting point. I can highly recommend <a href="https://federicopereiro.com/notes-roth-breaking-free/">Geneen Roth’s Breaking Free from Emotional Eating</a>. This book taught me about how some of us use food as a way to comfort ourselves, numb ourselves, and generally avoid or withstand painful aspects of our lives. Another great recommendation is <a href="https://www.amazon.com/Let-Go-story-about-weight/dp/9081958437">Andrew Dasselaar’s Let Go</a>. Both books are based on harrowing personal trials by the authors. Geneen has been helping people out of the fat trap for decades.</p>



<p>I’m still not out of the mental fat trap, at least not yet. I’m mostly out, and very grateful to have made it this far, but I still worry about food every other day or so. For me, getting out of this trap is a process of letting go of those thoughts that make food the main problem of my life, as well as the go-to solution for any uncomfortable feelings that arise.</p>



<p>I hope to have helped you, and not hurt you. If you think any of this is wrong or counterproductive, please let me know (fpereiro@gmail.com) and I will carefully consider your opinion.</p>

		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Jepsen: TigerBeetle 0.16.11 (128 pts)]]></title>
            <link>https://jepsen.io/analyses/tigerbeetle-0.16.11</link>
            <guid>44199592</guid>
            <pubDate>Fri, 06 Jun 2025 10:53:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/tigerbeetle-0.16.11">https://jepsen.io/analyses/tigerbeetle-0.16.11</a>, See on <a href="https://news.ycombinator.com/item?id=44199592">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://tigerbeetle.com/">TigerBeetle</a> is a distributed OLTP database oriented towards financial transactions. We tested TigerBeetle 0.16.11 through 0.16.30. We discovered seven client and server crashes, including a segfault on client close and several panics during server upgrades. Single-node failures could cause significantly elevated latencies for the duration of the fault, and requests were intentionally retried forever, which complicates error handling. We found only two safety issues: missing results for queries with multiple predicates, and a minor issue with a debugging API returning incorrect timestamps. TigerBeetle offered exceptional resilience to disk corruption, including damage to every replica’s files. However, it lacked a way to handle the total loss of a node’s data. As of version 0.16.30, TigerBeetle appeared to meet its promise of Strong Serializability. As of 0.16.45, TigerBeetle had addressed every issue we found, with the exception of indefinite retries. TigerBeetle has written a <a href="https://tigerbeetle.com/blog/2025-06-06-fuzzer-blind-spots-meet-jepsen/">companion blog post</a> to this work. This report was funded by TigerBeetle, Inc., and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p>TigerBeetle is an <a href="https://tigerbeetle.com/blog/2024-07-23-rediscovering-transaction-processing-from-history-and-first-principles">Online Transactional Processing (OLTP)</a> database built for double-entry accounting with a strong emphasis on safety and speed. It builds on the <a href="https://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication (VR)</a> consensus protocol to offer <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializable</a> consistency. Unlike general-purpose databases, TigerBeetle stores only accounts and transfers between them. This data model is well-suited for financial transactions, inventory, ticketing, or utility metering. To store other kinds of information, users typically pair TigerBeetle with other databases, linking them through user-defined identifiers.</p>
<p>TigerBeetle optimizes for <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/oltp#business-transactions-dont-shard-well">high-contention and high-throughput workloads</a>, such as central bank switches or brokerages. A central bank exchange might have only a half-dozen to a few hundred account records—one for each partner bank—and process <a href="https://www.bcb.gov.br/en/statistics/graphicdetail/graficospix/PixTransactionsAmount">hundreds of millions</a> of <a href="https://www.npci.org.in/what-we-do/upi/product-statistics">transactions per day</a> between 647 banks. A large brokerage, after the trading day closes, might need to settle the entire day’s trades as quickly as possible.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> These trades also tend to be concentrated on a small number of popular stocks. Under high contention, per-object concurrency control mechanisms can be the limiting factor in throughput. Instead, TigerBeetle funnels all writes through <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/performance#single-core-by-design">a single core on the primary VR node</a>. This limits throughput to whatever a single node can execute: TigerBeetle is firmly scale-up, not scale-out. To make that single node as fast as possible, TigerBeetle makes extensive use of batching, IO parallelization, a fixed schema, and hardware-friendly optimizations—such as fixed-size, cache-aligned data structures.</p>
<p>Refreshingly, TigerBeetle stresses fault tolerance in their marketing and documentation. They offer <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/safety">explicit models</a> for memory, process, clock, storage, and network faults. ECC RAM is assumed to be correct. Processes may pause or crash. Clocks may jump forward and backward in time. Disks are assumed to not only fail completely, but to tear individual writes or corrupt data. Networks may delay, drop, duplicate, misdirect, and corrupt messages. To mitigate these faults, TigerBeetle combines Viewstamped Replication with techniques from <a href="https://www.usenix.org/system/files/conference/fast18/fast18-alagappan.pdf">Protocol-Aware Recovery</a>, uses extensive checksums stored separately from data blocks, and for critical data, writes and reads multiple copies. TigerBeetle also makes extensive use of runtime correctness assertions to identify and limit the damage from faults and bugs alike.</p>
<p>Unlike most distributed systems, TigerBeetle claims to keep running without data loss <a href="https://web.archive.org/web/20241213103525/docs.tigerbeetle.com/about/safety#durability">if even a single replica retains a copy</a> of a record:</p>
<blockquote>
<p>A record would need to get corrupted on all replicas in a cluster to get lost, and even in that case the system would safely halt.</p>
</blockquote>
<p>To test safety under faults, TigerBeetle employs <a href="https://notes.eatonphil.com/2024-08-20-deterministic-simulation-testing.html">deterministic simulation testing</a>: tests which perform reproducible, pseudo-random operations against the system and ensure that some property holds.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> The <a href="https://web.archive.org/web/20250126010247/https://docs.tigerbeetle.com/about/vopr/">Viewstamped Operation Replicator (VOPR)</a> test simulates an entire TigerBeetle cluster, including clock, disk, and network interfaces. It simulates clock skew, corrupts reads and writes, loses and reorders network messages, and so on. There are other simulation tests which stress specific subsystems, as well as <a href="https://github.com/tigerbeetle/tigerbeetle/blob/092713075e797e633b6c306fb6d32b2523475f30/docs/internals/HACKING.md">a variety of more traditional integration and unit tests</a>.</p>
<p>TigerBeetle also offers a noteworthy approach to upgrades. Each TigerBeetle binary includes the code not just for that particular version, but several previous versions. For example, the 0.16.21 binary can run 0.16.17, 0.16.18, and so on through 0.16.21. To upgrade, one simply <a href="https://docs.tigerbeetle.com/operating/upgrading/">replaces the binary on disk</a>. TigerBeetle loads the new binary, but continues running with the current version. It then coordinates across the cluster to smoothly roll out each successive version, until all nodes are running the latest version available. This approach does not require operators to carefully sequence the upgrade process. Instead, upgrades are performed automatically, and coupled to the replicated state machine. This also allows TigerBeetle to ensure that an operation which commits on version <span><em>x</em></span> will never commit on any other version—guarding against state divergence.</p>
<h2 data-number="1.1" id="time"> Time</h2>
<p>TigerBeetle defines an <a href="https://docs.tigerbeetle.com/coding/time/">explicit model of time</a>. Viewstamped Replication forms a totally ordered sequence of state transitions, and its view and op numbers can be used as a totally ordered logical clock. Financial systems usually prefer wall clocks, so most TigerBeetle timestamps are in “physical time,” which, like <a href="https://cse.buffalo.edu/tech-reports/2014-04.pdf">Hybrid Logical Clocks</a>, approximate POSIX time with stronger ordering guarantees. Specifically, TigerBeetle leaders <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/vsr/clock.zig#L23">collect POSIX timestamps</a> from all replicas<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> and try to find a time which falls within a reasonable margin of error across a quorum of nodes.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> Those timestamps are incorporated into the VR-replicated state machine, and constrained to be strictly monotonic. When no quorum of clocks falls within a <a href="https://github.com/tigerbeetle/tigerbeetle/blob/5aa44f58870a510155cab8cb0ff56e629724bc74/src/config.zig#L122">twenty-second window</a> for longer than sixty seconds, the cluster refuses requests until clocks come back in sync.</p>
<p>As of October 2024, TigerBeetle’s documentation <a href="http://web.archive.org/web/20240823231836/https://docs.tigerbeetle.com/reference/account/#timestamp">described TigerBeetle timestamps</a> as “nanoseconds since UNIX epoch”. This is <a href="https://aphyr.com/posts/378-seconds-since-the-epoch">not quite true</a>: POSIX time is presently twenty-seven seconds less than the actual number of seconds since the epoch. During leap seconds or other negative time adjustments, TigerBeetle’s clock slows to a crawl until values from <code>CLOCK_REALTIME</code> catch up.</p>
<h2 data-number="1.2" id="data-model"> Data Model</h2>
<p><a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/coding/#data-model-overview">TigerBeetle’s data model</a> is specifically intended for <a href="https://en.wikipedia.org/wiki/Double-entry_bookkeeping">double-entry bookkeeping</a>. It has no way to represent arbitrary rows, objects, graphs, blobs, and so on.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> Instead TigerBeetle stores two types of data: <em>accounts</em>, and <em>transfers</em> between them. All fields are fixed-size,<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> and numbers are generally unsigned integers. All values are, with limited exceptions, immutable.</p>
<p>An <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/account/">account</a> represents an entity which sends and receives something. For example, a “Gross Revenues” account might accrue dollars, “Meadow Lake Wind Farm” might generate kilowatt-hours of electricity, and “Beyoncé” would obviously hold an ever-growing number of Grammy awards. Accounts are uniquely identified by a user-defined 128-bit <code>id</code>, a <code>ledger</code> which determines which accounts can interact with each other, a bitfield of <code>flags</code> controlling <a href="https://docs.tigerbeetle.com/reference/account/#flags">various behaviors</a>, a creation <code>timestamp</code>, a user-defined <code>code</code>, and three custom fields of varying sizes: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. There are also four derived fields which represent the current sum of transfers into (<em>credits</em>) and out of (<em>debits</em>) the account: <code>debits_pending</code>, <code>debits_posted</code>, <code>credits_pending</code>, and <code>credits_posted</code>.</p>
<p>A <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/transfer/">transfer</a> is an immutable record which represents an integer quantity moving from one account to another. Like accounts, transfers have a unique, user-specified, 128-bit <code>id</code>, a <code>code</code>, a <code>ledger</code>, a bitfield of <a href="https://docs.tigerbeetle.com/reference/transfer/#flags"><code>flags</code></a>, and three custom fields: <code>user_data_32</code>, <code>user_data_64</code>, and <code>user_data_128</code>. Transfers also include the <code>debit_account_id</code> and <code>credit_account_id</code> of the two accounts involved, and the integer <code>amount</code> transferred between them.</p>
<p>A single-phase transfer takes effect, or <em>posts</em>, immediately. A transfer can also be executed in <a href="https://docs.tigerbeetle.com/coding/two-phase-transfers/">two phases</a>, represented by two transfer records. The first phase, <em>pending</em>, reserves capacity in the debit and credit account for the given amount. The second phase posts the pending transfer, transferring at most the pending amount. Pending transfers can be explicitly <em>voided</em>, which cancels them, or automatically <em>expire</em>, which is controlled by a <code>timeout</code> field. Posting and voiding transfers use a flag and a <code>pending_id</code> field to indicate which pending transfer they resolve. Pending transfers resolve <a href="https://web.archive.org/web/20240828091507/https://docs.tigerbeetle.com/reference/transfer">at most once</a>.</p>
<p>A special kind of transfer can <a href="https://web.archive.org/web/20250128232234/https://docs.tigerbeetle.com/coding/recipes/close-account/"><em>close</em></a> an account, preventing it from participating in later transfers. Closing transfers are always pending. Account closures can be “un-done” by voiding the closing transfer.</p>
<p>Accounts are immutable with five exceptions: a <code>closed</code> flag, which is derived from closing and re-opening transfers, and four balance fields, which are derived from the sum of pending and posted transfers. Transfers are always immutable. One alters or un-does a transfer by creating a new, compensating transfer.</p>
<h2 data-number="1.3" id="operations"> Operations</h2>
<p>TigerBeetle clients make <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/"><em>requests</em></a> to update or query database state. Each request represents a single kind of logical operation, like <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts">creating accounts</a> or <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers">querying transfers</a>. Requests and their corresponding responses usually involve a batch of <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#batching-events">up to</a> 8190 <em>events</em>, all of the same type. For example, a <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers">create-transfers</a> request includes a batch of transfers to create, and logically<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> returns a batch of results, one per transfer. Read operations generally take a list of IDs, or a query predicate, and return a batch of matching records.</p>
<p>From a database perspective, each TigerBeetle request is a single transaction: an ordered group of micro-operations which execute atomically. Events within a request <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">are executed in order</a>. Each event observes a unique, strictly increasing timestamp.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> There are no interactive transactions, mixed read-write transactions, or indeed any kind of multi-request transactions.</p>
<p>TigerBeetle’s <a href="https://tigerbeetle.com/">home page</a> promises <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>, and the documentation is consistent with this promise. Requests execute <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/#guarantees">at most once</a>, and events within a request “do not interleave with events from other requests.” TigerBeetle also promises several <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/sessions/#guarantees">session safety properties</a>: a session “reads its own writes” and “observes writes in the order that they occur on the cluster.” These are guaranteed by <a href="https://cs.uwaterloo.ca/~kmsalem/pubs/DaudjeeICDE04.pdf">Strong Session Serializability</a>, which in turn is implied by Strong Serializability.</p>
<p>There are two kinds of write requests. The <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_accounts"><code>create_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/create_transfers"><code>create_transfers</code></a> requests add a series of accounts or transfers to the database. There are also six read requests. Users look up specific accounts or transfers by ID using <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_accounts"><code>lookup_accounts</code></a> and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/lookup_transfers"><code>lookup_transfers</code></a>. To query accounts or transfers matching a predicate, one uses <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_accounts"><code>query_accounts</code></a>, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/query_transfers"><code>query_transfers</code></a>, and <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_transfers"><code>get_account_transfers</code></a>. Finally, <a href="https://web.archive.org/web/20241213103525/https://docs.tigerbeetle.com/reference/requests/get_account_balances"><code>get_account_balances</code></a> reads historical balance information.</p>
<p>Requests are atomic in the sense that either all or none of a request’s events execute. However, specific events in a committed request can <em>logically</em> fail, returning error codes. For example, a <code>create_transfers</code> request might try to create two transfers, the first of which fails due to a balance constraint, and the second of which succeeds. This request can still commit, even though only one of its two transfers was added to the database.</p>
<p>To make one event contingent on another, TigerBeetle offers a sort of logical sub-transaction within a request, called a <em>chain</em>. Each event in a chain succeeds if and only if all others succeed. This allows users to express <a href="https://docs.tigerbeetle.com/coding/recipes/multi-debit-credit-transfers">complex, multi-step transfers</a> that succeed or fail atomically.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>We built a <a href="https://github.com/jepsen-io/tigerbeetle/">test suite</a> for TigerBeetle using the <a href="https://github.com/jepsen-io/jepsen">Jepsen testing library</a>, which combines property-based testing with fault injection. We tested TigerBeetle versions 0.16.11 through 0.16.30, including several development builds. Our tests ran on clusters of three to six<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> Debian nodes, both in LXC containers and on EC2 VMs.</p>
<p>TigerBeetle offers only a “smart” client which connects to every node in the cluster. These clients can mask concurrency errors by routing all requests to a single server.<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a> In addition to testing this smart-client behavior, we also ran tests with each client restricted to a single node, by passing invalid addresses for the other nodes. Since TigerBeetle followers do not proxy <code>register</code> requests to the leader, most clients spend their time attempting futile requests against inexorable followers. This is fine for safety testing, so long as they time out quickly enough to keep up with leader elections.</p>
<p>TigerBeetle’s domain-specific data model poses a challenge for validation. Jepsen has <a href="https://github.com/jepsen-io/elle">well-established tricks</a> for checking Strict Serializability of lists, sets, and registers, but TigerBeetle has no direct analogue to these structures.</p>
<p>As in our 2022 work on <a href="https://jepsen.io/analyses/radix-dlt-1.0-beta.35.1">Radix DLT</a>, we considered interpreting each account as a list of transfers. Creating a transfer would be interpreted as a pair of appends to the debit and credit accounts. A balance read could, with the help of a constraint solver, often be mapped to a read of a specific set of transfers. However, this leaves account creation and most queries untested. It also makes it difficult to validate the rich semantics of TigerBeetle transfers. For example, TigerBeetle supports balancing transfers, which adjusts the amount of a transfer to ensure the debit (and/or credit) account maintains certain invariants, like a positive or negative balance.</p>
<p>Instead, we decided to take advantage of TigerBeetle’s explicit total order of transactions. In broad strokes, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj">our checker</a> splits the problem into two interlocking parts. First, we check that the apparent timestamps of operations are Strong Serializable. Second, we check that the <em>semantics</em> of those operations, when executed in timestamp order, make sense.</p>
<h2 data-number="2.1" id="timestamp-order"> Timestamp Order</h2>
<p>Verifying timestamp order was relatively straightforward. TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new client API</a> which allowed us to read the timestamp assigned to every successful request. For operations which failed or timed out, we inferred their timestamps from the timestamp assigned to any of their effects. For instance, if the creation of account 3 timed out, but we later read account 3 with timestamp 72, we assumed that write executed at timestamp 72. TigerBeetle’s promise that timestamps are strictly ordered both within and between requests means that this inference should yield an order compatible with the request timestamps. We ignored any failed reads, whether definite or indefinite—this is safe as reads have no semantic side effects.</p>
<p>Timestamp inference required that we eventually observe the effect of every attempted write. We divided the test into two phases: a <em>main phase</em> involving writes and reads, and a <em>final read phase</em> where we tried to read any unseen writes until TigerBeetle definitively responded “yes, this write exists”, or “no, it does not (yet) exist.” Our goal was to infer exactly which operations executed during the main phase, and the timestamps of those operations.<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<p>If a write was observed, and its inferred timestamp fell before the timestamp of the last successfully acknowledged write,<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a> we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/checker.clj#L269-L310">inferred that it executed</a> during the main phase. If a write was <em>not</em> observed, we assumed that it did not execute during the main phase. There are two possible scenarios:</p>
<ul>
<li><p>TigerBeetle is Strong Serializable. If the write had executed during the main phase, Strong Serializability would have ensured its visibility during the final read phase. Our inference is correct.</p></li>
<li><p>TigerBeetle is not Strong Serializable. If the write did not execute during the main phase, our inference is correct. If it <em>did</em> execute during the main phase, our inference is incorrect. We might encounter false positives or false negatives—but in either case, TigerBeetle has failed to maintain a promised invariant.</p></li>
</ul>
<p>If TigerBeetle were Strong Serializable, our checker would not falsely report an error. If TigerBeetle were to (e.g.) exhibit a <a href="https://jepsen.io/consistency/phenomena/stale-read">stale read</a> or another violation of Strong Serializability, we might detect it indirectly. It could, for example, manifest as a model-checker error on a different operation much earlier in the history. This non-locality is not ideal, but we found it an acceptable tradeoff in exchange for excellent coverage.</p>
<p>Having inferred a set of operations executed during the main phase, and timestamps for each, we used <a href="https://github.com/jepsen-io/elle">Elle</a> to construct a graph over operations. We linked operations by real-time edges when operation A ended before operation B began.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a> We also linked operations in ascending timestamp order. A violation of Strong Serializability (as far as timestamps were concerned) would manifest as a cycle in this graph. Elle checks for cycles in roughly linear time, and constructs compact exemplars of consistency violations.</p>
<h2 data-number="2.2" id="model-checking"> Model Checking</h2>
<p>To verify that the semantics of TigerBeetle’s requests and responses were correct, we built a detailed, <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">single-threaded model</a> of the TigerBeetle state machine based on the documentation. This model is essentially a datatype with an initial state <span><em>i</em><em>n</em><em>i</em><em>t</em></span> and a transition function <span><em>s</em><em>t</em><em>e</em><em>p</em>(<em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>,&nbsp;<em>i</em><em>n</em><em>v</em><em>o</em><em>k</em><em>e</em>,&nbsp;<em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>t</em><em>e</em>) → <em>s</em><em>t</em><em>a</em><em>t</em><em>e</em>′</span>, which takes a state, the invocation of a request, and the completion of that request, and returns a new state. Illegal transitions (for instance, a read whose completion value does not agree with the state) returned a special <em>invalid</em> state. Given the inferred list of timestamp-sorted operations from the main phase, we stepped through each operation in order. Any invalid state was reported as an error.</p>
<p>We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">modeled the state</a> as an immutable data structure including the current timestamps,<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a> maps of IDs to accounts and transfers, transient errors<a href="#fn15" id="fnref15" role="doc-noteref"><sup>15</sup></a>, a set of indices to support efficient querying, and a few internal statistics. To model the flow of clocks, we provided each state with a pre-computed map of IDs to timestamps, derived from the reads performed during the test. Whenever one of those IDs was created, we <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">advanced the clock</a> to that timestamp.</p>
<p>The state machine is surprisingly complex, involving over 1,600 lines of Clojure and an <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">extensive test suite</a>. A <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj">broad array of error conditions</a> had to be handled, including duplicate IDs, non-monotonic timestamps, balance constraints, incompatible flags, and more. Linked chains of events required <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/model.clj#L392-L402">speculative execution and rollback</a> of the state—made simpler by our pure, functional approach. We made extensive use of Zach Tellman’s <a href="https://github.com/lacuna/bifurcan">Bifurcan</a>, a thoroughly tested library of high performance persistent data structures.</p>
<p>Modeling the full state machine takes time, but allows extraordinarily detailed verification of correctness. To make checking computationally tractable, typical Jepsen tests use only a handful of carefully selected data types and operations on them. The implicit assumption is that if the database’s concurrency control protocol handles that selected example correctly, it is likely correct for other workloads as well. Modeling the state machine in detail allowed us to check almost<a href="#fn16" id="fnref16" role="doc-noteref"><sup>16</sup></a> every<a href="#fn17" id="fnref17" role="doc-noteref"><sup>17</sup></a> operation TigerBeetle can perform. We verified that observed results of queries matched exactly, down to specific error codes. As discussed in this report, this approach found bugs we would have otherwise missed.</p>
<h2 data-number="2.3" id="generating-operations"> Generating Operations</h2>
<p>The downside of testing so much of TigerBeetle’s state machine is that we must then generate requests which <em>exercise</em> it. Generating syntactically valid requests is easy, but generating requests which often succeed, or queries which return non-empty results, is surprisingly hard.</p>
<p>Our <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/workload/generator.clj">generator</a> maintained extensive in-memory state throughout each test, including probabilistic models of which account and transfer IDs were likely to exist, which transfers were likely pending, what timestamps were likely extant, and what each worker process was currently doing. The generator updated this state with each operation’s invocation and completion.</p>
<p>We selected Zipfian distributed IDs, ledgers, codes, and so on, ensuring a mix of very hot and very cool objects. We used a broad set of parameters to guide stochastic choices of request types, account and transfer IDs, chain lengths, flags, queries, and probabilistic state updates. These parameters were carefully tuned across a variety of concurrencies, request rates, hardware environments, and fault conditions to find a reasonable balance of successes and failures, non-empty query results, attempted invariant violations, and so on.</p>
<h2 data-number="2.4" id="fault-injection"> Fault Injection</h2>
<p>Jepsen provides several kinds of faults “out of the box.” We <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj">stressed TigerBeetle</a> with process crashes (<code>SIGKILL</code>), pauses (<code>SIGSTOP</code>), a variety of transitive and non-transitive network partitions, and clock changes ranging from milliseconds to hundreds of seconds, as well as strobing the clock rapidly back and forth. We also <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/nemesis.clj#L395-L421">upgraded nodes through several versions</a> during tests.</p>
<p>We also introduced a variety of storage faults via a <a href="https://github.com/jepsen-io/jepsen/blob/8de1c9c9c1f7ac08fdf5c1eae3f709aa19cc3f9b/jepsen/resources/corrupt-file.c">new file corruption nemesis</a>. We flipped random bits to simulate (e.g.) cosmic ray interference. We replaced chunks of the file with other chunks, in an attempt to simulate <a href="https://pages.cs.wisc.edu/~remzi/OSTEP/file-integrity.pdf">misdirected writes</a>. We also took snapshots of chunks of the file, then restored them later, to simulate lost writes.</p>
<p>Each TigerBeetle node has a single data file, which is divided into <em>zones</em> at predictable offsets. Each zone stores a single kind of fixed-sized record. We scoped our faults to specific zones—corrupting, for example, only the write-ahead-log (WAL)’s headers, or restoring a snapshot of just one of the four redundant copies in the superblock zone. In many tests we corrupted multiple zones, or the entire file.</p>

<p>We also targeted a variety of nodes for file corruption. In one scenario, we corrupted data throughout (e.g.) the superblock, but only on a minority of nodes. In a second scenario, we corrupted every node’s data, but selected different chunks of the file for each node. For example, one node in a three-node cluster might corrupt the first, fourth, seventh, and tenth chunks of the grid; another would corrupt the second, fifth, eighth, and so on. We called this a <em>helical</em> disk fault. If you imagine arranging the cluster’s nodes into a ring, and drawing their file offsets along the ring’s symmetry axis, the corrupted chunks “spin” around the ring, forming a helix. Because TigerBeetle’s file layout is (generally speaking) bit-for-bit identical between up-to-date replicas, this avoids corrupting any single record in the database beyond repair.<a href="#fn18" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<h2 data-number="3" id="results"> Results</h2>
<h2 data-number="3.1" id="requests-never-time-out-206"> Requests Never Time Out (#206)</h2>
<p>Our first tests of TigerBeetle routinely stalled forever. For example, in <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/first-request-stalls.zip">this test run</a> the very first request never returned, which prevented the test from ever completing. This turned out to be a consequence of an unusual design decision: TigerBeetle <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/requests/#guarantees">actually guaranteed</a> that requests would never time out:</p>
<blockquote>
<p>Requests do not time out. Clients will continuously retry requests until they receive a reply from the cluster. This is because in the case of a network partition, a lack of response from the cluster could either indicate that the request was dropped before it was processed or that the reply was dropped after the request was processed.</p>
</blockquote>
<p>The <a href="https://web.archive.org/web/20240712120653/https://docs.tigerbeetle.com/reference/sessions/#retries">session documentation</a> reaffirmed this stance: a TigerBeetle client “will never time out” and “does not surface network errors”. This is particularly surprising since most systems do expose network errors, whether Strong Serializable or otherwise.</p>
<blockquote>
<p>With TigerBeetle’s strict consistency model, surfacing these errors at the client/application level would be misleading. An error would imply that a request did not execute, when that is not known[.]</p>
</blockquote>
<p>There are, broadly speaking, two classes of errors in distributed systems. A <em>definite</em> error, like a constraint violation, signifies that an operation has not and will never happen. An <em>indefinite</em> error, like a timeout, signifies that the operation may have already happened, might happen later, or might never happen.<a href="#fn19" id="fnref19" role="doc-noteref"><sup>19</sup></a> Consistent with the documentation, TigerBeetle tries to conceal both kinds of error through an unbounded internal retry loop.</p>
<p>However, TigerBeetle clients actually <em>can</em> produce timeout errors. The Java client’s asynchronous methods, like <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>createTransfersAsync</code></a>, return <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/latest/com.tigerbeetle/com/tigerbeetle/Client.html"><code>CompletableFutures</code></a>. <code>CompletableFuture</code> usually represents operations, like network requests, which are subject to indefinite failures. Indeed, timeouts are integral to the datatype: one awaits a future using <code>.get(timeout, timeUnit)</code>, or wraps it in <code>.orTimeout(seconds, timeUnit)</code> to throw a timeout automatically. Likewise, the .Net client’s <a href="https://github.com/tigerbeetle/tigerbeetle/blob/274de35df357b790f0ccddded3531bb2592fbe2f/src/clients/dotnet/TigerBeetle/Client.cs#L65"><code>createTransferAsync</code></a> and friends return <a href="https://learn.microsoft.com/en-us/dotnet/api/system.threading.tasks.task.wait?view=net-9.0#system-threading-tasks-task-wait(system-int32)"><code>Task</code></a> objects which offer timeout-driven <code>Wait()</code> methods.</p>
<p>Even if users constrain themselves to synchronous calls, applications rarely have unbounded time to run. It seems likely that applications will wrap TigerBeetle calls in their own timeouts. If they do not, the application may eventually terminate, which is a worse kind of indefinite failure. Even when applications can wait, their clients (or the human beings waiting for an operation), may give up at any time. The challenge of indefinite errors is intrinsic to asynchronous networks and cannot be eliminated.</p>
<p>Because TigerBeetle clients handle all failures through a silent internal retry mechanism, they unnecessarily convert definite errors into indefinite ones. For example, imagine a common fault: a TigerBeetle server has crashed. An application makes a <code>createTransfer</code> request. Its client attempts to open a TCP connection to submit the request, and receives <code>ECONNREFUSED</code>. The client knows internally that this request cannot possibly have executed: it has a definite failure. However, it refuses to inform the caller, and instead retries again and again. The caller’s only sign of an error is that the client appears to have stalled. When the caller times out or eventually shuts down, that definite failure becomes indefinite. Instead of making indefinite errors impossible, TigerBeetle’s client design <em>proliferates</em> them.</p>
<p>This is an ongoing discussion within TigerBeetle (#<a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a>). Jepsen recommended that TigerBeetle develop a first-class representation for definite and indefinite errors, and return those errors to callers when problems occur. It is perfectly fine to keep automatic retries—perhaps even unbounded ones—but this behavior should be configurable. TigerBeetle clients should take options controlling the maximum time allowed for opening a connection, and for awaiting responses from a submitted request. Users can request unbounded timeouts if desired.</p>
<h2 data-number="3.2" id="client-uninitialized-memory-access-2435"> Client Uninitialized Memory Access (#2435)</h2>
<p>Because synchronous client operations never timed out, our early tests generally failed to terminate. To avoid this problem, we tried wrapping calls to TigerBeetle clients in two kinds of timeouts. In the first, we spawned a new thread to make a synchronous call, and interrupted that thread if it did not complete within a few seconds.<a href="#fn20" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<div id="cb1"><pre><code><span id="cb1-1">(<span>let</span> [worker (<span>future</span> (.createAccounts</span>
<span id="cb1-2">                        client accounts))</span>
<span id="cb1-3">      ret    (<span>deref</span> worker <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb1-4">  (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb1-5">    (<span>do</span> (<span>future-cancel</span> worker)</span>
<span id="cb1-6">        (<span>throw</span> {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb1-7">    retval))</span></code></pre></div>
<p>In 0.16.11, this immediately <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/interrupt-segfault.log">segfaulted the entire JVM</a>. TigerBeetle’s Java client is implemented via a <a href="https://en.wikipedia.org/wiki/Java_Native_Interface">JNI</a> binding to a client library written in <a href="https://ziglang.org/">Zig</a>, and a Zig panic crashes the JVM as well. Concerned that our use of multiple threads or a thread interrupt might be at fault, we tried an alternate approach, using the client’s asynchronous methods which returned a <code>CompletableFuture</code>. If that future did not produce a result within a few seconds, we closed the client.</p>
<div id="cb2"><pre><code><span id="cb2-1">(<span>let</span> [<span>future</span> (.createAccountsAsync</span>
<span id="cb2-2">                c (account-batch accounts))</span>
<span id="cb2-3">       ret   (<span>deref</span> <span>future</span> <span>5000</span> <span>::timeout</span>)]</span>
<span id="cb2-4">    (<span>if</span> (<span>=</span> ret <span>::timeout</span>)</span>
<span id="cb2-5">      (<span>do</span> (close! client)</span>
<span id="cb2-6">          (throw+ {<span>:type</span> <span>:timeout</span>}))</span>
<span id="cb2-7">      ret))</span></code></pre></div>
<p>—this too <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/async-close-segfault.log">segfaulted the JVM</a>.</p>
<p>In a closely related issue, calling <code>client.close()</code> in 0.16.11, on a freshly opened client, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/close-unreachable.log">caused the JVM to panic</a> with a <code>reached unreachable code</code> error in <code>tb_client.zig:122</code>.</p>
<p>TigerBeetle traced these problems to uninitialized fields in the client’s request data structure (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">#2435</a>). These fields were normally filled in during request submission. However, if the client was closed between request creation and submission, it would dereference uninitialized pointers from that structure, jumping to random offsets in memory. This issue was fixed in 0.16.12.</p>
<h2 data-number="3.3" id="client-crash-on-eviction-2484"> Client Crash on Eviction (#2484)</h2>
<p>The official TigerBeetle clients crashed the entire process when a server informed them that their session had been evicted. TigerBeetle allows only <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/sessions/#eviction">64 concurrent sessions</a> by default, making it relatively easy to hit this limit.<a href="#fn21" id="fnref21" role="doc-noteref"><sup>21</sup></a> TigerBeetle also evicts clients which use a newer client version than the server.</p>
<p>This behavior made it impossible for clients to cleanly recover from eviction, or to back off under load. TigerBeetle changed this behavior in <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">#2484</a>. As of 0.16.13, clients return errors to their callers on eviction, rather than crashing the entire process.</p>
<h2 data-number="3.4" id="elevated-latencies-on-single-node-faults-2739"> Elevated Latencies on Single-Node Faults (#2739)</h2>
<p>When a single node failed, we often saw client latencies jump by three to five orders of magnitude. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/0241210T170510.624-single-node-latency-jump.zip">this test of a five-node cluster, with clients constrained to a single node each</a>, killing a single node caused minimum latencies to rise from less than one millisecond to ten seconds. There were fluctuations down to one second, but in general elevated latencies lasted for the full duration of a fault.</p>
<p><img src="https://jepsen.io/analyses/tigerbeetle-0.16.11/single-node-latency-jump-2.png" alt="A time-series plot of latencies. A red bar shows the period where n3 was dead. Latencies jump dramatically."><br>
</p>
<p>Under higher load, the situation could become considerably worse. Consider <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250214T110802.529-single-node-latency-jump-3.zip">this test run</a> with a three-node cluster, where each client was allowed to connect to all three nodes. A few seconds into the test, we killed node <code>n3</code>. This drove latencies on every client from between 1–50 milliseconds to roughly a hundred seconds per request. This situation persisted for almost a thousand seconds, until we restarted <code>n3</code>.</p>
<p>In the original <a href="https://pmg.csail.mit.edu/papers/vr.pdf">Viewstamped Replication</a> and <a href="https://www.pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a>, a primary sends a <em>prepare</em> message to every secondary when it wants to perform an operation. The secondaries send acknowledgements back to the primary, which can commit once <span><em>f</em></span> replicas have acknowledged. The failure of any single node (shown in red) causes a single acknowledgement to be lost, but does not affect any of the other nodes or their acknowledgements. The system as a whole is relatively insensitive to single-node failures.</p>

<p>TigerBeetle approaches prepares differently. Nodes are arranged in a ring, and the primary sends a single prepare message to the next secondary in the ring. That secondary sends a prepare to the following secondary, and so on, until all nodes have received the message. Acknowledgements are sent directly to the primary. This approach reduces the bandwidth requirements for any single node, but creates a weakness: if the primary must receive <span><em>f</em></span> acknowledgements to commit, the failure of any one of the next <span><em>f</em></span> replicas in the ring will prevent commit entirely. The effect of a single-node failure cascades through the rest of the ring. We opened issue <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">#2739</a> to track this issue.</p>
<p>Version 0.16.30 includes a new tactic to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2761">mitigate this problem</a>. Sending every other prepare message <em>backwards</em> around the ring allows half of prepares to bypass the faulty node. Since prepares must be processed in order, the replicas which receive these counter-ringward prepares must repair the ringward prepare messages they missed. Repairing takes time, but the overall effect is significant. Rather than hundred-second latencies during a single-node fault, 0.1.16 delivered 1–30 second latencies in our tests.</p>
<p>After our collaboration, TigerBeetle continued work on single-node fault tolerance, adding a new series of deterministic performance tests to their simulation framework. As of version 0.16.43, TigerBeetle includes <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">a host of performance improvements</a>. Nodes replicate in both directions around the ring, which reduces latencies and the impact of single failures. The ring topology is now dynamic: and the cluster continually adjusts the order of nodes to minimize latency based on network conditions and faults.</p>

<p>To support Jepsen’s tests, TigerBeetle added a <a href="https://javadoc.io/doc/com.tigerbeetle/tigerbeetle-java/0.16.13/com.tigerbeetle/com/tigerbeetle/Batch.Header.html">new, experimental API</a> in 0.16.13 for obtaining the execution timestamp for each request from a header included in response messages. In the Java client, this API routinely returned <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250106T115221.401-duplicate-timestamps.zip">incorrect and duplicate timestamps</a>. For example, both of these <code>create-transfers</code> operations returned identical timestamps.</p>
<div id="cb3"><pre><code><span id="cb3-1">{<span>:index</span>     <span>5827</span>,</span>
<span id="cb3-2"> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-3"> <span>:process</span>   <span>1</span>,</span>
<span id="cb3-4"> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-5"> <span>:value</span>     [<span>:ok</span>],</span>
<span id="cb3-6"> <span>:timestamp</span> <span>1736185975365035812</span>}</span>
<span id="cb3-7">{<span>:index</span>     <span>5829</span>,</span>
<span id="cb3-8"> <span>:type</span>      <span>:ok</span>,</span>
<span id="cb3-9"> <span>:process</span>   <span>11</span>,</span>
<span id="cb3-10"> <span>:f</span>         <span>:create-transfers</span>,</span>
<span id="cb3-11"> <span>:value</span>     [<span>:ok</span> <span>:ok</span> <span>:ok</span> <span>:ok</span>],</span>
<span id="cb3-12"> <span>:timestamp</span> <span>1736185975365035812</span>}</span></code></pre></div>
<p>TigerBeetle traced this bug to a <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">mutable singleton response object</a> in the Java client: <code>Batch.EMPTY</code>. Every empty response used the same instance of this object, updating its header to reflect that response’s timestamp. As responses overwrote each other’s headers, callers observed incorrect timestamps. Since TigerBeetle represents entirely-successful responses as empty batches, this happened quite often.</p>
<p>This bug (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">#2495</a>) was fixed in 0.16.14, just seven days after 0.6.13’s release. It did not affect the correctness of the actual data involved, only request timestamps from the Java client’s header API. We believe the impact to users was likely nil.</p>
<h2 data-number="3.6" id="missing-query-results-2544"> Missing Query Results (#2544)</h2>
<p>In version 0.16.13, responses for <code>query_accounts</code>, <code>query_transfers</code>, and <code>get_account_transfers</code> routinely omitted some or all results.<a href="#fn22" id="fnref22" role="doc-noteref"><sup>22</sup></a> Missing results were always at the end: each response was a (possibly empty) prefix of the correct results. This behavior occurred frequently in healthy clusters. For example, take <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20241205T192824.267-missing-query-results.zip">this test run</a>, where 281 seconds into the test, a client called <code>query_transfers</code> with the following filter:</p>
<div id="cb4"><pre><code><span id="cb4-1">{<span>:flags</span>  #{<span>:reversed</span>}</span>
<span id="cb4-2"> <span>:limit</span>  <span>9</span></span>
<span id="cb4-3"> <span>:ledger</span> <span>3</span></span>
<span id="cb4-4"> <span>:code</span>   <span>289</span>}</span></code></pre></div>
<p>This query returned a single result:</p>
<div id="cb5"><pre><code><span id="cb5-1">[{<span>:amount</span>            34N,</span>
<span id="cb5-2">  <span>:ledger</span>            <span>3</span>,</span>
<span id="cb5-3">  <span>:debit-account-id</span>  3137N,</span>
<span id="cb5-4">  <span>:pending-id</span>        0N,</span>
<span id="cb5-5">  <span>:credit-account-id</span> 1483N,</span>
<span id="cb5-6">  <span>:user-data</span>         <span>9</span>,</span>
<span id="cb5-7">  <span>:id</span>                327610N,</span>
<span id="cb5-8">  <span>:code</span>              <span>289</span>,</span>
<span id="cb5-9">  <span>:timeout</span>           <span>0</span>,</span>
<span id="cb5-10">  <span>:timestamp</span>         <span>1733448783658756894</span>,</span>
<span id="cb5-11">  <span>:flags</span>             #{<span>:linked</span>}}]</span></code></pre></div>
<p>However, our model expected eight additional transfers which TigerBeetle omitted. For instance, transfer <code>326112</code> had ledger 3 and code 289, and was successfully acknowledged five seconds before this query began. It should have been included in these results, but was not.</p>
<div id="cb6"><pre><code><span id="cb6-1">{<span>:amount</span>            <span>21</span>,</span>
<span id="cb6-2"> <span>:ledger</span>            <span>3</span>,</span>
<span id="cb6-3"> <span>:debit-account-id</span>  123076N,</span>
<span id="cb6-4"> <span>:pending-id</span>        0N,</span>
<span id="cb6-5"> <span>:credit-account-id</span> 51358N,</span>
<span id="cb6-6"> <span>:user-data</span>         <span>2</span>,</span>
<span id="cb6-7"> <span>:id</span>                326112N,</span>
<span id="cb6-8"> <span>:code</span>              <span>289</span>,</span>
<span id="cb6-9"> <span>:timeout</span>           <span>0</span>,</span>
<span id="cb6-10"> <span>:timestamp</span>         <span>1733448782536800935</span>,</span>
<span id="cb6-11"> <span>:flags</span>             #{}}</span></code></pre></div>
<p>Note that this query asked for transfers matching both <code>ledger = 3</code> and <code>code = 289</code>. Queries which filtered on only a single field did not exhibit this problem. TigerBeetle traced the cause to a bug in the zig-zag merge join between multiple indices (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">#2544</a>). When traversing an index, a bounds check prevented scanning the same chunk of records twice. During a join between two indices, the scans informed one another that some records can be safely skipped. This process could push the highest (or lowest) key outside the bounds check in the wrong direction, causing the scan to terminate early. The issue was fixed in version 0.16.17.</p>
<p>This bug went undetected by all four TigerBeetle fuzzers which perform index scans. Two fuzzers, <code>fuzz_lsm_tree</code> and <code>fuzz_lsm_forest</code>, did not perform joins. The other two, <code>vopr</code> and <code>fuzz_lsm_scan</code>, generated objects which happened to appear consecutively in each index—the “zig-zag” part of the merge join was never executed. <a href="https://github.com/tigerbeetle/tigerbeetle/commit/73f13ee2d092fe6998c315b0c4ae5fbc2983154a">Rewriting the scan fuzzer</a> to generate unpredictable objects helped it reproduce this bug quickly.</p>
<h2 data-number="3.7" id="panic-at-the-disk-0-2681a"> Panic! At the Disk 0 (#2681a)</h2>
<p>Occasionally, tests with single-bit file corruption in the superblock, WAL, or grid zones caused TigerBeetle 0.16.20 to <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250127T183251.857-bitflip-padding-panic.zip">crash on startup</a>. The process would print <code>panic: reached unreachable code</code>, then exit.<a href="#fn23" id="fnref23" role="doc-noteref"><sup>23</sup></a></p>
<p>These crashes were caused by three near-identical bugs in checking sector padding. For example, each superblock header in TigerBeetle’s data file contains an unused padding region normally filled with zeroes. Similarly, entries in the WAL and grid blocks may have zero padding bytes at the end. TigerBeetle’s checksums cover the data stored in each chunk, but exclude the padding. If a bit in the padding flipped from zero to one, the chunk’s checksum would still pass. Then, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/blob/9e754a3dbddfbd6b51ce44805b2718061f799a97/src/vsr/grid.zig#L1080">checked to make sure</a> the padding bytes were still zeroed. When it encountered the flipped bit, that failed assertion caused the server to crash. This is perhaps worth logging, but damage to padding bytes does not compromise safety. The corrupted padding could be re-zeroed or repaired from other replicas.</p>
<p>TigerBeetle’s internal testing with the VOPR did not discover this bug because it corrupted entire sectors, rather than single bits. Corrupting a sector caused the checksum to fail and triggered the repair process. The zero-padding assertion was never reached! TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">revised the VOPR (#2681)</a> to introduce single-byte errors, which reproduced the bugs. As of 0.16.26, TigerBeetle repairs sectors with corrupt padding, instead of crashing.</p>
<h2 data-number="3.8" id="panic-due-to-superblock-bitflips-2681b"> Panic Due to Superblock Bitflips (#2681b)</h2>
<p>In a closely related bug, TigerBeetle could crash with an identical <code>panic: reached unreachable code</code> error, when we flipped bits in the superblock’s region rather than padding. Each of the superblock’s four copies includes a unique two-byte <code>copy</code> number, so that TigerBeetle can detect if a write or read of the superblock was misdirected by the disk. However, each copy of the superblock was supposed to have an identical checksum. Those checksums therefore skipped over the copy number.</p>
<p>When writing a superblock back to disk, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">checked to make sure (#2681)</a> that the copy number was between 0 and 3. If the copy number had been corrupted on disk, and that corrupted version read into memory, that assertion would fail at write time, causing a panic. TigerBeetle resolved this in version 0.16.26 by resetting the copy number, rather than crashing.</p>
<h2 data-number="3.9" id="checkpoint-divergence-on-upgrade-2745"> Checkpoint Divergence on Upgrade (#2745)</h2>
<p>When testing upgrades from 0.16.25 and before to 0.16.26 and higher, we observed repeated TigerBeetle crashes with log messages like <code>panic: checkpoint diverged</code>. For example, this one-minute test <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250213T152133.089-0.16.26-upgrade-diverge.zip">upgraded a five-node cluster from 0.16.25 to 0.16.26</a>, with no other faults. Node n5 detected the new binary at 21:48, and switched to executing 0.16.26 at 22:01. Immediately after starting, it panicked at <code>replica.zig:1766</code>:</p>
<pre><code>2025-02-13 21:22:06.159Z error(replica):
4: on_prepare: checkpoint diverged (op=23040
expect=3779fc8a6a13bf5cf9f995b8895c2609
received=05383d884c680d15e726071358854f67
from=2)
thread 227936 panic: checkpoint diverged</code></pre>
<p>TigerBeetle traced this crash to a change in the <code>CheckpointState</code> structure in 0.16.26. Between checkpoints, TigerBeetle tracks a set of released blocks in the file. In 0.16.26, TigerBeetle changed when that set was flipped to empty. The old version of <code>CheckpointState</code> did not need to track released blocks, because that set was always empty at checkpoint time. The new version included released blocks. To ensure older replicas could still sync state from newer ones, nodes running 0.16.26 could send both the old and new versions of <code>CheckpointState</code>. This allowed a node running 0.16.26 to send a backwards-compatible <code>CheckpointState</code> with an empty set of released blocks to a node running 0.16.25. If that node then restarted on 0.16.26, it would be missing the released blocks which other replicas knew about. Thankfully, the assertion detected this divergence and crashed the node, rather than allowing clients to observe inconsistent data.</p>
<p>We found this bug after several later versions had already been released. Because it requires state synchronization, rather than the normal replication path, we believe healthy and non-lagging clusters shouldn’t encounter this bug. The impact should also be limited to a minority of nodes. Based on these factors, and a lack of test coverage for upgrades in general, TigerBeetle opted not to release a patched version of 0.16.26. Instead, the team updated the changelog (<a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">#2745</a>) to inform customers of the hazard. Operators should pause all clients and wait for replicas to catch up before upgrading to (or past) 0.16.26.</p>
<h2 data-number="3.10" id="panic-in-release_transition-on-multiple-upgrades-2758"> Panic in <code>release_transition</code> on Multiple Upgrades (#2758)</h2>
<p>In upgrade tests from 0.16.16 to 0.16.28, we found that TigerBeetle could crash with an assertion failure in <code>replica.zig</code>’s <code>release_transition</code> function. This happened when we executed multiple upgrades within ~20 seconds of one another, or when nodes crashed or paused during the upgrade process. We could reproduce this bug reliably–with process pauses, it manifested <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250220T153519-release-transition-panic.zip">a few times per minute</a>.</p>
<p>TigerBeetle traced this problem to <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">an over-zealous assertion</a> in the upgrade code (#2758).</p>
<p>Imagine a node currently runs version <span><em>A</em></span>, and an operator replaces its binary with version <span><em>B</em></span>. The node detects that version <span><em>B</em></span> is available, opens the new binary with a <a href="https://man7.org/linux/man-pages/man2/memfd_create.2.html"><code>memfd</code></a>, and uses <code>exec()</code> to replace the running process with that new code. Meanwhile, an operator replaces the binary with version <span><em>C</em></span>. The replica starts up with <span><em>B</em></span>, and as a safety check, asserts that the binary on disk (not the <code>memfd</code>!) has version header <span><em>B</em></span>. This assertion fails, since the binary is actually version <span><em>C</em></span>.</p>
<p>TigerBeetle resolved this issue in 0.16.29 by replacing the assertion with a warning message; running a different version than the binary on disk does not actually break safety.</p>
<h2 data-number="3.11" id="panic-on-deprecated-message-types-2763"> Panic on Deprecated Message Types (#2763)</h2>
<p>We encountered another occasional crash in the upgrade from 0.16.26 to 0.16.27. In <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250221T134853-deprecated-panic.zip">this test run</a>, two nodes crashed shortly after the upgrade. Both logged <code>panic: switch on corrupt value</code>, originating in <code>message_header.zig</code>’s <code>into_any</code> function. The crashed nodes recovered after a restart.</p>
<p>This crash was caused by a switch expression which <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">dispatched based on the type of a message</a>. Prior to 0.16.28, Tigerbeetle removed deprecated message types from these switch expressions. An older node could send a network message of a type that the newer node would have no corresponding <code>switch</code> case for. TigerBeetle resolved the issue in version 0.16.29 by adding the deprecated message types back into the switch statements, and simply ignoring them.</p>
<h2 data-number="3.12" id="no-recovery-path-for-single-node-disk-failure-2767"> No Recovery Path for Single-Node Disk Failure (#2767)</h2>
<p>TigerBeetle offers exceptional resilience to data file corruption. However, disk failure, fires, EBS volume errors, operator errors, and more can cause a node to lose its entire data file, or to corrupt that data beyond repair. TigerBeetle is fault tolerant and can continue running safely with a minority of nodes offline. However, failed nodes do need to be replaced eventually, and most distributed systems have a mechanism for doing so. In systems which support membership changes, the best path is often to add a new, replacement node to the cluster, and to remove the failed node. Others have dedicated replacement procedures.</p>
<p>TigerBeetle, surprisingly, has no story for replacing a failed node. The documentation says nothing on the matter. There is an undocumented recovery procedure: users can run <code>tigerbeetle format</code> to re-initialize the node with an empty data file, and allow TigerBeetle’s automatic repair mechanisms to transfer the data from other nodes. Since our tests often damaged data files beyond repair, we used this reformat approach regularly.</p>
<p>Reformatting nodes works most of the time, but as TigerBeetle explained to Jepsen, it may be unsafe. For example, imagine a committed operation <code>op</code> is present on two out of three nodes, and one of those nodes is reformatted. The cluster now has a two-thirds majority which can execute a view change <em>without</em> observing <code>op</code>; the operation is then lost. In our testing, data loss was infrequent, and limited to just a few operations. For example, <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250226T161747-upgrade-reformat.zip">this run</a> lost five acknowledged transfers which were created in two separate requests. Another problem arises when nodes are upgraded. If a node is formatted using a newer binary, but the cluster has not yet completed the transition to that version, the node will <a href="https://s3.amazonaws.com/jepsen.io/analyses/tigerbeetle-0.16.11/20250225T043429-upgrade-reformat.zip">crash on startup</a> during <code>replica.zig/open</code>.</p>
<p>TigerBeetle had been aware of <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">this issue #2767</a> for some time, and planned to add a safe recovery path for the loss of a node. However, it took time to design, build, and document. After our collaboration, TigerBeetle completed this work. Version 0.16.43 incorporates a new <a href="https://docs.tigerbeetle.com/operating/recovering/"><code>tigerbeetle recover</code></a> command to recovera node which has suffered catastrophic data loss.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/206">206</a></td>
<td>Requests never time out</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2435">2435</a></td>
<td>Client uninitialized memory access on close</td>
<td>Interrupt or close a client</td>
<td>0.16.12</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2484">2484</a></td>
<td>Client crash on eviction</td>
<td>Newer, or too many, clients</td>
<td>0.16.13</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739">2739</a></td>
<td>Elevated latencies on single-node fault</td>
<td>Pause or crash</td>
<td>0.16.43</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2495">2495</a></td>
<td>Incorrect header timestamp from Java client</td>
<td>None</td>
<td>0.16.14</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2544">2544</a></td>
<td>Missing query results</td>
<td>None</td>
<td>0.16.17</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681">2681a</a></td>
<td>Panic on bitflips in chunk padding</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2681/commits/f38f2921435c71ad1eb20369bc52674edb1dcfe6">2681b</a></td>
<td>Panic on bitflips in superblock copy number</td>
<td>Bitflip and restart</td>
<td>0.16.26</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2745">2745</a></td>
<td>Checkpoint divergence</td>
<td>0.16.26 upgrade during sync</td>
<td>Documented</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2758">2758</a></td>
<td>Panic in <code>release_transition</code> on upgrades</td>
<td>Upgrades in quick succession</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/pull/2763">2763</a></td>
<td>Panic on deprecated message types</td>
<td>Upgrade</td>
<td>0.16.29</td>
</tr>
<tr>
<td><a href="https://github.com/tigerbeetle/tigerbeetle/issues/2767">2767</a></td>
<td>No recovery path for single-node disk failure</td>
<td>Single-node disk failure</td>
<td>0.16.43</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>We found two safety issues in TigerBeetle.<a href="#fn24" id="fnref24" role="doc-noteref"><sup>24</sup></a><a href="#fn25" id="fnref25" role="doc-noteref"><sup>25</sup></a> Prior to version 0.16.17, TigerBeetle often omitted results from queries with multiple filters, even in healthy clusters. We also found a very minor issue in which a debugging API in the Java client, added specifically for our tests, returned incorrect and duplicate timestamps for operations. As of 0.16.26 and higher, our findings were consistent with TigerBeetle’s claims of <a href="https://jepsen.io/consistency/models/strong-serializable">Strong Serializability</a>—one of the strongest consistency models for concurrent systems. TigerBeetle preserved this property through various combinations of process pauses, crashes, network partitions, clock errors, disk corruption, and upgrades.</p>
<p>We also found seven crashes in TigerBeetle. Two affected the Java client: an uninitialized memory access caused by a shared mutable data structure, and a design choice to crash the entire process when a server evicted a client. Both were fixed by 0.16.13. Five involved servers: two panics on disk corruption, and three more involving upgrades. All crashes were resolved by 0.16.29, with the exception of #2745, which is now documented.</p>
<p>We found some surprising performance and availability issues in TigerBeetle. Server latencies jumped dramatically when even a single node was unavailable. This behavior is unusual—most consensus systems are relatively insensitive to single-node failures—and stemmed from a design choice to replicate data in a ring, rather than broadcasting from the primary to all backups directly. This behavior was somewhat improved in 0.16.30, but still quite noticeable. After our collaboration, TigerBeetle extended their simulation tests to measure performance under various faults, and used those tests to drive <a href="https://github.com/tigerbeetle/tigerbeetle/issues/2739#issuecomment-2919767029">extensive improvements</a> in 0.16.43. The ring topology now continually adapts to observed latencies, and messages are broadcast in both directions around the ring. We believe these improvements should significantly mitigate the latency impact of failures.</p>
<p>TigerBeetle also lacked a safe path to recover a node which had suffered catastrophic disk failure. After our collaboration, TigerBeetle <a href="https://github.com/tigerbeetle/tigerbeetle/pull/2996">built</a> a new <a href="https://docs.tigerbeetle.com/operating/recovering/">recovery command</a>, which is available as of 0.16.43.</p>
<p>Only one issue remains unresolved. By design, client requests are retried forever, which complicates error handling. TigerBeetle plans to address this, but the work will take some time.</p>
<p>We recommend users upgrade to 0.16.43, which addresses all but one of the issues reported here. Users should exercise particular caution during the upgrade to (or past) 0.16.26; consult <a href="https://github.com/tigerbeetle/tigerbeetle/blob/main/CHANGELOG.md#tigerbeetle-01626">the release notes</a>. We also suggest that users simulate single-node failures in a testing environment, and measure how their application responds to elevated latencies.</p>
<p>TigerBeetle exhibits a refreshing dedication to correctness. The architecture appears sound: Viewstamped Replication is a well-established consensus protocol, and TigerBeetle’s integration of flexible quorums and protocol-aware recovery seem to have allowed improved availability and extreme resilience to data file corruption. Integrating these protocols does not appear to have compromised the key invariant of Strong Serializability. Most of our findings involved crashes or performance degradation, rather than safety errors. Moreover, several of those crashes were due to overly cautious assertions.</p>
<p>We attribute this robustness in large part to TigerBeetle’s extensive simulation, integration, and property-based tests, which caught a broad range of safety bugs both before and during our engagement. As we brought new issues to the TigerBeetle team, they quickly expanded their internal test suite to reproduce them. We are confident that TigerBeetle’s investment in careful engineering and rigorous testing will continue to pay off, and we’re excited to see these techniques adopted by more databases in the future.</p>
<p>As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. While we make extensive efforts to find problems, we cannot prove correctness.</p>
<h2 data-number="4.1" id="disk-faults"> Disk Faults</h2>
<p>TigerBeetle offers exceptional resilience to disk faults. In our tests, it recovered from bitflips and other kinds of file corruption in almost every part of a node’s data file, so long as corruption was limited to a minority of nodes. In some file zones, like the grid, TigerBeetle tolerated the loss or corruption of all but one copy. In the superblock, client replies, and grid zones of the data file, TigerBeetle could recover our “helical” faults, in which every node experienced data corruption spread across disjoint regions of the file.</p>
<p>As previously noted, bitflips in the superblock copy number, or in various zero-padding regions, could cause TigerBeetle to crash. These issues have been resolved as of 0.16.26.</p>
<p>Rolling back all four copies of a node’s superblock to an earlier version can permanently disable a TigerBeetle node; it will crash upon detecting WAL entries newer than the superblock. TigerBeetle considers this outside their fault model, and we concur. Given that TigerBeetle performs four separate, sequential writes of the superblock and reads them back to confirm their presence, it seems remarkably unlikely to encounter this by accident.</p>
<p>Helical faults in the WAL can permanently disable a TigerBeetle cluster. The most recent “head” entry of the WAL is critical, and since some nodes may lag behind others, they may have heads at different file offsets. In our tests, helical faults often corrupted the head of the WAL on a majority of nodes, rendering the entire cluster unusable.</p>
<p>When a node’s disk file is lost or corrupted beyond repair, TigerBeetle currently has no safe path for recovery. We recommend users exercise caution when reformatting a failed node. Avoid upgrades when a node is down, and try to reformat a node only when the remainder of the cluster appears healthy. If possible, pause clients and check node logs before upgrading: none should be logging sync-related messages.</p>
<h2 data-number="4.2" id="retries"> Retries</h2>
<p>Users should think carefully about the official TigerBeetle clients’ retry behavior. By default, clients retry operations forever. Synchronous operations will never time out; you may need to implement your own timeouts. The futures returned by asynchronous calls do offer APIs with timeouts, but the client will continue retrying those operations forever. Long-lasting unavailability could cause TigerBeetle clients to consume unbounded memory as they attempt to buffer and retry an ever-growing set of requests.</p>
<p>This retry behavior flattens definite and indefinite failures into indefinite ones: <em>everything</em> becomes a timeout. Contrary to TigerBeetle’s documentation, indefinite network errors are very much possible. Indeed, they are more likely in TigerBeetle than in systems which return distinct network errors! Moreover, TigerBeetle users may find it more difficult to implement (e.g.) exponential backoff or load-shedding circuit breakers: in order to abandon a single request, the entire client must be torn down.</p>
<p>Jepsen recommends that users carefully consider and test their timeout behavior during faults. We also suggest TigerBeetle introduce at least two kinds of error, so users can distinguish between definite and indefinite faults. Finally, clients should take configurable timeouts, so users can bound their time and memory consumption.</p>
<h2 data-number="4.3" id="crashing-as-a-way-of-life"> Crashing as a Way of Life</h2>
<p>TigerBeetle prizes safety, and employs defensive programming techniques to ensure it. In addition to carefully designed algorithms and extensive testing, both client and server code are full of assertions which double-check that intended invariants have been preserved. Assertion failures crash the entire program to preserve safety. When this happens, clients or servers may be partially or totally unavailable, sometimes for minutes, sometimes permanently. Many of our findings involved an assertion which turned what <em>would</em> have been a safety hazard into a simple crash: a welcome tradeoff for safety-critical systems.</p>
<p>This is a sensible approach. Complex systems <a href="https://how.complexsystems.fail/">ensure safety</a> through an interlocking set of guards: each guard screens out errors the others might have missed. Abandoning possibly-incorrect execution is also a core tenet of Erlang/OTP’s <a href="https://erlang.org/pipermail/erlang-questions/2003-March/007870.html">“let it crash”</a> ethos. However, TigerBeetle’s approach is not without drawbacks.</p>
<p>First, several of the crashes we found in this report were due to overly conservative assertions. For instance, prior to 0.16.26, TigerBeetle crashed on encountering non-zero bytes in unused padding regions on disk. Safety was never endangered by these errors, but the crashes compromised availability—and could push users into the dangerous recovery path of reformatting.</p>
<p>Second, in 0.16.11, TigerBeetle’s client library forcibly crashed the entire application process when a client used a newer version than the server, or when the server simply had too many connections. These are errors that a well-designed application can and should recover from—for instance, through an exponential backoff and retry system, or by coordinating with other clients. Crashing the process, instead of returning an error code or throwing an exception, denies the application the ability to make these mitigations.</p>
<p>In Erlang, “let it crash” means more than simply abandoning computation early. It is integrally linked with Erlang’s actor model, which allows actors to crash independently of one another. It also relies on Erlang’s <em>supervisor trees</em>: every actor has a supervisor which is notified of a crash and can restart the failed computation. In TigerBeetle, the failure domain is the entire POSIX process, and the supervisor, where one exists, is something like the init system or Kubernetes. These supervisors generally lack visibility into <em>why</em> the crash happened or how to recover, and they are often not equipped to adapt to changing circumstances. They may restart the process over and over again, crashing every time. On repeated crashes, they may give up on the process forever.</p>
<p>Despite these limitations, we feel TigerBeetle makes a reasonable compromise. TigerBeetle is intended for financial systems of record where integrity is key, and overly-cautious assertions can be fixed easily as they arise. Those assertions also help to experimentally validate and guide the mental models of engineers working on TigerBeetle. TigerBeetle’s clients have shifted more towards returning error codes, rather than crashing outright.</p>
<p>More generally, we encourage engineers to think about fault domains when designing error paths. Ask, “If we must crash, how can we keep a part of the system running?” And after a crash, “How will that part recover?” This is especially important for client libraries, which are guests in another system’s home.</p>
<h2 data-number="4.4" id="future-work"> Future Work</h2>
<p>TigerBeetle includes a <a href="https://docs.tigerbeetle.com/reference/transfer/#timeout">timeout mechanism</a> for pending transfers. We do not know how to robustly test this system, since timeouts may, by design, not void a transfer until some time after their deadline has passed. We would like to revisit timeout semantics with an eye towards establishing quantitative bounds.</p>
<p>During the course of this research, Jepsen, TigerBeetle, and <a href="https://antithesis.com/">Antithesis</a> collaborated to run Jepsen’s TigerBeetle test suite within Antithesis’s environment—taking advantage of Antithesis’ deterministic simulation, fault injection, and time-travel debugging capabilities. These experiments are still in the early stages, but could lay the groundwork for a powerful, complementary analysis of distributed systems.</p>
<p>Multi-version systems are also devilishly hard to pull off. While TigerBeetle already had excellent test coverage for single versions, they lacked fuzz tests for cross-version upgrades. Our tests found several issues in the upgrade process, and TigerBeetle plans to expand their testing of upgrades in the future. Similarly, membership changes in distributed systems are notoriously challenging, and currently unimplemented in TigerBeetle. As TigerBeetle builds support for adding and removing nodes, we anticipate a rich opportunity for further testing.</p>
<p>Finally, TigerBeetle’s approach to retries has been the subject of ongoing discussion, and redesigning their approch will take time. We anticipate further work towards a robust client representation of errors.</p>
<p><em>This work would not have been possible without the invaluable assistance of the TigerBeetle team, including Fabio Arnold, Rafael Batiati, Chaitanya Bhandari, Lewis Daly, Joran Dirk Greef, djg, Alex Kladov, Federico Lorenzi, and Tobias Ziegler. Our thanks to <a href="https://github.com/duckinator">Ellen Marie Dash</a> for helping write the new file-corruption nemesis used in this research. We are grateful to <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her editorial support. This report was funded by TigerBeetle, Inc.&nbsp;and conducted in accordance with the <a href="https://jepsen.io/analyses/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>To give some idea of the rates involved—India’s Unified Payments Interface <a href="https://www.npci.org.in/what-we-do/upi/product-statistics">processes roughly 16 billion transfers per month</a>, which works out to about 6,000 per second, on average. <a href="https://www.clearstreet.io/">Clear Street</a>, a brokerage in New York, indicates that they process on the order of 30,000 debit-credit transfers per second after the market closes.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Deterministic simulation testing is essentially <a href="https://dl.acm.org/doi/pdf/10.1145/3597503.3639581">property-based testing</a> with techniques to turn non-deterministic systems into deterministic ones. The clock, disk state, scheduler, network delivery, external services, and so on are controlled to ensure reproducibility. For more on this approach, you might start with <a href="https://smallbone.se/papers/finding-race-conditions.pdf">PULSE</a>, <a href="https://www.youtube.com/watch?v=N5HyVUPuU0E">Simulant</a>, <a href="https://www.foundationdb.org/files/fdb-paper.pdf">FoundationDB</a>, and <a href="https://antithesis.com/solutions/problems_we_solve/">Antithesis</a>.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Timestamps are derived from <code>CLOCK_REALTIME</code>, which is presumably synchronized via NTP, PTP, etc. The primary uses <a href="https://tigerbeetle.com/blog/2021-08-30-three-clocks-are-better-than-one"><code>CLOCK_BOOTTIME</code></a> to estimate and compensate for network latency in clock messages.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Many consensus systems use a majority of nodes as a quorum. As Heidi Howard <a href="http://hh360.user.srcf.net/blog/2016/08/majority-agreement-is-not-necessary/">showed in 2016</a>, Paxos can use <em>different</em> quorums for its leader election and replication phases; these two quorums must intersect, but one may be less than a majority. TigerBeetle applies this “flexible quorum” approach to Viewstamped Replication. It requires only half, not a majority, of clocks to agree.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>TigerBeetle’s core is designed to replicate arbitrary state machines, so this may change in the future.<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>This representation is unusual: most databases allow user-defined schemas, a variety of types, and variable-size data. However, TigerBeetle’s domain is well-understood: the broad shape of financial record-keeping has not changed in centuries. Moreover, a rigid, fixed-size schema provides significant performance advantages: efficient encoding and decoding, zero-copy transfer of structures between network and disk, prefetcher/branch prediction friendliness, cache line alignment, and so on.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>For efficiency, TigerBeetle omits successful results from the actual response messages, and returns only errors, if present.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><a href="https://www.datomic.com/">Datomic</a>, <a href="https://fauna.com/">FaunaDB</a>, and TigerBeetle are all Strong Serializable temporal databases. However, they choose varying semantics for the flow of time and effects within a transaction. Datomic evaluates the parts of a transaction <a href="https://jepsen.io/analyses/datomic-pro-1.0.7075#intra-transaction-semantics">concurrently</a>, and assigns them all a single timestamp. Fauna executes them sequentially, but all operations <a href="https://docs.fauna.com/fauna/current/reference/fql-api/time/now/">observe a single timestamp</a>. TigerBeetle executes sequentially <em>and</em> gives each micro-operation a distinct timestamp.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>Unlike most consensus systems, which use majority quorums and work best with an odd number of nodes, TigerBeetle uses flexible quorums which allows some operations to commit with (e.g.) just three out of six nodes.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Imagine a write <span><em>w</em></span> is acknowledged by node <span><em>a</em></span>, but node <span><em>b</em></span> lags behind, such that a read sent to <span><em>b</em></span> would not observe <span><em>w</em></span>. This would be a <a href="https://jepsen.io/consistency/phenomena/stale-read">stale read</a>—a violation of Strong Serializability. Smart clients tend to route all requests to <em>either</em> <span><em>a</em></span> or <span><em>b</em></span>, rather than balancing requests between them. A test suite using such a client would likely miss the stale read.<a href="#fnref10" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>This technique does not work for imported events, where reads tell us the <em>imported</em> timestamp, rather than the <em>execution</em> timestamp. When testing imports, we used very long timeouts, and required that every operation succeed in order to check the history.<a href="#fnref11" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>We used the timestamp of the last successful write as the upper bound on the main phase. Writes may have been executed during the final read phase (e.g.&nbsp;due to network delays), but we ignored them for safety checking.<a href="#fnref12" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p>For efficiency, we actually computed a transitive reduction of the real-time dependency graph.<a href="#fnref13" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p>TigerBeetle has three internal timestamps that constrain clock values: the “current” timestamp, and two separate clocks for imported accounts and transfers, whose timestamps are still monotonic, but lag behind the current time.<a href="#fnref14" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p>TigerBeetle guarantees that certain classes of errors, called <a href="https://web.archive.org/web/20241222161312/https://docs.tigerbeetle.com/reference/requests/create_transfers/#id_already_failed"><em>transient errors</em></a>, ensure that a transfer will <em>always</em> fail, even if resubmitted under conditions where it would otherwise succeed. These errors are transient in the sense that they are caused by (potentially) short-lived conditions in the database state, but persistent in the sense that the database must remember them for all time.<a href="#fnref15" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p>Our strategy does require that a single ID is never written twice. We complemented the main workload with a dedicated <a href="https://github.com/jepsen-io/tigerbeetle/blob/815e32d20185db00b4b72bbfd8c1e43e2dff5e8b/src/jepsen/tigerbeetle/workload/idempotence.clj">idempotence</a> workload which verifies that duplicate attempts to write the same data never succeed, and never lead to divergent values for the same ID.<a href="#fnref16" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p>TigerBeetle includes an automatic timeout mechanism for pending writes, but timeouts are not exactly deterministic, which makes it difficult to model-check.<a href="#fnref17" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p>One notable exception to this rule is the WAL. The write-ahead log is built as a ring buffer. The head of the write-ahead log is critical: if the head of the WAL is corrupted on one node, that node cannot trust its own data file and must ask the other nodes to help it repair the damage. Because some nodes may lag behind others, it is possible that the head of the WAL could be at different file offsets on different nodes. A helical fault could corrupt the head on a majority of nodes, preventing the cluster from recovering.<a href="#fnref18" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p>To be clear: a definite failure can be retried, and that retry operation might succeed. When we say a definite error means an operation will “never happen,” we refer to the original operation, not retries.<a href="#fnref19" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p>This is Jepsen’s <a href="https://github.com/jepsen-io/jepsen/blob/ce07779ca3b81feb8553276581faa73d963a95b7/jepsen/src/jepsen/util.clj#L471-L482">standard timeout macro</a>, used when clients don’t time out reliably on their own. For clarity, we’ve omitted some error handling.<a href="#fnref20" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p>This low session limit is an intentional design choice: TigerBeetle benefits from large batches of requests, and enforcing a smaller number of clients nudges users towards designs which can perform client-side batching efficiently. One imagines a <a href="https://www.pgbouncer.org/">PgBouncer-style</a> proxy might also come in handy here.<a href="#fnref21" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p>This also occurred with <code>get_account_balances</code>, but our test harness didn’t yet cover that API call.<a href="#fnref22" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p>All TigerBeetle assertion failures printed <code>reached unreachable code</code> then exited—there was no error message to tell them apart. Debugging builds offered a stacktrace.<a href="#fnref23" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p>Issue 2745 is in some sense both a safety and liveness issue. Replicas disagree on which blocks are free, violating a key safety property in TigerBeetle’s design: replicas should have identical on-disk state. However, a defensive assertion converts this safety violation to a crash, which prevents users from observing the divergence. In this sense it is a liveness issue, and we report it as such.<a href="#fnref24" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p>For the formal verification enthusiasts in the crowd: yes, recoverable crashes, transient availability issues, and high latency are all technically safety issues, in that they involve <a href="https://www.hillelwayne.com/post/safety-and-liveness">finite counterexamples</a>.<a href="#fnref25" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infomaniak comes out in support of controversial Swiss encryption law (173 pts)]]></title>
            <link>https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law</link>
            <guid>44199377</guid>
            <pubDate>Fri, 06 Jun 2025 10:13:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law">https://www.tomsguide.com/computing/vpns/infomaniak-breaks-rank-and-comes-out-in-support-of-controversial-swiss-encryption-law</a>, See on <a href="https://news.ycombinator.com/item?id=44199377">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-widget-type="contentparsed" id="content">
<section>
<div>
<div>
<picture data-new-v2-image="true">
<source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1280-80.jpg.webp 1920w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1200-80.jpg.webp 1200w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-320-80.jpg.webp 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)">
<img src="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" alt="Swiss flag, red with white cross, flying in front of a building on sunny day" srcset="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1280-80.jpg 1920w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1200-80.jpg 1200w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem-320-80.jpg 320w" sizes="(min-width: 1000px) 600px, calc(100vw - 40px)" data-original-mos="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/xAw9pt6qKPwTyceQerobem.jpg" data-pin-nopin="true" fetchpriority="high">
</picture>
</div>
<figcaption>
<span>(Image credit: Sunphol Sorakul / Getty Images)</span>
</figcaption>
</div>

<div id="article-body">
<p>In Switzerland, some of the <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/best-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/best-vpn">best VPNs</a> are in the firing line as a result of the country's <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/proposed-swiss-encryption-laws-may-have-a-severe-impact-on-vpns-what-you-need-to-know" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/proposed-swiss-encryption-laws-may-have-a-severe-impact-on-vpns-what-you-need-to-know">proposed changes to encryption laws</a>.</p><p>The law's revision would extend surveillance obligations and require companies to collect information and identification on their users – a move that would significantly impact online privacy.</p><p>Swiss-based VPNs <a data-analytics-id="inline-link" href="https://www.tomsguide.com/reviews/protonvpn-review" data-before-rewrite-localise="https://www.tomsguide.com/reviews/protonvpn-review">Proton VPN</a> and <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/introducing-nymvpn-could-this-be-the-worlds-most-secure-vpn" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/introducing-nymvpn-could-this-be-the-worlds-most-secure-vpn">NymVPN</a> would be affected, and Proton CEO <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/proton-vpn-boss-compares-switzerland-to-russia-and-claims-it-could-leave-the-country-over-proposed-law" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/proton-vpn-boss-compares-switzerland-to-russia-and-claims-it-could-leave-the-country-over-proposed-law">Andy Yen said the privacy-focused company would rather leave its Swiss base</a> than risk the privacy of its users.</p><p>The <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/most-private-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/most-private-vpn">most private VPNs</a> uphold strict <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/why-windscribes-court-case-proves-how-important-vpn-no-logging-policies-are" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/why-windscribes-court-case-proves-how-important-vpn-no-logging-policies-are">no-logs policies</a> and collect very little information about users. This law would see these policies undermined.</p><p>Despite widespread opposition from across the country, Swiss cloud security company Infomaniak is supporting the law.</p><p>Infomaniak describes itself as an "ethical cloud" company and one that doesn't compromise on "ecology, privacy, or people." It's surprising, then, that they are seemingly the only privacy-focused company in Switzerland supporting the law change.</p><h2 id="infomaniak-opposes-anonymity-3">Infomaniak opposes anonymity</h2><p>In a <a data-analytics-id="inline-link" href="https://www.rts.ch/play/tv/les-beaux-parleurs/video/faire-bonne-figure?urn=urn:rts:video:f357b48c-e80a-3330-b5c6-972e254d401f" target="_blank" rel="nofollow" data-url="https://www.rts.ch/play/tv/les-beaux-parleurs/video/faire-bonne-figure?urn=urn:rts:video:f357b48c-e80a-3330-b5c6-972e254d401f" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">debate on Radio Télévision Suisse</a> (RTS), and <a data-analytics-id="inline-link" href="https://www.clubic.com/actualite-566385-infomaniak-tacle-proton-et-introduira-le-chiffrement-des-mails-sans-anonymat.html" target="_blank" rel="nofollow" data-url="https://www.clubic.com/actualite-566385-infomaniak-tacle-proton-et-introduira-le-chiffrement-des-mails-sans-anonymat.html" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">reported in Clubic</a>, Infomaniak spokesperson Thomas Jacobsen addressed Andy Yen's comments on the law.</p><p>Jacobsen believed Yen showed a "lack of knowledge of Swiss political institutions" and called for finding the right balance, not looking for extremes.</p><p>Infomaniak argued that anonymity prevents justice, saying there must be a "happy medium" to prevent the digital landscape becoming a "Wild West."</p><p>Proton was cited as a company that advocates for anonymity, but this isn't technically the case. Proton, and Proton VPN, advocates for privacy – and there is a subtle but important difference between the two.</p><p>Confusing privacy and anonymity is common – a Tom's Guide VPN survey found that <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/we-surveyed-toms-guide-readers-about-vpns-and-i-need-to-bust-some-myths" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/we-surveyed-toms-guide-readers-about-vpns-and-i-need-to-bust-some-myths">29% of readers think VPNs make you anonymous</a> – but they don't mean the same thing. Anonymity is when your identity isn't known and no trace of your activity is left behind, with the Tor Network being an example.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg" alt="Shadowy hooded figure stood crossed armed in front of red and dark blue background" srcset="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/2kLGpu5zaMJ9qcC8sDqNhk.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: urbazon / Getty Images)</span></figcaption></figure><p>VPNs protect your data and your privacy. Many can still see some identifiable information and most don't claim to offer anonymity. Although your data is encrypted and reputable VPN providers can't see your internet activity, they can still see your connecting IP address and your payment information. The key point is that they never log or share it.</p><p>Hackers, third-parties, or your ISP can't see what you're doing, and that is the privacy VPNs offer. Infomaniak is incorrect in saying Proton advocates for anonymity.</p><h2 id="calling-out-free-vpns-3">Calling out free VPNs</h2><p>Infomaniak also took issue with free services, such as free VPNs. In the debate, Jacobsen said how these free services allow anyone to hide from the law by enabling anonymity.</p><p>While VPNs can be misused by bad actors for criminal endeavours, something all reputable VPNs and Tom's Guide opposes, this doesn't mean they should be taken away or targeted.</p><p>Almost every kind of technology and device can be used for illicit purposes. We have to accept that not everything can, or should, be controlled in order to target a small minority.</p><p>This trade-off would take away the right to privacy of millions of genuine users.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg" alt="Broken speech bubble on red background" srcset="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/atrrkzhMVawuwLjXF7bHVL.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: rob dobi / Getty Images)</span></figcaption></figure><p>VPNs, and especially the <a data-analytics-id="inline-link" href="https://www.tomsguide.com/best-picks/best-free-vpn" data-before-rewrite-localise="https://www.tomsguide.com/best-picks/best-free-vpn">best free VPNs</a>, are a lifeline for people living under censorship and internet restrictions. Without them, they would be unable to access a free and open internet and would suffer at the hands of authoritarian regimes.</p><p>Proton VPN has a host of <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/what-are-anti-censorship-features-and-how-is-proton-vpn-leading-the-way" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/what-are-anti-censorship-features-and-how-is-proton-vpn-leading-the-way">dedicated anti-censorship features</a>, aimed at protecting the privacy of those who need it most – including the free service <a data-analytics-id="inline-link" href="https://www.tomsguide.com/reviews/proton-vpn-free-review" data-before-rewrite-localise="https://www.tomsguide.com/reviews/proton-vpn-free-review">Proton VPN Free</a>. Many VPNs also offer <a data-analytics-id="inline-link" href="https://www.tomsguide.com/computing/vpns/world-press-freedom-day-2025-how-vpns-are-helping-fight-for-a-free-and-open-internet" data-before-rewrite-localise="https://www.tomsguide.com/computing/vpns/world-press-freedom-day-2025-how-vpns-are-helping-fight-for-a-free-and-open-internet">free emergency VPNs</a> for journalists or activists.</p><p>VPNs cannot just be for those who can afford them, so Infomaniak's targeting of free privacy services fails to consider the appropriate repercussions.</p><p>The article quotes Infomaniak's founder, Boris Siegenthaler, as saying "the answer is clear: the day activists for important climate, humanitarian, or democratic causes are in the crosshairs, we will oppose this request."</p><p>However, many argue that the Swiss government's request <em>would</em> put those people in the crosshairs, and they wouldn't be protected.</p><p>Infomaniak doesn't advocate for widespread surveillance, but that would not be needed under these plans. Metadata collection could form a large part of the new surveillance law, and it's seemingly something Infomaniak supports.</p><p>In a separate <a data-analytics-id="inline-link" href="https://www.rts.ch/play/tv/forum/video/succes-dinfomaniak-le-fournisseur-suisse-de-services-cloud-interview-de-thomas-jacobsen?urn=urn:rts:video:1e5a3af0-bb0b-3ba2-9d56-34842591ee9c" target="_blank" rel="nofollow" data-url="https://www.rts.ch/play/tv/forum/video/succes-dinfomaniak-le-fournisseur-suisse-de-services-cloud-interview-de-thomas-jacobsen?urn=urn:rts:video:1e5a3af0-bb0b-3ba2-9d56-34842591ee9c" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">interview with RTS</a>, Jacobsen argues that metadata collection is acceptable in order to help prosecute individuals who "carry out illicit activities" anonymously. He says how in other aspects of life, we don't accept that, saying you need ID for taking out a phone number and SIM card.</p><p>"The outside of the package is enough to bring justice," he said – referring to metadata. The contents of messages or communications will remain encrypted, but the metadata will be seen and collected.</p><p>Metadata can include geolocation, date and time, IP addresses, file size, device identifiers, plus who sent and received the message. So, even though the actual content of the message remains encrypted and hidden, you can identify and subsequently prosecute individuals based on analysis of metadata.</p><p>The opposition claims this is a fundamental privacy risk if handled in the wrong way, and something that should be opposed, not lauded.</p><figure data-bordeaux-image-check=""><div><p><picture><source type="image/webp" srcset="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-320-80.jpg.webp 320w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-480-80.jpg.webp 480w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-650-80.jpg.webp 650w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-970-80.jpg.webp 970w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1024-80.jpg.webp 1024w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1200-80.jpg.webp 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)"><img src="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg" alt="Floating eyeballs watching a red laptop" srcset="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-320-80.jpg 320w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-480-80.jpg 480w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-650-80.jpg 650w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-970-80.jpg 970w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1024-80.jpg 1024w, https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33-1200-80.jpg 1200w" sizes="(min-width: 1000px) 970px, calc(100vw - 40px)" loading="lazy" data-original-mos="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg" data-pin-media="https://cdn.mos.cms.futurecdn.net/vHTTmCZU5J94BTnGKziW33.jpg"></picture></p></div><figcaption itemprop="caption description"><span itemprop="copyrightHolder">(Image credit: J Studios / Getty Images)</span></figcaption></figure><p>Infomaniak's approach to metadata has also received backlash from others in the industry. A <a data-analytics-id="inline-link" href="https://www.linkedin.com/posts/boris-siegenthaler-7808431_infomaniak-tacle-proton-et-introduira-le-activity-7331320049992888320-ha3X/?rcm=ACoAACTv6S0B-_lKpcaXH3R2J0865yGHmmFsV20" target="_blank" rel="nofollow" data-url="https://www.linkedin.com/posts/boris-siegenthaler-7808431_infomaniak-tacle-proton-et-introduira-le-activity-7331320049992888320-ha3X/?rcm=ACoAACTv6S0B-_lKpcaXH3R2J0865yGHmmFsV20" referrerpolicy="no-referrer-when-downgrade" data-hl-processed="none">LinkedIn post by founder Boris Siegenthaler</a> saw disagreement with Infomaniak's position in the comments.</p><p>A journalist said he was concerned about metadata, should it be collected, falling into the wrong hands. He claimed certain people and sources would be at risk and communication methods must preserve their safety.</p><p>One comment argued that "metadata protection is important to avoid profiling," and another said a middle ground between Proton and Infomaniak's position was needed.</p><p>Infomanaik has said it's moving to an encrypted email service. Jacobsen said the content of emails will be protected, "but without anonymity." Infomaniak's own service would therefore appear to be affected by the law change and they'd be required to collect and store the metadata of its users and their emails.</p><p>The Swiss government's consultation on the proposed law change ended on May 6 2025. Its findings are still not known, but we will monitor its progress closely.</p><p><em>Quotes in this article have been translated from French to English.</em></p><div id="slice-container-freeText-vcmpQU2qC7th8tP5LMQF6K-6O8wESLvgW7uppVg8yrcy8O70avqNoWI"><p>We test and review VPN services in the context of legal recreational uses. For example: 1. Accessing a service from another country (subject to the terms and conditions of that service). 2. Protecting your online security and strengthening your online privacy when abroad. We do not support or condone the illegal or malicious use of VPN services. Consuming pirated content that is paid-for is neither endorsed nor approved by Future Publishing.</p></div>
</div>



<!-- Drop in a standard article here maybe? -->

<div id="slice-container-authorBio-vcmpQU2qC7th8tP5LMQF6K"><p>George is a Staff Writer at Tom's Guide, covering&nbsp;VPN, privacy, and cybersecurity news. He is especially interested in digital rights and censorship, and its interplay with politics. Outside of work, George is passionate about music, Star Wars, and Karate.</p></div>
</section>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Czech Republic: Petition for open source in public administration (126 pts)]]></title>
            <link>https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave</link>
            <guid>44199299</guid>
            <pubDate>Fri, 06 Jun 2025 09:57:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave">https://portal.gov.cz/e-petice/1205-petice-za-povinne-zverejneni-zdrojovych-kodu-softwaru-pouzitych-ve-verejne-sprave</a>, See on <a href="https://news.ycombinator.com/item?id=44199299">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <section>
                <div>
                    <h3>Informace</h3>

                    <ul>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/osobni-udaje-a-cookies">Zpracování osobních údajů a cookies</a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/pro-media">Kontakty pro média</a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/mapa-webu">Mapa webu </a>
                        </li>
                        
                        <li>
                            <a href="https://portal.gov.cz/tiraz/prohlaseni-o-pristupnosti">Prohlášení o přístupnosti</a>
                        </li>
                        
                        <li>
                            <a href="https://postaticfiles.z6.web.core.windows.net/PVS_prirucka_uzivatel.pdf" target="_blank">Uživatelská příručka</a>
                        </li>
                        
                    </ul>
                </div>
                <div>
                    <h3>Máte dotaz? Napište nám</h3>

                    <ul>
                        <li>
                            <a href="mailto:portalobcana@dia.gov.cz">portalobcana@dia.gov.cz</a>
                            <a title="Zkopírovat do schránky" onclick="navigator.clipboard.writeText('portalobcana@dia.gov.cz')">
                                ⧉
                            </a>
                        </li>
                    </ul>

                    <h3>Sledujte český eGovernment</h3>
                    <ul>
                        <li>
                            <a href="https://www.facebook.com/cz.eGovernment/" onclick="window.open(this.href); return false">
                                <span></span>
                            </a>
                        </li>
                        <li>
                            <a href="https://x.com/gov_cz" onclick="window.open(this.href); return false">
                                <svg style="vertical-align: middle" width="23.25" height="21.25" viewBox="0 0 1200 1227" fill="none" xmlns="http://www.w3.org/2000/svg">
                                    <path d="M714.163 519.284L1160.89 0H1055.03L667.137 450.887L357.328 0H0L468.492 681.821L0 1226.37H105.866L515.491 750.218L842.672 1226.37H1200L714.137 519.284H714.163ZM569.165 687.828L521.697 619.934L144.011 79.6944H306.615L611.412 515.685L658.88 583.579L1055.08 1150.3H892.476L569.165 687.854V687.828Z" fill="white"></path>
                                </svg>
                            </a>
                        </li>
                        <li>
                            <a href="https://www.youtube.com/channel/UCCEISjcwdeqghfa4Z2mWMOg" onclick="window.open(this.href); return false">
                                <span></span>
                            </a>
                        </li>
                    </ul>

                </div>
                <div>
                    <h3>Portál veřejné správy vám přináší</h3>
                    <ul>
                        <li>
                            <a href="https://www.dia.gov.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-dia.svg" alt="Logo DIA">
                            </a>
                        </li>
                        <li>
                            <a href="https://nakit.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-nakit.svg" alt="Logo NAKIT">
                            </a>
                        </li>
                    </ul>
                    <br>
                    <ul>
                        <li>
                            <a href="https://european-union.europa.eu/live-work-study/funding-grants-subsidies_cs" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-eu.svg" alt="Logo NextGeneration EU">
                            </a>
                        </li>
                        <li>
                            <a href="https://www.planobnovycr.cz/" onclick="window.open(this.href); return false">
                                <img src="https://portal.gov.cz/static/images/logo-npo.svg" width="120" alt="Logo Národní plán obnovy">
                            </a>
                        </li>
                    </ul>

                </div>
            </section>

            <hr>

        <section>
            <p>
                2025 © Digitální a informační agentura • Informace jsou poskytovány v&nbsp;souladu se zákonem č.&nbsp;106/1999 Sb., o&nbsp;svobodném přístupu k&nbsp;informacím.
            </p>

            <p>
                Verze 4.2.200
            </p>
        </section>

        <!-- Pixel: "egov all" -->



        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Self-hosting your own media considered harmful according to YouTube (1137 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful</link>
            <guid>44197932</guid>
            <pubDate>Fri, 06 Jun 2025 04:59:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful">https://www.jeffgeerling.com/blog/2025/self-hosting-your-own-media-considered-harmful</a>, See on <a href="https://news.ycombinator.com/item?id=44197932">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I just received my <a href="https://github.com/geerlingguy/youtube/issues/12">second community guidelines violation</a> for my video demonstrating the use of LibreELEC on a Raspberry Pi 5, for 4K video playback.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/community-guidelines-strike.jpg" width="700" height="469" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-876a116a-6da8-4656-bf14-863c5958ed02" data-insert-attach="{&quot;id&quot;:&quot;876a116a-6da8-4656-bf14-863c5958ed02&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Community Guidelines Strike - YouTube"></p>

<p>I purposefully avoid demonstrating any of the tools (with a suffix that rhymes with "car") that are popularly used to circumvent purchasing movie, TV, and other media content, or any tools that automatically slurp up YouTube content.</p>

<p>In fact, in my own house, for multiple decades, I've purchased physical media (CDs, DVDs, and more recently, Blu-Rays), and only have legally-acquired content on my NAS. Streaming services used to be a panacea but are now fragmented and mostly full of garbage—and lots of ads. We just wanted to be able to watch TV shows and movies without hassle (and I'm happy to pay for physical media that I want to watch).</p>

<p>But this morning, as I was finishing up work on a video about a new mini Pi cluster, I got a cheerful email from YouTube saying my video on LibreELEC on the Pi 5 was removed because it promoted:</p>

<blockquote>
  <p><strong>Dangerous or Harmful Content</strong><br>
  Content that describes how to get unauthorized or free access to audio or audiovisual content, software, subscription services, or games that usually require payment isn't allowed on YouTube.</p>
</blockquote>

<p>I never described any of that stuff, only how to self-host your own media library.</p>

<p>This wasn't my first rodeo—in October last year, I got a <a href="https://github.com/geerlingguy/youtube/issues/13">strike for showing people how to install Jellyfin</a>!</p>

<p>In <em>that</em> case, I was happy to see my appeal granted within an hour of the strike being placed on the channel. (Nevermind the fact the video had been live for <em>over two years</em> at that point, with nary a problem!)</p>

<p>So I thought, this case will be similar:</p>

<ul>
<li>The video's been up for over a year, without issue</li>
<li>The video's had over half a million views</li>
<li>The video doesn't promote or highlight any tools used to circumvent copyright, get around paid subscriptions, or reproduce any content illegally</li>
</ul>

<p>Slam-dunk, right? Well, not according to whomever reviewed my appeal. Apparently self-hosted open source media library management is harmful.</p>

<p>Who knew open source software could be so <em>subversive</em>?</p>

<h2>The video</h2>

<p>So along that theme, I've re-uploaded the video to Internet Archive, free for anyone to download and view at their leisure.</p>

<p><em>Yes, even those rebels running LibreELEC on their Raspberry Pis!</em></p>

<p>Here it is: <a href="https://archive.org/details/libreelec-raspberry-pi-5">LibreELEC on the Raspberry Pi 5 - Internet Archive</a>.</p>

<p><a href="https://archive.org/details/libreelec-raspberry-pi-5"><img src="https://www.jeffgeerling.com/sites/default/files/images/jeff-geerling-video-libreelec-pi-5.jpg" width="400" height="auto" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-906ddc3d-671e-4d19-9597-36ab08016d49" data-insert-attach="{&quot;id&quot;:&quot;906ddc3d-671e-4d19-9597-36ab08016d49&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="LibreELEC on Pi 5 video thumbnail with play button"></a></p>

<p>I've also uploaded it <a href="https://www.floatplane.com/post/bNx4Mhzqvu">on Floatplane</a>, for subscribers.</p>

<h2>Alternatives</h2>

<p>I've been slowly uploading my back catalog to <a href="https://www.floatplane.com/channel/JeffGeerling/home">my channel on Floatplane</a>, though not all my content is there yet.</p>

<p>Some in the fediverse ask why I'm not on Peertube. Here's the problem (and it's not insurmountable): <em>right now</em>, there's no easy path towards sustainable content production when the audience for the content is 100x smaller, and the number of patrons/sponsors remains proportionally the same.</p>

<p>I was never able to sustain my open source work based on patronage, and content production is the same—just more expensive to maintain to any standard (each video takes between 10-300 hours to produce, and I have a family to feed, and <a href="https://www.jeffgeerling.com/tags/crohns">US health insurance companies to fund</a>).</p>

<p>YouTube was, and still is, a creative anomaly. I'm hugely thankful to my <a href="https://www.patreon.com/c/geerlingguy">Patreon</a>, <a href="https://github.com/sponsors/geerlingguy">GitHub</a>, and <a href="https://www.floatplane.com/channel/JeffGeerling/home">Floatplane</a> supporters—and I hope to have direct funding fully able to support my work someday. But until that time, YouTube's AdSense revenue and vast reach is a kind of 'golden handcuff.'</p>

<p>The handcuff has been a bit tarnished of late, however, with Google recently adding AI summaries to videos—which <em>seems</em> to indicate maybe <a href="https://www.msn.com/en-us/news/technology/google-gemini-s-ai-video-summary-implies-youtube-doesn-t-care-about-content-creators/ar-AA1rMsoy">Gemini is slurping up my content and using it in their AI models</a>?</p>

<p>Maybe the handcuffs are fools-gold, and I just don't see it yet.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Test Postgres in Python Like SQLite (128 pts)]]></title>
            <link>https://github.com/wey-gu/py-pglite</link>
            <guid>44196945</guid>
            <pubDate>Fri, 06 Jun 2025 00:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/wey-gu/py-pglite">https://github.com/wey-gu/py-pglite</a>, See on <a href="https://news.ycombinator.com/item?id=44196945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">py-pglite</h2><a id="user-content-py-pglite" aria-label="Permalink: py-pglite" href="#py-pglite"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/1651790/451981298-3c6ef886-5075-4d82-a180-a6b1dafe792b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkyMDk3MDIsIm5iZiI6MTc0OTIwOTQwMiwicGF0aCI6Ii8xNjUxNzkwLzQ1MTk4MTI5OC0zYzZlZjg4Ni01MDc1LTRkODItYTE4MC1hNmIxZGFmZTc5MmIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI0YmFlYjMwNDEwMjQ1YTIxY2YzYmEwOTE2N2EyMjAwZmI1MGU5ZmIxNjU3NDgxNjc0MDZmNDY5N2Y2NmVmMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Y7StF80nv5tw4wN8UZ35RnkDmE65iPkS1MMjCQBh7Nw"><img src="https://private-user-images.githubusercontent.com/1651790/451981298-3c6ef886-5075-4d82-a180-a6b1dafe792b.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDkyMDk3MDIsIm5iZiI6MTc0OTIwOTQwMiwicGF0aCI6Ii8xNjUxNzkwLzQ1MTk4MTI5OC0zYzZlZjg4Ni01MDc1LTRkODItYTE4MC1hNmIxZGFmZTc5MmIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDYwNiUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA2MDZUMTEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmI0YmFlYjMwNDEwMjQ1YTIxY2YzYmEwOTE2N2EyMjAwZmI1MGU5ZmIxNjU3NDgxNjc0MDZmNDY5N2Y2NmVmMiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.Y7StF80nv5tw4wN8UZ35RnkDmE65iPkS1MMjCQBh7Nw" alt="image"></a></p>
<p dir="auto"><a href="https://github.com/wey-gu/py-pglite/actions/workflows/ci.yml"><img src="https://github.com/wey-gu/py-pglite/actions/workflows/ci.yml/badge.svg" alt="CI"></a>
<a href="https://codecov.io/gh/wey-gu/py-pglite" rel="nofollow"><img src="https://camo.githubusercontent.com/137353bbd51cf4cb94d56c7f9288a47c40c9564dd00c5901064fba896672b855/68747470733a2f2f636f6465636f762e696f2f67682f7765792d67752f70792d70676c6974652f6272616e63682f6d61696e2f67726170682f62616467652e7376673f746f6b656e3d594f55525f434f4445434f565f544f4b454e" alt="codecov" data-canonical-src="https://codecov.io/gh/wey-gu/py-pglite/branch/main/graph/badge.svg?token=YOUR_CODECOV_TOKEN"></a>
<a href="https://github.com/astral-sh/ruff"><img src="https://camo.githubusercontent.com/e9e401f08622ada6a3147f8bb1ba4ee849b38c7f123199ce3886ef05658299e6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7374796c652d727566662d626c75653f6c6f676f3d72756666266c6f676f436f6c6f723d7768697465" alt="Ruff" data-canonical-src="https://img.shields.io/badge/style-ruff-blue?logo=ruff&amp;logoColor=white"></a>
<a href="https://mypy.readthedocs.io/en/stable/introduction.html" rel="nofollow"><img src="https://camo.githubusercontent.com/c85115d7ffc8f4ff2c49c03cb9d2973ddc81a476bb4c06c9427848f918916f03/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f747970655f636865636b65642d6d7970792d696e666f726d6174696f6e616c2e737667" alt="MyPy" data-canonical-src="https://img.shields.io/badge/type_checked-mypy-informational.svg"></a>
<a href="https://badge.fury.io/py/py-pglite" rel="nofollow"><img src="https://camo.githubusercontent.com/6d08914750fc8f1671c104bb431d8bd17d8b46cb54d00364ca0e7b7365267ef7/68747470733a2f2f62616467652e667572792e696f2f70792f70792d70676c6974652e737667" alt="PyPI version" data-canonical-src="https://badge.fury.io/py/py-pglite.svg"></a>
<a href="https://pypi.org/project/py-pglite/" rel="nofollow"><img src="https://camo.githubusercontent.com/337ddbbe5fa88fd4643f6421c27687d2d1c74a97536f3b0b12a872bdcbca29d2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f70792d70676c6974652e737667" alt="Python" data-canonical-src="https://img.shields.io/pypi/pyversions/py-pglite.svg"></a>
<a href="https://github.com/wey-gu/py-pglite/blob/main/LICENSE"><img src="https://camo.githubusercontent.com/d3171074e95a4a5adb658c6465e6162b2c23a96a3a1408a73b0a5fb93d1db34d/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f70792d70676c6974652e737667" alt="License" data-canonical-src="https://img.shields.io/pypi/l/py-pglite.svg"></a></p>
<p dir="auto">A Python testing library that provides seamless integration between <a href="https://github.com/electric-sql/pglite">PGlite</a> and Python test suites. Get the full power of PostgreSQL in your tests without the overhead of a full PostgreSQL installation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🎯 Why py-pglite?</h2><a id="user-content--why-py-pglite" aria-label="Permalink: 🎯 Why py-pglite?" href="#-why-py-pglite"></a></p>
<ul dir="auto">
<li><strong>⚡ Blazing Fast</strong>: In-memory PostgreSQL for ultra-quick test runs</li>
<li><strong>🛠️ Effortless Setup</strong>: No PostgreSQL install needed—just Node.js(I know)!</li>
<li><strong>🐍 Pythonic</strong>: Native support for SQLAlchemy &amp; SQLModel in your tests</li>
<li><strong>🧊 Fully Isolated</strong>: Every test module gets its own fresh database</li>
<li><strong>🦾 100% Compatible</strong>: True PostgreSQL features via <a href="https://pglite.dev/" rel="nofollow">PGlite</a></li>
<li><strong>🧩 Pytest Plug-and-Play</strong>: Ready-to-use fixtures for instant productivity</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">📦 Installation</h2><a id="user-content--installation" aria-label="Permalink: 📦 Installation" href="#-installation"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Installation</h3><a id="user-content-basic-installation" aria-label="Permalink: Basic Installation" href="#basic-installation"></a></p>

<p dir="auto"><h3 tabindex="-1" dir="auto">With Optional Dependencies</h3><a id="user-content-with-optional-dependencies" aria-label="Permalink: With Optional Dependencies" href="#with-optional-dependencies"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# For SQLModel support
pip install &quot;py-pglite[sqlmodel]&quot;

# For FastAPI integration
pip install &quot;py-pglite[fastapi]&quot;

# For development
pip install &quot;py-pglite[dev]&quot;"><pre><span><span>#</span> For SQLModel support</span>
pip install <span><span>"</span>py-pglite[sqlmodel]<span>"</span></span>

<span><span>#</span> For FastAPI integration</span>
pip install <span><span>"</span>py-pglite[fastapi]<span>"</span></span>

<span><span>#</span> For development</span>
pip install <span><span>"</span>py-pglite[dev]<span>"</span></span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Requirements</h3><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><strong>Python</strong>: 3.10+</li>
<li><strong>Node.js</strong>: 18+ (for PGlite)</li>
<li><strong>SQLAlchemy</strong>: 2.0+</li>
</ul>
<p dir="auto">The library automatically manages PGlite npm dependencies.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🚀 Quick Start</h2><a id="user-content--quick-start" aria-label="Permalink: 🚀 Quick Start" href="#-quick-start"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Usage with Pytest</h3><a id="user-content-basic-usage-with-pytest" aria-label="Permalink: Basic Usage with Pytest" href="#basic-usage-with-pytest"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import pytest
from sqlmodel import Session, SQLModel, Field, select
from py_pglite import pglite_session

# Your models
class User(SQLModel, table=True):
    id: int | None = Field(default=None, primary_key=True)
    name: str
    email: str

# Test with automatic PGlite management
def test_user_creation(pglite_session: Session):
    user = User(name=&quot;Alice&quot;, email=&quot;alice@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    
    # Query back
    users = pglite_session.exec(select(User)).all()
    assert len(users) == 1
    assert users[0].name == &quot;Alice&quot;"><pre><span>import</span> <span>pytest</span>
<span>from</span> <span>sqlmodel</span> <span>import</span> <span>Session</span>, <span>SQLModel</span>, <span>Field</span>, <span>select</span>
<span>from</span> <span>py_pglite</span> <span>import</span> <span>pglite_session</span>

<span># Your models</span>
<span>class</span> <span>User</span>(<span>SQLModel</span>, <span>table</span><span>=</span><span>True</span>):
    <span>id</span>: <span>int</span> <span>|</span> <span>None</span> <span>=</span> <span>Field</span>(<span>default</span><span>=</span><span>None</span>, <span>primary_key</span><span>=</span><span>True</span>)
    <span>name</span>: <span>str</span>
    <span>email</span>: <span>str</span>

<span># Test with automatic PGlite management</span>
<span>def</span> <span>test_user_creation</span>(<span>pglite_session</span>: <span>Session</span>):
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Alice"</span>, <span>email</span><span>=</span><span>"alice@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Query back</span>
    <span>users</span> <span>=</span> <span>pglite_session</span>.<span>exec</span>(<span>select</span>(<span>User</span>)).<span>all</span>()
    <span>assert</span> <span>len</span>(<span>users</span>) <span>==</span> <span>1</span>
    <span>assert</span> <span>users</span>[<span>0</span>].<span>name</span> <span>==</span> <span>"Alice"</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Manual Management</h3><a id="user-content-manual-management" aria-label="Permalink: Manual Management" href="#manual-management"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import PGliteManager, PGliteConfig

# Custom configuration
config = PGliteConfig(
    timeout=30,
    cleanup_on_exit=True,
    log_level=&quot;DEBUG&quot;
)

# Manual management
with PGliteManager(config) as manager:
    engine = manager.get_engine()
    SQLModel.metadata.create_all(engine)
    
    with Session(engine) as session:
        # Your database operations here
        pass"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>PGliteManager</span>, <span>PGliteConfig</span>

<span># Custom configuration</span>
<span>config</span> <span>=</span> <span>PGliteConfig</span>(
    <span>timeout</span><span>=</span><span>30</span>,
    <span>cleanup_on_exit</span><span>=</span><span>True</span>,
    <span>log_level</span><span>=</span><span>"DEBUG"</span>
)

<span># Manual management</span>
<span>with</span> <span>PGliteManager</span>(<span>config</span>) <span>as</span> <span>manager</span>:
    <span>engine</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()
    <span>SQLModel</span>.<span>metadata</span>.<span>create_all</span>(<span>engine</span>)
    
    <span>with</span> <span>Session</span>(<span>engine</span>) <span>as</span> <span>session</span>:
        <span># Your database operations here</span>
        <span>pass</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔧 Features</h2><a id="user-content--features" aria-label="Permalink: 🔧 Features" href="#-features"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Pytest Fixtures</h3><a id="user-content-pytest-fixtures" aria-label="Permalink: Pytest Fixtures" href="#pytest-fixtures"></a></p>
<ul dir="auto">
<li><strong><code>pglite_engine</code></strong>: SQLAlchemy engine connected to PGlite</li>
<li><strong><code>pglite_session</code></strong>: Database session with automatic cleanup</li>
<li><strong><code>pglite_manager</code></strong>: Direct access to PGlite process management</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Automatic Management</h3><a id="user-content-automatic-management" aria-label="Permalink: Automatic Management" href="#automatic-management"></a></p>
<ul dir="auto">
<li>✅ Process lifecycle management</li>
<li>✅ Socket cleanup and health checks</li>
<li>✅ Graceful shutdown and error handling</li>
<li>✅ Per-test isolation with automatic cleanup</li>
<li>✅ Node.js dependency management</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import PGliteConfig

config = PGliteConfig(
    timeout=30,               # Startup timeout in seconds
    cleanup_on_exit=True,     # Auto cleanup on exit
    log_level=&quot;INFO&quot;,         # Logging level (DEBUG/INFO/WARNING/ERROR)
    socket_path=&quot;/tmp/.s.PGSQL.5432&quot;,  # Custom socket path
    work_dir=None,            # Working directory (None = temp dir)
    node_modules_check=True,  # Verify node_modules exists
    auto_install_deps=True,   # Auto-install npm dependencies
)"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>PGliteConfig</span>

<span>config</span> <span>=</span> <span>PGliteConfig</span>(
    <span>timeout</span><span>=</span><span>30</span>,               <span># Startup timeout in seconds</span>
    <span>cleanup_on_exit</span><span>=</span><span>True</span>,     <span># Auto cleanup on exit</span>
    <span>log_level</span><span>=</span><span>"INFO"</span>,         <span># Logging level (DEBUG/INFO/WARNING/ERROR)</span>
    <span>socket_path</span><span>=</span><span>"/tmp/.s.PGSQL.5432"</span>,  <span># Custom socket path</span>
    <span>work_dir</span><span>=</span><span>None</span>,            <span># Working directory (None = temp dir)</span>
    <span>node_modules_check</span><span>=</span><span>True</span>,  <span># Verify node_modules exists</span>
    <span>auto_install_deps</span><span>=</span><span>True</span>,   <span># Auto-install npm dependencies</span>
)</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Utility Functions</h3><a id="user-content-utility-functions" aria-label="Permalink: Utility Functions" href="#utility-functions"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from py_pglite import utils

# Database cleanup utilities
utils.clean_database_data(engine)                    # Clean all data
utils.clean_database_data(engine, exclude_tables=[&quot;users&quot;])  # Exclude tables
utils.reset_sequences(engine)                        # Reset auto-increment sequences
utils.verify_database_empty(engine)                  # Check if database is empty

# Schema operations
utils.create_test_schema(engine, &quot;test_schema&quot;)      # Create test schema
utils.drop_test_schema(engine, &quot;test_schema&quot;)        # Drop test schema

# Get table statistics
row_counts = utils.get_table_row_counts(engine)      # Dict of table row counts"><pre><span>from</span> <span>py_pglite</span> <span>import</span> <span>utils</span>

<span># Database cleanup utilities</span>
<span>utils</span>.<span>clean_database_data</span>(<span>engine</span>)                    <span># Clean all data</span>
<span>utils</span>.<span>clean_database_data</span>(<span>engine</span>, <span>exclude_tables</span><span>=</span>[<span>"users"</span>])  <span># Exclude tables</span>
<span>utils</span>.<span>reset_sequences</span>(<span>engine</span>)                        <span># Reset auto-increment sequences</span>
<span>utils</span>.<span>verify_database_empty</span>(<span>engine</span>)                  <span># Check if database is empty</span>

<span># Schema operations</span>
<span>utils</span>.<span>create_test_schema</span>(<span>engine</span>, <span>"test_schema"</span>)      <span># Create test schema</span>
<span>utils</span>.<span>drop_test_schema</span>(<span>engine</span>, <span>"test_schema"</span>)        <span># Drop test schema</span>

<span># Get table statistics</span>
<span>row_counts</span> <span>=</span> <span>utils</span>.<span>get_table_row_counts</span>(<span>engine</span>)      <span># Dict of table row counts</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">📚 Examples</h2><a id="user-content--examples" aria-label="Permalink: 📚 Examples" href="#-examples"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">FastAPI Integration</h3><a id="user-content-fastapi-integration" aria-label="Permalink: FastAPI Integration" href="#fastapi-integration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from fastapi import Depends, FastAPI
from fastapi.testclient import TestClient
from sqlmodel import Session
from py_pglite import pglite_engine

app = FastAPI()

def get_db():
    # Production database dependency
    pass

@app.post(&quot;/users/&quot;)
def create_user(user_data: dict, db: Session = Depends(get_db)):
    # Your endpoint logic
    pass

# Test with PGlite
def test_create_user_endpoint(pglite_engine):
    # Override database dependency
    def override_get_db():
        with Session(pglite_engine) as session:
            yield session
    
    app.dependency_overrides[get_db] = override_get_db
    
    with TestClient(app) as client:
        response = client.post(&quot;/users/&quot;, json={&quot;name&quot;: &quot;Bob&quot;})
        assert response.status_code == 200"><pre><span>from</span> <span>fastapi</span> <span>import</span> <span>Depends</span>, <span>FastAPI</span>
<span>from</span> <span>fastapi</span>.<span>testclient</span> <span>import</span> <span>TestClient</span>
<span>from</span> <span>sqlmodel</span> <span>import</span> <span>Session</span>
<span>from</span> <span>py_pglite</span> <span>import</span> <span>pglite_engine</span>

<span>app</span> <span>=</span> <span>FastAPI</span>()

<span>def</span> <span>get_db</span>():
    <span># Production database dependency</span>
    <span>pass</span>

<span>@<span>app</span>.<span>post</span>(<span>"/users/"</span>)</span>
<span>def</span> <span>create_user</span>(<span>user_data</span>: <span>dict</span>, <span>db</span>: <span>Session</span> <span>=</span> <span>Depends</span>(<span>get_db</span>)):
    <span># Your endpoint logic</span>
    <span>pass</span>

<span># Test with PGlite</span>
<span>def</span> <span>test_create_user_endpoint</span>(<span>pglite_engine</span>):
    <span># Override database dependency</span>
    <span>def</span> <span>override_get_db</span>():
        <span>with</span> <span>Session</span>(<span>pglite_engine</span>) <span>as</span> <span>session</span>:
            <span>yield</span> <span>session</span>
    
    <span>app</span>.<span>dependency_overrides</span>[<span>get_db</span>] <span>=</span> <span>override_get_db</span>
    
    <span>with</span> <span>TestClient</span>(<span>app</span>) <span>as</span> <span>client</span>:
        <span>response</span> <span>=</span> <span>client</span>.<span>post</span>(<span>"/users/"</span>, <span>json</span><span>=</span>{<span>"name"</span>: <span>"Bob"</span>})
        <span>assert</span> <span>response</span>.<span>status_code</span> <span>==</span> <span>200</span></pre></div>
<p dir="auto">See also <a href="https://github.com/wey-gu/py-pglite/blob/main/examples/test_fastapi_auth_example.py">examples/test_fastapi_auth_example.py</a> for an example of how to use py-pglite with FastAPI e2e test that includes authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Complex Testing Scenario</h3><a id="user-content-complex-testing-scenario" aria-label="Permalink: Complex Testing Scenario" href="#complex-testing-scenario"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="def test_complex_operations(pglite_session: Session):
    # Create related data
    user = User(name=&quot;Alice&quot;, email=&quot;alice@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    pglite_session.refresh(user)
    
    # Create dependent records
    orders = [
        Order(user_id=user.id, amount=100.0),
        Order(user_id=user.id, amount=250.0),
    ]
    pglite_session.add_all(orders)
    pglite_session.commit()
    
    # Complex query with joins
    result = pglite_session.exec(
        select(User.name, func.sum(Order.amount))
        .join(Order)
        .group_by(User.name)
    ).first()
    
    assert result[0] == &quot;Alice&quot;
    assert result[1] == 350.0"><pre><span>def</span> <span>test_complex_operations</span>(<span>pglite_session</span>: <span>Session</span>):
    <span># Create related data</span>
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Alice"</span>, <span>email</span><span>=</span><span>"alice@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    <span>pglite_session</span>.<span>refresh</span>(<span>user</span>)
    
    <span># Create dependent records</span>
    <span>orders</span> <span>=</span> [
        <span>Order</span>(<span>user_id</span><span>=</span><span>user</span>.<span>id</span>, <span>amount</span><span>=</span><span>100.0</span>),
        <span>Order</span>(<span>user_id</span><span>=</span><span>user</span>.<span>id</span>, <span>amount</span><span>=</span><span>250.0</span>),
    ]
    <span>pglite_session</span>.<span>add_all</span>(<span>orders</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Complex query with joins</span>
    <span>result</span> <span>=</span> <span>pglite_session</span>.<span>exec</span>(
        <span>select</span>(<span>User</span>.<span>name</span>, <span>func</span>.<span>sum</span>(<span>Order</span>.<span>amount</span>))
        .<span>join</span>(<span>Order</span>)
        .<span>group_by</span>(<span>User</span>.<span>name</span>)
    ).<span>first</span>()
    
    <span>assert</span> <span>result</span>[<span>0</span>] <span>==</span> <span>"Alice"</span>
    <span>assert</span> <span>result</span>[<span>1</span>] <span>==</span> <span>350.0</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">🤝 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 🤝 Contributing" href="#-contributing"></a></p>
<p dir="auto">Contributions welcome! Please read our <a href="https://github.com/wey-gu/py-pglite/blob/main/CONTRIBUTING.md">Contributing Guide</a>.</p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create a feature branch</li>
<li>Make your changes</li>
<li>Add tests for new functionality</li>
<li>Run the development workflow: <code>python hacking.py</code></li>
<li>Submit a pull request</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">📄 License</h2><a id="user-content--license" aria-label="Permalink: 📄 License" href="#-license"></a></p>
<p dir="auto">Apache 2.0 License - see <a href="https://github.com/wey-gu/py-pglite/blob/main/LICENSE">LICENSE</a> file.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🙏 Acknowledgments</h2><a id="user-content--acknowledgments" aria-label="Permalink: 🙏 Acknowledgments" href="#-acknowledgments"></a></p>
<ul dir="auto">
<li><a href="https://github.com/electric-sql/pglite">PGlite</a> - The amazing in-memory PostgreSQL</li>
<li><a href="https://www.sqlalchemy.org/" rel="nofollow">SQLAlchemy</a> - Python SQL toolkit</li>
<li><a href="https://sqlmodel.tiangolo.com/" rel="nofollow">SQLModel</a> - Modern Python SQL toolkit</li>
<li><a href="https://pytest.org/" rel="nofollow">Pytest</a> - Testing framework</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Best Practices</h2><a id="user-content-best-practices" aria-label="Permalink: Best Practices" href="#best-practices"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Multiple Database Sessions</h3><a id="user-content-multiple-database-sessions" aria-label="Permalink: Multiple Database Sessions" href="#multiple-database-sessions"></a></p>
<p dir="auto">For multiple database connections, use <strong>multiple sessions with the same engine</strong> rather than multiple engines:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# ✅ Recommended: Multiple sessions with same engine
with PGliteManager() as manager:
    engine = manager.get_engine()
    
    # Multiple sessions work perfectly
    session1 = Session(engine)
    session2 = Session(engine)
    session3 = Session(engine)

# ❌ Not recommended: Multiple engines from same manager
with PGliteManager() as manager:
    engine1 = manager.get_engine()  # Can cause connection conflicts
    engine2 = manager.get_engine()  # when used simultaneously"><pre><span># ✅ Recommended: Multiple sessions with same engine</span>
<span>with</span> <span>PGliteManager</span>() <span>as</span> <span>manager</span>:
    <span>engine</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()
    
    <span># Multiple sessions work perfectly</span>
    <span>session1</span> <span>=</span> <span>Session</span>(<span>engine</span>)
    <span>session2</span> <span>=</span> <span>Session</span>(<span>engine</span>)
    <span>session3</span> <span>=</span> <span>Session</span>(<span>engine</span>)

<span># ❌ Not recommended: Multiple engines from same manager</span>
<span>with</span> <span>PGliteManager</span>() <span>as</span> <span>manager</span>:
    <span>engine1</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()  <span># Can cause connection conflicts</span>
    <span>engine2</span> <span>=</span> <span>manager</span>.<span>get_engine</span>()  <span># when used simultaneously</span></pre></div>
<p dir="auto"><strong>Why?</strong> Creating multiple SQLAlchemy engines from the same PGlite manager can cause connection pool conflicts since they all connect to the same Unix socket.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Performance Tips</h3><a id="user-content-performance-tips" aria-label="Permalink: Performance Tips" href="#performance-tips"></a></p>
<ul dir="auto">
<li>Use <code>pglite_session</code> fixture for automatic cleanup between tests</li>
<li>Use <code>pglite_engine</code> fixture when you need direct engine access</li>
<li>Use utility functions for efficient database operations</li>
<li>Consider custom configurations for specific test requirements</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Testing Patterns</h3><a id="user-content-testing-patterns" aria-label="Permalink: Testing Patterns" href="#testing-patterns"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Pattern 1: Simple CRUD testing
def test_user_crud(pglite_session):
    # Create
    user = User(name=&quot;Test&quot;, email=&quot;test@example.com&quot;)
    pglite_session.add(user)
    pglite_session.commit()
    
    # Read
    found_user = pglite_session.get(User, user.id)
    assert found_user.name == &quot;Test&quot;
    
    # Update
    found_user.name = &quot;Updated&quot;
    pglite_session.commit()
    
    # Delete
    pglite_session.delete(found_user)
    pglite_session.commit()

# Pattern 2: Custom cleanup
def test_with_custom_cleanup(pglite_engine):
    SQLModel.metadata.create_all(pglite_engine)
    
    with Session(pglite_engine) as session:
        # Your test logic
        pass
    
    # Custom cleanup if needed
    utils.clean_database_data(pglite_engine)"><pre><span># Pattern 1: Simple CRUD testing</span>
<span>def</span> <span>test_user_crud</span>(<span>pglite_session</span>):
    <span># Create</span>
    <span>user</span> <span>=</span> <span>User</span>(<span>name</span><span>=</span><span>"Test"</span>, <span>email</span><span>=</span><span>"test@example.com"</span>)
    <span>pglite_session</span>.<span>add</span>(<span>user</span>)
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Read</span>
    <span>found_user</span> <span>=</span> <span>pglite_session</span>.<span>get</span>(<span>User</span>, <span>user</span>.<span>id</span>)
    <span>assert</span> <span>found_user</span>.<span>name</span> <span>==</span> <span>"Test"</span>
    
    <span># Update</span>
    <span>found_user</span>.<span>name</span> <span>=</span> <span>"Updated"</span>
    <span>pglite_session</span>.<span>commit</span>()
    
    <span># Delete</span>
    <span>pglite_session</span>.<span>delete</span>(<span>found_user</span>)
    <span>pglite_session</span>.<span>commit</span>()

<span># Pattern 2: Custom cleanup</span>
<span>def</span> <span>test_with_custom_cleanup</span>(<span>pglite_engine</span>):
    <span>SQLModel</span>.<span>metadata</span>.<span>create_all</span>(<span>pglite_engine</span>)
    
    <span>with</span> <span>Session</span>(<span>pglite_engine</span>) <span>as</span> <span>session</span>:
        <span># Your test logic</span>
        <span>pass</span>
    
    <span># Custom cleanup if needed</span>
    <span>utils</span>.<span>clean_database_data</span>(<span>pglite_engine</span>)</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we’re responding to The NYT’s data demands in order to protect user privacy (224 pts)]]></title>
            <link>https://openai.com/index/response-to-nyt-data-demands/</link>
            <guid>44196850</guid>
            <pubDate>Fri, 06 Jun 2025 00:35:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/response-to-nyt-data-demands/">https://openai.com/index/response-to-nyt-data-demands/</a>, See on <a href="https://news.ycombinator.com/item?id=44196850">Hacker News</a></p>
Couldn't get https://openai.com/index/response-to-nyt-data-demands/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Anthropic co-founder on cutting access to Windsurf (105 pts)]]></title>
            <link>https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/</link>
            <guid>44196807</guid>
            <pubDate>Fri, 06 Jun 2025 00:24:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/">https://techcrunch.com/2025/06/05/anthropic-co-founder-on-cutting-access-to-windsurf-it-would-be-odd-for-us-to-sell-claude-to-openai/</a>, See on <a href="https://news.ycombinator.com/item?id=44196807">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">Anthropic co-founder and Chief Science Officer Jared Kaplan said his company cut Windsurf’s direct access to Anthropic’s Claude AI models largely because of rumors and reports that OpenAI, its largest competitor, is acquiring the AI coding assistant.</p>

<p>“We really are just trying to enable our customers who are going to sustainably be working with us in the future,” said Kaplan during an onstage interview Thursday with TechCrunch at <a href="https://techcrunch.com/events/tc-sessions-ai/">TC Sessions: AI 2025</a>.</p>







<p>“I think it would be odd for us to be selling Claude to OpenAI,” Kaplan said.</p>

<p>The comment comes just a few weeks after Bloomberg reported that <a href="https://www.bloomberg.com/news/articles/2025-05-06/openai-reaches-agreement-to-buy-startup-windsurf-for-3-billion" target="_blank" rel="noreferrer noopener nofollow">OpenAI was acquiring Windsurf for $3 billion</a>. Earlier this week, Windsurf said that <a href="https://techcrunch.com/2025/06/03/windsurf-says-anthropic-is-limiting-its-direct-access-to-claude-ai-models/">Anthropic cut its direct access to Claude 3.5 Sonnet and Claude 3.7 Sonnet</a>, two of the more popular AI models for coding, forcing the startup to find third-party computing providers on relatively short notice. Windsurf said it was disappointed in Anthropic’s decision and that it might cause short-term instability for users trying to access Claude via Windsurf.</p>

<p>Windsurf declined to comment on Kaplan’s remarks, and an OpenAI spokesperson did not immediately respond to TechCrunch’s request. The companies have not confirmed the acquisition rumors.</p>

<p>Part of the reason Anthropic cut Windsurf’s access to Claude, according to Kaplan, is because the company is quite computing-constrained today. Anthropic would like to reserve its computing for what Kaplan characterized as “lasting partnerships.”</p>

<p>However, Kaplan said the company hopes to greatly increase the availability of models it can offer users and developers in the coming months. He added that Anthropic has just started to unlock capacity on a new computing cluster from its partner, Amazon, which he says is “really big and continues to scale.”</p>


<p>As Anthropic pulls away from Windsurf, Kaplan said he’s collaborating with other customers building AI coding tools, such as Cursor — a company Kaplan said Anthropic expects to work with for a long time. Kaplan rejected the idea that Anthropic was in competition with companies like Cursor, which is developing its own AI models.</p>

<p>Meanwhile, Kaplan says Anthropic is increasingly focused on developing its own agentic coding products, such as Claude Code, rather than AI chatbot experiences. While companies like OpenAI, Google, and Meta are competing for the most popular AI chatbot platform, Kaplan said the chatbot paradigm was limiting due to its static nature, and that AI agents would in the long run be much more helpful for users.</p>


</div><div>
	
	
	
	

	
<div>
	<p>
		Maxwell Zeff is a senior reporter at TechCrunch specializing in AI and emerging technologies. Previously with Gizmodo, Bloomberg, and MSNBC, Zeff has covered the rise of AI and the Silicon Valley Bank crisis. He is based in San Francisco. When not reporting, he can be found hiking, biking, and exploring the Bay Area’s food scene.	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/maxwell-zeff/" data-event="button" href="https://techcrunch.com/author/maxwell-zeff/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I do not remember my life and it's fine (244 pts)]]></title>
            <link>https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine</link>
            <guid>44196576</guid>
            <pubDate>Thu, 05 Jun 2025 23:26:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine">https://aethermug.com/posts/i-do-not-remember-my-life-and-it-s-fine</a>, See on <a href="https://news.ycombinator.com/item?id=44196576">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p><em>This post is part of the <a href="https://aethermug.com/posts/a-list-of-introspective-descriptions">List of Introspective Descriptions</a>.</em></p>

<p>I've <a href="https://aethermug.com/posts/aphantasia">written</a> <a href="https://aethermug.com/posts/aphantasia-is-no-creativity-killer">about</a> <a href="https://aethermug.com/posts/new-aphantasia-article-on-nautilus">aphantasia</a> <a href="https://aethermug.com/posts/reading-blood-meridian-with-aphantasia">several</a> <a href="https://aethermug.com/posts/normality-and-surprise-in-an-image-free-mind">times</a> <a href="https://aethermug.com/posts/an-aphantasic-s-observations-on-the-imagination-of-shapes">before</a> on this blog, and many people have shown interest in the topic. Most readers are simply curious when I say that I can't form any kind of image, sound, or other sensation in my mind. Occasionally, someone shows pity or commiseration towards me, as if I were in constant, daily suffering from a crippling disability. Nothing could be further from the truth, of course. I've been successful at most of what I've tried to accomplish in my life until now, and never had to battle with a sense of being disadvantaged. On top of that, even aphantasia experts generally agree that <a href="https://onlinelibrary.wiley.com/doi/10.1111/sjop.12887" rel="nofollow noopener noreferrer" target="_blank">it is not a disorder</a>.</p>
<p>That is not to say that I feel as capable as anyone else at everything. In particular, there is an area in which I <em>do</em> feel—all too well—that I am weaker than most: my memory of past episodes.</p>
<p>For obvious reasons, my recollections lack a visual component, but that is only part of the story. I seem to have an extremely poor ability to "relive" past events mentally. In fact, my condition is accurately described as</p>
<blockquote>
<p>a mnemonic syndrome that is confined to an inability to mentally travel backwards in time in the absence of detectable neuropathology or significant daily handicap,</p>
</blockquote>
<p>which is the definition of a trait called SDAM, for <em>Severely Deficient Autobiographical Memory</em>.</p>
<p>SDAM was only <a href="https://www.sciencedirect.com/science/article/pii/S002839321500158X" rel="nofollow noopener noreferrer" target="_blank">discovered</a> in 2015, and it is still poorly understood. Yet there is mounting evidence that it has deep links with aphantasia: about half of the people with SDAM also <a href="https://www.youtube.com/watch?v=Zvam_uoBSLc" rel="nofollow noopener noreferrer" target="_blank">report</a> having aphantasia, and many people with aphantasia claim to have difficulties with recalling past episodes from their own lives. For these reasons, I believe I have SDAM or something closely resembling it.</p>
<p>What does that imply, though? That is what I aim to clarify with this post. I always find it very difficult to tell how much of my subjective experience is rare and how much of it is normal for most of humanity. I've never swapped brains with anyone to find out. The only solution <a href="https://aethermug.com/posts/normality-and-surprise-in-an-image-free-mind">I've found</a> is to try my best at explaining what the inner experience is like for me, and hope to receive comments from readers who have similar—or entirely different—experiences.</p>
<p>Below are some brief observations about the way my episodic memory works, based on notes I took over the past couple of years.</p>
<h2>Recalling Specific Episodes</h2>
<p>When I was looking for my first job, a Japanese company I applied to had me fill in a screening questionnaire. One question was something along these lines: "Write about a time during your university studies in which you faced a difficult problem, and what you did to overcome it." A perfectly reasonable question to ask a potential recruit with no employment history, but an impossibly hard question for me to answer.</p>
<p>I was completely stumped. In my university years, I worked on many research projects, and it wasn't always easy. I <em>knew</em> I had faced various kinds of problems during my graduate studies, and I assumed I had overcome them all before getting my degree. Why couldn't I come up with a single example?</p>
<p>This was the first time I noticed that something was off. Those questions about relevant episodes are pretty standard in certain industries, and I had never heard anyone complain about them specifically. Yet they were anathema for me.</p>
<figure><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/kolar.webp" alt="Original photo by Jan Antonin Kolar, Unsplash (modified)" title="A row of wooden drawers with metal handles and blotted-out labels."><figcaption>Original photo by Jan Antonin Kolar, Unsplash (modified)</figcaption></figure>
<p>My memory feels like a file cabinet without labels, a database without an index, a dictionary of randomly-ordered words without a table of contents. There are many memories there, but most of them can't be retrieved with convenient keywords like "a time when X happened".</p>
<p>Only with very specific cues and external help am I able to, sometimes, recall the events I'm looking for. In the case of the job application questionnaire, I struggled with it for several days, asked a friend for advice, and eventually managed to put together a lame but passable answer based on my research notes. Still, I was left with the nagging feeling that more fitting and relevant examples remained buried away in my psyche, somewhere out of reach.</p>
<p>I felt the same limitation very strongly again last year, when my grandfather passed away. I determined to sit down and write everything I could remember about him, and my relationship with him. I went back in my mind to his house in the Roman countryside and wrote things as they came to me.</p>
<p>He was kind and jovial with us grandchildren. He often involved us in making bread and pizza together in his stone oven, and I liked that. I could even write a general visual description (not by putting into words what I saw in my mind at the time of writing, but by recalling what I "knew" about his looks). And so on, I could muster a good number of generic, timeless <em>facts</em> about him, including my feelings related to him, but I soon realized that episodes and conversations were sorely absent: alright, he used to keep bees, and took me to see them more than once; but how many times? What did we say during those visits? What happened specifically?</p>
<p>Nothing that resembles a "scene" or sequence of events resurfaced in my memory. Everything I wrote was in the past progressive tense: "he used to be like this", "we would often do that", "more than once we did so".</p>
<p>Nowhere in what I wrote was any sense of sequential events, nor any specific conversation, not to mention specific utterances. I could write a good deal about him, but I had to rely on educated guesses in order to put together a coherent description of things that happened.</p>
<p>I wanted to bring back specific episodes, one-time events that we had experienced together, but I could find very little. He was there in my mind, no mistake about that, but in an intangible, elusive way. That day I felt disheartened, and dropped the project in the middle.</p>

<p>Most of the time this weakness in recollecting specific life episodes doesn't have major practical consequences. If necessary, others can help me bring back a memory, and I can remember the most consequential information as facts rather than episodes. In fact, a <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10598423/" rel="nofollow noopener noreferrer" target="_blank">recent study</a> has found that aphantasics aren't any worse than non-aphantasics as eyewitnesses: although the participants "recalled 30% less correct information and accounts were less complete," "they made no more errors and were as accurate as typical imagers."</p>
<p>The downsides, then, seem to be mostly emotional, not pragmatic.</p>
<p>(If you're wondering how I can remember this "episode" of me trying to write about my grandfather, it's because I have the unfinished text saved, and wrote my reflections about the attempt soon afterwards. I have what I need to reconstruct the episode without leaning very much on my remembering powers.)</p>
<h2>Memory Voids</h2>
<hr><figure id="floating-1"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/512200ldsdl.webp" alt="A painting of a desert scene with ruins of ancient buildings."><figcaption>A Bedouin encampment surrounded by ruins, Charles Théodore Frère</figcaption></figure>
<p>The blank I drew with my grandfather is just an example of what you might call a "memory void." It's not that I tend to forget people. Indeed, my loved ones are safe in my mind, albeit in that intangible and elusive form, with no risk of being forgotten—more on this later. My memory voids are specifically about <em>the concrete things I did in my life.</em></p>
<p>Ask me how my childhood was, or if I had fun in my twenties, and all I can answer is "I think so." Not because I wasn't sure about it at the time, but because I don't remember what I thought of it. With such broad and general questions, I have almost no hope of coming up with representative memories to help me answer those questions. No flashbacks to times I thought "this is great!" nor to moments of sadness. Again, many such events are buried as facts and observations somewhere in my memory, but that's not how I can recover them.</p>
<p>My past feels like someone else's. I know a great deal about it, more than anyone else in fact, yet I don't remember <em>being</em> in it. I can create a year-by-year history of my whole life with information such as the places I lived in, the schools I went to, the major turning points in my life; I can explain many facts about the key people and events of each time period; I can even arrange many of these in the form of stories or ordered stages of growth—yet none of this feels like things <em>I</em> did. It's like being the world's top expert about a stranger's life.</p>
<p>To be clear, this is not dissociative amnesia, trauma-induced selective forgetting, or anything like that. I know I had a good, sheltered childhood and early adulthood, with a caring and kind family, good friends, no financial difficulties, no scarring or traumatizing events. I was, by all measures, a happy and privileged kid. But I know that as a dry fact, not as a rush of nostalgic emotions.</p>
<p>Why aphantasia would do that to one's memory is still unclear. The topic itself hasn't been studied much yet, but this is slowly changing. In a very recent <a href="https://www.researchsquare.com/article/rs-6675918/v1" rel="nofollow noopener noreferrer" target="_blank">experiment</a>, Boere et al. (2025) used EEG (electroencephalography) to show that there might be fundamental differences in neural activity at the time of <em>forming</em> new episodic memories, rather than at the time of retrieving them. Aphantasics, they found, have lower levels of the kind of brain waves associated with attention and, crucially, memory updating.</p>
<p>This is very interesting in itself, but the following observation in their abstract is arguably even more important:</p>
<blockquote>
<p>Despite these neural differences, behavioral performance remained comparable, indicating possible compensatory strategies.</p>
</blockquote>
<p>In other words, people with aphantasia don't fare significantly worse in their practical use of memory—they just use it in a different way.</p>
<p>Whenever I think about a period of my life, all the "situational" and somewhat "concrete" memories I get are <em>averaged out</em>, all similarities between separate days and recurrences overlapping each other and blending together, while all the deviations from routine are washed away into oblivion: everything in the past progressive.</p>
<h2>Semantic and Spatial Memory Are Fine</h2>
<hr><figure id="floating-2"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/535271ldsdl.webp" alt="A painting of a group of people with turbans in a middle-eastern bazaar."><figcaption>A Bazaar in Cairo, Charles Théodore Frère</figcaption></figure>
<p>If the results of Boere et al. are confirmed, this could shed some fascinating light on how different non-episodic, or "semantic" memory is from the episodic kind. In my case, semantic memory seems to be perfectly intact, and only the episodic, autobiographical kind is impaired.</p>
<p>From the observations above, it's as if my memory-encoding neural circuits work by comparing new experiences with pre-existing <a href="https://aethermug.com/posts/a-framing-and-model-about-framings-and-models">mental models</a>, tweaking and tuning those mental models of the world with each new sensory input, rather than collecting separate instances of similar but slightly different situations.</p>
<p>This would explain why the important, recurrent facts remain, while all the fickle details are washed away as if by an averaging operation. Perhaps this is what goes on in everyone's brains, except that in most people the episode-storing circuits are also working at the same time, and the two processes feel inseparable.</p>
<p>The interpretation above would also explain why my mental models—the <a href="https://aethermug.com/posts/embedded-prophesy-devices">embedded prophesy devices</a> I rely on to predict the future and function in everyday life—are as good as anyone else's. Indeed, it might even explain why I seem to care and think about mental models, and about cognition in general, more than the average person. For me, mental models are <em>the main way</em> I benefit from my memory. They help me not only to form reasonable expectations about what might happen in the future, but also to "reconstruct" my past—the educated guesses about my own past I referred to before.</p>
<p>And we shouldn't forget spatial awareness. This "sense of space and location" plays a major role in my thinking system.</p>
<hr><figure id="floating-3"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/heidi-kaden-TJrorizZcPA-unsplash.webp" alt="A view of Florence's Duomo, seen from a small street."><figcaption>Photo by Heidi Kaden, Unsplash</figcaption></figure>
<p>For as long as I can remember, I've been very good at understanding maps and not getting lost. When I lived in Florence, at the age of nine or ten, I loved to be the one guiding my family through the city's meandering side-streets with the help of a paper map. I would choose (past progressive!) new and unknown routes every time, just for the fun of exploration, but we always ended up emerging into the square or courtyard I had intended.</p>
<p>For all the difficulties I have in recalling scenes that unfolded on specific days of my life, I have no trouble at all remembering the spatial layout of the places where those scenes took place. I can draw the floor map of any house I've lived in or spent more than a couple of days in the past 30 or more years. When I visit Rome, a city I haven't lived in for more than a decade, the routes to get anywhere familiar come back to me as clearly as if I had learned them the previous day. This is clearly another kind of memory, quite distinct from both the episodic and the semantic kinds.</p>
<p>As a matter of fact, spatial memory is the closest thing I have to an "index" for the musty file cabinet of my episodic memories. If I can remember <em>where</em> something happened, there is a good chance I can remember many more details about <em>what</em> happened.</p>
<p>This is a recent realization of mine, and I've taken to calling it the <em>Swoosh Effect</em>. Often my wife mentions an event or the name of a shop, saying something like "I miss the Flavor Savor hamburgers we used to go to when we lived in Nagareyama!" Usually, to her unconcealed dismay, I draw a complete blank: "what's the Flavor Savor?" We used to go there all the time, she says, and it wasn't even that long ago.</p>
<p>I get absolutely nothing. I frantically try to think of hamburger joints in Nagareyama: zero hits.</p>
<p>Then she adds some spatial information, like "it's on the last floor of the XYZ building in front of the station" and suddenly I'm transported there in a roller-coaster instant and it all comes back to me clearly. I almost <em>feel</em> the swooshing movement of going from the station to the entrance of XYZ building, then to the escalators, then up to the last floor, and finally homing into the entrance of the Flavor Savor, all in less than a second. Now all the semantic information pours out: "of course, the Flavor Savor! We went there, like, six times in a year. They have great avocado burgers and a tasty homemade sauce there!" If not too averaged-out, even some fragments of Flavor Savor episodes might come back to me at that point.</p>
<p>In short, I use my semantic and spatial memory to fill in what my episodic memory is unable to recover (or store). Most of the time this works fine, but in some cases that way of compensating doesn't work.</p>
<p>For example, I think I may also have mild face-blindness, the difficulty in recognizing faces and linking them with names. Usually, it doesn't cause major issues, and with some effort and repetition, I can learn to recognize people. But the face-blindness really rears its head when I meet someone not-so-familiar in an unexpected place, like random encounters on a train. Since I don't have the usual contextual cues to help me, in these cases I find it very hard to pin down who they are. They go "hey Marco, what's up?" and all I get is the vague sense that I know this person from <em>somewhere</em>. Only when they mention names or other contextual information do I have a chance of allocating them in their rightful place in my mental social network.</p>
<h2>Not Bad, All in All</h2>
<hr><figure id="floating-4"><img src="https://aethermug.com/assets/posts/i-do-not-remember-my-life-and-it-s-fine/535272ldsdl.webp" alt="A painting of a man in a boat on a peaceful middle-eastern river."><figcaption>Dhows on the Nile, Charles Théodore Frère</figcaption></figure>
<p>If you have intact episodic memory, some of the descriptions above might sound entirely alien to you. You might have many questions, and I don't know how to answer them all. Before concluding, though, I will try to address two of what might be your biggest doubts.</p>
<p>First, does my lack of remembered episodes and nostalgic flashbacks mean that the people in my life don't really exist as people in my mind, and that my forgotten experiences taught me nothing? No, and no.</p>
<p>It is hard to explain, but the things that matter do stay with me, even if I can't reminisce about the specific times they happened to me. I may not be able to play back fond memories of distinct interactions with my late grandfather, for instance, but I internalized them all. Intangible and invisible as he may be, he is there in my mind and will always be, and thinking about him does evoke many emotions in me—emotions I feel <em>now</em>, not replicas of past emotions. Something invisible can never fade.</p>
<p>More broadly, my mind's constant "averaging" work makes it very hard for me to build an encyclopedic memory—the minor details quickly escape me—but the <em>understanding</em> remains. The important insights stick with me, and my learning takes the form of better and better mental models: more sophisticated, more inclusive of many factors, more widely applicable or abstract. This, even when the specifics of how I obtained those insights refuse to be summoned back. Those vague "problems I had to overcome in university" that the job screening question wanted from me had happened, and I <em>had</em> learned my lessons from them, even though I forgot how they unfolded.</p>
<p>I consider this to be a key component of my intelligence because it allows me to concentrate on what is important. My experience is distilled directly into wisdom. Which brings me to the second doubt you might have: is SDAM a detestable handicap?</p>
<p>More so than with aphantasia, I can see how one might want to call SDAM a disorder. It does sound like a net negative, the removal of an ability that cannot be fully replaced, at least when your goal is to be "reunited" with a lost or distant loved one. Unlike aphantasia, I do often feel my weakness on this front, and I understand the many people with SDAM who bemoan their condition.</p>
<p>But—call me an indefatigable optimist—I also see benefits to having SDAM.</p>
<p>By doing away with reminiscences, flashbacks, and graphic visions of possible futures, I can stay focused on the now, and on what I can do now to improve tomorrow. I don't get intrusive scenes to distract me and sway me with sudden emotions.</p>
<p>Perhaps SDAM pushes me to work harder at interpreting the new information <em>as I perceive it</em>, because I know, deep down, that I will either "get" it now—updating a mental model—or I risk forever missing the opportunity to "get" it. This commitment to immediate understanding, in turn, helps me improve as a rational thinker.</p>
<p>And, once again, there is still no empirical proof that these memory "deficits" bring significant disadvantages <em>in practice</em>. The paper I mentioned at the beginning—the one that found no difference in eyewitness accuracy between people with and without aphantasia—makes this conclusion:</p>
<blockquote>
<p>Our pattern of results indicates reduced mental imagery ability might be compensated for by alternative self-initiated cognitive strategies.</p>
</blockquote>
<p>That is one fair way to put it. But here is another one, from my point of view: <em>having strong mental imagery and episodic memory doesn't seem to help much in practice.</em> It is just an alternative way to experience the world. ●</p>
<div><p>Cover image:</p><p><em>Caravane Au Coucher Du Soleil, Charles Théodore Frère</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Claude Composer (138 pts)]]></title>
            <link>https://github.com/possibilities/claude-composer</link>
            <guid>44196417</guid>
            <pubDate>Thu, 05 Jun 2025 22:53:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/possibilities/claude-composer">https://github.com/possibilities/claude-composer</a>, See on <a href="https://news.ycombinator.com/item?id=44196417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Claude Composer CLI</h2><a id="user-content-claude-composer-cli" aria-label="Permalink: Claude Composer CLI" href="#claude-composer-cli"></a></p>
<blockquote>
<p dir="auto">A tool for enhancing Claude Code with automation, configuration, and better UX</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Reduced interruptions</strong>: Auto-handles permission dialogs based on configurable rules</li>
<li><strong>Flexible control</strong>: Rulesets define which actions to allow automatically</li>
<li><strong>Tool management</strong>: Toolsets configure which tools Claude can use</li>
<li><strong>Enhanced visibility</strong>: System notifications keep you informed</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Install
npm install -g claude-composer

# Initialize configuration
claude-composer cc-init

# Run with default settings
claude-composer

# Use different rulesets
claude-composer --ruleset internal:yolo  # Accept all prompts
claude-composer --ruleset internal:safe  # Manual confirmation only"><pre><span><span>#</span> Install</span>
npm install -g claude-composer

<span><span>#</span> Initialize configuration</span>
claude-composer cc-init

<span><span>#</span> Run with default settings</span>
claude-composer

<span><span>#</span> Use different rulesets</span>
claude-composer --ruleset internal:yolo  <span><span>#</span> Accept all prompts</span>
claude-composer --ruleset internal:safe  <span><span>#</span> Manual confirmation only</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><strong>Prerequisites</strong>: Node.js 18+, npm/yarn/pnpm, Claude Code installed</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Global install
pnpm add -g claude-composer
# or
yarn global add claude-composer
# or
npm install -g claude-composer"><pre><span><span>#</span> Global install</span>
pnpm add -g claude-composer
<span><span>#</span> or</span>
yarn global add claude-composer
<span><span>#</span> or</span>
npm install -g claude-composer</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Configuration</h2><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Run <code>claude-composer cc-init</code> to create configuration:</p>
<div dir="auto" data-snippet-clipboard-copy-content="# Global config (default)
claude-composer cc-init

# Project-specific config
claude-composer cc-init --project"><pre><span><span>#</span> Global config (default)</span>
claude-composer cc-init

<span><span>#</span> Project-specific config</span>
claude-composer cc-init --project</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration Structure</h3><a id="user-content-configuration-structure" aria-label="Permalink: Configuration Structure" href="#configuration-structure"></a></p>
<div data-snippet-clipboard-copy-content="~/.claude-composer/          # Global
├── config.yaml
├── rulesets/*.yaml         # Custom rulesets
└── toolsets/*.yaml         # Custom toolsets

.claude-composer/           # Project-specific
├── config.yaml
├── rulesets/*.yaml
└── toolsets/*.yaml"><pre><code>~/.claude-composer/          # Global
├── config.yaml
├── rulesets/*.yaml         # Custom rulesets
└── toolsets/*.yaml         # Custom toolsets

.claude-composer/           # Project-specific
├── config.yaml
├── rulesets/*.yaml
└── toolsets/*.yaml
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Basic Configuration</h3><a id="user-content-basic-configuration" aria-label="Permalink: Basic Configuration" href="#basic-configuration"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# config.yaml
rulesets:
  - internal:cautious
  - my-custom-rules

toolsets:
  - internal:core
  - my-tools

roots:
  - ~/projects/work
  - ~/projects/personal

show_notifications: true
sticky_notifications: false"><pre><span><span>#</span> config.yaml</span>
<span>rulesets</span>:
  - <span>internal:cautious</span>
  - <span>my-custom-rules</span>

<span>toolsets</span>:
  - <span>internal:core</span>
  - <span>my-tools</span>

<span>roots</span>:
  - <span>~/projects/work</span>
  - <span>~/projects/personal</span>

<span>show_notifications</span>: <span>true</span>
<span>sticky_notifications</span>: <span>false</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/configuration.md">docs/configuration.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Rulesets</h2><a id="user-content-rulesets" aria-label="Permalink: Rulesets" href="#rulesets"></a></p>
<p dir="auto">Control which permission dialogs are automatically accepted.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Rulesets</h3><a id="user-content-built-in-rulesets" aria-label="Permalink: Built-in Rulesets" href="#built-in-rulesets"></a></p>
<ul dir="auto">
<li><strong><code>internal:safe</code></strong>: All dialogs require manual confirmation</li>
<li><strong><code>internal:cautious</code></strong>: Auto-accepts project operations, confirms global ones</li>
<li><strong><code>internal:yolo</code></strong>: Accepts all operations without confirmation</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Rulesets</h3><a id="user-content-using-rulesets" aria-label="Permalink: Using Rulesets" href="#using-rulesets"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Built-in
claude-composer --ruleset internal:cautious

# Custom global
claude-composer --ruleset my-workflow

# Project-specific
claude-composer --ruleset project:backend

# Chain multiple
claude-composer --ruleset internal:cautious --ruleset my-overrides"><pre><span><span>#</span> Built-in</span>
claude-composer --ruleset internal:cautious

<span><span>#</span> Custom global</span>
claude-composer --ruleset my-workflow

<span><span>#</span> Project-specific</span>
claude-composer --ruleset project:backend

<span><span>#</span> Chain multiple</span>
claude-composer --ruleset internal:cautious --ruleset my-overrides</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Ruleset Example</h3><a id="user-content-custom-ruleset-example" aria-label="Permalink: Custom Ruleset Example" href="#custom-ruleset-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# .claude-composer/rulesets/backend.yaml
name: backend
description: Backend development rules

accept_project_edit_file_prompts:
  paths:
    - 'src/**/*.js'
    - 'test/**'
    - '!**/*.env'

accept_project_bash_command_prompts: true
accept_fetch_content_prompts: false"><pre><span><span>#</span> .claude-composer/rulesets/backend.yaml</span>
<span>name</span>: <span>backend</span>
<span>description</span>: <span>Backend development rules</span>

<span>accept_project_edit_file_prompts</span>:
  <span>paths</span>:
    - <span><span>'</span>src/**/*.js<span>'</span></span>
    - <span><span>'</span>test/**<span>'</span></span>
    - <span><span>'</span>!**/*.env<span>'</span></span>

<span>accept_project_bash_command_prompts</span>: <span>true</span>
<span>accept_fetch_content_prompts</span>: <span>false</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/rulesets.md">docs/rulesets.md</a> for complete documentation.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Toolsets</h2><a id="user-content-toolsets" aria-label="Permalink: Toolsets" href="#toolsets"></a></p>
<p dir="auto">Configure which tools Claude can use and MCP server connections.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Built-in Toolsets</h3><a id="user-content-built-in-toolsets" aria-label="Permalink: Built-in Toolsets" href="#built-in-toolsets"></a></p>
<ul dir="auto">
<li><strong><code>internal:core</code></strong>: Provides Context7 documentation tools</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using Toolsets</h3><a id="user-content-using-toolsets" aria-label="Permalink: Using Toolsets" href="#using-toolsets"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Built-in
claude-composer --toolset internal:core

# Custom
claude-composer --toolset my-tools

# Multiple
claude-composer --toolset internal:core --toolset project:dev-tools"><pre><span><span>#</span> Built-in</span>
claude-composer --toolset internal:core

<span><span>#</span> Custom</span>
claude-composer --toolset my-tools

<span><span>#</span> Multiple</span>
claude-composer --toolset internal:core --toolset project:dev-tools</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Custom Toolset Example</h3><a id="user-content-custom-toolset-example" aria-label="Permalink: Custom Toolset Example" href="#custom-toolset-example"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# .claude-composer/toolsets/dev-tools.yaml
allowed:
  - Read
  - Write
  - Edit
  - Bash

disallowed:
  - WebSearch

mcp:
  my-server:
    type: stdio
    command: node
    args: [./tools/mcp-server.js]"><pre><span><span>#</span> .claude-composer/toolsets/dev-tools.yaml</span>
<span>allowed</span>:
  - <span>Read</span>
  - <span>Write</span>
  - <span>Edit</span>
  - <span>Bash</span>

<span>disallowed</span>:
  - <span>WebSearch</span>

<span>mcp</span>:
  <span>my-server</span>:
    <span>type</span>: <span>stdio</span>
    <span>command</span>: <span>node</span>
    <span>args</span>: <span>[./tools/mcp-server.js]</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/toolsets.md">docs/toolsets.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Command Line Options</h2><a id="user-content-command-line-options" aria-label="Permalink: Command Line Options" href="#command-line-options"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Core Options</h3><a id="user-content-core-options" aria-label="Permalink: Core Options" href="#core-options"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Configuration
--ruleset <name...>              # Use specified rulesets
--toolset <name...>              # Use specified toolsets
--ignore-global-config           # Ignore global config

# Safety
--dangerously-allow-in-dirty-directory
--dangerously-allow-without-version-control
--dangerously-suppress-automatic-acceptance-confirmation

# Notifications
--show-notifications / --no-show-notifications
--sticky-notifications / --no-sticky-notifications

# Debug
--quiet                          # Suppress preflight messages
--allow-buffer-snapshots         # Enable Ctrl+Shift+S snapshots
--log-all-pattern-matches        # Log to ~/.claude-composer/logs/"><pre><span><span>#</span> Configuration</span>
--ruleset <span>&lt;</span>name...<span>&gt;</span>              <span><span>#</span> Use specified rulesets</span>
--toolset <span>&lt;</span>name...<span>&gt;</span>              <span><span>#</span> Use specified toolsets</span>
--ignore-global-config           <span><span>#</span> Ignore global config</span>

<span><span>#</span> Safety</span>
--dangerously-allow-in-dirty-directory
--dangerously-allow-without-version-control
--dangerously-suppress-automatic-acceptance-confirmation

<span><span>#</span> Notifications</span>
--show-notifications / --no-show-notifications
--sticky-notifications / --no-sticky-notifications

<span><span>#</span> Debug</span>
--quiet                          <span><span>#</span> Suppress preflight messages</span>
--allow-buffer-snapshots         <span><span>#</span> Enable Ctrl+Shift+S snapshots</span>
--log-all-pattern-matches        <span><span>#</span> Log to ~/.claude-composer/logs/</span></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Subcommands</h3><a id="user-content-subcommands" aria-label="Permalink: Subcommands" href="#subcommands"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="# Initialize configuration
claude-composer cc-init [options]
  --project                      # Create in current directory
  --use-yolo-ruleset            # Use YOLO ruleset
  --use-cautious-ruleset        # Use cautious ruleset
  --use-safe-ruleset            # Use safe ruleset
  --use-core-toolset            # Enable core toolset"><pre><span><span>#</span> Initialize configuration</span>
claude-composer cc-init [options]
  --project                      <span><span>#</span> Create in current directory</span>
  --use-yolo-ruleset            <span><span>#</span> Use YOLO ruleset</span>
  --use-cautious-ruleset        <span><span>#</span> Use cautious ruleset</span>
  --use-safe-ruleset            <span><span>#</span> Use safe ruleset</span>
  --use-core-toolset            <span><span>#</span> Enable core toolset</span></pre></div>
<p dir="auto">All unrecognized options pass through to Claude Code.</p>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/cli-reference.md">docs/cli-reference.md</a> for complete reference.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Environment Variables</h2><a id="user-content-environment-variables" aria-label="Permalink: Environment Variables" href="#environment-variables"></a></p>
<ul dir="auto">
<li><code>CLAUDE_COMPOSER_CONFIG_DIR</code> - Override config directory</li>
<li><code>CLAUDE_COMPOSER_NO_NOTIFY</code> - Disable notifications</li>
<li><code>FORCE_COLOR</code> - Control color output</li>
</ul>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/environment-variables.md">docs/environment-variables.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roots Configuration</h2><a id="user-content-roots-configuration" aria-label="Permalink: Roots Configuration" href="#roots-configuration"></a></p>
<p dir="auto">Define trusted parent directories to auto-accept initial trust prompts:</p>
<div dir="auto" data-snippet-clipboard-copy-content="roots:
  - ~/projects # Trusts ~/projects/my-app, not ~/projects/my-app/src
  - $WORK_DIR/repos # Environment variable expansion supported"><pre><span>roots</span>:
  - <span>~/projects </span><span><span>#</span> Trusts ~/projects/my-app, not ~/projects/my-app/src</span>
  - <span>$WORK_DIR/repos </span><span><span>#</span> Environment variable expansion supported</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/roots-config.md">docs/roots-config.md</a> for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Development</h2><a id="user-content-development" aria-label="Permalink: Development" href="#development"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contributing</h3><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<ol dir="auto">
<li>Fork the repository</li>
<li>Create feature branch</li>
<li>Commit changes</li>
<li>Push branch</li>
<li>Open Pull Request</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Release</h3><a id="user-content-release" aria-label="Permalink: Release" href="#release"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="npm run release:patch  # Bug fixes
npm run release:minor  # New features
npm run release:major  # Breaking changes"><pre>npm run release:patch  <span><span>#</span> Bug fixes</span>
npm run release:minor  <span><span>#</span> New features</span>
npm run release:major  <span><span>#</span> Breaking changes</span></pre></div>
<p dir="auto">See <a href="https://github.com/possibilities/claude-composer/blob/main/docs/release-process.md">docs/release-process.md</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What a developer needs to know about SCIM (131 pts)]]></title>
            <link>https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim</link>
            <guid>44196393</guid>
            <pubDate>Thu, 05 Jun 2025 22:48:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim">https://tesseral.com/blog/what-a-developer-needs-to-know-about-scim</a>, See on <a href="https://news.ycombinator.com/item?id=44196393">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Why SCIM exists</h2>
<p>Imagine you’re running a relatively large company, one with a few thousand employees. All of those employees use at least some software to do their jobs. It’s probably safe to assume that you’re dealing with hundreds of different SaaS applications across the company. You’ll have an app for approving expenses, an app for managing salespeople’s compensation, an app for piping data into your data warehouse, and much more. There’s an awfully long list of stuff.</p>
<p>Every employee needs access to some subset of apps. They need to do their jobs, after all. But you can’t give everyone access to everything. That’d cause all kinds of security, compliance, and practical problems. You need a way to assign different permissions to different people.</p>
<p>To handle access and permissions all in one centralized place, companies tend to use IT management software like Entra, Okta, or OneLogin (among many others); people tend to describe these tools as identity providers.</p>
<p>An identity provider (IDP) behaves a bit like a database. It maintains a list of employees along with a bunch of information about each person. Similarly, it maintains a list of different software applications. It keeps track of the mappings between people and applications. It’s very easy for the IT team to modify and create relationships between records.</p>
<p>Simply having a list of users and their access privileges in a database doesn’t help anyone much, though. The identity provider also needs to communicate information about users with other software.</p>
<p>The identity provider basically needs to communicate three kinds of changes to other software:</p>
<ol>
<li>The addition of new users (e.g. new hires)</li>
<li>The change of any existing user’s attributes (e.g. name, job title, etc.)</li>
<li>The removal of any existing users (e.g. departing employees)</li>
</ol>
<p>Identity providers typically rely on a standard called SCIM (the System for Cross-domain Identity Management) for these three communication tasks. They use SCIM to make every integration with other software look roughly the same, which eliminates the need for complicated bespoke integrations with the myriad applications they need to support.</p>
<h2>What SCIM (basically) does</h2>
<p>At a certain level of abstraction, a SCIM implementation looks a bit like a CS101 problem set. All we’re doing is making one list in your software look like another list in your customer’s software.</p>
<p>Put very simply, SCIM just defines some rules for the JSON that the identity provider sends and the JSON that the identity provider expects to receive in response. The JSON we’re trading with the identity provider exists solely to help us perform matching CRUD operations.</p>
<p>Let’s go through a conceptual example.</p>
<p>Suppose you’re selling a new inventory management system to Dunder Mifflin. Their IT team wants you to provision users programmatically from their identity provider. They have a list of users that need access to the inventory software:</p>
<ul>
<li>Kevin Malone, Accountant</li>
<li>Darryl Philbin, Shipping Manager</li>
<li>Creed Bratton, Quality Assurance Representative</li>
</ul>
<p>Great – via SCIM, you’ll get a pretty standardized block of JSON telling you which users to create. With that block of JSON, you’ll modify your Users table to include records for Kevin, Darryl, and Creed.</p>
<p>Suppose Darryl gets transitioned to a new role as Marketing Director. Great, IT updates his title in the identity provider. Within a few moments, you’ll get a standard block of JSON telling you to change Darryl’s job title. You’ll again just modify your Users table. Nothing too crazy.</p>
<p>Given Darryl’s job change, the Dunder Mifflin IT team decides he doesn’t need access to his account in the inventory management system anymore. Great, they just remove the mapping between Darryl and your software in their identity provider. Before long, you receive a standard block of JSON that tells you to deprovision – remove – Darryl’s account. You’ll make the corresponding changes in your Users table.</p>
<p>That’s all that SCIM is trying to do.</p>
<h2>What SCIM isn’t</h2>
<p>We often meet people who think SCIM support means that they need to make major changes to their software. This really should not be the case. Here, I’ll go through a few common misconceptions.</p>
<p>SCIM doesn’t really have anything to do with compliance. SCIM isn’t really related to SOC 2, even though there’s probably overlap in the kinds of customers who care about SCIM and the customers who care about SOC 2.</p>
<p>SCIM doesn’t really have anything to do with data retention. This is usually a separate conversation you might need to have with your customer.</p>
<p>SCIM doesn’t have any direct effect on your single sign-on implementation. Although it’s typical for people to use SCIM in combination with SAML SSO, these two standards actually don’t need to exist together or interact at all. It would be unusual, but you could use SCIM to manage users that use vanilla passwords to access your software.</p>
<p>SCIM doesn’t have any direct effect on how you manage your sessions. You can use whatever session management tools you’d like. Relatedly, SCIM doesn’t require single log-out support. If you receive an instruction from your customer to deprovision a user, you don’t typically need to revoke any active sessions belonging to that user.</p>
<p>SCIM doesn’t require major changes to your users schema. As long as you have a way of translating the JSON you receive into the desired CRUD operations,</p>
<p>SCIM doesn’t even really mean you have to support hard-deleting users or their data. You should set clear expectations with your customers, but it’s usually sufficient from your customer’s perspective that de-provisioning a user results in their account appearing no longer to exist. If that simply means adding some boolean column in your database that looks like IS_DELETED, that’s probably fine.</p>
<p>SCIM doesn’t require real-time updates. In practice, it’s usually fine if you’re processing updates every few hours. Many companies’ identity providers can’t support real-time updates anyway.</p>
<h2>How SCIM works at a (minimally) technical level</h2>
<p>Earlier, I mentioned that SCIM just performs CRUD operations via JSON. We should take that relatively literally. SCIM will only ever handle creating, reading, updating, and deleting records.</p>
<p>To do so, SCIM maps pretty familiar HTTP verbs onto these CRUD tasks. We’ll need to support GET and POST, as you would expect. We’ll also need to spend time thinking about PUT, PATCH, and DELETE, which can get annoying.</p>
<p>There’s something really important to bear in mind as we venture a little deeper into SCIM here. If your customer wants your software to hook into their identity provider, your software becomes the server – and your customer’s identity provider becomes the client. This might feel a little weird at first. After all, your customer’s identity provider stores the data you need. But it should make sense as we go.</p>
<h2>Client/server relationship and authentication in SCIM</h2>
<p>For any given SCIM operation, your customer’s identity provider will send one or more requests to your software. You need to process the request correctly, then you need to respond in a manner that the identity provider expects and understands.</p>
<p>Your software will play a passive role in SCIM. It will stay online and process requests as they roll in. The customer’s identity provider is the client. Your software is the server.</p>
<p>Given its role as the client, your customer’s identity provider needs to authenticate itself to you. It needs to prove that it’s actually the identity provider – and not some attacker – to make the changes it’s requesting. (It would be really bad if anyone could come along and provision users!)</p>
<p>We have a few different ways of authenticating SCIM clients, but bearer tokens are the most widely-supported option. You as the server need to generate a secret, share it with your customer, and have the customer’s identity provider present that secret in HTTP headers when it makes requests. You'll consider the presentation of a valid bearer token to be sufficient proof of identity and, consequently, you'll honor any HTTP request that correctly presents a valid bearer token.</p>
<h2>The data that we handle in SCIM operations</h2>
<p>SCIM’s creators built it around a generic concept that they call a <em>resource</em>. In this context, we should understand a resource to mean a particular kind of record.</p>
<p>In principle, we can represent basically anything as a SCIM resource. In practice, though, we pretty much only care about two of them. SCIM has users, and it has groups. People don’t tend to use SCIM for anything else.</p>
<p>Users in SCIM work pretty much like you’d expect. They’re just records of the people who use your software. You can think of groups as lists of users. (We can also have groups of groups.)</p>
<h2>Read operations in SCIM</h2>
<p>Let’s say you’ve just correctly configured a client/server trust relationship with your customer’s identity provider.</p>
<p>Before anything else happens, your customer’s identity provider will send you an HTTP GET. It will look basically like this:</p>
<p><img src="https://images.ctfassets.net/336y06gasq8n/2Pc9062CO8ZZ9oP9jeZFhJ/e686a5d948168d2bb541d08544d3ed38/scimget.png" alt="scimget"></p>
<p>All the identity provider does here is to hit a given <em>/Users</em> endpoint – in this case with a <em>userName</em> query parameter. It’s effectively asking here, <em>tell me what data you have for creed.bratton@example.com</em>. You’ll reply with a standard response code and some JSON.</p>
<p>In this case, we don’t have any data for Creed, so we’ll respond with a status code 200 and the following JSON:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    "itemsPerPage"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>    "resources"</span><span>: [],</span></span>
<span><span>    "schemas"</span><span>: [</span></span>
<span><span>        "urn:ietf:params:scim:schemas:core:2.0:User"</span></span>
<span><span>    ],</span></span>
<span><span>    "startIndex"</span><span>: </span><span>1</span><span>,</span></span>
<span><span>    "totalResults"</span><span>: </span><span>0</span></span>
<span><span>}</span></span></code></pre>
<p>Notice that we’re using this schemas property here to reference <code>urn:ietf:params:scim:schemas:core:2.0:User</code>. We’re just referencing the built-in concept of users from the SCIM spec.</p>
<h2>Creation operations in SCIM</h2>
<p>The identity provider sees that we don’t have any data for Creed yet – based on the JSON we sent back – and so it instructs us to create a record.</p>
<p>To do so, it will use an HTTP POST with the following request body:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>    "name"</span><span>: {</span></span>
<span><span>        "familyName"</span><span>: </span><span>"Bratton"</span><span>,</span></span>
<span><span>        "givenName"</span><span>: </span><span>"Creed"</span></span>
<span><span>    },</span></span>
<span><span>    "userName"</span><span>: </span><span>"creed.bratton@example.com"</span></span>
<span><span>}</span></span></code></pre>
<p>We just take this JSON and use our preferred method of creating a user record for Creed in our backend.</p>
<h2>Update operations in SCIM</h2>
<p>Updates in SCIM will use either HTTP PUT or HTTP PATCH, depending on the identity provider. Some identity providers tend only to use PUT. Others tend only to use PATCH. It’s kind of chaotic.</p>
<p>PUT operations in SCIM are pretty simple. They basically communicate: replace this user’s data with the data I’m showing you here. For example, Okta might send the following PUT request:</p>
<pre tabindex="0"><code><span><span>PUT</span><span> /scim/v2/Users/23a35c27-23d3-4c03-b4c5-6443c09e7173 </span><span>HTTP</span><span>/</span><span>1.1</span></span>
<span><span>User-Agent</span><span>:</span><span> Okta SCIM Client 1.0.0</span></span>
<span><span>Authorization</span><span>:</span><span> &lt;Authorization credentials&gt;</span></span>
<span></span>
<span><span>{</span></span>
<span><span>    "schemas"</span><span>: [</span><span>"urn:ietf:params:scim:schemas:core:2.0:User"</span><span>],</span></span>
<span><span>    "id"</span><span>: </span><span>"23a35c27-23d3-4c03-b4c5-6443c09e7173"</span><span>,</span></span>
<span><span>    "userName"</span><span>: </span><span>"test.user@okta.local"</span><span>,</span></span>
<span><span>    "name"</span><span>: {</span></span>
<span><span>        "givenName"</span><span>: </span><span>"Another"</span><span>,</span></span>
<span><span>        "middleName"</span><span>: </span><span>"Excited"</span><span>,</span></span>
<span><span>        "familyName"</span><span>: </span><span>"User"</span></span>
<span><span>    },</span></span>
<span><span>    "emails"</span><span>: [{</span></span>
<span><span>        "primary"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>        "value"</span><span>: </span><span>"test.user@okta.local"</span><span>,</span></span>
<span><span>        "type"</span><span>: </span><span>"work"</span><span>,</span></span>
<span><span>        "display"</span><span>: </span><span>"test.user@okta.local"</span></span>
<span><span>    }],</span></span>
<span><span>    "active"</span><span>: </span><span>true</span><span>,</span></span>
<span><span>    "groups"</span><span>: [],</span></span>
<span><span>    "meta"</span><span>: {</span></span>
<span><span>        "resourceType"</span><span>: </span><span>"User"</span></span>
<span><span>    }</span></span>
<span><span>}</span></span></code></pre>
<p>You as the server will just look for the test.user@okta.local user data and replace it. Not too bad!</p>
<p>PATCH requests in SCIM can be a little more complicated. PATCH represents a partial update, whereas PUT represents a complete replacement. Here’s an example PATCH from the SCIM specification:</p>
<pre><code>PATCH /Groups/acbf3ae7-8463-...-9b4da3f908ce
Host: example.com
Accept: application/scim+json
Content-Type: application/scim+json
Authorization: Bearer h480djs93hd8
If-Match: W/"a330bc54f0671c9"

{
    "schemas": ["urn:ietf:params:scim:api:messages:2.0"], 
    "Operations":[{ 
            "Op":"add", 
            "Path":"members", 
            "Value":[{ 
                    "display": "Babs Jensen",
                    "$ref": "https://example.com/v2/Users/2819c223...413861904646",
                    "Value": "2819c223-7f76-453a-919d-413861904646" 
                }
            ]
        }
    ]
} 

</code></pre>
<p>Notice the <code>Operations</code> array here. The SCIM specification allows PATCH operations to add, remove, or replace data. This turns out to be a little complicated. (More on this later).</p>
<h2>Delete operations in SCIM</h2>
<p>According to the SCIM specification, the identity provider should send an HTTP DELETE request, something like the following:</p>
<pre><code>DELETE /Users/2819c223-7f76-453a-919d-413861904646
Host: example.com
Authorization: Bearer h480djs93hd8
If-Match: W/"c310cd84f0281b7"
</code></pre>
<p>This is sometimes how things work, but it’s not how things always work in practice.</p>
<p>For example, Okta doesn’t want to use DELETE. Instead, they PUT or PATCH a record such that the active property gets set to False.</p>
<h2>Is it a good idea to build SCIM?</h2>
<h2>The SCIM specification is basically good, but has some subtle details</h2>
<p>We really like the SCIM specification. Compared to other standards (looking at you, SAML), SCIM makes a lot of sense. It’s conceptually pretty simple, and it doesn’t come with major design flaws.</p>
<p>It does have some subtle quirks, though. I’ll mention just a few here.</p>
<p>As I mentioned earlier, PATCH can get a bit complicated. You have to handle a bunch of different kinds of operations to support PATCH. Don’t worry about the content – but here’s an excerpt from the SCIM spec on how PATCH’s add operation is supposed to work:</p>
<blockquote>
<p>The result of the add operation depends upon what the target location
indicated by "path" references:</p>
</blockquote>
<blockquote>
<p>If omitted, the target location is assumed to be the resource
itself.  The "value" parameter contains a set of attributes to be
added to the resource.</p>
</blockquote>
<blockquote>
<p>If the target location does not exist, the attribute and value are
added.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a complex attribute, a set of
sub-attributes SHALL be specified in the "value" parameter.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a multi-valued attribute, a new
value is added to the attribute.</p>
</blockquote>
<blockquote>
<p>If the target location specifies a single-valued attribute, the
existing value is replaced.</p>
</blockquote>
<blockquote>
<p>If the target location specifies an attribute that does not exist
(has no value), the attribute is added with the new value.</p>
</blockquote>
<blockquote>
<p>If the target location exists, the value is replaced.</p>
</blockquote>
<blockquote>
<p>If the target location already contains the value specified, no
changes SHOULD be made to the resource, and a success response
SHOULD be returned.  Unless other operations change the resource,
this operation SHALL NOT change the modify timestamp of the
resource.</p>
</blockquote>
<p>That’s a decent number of <em>if</em>s to support! Individually, none of these is too bad. But taken as a whole, they represent a decent chunk of code that you’ll have to write. More importantly, they represent an awful lot of tests that you’ll have to write. Yuck.</p>
<p>I alluded indirectly to this other quirk earlier. SCIM stakes some strange opinions, including the expected instant messaging providers (e.g. Yahoo) associate with a built-in user attribute. It does not, however, stake a strong opinion at all regarding the meaning of <em>active</em>. Whether <em>active</em> represents a concept like <em>is_deleted</em> or something else altogether is left up to you and the identity provider. That just creates some weird, needless ambiguity.</p>
<p>Open source repos like Authentik’s are just <a href="https://github.com/goauthentik/authentik/issues/6695">littered with weird issues</a> resulting from ambiguity. No one seems to have all of the answers.</p>
<h2>Identity providers don’t always implement the spec</h2>
<p>Some identity providers do some really weird stuff in non-compliance with the SCIM spec. And then they don’t document their weird choices properly. (I’m looking at you, Microsoft!) You just have to collide with their weird SCIM APIs in the real world and tweak your implementation as you go.</p>
<p><img src="https://images.ctfassets.net/336y06gasq8n/2osXgz5SVQf4hQuaPJn5oV/2c7408109a4790e2b97c575406d543ec/dog_at_computer.jpg" alt="dog at computer"></p>
<p>Microsoft keeps a list of SCIM non-compliance issues <a href="https://learn.microsoft.com/en-us/entra/identity/app-provisioning/application-provisioning-config-problem-scim-compatibility">here</a>. You may notice that an issue called <em>Update PATCH behavior to ensure compliance (such as active as boolean and proper group membership removals)</em> still has a planned fix date of TBD ... on an article last updated in October 2023.</p>
<p>It turns out that Microsoft’s default behavior sends a boolean value as a string:</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "schemas"</span><span>: [</span></span>
<span><span>      "urn:ietf:params:scim:api:messages:2.0:PatchOp"</span></span>
<span><span>  ],</span></span>
<span><span>  "Operations"</span><span>: [</span></span>
<span><span>      {</span></span>
<span><span>          "op"</span><span>: </span><span>"Replace"</span><span>,</span></span>
<span><span>          "path"</span><span>: </span><span>"active"</span><span>,</span></span>
<span><span>          "value"</span><span>: </span><span>"False"</span></span>
<span><span>      }</span></span>
<span><span>  ]</span></span>
<span><span>}</span></span></code></pre>
<p>It should really be sending the below modified JSON,</p>
<pre tabindex="0"><code><span><span>{</span></span>
<span><span>  "schemas"</span><span>: [</span></span>
<span><span>      "urn:ietf:params:scim:api:messages:2.0:PatchOp"</span></span>
<span><span>  ],</span></span>
<span><span>  "Operations"</span><span>: [</span></span>
<span><span>      {</span></span>
<span><span>          "op"</span><span>: </span><span>"replace"</span><span>,</span></span>
<span><span>          "path"</span><span>: </span><span>"active"</span><span>,</span></span>
<span><span>          "value"</span><span>: </span><span>false</span></span>
<span><span>      }</span></span>
<span><span>  ]</span></span>
<span><span>}</span></span></code></pre>
<p>You can force Microsoft to send you the proper JSON if you use a certain feature flag (<code>aadOptscim062020</code>), but that’s really not an obvious solution! You really have to dig. It's more practical just to accept that Microsoft misbehaves and modify your code accordingly.</p>
<p>This sort of stuff is very time-consuming and demoralizing to resolve.</p>
<h2>You probably shouldn’t implement SCIM from scratch</h2>
<p>I really do not recommend building SCIM in-house. To be clear, you absolutely <em>could</em>. We're not exactly splitting the atom here.</p>
<p>It's just that you definitely have better things to spend your time on – things that are closer to the problems your customers care most about.</p>
<p>Although SCIM lacks much conceptual complexity, it comes with a bunch of annoying baggage. If you build SCIM yourself, it will become someone’s de facto job to babysit the SCIM API and debug unforeseen subtleties. Your customers will have a not-so-great experience. You will likely be very unhappy and distracted.</p>
<p>This is a case where you should probably look for an off-the-shelf solution and move on.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tokasaurus: An LLM Inference Engine for High-Throughput Workloads (188 pts)]]></title>
            <link>https://scalingintelligence.stanford.edu/blogs/tokasaurus/</link>
            <guid>44195961</guid>
            <pubDate>Thu, 05 Jun 2025 21:27:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scalingintelligence.stanford.edu/blogs/tokasaurus/">https://scalingintelligence.stanford.edu/blogs/tokasaurus/</a>, See on <a href="https://news.ycombinator.com/item?id=44195961">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="center">
          
          

          

          <hr>

          
            
              <p><img src="https://scalingintelligence.stanford.edu/imgs/teasers/tokasaurus.png" alt="">
                

              </p>
            

            <h2 id="tldr">TL;DR</h2>

<p>We’re releasing Tokasaurus, a new LLM inference engine optimized for throughput-intensive workloads. With small models, Tokasaurus benefits from very low CPU overhead and dynamic <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> grouping to exploit shared prefixes. For larger models, Tokasaurus supports async tensor parallelism for GPUs with NVLink and a fast implementation of pipeline parallelism for GPUs without. On throughput-focused benchmarks, Tokasaurus can outperform vLLM and SGLang by up to 3x+.</p>

<hr>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
  <li><a href="#intro">Intro</a></li>
  <li><a href="#optimizing-small-models">Optimizing Small Models</a></li>
  <li><a href="#optimizing-bigger-models">Optimizing Big Models</a></li>
  <li><a href="#try-it-out">Try it Out</a></li>
  <li><a href="#benchmarking-details">Benchmarking Details</a></li>
  <li><a href="#acknowledgements">Acknowledgements</a></li>
</ul>

<hr>

<h2 id="intro">Intro</h2>

<p>As LLMs get <a href="https://www.youtube.com/watch?v=gAjR4_CbPpQ">smarter, faster, and cheaper,</a> the community keeps finding new ways to use them. Our own recent work has explored using models to <a href="https://arxiv.org/abs/2501.14723">scan every file in a codebase</a>, <a href="https://arxiv.org/abs/2407.21787">sample 10,000 attempts for math and code problems</a>, and <a href="https://arxiv.org/abs/2502.15964">collaborate with other models to minimize cloud costs</a>. Inference is now also an important part of the training process, where we use models to <a href="https://arxiv.org/abs/2404.14219">generate synthetic data</a> or as part of <a href="https://arxiv.org/abs/2504.04736">RL pipelines</a> that generate and train on model completions.</p>

<p>Crucially, these new inference workloads look quite different than the original LLM use case of serving a chatbot. Here, we care primarily about the total time and cost required to complete a large batch of sequences, and we care much less (if at all) about the individual latency of a single generation. In other words, we want high throughput!</p>

<p>Open-source inference engines (i.e. dedicated systems for running efficient LLM inference) like <a href="https://arxiv.org/abs/2303.06865">FlexGen</a>, <a href="https://arxiv.org/abs/2309.06180">vLLM</a>, and <a href="https://arxiv.org/abs/2312.07104">SGLang</a> have been enormously valuable to the community. Inspired by and learning from these projects, we built a new engine, <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">Tokasaurus</a>, designed from the ground up to handle throughput-focused workloads. We’ve optimized Tokasaurus for efficiently serving large and small models alike, allowing it to outperform existing engines on throughput benchmarks. In the rest of this blog, we’ll walk through some of these optimizations and show off a few settings where Tokasaurus really shines.</p>

<hr>

<h2 id="optimizing-small-models">Optimizing Small Models</h2>

<p>To benchmark Tokasaurus with small models, we’ll use two workloads:</p>

<ul>
  <li>Completing chatbot prompts from the ShareGPT dataset (this is a common benchmark for testing inference engines).</li>
  <li>Reproducing an experiment from <a href="https://arxiv.org/abs/2407.21787">Large Language Monkeys</a>, where we take 128 problems from the <a href="https://arxiv.org/abs/2110.14168">GSM8K</a> math dataset and sample 1024 answers to every problem. The distinguishing feature of this workload is that there’s a lot of prefix sharing across sequences.</li>
</ul>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/small.png" alt="Tokasaurus small models">
  <img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/monkeys.png" alt="Tokasaurus large batch sampling">
</p>

<p>Tokasaurus outperforms vLLM and SGLang on both of these benchmarks, in particular achieving over 2x the throughput of other engines on the Large Language Monkeys workload. Two main features contribute to these wins with small models:</p>

<h3 id="minimizing-cpu-overhead">Minimizing CPU Overhead</h3>

<p>LLM engines perform many different tasks on the CPU, like handling web requests, tokenizing inputs/detokenizing outputs, managing KV cache allocation, and preparing inputs for the model. If these CPU-side tasks cause the GPU-side model to stall, throughput can take a <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">big hit</a>. To avoid these stalls, inference engines commonly make many CPU-side <a href="https://blog.vllm.ai/2024/09/05/perf-update.html">tasks</a> <a href="https://lmsys.org/blog/2024-12-04-sglang-v0-4/">asynchronous</a>: while the GPU runs a forward pass for batch N, the CPU-side of the engine post-processes the results from batch N-1 and prepares the inputs for batch N+1.</p>

<p>Tokasaurus goes one step further, making the CPU-side of the engine (what we call the manager) both asynchronous and adaptive. The manager’s goal is to maintain a deep queue of inputs for the model to run forward passes on. The manager monitors the size of this queue and can detect if the model is close to exhausting it (and therefore stalling the GPU). In these cases, the manager will automatically start skipping optional steps (like checking for stop strings and onboarding new sequences) until the model’s input queue is sufficiently deep again. This combination of asynchrony and adaptivity lets Tokasaurus serve small models with much less CPU overhead.</p>

<h3 id="dynamic-prefix-identification-and-exploration">Dynamic Prefix Identification and Exploration</h3>

<p>Prefix sharing comes up all the time in LLM inference — not just when repeatedly sampling like in the Large Language Monkeys benchmark, but also when asking many questions about a long document or reusing a system prompt across many chatbot conversations.</p>

<p>Shared prefixes allow attention to be computed more efficiently. We first explored this idea last year with <a href="https://arxiv.org/abs/2402.05099">Hydragen</a> (aka <a href="https://flashinfer.ai/2024/02/02/cascade-inference.html">cascade attention</a> and <a href="https://arxiv.org/abs/2403.08845">bifurcated attention</a>), but at the time we didn’t address the problem of detecting these shared prefixes in an engine where sequences are constantly starting and finishing. With Tokasaurus, we solve this detection problem by running a greedy depth-first search algorithm before every model forward pass that iteratively finds the longest shared prefixes possible. Hydragen is most impactful for small models, which spend a relatively larger fraction of total FLOPs on attention.</p>

<hr>

<h2 id="optimizing-bigger-models">Optimizing Bigger Models</h2>

<p>Tokasaurus can also efficiently serve bigger models across multiple GPUs! Here, the most important optimizations are our implementations of pipeline parallelism (PP) and tensor parallelism (TP), which allow us to maximize throughput on GPUs with or without NVLink.</p>

<h3 id="pipeline-parallelism-for-the-gpu-poor">Pipeline Parallelism for the GPU Poor</h3>

<p>One of our original goals with Tokasaurus was to efficiently run multi-GPU inference on our lab’s L40S GPUs, which don’t have fast inter-GPU NVLink connections. Without NVLink, the communication costs incurred running TP across a node of eight GPUs are substantial. Therefore, efficient support for PP (which requires much less inter-GPU communication) was a high priority. PP needs a large batch in order to run efficiently, since batches from the manager are subdivided into microbatches that are spread out across pipeline stages. When optimizing for throughput, we’re generally already using the largest batch size that fits in GPU memory, so PP is often a natural fit for throughput-focused workloads. When benchmarking against vLLM’s and SGLang’s pipeline parallel implementations using Llama-3.1-70B on eight L40S GPUs, Tokasaurus improves throughput by over 3x:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/pipeline.png" alt="Tokasaurus small models">
</p>

<h3 id="async-tensor-parallel-for-the-gpu-rich">Async Tensor Parallel for the GPU Rich</h3>

<p>If you do have GPUs with NVLink (e.g. B200s and certain models of H100s and A100s), Tokasaurus has something for you too! Models in Tokasaurus can be torch compiled end-to-end, allowing us to take advantage of <a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487">Async Tensor Parallelism (Async-TP)</a>. This is a relatively new feature in PyTorch that can overlap inter-GPU communication with other computations, partially hiding the cost of communication. In our benchmarks, we found that Async-TP adds a lot of CPU overhead to the model forward pass and only starts improving throughput with very large batch sizes (e.g. 6k+ tokens). Tokasaurus maintains torch-compiled versions of our models with and without Async-TP enabled, allowing us to automatically switch on Async-TP whenever the batch size is big enough:</p>

<p><img src="https://scalingintelligence.stanford.edu/imgs/blog/tokasaurus/big.png" alt="Tokasaurus small models">
</p>

<hr>

<h2 id="try-it-out">Try it Out</h2>

<p>Tokasaurus started as an internal lab effort to run our inference experiments faster, and we’re excited to share it more broadly! You can check out the Tokasaurus code on <a href="https://github.com/ScalingIntelligence/tokasaurus/tree/main">GitHub</a> and install the package from PyPI with:</p>



<p>Currently, we support models from the Llama-3 and Qwen-2 families and support any combination of data, tensor, and pipeline parallel within a single node.</p>

<p>Tokasaurus is written in pure Python (although we do use attention and sampling ops from the excellent <a href="https://arxiv.org/abs/2501.01005">FlashInfer</a> package). We hope that this makes the engine easier to fork and hack on, à la <a href="https://github.com/pytorch-labs/gpt-fast">GPT-fast</a>.</p>

<h2 id="benchmarking-details">Benchmarking Details</h2>

<p>The commands for reproducing our benchmarks are available <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/main/logs/blog_commands.md">here</a>. For each benchmark, we configure all engines with the same KV cache size and maximum number of running requests. We’ve made a best effort to tune each engine’s remaining parameters. We report the average throughput across runs after completing a warmup run. For each benchmark, all engines are run on the same machine.</p>

<p>We use <a href="https://github.com/sgl-project/sglang/blob/7e257cd666c0d639626487987ea8e590da1e9395/python/sglang/bench_serving.py">this script</a> from SGLang for our ShareGPT benchmarks and <a href="https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/monkeys_gsm8k.py">this custom script</a> for the Large Language Monkeys benchmark. To standardize our benchmarking scripts and interface, all experiments send requests through the OpenAI API. We also experimented with vLLM’s Python API (i.e. <code>LLM.generate()</code>) on the Large Language Monkeys benchmark with Llama-1B and measured roughly a 5% throughput increase (thanks to the vLLM team for the tip!).</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>Huge thanks to <a href="https://www.primeintellect.ai/">Prime Intellect</a> and <a href="https://www.together.ai/">Together AI</a> for providing us with compute for this project.</p>

<p>Also, we’re grateful to Dan Biderman, Simon Guo, Manat Kaur, and Avanika Narayan for beta testing the engine!</p>

<hr>

<p>If you find Tokasaurus useful, please use the following citation:</p>

<div><pre><code><span>@misc</span><span>{</span><span>juravsky2025tokasaurus</span><span>,</span>
  <span>author</span>       <span>=</span> <span>{Jordan Juravsky and Ayush Chakravarthy and Ryan Ehrlich and Sabri Eyuboglu and Bradley Brown and Joseph Shetaye and Christopher R{\'e} and Azalia Mirhoseini}</span><span>,</span>
  <span>title</span>        <span>=</span> <span>{Tokasaurus: An LLM Inference Engine for High-Throughput Workloads}</span><span>,</span>
  <span>year</span>         <span>=</span> <span>{2025}</span><span>,</span>
  <span>howpublished</span> <span>=</span> <span>{\url{https://scalingintelligence.stanford.edu/blogs/tokasaurus/}}</span>
<span>}</span>
</code></pre></div>


          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[APL Interpreter – An implementation of APL, written in Haskell (2024) (120 pts)]]></title>
            <link>https://scharenbroch.dev/projects/apl-interpreter/</link>
            <guid>44195931</guid>
            <pubDate>Thu, 05 Jun 2025 21:22:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scharenbroch.dev/projects/apl-interpreter/">https://scharenbroch.dev/projects/apl-interpreter/</a>, See on <a href="https://news.ycombinator.com/item?id=44195931">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>APL Interpreter</h2><h4>January 11, 2024</h4><h3 id="github-linkhttpsgithubcomlucasscharenbrochapl-interpreter">(<a href="https://github.com/lucasscharenbroch/apl-interpreter" target="_blank">Github Link</a>)</h3><h2 id="why-apl">Why APL?</h2><p><a href="https://en.wikipedia.org/wiki/APL_%28programming_language%29" target="_blank">APL</a> is an <em>array</em> programming language<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.
Its <strong>only data type</strong> is the (multidimensional) <em>array</em>.
While this might seem like a huge limitation, the generality it provides leads to a syntax that is incredibly compact and expressive, which forces the programmer to approach problems at a higher level.</p><p>I was first drawn to APL by the nature of its syntax: with the exception of user-defined variables, all built-in functions and operators are single<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> unicode symbols.
As a result, code looks like this.</p><div><pre tabindex="0"><code data-lang="apl"><span><span><span>⍝ My solution to day 7 of Advent of Code '23</span>
</span></span><span><span>
</span></span><span><span>c2n <span>←</span> <span>'23456789TJQKA'</span><span>∘</span><span>⍳</span>
</span></span><span><span>classify <span>←</span> (<span>5</span><span>-≢</span><span>⍤</span><span>∪</span>)<span>,</span>(<span>⌈</span><span>/</span>(<span>+</span><span>/∘.</span><span>=</span><span>⍨</span>))<span>,</span>((<span>5</span><span>≡∪</span>)<span>×</span>(<span>⌈</span><span>/</span>c2n))
</span></span><span><span>h2n <span>←</span> <span>13</span><span>⊥</span>classify<span>,</span>(<span>¯1</span><span>∘</span><span>+</span>c2n)
</span></span><span><span>
</span></span><span><span>hands bids <span>←</span> <span>↓⍉↑</span>((<span>' '</span><span>∘</span><span>≠</span>)<span>⊆⊢</span>)<span>¨</span> input
</span></span><span><span>bids <span>←</span> <span>⍎</span><span>¨</span> bids
</span></span><span><span>
</span></span><span><span>⎕ <span>←</span> <span>+</span><span>/</span> (<span>⍳≢</span>bids) <span>×</span> bids<span>[</span><span>⍋</span> h2n<span>¨</span> hands<span>]</span>
</span></span><span><span>
</span></span><span><span>modes <span>←</span> <span>{</span> ⍵<span>≡</span><span>⍬</span> : <span>⍬</span> ⋄ <span>∪</span>⍵<span>⌷</span><span>⍨</span><span>⊂</span>(<span>⍸⌈</span><span>/</span><span>=⊢</span>)<span>+</span><span>/∘.</span><span>=</span><span>⍨</span>⍵ <span>}</span>
</span></span><span><span>change_joker <span>←</span> <span>{</span> (<span>⊃</span> <span>'A'</span> <span>,</span><span>⍨</span> modes (⍵<span>~</span><span>'J'</span>))<span>@</span>(<span>⍸</span>⍵<span>=</span><span>'J'</span>) <span>⊢</span> ⍵ <span>}</span>
</span></span><span><span>
</span></span><span><span>c2n <span>←</span> <span>'J23456789TQKA'</span><span>∘</span><span>⍳</span>
</span></span><span><span>h2n <span>←</span> <span>13</span><span>⊥</span>(classify change_joker)<span>,</span>(<span>¯1</span><span>∘</span><span>+</span>c2n)
</span></span><span><span>
</span></span><span><span>⎕ <span>←</span> <span>+</span><span>/</span> (<span>⍳≢</span>bids) <span>×</span> bids<span>[</span><span>⍋</span> h2n<span>¨</span> hands<span>]</span></span></span></code></pre></div><p>As you might expect, learning to write programs like this<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> requires a totally different mind-set.
Array programming is similar to functional programming – the primary way to control execution involves composition of functions – but APL tends to encourage the reliance on global properties and sweeping operations rather than low-level recursion<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p><p>I’d like to think that learning to approach problems this way provides greater insights for programming in general.</p><h2 id="why-haskell">Why Haskell?</h2><p>I originally planned this project to be a deep-dive into APL, and learning Haskell was more of a side-quest.
It ended up the other way around: the hardest aspect of this project, by far, was learning work with Haskell<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>.</p><p>In all honesty, Haskell probably isn’t an ideal tool for an array-language interpreter:
it makes parsing and combination of functions much more elegant, but at the expense of
ease of working with state, data structures, and performance.</p><p>As a result, this project isn’t intended to be especially practically useful, nor to be a replacement for existing interpreters.</p><h2 id="the-big-picture">The Big Picture</h2><p>The program at large works exactly how you would expect any interpreter to work.</p><ul><li>Read Text As Input</li><li>Convert Raw Text to Tokens (Lexing/Scanning)</li><li>Convert Token Stream Into Syntax Tree (Parsing)</li><li>Evaluate the parsed tree</li><li>Print the result</li><li>Repeat</li></ul><p>The interpreter state (a mapping from variable names to values) is read/updated throughout.</p><h2 id="parsing">Parsing</h2><p>Haskell has some powerful parsing libraries, but I decided to write my parser from scratch, partly to make sure I really understood what the code was doing, and partly because I was scared off by the type signatures<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup> of said libraries.
I initially attempted to write the parser in a similar style to those in my <a href="https://scharenbroch.dev/projects/bash-with-floats">other</a> <a href="https://scharenbroch.dev/projects/graphing-calculator">projects</a>, but was forced into using a series of helper “match-functions” to translate the imperative code into a functional style.
Then, through a series of refactors, the majority of those helpers dissolved into calls to standard Haskell functions, resulting in syntax very similar to the aforementioned parsing libraries.</p><h3 id="parser-version-1-context-free">Parser, Version 1 (Context-Free)</h3><p>The core unit of logic in the parser if the <strong><code>MatchFn</code></strong>. Its definition changed across the versions of the parser, but its main purpose did not: it takes a list of tokens as input, and possibly returns the thing that was matched, along with the new list of tokens.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>type</span> <span>MatchFn</span> a <span>=</span> [<span>Token</span>] <span>-&gt;</span> <span>Maybe</span> (a, [<span>Token</span>])</span></span></code></pre></div><p><strong><code>MatchFn</code></strong>’s for <a href="https://en.wikipedia.org/wiki/Terminal_and_nonterminal_symbols" target="_blank">terminals</a> can now be trivially described.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>data</span> <span>Token</span> <span>=</span> <span>NumTok</span> <span>Double</span>
</span></span><span><span>           <span>|</span> <span>StrTok</span> <span>String</span>
</span></span><span><span>           <span>|</span> <span>IdTok</span> <span>String</span>
</span></span><span><span>           <span>|</span> <span>ChTok</span> <span>Char</span>
</span></span><span><span>
</span></span><span><span><span>matchCh</span> <span>::</span> <span>Char</span> <span>-&gt;</span> <span>MatchFn</span> <span>Char</span>
</span></span><span><span><span>matchCh</span> c (<span>ChTok</span> c'<span>:</span>ts)
</span></span><span><span>    <span>|</span> c <span>==</span> c' <span>=</span> <span>Just</span> (c, ts)
</span></span><span><span>    <span>|</span> otherwise <span>=</span> <span>Nothing</span>
</span></span><span><span><span>matchCh</span> <span>_</span> <span>_</span> <span>=</span> <span>Nothing</span>
</span></span><span><span>
</span></span><span><span><span>matchId</span> <span>::</span> <span>MatchFn</span> <span>String</span>
</span></span><span><span><span>matchId</span> (<span>IdTok</span> s<span>:</span>ts) <span>=</span> <span>Just</span> (s, ts)
</span></span><span><span><span>matchId</span> <span>_</span> <span>=</span> <span>Nothing</span>
</span></span><span><span>
</span></span><span><span><span>matchStrLiteral</span> <span>::</span> <span>MatchFn</span> <span>String</span>
</span></span><span><span><span>matchStrLiteral</span> (<span>StrTok</span> s<span>:</span>ts) <span>=</span> <span>Just</span> (s, ts)
</span></span><span><span><span>matchStrLiteral</span> <span>_</span> <span>=</span> <span>Nothing</span>
</span></span><span><span>
</span></span><span><span><span>matchNumLiteral</span> <span>::</span> <span>MatchFn</span> <span>Double</span>
</span></span><span><span><span>matchNumLiteral</span> (<span>NumTok</span> n<span>:</span>ts) <span>=</span> <span>Just</span> (n, ts)
</span></span><span><span><span>matchNumLiteral</span> <span>_</span> <span>=</span> <span>Nothing</span></span></span></code></pre></div><sup id="fnref:7"><a href="#fn:7" role="doc-noteref">7</a></sup><p>From here, it isn’t really obvious how to combine the above functions to parse arbitrary nonterminals.
This is addressed by adding a few more helpers that combine (chain) <strong><code>MatchFn</code></strong>’s.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>matchOne</span> <span>::</span> [<span>MatchFn</span> a] <span>-&gt;</span> <span>MatchFn</span> a
</span></span><span><span><span>-- return first successful match, else Nothing</span>
</span></span><span><span><span>matchOne</span> fns toks <span>=</span> foldl try <span>Nothing</span> fns
</span></span><span><span>    <span>where</span> try (<span>Just</span> x) <span>_</span> <span>=</span> <span>Just</span> x <span>-- already found match</span>
</span></span><span><span>          try <span>Nothing</span> f <span>=</span> f toks
</span></span><span><span>
</span></span><span><span><span>matchAll</span> <span>::</span> [<span>MatchFn</span> a] <span>-&gt;</span> <span>MatchFn</span> [a]
</span></span><span><span><span>-- match every function in list (sequentially), returning their results, else Nothing</span>
</span></span><span><span><span>matchAll</span> fns toks <span>=</span> chFst (reverse) <span>.</span> foldl try (<span>Just</span> (<span>[]</span>, toks)) <span>$</span> fns
</span></span><span><span>    <span>where</span> try <span>Nothing</span> <span>_</span> <span>=</span> <span>Nothing</span>
</span></span><span><span>          try (<span>Just</span> (rs, ts)) f <span>=</span> <span>case</span> f ts <span>of</span>
</span></span><span><span>              <span>Just</span> (r, ts') <span>-&gt;</span> <span>Just</span>(r<span>:</span>rs, ts')
</span></span><span><span>              <span>_</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>
</span></span><span><span><span>matchMax</span> <span>::</span> [<span>MatchFn</span> a] <span>-&gt;</span> <span>MatchFn</span> [[a]]
</span></span><span><span><span>-- match 0 or more repetitions of the entire function list</span>
</span></span><span><span><span>matchMax</span> fns toks <span>=</span> <span>case</span> matchAll fns toks <span>of</span>
</span></span><span><span>    <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>    <span>Just</span> (r, ts) <span>-&gt;</span> <span>case</span> matchMax fns ts <span>of</span>
</span></span><span><span>        <span>Nothing</span> <span>-&gt;</span> <span>Just</span> ([r], ts)
</span></span><span><span>        <span>Just</span> (rs, ts') <span>-&gt;</span> <span>Just</span> (r<span>:</span>rs, ts')
</span></span><span><span>
</span></span><span><span><span>matchAllThenMax</span> <span>::</span> [<span>MatchFn</span> a] <span>-&gt;</span> <span>MatchFn</span> [[a]]
</span></span><span><span><span>-- ... similar definition</span></span></span></code></pre></div><p>The <strong><code>chFst</code></strong> function is a convenient way to conditionally apply an arbitrary function to the result of a MatchFn (<strong><code>Maybe (a, [Token])</code></strong>) without doing a case-analysis.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>chFst</span> <span>::</span> (a <span>-&gt;</span> b) <span>-&gt;</span> <span>Maybe</span> (a, c) <span>-&gt;</span> <span>Maybe</span> (b, c)
</span></span><span><span><span>-- chain first: apply function to first element of maybe-wrapped tuple</span>
</span></span><span><span><span>chFst</span> f m <span>=</span> <span>case</span> m <span>of</span>
</span></span><span><span>    <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>    <span>Just</span> (x, y) <span>-&gt;</span> <span>Just</span> (f x, y)</span></span></code></pre></div><p>There’s also another variant, <strong><code>mchFst</code></strong>, which allows the chaining-function to fail (return Nothing), in which case the entire construct also becomes nothing.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>mchFst</span> <span>::</span> (a <span>-&gt;</span> <span>Maybe</span> b) <span>-&gt;</span> <span>Maybe</span> (a, c) <span>-&gt;</span> <span>Maybe</span> (b, c)
</span></span><span><span><span>-- maybe chain first</span>
</span></span><span><span><span>mchFst</span> f m <span>=</span> <span>case</span> m <span>of</span>
</span></span><span><span>    <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>    <span>Just</span> (x, y) <span>-&gt;</span> <span>case</span> (f x) <span>of</span>
</span></span><span><span>        <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>        <span>Just</span> z <span>-&gt;</span> <span>Just</span>(z, y)</span></span></code></pre></div><p>All of the above helper match-functions only deal with combining homogeneous <strong><code>MatchFn</code></strong>’s (i.e. <strong><code>MatchFn Int</code></strong> and <strong><code>MatchFn Int</code></strong>, not <strong><code>MatchFn String</code></strong> and <strong><code>MatchFn Int</code></strong>).
Tuples are one solution to this.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>matchT2</span> <span>::</span> (<span>MatchFn</span> a, <span>MatchFn</span> b) <span>-&gt;</span> <span>MatchFn</span> (a, b)
</span></span><span><span><span>matchT2</span> (fa, fb) ts <span>=</span> <span>case</span> fa ts <span>of</span>
</span></span><span><span>    <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>    <span>Just</span> (a, ts') <span>-&gt;</span> <span>case</span> fb ts' <span>of</span>
</span></span><span><span>        <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>        <span>Just</span> (b, ts'') <span>-&gt;</span> <span>Just</span> ((a, b), ts'')
</span></span><span><span>
</span></span><span><span><span>matchT3</span> <span>::</span> (<span>MatchFn</span> a, <span>MatchFn</span> b, <span>MatchFn</span> c) <span>-&gt;</span> <span>MatchFn</span> (a, b, c)
</span></span><span><span><span>matchT3</span> (fa, fb, fc) ts <span>=</span> <span>case</span> fa ts <span>of</span>
</span></span><span><span>    <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>    <span>Just</span> (a, ts') <span>-&gt;</span> <span>case</span> fb ts' <span>of</span>
</span></span><span><span>        <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>        <span>Just</span> (b, ts'') <span>-&gt;</span> <span>case</span> fc ts'' <span>of</span>
</span></span><span><span>            <span>Nothing</span> <span>-&gt;</span> <span>Nothing</span>
</span></span><span><span>            <span>Just</span> (c, ts''') <span>-&gt;</span> <span>Just</span> ((a, b, c), ts''')
</span></span><span><span>
</span></span><span><span><span>matchT4</span> <span>::</span> (<span>MatchFn</span> a, <span>MatchFn</span> b, <span>MatchFn</span> c, <span>MatchFn</span> d) <span>-&gt;</span> <span>MatchFn</span> (a, b, c, d)
</span></span><span><span><span>-- ... etc.</span></span></span></code></pre></div><p>These helpers alone make for a relatively elegant<sup id="fnref:8"><a href="#fn:8" role="doc-noteref">8</a></sup> parser, with syntax kind of similar to <a href="https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form" target="_blank">BNF</a> (or at least a lot closer than an average imperative implementation).</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>-- der_arr =&gt; train der_arr</span>
</span></span><span><span><span>--         =&gt; arr der_fn der_arr</span>
</span></span><span><span><span>--         =&gt; arr</span>
</span></span><span><span>
</span></span><span><span><span>parseDerArr</span> <span>::</span> <span>MatchFn</span> <span>ArrTreeNode</span>
</span></span><span><span><span>parseDerArr</span> <span>=</span> matchOne [
</span></span><span><span>        chFst (<span>\</span>(t, da) <span>-&gt;</span> <span>ArrInternalMonFn</span> t da) <span>.</span> matchT2 (parseTrain, parseDerArr),
</span></span><span><span>        chFst (<span>\</span>(lhs, f, rhs) <span>-&gt;</span> <span>ArrInternalDyadFn</span> f lhs rhs) <span>.</span> matchT3 (
</span></span><span><span>            parseArr,
</span></span><span><span>            parseDerFn,
</span></span><span><span>            parseDerArr
</span></span><span><span>        ),
</span></span><span><span>        parseArr
</span></span><span><span>    ]</span></span></code></pre></div><sup id="fnref:9"><a href="#fn:9" role="doc-noteref">9</a></sup><h3 id="parser-version-2-context">Parser, Version 2 (+Context)</h3><p>It turns out that APL doesn’t have a <a href="https://en.wikipedia.org/wiki/Context-free_grammar" target="_blank">context-free grammar</a>.<sup id="fnref:10"><a href="#fn:10" role="doc-noteref">10</a></sup>
This means that it requires more information than the tokens alone to form the syntax tree, namely the values of variables.</p><p>This foils the current definition of <strong><code>MatchFn</code></strong>: because Haskell is purely functional, <strong><code>MatchFn</code></strong>’s must solely deal in their arguments<sup id="fnref:11"><a href="#fn:11" role="doc-noteref">11</a></sup> (otherwise they have side-effects), so the global state (<strong><code>IdMap</code></strong>) must be added as an argument to <strong><code>MatchFn</code></strong>. Parsing shouldn’t modify the <strong><code>IdMap</code></strong>, so the return value can remain the same.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>type</span> <span>MatchFn</span> a <span>=</span> (<span>IdMap</span>, [<span>Token</span>]) <span>-&gt;</span> <span>Maybe</span> (a, [<span>Token</span>])</span></span></code></pre></div><sup id="fnref:12"><a href="#fn:12" role="doc-noteref">12</a></sup><p>The beauty of this change is that only the terminal matching functions and the matching helpers (only the functions with “match” as a prefix) need to be changed – this is because the nonterminal parsing (“parse-”) functions never actually directly touch to the arguments to MatchFn: they solely serve as <em>definitions of combinations</em> of helper functions<sup id="fnref:13"><a href="#fn:13" role="doc-noteref">13</a></sup>.</p><p>The “match-” functions can be easily updated, with some minor syntactic inconvenience.</p><div><pre tabindex="0"><code data-lang="diff"><span><span> matchCh :: Char -&gt; MatchFn Char
</span></span><span><span><span>-matchCh c (ChTok c':ts)
</span></span></span><span><span><span></span><span>+matchCh c (_, (ChTok c':ts))
</span></span></span><span><span><span></span>     | c == c' = Just (c, ts)
</span></span><span><span>     | otherwise = Nothing
</span></span><span><span> matchCh _ _ = Nothing
</span></span><span><span>
</span></span><span><span>
</span></span><span><span> matchStrLiteral :: MatchFn String
</span></span><span><span><span>-matchStrLiteral (StrTok s:ts) = Just (s, ts)
</span></span></span><span><span><span></span><span>+matchStrLiteral (_, (StrTok s:ts)) = Just (s, ts)
</span></span></span><span><span><span></span> matchStrLiteral _ = Nothing
</span></span><span><span>
</span></span><span><span> -- ... etc.
</span></span><span><span>
</span></span><span><span> matchOne :: [MatchFn a] -&gt; MatchFn a
</span></span><span><span> -- return first successful match, else Nothing
</span></span><span><span><span>-matchOne fns toks = foldl try Nothing fns
</span></span></span><span><span><span></span><span>+matchOne fns args = foldl try Nothing fns
</span></span></span><span><span><span></span>     where try (Just x) _ = Just x -- already found match
</span></span><span><span><span>-          try Nothing f = f toks
</span></span></span><span><span><span></span><span>+          try Nothing f = f args
</span></span></span><span><span><span></span>
</span></span><span><span> matchAll :: [MatchFn a] -&gt; MatchFn [a]
</span></span><span><span> -- match every function in list (sequentially), returning their results, else Nothing
</span></span><span><span><span>-matchAll fns toks = chFst (reverse) . foldl try (Just ([], toks)) $ fns
</span></span></span><span><span><span></span><span>+matchAll fns (idm, toks) = chFst (reverse) . foldl try (Just ([], toks)) $ fns
</span></span></span><span><span><span></span>     where try Nothing _ = Nothing
</span></span><span><span><span>-          try (Just (rs, ts)) f = case f ts of
</span></span></span><span><span><span></span><span>+          try (Just (rs, ts)) f = case f (idm, ts) of
</span></span></span><span><span><span></span>               Just (r, ts') -&gt; Just(r:rs, ts')
</span></span><span><span>               _ -&gt; Nothing
</span></span><span><span>
</span></span><span><span> -- ... etc.
</span></span><span><span>
</span></span><span><span> matchT2 :: (MatchFn a, MatchFn b) -&gt; MatchFn (a, b)
</span></span><span><span><span>-matchT2 (fa, fb) ts = case fa ts of
</span></span></span><span><span><span></span><span>+matchT2 (fa, fb) (idm, ts) = case fa (idm, ts) of
</span></span></span><span><span><span></span>     Nothing -&gt; Nothing
</span></span><span><span><span>-    Just (a, ts') -&gt; case fb ts' of
</span></span></span><span><span><span></span><span>+    Just (a, ts') -&gt; case fb (idm, ts') of
</span></span></span><span><span><span></span>         Nothing -&gt; Nothing
</span></span><span><span>         Just (b, ts'') -&gt; Just ((a, b), ts'')
</span></span><span><span> -- ... etc.
</span></span></code></pre></div><h3 id="parser-version-3-monads">Parser, Version 3 (+Monads)</h3><p>Throughout writing and refactoring the parser, while I was generally happy with its functionality, there were three main inconveniences that bothered me.</p><ol><li>The existence of the <strong><code>parseT</code></strong> functions: they are too hard-coded. There must be a more general way to fix this.</li><li>The existence and syntactic clumsiness of <strong><code>chFst</code></strong> and <strong><code>mchFst</code></strong>.</li><li><strong><code>parseDerFn</code></strong></li></ol><p>The majority of the “parse-” functions were relatively concise and readable, but there was one particular function that had a heavy reliance on the global state (IdMap) that made its implementation monstrous.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>-- der_fn =&gt; (f|a) op [f|a] {op [f|a]}       (where (f|a) is fn or arr; match the</span>
</span></span><span><span><span>--                                            optional iff op is dyadic)</span>
</span></span><span><span>
</span></span><span><span><span>parseDerFn</span> <span>::</span> <span>MatchFn</span> <span>FnTreeNode</span>
</span></span><span><span><span>parseDerFn</span> (idm, ts) <span>=</span> matchOne [
</span></span><span><span>        (<span>=&lt;&lt;</span>) (parseDerFnRec) <span>.</span> (<span>=&lt;&lt;</span>) (finishOpMatch) <span>.</span> matchT2 (_parseArg, parseOp),
</span></span><span><span>        parseFn
</span></span><span><span>    ] (idm, ts)
</span></span><span><span>    <span>where</span> _parseArg <span>=</span> matchOne [parseFn, chFst (<span>FnLeafArr</span>) <span>.</span> parseArr]
</span></span><span><span>          finishOpMatch <span>::</span> ((<span>FnTreeNode</span>, <span>Operator</span>), [<span>Token</span>]) <span>-&gt;</span> <span>Maybe</span> (<span>FnTreeNode</span>, [<span>Token</span>])
</span></span><span><span>          finishOpMatch ((lhs, op<span>@</span>(<span>DyadOp</span> <span>_</span> <span>_</span>)), toks) <span>=</span> chFst (<span>FnInternalDyadOp</span> op lhs) <span>$</span> _parseArg (idm, toks)
</span></span><span><span>          finishOpMatch ((lhs, op), toks) <span>=</span> <span>Just</span> (<span>FnInternalMonOp</span> op lhs, toks)
</span></span><span><span>          parseDerFnRec <span>::</span> (<span>FnTreeNode</span>, [<span>Token</span>]) <span>-&gt;</span> <span>Maybe</span> (<span>FnTreeNode</span>, [<span>Token</span>])
</span></span><span><span>          parseDerFnRec (lhs, toks) <span>=</span> <span>case</span> (<span>=&lt;&lt;</span>) (finishOpMatch) <span>.</span> chFst (<span>\</span>op <span>-&gt;</span> (lhs, op)) <span>$</span> parseOp (idm, toks) <span>of</span>
</span></span><span><span>              <span>Nothing</span> <span>-&gt;</span> <span>Just</span> (lhs, toks)
</span></span><span><span>              <span>Just</span> res <span>-&gt;</span> parseDerFnRec res</span></span></code></pre></div><p><strong><code>parseDerFn</code></strong> is the exception to the above statement that “parse-” functions never directly manipulate the argument to <strong><code>MatchFn</code></strong> (<strong><code>(IdMap, [Token])</code></strong>).
<strong><code>parseDerFn</code></strong> breaks this rule because it needs to have the <strong><code>IdMap</code></strong> to check whether the parsed operator is monadic or dyadic.
This can’t be done with <strong><code>chFst</code></strong> or <strong><code>mchFst</code></strong>, because neither takes the <strong>IdMap</strong> as an argument, which <strong><code>finishOpMatch</code></strong> requires.</p><p>At this point, I had already realized the similarity between <strong><code>mchFst</code></strong> and <strong><code>(=&lt;&lt;)</code></strong>.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span>(<span>=&lt;&lt;</span>) <span>::</span> (a <span>-&gt;</span> m b) <span>-&gt;</span> m a <span>-&gt;</span> m b
</span></span><span><span><span>-- (let a = (a, c); b = (b, c); m = Maybe in the definition of (=&lt;&lt;))</span>
</span></span><span><span><span>-- (=&lt;&lt;) :: ((a, c) -&gt; Maybe (b, c)) -&gt; Maybe (a, c) -&gt; Maybe (b, c)</span>
</span></span><span><span><span>-- let c = [Token]</span>
</span></span><span><span><span>-- (=&lt;&lt;) :: ((a, [Token]) -&gt; Maybe (b, [Token])) -&gt; Maybe (a, [Token]) -&gt; Maybe (b, [Token])</span>
</span></span><span><span>
</span></span><span><span><span>mchFst</span> <span>::</span> (a <span>-&gt;</span> <span>Maybe</span> b) <span>-&gt;</span> <span>Maybe</span> (a, c) <span>-&gt;</span> <span>Maybe</span> (b, c)
</span></span><span><span><span>-- let c = [Token]</span>
</span></span><span><span><span>-- mchFst :: (a -&gt; Maybe b) -&gt; Maybe (a, [Token]) -&gt; Maybe (b, [Token])</span>
</span></span><span><span>
</span></span><span><span><span>-- (=&lt;&lt;) ::  ((a, [Token]) -&gt; Maybe (b, [Token])) -&gt; Maybe (a, [Token]) -&gt; Maybe (b, [Token])</span>
</span></span><span><span><span>-- mchFst :: (a            -&gt; Maybe b           ) -&gt; Maybe (a, [Token]) -&gt; Maybe (b, [Token])</span></span></span></code></pre></div><p>So <strong><code>mchFst</code></strong> is just a specialized version of <strong><code>(=&lt;&lt;)</code></strong> that assumes that the token-list remains unchanged.</p><p>I decided I ought to try to figure out what other patterns in my parser were mimicking those of built-in monadic functions<sup id="fnref:14"><a href="#fn:14" role="doc-noteref">14</a></sup>, and attempt to refactor accordingly.</p><p><strong><code>chFst</code></strong> and <strong><code>mchFst</code></strong> could not directly be substituted for monadic functions, because they heavily rely on the knowledge that the value they receive is a tuple (wrapped in a Maybe), and that the first element of that tuple is the one that gets modified, while the second element (the <strong><code>[Token]</code></strong>) stays the same<sup id="fnref:15"><a href="#fn:15" role="doc-noteref">15</a></sup>.</p><p>The key point here is that <strong><code>chFst</code></strong> and <strong><code>mchFst</code></strong> are <em>not general enough</em>. Their main use is to (in conjunction with composition) convert a <strong><code>MatchFn</code></strong> parameterized by one type to a <strong><code>MatchFn</code></strong> parameterized by another (see below).
But they assume that that function (<strong><code>f</code></strong>) only deals with the type parameter of <strong><code>MatchFn</code></strong> (<strong><code>a</code></strong>/<strong><code>b</code></strong>), and has no effect on the <strong><code>[Token]</code></strong>, and cannot read the <strong><code>IdMap</code></strong>.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>-- (MatchFn from version 2)</span>
</span></span><span><span><span>type</span> <span>MatchFn</span> a <span>=</span> (<span>IdMap</span>, [<span>Token</span>]) <span>-&gt;</span> <span>Maybe</span> (a, [<span>Token</span>])
</span></span><span><span>
</span></span><span><span><span>mfFmap</span>  f <span>=</span> (<span>.</span>) (chFst  f) <span>::</span> (a <span>-&gt;</span>       b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBind</span> f <span>=</span> (<span>.</span>) (mchFst f) <span>::</span> (a <span>-&gt;</span> <span>Maybe</span> b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b</span></span></code></pre></div><sup id="fnref:16"><a href="#fn:16" role="doc-noteref">16</a></sup><p>This lack of generality is clearly the cause of inconvenience #3.</p><p>One solution to this is to expand the definition of <strong><code>MatchFn</code></strong> to also return an IdMap (so chained functions can access it), and to write a few new chaining helper-functions that allow access to different parts of the returned tuple.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>type</span> <span>MatchFn</span> a <span>=</span> (<span>IdMap</span>, [<span>Token</span>]) <span>-&gt;</span> <span>Maybe</span> (a, [<span>Token</span>], <span>IdMap</span>)
</span></span><span><span>
</span></span><span><span><span>mfFmap</span>            <span>::</span> (         a <span>-&gt;</span>         b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBindMaybe</span>      <span>::</span> (         a <span>-&gt;</span> <span>Maybe</span>   b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBindIdm</span>        <span>::</span> (<span>IdMap</span> <span>-&gt;</span> a <span>-&gt;</span>         b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBindIdm'</span>       <span>::</span> (<span>IdMap</span> <span>-&gt;</span> a <span>-&gt;</span> <span>MatchFn</span> b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBindIdmMaybe</span>   <span>::</span> (<span>IdMap</span> <span>-&gt;</span> a <span>-&gt;</span> <span>Maybe</span>   b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b
</span></span><span><span><span>mfFBind</span>           <span>::</span> (         a <span>-&gt;</span> <span>MatchFn</span> b) <span>-&gt;</span> <span>MatchFn</span> a <span>-&gt;</span> <span>MatchFn</span> b</span></span></code></pre></div><p>This isn’t a very good solution, though, as it adds a lot of boilerplate (much of which is rarely used), and it doesn’t fix the other inconveniences.
Additionally, by adding <strong><code>IdMap</code></strong> to the return type, it introduces the (albeit unlikely) possibility that a parsing function may modify the <strong><code>IdMap</code></strong>.</p><p>A better solution is to change <strong><code>MatchFn</code></strong> into a monad whose functions exactly match the desired behavior, then to use generic monadic functions instead of chFst and mchFst.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>type</span> <span>MatchFn</span> a <span>=</span> <span>StateT</span> [<span>Token</span>] (<span>MaybeT</span> (<span>Reader</span> <span>IdMap</span>)) a</span></span></code></pre></div><p>It’s probably a little steep to try to completely explain this type, so I’ll keep it high-level.
The names that end with ‘T’ are <a href="https://en.wikipedia.org/wiki/Monad_transformer" target="_blank">Monad Transformers</a>, which take another monad as one of their type parameters, to form a single resulting monad. Each of the following is a monad.</p><ul><li><strong><code>Reader IdMap</code></strong></li><li><strong><code>MaybeT (Reader IdMap)</code></strong></li><li><strong><code>StateT [Token] (MaybeT (Reader IdMap))</code></strong></li></ul><p>Monad transformers preserve the behavior of the monad they receive, and add their own behavior on top<sup id="fnref:17"><a href="#fn:17" role="doc-noteref">17</a></sup>.</p><p>It’s interesting to note that <strong><code>MatchFn</code></strong> itself is no longer a function: it is a monad whose state internal state is a function<sup id="fnref:18"><a href="#fn:18" role="doc-noteref">18</a></sup>.</p><p>This requires another reworking of the “match-” functions, but the “parse-” functions can (again) stay mostly the same, except <strong><code>(&lt;$&gt;)</code></strong> (fmap) replaces <strong><code>chFst</code></strong>, and <strong><code>(=&lt;&lt;)</code></strong> (or <strong><code>(&gt;&gt;=)</code></strong> or do-notation) replaces <strong><code>mchFst</code></strong>.</p><div><pre tabindex="0"><code data-lang="diff"><span><span><span>+evalMatchFn :: IdMap -&gt; [Token] -&gt; MatchFn a -&gt; Maybe (a, [Token])
</span></span></span><span><span><span>+evalMatchFn idm toks = (flip runReader) idm . runMaybeT . (flip runStateT) toks
</span></span></span><span><span><span></span>
</span></span><span><span><span>+maybeMatch :: MatchFn a -&gt; MatchFn (Maybe a)
</span></span></span><span><span><span>+maybeMatch f = do
</span></span></span><span><span><span>+    idm &lt;- getIdm
</span></span></span><span><span><span>+    toks &lt;- get
</span></span></span><span><span><span>+    case evalMatchFn idm toks f of
</span></span></span><span><span><span>+        Nothing -&gt; return Nothing
</span></span></span><span><span><span>+        Just (x, toks') -&gt; do
</span></span></span><span><span><span>+            put toks'
</span></span></span><span><span><span>+            return $ Just x
</span></span></span><span><span><span></span>
</span></span><span><span> matchOne :: [MatchFn a] -&gt; MatchFn a
</span></span><span><span> -- return first successful match, else Nothing
</span></span><span><span><span>-matchOne fns args = foldl try Nothing fns
</span></span></span><span><span><span>-    where try (Just x) _ = Just x -- already found match
</span></span></span><span><span><span>-          try Nothing f = f args
</span></span></span><span><span><span></span><span>+matchOne [] = mzero
</span></span></span><span><span><span>+matchOne (f:fs) = do
</span></span></span><span><span><span>+    mb &lt;- maybeMatch f
</span></span></span><span><span><span>+    case mb of
</span></span></span><span><span><span>+        Nothing -&gt; matchOne fs
</span></span></span><span><span><span>+        Just x -&gt; return x
</span></span></span><span><span><span></span>
</span></span><span><span> matchAll :: [MatchFn a] -&gt; MatchFn [a]
</span></span><span><span><span>--- match every function in list (sequentially), returning their results, else Nothing
</span></span></span><span><span><span>-matchAll fns (idm, toks) = chFst (reverse) . foldl try (Just ([], toks)) $ fns
</span></span></span><span><span><span>-    where try Nothing _ = Nothing
</span></span></span><span><span><span>-          try (Just (rs, ts)) f = case f (idm, ts) of
</span></span></span><span><span><span>-              Just (r, ts') -&gt; Just(r:rs, ts')
</span></span></span><span><span><span>-              _ -&gt; Nothing
</span></span></span><span><span><span></span><span>+matchAll [] = return []
</span></span></span><span><span><span>+matchAll (f:fs) = do
</span></span></span><span><span><span>+    a &lt;- f
</span></span></span><span><span><span>+    as &lt;- matchAll fs
</span></span></span><span><span><span>+    return $ a:as
</span></span></span><span><span><span></span>
</span></span><span><span> -- ... etc.
</span></span><span><span>
</span></span><span><span> matchT2 :: (MatchFn a, MatchFn b) -&gt; MatchFn (a, b)
</span></span><span><span><span>-matchT2 (fa, fb) (idm, ts) = case fa (idm, ts) of
</span></span></span><span><span><span>-    Nothing -&gt; Nothing
</span></span></span><span><span><span>-    Just (a, ts') -&gt; case fb (idm, ts') of
</span></span></span><span><span><span>-        Nothing -&gt; Nothing
</span></span></span><span><span><span>-        Just (b, ts'') -&gt; Just ((a, b), ts'')
</span></span></span><span><span><span></span><span>+matchT2 (fa, fb) = do
</span></span></span><span><span><span>+    a &lt;- fa
</span></span></span><span><span><span>+    b &lt;- fb
</span></span></span><span><span><span>+    return (a, b)
</span></span></span><span><span><span></span>
</span></span><span><span> -- .. etc.
</span></span><span><span>
</span></span><span><span> parseOpOrFn :: MatchFn (Operator, FnTreeNode)
</span></span><span><span> parseOpOrFn = matchOne [
</span></span><span><span><span>-        chFst (\_ -&gt; (oReduce, FnLeafFn fReplicate)) . matchCh '/',
</span></span></span><span><span><span>-        chFst (\_ -&gt; (oScan, FnLeafFn fExpand)) . matchCh '\\',
</span></span></span><span><span><span>-        chFst (\_ -&gt; (oReduceFirst, FnLeafFn fReplicateFirst)) . matchCh '⌿',
</span></span></span><span><span><span>-        chFst (\_ -&gt; (oScanFirst, FnLeafFn fExpandFirst)) . matchCh '⍀'
</span></span></span><span><span><span></span><span>+        (\_ -&gt; (oReduce, FnLeafFn fReplicate)) &lt;$&gt; matchCh '/',
</span></span></span><span><span><span>+        (\_ -&gt; (oScan, FnLeafFn fExpand)) &lt;$&gt; matchCh '\\',
</span></span></span><span><span><span>+        (\_ -&gt; (oReduceFirst, FnLeafFn fReplicateFirst)) &lt;$&gt; matchCh '⌿',
</span></span></span><span><span><span>+        (\_ -&gt; (oScanFirst, FnLeafFn fExpandFirst)) &lt;$&gt; matchCh '⍀'
</span></span></span><span><span><span></span>     ]
</span></span><span><span>
</span></span><span><span> parseArr :: MatchFn ArrTreeNode -- parse an entire literal array
</span></span><span><span><span>-parseArr = chFst (_roll) . matchT2 (
</span></span></span><span><span><span></span><span>+parseArr = (_roll) &lt;$&gt; matchT2 (
</span></span></span><span><span><span></span>         parseArrComp,
</span></span><span><span><span>-        chFst (concat) . matchMax [ matchOne [
</span></span></span><span><span><span>-            chFst (Left) . parseArrComp,
</span></span></span><span><span><span>-            chFst (\(_, il, _) -&gt; Right il) . matchT3 (
</span></span></span><span><span><span></span><span>+        (concat) &lt;$&gt; matchMax [ matchOne [
</span></span></span><span><span><span>+            (Left) &lt;$&gt; parseArrComp,
</span></span></span><span><span><span>+            (\(_, il, _) -&gt; Right il) &lt;$&gt; matchT3 (
</span></span></span><span><span><span></span>                 matchCh '[',
</span></span><span><span>                 parseIdxList,
</span></span><span><span>                 matchCh ']'
</span></span><span><span>             )
</span></span><span><span>         ]]
</span></span><span><span>     )
</span></span><span><span>
</span></span><span><span> -- .. etc.
</span></span></code></pre></div><p>Evaluating <strong><code>mzero</code></strong> in the monad at any time causes the internal function to unconditionally return Nothing (effectively short-circuit).
This behavior comes from <strong><code>MaybeT</code></strong>.</p><p>The new version of parseDerFn:</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>parseDerFn</span> <span>=</span> matchOne [_parseOpExpr, parseFn]
</span></span><span><span>    <span>where</span> _parseOpExpr <span>=</span> <span>do</span>
</span></span><span><span>              lhs <span>&lt;-</span> _parseArg
</span></span><span><span>              otn <span>&lt;-</span> parseOp
</span></span><span><span>              _parseOpExprRec lhs otn
</span></span><span><span>          _parseArg <span>=</span> matchOne [parseFn, <span>FnLeafArr</span> <span>&lt;$&gt;</span> parseArr]
</span></span><span><span>          _parseOpExprRec <span>::</span> <span>FnTreeNode</span> <span>-&gt;</span> <span>OpTreeNode</span> <span>-&gt;</span> <span>MatchFn</span> <span>FnTreeNode</span>
</span></span><span><span>          _parseOpExprRec lhs otn <span>=</span> <span>do</span>
</span></span><span><span>              df <span>&lt;-</span> <span>case</span> unwrapOpTree otn <span>of</span>
</span></span><span><span>                        (<span>MonOp</span> <span>_</span> <span>_</span>) <span>-&gt;</span> return <span>$</span> <span>FnInternalMonOp</span> otn lhs
</span></span><span><span>                        (<span>DyadOp</span> <span>_</span> <span>_</span>) <span>-&gt;</span> <span>do</span> rhs <span>&lt;-</span> _parseArg
</span></span><span><span>                                           return <span>$</span> <span>FnInternalDyadOp</span> otn lhs rhs
</span></span><span><span>              mb <span>&lt;-</span> maybeMatch parseOp
</span></span><span><span>              <span>case</span> mb <span>of</span>
</span></span><span><span>                   <span>Nothing</span> <span>-&gt;</span> return <span>$</span> df
</span></span><span><span>                   (<span>Just</span> otn2) <span>-&gt;</span> _parseOpExprRec df otn2</span></span></code></pre></div><h3 id="parser-version-4-applicative">Parser, Version 4 (+Applicative)</h3><p>My favorite part about Haskell: realizing a built-in function does precisely what you want.</p><p>It turns out that, <a href="https://wiki.haskell.org/Functor-Applicative-Monad_Proposal" target="_blank">circa 2014</a>, all Monads in Haskell are also <a href="https://hackage.haskell.org/package/base-4.19.0.0/docs/Control-Applicative.html" target="_blank">Applicative Functors</a>.
<strong><code>(&lt;*&gt;)</code></strong>, <strong><code>(&lt;*)</code></strong>, and <strong><code>(*&gt;)</code></strong> remove the need for the <strong><code>MatchT</code></strong> functions, and the vast majority of lambdas (typically as the LHS’s of <strong><code>&lt;$&gt;</code></strong>).</p><div><pre tabindex="0"><code data-lang="diff"><span><span> parseDerArr :: MatchFn ArrTreeNode
</span></span><span><span> parseDerArr = matchOne [
</span></span><span><span>         parseArrAss,
</span></span><span><span><span>-        (\(f, da) -&gt; ArrInternalMonFn f da) &lt;$&gt; matchT2 (parseDerFn, parseDerArr),
</span></span></span><span><span><span>-        (\(lhs, f, rhs) -&gt; ArrInternalDyadFn f lhs rhs) &lt;$&gt; matchT3 (
</span></span></span><span><span><span>-            parseArr,
</span></span></span><span><span><span>-            parseDerFn,
</span></span></span><span><span><span>-            parseDerArr
</span></span></span><span><span><span>-        ),
</span></span></span><span><span><span></span><span>+        ArrInternalMonFn &lt;$&gt; parseDerFn &lt;*&gt; parseDerArr,
</span></span></span><span><span><span>+        (flip ArrInternalDyadFn) &lt;$&gt; parseArr &lt;*&gt; parseDerFn &lt;*&gt; parseDerArr,
</span></span></span><span><span><span></span>         parseArr
</span></span><span><span>     ]
</span></span><span><span>
</span></span><span><span> parseArrAss :: MatchFn ArrTreeNode
</span></span><span><span> parseArrAss = matchOne [
</span></span><span><span><span>-        (\(id, _, da) -&gt; ArrInternalAssignment id da) &lt;$&gt; matchT3 (
</span></span></span><span><span><span>-            matchId,
</span></span></span><span><span><span>-            matchCh '←',
</span></span></span><span><span><span>-            parseDerArr
</span></span></span><span><span><span>-        ),
</span></span></span><span><span><span>-        (\(id, df, _, da) -&gt; ArrInternalModAssignment id df da) &lt;$&gt; matchT4 (
</span></span></span><span><span><span>-            matchId,
</span></span></span><span><span><span>-            parseDerFn,
</span></span></span><span><span><span>-            matchCh '←',
</span></span></span><span><span><span>-            parseDerArr
</span></span></span><span><span><span>-        )
</span></span></span><span><span><span></span><span>+        ArrInternalAssignment &lt;$&gt; matchId &lt;*&gt; (matchCh '←' *&gt; parseDerArr),
</span></span></span><span><span><span>+        ArrInternalModAssignment &lt;$&gt; matchId &lt;*&gt; parseDerFn &lt;*&gt; (matchCh '←' *&gt; parseDerArr)
</span></span></span><span><span><span></span>     ]
</span></span><span><span>
</span></span><span><span> -- ... etc.
</span></span></code></pre></div><p>Version 4 is over 100 lines shorter than version 3.</p><h2 id="evaluation">Evaluation</h2><p>This project was relatively large in scope (at least in comparison to what I’m used to), so there are a lot of nuances in the evaluation of the syntax trees.
I’ve picked a few of the more-interesting ones to highlight.</p><h3 id="functions-as-data">Functions as Data</h3><p>APL makes combination of functions very natural; adjacent functions and operators alone form trees (even before application to arrays)<sup id="fnref:19"><a href="#fn:19" role="doc-noteref">19</a></sup>.</p><p>For example, here’s the tree for the function <strong><code>h2n</code></strong> (from an earlier example):</p><pre tabindex="0"><code>    c2n ← '23456789TJQKA'∘⍳
    classify ← (5-≢⍤∪),(⌈/(+/∘.=⍨)),((5≡∪)×(⌈/c2n))
    h2n ← 13⊥classify,(¯1∘+c2n)
    h2n
 ┌──┼────────────┐
 13 ⊥ ┌──────────┼────────────────────────┐
  ┌───┼────────┐ ,                   ┌────┴─────┐
┌─┼─┐ ,    ┌───┼───────┐             ∘          ∘
5 - ⍤    ┌─┴─┐ ,   ┌───┼─────┐      ┌┴─┐ ┌──────┴──────┐
   ┌┴┐   / ┌─┴─┐ ┌─┼─┐ × ┌───┴────┐ ¯1 + 23456789TJQKA ⍳
   ≢ ∪ ┌─┘ /   ⍨ 5 ≡ ∪   /        ∘
       ⌈ ┌─┘ ┌─┘       ┌─┘ ┌──────┴──────┐
         +   ∘.        ⌈   23456789TJQKA ⍳
           ┌─┘
           =
</code></pre><p><sup id="fnref:20"><a href="#fn:20" role="doc-noteref">20</a></sup></p><p>Since functions are higher-order in Haskell, it’s natural to store them as normal data.
Thus, upon evaluation (in this case, evaluation happens when <code>h2n</code> is assigned to<sup id="fnref:21"><a href="#fn:21" role="doc-noteref">21</a></sup>), the variable <strong><code>h2n</code></strong> no longer has access to the syntax tree, it only holds a primitive haskell function, a string which represents the scrapped tree, and a few other values describing the behavior of the function.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>type</span> <span>FuncM</span> <span>=</span> <span>Array</span> <span>-&gt;</span> <span>StateT</span> <span>IdMap</span> <span>IO</span> <span>Array</span>
</span></span><span><span><span>type</span> <span>FuncD</span> <span>=</span> <span>Array</span> <span>-&gt;</span> <span>FuncM</span>
</span></span><span><span>
</span></span><span><span><span>data</span> <span>Function</span> <span>=</span> <span>MonFn</span> <span>FnInfoM</span> <span>FuncM</span>
</span></span><span><span>              <span>|</span> <span>DyadFn</span> <span>FnInfoD</span> <span>FuncD</span>
</span></span><span><span>              <span>|</span> <span>AmbivFn</span> <span>FnInfoA</span> <span>FuncM</span> <span>FuncD</span>
</span></span><span><span>
</span></span><span><span><span>-- "function tree": a tree that makes up a derived function:</span>
</span></span><span><span><span>-- the internal nodes are operators, and the leaves are functions or (derived) arrays</span>
</span></span><span><span><span>data</span> <span>FnTreeNode</span> <span>=</span> <span>FnLeafFn</span> <span>Function</span>
</span></span><span><span>                <span>|</span> <span>FnLeafVar</span> <span>String</span>
</span></span><span><span>                <span>|</span> <span>FnLeafArr</span> <span>ArrTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalMonOp</span> <span>OpTreeNode</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalDyadOp</span> <span>OpTreeNode</span> <span>FnTreeNode</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalAtop</span> <span>FnTreeNode</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalFork</span> <span>FnTreeNode</span> <span>FnTreeNode</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalAssignment</span> <span>String</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalQuadAssignment</span> <span>FnTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalAxisSpec</span> <span>FnTreeNode</span> <span>ArrTreeNode</span>
</span></span><span><span>                <span>|</span> <span>FnInternalDummyNode</span> <span>FnTreeNode</span>
</span></span><span><span>
</span></span><span><span><span>evalFnTree</span> <span>::</span> <span>FnTreeNode</span> <span>-&gt;</span> <span>StateT</span> <span>IdMap</span> <span>IO</span> (<span>Either</span> <span>Array</span> <span>Function</span>)</span></span></code></pre></div><h3 id="practical-typeclasses">Practical Typeclasses</h3><p>Like the parser, evaluation uses monads to handle state.
Since any arbitrary function might have full control over the program state, in order to have a universal function type, that type (<strong><code>Function</code></strong> (see above)) must be impure<sup id="fnref:22"><a href="#fn:22" role="doc-noteref">22</a></sup>.</p><p>The majority of built-in <strong><code>Functions</code></strong> don’t touch the global state, and the ones that do usually only use it for a very narrow purpose.
I wanted a mechanism that allowed me to define these <strong><code>Functions</code></strong> with maximally constraining types, yet to easily convert them into the full (non-constrained) monad (<strong><code>EvalM</code></strong>) without causing a combinatorial explosion of helper functions.
I used the typeclass <strong><code>SubEvalM</code></strong> to do this.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>{- SubEvalM (subset of EvalM): typeclass for wrapper monads -}</span>
</span></span><span><span>
</span></span><span><span><span>type</span> <span>EvalM</span> <span>=</span> <span>StateT</span> <span>IdMap</span> <span>IO</span>
</span></span><span><span>
</span></span><span><span><span>class</span> (<span>Monad</span> m) <span>=&gt;</span> <span>SubEvalM</span> m <span>where</span>
</span></span><span><span>    toEvalM <span>::</span> m a <span>-&gt;</span> <span>EvalM</span> a
</span></span><span><span>
</span></span><span><span><span>instance</span> <span>SubEvalM</span> <span>Identity</span> <span>where</span>
</span></span><span><span>    toEvalM <span>=</span> return <span>.</span> runIdentity
</span></span><span><span>
</span></span><span><span><span>newtype</span> <span>IdxOriginM</span> a <span>=</span> <span>IdxOriginM</span> { unIdxOriginM <span>::</span> <span>Reader</span> <span>Int</span> a }
</span></span><span><span>    <span>deriving</span> (<span>Functor</span>, <span>Applicative</span>, <span>Monad</span>, <span>MonadReader</span> <span>Int</span>) via (<span>Reader</span> <span>Int</span>)
</span></span><span><span>
</span></span><span><span><span>instance</span> <span>SubEvalM</span> <span>IdxOriginM</span> <span>where</span>
</span></span><span><span>    toEvalM iom <span>=</span> <span>do</span>
</span></span><span><span>        idm <span>&lt;-</span> get
</span></span><span><span>        <span>let</span> iO <span>=</span> <span>case</span> mapLookup <span>"⎕IO"</span> idm <span>of</span>
</span></span><span><span>                  <span>Just</span> (<span>IdArr</span> a)
</span></span><span><span>                      <span>|</span> <span>ScalarNum</span> n <span>&lt;-</span> a `at` <span>0</span> <span>-&gt;</span> <span>Prelude</span><span>.</span>floor <span>$</span> n
</span></span><span><span>                  <span>Just</span> <span>_</span> <span>-&gt;</span> undefined <span>-- unexpected val for ⎕IO</span>
</span></span><span><span>                  <span>_</span> <span>-&gt;</span> undefined <span>-- no val for ⎕IO</span>
</span></span><span><span>        return <span>.</span> (flip runReader) iO <span>.</span> unIdxOriginM <span>$</span> iom
</span></span><span><span>
</span></span><span><span><span>newtype</span> <span>RandAndIoM</span> a <span>=</span> <span>RandAndIoM</span> { unRandomAndIoM <span>::</span> <span>StateTStrict</span><span>.</span><span>StateT</span> <span>StdGen</span> (<span>Reader</span> <span>Int</span>) a }
</span></span><span><span>    <span>deriving</span> (<span>Functor</span>, <span>Applicative</span>, <span>Monad</span>, <span>MonadReader</span> <span>Int</span>, <span>MonadState</span> <span>StdGen</span>) via <span>StateTStrict</span><span>.</span><span>StateT</span> <span>StdGen</span> (<span>Reader</span> <span>Int</span>)
</span></span><span><span>
</span></span><span><span><span>instance</span> <span>SubEvalM</span> <span>RandAndIoM</span> <span>where</span>
</span></span><span><span>    toEvalM rm <span>=</span> <span>do</span>
</span></span><span><span>        gen <span>&lt;-</span> lift <span>$</span> newStdGen
</span></span><span><span>        toEvalM <span>.</span> <span>IdxOriginM</span> <span>.</span> runStateGenT_ gen <span>$</span> <span>\</span><span>_</span> <span>-&gt;</span> unRandomAndIoM rm
</span></span><span><span>
</span></span><span><span>
</span></span><span><span><span>{- Monad Wrappers -}</span>
</span></span><span><span>
</span></span><span><span><span>mkMonFn</span> <span>::</span> <span>SubEvalM</span> m <span>=&gt;</span> <span>FnInfoM</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> m <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>mkMonFn</span> i f <span>=</span> <span>MonFn</span> i (<span>\</span>a <span>-&gt;</span> toEvalM <span>$</span> f a)
</span></span><span><span>
</span></span><span><span><span>mkDyadFn</span> <span>::</span> <span>SubEvalM</span> m <span>=&gt;</span> <span>FnInfoD</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> m <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>mkDyadFn</span> i f <span>=</span> <span>DyadFn</span> i (<span>\</span>a b <span>-&gt;</span> toEvalM <span>$</span> f a b)
</span></span><span><span>
</span></span><span><span><span>mkAmbivFn</span> <span>::</span> (<span>SubEvalM</span> m0, <span>SubEvalM</span> m1) <span>=&gt;</span> <span>FnInfoA</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> m0 <span>Array</span>) <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> m1 <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>mkAmbivFn</span> ia fm fd <span>=</span> <span>AmbivFn</span> ia (<span>\</span>a <span>-&gt;</span> toEvalM <span>$</span> fm a) (<span>\</span>a b <span>-&gt;</span> toEvalM <span>$</span> fd a b)
</span></span><span><span>
</span></span><span><span><span>-- (mkMonOp, ... etc.)</span>
</span></span><span><span>
</span></span><span><span><span>{- Pure Wrappers -}</span>
</span></span><span><span>
</span></span><span><span><span>pureMonFn</span> <span>::</span> <span>FnInfoM</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>pureMonFn</span> i f <span>=</span> mkMonFn i (<span>Identity</span> <span>.</span> f)
</span></span><span><span>
</span></span><span><span><span>pureDyadFn</span> <span>::</span> <span>FnInfoD</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>pureDyadFn</span> i f <span>=</span> mkDyadFn i (<span>Identity</span> <span>.:</span> f)
</span></span><span><span>
</span></span><span><span><span>pureAmbivFn</span> <span>::</span> <span>FnInfoA</span> <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span>) <span>-&gt;</span> (<span>Array</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>Array</span>) <span>-&gt;</span> <span>Function</span>
</span></span><span><span><span>pureAmbivFn</span> ia fm fd <span>=</span> mkAmbivFn ia (<span>Identity</span> <span>.</span> fm) (<span>Identity</span> <span>.:</span> fd)
</span></span><span><span>
</span></span><span><span><span>-- (pureMonOp, ... etc.)</span></span></span></code></pre></div><h3 id="selective-assignment">Selective Assignment</h3><p>APL has a feature called “selective assignment” where the left-hand-side of an assignment can be an expression, so long as it only uses (pre-verified) functions that solely permute/select items of the argument array.
For example:</p><pre tabindex="0"><code>    x ← 2 2⍴⍳4
    x
1 2
3 4
    (,⌽x) ← ⍳4 ⍝ assign ⍳4 to the ravel (,) of
    x          ⍝ the reversed (along the last axis) (⌽) x
2 1
4 3
</code></pre><p>At first glance, this seems like implementing this will require a re-implementation of each selectable function, which can be called to determine which cells of the variable are selected, and their shape.
However, after making the above assumption (all selectable functions do not mutate the cells of their argument), the selected array (x, in the above example) can simply be replaced by an array of the same shape whose cells are the indices of x (⍳⍴x), then, after applying the selecting functions, the result will be some permutation/selection of the indices of (⍳⍴x), which can then be assigned to.
So, for the above example, we have</p><pre tabindex="0"><code>    x ← 2 2⍴⍳4
    x
1 2
3 4
    ⍴x
2 2
    ⍳⍴x
┌───┬───┐
│1 1│1 2│
├───┼───┤
│2 1│2 2│
└───┴───┘
    ⌽⍳⍴x
┌───┬───┐
│1 2│1 1│
├───┼───┤
│2 2│2 1│
└───┴───┘
    ,⌽⍳⍴x
┌───┬───┬───┬───┐
│1 2│1 1│2 2│2 1│
└───┴───┴───┴───┘
    x[,⌽⍳⍴x]
2 1 4 3
    x[,⌽⍳⍴x] ← ⍳4
    x
2 1
4 3
</code></pre><h3 id="higher-dimensional-arrays-its-just-indexing">Higher-Dimensional Arrays: It’s Just Indexing</h3><p>One of the hardest aspects of implementing the built-in functions/operators was making them work on higher-dimensional arrays.
The majority APL primitives are trivially implemented on vectors and scalars, but their behavior becomes much harder to understand when they operate on n-dimensional arrays.</p><p>For example, dyadic (<strong><code>,</code></strong>) ([con]catenate).
Just put the two arrays together, no problem, right?</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>catenate</span> <span>::</span> <span>Double</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>IdxOriginM</span> <span>Array</span>
</span></span><span><span><span>catenate</span> ax x y
</span></span><span><span>    <span>|</span> isIntegral ax <span>=</span> ask <span>&gt;&gt;=</span> <span>\</span>iO <span>-&gt;</span> <span>let</span> ax' <span>=</span> (<span>Prelude</span><span>.</span>floor ax) <span>-</span> iO <span>+</span> <span>1</span>
</span></span><span><span>                                     <span>in</span> return <span>$</span> _catenate ax'
</span></span><span><span>    <span>|</span> otherwise <span>=</span> ask <span>&gt;&gt;=</span> <span>\</span>iO <span>-&gt;</span> <span>let</span> ax' <span>=</span> (<span>Prelude</span><span>.</span>ceiling ax) <span>-</span> iO
</span></span><span><span>                                 <span>in</span> return <span>$</span> _laminate ax'
</span></span><span><span>    <span>where</span> _catenate ax'
</span></span><span><span>              <span>|</span> ax' <span>&lt;=</span> <span>0</span> <span>||</span> ax' <span>&gt;</span> rank <span>=</span> throw <span>.</span> <span>RankError</span> <span>$</span> <span>"(,): invalid axis"</span>
</span></span><span><span>              <span>|</span> x <span>==</span> zilde <span>=</span> y
</span></span><span><span>              <span>|</span> y <span>==</span> zilde <span>=</span> x
</span></span><span><span>              <span>|</span> (shape'' x') <span>/=</span> (shape'' y') <span>=</span> throw <span>.</span> <span>LengthError</span> <span>$</span> <span>"(,): mismatched argument shapes"</span>
</span></span><span><span>              <span>|</span> otherwise <span>=</span> zipVecsAlongAxis ax'' ax'' ax'' (<span>++</span>) x' y'
</span></span><span><span>              <span>where</span> (x', y') <span>=</span> _rankMorph (x, y)
</span></span><span><span>                    rank <span>=</span> arrRank x'
</span></span><span><span>                    _rankMorph (a, b)
</span></span><span><span>                        <span>|</span> shape a <span>==</span> [<span>1</span>] <span>&amp;&amp;</span> arrNetSize b <span>&gt;</span> <span>0</span> <span>=</span> (shapedArrFromList (shape'1 b) <span>$</span> <span>Prelude</span><span>.</span>replicate (foldr (<span>*</span>) <span>1</span> <span>$</span> shape'1 b) (a `at` <span>0</span>), b)
</span></span><span><span>                        <span>|</span> shape b <span>==</span> [<span>1</span>] <span>&amp;&amp;</span> arrNetSize a <span>&gt;</span> <span>0</span> <span>=</span> (a, shapedArrFromList (shape'1 a) <span>$</span> <span>Prelude</span><span>.</span>replicate (foldr (<span>*</span>) <span>1</span> <span>$</span> shape'1 a) (b `at` <span>0</span>))
</span></span><span><span>                        <span>|</span> arrRank a <span>==</span> arrRank b <span>=</span> (a, b)
</span></span><span><span>                        <span>|</span> arrRank a <span>==</span> arrRank b <span>+</span> <span>1</span> <span>&amp;&amp;</span> ax' <span>&lt;=</span> arrRank a <span>=</span> (a, b {shape <span>=</span> take _ax' (shape b) <span>++</span> [<span>1</span>] <span>++</span> drop _ax' (shape b)})
</span></span><span><span>                        <span>|</span> arrRank a <span>+</span> <span>1</span> <span>==</span> arrRank b <span>&amp;&amp;</span> ax' <span>&lt;=</span> arrRank b <span>=</span> (a {shape <span>=</span> take (_ax' <span>+</span> <span>1</span>) (shape a) <span>++</span> [<span>1</span>] <span>++</span> drop (_ax' <span>+</span> <span>1</span>) (shape a)}, b)
</span></span><span><span>                        <span>|</span> otherwise <span>=</span> throw <span>.</span> <span>RankError</span> <span>$</span> <span>"(,): mismatched argument ranks"</span>
</span></span><span><span>                        <span>where</span> _ax' <span>=</span> ax' <span>-</span> <span>1</span>
</span></span><span><span>                    shape'1 a <span>=</span> take (ax' <span>-</span> <span>1</span>) (shape a) <span>++</span> [<span>1</span>] <span>++</span> drop ax' (shape a)
</span></span><span><span>                    shape'' a <span>=</span> take (ax'' <span>-</span> <span>1</span>) (shape a) <span>++</span> drop ax'' (shape a)
</span></span><span><span>                    ax'' <span>=</span> <span>if</span> arrRank x' <span>==</span> arrRank x <span>+</span> <span>1</span> <span>then</span> ax' <span>+</span> <span>1</span> <span>else</span> ax'
</span></span><span><span>          _laminate ax'
</span></span><span><span>              <span>|</span> ax' <span>&lt;</span> <span>0</span> <span>||</span> ax' <span>&gt;</span> rank <span>=</span> throw <span>.</span> <span>RankError</span> <span>$</span> <span>"(,): invalid axis"</span>
</span></span><span><span>              <span>|</span> (shape x') <span>/=</span> (shape y') <span>=</span> throw <span>.</span> <span>LengthError</span> <span>$</span> <span>"(,): mismatched argument shapes"</span>
</span></span><span><span>              <span>|</span> otherwise <span>=</span> unAlongAxis (ax' <span>+</span> <span>1</span>) [x', y']
</span></span><span><span>              <span>where</span> (x', y') <span>=</span> _rankMorph (x, y)
</span></span><span><span>                    rank <span>=</span> arrRank x'
</span></span><span><span>                    _rankMorph (a, b)
</span></span><span><span>                        <span>|</span> shape a <span>==</span> [<span>1</span>] <span>&amp;&amp;</span> arrNetSize b <span>&gt;</span> <span>0</span> <span>=</span> (shapedArrFromList (shape b) <span>$</span> <span>Prelude</span><span>.</span>replicate (arrNetSize b) (a `at` <span>0</span>), b)
</span></span><span><span>                        <span>|</span> shape b <span>==</span> [<span>1</span>] <span>&amp;&amp;</span> arrNetSize a <span>&gt;</span> <span>0</span> <span>=</span> (a, shapedArrFromList (shape a) <span>$</span> <span>Prelude</span><span>.</span>replicate (arrNetSize a) (b `at` <span>0</span>))
</span></span><span><span>                        <span>|</span> shape a <span>==</span> shape b <span>=</span> (a, b)
</span></span><span><span>                        <span>|</span> otherwise <span>=</span> throw <span>.</span> <span>RankError</span> <span>$</span> <span>"(,): mismatched argument ranks"</span></span></span></code></pre></div><p>The majority of this code is actually just edge-cases (which are difficult in their own right), but the key functions to note are <strong><code>zipVecsAlongAxis</code></strong> and <strong><code>unAlongAxis</code></strong> (which are applied after the edge-cases are resolved).
All primitives functions that operate on higher-dimensional arrays can be expressed using a handful of helper functions.
All of those helpers are built on top of three core functions.</p><div><pre tabindex="0"><code data-lang="haskell"><span><span><span>alongAxis</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> [<span>Array</span>]
</span></span><span><span><span>alongAxis</span> ax a
</span></span><span><span>    <span>|</span> ax <span>-</span> _iO <span>&gt;=</span> (length <span>.</span> shape <span>$</span> a) <span>=</span> throw <span>$</span> <span>RankError</span> <span>"invalid axis"</span>
</span></span><span><span>    <span>|</span> (length <span>.</span> shape <span>$</span> a) <span>==</span> <span>0</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>|</span> arrNetSize a <span>==</span> <span>0</span> <span>=</span> <span>[]</span>
</span></span><span><span>    <span>|</span> otherwise <span>=</span> map (subarrayAt) [<span>0</span><span>..</span>(n <span>-</span> <span>1</span>)]
</span></span><span><span>        <span>where</span> n <span>=</span> (shape a) <span>!!</span> (ax <span>-</span> _iO)
</span></span><span><span>              shape' <span>=</span> <span>if</span> (length <span>.</span> shape <span>$</span> a) <span>==</span> <span>1</span>
</span></span><span><span>                       <span>then</span> [<span>1</span>]
</span></span><span><span>                       <span>else</span> take (ax <span>-</span> _iO) (shape a) <span>++</span> drop (ax <span>-</span> _iO <span>+</span> <span>1</span>) (shape a)
</span></span><span><span>              sz <span>=</span> arrNetSize a
</span></span><span><span>              subarrayAt i <span>=</span> shapedArrFromList shape' <span>.</span> map (arrIndex a) <span>$</span> indicesAt i
</span></span><span><span>              indicesAt i <span>=</span>  map (<span>\</span>is <span>-&gt;</span> take (ax <span>-</span> _iO) is <span>++</span> [i] <span>++</span> drop (ax <span>-</span> _iO) is) <span>$</span> map (calcIndex) [<span>0</span><span>..</span>(sz `div` n <span>-</span> <span>1</span>)]
</span></span><span><span>              indexMod <span>=</span> tail <span>.</span> scanr (<span>*</span>) <span>1</span> <span>$</span> shape'
</span></span><span><span>              calcIndex i <span>=</span> map (<span>\</span>(e, m) <span>-&gt;</span> i `div` m `mod` e) <span>$</span> zip shape' indexMod
</span></span><span><span>              _iO <span>=</span> <span>1</span> <span>-- axis supplied is with respect to _iO = 1, not actual ⎕IO</span>
</span></span><span><span>
</span></span><span><span><span>alongRank</span> <span>::</span> <span>Int</span> <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>Array</span>
</span></span><span><span><span>alongRank</span> r a
</span></span><span><span>    <span>|</span> foldr (<span>*</span>) <span>1</span> (shape a) <span>==</span> <span>0</span> <span>=</span> zilde
</span></span><span><span>    <span>|</span> n <span>&lt;=</span> <span>0</span> <span>=</span> listToArr [<span>ScalarArr</span> a]
</span></span><span><span>    <span>|</span> n <span>&gt;=</span> (length <span>$</span> shape a) <span>=</span> a
</span></span><span><span>    <span>|</span> otherwise <span>=</span> shapedArrFromList outerShape <span>.</span> map (<span>ScalarArr</span> <span>.</span> shapedArrFromList innerShape) <span>.</span> groupsOf groupSz <span>.</span> arrToList <span>$</span> a
</span></span><span><span>        <span>where</span> outerShape <span>=</span> take n <span>$</span> shape a
</span></span><span><span>              innerShape <span>=</span> drop n <span>$</span> shape a
</span></span><span><span>              n <span>=</span> <span>if</span> r <span>&gt;=</span> <span>0</span> <span>then</span> (length <span>$</span> shape a) <span>-</span> r <span>else</span> <span>-</span><span>1</span> <span>*</span> r
</span></span><span><span>              groupSz <span>=</span> foldr (<span>*</span>) <span>1</span> innerShape
</span></span><span><span>
</span></span><span><span><span>arrReorderAxes</span> <span>::</span> [<span>Int</span>] <span>-&gt;</span> <span>Array</span> <span>-&gt;</span> <span>Array</span>
</span></span><span><span><span>arrReorderAxes</span> targetIdxs a
</span></span><span><span>    <span>|</span> length targetIdxs <span>/=</span> (length <span>$</span> shape a) <span>=</span> undefined
</span></span><span><span>    <span>|</span> otherwise <span>=</span> shapedArrFromList shape' vals'
</span></span><span><span>        <span>where</span> shape' <span>=</span> map (foldr min intMax <span>.</span> map snd) <span>.</span> groupBy (on (<span>==</span>) fst) <span>.</span> sortBy (on compare fst) <span>$</span> zip targetIdxs (shape a)
</span></span><span><span>              n <span>=</span> foldr (<span>*</span>) <span>1</span> shape'
</span></span><span><span>              vals' <span>=</span> map ((a`arrIndex`) <span>.</span> calcIndex) [<span>0</span><span>..</span>(n <span>-</span> <span>1</span>)]
</span></span><span><span>              calcIndex i <span>=</span> map ((idxList<span>!!</span>) <span>.</span> (<span>+</span>(<span>-</span><span>1</span>))) targetIdxs
</span></span><span><span>                  <span>where</span> idxList <span>=</span> calcIndex' i
</span></span><span><span>              calcIndex' i <span>=</span> map (<span>\</span>(e, m) <span>-&gt;</span> i `div` m `mod` e) <span>$</span> zip shape' indexMod
</span></span><span><span>              indexMod <span>=</span> tail <span>.</span> scanr (<span>*</span>) <span>1</span> <span>$</span> shape'</span></span></code></pre></div><p>The details of the implementation doesn’t matter so much as the fact that these functions only do two main things:</p><ol><li>Index arithmetic</li><li>Manipulation of shapes (array dimensions)</li></ol><p>That’s all it takes to work with multi-dimensional arrays.</p><h2 id="mimicking-dyalog">Mimicking Dyalog</h2><p><a href="https://www.dyalog.com/" target="_blank">Dyalog</a> APL is the de-facto modern implementation of APL.
This project is heavily based off of Dyalog APL.
All of the syntax and glyphs come directly from Dyalog, and
the behavior of the functions and operators was almost entirely taken/tweaked from the <a href="https://docs.dyalog.com/latest/Dyalog%20APL%20Language%20Reference%20Guide.pdf" target="_blank">Dyalog Reference Guide</a>.</p><p>This was convenient in many ways, because it provided an oracle to test against<sup id="fnref:23"><a href="#fn:23" role="doc-noteref">23</a></sup>, and at any point when I was uncertain of the behavior of something, it gave me a definitive answer, or at least a model which I could analyze to come to my own answer.</p><p>However, attempting to clone Dyalog made the project harder in a lot of ways, namely</p><ul><li>Getting printing of arrays, function trees, and (high-precision, scientific-notation, floating-point) numbers to exactly match Dyalog</li><li>Trying to match Dyalog’s behavior in weird edge-cases</li><li>The sheer generality of many of the glyphs (catenate (above) is a good example of this)</li></ul><p>I ended up making a lot of compromises between what I thought was doable (or even possible: some behavior of Dyalog was seemingly incomprehensible), what seemed the most simple<sup id="fnref:24"><a href="#fn:24" role="doc-noteref">24</a></sup>, and what Dyalog did.</p><p>The following is a (far from complete) laundry-list of discrepancies between my interpreter and Dyalog’s.</p><ul><li>Differing properties:<ul><li>Amount of whitespace when printing arrays with non-zero rank, but with 0 ∊ shape</li><li>Whitespace and some number formatting when printing matrices of real-numbers</li><li>Printing format of arrays as leaves of function trees</li><li>Printing format of arrays as axis specifications</li><li>Printing of dfns/dops (∇)</li><li>Exact rules for sorting order</li><li>Propigation rules for function properties (i.e. identity, ability to select, axis specification) on derived functions</li><li>Fine-grained behavior of execute (⍎) and format (⍕)</li></ul></li><li>In Dyalog, …<ul><li>Variables have nameclasses and weird behavior after reassignment<sup id="fnref:25"><a href="#fn:25" role="doc-noteref">25</a></sup></li><li>Some dops can’t be used inline</li><li>Dyadic operators can’t be in the right-hand-side of assignments</li><li>The default ⍺ can be a function in dops</li></ul></li><li>In my interpreter, …<ul><li>Dops/dfns are not strongly short-circuited: if the condition in a guard is false, the rhs isn’t evaluated, but it is still parsed (in Dyalog, it isn’t parsed)</li><li>Scalars are not 0-rank: they are vectors with a single element (this approach seems more simple<sup id="fnref:26"><a href="#fn:26" role="doc-noteref">26</a></sup>, but it causes some incompatibilities in the behavior of certain functions)</li><li>Arrays with non-1 rank cannot have zero as an element in their shape (Dyalog allows this)</li><li>LCM, GCD, factorial, and binomial only work on integers (Dyalog allows reals and complex numbers)</li><li>The circle function (dyadic ○) only supports 1 2 3, ¯1, ¯2, ¯3</li><li>Dfns cannot <em>modify</em> global variables</li><li>⎕IO can be <em>any integer</em> (not just 0 or 1)</li><li>Axis spec operator is more limited (several functions don’t support it (e.g. ⊂)), it also only takes singleton numbers, never vectors</li><li>Dyadic ⍒/⍋ is limited to vector (not higher-dimensional array) collation sequences</li><li>Reduce can’t take a negative or zero argument on left (window)</li><li>Encode only works on integers</li></ul></li><li>My interpreter does not support …<ul><li>Complex numbers</li><li>Dyadic thorn (⍎) and dyadic hydrant (⍕)</li><li>&amp;, ⌸, ⌺, and ⌹</li><li>Monadic squad (⌷)</li><li>I-Beam (⌶)</li><li>Traditional Functions</li><li>Array prototypes, fill elements other than 0</li><li>Many System Names</li><li>Many other Dyalog features</li></ul></li></ul><h2 id="haskell-the-good-and-the-ugly">Haskell: the Good and the Ugly</h2><h3 id="the-good">The Good</h3><h4 id="the-compiler">The Compiler</h4><p>I am generally a fan of the guarantees the compiler gives, and the Haskell complier has given me more guarantees<sup id="fnref:27"><a href="#fn:27" role="doc-noteref">27</a></sup> than any other I have used.
The time I spent debugging runtime errors in Haskell was significantly smaller in proportion to imperative languages, and there were many times where I was astonished<sup id="fnref:28"><a href="#fn:28" role="doc-noteref">28</a></sup> that my code worked after I fixed the compilation errors.</p><p>That being said, runtime errors still happened, and many of them had similar nature to those in other languages (typically logic errors, or panics). Such errors grew more common and more difficult to debug as the project grew in scale<sup id="fnref:29"><a href="#fn:29" role="doc-noteref">29</a></sup>.</p><p>It was also relatively difficult to get code to compile (and writing code in general) (in comparison to in other languages).
It’s hard to tell how much of this has to do with the fact that I’m new to Haskell, and how much easier it will become over time.
It’s hard to gage my progress over the course of project, because the nature of the problems I’m solving (and their complexity) has fluctuated a lot.
I’ve certainly become more fluent in Haskell, but I’ve continued to battle compilation errors.</p><h4 id="the-libraries">The Libraries</h4><p>The standard libraries have a lot of useful functions.
I made frequent use of Data.List, Control.Monad, and Data.Function, among others.</p><p>Most simple operations you might want to do are often well within reach, and it’s a lot of fun using standard functions to replace boilerplate.</p><h4 id="currying-combinators-tacit">Currying, Combinators, Tacit</h4><p>I probably used these way too much, but they’re a lot of fun.</p><h3 id="the-ugly">The Ugly</h3><h4 id="the-elephant-in-the-room">The Elephant In The Room</h4><p>Haskell is incomprehensible if you haven’t spent time with it (or languages like it).
Functional style alone is probably a big enough gap from imperative to make it difficult<sup id="fnref:30"><a href="#fn:30" role="doc-noteref">30</a></sup>, but combine that with all the category theory and crazy type stuff, and the learning curve becomes a cliff.</p><p>I’m not suggesting that Haskell should be watered down – I see the utility in its more esoteric style – but this doesn’t change the fact that it makes communication (one of the most important aspects of programming) difficult in many situations.</p><h4 id="over-generalization">Over-Generalization</h4><p>Perhaps this is a contradiction (you can’t have your cake and eat it), since I just raved about the benefits of generality, but I found that it goes both ways: generality can be bad in excess.</p><p>My best example of this is the System.Random (and System.Random.Stateful) library: even after I felt like I had a solid grasp on monads and monad transformers, it took me several hours (and even reading some of the source) to comprehend the library<sup id="fnref:31"><a href="#fn:31" role="doc-noteref">31</a></sup> enough to feel comfortable using it<sup id="fnref:32"><a href="#fn:32" role="doc-noteref">32</a></sup>.</p><h4 id="efficiency">Efficiency</h4><p>This goes without saying.
Apparently Haskell’s performance isn’t <em>that bad</em>, but I don’t plan on using Haskell for anything that is remotely performance-sensitive.</p><p>Trying to optimize Haskell code does sound like an interesting problem, but it might be a lost cause.</p><h4 id="laziness">Laziness</h4><p>Generally it doesn’t matter <em>when values are evaluated</em>, so long as you have them when you need them, but since I made excess use of <em>exceptions</em> and <em>undefined</em>’s in this program, understanding laziness became important.</p><p>It’s funny how, when using Haskell, laziness creeps into your program where you don’t expect it.</p><p>After implementing assignment, it took me a while to realize that, if the RHS of an expression threw an error when evaluated, that error wouldn’t show itself until I tried to <em>print</em> the variable to which the value was given. For example:</p><pre tabindex="0"><code>    x ← 1 + 'c' ⍝ this statement succeeds
    1 + 2       ⍝ execution continues as normal
3
    y ← x + 1   ⍝ this is fine as well
    y           ⍝ x is only evaluated when the concrete value of y is needed for printing
DOMAIN ERROR: expected number
</code></pre><p>Funnily enough, I thought I fixed this, but after running this example, the current version of the interpreter produces the same result.</p><p>Laziness can be convenient when you want to avoid throwing errors unless absolutely necessary (which is usually the case), but when you want to deliberately throw an error, you must force execution into a single path towards that error.</p><p>This uncertainty of time of evaluation also makes catching errors difficult, because calling <strong><code>catch</code></strong> on the function that throws the error <em>will not necessarily catch that error</em>.
Catch must be called on the function that <em>forces evaluation</em> on that error.
This is something that is hard to trace, and something that types don’t help much with.</p><p>The solution to this is probably to use monads instead of exceptions.</p><h4 id="debugging">Debugging</h4><p>Haskell’s lazy evaluation also makes debugging much harder.
Evaluation is not linear: stepping doesn’t go line-by-line: it jumps around between functions as their values are needed.</p><p>GHCi does have a built-in debugger (which is pretty impressive), but for this reason, it’s not really the most useful.</p><p>As I mentioned earlier, runtime errors are much less prevalent due to strict typing, but when they do arise, this could become an issue.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SkyRoof: New Ham Satellite Tracking and SDR Receiver Software (102 pts)]]></title>
            <link>https://www.rtl-sdr.com/skyroof-new-ham-satellite-tracking-and-sdr-receiver-software/</link>
            <guid>44194859</guid>
            <pubDate>Thu, 05 Jun 2025 19:15:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rtl-sdr.com/skyroof-new-ham-satellite-tracking-and-sdr-receiver-software/">https://www.rtl-sdr.com/skyroof-new-ham-satellite-tracking-and-sdr-receiver-software/</a>, See on <a href="https://news.ycombinator.com/item?id=44194859">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="site" role="main">
			
	
			
		<article id="post-63994">
			<header>
								
			  				  <time datetime="2025-06-05T05:26:05+00:00" pubdate="">June 5, 2025</time>
								
									
							</header>
		
			<div>
				<p>Recently VE3NEA has <a href="https://ve3nea.github.io/SkyRoof/index.html" target="_blank" rel="noopener">released a new Windows program called "SkyRoof"</a>. SkyRoof is both a satellite tracking and SDR receiver program. It supports the RTL-SDR as well as Airspy and SDRplay devices.</p>
<p>The software is designed for tracking and receiving ham radio satellites, and it can provide detailed information about all ham satellites, tracking them in real time, and provide pass prediction. It also shows a skymap and SDR waterfall display. The receiver software supports demodulation of SSB/CW/FM, and it automatically compensates for doppler. It can also interface with antenna rotators that support hamlib.</p>

<figure id="attachment_63995" aria-labelledby="figcaption_attachment_63995"><a href="https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof.png"><img decoding="async" src="https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof.png" alt="SkyRoof Satellite Tracking and SDR Receiver Software Screenshot" width="1000" height="537" srcset="https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof.png 1000w, https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof-700x376.png 700w, https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof-768x412.png 768w, https://www.rtl-sdr.com/wp-content/uploads/2025/06/skyroof-600x322.png 600w" sizes="(max-width: 1000px) 100vw, 1000px"></a><figcaption id="figcaption_attachment_63995">SkyRoof Satellite Tracking and SDR Receiver Software Screenshot</figcaption></figure>

<p>Over on YouTube Johnson's Techworld has also recently uploaded a video showing him testing out Skyroof, which may be of interest to some.</p>
<div id="WYL_1UnWr8q7Gc8" data-src="https://www.rtl-sdr.com/wp-content/plugins/wp-youtube-lyte/lyteCache.php?origThumbUrl=https%3A%2F%2Fi.ytimg.com%2Fvi%2F1UnWr8q7Gc8%2Fhqdefault.jpg" title="Brief review of SkyRoof, the latest satellite tracking software!"><p>Brief review of SkyRoof, the latest satellite tracking software!</p></div>
					
					
			</div>
			<!-- / .content -->
			
			
		</article>
		<!-- / #post-63994 -->

		    
    
    
    
    		
				  
			<!-- / .post-nav -->
				
		



		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Eleven v3 (257 pts)]]></title>
            <link>https://elevenlabs.io/v3</link>
            <guid>44194521</guid>
            <pubDate>Thu, 05 Jun 2025 18:41:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://elevenlabs.io/v3">https://elevenlabs.io/v3</a>, See on <a href="https://news.ycombinator.com/item?id=44194521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-state="closed" id="radix-«R7fphtndibnb»" role="region" aria-labelledby="radix-«R3fphtndibnb»" data-orientation="vertical"><p>Afrikaans (afr), Arabic (ara), Armenian (hye), Assamese (asm), Azerbaijani (aze), Belarusian (bel), Bengali (ben), Bosnian (bos), Bulgarian (bul), Catalan (cat), Cebuano (ceb), Chichewa (nya), Croatian (hrv), Czech (ces), Danish (dan), Dutch (nld), English (eng), Estonian (est), Filipino (fil), Finnish (fin), French (fra), Galician (glg), Georgian (kat), German (deu), Greek (ell), Gujarati (guj), Hausa (hau), Hebrew (heb), Hindi (hin), Hungarian (hun), Icelandic (isl), Indonesian (ind), Irish (gle), Italian (ita), Japanese (jpn), Javanese (jav), Kannada (kan), Kazakh (kaz), Kirghiz (kir), Korean (kor), Latvian (lav), Lingala (lin), Lithuanian (lit), Luxembourgish (ltz), Macedonian (mkd), Malay (msa), Malayalam (mal), Mandarin Chinese (cmn), Marathi (mar), Nepali (nep), Norwegian (nor), Pashto (pus), Persian (fas), Polish (pol), Portuguese (por), Punjabi (pan), Romanian (ron), Russian (rus), Serbian (srp), Sindhi (snd), Slovak (slk), Slovenian (slv), Somali (som), Spanish (spa), Swahili (swa), Swedish (swe), Tamil (tam), Telugu (tel), Thai (tha), Turkish (tur), Ukrainian (ukr), Urdu (urd), Vietnamese (vie), Welsh (cym)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I made a search engine worse than Elasticsearch (2024) (106 pts)]]></title>
            <link>https://softwaredoug.com/blog/2024/08/06/i-made-search-worse-elasticsearch</link>
            <guid>44194468</guid>
            <pubDate>Thu, 05 Jun 2025 18:37:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://softwaredoug.com/blog/2024/08/06/i-made-search-worse-elasticsearch">https://softwaredoug.com/blog/2024/08/06/i-made-search-worse-elasticsearch</a>, See on <a href="https://news.ycombinator.com/item?id=44194468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	  <p>I want you to share in my shame at daring to make a search library. And in this shame, you too, can experience the humility and understanding of what a real, honest-to-goodness, not side-project, search engine does to make lexical search fast.</p>

<p><a href="https://github.com/beir-cellar/beir">BEIR</a> is a set of Information Retrieval benchmarks, oriented around question-answer use cases.</p>

<p>My side project, <a href="http://github.com/softwaredoug/searcharray">SearchArray</a> adds full text search to Pandas. So naturally, to see stand in awe at my amazing developer skills, I wanted to use BEIR to compare SearchArray to Elasticsearch (w/ same query + tokenization). So I spent a Saturday integrating SearchArray into BEIR, and measuring its relevence and performance on MSMarco Passage Retrieval corpus (8M docs).</p>

<p>… and 🥁</p>

<table>
  <thead>
    <tr>
      <th>Library</th>
      <th>Elasticsearch</th>
      <th>SearchArray</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>NDCG@10</td>
      <td>0.2275</td>
      <td>0.225</td>
    </tr>
    <tr>
      <td>Search Throughput</td>
      <td>90 QPS</td>
      <td>~18 QPS</td>
    </tr>
    <tr>
      <td>Indexing Throughput</td>
      <td>10K Docs Per Sec</td>
      <td>~3.5K Docs Per Sec</td>
    </tr>
  </tbody>
</table>

<p>… Sad trombone 🎺</p>

<p>It’s worse in every dimension</p>

<p>At least <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG@10</a> is nearly right, so our BM25 calculation is correct (probably due to negligible differences in tokenization)</p>

<h2 id="imposter-syndrome-anyone">Imposter Syndrome anyone?</h2>

<p>Instead of wallowing in my shame, I DO know exactly what’s going on… And it’s fairly educational. Let’s chat about why a <strong>real</strong>, non side-project, search engine is fast.</p>

<h2 id="a-magic-wand">A Magic WAND</h2>

<p>(Or how SearchArray is top 8m retrieval while Elasticsearch == top K retrieval)</p>

<p>In lexical search systems, you search for multiple terms. You take the <a href="https://en.wikipedia.org/wiki/Okapi_BM25">BM25</a> score of each term, and then finally, combine those into a final score for the document. IE, a search for <code>luke skywalker</code> really means: <code>BM25(luke) ??? BM25(skywalker)</code> where ??? is some mathematical operator.</p>

<p>In a simple “OR” query, you just take the SUM of each term for each doc, IE, a search for <code>luke skywalker</code> is <code>BM25(luke) + BM25(skywalker)</code> like so:</p>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>Doc A (BM25)</th>
      <th>Doc B (BM25)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>luke</td>
      <td>1.90</td>
      <td>1.45</td>
    </tr>
    <tr>
      <td>skywalker</td>
      <td>11.51</td>
      <td>4.3</td>
    </tr>
    <tr>
      <td>Combined doc score  (SUM)</td>
      <td>13.41</td>
      <td>5.75</td>
    </tr>
  </tbody>
</table>

<p>SearchArray just does BM25 scoring. You get back big numpy arrays of <strong>every document</strong>’s BM25 score. Then you combine the scores – literally using <code>np.sum</code>. Of course, that’s not what a search engine like Elasticsearch would do. Instead it has a different guarantee, it gets <strong>the highest scoring top N</strong> of your specified OR query.</p>

<p>This little bit of seemingly minute wiggle room gives search engines a great deal of latitude. Search engines can use an algorithm called <a href="https://www.elastic.co/blog/faster-retrieval-of-top-hits-in-elasticsearch-with-block-max-wand">Weak-AND or WAND</a> to avoid work when combining multiple term scores into the final top N results.</p>

<p>I won’t get into the full nitty gritty of the algorithm, but here’s a key intuition to noodle over:</p>

<p>A scoring system like BM25 depends heavily on document frequency of a term. So rare terms - a high (1 / document frequency) - have a higher likelihood of impacting the final score, and ending up in the top K. Luckily these terms (like <code>skywalker</code>) occur on fewer documents. So we can fetch these select, elite few docs quickly in the data structure that maps <code>skywalker -&gt; [... handful of matching doc ids...]</code> (aka postings). We can reach deeply into this list.</p>

<p>On the other hand, we can be much more circumspect about the boring, common term, <code>luke</code>. And that’s useful because <code>luke</code> has a very extensive postings list <code>luke -&gt; [... a giant honking list of documents...]</code>. We’d prefer to avoid scanning all of these.</p>

<p>We might imagine that these lists of document ids, also is paired with its <strong>term frequency</strong> how often that term occurs in that document - the other major input of BM25. And if its SORTED from highest -&gt; lowest term frequency, we can go down this list until its impossible for the BM25 score of a term to have any chance of making the top K results. Then exit early.</p>

<p>While WAND - and similar optimizations - helps Elasticsearch avoid work, SearchArray, gleefully does this work like an ignoramus happily giving you a giant idiotic array of BM25 scores.</p>

<p>When you look at this icicle graph of SearchArray’s performance doing an “OR” search, you can see all the time spent <strong>summing</strong> a giant array and also needlessly BM25 scoring many many documents.</p>

<p><img width="634" alt="image" src="https://softwaredoug.com/assets/media/2024/search-array-perf1.png"></p>

<h2 id="searcharray-doesnt-directly-store-postings">SearchArray doesn’t directly store postings</h2>

<p>Unlike most search engines, SearchArray doesn’t have postings lists of terms -&gt; documents.</p>

<p>Instead, under the hood, SearchArray stores a positional index, built first-and-foremost for <strong>phrase matching</strong>. You give SearchArray a list of terms <code>['mary', 'had', 'a', 'little', 'lamb']</code>. It then finds every place <code>mary</code> occurs one position before <code>had</code>, etc. It does this by storing, for each terms, the positions as a <a href="https://softwaredoug.com/blog/2024/01/21/search-array-phrase-algorithm">roaring bitmap</a>.</p>

<p>In our roaring bitmap, each 64 bit word has a header indicating where the positions occur (document and region in the doc). Each bit position corresponds to a position in the document. A 1 indicates this term is present, a 0 missing.</p>

<p>So to collect phrase matches, for <code>mary had</code> we can simply find places where one term’s bits occur adjacent to another. This can be done very fast with simple bit arithmetic.</p>

<div><pre><code>mary
 &lt;Roaring Headers (doc id, etc)&gt;  00000010000000    | 00000000000010
had
 &lt;Roaring Headers (doc id, etc)&gt;  00000001000000    | 00000000000001      #&lt; Left shift 1, AND, count bits to get phrase freq
...
</code></pre></div>

<p>But a nice property of this, and alleviating maintenance for this one person project, is the fact that we can also use this to compute term frequencies. Simply by performing a <code>popcount</code> (counting the number of set bits), then collecting those documents for a term, we get a mapping of doc ids -&gt; term frequencies.</p>

<p>So we spend a fair amount of time doing that, as you can see here:</p>

<p><img width="634" alt="image" src="https://softwaredoug.com/assets/media/2024/search-array-perf2.png"></p>

<p>Now I lied actually, while this is the core mechanism for storing term frequencies, we do cache. A cache that remembers the doc id -&gt; term frequencies when the roaring bit array is &gt; N 64 bit words. This lets users tune N to your memory / speed tradeoff, and get closer to a postings list.</p>

<h2 id="caching-non-term-dependent-bm25-components">Caching Non term-dependent BM25 components</h2>

<p>Take a look at this snippet for computing BM25:</p>

<div><pre><code>        bm25 = (
            term_freq / (term_freq + (k1 * ((1 - b) + (b * (doc_len / avg_doc_lens)))))
        ) * idf
</code></pre></div>

<p>Notice there is a BIG PART of this calculation that has nothing to do with the terms being searched:</p>

<div><pre><code> (k1 * ((1 - b) + (b * (doc_len / avg_doc_lens)))))
</code></pre></div>

<p>In my testing, a bit of latency (1ms on 1 million docs) can be shaved by caching everything in the <code>k1 + ... avg_doc_lens</code> somewhere. If <code>doc_len</code> corresponds to array with a doc length value for every document, you can create an array with this formula cached. But it’s a bit of a maintenance burden to have one additional, globally shared cache. So I have avoided this so far.</p>

<h2 id="caching-the-full-query-not-just-individual-bm25-term-scoring">Caching the FULL query, not just individual BM25 term scoring</h2>

<p>SearchArray is just a system for computing BM25 scores (or whatever similarity). You USE it to build up an “or query” or whatever using numpy… it doesn’t do it for you. IE the code implemented in BEIR is simply:</p>

<div><pre><code><span>def</span> <span>bm25_search</span><span>(</span><span>corpus</span><span>,</span> <span>query</span><span>,</span> <span>column</span><span>):</span>
    <span>tokenizer</span> <span>=</span> <span>corpus</span><span>[</span><span>column</span><span>].</span><span>array</span><span>.</span><span>tokenizer</span>
    <span>query_terms</span> <span>=</span> <span>tokenizer</span><span>(</span><span>query</span><span>)</span>
    <span>scores</span> <span>=</span> <span>np</span><span>.</span><span>zeros</span><span>(</span><span>len</span><span>(</span><span>corpus</span><span>))</span>
    <span>query_terms</span> <span>=</span> <span>set</span><span>(</span><span>query_terms</span><span>)</span>
    <span>for</span> <span>term</span> <span>in</span> <span>query_terms</span><span>:</span>
            <span>scores</span> <span>+=</span> <span>corpus</span><span>[</span><span>column</span><span>].</span><span>array</span><span>.</span><span>score</span><span>(</span><span>term</span><span>)</span>
    <span>return</span> <span>scores</span>
</code></pre></div>

<p>But in a regular search engine like Solr, Elasticsearch, OpenSearch, or Vespa, this logic is expressed in the search engine’s Query DSL. So the search engine can plan+cache the complete calculation, whereas SearchArray gives you all the tools to shoot yourself in the foot, performance wise (not to mention the earlier point about WAND, etc).</p>

<h2 id="thats-why-you-should-hug-a-search-engineer">That’s why you should hug a search engineer</h2>

<p>There you have it!</p>

<p>SearchArray is a tool for prototyping, using normal Pydata tooling, not for building giant retrieval systems like Elasticsearch. It’s good to know the tradeoffs behind your lexical system, as they focus on different tradeoffs. You might find it useful for dorking around on &lt; 10 million doc datasets.</p>

<p>What would be great would be if we COULD express our queries in such a dataframe-oriented DSL. IE a Polars-esque lazy top-N retrieval system that pulled from different retrieval sources, scored them, summed them, and did whatever <em>arbitrary math</em> to the underlying scores. I can cross my fingers such a thing might exist. So far people build these DAGs in less expressive ways: as part of their Torch model DAG, or some homegrown query-time DAG system.</p>

<p>In any case, I’m absolutely humbled by folks that work on big, large scale, distributed lexical search engines like (Vespa, Lucene, OpenSearch, Elasticsearch, Solr). These folks ought to be your hero too, they do this grunt work for us, and we should NOT take it for granted.</p>

<p>Below are some notes and appendices for BEIR and the different benchmarking scripts, in case you’re curious</p>

<hr>

<h4 id="appendix-links-to-scripts">Appendix links to scripts</h4>

<ul>
  <li><a href="https://github.com/softwaredoug/searcharray-beir/blob/main/evaluate_elasticsearch.py">Elasticsearch Script</a> | <a href="https://github.com/beir-cellar/beir/blob/main/beir/retrieval/search/lexical/elastic_search.py">Elasticsearch Config</a></li>
  <li><a href="https://github.com/softwaredoug/searcharray-beir/blob/main/search_array.py">SearchArray Script</a></li>
</ul>

<h4 id="appendix---how-to-integrate-with-beir">Appendix - How to integrate with BEIR…</h4>

<p>BEIR has a set of built-in datasets and metrics tools, if you implement a <code>BaseSearch</code> class with the following signature:</p>

<div><pre><code>    <span>class</span> <span>SearchArraySearch</span><span>(</span><span>BaseSearch</span><span>):</span>

        <span>def</span> <span>search</span><span>(</span><span>self</span><span>,</span>
                   <span>corpus</span><span>:</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>]],</span>
                   <span>queries</span><span>:</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>str</span><span>],</span>
                   <span>top_k</span><span>:</span> <span>int</span><span>,</span>
                   <span>*</span><span>args</span><span>,</span>
                   <span>**</span><span>kwargs</span><span>)</span> <span>-&gt;</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>Dict</span><span>[</span><span>str</span><span>,</span> <span>float</span><span>]]:</span>
</code></pre></div>

<p>The inputs:</p>

<ul>
  <li>Corpus: A dict pointing a document id to a set of fields to index, ie</li>
</ul>

<div><pre><code>{'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.',
 'title': ''}
...
</code></pre></div>

<ul>
  <li>Queries: A dict pointing a query id -&gt; query:</li>
</ul>

<div><pre><code>{"1": "Who was the original governor of the plymouth colony"}
...
</code></pre></div>

<p>Finally the output is a dictionary of query ids -&gt; {doc ids -&gt; scores} - each query w/ <code>top_k</code> scored.</p>

<p>So when search is called you need to</p>

<ol>
  <li>Index the corpus</li>
  <li>Issue all queries and gather scores</li>
</ol>

<p>Essentially this looks something like:</p>

<div><pre><code>def search(self,
           corpus: Dict[str, Dict[str, str]],
           queries: Dict[str, str],
           top_k: int,
           *args,
           **kwargs) -&gt; Dict[str, Dict[str, float]]:
    corpus = self.index_corpus(corpus)     # &lt;- add tokenized columns to dataframe
    results = {}

    for query_id, query in queries.items():    #&lt;- search and gather results
        results_for_query = some_search_function(corpus, query)
        results[query_id] = get_top_k(results_for_query)
    return results
</code></pre></div>

<p>How does this look for SearchArray?</p>

<p>To index, we loop over each str column, and add a SearchArray column to the DF. Below, tokenized with a snowball tokenizer:</p>

<div><pre><code>            for column in corpus.columns:
                if corpus[column].dtype == 'object':
                    corpus[column].fillna("", inplace=True)
                    corpus[f'{column}_snowball'] = SearchArray.index(corpus[column],
                                                                     data_dir=DATA_DIR,
                                                                     tokenizer=snowball_tokenizer)
</code></pre></div>

<p>Then replace <code>some_search_function</code> above w/ something that searches the SearchArray columns. Maybe this simple bm25_search:</p>

<div><pre><code>def bm25_search(corpus, query):
    query = snowball_tokenizer(query)
    scores = np.zeros(len(corpus))
    for q in query:
        scores += corpus['text_snowball'].array.score(q)
    return scores
</code></pre></div>

<p>(Leaving out some annoying threading, but you can look at <a href="https://github.com/softwaredoug/searcharray-beir">the code all here</a> )</p>

      


	  </div><p>I hope you join me at <a href="https://maven.com/softwaredoug/cheat-at-search">Cheat at Search with LLMs</a> to learn how to apply LLMs to search applications. Check out <a href="https://github.com/softwaredoug/softwaredoug.com/edit/master/_includes/post.html">this post</a> for a sneak preview.

		</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ClickStack – Open-source Datadog alternative by ClickHouse and HyperDX (222 pts)]]></title>
            <link>https://github.com/hyperdxio/hyperdx</link>
            <guid>44194082</guid>
            <pubDate>Thu, 05 Jun 2025 18:01:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hyperdxio/hyperdx">https://github.com/hyperdxio/hyperdx</a>, See on <a href="https://news.ycombinator.com/item?id=44194082">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">
  <a href="https://hyperdx.io/" rel="nofollow">
    <themed-picture data-catalyst-inline="true"><picture>
      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_dark.png#gh-dark-mode-only">
      <img alt="hyperdx logo" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/logo_light.png#gh-light-mode-only">
    </picture></themed-picture>
  </a>
</p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">HyperDX</h2><a id="user-content-hyperdx" aria-label="Permalink: HyperDX" href="#hyperdx"></a></p>
<p dir="auto"><a href="https://hyperdx.io/" rel="nofollow">HyperDX</a>, a core component of
<a href="https://clickhouse.com/use-cases/observability" rel="nofollow">ClickStack</a>, helps engineers
quickly figure out why production is broken by making it easy to search &amp;
visualize logs and traces on top of any ClickHouse cluster (imagine Kibana, for
ClickHouse).</p>
<p dir="auto">
  <a href="https://clickhouse.com/docs/use-cases/observability/clickstack/overview" rel="nofollow">Documentation</a> • <a href="https://hyperdx.io/discord" rel="nofollow">Chat on Discord</a>  • <a href="https://play.hyperdx.io/search" rel="nofollow">Live Demo</a>  • <a href="https://github.com/hyperdxio/hyperdx/issues/new">Bug Reports</a> • <a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contributing</a> • <a href="https://clickhouse.com/use-cases/observability" rel="nofollow">Website</a>
</p>
<ul dir="auto">
<li>🕵️ Correlate/search logs, metrics, session replays and traces all in one place</li>
<li>📝 Schema agnostic, works on top of your existing ClickHouse schema</li>
<li>🔥 Blazing fast searches &amp; visualizations optimized for ClickHouse</li>
<li>🔍 Intuitive full-text search and property search syntax (ex. <code>level:err</code>),
SQL optional!</li>
<li>📊 Analyze trends in anomalies with event deltas</li>
<li>🔔 Set up alerts in just a few clicks</li>
<li>📈 Dashboard high cardinality events without a complex query language</li>
<li><code>{</code> Native JSON string querying</li>
<li>⚡ Live tail logs and traces to always get the freshest events</li>
<li>🔭 OpenTelemetry supported out of the box</li>
<li>⏱️ Monitor health and performance from HTTP requests to DB queries (APM)</li>
</ul>

<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/hyperdxio/hyperdx/blob/main/.github/images/search_splash.png"><img alt="Search logs and traces all in one place" src="https://github.com/hyperdxio/hyperdx/raw/main/.github/images/search_splash.png" title="Search logs and traces all in one place"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Spinning Up HyperDX</h2><a id="user-content-spinning-up-hyperdx" aria-label="Permalink: Spinning Up HyperDX" href="#spinning-up-hyperdx"></a></p>
<p dir="auto">HyperDX can be deployed as part of ClickStack, which includes ClickHouse,
HyperDX, OpenTelemetry Collector and MongoDB.</p>
<div dir="auto" data-snippet-clipboard-copy-content="docker run -p 8080:8080 -p 4317:4317 -p 4318:4318 docker.hyperdx.io/hyperdx/hyperdx-all-in-one"><pre>docker run -p 8080:8080 -p 4317:4317 -p 4318:4318 docker.hyperdx.io/hyperdx/hyperdx-all-in-one</pre></div>
<p dir="auto">Afterwards, you can visit <a href="http://localhost:8080/" rel="nofollow">http://localhost:8080</a> to access the HyperDX UI.</p>
<p dir="auto">If you already have an existing ClickHouse instance, want to use a single
container locally, or are looking for production deployment instructions, you
can view the different deployment options in our
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/deployment" rel="nofollow">deployment docs</a>.</p>
<blockquote>
<p dir="auto">If your server is behind a firewall, you'll need to open/forward port 8080,
8000 and 4318 on your firewall for the UI, API and OTel collector
respectively.</p>
</blockquote>
<blockquote>
<p dir="auto">We recommend at least 4GB of RAM and 2 cores for testing.</p>
</blockquote>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hosted ClickHouse Cloud</h3><a id="user-content-hosted-clickhouse-cloud" aria-label="Permalink: Hosted ClickHouse Cloud" href="#hosted-clickhouse-cloud"></a></p>
<p dir="auto">You can also deploy HyperDX with ClickHouse Cloud, you can
<a href="https://console.clickhouse.cloud/signUp" rel="nofollow">sign up for free</a> and get started in
just minutes.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Instrumenting Your App</h2><a id="user-content-instrumenting-your-app" aria-label="Permalink: Instrumenting Your App" href="#instrumenting-your-app"></a></p>
<p dir="auto">To get logs, metrics, traces, session replay, etc into HyperDX, you'll need to
instrument your app to collect and send telemetry data over to your HyperDX
instance.</p>
<p dir="auto">We provide a set of SDKs and integration options to make it easier to get
started with HyperDX, such as
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/browser" rel="nofollow">Browser</a>,
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/nodejs" rel="nofollow">Node.js</a>,
and
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack/sdks/python" rel="nofollow">Python</a></p>
<p dir="auto">You can find the full list in
<a href="https://clickhouse.com/docs/use-cases/observability/clickstack" rel="nofollow">our docs</a>.</p>
<p dir="auto"><strong>OpenTelemetry</strong></p>
<p dir="auto">Additionally, HyperDX is compatible with
<a href="https://opentelemetry.io/" rel="nofollow">OpenTelemetry</a>, a vendor-neutral standard for
instrumenting your application backed by CNCF. Supported languages/platforms
include:</p>
<ul dir="auto">
<li>Kubernetes</li>
<li>Javascript</li>
<li>Python</li>
<li>Java</li>
<li>Go</li>
<li>Ruby</li>
<li>PHP</li>
<li>.NET</li>
<li>Elixir</li>
<li>Rust</li>
</ul>
<p dir="auto">(Full list <a href="https://opentelemetry.io/docs/instrumentation/" rel="nofollow">here</a>)</p>
<p dir="auto">Once HyperDX is running, you can point your OpenTelemetry SDK to the
OpenTelemetry collector spun up at <code>http://localhost:4318</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We welcome all contributions! There's many ways to contribute to the project,
including but not limited to:</p>
<ul dir="auto">
<li>Opening a PR (<a href="https://github.com/hyperdxio/hyperdx/blob/main/CONTRIBUTING.md">Contribution Guide</a>)</li>
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Submitting feature requests or bugs</a></li>
<li>Improving our product or contribution documentation</li>
<li>Voting on <a href="https://github.com/hyperdxio/hyperdx/issues">open issues</a> or
contributing use cases to a feature request</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Motivation</h2><a id="user-content-motivation" aria-label="Permalink: Motivation" href="#motivation"></a></p>
<p dir="auto">Our mission is to help engineers ship reliable software. To enable that, we
believe every engineer needs to be able to easily leverage production telemetry
to quickly solve burning production issues.</p>
<p dir="auto">However, in our experience, the existing tools we've used tend to fall short in
a few ways:</p>
<ol dir="auto">
<li>They're expensive, and the pricing has failed to scale with TBs of telemetry
becoming the norm, leading to teams aggressively cutting the amount of data
they can collect.</li>
<li>They're hard to use, requiring full-time SREs to set up, and domain experts
to use confidently.</li>
<li>They requiring hopping from tool to tool (logs, session replay, APM,
exceptions, etc.) to stitch together the clues yourself.</li>
</ol>
<p dir="auto">We hope you give HyperDX in ClickStack a try and let us know how we're doing!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<ul dir="auto">
<li><a href="https://github.com/hyperdxio/hyperdx/issues/new">Open an Issue</a></li>
<li><a href="https://discord.gg/FErRRKU78j" rel="nofollow">Discord</a></li>
<li><a href="mailto:support@hyperdx.io">Email</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">HyperDX Usage Data</h2><a id="user-content-hyperdx-usage-data" aria-label="Permalink: HyperDX Usage Data" href="#hyperdx-usage-data"></a></p>
<p dir="auto">HyperDX collects anonymized usage data for open source deployments. This data
supports our mission for observability to be available to any team and helps
support our open source product run in a variety of different environments.
While we hope you will continue to support our mission in this way, you may opt
out of usage data collection by setting the <code>USAGE_STATS_ENABLED</code> environment
variable to <code>false</code>. Thank you for supporting the development of HyperDX!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto"><a href="https://github.com/hyperdxio/hyperdx/blob/main/LICENSE">MIT</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[X changes its terms to bar training of AI models using its content (165 pts)]]></title>
            <link>https://techcrunch.com/2025/06/05/x-changes-its-terms-to-bar-training-of-ai-models-using-its-content/</link>
            <guid>44193390</guid>
            <pubDate>Thu, 05 Jun 2025 16:52:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2025/06/05/x-changes-its-terms-to-bar-training-of-ai-models-using-its-content/">https://techcrunch.com/2025/06/05/x-changes-its-terms-to-bar-training-of-ai-models-using-its-content/</a>, See on <a href="https://news.ycombinator.com/item?id=44193390">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	<div>
		<figure><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?w=1024" alt="" decoding="async" fetchpriority="high" srcset="https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg 1920w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=150,84 150w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=300,169 300w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=768,432 768w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=680,383 680w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=1200,675 1200w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=1280,720 1280w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=430,242 430w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=720,405 720w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=900,506 900w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=800,450 800w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=1536,864 1536w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=668,375 668w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=1097,617 1097w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=708,398 708w, https://techcrunch.com/wp-content/uploads/2023/08/twitter-x-logo-musk-pattern-2.jpg?resize=50,28 50w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><strong>Image Credits:</strong>TechCrunch</figcaption></figure>	</div>
	<div>
						<p><time datetime="2025-06-05T06:09:55-07:00">6:09 AM PDT · June 5, 2025</time></p>											</div>
</div><div>
		
		<div>
			<div>
<p id="speakable-summary">Social network X has changed its developer agreement to prevent third parties from using the platform’s content to train large language models.</p>

<p>In an update on Wednesday, the company added a line under “Reverse Engineering and other Restrictions,” a subsection of restrictions on use: “You shall not and you shall not attempt to (or allow others to) […] use the X API or X Content to fine-tune or train a foundation or frontier model,” it reads. </p>







<p>This change comes after Elon Musk’s AI company xAI acquired X in March — understandably, xAI wouldn’t want to give its competitors free access to the social platform’s data without a sale agreement.</p>

<p>In 2023, X changed its privacy policy to <a href="https://techcrunch.com/2023/09/01/xs-privacy-policy-confirms-it-will-use-public-data-to-train-ai-models/">use public data on its site to train AI models</a>. Last October, it made further changes to allow <a href="https://techcrunch.com/2024/10/17/elon-musks-x-is-changing-its-privacy-policy-to-allow-third-parties-to-train-ai-on-your-posts/">third parties to train their models</a>. </p>

<p>Reddit has also put in place <a href="https://techcrunch.com/2024/06/25/reddits-upcoming-changes-attempt-to-safeguard-the-platform-against-ai-crawlers/">safeguards against AI crawlers</a>, and last month, The Browser Company added <a href="https://www.diabrowser.com/termsofuse" target="_blank" rel="noreferrer noopener nofollow">a similar clause</a> to its AI-focused browser Dia’s terms of use.</p>
</div>

			

			


			
			
			

			




			
			
			

			



			
<div>
	
	
	
	

	
<div>
	<p>
		Ivan covers global consumer tech developments at TechCrunch. He is based out of India and has previously worked at publications including Huffington Post and The Next Web. You can reach out to him at im[at]ivanmehta[dot]com	</p>
</div>


	
	<p>
		<a data-ctatext="View Bio" data-destinationlink="https://techcrunch.com/author/ivan-mehta/" data-event="button" href="https://techcrunch.com/author/ivan-mehta/">View Bio <svg style="width: 1em;" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"><path fill="var(--c-svg, currentColor)" d="M16.5 12 9 19.5l-1.05-1.05L14.4 12 7.95 5.55 9 4.5z"></path></svg></a>
	</p>
	
</div>


		</div>
		

		
		<div id="wp-block-techcrunch-most-popular-posts__heading">
<h2 id="h-most-popular">Most Popular</h2>

</div>
		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemini-2.5-pro-preview-06-05 (331 pts)]]></title>
            <link>https://deepmind.google/models/gemini/pro/</link>
            <guid>44193328</guid>
            <pubDate>Thu, 05 Jun 2025 16:44:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/models/gemini/pro/">https://deepmind.google/models/gemini/pro/</a>, See on <a href="https://news.ycombinator.com/item?id=44193328">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="modal-content-pro" tabindex="-1">
        
  
  
  
    
      

      
      
        
          
            <div>
              
                
                
                  
                  <div><div>
    <p>Preview</p>

    
      <h2>Gemini 2.5 Pro</h2>
    

    <p data-block-key="6huf1">Best for coding and complex prompts</p>

    
    <a data-gtm-tag="cta-selection" href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-06-05&amp;utm_source=deepmind.google&amp;utm_medium=referral&amp;utm_campaign=gdm&amp;utm_content=" rel="noopener" target="_blank">
      <span>Try in Google AI Studio</span>
      
    </a>
  </div>
      
    
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="1920" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1920-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w3840-h2400-n-nu-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="1440" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1440-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w2880-h2400-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="1024" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1024-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w2048-h2400-n-nu-rw 2x"><source type="image/webp" width="600" height="1200" srcset="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w600-h1200-n-nu-rw 1x, https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1200-h2400-n-nu-rw 2x">
      <img alt="" height="1200" role="presentation" src="https://lh3.googleusercontent.com/HTs_nyUe37i_bc2SU2dz5EH283AdGn5JR_YMMk7BfE_LZT_omu7OT5IQG6f9r3iqcRef4layxxRhXLuGZ-Cibaz8Fuy4U-mQEoZdhQgYA--qQuKiR24=w1440-h1200-n-nu" width="1440">
    </picture>
    
  
  </div>
                
              
                
                
                  
                  <gemini-large-text>
  <p data-block-key="6ej0f">Gemini 2.5 Pro is our most advanced model yet, excelling at coding and complex prompts.</p></gemini-large-text>
                
              
                
                
                  
                  <p>
    
    <h2 data-in-view="">Pro performance</h2>
    
    
  </p>
                
              
                
                
                  
                  <section>
  <ul>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w306-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w415-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w460-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w500-rw 1x, https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/FuUQuVu6jpSS6k5vE3lQy1Z2U_vr3x1ikx0_WgfWMe_bS9byPbqs4Y1afoMTnF_Xe_uTElwQerxRTevNdAXsj9eCCKKLvZrT_v5y7Dp9vWlM3DdEIg=w415" width="415">
    </picture>
    
  <div><p>Enhanced reasoning</p><p data-block-key="ym6bt">State-of-the-art in key math and science benchmarks.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w306-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w415-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w460-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w500-rw 1x, https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/1LFgm7CAbuqmBskCmTVuo7zBHpa9UwbCJAZN4lWs2WfFGMwID9QmqGyo_Oho6gj_4aTDOTz2NDp3cW1xC-RPgXbhuFxeU_HqcMu1IlzTchoQ1Sqx=w415" width="415">
    </picture>
    
  <div><p>Advanced coding</p><p data-block-key="ym6bt">Easily generate code for web development tasks.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w306-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w415-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w460-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w500-rw 1x, https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/PVyBgeY20xCZfbtqOE2lDP7YBB80jnXmgu9AYMM6SB_dse9q63BdebwprrCq63M1noPw5CmvjO4SDtcoCrs5k0d1Qij-ASHHzrus8Yw7JUwE00v6jA=w415" width="415">
    </picture>
    
  <div><p>Natively multimodal</p><p data-block-key="ym6bt">Understands input across text, audio, images and video.</p></div></div>
      </li>
    
      <li>
        


<div>
    
    
    <picture>
      <source media="(min-width: 1440px)" type="image/webp" width="306" height="172" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w306-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w612-rw 2x"><source media="(min-width: 1024px)" type="image/webp" width="415" height="233" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w415-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w830-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="460" height="258" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w460-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w920-rw 2x"><source type="image/webp" width="500" height="281" srcset="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w500-rw 1x, https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w1000-rw 2x">
      <img alt="" height="233" loading="lazy" role="presentation" src="https://lh3.googleusercontent.com/VUNQxDBiDi-DqY2DpsZ4evg1UXpfoEJ1ySyvlUcIyPbPlFS0tMVf8rIrjT7AN_jKGHtUi0X2-314iOlOMA27z1dJIaCF_whEPuxyfjR0vGscLHXypUE=w415" width="415">
    </picture>
    
  <div><p>Long context</p><p data-block-key="ym6bt">Explore vast datasets with a 1-million token context window.</p></div></div>
      </li>
    
  </ul>
</section>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Deep Think</h2>
    <p data-block-key="qbz3s">We’re making Gemini 2.5 Pro even better by introducing an enhanced reasoning mode called Deep Think.</p>
    
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <gemini-large-text>
  <p data-block-key="1m4qq">It uses our latest cutting edge research in reasoning - including parallel thinking techniques - resulting in incredible performance.</p></gemini-large-text>
                
              
                
                
                  
                  
                
              
                
                
                  
                  <div>
    <p data-in-view="">Methodology</p>
    
    <p data-block-key="943wb">All Gemini results come from our runs. USAMO 2025: https://matharena.ai. LiveCodeBench V6: * o3 High: Internal runs since numbers are not available in official leaderboard, o4-mini High: https://livecodebench.github.io/leaderboard.html (2/1/2025-5/1/2025). MMMU: Self reported by OpenAI</p>
    
  </div>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    <p data-in-view="">Preview</p>
    <h2 data-in-view="">Native audio</h2>
    <p data-block-key="qbz3s">Converse in more expressive ways with native audio outputs that capture the subtle nuances of how we speak. Seamlessly switch between 24 languages, all with the same voice.</p>
    <p data-in-view=""><a data-gtm-tag="cta-selection" href="https://aistudio.google.com/prompts/new_chat" rel="noopener" target="_blank">
      <span>Try in Google AI Studio</span>
      
    </a></p>
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <hr>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Vibe-coding nature with 2.5 Pro</h2>
    <p data-block-key="92f1c">Images transformed into code-based representations of its natural behavior.</p>
    
  </div>
                
              
                
                
                  
                  





<figure>
  

  
</figure>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Hands-on with 2.5 Pro</h2>
    <p data-block-key="92f1c">See how Gemini 2.5 Pro uses its reasoning capabilities to create interactive simulations and do advanced coding.</p>
    
  </div>
                
              
                
                
                  
                  


<gdm-carousel id="carousel-1b2e262f-afdc-4a18-b0ec-2d5c57938c97" data-in-view="">
  
  
  <div>




<div aria-label="Item 1" id="carousel-item-47e360bb-1052-4a98-b031-e53c2babca0b"><h3>Make an interactive animation</h3><p>See how Gemini 2.5 Pro uses its reasoning capabilities to create an interactive animation of “cosmic fish” with a simple prompt.</p></div>




<div aria-label="Item 2" id="carousel-item-8762ba04-d45f-460e-bd31-47513806e315"><h3>Create your own dinosaur game</h3><p>Watch Gemini 2.5 Pro create an endless runner game, using executable code from a single line prompt.</p></div>




<div aria-label="Item 3" id="carousel-item-64fc0ed6-9c1d-4ee1-897d-d100a0b5f686"><h3>Code a fractal visualization</h3><p>See how Gemini 2.5 Pro creates a simulation of intricate fractal patterns to explore a Mandelbrot set.</p></div>




<div aria-label="Item 4" id="carousel-item-49ab3970-cfaf-4be3-8a0f-8227d96efc2a"><h3>Plot interactive economic data</h3><p>Watch Gemini 2.5 Pro use its reasoning capabilities to create an interactive bubble chart to visualize economic and health indicators over time.</p></div>




<div aria-label="Item 5" id="carousel-item-fb3da96d-96a2-4ece-a55c-6fada6aa69bc"><h3>Animate complex behavior</h3><p>See how Gemini 2.5 Pro creates an interactive Javascript animation of colorful boids inside a spinning hexagon.</p></div>




<div aria-label="Item 6" id="carousel-item-88b1474a-0698-4f5d-ad60-f34fa00598bc"><h3>Code particle simulations</h3><p>Watch Gemini 2.5 Pro use its reasoning capabilities to create an interactive simulation of a reflection nebula.</p></div></div>
  
</gdm-carousel>
                
              
                
                
                  
                  <div>
    
    <h2 data-in-view="">Benchmarks</h2>
    <p data-block-key="92f1c">Gemini 2.5 Pro leads common benchmarks by meaningful margins.</p>
    
  </div>
                
              
                
                
                  
                  


                
              
                
                
                  
                  
                
              
                
                
                  
                  


                
              
            </div>
          
        
      

      
    
  
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Restricts Android Sideloading–What It Means for User Autonomy and Freedom (412 pts)]]></title>
            <link>https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/</link>
            <guid>44193198</guid>
            <pubDate>Thu, 05 Jun 2025 16:29:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/">https://puri.sm/posts/google-restricts-android-sideloading-what-it-means-for-user-autonomy-and-the-future-of-mobile-freedom/</a>, See on <a href="https://news.ycombinator.com/item?id=44193198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><ul><li><a href="#abh_about">About</a></li><li><a href="#abh_posts">Latest Posts</a></li></ul><div><section><p><a href="https://puri.sm/posts/author/purism/" title="Purism"><img src="https://puri.sm/wp-content/uploads/gravatar/head-logo.jpg" width="80" alt="Purism"></a></p><div><h3><a href="https://puri.sm/posts/author/purism/">Purism</a></h3><p>Beautiful, Secure, Privacy-Respecting Laptops, Tablets, PCs, and Phones</p></div></section><section><p><a href="https://puri.sm/posts/author/purism/" title="Purism"><img src="https://puri.sm/wp-content/uploads/gravatar/head-logo.jpg" width="80" alt="Purism"></a></p></section></div></div><p dir="auto" data-sourcepos="5:1-5:530">Google has recently implemented new restrictions on sideloading Android apps, citing growing security concerns. In a pilot program launched in Singapore, the tech giant now blocks the installation of certain sideloaded apps—particularly those requesting sensitive permissions such as SMS access or accessibility services—if they are downloaded via web browsers, messaging apps, or file managers. The move, developed in partnership with Singapore’s Cyber Security Agency, is designed to prevent fraud and malware-enabled scams.</p><p dir="auto" data-sourcepos="7:1-7:398">In parallel, Google has rolled out its Play Integrity API, which allows developers to limit app functionality when sideloaded, effectively pushing users to install apps only through the Google Play Store. These policies reinforce Google’s control over Android’s ecosystem under the guise of security but have sparked renewed concern over digital autonomy, innovation suppression, and user rights.</p><p dir="auto" data-sourcepos="9:1-9:377">Critics argue that while these measures may reduce malicious activity, they also further consolidate Google’s monopolistic grip over app distribution—restricting user freedom, innovation, and competition. Sideloading, a longstanding pillar of Android’s openness, is now being marginalized, placing the Android platform closer to the walled-garden approach of Apple’s iOS.</p><h2>A Secure, Private Alternative: Purism’s PureOS, Secure Apps, and Librem Phones</h2><p dir="auto" data-sourcepos="11:1-12:398">In response to the increasing surveillance, manipulation, and corporate overreach embedded in mainstream mobile ecosystems, Purism offers a privacy-respecting solution. Powered by PureOS, a Debian-based Linux operating system, the Librem 5 and Liberty Phones enable full user autonomy, privacy, and data sovereignty—free from the predatory surveillance capitalism that fuels targeted advertising.</p><p><img src="https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop.png" alt="" width="700" height="438" srcset="https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop.png 700w, https://puri.sm/wp-content/uploads/2022/07/l5-front-apps-shadow-crop-300x188.png 300w" sizes="(max-width: 700px) 100vw, 700px"></p><p dir="auto" data-sourcepos="14:1-14:318">PureOS supports secure, free and open source applications that do not rely on exploitative data mining, addictive algorithms, or behavioral manipulation. With no forced reliance on corporate app stores or intrusive APIs, Purism’s ecosystem restores control to the user while ensuring high levels of security and transparency.</p><p dir="auto" data-sourcepos="16:1-16:198">As Google further locks down Android and Big Tech doubles down on user exploitation, Purism stands as a beacon for those seeking ethical, secure, and open alternatives in the mobile computing space.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rare black iceberg spotted off Labrador coast could be 100k years old (138 pts)]]></title>
            <link>https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078</link>
            <guid>44193120</guid>
            <pubDate>Thu, 05 Jun 2025 16:21:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078">https://www.cbc.ca/news/canada/newfoundland-labrador/black-iceberg-labrador-coast-1.7551078</a>, See on <a href="https://news.ycombinator.com/item?id=44193120">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="detailContent"><!--$--><p><span><a href="https://www.cbc.ca/news/canada/newfoundland-labrador"><span>NL</span></a></span></p><!--/$--><p>A fish harvester from Carbonear, N.L., snapped a photo of a black iceberg while fishing for shrimp of the coast of Labrador in mid May. It caused a sensation on social media, and impressed a Memorial University professor who says it's likely a very old piece of ice dating back thousands of years.</p><h2 lang="en">Berg may contain ice coloured by millennia of dirt — or even a meteorite strike</h2><!--$--><!--/$--><!--$--><div data-cy="storyWrapper"><!--$--><figure><p><img alt="A pitch-black diamond shaped iceberg floats in the distance surrounded by other icebergs and chunks of floating ice.  " src="https://i.cbc.ca/1.7551191.1748976256!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/black-iceberg.jpg?im=" data-cy="leadmedia-story-img" fetchpriority="high"></p><figcaption>This black iceberg was spotted more than 100 kilometres off the coast of Labrador in mid-May. Fisher Hallur Antoniussen took a photo of it to show crewmates, but it quickly took off after being posted on social media. <!-- --> <!-- -->(Submitted by Hallur Antoniussen)</figcaption></figure><!--/$--><div><p dir="ltr">A rare black iceberg spotted off the coast of Labrador is making a splash on social media after a fish harvester living in Carbonear, N.L., took a photo of it while fishing for shrimp last month.</p><p dir="ltr">Originally from the Faroe Islands, Hallur Antoniussen was working with a crew on board the Saputi factory freezer trawler off the coast of Labrador in mid-May.&nbsp;</p><p dir="ltr">He'd never seen an iceberg like this one before.&nbsp;</p><p dir="ltr">"I have seen icebergs that are rolled, what they say have rolled in the beach with some rocks in it. This one here is completely different. It's not only that he is all black. He is almost ... in a diamond shape," Antoniussen said in an interview with CBC Radio's <em>Labrador Morning</em>.</p><p dir="ltr">He spotted the berg after going up the ship's crane when they were more than 100 kilometres offshore&nbsp;in the Hopedale channel, located between Nain and Hopedale.&nbsp;</p><p dir="ltr">A crew member had counted 47 icebergs in the area just the day before.&nbsp;&nbsp;</p><p dir="ltr">Antoniussen doesn't think it's a berg that tipped over — or rolled on the beach — picking up dirt and rocks after getting grounded. He's seen a lot of icebergs over his 50 years of fishing off of Greenland, and more recently off&nbsp;the Labrador coast since 1989.&nbsp;</p><p dir="ltr">The 64-year-old said it was hard to estimate the size of the iceberg at sea but figured it was at least three times the size of a regular bungalow.&nbsp;</p><p dir="ltr">He took a picture from roughly six kilometres away.&nbsp;</p><p dir="ltr">"It's something you don't see very often, and a camera is not something I run around [with] when I'm working. So, I just ran to my room and took my phone and snapped this picture," he said.&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="An aerial shot of icebergs floating in a fiord surrounded by mountains in Greenland. " srcset="https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D300 300w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D460 460w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D620 620w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D780 780w,https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=Resize%3D1180 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7551226.1748977633!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/kangerlussuaq-fjord-outlet-in-greenland.jpg?im=" data-cy="image-img"></p><figcaption>Memorial professor Lev Tarasov says ice from all over Greenland is converging toward its coastline, and then breaks off to form icebergs after reaching the water. Pictured here is the Kangerlussuaq Fjord outlet in Greenland. <!-- --> <!-- -->(Submitted by Lev Tarasov )</figcaption></figure></div><p dir="ltr">Antoniussen said the berg looked like a rock with lots of really dark greys and black veins in it, and quickly ruled out that a shadow was being cast on it.&nbsp;</p><p dir="ltr">He took the photo to show other crew members on the fishing boat. Then Antoniussen posted it on Facebook, and it soon took off, garnering hundreds of comments after being shared around.&nbsp;</p><p dir="ltr">Commenters have mused about everything from aliens to precious metals, and even dinosaurs being hidden in the ice.</p><p dir="ltr">"It's an Oil Berg," said one poster.</p><p dir="ltr">"Looks like a giant [woolly] mammoth!" exclaimed another.</p><p dir="ltr">Antoniessen is clear: this is a real photo.&nbsp;</p><p dir="ltr">Other people wondered if the iceberg has volcanic ash in it, a result of some ancient eruption.&nbsp;</p><h2 dir="ltr"><strong>An impressive iceberg&nbsp;</strong></h2><p dir="ltr">Lev Tarasov, a Memorial University professor of physical oceanography, doesn't rule that last theory&nbsp;out completely.&nbsp;</p><p dir="ltr">Tarasov&nbsp;says there are volcanoes beneath the ice caps of Iceland, and while he's not exactly sure about volcanoes in Greenland, he added that scientists have measured hotspots in the landmass's central region.</p><p dir="ltr">Like Antoniussen, he hasn't seen an iceberg quite like this one before.&nbsp;</p><p dir="ltr">Tarasov observed smaller versions of the black iceberg during his fieldwork on the Kangerlussuaq Fjord in Greenland last summer — just not as impressive, he said.&nbsp;&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="Using a saw, a man cuts into rock in Greenland with tall mountains and icebergs dotting the water below.   " src="https://i.cbc.ca/1.7551199.1748976666!/fileImage/httpImage/lev-tarasov.jpg" data-cy="image-img"></p><figcaption>Memorial University's Lev Tarasov is shown here conducted fieldwork in Greenland last summer. The professor says the black iceberg might contain ice that's more than 100,000 years old.<!-- --> <!-- -->(Submitted by Lev Tarasov)</figcaption></figure></div><p dir="ltr">He guesses the ice in the berg is at least 1,000 years old, but could also be exponentially more ancient —&nbsp;even formed as many as&nbsp;100,000 years ago.</p><p dir="ltr">Tarasov said ice from all over Greenland is slowly converging toward its coastline, and when it gets there, it breaks off to form icebergs.</p><p dir="ltr">Those icebergs can take one to three years before reaching the Newfoundland and Labrador coastline.&nbsp;</p><h2 dir="ltr"><strong>A terrestrial journey&nbsp;</strong></h2><p dir="ltr">Tarasov says it's a reminder just how dynamic ice can be.&nbsp;</p><p dir="ltr">Ice streams, also&nbsp;known as outlet glaciers,&nbsp;move much faster than other parts of the ice sheet; they carry ice from the interior, traveling through deep valleys or channels out to the coast.&nbsp;</p><p dir="ltr">They pick up rocks and dirt along the way.&nbsp;&nbsp;</p><p dir="ltr">"There's parts of the ice that are actually flowing up to 20 kilometres per year, which would mean that ... the ice is moving maybe a few metres every hour," Tarasov said.&nbsp;</p><p dir="ltr">The bottom of the ice grinds against the earth's crust, he explained. There's a whole lot of churning, turning&nbsp;all that rock and sediment into a powder that then spreads up through columns of ice.&nbsp;</p><p dir="ltr">It would take a long time for that ground-up rock to spread so uniformly throughout the ice, Tarasov said.</p><h2 dir="ltr"><strong>Tip of the iceberg&nbsp;</strong></h2><p dir="ltr">Tarasov&nbsp;theorizes that the black berg was probably part of a much larger chunk of ice before it broke off into the water.</p><p dir="ltr">"Over time, as it travels around Baffin Bay and down the coast of Labrador, it's melting away. So I think a lot of that ice is melted away. Maybe the part that's clean is underneath, right? Again, 90 per cent&nbsp;of the ice is underneath the water. So we're only seeing the tip of the iceberg on top," he said.&nbsp;</p><div dir="ltr"><figure><p><img loading="lazy" alt="This is a scene of a rocky landscape with glacial ice in Greenland. Two human subjects are dwarfed by the immensity of it.  " srcset="https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D300 300w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D460 460w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D620 620w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D780 780w,https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=Resize%3D1180 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.7551213.1748977038!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/human-scale-ice-in-greenland.jpg?im=" data-cy="image-img"></p><figcaption>Tarasov conducted fieldwork on the Kangerlussuaq Fjord in Greenland last summer, here showing the magnitude of the landscape and ice on a human scale. Note the size of human subjects in the middle of the photo. <!-- --> <!-- -->(Submitted by Lev Tarasov )</figcaption></figure></div><p dir="ltr">Tarasov thinks the iceberg rolled over at some point, and is now showing its underbelly.&nbsp;</p><p dir="ltr">He also offers another possible explanation for the iceberg's intriguing colour.</p><p dir="ltr">There is strong evidence showing that an asteroid struck the northwest corner of Greenland some 12,000 years ago, he said. The iceberg could have some dust from that meteorite strike if it came from the area.&nbsp;&nbsp;</p><p dir="ltr">No matter what, the ice likely isn't new:&nbsp;it's quite possible the dirt on the iceberg may not have seen the "light of day for hundreds of thousands of years," Tarasov said.</p><p dir="ltr"><em><strong>Download our&nbsp;</strong></em><a href="https://www.cbc.ca/newsapp/"><em><strong>free CBC News app</strong></em></a><em><strong>&nbsp;to sign up for push alerts for CBC Newfoundland and Labrador.&nbsp;</strong></em></p></div></div><!--/$--><!--$--><!--/$--><!--$--><!--/$--><!--$--><!--/$--></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I think I'm done thinking about GenAI for now (153 pts)]]></title>
            <link>https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html</link>
            <guid>44193018</guid>
            <pubDate>Thu, 05 Jun 2025 16:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html">https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html</a>, See on <a href="https://news.ycombinator.com/item?id=44193018">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <h2 id="the-problem">The Problem</h2>
<p>Like many other self-styled thinky programmer guys, I like to imagine myself as
a sort of <a href="https://en.wikipedia.org/wiki/Sherlock_Holmes">Holmesian</a> genius,
making trenchant observations, collecting them, and then synergizing them into
brilliant deductions with the keen application of my powerful mind.</p>
<p>However, several years ago, I had an epiphany in my self-concept.  I finally
understood that, to the extent that I <em>am</em> usefully clever, it is less in a
Holmesian idiom, and more, shall I we,
<a href="https://en.wikipedia.org/wiki/Adrian_Monk">Monkesque</a>.</p>
<p>For those unfamiliar with either of the respective franchises:</p>
<ul>
<li>Holmes is a towering intellect honed by years of training, who catalogues
  intentional, systematic observations and deduces logical, factual conclusions
  from those observations.</li>
<li>Monk, on the other hand, while also a reasonably intelligent guy, is highly
  neurotic, wracked by unresolved trauma and profound grief.  As both a
  consulting job and a coping mechanism, he makes a habit of erratically
  wandering into crime scenes, and, driven by a carefully managed jenga tower
  of mental illnesses, leverages his dual inabilities to solve crimes.  First,
  he is unable to filter out apparently inconsequential details, building up a
  mental rat’s nest of trivia about the problem; second, he is unable to let go
  of any minor incongruity, obsessively ruminating on the collection of facts
  until they all make sense in a consistent timeline.</li>
</ul>
<p>Perhaps surprisingly, this tendency serves both this fictional wretch of a
detective, and myself, reasonably well.  I find annoying incongruities in
abstractions and I fidget and fiddle with them until I end up building
something that <a href="https://twisted.org/">a lot of people like</a>, or perhaps
something that a smaller number of people get <a href="https://automat.readthedocs.io/en/latest/"><em>really</em> excited
about</a>.  At worst, at least <a href="https://fritter.readthedocs.io/en/latest/"><em>I</em>
eventually understand what’s going
on</a>.  This is a self-soothing
activity but it turns out that, managed properly, it can very effectively
soothe others as well.</p>
<p>All that brings us to today’s topic, which is an incongruity I cannot smooth
out or fit into a logical framework to make sense.  I am, somewhat reluctantly,
a <a href="https://blog.glyph.im/2024/05/grand-unified-ai-hype.html">genAI</a>
<a href="https://blog.glyph.im/2025/03/a-bigger-database.html">skeptic</a>.  However, I am, <em>even more</em>
reluctantly, exposed to genAI Discourse every damn minute of every damn day.
It is relentless, inescapable, and exhausting.</p>
<p>This preamble about personality should hopefully help you, dear reader, to
understand how I usually address problematical ideas by thinking and thinking
and fidgeting with them until I manage to write some words — or perhaps a new
open source package — that logically orders the ideas around it in a way which
allows my brain to calm down and let it go, and how that process is important
to me.</p>
<p>In this particular instance, however, genAI has defeated me.  I cannot make it
make sense, but I need to stop thinking about it anyway.  It is too much and I
need to give up.</p>
<p>My goal with this post is not to <em>convince</em> anyone of anything in particular —
and we’ll get to why that is a bit later — but rather:</p>
<ol>
<li>to set out my current understanding in one place, including all the various
   negative feelings which are still bothering me, so I can stop repeating it
   elsewhere,</li>
<li>to explain <em>why</em> I cannot build a case that I think <em>should</em> be particularly
   convincing to anyone else, particularly to someone who actively disagrees
   with me,</li>
<li>in so doing, to illustrate why I think the discourse is so fractious and
   unresolvable, and finally</li>
<li>to give myself, and hopefully by proxy to give others in the same situation,
   permission to just peace out of this nightmare quagmire corner of the
   noosphere.</li>
</ol>
<p>But first, just because I can’t <em>prove</em> that my interlocutors are <a href="https://xkcd.com/386/">Wrong On The
Internet</a>, doesn’t mean I won’t explain why I <em>feel</em>
like they are wrong.</p>
<h2 id="the-anti-antis">The Anti-Antis</h2>
<p>Most recently, at time of writing, there have been a spate of “the genAI
discourse is bad” articles, almost exclusively written from the perspective of,
not <em>boosters</em> exactly, but pragmatically minded (albeit concerned) genAI
users, wishing for the skeptics to be more pointed and accurate in our
critiques.  This is anti-anti-genAI content.</p>
<p>I am not going to link to any of these, because, as part of their
self-fulfilling prophecy about the “genAI discourse”, they’re <em>also</em> all bad.</p>
<p>Mostly, however, they had very little worthwhile to respond to because they
were straw-manning their erstwhile interlocutors.  They are all getting annoyed
at “bad genAI criticism” while failing to engage with — and often failing to
even <em>mention</em> — most of the actual <em>substance</em> of any serious genAI
criticism.  At least, any of the criticism that I’ve personally read.</p>
<p>I understand wanting to avoid a callout or Gish-gallop culture and just express
your own ideas.  So, I understand that they didn’t link directly to particular
sources or go point-by-point on anyone else’s writing.  Obviously I get it,
since that’s exactly what this post is doing too.</p>
<p>But if you’re going to talk about how bad the genAI conversation is, without
even <em>mentioning</em> huge categories of problem like “climate impact” or
“disinformation”<sup id="fnref:1:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:1:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:1">1</a></sup> even once, I honestly don’t know what conversation you’re
even talking about.  This is peak “make up a guy to get mad at” behavior, which
is especially confusing in this circumstance, because there’s an absolutely
<em>huge</em> crowd of actual people that you could already be mad at.</p>
<p>The people writing these pieces have historically seemed very thoughtful to me.
Some of them I know personally.  It is worrying to me that their critical
thinking skills appear to have substantially degraded <em>specifically</em> after
spending a bunch of time intensely using this technology which I believe has a
<em>scary</em> risk of <a href="https://skepchick.org/2025/05/chatgpt-is-creating-cult-leaders/">degrading one’s critical thinking
skills</a>.
Correlation is not causation or whatever, and sure, from a rhetorical
perspective this is “post hoc ergo propter hoc” and maybe a little “ad hominem”
for good measure, but correlation can still be <em>concerning</em>.</p>
<p>Yet, I cannot <em>effectively</em> respond to these folks, because they are making a
<em>practical</em> argument that I cannot, despite my best efforts, find compelling
evidence to refute categorically.  <em>My</em> experiences of genAI are all extremely
bad, but that is barely even anecdata.  <em>Their</em> experiences are
neutral-to-positive.  Little scientific data exists.  How to resolve this?<sup id="fnref:2:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:2:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:2">2</a></sup></p>
<h2 id="the-aesthetics">The Aesthetics</h2>
<p>As I begin to state my <em>own</em> position, let me lead with this: my factual
analysis of genAI is hopelessly negatively biased.  I find the vast majority of
the aesthetic properties of genAI to be <em>intensely</em> unpleasant.</p>
<p>I have been trying <em>very</em> hard to correct for this bias, to try to pay
attention to the facts and to have a clear-eyed view of these systems’
capabilities. But the feelings are visceral, and the effort to compensate is
tiring.  It is, in fact, the desire to stop making this <em>particular</em> kind of
effort that has me writing up this piece and trying to take an intentional
break from the subject, despite its intense relevance.</p>
<p>When I say its “aesthetic qualities” are unpleasant, I don’t just mean the
aesthetic elements of output of genAIs themselves. The aesthetic quality of
genAI writing, visual design, animation and so on, while <em>mostly</em> atrocious, is
also highly variable.  There are cherry-picked examples which look… fine.
Maybe even good.  For years now, there have been, famously, literally
award-winning aesthetic outputs of genAI<sup id="fnref:3:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:3:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:3">3</a></sup>.</p>
<p>While I am ideologically predisposed to see any “good” genAI art as accruing
the benefits of either a survivorship bias from thousands of terrible outputs
or simple plagiarism rather than its own inherent quality, I cannot deny that
in many cases it <em>is</em> “good”.</p>
<p>However, I am not just talking about the product, but the process; the
aesthetic experience of interfacing with the genAI system itself, rather than
the aesthetic experience of the outputs of that system.</p>
<p>I am not a visual artist and I am not really a writer<sup id="fnref:4:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:4:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:4">4</a></sup>, particularly not a
writer of fiction or anything else whose experience is primarily aesthetic.  So
I will speak directly to the experience of software development.</p>
<p>I have seen very few successful examples of using genAI to produce whole,
working systems.  There are no shortage of highly public <a href="https://neuromatch.social/@jonny/114622547164112473">miserable
failures</a>, particularly
from <a href="https://old.reddit.com/r/ExperiencedDevs/comments/1krttqo/my_new_hobby_watching_ai_slowly_drive_microsoft/">the vendors of these systems
themselves</a>,
where the outputs are confused, self-contradictory, full of subtle errors and
generally unusable.  While few studies exist, it sure <em>looks</em> like this is an
automated way of producing a <a href="https://wiki.c2.com/?NetNegativeProducingProgrammer">Net Negative Productivity
Programmer</a>, throwing out
chaff to slow down the rest of the team.<sup id="fnref:5:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:5:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:5">5</a></sup></p>
<p>Juxtapose this with my aforementioned psychological motivations, to wit, I want
to have everything in the computer be <em>orderly</em> and <em>make sense</em>, I’m sure most
of you would have no trouble imagining that sitting through this sort of
practice would make me <em>extremely</em> unhappy.</p>
<p>Despite this plethora of negative experiences, executives are aggressively
mandating the use of AI<sup id="fnref:6:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:6:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:6">6</a></sup>.  It looks like <em>without</em> such mandates, most
people will not bother to use such tools, so the executives will need muscular
policies to enforce its use.<sup id="fnref:7:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:7:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:7">7</a></sup></p>
<p>Being forced to sit and argue with a robot while it struggles and fails to
produce a working output, while you have to rewrite the code at the end anyway,
is incredibly demoralizing.  This is the kind of activity that activates <a href="https://hbr.org/2019/07/6-causes-of-burnout-and-how-to-avoid-them">every
single major cause of
burnout</a> at
once.</p>
<p>But, at least in that scenario, the thing <em>ultimately doesn’t work</em>, so there’s
a hope that after a very stressful six month pilot program, you can go to
management with a pile of meticulously collected evidence, and shut the whole
thing down.</p>
<p>I am inclined to believe that, in fact, it doesn’t work well enough to be used
this way, and that we are going to see a big crash.  But that is not the most
aesthetically distressing thing.  The most distressing thing is that maybe it
<em>does</em> work; if not well enough to actually do the work, at least ambiguously
enough to fool the executives long-term.</p>
<p><a href="https://github.com/cloudflare/workers-oauth-provider?tab=readme-ov-file#written-using-claude">This
project</a>,
in particular, stood out to me as an example.  Its author, a self-professed “AI
skeptic” who “thought LLMs were glorified Markov chain generators that didn’t
actually understand code and couldn’t produce anything novel”, did a
green-field project to test this hypothesis.</p>
<p>Now, this particular project is not <em>totally</em> inconsistent with a world in
which LLMs cannot produce anything novel.  One could imagine that, out in the
world of open source, perhaps there is enough “OAuth provider written in
TypeScript” blended up into the slurry of “borrowed<sup id="fnref:8:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:8:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:8">8</a></sup>” training data that the
minor constraint of “make it work on Cloudflare Workers” is a small tweak<sup id="fnref:9:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:9:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:9">9</a></sup>. It
is not fully dispositive of the question of the viability of “genAI coding”.</p>
<p>But it is a data point related to that question, and thus it did make me
contend with what might happen if it <em>were</em> actually a fully demonstrative
example.  I reviewed the commit history, as the author suggested.  For the sake
of argument, I tried to ask myself if I would like working this way.  Just for
clarity on this question, I wanted to suspend judgement about everything else;
assuming:</p>
<ul>
<li>the model could be created with ethically, legally, voluntarily sourced
  training data</li>
<li>its usage involved consent from labor rather than authoritarian mandates</li>
<li>sensible levels of energy expenditure, with minimal CO2 impact</li>
<li>it is substantially more efficient to work this way than to just write the
  code yourself</li>
</ul>
<p>and so on, and so on… would I <em>like</em> to use this magic robot that could mostly
just emit working code for me?  Would I use it if it were <em>free</em>, in all senses
of the word?</p>
<p>No. I absolutely would not.</p>
<p>I found the experience of reading this commit history and imagining myself
using such a tool — without exaggeration — nauseating.</p>
<p>Unlike <a href="https://duckduckgo.com/?q=i+hate+code+review">many programmers</a>, I love
code review.  I find that it is one of the best parts of the process of
programming.  I can help people learn, and develop their skills, and learn
<em>from</em> them, and appreciate the decisions they made, develop an impression of a
fellow programmer’s style.  It’s a great way to build a mutual theory of mind.</p>
<p>Of course, it can still be really annoying; people make mistakes, often can’t
see things I find obvious, and in particular when you’re reviewing a lot of
code from a lot of different people, you often end up having to repeat
explanations of the <em>same</em> mistakes.  So I can see why many programmers,
particularly those more introverted than I am, hate it.</p>
<p>But, ultimately, when I review their code and work hard to provide clear and
actionable feedback, people learn and grow and it’s worth that investment in
inconvenience.</p>
<p>The process of coding with an “agentic” LLM appears to be the process of
carefully distilling all the worst parts of code review, and removing and
discarding all of its benefits.</p>
<p>The lazy, dumb, lying robot asshole keeps making the same mistakes over and
over again, never improving, never genuinely reacting, always obsequiously
<em>pretending</em> to take your feedback on board.</p>
<p>Even when it “does” actually “understand” and manages to load your instructions
into its context window, 200K tokens later it will slide cleanly out of its
memory and you will have to say it again.</p>
<p>All the while, it is attempting to trick you.  It gets most things right, but
it consistently makes mistakes in the places that you are least likely to
notice.  In places where a person <em>wouldn’t</em> make a mistake.  Your brain keeps
trying to develop a theory of mind to predict its behavior but there’s no mind
there, so it always behaves infuriatingly randomly.</p>
<p><a href="https://youtu.be/_2C2CNmK7dQ">I don’t think I am the only one who feels this way.</a></p>
<h2 id="the-affordances">The Affordances</h2>
<p>Whatever our environments <a href="https://en.wikipedia.org/wiki/Affordance">afford</a>,
we tend to do more of.  Whatever they resist, we tend to do less of.  So in a
world where we were all writing all of our code and emails and blog posts and
texts to each other with LLMs, what do they afford that existing tools do not?</p>
<p>As a weirdo who enjoys code review, I also enjoy process engineering.  The
central question of almost all process engineering is to continuously ask: how
shall we shape our tools, to better shape ourselves?</p>
<p>LLMs are an affordance for <em>producing more text, faster</em>.  How is that going to
shape us?</p>
<p>Again arguing in the alternative here, assuming the text is free from errors
and hallucinations and whatever, it’s all correct and fit for purpose, that
means it reduces the pain of circumstances where you have to repeat yourself.
Less pain!  Sounds great; I don’t like pain.</p>
<p>Every codebase has places where you need boilerplate.  Every organization has
defects in its information architecture that require repetition of certain
information rather than a link back to the authoritative source of truth.
Often, these problems persist for a very long time, because it is difficult to
overcome the institutional inertia required to make real progress rather than
going along with the status quo.  But this is often where the highest-value
projects can be found. <a href="https://www.phrases.org.uk/meanings/408900.html">Where there’s muck, there’s
brass</a>.</p>
<p>The process-engineering function of an LLM, therefore, is to prevent
fundamental problems from ever getting fixed, to reward the rapid-fire
overwhelm of infrastructure teams with an immediate, catastrophic cascade of
legacy code that is now much harder to delete than it is to write.</p>
<hr>
<p>There is a scene in Game of Thrones where Khal Drogo kills himself.  He does so
by replacing a stinging, burning, therapeutic antiseptic wound dressing with
some cool, soothing mud.  The mud felt nice, addressed the immediate pain,
removed the discomfort of the antiseptic, and immediately gave him a lethal
infection.</p>
<p>The pleasing feeling of immediate progress when one prompts an LLM to solve
some problem feels like cool mud on my brain.</p>
<h3 id="the-economics">The Economics</h3>
<p>We are in the middle of a <a href="https://blog.glyph.im/2024/05/grand-unified-ai-hype.html">mania</a> around
this technology.  As I have written about before, I believe the mania will end.
There will then be a crash, and a “winter”.  But, as I may not have stressed
sufficiently, this crash will be the biggest of its kind — so big, that it is
arguably not of a kind at all.  The level of investment in these technologies
is <em>bananas</em> and the possibility that the investors will recoup their
investment seems close to zero.  Meanwhile, that cost keeps going up, and up,
and up.</p>
<p>Others have reported on this in detail<sup id="fnref:10:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:10:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:10">10</a></sup>, and I will not reiterate that all
here, but in addition to being a looming and scary industry-wide (if we are
lucky; more likely it’s probably “world-wide”) economic threat, it is also
going to drive some panicked behavior from management.</p>
<p><a href="https://en.wikipedia.org/wiki/Elite_panic">Panicky behavior from management</a>
stressed that their idea is not panning out is, famously, the cause of much
human misery.  I expect that even in the “good” scenario, where <em>some</em> profit
is ultimately achieved, will still involve mass layoffs rocking the industry,
panicked re-hiring, destruction of large amounts of wealth.</p>
<p>It feels bad to think about this.</p>
<h3 id="the-energy-usage">The Energy Usage</h3>
<p>For a long time I believed that the energy impact was overstated.  I am even on
record, <a href="https://mastodon.social/@glyph/112242020641010867">about a year ago</a>,
saying I didn’t think the energy usage was a big deal.  I think I was wrong
about that.</p>
<p>It initially seemed like it was letting regular old data centers off the hook.
But recently I have learned that, while the numbers are incomplete because the
vendors aren’t sharing information, they’re also <em>extremely</em> bad.<sup id="fnref:11:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:11:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:11">11</a></sup></p>
<p>I think there’s probably a version of this technology that isn’t a climate
emergency nightmare, but that’s not the version that the general public has
access to today.</p>
<h2 id="the-educational-impact">The Educational Impact</h2>
<p>LLMs are making academic cheating <em>incredibly</em> rampant.<sup id="fnref:12:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:12:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:12">12</a></sup></p>
<p>Not only is it so common as to be nearly universal, it’s also extremely harmful
to learning.<sup id="fnref:13:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:13:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:13">13</a></sup></p>
<p>For learning, genAI is a <a href="https://bsky.app/profile/samhalpert.bsky.social/post/3lmt3coqvqk2w">forklift at the
gym</a>.</p>
<p>To some extent, LLMs are simply revealing a structural rot within education and
academia that has been building for decades if not centuries.  But it was
within those inefficiencies and the inconveniences of the academic experience
that real learning <em>was</em>, against all odds, still happening in schools.</p>
<p>LLMs produce a frictionless, streamlined process where students can
effortlessly glide through the entire credential, learning nothing.  Once
again, they dull the pain without regard to its cause.</p>
<p>This is not good.</p>
<h2 id="the-invasion-of-privacy">The Invasion of Privacy</h2>
<p>This is obviously only a problem with the big cloud models, but then, the big
cloud models are the only ones that people actually use.  If you are having
conversations about anything private with ChatGPT, you are sending all of that
private information directly to Sam Altman, to do with as he wishes.</p>
<p>Even if you don’t think he is a particularly bad guy, maybe he won’t even
create the privacy nightmare on purpose.  Maybe he will be forced to do so as a
result of some bizarre kafkaesque accident.<sup id="fnref:14:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:14:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:14">14</a></sup></p>
<p>Imagine the scenario, for example, where a woman is tracking her cycle and
uploading the logs to ChatGPT so she can chat with it about a health concern.
Except, surprise, you don’t have to imagine, you can just search for it, as <em>I</em>
have personally, organically, seen three separate women on YouTube, at least
one of whom <em>lives in Texas</em>, not only do this on camera but <em>recommend doing
this to their audiences</em>.</p>
<p>Citation links withheld on this particular claim for hopefully obvious reasons.</p>
<p>I assure you that I am neither particularly interested in menstrual products
nor genAI content, and if <em>I</em> am seeing this more than once, it is probably a
distressingly large trend.</p>
<h2 id="the-stealing">The Stealing</h2>
<p>The training data for LLMs is stolen.  I don’t mean like “pirated” in the sense
where someone illicitly shares a copy they obtained legitimately; I mean their
scrapers are ignoring both norms<sup id="fnref:15:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:15:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:15">15</a></sup> and laws<sup id="fnref:16:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:16:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:16">16</a></sup> to obtain copies under false
pretenses, destroying other people’s infrastructure<sup id="fnref:17:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:17:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:17">17</a></sup>.</p>
<h2 id="the-fatigue">The Fatigue</h2>
<p>I have provided references to numerous articles outlining rhetorical and
sometimes data-driven cases for the existence of certain properties and
consequences of genAI tools.  But I can’t <em>prove</em> any of these properties,
either at a point in time or as a durable ongoing problem.</p>
<p>The LLMs themselves are simply too large to model with the usual kind of
heuristics one would use to think about software.  I’d sooner be able to
predict the physics of dice in a casino than a 2 trillion parameter neural
network.  They resist scientific understanding, not just because of their size
and complexity, but because unlike a natural phenomenon (which could of course
be considerably larger and more complex) they <em>resist experimentation</em>.</p>
<p>The first form of genAI resistance to experiment is that every discussion is a
<a href="https://en.wikipedia.org/wiki/Motte-and-bailey_fallacy">motte-and-bailey</a>.  If
I use a free model and get a bad result I’m told it’s because I should have
used the paid model.  If I get a bad result with ChatGPT I should have used
Claude. If I get a bad result with a chatbot I need to start using an agentic
tool. If an agentic tool deletes my hard drive by putting <code>os.system(“rm -rf
~/”)</code> into <code>sitecustomize.py</code> then I guess I should have built my own MCP
integration with a completely novel heretofore never even considered security
sandbox or something?</p>
<p>What configuration, exactly, would let me make a categorical claim about these
things?  What specific methodological approach should I stick to, to get
reliably adequate prompts?</p>
<p>For the record though, if the idea of the free models is that they are going to
be provocative demonstrations of the impressive capabilities of the commercial
models, and the results are consistently dogshit, I am finding it increasingly
<a href="https://ioc.exchange/@kevinriggle/114617713278070348">hard to care</a> how much
better the paid ones are supposed to be, especially since the “better”-ness
cannot really be quantified in any meaningful way.</p>
<p>The motte-and-bailey doesn’t stop there though.  It’s a war on all fronts.
Concerned about energy usage?  That’s OK, you can use a local model.  Concerned
about infringement?  That’s okay, somewhere, somebody, maybe, has figured out
how to train models consensually<sup id="fnref:18:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:18:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:18">18</a></sup>.  Worried about the politics of enriching
the richest monsters in the world?  Don’t worry, you can always download an
“open source” model from Hugging Face.  It doesn’t matter that many of these
properties are mutually exclusive and attempting to fix one breaks two others;
there’s always an answer, the field is so abuzz with so many people trying to
pull in so many directions at once that it is legitimately difficult to
understand what’s going on.</p>
<p>Even here though, I can see that characterizing everything this way is unfair
to a hypothetical sort of person.  If there is someone working at one of these
thousands of AI companies that have been springing up like toadstools after a
rain, and they <em>really are</em> solving one of these extremely difficult problems,
how can I handwave that away?  We need people working on problems, that’s like,
the whole point of having an economy.  And I really don’t like shitting on
other people’s earnest efforts, so I try not to dismiss whole fields.  Given
how AI has gotten into <em>everything</em>, in a way that e.g. cryptocurrency never
did, painting with that broad a brush inevitably ends up tarring a bunch of
stuff that isn’t even really AI at all.</p>
<p>The second form of genAI resistance to experiment is the inherent obfuscation
of productization.  The models themselves are already complicated enough, but
the <em>products</em> that are built around the models are evolving extremely rapidly.
ChatGPT is not just a “model”, and with the rapid<sup id="fnref:19:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:19:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:19">19</a></sup> deployment of Model
Context Protocol tools, the edges of all these things will blur even further.
Every LLM is now just an enormous unbounded soup of arbitrary software doing
arbitrary whatever.  How could I possibly get my arms around that to understand
it?</p>
<h2 id="the-challenge">The Challenge</h2>
<p>I have woefully little experience with these tools.</p>
<p>I’ve tried them out a little bit, and almost every single time the result has
been a disaster that has not made me curious to push further.  Yet, I keep
hearing from all over the industry that I should.</p>
<p>To some extent, I feel like the motte-and-bailey characterization above is
fair; if the technology itself can really do real software development, it
ought to be able to do it in multiple modalities, and there’s nothing anyone
can <em>articulate</em> to me about GPT-4o which puts it in a fundamentally different
class than GPT-3.5.</p>
<p>But, also, I consistently hear that the <em>subjective experience</em> of using the
premium versions of the tools is actually good, and the free ones are actually
bad.</p>
<p>I keep struggling to find ways to try them “the right way”, the way that people
I know and otherwise respect claim to be using them, but I haven’t managed to
do so in any meaningful way yet.</p>
<p>I do not want to be using the cloud versions of these models with their
potentially hideous energy demands; I’d like to use a local model.  But there
is obviously not a nicely composed way to use local models like this.</p>
<p>Since there are apparently <em>zero</em> models with ethically-sourced training data,
and litigation is ongoing<sup id="fnref:20:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:20:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:20">20</a></sup> to determine the legal relationships of training
data and outputs, even if I can be comfortable with some level of plagiarism on
a project, I don’t feel that I can introduce the existential legal risk into
<a href="https://pypi.org/user/glyph/">other people’s infrastructure</a>, so I would need
to make a <em>new</em> project.</p>
<p>Others have differing opinions of course, including some within my dependency
chain, which does worry me, but I still don’t feel like I can freely contribute
further to the problem; it’s going to be bad enough to unwind any impact
upstream.  Even just for my own sake, I don’t want to make it worse.</p>
<p>This especially presents a problem because I have <a href="https://mastodon.social/@glyph/112498550495367755">way too much stuff going
on</a> already.  A new project
is not practical.</p>
<p>Finally, even if I <em>did</em> manage to satisfy all of my quirky<sup id="fnref:21:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:21:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:21">21</a></sup> constraints,
would this experiment really be worth anything?  The models and tools that
people are raving about are the big, expensive, harmful ones.  If I proved to
myself yet again that a small model with bad tools was unpleasant to use, I
wouldn’t really be addressing my opponents’ views.</p>
<p>I’m stuck.</p>
<h2 id="the-surrender">The Surrender</h2>
<p>I am writing this piece to make <em>my</em> peace with giving up on this topic, at
least for a while.  While I do idly hope that some folks might find bits of it
convincing, and perhaps find ways to be more mindful with their own usage of
genAI tools, and consider the harm they may be causing, that’s not actually the
goal.  And that is not the goal because it is just so much goddamn <em>work</em> to
prove.</p>
<p>Here, I must return to my philosophical hobbyhorse of
<a href="https://en.wikipedia.org/wiki/Language_game_(philosophy)">sprachspiel</a>.  In
this case, specifically to use it as an analytical tool, not just to understand
<em>what</em> I am trying to say, but what the <em>purpose</em> for my speech is.</p>
<p>The concept of sprachspiel is most frequently deployed to describe the <em>goal</em>
of the language game being played, but in game theory, that’s only half the
story.  Speech — particularly rigorously justified speech — has a <em>cost</em>, as
well as a benefit.  I can make shit up pretty easily, but if I want to do
anything remotely like scientific or academic rigor, that cost can be
astronomical.  In the case of developing an abstract understanding of LLMs, the
cost is just too high.</p>
<p>So what is my goal, then?  To be king Canute, standing astride the shore of
“tech”, whatever that is, commanding the LLM tide not to rise?  This is a
multi-trillion dollar juggernaut.</p>
<p>Even the rump, loser, also-ran fragment of it has the power to literally
suffocate us in our homes<sup id="fnref:22:i-think-im-done-thinking-about-genai-for-now-2025-6"><a href="#fn:22:i-think-im-done-thinking-about-genai-for-now-2025-6" id="fnref:22">22</a></sup> if they so choose, completely insulated from any
consequence.  If the power curve starts there, imagine what the <em>winners</em> in
this industry are going to be capable of, irrespective of the technology
they’re building - just with the resources they have to hand.  Am I going to
write a blog post that can rival their propaganda apparatus?  Doubtful.</p>
<p>Instead, I will just have to concede that maybe I’m wrong.  I don’t have the
skill, or the knowledge, or the energy, to demonstrate with any level of rigor
that LLMs are generally, in fact, hot garbage.  Intellectually, I will have to
acknowledge that maybe the boosters are right.  Maybe it’ll be OK.</p>
<p>Maybe the carbon emissions aren’t so bad.  Maybe everybody is keeping them
secret in ways that they don’t for other types of datacenter for perfectly
legitimate reasons.  Maybe the tools really can write novel and correct code,
and with a little more tweaking, it won’t be so difficult to get them to do it.
Maybe by the time they become a mandatory condition of access to developer
tools, they won’t be miserable.</p>
<p>Sure, I even sincerely agree, intellectual property really has been a pretty
bad idea from the beginning.  Maybe it’s OK that we’ve made an exception to
those rules.  The rules were stupid anyway, so what does it matter if we let a
few billionaires break them?  Really, everybody should be able to break them
(although of course, regular people can’t, because we can’t afford the lawyers
to fight off the MPAA and RIAA, but that’s a problem with the legal system, not
tech).</p>
<p>I come not to praise “AI skepticism”, but to bury it.</p>
<p>Maybe it really is all going to be fine.  Perhaps I am simply catastrophizing;
I have been known to do that from time to time.  I can even sort of believe it,
in my head.  Still, even after writing all this out, I can’t quite manage to
believe it in the pit of my stomach.</p>
<p>Unfortunately, that feeling is not something that you, or I, can argue with.</p>
<hr>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>Thank you to <a href="https://blog.glyph.im/pages/patrons.html">my patrons</a>. Normally, I would say, “who are
supporting my writing on this blog”, but in the case of this piece, I feel more
like I should apologize to them for this than to thank them; these thoughts
have been preventing me from thinking more productive, useful things that I
actually have relevant skill and expertise in; this felt more like a creative
blockage that I just needed to expel than a deliberately written article.  If
you like what you’ve read here and you’d like to read more of it, well, too
bad; I am <em>sincerely</em> determined to stop writing about this topic.  But, if
you’d like to read more stuff like <em>other</em> things I have written, or you’d like
to support my <a href="https://github.com/glyph/">various open-source endeavors</a>, you
can <a href="https://blog.glyph.im/pages/patrons.html">support my work as a sponsor</a>!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Seven Days at the Bin Store (192 pts)]]></title>
            <link>https://defector.com/seven-days-at-the-bin-store</link>
            <guid>44192995</guid>
            <pubDate>Thu, 05 Jun 2025 16:07:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://defector.com/seven-days-at-the-bin-store">https://defector.com/seven-days-at-the-bin-store</a>, See on <a href="https://news.ycombinator.com/item?id=44192995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>This spring, a new business opened on the main drag of my West Philadelphia neighborhood, provoking both excitement and trepidation.</p><p>“I saw it just the other day and feared it,” one friend texted. “Like what the actual fuck is that shit,” said another. “Why?!!!” said a third. “Who is that for?”</p><p>Until last summer, the corner storefront at Baltimore Avenue and S. Melville Street was a moderately overpriced hipster vintage store. After it closed, it sat empty for nearly a year. Then, in March, it sprang to life.&nbsp;</p><p>A “grand opening” banner went up over the door. A miniature wacky inflatable tube man flailed around outside. Posters with the logos for Walmart, Amazon, Costco, and Best Buy covered the windows, declaring “CRAZY DEALS, AMAZING BINZ.” I had to check it out.</p><p>AMAZING BINZ is on the first floor of a rowhome, narrow and long. There’s one central aisle, and on either side, big wooden tray tables—the proverbial binz—overflowing with undifferentiated piles of consumer <em>stuff</em>: unopened Halloween costumes, an ice mold shaped like a penis, a banner of many glittery penises wearing grass skirts, a staggering number of “reusable hot and cold gel compression sleeve[s] for elbow,” a single loose pregnancy test, something called Wokaar for “waxing the nose beard.”</p><p>At least half the products are still in boxes, and there are signs on the walls warning customers to NOT OPEN THE BOXES, so you have to scan the barcode with your phone or decipher clipped product descriptions: “module stool,” “gratitude journal,” “Xmas tea light green.” But the great innovation of Amazing Binz is its pricing structure, which is splashed across the facade in Spanish and English and makes good on the promise of CRAZY DEALS. On Fridays, when the bins are freshly stocked, everything costs $10. On Saturdays, $8. Sundays: $6. Mondays: $4. Tuesdays: $2. Wednesdays: $1. Thursday is bin store Sabbath, when the shop is closed and restocked.&nbsp;</p><p>The store is like nothing else on the block, which boasts, among other things, a yoga studio, two vape shops, a volunteer-run book store <em>and </em>a Marxist reading room, and no fewer than four Ethiopian joints.&nbsp;</p><p>Where did all this stuff come from? Who opened this store, and why? Is it profitable? What does it mean for the uneven gentrification of Baltimore Avenue and West Philadelphia? I decided to spend a week visiting Amazing Binz every day. Here is what I found.</p><h2><strong>Thursday: Restock Day</strong></h2><p>On Thursdays, Amazing Binz is closed. One of the owners, Ahmed, has graciously allowed me to observe the restock. Overall, Ahmed has been very gracious about my fixation on his store.</p><p>When I arrive at 10:30 a.m., he’s smoking a cigarette and nervously awaiting a delivery truck with 18 pallets of mixed merchandise from Target. Amazing Binz gets its inventory from major corporations’ overstock and returned products, taking advantage of the vast weird world of “reverse logistics.”&nbsp;</p><p>“ You've got the front end, where you order the product, it comes into the port, it goes to a warehouse, to a store, and then to our front door,” says Cathy Roberson, a researcher at the Reverse Logistics Association, an industry trade group. “What if once it hits our front door, we don't like it, or it's broken or something? That's the reverse part. ”&nbsp;</p><p>Returns, repairs, refurbished products, and even recalls fall into the purview of reverse logistics. They are joined there by products that never made it to a consumer because the season ended, or a box was a little dented, or the purchaser never picked up their order, or a retailer was just running out of room in their warehouse.</p><p>That pile of excess stuff is growing. About 17 percent of all merchandise gets returned, according to the National Retail Federation. That’s up from just eight percent in 2019. For online purchases, it’s almost 30 percent.&nbsp;</p><p>Liquidators are nothing new: T.J. Maxx, Ocean State Job Lot, Nordstrom Rack. But as the scale of the excess grows, the reverse logistics industry is expanding, and methods of disposal are diversifying. Some corporations, like Amazon, do their own reselling through a <a href="https://www.amazon.com/Amazon-Bulk-Liquidations/b?ie=UTF8&amp;node=23511005011" target="_blank" rel="noreferrer noopener">bulk liquidation page</a>. There are middlemen like <a href="https://bstock.com/amazoneu/" target="_blank" rel="noreferrer noopener">B-Stock</a>, an eBay-like site where anyone can shop for truckloads of unwanted stuff. Influencers are buying pallets to unbox on stream, capitalizing on both the goods and the views. It’s a whole universe of brokers, wholesalers, and secondhand retail all trying to claw back a little bit of money from the growing pile. That’s where Amazing Binz comes in.</p><p>“The goal for all the reverse logistics stuff,” says Roberson, “is to keep things out of the landfill.”&nbsp;</p><p>Today, Ahmed is expecting half a truckload with about 3,500 to 4,500 individual pieces, ranging from kitchen appliances to toys to clothing; he doesn’t know exactly what. The semi-truck pulls up right on time, and Ahmed unloads it with a forklift, cigarette still in mouth. Pallets fill the sidewalk, glistening shrink-wrapped towers of stuff: air fryers, microwaves, a vacuum, a sled, a machine that tosses a football, My Little Pony-branded diapers, and a go-kart.&nbsp;</p><figure><img alt="Shrinkwrapped pallets outside the bin store." src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2282.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>Inside the store, the bins are still half-full from the last truckload, which came from Amazon. There are many products I’ve only ever seen on the internet: a carrying case and monogrammed straw cover for a Stanley cup, a big plastic grinder for shredding chicken, silicone molds for making the viral Dubai chocolate bar at home.&nbsp;</p><p>For this restock, Ahmed was hoping for a different caliber of item. It’s a mixed success: some quality products, but more clothing and returned items than he was hoping for. Electronics are the holy grail; appliances and home goods do well, too. The best items are the ones that retain their value, because Ahmed sees his customers as agents in the reverse logistics economy, too.</p><p>“You can buy stuff from here and then you can resell it,” Ahmed tells me. “Through eBay or Amazon. You can sell on Facebook market. You will get your money back in less than one day if you wanna resell it. Which is a good thing for everybody right now, for the public. A lot of people need to work.”</p><p>It’s a slog of a day. I leave after a few hours, and come back at 6:00 p.m., nearly eight hours into this operation. Ahmed and two helpers are still unloading and stocking. They won’t be done until after 10:00 p.m.</p><h2><strong>Friday: $10</strong></h2><p>I get to Amazing Binz at 9:35 a.m. It opens at 10:00 a.m. Already, two people are standing at the door with their faces pressed to the glass.&nbsp;</p><p>Soon, six people are waiting, then a dozen. A woman and her kids show up in a taxi. At least one person has come from the county over. Amazing Binz <a href="https://www.instagram.com/amazingbinz/" target="_blank" rel="noreferrer noopener">posts their new hauls on Instagram</a>, so some people have already scouted what they want: a kid’s scooter, a shower caddy. Fridays offer the highest upside: a chance to score something really valuable for just $10. All the real shoppers are lined up this morning, looking for an edge.&nbsp;</p><p>People are arriving every minute; the line swells to 25. Most of the people waiting are middle-aged black men and women. Some are resellers, and many seem well-versed in Philadelphia’s liquidation economy. A few bring up <a href="https://turn7.com/" target="_blank" rel="noreferrer noopener">Turn 7</a>, a giant liquidation warehouse with a bin model that closed last month. I learn about another bin store in New Jersey, and one in Northeast Philly called <a href="https://www.instagram.com/blackfridayoutletphilly/?hl=en" target="_blank" rel="noreferrer noopener">Black Friday Outlet</a>, a nationwide bin store chain. Someone tells me there’s another bin store opening up <em>today</em>, just 10 blocks away.</p><p>“When I first got into this industry, there were no more than 10 bin stores in the entire country,” says Nebraska-based wholesaler and content creator Colton Carlson, who opened his first store in 2018. “Fast forward a few years, [and] there ended up being 3,000 bin stores.” Today, he thinks there could be as many as 10,000 in the U.S.&nbsp;</p><p>Bin stores are distinguished by their pricing model—a set price per item or weight, and falling prices to incentivize quick sales—and by their agnostic approach to inventory. They’ll take nearly anything and everything.</p><p>“Once people caught on that you could buy a truckload of inventory, you dump it in your bins and you make a ton of money, then they really just started opening that rapid-fire at that point,” says Carlson.</p><p>The explosion of online returns got the bin-store craze started, but the pandemic put it into overdrive. Factory shutdowns and slowdowns at ports meant for weird lags in the supply chain that led to seasonal products arriving past their season, or goods just being abandoned at port. To try and weather the uncertainty, and keep up with exploding online shopping, retailers overstocked on inventory.</p><p>“In the beginning of the pandemic, we bought everything in sight,” says Roberson. “Then all of a sudden we, the consumer, decided we've had enough. We've bought everything. We've gotta tighten our belts now.”</p><p>Consumer spending dropped, and retailers were left holding the bag in overstuffed warehouses. Some, like Target, “ kind of got caught with their pants down,” says Roberson. They were stuck with <a href="https://www.cnbc.com/2022/06/07/target-markdowns-plan-to-cut-inventory.html" target="_blank" rel="noreferrer noopener">so much excess inventory</a>, they sold it for pennies on the dollar. The secondhand market flooded with more cheap stuff than normal, creating opportunities for resellers.</p><p>“On opening day, we had about 300 people lined up down the street,” says Carlson, who thinks his was the biggest bin store in the country at the time. “Every weekend from that, it was just the same thing: a line down the street with 200, 300 people ready to find a deal.”</p><figure><img alt="Wokaar nose hair waxing kit on top of a pile of goods." src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1842.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>This Friday morning at Amazing Binz, there are now over 30 people waiting to be let inside.</p><p>Finally, the doors open and the crowd floods into the narrow shop, honing in on the premier items. Scooters and grills and Target-branded home goods fly out of the bins. People are hauling around air fryers so no one else can take them. It is, frankly, a bit of a mad house. One reseller, who didn’t find anything he wanted, says, “One of these days, someone’s going to get shot.” A woman declares, “I will never do this again.”</p><p>Generally, people seem happy with their finds. The guy who wanted the scooters gets the scooters; the woman who wanted the shower caddy gets the shower caddy. It all happens fast. By the end of the day, the first two bins are completely empty.&nbsp;</p><h2><strong>Saturday: $8</strong></h2><p>It’s farmer’s market day in West Philadelphia, Baltimore Avenue’s busiest day. The store is much calmer than Friday morning, but still doing a brisk business. Where yesterday there were piles of home goods and appliances, the bins have been restocked with less high-end fare: soccer balls, bike inner tubes, sand art kits, clothing, shoes.</p><p>I want to get a sense of how neighbors are feeling. Once, while digging in the bins, I heard a young guy muttering to himself. “This is so weird. What am I looking at?” We made eye contact. “Have you ever seen a store like this?” he asked me. “It doesn’t feel right. I don’t think this will be here long.”&nbsp;</p><p>I check West Willy, the easily parodied local Facebook group. A post about Amazing Binz has over 50 comments, ranging from “omg I am EXCITED,” to “Maybe I can find things I need and not weep over the price,” to “No offense to the owners but this store feels like where late stage capitalism goes for one final hurrah.”</p><p>I ask the book vendor who sets up outside the Marxist storefront how he feels about Amazing Binz. “It shows the corporation’s incursion into the neighborhood at every level, big to small,” he says. He adds that he likes that it’s a minority-owned business.</p><p>I ask a resident who happens to be walking by, a long-timer who moved to the neighborhood in the 1970s when it was, as he says, Philadelphia’s answer to Haight-Ashbury. “It means a potential downturn of the local market that had been popping up with restaurants and boutiques,” he says.</p><p>Baltimore Avenue runs from the University of Pennsylvania campus in the east, west through a neighborhood that’s a mixture of million-dollar houses, squats-turned-group-houses, and some of the city’s most persistent poverty. For the six-ish years I’ve lived here, I’ve watched a handful of bougie businesses open and quickly fold, leaving behind empty storefronts. Housing prices just go up and up, but the retail strip suggests an uneven gentrification. Those fluctuating fortunes are mirrored in starkly divergent views of whether Amazing Binz is an “amazing store with amazing stuff and amazing prices” (their first Google review) or “a place that sells crap” (a post on West Willy).&nbsp;</p><p>It’s an odd location for a store like this. Liquidators usually look for huge spaces, in industrial areas or big shopping centers at the edge of town. When I ask Ahmed how he came to open Amazing Binz here, he said it was actually not his first choice. He wanted to open a cafe or a sweet shop, but says he couldn’t get the permits or the zoning. The landlord wouldn’t give him a break on rent, which is nearly $4,000 a month.&nbsp;</p><p>So after about eight months of sitting on the location, Ahmed decided he needed to open something with low overhead and little startup capital. There’s a few bin stores in Northeast Philly, where he lives, so he’d seen this model before, and thought it would be good for the neighborhood. At the start of this week, he hung a Free Palestine flag behind the counter. He’s from the West Bank and felt comfortable hanging the flag because, he says, “Our neighbors support us.”</p><p>Nearly every time I’m here, there are happy customers, usually quite a few. There's a real mix of the neighborhood. Serious diggers, often in headphones, methodically overturn every pile. A PTA mom sends her husband home for the car after she fills three baskets with school supplies. Some gigglers revel in the slop and the strangely sentimental: the Dick Pics in Nature calendar, the crystal heart paperweight engraved with a 25th wedding anniversary message.</p><figure><figure><img alt="Dick Pics in Nature calendar" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1925.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><figure><img alt="crystal heart paperweight engraved with a 25th wedding anniversary message" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2303.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><span><figure></figure></span></figure><p>But there are haters. At parties and potlucks across West Philadelphia, my neighbors split into camps: for or against the bin store. Houses divide against themselves. Pros: It’s cheap, it’s fun, and at least the stuff isn’t getting thrown away. Cons: An uncanny feeling of glimpsing the collective consumer psyche, AI slop in consumer form, the horrors of production for production’s sake.&nbsp;</p><p>At the farmer’s market, a friend of a friend calls Amazing Binz “dark and sinister,” and compares it to a big sifting funnel. All the consumer drivel of the world goes in at the top, and all the unsaleable, undigested, unwanted stuff just falls out the bottom. “The bin store,” he says, “is like the last step before they just throw it in the ocean.”</p><h2><strong>Sunday: $6</strong></h2><p>When I get to Amazing Binz, Omran—Amazing Binz’s social media hype man—is <a href="https://www.instagram.com/p/DIZPZP2PCee/" target="_blank" rel="noreferrer noopener">filming a video</a> with a shopper who found the toner she usually buys at Sephora, for $65, here in the bins for $6.</p><p>This is one of Omran’s Instagram shticks. He asks customers what they bought and how much they paid, and then replies, “I know daht’s right,” a phrase also tagged on all the posts.&nbsp;</p><p>Omran does some of the ordering for Amazing Binz. He says he buys his inventory through direct relationships with people at warehouses, not through auction sites, where the prices are often higher. Ahmed tells me the average truckload costs about $16,000, and will contain thousands of individual pieces. The key is to keep the cost per item at around $2, and to get a mix of higher- and lower-value items so they balance each other out.&nbsp;</p><p>Most bin stores do a hybrid model, where they sell higher-quality stuff on the side at prices above $10. Amazing Binz calls it the VIP section. With this week’s Target haul, I assumed that the nicer items would end up there. But no, the owners priced most of it at $10, hoping the allure of a huge discount on a big-ticket item draws people in and gets them to spend more.&nbsp;</p><p>This is the bin-store promise: teasing the chance to find Oura rings and AirPods for just $10. Once, while I talked to Ahmed,<strong> </strong>a neighbor walked by and waved. “She got those Beats headphones here,” he says. “Ten dollars. She was so happy.”&nbsp;</p><p>But the price flattening works in both directions, I realize on a $6 Sunday. A friend is shopping for a mermaid-themed bachelorette party, and we find plenty: That previously mentioned garland of penises is still here (two, actually), as well as a sheet of mermaid stickers and a curly straw that spells "BRIDE."</p><p>The problem is that $6 is too expensive for a straw, especially a straw that I know has been here for three weeks, at least. This is one effect of the bin store: There’s plenty of legitimately good stuff, at good prices, but that&nbsp; usually gets snapped up in the first few days. Anything that remains through the whole pricing cycle and back again is revealed to be worth nothing at all.</p><p>My friend, digging for penis straws, says maybe it’s good to see. We have to confront this stuff. This is the cost of our collective Amazon addiction.&nbsp;</p><p>There are times when the bin store does feel like an art installation, a message, or a warning. Wander through it for a few days and you will start to feel like you are in a room in a haunted house, one where the corporeal forms of the world economy’s least-wanted products are trapped, unable to move on.</p><p>With or without Amazing Binz, all of this stuff is just out there, already made. A lot of it is useful but excessive, rendered obsolete by an overheated economy. Shopping here can feel a bit like fighting the last, losing battle of reverse logistics. What can still be made useful enough to be kept out of the landfill, just a little longer?</p><h2><strong>Monday: $4</strong></h2><p>It’s a slow day at Amazing Binz, again. This week has been big on gloomy weather, and low on foot traffic. I find a Trump 2024 Punisher logo drink koozie in the pile. These have been popping up in the bins since that first Amazon haul, along with MAGA and Let’s Go Brandon flags. Every time a customer points one out to the owners, they throw it away.</p><figure><img alt="" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1851.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><p>A woman approaches the counter with a pair of leopard-print flats, but the two shoes are connected by a stretchy white loop of elastic that’s gotten tangled up with at least five other pairs’ elastic, creating an unholy rats nest of shoes that she needs help untangling.&nbsp;</p><p>On slower days, it’s hard not to wonder how long Amazing Binz will last. Philadelphia may be behind the times. In the rest of the country, the bin-store bubble has started to burst. On YouTube, I find a cadre of wholesale and reselling influencers, including a bin-store niche. I watch their videos touting the model, how easy it is to start one, and how to succeed. Then I watch them change their tunes.</p><p>“<a href="https://www.youtube.com/watch?v=8c9US81xrM8&amp;t=10s" target="_blank" rel="noreferrer noopener">Unpopular opinion: Bin stores really suck right now</a>,” declared creator Lindey Glenn in November 2023. She linked to Carlson’s most recent video: “<a href="https://www.youtube.com/watch?v=OfFRAfKmTfQ" target="_blank" rel="noreferrer noopener">Bin store burnout</a>.”</p><p>After opening his first store in late 2018, Carlson had a few good years riding high on the novelty of the business model and the glut of unwanted pandemic-era merchandise. But as more bin stores opened, the demand and prices for the same truckloads went up. Colton says that after a while, he was just breaking even on the bins, which at his store, started at a high of only $5.&nbsp;</p><p>He found other revenue streams, like selling entire pallets to resellers, and sorting out the niche and expensive products to flip on Poshmark or eBay. But the challenges persisted. The bin store is a finely tuned operation. A broker might promise a great haul with new electronics, and then deliver a bunch of dirty and broken returns. A truckload could arrive late, or not at all, leaving owners in the lurch on what should be their most profitable day. These problems are exacerbated in small stores, like Amazing Binz, where it’s hard to store extra merchandise to weather that uncertainty.</p><p>And the stuff itself creates logistical headaches. Boxes get opened, sometimes from theft, sometimes just from jostling around in the bins. Pieces get lost, trash and detritus build up alongside the unsold products, the difference between them increasingly hard to discern. In fact, when it was time to clear his bins, Carlton would dump all of the unsold inventory mixed with trash back onto a pallet and sell the whole thing for $50, with the caveat that the buyer had to go toss the garbage somewhere else.</p><p>Carlson saw the writing on the wall. He sold his bin store in 2023. Now he’s focusing on his wholesale business, and opening a side-by-side discount grocery and clothing store, which is where he thinks the secondhand retail trends are headed.</p><p>The Reverse Logistics Association has also seen a decline in bin-store fortunes. Roberson says they lost a few bin-store members because of bankruptcy. After the pandemic, “it was getting more difficult to buy up that excess inventory because there really wasn't excess inventory.”</p><p>But economic shocks are good for the secondhand economy. Roberson thinks tariffs could, eventually, lead to another bin-store bump. At the start of 2025, she says, many retailers overstocked on inventory again, filling their warehouses to get ahead of price hikes. That could mean they’ll soon be sitting on overstock, especially if consumers decide they’re not willing to pay higher prices. Supply chain disruptions will produce more excess. Already, <a href="https://www.wsj.com/economy/trade/importers-china-trade-chaos-tariffs-b3463832?st=XFNmLu&amp;reflink=article_copyURL_share" target="_blank" rel="noreferrer noopener">goods are stalled</a> at ports around the world.&nbsp;</p><p>Roberson predicts a temporary dip in the fortunes of bin-store owners, as secondhand goods get more expensive along with everything else, and then a resurgence. Carlson agrees. “ I think the secondhand market is going to keep growing because that's the whole reason people are shopping at these stores [to see] if they can save $10. I don't see consumer spending going down for this industry, or even like thrift stores at all. I think that's gonna continuously go up. However, the problem's gonna lie on whoever owns the bin store.”</p><h2><strong>Tuesday: $2</strong></h2><p>Ahmed seems extremely tired today. He’s barely taken a day off since they opened, not even for Eid. Everything is pretty picked over. The sand art is all gone, the bedding and curtains. There’s just two soccer balls left. I do my customary scan of the bins, from the back of the store to the front. There’s nothing new, just increasingly empty boxes of gel compression elbow sleeves, loose whippit canisters,<strong> </strong>and batteries rolling across the bottom.</p><figure><figure><img alt="A bin overflowing with items" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_1920.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><figure><img alt="A pregnancy test in a bin" src="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=710" srcset="https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=425&amp;quality=75 425w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=850&amp;quality=75 850w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=585&amp;quality=75 585w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=1170&amp;quality=75 1170w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=710&amp;quality=75 710w, https://lede-admin.defector.com/wp-content/uploads/sites/28/2025/06/IMG_2107.jpg?w=1420&amp;quality=75 1420w" sizes="(max-width: 30rem) 425px, (min-width: 30rem) and (max-width: 40rem) 585px, (min-width: 40rem) 710px" loading="lazy"><figcaption><span>Jen Kinney</span></figcaption></figure><span><figure></figure></span></figure><p>According to Ahmed, the cost of pallets is going up. On a recent load of electronics, he paid $12 per item. For the time being, Amazing Binz is just breaking even, but it’s not sustainable. Ahmed says he may have to raise prices, and that he’ll even have to reevaluate the whole business in a few months. “This type of business, it’s not for this neighborhood,” he says.</p><p>Omran, ever the salesman, is darkly optimistic that they can keep prices low. "Anything to save the community some money,” he says.  "We're not looking to get rich. Being rich is pointless nowadays. It's about, how can you survive? How can I feed my kids? How can we pay our bills?”</p><p>I’m reminded of a conversation I had with the truck driver, Nick, who dropped off the Target load. He used to drive a taxi, until Uber ruined that. He prefers truck driving, but it was better at the height of the pandemic, when everyone was shopping online.&nbsp;</p><p>Around that time, Nick’s wife was a third-party seller on Amazon. She stocked her inventory based on social media trends, a task complicated by the algorithm. The more she looked at videos of, say, people showing off their Stanley cup accessories, the more TikTok fed her those videos, until it was hard to know if people were actually buying them in the real world, or if she was just in an online feedback loop. Plus, Amazon charges high storage fees, so unsold product just sitting in their warehouse cost her money. She quit, and abandoned her merchandise. That’s where a lot of the stuff in the Amazing Binz came from: failing third-party vendors, looking to offload the detritus of brief and bygone trends.</p><p>For a second, an image of the 21st century economy seems to come into focus here at the bin store. Nick, his wife, Amazing Binz’s owners, and even me, a freelance journalist, we’re all supposedly free agents, “entrepreneurs,” but in reality, we’re limited to buying whatever gets kicked down the funnel, and flipping it for all it’s worth. It’s like I can see our labor getting discounted by the day. Already, I've asked my editor if I can resell a version of this story elsewhere, to recoup a little more of my Amazing Binz investment.</p><h2><strong>Wednesday: $1</strong></h2><p>At 6:45 p.m. on Wednesday, a little over an hour until closing time, the shop is packed. Everything is now one dollar.</p><p>An elderly woman finds a shoe she likes, a black slide with gold studs all over. But it's just one shoe. Ahmed is insisting she buy it, and says he’ll keep the other shoe for her when he finds it. “For a dollar you can’t beat it,” he says. He’s in a good mood, joking with this woman and her daughter. She’s apparently been looking for three days.&nbsp;</p><p>“Those shoes are really cute,” I tell her.&nbsp;</p><p>“Do you work here?” she asks. Maybe I’ve been here too long.</p><p>After days of finding empty boxes, I finally find a loose elbow gel sleeve. It is heavy, slinky, comforting. After this, I find many of them, hiding in the bins like eels. Everything swims around the piles, and I revisit them as old friends, including the unopened boxes, known only by their clipped epithets: “Kiss Me I’m Irish wooden sign with lights,” “Lazy Susan for Refrigerator New.” The Jujube Enucleator box is full one day, and empty the next, without me ever seeing what was inside. The Dick Pics in Nature calendar remains, week after week. Who knows how many times these items have been bought and sold before they arrived here, how many more times they might be bought and sold. I imagine it all ending up in the same places eventually: the landfill or the waterways, where it will exist forever, first as choking hazard, then as nurdle.&nbsp;</p><p>In the dwindling bins, I find another Trump flag, folded in plastic but unmistakable. Omran says I can have it. I unfurl it, stiff and plasticky, creased into a grid. I don’t want it, but what am I supposed to do? It will end up in the garbage one way or another. I stuff it in my bag.&nbsp;</p><p>I ask, “How long have you guys been open?” It’s April 16, and Omran says they opened the doors on March 21.</p><p>“I thought it was more than a month,” says Ahmed.&nbsp;</p><p>“It’s been a long month,” says Omran.</p><p>The Trump flag joins a small museum of items I have purchased since the bin store opened: a cat bed ($10), a felted flower garland ($6), four post-Valentine heart-shaped chocolate boxes ($5 for all), two small spring-form pans ($4 each), a large penis-shaped ice cube mold ($4), four bike inner tubes ($2 each), a box of ant traps that don’t work ($2).&nbsp;</p><p>Did I need any of these things? Maybe the ant traps, if they’d worked, maybe the inner tubes. Mostly, I bought them because they were there, the same way I watch an Instagram reel because it pops up next. The irony is that if I were in the market for new pans or bedding, Amazing Binz would at this point be my best local option. It’s also simultaneously the most fun and most unsettling store on Baltimore Avenue, a combination that I find addictive, as many bin enjoyers do.</p><p>I take one last lap, back into the farthest corner of the store, where the oldest stuff has accumulated in deep drifts, where the Wokaar lives, now separated from its box. Looking for what? I don’t know. It’s concentrated back here, layers of unwanted product, filtered all the way down the supply chain to this last bin, where it’s piling up like sand. As I stare at the piles, I almost expect them to start breaking down in front of me, each individual item decomposing into a pool of microplastics, what remains of its value finally depleted. I leave the store, and the bins where they are. They’ll be full again tomorrow.</p></div></div>]]></description>
        </item>
    </channel>
</rss>