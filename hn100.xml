<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 12 Oct 2023 19:00:08 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[YouTube TV, which costs $73 a month, agrees to end "$600 less than cable" ads (145 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2023/10/youtube-tv-which-costs-73-a-month-agrees-to-end-600-less-than-cable-ads/</link>
            <guid>37859659</guid>
            <pubDate>Thu, 12 Oct 2023 16:48:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2023/10/youtube-tv-which-costs-73-a-month-agrees-to-end-600-less-than-cable-ads/">https://arstechnica.com/tech-policy/2023/10/youtube-tv-which-costs-73-a-month-agrees-to-end-600-less-than-cable-ads/</a>, See on <a href="https://news.ycombinator.com/item?id=37859659">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      Truthiness in advertising    —
</h4>
            
            <h2 itemprop="description">Google to "modify or cease" ads after industry review board rejects appeal.</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/10/youtube-tv-600-800x546.jpg" alt="Screenshot of a YouTube TV ad that claims the service costs $600 less than cable.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/10/youtube-tv-600.jpg" data-height="1481" data-width="2170">Enlarge</a> <span>/</span> The disputed YouTube TV ad.</p></figcaption>  </figure>

  




<!-- cache hit 640:single/related:f8c230bd2b2efc88be15a1e3fff9428d --><!-- empty -->
<p>Google has agreed to stop advertising YouTube TV as "$600 less than cable" after losing an appeal of a previous ruling that went against the company. Google said it will "modify or cease the disputed advertising claim."</p>
<p>The case was handled in the advertising industry's self-regulatory system, not in a court of law. The National Advertising Review Board (NARB) <a href="https://bbbprograms.org/media-center/dd/narb-google-youtube-tv">announced today</a> that it rejected Google's appeal and recommended that the company discontinue the YouTube TV claim.</p>
<p>YouTube TV launched in 2017 for $35 a month, but the base package is $72.99 after the <a href="https://arstechnica.com/gadgets/2023/03/youtube-tv-jumps-in-price-again-its-now-72-99-per-month/">latest price hike</a> in March 2023. Google's "$600 less than cable" claim was challenged by Charter, which uses the brand name Spectrum and is the second-biggest cable company after Comcast. The National Advertising Division (NAD) previously ruled in Charter's favor but Google <a href="https://bbbprograms.org/media-center/dd/google-youtube-tv">appealed the decision</a> to the NARB in August.</p>
<p>"Charter contended the $600 figure was inaccurate, arguing that its Spectrum TV Select service in Los Angeles only cost around $219 a year more than Google's YouTube TV service," according to a <a href="https://www.mediapost.com/publications/article/388447/youtube-tv-told-to-stop-boasting-it-costs-600-les.html">MediaPost article</a> in August.</p>
<p>A <a href="https://www.youtube.com/watch?v=9g3WtdwtIok&amp;t">Google ad</a> claimed that YouTube TV provided $600 in "annual average savings" compared to cable as of January 2023. A disclosure on the ad said the price was for "new users only" and that the $600 annual savings was "based on a study by SmithGeiger of the published cost of comparable standalone cable in the top 50 Nielsen DMAs, including all fees, taxes, promotion pricing, DVR box rental and service fees, and a 2nd cable box."</p>                                            
                                                        
<h2>Disclosures “not clear and conspicuous”</h2>
<p>Agreeing with the NAD decision, an NARB panel found that the price comparison provided by Google did not justify the "$600 less" claim. The panel "determined that the commercial disclosures were not clear and conspicuous" and "concluded that at least one reasonable interpretation of the challenged claim is that YouTube TV is $600 less than any comparable service available from companies traditionally associated with cable services."</p>
<p>The NARB also said that Google's "comparison does not align with the challenged claim" for the following reasons:</p>
<blockquote>
<ul>
<li>Many households can subscribe to basic Spectrum service without renting cable boxes, therefore Google failed to justify the cost of two set-top boxes in its price comparison, and</li>
<li>In certain markets, cable providers offer regional sports networks (RSNs) but YouTube does not, therefore Google did not have a valid reason for adding the cost of Spectrum's Sports View option to the price comparison.</li>
</ul>
</blockquote>
<p>The NARB announcement quotes Google's response in which it agreed to stop making the advertising claim. Google said it "disagrees with NARB's determination that people watching the challenged commercials will somehow understand 'cable' to mean something other than traditional cable television," but "intends to modify or cease the disputed advertising claim." Google also said that at a later date, it "may reconsider the claim based on updated information."</p>
<p>When contacted by Ars today, Google said it had no further comment.</p>
<p><em>Disclosure: The Advance/Newhouse Partnership, which owns 12.4 percent of Charter, is part of Advance Publications, which also owns Ars Technica parent Condé Nast.</em></p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Midwit Smart Home (144 pts)]]></title>
            <link>https://dynomight.substack.com/p/midwit-home</link>
            <guid>37859437</guid>
            <pubDate>Thu, 12 Oct 2023 16:31:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dynomight.substack.com/p/midwit-home">https://dynomight.substack.com/p/midwit-home</a>, See on <a href="https://news.ycombinator.com/item?id=37859437">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Reading a book one night, you decide to turn on the lights. And suddenly it’s obvious. Hauling your body across the room just to flip a switch is absurd. So you decide to get smart lights. Two hours later, your world is pain:</p><ul><li><p>Z-Wave</p></li><li><p>Zigbee</p></li><li><p>MQTT</p></li><li><p>Matter</p></li><li><p>Thread</p></li><li><p>Echo</p></li><li><p>Google Home</p></li><li><p>Apple Home</p></li><li><p>HomeKit</p></li><li><p>Homebridge</p></li><li><p>Home Assistant</p></li><li><p>Hue</p></li><li><p>deCONZ</p></li><li><p>Sharp tools</p></li><li><p>IFTTT</p></li><li><p>Tuya</p></li><li><p>Nest</p></li><li><p>RBOY</p></li><li><p>TRÅDFRI</p></li><li><p>Hubitat</p></li><li><p>SmartThings</p></li><li><p>WiFi repeaters</p></li><li><p>Mesh networks</p></li><li><p>Powerline networks</p></li><li><p>WiFi AX</p></li><li><p>WiFi 2.4 GHz</p></li><li><p>WiFi 5 GHz</p></li><li><p>WiFi 6</p></li><li><p>high gain antennas</p></li><li><p>Integrations</p></li><li><p>MQTT</p></li><li><p>ESPHome</p></li><li><p>KNX</p></li></ul><p>The hell? But people seem to think that Home Assistant is good. (Something about subscription fees and invasive apps and forced obsolescence?) So you search for “how to get a Home Assistant”. This reveals a recursive landscape of terror:</p><ul><li><p>Home Assistant Operating System</p></li><li><p>Home Assistant Container</p></li><li><p>Home Assistant Supervised</p></li><li><p>Home Assistant Core</p></li><li><p>Home Assistant OS</p></li><li><p>Home Assistant Yellow</p></li><li><p>Home Assistant Green</p></li><li><p>curl</p></li><li><p>tarball</p></li><li><p>sudo</p></li><li><p>ssh</p></li><li><p>NUC</p></li><li><p>Rasberry Pi</p></li><li><p>Blueprints</p></li><li><p>NAS</p></li><li><p>VirtualBox</p></li><li><p>Docker</p></li><li><p>Ubuntu</p></li><li><p>Debian</p></li><li><p>Python</p></li><li><p>pip3</p></li><li><p>Virtual environments</p></li><li><p>Proxmox</p></li><li><p>Hypervisor</p></li><li><p>LXC</p></li><li><p>M.2 slots</p></li><li><p>YAML</p></li><li><p>SQLite</p></li><li><p>Wireguard</p></li><li><p>SkyConnect</p></li><li><p>Z-WaveJS2MQTT</p></li></ul><p>In desperation you go to a smart home forum and make a post titled “smart lights no agony please help dear god”. The first response is:</p><blockquote><p><span>Just get a Qetzl hub, a OTOROXv3.2 bridge, and any MongoChopper compatible bulbs. After matroid paring, you can connect </span><code>xmpf12</code><span> beacons and trigger them with plain-old SkyDust switches. Are you </span><em>trying</em><span> to make this complicated?</span></p></blockquote><p>Which sounds lovely. But then there’s many replies like this:</p><blockquote><p>MongoChopper only works in reticulated mode, which newer Qetzl hubs don’t support. Easier to just make a custom Hellfire demarkation loop.</p></blockquote><p>(Just. Always “just”.)</p><p>Eventually you admit you’ve already suffered more than a lifetime of flipping switches and go to bed, mournful of your four lost hours of reading. You dream of a giant planetary light switch, glowing in brutal simplicity.</p><p><span>So. Many people feel this way. Dumb homes are fine, but require flipping switches. Smart homes </span><em>should</em><span> be better, but in practice the cost of administering a new home IT nightmare outweighs the benefits.</span></p><p>But perhaps there’s a third way. What if you try to achieve the goals of a smart home, but you rule out anything that involves getting two computers to talk to each other? So apps, phones, and hubs are out. But doorbells are good. Garage door openers are good. Dishwashers and smoke alarms are great. What else is there like that?</p><p>Quite a lot, arguably.</p><p><span>I have neurotic “no club that would have me” relationship with product links: They’re convenient, but it’s </span><a href="https://dynomight.net/ikea-purifier/" rel="">hard to be sure</a><span> of the motivations of anyone who provides them. So no links here, affiliate or otherwise. They wouldn’t be very useful anyway since most of this stuff is sold by brands like ELFPUFF that disappear after two weeks.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg" width="242" height="238.77333333333334" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:740,&quot;width&quot;:750,&quot;resizeWidth&quot;:242,&quot;bytes&quot;:53687,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F132002da-bb06-42ee-a333-b1c5cdb183f8_750x740.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Remote-controlled outlets.</strong><span> These are probably the easiest win for most people. Plug stuff into the cubes, plug the cubes into the wall, and use the remote to control which cubes are active. The buttons work </span><em>instantly</em><span>, with a range of 30 m (100 ft) even through walls. There is no app or WiFi. One button can control multiple plugs and the same plug can be paired to multiple remotes. (Cost: Around $25 for five plugs and two remotes.)</span></p><p>The obvious use for these is lights. When your roommate complains about you strobing them (it’s irresistible) remind them that LED bulbs’ lifespans are not shortened by on/off cycles. They’re also great for fans, air purifiers, and humidifiers.</p><p>Or, you can use this as a remote for a dumb window-unit air conditioner. (Make sure to check the rated current.)</p><p><span>Or, you can use this as a </span><em>signal</em><span>—put a light in the basement for grandma to activate if she needs you. Or put lights in rooms around the house all paired to one button, and strobe them when you want everyone to gather for dinner.</span></p><p>Or, you can use this for tea. My usual process for making tea is to walk to the kitchen and start the kettle. Then, because it takes an eternity for water to boil, I go back to my desk to wait. Then I forget about the tea. With this remote you can leave the kettle in a “ready” state: filled with water, kettle on, outlet off. Activate the remote a few minutes ahead of time and make tea in a single trip with no waiting.</p><p>Or, my printer is cursed. After being on for a few days it refuses to print anything longer than two pages. I know this shouldn’t be possible, but I’ve exhausted every avenue for fixing it. I could get a new printer, but maybe it’s a network issue or something?</p><p>Restarting the printer fixes it. But the printer has one of those infuriating power buttons where you hold it for ten seconds and then it ignores you. So for a year, I’d walk to the room with the printer and reach deep inside the cabinet to physically unplug and re-plug it. Now I use one of these cubes and remotely power cycle it when necessary. Utterly graceless, but effective.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg" width="276" height="272.32" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:740,&quot;width&quot;:750,&quot;resizeWidth&quot;:276,&quot;bytes&quot;:53687,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce262d42-2d80-47ff-9a36-f54f21ce8983_750x740.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Remote-controlled light sockets</strong><span>. These are the same as the above, except they work on light sockets rather than outlets. Less versatile, but work with built-in light fixtures. ($30 for 5 sockets and 2 remotes)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg" width="214" height="203.859477124183" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:583,&quot;width&quot;:612,&quot;resizeWidth&quot;:214,&quot;bytes&quot;:45902,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F109fa4d7-b932-4184-913c-c0e7c9bfab76_612x583.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Remote-controlled light bulbs.</strong><span> Personally, I’d never buy these, because I’m fanatical about color quality. (It’s futile to start with low-quality photons and then try to arrange matter to make them look good.) And if you have multiple bulbs and one remote, what do you do when one bulb breaks?</span></p><p>But they do exist. The remote talks directly to the light bulbs with no WiFi. And if you want dimming or changing colors, this seems to be the only midwit option. ($20 for 4 bulbs and 2 remotes)</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg" width="348" height="178.04651162790697" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:660,&quot;width&quot;:1290,&quot;resizeWidth&quot;:348,&quot;bytes&quot;:57627,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8070f6f1-f059-48c2-9ab3-356504bfaa72_1290x660.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Motion sensing light bulbs</strong><span>. These have a small sensor at the tip. Put them into any light figure and then leave it on. After sensing motion, the bulb actives for a while. Great for dark stairs. ($5-10 per bulb)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg" width="382" height="174.80652173913043" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:421,&quot;width&quot;:920,&quot;resizeWidth&quot;:382,&quot;bytes&quot;:56663,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F12b15c0b-8691-4fa6-92f1-2015f966b20d_920x421.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Motion sensing bulb sockets</strong><span>. These turn any bulb into a motion-sensing bulb. In principle, it seems good to separate the motion sensing bits from the glowy bits. But in practice the front assembly blocks the sensor from seeing straight ahead, so it may not be the way to go. It also looks very, uhh, long. ($11)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg" width="194" height="265.0273224043716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1000,&quot;width&quot;:732,&quot;resizeWidth&quot;:194,&quot;bytes&quot;:26791,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b4b533e-050a-4d24-9812-f3ea2176c5ac_732x1000.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Motion sensing outlets</strong><span>. When these sense motion, they activate the plug for a configurable amount of time. ($20 per plug)</span></p><p><span>One application is to </span><em>sense</em><span> motion in a different place from where you want light. But there are many other uses. Some people with kids put these on window unit air conditioners in rarely used rooms, so they aren’t constantly nagging them to turn things off.</span></p><p>Or, despite cats’ contempt for human life, many people keep them as pets. When cats use litter boxes, they produce undesirable smells. You can connect an air purifier to one of these to automatically turn on and remove those smells.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg" width="264" height="155.232" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:294,&quot;width&quot;:500,&quot;resizeWidth&quot;:264,&quot;bytes&quot;:11303,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F277d507a-8dd3-4374-8331-19abc06af546_500x294.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Remote motion sensing outlets</strong><span>. For even more flexibility, these use battery-operated motion sensors, which can be far away from the outlet. ($26)</span></p><p>Not totally sure where these would be useful. Maybe if you have a bunch of roommates and one bathroom, you could put the sensor in the bathroom and a light in a public room, so people would know when the bathroom is free? Seems weird.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg" width="248" height="239.568" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:500,&quot;resizeWidth&quot;:248,&quot;bytes&quot;:31041,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdcc83558-405d-49f4-9c34-187d7bbe2ee2_500x483.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Battery-powered motion sensing lights.</strong><span> These are small battery-powered lights that activate when they sense motion. Every time you open your closet and the light goes on, it feels like a little message from the universe that you matter. ($3-10 each.)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg" width="262" height="337.98" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:645,&quot;width&quot;:500,&quot;resizeWidth&quot;:262,&quot;bytes&quot;:123701,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe97a15a4-3a16-4937-800b-8a5f82cc7b63_500x645.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>The Clapper</strong><span>. When this outlet hears sounds at the right frequency separated by the right interval, it switches on or off. It was first sold in 1984 and as far as I can tell hasn’t been significantly updated since except to add a Bob Ross module. (Yes, this really exists. It gives quotes when you clap three times.) ($20)</span></p><p><span>I’m confused. The Clapper seems to work fine. Why isn’t it more popular? I </span><em>think</em><span> it’s partly that it was originally marketed as an aid to the infirm and elderly rather than Retro Home Automation for Your Active Lifestyle. But why aren’t there new variants? The </span><a href="https://patents.google.com/patent/US5493618" rel="">patent</a><span> is long expired, but there seem to be few (and bad) competitors. There are even rumors that quality has declined for brand-name Clappers.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg" width="278" height="200.716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:361,&quot;width&quot;:500,&quot;resizeWidth&quot;:278,&quot;bytes&quot;:30030,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3b44ffea-3fc5-463e-9b9a-b3e9ab48f70d_500x361.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Motorized shades/blinds with physical remotes</strong><span>. If you look at reviews for IKEA’s shades or blinds, there are many complaints about trying to pair them with a hub and phone. But you don’t need a hub! They come with a physical remote and there’s physical process to control how far they drop. Though sadly you lose the ability to set daily schedule or program them to move randomly and scare your dog. ($130-infinity)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg" width="238" height="259.42" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:545,&quot;width&quot;:500,&quot;resizeWidth&quot;:238,&quot;bytes&quot;:64448,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F95bf39d4-fb0b-4011-a903-1769f1812af9_500x545.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Mechanical outlet timers</strong><span>: You plug this into an outlet, it slowly spins once per day, and the pins determine is the outlet is active for each 30 minute period. ($5-10)</span></p><p><span>This! This is the kind of thing I need more of in my life! I </span><em>really</em><span> think it deserves to be more celebrated as a triumph of design. But it’s existed for decades and I can’t find the original inventor.</span></p><p><span>Many use these with light for plants. Or, it can be nice to wake up with light rather than sound. Instead of buying a light alarm clock, you can just plug a lamp into one of these. Or, if you buy into the rationalist trend of </span><em>very bright</em><span> lights, then you can rig up one of these up and have each day greet you with 100k lumens gently screaming into your face.</span></p><p><span>Or, in winter, I use a (</span><a href="https://dynomight.nethumidifiers/" rel="">non-ultrasonic</a><span>) humidifier. But it’s only really useful for a couple of hours before going to bed, after which humans provide plenty of humidity. Using a timer automates things, saves energy, and avoids humidity getting out of control in the middle of the night. Even better, it reduces how often you need to refill the tank.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg" width="172" height="293.088" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:852,&quot;width&quot;:500,&quot;resizeWidth&quot;:172,&quot;bytes&quot;:89725,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff95e08c2-b5f9-4858-a924-624b0e3b6d75_500x852.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Digital outlet timers</strong><span>: A good alternative if you need more precise timing or find mechanical timers too beautiful and intuitive and bulletproof. ($15)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg" width="274" height="150.7" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:275,&quot;width&quot;:500,&quot;resizeWidth&quot;:274,&quot;bytes&quot;:29733,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff848a23c-b57c-44bd-92be-cc66082c7f9c_500x275.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Countdown outlet timer</strong><span>. Press a button and activate an outlet for a given period of time. Good for lights, air purifiers, heaters, and the paranoid. ($10-20)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg" width="206" height="215.064" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:522,&quot;width&quot;:500,&quot;resizeWidth&quot;:206,&quot;bytes&quot;:37264,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4cb3f-bbed-49e5-90f6-075ff7ae32c8_500x522.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Door alarm / reminder.</strong><span> You mount a magnet on one side of a door and the detector on the other. You can configure it to either make noise immediately when opened or after a certain amount of time. Useful for doors or windows or refrigerator doors that are sometimes left open. ($4-20 per alarm, depending on how many you buy.)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg" width="252" height="203.112" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:403,&quot;width&quot;:500,&quot;resizeWidth&quot;:252,&quot;bytes&quot;:35452,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1f8d40c-0ffd-497e-98f4-088337b5de3e_500x403.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Key fobs for home doors</strong><span>. Unlock your home like you unlock a car. There’s even a battery-powered version that sticks on top of the inside lock and opens the door by physically turning the existing latch. And apparently this sort of works? This makes me nervous, though I know physical locks aren’t very secure anyway. ($50+)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg" width="300" height="170.4878048780488" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:233,&quot;width&quot;:410,&quot;resizeWidth&quot;:300,&quot;bytes&quot;:17687,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c4b924f-9945-4bb9-896f-3fb9c2aceea9_410x233.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Watering stakes</strong><span>. The one true home decoration strategy is: plants. If you don’t have any plants, forget home intelligence levels and get some plants. Now, to keep them alive, you put one of these stakes in the soil and the end of the cable in a container of water. The soil then stays damp through voodoo magic. (It’s not a siphon because the water can be below the plant.) Purely physical, no electronics. In principle, this can work for a </span><em>very</em><span> long time if you have a big enough container of water. You can also make a crappy DIY version by drilling a tiny hole in the lid of a plastic bottle. ($5 / stake.)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg" width="294" height="199.92" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:340,&quot;width&quot;:500,&quot;resizeWidth&quot;:294,&quot;bytes&quot;:17370,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3dee161f-3019-4b2c-84a1-fa734d163030_500x340.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Water detector</strong><span>. This flashes and screams if water contacts the base. Batteries last for years. ($7-10 each)</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg" width="236" height="225.144" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:500,&quot;resizeWidth&quot;:236,&quot;bytes&quot;:32937,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffec4b6b9-f0bc-4ef6-bd4d-7873102c0a4a_500x477.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Robot vacuums with a physical remote</strong><span>. These still exist, for now! You can set a daily schedule without giving </span><s>iRobot</s><span> Amazon a map of your home.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg" width="178" height="264.508" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:743,&quot;width&quot;:500,&quot;resizeWidth&quot;:178,&quot;bytes&quot;:71376,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd353d472-6056-4d10-aad5-81fd142d9869_500x743.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Temperature controlled outlets</strong><span>. Plug a device into it and plug it into the wall. The plug turns on and off when programmable temperatures are reached. ($20-30)</span></p><p>Did the temperature sensor in your fridge break, meaning it stays on all the time? Does the manufacturer demand your first child for a new sensor? Plug the fridge into one of these.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg" width="311" height="179.93245227606462" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:394,&quot;width&quot;:681,&quot;resizeWidth&quot;:311,&quot;bytes&quot;:47271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd29fc74c-4382-4d4f-a6ec-4e7c494ab440_681x394.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Rechargeable batteries.</strong><span> You already knew these existed. But did you know how much better they’ve gotten in recent years? I bring this up because where possible I advise avoiding things with built-in batteries. Partly that’s so the lifespan isn’t limited by the battery. But it’s also more convenient: To recharge, you swap the batteries with the ones on the charger and you’re done. This is much easier than going through a remove / find charger / charge / wait / replace cycle.</span></p><p><span>One lesson is that </span><strong>lights do well</strong><span>. Half of smart homes seem to be about lights. By combining remotes, motion sensors, timers, and maybe Clappers, you can go quite far with lights just using midwit solutions.</span></p><p><span>A second is that </span><strong>power is the interface</strong><span>. All of the above items are either single devices, or devices that “communicate” with each other by the brute force method of turning power on or off for each other. It’s interesting how far that takes you.</span></p><p><span>Finally, </span><strong>dumber devices are often more interoperable</strong><span>. Physical on/off switches are good. If a device has one, then you can set it to “on” and then control it through an external remote or whatever. But increasingly many things have clever touch sensor power buttons. These can’t be activated without pressing the button, so none of this works.</span></p><p><span>(BTW, manufacturers, if you want to use touch sensors and you don’t want to lose the </span><em>massive</em><span> midwit market, make things that automatically turn on when first connected to power, without waiting for a touch.)</span></p><p><span>There are many things that would make midwit life better, and it seems like </span><em>could</em><span> be made, but aren’t.</span></p><ol><li><p><strong>Remote controlled dimmers.</strong><span> Modern dimmers work by truncating the lower-amplitude parts of the alternating current—basically switching the lights on and off very quickly. There should be light-bulb sockets with a remote to control the dimming level.</span></p></li><li><p><strong>Whistle controlled power switches.</strong><span> Clapping is (mildly) annoying. Couldn’t we use other sounds?</span></p></li><li><p><strong><span>PM</span><sub>2.5</sub><span> controlled outlets.</span></strong><span> The biggest advantage of commercial air purifiers over building DIY versions is that some integrate air quality sensors and turn on automatically when needed. I want a device that samples the air and activate and outlet if PM</span><sub>2.5</sub><span> is above a threshold.</span></p></li><li><p><strong>Remote-controlled wall-switch pressers.</strong><span> This is rather cursed, but I’d like a device to physically flip my existing wall switches when activated by a remote, so my lights can be be dumb </span><em>and</em><span> midwit. (There are “smart” variants now, but seemingly no midwit versions.)</span></p></li><li><p><strong>Power-controlled remote pressers.</strong><span> This is </span><em>utterly</em><span> cursed, but hear me out: I want a gadget that I physically attach to a remote. When the gadget gets power, it presses the “on” button on the remote. When it loses power, it presses the “off” button.</span><br></p><p><span>Why? Because then you could automate anything with a remote! If I have remote-controlled motorized blinds, I’d like to put the remote into this gadget, and then plug the gadget into an an outlet timer for a daily schedule. Or use a Clapper or motion sensor or whatever.</span><br></p><p>Like I said, cursed. Maybe it could just memorize and repeat the signal from remote instead of physically pressing buttons?</p></li><li><p><strong>Remote-controlled light sockets that respond to power cycles.</strong><span> The annoying thing about remote-controlled light sockets is they render your existing switches useless. There is no need! I want a variant that works like this: If the power is quickly turned off and on again, the outlet switches from powered to unpowered (or vice-versa). Then you could use remotes </span><em>and</em><span> existing switches, with the only downside that you need to remember to “double flip” all your lights.</span></p></li><li><p><strong>Gates.</strong><span> Say you have Clapper-activated lights in your bedroom, but you do a lot of sleep-clapping. Well, you can plug the lights into a Clapper, and the Clapper into an outlet timer. That’s basically an AND gate.</span><br></p><p><span>But what about OR gates? What if you want to turn on an air conditioner when there is motion </span><em>or</em><span> if the temperature goes above X degrees? I think an OR gate would just amount to a “splitter”, except with multiple inputs and one output. But this doesn’t seem to be something you can buy, perhaps because if the inputs came from different circuits that would cause explosions?</span><br></p><p><span>Or how about NOT gates? Maybe you have an air purifier that’s really loud, so you only want it to run when you’re not in the room. There should be a gadget that reverses the power signal provided by a motion sensor.</span><br></p><p><span>But then, how would a NOT gate work? The gadget would need to plug into the wall to provide power when the signal is off. Which means that </span><em>really</em><span> it would be a (NOT SIGNAL) AND (MAINS) gate. Or maybe could be an XOR gate instead?</span><br></p><p>Or can we hand NAND gates? Or NOR gates? There’s no obvious need, true. But I’d rest easier knowing my janky kettle control system was built out of functionally complete base units.</p></li></ol></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU "Chat Control" and Mandatory Client Side Scanning (147 pts)]]></title>
            <link>https://berthub.eu/articles/posts/client-side-scanning-dutch-parliament/</link>
            <guid>37859402</guid>
            <pubDate>Thu, 12 Oct 2023 16:28:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berthub.eu/articles/posts/client-side-scanning-dutch-parliament/">https://berthub.eu/articles/posts/client-side-scanning-dutch-parliament/</a>, See on <a href="https://news.ycombinator.com/item?id=37859402">Hacker News</a></p>
<div id="readability-page-1" class="page"><article lang="en">
  

  
  

  <div>
  <meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@bert_hubert">
<meta name="twitter:creator" content="@bert_hubert">
<meta name="twitter:title" content="Transcript Dutch partliamentary hearing on EU Chat Control and Client Side Scanning">
<meta name="twitter:description" content="Transcript Dutch partliamentary hearing on EU Chat Control and Client Side Scanning">
<meta name="twitter:image" content="https://berthub.eu/articles/bert-csc.jpeg">
<p>On the 11th of October, Dutch parliament organized a hearing on the <a href="https://www.patrick-breyer.de/en/posts/chat-control/">EU “Chatcontrol” proposal</a>, with a focus on client side scanning. Dutch parliament had earlier passed two motions calling on the Dutch government not to support this proposal, but our government has declared it will ignore those motions. This hearing was timely because next week, or even this Friday, EU member states vote on how to proceed with this terrible proposal.</p>
<center>
<p><img src="https://berthub.eu/articles/bert-csc.jpeg" alt=""></p>

</center>
<p>I’ve translated my introduction in the Dutch parliamentary hearing because I think it might add a little bit to the debate, and also because it sheds a light on the poor state of Dutch democracy, and some sunlight might help.</p>
<p>More context on this proposed EU law can be found, in Dutch, <a href="https://berthub.eu/articles/posts/client-side-scanning/">in a post I wrote earlier</a>. Automated translation will likely do a decent job on this. Or follow Patrick Breyer’s explanation <a href="https://www.patrick-breyer.de/en/posts/chat-control/">here</a>. He’s been on the case for years now.</p>
<p>Transcript (machine transcribed, machine translated, lightly edited, a few sentences added for international context):</p>
<p>I have been involved in this subject for about fifteen years now. I once supplied software to the Dutch police for investigations into CSAM. I was also a regulator of the Dutch intelligence and security agencies, so I know a bit about proportionality and subsidiarity and law.</p>
<p>For a long time I’ve also been on occasional talks between industry and government about how child pornography can be combated or made more difficult. So the subject is very familiar to me. It’s also about AI, because the AI will have to scan our communications. I have also written a small book about AI and a training course. So I hope I know a little bit about it.</p>
<p>And in contrast to my esteemed, highly educated predecessors, I will perhaps talk about the subject in very plain language.</p>
<p>[Holds up QR code] I want to show this to the viewers at home for a moment. You can find my position paper here. This worked very well last time I presented here.</p>
<p>And the QR code is perhaps good to keep in mind how many feelings this aroused among people around the corona crisis. What we are talking about now is that in all our WhatsApp groups, our Signal groups, our Telegram groups, a new participant will join the conversation, namely an EU logo.</p>
<p>And all the photos we put there, all the videos we put there, are scanned by a computer. And that computer first has a list of known bad material. But the EU proposal also says that we should also have AI and it should say, “Oops, that child photo is the wrong kind of child photo.”</p>
<p>In addition, the law is quite general about it, says the AI should detect the “approaching of children”. Well, that’s a very broad term, but they mean grooming. And if the system sees one of those things, then there will be a report to an EU official.</p>
<p>And this person gets reports all day long, “Is this CSAM or not?”. And only if the material is evidently incorrect, no action will be taken. So if the photo is of a toaster, and not a child, that is the end of it. But in almost all other cases, the process continues. Then the material goes to two places. It goes to a local authority that will do research there. And the law actually doesn’t say much about what happens then. And that is crucial. Because we often say “Think about the children.” Let’s do that too.</p>
<p>But there is a fantastic stream of reports from this system. Because it is still very difficult to determine the nature of a photo. For example, the Dutch Forensic Institute has said, “We are not going to look at a photo and say whether this is a minor or not.” The NFI has said, “We can’t do that. It doesn’t work. We need X-rays. We just can’t.”</p>
<p>But the European Union has said, “Our computer can do that.” This leads to a a fantastic stream of “Maybe it’s wrong, maybe it’s not wrong” material. And it is then passed on to a local police station or an authority that will look at it. They might start an investigation.</p>
<p>And the first thing a citizen notices is that they receive a letter from the police. At such and such time, the EU has found that you have shared a photo that we unfortunately cannot include here. Because it is possibly CSAM,  so we cannot show you. But at 5.34 p.m. we have found a photo that was not good and we started an investigation.</p>
<p>I just said, “Think of the children.” And we really have to do that. And we want to make their lives better. But their lives will not get any better if this system starts a huge stream of unjust investigations.</p>
<p>“When we found five justifiable investigations, how many unjust investigations have we launched. Did we make it worse?” Because an unjust investigation into someone can end up terribly bad. Especially if the person under investigation is not perfect.</p>
<p>And many people think, “It’s not that bad to be investigated, because then I go to the police station and I just explain it, I explain that my own child was in the bath and so on.” You can really forget that illusion.</p>
<p>At the moment the investigation starts, you have nothing more you can “just explain”. But you are in all kinds of databases. And I want to come back to that. The people at Europol have already said, “We are going to keep that database.” Including all the things that turned out to be unjust.</p>
<p>Because they said, “Who knows? Maybe we can do something with it.” And that brings me to my second topic here. This is not a law that needs to be tuned a little bit and then it is good. This has never happened before. In my old work, when I had to regulate the AIVD and the MIVD [Dutch Intelligence, Security and SIGINT agencies], we were asked individually whether we could listen to these ten people or not.</p>
<p>And then we held a meeting about it with two wise people and me. And now we are talking about 500 million Europeans, and saying, “Let’s just apply those scanners!” That is incredible. This is not something… If we approve this as a country, if we as the Netherlands vote in favour of this in Europe and say, “Do it,” we will cross a threshold that we have never crossed before.</p>
<p>Namely, every European must be monitored with a computer program, with a technology…</p>
<p>With a technology of which the vast, overwhelming majority of scientists have said, “It is not finished.” I mentioned earlier the example that the Dutch National Forensic Institute says, “We cannot do this by hand.” The EU has now said, “Our computer can do that.”</p>
<p>420 scientists have signed a petition saying, “We know this technology, some of us invented it, we just can’t do it.” We can’t even make a reliable spam filter. Making a spam filter is exactly the same technology, by the way, but then much easier. It just doesn’t work that well, but the consequences aren’t that scary for a spam filter.</p>
<p>Nevertheless, there are now MPs who say, “Well, I feel this is going to work. I have confidence in this.” While the scientists, including the real scientists who came here tonight, say, “Well, we don’t see how this could work well enough”.</p>
<p>And then government then says, “Let’s start this experiment with those 500 million Europeans.”</p>
<p>WhatsApp and Telegram - well, Telegram is not going to implement this law, but WhatsApp will. They will not hesitate to announce this to all their users with a large EU logo. “Pay attention, the EU is watching you. We have installed software on your phone, on behalf of the government that will keep an eye on you.”</p>
<p>And I don’t think that is going to go down very well.</p>
<p>Also, if we want to learn how to implement such scanning technology, if we want to learn internationally how to do this, there is only one country in the world that can help us.</p>
<p>And that is China. And I don’t know if that’s such a good prospect.</p>
<p>Thank you.</p>
<p>[“Chairperson: that certainly called a spade a spade”]</p>

</div>

  



</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Can't Be Fucked: Underrated Cause of Tech Debt (168 pts)]]></title>
            <link>https://jesseduffield.com/Can%27t-Be-Fcked/</link>
            <guid>37859311</guid>
            <pubDate>Thu, 12 Oct 2023 16:21:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jesseduffield.com/Can%27t-Be-Fcked/">https://jesseduffield.com/Can%27t-Be-Fcked/</a>, See on <a href="https://news.ycombinator.com/item?id=37859311">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <blockquote>
  <p><em>Can’t Be Fucked</em></p>

  <p>Aussie slang for not wanting to, or not having the energy and motivation to do something.</p>

  <p>“Man, i really can’t be fucked changing the channel, let’s just watch Springer.”</p>

  <p><em>- Urban Dictionary</em></p>
</blockquote>

<p>I used to think that if I could just learn a bunch of things about programming, that would make me a better programmer. Sure, the learning I’ve done so far <em>has</em> made me a better programmer, but the person I was striving to become was a little more… admirable.</p>

<p>Whether it’s at work or in my open source travels, I routinely come across developers who are the real deal: they’re conscientious and judicious in an unwavering way. They set a standard for themselves and do not compromise on it. Whether that’s a deliberate thing or whether they’re just built that way, it’s humbling to witness. If there’s a flaky test, they investigate it and fix it. If there’s a bug they spot in the wild, they make a ticket, and maybe even fix it then-and-there. If a new feature doesn’t gel well with the existing code, they refactor the code first rather than hacking the feature in. They’ll dive as far down the stack as necessary to get to the bottom of something. None of these things are necessary but great developers know that if they don’t address a problem early, and properly, it will only cost them more time in the long run.</p>

<p>‘But,’, you say, ‘premature optimisation is the root of all evil! Duplication is better than the wrong abstraction! Don’t be an architecture astronaut!’</p>

<p>The developers I’m thinking about already know of all those takes and have internalised them long ago. They know that sometimes ‘good enough’ <em>is</em> the right choice given the constraints of a project. They know that sometimes you need to cut scope to stay on-track. They know that sometimes it’s better to wait to learn more about a domain before rearchitecting a system. And yet in spite of those constraints their output remains golden. These are hard working motherf*ckers whose diligence and perseverance put other devs to shame.</p>

<p>Other devs… like me.</p>

<p>Sometimes, I just CBF. I spent months (part time, of course) building out an end-to-end test system for my open source project Lazygit, and every day I think of all the regressions that system has prevented and how much harder it would be to add that system now given how many tests have been written since. I know for a fact it was worth the effort. So why haven’t I added end-to-end tests to my other project, Lazydocker? Because I CBF.</p>

<p>I raised an issue in somebody else’s open source repo and they asked me to present them with a minimal repro git repo. Why haven’t I done that yet? Because I CBF.</p>

<p>Why do I still have so much of my code living in a God Struct? If I could just finish that big refactor I started over a year ago, my codebase would be far easier to navigate and contribute to. The answer, dear reader, is that I CBF.</p>

<p>Is it burnout? Maybe. Is it a lack of Growth Mindset? Hard to say. Is it just the reality of my personality? Who knows.</p>

<p>As I continue my journey as a developer I learn more about myself, and one of the things I’ve learnt is that my motivation ebbs and flows. I’ll have a good run, inhabiting the persona of the developers I look up to, however imperfectly, until I once again find myself with the constraint that trumps all the extrinsic constraints of the project at hand: a deficiency of motivation.</p>

<p>I’ve also learnt that knowledge only gets you so far. I’m still just scratching the surface of all the developer-y things there are to be learnt, but even for the topics I consider myself knowledgeable, knowing what’s right and doing what’s right are two very different things. Knowing the long-term pain of tech debt is certainly a motivator for avoiding it, but it’s no guarantee.</p>

<p>And sometimes knowledge can be wielded impiously: ‘I’d add more tests but too many tests creates a maintenance burden’. ‘I <em>could</em> refactor this but I want to wait a little to see how other features affect things’. ‘Keep It Simple, Stupid’. ‘Premature optimisation’, ‘Cut scope aggressively’, etc. These maxims can be deployed for evil just as well as good. Have you ever hid behind one of these lines to hide the fact that in reality you just CBF?</p>

<p>Maybe you don’t have the energy to write perfect code, but honesty requires less creativity than lying. It’s a breath of fresh air when a contributor admits some element of their pull request is lacking because they’re lazy. At least then you have the opportunity to judge for yourself whether their laziness exceeds your own standards or whether their time is indeed better spent working on the next thing. That’s a much easier game to play than bandying software engineering maxims.</p>

<p>So, if you find that you CBF, don’t be dismayed; it happens to everyone (except those very special people we all aspire to). If in the moment you don’t have the strength to overcome it, at least be honest. And if you’ve been going 100% for too long, maybe it’s time to take that holiday.</p>

<p>I’d like to end this post with an exploration about how laziness evolved for a reason and that despite us all wishing we had more motivation, there are benefits to being selective with our energy, but… I CBF ;)</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Desmos 3D graphing calculator (beta) (101 pts)]]></title>
            <link>https://www.desmos.com/3d</link>
            <guid>37859085</guid>
            <pubDate>Thu, 12 Oct 2023 16:05:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.desmos.com/3d">https://www.desmos.com/3d</a>, See on <a href="https://news.ycombinator.com/item?id=37859085">Hacker News</a></p>
Couldn't get https://www.desmos.com/3d: Error: Request failed with status code 404]]></description>
        </item>
        <item>
            <title><![CDATA[Want to get started with LLMs? Here's what you need to know (114 pts)]]></title>
            <link>https://flyte.org/blog/getting-started-with-large-language-models-key-things-to-know#what-are-llms</link>
            <guid>37858369</guid>
            <pubDate>Thu, 12 Oct 2023 15:15:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flyte.org/blog/getting-started-with-large-language-models-key-things-to-know#what-are-llms">https://flyte.org/blog/getting-started-with-large-language-models-key-things-to-know#what-are-llms</a>, See on <a href="https://news.ycombinator.com/item?id=37858369">Hacker News</a></p>
<div id="readability-page-1" class="page"><div fs-readtime-element="contents" fs-codehighlight-element="code" fs-codehighlight-theme="a11y-light" fs-richtext-element="rich-text" fs-toc-element="contents" fs-toc-offsettop="2rem" fs-toc-hideurlhash="true"><p><em>An introductory guide to LLMs</em></p><p>As a machine learning engineer who has witnessed the rise of Large Language Models (LLMs), I find it daunting to comprehend how the ecosystem surrounding LLMs is developing. Every week, I come across new tools and techniques related to LLMs on my Twitter feed. It can be difficult to keep up with ways in which the LLM ecosystem is evolving. And if you’re just starting to use LLMs, the stream may seem to be moving too quickly to jump in!&nbsp;&nbsp;</p><p>The good news is that we've now reached a point where there are reliable and easy-to-use tools to work with LLMs. While more advanced tools will likely appear in the future, the current choices for fine-tuning models or making predictions are pretty impressive. They empower you to create some truly powerful applications.</p><p>In this post, I dive into the core principles of LLMs and the tools and techniques you’ll need to get started with LLMs.</p><h2><strong>What are LLMs?</strong></h2><p>A Large Language Model, as the name implies, refers to a model trained on large datasets to comprehend and generate content. Essentially, it's a transformer model on a large scale. The transformer model itself is a neural network designed to grasp context and meaning through the analysis of relationships within sequential data.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7acad02c40d777c57e_vhiySUD-e-bRSZISk2VyN92nxWrMQGP9AqqiDPVsTm2_sWaccjGWkxihFg4DKiL2lfhbg3xonfvMRpzDnvwuvIuiJhto8cPiltZgjhY4Y6vL6-yzH-f4-eqNApwrO8z6lqmy9iRyzNMuqHq6gkAeYX4.png" alt=""></p><figcaption>The architecture of transformer model (<a href="https://arxiv.org/pdf/1706.03762.pdf">Source</a>)</figcaption></figure><p>Transformers are great for LLMs because they have two important features: positional encodings and self-attention.</p><p>Positional encodings help the model understand the order of words in a sequence and include this information in the word <a href="https://vickiboykis.com/what_are_embeddings/">embeddings</a>. Here's a bit more about it from Brandon Rohrer’s article "<a href="https://e2eml.school/transformers.html">Transformers from Scratch</a>":</p><p>“There are several ways that position information could be introduced into our embedded representation of words, but the way it was done in the original transformer was to add a circular wiggle.”</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526df6b5c3a19649d0943e5_circular-wiggle.jpg" loading="lazy" alt=""></p></figure><p>“The position of the word in the embedding space acts as the center of a circle. A perturbation is added to it, depending on where it falls in the order of the sequence of words. For each position, the word is moved the same distance but at a different angle, resulting in a circular pattern as you move through the sequence. Words that are close to each other in the sequence have similar perturbations, but words that are far apart are perturbed in different directions.”</p><p>Self-attention allows the words in a sequence to interact with each other and find out who they should pay more attention to. To help you understand this concept better, I’ve borrowed an example from Jay Alammar’s “<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>” article:</p><p>“Say the following sentence is an input sentence we want to translate:”</p><p>‘The animal didn’t cross the street because it was too tired’</p><p>“What does ‘it’ in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.”</p><p>“When the model is processing the word ‘it,’ self-attention allows it to associate ‘it’ with ‘animal.’”</p><p>Transformers utilize a powerful attention mechanism known as multi-head attention. Think of it as combining several self-attentions. This way, we can capture various aspects of language and better understand how different entities relate to each other in a sequence.</p><p>If you're interested in a visual guide to understand LLMs and transformers, <a href="https://ig.ft.com/generative-ai/">this resource</a> is worth exploring.</p><h2><strong>What can LLMs do?</strong></h2><p>With LLMs, the possibilities are vast — from creating AI assistants and chatbots to engaging in conversations with your data and enhancing search capabilities.&nbsp;</p><h3><strong>AI assistants</strong></h3><p>The growth of LLMs has led to the development of many AI assistants, each designed for specific tasks like pair programming, scheduling and making reservations. Ongoing research aims to create a universal assistant capable of assisting with a wide range of tasks. Examples of such assistants include <a href="https://pi.ai/talk">Pi</a> and ChatGPT.</p><h3><strong>Chatbots</strong></h3><p>You can create chatbots tailored to specific tasks, like answering user or customer questions by fine-tuning them on custom data. For example, one of our community members has built a <a href="https://flyte.org/blog/building-flytegpt-on-flyte-with-langchain">chatbot to address Flyte-related inquiries</a>.</p><p>What’s more, you have the flexibility to develop chatbots that mimic the speech patterns of various personas, such as game characters or celebrities. <a href="https://beta.character.ai/">Character.ai</a> is an intriguing example.</p><h3><strong>Generation</strong></h3><p>LLMs are trained to generate the next piece of text given some input text. You can use them to write stories, create marketing content or even generate code. They're great at understanding what comes next in a text, making them handy for all sorts of writing tasks.</p><h3><strong>Translation</strong></h3><p>An LLM excels at translation tasks, which are considered relatively straightforward for them. They can also handle more complex tasks like translating text into code. If you provide a user instruction to generate a code snippet in any programming language, an LLM should be able to generate the code for you. A popular example of such a model is <a href="https://about.fb.com/news/2023/08/code-llama-ai-for-coding/">Code Llama</a>.</p><h3><strong>Summarization</strong></h3><p>With LLMs, you can create applications that automatically summarize long documents, research papers, and meeting notes and articles, making information more accessible.&nbsp;</p><h3><strong>Search</strong></h3><p>LLMs are great at grasping natural language queries. Unlike simple keyword-based searches, LLM-powered search engines like <a href="https://blog.google/products/search/google-search-generative-ai-learning-features/">Google's generative AI search</a> and <a href="https://www.perplexity.ai/">Perplexity AI</a> offer more relevant and context-aware results. These advancements have revolutionized real-time search capabilities.</p><p>These use cases represent just the tip of the iceberg; LLMs offer many more possibilities, including personalization, recommendation systems, interactive fiction, gaming and much more.</p><h2><strong>Putting LLMs to the test</strong></h2><p><a href="https://chat.openai.com/">ChatGPT</a> is the go-to choice for testing LLMs. However, if you're a fan of open-source solutions like we are, you might want to explore <a href="https://huggingface.co/chat/">HuggingChat</a>. Additionally, there's <a href="https://bard.google.com/">Google Bard</a> and other open-source LLMs available on Hugging Face spaces. These can be used as hosted applications or accessed programmatically through the API.</p><p>You can find a list of open-source LLMs on Hugging Face in the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Open LLM leaderboard</a>. These models are assessed using the <a href="https://github.com/EleutherAI/lm-evaluation-harness">Eleuther AI Language Model Evaluation Harness</a> framework, which evaluates generative language models across a wide range of tasks.</p><p>It's important to note that not all chatbots or applications use the same LLM. ChatGPT employs the GPT-series models, HuggingChat <em>currently</em> utilizes the <a href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor">OpenAssistant LLaMa 30B SFT 6</a> model, and Google Bard is <em>currently</em> powered by the <a href="https://ai.google/discover/palm2/">PaLM 2</a> model. It's worth noting that these models can change over time as newer, more efficient models are continually trained on larger datasets and improved model architectures.</p><h3><strong>How do LLMs differ?</strong></h3><p>An important question we need to address: What's the purpose of having multiple language models? It's important to note that LLMs are still an active area of research, and we haven't reached a point where we can rely on a single model for all tasks. Whether we will ever reach that stage remains uncertain. However, the community is actively working to discover the best models for various applications we intend to build upon and use.</p><p>The following are some key factors that contribute to the differences between LLMs:</p><h4><strong>Model architecture</strong></h4><p>LLMs can have different architectures depending on their objectives, computational resources and training tasks.</p><h4><strong>Data</strong></h4><p>The quality and amount of training data vary among models. PaLM 2, for instance, utilizes five times more data than its predecessor.</p><h4><strong>Parameter count</strong></h4><p>A higher parameter count indicates a more powerful model. For example, GPT-3 has 175 billion parameters. Meta's <a href="https://ai.meta.com/llama/">Llama 2</a> includes models ranging from 7 billion to 70 billion parameters.</p><h4><strong>Training objective</strong></h4><p>While some LLMs excel at text completion tasks, others are designed for specific use cases. For instance, MosaicML introduced the <a href="https://huggingface.co/mosaicml/mpt-7b-storywriter">StoryWriter</a> model, developed to read and write fictional stories with extended context lengths (65K tokens). At inference time, it can go beyond 65K tokens.&nbsp;</p><h4><strong>Computational resources</strong></h4><p>Some LLMs are designed to be resource-intensive for better performance, while others prioritize efficiency for faster inference or when resources are limited. For example, <a href="https://simonwillison.net/2023/Aug/1/llama-2-mac/">Llama 2 can run on your Mac</a>.</p><h2><strong>What are prompts?</strong></h2><p>A prompt is the input given by a user, and the model responds based on that input. A prompt can be a question, a command or any kind of input, depending on what's needed for a particular use case.&nbsp;</p><div>
  <p>Copied to clipboard!</p>
  <pre><code fs-copyclip-element="copy-this-1">Classify the text into positive, negative or neutral.

Text: The movie isn’t that great.
Sentiment:</code></pre>
</div><p>This is a prompt that can be provided to an LLM to guide it. An LLM typically responds with 'positive,' 'negative' or 'neutral'.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7abdca412177ebf5e0_MJ7kAA472mdMZpxlpZdCkp5SHefoHsDt2LGS6hRCHHeZ0Wp-_4R0iNMl08HfYWcbwpcEGZHhAhjdCKArL3wfEX_qydata-WmQt_fI-M9DeBTAiUkie5RFqwsqKhdqtcTt6YZ4n8YtYP4Khpm2urtgV8.png" alt=""></p><figcaption>Response generated by ChatGPT</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a973293482c53741a_QerwpukKzVys4dS78_GFbu9M6RzbRxsKpZN2LtqoJzfl2KWyTlnM3oLUcvW0YDepcKDXbNhcdKkcEXBaVRNzPm5dVfvJGOFC6rn1ueApLkW_ZZgcOtUFG2B7jECca-kbycfXBNMN6PmY9WQT9ZzD41E.png" alt=""></p><figcaption>Response generated by Llama 2 7B Chat model</figcaption></figure><p>From the examples above, you can see that different LLMs create different results. This happens because of how they were trained, similar to how each of us has different opinions 🧠 about the sentiment that exact piece of text conveys.</p><h3><strong>Prompt format</strong></h3><p>A straightforward way to interact with an LLM is by offering an incomplete sentence and allowing the model to complete it. This approach aligns with the nature of pre-trained LLMs, which excel at text completion.</p><div>
  <p>Copied to clipboard!</p>
  <pre><code fs-copyclip-element="copy-this-2">Flamingos are</code></pre>
</div><p>When presented with this prompt, an LLM provides information about what flamingos are.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7b523a12e5ee41a97e_t1qNIPgILazRwUbuIKISqiEZ1ePUqd_hEQan3gLP3WQhxSRuP1LMTu_MLzxjEgkdWVB119SPF_rlwicEHzIB8OLFANzaMVpGLVZD901dbzj9vGAvybZCOGlD_WBJmvR2BtQB8g8_ebZHJhWaeHOrBgw.png" alt=""></p><figcaption>Google Bard not only provided a definition of flamingos but also included a citation.</figcaption></figure><p>LLMs can do more than just complete text; they can summarize, generate code, solve math problems and more.&nbsp;</p><div>
  <p>Copied to clipboard!</p>
  <pre><code fs-copyclip-element="copy-this-3">&lt;Q&gt;: Two missiles speed directly toward each other, one at 9,000 miles per hour and the other at 21,000 miles per hour. They start 1,317 miles apart. Without using pencil and paper, calculate how far apart they are one minute before they collide.
&lt;A&gt;:</code></pre>
</div><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7acad02c40d777c5be_uQsLsVTB6uSL-VfRzivujHxja8-wvSF2OL4GptGM_4FTvSkaJ2CjmZMvOYqyIV2wSmMT5rfdtN3DXrgbJuLmU5T7O5A8dDvm2kSMVJoSyySV5-9IGfz1zFX4qbq66rftK6JX9LjwR7T_FCZJYMXW0hI.png" alt=""></p><figcaption>Response generated by ChatGPT&nbsp;</figcaption></figure><p>In this case, the format of the prompt is `&lt; Q &gt;` followed by `&lt; A &gt;`. However, the format can vary depending on the specific LLM you're using and the response you're aiming for. For instance, there's a detailed guide on how to <a href="https://replicate.com/blog/how-to-prompt-llama">prompt Llama 2</a>.&nbsp;</p><p>Creating clear and effective prompts is crucial to achieve the results you want, a process known as prompt engineering.</p><h3><strong>Prompt engineering</strong></h3><p>Prompt engineering is all about coaxing the model into a region of its latent space by manipulating the input prompt so the probability distribution of the next-tokens it predicts matches your intent. In simpler terms, it means guiding the model to generate the desired output.</p><p>There are many ways of doing this, such as providing examples of the kinds of outputs you want, instructing the model to write “in the style of” certain authors, using chain-of-thought reasoning, enabling the model to use external tools, and more.</p><h3><strong>Zero-shot prompts</strong></h3><p>A prompt that doesn't provide specific examples for how the model should respond is called a “zero-shot prompt.” The flamingos prompt and the classification prompt are both examples of zero-shot prompts. Zero-shot prompts are effective in cases where the model understands the instruction.</p><h3><strong>Few-shot prompts</strong></h3><p>LLMs excel with zero-shot prompts but may struggle with complex tasks. Few-shot prompts, which include examples within the prompt, enable in-context learning. In this approach, the model learns from both the instructions and the provided examples to understand its task.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7ad98b0710b7a4b5bd_p5FGHZ4oQhjv46aTd2pkBcLbOrTpljqqFuCLK1nYDxkjO-vkC-B2YQ0vHD26xp4qibYyYyYvcmK4N5Zvtd6R_Qtg--JoomGJpj0AkW0LbYOSeft-k-pZanHyGIe2TVB6rhP0Y5HOgTAy2x7uNu4R-xs.png" alt=""></p><figcaption>Vicuna 33B: Few-shot prompting&nbsp;</figcaption></figure><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a9e5946ce6be0da2f_JaZuPahPaKwejaT7XxcHuWed6EEiXwNjAmmaFxEA0utxXiSlaPNis24cARKD2iJnVytBTmgE34r9WPikAssIl2v2HATndM3mP_COb1B2bzCd1xTW6jhjd53X8-N1vQVgdeOGOB3FPZ2Lzs8t1Z4arC8.png" alt=""></p><figcaption>HuggingChat: Few-shot prompting</figcaption></figure><h3><strong>Chain of thought (CoT) prompting</strong></h3><p>CoT prompting encourages the LLM to provide explanations for its reasoning. Combining it with few-shot prompting can yield improved results for more intricate tasks that demand prior reasoning before generating responses.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a2ef13c9c9ea4e49a_0DUqKmq_GlbM6o3Mo-T19l-lwOKhwHNdqPkz9Tv6NmkLxAYVRIdN1K-WIZJtyX5RLUBYdalFjrxkX5EAYd4kWJgjVcyRe-j9YOLNELdTr4wHv2gDhBgYdFVxYEuWHTwPgaEkh6jMZbeEM536d_00fyc.png" alt=""></p><figcaption>Standard prompting vs CoT prompting (<a href="https://arxiv.org/abs/2201.11903">Source</a>)</figcaption></figure><p>The core concept behind CoT is that by presenting the LLM few-shot examples that include reasoning, it will subsequently incorporate the reasoning process into its responses when addressing prompts.</p><h4><strong>Let’s think step by step</strong></h4><p>Instead of including examples with reasoning in the prompt, you can achieve accurate answers by employing zero-shot prompting simply by adding "Let's consider step by step" to the end of the question. This approach has been demonstrated in the <a href="https://arxiv.org/pdf/2205.11916.pdf">Large Language Models are Zero-Shot Reasoners</a> paper to yield reliable results.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a2c0d5b9fc0b70522_sYkpR7MFfYX27TGzGSHXvAqeaNR_A3uc-SfHOfdB6ghzJCXz0zO1INi3QzlxXXkSslSud78f3u3zCCVuxHug09D5s4GoQlormtm0unE61sO-Q8iQTJmAAuIql_ceeVvOiEZASNej4K_hWBnruFpR8lk.png" alt=""></p><figcaption>Zero-shot CoT (<a href="https://arxiv.org/pdf/2205.11916.pdf">Source</a>)</figcaption></figure><h2><strong>In-context learning (ICL)</strong></h2><p>In-context learning (ICL) entails providing context within prompts, which can be in the form of examples (few-shot prompting) or additional information. This approach empowers pre-trained LLMs to assimilate new knowledge.&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7b00210dacf8dbaba5_prxmosontgXmGuQJ0WxEkacJwbStS50flJkzX13eJmTG1HbGiDkXzJ4pTMM1uEuu8epop2pRhNDnWA-kvJJgBSiQJHqt3g1wwLMunYF_21_t1w_t26BhzJgrmj-TwlnD9MyR_lB9rruBm-iTqmFscC4.png" alt=""></p><figcaption>Teaching an LLM new information: HuggingChat</figcaption></figure><p>The categorization in this example can be easily understood — even at a glance, — because it represents a distinction between two- and four-wheelers. The LLM accurately categorizes based on the context provided in the prompt, showing its ability to infer. This approach can be extended to more-complex scenarios. For instance, a chatbot that interacts with data and incorporates context from provided documentation will be limited by the context window's size because it restricts the information you can include in the prompt. Fortunately, you can overcome this limitation by using vector databases.</p><h3><strong>Vector databases</strong></h3><p>Vector databases store context vectors or embeddings. If you want to input documents into your LLM, you must first convert the text into vectors and store them in a vector database. Later, these vectors can be used to provide “relevant” context within prompts to the LLM. Relevancy, referred to as <strong>semantic search</strong>, involves finding similarities between input queries and documents, both of which are represented as vectors. This allows us to fetch selectively from the vector database and provide relevant documents to the LLM without overwhelming the prompt context window.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7ac957a70292c794f4_598lLYbp9pdY6jShouCZuXQK_X_FxnWPxbFP_kkIv1WYnaispQMazodOxpKamY4jxYlvxAykrDIUFfLoFktBhlakfaIzsm1vCdpMfZp4ymOvSue0paxBzJte9QBCuXfr7ImAxTnarX5PRma9sc_cX44.png" alt=""></p><figcaption>Semantic search (<a href="https://txt.cohere.com/what-is-semantic-search/">Source</a>)</figcaption></figure><p>Examples of vector databases include <a href="https://www.pinecone.io/">Pinecone</a>, <a href="https://www.trychroma.com/">ChromaDB</a> and <a href="https://weaviate.io/">Weaviate</a>.</p><p>The generic pipeline for semantic search and model prompting is as follows:</p><ol role="list"><li>Users input their query.</li><li>The query is embedded using identical embeddings as document vectors.</li><li>Semantic search retrieves the k-most similar documents from the vector database.</li><li>The output of semantic search, along with the user query, is sent to the model to generate a coherent response.</li></ol><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a9358df9297a745c3_VcuXY6_891-FjYOPlzxwIQPzmMo4oLOfJE8vKFpK_Jo45mEuy6zTkmK0QoXKFoYqcQ7-8A8t43tOks7Xa1gicD6NZxmJRF4e7fICOmUG41PnaGASjezWGi9NbRZIZQGre3_BvIUs2-AqfWUEG5CMPTc.png" alt=""></p><figcaption>Generic pipeline for in-context learning with semantic search (<a href="https://www.pinecone.io/learn/retrieval-augmented-generation/">Source</a>)</figcaption></figure><p><a href="https://www.langchain.com/">LangChain</a> is a popular tool to build context-aware applications on top of LLMs.</p><h3><strong>What is Retrieval Augmented Generation (RAG)?</strong></h3><p>The documents stored in the vector database may not provide up-to-date information for your LLM to answer accurately. This limitation arises because the model's knowledge is restricted to the data it was trained on, which has a cutoff date. If you ask your model about recent events not included in its knowledge, it may produce inaccurate answers, a phenomenon often referred to as <strong>hallucination</strong>.</p><p>Retrieval Augmented Generation (RAG) addresses this issue by retrieving current context-specific information from an external database. This updated information is then fed into the LLM to generate accurate responses. RAG also enables the LLM to cite resources while generating responses.&nbsp;</p><h4><strong>How does RAG differ from regular in-context learning?</strong></h4><p>RAG can be conceptualized as an online, in-context learning method. The process of generating vectors, storing them in a vector database and updating the index occurs in real time. This effectively addresses the recency issue commonly observed in LLM applications. For instance, you can write your code to ensure that whenever there's a data update, the indexing module upserts the data in the vector database.&nbsp;</p><p>RAG can be implemented through LangChain, HuggingFace, OpenAI and so on.</p><h2><strong>LLM parameters</strong></h2><p>When provided with a prompt, an LLM can generate a long list of potential responses. It operates like a prediction engine. However, in practice, LLMs typically provide a single output that represents the most likely response according to the model. The model calculates the probabilities of different words appearing in a response and returns only those words that meet the set parameters.</p><h3><strong>Model</strong></h3><p>The performance of a pre-trained LLM relies on its size: Larger models tend to produce higher-quality responses. However, bigger models increase costs and require more computational resources.</p><h3><strong>Temperature</strong></h3><p>Temperature influences the model's creativity. Lower temperatures yield consistent and predictable results, while higher temperatures introduce randomness, resulting in more creative outputs. At a temperature of 0, the model always produces the same output. However, with higher temperatures, the model outputs words associated with varying probabilities.</p><h3><strong>Top-p and Top-k</strong></h3><p>Top-p selection involves choosing tokens from the highest-probability options; the sum of their probabilities determine the selection. For example, if `p` is set to 0.15, the model will select tokens like “United” and”'Netherlands” because their combined probabilities add up to 14.7%.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a58abce2cf80c7ec4_3xuGN41Y7YS_FrN2v4e_OuSrPloFU9IR0BQCRDsrR6f6tzParykxZ31uazO5Z1njqicoXsfPWdmOBcmK3p5EGffZSKC_mT6AoBTJhD8r6Eobbkeu0MUYJ52aUPXixE2gqQ7g--T8lXCSHCiV5boZX08.png" alt=""></p><figcaption>Top-p (<a href="https://txt.cohere.com/llm-parameters-best-outputs-language-ai/">Source</a>)</figcaption></figure><p>The lower the value of `p`, the more deterministic the responses generated by the model are. The general recommendation is to alter either temperature or top-p, but not both.</p><p>Top-k selection involves selecting the next token from the list of the highest `k` tokens, which are sorted by their probability. For instance, if `k` is set to 3, the model will choose from the top 3 options.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a85ec36671da4beed_LFlGG_lEIghK6EDT9JX6eRFV0OCeg5V3XU55bK6KOWfyK587162XIwJWp4qUwgSo6Maq2sELAh7x4lYmmwxB4rxwYZdti5WWVx4e-7amly4G-PQ-xFryT8ypOcwa5HIjlcIpGm92GutgTxd_bJgn61w.png" alt=""></p><figcaption>Top-k (<a href="https://txt.cohere.com/llm-parameters-best-outputs-language-ai/">Source</a>)</figcaption></figure><h3><strong>Number of tokens</strong></h3><p>Tokens serve as the fundamental units of text in LLMs. A token doesn't always represent a single word; it can also encompass a group of characters. As a general rule of thumb, one token is roughly equivalent to four characters of English text.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a3a01cebbc6fede41_3kV3_EHVsIsvhqHiDvj68BpsPSwvyFIOv7RV8ZB6zfxJdulGqzsWT2kZDJHwZMc99Zhrstw29L3vEDyC7YiRSQC1AlS4KdvEf_AvzLRS002MFUD2El5-YnmS13FgiNhbgJF9HCOgcx6II0tjFRvIJJw.png" alt=""></p><figcaption>The highlighted text corresponds to tokens. (<a href="https://platform.openai.com/tokenizer">Source</a>)</figcaption></figure><p>The number of tokens in a model corresponds to the maximum number of input tokens the model can accept and the maximum number of output tokens it can produce. Typically, this number is set at 1,024, 2,048 or 4,096, but for some models, it can be even larger.</p><h3><strong>Stop sequences</strong></h3><p>Stop sequences can be employed to instruct the model to stop its token generation at a specific point, such as the end of a sentence or a list. It proves useful when you intend to stop token generation immediately upon encountering a stop sequence. This approach can be tailored to specific use cases or employed to reduce the cost of token generation.</p><h3><strong>Repetition penalty</strong></h3><p>Repetition penalty discourages the repetition of tokens that have appeared recently in the generated text. It encourages the model to produce more diverse tokens by reducing the likelihood of selecting tokens with higher scores.</p><p>When the repetition penalty is set to 1.0, there is no penalty applied. A value of 1.2 strikes a good balance between maintaining accuracy in generation and minimizing repetition, as described in <a href="https://arxiv.org/pdf/1909.05858.pdf">this paper</a>.</p><h2><strong>When to fine-tune?</strong></h2><p>Prompt engineering, with or without few-shot prompts and in-context learning, is suitable when the model needs data input. However, for scenarios requiring specific styles, patterns, specialized skills or internal model improvement, fine-tuning is a better choice. Fine-tuning involves updating model parameters through training on selected data, enhancing the model from within, while prompt engineering enhances the model externally. Fine-tuning is a more advanced technique, often requiring a substantial level of expertise in LLMs.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a85ec36671da4bee9_XFLTneVrI20STsazT3u69bQJtnNhytvSSqjYFNh5DXc9SHjnxgBsxnfPBuIHtvK0WL8No9Z835qaXq826WF48TtUQ7g-_CubAHjb69UvEErXoR7pKAsaRDn6CwVRA1QpS4XoFkfFWGCHCthGk80o0Rs.png" alt=""></p><figcaption>The three fine-tuning approaches (<a href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">Source</a>)</figcaption></figure><p>In the feature-based approach, we load a pre-trained LLM, generate output embeddings for the training set and use them as input features to train a classifier. In fine-tuning I, the output layers are pre-trained, keeping the LLM frozen. In fine-tuning II, all model layers are updated — that’s expensive and requires a lot more compute power.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7ab6869df20b2f7310_L1lzUbREy9_abXbHFOfWJvvoC3-j1QySAl5JkpmLlhrwQt58VxGJ3ixT_xvTUcgIwbP_2WBesT_xkCELMKE3gQTztlcCEiMOlWY9N1Av4ugXO1OVuo7MUuv1jPM1gVJWfXL9C3d67wutg1y_GhYIqak.png" alt=""></p><figcaption>Performance of pre-trained DistilBERT models fine-tuned on the IMDB Movie review dataset (<a href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">Source</a>)</figcaption></figure><p>Updating all layers (fine-tuning II) yields better performance than updating only the output layers (fine-tuning I). This can be seen in the performance plot from an experiment conducted by the blog post author, Sebastian Raschka. The graph shows performance plateaus when training the last two layers and transformer blocks, with no improvement in accuracy between the final stages. In other words, performing a full parameter fine-tuning becomes inefficient in terms of computation resources and time.</p><h3><strong>Parameter-efficient fine-tuning (PEFT)</strong></h3><p>PEFT (Parameter Efficient Fine-Tuning) reuses a pre-trained model to reduce computational and resource requirements. PEFT encompasses techniques that fine-tune a limited number of model parameters while preserving accuracy. Examples of PEFT techniques include prompt tuning and low rank adaptation (LoRA).</p><h4><strong>Prompt tuning</strong></h4><p>Prompt tuning falls between prompt engineering and fine-tuning. Unlike fine-tuning, it doesn't modify the model parameters. Instead, it involves passing prompt embeddings along with each prompt sent to the model. Essentially, the model updates the prompt itself, creating what we call <strong>soft prompts</strong>. These soft prompts comprise embeddings, numeric representations derived from the larger model's knowledge. In contrast to <strong>hard prompts</strong>, which are manually crafted with discrete input tokens, soft prompts cannot be directly viewed or edited as text.</p><h4><strong>Low rank adaptation (LoRA)</strong></h4><p>Another popular PEFT technique is LoRA, which facilitates fine-tuning specific adapters loaded into the model. LoRA achieves this by transforming weights into a lower-dimensional space, effectively reducing computational and storage demands.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7aafb7a686d8643a80_VTe0h3y2KO_ScFQ4BweoovzGq_nm_xdECSxbcp4ynct9HtgwQu_uYm09j5RVIcSqYW8xErsn1nujOp50k8XeyzwfWgdtFkOZ_ybBcJzFeblbvVb1jgWB_FpTy1PlIHSde84RBNjp7CbSX1WBAXdp7mI.png" alt=""></p><figcaption>Pretrained weights W are frozen while the trainable parameters A and B approximate the gradient update, ∆W (<a href="https://arxiv.org/pdf/2106.09685.pdf">Paper</a>)</figcaption></figure><h3><strong>Reinforcement learning with human feedback (RLHF)</strong></h3><p>In RLHF, a pre-trained model is fine-tuned via a combination of supervised and reinforcement learning. Human feedback is collected by ranking or rating various model outputs, creating a reward signal. These reward labels train a reward model, which, in turn, guides the LLM’s adaptation to human preferences.&nbsp;</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7aa204e991feec3a9a_SgR3MGQN8PV_rxDdoPawloL1aMOrdfU-kutd96ihVc4-EBKG-XA-dFCsZYbZAI-1bM4_jM60mpOXLEWUTpYmek7IjP2s3Cx6oSCWo-Qvodr_bcc2oXcL6Ww1E8os-scKENqu6FTFIuVq65vlFBsytXs.png" alt=""></p><figcaption>A typical RLHF pipeline</figcaption></figure><p>The reward model can be initialized from a supervised fine-tuning model (SFT), where the model is fine-tuned through supervised learning. Then, the reward model calculates the loss and updates the pre-trained LLM using a form of reinforcement learning known as proximal policy optimization (PPO).</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a3a01cebbc6fede47_VyYrZ4BkcSSJxHHcnfsfZr4UUETO1JcliFMEr1bBkRXGjw-g-0kM_BtQN9golSFOESm1uLjlsxNVlYbJHriSCJUX-XDIgd_CSh4k37XyMPazzF-203QHagbLY8An0lwNbO4XUYG3WnsgBxnNFZxgTiE.png" alt=""></p><figcaption>RLHF process applied to InstructGPT model: supervised fine-tuning, reward model and reinforcement learning via PPO (<a href="https://arxiv.org/pdf/2203.02155.pdf">Source</a>)</figcaption></figure><p>The <a href="https://github.com/huggingface/trl">Transformer Reinforcement Learning (TRL) library</a> can be used to train transformer language models and stable diffusion models from SFT, reward modeling to PPO. It is built on top of the <a href="https://github.com/huggingface/transformers">transformers</a> library.</p><h2><strong>Optimization techniques for fine-tuning</strong></h2><p>To enhance fine-tuning efficiency, consider employing quantization and zero-redundancy optimization.&nbsp;</p><p>Note: This section is excerpted from Niels Bantilan’s blog post titled “<a href="https://www.union.ai/blog-post/fine-tuning-vs-prompt-tuning-large-language-models">Fine-Tuning vs. Prompt Engineering Large Language Models</a>” on Union.ai.&nbsp;</p><h3><strong>Quantization reduces memory utilization</strong></h3><p>Quantization is the process of reducing the precision of numerical data so it consumes less memory and increases processing speed. However, lower precision leads to lower accuracy because less information is being stored within each layer. This doesn’t just apply to neural networks; if you’re a data scientist or ML engineer who has used Numpy, Pandas or any other numerical library, you’ve probably encountered `float64`, `float32` and `float16` data types. The numbers 64, 32 and 16, respectively, indicate how many bits are used to represent, in this case, floating point numbers.</p><p>Deep learning frameworks like <a href="https://pytorch.org/docs/stable/notes/amp_examples.html">PyTorch</a>, <a href="https://www.tensorflow.org/guide/mixed_precision">TensorFlow</a> and <a href="https://github.com/deepmind/jmp">Jax</a>, commonly provide utilities to do mixed-precision training; this lets the framework automatically cast the weights, biases, activations and gradients to lower floating-point precision (e.g. float16) when appropriate, and then cast them to higher precision representations (e.g. float32) when numerical stability matters (for instance in gradient accumulation or loss scaling). One library that may be of interest to fine-tuners is the <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a> library, which uses 8-bit optimizers to reduce the memory footprint significantly during model training.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a7583bb32b0bb9207_fmaeYH7UD8eOrpmDv55DuubnHoj9kXUFkKWN2dKt4g1qvFMSsyZhKF92eicJfCryhoLuwiAKp5EyP9stqFYiM-nz0bp6DBo-u9pqHUFMKQUOIJwvLjTlGzFB8CE7uRrPqhjQBSCixHWg37A5YmWvKXg.png" alt=""></p><figcaption>Converting an FP-16 vector to INT8 through quantization (<a href="https://huggingface.co/blog/hf-bitsandbytes-integration#introduction-to-model-quantization">Source</a>)</figcaption></figure><h3><strong>Zero-redundancy optimization shards and offloads model state</strong></h3><p>A few years ago, the only sort of data parallelism that you could easily leverage with deep learning libraries consumed a lot of GPU memory. If you had a machine with four GPUs, you could replicate the model four times, and if you could train a batch size of 8 on each GPU, you would obtain an effective batch size of 32.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a9358df9297a7459f_0aW9iRLuTbXJ9QyE35YRzXID3pjvD59afhrq1U_9_9AA2gvlGKera8SDiGMRYU6bbKzLwtoIwJo18GWU74xuEJ3szLOWt1YCtUAZXqF32unos4K3qAlG4yw2iqaypNwQoLAvJvTQUDe8Dl6XDGYkM8k.png" alt=""></p><figcaption>Conventional data parallelism maintains a full replica of the model across the available cores, allowing you to increase the effective batch size during training. (<a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Source</a>)</figcaption></figure><p>The<a href="https://arxiv.org/pdf/1910.02054.pdf"> ZeRO paper</a>, which is available via<a href="https://github.com/microsoft/DeepSpeed"> DeepSpeed</a> and Pytorch’s<a href="https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html"> FSDP</a> implementation, enables you to shard the optimizer state, gradients and parameters across the available GPUs and CPUs in the training system. This is done in a layer-wise fashion, so only the model state required for a specific local forward/backward pass is replicated across GPUs. For example, if you’re training a neural net with three layers, the ZeRO protocol replicates the model state required for the forward pass of the first layer, freeing up memory once it obtains the activations. This is repeated in the forward passes for layers two and three. Finally, this process is applied to the backward passes, resulting in updates to the parameters of the model.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a87a474ff8229fb12_138XdFxUt2Ok4xbwlf46-k7bHYFLKjAYo63p8E-19o7W51m3aZEHNb3zFfb75cQqQlfr0WSDbINgNsy76DCfY6GMy5E_5jTLSESRAn8zLXi36wog1VXzrTotVyQdiaNSue-vIsKTXkICklq-shNfM7Y.png" alt=""></p><figcaption>ZeRO is a type of data parallelism that only loads the weights needed for a particular layer-wise forward/backward pass, so you never load a full model into memory. (<a href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">Source</a>)</figcaption></figure><p>The ZeRO framework also allows for offloading of model states to CPU or <a href="https://en.wikipedia.org/wiki/NVM_Express">NVMe</a> where appropriate, which further reduces GPU memory consumption and improves training speed.</p><h2><strong>What is hallucination in LLMs?</strong></h2><p>A hallucination in an LLM occurs when the model generates text that seems plausible but is actually incorrect or nonsensical. While <a href="https://bard.google.com/">Google Bard</a>, Perplexity AI, and <a href="https://www.bing.com/search?q=Bing+AI&amp;showconv=1&amp;FORM=hpcodx">Bing Chat</a> show citations, they may reduce hallucinations to some extent but don't fully eliminate them.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a98bd2645ac8cb546_2oYfMA1IYxhebWoUqcNNgifKRQz0dovbka_dE8SfAfSr9GfcENkc0DoxcvbaN9eQL-vNa5XBbzV8wWb_51wwEU7nuljL02RjsG3phGSbOkyZjYZPwfN3WQBlsx5hCzoVvzTzbkQFnCMeh990LxGCMBU.png" alt=""></p><figcaption>ChatGPT got deceived! 😱(<a href="https://flyingbisons.com/blog/hallucinations-of-chatgpt-4-even-the-most-powerful-tool-has-a-weakness">Source</a>)</figcaption></figure><h3><strong>Why do LLMs hallucinate?</strong></h3><p>LLMs tend to hallucinate when they have an incomplete understanding of the prompt's context. In such cases, they may make educated guesses based on learned patterns. Hallucinations can also be a result of incomplete or incorrect training data. Since LLMs rely on their training data rather than real-world information, their outputs may sometimes be irrelevant. However, techniques like Retrieval Augmented Generation (RAG) can be employed to incorporate real-time information and mitigate this issue.</p><h3><strong>How can you reduce hallucinations?</strong></h3><p>Hallucination can be desirable in certain scenarios, such as when creative responses are needed, like in story writing or marketing content creation. However, when accuracy and factual information are required, hallucination becomes undesirable.</p><p>Here are ways to mitigate hallucinations:</p><ol role="list"><li>Prompt engineering: By including more contextual information in the prompt, the language model gains a better understanding of the context and can generate more appropriate responses.</li><li>In-context learning: When the model receives contextual information as part of the prompt, it has access to additional data that aids in generating accurate responses. A lack of context is a common cause of hallucinations.</li><li>Controlled generation: Imposing sufficient constraints in the prompt can restrict the model's freedom to produce hallucinatory content.</li></ol><p>However, it's important to note that hallucinations cannot always be completely eliminated. Models may still generate errors that are challenging to detect.</p><h2><strong>Running LLMs on local machines</strong></h2><p>Indeed, you heard correctly! You can run LLMs on your local machine to generate predictions. You can utilize <a href="https://github.com/mlc-ai/mlc-llm">Machine Learning Compilation (MLC)</a> for LLM or employ <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> for the same purpose.</p><h3><strong>MLC</strong></h3><p>MLC enables the native deployment of any LLM through native APIs with compiler acceleration. It offers support for the following platforms and hardware:</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7a2c0d5b9fc0b70519_WftY87s-mCLRQ2vZLh2vlb05ZEmhFNAHbSBmMV88SkK6RmrT7HT_NT94F7Zo6ePrKy0eP72oXIxEkMznNQU-EGaK8N7j07aNio93WsxqG18mvRYdRPGAKxCbe5CAeRgRLHIRhzpH_m6XSSyWDa-pRUc.png" alt=""></p><figcaption>Platforms and hardware supported by MLC</figcaption></figure><h3><strong>llama.cpp</strong></h3><p>llama.cpp empowers you to execute the Llama model using 4-bit integer quantization on a MacBook. It conducts the inference of the Llama model in pure C/C++.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7ada8b9b0182bf5fd7_BPQqPJjzyeKnknmYc4z9UKdfa-kyBOgQKFjOYl4OlTJ6DIN7MgxXNsDNuU6Ah2XGVxoLeAK_HUcsZWJFyTm0SESPLVy4ZllNa4lYROoaljzC43SXFE6yqpiBgWG7sBey21WfdwpiicntBVxkig8YTDE.png" alt=""></p><figcaption>Platforms, models and bindings supported by llama.cpp&nbsp;</figcaption></figure><h2><strong>Conclusion</strong></h2><p>This blog post introduced LLMs and discussed the various ways we could interact with them. It began with an overview of how LLMs had been developed and their applications. Following that, we delved into how LLMs were tested and why different types of LLMs were necessary, highlighting their differences.</p><p>We then explored the concept of prompts, which interact with the model and encourage it to generate accurate responses through various manipulations. Subsequently, we explored LLM parameters and examined some reasons for fine-tuning an LLM, along with optimization strategies to reduce computational and storage costs.</p><p>Finally, we addressed the issue of hallucination in LLMs and provided insights into running LLMs on local machines.</p><h3><strong>Fine-tuning and running LLMs in production</strong></h3><p>For those eager to explore further, you may want to know how to fine-tune an LLM with an <a href="https://www.union.ai/blog-post/orchestration-for-data-machine-learning-and-infrastructure">orchestrated</a> pipeline and perform low-latency, high-throughput inference.</p><p>Successful fine-tuning typically demands a significant amount of GPUs, RAM and memory resources, although it's worth noting that some models can be fine-tuned on local machines. For instance, you can find guidance on fine-tuning Llama 2 on a local machine <a href="https://www.youtube.com/watch?v=3fsn19OI_C8">here</a>. The specific resource requirements vary depending on the size of the model you're working with.</p><p><a href="http://github.com/flyteorg/flyte">Flyte</a> enables the seamless integration of PyTorch Elastic Trainer, PEFT, quantization and various optimization techniques into your LLM pipeline for fine-tuning. Additionally, it offers the flexibility to implement strategies such as caching, recovery, checkpointing and running on spot instances. These features are part of the orchestration toolkit, which proves invaluable when optimizing resource allocation to lower costs and manage jobs more efficiently.</p><p>To delve deeper into the advantages offered by orchestrators like Flyte, you can explore further details <a href="https://flyte.org/features">here</a>.</p><p>For inference, various tools like <a href="https://github.com/vllm-project/vllm">vLLM</a>, <a href="https://github.com/OpenNMT/CTranslate2">CTranslate2</a>, <a href="https://docs.ray.io/en/latest/serve/index.html">Ray serve</a> and <a href="https://github.com/huggingface/text-generation-inference">text generation inference</a> can be employed to meet your specific needs.</p><figure><p><img src="https://assets-global.website-files.com/63bc83b29094ec80844b6dd5/6526dc7b7cf1df604e4a5c23_35MRswvSORpeJB6VZMfGzoOn85yxbufQWQJgP7klZHsoTvxUZijkv0juf-mwQZGeZ0i1-fc3K-CyVw4KH5GTzj8OikV3AQzweML2Ufr3-AKNXTaAx3FB3N3USM_uGxTftgL97x5a_gYhw-EsDSc_NPs.png" alt=""></p><figcaption>Comparison of frameworks for LLM inference (<a href="https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407">Source</a>)</figcaption></figure><p>The world of LLMs is constantly evolving. As people venture into this field, we aim to ensure that all relevant and up-to-date information is included in this post. This is an ongoing effort, and we encourage you to share your thoughts and ideas with us on <a href="https://slack.flyte.org/">Slack</a>, as well as your feedback on this post.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built a virtual tabletop for playing Dungeons and Dragons (154 pts)]]></title>
            <link>https://www.diceright.com/</link>
            <guid>37857765</guid>
            <pubDate>Thu, 12 Oct 2023 14:35:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.diceright.com/">https://www.diceright.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37857765">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="homepage-feature-list"> 
    <p>Features</p>
      <div>
          <p>Simple Alerts</p><p>
          DiceRight has a system of alerts that let you know when you need to do something, like when you take damage or when it's your turn.  This helps the game keep moving, with fewer delays.

        </p></div>


      <div>
          <p>Build Characters by Level</p><p>
          DiceRight makes it easier to track everything about your characters because you build them level by level.  So you're never left wondering how you got a certain stat or where an ability came from. 

        </p></div>


      <div>
          <p>Dynamic Lighting</p><p>
          Our simple tools let you add walls and lighting to any map.

        </p></div>


      <div>
          <p>Customize Everything</p><p>
          You can customize everything about your campaign - the races, classes, items, skills, spells, currencies, languages, and more.

        </p></div>


      <div>
          <p>Streamlined Combat</p><p>
          DiceRight automates the monotonous parts of combat.  No more asking if you hit, no more asking how much damage you took.

        </p></div>


      <div>
          <p>Accessible Everywhere</p><p>
          DiceRight runs right in the browser so no installs are needed and it's easy to get started.  It’s even optimized for mobile.

        </p></div>


  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Twelve-Factor App (284 pts)]]></title>
            <link>https://12factor.net/</link>
            <guid>37857544</guid>
            <pubDate>Thu, 12 Oct 2023 14:19:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://12factor.net/">https://12factor.net/</a>, See on <a href="https://news.ycombinator.com/item?id=37857544">Hacker News</a></p>
<div id="readability-page-1" class="page">
  

  <header>
    
  </header>

  <section>
  <article>
<h2 id="introduction">Introduction</h2>

<p>In the modern era, software is commonly delivered as a service: called <em>web apps</em>, or <em>software-as-a-service</em>. The twelve-factor app is a methodology for building software-as-a-service apps that:</p>

<ul>
<li>Use <strong>declarative</strong> formats for setup automation, to minimize time and cost for new developers joining the project;</li>

<li>Have a <strong>clean contract</strong> with the underlying operating system, offering <strong>maximum portability</strong> between execution environments;</li>

<li>Are suitable for <strong>deployment</strong> on modern <strong>cloud platforms</strong>, obviating the need for servers and systems administration;</li>

<li><strong>Minimize divergence</strong> between development and production, enabling <strong>continuous deployment</strong> for maximum agility;</li>

<li>And can <strong>scale up</strong> without significant changes to tooling, architecture, or development practices.</li>
</ul>

<p>The twelve-factor methodology can be applied to apps written in any programming language, and which use any combination of backing services (database, queue, memory cache, etc).</p>
</article>
  <article>
<h2 id="background">Background</h2>

<p>The contributors to this document have been directly involved in the development and deployment of hundreds of apps, and indirectly witnessed the development, operation, and scaling of hundreds of thousands of apps via our work on the <a href="http://www.heroku.com/" target="_blank">Heroku</a> platform.</p>

<p>This document synthesizes all of our experience and observations on a wide variety of software-as-a-service apps in the wild. It is a triangulation on ideal practices for app development, paying particular attention to the dynamics of the organic growth of an app over time, the dynamics of collaboration between developers working on the app’s codebase, and <a href="http://blog.heroku.com/archives/2011/6/28/the_new_heroku_4_erosion_resistance_explicit_contracts/" target="_blank">avoiding the cost of software erosion</a>.</p>

<p>Our motivation is to raise awareness of some systemic problems we’ve seen in modern application development, to provide a shared vocabulary for discussing those problems, and to offer a set of broad conceptual solutions to those problems with accompanying terminology. The format is inspired by Martin Fowler’s books <em><a href="https://books.google.com/books/about/Patterns_of_enterprise_application_archi.html?id=FyWZt5DdvFkC" target="_blank">Patterns of Enterprise Application Architecture</a></em> and <em><a href="https://books.google.com/books/about/Refactoring.html?id=1MsETFPD3I0C" target="_blank">Refactoring</a></em>.</p>
</article>
  <article>
<h2 id="who_should_read_this_document">Who should read this document?</h2>

<p>Any developer building applications which run as a service. Ops engineers who deploy or manage such applications.</p>
</article>
</section>

<section>
  <article>
<h2 id="the_twelve_factors">The Twelve Factors</h2>

<h2 id="i_codebase"><a href="https://12factor.net/codebase">I. Codebase</a></h2>

<h3 id="one_codebase_tracked_in_revision_control_many_deploys">One codebase tracked in revision control, many deploys</h3>

<h2 id="ii_dependencies"><a href="https://12factor.net/dependencies">II. Dependencies</a></h2>

<h3 id="explicitly_declare_and_isolate_dependencies">Explicitly declare and isolate dependencies</h3>

<h2 id="iii_config"><a href="https://12factor.net/config">III. Config</a></h2>

<h3 id="store_config_in_the_environment">Store config in the environment</h3>

<h2 id="iv_backing_services"><a href="https://12factor.net/backing-services">IV. Backing services</a></h2>

<h3 id="treat_backing_services_as_attached_resources">Treat backing services as attached resources</h3>

<h2 id="v_build_release_run"><a href="https://12factor.net/build-release-run">V. Build, release, run</a></h2>

<h3 id="strictly_separate_build_and_run_stages">Strictly separate build and run stages</h3>

<h2 id="vi_processes"><a href="https://12factor.net/processes">VI. Processes</a></h2>

<h3 id="execute_the_app_as_one_or_more_stateless_processes">Execute the app as one or more stateless processes</h3>

<h2 id="vii_port_binding"><a href="https://12factor.net/port-binding">VII. Port binding</a></h2>

<h3 id="export_services_via_port_binding">Export services via port binding</h3>

<h2 id="viii_concurrency"><a href="https://12factor.net/concurrency">VIII. Concurrency</a></h2>

<h3 id="scale_out_via_the_process_model">Scale out via the process model</h3>

<h2 id="ix_disposability"><a href="https://12factor.net/disposability">IX. Disposability</a></h2>

<h3 id="maximize_robustness_with_fast_startup_and_graceful_shutdown">Maximize robustness with fast startup and graceful shutdown</h3>

<h2 id="x_devprod_parity"><a href="https://12factor.net/dev-prod-parity">X. Dev/prod parity</a></h2>

<h3 id="keep_development_staging_and_production_as_similar_as_possible">Keep development, staging, and production as similar as possible</h3>

<h2 id="xi_logs"><a href="https://12factor.net/logs">XI. Logs</a></h2>

<h3 id="treat_logs_as_event_streams">Treat logs as event streams</h3>

<h2 id="xii_admin_processes"><a href="https://12factor.net/admin-processes">XII. Admin processes</a></h2>

<h3 id="run_adminmanagement_tasks_as_oneoff_processes">Run admin/management tasks as one-off processes</h3>
</article>
</section>


  


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The AI research job market shit show (and my experience) (118 pts)]]></title>
            <link>https://www.interconnects.ai/p/ai-research-job-market</link>
            <guid>37857521</guid>
            <pubDate>Thu, 12 Oct 2023 14:18:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.interconnects.ai/p/ai-research-job-market">https://www.interconnects.ai/p/ai-research-job-market</a>, See on <a href="https://news.ycombinator.com/item?id=37857521">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>It’s pretty wild that everyone is so interested in the where’s and when’s of researchers in AI these days. It's becoming a bit like transfer news in all of our favorite sports leagues. It's more than just drama and gossip. Zoomed-in, it's the leading indicator of which companies are going to gain and fall behind. Zoomed-out, it's a way to measure the consolidation vs. dispersion of talent in AI. Until ChatGPT, talent was always incrementally aggregating at Brain, FAIR, etc. Now, AI talent is so spread thin that it's a defining feature of working in AI: it's extremely hard to get the people you want.</span></p><p>The investment in GenAI is an ongoing catalyst for a job market shakeup. Jobs in GenAI and LLMs are plentiful, everything else is on ice (but thawing). Many people trying to hire are super stressed about getting the people they want. Too many organizations trying to ship LLMs, not enough skilled people. Once the balance shifts in the direction of extreme scarcity, a sense of normalcy in a job search is hard to achieve.</p><p>The eyes on researchers' movements are the biggest unspoken confirmation we can get that AI companies need researchers in order the make the transition from concept to experiment to product. These are the people who are keeping training and product decisions on track with the big-picture trends, which can change on a dime in a day. It is worth every penny to have someone who knows how to figure out which of ten papers you notice in a month means you need to pivot your company.</p><p>This is where we talk about the truckload’s researchers are getting paid and reasoning about. Most people, ie all except the most academic few (who are still compensated well, just not phenomenally), are balancing the opportunities of huge mostly guaranteed compensation for the foreseeable future versus the idea of making “at least millions even if my startup fails” (yes I’ve heard that said). The market is one where everyone is trying to make their buck. </p><p>This feeding frenzy is causing quite an upheaval in where people want to work. So many places have high turnover and attrition that everyone is unsettled. It started with and is most known in big tech companies, but it's not limited to them. I've seen top researchers join different companies and leave within 6 months. Given all of this, people are unsettled and the grass isn’t always that much greener on the other side, because any happy hour will quickly tell you how nuts it is to work at your five closest competitors. I certainly felt this.</p><p><span>The compensation numbers we’re talking about are things like top researchers (e.g. people on a paper almost as important as&nbsp;</span><em>Attention is All You Need</em><span>&nbsp;or those with longer careers) are being offered base salaries of about $ 1 million from OpenAI. Packages for new grad Ph. D.s were topping out at $500-600k pre ChatGPT, now that number is closer to $850k for the absolute best (here's some&nbsp;</span><a href="https://www.teamrora.com/post/ai-researchers-salary-negotiation-report-2023" rel="">data from earlier in the year</a><span>). Everyone else is pulled up from there, as long as they're willing to express a vague interest in GenAI.</span></p><p><span>Google's hiring is a good indicator of the space. It's well known on the street that Google DeepMind has split all projects into three categories: Gemini (the large looming model), Gemini-related in 6-12months (applied research), and fundamental research, which is oddly only &gt; 12 months out.&nbsp;</span><strong>All of Google DeepMind's headcount is in the first two categories, with most of it being in the first</strong><span>.</span></p><p><span>The other shining light in the previous era of industry research was Meta. Now, they have a different way of framing prioritization. To paraphrase someone on the Llama team, everyone on&nbsp;</span><strong>Meta's GenAI technical staff should spend</strong><span>&nbsp;</span><strong>about 70% of the time directly on incremental model improvements and 30% of the time on ever-green work.</strong><span>&nbsp;This is more aligned with me, but we're likely to know which pillars Meta is playing in (LLM, text-to-image, audio, etc) and they'll push these models very fast.</span></p><p>Structurally, the places where open research and science are the priorities is exceedingly rare. Even if someone joins as an academically minded research scientist, realities will always pull people into the business needs (especially at startups).</p><p>The result of all of this that I'm most excited about in all of this investment is that we’re going to get way further with the Transformer architecture than most ideas in the past. Spreading a bunch of top minds out who have similar interests with varying backgrounds is a magnificent way to ensure we exploit the maximum potential of the Transformer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png" width="1376" height="864" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1376,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;natolambert\\_A\\_bunch\\_of\\_researchers\\_fleeing\\_in\\_terror\\_from\\_tech\\_\\_d1f86215-52b9-40b4-8134-0b4144c3a3b1.png&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="natolambert\_A\_bunch\_of\_researchers\_fleeing\_in\_terror\_from\_tech\_\_d1f86215-52b9-40b4-8134-0b4144c3a3b1.png" title="natolambert\_A\_bunch\_of\_researchers\_fleeing\_in\_terror\_from\_tech\_\_d1f86215-52b9-40b4-8134-0b4144c3a3b1.png" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F43e8e4a4-5ee7-4f30-810b-ffb03a860559_1376x864.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption>Midjourney</figcaption></figure></div><p>The academic side of the ML community will still have a part to play in the GenAI revolution. It's unlikely that they're training very relevant base models 2-5 years out as compute requirements grow exponentially and before any government/public compute infrastructure can be set up. In the meantime, there's plenty of room for them to play in post-training research (fine-tuning, RLHF, safety, etc), societal impacts work, and specific-purpose fine-tunes.</p><p><span>We've already seen some academics get noticed by going artifact rather than paper first. My favorite example is&nbsp;</span><a href="https://www.lmsys.org/" rel="">LMSYS</a><span>&nbsp;which trained the Vicuna model, created a popular inference and training repository, collected lots of model comparison data, designed a chat benchmark, and more. All of these things are impactful on their own, and only some of them were followed by papers.</span></p><p>This is at the same time as the decentralized, online research organizations like EluetherAI and LAION continue to be successful.</p><p>I'm excited to see who I notice first. I've noticed the rate of RLHF research, as measured by the number of Arxiv entries, has increased a lot since August. With a new batch of graduate students starting there's even more free capital to explore the new set of research ideas that ChatGPT unleashed.</p><p>Some practical predictions:</p><ul><li><p>Paper submissions at top ML conferences at least slow down, if not decrease, due to the drop in participation from Big Tech companies (in the face of way more money in the area).</p></li><li><p>The population of graduate students in ML changes. The financial opportunity costs of doing a PhD in AI went way up with the state of the industry. It's good to push people to figure out why they want to go to graduate school.</p></li></ul><p><span>I finished up at HuggingFace this week, which was a great partnership (I shared lessons&nbsp;</span><a href="https://twitter.com/natolambert/status/1709598547178582373" rel="">here</a><span>), with the most surprising being&nbsp;</span><em>getting visibility for your work is harder than doing good work</em><span>. Love or hate the media strategy of startups like HuggingFace, they know how to help themselves grow in the public opinion, which matters. I knew this job change was coming, so I needed to make sense of the marketplace of jobs.</span></p><p><span>I wrote about my&nbsp;</span><a href="https://www.natolambert.com/writing/ai-phd-job-hunt" rel="">job search when I finished my Ph.D.</a><span>, which was also quite popular. The takeaways on the values of networking, being proactive, being visible, and being positive all apply. The specifics of what I did this time around revolved around being in a highly specialized and in-demand area, but I was more specific about my goals.</span></p><p><span>My goals were to find somewhere that enables me to continue to learn about RLHF (both scientifically and with engineering systems) and that is open enough for me to continue Interconnects/podcasting. I did not want to be any of a founder, a founding engineer, a cog in a large company, somewhere not open to sharing, etc. In filtering this list down, I withdrew from places like Apple, Google DeepMind (hiring&nbsp;</span><em>a ton</em><span>&nbsp;in RLHF), Boston Dynamics AI Institute, lots of startups I chatted with, etc.</span></p><p>Even with clear goals, it is very hard to stay on track. There are substantial equity upsides to be had at a lot of other companies. I happily wasted plenty of time engaging with and weighing these options.</p><p>Even with the companies I felt very resonant with, I found a lot of them to have a hard time articulating exactly what I would be doing. The nature of a lot of the companies is that they're trying to build out the initial teams to implement an RLHF pipeline, or something similar. Having just hustled super hard to set up an RLHF pipeline for the first 8 months of this year, doing it over right away was not that exciting.</p><p>Regardless, the fact is that most jobs don't know what you will work on now and you still won't be 100% able to choose your direction. The less academic a place, the more you can expect to be given some high-level agenda to attend to. I knew I would be in for a culture shock if there were strong directions where I ended up, as the independence of HuggingFace was one of my favorite things. OpenAI, and maybe parts of Gemini, are the outliers where engineering is extremely organized. I was not that interested in being a cog and it is simply not what I'm best at. Anthropic also seems to be barely hiring for most researchers. The last I heard is they're happy with the RLHF tooling they have internally and mostly build with it rather than developing it further.</p><p>This left me with not that many options (ranked from most to least academic, with where I'm going hidden for the actual readers and fans):</p><ul><li><p><strong>Cohere for AI (rejected after onsite)</strong><span>: I would've joined a small team and been the "RL" person. They have an intentionally small team to do some research, give the outlet for Cohere engineers that want to publish to dabble in it, uplift underrepresented groups, and be all around solid brand building. Everyone on this team is good and for a remote environment, I would see it being very positive. It's hard to join a team that is not central to their product and growth, though (given my experiences at HuggingFace) -- at this point, such a sideshow makes me think my goals are bigger, and finding core synergy lets that flourish.</span></p></li><li><p><strong>Allen Institute for AI (offer)</strong><span>: This was one I didn't expect. Shifting what was historically a hybrid between something like O.G. Google Brain and an academic research group towards a slightly more engineering focus (to train a real LLM). Therein, the role is somewhat undefined in the spectrum of advising, engineering, self-led research, etc., but they have a very strong commitment to figuring out how NLP works. Talking to folks, it almost seemed like they were late adopters of RLHF and now realize they need someone to help understand it. It will be exciting to join them.</span></p></li><li><p><strong>Scale AI (verbal offer)</strong><span>: They're trying to get into the RLHF and post-training research space to synergize with their massively growing business as the specialized data provider for LLM training companies. This idea makes sense, join, do some research, have access to the most data out of anyone doing RLHF research, help customer integrations at times, and set up a lab. On paper, this is probably the most exciting place I interviewed. For some reason, I felt that I didn't resonate strongly with the people I talked to -- sometimes that is just how it is, not all good people will work together on a team. This uncertainty, with the high likelihood of long hours, made it something I couldn't commit to at this stage of my life.</span></p></li><li><p><strong>Mosaic, now Databricks (offer)</strong><span>: As every training startup is, Mosaic wants to make RLHF easy to use and impactful. The team there is very solid and there's a likelihood they continue to release some models (and maybe papers) to the public. Good for me. Structural hesitation exists to join a startup just after the acquisition because motivation is likely to be lower and policies will change. I was worried that Databricks would have less incentive to openness than Mosaic because their best ML customer pipeline will be those already spending big money on their data products. We've seen almost every company walk back their openness in the recent years of ML, so it is likely to happen again.</span></p></li><li><p><strong>Meta, Llama Team (rejected after onsite)</strong><span>: This was a pretty simple idea -- work on the best open-source models for a bit. My resistance to joining a big company and a big team would make this hard, but if you care about open science, going to Meta right now should always be on your radar.</span></p></li><li><p><strong>Google DeepMind (withdrawn)</strong><span>: Google is hiring all over the place for RLHF. As the most closed-off company I talked to, it was always funny to try and get details of what you'll be doing. Across a few teams, it seems like they mostly have an area of expertise and are figuring out research to improve tools in that area. The resources and infra seem unparalleled. Two I talked to were multimodal RLHF and LLM agents. While cool, the vagueness would make an offer much less appealing than something like the Llama team. That's why I let the interview process kind of fall to the back burner.</span></p></li><li><p><strong>Contextual AI (verbal offer)</strong><span>: Of many startups I talked to, the way they're going about things just really makes sense to me. They're very pro-obvious, but they're building customer relationships and different model pipelines first. Thinking about the good things Contextual is doing made me realize the next step of my&nbsp;</span><a href="https://www.interconnects.ai/p/are-open-llms-viable" rel="">argument against "just train model" startups like Mistal</a><span>&nbsp;-- the longer you put off building customer relationships, the more companies looking to spend on GenAI will go elsewhere. Double this by putting off the process to build a customer-focused culture that's needed to build a viable business and not just roll the acquisition dice. Knowing the founders of Contextual they offered me a nice hybrid technical staff / scientific communicator role I was very excited about. If they were in SF and my commute wasn't going to be awful, I think I really may have ended up there.</span></p></li></ul><p>All of these options I was considering were very positive. It was a very privileged position to be. The number of people who have done RLHF is quite low still, which points to the structural blockers at play in spinning up impactful data workstreams. In 6months, that'll be different, but the baseline level of using RLHF will likely be even more complex.</p><p><strong>Pre-empting questions on “why didn’t you talk with XXX”</strong></p><p><strong>:</strong><span> OpenAI didn’t really need me. They have so many RLHF experts and seemed like no blog territory. I didn’t really try to get in there. I gave a soft attempt at Anthropic, but doesn’t seem like they’re hiring many researchers. Inflection rejected my online application, but I didn’t really try by reaching out to people there. I had offers for introductions there (and many places), it just didn’t seem worth it unless they were excited about me to start with.</span></p><p>The interviews here were very light, which is why I didn’t outline the timeline and scope like in my first job search post. More research chats, some coding interviews, lots of trying to figure out what the right fit is. If you’re looking for a job in LLMs, most of the questions will be about what different internal components do. I would recommend you know:</p><ul><li><p><strong>Exact details of how attention works at an implementation level. </strong><span>You can look at </span><a href="https://github.com/karpathy/nanoGPT" rel="">nanoGPT</a><span> or many other sources.</span></p></li><li><p><strong>Basics of multi-GPU training, estimating VRAM usage, hyperparameters to make the model footprint shrink (e.g. quantization). </strong></p></li><li><p><strong>Regularization tools like batch norm, dropout, weight decay, etc.</strong></p></li></ul><p>That’ll cover most of your technical interviews. Yes, you’ll need to estimate some program runtime, but mostly at less exciting companies that haven’t updated their interview process.</p><p><span>Other than the technical side, </span><strong>always have a company-specific story of what you would work on with them</strong><span>. If you say the same thing to everyone, you’ll get much less traction (unless you’re very famous). Also, I found </span><strong>many companies looked at GitHub repos and HuggingFace artifacts listed on my CV</strong><span>, which may make for easier things to talk about than messy research projects.</span></p><p>On a personal note, since two years ago, people have taken reinforcement learning much, much more seriously. That’s a testament to changes in the field, mostly thanks to RLHF, but I’m excited.</p><p>Elsewhere:</p><ul><li><p><span> My recent team at HuggingFace released a 7 billion parameter chat model that is trained with RLHF. It outperforms Llama 70b chat on MT Bench and excites me because </span><strong><span>we’re starting to figure out RLHF in open-source.</span><br></strong><span>Model info is </span><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha" rel="">here</a><span> and we’re building a recipes repo </span><a href="https://github.com/huggingface/alignment-handbook" rel="">here</a><span>.</span></p></li></ul><p>Reads:</p><ul><li><p><span>A great </span><a href="https://fchollet.substack.com/p/how-i-think-about-llm-prompt-engineering" rel="">read</a><span> from Francois Chollet on links between prompting LLMs, word2vec, and attention. One of the best ML posts I’ve read in a while.</span></p></li><li><p><a href="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/mobilepresent?pli=1&amp;slide=id.g2885e521b53_0_5" rel="">Slides</a><span> from Hyung Won Chung’s (OpenAI) talk on LLMs. Great summary of intuitions for the different parts of training. The key point: </span><em>We can get further with RLHF because the objective function is flexible.</em></p></li></ul><p>Housekeeping:</p><ul><li><p><strong>Interconnects referrals: </strong><span>I’ll give you a free paid sub if you use a referral link you find on the </span><a href="https://www.interconnects.ai/leaderboard" rel="">Interconnects Leaderboard</a><span>. Sharing really helps the blog.</span></p></li><li><p><strong>Student discounts: </strong><span>Want a large paid student discount, go to the </span><a href="https://www.interconnects.ai/about" rel="">About page</a><span>.</span></p></li><li><p><strong>Like this?</strong><span> A comment or like helps Interconnects grow!</span></p></li></ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[First word discovered in unopened Herculaneum scroll by 21yo CS student (190 pts)]]></title>
            <link>https://scrollprize.org/firstletters</link>
            <guid>37857417</guid>
            <pubDate>Thu, 12 Oct 2023 14:11:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://scrollprize.org/firstletters">https://scrollprize.org/firstletters</a>, See on <a href="https://news.ycombinator.com/item?id=37857417">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><div><article><div><p><img loading="lazy" src="https://scrollprize.org/img/firstletters/composite_thumb.png"></p><p>Vesuvius Challenge $700,000 Grand Prize “now definitely achievable”</p><p>October 12th, 2023</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/image_3840-clipped-new.png"><figcaption>This handwriting has been hidden for 2000 years.</figcaption></figure><p><strong>Join our <a href="https://scrollprize.org/livestream">livestream</a> today (Oct 12th) at 4:00pm EDT. Code is on Github: <a href="https://github.com/lukeboi/scroll-first-letters" target="_blank" rel="noopener noreferrer">Luke</a>, <a href="https://github.com/younader/Vesuvius-First-Letters" target="_blank" rel="noopener noreferrer">Youssef</a>.</strong></p><p>The Herculaneum papyri, ancient scrolls housed in the library of a private villa near Pompeii, were buried and carbonized by the eruption of Vesuvius in 79 AD. For almost 2,000 years, this lone surviving library from antiquity was buried underground under 20 meters of volcanic mud. In the 1700s, they were excavated, and while they were in some ways preserved by the eruption, they were so fragile that they would turn to dust if mishandled. How do you read a scroll you can’t open? For hundreds of years, this question went unanswered.</p><p>That is until Luke Farritor, a contestant of the <a href="https://scrollprize.org/">Vesuvius Challenge</a>, became the first person in two millennia to see an entire word from within an unopened scroll this August. For that, we are thrilled to award Luke a $40,000 First Letters Prize, which required contestants to find at least 10 letters in a 4 cm<sup>2</sup> area in a scroll.</p><p>Shortly after that, another contestant, Youssef Nader, <em>independently</em> discovered the same word in the same area, with even clearer results — winning the second place prize of $10,000.</p><p>These breakthroughs were both inspired by contestant Casey Handmer, who was the first person to find substantial, convincing evidence of ink within the unopened scrolls, as explained in his <a href="https://caseyhandmer.wordpress.com/2023/08/05/reading-ancient-scrolls/" target="_blank" rel="noopener noreferrer">blog post</a> and <a href="https://photos.google.com/share/AF1QipPC8Z9P1MxxS6UdoCJWDGofjxSHfBYGgDp47gSOoJW1gtKBGeIeXzAc6AbOVyhsLQ/photo/AF1QipNlEs7d9PS5whLeu2EIdGcxBfgpUb4tvaIi1w5i?key=cHJ2ZDFmVVBfVG8tbV80WUkwREVIVTN1aUYxQXhR" target="_blank" rel="noopener noreferrer">this video</a>. His insights led directly to Luke’s discovery, as well as an improved understanding of the ink signal. We’re awarding him a $10,000 First Ink Prize. Congratulations to Casey, Luke, and Youssef!</p><p>So how did we get here, and how do these models work? Let’s start with a little history.</p><h3 id="educelab-scans">EduceLab scans<a href="#educelab-scans" title="Direct link to heading">​</a></h3><p>Our story starts in 2019, when professor Brent Seales at the University of Kentucky’s EduceLab <a href="https://www.theguardian.com/science/2019/oct/03/ancient-scrolls-charred-by-vesuvius-could-be-read-once-again" target="_blank" rel="noopener noreferrer">imaged Herculaneum scrolls</a> in a particle accelerator, generating 3D CT-scans at resolutions as high as 4 µm.</p><figure><img loading="lazy" src="https://scrollprize.org/img/landing/brent1.webp"><figcaption>Professor Seales and team scanning at the particle accelerator.</figcaption></figure><p>His team also scanned and photographed detached scroll fragments bearing visible ink, thus providing a ground truth dataset.</p><figure><img loading="lazy" src="https://scrollprize.org/img/tutorials/ml-overview-alpha.png"><figcaption>Training a machine learning model on the ground truth data from the detached fragments. From Stephen Parsons’ <a href="https://uknowledge.uky.edu/cs_etds/138/" target="_blank" rel="noopener noreferrer">PhD dissertation</a>.</figcaption></figure><p>Professor Seales’ graduate student, Stephen Parsons, worked on detecting ink from the CT-scans using machine learning models and <a href="https://scrollprize.org/tutorial4">found success with the detached fragments</a>. That success caught the eye of tech entrepreneurs Nat Friedman and Daniel Gross, who started the Vesuvius Challenge to accelerate this progress. They launched an open competition March of 2023, and — alongside a $700,000 Grand Prize — awarded several smaller prizes for the development of open source tools and techniques.</p><p>Early in the summer, a small team of annotators (the “segmentation team”) joined our effort. They began <a href="https://scrollprize.org/tutorial3">mapping the 3D structure of the scroll</a> using tools initially built by EduceLab and improved by our community. By July we had segmented and “virtually flattened” hundreds of cm2 of papyrus.</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/segmentation-chart.webp"><figcaption>Progress of mapping the scrolls, in area (cm²), from the <a href="https://docs.google.com/spreadsheets/d/1zC_5vkqWgb_5z4Q9BYsETF7_3r1BYPccdAnS_GRYOaQ/edit#gid=2051117465" target="_blank" rel="noopener noreferrer">Segment Directory spreadsheet</a>.</figcaption></figure><h3 id="caseys-crackle-pattern">Casey’s crackle pattern<a href="#caseys-crackle-pattern" title="Direct link to heading">​</a></h3><p>In early August, contestant Casey Handmer, an ex-JPL startup founder and polymath, wrote a <a href="https://caseyhandmer.wordpress.com/2023/08/05/reading-ancient-scrolls/" target="_blank" rel="noopener noreferrer">blog post</a> about his discovery of a “crackle pattern” that looks like ink.</p><p>Casey found the pattern by staring at the segmented CT scans for hours on end. This was a major and surprising discovery. Stephen Parsons had seen <a href="https://scrollprize.org/tutorial4">direct evidence of ink</a> in detached fragments before, but not yet in the scrolls.</p><p>Casey was the first person in 2,000 years to find ink — and a letter — inside an unopened scroll.</p><div><figcaption>Left: Ink visible as cracked texture. Right: annotation showing ink location. It could be a “pi” or the bottom of a capital “eta”. From Casey’s <a href="https://caseyhandmer.wordpress.com/2023/08/05/reading-ancient-scrolls/" target="_blank" rel="noopener noreferrer">blog post</a>.</figcaption></div><h3 id="luke-farritors-model">Luke Farritor’s model<a href="#luke-farritors-model" title="Direct link to heading">​</a></h3><p>After this discovery, several contestants looked for more crackle, but it seemed quite rare. Luke Farritor, a college student and SpaceX summer intern working at Starbase, had heard about the Vesuvius Challenge from Dwarkesh Patel’s <a href="https://www.youtube.com/watch?v=qcvMjoJdck4" target="_blank" rel="noopener noreferrer">podcast interview</a> with Nat.</p><p>He saw Casey’s crackle pattern being discussed in the Discord, and began spending his evenings and late nights training a machine learning model on the crackle pattern. With each new crackle found, the model improved, revealing more crackle in the scroll — a cycle of discovery and refinement.</p><p>He found a few dozen ink strokes — and some complete letters — that could be labeled and used as training data.</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/ink-label.png"><figcaption>Left: cracked ink visible against papyrus fiber background. Right: Resulting binary ink label.</figcaption></figure><p>Before long, the model was unveiling traces of crackle invisible to his own eye. Soon, these traces began to form letters and hints of actual words.</p><p>Luke then made a submission to our First Letters Prize, which required contestants to find at least 10 letters in a 4 cm2 area. This was his first submission:</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/luke-first.png"><figcaption>Luke’s first submission, faintly showing the word ΠΟΡΦΥΡΑϹ (porphyras).</figcaption></figure><p>When professor Seales showed this image to our team of papyrologists, scholars specializing in works on papyrus, they gasped: they could immediately read the word “porphyras,” despite the letters being faint.</p><p>After thorough technical review, we sent a newer version of his picture to the panel of papyrologists. Independently and unanimously, they annotated 13 letters, albeit with varying levels of confidence:</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/luke-boxes.png"><figcaption>Each square represents one review. <span>Green</span>: over 80% confidence. <span>Yellow</span>: 50-80% confidence. <span>Red</span>: under 50% confidence.</figcaption></figure><p>Indeed, the word held up to scrutiny. “Porphyras” is an exciting word: it means “purple” and is quite rare in ancient texts.</p><p>One papyrologist noted:</p><p>“The sequence πορφυ̣ρ̣ας̣ may be πορφύ̣ρ̣ας̣ (noun, purple dye or cloths of purple) or πορφυ̣ρ̣ᾶς̣(adjective, purple). Due to the lack of context it is not possible to exclude πορφύ̣ρ̣α ς̣κ[ or πορφυ̣ρ̣ᾶ ς̣κ[.”</p><p>If you’re trying to find these letters in the image, keep in mind that our modern characters look a little bit different. The letters in this ancient script look more like this: ΠΟΡΦΥΡΑϹ. Note that texts from this time didn’t use spaces, making it harder to determine word boundaries.</p><p>Luke’s First Letters Prize submission is available now on <a href="https://github.com/lukeboi/scroll-first-letters" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/luke-reaction.gif"><figcaption>Luke’s reaction when we told him he won the prize a few days ago.</figcaption></figure><h3 id="youssefs-discovery">Youssef’s discovery<a href="#youssefs-discovery" title="Direct link to heading">​</a></h3><p>Meanwhile, another contestant, Youssef Nader, an Egyptian biorobotics grad student in Berlin, pursued a different approach. Motivated by Casey and Luke’s findings, he sifted through the winning entries of the <a href="https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection" target="_blank" rel="noopener noreferrer">Ink Detection prize on Kaggle</a> — which was focused on improving Stephen Parsons’ approach of machine learning in detached fragments. He used a domain transfer technique to adapt these models to the scrolls: <a href="https://github.com/younader/VesuviusPretraining" target="_blank" rel="noopener noreferrer">unsupervised pretraining</a> on the scroll data, followed by fine-tuning on the fragment labels.</p><figure><img loading="lazy" src="https://scrollprize.org/img/data/francoise.png"><figcaption>One of the detached fragments with known ground truth being scanned at the particle accelerator.</figcaption></figure><p>He submitted his idea for an “Ink Detection Followup Prize” and won a small prize. The idea seemed promising, but as far as we knew, that was that. Several weeks later, Youssef made his own submission to the First Letters prize. He had seen Luke’s early results which had been shared on <a href="https://twitter.com/natfriedman/status/1695870954734490003" target="_blank" rel="noopener noreferrer">Twitter</a> and Discord and decided to focus on the same area within the scroll.</p><p>With this modified model from the Kaggle competition, he managed to find some letters, though entirely without relying on Casey’s method of manually looking for crackle. He then annotated what looked like letter shapes to the label data.</p><p>He repeated this pseudo-labeling iteratively, resulting in speculative labels for a number of segments within the scroll. Models trained on these labels were then capable of detecting ink from within the scroll and the training data from the detached scroll fragments were ultimately removed.</p><p>The final models trained solely on internal scroll segments resulted in the image below, securing Youssef the prize.</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/youssef-submission2.png"><figcaption>Youssef’s final submission.</figcaption></figure><p>This time, the papyrologists agreed more strongly about the letters. They even began speculating on possible words above (ανυοντα / ANYONTA, “achieving”) and below (ομοιων / OMOIωN, “similar”).</p><p>If these words are indeed what we think they are, this papyrus scroll likely contains an entirely new text, unseen by the modern world.</p><p>Youssef’s First Letters Prize submission is available now on <a href="https://github.com/younader/Vesuvius-First-Letters" target="_blank" rel="noopener noreferrer">GitHub</a>.</p><h3 id="why-were-we-successful">Why were we successful?<a href="#why-were-we-successful" title="Direct link to heading">​</a></h3><p>There were many contributions from different people in the critical path for these discoveries. Our combination of competition and open source (through “progress prizes”) seems to work! To highlight a few key contributions:</p><ul><li>Youssef used a model from the Kaggle competition and was inspired by Luke’s results to look in the same area.</li><li>Luke’s search for crackle was directly inspired by Casey’s work.</li><li>Casey was able to look through many sheets of papyrus because our segmentation team had mapped out hundreds of cm2.</li><li>The segmentation team was able to map out a lot of papyrus because of tooling built by contestants who worked on “Segmentation Tooling Prizes” (work by Julian Schilliger, Chuck, Yao Hsiao, and many others).</li><li>The segmentation tooling advances were possible because contestants built on top of existing open source tools by professor Seales’ team (work by Seth Parker, Stephen Parsons, and many others). And of course, the contest itself wouldn’t have been possible without the foundation that Dr. Seales and his team, along with their <a href="https://scrollprize.org/#educelab-funders">funders</a>, have laid out and continue to support.</li></ul><p>Looking back at what got us to this point, it seems that almost every single thing we did in running this contest so far has been load-bearing. We’re not quite sure what to make of this! Perhaps that progress is more fragile and success is more contingent than it often seems in retrospect.</p><h3 id="whats-next">What’s next?<a href="#whats-next" title="Direct link to heading">​</a></h3><p>The segmentation team and contestants continue to make progress, and a few days ago Youssef’s model generated a new image of shocking clarity and size:</p><figure><img loading="lazy" src="https://scrollprize.org/img/firstletters/youssef-new.png"><figcaption>Youssef’s <a target="_blank" href="https://scrollprize.org/img/firstletters/youssef-new.png">latest image</a> (<a target="_blank" href="https://scrollprize.org/img/firstletters/composite_fullsize.png">variant</a>), from segments <a href="http://dl.ash2txt.org/hari-seldon-uploads/team-finished-paths/scroll1/20230929220924/" target="_blank" rel="noopener noreferrer">20230929220924</a> and <a href="http://dl.ash2txt.org/hari-seldon-uploads/team-finished-paths/scroll1/20231005123333/" target="_blank" rel="noopener noreferrer">20231005123333</a>.</figcaption></figure><p>In this image you can clearly see four and a half columns of text, separated by margins. Many more letters are now visible, though not all are immediately legible. Our papyrological team is working hard to further investigate this result, and we’ll have updates on this soon.</p><p>These advancements demonstrate that the <a href="https://scrollprize.org/grand_prize">$700,000 Grand Prize</a> is within reach. Our optimism is at an all time high.</p><p>Now is the best time to get involved! Join our vibrant <a href="https://discord.gg/6FgWYNjb4N" target="_blank" rel="noopener noreferrer">Discord community</a>, sign up to receive <a href="https://scrollprize.substack.com/" target="_blank" rel="noopener noreferrer">newsletters via Substack</a>, or follow <a href="https://twitter.com/scrollprize" target="_blank" rel="noopener noreferrer">@scrollprize</a> on X. To get started, download some <a href="https://scrollprize.org/data">data</a>, walk through some of our <a href="https://scrollprize.org/tutorial1">tutorials</a>, and catch up on the progress made by contestants by looking at the <a href="https://scrollprize.org/winners">prize winners</a> and <a href="https://scrollprize.org/community_projects">community tools</a>.</p><p>Will you be the one unlocking the knowledge in hundreds of scrolls — doubling the amount of texts from antiquity — and potentially thousands more that are yet to be excavated, becoming the last hero of the Roman Empire and winning $700,000 while you’re at it?</p><p>The race is on.</p><p><strong>Join our <a href="https://scrollprize.org/livestream">livestream</a> today (Oct 12th) at 4:00pm EDT. Code is on Github: <a href="https://github.com/lukeboi/scroll-first-letters" target="_blank" rel="noopener noreferrer">Luke</a>, <a href="https://github.com/younader/Vesuvius-First-Letters" target="_blank" rel="noopener noreferrer">Youssef</a>.</strong></p></div></article></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atlassian to buy Loom for nearly $1B (202 pts)]]></title>
            <link>https://www.reuters.com/markets/deals/atlassian-agrees-buy-video-messaging-provider-loom-nearly-1-bln-2023-10-12/</link>
            <guid>37856895</guid>
            <pubDate>Thu, 12 Oct 2023 13:33:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/markets/deals/atlassian-agrees-buy-video-messaging-provider-loom-nearly-1-bln-2023-10-12/">https://www.reuters.com/markets/deals/atlassian-agrees-buy-video-messaging-provider-loom-nearly-1-bln-2023-10-12/</a>, See on <a href="https://news.ycombinator.com/item?id=37856895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div confirmation_email_sent_page_url="/confirmationsent/" legal_marketing_page_url="https://legal.thomsonreuters.com/en/products/reuters-legal-news" sign_in_page_url="/account/sign-in/" sign_up_page_url="/account/register/sign-up/" terms_page_url="/info-pages/terms-of-use/"><p data-testid="paragraph-0">Oct 12 (Reuters) - Atlassian <a data-testid="Link" href="https://www.reuters.com/markets/companies/TEAM.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(TEAM.O)</a> said on Thursday it had agreed to acquire privately held video messaging platform Loom for about $975 million, beefing up its team collaboration tools to tap into resilient demand fueled by the adoption of hybrid work.</p><p data-testid="paragraph-1">Integration of Loom's technology into Atlassian software such as collaboration tools Jira and Confluence will help users use video in their workflows.</p><p data-testid="paragraph-2">The acquisition of Loom, which has more than 25 million users globally, will enable customers communicate and collaborate more effectively, Atlassian said.</p><p data-testid="paragraph-3">The company's shares fell 1% in premarket trading.</p><p data-testid="paragraph-4">Loom, which counts Sequoia, Kleiner Perkins and a16z among its investors, makes tools that help users record their screens, camera and microphone to make and share videos.</p><p data-testid="paragraph-5">The total consideration will comprise about $880 million in cash and the rest in shares.</p><p data-testid="paragraph-6">The deal is expected to close in the quarter-ending March 2024 and will be funded with existing cash balances, Atlassian said.</p><p data-testid="paragraph-7">The Australia-based company said it expects the acquisition to dilute operating margins in fiscal years ending June 2024 and 2025.</p><p data-testid="Body">Reporting by Akash Sriram in Bengaluru; Editing by Shailesh Kuber and Sriraj Kalluvila</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA: Capillary Cup (121 pts)]]></title>
            <link>https://www.rit.edu/vignellicenter/product-timecapsule/nasa-capillary-cup</link>
            <guid>37855719</guid>
            <pubDate>Thu, 12 Oct 2023 11:18:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rit.edu/vignellicenter/product-timecapsule/nasa-capillary-cup">https://www.rit.edu/vignellicenter/product-timecapsule/nasa-capillary-cup</a>, See on <a href="https://news.ycombinator.com/item?id=37855719">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>“We take gravity for granted,” Pettit said. “Generally, we are unaware of the weight of our hands, or how easily we pour coffee into a cup. We don’t stop to think, ‘Will the coffee rise up and pour out?’ or ‘Can we pour the coffee?’ We just do it. Gravity handles this for us by applying force to the coffee, pulling downward.”</p>

<p>As an astronaut and someone standing right on the frontier, Pettit always keeps his eyes open for new opportunities and experimentation — or, in his own words, “just goofing around.”<br>
The cup began as a series of cheaply equipped experiments out of Pettit’s curiosity about liquid behavior in space. The project was eventually financed and studied by NASA, with its results published in a number of papers.</p>

<p>“Consider what will happen when the pull of gravity goes away from the cup of coffee,” Pettit said. “From what we know about earthbound coffee drinking, there will be no gravity to pull the coffee downward as we tilt the cup. We tilt the cup, and the coffee stays level. As the cup tilts, the edge of the mug reaches the edge of the coffee and liquid simply pours out. It’s that simple.”</p>

<p>The first prototype, fabricated by Pettit on the International Space Station, is made of repurposed Mylar sheeting and Kapton tape. Applying the principle of capillary channel flow, he formed the cup, pinched into a teardrop shape as the fluid flows along a narrow channel to the rim for drinking.</p>

<p>The prototype was developed into the second version, designed by Mark Weislogel and his team at Portland State University. The cup is made out of 3D-printed food-grade plastic and has been flight-tested by NASA. The use of complex fluid dynamic geometry enhances the hydration experience that is closer to what we’re accustomed to on Earth.</p>

<p>Later, Pettit crafted his own handmade version of the capillary cup based on Weislogel’s design. This porcelain ceramic version is now on the International Space Station and is the first patented product invented in orbit.</p>

<p>It is a prime example of the intersection of design, science, and engineering. It allows crews to drink and toast from an open container commensurate like how we do on Earth.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft reveals IRS notice asking for $28.9B in back taxes (224 pts)]]></title>
            <link>https://www.engadget.com/microsoft-reveals-irs-notice-asking-for-289-billion-in-back-taxes-055326006.html</link>
            <guid>37855213</guid>
            <pubDate>Thu, 12 Oct 2023 09:55:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.engadget.com/microsoft-reveals-irs-notice-asking-for-289-billion-in-back-taxes-055326006.html">https://www.engadget.com/microsoft-reveals-irs-notice-asking-for-289-billion-in-back-taxes-055326006.html</a>, See on <a href="https://news.ycombinator.com/item?id=37855213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Microsoft owes the <a data-i13n="cpos:1;pos:1" href="https://www.engadget.com/irs-accidentally-exposed-confidential-information-134736114.html" data-ylk="slk:Internal Revenue Service;cpos:1;pos:1;elm:context_link;itc:0">Internal Revenue Service</a> (IRS) $28.9 billion in back taxes, not including penalties and interest, at least according to the tax authority. The tech giant has <a data-i13n="elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1" href="https://microsoft.gcs-web.com/node/31951/html" rel="nofollow noopener" target="_blank" data-ylk="slk:revealed in a filing;elm:context_link;elmt:doNotAffiliate;cpos:2;pos:1;itc:0">revealed in a filing</a> with the Securities and Exchange Commission that it received a series of Notices of Proposed Adjustment (NOPAs) from the IRS for the tax years 2004 to 2013. In its filing, it said that it's been working with the IRS for nearly a decade to address the authority's questions about how it distributed its profits among countries and jurisdictions, and this is the agency's decision after a lengthy investigation.</p><p>To be exact, the IRS audit centered around a practice known as "transfer pricing," which legally allowed companies to allocate profits and expenses between their operations in different regions. Microsoft explained that a lot of large multinational corporations practice this cost-sharing scheme to reflect "the global nature of their business." In its case, its subsidiaries shared in the costs of developing some IPs, which means that they're also entitled to the related profits. As <a data-i13n="cpos:3;pos:1" href="https://apnews.com/article/microsoft-taxes-irs-96eb66abe86de19f1108209a8d57431a" rel="nofollow noopener" target="_blank" data-ylk="slk:AP;cpos:3;pos:1;elm:context_link;itc:0"><em>AP</em></a> notes, though, critics of the regulation argue that companies frequently use it to minimize the taxes they have to pay by reporting lower profits in high tax countries, and vice versa.</p><p>Microsoft explained that the issues raised by the IRS are only relevant to those aforementioned years, because it has since changed its corporate structure and practices. Nevertheless, the IRS believes Microsoft owes $28.9 billion in back taxes. The tech giant disagrees, as expected, and said that newer tax laws could reduce the back taxes it owes from this particular audit by $10 billion. Based on its plan of action shared with the SEC, the company intends to contest the decision to the best of its ability: Microsoft said that it will pursue an appeal within the IRS, which typically takes years to complete, and will even "contest any unresolved issues through the courts" if needed.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[47 Anime for 47 Prefectures in Japan (179 pts)]]></title>
            <link>https://www.tokyoweekender.com/art_and_culture/entertainment-art_and_culture/47-anime-locations-47-prefectures-japan/</link>
            <guid>37855057</guid>
            <pubDate>Thu, 12 Oct 2023 09:26:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tokyoweekender.com/art_and_culture/entertainment-art_and_culture/47-anime-locations-47-prefectures-japan/">https://www.tokyoweekender.com/art_and_culture/entertainment-art_and_culture/47-anime-locations-47-prefectures-japan/</a>, See on <a href="https://news.ycombinator.com/item?id=37855057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Japanese animation can be a vehicle to take you all around Japan, from legendary temples and nature spots, to small-town streets and convenience stores. Very often the locations are drawn in so much detail, that they seem like copies of real-life places. Here’s a list of all 47 prefectures in Japan and anime that take place in each prefecture.</p>
<h2 id="651f78b1ef141">Hokkaido</h2>
<p><iframe title="Golden Kamuy (Anime-Trailer)" width="500" height="281" src="https://www.youtube.com/embed/HrHcN08d7dQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>

<h3>Golden Kamuy</h3>
<p>This is the most representative story from Hokkaido, with both the anime and manga showcasing various locations across the island. &nbsp;The story takes place after the Russo-Japanese War (1904-1905) and gives a spotlight to Hokkaido’s indigenous Ainu people, even featuring the Ainu language.</p>
<h2 id="651f78b1ef169">Tohoku</h2>
<p><iframe loading="lazy" title="Flying Witch Official Trailer" width="500" height="281" src="https://www.youtube.com/embed/tlS3BXTq7wQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Aomori</h3>
<p>Aomori is the scene for an anime featuring a young witch and her black cat. It’s not Kiki, but Makoto Kowata and she makes the opposite move — from the big city to the countryside, more specifically from Yokohama to rural Aomori. <a href="https://myanimelist.net/anime/31376/Flying_Witch" target="_blank" rel="noopener"><em>Flying Witch</em></a> is a 2017 anime, showing the budding trend of escaping to nature.</p>
<h3>Akita</h3>
<p>Fans of the Akita breed of dog and the famous Hachiko should watch <em><a href="https://myanimelist.net/anime/589/Ginga_Nagareboshi_Gin" target="_blank" rel="noopener">Silverfang: The Shooting Star Gin</a>. </em>The main character is an Akita puppy who leaves his master to join a pack of wild dogs and takes us across the Akita wilderness.</p>
<h3>Iwate</h3>
<p><a href="https://myanimelist.net/anime/4062/Musashi_no_Ken" target="_blank" rel="noopener"><em>Musashi no Ken (Musashi’s Sword)</em></a> is both a way to see Iwate Prefecture in anime and to look into the world of the martial art, kendo. It shows the peaceful environments of small towns in Iwate and the characters often speak in the local dialect. The anime even manages to weave in the poetry of literary great Kenji Miyazawa, an Iwate native.</p>
<p><iframe loading="lazy" title="Haikyu!! - Official Trailer" width="500" height="281" src="https://www.youtube.com/embed/JOGp2c7-cKc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Miyagi</h3>
<p>The widely popular volleyball anime <a href="https://myanimelist.net/anime/20583/Haikyuu?q=haikyu&amp;cat=anime" target="_blank" rel="noopener"><em>Haikyu!!</em></a> takes place in Miyagi Prefecture, mostly in its capital Sendai. Since it’s a sports-themed anime, you can see depictions of real-life gymnasiums and sports centers in the prefecture, as well as in Tokyo.</p>
<h3>Yamagata</h3>
<p>Excluding theories that the bathhouse in <em>Spirited Away</em> is based on Ginzan Onsen in Yamagata, this prefecture is best viewed through the lens of another, albeit less known, Ghibli film. The main character in <a href="https://myanimelist.net/anime/1029/Omoide_Poroporo" target="_blank" rel="noopener"><em>Only Yesterday</em></a> travels from Tokyo to Yamagata where she falls in love with the place and with a Yamagata local.</p>
<h3>Fukushima</h3>
<p><a href="https://myanimelist.net/anime/32248/Masamune_Datenicle" target="_blank" rel="noopener"><em>Masamune Datenicle</em></a> (a play on “Date” and “chronicle”) is a 2016 anime produced by a studio in Fukushima and aims to promote the prefecture’s history and culture. Date Masamune was a famous Tohoku samurai who still has a lot of fans to this day.</p>
<h2 id="651f78b1ef16e">Chubu</h2>
<p><iframe loading="lazy" title="Laid-Back Camp Season 2 Official Trailer | Anime Clips" width="500" height="281" src="https://www.youtube.com/embed/8iQLfcQXQIs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Yamanashi</h3>
<p><a href="https://myanimelist.net/anime/34798/Yuru_Camp%E2%96%B3" target="_blank" rel="noopener"><em>Laid Back Camp</em></a> takes you to several nature spots in Japan, but many are in Yamanashi Prefecture. Mount Fuji and its volcanic lakes feature prominently.</p>
<h3>Shizuoka</h3>
<p>This prefecture also pops up in <em>Laid Back Camp</em>, but <a href="https://myanimelist.net/anime/37105/Grand_Blue?cat=anime" target="_blank" rel="noopener"><em>Grand Blue</em></a> showcases the marine beauty of the Izu Peninsula. The story follows a character living above his uncle’s diving shop. He eventually enjoys the laid-back lifestyle there, despite initial reluctance.</p>
<h3>Niigata</h3>
<p>The anime <a href="https://myanimelist.net/anime/33003/Mahou_Shoujo_Ikusei_Keikaku" target="_blank" rel="noopener"><em>Magical Girl Raising Project</em></a>, based on the light novels by Asari Endo, takes place in the cryptic N-city. However, with exact depictions of Takada Station, Takada Castle and Takada Park’s Gokurakubashi Bridge (to name a few), we can see it’s based on Joetsu city, the author’s hometown.</p>
<p><iframe loading="lazy" title="Summer Wars - Official Trailer" width="500" height="375" src="https://www.youtube.com/embed/2BB5V6CgDOg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Nagano</h3>
<p>Mamoru Hosoda’s film <a href="https://myanimelist.net/anime/5681/Summer_Wars" target="_blank" rel="noopener"><em>Summer Wars</em></a> takes us to Ueda, a small town in Nagano, where we see Ueda Castle and the lush Nagano countryside.</p>
<h3>Aichi</h3>
<p><a href="https://myanimelist.net/anime/37940/Yatogame-chan_Kansatsu_Nikki" target="_blank" rel="noopener"><em>Yatogame-chan Kansatsu Nikki</em></a> follows a Tokyo high schooler moving to Nagoya. He learns about the new place after meeting the quirky Monaka Yatogame, a fellow student who speaks in the Nagoya dialect.</p>
<h3>Gifu</h3>
<p>Half of <a href="https://myanimelist.net/anime/32281/Kimi_no_Na_wa" target="_blank" rel="noopener"><em>Your Name</em></a>, Makoto Shinkai’s hit anime film, takes place in Tokyo, while the other half happens in the small town of Itomori. Itomori is based on a real-life location in Gifu Prefecture.</p>

<h3>Ishikawa</h3>
<p>Set in a traditional onsen (based on the historic Yuwaku Spa), <a href="https://myanimelist.net/anime/9289/Hanasaku_Iroha" target="_blank" rel="noopener"><em>Hanasaku Iroha</em></a> follows a Tokyo teenager sent to live and work in rural Ishikawa Prefecture.</p>
<h3>Toyama</h3>
<p>Mamoru Hosoda’s film <a href="https://animetourism88.com/en/88AnimeSpot/okamikodomo" target="_blank" rel="noopener"><em>Wolf Children</em></a> takes place in the Toyama countryside. The house where the characters live is a real place dating back 130 years.</p>
<h3>Fukui</h3>
<p>The main characters of <a href="https://myanimelist.net/anime/19257/Megane-bu" target="_blank" rel="noopener"><em>Meganebu!</em></a> are glass-wearing nerds on a quest to make the best glasses. The anime takes place in Fukui, a prefecture known for eyewear production.</p>
<h2 id="651f78b1ef170">Kanto</h2>
<p><img decoding="async" loading="lazy" src="https://www.tokyoweekender.com/wp-content/uploads/2022/03/shutterstock_1342808972-1024x683.jpg" alt="real-life anime locations in tokyo" width="1024" height="683"></p>
<h3>Tokyo</h3>
<p>Tokyo is the setting of so many anime, that it’s impossible to choose one. Check out our article about <a href="https://www.tokyoweekender.com/art_and_culture/entertainment-art_and_culture/movies-tv/anime-locations-in-tokyo/" target="_blank" rel="noopener">10 anime locations in Tokyo</a>, as well as our <a href="https://www.tokyoweekender.com/japan-life/nana-anime-locations-jackson-hole/" target="_blank" rel="noopener"><em>Nana</em> anime article</a> and <a href="https://www.tokyoweekender.com/art_and_culture/entertainment-art_and_culture/bocchi-the-rock-anime-locations-shimokitazawa/" target="_blank" rel="noopener"><em>Bocchi the Rock</em> locations in Shimokitazawa</a>.</p>
<h3>Saitama</h3>
<p>The prefecture’s Chichibu area is the location for the iconic anime <a href="https://myanimelist.net/anime/9989/Ano_Hi_Mita_Hana_no_Namae_wo_Bokutachi_wa_Mada_Shiranai" target="_blank" rel="noopener"><em>Anohana: The Flower We Saw That Day,</em></a> with Chichibu Bridge featuring in promotional imagery.</p>
<h3>Chiba</h3>
<p><em><a href="https://myanimelist.net/anime/14813/Yahari_Ore_no_Seishun_Love_Comedy_wa_Machigatteiru" target="_blank" rel="noopener">Yahari Ore no Seishun Love Comedy wa Machigatteiru</a> (My Teen Romantic Comedy SNAFU)</em> takes place in a high school in Chiba, with outings showing us parks and beaches in Chiba as well as the Chiba Monorail.</p>
<h3>Kanagawa</h3>
<p>The iconic basketball anime<a href="https://myanimelist.net/anime/170/Slam_Dunk" target="_blank" rel="noopener"><em> Slam Dunk</em></a> takes place in Tokyo-adjacent Kanagawa. The featured locations, including the Enoden (Enoshima Electric Railway), are some of the most visited real-life anime locations in Japan.</p>
<h3>Ibaraki</h3>
<p>The seaside town of Oarai is the locale for the <a href="https://myanimelist.net/anime/14131/Girls___Panzer" target="_blank" rel="noopener"><em>Girls &amp; Panzer</em></a> anime. Oarai has embraced this connection with many posters and character cutouts around the town, as well as holding the fictional tank parades from the anime in real life.</p>
<h3>Tochigi</h3>
<p>One part of the film<em>&nbsp;<a href="https://www.imdb.com/title/tt0983213/">5 Centimeters Per Second</a></em> takes place at Iwafune Station in Tochigi Prefecture. It also shows landscapes of the prefecture, such as Mount Iwafune.</p>
<h3>Gunma</h3>
<p><em><a href="https://myanimelist.net/anime/37258/Omae_wa_Mada_Gunma_wo_Shiranai" target="_blank" rel="noopener">You Don’t Know Gunma Yet</a>‘</em>s whole plot is to shatter negative stereotypes about Gunma as we follow a protagonist from Chiba who moves to the prefecture.</p>
<h2 id="651f78b1ef17d">Kansai</h2>
<p><iframe loading="lazy" title="『夜は短し歩けよ乙女』予告編" width="500" height="281" src="https://www.youtube.com/embed/v6vQwCCz_OU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Kyoto</h3>
<p>Just like Tokyo, there are many anime set in Kyoto or visiting Kyoto in some episodes. To pick one example, watch <a href="https://myanimelist.net/anime/34537/Yoru_wa_Mijikashi_Arukeyo_Otome" target="_blank" rel="noopener"><em>The Night is Short, Walk On Girl</em></a>, an anime film that depicts Kyoto by night.</p>
<h3>Nara</h3>
<p>A mundane part of Nara (not the touristy temples and deer) is the setting for <a href="https://myanimelist.net/anime/18153/Kyoukai_no_Kanata" target="_blank" rel="noopener"><em>Beyond the Boundary</em></a>, a supernatural anime that depicts real-life locations.</p>
<h3>Shiga</h3>
<p>An anime that follows a high school band, <a href="https://myanimelist.net/anime/7791/K-On" target="_blank" rel="noopener"><em>K-On!</em></a> takes place in Shiga, near Kyoto. It features real-life schools, concert halls, restaurants and parks from Shiga, as well as some Kyoto spots.</p>
<h3>Hyogo</h3>
<p>Nishinomiya in Hyogo is the location of <a href="https://myanimelist.net/anime/849/Suzumiya_Haruhi_no_Yuuutsu" target="_blank" rel="noopener"><em>The Melancholy of Haruhi Suzumiya</em></a> and it’s the original story’s writer’s hometown too. Many of the cafés, temples and streets in the anime scenes can be found in the town.</p>

<h3>Osaka</h3>
<p>For a taste of Osaka, watch <a href="https://myanimelist.net/anime/2034/Lovely%E2%98%85Complex" target="_blank" rel="noopener"><em>Lovely Complex</em></a>, a romantic comedy that depicts many of the city’s landmarks. The characters in this one also talk in the lovable Kansai dialect.</p>
<h3>Wakayama</h3>
<p><a href="https://myanimelist.net/anime/47194/Summertime_Render" target="_blank" rel="noopener"><em>Summer Time Rendering</em></a> takes place on the fictional Hitogashima Island, which is entirely based on the real Tomogashima Island off the coast of Wakayama Prefecture.</p>
<h3>Mie</h3>
<p><a href="https://myanimelist.net/anime/587/Hanbun_no_Tsuki_ga_Noboru_Sora" target="_blank" rel="noopener"><em>Looking Up at the Half-Moon</em></a> is a short anime series taking place in Mie Prefecture, following teenagers staying in a hospital. The subsequent live-action film was also shot on location in Mie.</p>
<h2 id="651f78b1ef17f">Chugoku</h2>
<p><iframe loading="lazy" title="TOTTORI SAND DUNES CONAN AIRPORT - Hometown of &quot;Detective Conan&quot; Part 1" width="500" height="281" src="https://www.youtube.com/embed/F6w_oj6WuoE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Tottori</h3>
<p>Tottori is a special case as it’s the hometown of Gosho Aoyama, the author of <em>Case Closed</em> (Detective Conan). The long-running anime takes place all over Japan, but you can experience it in real life the most in Tottori. With Conan appearances starting at Tottori Airport, the whole prefecture is a treasure trove for Detective Conan&nbsp;fans.</p>
<h3>Shimane</h3>
<p><a href="https://myanimelist.net/anime/1026/Yakumotatsu" target="_blank" rel="noopener"><em>Yakumotatsu (Eight Clouds Rising)</em></a> is set in Shimane Prefecture and Izumo, the birthplace of Japanese mythology. Its plot delves into myths, superpowers and millennia-old traditions.</p>
<h3>Okayama</h3>
<p>The quirky alien comedy <a href="https://myanimelist.net/anime/696/Tenchi_Muyou" target="_blank" rel="noopener"><em>Tenchi Muyo!</em></a> takes place at Tenchi’s house, which is in the grounds of Masaki Shrine in Okayama Prefecture. Many of the characters’ names also match with Okayama toponyms.</p>
<h3>Hiroshima</h3>
<p><a href="https://myanimelist.net/anime/231/Asagiri_no_Miko" target="_blank" rel="noopener"><em>Shrine of the Morning Mist</em></a>‘s location is based on the city of Miyoshi, Hiroshima, known for its morning fog and old temples.</p>
<h3>Yamaguchi</h3>
<p>Set in the 1950s, <a href="https://myanimelist.net/anime/5084/Mai_Mai_Shinko_to_Sennen_no_Mahou" target="_blank" rel="noopener"><em>Mai Mai Miracle</em></a> is a feature anime film set in Hofu city of Yamaguchi Prefecture. Although the city has changed somewhat since the ’50s, the overall resemblance can still be seen.</p>
<h2 id="651f78b1ef181">Shikoku</h2>
<blockquote>
<p dir="ltr" lang="ja">女の子がやるから「おへんろ。」になるのであって、おっさんがやると水曜どうでしょうになるんですよ。 <a href="https://twitter.com/hashtag/%E3%81%8A%E3%81%B8%E3%82%93%E3%82%8D?src=hash&amp;ref_src=twsrc%5Etfw">#おへんろ</a> <a href="https://t.co/T3ogIeeyzZ">pic.twitter.com/T3ogIeeyzZ</a></p>
<p>— リジス (@lidges) <a href="https://twitter.com/lidges/status/1055445998531600384?ref_src=twsrc%5Etfw">October 25, 2018</a></p></blockquote>

<h3>Tokushima</h3>
<p><a href="https://japan-programcatalog.com/en/program/ohenro" target="_blank" rel="noopener"><em>Ohenro</em></a> takes viewers around the famous <a href="https://www.tokyoweekender.com/travel/shikoku-tales-travelers-tackling-shikoku-pilgrimage-will-stories-share-three-historic-sites-tokushima-prefecture/" target="_blank" rel="noopener">88-temple pilgrimage</a> on the island of Shikoku. This anime brings us to several prefectures in Shikoku, but with a strong focus on Tokushima. The story also began as a column in a Tokushima newspaper.</p>
<h3>Kagawa</h3>
<p>The prefecture with the best udon is naturally the setting for the <a href="https://myanimelist.net/anime/32673/Udon_no_Kuni_no_Kiniro_Kemari" target="_blank" rel="noopener"><em>Poco’s Udon World</em></a> anime, particularly in Takamatsu and Shodo Island. It shows real-life udon restaurants, shopping streets and nature spots.</p>
<h3>Ehime</h3>
<p>Though the island where <a href="https://myanimelist.net/anime/98/Mai-HiME" target="_blank" rel="noopener"><em>Mai-HiME</em></a> takes place is not named or specified, many believe it to be in Ehime Prefecture.</p>
<h3>Kochi</h3>
<p><a href="https://myanimelist.net/anime/743/Umi_ga_Kikoeru" target="_blank" rel="noopener"><em>Ocean Waves</em></a> is a high school romance that takes place in Kochi Prefecture and, as the title suggests, it takes us to the seaside and small towns.</p>
<h2 id="651f78b1ef182">Kyushu</h2>
<p><iframe loading="lazy" title="Hakata Tonkotsu Ramens - Trailer" width="500" height="281" src="https://www.youtube.com/embed/DtnJHPtVzqo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3>Fukuoka</h3>
<p>The city known for <a href="https://www.tokyoweekender.com/travel/ichiran-tonkotsu-ramen/" target="_blank" rel="noopener">tonkotsu ramen</a> and riverside stalls is best enjoyed in the <a href="https://myanimelist.net/anime/35889/Hakata_Tonkotsu_Ramens"><em>Hakata Tonkotsu Ramens</em></a> anime that shows us Fukuoka nightlife and a fictional criminal underbelly.</p>
<h3>Saga</h3>
<p>The characters in <a href="https://myanimelist.net/anime/32995/Yuri_on_Ice" target="_blank" rel="noopener"><em>Yuri!! On Ice</em></a> figure skate their way to Barcelona, but the main plot takes place in Saga Prefecture, particularly the castle town of Karatsu. Karatsu has embraced this connection and displays the characters around town.</p>
<h3>Nagasaki</h3>
<p><a href="https://myanimelist.net/anime/22789/Barakamon" target="_blank" rel="noopener"><em>Barakamon</em></a> follows a Tokyo calligrapher sent to the <a href="https://www.tokyoweekender.com/travel/people-churches-goto-islands-hidden-christians-japan/" target="_blank" rel="noopener">Goto Islands</a> in Nagasaki Prefecture. Locations include Fukue Airport, beaches and temples.</p>
<h3>Oita</h3>
<p><a href="https://myanimelist.net/anime/2422/Kenritsu_Chikyuu_Boueigun" target="_blank" rel="noopener"><em>Prefectural Earth Defense Force</em></a> takes place in Kyushu, and though the prefecture is not specified, people firmly believe it’s Oita.</p>
<p>Bonus: the over-the-top comical <a href="https://myanimelist.net/anime/3702/Detroit_Metal_City" target="_blank" rel="noopener"><em>Detroit Metal City</em></a> takes place in Tokyo, but the protagonist is from Inukai, Oita Prefecture.</p>
<h3>Kumamoto</h3>
<p><a href="https://myanimelist.net/anime/4081/Natsume_Yuujinchou" target="_blank" rel="noopener"><em>Natsume’s Book of Friends</em></a> acquaints us with the supernatural folklore of yokai spirits and takes place in the Hitoyoshi Kuma region of Kumamoto Prefecture. Due to its theme, it shows many old revered temples and shrines, in addition to natural landscapes.</p>
<h3>Miyazaki</h3>
<p>Also known for traditional spirituality, Miyazaki Prefecture is the setting for <a href="https://myanimelist.net/anime/29854/Ushio_to_Tora_TV" target="_blank" rel="noopener"><em>Ushio and</em>&nbsp;Tora,</a> an anime that takes place in a temple and deals with the supernatural.</p>
<h3>Kagoshima</h3>
<p>Tanegashima Island in Kagoshima Prefecture is the setting of the <a href="https://myanimelist.net/anime/13599/Robotics_Notes" target="_blank" rel="noopener"><em>Robotics;Notes</em></a> anime. Tanegashima Space Center, the main rocket launch facility for JAXA, features heavily because the main plot centers around the creation of a giant mecha robot.</p>
<h2 id="651f78b1ef184">Okinawa</h2>
<p>It seems that every other anime has a filler episode including a trip to Okinawa. There are, however, anime set entirely in Okinawa, including <em><a href="https://myanimelist.net/anime/15043/Haitai_Nanafa">Haitai Nanafa</a>, </em>which features a local soba shop and two sisters helping their grandmother run it. As well as Okinawan food, you can also see beautiful architecture and landscapes.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Email and Git = <3 (238 pts)]]></title>
            <link>https://git-send-email.io/</link>
            <guid>37854995</guid>
            <pubDate>Thu, 12 Oct 2023 09:16:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git-send-email.io/">https://git-send-email.io/</a>, See on <a href="https://news.ycombinator.com/item?id=37854995">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
      <p>
        <a target="_blank" rel="noopener" href="https://git-scm.com/">Git</a>
        ships with built-in tools for collaborating over email. With this
        guide, you'll be contributing to email-driven projects like the Linux
        kernel, PostgreSQL, or even git itself in no time.
      </p>
      <div id="step-2">
        <h2>
          <small>Step two</small> Configuration
        </h2>
        <div>
          <p>
            You only need to complete this step once for each machine you
            intend to send emails from.
          </p>
          <p>
            Use <code>git config --global --edit</code> to open your <strong>global
            configuration file</strong> in your default editor.
          </p>
          <div>
            <div>
              <p><label for="mailer-gmail">Gmail</label></p><div>
                <p>
                  There are two possible ways of using Gmail with
                  <code>git send-email</code>:
                </p>
                <ol>
                  <li>
                    First, make sure that <a href="https://support.google.com/accounts/answer/185839" target="_blank">two-factor authentication</a> is enabled for your Google
                    account. Then, obtain an application-specific password
                    for git from the <a href="https://security.google.com/settings/security/apppasswords" target="_blank">app passwords</a> page. To store this password with git,
                    run this command:
                    <pre>git config --global sendemail.smtpPass 'your password'</pre>
                    Next step is to add these details to your global
                    configuration file:
                    <pre>[sendemail]
       smtpserver = smtp.gmail.com
       smtpuser = you@gmail.com
       smtpencryption = tls
       smtpserverport = 587</pre>
                    Be sure to fill in your own email address under
                    <code>smtpuser</code>.
                  </li>
                  <li>
                    It is still possible to use <code>git send-email</code>
                    without two-factor authentication and application-specific
                    password, but this requires installation and configuration
                    of <a href="https://github.com/google/gmail-oauth2-tools/tree/master/go/sendgmail" target="_blank">the special tool</a>, which mimics regular
                    <code>sendmail</code>.
                  </li>
                </ol>
                <p>
                  Also, if you haven't yet, run these
                  commands - again, making the appropriate changes:
                </p>
                <pre>git config --global user.email "you@gmail.com"
<!--              -->git config --global user.name "Your Name"</pre>
                <p><a href="#step-3">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="mailer-protonmail">Protonmail</label></p><div>
                <p>
                  Protonmail does not support the open, industry-standard
                  protocols necessary for git send-email to work
                  out-of-the-box. To solve this, Protonmail offers
                  <a href="https://protonmail.com/bridge/">Protonmail Bridge</a>.
                  Otherwise, you  need to install &amp; configure
                  <a href="https://github.com/emersion/hydroxide">Hydroxide</a>
                  to connect to Protonmail with SMTP. Once you have configured
                  Protonmail Bridge or Hydroxide, use its SMTP details along
                  with the generic instructions below.
                </p>
                <p>
                  <strong>Protonmail bridge users:</strong> the bridge uses SSL
                  authentication, but it self-signs the certificate used for 
                  SSL, so you will need to disable SSL verification with the 
                  daemon in your git config by setting an empty value for 
                  sendemail.smtpsslcertpath.
                </p>
                <p>
                  Be advised that Protonmail is generally known to be a pretty
                  bad email host. They will munge up your outgoing emails and
                  your patches may fail to apply when received by the other end.
                  Not to mention their mistreatment of open source and false
                  promises of security! You should consider a different mail
                  provider.
                </p>
                <p><a href="#step-3">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="mailer-fastmail">Fastmail</label></p><div>
              <p>
                Visit <a href="https://www.fastmail.help/hc/en-us/articles/360058752854">the
                Fastmail instructions for adding a new third-party app</a>.
                Add the following details, making the appropriate changes,
                including the "app password" that was generated from following
                those Fastmail instructions:
              </p>
              <pre>[sendemail]
    smtpserver = smtp.fastmail.com
    smtpuser = your-username@fastmail.whatev
    smtpencryption = ssl
    smtpserverport = 465
    smtppass = whatwasautogenerated</pre>
              <p>
              	Also, if you haven't yet, run these commands - again making the
                appropriate changes:
              </p>
              <pre>git config --global user.email "you@example.org"
<!--            -->git config --global user.name "Your Name"</pre>
              <p>
                <a href="#step-3">Next</a>
              </p>
              </div>
            </div>
            <div>
              <p><label for="mailer-local">Local SMTP client (msmtp, nbSMTP, sendmail)</label></p><div>
                <p>
                  If you already have an SMTP client installed, you can configure
                  git send-email to use it by setting <code>sendemail.smtpserver</code>
                  to the path of its executable. For example, to use msmtp you
                  could add this to your global configuration file:
                  </p><pre>[sendemail]
    smtpserver = /usr/bin/msmtp</pre>
                
                <p>
                  If you have multiple accounts configured for your SMTP client and want to specify one, you can use the
                  <code>sendemail.smtpserveroption</code> setting. For example, to choose an account named
                  <code>work</code> configured for msmtp:
                  </p><pre>[sendemail]
    smtpserver = /usr/bin/msmtp
    smtpserveroption = -a
    smtpserveroption = work</pre>
                
                <p><a href="#step-3">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="mailer-generic">Generic instructions</label></p><div>
                <p>
                  Your email provider should have instructions somewhere for
                  <strong>SMTP access</strong>. Look these details up, and then
                  add the following details from your mail provider:
                </p>
                <pre>[sendemail]
	smtpserver = mail.example.org
	smtpuser = you@example.org
	smtpencryption = ssl
	smtpserverport = 465</pre>
                <p>
                  Be sure to fill in the appropriate values for your email
                  provider - you will probably only have to fill in
                  <code>smtpserver</code> and <code>smtpuser</code>. Also, if
                  you haven't yet, run these commands - again, making the
                  appropriate changes:
                </p>
                <pre>git config --global user.email "you@example.org"
<!--              -->git config --global user.name "Your Name"</pre>
                <p><a href="#step-3">Next</a></p><p>
                  This configuration assumes direct TLS. If your provider only
                  offers STARTTLS, set <code>smtpencryption = tls</code> and
                  <code>smtpserverport = 587</code>.
                </p>
                <p>
                  If your system is already set up with <code>sendmail</code>,
                  you may skip the <code>[sendemail]</code> git configuration instructions.
                </p>
              </div>
            </div>
          </div>
        </div>
        </div>
      <div id="step-3">
        <h2>
          <small>Step three</small> Give it a shot!
        </h2>
        <div>
          <p>
            It's time to take it for a spin - we're going to send a patch.
            Ready?
          </p>
          <div>
            <ol>
              <li>
                <strong>Clone the upstream repository</strong>. No need to make a
                fork! We have prepared a repository for you to test with:
                <pre>git clone <a href="https://git.sr.ht/~sircmpwn/email-test-drive">https://git.sr.ht/~sircmpwn/email-test-drive</a>
<!--              -->cd email-test-drive</pre>
              </li>
              <li>
                <strong>Make your changes</strong>. Let's add a file with your
                progress so far:
                <pre>echo "I'm about to try git send-email" &gt;your-name</pre>
                <p>
                  Be sure to change <code>your-name</code> to your own!
                </p>
              </li>
              <li>
                <strong>Commit your changes</strong>. Check out the official
                (and free) <a href="https://git-scm.com/book/en/v2" target="_blank" rel="noopener">Pro Git</a> book if you don't know how to do this.
                <pre>git add your-name
<!--              -->git commit -m "Demonstrate that I can use git send-email"</pre>
              </li>
              <li>
                <strong>Send the patch</strong>! If you check out the
                <code>README.md</code> file, you'll note that patches should be
                sent to <a href="mailto:~sircmpwn/email-test-drive@lists.sr.ht">
                  ~sircmpwn/email-test-drive@lists.sr.ht</a>.
                <pre>git send-email --to="~sircmpwn/email-test-drive@lists.sr.ht" HEAD^</pre>
                <p>
                  If prompted for an <code>In-Reply-To</code>, you can ignore
                  it for now (just press enter). Follow the rest of the prompts
                  and you've done it! If you have any problems at this step,
                  feel free to
                  <a href="mailto:~sircmpwn/sr.ht-discuss@lists.sr.ht?subject=git-send-email%20help">
                    email us
                  </a> for help. If it worked, you should see your email appear
                  in the <a target="_blank" rel="noopener" href="https://lists.sr.ht/~sircmpwn/email-test-drive">mailing list archives</a> momentarily!
                </p>
              </li>
              <li>
                <strong>Check your inbox</strong>... someone has some feedback
                on your patch!
              </li>
            </ol>
            <p><a href="#step-4">Next</a>
          </p></div>
          <p><strong>Warning!</strong> Some people think that they can get
            away with sending patches through some means other than <code>
            git send-email</code>, but you can't. Your patches will be broken
            and a nuisance to the maintainers whose inbox they land in.
            Follow the golden rule: <em>just use git send-email</em>.
          </p>
        </div>
        </div>
      <div id="step-4">
        <h2>
          <small>Step four</small> Dealing with feedback
        </h2>
        <div>
            <p>
              No one ever gets it right on the first try. Don't worry, it's all
              part of the process! You may receive some feedback on your patch
              from the maintainers of the software. This feedback will arrive in
              the form of a reply to your email. If you have any questions,
              just reply back - and remember to "reply all"! In the meantime,
              let's fix the patch.
            </p>
            <p>
              In this tutorial, the feedback was generated by a bot, but feel
              free to reply to get the hang of it. You need to make sure your
              email client is configured to write emails in <strong>plain
              text</strong> before you do, and please try to avoid
              <a href="https://git-send-email.io/top-posting.html" target="_blank">top posting</a>.
              We also have an
              <a target="_blank" href="https://man.sr.ht/lists.sr.ht/etiquette.md">etiquette guide</a> that'll help you avoid any faux pas.
            </p>
            <ol>
              <li>
                <strong>Make the changes</strong>. Update the files to match the
                changes requested by the maintainers. We'll leave this to you.
              </li>
              <li>
                <strong>Amend your commit</strong>. Git is designed for you to
                <em>edit</em> your commit history. It's not set in stone! The
                maintainers reviewing your work don't want to merge a patch
                which has mistakes, even if it's followed up by a fix, so
                you'll have to <strong>amend</strong> your previous commit:
                <pre>git commit -a --amend</pre>
                <p>
                  First time amending a commit? Amending and rebasing commits
                  is an essential skill in this workflow. Check out our
                  <a href="https://git-rebase.io/" target="_blank">comprehensive guide to git rebase</a> if you'd like to learn
                  more.
                </p>
              </li>
              <li>
                <strong>Set the default "to" address</strong>. Let's make this
                easier on ourselves by setting the default email address for
                this repo, so we needn't enter it every time:
                <pre>git config sendemail.to "~sircmpwn/email-test-drive@lists.sr.ht"</pre>
              </li>
              <li>
                <strong>Send the new patch</strong>! This time we'll use
                <code>-v2</code> to indicate that this is the second version of
                this patch. If we do this again, we'll use <code>-v3</code>.
                <pre>git send-email --annotate -v2 HEAD^</pre>
                <p>
                  Note that we also specified the "--annotate" flag. This is
                  going to open the email in our editor before sending it out,
                  so we can make any changes. We're going to add some "timely
                  commentary". Look for the "---" and add a short summary of
                  the differences since the first patch on the next line. It
                  should look something like this:
                </p>
                <pre>Subject: [PATCH v2] Demonstrate that I can use git send-email

<!--              -->---
<!--              -->This fixes the issues raised from the first patch.

<!--              -->your-name | 1 +
<!--              -->1 file changed, 1 insertion(+)</pre>
								<p>
                  This text gives the maintainers some extra context about your
                  patch, but doesn't make it into the final git log. Close your
                  editor, follow the prompts again, and that's it - you're done!
									Congratulations!
                </p>
                <p>
                  If you want some more tips on using git send-email, check out
                  the next page.
								</p>
              </li>
            </ol>
						<p><a href="#step-5">Next</a>
					</p></div>
        </div>
      <div id="step-5">
        <!-- TODO: Move step 5 onto a separate page so each tip is linkable? -->
        <h2>
          Tips &amp; tricks
        </h2>
        <div>
          <p>
            Miscellaneous tips and tricks which may serve you well as you use
            git send-email.
          </p>
          <div>
            <h3>Sending several patches at once</h3>
            <p>Use this to send the last 3 commits:</p>
            <pre>git send-email HEAD~3</pre>
            <p>Or all commits since a particular one:</p>
            <pre>git send-email 209210d</pre>
            <p>Or just the second-to-last commit:</p>
            <pre>git send-email -1 HEAD^^</pre>
            <p>
              See <a href="https://git-scm.com/book/en/v2/Git-Tools-Revision-Selection" target="_blank">Revision Selection</a> for more.
            </p>
          </div>
          <div>
            <h3>Specifying a sub-project</h3>
            <p>
              Some projects use a single mailing list for several git
              repositories. Try this to clarify that you're working on the
              "foobar" project:
            </p>
            <pre>git config format.subjectPrefix "PATCH foobar"</pre>
          </div>
          <div>
            <h3>Using --annotate every time</h3>
            <pre>git config --global sendemail.annotate yes</pre>
          </div>
          <div>
            <h3>"Signing off" on your commits</h3>
            <p>
              Some projects, such as the Linux kernel, will ask you to "sign
              off" on your commits. To do this, add <code>--signoff</code> (or
              <code>-s</code>) to <code>git send-email</code>. To set it as the
              default for that git repository:
            </p>
            <pre>git config format.signOff yes</pre>
            <p>
              The meaning of a signoff depends on the project to which you’re
              committing. For example, it may certify that the committer has
              the rights to submit the work under the project’s license or
              agrees to a
              <a href="https://developercertificate.org/" target="_blank" rel="noopener">Developer Certificate of Origin (DCO)</a>.
              Consult the documentation or leadership of the project to which
              you’re contributing to understand how the signoffs are used in
              that project.
            </p>
          </div>
          <div>
            <h3>More approaches to authentication</h3>
            <p>
              This tutorial configures git in a way that causes send-email to
              prompt for your password, which you may find annoying. There are
              other ways to authenticate - the simplest of which is:
            </p>
            <pre>git config --global sendemail.smtpPass 'your password'</pre>
	    <p>
	      You can also have your password cached in memory for a certain
	      period of time. To cache it for one hour, use:
	    </p>
	    <pre>git config --global credential.helper 'cache --timeout 3600'</pre>
            <p>
              For more sophisticated solutions, such as integration with your
              keyring, see the <a href="https://git-scm.com/docs/gitcredentials">git-credential</a> man page.
            </p>
          </div>
          <p><a href="#step-1">Back to the start</a>
        </p></div>
        </div>
      <div id="step-1">
        <h2>
          <small>Step one</small> Installation
        </h2>
        <div>
          <p>
            Let's start by installing the appropriate packages for your
            operating system.
          </p>
          <div>
            <div>
              <p><label for="os-arch">
                <img src="https://git-send-email.io/static/arch.png">
                Arch Linux
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email tools,
                  though you might need to install a few additional packages to
                  get it fully working.
                  Run this to install it:
                </p>
                <pre>sudo pacman -Syu --needed git perl-authen-sasl perl-io-socket-ssl</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-alpine">
                <img src="https://git-send-email.io/static/alpine.png">
                Alpine Linux
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo apk add git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-centos">
                <img src="https://git-send-email.io/static/centos.png">
                CentOS
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo yum install git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-debian">
                <img src="https://git-send-email.io/static/debian.png">
                Debian
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo apt install git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-fedora">
                <img src="https://git-send-email.io/static/fedora.png">
                Fedora
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo dnf install git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-freebsd">
                <img src="https://git-send-email.io/static/freebsd.png">
                FreeBSD
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email tools.
                  Run this to install it:
                </p>
                <pre>pkg install git</pre>
                <p>
                  Or, to install from ports:
                </p>
                <pre>cd /usr/ports/devel/git &amp;&amp; make install clean</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-gentoo">
                <img src="https://git-send-email.io/static/gentoo.png">
                Gentoo
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email tools.
                  Run this to install it:
                </p>
                <pre>emerge dev-vcs/git</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-guix">
                <img src="https://git-send-email.io/static/guix.png">
                Guix
              </label></p><div>
                <p>
                  The <code>git</code>
                  package's <code>send-email</code> output contains
                  the git email tools. Run this to install both git
                  and the email tools:
                </p>
                <pre>guix package -i git git:send-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-macos">
                <img src="https://git-send-email.io/static/apple.png">
                macOS
              </label></p><div>
                <p>
                  All of the usual ways to install <code>git</code> on macOS
                  include the git email tools by default, including Apple's
                  developer tools as well as
                  <a href="https://brew.sh/">Homebrew</a>,
                  <a href="https://www.macports.org/install.php">MacPorts</a>,
                  and the official git bundle from
                  <a href="https://git-scm.com/download/mac">git-scm.com</a>.
                </p>
                <p>
                  <br>
                  The easiest way is to just run:
                </p>
                <pre>git</pre>
                <p>
                  If <code>git</code> is not installed already, you will be
                  prompted to install the Apple command line developer tools.
                </p>
                <p>
                  It may be necessary to install missing perl SSL modules by
                  hand:
                  </p><pre>sudo -H cpan Net::SMTP::SSL IO::Socket::SSL</pre>
                
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-nixos">
                <img src="https://git-send-email.io/static/nixos.png">
                Nix
              </label></p><div>
                <p>
                  The
                  <a href="https://search.nixos.org/packages?type=packages&amp;query=gitFull"><code>gitFull</code></a>
                  package includes the git email tools.
                  To install it imperatively, run:
                </p>
                <pre>nix-env -iA nixos.gitFull</pre>
                <p>
                  Alternatively, to install it declaratively, add:
                </p>
                <pre>programs.git.package = pkgs.gitFull;</pre>
                <p>
                  to <code>configuration.nix</code>, or
                  <code>home.nix</code> if you use home-manager.
                  Make sure <code>programs.git.enable</code> is enabled.
                </p>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-omnios">
                <img src="https://git-send-email.io/static/omnios.png">
                OmniOS
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email tools.
                  Run this to install it:
                </p>
                <pre>pkg install git</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-openbsd">
                <img src="https://git-send-email.io/static/openbsd.png">
                OpenBSD
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email tools.
                  Run this to install it:
                </p>
                <pre>pkg_add git p5-Authen-SASL p5-Net-SMTP-SSL</pre>
                <p>
                  Or, to install from ports:
                </p>
                <pre>cd /usr/ports/devel/git &amp;&amp; make install clean &amp;&amp; cd ../../security/p5-Authen-SASL &amp;&amp; make install clean &amp;&amp; cd ../../net/p5-Net-SMTP-SSL &amp;&amp; make install clean
                </pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-openmandriva">
                <img src="https://git-send-email.io/static/openmandriva.png">
                OpenMandriva
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo dnf install git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-opensuse">
                <img src="https://git-send-email.io/static/suse.png">
                openSUSE
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo zypper install git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-pkgsrc">
                <img src="https://git-send-email.io/static/pkgsrc.png">
                pkgsrc
              </label></p><div>
                <p>
                  The <code>devel/git</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo pkg_add git</pre>
                <p>
                  Or, to build from source:
                </p>
                <pre>cd /usr/pkgsrc/devel/git &amp;&amp; make install </pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-rocky">
                <img src="https://git-send-email.io/static/rocky.png">
                Rocky Linux
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo dnf install git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-solus">
                <img src="https://git-send-email.io/static/solus.png">
                Solus
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo eopkg install git</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-ubuntu">
                <img src="https://git-send-email.io/static/ubuntu.png">
                Ubuntu
              </label></p><div>
                <p>
                  The <code>git-email</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo apt install git git-email</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-void">
                <img src="https://git-send-email.io/static/void.png">
                Void Linux
              </label></p><div>
                <p>
                  The <code>git</code> package includes the git email
                  tools. Run this to install it:
                </p>
                <pre>sudo xbps-install -S git</pre>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
            <div>
              <p><label for="os-windows">
                <img src="https://git-send-email.io/static/windows.png">
                Microsoft Windows
              </label></p><div>
                <p>
                  The official git bundle from
                    <a href="https://git-scm.com/download/windows">git-scm.com</a>
                  includes the git email tools.
                </p>
                <p><a href="#step-2">Next</a>
              </p></div>
            </div>
          </div>
          <p>
            Don't see your OS here? Consult your OS documentation for
            installation, and once you finish this tutorial, send a patch
            adding yours to our
            <a href="https://git.sr.ht/~sircmpwn/git-send-email.io" target="_blank">git repository</a>.
          </p>
        </div>
        </div>
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Introduction to Modern Statistics (475 pts)]]></title>
            <link>https://openintro-ims2.netlify.app/</link>
            <guid>37854846</guid>
            <pubDate>Thu, 12 Oct 2023 08:45:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openintro-ims2.netlify.app/">https://openintro-ims2.netlify.app/</a>, See on <a href="https://news.ycombinator.com/item?id=37854846">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-document-content"><p><a href="https://openintro-ims2.netlify.app/images/IMS2_front_cover_WIP.png" data-gallery="quarto-lightbox-gallery-1"><img src="https://openintro-ims2.netlify.app/images/IMS2_front_cover_WIP.png" title="Introduction to Modern Statistics (2nd Ed)"></a></p>
<section id="welcome-to-ims2"><h2>Welcome to IMS2</h2>
<p>This is the website for <strong>Introduction to Modern Statistics</strong>, Second Edition by Mine Çetinkaya-Rundel and Johanna Hardin. Introduction to Modern Statistics, which we’ll refer to as IMS going forward, is a textbook from the <a href="https://www.openintro.org/">OpenIntro</a> project.</p>
<!--
The book will always be available for free here. It is also available in PDF (for free or for the amount you choose to contribute) on Leanpub and in black&white paperback for purchase for $20.
<br><br>
<a href="https://leanpub.com/imstat" target="_blank"><img src="images/_icons/file.png" width="30px">&nbsp;<strong>Download PDF</strong></a>
<br>
<a href="https://www.openintro.org/go?id=ims1_bw_pb&referrer=/book/ims/online"><img src="images/_icons/book.png" width="30px">&nbsp;<strong>Purchase paperback</strong></a>
<br><br>
---
<br><br>
-->
<p>Copyright © 2023.</p>
<p>Second Edition.</p>
<p>Version date: September 10, 2023.</p>
<p>This textbook and its supplements, including slides, labs, and interactive tutorials, may be downloaded for free at<br><a href="http://openintro.org/book/ims"><strong>openintro.org/book/ims</strong></a>.</p>
<p>This textbook is a derivative of <em>OpenIntro Statistics</em> 4th Edition and <em>Introduction to Statistics with Randomization and Simulation</em> 1st Edition by Diez, Barr, and Çetinkaya-Rundel, and it’s available under a Creative Commons Attribution-ShareAlike 3.0 Unported United States License. License details are available at the Creative Commons website:<br><a href="https://www.openintro.org/go/?id=creativecommons_org&amp;referrer=ims1_pdf"><strong>creativecommons.org</strong></a>.</p>
<p>Source files for this book can be found on GitHub at<br><a href="https://github.com/OpenIntroStat/ims">github.com/OpenIntroStat/ims</a>.</p>
</section>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bentham's Mugging (2022) (101 pts)]]></title>
            <link>https://www.cambridge.org/core/journals/utilitas/article/benthams-mugging/9C67002F344B20661A6C35C960F25A86</link>
            <guid>37854358</guid>
            <pubDate>Thu, 12 Oct 2023 07:16:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cambridge.org/core/journals/utilitas/article/benthams-mugging/9C67002F344B20661A6C35C960F25A86">https://www.cambridge.org/core/journals/utilitas/article/benthams-mugging/9C67002F344B20661A6C35C960F25A86</a>, See on <a href="https://news.ycombinator.com/item?id=37854358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p> <span>Mugger.</span> Excuse me a moment, would you sir? I'm a bit short on cash.</p>
<p> <span>Bentham.</span> Sorry.</p>
<p> <span>Mugger.</span> <em>(notices a utilitarian pin on</em> Bentham<em>'s lapel)</em> But you're a utilitarian, right?</p>
<p> <span>Bentham.</span> Indeed, I am an <em>Act Utilitarian</em>: I believe I ought to perform an act if that act would produce more utility than any alternative act.<a href="#fn1"><span>Footnote </span><sup>1</sup></a></p>
<p> <span>Mugger.</span> That's grand. How about you give me ten pounds?</p>
<p> <span>Bentham.</span> Now, as an Act Utilitarian, I would happily part with ten pounds <em>if</em> I were convinced that you would bring more utility to the world with that money than I would. The trouble is I know I would put the money to good use myself – whereas you, I surmise, would not.</p>
<p> <span>Mugger.</span> Fine. I suspected as much. But what if I sweeten the deal? If you don't give me the money, I'll cut off a finger!</p>
<p> <span>Bentham.</span> You're threatening me?!</p>
<p> <span>Mugger.</span> Wait, no. I'm not threatening <em>you</em>. That would be illegal. I'm saying that, if you don't give me the money, I'll cut off <em>my</em> finger.</p>
<p> <span>Bentham.</span> Why on earth would you do that?</p>
<p> <span>Mugger.</span> I am a <em>Deontologist</em>. I am true to my word.</p>
<p> <span>Bentham.</span> <em>(notices a deontological pin on</em> Mugger<em>'s lapel)</em> I see. But then, if you don't mind my asking, why did you promise to cut off your finger?</p>
<div><p> <span>Mugger.</span> Look, what happened happened. Let's cut to the chase. I have diagrammed our situation:</p>
<p> <em>(unfolds a large poster)</em></p>
<p> <img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline1.png?pub-status=live" width="930" height="330" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline1.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline1.png"></p></div>
<p> <span>Bentham.</span> You had that diagram ready all along?</p>
<p> <span>Mugger.</span> You, at the first node, have a choice between giving me ten pounds (going up) or keeping the money (going down). If you keep the money, I have a choice at the second node whether or not to cut off my finger. The thick line denotes that I would, in fact, do so.</p>
<p> <span>Bentham.</span> <em>(nods)</em></p>
<p> <span>Mugger.</span> Here's the thing: there is, clearly, more utility in me keeping my finger than in you keeping your measly ten pounds. So there would be <em>more</em> utility in the world if you gave me the money than if you didn't.</p>
<p> <span>Bentham.</span> I think you should just keep your finger.</p>
<p> <span>Mugger.</span> And go back on my word? No. (<em>chuckle</em>) What if everyone did that? Besides, what should matter to you, as an Act Utilitarian, is that I <em>would</em> cut off my finger – not whether I should.</p>
<p> <span>Bentham.</span> Fair enough. But, even so, I worry that giving you the money would set a bad precedent, encouraging copycats to run similar schemes.</p>
<p> <span>Mugger.</span> Don't. This transaction will be our little secret. You have my word.</p>
<p> <span>Bentham.</span> <em>(not entirely convinced)</em></p>
<p> <span>Mugger.</span> You're playing hardball? <em>(sigh)</em> All right, let's make the deal sweeter still: If I don't get the money, I'll cut off <em>two</em> fingers.</p>
<p> <span>Bentham.</span> This conversation has sure taken a regrettable turn.</p>
<p> <span>Mugger.</span> I'm sure going to miss those fingers.</p>
<div><p> <span>Bentham.</span> <em>(pause)</em> Okay. Fine.</p>
<p> <em>(hands over £10)</em></p></div>
<div><p> <span>Mugger.</span> Excellent.</p>
<p> <em>(pockets the money and folds, carefully, the poster)</em></p></div>
<p> <span>Bentham.</span> I somehow feel I got mugged.</p>
<p> <span>Mugger.</span> Not at all. You made the world a better place.</p>
<p> * * *</p>
<p> <span>Mugger.</span> <em>(sees</em> Bentham<em>)</em> It's been a while, hasn't it?</p>
<p> <span>Bentham.</span> Oh, … hi.</p>
<div><p> <span>Mugger.</span> What's the matter?</p>
<p> <em>(notices</em> Bentham<em>'s unadorned lapel)</em></p>
<p> Where's your pin?</p></div>
<div><p> <span>Bentham.</span> Alas, wearing it in public became too costly.</p>
<p> <em>(notices</em> Mugger<em>'s hand)</em></p>
<p> What happened to your hand?</p></div>
<p> <span>Mugger.</span> Funny you should ask. It turns out that some so-called ‘Act Utilitarians’ are Act Utilitarian in name only. To cut a long story short, fingers were … cut. And it hurt <em>a lot</em>. And having the fingers sewn back on cost <em>a lot</em>. So – while I'm back to ten – I find myself, once more, a bit short on cash.</p>
<p> <span>Bentham.</span> Sad. Very sad to hear. But, before you reattempt your scheme, I'd like to share some news. I'm no longer an Act Utilitarian. I'm now a <em>Rule Utilitarian</em>: I believe I ought to perform an act if that act is required by a rule that prescribes a possible combination of everyone's acts that produces more utility than any other combination.<a href="#fn2"><span>Footnote </span><sup>2</sup></a></p>
<p> <span>Mugger.</span> I like this! It feels almost deontological.</p>
<p> <span>Bentham.</span> And, crucially, I'm no longer susceptible to your finger scheme, since the best combination of our acts didn't include my giving you money. A plausible moral theory shouldn't lay one open to that kind of exploitation. I wonder why I never saw this fault in utilitarian thinking. But no matter – the theory is fixed now.</p>
<p> <span>Mugger.</span> May I suggest a collaboration?</p>
<p> <span>Bentham.</span> Sure.</p>
<p> <span>Mugger.</span> There's this new course called <em>Effective Benevolence: Morality Made Easy</em>.<a href="#fn3"><span>Footnote </span><sup>3</sup></a> If I took this course, I would become an effective altruist just like you.</p>
<p> <span>Bentham.</span> Sounds great.</p>
<p> <span>Mugger.</span> The trouble is the course costs ten pounds. And here's where you could help out. Would you contribute ten pounds to let me realize this dream?</p>
<p> <span>Bentham.</span> Tempting.</p>
<div><p> <span>Mugger.</span> I've diagrammed our new situation:</p>
<p> <em>(unfolds another large poster)</em></p>
<p> <img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline2.png?pub-status=live" width="1135" height="330" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline2.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline2.png"></p>
<p> At the first node, you have a choice between giving me the money (going up) or keeping it (going down). If you give me the money, I have a choice between using that money to take the course (going up) or keeping it for myself (going down). Surely, me becoming an effective altruist is better than you keeping your ten pounds. Hence a rule prescribing your giving me the money and my using that money to take the course is a rule that prescribes the best possible combination of our acts.</p></div>
<p> <span>Bentham.</span> So I should give you the money.</p>
<p> <span>Mugger.</span> <em>(noticeably impressed by</em> Bentham<em>'s deduction)</em> I truly cherish working with sharp minds.</p>
<p> <span>Bentham.</span> Wait – why is one line thicker than the others?</p>
<p> <span>Mugger.</span> Oh, that denotes that I wouldn't take the course. If you gave me the money, I would in fact keep it for myself.</p>
<p> <span>Bentham.</span> You left that datum out of your pitch.</p>
<p> <span>Mugger.</span> I don't see how it would be relevant for a Rule Utilitarian. Your giving me the money would be part of the best possible combination of everyone's acts no matter whether I would take the course. What should matter to you is that I <em>could</em> do so – not whether I would.</p>
<p> <span>Bentham.</span> Suddenly, it seems that it may have been a mistake to assess rules by their being adopted by <em>everyone</em>. There will be deviants. I now think I'm a <em>Partial-Compliance Rule Utilitarian</em>: I believe that I ought to perform an act if that act is required by a code of rules that is optimal in the sense that its internalization by the <em>overwhelming majority</em> would be best.<a href="#fn4"><span>Footnote </span><sup>4</sup></a></p>
<p> <span>Mugger.</span> <em>(clears throat)</em> I have an announcement to make. I'd like to make it known that, if a code of rules were internalized by the overwhelming majority, I would internalize it too.</p>
<p> <span>Bentham.</span> You would?</p>
<p> <span>Mugger.</span> At that point, I feel, it would be antisocial not to.</p>
<p> <span>Bentham.</span> So, when I assess different codes of rules, I should assess their being internalized by the overwhelming majority including you?</p>
<p> <span>Mugger.</span> That's right.</p>
<p> <span>Bentham.</span> But, since the optimal code of rules won't actually be internalized by the overwhelming majority, you won't actually internalize that code of rules.</p>
<p> <span>Mugger.</span> Well, yeah.</p>
<p> <span>Bentham.</span> So, even though the optimal code of rules would, plausibly, prescribe me giving you the money and you taking the course (since that code is optimal given that it's internalized by the overwhelming majority including you), you would not take the course.</p>
<p> <span>Mugger.</span> Uh-huh.</p>
<p> <span>Bentham.</span> I'm getting second thoughts about Rule Utilitarianism all together.</p>
<p> <span>Mugger.</span> Very well. So you're going back to Act Utilitarianism? In that case, let me offer a deal–</p>
<p> <span>Bentham.</span> Let me cut in right here. I now think I'm a <em>Self-Harm-Discounting Act Utilitarian</em>: I believe that I ought to perform an act if that act would produce more utility than any alternative act with utility measured so that saving people from harm does not count towards utility if these people can save themselves.<a href="#fn5"><span>Footnote </span><sup>5</sup></a></p>
<p> <span>Mugger.</span> This is a major departure from standard Act Utilitarianism.</p>
<p> <span>Bentham.</span> True. But, with this modification, the theory is immune to your finger scheme. Since you could still avoid cutting off your finger in case I don't give you money, that avoidable harm does not count towards overall utility.</p>
<p> <span>Mugger.</span> Could you stick around a bit? I need to run a quick errand.</p>
<p> <span>Bentham.</span> No worries.</p>
<p> <span>Mugger.</span> And, just to double-check, when you say that harms don't count if people can avoid them themselves, you mean harms that people can <em>still</em> avoid themselves? That is, you aren't a <em>Retrospective Self-Harm-Discounting Act Utilitarian</em>, believing that you ought to perform an act if that act would produce more utility than any alternative act with utility measured so that saving people from harm does not count towards utility if these people can save themselves <em>or could have saved themselves if they had chosen otherwise in the past</em>?</p>
<p> <span>Bentham.</span> No – I'm not a monster. We have all made mistakes. The moral agent looks forward. If I found you drowning in a pond, I should save you regardless of whether you went in freely.<a href="#fn6"><span>Footnote </span><sup>6</sup></a></p>
<p> <span>Mugger.</span> Great, just as I thought. Stay put. I'll be back in a jiff.</p>
<p> * * *</p>
<p> <span>Bentham.</span> What took you so long?</p>
<div><p> <span>Mugger.</span> Sorry, I had to make a binding, unalterable arrangement with a thug, who will cut off my finger if you don't give me ten pounds. I've diagrammed our current predicament:</p>
<p> <em>(unfolds a third poster)</em></p>
<p> <img data-src="https://static.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline3.png?pub-status=live" width="930" height="330" data-original-image="/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline3.png" data-zoomable="true" src="https://www.cambridge.org/binary/version/id/urn:cambridge.org:id:binary:20221108004517207-0367:S0953820822000218:S0953820822000218_inline3.png"></p>
<p> As before, you have a choice at the first node between giving me ten pounds (going up) or keeping the money (going down). If you don't give me the money, the thug has a choice at the second node whether or not to cut off my finger – and the thug would do so, no matter what I do. So, if you don't give me the money, I can't avoid being harmed. If you don't give me the money, the thug will cut off my finger. So my finger is in your hand, so to speak. <em>(chuckle)</em></p></div>
<p> <span>Bentham.</span> Funny you should say that. While you were away, I had some time to reflect on morality. I now think I'm more of a <em>Mugging-Restricted Act Utilitarian</em>: I believe that I ought to perform an act if that act would produce more utility than any alternative act and, in addition, it wouldn't make me vulnerable to blatant muggings, threats, or blackmail.</p>
<p> <span>Mugger.</span> I don't want to rag on your new philosophy, but where's the theoretical purity of standard Act Utilitarianism? This theory is soiled with muddy, ambiguous terms. What, more precisely, is a ‘mugging’?</p>
<p> <span>Bentham.</span> I know one when I see one. And your latest scheme, I'm sure, is one.</p>
<p> <span>Mugger.</span> That's not a very satisfying answer.</p>
<p> <span>Bentham.</span> I'm afraid it will have to do for now.</p>
<p> <span>Mugger.</span> Also – and I hate to say this – your latest theory is, <em>more than a little</em>, ad hoc.</p>
<p> <span>Bentham.</span> Well, what it lacks in beauty, it makes up in expense minimization.</p>
<p> <span>Mugger.</span> Look, even if your theory tells you what you ought to do, it lacks <em>explanatory power</em>. A moral theory may tell us not only what ought be done but <em>why</em> it ought be done. Why settle for less?</p>
<div><p> <span>Bentham.</span> If I find an unmuggable version of utilitarianism with more explanatory power, I'll let you know.</p>
<p> <em>(A</em> Thug <em>approaches.)</em></p></div>
<p> <span>Mugger.</span> This is disappointing.</p>
<p> <span>Bentham.</span> <em>I'm not going to give you more money.</em></p>
<p> <span>Mugger.</span> Okay. Fine. I'll cut off <em>three</em> fingers if–</p>
<p> <span>Bentham.</span> Sorry, but here I must cut you off.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Slack's Google Drive App can share your private Docs and Drive files (206 pts)]]></title>
            <link>https://www.kapwing.com/blog/slacks-google-drive-integration-shares-private-documents/</link>
            <guid>37854159</guid>
            <pubDate>Thu, 12 Oct 2023 06:44:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.kapwing.com/blog/slacks-google-drive-integration-shares-private-documents/">https://www.kapwing.com/blog/slacks-google-drive-integration-shares-private-documents/</a>, See on <a href="https://news.ycombinator.com/item?id=37854159">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<article>
<header>


<figure>
<img srcset="https://www.kapwing.com/blog/content/images/size/w300/2023/10/video_image-O3EfROc4J.jpeg 300w,
                    https://www.kapwing.com/blog/content/images/size/w720/2023/10/video_image-O3EfROc4J.jpeg 720w,
                    https://www.kapwing.com/blog/content/images/size/w960/2023/10/video_image-O3EfROc4J.jpeg 960w,
                    https://www.kapwing.com/blog/content/images/size/w1200/2023/10/video_image-O3EfROc4J.jpeg 1200w,
                    https://www.kapwing.com/blog/content/images/size/w2000/2023/10/video_image-O3EfROc4J.jpeg 2000w" sizes="(max-width: 1200px) 100vw, 1200px" src="https://www.kapwing.com/blog/content/images/size/w1200/2023/10/video_image-O3EfROc4J.jpeg" alt="Slack's Google Drive App can share your private Docs and Drive files">
</figure>
</header>
<section>
<p>In 2017, Slack launched a popular Google Drive integration that makes it easy to embed, share, and get notified about new items added to Google Drive, including files, images, docs, and more. We use it daily here at <a href="https://www.kapwing.com/">Kapwing</a>, an online <a href="https://www.kapwing.com/">video editing</a> startup, since we run internally on top of Google Docs and Google Drive. Recently, however, I discovered that using file previews with the Google Drive Slack App will allow it to share completely private, unshared documents and files within your workspace.</p><figure><img src="https://www.kapwing.com/blog/content/images/2023/10/image-3.png" alt="" loading="lazy" width="1049" height="343" srcset="https://www.kapwing.com/blog/content/images/size/w600/2023/10/image-3.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2023/10/image-3.png 1000w, https://www.kapwing.com/blog/content/images/2023/10/image-3.png 1049w" sizes="(min-width: 720px) 720px"><figcaption>The popular Google Drive Slack Bot</figcaption></figure><p>The Slack Google Drive integration has a feature called File Previews that is enabled by default. This is usually pretty handy, because when you share a link to a Google Doc, or any file in Google Drive, the bot will automatically show a preview of the document. </p><figure><img src="https://www.kapwing.com/blog/content/images/2023/10/image-5.png" alt="" loading="lazy" width="1128" height="366" srcset="https://www.kapwing.com/blog/content/images/size/w600/2023/10/image-5.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2023/10/image-5.png 1000w, https://www.kapwing.com/blog/content/images/2023/10/image-5.png 1128w" sizes="(min-width: 720px) 720px"><figcaption>The Slack Bot preview for Google Documents</figcaption></figure><p>However, recently I noticed that it was doing this for documents that I had not yet shared. I also noticed that when certain links were sent to me, I would see a preview of the document even though I didn't have access to it yet. </p><figure><img src="https://www.kapwing.com/blog/content/images/2023/10/image-1.png" alt="" loading="lazy" width="1140" height="683" srcset="https://www.kapwing.com/blog/content/images/size/w600/2023/10/image-1.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2023/10/image-1.png 1000w, https://www.kapwing.com/blog/content/images/2023/10/image-1.png 1140w" sizes="(min-width: 720px) 720px"><figcaption>Me, sharing a private document with Luke which shows the full content of the document</figcaption></figure><p>If I open up the URL of the file preview, the preview is a full resolution 800 × 1035 image that shows the entire first page of the document. </p><p>The app does this for private images uploaded to Google Drive as well:</p><figure><img src="https://www.kapwing.com/blog/content/images/2023/10/image-6.png" alt="" loading="lazy" width="1140" height="680" srcset="https://www.kapwing.com/blog/content/images/size/w600/2023/10/image-6.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2023/10/image-6.png 1000w, https://www.kapwing.com/blog/content/images/2023/10/image-6.png 1140w" sizes="(min-width: 720px) 720px"><figcaption>Me, sharing a private image to a coworker that he is not supposed to see.</figcaption></figure><p>It seems like what's happening here is that when I share a document or file via a link, the Google Drive Slack app will automatically create a preview image of that file, using my own Google permissions. The problem is, that preview image gets re-uploaded to Slack's CDN, and is in high resolution and is now accessible to everyone in my Slack workspace. </p><p>You can test this yourself by creating a private Google document, or uploading an image accessible only to you to your Google Drive, and then sending the link to a colleague in Slack. If you use the web Slack client, you'll be able to inspect the full link to the image preview as well:</p><figure><img src="https://www.kapwing.com/blog/content/images/2023/10/image-7.png" alt="" loading="lazy" width="1324" height="743" srcset="https://www.kapwing.com/blog/content/images/size/w600/2023/10/image-7.png 600w, https://www.kapwing.com/blog/content/images/size/w1000/2023/10/image-7.png 1000w, https://www.kapwing.com/blog/content/images/2023/10/image-7.png 1324w" sizes="(min-width: 720px) 720px"><figcaption>The full image preview that gets sent to my collegue</figcaption></figure><p>It might not be the biggest deal to some people, but I think it can definitely be a problem if you are not being careful with sharing private Google documents in a public slack channel or workspace, especially since File Previews are enabled by default. Going forward, I think it would be better for the app to make this clear, since it can definitely be used by folks in Slack to see something they weren't allowed to. </p><p>I hope this helps others out there using the Google Drive Slack app!</p>
<a href="https://www.kapwing.com/signin" id="post-upsell" data-utm-campaign="inline cta">Create content faster with Kapwing's online video editor&nbsp;→ </a>
 </section>
</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Journey building a fast JSON parser and full JSONPath (101 pts)]]></title>
            <link>https://github.com/ohler55/ojg/blob/develop/design.md</link>
            <guid>37854115</guid>
            <pubDate>Thu, 12 Oct 2023 06:36:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/ohler55/ojg/blob/develop/design.md">https://github.com/ohler55/ojg/blob/develop/design.md</a>, See on <a href="https://news.ycombinator.com/item?id=37854115">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:ohler55/ojg" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Xy8YkOWfBCjDZzgV1Y53ct74QGgfXE3HFNQGklJAIoLKRBnyrpMVJvJqpLk0bg7rBdLUo1e5Imwc-3pGOCFDSA" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="ohler55/ojg" data-current-org="" data-current-owner="ohler55" data-logged-in="false">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<div data-modal-dialog-overlay="">
  <modal-dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" role="dialog" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      
      
</modal-dialog></div>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=ohler55%2Fojg" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/ohler55/ojg/blob/develop/design.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="9285d5ef2d8e28e1d50b8f2c1364cc793aaa962d77022e9a96e657d1277e83aa" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Metric Time (305 pts)]]></title>
            <link>https://metric-time.com/</link>
            <guid>37853181</guid>
            <pubDate>Thu, 12 Oct 2023 03:27:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://metric-time.com/">https://metric-time.com/</a>, See on <a href="https://news.ycombinator.com/item?id=37853181">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>
					<h2>Metric Minute</h2>
					<h3>A metric minute is broken into 100 seconds. </h3>
				</p><div id="rationale-container">			
				<h2>Rationale for Metric Time</h2>
				<p>
					What makes metric time so cool is that would make all the mental math we have to do when adding and subtracting time so much easier—especially when it comes to different timezones. Working with base-10 numbers is so much easier than trying to think in base-60, base-12, and base-24.
				</p>
				<p>
					There is no AM or PM with metric time. Just 10 hours in the day. To get a good night sleep (8 standard hours) you'd sleep for 3.33 metric hours. If you go to bed at 9:50 metric time (about 10:45pm in standard time), wanting to get a good night sleep, you'd wake up at 2:75 metric time (roughly 6:45am standard time). Remember, a half hour in metric time is 50 minutes, and a full hour is 100 minutes. One metric minute before "midnight" (10:00 on the metric clock) would be 9:99 in metric time.
				</p>
				<h2>Time is Money</h2>
				<p>
					The logic you use to tell time with metric time is very similar to how you think about money (assuming US Dollars). One metric hour is like a 100-dollar bill. One metric minute is a 1 dollar bill. And each metric second is a penny. Each day (10 metric hours), has 1,000 minutes, and 100,000 seconds. It's an interesting way to think about the time we get to spend each day.
				</p>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How LSP could have been better (222 pts)]]></title>
            <link>https://matklad.github.io/2023/10/12/lsp-could-have-been-better.html</link>
            <guid>37852944</guid>
            <pubDate>Thu, 12 Oct 2023 02:45:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2023/10/12/lsp-could-have-been-better.html">https://matklad.github.io/2023/10/12/lsp-could-have-been-better.html</a>, See on <a href="https://news.ycombinator.com/item?id=37852944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>

    <h2>
    <a href="#LSP-could-have-been-better"><span>LSP could have been better</span> <time datetime="2023-10-12">Oct 12, 2023</time></a>
    </h2>

<figure>
<blockquote><p><span>We talk about programming like it is about writing code, but the code ends up being less important</span>
<span>than the architecture, and the architecture ends up being less important than social issues.</span></p>
</blockquote>
<figcaption><cite><a href="https://neugierig.org/software/blog/2020/05/ninja.html"><span>The Success and Failure of Ninja</span></a></cite></figcaption>
</figure>
<p><span>The  </span><a href="https://matklad.github.io/2022/04/25/why-lsp.html"><em><span>Why LSP</span></em></a><span> post discusses the </span>“<span>social</span>
<span>issues</span>”<span> solved by LSP. LSP (as a part of overarching Microsoft strategy) is brilliant, because it</span>
<span>moved the world to a new equilibrium where not having basic IDE support is frowned upon. This post</span>
<span>instead discusses architectural aspects of LSP, which I personally find not as brilliant(especially given that</span>
<a href="https://htmlpreview.github.io/?https://github.com/dart-lang/sdk/blob/8e6a02d899ef62ef5b8405518b36340e609198e2/pkg/analysis_server/doc/api.html"><span>Dart Analysis Protocol</span></a>
<span>predates LSP and is technically superior in some aspects). Perhaps it</span>
<span>could be useful for someone designing other LSP-shaped protocols! Note that it</span>’<span>s been couple of</span>
<span>years since I was actively involved in LSP, probably the grass is greener these days!</span></p>
<p><span>Let</span>’<span>s get to the list of properties, good and bad, in no particular order.</span></p>
<section id="Focus-on-Presentation">

    <h2>
    <a href="#Focus-on-Presentation"><span>Focus on Presentation</span> </a>
    </h2>
<p><span>And let</span>’<span>s start with an aspect of the architecture which is genius, and which, I think, is</span>
<span>responsible for a big share of LSP success on the technical side. If you build a tool for working</span>
<span>with </span><em><span>multiple</span></em><span> programming languages, one of the biggest questions is how to find common ground</span>
<span>among different, but ultimately similar, languages. A first attempt is to uncover essential</span>
<span>commonality: after all, all languages have files, variables, functions, classes, right? This is </span>…
<span>maybe not necessary a dead end, but definitely a thorny and treacherous path </span>—<span> languages are</span>
<span>different, each language is weird in at least some of its aspects, and common ground risks to level</span>
<span>away meaningful distinctions.</span></p>
<p><span>So, what does LSP do here? It just doesn</span>’<span>t provide a semantic model of the code base. Instead, it is</span>
<span>focused squarely on the presentation. No matter how different each programming language is, they</span>
<span>all, in the end, use the same completion widget. So LSP is formulated in terms of what</span>’<span>s shown in</span>
<span>the completion widget, not in terms of the underlying semantic language entities. That means that</span>
<span>each language has an internal semantic model which is full fidelity </span><em><span>for this particular language</span></em><span>,</span>
<span>and uses it to provide the best completion experience which is possible for a given completion</span>
<span>widget. This is how rust-analyzer is structured internally as well:</span></p>
<ol>
<li>
<span>Compiler layer deals with the messy language analysis tasks, it derives more structured</span>
<span>information (types) from less structured information (source text), explicitly tracking analysis</span>
<span>layers and phases.</span>
</li>
<li>
<span>The HIR (high-level intermediate representation) is a façade around the compiler, which provides</span>
<span>a rich graph-based object model of code which looks as if all derived information, like types, is</span>
<span>pre-computed.</span>
</li>
<li>
<span>The IDE layer uses HIR to compute things like completions, and presents them as Rust-specific,</span>
<span>but semantics-less POD structures to be shown to the user in GUI more or less as is.</span>
</li>
</ol>
<p><span>One consequence of this architecture is that LSP requests map to editor widgets, and not to the</span>
<span>underlying language concepts, even when several different widgets are powered by the same underlying</span>
<span>data. For example, LSP has separate requests for:</span></p>
<ul>
<li>
<span>hierarchical outline of a file displayed in the side bar,</span>
</li>
<li>
“<span>breadcrumbs</span>”<span> shown in the header,</span>
</li>
<li>
<span>syntax-aware selection ranges,</span>
</li>
<li>
<span>code folding.</span>
</li>
</ul>
<p><span>Although all four features are just different views into an AST, there</span>’<span>s no </span>“<span>get AST</span>”<span> request in the</span>
<span>LSP. Different requests allow to fine-tune presentation for the  different use-cases, and the</span>
<span>details do differ! Semantic selection might contain some sub-syntax ranges inside string literals</span>
<span>and comments, breadcrumb need to include things like conditionals of </span><code>if</code><span> expressions, while the</span>
<span>outline might want to get rid of less important nodes. Attentive reader will notice that breadcrumbs</span>
<span>and the outline actually use the same LSP request. Even LSP doesn</span>’<span>t follow LSP philosophy fully!</span></p>
</section>
<section id="Transport">

    <h2>
    <a href="#Transport"><span>Transport</span> </a>
    </h2>
<p><span>After a big thing that LSP did right, let</span>’<span>s look at a small thing that it got wrong. Let</span>’<span>s look at</span>
<span>how information is transmitted over the wire.</span></p>
<p><span>JSON is actually OK! Many people complain that JSON is slow, but that</span>’<span>s not actually the case</span>
<span>generally. There are some edge cases, where particular client libraries can be slow as was the case</span>
<span>at least at some point with Swift and Emacs, but JSON is definitely fast enough for Rust, Java and</span>
<span>JavaScript. Of course, something substantially better than JSON is possible in </span><em><span>theory</span></em><span>.</span></p>
<p><span>I think ideally we need </span>“<span>WebAssembly for IPC</span>”<span>, a format that:</span></p>
<ul>
<li>
<span>has dual text and binary encoding,</span>
</li>
<li>
<span>is stupidly simple,</span>
</li>
<li>
<span>is thoroughly, readably, and precisely specified,</span>
</li>
<li>
<span>and, in general, is principled and a joy to use.</span>
</li>
</ul>
<p><span>There</span>’<span>s no such format yet, so JSON it is. Good enough.</span></p>
<p><span>HTTP framing is not OK. On the wire, the messages framed like this:</span></p>

<figure>


<pre><code><span>Content-Length: 92 \r\n</span>
<span>\r\n</span>
<span>Actual message</span></code></pre>

</figure>
<p><span>That is:</span></p>
<ul>
<li>
<span>case-insensitive </span>“<span>content-length</span>”<span> header,</span>
</li>
<li>
<span>followed by length of the following message, formatted as a decimal number in ASCII,</span>
</li>
<li>
<span>followed by double </span><code>\r\n</code><span>,</span>
</li>
<li>
<span>followed by the actual message.</span>
</li>
</ul>
<p><span>This resembles HTTP, but is not actual HTTP, so you need to write a bit of custom code to deal</span>
<span>with the framing. That</span>’<span>s not hard:</span></p>

<figure>


<pre><code><span>  <span>let</span> <span>mut </span><span>size</span> = <span>None</span>;</span>
<span>  <span>let</span> <span>mut </span><span>buf</span> = <span>String</span>::<span>new</span>();</span>
<span>  <span>loop</span> {</span>
<span>    buf.<span>clear</span>();</span>
<span>    <span>if</span> inp.<span>read_line</span>(&amp;<span>mut</span> buf)? == <span>0</span> {</span>
<span>      <span>return</span> <span>Ok</span>(<span>None</span>);</span>
<span>    }</span>
<span>    <span>if</span> !buf.<span>ends_with</span>(<span>"\r\n"</span>) {</span>
<span>      <span>return</span> <span>Err</span>(invalid_data!(<span>"malformed header: {:?}"</span>, buf));</span>
<span>    }</span>
<span>    <span>let</span> <span>buf</span> = &amp;buf[..buf.<span>len</span>() - <span>2</span>];</span>
<span>    <span>if</span> buf.<span>is_empty</span>() {</span>
<span>      <span>break</span>;</span>
<span>    }</span>
<span>    <span>let</span> <span>mut </span><span>parts</span> = buf.<span>splitn</span>(<span>2</span>, <span>": "</span>);</span>
<span>    <span>let</span> <span>header_name</span> = parts.<span>next</span>().<span>unwrap</span>();</span>
<span>    <span>let</span> <span>header_value</span> = parts.<span>next</span>().<span>ok_or_else</span>(|| {</span>
<span>      invalid_data!(<span>"malformed header: {:?}"</span>, buf)</span>
<span>    })?;</span>
<span>    <span>if</span> header_name.<span>eq_ignore_ascii_case</span>(<span>"Content-Length"</span>) {</span>
<span>      size = <span>Some</span>(</span>
<span>        header_value.parse::&lt;<span>usize</span>&gt;().<span>map_err</span>(invalid_data)?,</span>
<span>      );</span>
<span>    }</span>
<span>  }</span>
<span>  <span>let</span> <span>size</span>: <span>usize</span> =</span>
<span>    size.<span>ok_or_else</span>(|| invalid_data!(<span>"no Content-Length"</span>))?;</span>
<span>  <span>let</span> <span>mut </span><span>buf</span> = buf.<span>into_bytes</span>();</span>
<span>  buf.<span>resize</span>(size, <span>0</span>);</span>
<span>  inp.<span>read_exact</span>(&amp;<span>mut</span> buf)?;</span>
<span>  <span>let</span> <span>buf</span> = <span>String</span>::<span>from_utf8</span>(buf).<span>map_err</span>(invalid_data)?;</span></code></pre>

</figure>
<p><span>But, still, decoding ASCII message length from variable-length header? That</span>’<span>s accidental complexity.</span>
<span>Just separate json objects with newlines instead:</span></p>
<p><a href="https://jsonlines.org/">https://jsonlines.org</a></p>
<p><span>Framing using </span><code>\n</code><span> as a separator is almost certainly available out of the box in the programming</span>
<span>language of choice.</span></p>
<p><span>Wiping away the tears and peeling one more layer from the onion, we see json-rpc:</span></p>

<figure>


<pre><code><span><span>{</span></span>
<span>    <span>"jsonrpc"</span><span>:</span> <span>"2.0"</span><span>,</span></span>
<span>    <span>"method"</span><span>:</span> <span>"initialize"</span><span>,</span></span>
<span>    <span>"id"</span><span>:</span> <span>1</span><span>,</span></span>
<span>    <span>"params"</span><span>:</span> <span>{</span> ... <span>}</span></span>
<span><span>}</span></span></code></pre>

</figure>
<p><span>This again is a bit of needless accidental complexity. Again, not hard to handle:</span></p>

<figure>


<pre><code><span><span>fn</span> <span>_write</span>(<span>self</span>, w: &amp;<span>mut</span> <span>dyn</span> Write) <span>-&gt;</span> io::<span>Result</span>&lt;()&gt; {</span>
<span>  <span>#[derive(Serialize)]</span></span>
<span>  <span>struct</span> <span>JsonRpc</span> {</span>
<span>    jsonrpc: &amp;<span>'static</span> <span>str</span>,</span>
<span>    <span>#[serde(flatten)]</span></span>
<span>    msg: Message,</span>
<span>  }</span>
<span>  <span>let</span> <span>text</span> = serde_json::<span>to_string</span>(&amp;JsonRpc {</span>
<span>    jsonrpc: <span>"2.0"</span>,</span>
<span>    msg: <span>self</span>,</span>
<span>  })?;</span>
<span>  <span>write_msg_text</span>(w, &amp;text)</span>
<span>}</span></code></pre>

</figure>
<p><span>But:</span></p>
<ul>
<li>
<span>Prone to complexity amplification, invites jsonrpc framework with all the latest patterns.</span>
</li>
<li>
<code>"jsonrpc": "2.0"</code><span> is meaningless noise which you have to look at during debugging.</span>
</li>
<li>
<span>Error codes like </span><code>-32601</code><span> (ah, that comes from xml-rpc!).</span>
</li>
<li>
<span>Includes notifications. Notification are a big anti-pattern in RPC, for a somewhat subtle reason.</span>
<span>More on this later.</span>
</li>
</ul>
<p><span>What to do instead? Do what Dart does, some excerpts from </span><a href="https://htmlpreview.github.io/?https://github.com/dart-lang/sdk/blob/8e6a02d899ef62ef5b8405518b36340e609198e2/pkg/analysis_server/doc/api.html"><span>the specification</span></a><span>:</span></p>

<figure>
<blockquote><p><span>Messages are delineated by newlines. This means,</span>
<span>in particular, that the JSON encoding process must not introduce newlines within a message. Note</span>
<span>however that newlines are used in this document for readability.</span></p>
<p><span>To ease interoperability with Lisp-based clients (which may not be able to easily distinguish</span>
<span>between empty lists, empty maps, and null), client-to-server communication is allowed to replace any</span>
<span>instance of </span>“<code>{}</code>”<span> or </span>“<code>[]</code>”<span> with null. The server will always properly represent empty lists as </span>“<code>[]</code>”
<span>and empty maps as </span>“<code>{}</code>”<span>.</span></p>
<p><span>Clients can make a request of the server and the server will provide a response for each request</span>
<span>that it receives. </span><strong><span>While many of the requests that can be made by a client are informational in</span>
<span>nature, we have chosen to always return a response so that clients can know whether the request was</span>
<span>received and was correct.</span></strong></p>
<p><span>Example request:</span></p>

<figure>


<pre><code><span>request: {</span>
<span>  "id": String</span>
<span>  "method": "server.getVersion"</span>
<span>}</span>
<span></span>
<span>response: {</span>
<span>  "id": String</span>
<span>  "error": optional RequestError</span>
<span>  "result": {</span>
<span>    "version": String</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
</blockquote>

</figure>
<p><span>That</span>’<span>s basically jsonrpc, the good parts, including using </span><code>"UNKNOWN_REQUEST"</code><span> instead of </span><code>-32601</code><span>.</span></p>
</section>
<section id="Coordinates">

    <h2>
    <a href="#Coordinates"><span>Coordinates</span> </a>
    </h2>
<p><span>LSP uses </span><code>(line, column)</code><span> pairs for coordinates. The neat thing here is that this solves significant</span>
<span>chunk of </span><code>\n</code><span> vs </span><code>\r\n</code><span> problems </span>—<span> client and server may represent line endings differently, but</span>
<span>this doesn</span>’<span>t matter, because coordinates are the same.</span></p>
<p><span>Focus on the presentation provides another motivation, because location information received by the</span>
<span>client can be directly presented to the user, without the need to parse the underlying file. I have</span>
<span>mixed feelings about this.</span></p>
<p><span>The problem, </span><code>column</code><span> is counted using UTF-16 code units. This is, like, </span>“<span>no</span>”<span>. For many reasons,</span>
<span>but in particular, UTF-16 is definitely the wrong number to show to the user as a </span>“<span>column</span>”<span>.</span></p>
<p><span>There</span>’<span>s no entirely obvious answer what should be used instead. My personal favorite would be</span>
<span>counting utf-8 code units (so, just bytes). You need </span><em><span>some</span></em><span> coordinate space. Any reasonable</span>
<span>coordinate space won</span>’<span>t be useful for presentation, so you might as well use the space that matches</span>
<span>the underlying utf-8 encoding, so that accessing substrings is O(1).</span></p>
<p><span>Using unicode codepoints would perhaps be the most agreeable solution. Codepoints are useless </span>—
<span>you</span>’<span>ll need to convert to grapheme clusters for presentation, and to utf-8 code units to do anything</span>
<span>with the string. Still, codepoints are a common denominator, they are more often correct if</span>
<span>incorrectly used for presentation, and they have a nice property that any index less then length is</span>
<span>valid irrespective of the actual string.</span></p>
</section>
<section id="Causality-Casualty">

    <h2>
    <a href="#Causality-Casualty"><span>Causality Casualty</span> </a>
    </h2>
<p><span>As mentioned above, one drawback of one-way notifications from jsonrpc is that they don</span>’<span>t allow</span>
<span>signaling errors. But there</span>’<span>s a more subtle problem here: because you don</span>’<span>t receive response to a</span>
<span>notification, it might be hard to order it relative to other events. The Dart protocol is pretty</span>
<span>strict about the ordering of events:</span></p>

<figure>
<blockquote><p><span>There is no guarantee concerning the order in which responses will be returned, but there is a</span>
<span>guarantee that the server will process requests in the order in which they are sent as long as the</span>
<span>transport mechanism also makes this guarantee.</span></p>
</blockquote>

</figure>
<p><span>This guarantee ensures that the client and the server mutually understand each other</span>’<span>s state. For</span>
<span>every request the client knows which file modifications happened before it, and which came afterwards.</span></p>
<p><span>In LSP, when the client wants to modify the state of a file on the server, it sends a notification.</span>
<span>LSP also supports server-initiated edits. Now, if the client sends a </span><code>didChangeTextDocument</code>
<span>notification, and then receives a </span><code>workspace/applyEdit</code><span> request from the server, there</span>’<span>s no way for</span>
<span>the client to know whether the edit takes the latest change into the account or not. Were</span>
<code>didChangeTextDocument</code><span> a request instead, the client could have looked at the relative order of the</span>
<span>corresponding response and </span><code>workspace/applyEdit</code><span>.</span></p>
<p><span>LSP papers over this fundamental loss of causality by including numeric versions of the documents</span>
<span>with every edit, but this is a best effort solution. Edits might be invalidated by changes to</span>
<span>unrelated documents. For example, for a rename refactor, if a new usage was introduced in a new file</span>
<span>after the refactor was computed, version numbers of the changed files would wrongly tell you that</span>
<span>the edit is still correct, while it will miss this new usage.</span></p>
<p><span>Practically, this is a small problem </span>—<span> it works most of the  time (I </span><em><span>think</span></em><span> I have seen zero</span>
<span>actual bugs caused by causality loss), and even the proper solution can</span>’<span>t order events originating</span>
<span>from the client relative to the events originating from the file system. But the fix is also very</span>
<span>simple </span>—<span> just don</span>’<span>t voluntarily lose causality links!</span></p>
</section>
<section id="Remote-Procedural-State-Synchronization">

    <h2>
    <a href="#Remote-Procedural-State-Synchronization"><span>Remote Procedural State Synchronization</span> </a>
    </h2>
<p><span>And this touches what I think is the biggest architectural issue with LSP. LSP is an RPC protocol</span>
—<span> it is formed by </span>“<span>edge triggered</span>”<span> requests that make something happen on the other side. But this</span>
<span>is not how most of IDE features work. What actually is needed is </span>“<span>level triggered</span>”<span> </span><strong><span>state</span>
<span>synchronization</span></strong><span>. The client and the server need to agree what something </span><em><span>is</span></em><span>, deciding the course</span>
<span>of action is secondary. It is </span>“<span>to be or not to be</span>”<span> rather than </span>“<span>what is to be done</span>”<span>.</span></p>
<p><span>At the bottom is synchronization of text documents </span>—<span> the server and the client need to agree which</span>
<span>files there are, and what is there content.</span></p>
<p><span>Above is synchronization of derived data. For example, there</span>’<span>s a set of errors in the project. This</span>
<span>set changes when the underlying text files change. Errors change with some lag, as it takes time to</span>
<span>compute them (and sometimes files changes faster than the errors could be re-computed).</span></p>
<p><span>Things like file outline, syntax highlighting, cross-reference information, e.t.c, all follow the</span>
<span>same pattern.</span></p>
<p><span>Crucially, predicting which changes to the source invalidate which derived data requires language</span>
<span>specific knowledge. Changing the text of </span><code>foo.rs</code><span> might affect syntax highlighting in </span><code>bar.rs</code><span> (as</span>
<span>syntax highlighting is affected by types).</span></p>
<p><span>In LSP, highlighting and such are requests. This means that either the client is incorrect and shows</span>
<span>stale highlighting results, or it conservatively re-queries all highlighting results after every</span>
<span>change, wasting the CPU, and </span><em><span>still</span></em><span> showing stale results sometimes, when an update happens outside</span>
<span>of the client (eg, when </span><code>cargo</code><span> finished downloading external crates).</span></p>
<p><span>The Dart model is more flexible, performant and elegant. Instead of highlighting being a request, it</span>
<span>is a </span><em><span>subscription</span></em><span>. The client subscribes to syntax highlighting of particular files, the server</span>
<span>notifies the client whenever highlights for the selected files change. That is, two pieces of state</span>
<span>are synchronized between the client and the server:</span></p>
<ul>
<li>
<span>The set of file the client is subscribed to</span>
</li>
<li>
<span>The actual state of syntax highlighting for these files.</span>
</li>
</ul>
<p><span>The former is synchronized by sending the whole </span>“<span>current set</span>”<span> of files in a request, whenever the</span>
<span>set changes. The latter is synchronized by sending incremental updates.</span></p>
<p><span>Subscriptions are granular both in terms of the file set, as well as in terms of features. The</span>
<span>client might subscribe for errors in the whole project, and for highlights in the currently opened</span>
<span>documents only.</span></p>
<p><span>Subscriptions are implemented in terms of RPC, but they are an overarching organizational pattern</span>
<span>followed by the majority of the requests. LSP doesn</span>’<span>t have an equivalent, and has real bugs with</span>
<span>outdated information shown to the user.</span></p>
<p><span>I don</span>’<span>t think Dart goes as far as possible here. JetBrains Rider, if I understand correctly, does</span>
<span>something smarter:</span></p>
<p><a href="https://www.codemag.com/Article/1811091/Building-a-.NET-IDE-with-JetBrains-Rider">https://www.codemag.com/Article/1811091/Building-a-.NET-IDE-with-JetBrains-Rider</a></p>
<p><span>I think the idea behind the rider protocol is that you directly define the state you want to</span>
<span>synchronize between the client and the server as state. The protocol then manages </span>“<span>magic</span>”
<span>synchronization of the state by sending minimal diffs.</span></p>
</section>
<section id="Simplistic-Refactorings">

    <h2>
    <a href="#Simplistic-Refactorings"><span>Simplistic Refactorings</span> </a>
    </h2>
<p><span>Let</span>’<span>s unwind to something more down to earth, like refactorings. Not the simple ones, like rename,</span>
<span>but complex ones, like </span>“<span>change signature</span>”<span>:</span></p>
<p><a href="https://www.jetbrains.com/idea/guide/tips/change-signature/">https://www.jetbrains.com/idea/guide/tips/change-signature/</a></p>
<p><span>In this refactoring, the user selects a function declaration, then rearranges</span>
<span>parameters in some way (reorders, removes, adds, renames, changes types, whatever), and then the IDE</span>
<span>fixes all call-sites.</span></p>
<p><span>The thing that makes this refactor complex is that it is interactive </span>—<span> it</span>’<span>s not an atomic request</span>
“<span>rename </span><code>foo</code><span> to </span><code>bar</code>”<span>, it</span>’<span>s a dialog between the IDE and the user. There are many parameters that</span>
<span>the user tweaks based on the analysis of the original code and the already specified aspects of the</span>
<span>refactoring.</span></p>
<p><span>LSP doesn</span>’<span>t support this workflows. Dart somewhat supports them, though each refactoring gets to use</span>
<span>custom messages (that is, there</span>’<span>s quite good overall protocol for multistep refactorings, but each</span>
<span>refactoring essentially sends </span><code>any</code><span> over the wire, and the IDE on the other side hard-codes specific</span>
<span>GUIs for specific refactorings). This per-refactoring work is not nice, but it is much better than</span>
<span>not having these complex refactorings at all.</span></p>
</section>
<section id="Dynamic-Registration">

    <h2>
    <a href="#Dynamic-Registration"><span>Dynamic Registration</span> </a>
    </h2>
<p><span>A small one to conclude. Significant chunk of conceptual LSP complexity comes from support for</span>
<span>dynamic registration of capabilities. I don</span>’<span>t understand why that features is there, rust-analyzer</span>
<span>uses dynamic registration only for specifying which files should be watched. And that would be much</span>
<span>simpler if it used a plain request (or a subscription mechanism).</span></p>
</section>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anyone gotten complete, permanent relief from tinnitus? (309 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=37852711</link>
            <guid>37852711</guid>
            <pubDate>Thu, 12 Oct 2023 02:10:00 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=37852711">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="37855076"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855076" href="https://news.ycombinator.com/vote?id=37855076&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I’ve had tinnitus for about one year now. There is no permanent relief or treatment. Lots of snake oil.<p>My hearing was tested by an Audiologist and my hearing was normal.</p><p>The “sound” I observe is high pitched. Using a tone generator I matched it to around 16,500 Hz. Interestingly, if I play that tone I get temporary relief on the order of 2-10 minutes.</p><p>As far as I understand my tinnitus is the result of something going wonky with the signal processing in the brain.</p><p>If you’re suffering from tinnitus for the first time it’s important to remain calm. There is defiantly an amplifying effect from the psychological aspect of tinnitus. Eventually the body will “habituate” if it does not go away. It took me around 3-6 months to be able to ignore it. During the day I rarely hear it. At night a little more. Playing sounds at a low volume on a Bluetooth speaker helps. For example on Spotify an artist called “TMSOFT” has good stuff. In the day I’ll listen to jazz or lofi.</p><p>It’s important to protect your ears. iPhone has a hearing protection feature for headphones and I have it on the lowest setting. I would avoid in-ear headphones. Use hearing protection at concerts.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855534"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855534" href="https://news.ycombinator.com/vote?id=37855534&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I’ve had tinnitus for something like 5 years now and mine varies from day-to-day. Mine seems to be somewhat connected to diet. I think salt might be a trigger. If I have a French Fries or some other salty snack, I’m in for louder ringing an hour or two later.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855318"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855318" href="https://news.ycombinator.com/vote?id=37855318&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Aren’t in ear headphones with noise cancellation better than not using them and instead raising the volume to overcome background noise?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855257"><td></td></tr>
                <tr id="37855390"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37855390" href="https://news.ycombinator.com/vote?id=37855390&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Phase is a physical property of sound waves that cannot influence a signal originating from the brain or cochlea.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855541"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37855541" href="https://news.ycombinator.com/vote?id=37855541&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>It's fun to test what happens when you play the same wave through your headphones but one channel is inverted.<p>Funny enough lots of stereo widening magic relies on a physical property of sound not existing when it's in your head.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855431"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37855431" href="https://news.ycombinator.com/vote?id=37855431&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Yea, but he proposed shifting the phase of the physical signal (16.5khz) which supposedly matches the frequency of the imaginary signal.<p>I still think it's unlikely to work.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855503"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_37855503" href="https://news.ycombinator.com/vote?id=37855503&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I understand, and there could very well be some psychoacoustic cancellation when stimulating with a matching tone. But any effect will be regardless of the phase of the signal, because phase cancellation is a physical effect of sound waves that occurs before they are converted to neural activity (where tinnitus originates from).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855424"><td></td></tr>
                              <tr id="37855366"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855366" href="https://news.ycombinator.com/vote?id=37855366&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I also have what I'd now say is quiet tinnitus, when I used to party a lot it was pretty bad. I won't notice it during the day and I don't really notice it at night, however that wasn't the case a year or two ago.<p>Things I've done to try prevent it getting worse:</p><p>* iOS has a limit volume option - that's on permanently.</p><p>* I use a speaker on soft volume in the room when wfh rather than headphones when listening to music.</p><p>* I have ear plugs on my keys for when I end up going out to music events and festivals (I use Alpine, partner uses Loop). This made a huge difference and I'll go to bed without ringing ears.</p><p>* I try have days where I don't listen to music. At first it was weird, now I'm quite used to it.</p><p>* Online meetings, keep the volume low where possible.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855249"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855249" href="https://news.ycombinator.com/vote?id=37855249&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I got mine under control, though there is still a bit left that probably wont go away, but is totally ok for me. Got rid of my 2 main stress sources, changed to a much more healthy diet, drinking more regularly and if there is a new tinnitus attack (i still do not know the propper english word, in german its hörsturz) i do breathing and ear massaging for half a minute. Not just massaging the outer ear but also gently pumping with air pressure similar to adjusting after a flight. I noticed time is super critical here. Just before the tinnitus starts there is 1 or 2 seconds where you feel the hearing disappears its as if the volume is turned down. If the massaging and deep breathing starts in this moment before the tinnitius beeping and noise sets in, it seems to always go away completely after.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855335"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855335" href="https://news.ycombinator.com/vote?id=37855335&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span><pre><code>   i still do not know the propper english word, in german its hörsturz
</code></pre>
In several forums I've seen them talk about "spikes"</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855337"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855337" href="https://news.ycombinator.com/vote?id=37855337&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I think I'm an odd case, in that I did have "tinnitus" (a constant high pitched buzz in my right ear), along with a constant tension headache that resulted in me not getting any sleep. I was prescribed sleeping pills and then a visit to the neurologist said all my conditions were caused by anxiety. I actually didn't think I was anxious (I think it was the result of a bad night on cocaine).
Err... Anyway, a large career break with lots of relaxation gave me relief from both my daily headaches and tennitus. I was doing lots of exercise and also drinking a lot of water mixed with corriander (I was convinced this would help to detox me for some reason).
The tennitus went away after about 2 months and it took about a year to get away from the constant headaches. I haven't done class A drugs since...
Probably not at all helpful, but I thought I'd share.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855217"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855217" href="https://news.ycombinator.com/vote?id=37855217&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I have it for 4 years and got almost completely desensitized. It <i>is</i> a relief. It’s still there (and right now it rings). But it doesn’t create colorful reactions anymore. During the day, at night, before sleep, it rarely bothers me. Sometimes I even feel comfy about it.<p>In my case the secret was a <i>sort</i> of a therapy. At the second year I’ve become quite suicidal about it. There was a point in time after collecting all the information there is, when I absolutely ultimately had to make a decision. As a result of this thought process, “I will die with it” got fully accepted either way. Somehow this deadened my reactions and after a short while it became just a part of my life that I ignore. My brain got so good at masking it that I have to carefully listen sometimes (yeah, it’s there and it’s loud). I also stopped looking for a cure, relief, methods, threads like this. Not like “I shouldn’t”, but like “not interested”.</p><p>Pretty sure I must not recommend this way or leave it without a disclaimer: if you feel the same, then get professional help, don’t go through it alone.</p><p>My key insight is that you suffer while the hope lives, not that you have to lean over the edge to realize that.</p><p>Edit: upvote to <a href="https://mynoise.net/" rel="nofollow noreferrer">https://mynoise.net</a>, it helped to mask it at early stages much better than colored noises or youtube videos. You can tune sound components to your case and there’s a lot of presets now.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853407"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853407" href="https://news.ycombinator.com/vote?id=37853407&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I'm a physician with lifelong bilateral tinnitus. Multiple pitches, quite loud, and acutely worse on the right side for the last 1.5 years or so (with some slight hearing loss).<p>I've spent a <i>lot</i> of time on PubMed, but so far I've not found anything that helps mine, even partially or temporarily. I've consulted with close friends that have significant expertise is relevant fields, which has not been fruitful.</p><p>Thankfully it's just part of existence as I've always known it, and so it's usually not too difficult, but can really be maddening when trying to fall asleep.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855554" href="https://news.ycombinator.com/vote?id=37855554&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Have you talked about the Lenire device with your colleagues at all? It got FDA approval and is slowly rolling out as they train audiologists.<p>There’s a Lenire provider where I live (Austin) and I’m on the fence about scheduling an appointment to get one.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853468"><td></td></tr>
                <tr id="37853960"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37853960" href="https://news.ycombinator.com/vote?id=37853960&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Not the same user, but I've found the most relief from pink noise like rain or a waterfall played through decent speakers. I find it to be easier on the ears and less distracting than white noise.<p>I'd recommend anyone suffering play around with the generators on myNoise. It wouldn't be an exaggeration to say they kept me going when I was first adjusting to the ringing.</p><p><a href="https://mynoise.net/NoiseMachines/rainNoiseGenerator.php" rel="nofollow noreferrer">https://mynoise.net/NoiseMachines/rainNoiseGenerator.php</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855038"><td></td></tr>
            <tr id="37854008"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37854008" href="https://news.ycombinator.com/vote?id=37854008&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I also listen to pink noise or rain sounds on headphones throughout my work day. I worry about the side effects of listening to random noise for so long, but I continue (not loudly) because it effectively masks my tinnitus (and helps with my task focus).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37854828"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37854828" href="https://news.ycombinator.com/vote?id=37854828&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Again a different person but my tinnitus souds like white noise - but at a different pitch so not much use.<p>Unless I put the white noise very loud to drwon out tinnitus which is not a good thing to do.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854870"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37854870" href="https://news.ycombinator.com/vote?id=37854870&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>You can modulate the white noise very effectively. I dabble with analog synthesizers which let you use low/high pass filters on white noise to get very interesting sounds. I wonder if you carefully matched it, it might alleviate the discomfort. Worth trying I suppose.</span></p></div></td></tr>
        </tbody></table></td></tr>
                              <tr id="37853779"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853779" href="https://news.ycombinator.com/vote?id=37853779&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>My tinnitus was caused by flying long distance while having a ear infection. The first year was horrible, but after that it got a better every year and about 5 years later it's basically gone. I can only hear it now when I am in a very quiet environment or (this is the simplest way to revive it) bite down hard on my teeth. I think my brain has adjusted to the sound by masking it.<p>Most doctor visits were quite disappointing since they didn't do anything. I heard from many people that early therapy can help a lot (like oxygen therapy) but my doctor only tried some of these things after I repeatedly asked for it. But by that time it was already too late (like 3-4 weeks later).</p><p>What helped me was:
- distraction  
- constant white noise in the background  
- avoiding any source of loud sounds/music/etc (I am very sensitive on that now)  
- relaxing my jaw muscles  
- distraction  
- going to the doctor at the earliest sign of possible ear infections to stop infections from spreading to the inner ear.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37854901"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854901" href="https://news.ycombinator.com/vote?id=37854901&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I'm not saying this will work for everyone, but I just lie to myself.<p>I know this isn't true, but I've told myself that having tinnitus is a normal thing, that everyone has.</p><p>The "fact" that it's normal and that everyone has it has removed the suffering for me.</p><p>Weird, but maybe it will help someone here.</p><p>"Pain is inevitable, suffering is optional."
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853075"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853075" href="https://news.ycombinator.com/vote?id=37853075&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Yes, mine is not solved in theory but is solved in practice. The ringing is still there if I listen for it, but it’s effectively unnoticeable and I’m now unbothered by it.<p>The solution that worked for my was basically “acceptance and commitment therapy” - I think I learned it from a book written by a Dr Russell or something like that.</p><p>Would recommend, am very glad I did it. It seems kind of kooky though, it’s almost like you pretend the tinnitus is a part of you and you have a conversation with it and welcome it and all that kinda stuff over time, and then eventually it just kinda stops being bothersome. Doesn’t really make sense, but worked well.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37853944"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853944" href="https://news.ycombinator.com/vote?id=37853944&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I think this is the only way as of moment. Just desensitize yourself to the sound until there's no effect on you.<p>One thing I tried as I was still anxious about the matter that I had damaged my hearing permanently, was sitting in a pressurized chamber with a lot of oxygen. Ridiculously expensive and did basically nothing. Possibly if you go immediately after it happens it might help but I'm doubtful of its benefits (the company was making a buck though as single-person business).</p><p>I just remember this lady who had hit a garbage can's lid too hard which had made her ears ring. How unlucky. She was quite stressed about it as well.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853448"><td></td></tr>
            <tr id="37855134"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855134" href="https://news.ycombinator.com/vote?id=37855134&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I've had it for as long as I can remember soI don't know what life is without.<p>I've had some curiousity from time to time, but that's the extent of it.</p><p>I truly believe acceptance is the best cure when no cure exists in this case, but it's also the most difficult method in all aspects of life.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855295"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37855295" href="https://news.ycombinator.com/vote?id=37855295&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I know stress is an aggrevator of tinnitus for me, clenched jaw and tight neck muscles etc.<p>So in a practical sense, worrying about it literally made it worse by triggering those stress reactions which worsened the tinnitus.</p><p>It's that mechanism that acceptance helps me with.</p><p>I am not always accepting, but if I can calm myself down and just deal with it, it lessens drastically.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855322"><td></td></tr>
                        <tr id="37854734"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854734" href="https://news.ycombinator.com/vote?id=37854734&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>My tinnitus is there pretty much all of the time, and then for an hour or so it will disappear.<p>Over the last year or so I've developed really bad hyperacusis, this coupled with misophonia had made me severely suicidal, I even contacted the local mental health services and autism charities.</p><p>I've reduced that by wearing coloured lenses, I had been diagnosed with irlen but I see it as a little bit woo, however the coloured lenses have really worked.</p><p>My theory - I have a sensory fuck-it-bucket - I'm bombarded by all this crap every day, and if it overflows I'm under real stress, by reducing overload from other senses my bucket fills much slower, and the sound and tinnitus doesn't seem to affect me so much.</p><p>Strange to think that a £16 pair of cheap glasses has saved me.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854987"><td></td></tr>
                  <tr id="37855478"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855478" href="https://news.ycombinator.com/vote?id=37855478&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I had tinnitus for 1 month or so when I was living in London and it went away on its own. Can't remember if it stopped before or after I moved to Germany. In London I was living in a busy area above a grocery shop overlooking a street with a lot of buses going past so there was a constant hum from below. Not sure if it was related but my tinnitus just wouldn't go away. I had had tinnitus before but only for short periods if time and not so intense; only if the environment around me was totally quiet and I was very tired.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853869"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853869" href="https://news.ycombinator.com/vote?id=37853869&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I'll use this thread as an opportunity to say this: if you ever have sudden hearing loss in one or both ears, seek immediate medical attention. Demand a proper consultation and don't take "give it a few weeks" as an answer from a GP.<p>Sudden hearing loss can be reversed with prompt steroid injections but if it's left then it will become permanent. My mother woke up one day with no hearing in one ear. Unfortunately, by the time she got a proper diagnosis it was too late to do anything about it. Since then she's had tinnitus and vertigo to go with it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855053"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855053" href="https://news.ycombinator.com/vote?id=37855053&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>So far I can see, sudden loss of any sense (gettin hearing, smell, sight) is always a cause for concern. I will request my doctor to do a deep investigation.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853961"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853961" href="https://news.ycombinator.com/vote?id=37853961&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>That’s a real thing. Sadly science is clueless about this and doctors even more. Specialized clinics threat sudden hearing loss with intravenous injection cocktails containing steroids and sedatives over the course of few weeks with mixed results. On other hand the cause is not well understood and probably universal treatment does not fit all cases.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855485"><td></td></tr>
            <tr id="37855146"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855146" href="https://news.ycombinator.com/vote?id=37855146&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Yeah! But I’m gonna say I’m really odd, it’s a lot of vibes and unless I convince a doctor of a case study, I’d never be believed.<p>I have/had Tinnitus (it’s nowhere near as bad as it was 7/10 to it’s current 1/10). It’s been gone 7-8 years now.</p><p>1. I listen to music much quieter, and let my ears adjust.</p><p>2. This is the weird part. I would get itchy all the time, random pin pricks I’d feel often. I read about monks who meditated so long they could turn off their hearts. So I sat in bed for 3 months (before going to sleep) and tried really hard to look at where I felt an itch and see it was my body was wrong. There was no reason to give me a cue to itch. Nothing was happening.</p><p>I can easily ‘feel’ the pin pricks if I desire but don’t anymore. It’s like a weird mental trick. I can also feel mosquitos and really anything touch me and no longer get false cues.</p><p>Anyhow I suffered from tinnitus and did the normal suggested stuff but it didn’t work. So I remembered the time I got rid of my itching and tried to replicate what I did.</p><p>I sat and listened to ‘true noise’ and untrue ‘noise’ and it wasn’t instant relief but over 2-3 weeks it went to level it’s at now. I only notice it if I desire. It’s louder an extremely quiet environment but I swear it’s almost like I can hear my blood pump.</p><p>Listen to really low noise. Quieter than whispers. Then up and up. Train your ear to understand sound and not sound. Then go back down again. Sadly the truest quiet will cost you (some place remote with no bugs or wind) but I did it fine at home because I could remember before I had tinnitus. Earplugs I think don’t work because you hear your blood pump.</p><p>Anyway I’m sorry you’re suffering and I know what I wrote sounds really dumb/unbelievable but I do pretty good on prediction markets… ;) it might work for you. Very weird - no proof in the literature but it worked me.</p><p>-(I did the itch cue training around 12 or 13, tinnitus around 22)
-(I didn’t use any drugs)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855253"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855253" href="https://news.ycombinator.com/vote?id=37855253&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>In my experience, early intervention can help. I got tinnitus at the age of 30 (from a pressure trauma during scuba diving).
I received some kind of dementia medication after 2 weeks, and the tinnitus was gone not long after that (and never reappeared). The doctor funnily said that a side effect is that I'm gonna feel smarter :)
I'm not sure if it was the medication, or the tinnitus was just temporary anyway, but it was a really loud and unbearable noise for a couple of weeks.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855356"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855356" href="https://news.ycombinator.com/vote?id=37855356&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>YES! Unfortunately I couldn't tell you how it went away as it just went away over time and never came back. Not even trolling.<p>Had a mild(?) case of tinnitus from about 17 to 20 years of age. Haven't really even thought about it since then but in my case the prime suspect was probably loud music. During that time period I was super depressed and angry at the world so I used to listen to loud heavy metal music nearly all day every day from my earbuds.</p><p>I stopped doing that because of the tinnitus, but it lingered on for long enough such that I can't draw a clear link between the music and tinnitus. Other potential suspects have been quitting dairy, cigarettes and weed, which I also quit in that same time period as the loud music.</p><p>If I had to guess, it was probably damaged stereocilia as @RelativeDelta mentioned.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855414"><td></td></tr>
                  <tr id="37854946"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854946" href="https://news.ycombinator.com/vote?id=37854946&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>At least for me, ignoring it seems to work as well as ignoring eye floaters seems to. They're there, but I can filter them out to the point where I forget that they're there.<p>That's it. It's not permanent, but I can forget it's there.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855132"><td></td></tr>
                  <tr id="37855420"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855420" href="https://news.ycombinator.com/vote?id=37855420&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I gave up a long time ago (almost 20yrs) after looking into it but recently I had a video pop up on my YT-short feed that looked interesting. It seemed scammy, and it was, but it got me looking more into it again.<p>I came across an FDA-approved device called lenire. Reviews seem hit or miss, but if I'm feeling flush I may try it. I can mostly put up with my tinnitus these days but it can get quite frustrating when tired &amp; I want to sleep.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853098"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853098" href="https://news.ycombinator.com/vote?id=37853098&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>It's impossible to cure.<p>The 'ringing' sound people hear isn't actually a sound. It is how the brain processes signals produced by damaged Stereocilia.</p><p>If the 'ringing' is constant it means the cilia are permanently damaged. While it would, in theory, be possible to use surgery on the ear and some sort of lazer to completely remove all damaged cilia to avoid them outputting a damaged signal, this procedure would be incredibly invasive and risky. I don't believe it's ever been done and i would find it hard to believe any Otolaryngologist willing to try.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37853613"><td></td></tr>
                <tr id="37854883"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37854883" href="https://news.ycombinator.com/vote?id=37854883&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I'm not a doctor but I have good hearing, and heard a friend's tinnitus. Apparently they had a constant muscle spasm that was causing a "buzzing" sound, and if you listened carefully, you could hear it. They eventually got it botoxed and that fixed it (injections 2-3 times a year I think, ongoing).</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37853537"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853537" href="https://news.ycombinator.com/vote?id=37853537&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>There's a trick which I can't recall the name of, which involves thumping your fingers across the back of your neck, which temporarily resolves tinnitus in some people.  I have very mild tinnitus, and have noticed that that does quiet it.<p>This is a question for everyone I suppose, but does anyone know why that works? Could it be possible to develop an implant or something which generates the same effect?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37853750"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37853750" href="https://news.ycombinator.com/vote?id=37853750&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Your tinnitus is likely being caused by tight sternocleidomastoid muscles (SCM). I would look into stretches to resolve that.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855050"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37855050" href="https://news.ycombinator.com/vote?id=37855050&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I thought that works because it creates a complex of sensations that overload your audio “tract”. For example if I do the described trick without isolating/covering my my ears with palms, it does nothing. The difference is not in pressure, but in the fact whether I can/can’t deep-hear the punches that my fingers create. Not a doctor, but something tells me there’s more to that. All scans shown that I have zig-zagged vessels in my neck, but within what they see as a “norm”.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853797"><td></td></tr>
                  <tr id="37853601"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37853601" href="https://news.ycombinator.com/vote?id=37853601&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>It works for people that have tinnitus due to tight neck muscles. Tinnitus due to damaged ears is a whole different beast.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37853725"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37853725" href="https://news.ycombinator.com/vote?id=37853725&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I went to a lot of loud concerts when I was younger and just assumed that I had tinnitus from those, and until now, I had no idea there were multiple causes of tinnitus.<p>I had assumed this was incurable but if my tinnitus is from tight neck muscles, that seems fixable. I'm going to reach out to my doctor about this. Thank you, I think you might have just greatly improved my life.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37853857"><td></td></tr>
                  <tr id="37853936"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853936" href="https://news.ycombinator.com/vote?id=37853936&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>&gt;While it would, in theory, be possible to use surgery on the ear and some sort of lazer to completely remove all damaged cilia to avoid them outputting a damaged signal, this procedure would be incredibly invasive and risky.<p>It sounds like what we need is nanobots to do this surgery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855075"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855075" href="https://news.ycombinator.com/vote?id=37855075&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Short answer is no.<p>But I have had complete temporary relief for a few days at a time that coincided with other signs of healing in my body, which hints at what could bring about permanent abatement.</p><p>I’ve had mild tinnitus since I was about 12-13yrs old (I’m in my 40s now).</p><p>It’s never been especially debilitating; it doesn’t impair my hearing or diminish quality of life, it’s just always there in the background.</p><p>Since about the same age I’ve had signs of inflammation in the digestive system/respiratory system and other mild/moderate symptoms that research suggests are related to microbiome issues - EBV, CMV, etc. at its worst it’s been like chronic fatigue or fibromyalgia, though not always and not these days.</p><p>I’ve tried a lot of things over the years to try and resolve these issues - diet/nutrition, detoxing, cleansing, infrared, occasionally more extreme things I won’t mention here. On a few precious occasions everything seemed to click into place and I just felt a really pleasant, energized feeling through my body, the signs of inflammation went away, my digestion improved and the tinnitus abated and my hearing was completely clear.</p><p>Every time however, the baseline symptoms, including the the tinnitus, returned within a few days and remain to this day (though probably at a milder level than a few years ago).</p><p>I don’t have any definitive takeaways from all this, but it suggests to me that if there’s a way to fully resolve chronic inflammation and microbiome issues (which are quite common), it may bring about a complete abatement of tinnitus.</p><p>(You can Google for CMV tinnitus/EBV tinnitus to find research papers and articles/discussions on correlations between these conditions).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853845"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853845" href="https://news.ycombinator.com/vote?id=37853845&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I had tinnitus for a while, and tried a few things that didn't work but this thomping technique I found on reddit one day stops it, and I just do it every couple of days and it disappears for a while. Not permanent but whenever it bothers me now I just do this thumping and it disappears in a few seconds:<p><a href="https://www.youtube.com/watch?v=2yDCox-qKbk">https://www.youtube.com/watch?v=2yDCox-qKbk</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854721"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37854721" href="https://news.ycombinator.com/vote?id=37854721&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Weird. I tried it and didn't get much effect, but it's interesting to see so many positive comments on it. I'll try it some more over the next couple days.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855465"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37855465" href="https://news.ycombinator.com/vote?id=37855465&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I think that's because tinnitus comes in very different forms and intensity, and since it can't be objectively measured, it's difficult to compare between individuals. My guess ist that you can only silence very low intensity tinnitus with this technique.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855293"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37855293" href="https://news.ycombinator.com/vote?id=37855293&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Try a constant pressure massage instead of the thumping. Pressure should be high enough that the sensation is intense but not painful. The goal is to get muscles around the base of your skull to relax. I was amazed at how quickly it had an effect</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="37853102"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853102" href="https://news.ycombinator.com/vote?id=37853102&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I've been dealing with tinnitus for about a year now. The only thing I've found that helps is Ginko Biloba. I'm buying that at a "Dollar General" store so the source is iffy and so is the effectiveness, but it certainly does help.<p>My wife has a friend who told her just a few days ago that her husband has had some success toning his done by chewing Bay leaves. I've not tried that yet, but I will soon.</p><p>From what I've read there is still no "cure". But those two things may contain a clue that's worth looking for.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855094"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855094" href="https://news.ycombinator.com/vote?id=37855094&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Still have it. However:<p>I worried about my tinnitus for so long. It drove me nuts at the beginning. Then one day I stopped worrying. I can't even tell you why, or when exactly that happened, just that it happened.</p><p>I remember reading all the comments saying "you'll get used to it", and then lying awake at night and thinking "I'll NEVER get used to it". It felt like I would have to accept a huge handicap. I really didn't understand how people could just say it doesn't bother them anymore. Really, that loud sound doesn't bother you?</p><p>And here I am today. I don't even think about it on most days. And most importantly, it does not feel like a handicap at all. It's mostly just gone. Still there and clearly audible when I think about it, but otherwise gone. Zero impact on my regular life, on my music listening or music making behaviour.</p><p>I also have a pretty strong hayfever allergy. Many people told me I'd get used to that, too, and I have to disagree: no, I don't get used to it, I'm 35 years old and it drives me nuts every single year. But the tinnitus is different. I have it for over 15 years now, and it just became irrelevant.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855159"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855159" href="https://news.ycombinator.com/vote?id=37855159&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>For hayfever, different kinds of solutions exist: "vaccines" can help in severe cases; antihistamine pills are the main solution, but I found that some don't work for me and others do: you should ask a doctor and try another one; cortison-based nasal spray and eyedrops also work very well for me.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855081"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855081" href="https://news.ycombinator.com/vote?id=37855081&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I developed T after a series of encounters with benzodiazepines to lessen my nerves when I was coping with burnout. I didn't know I had burnout, I didn't know what was happening to me, I just wanted it to stop. My tinnitus became unbearable in the weeks after a psychiatrist took me of an benzo and put me on an atypical antipsychotic medication. I always blamed the new medication as the cause of my symptoms (of which T was one) when in reality it was the cessation of the benzo.<p>After 5 years of Tinnitus-hell I was put on yet another benzo and my symptoms were _gone_. It lasted only a number of months, though and when I stopped taking the new benzo ... it all came back.</p><p>I came to the diagnosis of benzo-dependency myself and found a psychiatrist who agreed to assist in a 6-month withdrawal with a gradual taper. After each step my tinnitus came back a bit for a few days and then dropped again. After a number of iterations it became what is known as "baseline", i.e. a background tinnitus I can live with.</p><p>In recent months I started taking Magnesium supplements in the form of citrates and Taurine. I then found this on reddit:
<a href="https://www.reddit.com/r/tinnitus/comments/znyuyj/partial_success_with_ltheanine_and_magnesium/" rel="nofollow noreferrer">https://www.reddit.com/r/tinnitus/comments/znyuyj/partial_su...</a>
The top comment by kenzocarj is my current working model for dealing with Tinnitus. I'd like to try L-Theanine but it's illegal in my country - you can buy it in a form that has added glutamine which worsens my T. I'm now drinking matcha tea as a source of L-Theanine.</p><p>Hope this helps.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37855307"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855307" href="https://news.ycombinator.com/vote?id=37855307&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>If the symptoms go away with benzos, it seems that the source of your tinnitus is some form of generalised anxiety syndrome, with all the niceties that come with it: tension headaches, teeth clenching, TMJ disorder...
The burnout stresses were eased thanks to the pills, but once withdrawn came back probably harder which could have then triggered your tinnitus.<p>This is why benzos should never be prescribed without any other therapeutic approach alongside. They just hide the pain under the carpet for a while only to see it come back worse after and the root cause needs to be addressed with other means in the meantime.</p><p>Good luck with the road back to balance. There's no secret remedy and it's probably best to take a holistic approach for it and work on your well being from multiple fronts.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855135"><td></td></tr>
                <tr id="37855276"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37855276" href="https://news.ycombinator.com/vote?id=37855276&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>The magnesium had a dampening effect which started gradually after about 2-3 days - I take it daily around noon in a 400 mg dose. It also had a soothing effect on my restless leg syndrome (one of my other benzo-induced symptoms) which eventually disappeared altogether.<p>My Tinnitus is coupled to poor sleep (both quality &amp; quantity) and I noticed an improvement on that front as well after a few days of magnesium.</p><p>I started taking L-Theanine in the form of matcha tea together with my girlfriend.  Since then we're both experiencing vivid dreams. Unfortunately we've also started taking this at the same time before bed:
<a href="https://www.physalishealth.com/en/food-supplements/stress-sleep-good-mood/relax-and-sleep/" rel="nofollow noreferrer">https://www.physalishealth.com/en/food-supplements/stress-sl...</a>
... so we're not quite sure which one is improving the vivid dreams.</p><p>I started taking magnesium several weeks before I started with the tea and the relax &amp; sleep supplements. It was because of the apparent success with magnesium that I'm now exploring these paths. I'm not sure whether Taurine has any effect (the supplement I started after Magnesium and before L-theanine).
I'm sticking to the Glutamine-Glutamate-Gaba model and found this site that has a number of pointers and supplements in that area:
<a href="https://bebrainfit.com/glutamate-neurotransmitter/" rel="nofollow noreferrer">https://bebrainfit.com/glutamate-neurotransmitter/</a></p><p>I'd advise to try one at a time, around the same time every day and evaluate after a week. Start with Magnesium (not in the form of an oxide)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="37853969"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853969" href="https://news.ycombinator.com/vote?id=37853969&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I've had it since I can remember. One of my earliest memories, I was 1 or 2 years old, was "playing" with it while falling asleep at night: I'd focus on it and made the ringing grow louder and louder till it was the only sound in my ears, then it steadily grew quiter till I fell asleep.
Nowadays, I'm 27 years old, my brain filters it out, but I can still hear it when it's quite. Perhaps it's one of the reasons I always have some background lofi music playing.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37854971"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37854971" href="https://news.ycombinator.com/vote?id=37854971&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Also had it as long as I remember, at least since I was about 7 or so, which makes me think it's not related to high level noise exposure.  I know that because I have clear memories of asking other kids whether in times of silence they could hear anything.  It surprised me that they couldn't.<p>My tinnitus is a kind of hissing, predominantly at a very high pitch.  I'm in my mid 40's so grew up around CRT TVs. I think it's about the 15kHz horizontal refresh frequency, which makes me wonder whether my brain was trained at a young age to expect CRT TV sounds to be around and now it's just a permanent artefact, like a noise cancellation circuit that's gone wrong.</p><p>I find it gets worse if I'm run down, or eg. If I drink too much alcohol, or if I wear headphones for too long.</p><p>If I think about it, I can hear it at any time, but generally it's not too bad.  I know it's not very helpful, but acceptance of it and shifting focus to other things definitely helps me, to the point that it usually fades away and is generally not noticeable at all.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37854888"><td></td></tr>
                <tr id="37855262"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855262" href="https://news.ycombinator.com/vote?id=37855262&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Just tried this and it did temporarily stop the tone for a moment. Easy to do, instant relief. Simple massage to the muscles near the base of the skull.<p>A physical therapist commented that tight muscles contribute. Hopefully practicing this regularly will prolong the effects.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37854809"><td></td></tr>
            <tr id="37855260"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855260" href="https://news.ycombinator.com/vote?id=37855260&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>There is an entire system of Yoga called Nada Yoga; that deals with listening to so called internal music, usually described/experienced as high pitched tones/chords.<p>I have no idea how that relates to tinnitus; I hear high pitched tones/chords outside of meditation occasionally, but it's never bothering/disturbing.</p><p>My gut feeling says that sometimes, labeling an experience as a problem is why it becomes a problem.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855398"><td></td></tr>
            <tr id="37855312"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855312" href="https://news.ycombinator.com/vote?id=37855312&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I just realized reading all these replies with people who also suffer from it does somehow help. It helps to worry less, because evidently there's not much that can be done medically (at least for now). So, thanks for this!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855438"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855438" href="https://news.ycombinator.com/vote?id=37855438&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Yes, especially /u/n8henrie's comment. They have the expertise (assuming they are who they say they are), and have put in the time and research, and came to a conclusion. I'm not going to double up on their work.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855208"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855208" href="https://news.ycombinator.com/vote?id=37855208&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I’m almost completely deaf in one ear and had problems with tinnitus in the beginning (~7 years ago), especially when trying to sleep. Nowadays, I really only notice it when I focus on it, and it has no impact on my quality of life whatsoever. I think a lot of it is psychological.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37854839"><td></td></tr>
            <tr id="37855174"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855174" href="https://news.ycombinator.com/vote?id=37855174&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I had both high-pitch and pulsatile tinnitus after covid; benadryl often just turned off the high-pitch one within 30-60 minutes of taking it; pulsatile one went away after supplementing iron bisglycinate, lactoferrin and liposomal B12 for a while.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37854689"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854689" href="https://news.ycombinator.com/vote?id=37854689&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I had a period of bacterial imbalance in my gut caused by Plastocystis Hominis. At it's worst, I got periods of tinnitus. After being cured, took a year to find the cause, the tinnitus also disappeared. Worth checking out the bacterial balance in the gut and trying some probiotics.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37854902"><td></td></tr>
            <tr id="37853341"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853341" href="https://news.ycombinator.com/vote?id=37853341&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>One month was the worst point. It gets better from there until it's unnoticeable unless in a silent room. You will be surprised how the brain is able to both cancel out and then rebalance the sound input to normal (should your tinnitus be one-sided). Same goes for "cracks" that you might hear if the sounds are too loud -- they also go away in time.<p>I might as well note that anecdotally, barotrauma makes the tinnitus better temporarily. Maybe there is an explanation -- traumatic tinnitus can be at least alleviated by going into an overpressurization chamber immediately after the traumatic event. It seems like even after the fact, a pressure difference has temporary effect. For example, my tinnitus feels better after ascend on flights, even though in this case it's underpressure.</p><p>The best motivational video that I still think of after 10 years is this: <a href="https://youtu.be/eOU0JhkHY3w" rel="nofollow noreferrer">https://youtu.be/eOU0JhkHY3w</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855486"><td></td></tr>
            <tr id="37853806"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853806" href="https://news.ycombinator.com/vote?id=37853806&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>No permanent relief. It goes away when I meditate but that is temporary. The first few weeks I started vyvanse it completely disappeared, even if I focused on it, but once my body normalized to it, it came back.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37854932"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854932" href="https://news.ycombinator.com/vote?id=37854932&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I have mild tinnitus on one side. It doesn't cause too much discomfort but it's there. I am pretty sure it is caused by something pressing on a nerve on my neck so it's probably not the kind that most people have. I am hyphotizing that it would go away if I fixed the neck issue but that itself isn't so easy to do either.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855279"><td></td></tr>
            <tr id="37853643"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853643" href="https://news.ycombinator.com/vote?id=37853643&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Considering all of our hearing gets damaged slowly over the course of our lives, why does tinnitus only effect some people? I've had it for short periods of time after listening to loud live music, but hours later it just fades out to disappear. A musician once told me that it was the sound of your ear getting damaged and when you hear that you'll likely never hear those frequencies again.<p>I'm not convinced it's an entirely physical issue though. Our brains can filter out all kinds of excessive external stimuli both visual and from other senses so why not that ringing also?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37853665"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853665" href="https://news.ycombinator.com/vote?id=37853665&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>&gt;A musician once told me that it was the sound of your ear getting damaged and when you hear that you'll likely never hear those frequencies again.<p>This is a line in the film Children of Men (highly recommended), can't remember if it's in the book but either way it isn't correct.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853676"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853676" href="https://news.ycombinator.com/vote?id=37853676&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Definitely a mix of physical and nerve stimulation.<p>Either way temporary exposure can cause temporary issues, once it's permanent though it seems irreversible for now
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855044"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855044" href="https://news.ycombinator.com/vote?id=37855044&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I'm just speculating here. If somehow we could determine the exact waveform of the ringing, wouldn't be possible to just play the reverse waveform and cancel it out? Like how active noise cancelling headphones are doing.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="37855083"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855083" href="https://news.ycombinator.com/vote?id=37855083&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Tinnitus isn’t sound in the “vibrating molecules of air” sense—it’s a neurological phenomenon. Anything’s possible—but our perception of sound is not obligated to play by the same rules as physical sound does, and ANC relies on those rules.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855078"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855078" href="https://news.ycombinator.com/vote?id=37855078&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I have tinnitus which is within the frequency range I can hear, what I've found using headphones and a web tone generator is that I can establish a 'beat' frequency but I can't cancel it out entirely. That leads me to suspect that the frequency and phase of the tinnitus isn't consistent enough to cancel without actively tracking it, and how do you do that?</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855066"><td></td></tr>
            <tr id="37855062"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855062" href="https://news.ycombinator.com/vote?id=37855062&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Unlikely, because tinnitus is (from what I understand) a malfunction of the auditory part of the brain. There is nothing physical to cancel out.<p>Also, mine for example, is not completely static.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37855059" href="https://news.ycombinator.com/vote?id=37855059&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>My tinnitus, that my second covid infection gave me, is outside my current hearing frequency range.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="37853669"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853669" href="https://news.ycombinator.com/vote?id=37853669&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>In highschool I basically lost my hearing on one side. It went silent for a little bit and now I have a static/beeping on it almost constantly.<p>Had overpressure therapy in the weeks/months after but didn't help.
Have a bone anchored heading aid (via implanted screw) but doesn't work well.</p><p>Learned to live with it but mostly terrified if something will happen to my other ear... still have a few decades to go I hope, so better hold on to the hearing I have or science better hurry up
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37855321"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855321" href="https://news.ycombinator.com/vote?id=37855321&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>This is a little bit off topic but sort of related in how there seems to be at least 2 kinds of tinnitus which might have different cures, if any.<p>I have the kind I believe is called "somatic tinnitus". It's very high pitch and multiple tones. If I stretch my body or clench my teeth it becomes louder. It does not really sound like they come from one ear or the other.</p><p>It can also become louder if I'm drunk, or if I get a nicotine rush. It also becomes temporary louder if I listen to noise for a long time and then turn it off.</p><p>But from time to time I also get the occasional high pitch ringing in one of my ears that last for a few seconds. I've also gotten temporary tinnitus from a loud concert. While they are similar they seem different to me.</p><p>Just having casually talked to other people about this, I wonder if a lot of people have somatic tinnitus but are sensitive to it to varying degrees.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853306" href="https://news.ycombinator.com/vote?id=37853306&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>In my case it has 2 modes. Super loud and very attenuated. I still have no idea what triggers it, I thought it was loud noises, caffeine, tiredness etc, but seems like its not a 100% rule. I have been super tired and done all those things and suddenly its super attenuated. So it's super random. Im sure for certain people there must be something that helps reduce it, maybe something related to the nervous system. I honestly don’t mind it anymore as long as I can hear. You can learn to tune it out for sure</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853770"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853770" href="https://news.ycombinator.com/vote?id=37853770&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I have had very mild tinnitus my entire life - since as early as I can remember in childhood. It's never been particularly bothersome but in a quiet room I can clearly hear it.<p>However, when I received the second dose of the Pfizer mRNA vaccine in 2021, within a day I had a substantial increase in tinnitus in both ears, particularly my right. I also had a persistent sensation of "fullness" in my right ear.</p><p>I went to an ENT. They were pretty skeptical of the connection to the vaccine, said they couldn't see anything physically wrong with my ears, but we had a good conversation about tinnitus in general. They told me there wasn't much anyone could do, but anecdotally, they did think that many of their patients reported actual improvement from an OTC nutritional supplement, "Lipo-flavonoid", specifically marketed for tinnitus.</p><p><a href="https://www.amazon.com/Lipo-Flavonoid-Supplement-Recommended-Effective-Treatment/dp/B07GTHKNC6" rel="nofollow noreferrer">https://www.amazon.com/Lipo-Flavonoid-Supplement-Recommended...</a></p><p>I ordered a bottle and tried it out, and sure enough, I do think it made a difference - it did seem to quiet down the tinnitus, but not back to the level I was used to.</p><p>After about 2 months the tinnitus and sensation of fullness faded.</p><p>I elected not to get the booster.</p><p>Best of luck to you. I do suggest giving this supplement a try.</p><p>For those of you skeptical about any connection to the covid vaccines, you might find this article interesting quoting the Editor-in-Chief of the journal VACCINE and head of vaccine research at Mayo Clinic. <a href="https://www.upi.com/Health_News/2022/08/08/tinnitus-widespread-study/6281659985508/" rel="nofollow noreferrer">https://www.upi.com/Health_News/2022/08/08/tinnitus-widespre...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854895"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37854895" href="https://news.ycombinator.com/vote?id=37854895&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Anecdotally I had a similar experience but much more short-lasting. First and second dose caused pretty severe flu-like-symptoms and ringing in my ears for a few days. I'm prone to ear infections and it was very much like that. Thankfully both times it went back to normal in a week or so.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853930"><td></td></tr>
                <tr id="37853973"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_37853973" href="https://news.ycombinator.com/vote?id=37853973&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I recall some fatigue and maybe light muscle soreness or something like that, but pretty mild symptoms overall.<p>I’d say I get a bad cold once or twice a year and rarely experience anything more severe than that.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854033"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_37854033" href="https://news.ycombinator.com/vote?id=37854033&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I wondered if the vaccine flu symptoms might've created congestion in your middle ear but sounds like it didn't.<p>Perhaps you could do an experiment on yourself with the supplement to figure out if it's really having an effect or just placebo.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="37854575"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37854575" href="https://news.ycombinator.com/vote?id=37854575&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>I had this for months when I stopped taking benzos. I remember it made all the music I loved sound out of pitch and awful. It kind of makes you feel out of touch with the world too when you're getting this weird distorted version of it. All I can say is I hope you can learn some good coping strategies. I don't have this symptom any more but I sympathize.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37855125"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855125" href="https://news.ycombinator.com/vote?id=37855125&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>There are upcoming meds that do help but it's still in trials such as RL-81<p>It's gonna be a while
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853675"><td></td></tr>
            <tr id="37853740"><td></td></tr>
            <tr id="37853392"><td></td></tr>
            <tr id="37854026"><td></td></tr>
                <tr id="37854755"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37854755" href="https://news.ycombinator.com/vote?id=37854755&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I'd heard about this. Their studies seem to have mixed results, and there's mixed anecdotal evidence on /r/tinnitus at best. I looked into it, there's a clinic near me that offers it, but it's $5000!<p>My tinnitus isn't so bad that I feel compelled to drop 5g on fixing it, but I think their idea seems promising and I hope they continue to research it and find new ways to treat it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37853799"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853799" href="https://news.ycombinator.com/vote?id=37853799&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Tried a bit for almost a year, got myself rested - realised mine was low enough to barely show up in the test.<p>At that point I started to ignore it, and it works well enough. Except for when I am trying to fall asleep. I sometimes go by many weeks before I notice it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="37853816"><td></td></tr>
            <tr id="37853332"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853332" href="https://news.ycombinator.com/vote?id=37853332&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>i think there is research being done on cilia regeneration:
<a href="https://news.mit.edu/2022/frequency-therapeutics-hearing-regeneration-0329" rel="nofollow noreferrer">https://news.mit.edu/2022/frequency-therapeutics-hearing-reg...</a><p>obviously time to get approved/confirm it works - etc lies ahead.</p><p>other than that - i had taken up drums and was paranoid of tinnitus, i could hear ringing etc.  once i got audiologist confirmation my hearing was 20/20 guess what? It went away.  Anxiety levels can affect it...obviously it was mild for me - chronic sufferers wont get away with "dont worry about it and it will go away".
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37853433"><td></td></tr>
            <tr id="37853560"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853560" href="https://news.ycombinator.com/vote?id=37853560&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>&gt; audiologist confirmation my hearing was 20/20<p>it’s possible you didn’t go to an audiologist.</p><p>(/s)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854853"><td></td></tr>
            <tr id="37853892"><td></td></tr>
            <tr id="37853645"><td></td></tr>
                        <tr id="37853638"><td></td></tr>
                <tr id="37853931"><td></td></tr>
            <tr id="37853974"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37853974" href="https://news.ycombinator.com/vote?id=37853974&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Monarda is many plants. Any one of them in particular?<p>Will I get the same effect drinking  Earl Grey tea?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37853629"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853629" href="https://news.ycombinator.com/vote?id=37853629&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Not yet. I have absolutely insane tinnitus -- crazy high pitched, can hear it over any sound, too many tones to count. Also have hyperacusis but that's another topic. In any case, historically I've paid a lot of attention to this space.<p>Perhaps the most notable attempt at a cure was Frequency Therapeutics, whose efforts were promising, but unfortunately didn't pan out.</p><p>There's a constant churn of biotech companies trying to solve this, as curing tinnitus (and/or hearing loss) would be a goldmine, but no one has really gotten anywhere yet.</p><p>If you want to stay on top of it, the /r/tinnitusresearch is a pretty good hub, and there are a few mailing lists out there.</p><p>That said, over time I found that ceasing to follow all the promises and inevitable disappointments helped me better achieve the only real "treatment" we have right now, which is accepting it and slowly learning to live with it.</p><p>When there's a real deal cure, you'll know about it. Until then, I suggest not paying attention to the churn and the extremely dubious "treatments" out there.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="37854784"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_37854784" href="https://news.ycombinator.com/vote?id=37854784&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>Good advice. I've had it for a few years now and mostly learned early on that the best strategy is to just try to live your life and not pay too much attention to it. Occasionally I hear about some "therapies" though and I figured trying to ask the community and specifically asking about "full relief" would be the best way to know for sure if I was leaving anything on the table.<p>I read about the Frequency Therapeutics stuff, and saw the initial promise and subsequent disappointment. Oh well, maybe next year :)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="37855190"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37855190" href="https://news.ycombinator.com/vote?id=37855190&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><br><div>
                  <p><span>Nope! Protect your hearing kids, by the time you notice the problem it's far too late (but we also live in an age of miracles, I'm hoping some type of drug will turn up within my lifetime - if Semaglutide can exist, so can this).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="37853947"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_37853947" href="https://news.ycombinator.com/vote?id=37853947&amp;how=up&amp;goto=item%3Fid%3D37852711"></a></center>    </td><td><p><span>I had mild tinnitus years ago, but it went away over time while I happened to be living in a mostly silent environment off-grid near JTNP.<p>No idea if the long-term quiet environment played a role, but I also occasionally would supplement with sublingual b12[0] methylcobalamin supplements.</p><p>It's impossible to say what's responsible, there's far too many variables.  Maybe just time passing since quitting riding motorcycles is all that was needed.</p><p>[0] <a href="https://en.wikipedia.org/wiki/Vitamin_B12_deficiency" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Vitamin_B12_deficiency</a>  (search in page for tinnitus)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using OpenBSD Relayd(8) as an Application Layer Gateway (121 pts)]]></title>
            <link>https://www.tumfatig.net/2023/using-openbsd-relayd8-as-an-application-layer-gateway/</link>
            <guid>37852135</guid>
            <pubDate>Thu, 12 Oct 2023 00:34:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tumfatig.net/2023/using-openbsd-relayd8-as-an-application-layer-gateway/">https://www.tumfatig.net/2023/using-openbsd-relayd8-as-an-application-layer-gateway/</a>, See on <a href="https://news.ycombinator.com/item?id=37852135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><h4><i></i>&nbsp;
<time datetime="2023-10-10">2023-10-10</time>
&nbsp;<i></i>&nbsp;</h4><nav id="TableOfContents"><ol><li><a href="#what-is-relayd8">What is relayd(8)?</a></li><li><a href="#how-to-manage-relayd8">How to manage relayd(8)?</a></li><li><a href="#terminology">Terminology</a></li><li><a href="#simplest-http-relay">Simplest HTTP relay</a></li><li><a href="#better-simple-http-relay">Better simple HTTP relay</a></li><li><a href="#encrypt-http-relay-using-transport-layer-security-tls">Encrypt HTTP relay using Transport Layer Security (TLS)</a></li><li><a href="#load-balancing--failover">Load balancing &amp; Failover</a></li><li><a href="#fallback-servers---automatic-switch">Fallback server(s) - automatic switch</a></li><li><a href="#fallback-servers---manual-switch">Fallback server(s) - manual switch</a><ol><li><a href="#initial-expected-state">Initial expected state</a></li><li><a href="#primary-servers-go-down">Primary servers go down</a></li><li><a href="#switch-to-the-secondary-servers-pool">Switch to the secondary servers pool</a></li></ol></li><li><a href="#relaying-multiple-fqdns">Relaying multiple FQDNs</a></li><li><a href="#relaying-multiple-pathnames">Relaying multiple pathnames</a></li><li><a href="#solving-problems-with-http-headers">Solving problems with HTTP headers</a><ol><li><a href="#unencrypted-connection">Unencrypted connection</a></li><li><a href="#leaking-headers">Leaking headers</a></li><li><a href="#improve-users-security-and-privacy">Improve users security and privacy</a></li></ol></li><li><a href="#log-management">Log management</a><ol><li><a href="#one-for-log-and-log-for-one">One for log and log for one</a></li><li><a href="#logging-options">Logging options</a></li></ol></li><li><a href="#conditional-filtering">Conditional filtering</a><ol><li><a href="#filtering-branched-on-fqdn-12">Filtering branched on FQDN #(1,2)</a></li><li><a href="#filtering-branched-on-fqdn-3">Filtering branched on FQDN #3</a></li><li><a href="#filtering-branched-on-fqdn-4">Filtering branched on FQDN #4</a></li><li><a href="#connecting-the-branches">Connecting the branches</a></li></ol></li><li><a href="#one-more-thing">One more thing</a></li></ol></nav><p>I was lucky enough to attend to EuroBSDCon 2023 and offered the
opportunity to talk about one of my favorite OpenBSD stock daemon:
relayd(8).</p><p>The talk was recorded and made <a href="https://www.youtube.com/watch?v=yW8QSZyEs6E" target="_blank" rel="external nofollow">available on the EuroBSDCon YouTube
channel.</a>
. One may check
the <a href="https://2023.eurobsdcon.org/program.html" target="_blank" rel="external nofollow">EuroBSDCon 2023 program</a>
for more material.</p><p>This post attempts a reboot of the slides content in a more
browser-friendly format.</p><h2 id="what-is-relayd8">What is relayd(8)?</h2><ul><li>Multi-purpose daemon available on OpenBSD since 4.3*:<ul><li>load-balancer.</li><li>application layer gateway.</li><li>transparent proxy.</li></ul></li><li>Capable of monitoring groups of hosts for high-availability.</li><li>Operates as:<ul><li>Layer 3 redirection via communication with pf(4).</li><li>Layer 7 relaying with application level filtering via itself.</li></ul></li></ul><p><a target="_blank" href="https://www.tumfatig.net/images/2023/relayd-rp.png"><img src="https://www.tumfatig.net/images/2023/relayd-rp.png" alt="Using relayd(8) as a reverse-proxy"></a></p><p>* relayd was known as hoststated in OpenBSD 4.1.</p><h2 id="how-to-manage-relayd8">How to manage relayd(8)?</h2><p>The man pages are a must read before proceeding further.</p><pre tabindex="0"><code># man relayd
# man relayd.conf
# man relayctl
</code></pre><p>The configuration file is expected in the standard <code>etc</code> directory. An
example is available if you need more inspiration.</p><pre tabindex="0"><code># more /etc/examples/relayd.conf
# vi /etc/relayd.conf
</code></pre><p>One can check for configuration errors using:</p><pre tabindex="0"><code># relayd -dvn
</code></pre><p>The service is enabled and started using the standard <code>rcctl</code> utility:</p><pre tabindex="0"><code># rcctl enable relayd
# rcctl start relayd
# rcctl stop relayd
</code></pre><p>A dedicated command can be used to get more information about relayd(8)
state and apply specific actions:</p><pre tabindex="0"><code># relayctl 𝘤𝘰𝘮𝘮𝘢𝘯𝘥 [𝘢𝘳𝘨𝘶𝘮𝘦𝘯𝘵 ...]
</code></pre><p>More on this later on.</p><h2 id="terminology">Terminology</h2><ul><li><p>Macros: user-defined variables that can be used later on.</p></li><li><p>Tables: host or a group of hosts defining traffic targets.</p></li><li><p>Protocols: settings and filter rules for relays.</p></li><li><p>Relays: layer 7 proxying instances.</p></li></ul><h2 id="simplest-http-relay">Simplest HTTP relay</h2><p>This is the simplest HTTP Reverse proxy configuration that you can get:</p><pre tabindex="0"><code data-lang="pf">http protocol www {
    pass
}

relay www {
    listen on 203.0.113.1 port 80
    protocol www
    forward to 192.0.2.10 port 80
}
</code></pre><p>The first section defines an HTTP PROTOCOL object. Name has been set to
‘www’ but it can be anything.</p><p>The RELAY section defines a listening address and port. It links the
relay to the previously configured protocol. It defines the backend
server that will receive the HTTP requests.</p><h2 id="better-simple-http-relay">Better simple HTTP relay</h2><p>This example expands the previous HTTP Reverse proxy configuration with
usage of reusable variables (MACROS) and logging of state changes and
remote connections.</p><pre tabindex="0"><code data-lang="pf"># Macros -----------------------------------
ext_addr="203.0.113.1"
webhost1="192.0.2.10"

# Global configuration ---------------------
log state changes
log connection

# Tables -----------------------------------
table &lt;webhosts&gt; { $webhost1 }

# Protocols &amp; Relays -----------------------
http protocol www {
    pass
}

relay www {
    listen on $ext_addr port 80
    protocol www

    forward to &lt;webhosts&gt; port 80
}
</code></pre><h2 id="encrypt-http-relay-using-transport-layer-security-tls">Encrypt HTTP relay using Transport Layer Security (TLS)</h2><p>Previous examples are using plain text HTTP. Switching to HTTPS provides
secure communication and data transfer between the client and the
website.</p><p>You’ll need to acquire a TLS certificate. That steps is beyong the scope
of this post. But have a look at acme-client(1) and httpd(8) manpages.
Those will guide you through the process of getting an HTTPS certificate.</p><p>Once acquired, install the certificate under <code>/etc/ssl</code>. If you used
acme-client(1), you should get files such as:</p><pre tabindex="0"><code>/etc/ssl/private/relayd.example.key
/etc/ssl/relayd.example.crt
</code></pre><p>Reference the certificate name in the protocol section. Then replace the
listen directive of the relay section to specify the usage of <code>tls</code>.</p><pre tabindex="0"><code data-lang="pf"># Macros -----------------------------------
ext_addr="203.0.113.1"
webhost1="192.0.2.10"

# Global configuration ---------------------
log state changes
log connection

# Tables -----------------------------------
table &lt;webhosts&gt; { $webhost1 }

# Protocols &amp; Relays -----------------------
http protocol wwwtls {
    tls keypair relayd.example
}

relay wwwtls {
    listen on $ext_addr port 443 tls
    protocol wwwtls

    forward to &lt;webhosts&gt; port 80
}
</code></pre><p>Renaming the protocol and relay names is not mandatory. I only did it to
make it clear what I’m doing.</p><h2 id="load-balancing--failover">Load balancing &amp; Failover</h2><p>relayd(8) allows to distribute incoming requests to several backend
servers. Depending on its configuration, you can balance the load on
those servers and/or keep the service up and running since you only
encounted n-1 failure(s).</p><pre tabindex="0"><code data-lang="pf">ext_addr="203.0.113.1"
whost1="192.0.2.11"
whost2="192.0.2.12"
whost3="192.0.2.13"
interval 5
table &lt;webhosts&gt; { $whost1, $whost2, $whost3 }

http protocol wwwtls {
   tls keypair relayd.example
}

relay wwwtls {
    listen on $ext_addr port 443 tls
    protocol wwwtls
    # l/b using source-IP, check HTTP return code
    forward to &lt;webhosts&gt; port 80  \
      mode loadbalance             \
      check "/health-check" code 200
}
</code></pre><p>In this example, a TABLE has been created that references all the backend
servers - those that will receive the HTTP requests.</p><p>There are many scheduling algorithms (aka MODE) available. Check the man
page for more details. The default is using roundrobin and no health
checks. Here, we’re using the loadbalance algorythm and return code check.</p><h2 id="fallback-servers---automatic-switch">Fallback server(s) - automatic switch</h2><p>There are cases when you want to implement automatic reaction on
server(s) outage events. You may want to switch the whole service to a
secondary server pool. You may display an incident status page
rather that an HTTP/500 error page. You should probably display a static
“be back soon” page while performing maintenance.</p><p>This is what the fallback feature can be used for.</p><pre tabindex="0"><code data-lang="pf">ext_addr="203.0.113.1"
whost1="192.0.2.11"
whost2="192.0.2.12"
whost3="192.0.2.13"
interval 5
table &lt;webhosts&gt; { $whost1, $whost2 }
table &lt;fallback&gt; { $whost3 }

http protocol wwwtls {
   tls keypair relayd.example
}

relay wwwtls {
  listen on $ext_addr port 443 tls
  protocol wwwtls

  # l/b using round-robin, check HTTP return code
  forward to &lt;webhosts&gt; port 80 mode roundrobin \
    check http "/" code 200
  # switch service if all previous checks fail
  forward to &lt;fallback&gt; port 80
}
</code></pre><p>Two TABLEs have been defined. One for the primary server(s). One for the
fallback server(s). Then, everything happens in the relay section. The
first forward directive load-balances the HTTP requests to the primary
servers pool. The second forward directive acts as the fallback target.
It will be triggered as soon as no servers from the primary pool are
known to be working.</p><h2 id="fallback-servers---manual-switch">Fallback server(s) - manual switch</h2><p>In a use-case where you prefer managed operations on server(s) outage,
you may configure a non-automatic switch. This mostly apply to Business
Continuity Plan where the secondary servers pool is remote or mutualized
or resources limited etc.</p><pre tabindex="0"><code data-lang="pf">ext_addr="203.0.113.1"
whost1="192.0.2.11"
whost2="192.0.2.12"
whost3="192.0.2.13"
whost4="192.0.2.14"
interval 5
table &lt;webhosts&gt;         { $whost1, $whost2 }
table &lt;fallback&gt; disable { $whost3, $whost4 }

http protocol wwwtls {
   tls keypair relayd.example
}

relay wwwtls {
  listen on $ext_addr port 443 tls
  protocol wwwtls

  # l/b using source-IP, check HTTP return code
  forward to &lt;webhosts&gt; port 80 mode loadbalance \
    check http "/" code 200
  # l/b using round-robin, check HTTP return code
  forward to &lt;fallback&gt; port 80 mode roundrobin \
    check http "/" code 200
}
</code></pre><p>The main difference with the previous configuration is the <code>disable</code>
property of the fallback table. This implies that it won’t be used by
relayd(8) unless being told to.</p><h2 id="initial-expected-state">Initial expected state</h2><p>Usage of the <code>relayctl</code> command confirms that the primary servers pool
is working as expected.</p><pre tabindex="0"><code># relayctl show summary
Id      Type            Name                            Avlblty Status
1       relay           wwwtls                                  active
1       table           webhosts:80                             active (2 hosts)
1       host            192.0.2.11                      100.00% up
2       host            192.0.2.12                      100.00% up
2       table           fallback:80                             disabled
</code></pre><ul><li>Primary hosts are up and running.</li><li>Secondary hosts are disabled.</li></ul><p>The service is UP.</p><h2 id="primary-servers-go-down">Primary servers go down</h2><p>On a clear service breakdown, <code>relayctl</code> will indicate that the primary
hosts are down.</p><pre tabindex="0"><code># relayctl show summary
Id      Type            Name                            Avlblty Status
1       relay           wwwtls                                  active
1       table           webhosts:80                             empty
1       host            192.0.2.11                      95.56%  down
2       host            192.0.2.12                      95.56%  down
2       table           fallback:80                             disabled
</code></pre><ul><li>Primary hosts are down.</li><li>Secondary hosts are disabled.</li></ul><p>The service is DOWN. Nothing happens as relayd(8) was told to start the
fallback table disabled.</p><h2 id="switch-to-the-secondary-servers-pool">Switch to the secondary servers pool</h2><p><code>relayctl</code> is used to enable the disabled fallback. This happens
without the need of restarting relayd.</p><pre tabindex="0"><code># relayctl table enable 2
command succeeded

# relayctl show summary
Id      Type            Name                            Avlblty Status
1       relay           wwwtls                                  active
1       table           webhosts:80                             empty
1       host            192.0.2.11                      76.79%  down
2       host            192.0.2.12                      76.79%  down
2       table           fallback:80                             active (2 hosts)
3       host            192.0.2.13                      100.00% up
4       host            192.0.2.14                      100.00% up
</code></pre><ul><li>Primary hosts are down.</li><li>Secondary hosts are enabled.</li></ul><p>The service is UP.</p><p>Note that failback shall happen as soon as relayd detects a Primary host
up. If this is not something you want to happen, use <code>relayctl table disable 1</code> to prevent such an automatic failback.</p><h2 id="relaying-multiple-fqdns">Relaying multiple FQDNs</h2><p>What if you want to expose multiple hostnames using a single IP?<br>You can do Apache Virtual Hosts or nginx server blocks. Or you can:</p><pre tabindex="0"><code data-lang="pf">(...)
table &lt;blog&gt;  { $whost1, $whost2 }
table &lt;cloud&gt; { $whost3 }

http protocol wwwtls {
  tls keypair blog.example
  tls keypair nextcloud.example

  block
  pass request header "Host" value "blog.example"  \
    forward to &lt;blog&gt;
  pass request header "Host" value "cloud.example" \
    forward to &lt;cloud&gt;
}

relay wwwtls {
  listen on $ext_addr port 443 tls
  protocol wwwtls
  forward to &lt;blog&gt;  port 80 mode roundrobin \
    check http "/" code 200
  forward to &lt;cloud&gt; port 80
}
</code></pre><p>This configuration example checks every HTTP requests’ header and route
them to the proper backend server depending on the value of the “Host”
header. The backend servers are referenced in tables.</p><p>Using multiple “keypair” directives to reference the HTTPS certificates
enables the TLS Server Name Indication (SNI) feature of relayd(8).</p><h2 id="relaying-multiple-pathnames">Relaying multiple pathnames</h2><p>Apache has location directives. nginx has location blocks. To design
reaction rules (allow, deny, forward…) depending on URL paths,
relayd(8) can use FILTER RULES based on the “path” keyword.</p><pre tabindex="0"><code data-lang="pf">(...)
table &lt;blog&gt;  { $whost1, $whost2 }
table &lt;cloud&gt; { $whost3 }

http protocol wwwtls {
  tls keypair relayd.example

  block quick path "/cgi-bin*"
  block quick path "/wp-admin*"
  pass  quick path "/nextcloud/*" forward to &lt;cloud&gt;
  pass  request                   forward to &lt;blog&gt;
}

relay wwwtls {
  listen on $ext_addr port 443 tls
  protocol wwwtls

  forward to &lt;blog&gt; port 80 mode roundrobin \
    check http "/" code 200
  forward to &lt;cloud&gt; port 80
}
</code></pre><p>This configuration blocks any attempt to access paths that look like
<code>/cgi-bin</code> and <code>/wp-admin</code>. It also routes any URL matching
<code>https://relayd.example/nextcloud/</code> to the “cloud” servers pool. Any
other URL will be routed to the “blog” table.</p><p>relayd(8) can add, remove or modify HTTP header on the fly. This allows
solving various kinds of issues with exposed Web services.</p><h2 id="unencrypted-connection">Unencrypted connection</h2><p>There are software like Baikal, Mastodon or SearxNG that refuse to serve
unencrypted content. If you still want to run them using plain text HTTP
on the backend server and feel confident about using relayd(8) as an SSL
terminator, you shall add an HTTP header to the requests reaching the backend
HTTP server.</p><pre tabindex="0"><code data-lang="pf">(...)
http protocol wwwtls {
  tls keypair blog.example
  tls keypair nextcloud.example

  block
  pass request header "Host" value "blog.example"  forward to &lt;blog&gt;
  pass request header "Host" value "cloud.example" forward to &lt;cloud&gt;

  match request header set "X-Forwarded-Proto" value "https"
}
(...)
</code></pre><p>In this particular case, the <code>X-Forwarded-Proto</code> is set to “https” and
passed to the backend server to confirm that the communication is
secured using TLS.</p><p>From time to time, you discover that Web services fill their HTTP
replies with too many information. Too many meaning not mandatory to
provide a functionnal user experience while still leaking sensible
information to the external world.</p><pre tabindex="0"><code data-lang="pf">(...)
http protocol wwwtls {
  tls keypair relayd.example

  pass  quick path "/nextcloud/*" forward to &lt;cloud&gt;
  pass  request                   forward to &lt;blog&gt;

  match response header remove "X-Powered-By"
  match response header set "Server" value "Microsoft-IIS/8.5"
}
(...)
</code></pre><p>This configuration removes any <code>X-Powered-By</code> information from every HTTP
replies. It also sets the <code>Server</code> HTTP header to some specific value that
can be used to fool script kiddies ; or to deal with faulty Web clients
that expects a specific value.</p><h2 id="improve-users-security-and-privacy">Improve users security and privacy</h2><p>Some software don’t really bother about security and privacy. Sometimes, it’s
just that they expect you to use an htaccess-compatible Web server to provide
a couple of HTTP headers that can help protecting the user.</p><pre tabindex="0"><code data-lang="pf">(...)
http protocol wwwtls {
  tls keypair relayd.example

  pass  quick path "/nextcloud/*" forward to &lt;cloud&gt;
  pass  request                   forward to &lt;blog&gt;

  match response header set "X-XSS-Protection"       value "1; mode=block"
  match response header set "X-Content-Type-Options" value "nosniff"
  match response header set "Permissions-Policy"     value "accelerometer=(),
ambient-light-sensor=(),autoplay=(),camera=(),encrypted-media=(),
focus-without-user-activation=(),geolocation=(),gyroscope=(),
magnetometer=(),microphone=(),midi=(),payment=(),picture-in-picture=(),
speaker=(),sync-xhr=(),usb=(),vr=()"
}
(...)
</code></pre><p>This example only shows a subset of HTTP headers that can be set to
improve users privacy and protection. Check your Web application using
tools like <a href="https://observatory.mozilla.org/" target="_blank" rel="external nofollow">Mozilla Observatory</a>
to
get a better overview of what should / could be achieved on your server.</p><h2 id="log-management">Log management</h2><p>The default relayd(8) log configuration doesn’t suit me well. Traces
appear in several log files and are not detailed enough to my linkings
when it comes to debugging issues.</p><h2 id="one-for-log-and-log-for-one">One for log and log for one</h2><p>To have relayd(8) log in its own dedicated log file, I’d rather tune
syslogd(8):</p><pre tabindex="0"><code># touch /var/log/relayd

# vi /etc/syslog.conf
!!relayd
*.* /var/log/relayd
!*
(...)

# vi /etc/newsyslog.conf
(...)
/var/log/relayd root:_relayd 640 7 * $D0 ZB

# rcctl restart syslogd
</code></pre><h2 id="logging-options">Logging options</h2><p>Some matching FILTER RULES can be defined to have more information
appear in the log files.</p><pre tabindex="0"><code data-lang="pf">http protocol wwwtls {
  tls keypair relayd.example

  match url log
  match header log "Host"
  match header log "User-Agent"
  match response header log "Content-Type"
  match response header log "Content-Length"

  pass  quick path "/nextcloud/*" forward to &lt;cloud&gt;
  pass  request                   forward to &lt;blog&gt;
}
</code></pre><p>This particular example provides HTTP header information in the
relayd(8) logs.</p><pre tabindex="0"><code>Sep 17 14:35:12 ebsdc relayd[34137]: relay wwwtls, session 1 (1 active), 0, 
    203.0.113.1 -&gt; 127.0.0.1:80, done, 
    [blog.example/about] [Host: blog.example] [User-Agent: curl/8.2.0] 
    GET -&gt; 127.0.0.1:80 {Content-Type: text/html} {Content-Length: 41};
</code></pre><p>Any HTTP header can be matched and rendered in the logs. Select yours.</p><h2 id="conditional-filtering">Conditional filtering</h2><p>Using TAGS and INCLUDES, relayd(8) can perform different computation and
actions depending on wether or not conditions evaluate to true or false.
Here’s an example of some slightly complex conditional filtering that
can be designed in relayd(8).</p><p><a target="_blank" href="https://www.tumfatig.net/images/2023/relayd-cf.png"><img src="https://www.tumfatig.net/images/2023/relayd-cf.png" alt="Using relayd(8) as a reverse-proxy"></a></p><h2 id="filtering-branched-on-fqdn-12">Filtering branched on FQDN #(1,2)</h2><p>In a dedicated configuration file, a TAG is set if the “Host” HTTP
header of the requests matches one of the defined value. Then for each
TAGGED connections, relayd(8) will apply additionnal logging and improve
users security and privacy.</p><pre tabindex="0"><code data-lang="pf"># cat /etc/relayd-ssg.conf

# Mark using hostnames
match request header "Host" value "www.example"     tag "ssg"
match request header "Host" value "blog.example"    tag "ssg"

# Apply additionnal logging
match header log "Host"       tagged "ssg"
match header log "User-Agent" tagged "ssg"
match url    log              tagged "ssg"

# Improve Security and Privacy
match response tagged "ssg" header set \
  "Strict-Transport-Security" value "max-age=31536000; includeSubDomains; preload"
match response tagged "ssg" header set \
  "X-XSS-Protection" value "1; mode=block"
match response tagged "ssg" header set \
  "X-Content-Type-Options" value "nosniff"
</code></pre><h2 id="filtering-branched-on-fqdn-3">Filtering branched on FQDN #3</h2><p>In a dedicated configuration file, a TAG is set if the “Host” HTTP
header of the requests matches the defined value. Then for each TAGGED
connections, a check is done on the “User-Agent” HTTP header and a block
happens for the referenced values. Another check is done, based on the
URL and the source IP, to ensure only trusted computer can acces an
adminstrative URL. In the end, any acceptable HTTP session will have the
“Server” HTTP header removed from the HTTP reply.</p><pre tabindex="0"><code data-lang="pf"># cat /etc/relayd-nextcloud.conf

# Mark using hostname
match request header "Host" value "cloud.example"   tag "nextcloud"

# Block User Agents
block request quick tagged "nextcloud" header "User-Agent" value "Googlebot/*"
block request quick tagged "nextcloud" header "User-Agent" value "YandexBot/*"

# Only allow "admin" path from specific subnet
match request                   url "cloud.example/admin/" tag "forbidden"
match request from 192.0.2.0/24 url "cloud.example/admin/" tag "nextcloud"

# Don't let version leak via HTTP header
match response tagged "nextcloud" header remove "Server"
</code></pre><h2 id="filtering-branched-on-fqdn-4">Filtering branched on FQDN #4</h2><p>In a dedicated configuration file, a TAG is set if some trusted computer
try to access an allowed FQDN. Any TAGGED connections will have its path
checked for some regex and tagged if matched. Those connections will
have an HTTP header set in their reply.</p><pre tabindex="0"><code data-lang="pf"># cat /etc/relayd-grafana.conf

# Mark using client source IP and path
match request from 192.0.2.0/24    url "metrics.example/" tag "grafana"
match request from 198.51.100.8/32 url "metrics.example/" tag "grafana"

# Overwrite caching
match request tagged "grafana" path "*.css" tag "g-cache"
match request tagged "grafana" path "*.js"  tag "g-cache"
match request tagged "grafana" path "*.png" tag "g-cache"

match response tagged "g-cache" header set "Cache-Control" value "max-age=86400"
</code></pre><h2 id="connecting-the-branches">Connecting the branches</h2><p>In the main relayd(8) configuration file, the dedicated config files are
included so that tagging happens, or not. The routing to the proper
backend servers will happen for every TAGGED connections. The <code>block</code>
directive will drop any connection that has not been matched by the
filtering definition.</p><pre tabindex="0"><code data-lang="pf"># cat /etc/relayd.conf

table &lt;blog&gt;    { $whost1, $whost2 }
table &lt;cloud&gt;   { $whost3 }
table &lt;grafana&gt; { $whost4 }

http protocol wwwtls {
  tls keypair www.example
  tls keypair cloud.example
  tls keypair metrics.example

  block

  include "/etc/relayd-ssg.conf"
  include "/etc/relayd-nextcloud.conf"
  include "/etc/relayd-grafana.conf"

  pass request tagged "ssg"       forward to &lt;blog&gt;
  pass request tagged "nextcloud" forward to &lt;cloud&gt;
  pass request tagged "grafana"   forward to &lt;grafana&gt;
  pass request tagged "g-cache"   forward to &lt;grafana&gt;
}
</code></pre><h2 id="one-more-thing">One more thing</h2><p>Remarks and answers given during the live session:</p><ul><li>Beware that pf(4) and relayd(8) TABLES are not the same things although they
share the same name.</li><li>PROTOCOL and RELAY names can be any string you like. I had some weird
behaviour when using underscores or hyphens. I don’t remember which
exactly. So I just avoid them. They just have to match with one
another.</li><li>I have no load metrics. I use relayd(8) on personal projects and have
not faced any specific issues. But if you expect lots of connections,
you may need to increase the <code>prefork</code> value and/or tune the <code>tcp</code>
options.</li><li>In real Enterprise World, you may combine relayd(8) and carp(4) to
achieve a better SLA.</li></ul><p>The original <a href="https://www.tumfatig.net/files/2023/eurobsdcon2023-relayd.pdf" target="_blank" rel="external nofollow">slides are available
here</a>
. Because OpenBSD and
because fun, you can get the <a href="https://www.tumfatig.net/files/2023/eurobsdcon2023-relayd-sketch.pdf" target="_blank" rel="external nofollow">same slides rendered in Comic font</a>
.</p></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kagi finally let me lay Google Search to rest (208 pts)]]></title>
            <link>https://dannb.org/blog/2023/how-kagi-beats-google/</link>
            <guid>37852133</guid>
            <pubDate>Thu, 12 Oct 2023 00:34:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dannb.org/blog/2023/how-kagi-beats-google/">https://dannb.org/blog/2023/how-kagi-beats-google/</a>, See on <a href="https://news.ycombinator.com/item?id=37852133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://dannb.org/images/blog/2023/09/kagi-search-logo-dog.jpg" alt="Kagi search dog and logo"></p><p>I’m a <a href="https://kagi.com/">Kagi</a> convert, having switched from Google Search in July of 2022 and never looking back. I’m also a vocal supporter of Kagi, frequently mentioning it in <a href="https://dannberg.substack.com/">my newsletter</a> and encouraging people to give Kagi a try whenever the opportunity arises.</p><p>For those unfamiliar, Kagi is a search engine with a novel<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> business model: end users pay for a service. Instead of monetizing users through advertising and affiliate marketing (like every other search engine), Kagi charges a monthly fee.</p><p>I’m both a user and vocal supporter of Kagi, and I also have big <em>thoughts and opinions</em>. Both on the state of search today, where it’s going in the future, and why it’s important to support companies like Kagi. Additionally, I was blown away by the quality of Kagi’s search results, which is what first encouraged me to make the switch, and I wanted to dive into the details in case anyone reading is on the cusp of trying this new search engine.</p><p>This post is not an ad; I’m just a happy customer. And I’ll gladly pay for products I like, and actively hope others support these products, too, so that the companies that make them can survive.</p><p>With that said, let’s kick things off with a little background on <em>Internet Search</em> as it exists today.</p><h2 id="wow-google-search-sucks">Wow, Google Search <em>sucks</em></h2><p>It may be easy to miss, but <a href="https://dkb.blog/p/google-search-is-dying">Google Search is dying</a>. Search results are dominated by ads SEO-optimized junk. More and more, people append words like “reddit” to the end of their search to try and bubble up any semblance of a useful response.</p><p>According to the author above blog post (with whom I agree): “serving ads creates misaligned incentives for search engines.” It’s impossible to both provide the best search results <em>and</em> try to optimize for the highest amount of ad-clicks. And if you’re a public company with an ad-based business model, you are <em>legally required</em> to optimize for the latter. Ads <em>inherently</em> create misaligned incentives.</p><p>But Google continues to dominate as the most-used search engine worldwide. I think part of this has to do with the fact that it’s so ingrained into people’s lives that its flaws go unnoticed or overlooked. People just amend their behavior (like adding “reddit” to the end of a search, or immediately scrolling through pages of SEO-junk to get to a recipe) to compensate for Google’s shortcomings.</p><p>Google reigns supreme, too, in no small part because historically all its competitors all suck even more. Let’s take a quick look at the mainstream alternatives. As of August 2023, <a href="https://gs.statcounter.com/search-engine-market-share/desktop/united-states-of-america">US desktop search engine market share</a> is as follows:</p><ol><li>Google (78.96%)</li><li>Bing (14.42%)</li><li>Yahoo (3.89%)</li><li>DuckDuckGo (2.11%)</li></ol><p>That’s <em>rough</em>. If a user gets fed up with Google Search’s flaws and tries one of these big competitors as their default search engine, it’s only a matter of time before they come crawling back to Google.</p><p>This was best tested en masse in mid-2020 when <a href="https://www.forbes.com/sites/johnkoetsier/2020/06/08/apple-could-cost-google-15-billion-by-buying-duckduckgo-analyst-says/?sh=4c218fb21920">rumors were swirling</a> of Apple purchasing DuckDuckGo. I, like many others, saw the news and figured that maybe DuckDuckGo was possibly more of a competitor to Google than I originally thought. I made it my default search engine.</p><p>That only lasted a few months before I switched back to Google Search. I valued the privacy aspect of DuckDuckGo but the search results were straight up <em>lacking</em>. A vast majority of the time, the article or piece of information I was searching for was completely missing from the DuckDuckGo results. Other times, it was buried in the muck. I’d then switch back to Google search and find the result I wanted almost immediately. It wasn’t long before I switched my default back to Google to simply save time and effort.</p><p>I’ve experimented with Bing and Yahoo, but never felt inspired to even attempt either as my default search engine. As I result, I felt trapped by Google Search — painfully aware of its flaws but without a reasonable alternative.</p><p>I first heard about Kagi in <a href="https://twitter.com/helvetica/status/1552140803866230785">a tweet from indie gamemaker Zach Gage</a>, and the value proposition was instantly appealing: <em>search as a service I can pay for</em>. This solves the misaligned incentives that hamstring Google (and other search engines), which in <em>theory</em> should produce a better product. But historically, I had never used a search engine that matches in quality, let alone exceeds, even the low bar that Google sets.</p><p>I want to say<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> that I experimented with Kagi on an informal basis for maybe a week before feeling confident enough to set it as my default search engine. Initially, I’d frequently do side-by-side searches on both Kagi and Google to compare results, but stopped after another week or two. Kagi has been my daily driver workhorse search engine, on both my personal and work computer, as well as my smartphone, ever since. I’ve not looked back once.</p><h2 id="why-kagi-is-better">Why Kagi is better</h2><p>I don’t think side-by-side comparisons of search results from different providers is the best way to qualitatively compare quality (although they <em>are</em> <a href="https://twitter.com/helvetica/status/1555182325239091200">sometimes compelling</a>). This is because most results can look relatively similar at first blush, and nitpicking the small differences can come off overly critical and exaggerated.</p><p>Instead of providing screenshots from different search providers for different search terms, and delving into the differences and how they impact search quality, I’m instead going to talk about <em>how</em> I think about search, and provide you a foundation to think about your own search experiences. Then, I encourage you to give Kagi a try (and also test a few other Google Search alternatives) and come to your own conclusions.</p><h3 id="framework-for-thinking-about-search">Framework for thinking about search</h3><p>Almost every search engine you use will eventually serve a satisfying answer. Instead, the quality of a search engine depends on how <em>long it takes</em> to get to this end state, <em>how complicated the journey</em> to this end state, and the <em>quality of user experience</em> (UX) of the whole process.</p><p>Most online search queries fall into one of the following categories:</p><ol><li>Instructions (ie coding, recipe, how to)</li><li>Event (ie past or current news, )</li><li>Fact (ie movie run time, celebrity age)</li><li>Solution (ie product recommendation)</li><li>Exploration (ie research, education)</li></ol><p>A search has succeeded once you find a piece of information or reference that satisfies the original query. The more steps it takes to get to that final success state, the worse the search engine’s quality.</p><p>These steps include modifying the original search query (ie adding extra keywords or search operators), clicking through multiple results and comparing information, browsing through multiple pages of results.</p><p>Searches can be undertaken with either a specific <em>success state</em> in mind (ie How old is Tom Cruise? How do I apply a gaussian blur in Figma?) or have a non-specific success state (What should I do on my vacation in Istanbul?).</p><h3 id="measuring-the-quality-of-search-engines">Measuring the quality of search engines</h3><p>In addition to different search categories, there are two main contexts for search: ad hoc or relaxed.</p><p>Most searches are ad hoc — you will be performing some work or a task and hit a knowledge-based road block. In these cases, you want to be able to immediately execute a search, quickly find a satisfying result, and return to your previous context.</p><p>Alternatively (and less frequently) searches can be relaxed, which is to say exploratory and meandering. You might be researching a vacation to Istanbul, or coming up with activities for kids on rainy days. The goal is to look at several different websites and sources, collect data, and process it at a leisurely basis. There is no right or wrong answer, per say. Your goal instead is to have a greater understanding of a topic based on multiple sources.</p><p>When attempting to kick the tires of a search engine, often times we’ll input a more relaxed search query and attempt to measure the quality of the engine by the results. Instead, it’s really ad hoc searches where the differences in quality are most apparent.</p><p>This is why setting a new search engine as your default is really the best way to measure its quality. It’s those times when you’re working on something else and need a quick answer from a search engine when you’ll really be able to tell which results are up-to-snuff and which are lacking.</p><h2 id="kagi-has-cool-features-too">Kagi has cool features, too</h2><p>In addition to solid search results, Kagi also has several features that are just icing on the cake. Some of my favorites include:</p><h3 id="website-ranking-adjustment">Website Ranking Adjustment</h3><p>Kagi search results are already good, but users have powerful options to tweak and adjust to fit their needs. This takes the form of <a href="https://blog.kagi.com/kagi-features">blocking or boosting certain domains</a>. On the off chance that a Kagi search returns some SEO-garbage site, you can de-prioritize that domain for future searches. Likewise, boosting trusted sources and make those results rank higher for you.</p><p>This is <em>really</em> powerful. By subtly tweaking and adjusting my website ranking preferences over the past year+, I can almost guarantee that the search result I’m hoping for is near the very top of my results.</p><h3 id="lenses">Lenses</h3><p>I recently started using <a href="https://help.kagi.com/kagi/features/lenses.html">Lenses</a> more frequently, and it has helped me refine my searches quickly and reliably, with as little extra work as possible.</p><p>Essentially, Lenses allow users to pre-set search parameters — narrowing results by domain(s), region(s), keyword(s), date(s), and more. With Google, you can do this with <a href="https://ahrefs.com/blog/google-advanced-search-operators/">search operators</a> (which also exist on Kagi), but lenses allow you to pre-set frequently-used operators and thus easily narrow your search to various online worlds. Best of all, lenses are <em>sharable</em>!</p><p>The two I use most frequently are a <a href="https://kagi.com/lenses/IRpSaJ5igCVRuEF67ka7CkH7pldTolZz">Reddit lens</a> and a <a href="https://kagi.com/lenses/1aDMUsFO3QtaU5M5rE3ho2tnC750QsAH">Hacker News lens</a>. Both are incredibly simple (effectively the same as adding <code>site:reddit.com</code> or <code>site:news.ycombinator.com</code> after a search term).</p><p>Additionally, I’ve found several of the default lenses to be useful and interesting, especially PDFs, Forums, and Small Web.</p><h3 id="universal-summarizer">Universal Summarizer</h3><p>I also find myself using the <a href="https://blog.kagi.com/universal-summarizer">Universal Summarizer</a> feature <em>all the time</em>. Often, an article or blog post will look interesting but I don’t have the time or mental capacity to read it immediately. I use Readwise’s <a href="https://readwise.io/read">Reader</a> as a read-it-later, but my reading queue there is long and grows faster than I read.</p><p>If I’m not sure if the article is worth saving to read later, I’ll use Kagi’s Universal Summarizer to get a gist of the article. This will either convince me to save it, or at least allow me to make peace with releasing it back into the wild unconsumed.</p><p>But one of the coolest parts of Kagi’s Universal Summarizer is that it works on <em>YouTube videos</em> and <em>podcast episodes</em>. Feed it a YouTube URL or a podcast mp3 and within seconds you’ll get a detailed, high-quality summary. I know this is available through other third-party services, but Kagi makes it so easy when you’re fully integrated that I use it way more.</p><h3 id="assistant-closed-beta">Assistant (Closed Beta)</h3><p>Kagi is continuing to grow in the direction of AI search augmentation with tools like <a href="https://help.kagi.com/kagi/ai/assistant.html">Assistant</a>. Currently in closed beta, Assistant is a research tool backed by Kagi Search and large language models.</p><p>This is a alternative to the <a href="https://www.reuters.com/technology/openai-says-chatgpt-can-now-browse-internet-2023-09-27/">recent announcement</a> that ChatGPT would have real-time access to internet search results (as opposed to the dataset of large language models being limited by the training data date cutoff). This is only available to $20/mo ChatGPT Plus subscribers.</p><p>This functionality isn’t super useful to me — <a href="https://www.typingmind.com/">TypingMind</a> connected to my OpenAI API key is perfectly sufficient for 99% of my AI queries. However, this is a cool bonus feature for premium Kagi users who don’t want to <em>additionally</em> pay OpenAI $20/mo and want access to multiple different LLMs for more robust answers.</p><h3 id="orion-browser">Orion Browser</h3><p>I’d be remiss if I didn’t at least mention Kagi’s web browser <a href="https://browser.kagi.com/">Orion</a>. I haven’t put in the work to switch default browsers (which feels like a more Herculean task than switching search engines) but Orion seems like a compelling choice. Both speed and memory usage stats are compelling, and it’s compatible with both <a href="https://help.kagi.com/orion/why-orion/orion-vs-safari.html">Chrome and Firefox extensions</a>, giving users “users access to the largest extensions ecosystem in the world.”</p><h2 id="the-not-quite-there">The not-quite-there</h2><h3 id="maps">Maps</h3><p>Kagi does have <a href="https://kagi.com/maps">Maps</a> functionality, but I find it’s still lacking compared to Google Maps. As a result, I still use Google Maps for both searching points-of-interest and getting directions. I don’t blame them because mapping is hard — there are still iPhone users who refuse to use the now-superior Apple Maps, instead opting for Google Maps, due to a botched launch <em>eleven years ago</em>. To be fair, Kagi is not claiming feature parity with competitors, but it’ll still be a while before I personally switch from Google Maps to Kagi Maps.</p><h3 id="deep-integration-is-complex">Deep integration is complex</h3><p>This part isn’t Kagi’s fault — to the contrary, I feel like Kagi has made deep integration as easy as it possibly can given the constraints.</p><p>But search engines are so deeply ingrained into different parts of our lives that it becomes complex for one of the new kids on the block to become as deeply embedded itself.</p><p>For example, setting Kagi as your default desktop search for Chromium, Firefox, or Safari browsers requires a <a href="https://help.kagi.com/kagi/getting-started/setting-default.html">browser extension</a> and post-installation settings changes. Kagi makes it easy, as installs go, but it’s definitely a more advanced process than, say, switching from Google Search to DuckDuckGo.</p><p>Likewise, using Kagi in a Private Browsing window requires a Private Session Link, a URL that can be found in your Kagi settings, which needs to be entered into the plugin. It’s easy when following instructions, but otherwise may be unintuitive for those who like to plow ahead without reading documentation.</p><p>Setting Kagi as your default search engine for Safari on iOS also involves a mobile browser plugin. I got this to work immediately, but David Pierce at <em>The Verge</em> mentioned that <a href="https://www.theverge.com/23896415/kagi-search-google-meta-quest-3-chatgpt-macos-sonoma-installer-newsletter">he’s had trouble doing the same</a>. I guess YMMV?</p><h2 id="should-you-switch">Should you switch?</h2><p>A paid search engine is definitely a luxury item that’s not for everyone. Free search democratizes the internet, and will always be a necessary. But I’m happy that there’s now a viable alternative that has a more traditional business-to-consumer monetization structure.</p><p>If the value proposition of Kagi is compelling, I recommend giving it a try as your default search engine. The <a href="https://kagi.com/pricing">100 free searches</a> is definitely enough to give it a spin and see how the quality stacks up in your opinion. Then, if you have the means and believe in it philosophically, you may be inspired to switch.</p><p>All I know is that I’m a happy user, and there must be more people out there like me just waiting to discover a solution like Kagi.</p><section role="doc-endnotes"><hr><ol><li id="fn:1" role="doc-endnote"><p>It’s novel — unthinkable! — for search engines, but is really the oldest business model in the world. <a href="#fnref:1" role="doc-backlink">↩︎</a></p></li><li id="fn:2" role="doc-endnote"><p>Forgive the fuzzy language around these details. It was over a year ago, and I wasn’t taking close notes on what I thought would be a short-lived experiment. <a href="#fnref:2" role="doc-backlink">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AVX10/128 is a silly idea (128 pts)]]></title>
            <link>https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/</link>
            <guid>37851029</guid>
            <pubDate>Wed, 11 Oct 2023 22:17:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/">https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/</a>, See on <a href="https://news.ycombinator.com/item?id=37851029">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2><span data-contrast="none">Intro and lots of context&nbsp;&nbsp;</span></h2>
<p><span data-contrast="none">Intel recently unveiled the AVX10 specification as means for consolidating the vast majority of AVX-512 extensions into a single, easy-to-target specification. It </span>ai<span data-contrast="none">ms to solve a few issues, among which is the startling array of configurations, targets, and spaghetti of AVX-512 implementations with disjointed instructions support. Lest we forget, it also primarily serves as means of bringing together all the beloved AVX-512 goodies into smaller implementations, targeted at consumer, micro-edge and embedded that can’t or won’t have the 32 512-bit registers required by AVX-512.&nbsp;</span></p>
<p><span data-contrast="none">I’ve publicly expressed my enthusiasm for the specification since the initial publication. Relatedly, I’m giving a talk for the Easy Build/HPC communities under the title “AVX10 for HPC: a reasonable solution to the 7 levels of AVX-512 folly.” This article was originally slated to be a part of the talk, but I’m writing it out instead for the sake of reference (and because I’m already struggling to get the talk down to 90 minutes, let alone the 60 I have).</span> For those interested in attending, you can find the link at the end of the article.</p>
<h2><span data-contrast="none">AVX10, what is it?</span></h2>
<p><span data-contrast="none">To begin, let’s break down AVX10.N/M: AVX10 is the new “foundational” SIMD/vector instruction set for x86_64. The “.N” denotes the version of AVX10 as aa <span data-contrast="none">version modifier</span>, allowing incremental updates. It is important to note, if you support “AVX10.N+3,” you must support all of AVX10.N, N+1 and N+2. In simpler words, users are guaranteed supersets of previous instruction sets.</span></p>
<div>
<figure><img data-attachment-id="22566" data-permalink="https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/screen-shot-2023-10-07-at-5-35-38-pm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.35.38-PM.png?fit=416%2C1116&amp;ssl=1" data-orig-size="416,1116" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2023-10-07 at 5.35.38 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.35.38-PM.png?fit=416%2C1116&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.35.38-PM.png?fit=416%2C1116&amp;ssl=1" decoding="async" width="416" height="1116" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.35.38-PM.png?resize=416%2C1116&amp;ssl=1" alt="" data-recalc-dims="1"></figure></div>
<p><span data-contrast="none">What does the “/M” mean? It’s a reference to vector register implementation size of a given AVX10.N version. Specifically, it may be 512-bit, 256-bit, or the topic of this article, 128-bit wide. </span></p>
<p><span data-contrast="none">128-bit registers, a.k.a XMM registers, were introduced with SSE(1) for the 32-bit only</span> Pentium 3 in 1999. <span data-contrast="none">256-bit registers were introduced with AVX1, and first implemented in the Sandy Bridge micro architecture in 2011. 512-bit registers were specified by AVX-512 and released around 2016 with Xeon Phi, but were</span>n’<span data-contrast="none">t generally available until 2017 with the release of Skylake-X.</span></p>
<p>To give an idea of what each of those looks like, here’s a comparison of the “add packed single-precision floating-point values (ADDPS)” instruction, courtesy of the <a href="https://www.officedaytime.com/tips/simd.html">officedaytime.com SIMD instruction visualizer</a>.</p>
<p><a href="http://https//www.officedaytime.com/simd512e/simdimg/binop.php?f=addps"><img data-attachment-id="22564" data-permalink="https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/screen-shot-2023-10-07-at-5-26-36-pm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?fit=1616%2C1370&amp;ssl=1" data-orig-size="1616,1370" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2023-10-07 at 5.26.36 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?fit=1616%2C1370&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?fit=688%2C583&amp;ssl=1" decoding="async" width="533" height="452" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=533%2C452&amp;ssl=1" alt="A screen shot visualizing the various versions of the x86 SIMD instruction &quot;addps&quot; showing it's evolution from using 128 bits of FP32 between 2 registers to its modern AVX512 implementation with 16 FP32s that can be sourced from 2 registers while the answer is stored in a 3rd" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?w=1616&amp;ssl=1 1616w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=768%2C651&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=1536%2C1302&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=1200%2C1017&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=1600%2C1356&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?resize=1320%2C1119&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-5.26.36-PM.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 533px) 100vw, 533px" data-recalc-dims="1"></a></p>
<h2><span data-contrast="none">A note on naming</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></h2>
<h3><span data-contrast="none">From AVX, to AVX2, to AVX-512, to AVX10</span></h3>
<p>After <span data-contrast="none">speaking with some of my favourite folks from intel, officially there aren’t “specific” reasons the name AVX10 was chosen as the successor to AVX512, beyond “marketing is going to market.”</span></p>
<p><span data-contrast="none">I have an alternative theory:</span></p>
<p>The <span data-contrast="none">AVX-512 specification we know today started out as a much smaller VEX-encoded ISA, known as AVX3 internally, as well as some early marketing materials. AVX3 was “relatively” boring as it only expanded registers, stayed VEX and provided a more exhaustive fused multiply add, similar to what AMD attempted with the FMA4 instructions. Taking that view to the past, if you set the AVX512f extensions to be “AVX3” and then exclude the Xeon Phi-only extensions, AVX512 had ~6 groups of extension worthy of being called discrete generations. <span data-contrast="none">Roughly</span></span>,<span data-contrast="none"> you can categorize them into:</span></p>
<ul>
<li><span data-contrast="none">AVX3 F, CD, ER, PF</span></li>
<li><span data-contrast="none">AVX4 VL, DQ, BW</span></li>
<li><span data-contrast="none">AVX5 IFMA, VBMI</span></li>
<li><span data-contrast="none">AVX6 BF1</span>6</li>
<li><span data-contrast="none">AVX7 VPOPCNTDQ VNNI, VBMI2, BITALG</span></li>
<li><span data-contrast="none">AVX8 VP2INTERSECT – deprecated</span></li>
<li><span data-contrast="none">AVX9 FP16</span></li>
<li><span data-contrast="none">AVX10 – the new “big one”</span></li>
</ul>
<h2><span data-contrast="none">Back to the good stuff</span></h2>
<p><span data-contrast="none">On the server and HPC side, expect all implementations to conform to the AVX10.N/512 specification. In other words, you should expect implementations to use AVX10.N with 512-bit vectors. This ensures that any existing AVX-512 code is fully supported, and <span data-contrast="none">continues</span> the legacy of backward compatibility for <span data-contrast="none">x86_64</span>.</span></p>
<p><span data-contrast="none"><span>On the consumer side, having a massive register file with 32 registers, each of which 512-bits is considered problematic and non-viable.* </span>However, as seen in <a href="https://chipsandcheese.com/2022/11/05/amds-zen-4-part-1-frontend-and-execution-engine/">Zen 4</a>, Alder Lake, <a href="https://chipsandcheese.com/2022/07/15/caching-energy-efficiency-data-mobile-and-avx-512/">Tiger Lake</a> and <a href="https://chipsandcheese.com/2022/03/23/via-part-4-a-deep-dive-into-centaurs-last-cpu-core-cns/">more</a>, it’s rather doable. The problem is that small, “efficiency” cores, notably Intel’s recent Gracemont (in <a href="https://chipsandcheese.com/2021/12/02/popping-the-hood-on-golden-cove/">Alder Lake</a> and Raptor Lake), and Crestmont (in upcoming Meteor Lake and Sierra Forest) microarchitectures, prefers to only implement 128-bit physical ALUs, relying on so-called “double pumping” (a more limited version of register pipelining from the Vector Processor days) to achieve AVX2 support. This way, they can implement the 16 x 256-bit registers of AVX2, but only need to implement 128-bit floating point and integer units. This comes with a cost: while you save on die space and power, some workloads may see significant performance regressions.</span></p>
<p><span data-contrast="none">Knowing this, the fine folks at Intel designing the AVX10 spec mandated that all implementations must have 32 registers, but said registers would only need to be as wide as the given “/M”.</span> This means <span data-contrast="none">AVX10/256</span> <span data-contrast="none">would have the same instruction capabilities as AVX10/512, but only require the 32 registers be 256-bits wide.</span></p>
<p><span data-contrast="none">For the most part, any code written for the older AVX-512 extensions that were limited to 256-bit registers should* run fine with only a recompile. This sort of code came about as a result of the <span data-contrast="none">much fabled “AVX-512 down-clocking” menace that would “punish” you for using the 512-bit part of AVX-512</span>. The good news is there’s a lot of code already designed for a “simplified” 256-bit version of AVX-512, which will either be ready or easy to migrate when the time comes.</span></p>
<p><span data-contrast="none"><em>*It’s slightly more complicated than the above, but you can get it going between a few hours and a few days.</em></span></p>
<h2><span data-contrast="none">But what does the Spec Say?</span></h2>
<p><span data-contrast="none">If you read the AVX10 specification (</span><a href="https://cdrdv2-public.intel.com/784343/356368-intel-avx10-tech-paper.pdf" target="_blank" rel="noreferrer noopener">Intel White paper on AVX10</a><span data-contrast="none">, </span><a href="https://cdrdv2-public.intel.com/784267/355989-intel-avx10-spec.pdf" target="_blank" rel="noreferrer noopener">full AVX10 specification</a><span data-contrast="none">), a few things stand out.</span></p>
<div>
<figure><a href="https://cdrdv2-public.intel.com/784343/356368-intel-avx10-tech-paper.pdf"><img data-attachment-id="22571" data-permalink="https://chipsandcheese.com/2023/10/11/avx10-128-is-a-silly-idea-and-should-be-completely-removed-from-the-specification/screen-shot-2023-10-07-at-6-28-06-pm/" data-orig-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?fit=1708%2C1086&amp;ssl=1" data-orig-size="1708,1086" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screen Shot 2023-10-07 at 6.28.06 PM" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?fit=1708%2C1086&amp;ssl=1" data-large-file="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?fit=688%2C437&amp;ssl=1" decoding="async" width="688" height="437" src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=688%2C437&amp;ssl=1" alt="" srcset="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?w=1708&amp;ssl=1 1708w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=768%2C488&amp;ssl=1 768w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=1536%2C977&amp;ssl=1 1536w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=1200%2C763&amp;ssl=1 1200w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=1600%2C1017&amp;ssl=1 1600w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?resize=1320%2C839&amp;ssl=1 1320w, https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/10/Screen-Shot-2023-10-07-at-6.28.06-PM.png?w=1376&amp;ssl=1 1376w" sizes="(max-width: 688px) 100vw, 688px" data-recalc-dims="1"></a></figure></div>
<p><span data-contrast="none">Namely, the technical paper repeatedly uses the <span data-contrast="none"><span data-contrast="none">word “converged”</span></span>. What features are converging? The answer is all the distinctive features of AVX-512</span> that aren’t just “big registers”<span data-contrast="none">. Things like IEEE-754 half precision floating points? Supported as part of AVX10. What about brain floating point 16 (<span data-contrast="none">BF16</span>), a truncated version of FP32 used in the buzzword <em>du jour</em>, “AI”? Supported as a part of AVX10. What about every AVX-512 assembly programmers’ favourite dynamically reprogrammable ternary logic operator instructions? Supported as a part of AVX10. Basically, all the cool stuff assembly and compiler programmers want to use to speed up applications via smarter algorithm design are included as part of AVX10.</span></p>
<p><span data-contrast="none">Another important AVX10 <span data-contrast="none">requirement</span> is that all implementations must fully implement AVX2 and its 16 x 256 bit registers</span>. In turn, you’re guaranteed support for AVX2 code on your processor. For the maths folks, you can think of it as AVX10 having the full set of AVX2 within its own set. AVX2, in turn, requires all of AVX1.</p>
<h2><span data-contrast="none">So finally, the meat and potatoes: AVX10.N/128</span><br><span data-contrast="none">&nbsp;&nbsp;</span></h2>
<p><span data-contrast="none">I have 3 “core” problems:</span></p>
<ol>
<li><span data-contrast="none">Any and all implementations will be somewhat cursed</span>.</li>
<li><span data-contrast="none">It causes issues for the software that tries to implement the specification.&nbsp;&nbsp;</span></li>
<li><span data-contrast="none">It effectively triples the per-generation development burden. </span><br><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ol>
<h3>It’s cursed</h3>
<p><span data-contrast="none">Any AVX2 implementation must have 16 x 256-bit registers</span>. AVX10 requires 32 vector registers regardless of vector size. <span data-contrast="none">In the case of AVX10.N/128, that would be 32 x 128-bit registers.</span> <span data-contrast="none">From the (supposed) point of view of a core design engineer/architect, the decision tree of AVX10/128 would be:</span></p>
<p><span data-contrast="none">Any implementations that only supports up to AVX10/128 must support 16 256-bit YMM registers, a.k.a YMM0-15, and a secondary set of <span data-contrast="none">128-bit XMM</span> registers that span from XMM16-31. The <span data-contrast="none">architect</span> is left with a few more choices. </span></p>
<ul>
<li><span data-contrast="none">Do you choose to have 2 different classes of SIMD vector registers with different sizes?</span></li>
<li><span data-contrast="none">Do you <em>alias</em> the upper half of ymm0-15 bits 128-255 – to be xmm16-31 bits 0-127?</span></li>
<li><span data-contrast="none">Do you extend xmm16-31 to be 256 bits?</span></li>
</ul>
<p><span data-contrast="none">The third choice is the most likely for a “clean” implementation. But then a realization will hit you: Wait! I’ve now built the same register file need for AVX10/256! If I implement a little more control logic, I have a full, proper AVX10/256 implementation and can keep my 128-bit FPUs and ALUs!</span></p>
<p><span data-contrast="none">And guess what! We’ve done that before! Famously for Zen 4 and Zen4c, AMD implemented AVX512 using 256-bit FPUs. When Zen 1 adopted AVX2, they <em>also</em> double pumped a 128-bit integer unit. Previously, the Bulldozer microarchitecture implemented AVX1 with 128-bit FPUs! And it’s not just AMD! Intel does the same today with 128-bit FPUs and integer ALUs on Gracemont. </span></p>
<p><span data-contrast="none">So you take a step back and realize you’ve already implemented double pumping in the first place, because you need to support the 16 x 256-bit registers for AVX1 and 2! Specifically, AVX requires</span> <span data-contrast="none">logics to address, mask, load, and store both the high and low parts of a given register.</span></p>
<h3><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">Software implementation headaches</span></h3>
<p><span data-contrast="none">The next step is relatively simple: address issues for optimizing <span data-contrast="none">software</span> implementations by targeting modern ISA implementations. One of the guarantees of AVX10 is that any implementation supports all smaller valid implementations. In other word, while <span data-contrast="none">AVX10/512 platforms</span></span> support <span data-contrast="none">AVX10/256, <span data-contrast="none">AVX10/256 platforms</span></span> do not support <span data-contrast="none">AVX10/512. By extension, <span data-contrast="none">AVX10/256-512</span></span> platforms support<span data-contrast="none"> AVX10/128. </span></p>
<p><span data-contrast="none">But here’s the problem. From a software targeting point of view, when AVX10 becomes ubiquitous enough to be the default x86_64 target in about a decade,</span> <span data-contrast="none">AVX10/128</span>, as <span data-contrast="none">the most compatible </span>choice, <span data-contrast="none">ends up being a net downgrade <span data-contrast="none">over AVX2</span></span> <span data-contrast="none">for SIMD programs. If AVX10/128 is valid and makes its way to market, it becomes the <em>de facto</em> minimum target for AVX10, as it supports all server and consumer options. While it’s true that the best part of AVX-512 was not the 512 bits, it’s simultaneously true that a downgrade to 128-bit registers as a common target would be detrimental to SIMD code generation – a reminder as we moved past 128-bit registers on consumer platforms over a decade ago. Code generation has moved on. Do we really want to be stuck with a sidegrade to 128-bit registers with better instructions in a decade’s time?&nbsp;&nbsp;</span></p>
<h3>You want to make even <span><strong><em>more</em></strong></span> targets?!?!</h3>
<p><span data-contrast="none">My last point is that, <span data-contrast="none">from a software point of view</span></span>,<span data-contrast="none"> AVX10 with only 256-bit and 512-bit options <span data-contrast="none">effective</span></span>ly<span data-contrast="none"> doubles the burden for each generation. </span>It <span data-contrast="none">has already happened with <span data-contrast="none">consumer</span></span> <span data-contrast="none">Golden Cove vs <span data-contrast="none">enterprise</span></span> <span data-contrast="none">Golden Cove</span>. Namely, <span data-contrast="none">the former only supports up to AVX2, but the latter implements all of AVX512 (to the point of being compatible with AVX10.1/512).</span></p>
<p><span data-contrast="none">The “same” microarchitecture (uArch) may have different memory configurations, different amounts of <span data-contrast="none">Fused Multiply Add</span></span> <span data-contrast="none">(FMA) units, different amounts of vector add units, etc. </span><br><span data-contrast="none">Looking at Golden Cove, we have: <span data-contrast="none">consumer</span></span> <span data-contrast="none">AVX2 with DDR4 and DDR5, <span data-contrast="none">workstation</span></span> <span data-contrast="none">AVX-512 with DDR5, <span data-contrast="none">server</span></span> <span data-contrast="none">AVX-512 with DDR5, <span data-contrast="none">server</span> AVX-512 with HBM only, and <span data-contrast="none">server</span></span> <span data-contrast="none">AVX-512 with <span data-contrast="none">DDR5 main memory</span></span> and <span data-contrast="none">HBM cache. </span></p>
<figure><table><thead><tr><th>Target Market</th><th>ISA</th><th>Memory</th></tr></thead><tbody><tr><td>Consumer, low-cost</td><td>AVX2</td><td>2 x DDR4</td></tr><tr><td>Consumer, mainstream</td><td>AVX2</td><td>2 x DDR5</td></tr><tr><td>Workstation, mainstream</td><td>AVX-512</td><td>4 x DDR5</td></tr><tr><td>Workstation, high-end</td><td>AVX-512</td><td>8 x DDR5</td></tr><tr><td>Server, general purpose</td><td>AVX-512</td><td>8 x DDR5</td></tr><tr><td>Server, HPC/AI dedicated compute</td><td>AVX-512</td><td>HBM</td></tr><tr><td>Server, HPC/AI general purpose compute</td><td>AVX-512</td><td>8 x DDR5 + HBM</td></tr></tbody></table><figcaption>Various versions of the Golden Cove Micro Architecture</figcaption></figure>
<p>While <span data-contrast="none">HPC is used to kernels <span data-contrast="none">(fancy name for a maths routine)</span></span> <span data-contrast="none">with <span data-contrast="none">multiple versions</span> for different sub-SKUs of an ISA, consumer software avoids doing this at all cost. You’re lucky in consumer software if the maintainers turns on anything past SSE2, let alone AVX in any of its flavours.</span></p>
<p><span data-contrast="none">And I don’t blame them. From a maintenance point of view, it is unreasonable to ask every package manager to compile different versions of projects for different versions of ISAs, to tune for differently platforms, and somehow manage to always build and ship them. Now you’re going to add all of that on top of keeping up with the existing burdens of package management? I don’t think so. In HPC, you can rely on most users to recompile software for their clusters, but this simply doesn’t happen on consumer platforms. Heck, not even Arch Linux implementes that experimentally!</span></p>
<h2><span data-contrast="none">Conclusion: what do I want?</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></h2>
<p><span data-contrast="none">My request to Intel – more specifically the evangelists, fellows, VPs, principal engineers, etc. – is simple. Page 1-2 of Intel document 355989-001US, rev 1.0, currently reads:&nbsp;</span></p>
<blockquote>
<p><span data-contrast="none">For Intel AVX10/256, 32-bit opmask register lengths are supported. For Intel AVX10/512, 64-bit opmask are supported. There are currently no plans to support an Intel AVX10/128 implementation.&nbsp;</span></p>
</blockquote>
<p>I’d request that the above be changed to:</p>
<blockquote>
<p><span data-contrast="none">For Intel AVX10/256, 32-bit opmask register lengths are supported. For Intel AVX10/512, 64-bit opmask are supported. </span>Support for an Intel AVX10/128 only implementation is not provided for within this specification. All AVX10/256 and AVX10/512 implementations shall allow for operations on scalar and 128-bit vector registers.</p>
</blockquote>
<p><span data-contrast="none">The specific phrasing here is meant to make sure that should intel ever want to explore an <span data-contrast="none">AVX10</span></span>-based<span data-contrast="none"> architecture designed for a many-core product, conceptually like Xeon Phi, they can. This way, compilers, library developers, and other software vendors aren’t in a “will they won’t they” holding pattern. It avoids needing to leave hooks in for something that’s allowed to exist per spec but won’t make it to market. The changes would still allow them to build the product eventually, but those designing for the product can bear the burden of supporting it, leaving us normal dev folks alone. The product would probably be a “simple” atom core that implements the scalar versions of AVX10, each core having its own AMX unit. </span>But I’ll leave the rampant product speculation to a different parts of the industry </p>
<p><span data-contrast="none">So, I humbly ask: Intel, please, please, please make AVX10/128 an illegal implementation under the current specification.&nbsp;</span></p>
<div><p>And for those interested in the history of instruction sets on x86_64, from the original x87 FPU all the way to AVX10, my talk on AVX10 for HPC is Friday the 13th of October 2023. Link here: <a href="https://easybuild.io/tech-talks/008_avx10.html" target="_blank" rel="noreferrer noopener">https://easybuild.io/tech-talks/008_avx10.html</a></p><p>If you like our articles and journalism, and you want to support us in our endeavors, then consider heading over to our&nbsp;<a href="https://www.patreon.com/ChipsandCheese">Patreon</a>&nbsp;or our&nbsp;<a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ">PayPal</a>&nbsp;if you want to toss a few bucks our way. If you would like to talk with the Chips and Cheese staff and the people behind the scenes, then consider joining our&nbsp;<a href="https://discord.gg/TwVnRhxgY2">Discord</a>.</p></div>

<div data-post_id="10949" data-instance_id="1" data-additional_class="pp-multiple-authors-layout-boxed.multiple-authors-target-the-content" data-original_class="pp-multiple-authors-boxes-wrapper pp-multiple-authors-wrapper box-post-id-10949 box-instance-id-1">

<ul>
<li>
<p><img alt="Felix &quot;Poutine&quot; CLC" src="https://secure.gravatar.com/avatar/b878c9e9480ebebebc59fbe9c0d41a01?s=80&amp;d=identicon&amp;r=g" srcset="https://secure.gravatar.com/avatar/b878c9e9480ebebebc59fbe9c0d41a01?s=160&amp;d=identicon&amp;r=g 2x" height="80" width="80" loading="lazy" decoding="async"> </p>
<div>

<p>
TLDR 🇨🇦/🇫🇷🐧👨🏼‍💻🏎️🧗‍♂️
💩posting and discussing C, AVX10, AVX512, HPC, Aarch64, the Kernel &amp; Yet More Data Types </p>
<p>
<a href="https://chipsandcheese.com/author/felixleclair123/" title="View all posts">
<span>View all posts</span>
</a>
<a href="https://chipsandcheese.com/cdn-cgi/l/email-protection#7117141d18095f1d14121d10180340434231191e051c10181d5f121e1c" target="_blank" aria-label="Email" rel="nofollow">
<span></span>
</a>
</p>
</div>
</li>
</ul>
</div>





</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US citizens with permanent disabilities get free lifetime pass to National Parks (405 pts)]]></title>
            <link>https://www.nps.gov/subjects/accessibility/interagency-access-pass.htm</link>
            <guid>37850930</guid>
            <pubDate>Wed, 11 Oct 2023 22:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nps.gov/subjects/accessibility/interagency-access-pass.htm">https://www.nps.gov/subjects/accessibility/interagency-access-pass.htm</a>, See on <a href="https://news.ycombinator.com/item?id=37850930">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="cs_control_6690712">
<figure>
<picture>
<source type="image/png" srcset="https://www.nps.gov/subjects/accessibility/images/2023-Access-Pass_front_2.png">
<img alt="Accessibility Pass for Parks" title="2023 Access Pass_front" src="https://www.nps.gov/subjects/accessibility/images/2023-Access-Pass_front_2.png">
</picture>
<figcaption>America the Beautiful-The National Parks and Federal Recreational Lands Access Pass </figcaption>
</figure><!-- floating-image alignment  -->
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{115}" paraid="2111427410"><strong>What is the America the Beautiful- The National Parks and Federal Recreational Lands Access Pass? </strong></p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{125}" paraid="1717004537">The Interagency Access Pass is part of the America the Beautiful – The National Parks and Federal Recreational Lands Pass series and is available free for US citizens or permanent residents with permanent disabilities.  </p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{143}" paraid="1591179836"><strong>Who is eligible to get an Interagency Access Pass? </strong></p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{149}" paraid="527273195">The Interagency Access Pass may be issued to US citizens or permanent residents of any age that have been medically determined to have a permanent disability (does not have to be a 100% disability) that severely limits one or more major life activities.  </p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{167}" paraid="893907869"><strong>What documentation do I need to show for proof of eligibility?  </strong></p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{173}" paraid="146757578">Along with a valid photo ID such as a US passport, driver’s license, or state-issued ID, applicants must provide documentation of permanent disability with one (1) of the following:  </p>
<ul>
<li paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{191}" paraid="1093614014">A statement by a licensed physician (Statement must include that the individual has a PERMANENT disability, that it limits one or more aspects of their daily life, and the nature of those limitations.) </li>
<li paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{191}" paraid="1093614014">A document issued by federal agency such as the Veteran's Administration, Social Security Disability Income or, Supplemental Security Income </li>
<li paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{200}" paraid="832018928">A document issued by a state agency such as a vocational rehabilitation agency. </li>
</ul>
<p aria-level="2" paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{228}" paraid="264344596" role="heading"><strong>Where can I get an Interagency Access Pass? </strong></p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{234}" paraid="1534541633"><u>Get a Pass in Person</u> </p>
<p paraeid="{92ce2bb7-bf6e-4b2d-ac8d-c58fa5831a98}{254}" paraid="1254792940">You can get an Interagency Access Pass in person at a federal recreation site. Please be aware that passes are not available at all national park sites. Review the <a href="https://www.nps.gov/planyourvisit/pickup-pass-locations.htm" id="https://www.nps.gov/planyourvisit/pickup-pass-locations.htm|">Places to Get Interagency Passes</a> to find a location. </p>
<p aria-level="3" paraeid="{a574d854-7582-4b56-8df9-f782cc7d19eb}{48}" paraid="580377085" role="heading"><u>Get a Pass Online </u></p>
<p paraeid="{a574d854-7582-4b56-8df9-f782cc7d19eb}{56}" paraid="1818538870">You can also get an Interagency Access Pass online through the USGS Online Store or, through the mail using an application form (Note: While the pass itself is free, there is a shipping and processing cost to get a pass online or through the mail). </p>
<p paraeid="{a574d854-7582-4b56-8df9-f782cc7d19eb}{98}" paraid="499885614"><a href="https://store.usgs.gov/access-pass" id="https://store.usgs.gov/access-pass|">Interagency Access Passes are available online. </a></p>
<p paraeid="{a574d854-7582-4b56-8df9-f782cc7d19eb}{111}" paraid="738692707"><a href="https://store.usgs.gov/faq#Access-Pass" id="https://store.usgs.gov/faq#Access-Pass|">Frequently Asked Questions about the Interagency Access Pass </a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It’s time to allow researchers to submit manuscripts to multiple journals (145 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-03196-y</link>
            <guid>37850593</guid>
            <pubDate>Wed, 11 Oct 2023 21:40:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-03196-y">https://www.nature.com/articles/d41586-023-03196-y</a>, See on <a href="https://news.ycombinator.com/item?id=37850593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-03196-y/d41586-023-03196-y_26146716.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-03196-y/d41586-023-03196-y_26146716.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Portrait of Jon Gruda outside on a sunny day in a park near the ocean." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-03196-y/d41586-023-03196-y_26146716.jpg">
  <figcaption>
   <p><span>Dritjon Gruda thinks that it’s time to allow researchers to submit manuscripts to multiple journals.</span><span>Credit: Jon Gruda</span></p>
  </figcaption>
 </picture>
</figure><p>During the COVID-19 pandemic, my colleagues and I submitted a paper to a premier journal in our field, examining the effects of the onset of the pandemic on individuals’ mental health, on the basis of their personality traits. This was a time-sensitive piece because people began to adapt as the pandemic accelerated. The paper languished for several months without even being sent out for peer review. Multiple e-mails to the journal yielded no progress. We had to withdraw the paper and submit it elsewhere, losing valuable time. This could have been avoided had we been allowed to submit the manuscript elsewhere simultaneously.</p><p>Another time, we submitted a paper to a journal, and it was promptly reviewed by one reviewer. The editor, however, couldn’t find another and asked us to suggest any possible reviewers. We did, although with hesitation. Asking authors to recommend reviewers can compromise the objectivity and rigour of the peer-review process. There’s an obvious potential for bias because authors might suggest individuals who are more likely to provide favourable reviews. It took nine months and several proactive follow-ups for the editor to realize that our paper had been overlooked and was severely delayed. Eventually, after a revision request, our paper was published — almost a year later.</p><p>Imagine the absurdity of being able to apply for only one job at a time, waiting months for feedback before considering another opportunity. Such a scenario would undeniably hinder career progression. Yet, this exact practice persists in scientific publishing.</p><h2>Falling behind</h2><p>Scientific publishing has evolved to accommodate open access, preprints and even X (previously Twitter) threads. But the prohibition against simultaneously submitting a paper to several journals continues. This rule was conceived to protect the quality of the scientific record, with singular peer review acting as the key filter to ensure that only validated, high-quality research enters academia, one submission at a time. Yet, nowadays, this rule seems outdated and, at times, grossly unfair.</p><p>And sometimes a paper can undergo multiple rounds of review at one journal over the course of months or years, only to be rejected. Instead of accepting the existing system, why not change it?</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-023-01772-w" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-03196-y/d41586-023-03196-y_26146140.png"><p>Anonymizing peer review makes the process more just</p></a>
 </article><p>The multiple-submission ban stems from the pre-digital era, in which copyrights were more difficult to enforce, journal editors sifted through physical manuscripts and peer reviewers were scarce. But today, digitization has automated much of the administrative work. Identifying relevant reviewers, no matter their location, is easier and more straightforward than before. The fear that multiple submissions would overwhelm the peer-review system lacks empirical evidence and is outweighed by the burden placed on researchers. This is particularly detrimental to early-career scientists and those from under-represented backgrounds, for whom the delay isn’t just frustrating; it’s a barrier to career advancement.</p><p>The ban also hampers the speed of scientific dissemination, a crucial factor in many fields such as climate science, health and medicine, in which timely knowledge sharing is paramount. The rapid proliferation of preprint articles during the COVID-19 pandemic demonstrated the benefits of swift information sharing, even if those papers were not yet peer reviewed.</p><p>The time is ripe to reassess the single-submission policy.</p><h2>But what can authors do?</h2><p>While the scientific community awaits a much-needed overhaul of the single-submission policy, here are a few proactive suggestions to minimize the impact of protracted publishing timelines:</p><p>• Early communication: establish preliminary contact with journal editors to gauge interest in your paper before formal submission. Some editors offer constructive feedback and might even fast-track promising manuscripts.</p><p>• Journal tiering: have a well-researched list of target journals, tiered from high-impact to low-impact. If rejected from your top choice, you can quickly move on to the next journal on your list without wasting time on more research.</p><p>• Follow up: after submission, regularly check up with the journal. If a journal states that most reviews are completed in 90 days, nudge them shortly after that time. I have found this strategy to be the most useful. The key is to be respectful and professional in your communication throughout. Remember, respectful persistence. Don’t nag.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/s41563-023-01661-7" data-track="click" data-track-label="recommended article"><p>In praise of peer review</p></a>
 </article><p>• Professional networks: use your professional connections to find out which journals offer quicker review times or are known for efficient communication, even if their impact factor is not top tier. This is one of the many reasons why attending conferences is so important.</p><p>• Preprint archives: use preprint servers relevant to your field to upload your manuscript. This serves as a public record of your work and allows others to see, cite and build on it while it’s undergoing peer review.</p><p>• Use social media: share your preprint on platforms such as ResearchGate or LinkedIn to gather informal feedback and attract early attention to your work.</p><p>• Parallel projects: working on several projects can help to maintain productivity levels while one paper is stuck in the submission pipeline.</p><p>• Advocate change: join or initiate discussions that challenge the current system. Use academic blogs, webinars or professional meetings to shed light on the inefficiencies and inequities perpetuated by the single-submission rule.</p><p>Throughout my career, I’ve grappled with the frustrations of the single-submission rule. Each delay was more than a professional hiccup; it touched a nerve. Over coffee chats, my peers and I have shared these challenges, yearning for a better system. Although I haven’t spearheaded a publishing revolution, I’m driven by the collective hope of reshaping a system that recognizes our work’s value and the urgency of our time. Until publishing norms catch up with the needs and pace of contemporary research, the strategies that I’ve outlined can help authors to navigate the existing landscape more effectively. Nevertheless, in an era in which timely publications can make a difference in job security and research funding, a crucial re-evaluation of the single-submission rule is long overdue.</p>
                </div><p>This is an article from the <a href="https://www.nature.com/articles/d41586-019-03369-8" data-track="click" data-label="https://www.nature.com/articles/d41586-019-03369-8" data-track-category="body text link"><i>Nature</i> Careers Community</a>, a place for Nature readers to share their professional experiences and advice. Guest posts are encouraged.</p><div id="competing-interests" data-toggle="anchor-links-section" data-label="Competing Interests">
                    <h3>Competing Interests</h3>
                    <p>The author declares no competing interests.</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Roll Your Own All-Sky, Raspberry Pi Camera (249 pts)]]></title>
            <link>https://spectrum.ieee.org/all-sky-camera</link>
            <guid>37850485</guid>
            <pubDate>Wed, 11 Oct 2023 21:28:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/all-sky-camera">https://spectrum.ieee.org/all-sky-camera</a>, See on <a href="https://news.ycombinator.com/item?id=37850485">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-elid="2661714798" data-post-url="https://spectrum.ieee.org/all-sky-camera" data-authors="David Schneider" data-headline="Roll Your Own All-Sky, Raspberry Pi Camera" data-page-title="Roll Your Own All-Sky, Raspberry Pi Camera - IEEE Spectrum"><p>
	While driving home one night recently, I saw a spectacularly bright meteor flash across the sky in front of my car. A good-sized chunk of interplanetary detritus must have been on its way to a crash landing not too far away, I said to myself. My next thought was that if I had a bearing on that luminous streak, and if at least one other person in my region also had such information, we might be able to triangulate on it and narrow down where any landing zone might be. I’m, of course, not the only one to ponder this possibility—and, I soon learned, people have indeed successfully found meteorites this way.
</p><p>
	One example occurred in 2012, when a fireball lit up the sky over Northern California. Images of the meteor were recorded by a project called CAMS (
	<a href="https://www.sciencedirect.com/science/article/abs/pii/S0019103511003290" target="_blank">Cameras for Allsky Meteor Surveillance</a>)—a project of NASA and the <a href="https://www.seti.org/cams" target="_blank">SETI Institute</a>. These observations allowed the object’s trajectory and landing zone to be estimated, and coverage of the event in <em>The</em><em>San Francisco Chronicle</em> soon led to the discovery of what became known as the <a href="https://cams.seti.org/index-N.html" target="_blank">Novato Meteorite</a>.
</p><p>
	CAMS is not the only such project looking for meteors. Another is the 
	<a href="https://globalmeteornetwork.org/scientific-mission/" target="_blank">Global Meteor Network</a>, whose mission is to observe the night sky with “a global science-grade instrument.” Organizers of this network even <a href="https://globalmeteornetwork.org/wiki/index.php?title=Build_A_Camera" target="_blank">provide guidance</a> for how anyone can build a suitable camera based on the Raspberry Pi and how to contribute observations that can help determine the orbits of the parent asteroids that spawned particular meteors.
</p><p>
	I was tempted to join the cause, but after reading more on the subject I discovered alternative strategies for building a camera to survey the night sky. Ultimately, I decided that I wanted to capture attractive color images more than I wanted to contribute data to the Global Meteor Network, which uses black-and-white cameras because of their greater sensitivity.
</p><p><img id="406a3" data-rm-shortcode-id="9335e96628c5e30f871e50fcf0390cc6" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/this-illustration-shows-many-of-the-components-used-in-this-project.jpg?id=34161366&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/this-illustration-shows-many-of-the-components-used-in-this-project.jpg?id=34161366&amp;width=980" width="1280" height="1433" alt="This illustration shows many of the components used in this project. "><small placeholder="Add Photo Caption...">The required components include a Raspberry Pi microcomputer (case not shown), a Raspberry Pi High Quality camera, a lens, a dome-shaped transparent lens cover, a 5-volt power supply, and a waterproof bulkhead connector, allowing AC-mains power to pass through the wall of the waterproof enclosure  (not shown) holding the camra.</small><small placeholder="Add Photo Credit...">James Provost</small></p><p>
	So I opted to build a different kind of all-sky camera, one that is also based on a Raspberry Pi but that uses the Raspberry Pi High Quality color camera, following the lead of a project called, reasonably enough, 
	<a href="https://github.com/thomasjacquin/allsky" target="_blank">Allsky Camera</a>.
</p><p>
	The hardware for this project consists of a Raspberry Pi and either the 
	<a href="https://www.raspberrypi.com/products/raspberry-pi-high-quality-camera/" target="_blank">Raspberry Pi HQ camera</a> or one of the purpose-built <a href="https://astronomy-imaging-camera.com/product-category/planetary-cameras/" target="_blank">planetary cameras made by ZWO</a>. To be truly “all sky,” the camera should be equipped with a fish-eye lens having a 180-degree field of view. Recognizing that my home is surrounded by trees, I opted for <a href="https://www.arducam.com/product/arducam-cs-lens-for-raspberry-pi-hq-camera-120-degree-ultra-wide-angle-cs-mount-lens-3-2mm-focal-length-with-manual-focus-ln051" target="_blank">a lens with a narrower (120-degree) field of view</a>. A modern <a href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/" target="_blank">Raspberry Pi 4</a> is recommended, but I used a several-year-old <a href="https://www.raspberrypi.com/products/raspberry-pi-3-model-b/" target="_blank">Raspberry Pi 3 Model B</a> simply because I had it on hand. I decided to use a US $60 Raspberry Pi HQ camera over a ZWO camera because it offered higher resolution.
</p><p>
	To protect this hardware from the elements, I housed the Pi, camera, and a suitable wall wart for powering the Pi inside 
	<a href="https://www.amazon.com/DJC-Supply-Weather-Plastic-Junction/dp/B07ZBPJ9QL" target="_blank">a $25 waterproof plastic enclosure</a>. The opening I cut for the camera lens is covered with a $16 clear acrylic dome. The <a href="https://www.amazon.com/F-ber-Diameter-Acrylic-Replacement-Security/dp/B07L6GLTNP" target="_blank">first dome</a> I purchased distorted things, but I ordered <a href="https://www.amazon.com/JMX-Replacement-Security-Skylight-Transparent/dp/B07KGFTHL3" target="_blank">another one</a> that worked out much better. I also purchased <a href="https://www.amazon.com/iUniker-Raspberry-Cooling-Heatsink-Removable/dp/B079M96KWZ" target="_blank">an $11 case for the Raspberry Pi</a> (one that included a fan) and <a href="https://www.amazon.com/gp/product/B0043GD6XK/ref=ewc_pr_img_3?smid=A3RPN0HBLXDN8Z&amp;psc=1" target="_blank">a long extension cord</a>, which I cut and connected to <a href="https://www.amazon.com/SZJELEN-Electronics-Connector-waterproof-connector/dp/B07C4KQLPD" target="_blank">a waterproof bulkhead connector</a>. This means I can leave the unit outside even when it rains.
</p><p>
	Following the guidance provided in a very nice 
	<a href="https://www.youtube.com/watch?v=7TGpGz5SeVI" target="_blank">tutorial video</a>, I found it straightforward to set up the Allsky Camera software on my Pi, running it in a “headless” configuration—meaning without a monitor or keyboard. I access it wirelessly from a laptop through my local area network using <a href="https://www.ssh.com/academy/ssh" target="_blank">SSH</a>.
</p><p><img id="b6c84" data-rm-shortcode-id="e9158cbebdbe253d17f2d101ff0aea4c" data-rm-shortcode-name="rebelmouse-image" lazy-loadable="true" src="https://spectrum.ieee.org/media-library/this-illustration-shows-two-cameras-capturing-images-of-a-glowing-meteor-trail-depicted-as-though-these-images-were-projected-o.jpg?id=34161399&amp;width=980" data-runner-src="https://spectrum.ieee.org/media-library/this-illustration-shows-two-cameras-capturing-images-of-a-glowing-meteor-trail-depicted-as-though-these-images-were-projected-o.jpg?id=34161399&amp;width=980" width="2500" height="2495" alt="This illustration shows two cameras capturing images of a glowing meteor trail, depicted as though these images were projected on the walls of a planetarium. "><small placeholder="Add Photo Caption...">A meteor’s trajectory through the atmosphere can hold clues to the location of any part of it that survives and also reveals the orbit of the parent body around the sun. With images of the meteor trail captured by two cameras, that trajectory can be ascertained: The position of a glowing trail relative to the background stars in an image defines a plane, and the intersection of two planes defines the trajectory.</small><small placeholder="Add Photo Credit...">James Provost</small></p><p>
	I fired everything up—but the camera didn’t work at all. So I turned to the 
	appropriate troubleshooting section in the project’s <a href="https://htmlpreview.github.io/?https://raw.githubusercontent.com/thomasjacquin/allsky/master/html/documentation/index.html" target="_blank">ample documentation</a> and tried what was advised there—to enable “Glamor” graphic acceleration on the Pi. Still no images, though. Eventually, I discovered <a href="https://github.com/raspberrypi/libcamera-apps/issues/144" target="_blank">some tweaks to a configuration file</a> that are needed when using the HQ camera on a Pi 3B, which allowed me to obtain a hopelessly blurry image of the ceiling of my office.
</p><p>
	Through trial and error, I was able to get the manual focus of the camera dialed in properly. And slowly I learned how to adjust the multitude of settings available in the Allsky Camera software, which is done either by editing a configuration file or, more conveniently, through a Web interface this software provides.
</p><p>
	For example, I learned that I should reduce the resolution of the images used to make time-lapse videos, lest images saved at the impressive native resolution of the HQ camera (4,056 by 3,040 pixels) overwhelm the processing and storage available on my Pi. While that required tweaking a configuration file, other settings can be adjusted using the Web interface, which also makes it very easy to view live images, browse images collected earlier, and view and save time-lapse videos.
</p><p><span data-rm-shortcode-id="117d1d655dc870b1f0c3d66e20d51151"><iframe type="lazy-iframe" data-runner-src="https://www.youtube.com/embed/AF89ZiQA_oo?rel=0" width="100%" height="auto" frameborder="0" scrolling="no"></iframe></span><small placeholder="Add Photo Caption...">This timelapse video shows the night sky giving way to the rosy-fingered dawn, as captured by this all-sky camera.</small><small placeholder="Add Photo Credit...">Spectrum Staff</small></p><p>
	One thing that puzzled me early on was how well such a camera would work to catch meteors flashing by, given that the camera takes still images, not many-frames-per-second videos. But my concerns diminished after capturing images of the night sky over my home, some of which caught the light of passing aircraft. The long trails of light in those images made it apparent that the exposure time must be at least some tens of seconds long. I knew these were aircraft, not meteor trails, because the streaks included parallel tracks (from wingtip lights) and obvious pulsations from strobes.
</p><p>
	I hope yet to capture meteors some day with this gizmo. For that, I may go camping in the mountains in mid-August, when the 
	<a href="https://in-the-sky.org/news.php?id=20230813_10_100" target="_blank">Perseids</a> are hitting their peak. My family and I had taken such a trip years ago, but I didn’t have an all-sky camera at the time. So I returned home with only some now-fading memories of the wonderous show nature had put on display above our heads. Next time, I’ll have something I can view over and over!
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[QX82: Tiny JavaScript game/experience engine inspired by retro 80s computing (107 pts)]]></title>
            <link>https://btco.github.io/qx82/</link>
            <guid>37850459</guid>
            <pubDate>Wed, 11 Oct 2023 21:27:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://btco.github.io/qx82/">https://btco.github.io/qx82/</a>, See on <a href="https://news.ycombinator.com/item?id=37850459">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
  <p>QX82</p>
  <p><img id="deco" src="https://btco.github.io/qx82/images/deco.svg"></p>

  <p>QX82 is a tiny Javascript engine that lets you create games and
  experiences inspired by the look and feel of a retro 80s computer.
  It's not an emulator or a fantasy console: it's just a Javascript library.</p>

  <p>It's open source. You can download it from the
    <a href="https://github.com/btco/qx82">GitHub repo</a>.</p>

  <p><img src="https://btco.github.io/qx82/images/screenshot0.webp">
    <img src="https://btco.github.io/qx82/images/screenshot1.webp">
    <img src="https://btco.github.io/qx82/images/screenshot2.webp">
    <img src="https://btco.github.io/qx82/images/screenshot3.webp">
  </p>

  <p>Although this engine tries to recreate the feel of retro computers, you
  are not limited to the functionality of old computers: this is just
  Javascript and you can do whatever you want with no artificial
  restrictions.</p>

  <p>It's open-source and MIT-licensed, so you can freely see how it
  works and make modifications to it as you need.</p>

  <p>Want to see it in action? Check out the demos (use arrow keys
  to move): </p>
  

  <p>This engine was developed by <b>Bruno Oliveira</b>. I'm <b>@btco_code</b> on
  <a href="https://twitter.com/btco_code">Twitter</a> and on
  <a href="https://mastodon.gamedev.place/@btco_code">Mastodon</a>.
  If you want to write me an email, my email address is my first name followed
  by "tc" at gmail.com.</p>

  <p><b>Note about mobile support:</b> This engine was mostly designed for
  desktop, but on mobile devices it shows a virtual joystick with a DPAD and 2
  buttons. However, mobile support is currently <b>very experimental</b> and
  quite buggy, I'll try to fix it soon!</p>

  <h2>What does the API look like?</h2>
  <p>It's a straightforward immediate mode API inspired by BASIC:</p>

  <pre><em>// Clear screen (7 = white, 1 = blue).</em>
<b>qx.color</b>(7, 1);
<b>qx.cls</b>();
<em>// Print hello at pos (1,1).</em>
<b>qx.locate</b>(1, 1);
<b>qx.print</b>(<u>"Hello world!\n\n"</u>);
<em>// Ask the user's name.</em>
<b>qx.print</b>(<u>"What is your name? "</u>);
<em>// Wait for user to type:</em>
<u>const</u> name = <u>await</u> <b>qxa.readline</b>();
<em>// Print back the name in yellow (14).</em>
<b>qx.color</b>(14);
<b>qx.print</b>(<u>`\n\nHi, {name}.`</u>);</pre>

  <p>There's also a more traditional frame-based API where you
  get a callback to draw every frame, which works best for action games
  that run continuously without blocking for user input.</p>

  <h2>Graphics</h2>
  <p>Like in the old times, a lot of graphics is accomplished via
  characters. You can define the graphics for each character code,
  so you can use this for the player, enemies, projectiles, fancy
  borders, or anything else you want.</p>

  <p><img src="https://btco.github.io/qx82/images/chars.webp">
  </p>

  <p>You can of course change this font file to draw anything you
  want. Then you can use the character code (these are numbered from 0x00
  to 0xFF) to draw:</p>

  <pre><em>// Set color (yellow over blue).</em>
<b>qx.color</b>(14, 1);
<em>// Draw hero (char code 0xc0).</em>
<b>qx.locate</b>(5, 5);
<b>qx.print</b>(<u>"\uc0"</u>)</pre>
  
  <p>And if you don't want to be constrained by text-mode rows and
  columns, you can draw directly using pixel coordinates too:</p>

  <pre><em>// Draw hero at pixel pos (50, 50).</em>
<b>qx.spr</b>(0xc0, 50, 50);</pre>
  
  <p>Ready to create your game? Continue to
    the <a href="https://btco.github.io/qx82/howto.html">How-To section</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Grind – A first person shooter for Amiga 500 (297 pts)]]></title>
            <link>https://www.indieretronews.com/2023/10/grind-first-person-shooter-for-amiga.html</link>
            <guid>37850265</guid>
            <pubDate>Wed, 11 Oct 2023 21:06:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.indieretronews.com/2023/10/grind-first-person-shooter-for-amiga.html">https://www.indieretronews.com/2023/10/grind-first-person-shooter-for-amiga.html</a>, See on <a href="https://news.ycombinator.com/item?id=37850265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-4334126658415320957" itemprop="articleBody">
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicybcxLJAPSbuQyK3nzjBKqm0v9Katv4cgQbRDw6tL-VXvcIBMBFuZdV78u-gb1W2AQfQxur0pWPVSoJvQhtTOAH-BeQ_A6k9Us3HXxl80oXNi6mJWePdsPeztqbeCzL-hurHBPdaMHOOZu4PgVkYPbTPUwWa6bygnUQsS8GYj_DIc3mM8DXNPbFbVF5A/s1103/GRIND.jpg"><img data-original-height="796" data-original-width="1103" height="462" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicybcxLJAPSbuQyK3nzjBKqm0v9Katv4cgQbRDw6tL-VXvcIBMBFuZdV78u-gb1W2AQfQxur0pWPVSoJvQhtTOAH-BeQ_A6k9Us3HXxl80oXNi6mJWePdsPeztqbeCzL-hurHBPdaMHOOZu4PgVkYPbTPUwWa6bygnUQsS8GYj_DIc3mM8DXNPbFbVF5A/w640-h462/GRIND.jpg" width="640"></a></p><p>'Dread' has been featured many times on Indie Retro News, as with every new update the Amiga 500 version looked better than ever with fabulous new textures and new zones to visit. Well if you're looking for more gaming news on this upcoming first person shooter, we have not only been informed that a new demo has been made available, but the latest footage and detailed press release shows that John is true to his word in bringing a Doom-like experience to the Amiga as the holy-grail of Amiga gaming! So without further-ado, here's the latest blurb about this incredible looking game.<span></span></p><p><iframe allowfullscreen="" height="366" src="https://www.youtube.com/embed/doD7hmlKun8" width="640" youtube-src-id="doD7hmlKun8"></iframe></p><p><iframe allowfullscreen="" height="366" src="https://www.youtube.com/embed/11TYIX6TRyg" width="640" youtube-src-id="11TYIX6TRyg"></iframe></p><p>'"Darkenward east' is planned to be one of the early levels in the game, taking place in a city area. The map is in it's very early stages and will be overhauled once more levels are introduced and the enemy roster gets more complete (currently there's a lack of low tier enemies and other important classes). This release in general marks the complete transition of the project in regards to its visuals and now fully embraces the Steampunk/Lovecraftian aesthetic with the addition of the new HUD and protagonist, as well as brand new, high quality Weapon designs!"&nbsp;&nbsp;</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9uzjdulEMJTcR-BV-YkdPNFS1ZeZOFlW32H6HKqvy1RCyZm2x5PVzWZ4mVNip_I61GHq299dfMaEGY1mnIxmS_9IZb4N7aHdVkGzEqPC7kgr0tmXWuLSyJjyVGyFcqdWQ3NmOcj4Q8S_SPoepkipU_2qVVf5RPN6PxT5huEQYss9KqP0dhR67BtbSxqY/s1154/Clipboard01.jpg"><img data-original-height="832" data-original-width="1154" height="462" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9uzjdulEMJTcR-BV-YkdPNFS1ZeZOFlW32H6HKqvy1RCyZm2x5PVzWZ4mVNip_I61GHq299dfMaEGY1mnIxmS_9IZb4N7aHdVkGzEqPC7kgr0tmXWuLSyJjyVGyFcqdWQ3NmOcj4Q8S_SPoepkipU_2qVVf5RPN6PxT5huEQYss9KqP0dhR67BtbSxqY/w640-h462/Clipboard01.jpg" width="640"></a></p><p>Here's a list of features still missing (and planned to be added soon) as well as other details that are to be worked next, in order to reach the 'vertical slice' short-term goal:&nbsp;</p><ul><li>Music support for the Amiga version</li><li>SFX overhaul (includes adding ambient stuff and enemy growls)</li><li>Further polish on weapon models</li><li>Complete the weapon models replacement (Chaingun and Missile-launcher are still wip - the demo only features the pistol and shotgun for now)</li><li>Add secondary weapon attacks (using RMB)</li><li>Existing enemies polishing (various high-tech details still need altering or removing from some models)</li><li>Create and add new enemy types&nbsp; &nbsp;</li><li>Complete and add 2 more levels (the previous and the next one). This step will also put to the test the newly added level progression system and pave the way for more levels to come.&nbsp;</li></ul><div><p><b>System Requirements:</b></p><div><ul><li>Amiga - ADF version:</li><li>Minimum: a500 with 512 CHIP and 512 OTHER ram</li><li>Recommended: a1200 + fast ram&nbsp;</li></ul></div></div><p><b>Other news:</b></p><p>----------</p><p>"Lately the Grind team has grown as various, well-known coders have joined to help with the game's production: Namely BSzili (known for his Amiga ports of Dark Forces, Exhumed, Blood, Shadow Warrior, Blake Stone and others) who's handling the Amiga version and coder Kabuto from the demogroup Titan, looking at a possible Mega Drive version as well! Worth noting is that this latest Patreon-only binary release also included an Atari ST version (though it's not ready for a public showcase yet as it misses key features)."&nbsp;</p><p><b>Links:</b></p><p>-----</p><p>-Grind's Patreon: <a href="https://www.patreon.com/Grind_Amiga">https://www.patreon.com/Grind_Amiga</a>&nbsp;(Demo)</p><p>-Grind's Pixelglass page: <a href="https://pixelglass.org/#grind">https://pixelglass.org/#grind</a></p><p>-Grind's Discord server: <a href="https://discord.gg/QXGQbkRCxN">https://discord.gg/QXGQbkRCxN</a></p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: See library availabilities for your Goodreads want-to-read list (191 pts)]]></title>
            <link>https://projecttbr.com/?goodreadsProfile=121455547-bella-vice-van-heyde&amp;library=nypl,spl&amp;format=ebook-kindle</link>
            <guid>37850205</guid>
            <pubDate>Wed, 11 Oct 2023 21:01:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://projecttbr.com/?goodreadsProfile=121455547-bella-vice-van-heyde&#x26;library=nypl,spl&#x26;format=ebook-kindle">https://projecttbr.com/?goodreadsProfile=121455547-bella-vice-van-heyde&#x26;library=nypl,spl&#x26;format=ebook-kindle</a>, See on <a href="https://news.ycombinator.com/item?id=37850205">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[SEC Filing – Microsoft Corporation (208 pts)]]></title>
            <link>https://microsoft.gcs-web.com/node/31951/html</link>
            <guid>37850110</guid>
            <pubDate>Wed, 11 Oct 2023 20:52:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://microsoft.gcs-web.com/node/31951/html">https://microsoft.gcs-web.com/node/31951/html</a>, See on <a href="https://news.ycombinator.com/item?id=37850110">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="block-nir-pid1962-content">
  
    
      
    <xbrl>
<!--?xml version="1.0" encoding="utf-8" ?-->


<title>8-K</title>
<meta http-equiv="Content-Type" content="text/html">

                    
                    
    <div><p>UNITED STATES</p><p>SECURITIES AND EXCHANGE COMMISSION</p><p>WASHINGTON, D.C. 20549</p> <p>FORM <span><ix:nonnumeric name="dei:DocumentType" contextref="duration_2023-10-11_to_2023-10-11">8-K</ix:nonnumeric></span></p> <p>CURRENT REPORT</p><p>PURSUANT TO SECTION 13 OR 15(D)</p><p>OF THE SECURITIES EXCHANGE ACT OF 1934</p><p>Date of Report (Date of earliest event reported) <ix:nonnumeric name="dei:DocumentPeriodEndDate" contextref="duration_2023-10-11_to_2023-10-11" format="ixt:date-monthname-day-year-en">October&nbsp;11, 2023</ix:nonnumeric></p> <p><ix:nonnumeric name="dei:EntityRegistrantName" contextref="duration_2023-10-11_to_2023-10-11">Microsoft Corporation</ix:nonnumeric></p> 
<table>
<tbody><tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><span><ix:nonnumeric name="dei:EntityIncorporationStateCountryCode" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:stateprovnameen">Washington</ix:nonnumeric></span></td>
<td>&nbsp;</td>
<td><span><span><ix:nonnumeric name="dei:EntityFileNumber" contextref="duration_2023-10-11_to_2023-10-11">001-37845</ix:nonnumeric></span></span></td>
<td>&nbsp;</td>
<td><span><span><ix:nonnumeric name="dei:EntityTaxIdentificationNumber" contextref="duration_2023-10-11_to_2023-10-11">91-1144442</ix:nonnumeric></span></span></td></tr>
<tr>
<td><p>(State or Other Jurisdiction</p><p>of Incorporation)</p></td>
<td>&nbsp;</td>
<td><p>(Commission</p><p>File Number)</p></td>
<td>&nbsp;</td>
<td><p>(IRS Employer</p><p>Identification No.)</p></td></tr> </tbody></table>
<table>
<tbody><tr>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><span><ix:nonnumeric name="dei:EntityAddressAddressLine1" contextref="duration_2023-10-11_to_2023-10-11">One Microsoft Way</ix:nonnumeric>, <ix:nonnumeric name="dei:EntityAddressCityOrTown" contextref="duration_2023-10-11_to_2023-10-11">Redmond</ix:nonnumeric>, <ix:nonnumeric name="dei:EntityAddressStateOrProvince" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:stateprovnameen">Washington</ix:nonnumeric></span></td>
<td>&nbsp;</td>
<td><span><ix:nonnumeric name="dei:EntityAddressPostalZipCode" contextref="duration_2023-10-11_to_2023-10-11">98052-6399</ix:nonnumeric></span></td></tr> </tbody></table><p>(<ix:nonnumeric name="dei:CityAreaCode" contextref="duration_2023-10-11_to_2023-10-11">425</ix:nonnumeric>) <span><ix:nonnumeric name="dei:LocalPhoneNumber" contextref="duration_2023-10-11_to_2023-10-11">882-8080</ix:nonnumeric></span></p><p>www.microsoft.com/investor</p><p>Check the appropriate box below if the Form <span>8-K</span> filing is intended to simultaneously satisfy the filing obligation of the registrant under any of the following provisions (see General Instruction A.2. below):</p>
<table>
<tbody><tr>
<td><span><ix:nonnumeric name="dei:WrittenCommunications" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:boolballotbox">☐</ix:nonnumeric></span></td>
<td><p>Written communications pursuant to Rule 425 under the Securities Act (17 CFR 230.425)</p></td></tr> </tbody></table>
<table>
<tbody><tr>
<td><span><ix:nonnumeric name="dei:SolicitingMaterial" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:boolballotbox">☐</ix:nonnumeric></span></td>
<td><p>Soliciting material pursuant to Rule <span>14a-12</span> under the Exchange Act (17 CFR <span>240.14a-12)</span></p></td></tr> </tbody></table>
<table>
<tbody><tr>
<td><span><ix:nonnumeric name="dei:PreCommencementTenderOffer" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:boolballotbox">☐</ix:nonnumeric></span></td>
<td><p><span>Pre-commencement</span> communications pursuant to Rule <span>14d-2(b)</span> under the Exchange Act (17 CFR <span>240.14d-2(b))</span></p></td></tr> </tbody></table>
<table>
<tbody><tr>
<td><span><ix:nonnumeric name="dei:PreCommencementIssuerTenderOffer" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:boolballotbox">☐</ix:nonnumeric></span></td>
<td><p><span>Pre-commencement</span> communications pursuant to Rule <span>13e-4(c)</span> under the Exchange Act (17 CFR <span>240.13e-4(c))</span></p></td></tr> </tbody></table><p>Securities registered pursuant to Section&nbsp;12(b) of the Act:</p>
<table>
<tbody><tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td><p>Title of each class</p></td>
<td>&nbsp;&nbsp;</td>
<td><p>Trading&nbsp;Symbol</p></td>
<td>&nbsp;&nbsp;</td>
<td><p>Name&nbsp;of&nbsp;exchange&nbsp;on&nbsp;which&nbsp;registered</p></td></tr>
<tr>
<td><span><ix:nonnumeric name="dei:Security12bTitle" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_us-gaap-CommonStockMember">Common stock, $0.00000625 par value per share</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:TradingSymbol" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_us-gaap-CommonStockMember">MSFT</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:SecurityExchangeName" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_us-gaap-CommonStockMember">NASDAQ</ix:nonnumeric></span></td></tr>
<tr>
<td><span><ix:nonnumeric name="dei:Security12bTitle" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesThreePointOneTwoFivePercentDueDecemberSixTwentyTwentyEightMember">3.125% Notes due 2028</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:TradingSymbol" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesThreePointOneTwoFivePercentDueDecemberSixTwentyTwentyEightMember">MSFT</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:SecurityExchangeName" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesThreePointOneTwoFivePercentDueDecemberSixTwentyTwentyEightMember">NASDAQ</ix:nonnumeric></span></td></tr>
<tr>
<td><span><ix:nonnumeric name="dei:Security12bTitle" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesTwoPointSixTwoFivePercentDueMayTwoTwentyThirtyThreeMember">2.625% Notes due 2033</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:TradingSymbol" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesTwoPointSixTwoFivePercentDueMayTwoTwentyThirtyThreeMember">MSFT</ix:nonnumeric></span></td>
<td>&nbsp;&nbsp;</td>
<td><span><ix:nonnumeric name="dei:SecurityExchangeName" contextref="duration_2023-10-11_to_2023-10-11_us-gaap-StatementClassOfStockAxis_msft-NotesTwoPointSixTwoFivePercentDueMayTwoTwentyThirtyThreeMember">NASDAQ</ix:nonnumeric></span></td></tr> </tbody></table><p>Indicate by check mark whether the registrant is an emerging growth company as defined in Rule 405 of the Securities Act of 1933 (§230.405 of this chapter) or Rule <span>12b-2</span> of the Securities Exchange Act of 1934 <span>(§240.12b-2</span> of this chapter). Emerging growth company&nbsp;<span><ix:nonnumeric name="dei:EntityEmergingGrowthCompany" contextref="duration_2023-10-11_to_2023-10-11" format="ixt-sec:boolballotbox">☐</ix:nonnumeric></span></p><p>If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period for complying with any new or revised financial accounting standards provided pursuant to Section&nbsp;13(a) of the Exchange Act.&nbsp;<span>☐</span></p></div> <hr> <div><p>Item 8.01. Other Information</p><p>On October&nbsp;11, 2023, Microsoft Corporation announced the receipt of Notices of Proposed Adjustment (“NOPAs”) from the Internal Revenue Service (the “IRS”) for the tax years 2004 to 2013. The NOPAs were received on September&nbsp;26, 2023. The primary issues in the NOPAs relate to intercompany transfer pricing. In the NOPAs, the IRS is seeking an additional tax payment of $28.9&nbsp;billion plus penalties and interest. As of September&nbsp;30, 2023, we believe our allowances for income tax contingencies are adequate. We disagree with the proposed adjustments and will vigorously contest the NOPAs through the IRS’s administrative appeals office and, if necessary, judicial proceedings. We do not expect a final resolution of these issues in the next 12 months.&nbsp;Based on the information currently available, we do not anticipate a significant increase or decrease to our tax contingencies for these issues within the next 12 months.</p><p>This Form <span>8-K</span> contains forward-looking statements, which are any predictions, projections or other statements about future events based on current expectations and assumptions that are subject to risks and uncertainties, which are described in our filings with the Securities and Exchange Commission. Forward-looking statements speak only as of the date they are made. Readers are cautioned not to put undue reliance on forward-looking statements, and Microsoft undertakes no duty to update any forward-looking statement to conform the statement to actual results or changes in the company’s expectations.</p><p>Item 9.01. Financial Statements and Exhibits</p><p>(d) Exhibits:</p> <div>
<table>
<tbody><tr>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td>99.1</td>
<td>&nbsp;&nbsp;</td>
<td><a href="#d530324dex991.htm">Microsoft on the Issues Blog </a></td></tr>
<tr>
<td>104</td>
<td>&nbsp;&nbsp;</td>
<td>Cover Page Interactive Data File (embedded within the Inline XBRL document)</td></tr></tbody></table></div></div> <hr> <div><p>SIGNATURE</p><p>Pursuant to the requirements of the Securities Exchange Act of 1934, the registrant has duly caused this report to be signed on its behalf by the undersigned hereunto duly authorized.</p>
<table>
<tbody><tr>
<td></td>
<td></td>
<td></td></tr>
<tr>
<td></td>
<td>&nbsp;&nbsp;</td>
<td>MICROSOFT CORPORATION</td></tr>
<tr>
<td></td>
<td>&nbsp;&nbsp;</td>
<td>(Registrant)</td></tr>
<tr>
<td></td>
<td colspan="2"></td></tr>
<tr>
<td>Date: October 11, 2023</td>
<td>&nbsp;&nbsp;</td>
<td><p>/<small>S</small>/ A<small>LICE</small> L. J<small>OLLA</small></p></td></tr>
<tr>
<td></td>
<td>&nbsp;&nbsp;</td>
<td>Alice L. Jolla</td></tr>
<tr>
<td></td>
<td>&nbsp;&nbsp;</td>
<td>Corporate Vice President and Chief Accounting<br>Officer</td></tr></tbody></table></div>
</xbrl>

<title>EX-99.1</title>

                    
                    
 

<center></center>



<hr size="3">

<center><div>
 <p><span color="#666666">Microsoft on the Issues Blog – An update on our IRS tax audit </span></p>
<p><span color="#666666">By Daniel Goff, Corporate Vice President, Worldwide Tax and Customs </span></p>
<p><span color="#666666">Today, we’re sharing an update about our ongoing audit with the U.S. Internal Revenue Service (IRS), including background and context for this
specific case and what we generally expect next. </span></p> <p><span color="#666666"><b>Background on the IRS audit </b></span></p>
<p><span color="#666666">For nearly a decade, as we have previously disclosed in our financial statements, Microsoft has been working with the IRS to address questions
about how we allocated our income and expenses for tax years beginning as far back as 2004. We have changed our corporate structure and practices since the years covered by the audit, and as a result, the issues raised by the IRS are relevant to the
past but not to our current practices. </span></p> <p><span color="#666666">The IRS recently sent us a series of Notices of Proposed Adjustment (NOPAs), sharing with us
for the first time detailed information and explanations of their views about the issues in question. This marks the end of the audit covering 2004 to 2013, and the beginning of a new process to resolve these
<span>decades-old</span> issues. </span></p> <p><span color="#666666">The IRS says Microsoft owes an additional $28.9&nbsp;billion in tax for
2004 to 2013, plus penalties and interest. The IRS’s proposed adjustments do not represent a final determination. Not reflected in the proposed adjustments are taxes paid by Microsoft under the Tax Cuts and Jobs Act (TCJA), which could decrease
the final tax owed under the audit by up to $10&nbsp;billion. </span></p> <p><span color="#666666">Microsoft disagrees with these proposed adjustments and will pursue an
appeal within the IRS, a process expected to take several years. We believe we have always followed the IRS’s rules and paid the taxes we owe in the U.S. and around the world. Microsoft historically has been one of the top U.S. corporate income
taxpayers. Since 2004, we have paid over $67&nbsp;billion in taxes to the U.S. </span></p> <p><span color="#666666"><b>What the dispute is about </b></span></p>
<p><span color="#666666">The main disagreement is the way Microsoft allocated profits during this time period among countries and jurisdictions. This is commonly referred
to as transfer pricing and the IRS has established regulations that allow companies to use a specific arrangement for transfer pricing, called cost-sharing. </span></p>
<p><span color="#666666">Many large multinationals use cost-sharing because it reflects the global nature of their business. Because our subsidiaries shared in the costs
of developing certain intellectual property, under those IRS cost-sharing regulations, the subsidiaries were also entitled to the related profits. </span></p> <p><span color="#666666"><b>Next steps: Proposed adjustments and IRS Appeals </b></span></p> <p><span color="#666666">We strongly believe we have acted in accordance with IRS
rules and regulations and that our position is supported by case law. We welcome the IRS’s conclusion of its audit phase which will provide us with the opportunity to work through these issues at IRS Appeals, a separate division of the IRS
charged with resolving tax disputes. </span></p>
</div></center>



<hr size="3">

<center><div>
 <p><span color="#666666">It is important to note that the IRS Appeals process will take several years to complete, and if
we are unable to come to a direct agreement with the IRS, Microsoft will then have an opportunity to contest any unresolved issues through the courts. </span></p> <p><span color="#666666">We will continue to work with the IRS and hope to reach a mutual resolution to this issue over the coming years. We will also continue to share updates on significant developments through our public quarterly and annual reports and
financial statements, as we have through this entire process. As of September&nbsp;30, 2023, we believe our allowances for income tax contingencies are adequate. </span></p>
</div></center>




  </div></div>]]></description>
        </item>
    </channel>
</rss>