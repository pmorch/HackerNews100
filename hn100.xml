<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 30 Aug 2025 22:30:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Are we decentralized yet? (203 pts)]]></title>
            <link>https://arewedecentralizedyet.online/</link>
            <guid>45077291</guid>
            <pubDate>Sat, 30 Aug 2025 19:26:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arewedecentralizedyet.online/">https://arewedecentralizedyet.online/</a>, See on <a href="https://news.ycombinator.com/item?id=45077291">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>
        This page measures the concentration of the <a href="https://fediverse.party/">Fediverse</a> and the <a href="https://atproto.com/">Atmosphere</a> according to the
        <a href="https://en.wikipedia.org/wiki/Herfindahl%E2%80%93Hirschman_index">Herfindahl–Hirschman
          Index</a> (HHI),
        an indicator from economics used to measure competition between firms in
        an industry.
        Mathematically, HHI is the sum of the squares of market shares of all servers.
      </p>

      <p>
        Values close to zero indicate perfectly competitive markets (eg. many servers, with users
        spread evenly), while values close to 10000 indicate highly concentrated monopolies (eg.
        most users on a single server). In economics, values below 100 are considered
        "Highly Competitive", below 1500 is "Unconcentrated", and above 2500 is
        considered "Highly Concentrated".
      </p>

      <p>
        This site currently measures the concentration of user data for active users: in the
        Fediverse, this data is on <a href="https://w3c.github.io/activitypub/#server-to-server-interactions">servers</a> (also known as instances);
        in the Atmosphere, it is on the
        <a href="https://atproto.com/guides/glossary#pds-personal-data-server">PDSes</a>
        that host users' <a href="https://atproto.com/guides/glossary#data-repo">data repos</a>.
        All PDSes run by the company Bluesky Social PBC are aggregated in this
        dataset, since they are under the control of a single entity. Similarly,
        mastodon.social and mastodon.online are combined as they are run by the
        same company.
        Other metrics and
        comparisons are possible, and I'm interested in exploring them.
      </p>

      <p>
        Code and data are available <a href="https://github.com/ricci/distributed-social-networks/">on
          GitHub</a>.
        Comments and pull requests, including other metrics for measuring
        distribution and resiliency, are welcome!
      </p>

      <p>
        By Rob Ricci: <a href="https://discuss.systems/@ricci">@ricci@discuss.systems</a> / 
            <a href="https://bsky.app/profile/ricci.io">@ricci.io</a> 
      </p>

          
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Sex Recession: The Share of Americans Having Regular Sex Keeps Dropping (103 pts)]]></title>
            <link>https://ifstudies.org/blog/the-sex-recession-the-share-of-americans-having-regular-sex-keeps-dropping</link>
            <guid>45075699</guid>
            <pubDate>Sat, 30 Aug 2025 15:58:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ifstudies.org/blog/the-sex-recession-the-share-of-americans-having-regular-sex-keeps-dropping">https://ifstudies.org/blog/the-sex-recession-the-share-of-americans-having-regular-sex-keeps-dropping</a>, See on <a href="https://news.ycombinator.com/item?id=45075699">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>Between trade wars and armed conflicts across the world, there is talk of an impending economic downturn. And while we at the Institute for Family Studies (IFS) are not equipped to forecast an economic recession, we can document a recession that already has America in its grips:&nbsp;<a href="https://www.politico.com/magazine/story/2018/02/08/why-young-americans-having-less-sex-216953/">the sex recession</a>.</p>

<p>Americans are having a record low amount of sex. We find that in 1990, 55% of adults ages 18–64 reported having sex weekly, according to the&nbsp;<a href="https://gss.norc.org/">General Social Survey</a>&nbsp;(GSS). But around the turn of the millennium, that number began to dip: by 2010, less than half reported having sex weekly, and by 2024, of the more than 1,000 men and women queried on this topic by the GSS, that number had fallen to just 37 percent.&nbsp;</p>

<p><img alt="" src="https://ifstudies.org/ifs-admin/resources/1-newsex-recession-trend-w640.png"></p>

<p>The sex recession has been&nbsp;<a href="https://www.politico.com/magazine/story/2018/02/08/why-young-americans-having-less-sex-216953/">documented</a>&nbsp;previously, especially&nbsp;<a href="https://ifstudies.org/blog/sexless-america-young-adults-are-having-less-sex">here</a>&nbsp;at&nbsp;<a href="https://ifstudies.org/blog/is-the-sex-recession-over">IFS</a>. In 2016, Jean Twenge&nbsp;<a href="https://link.springer.com/article/10.1007/s10508-017-0953-1?correlationId=4d9ea02c-da9a-44df-a279-bfae59b39531">found</a>&nbsp;that the decline is largely a cohort effect — younger generations are having less sex than their predecessors did. The causes? A decline in steady partnering, especially in marriage, and a decline in sexual frequency within couples.</p>

<p>Twenge’s findings largely hold true today. Between 2014 and 2024, the share of young adults, ages 18–29, who reported living with a partner, both married and unmarried, fell 10 percentage points, from 42% to 32%, according to the GSS. Because partnered adults have the most consistent sex, and more young men and women are flying solo, the share of young adults who are having regular sex keeps&nbsp;<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/jomf.12723?casa_token=_qpkI5n3I4IAAAAA%3Aya7NHKX8gAGwIfk4oewEZFGFWzUmhu5LFVtB-yDlJTJjdY89xyB40xYgoXE9AVSPzbCsquFsEIQMPvs">falling</a>.&nbsp;&nbsp;</p>

<p>When it comes to sexlessness (“no sex in the last year”) among young adults, the biggest change comes post-2010. Prior and up to 2010, the share of young adults, ages 18-29, who reported not having sex held steady around 15 percent. But from 2010 to 2024, the share doubled, from 12% to 24% in the GSS.&nbsp;</p>

<p><img alt="" src="https://ifstudies.org/ifs-admin/resources/2-sexlessness-among-ya-w640.png"></p>

<p>This “hockey-stick” shape resembles the pattern identified by Jonathon Haidt in&nbsp;<a href="https://www.anxiousgeneration.com/book"><em>The Anxious Generation</em></a>. In his book, Haidt calls the period of 2010 to 2015 the “Great Rewiring.” Adolescents going through puberty over this period were subject to all-pervasive digital media, kicked off by the smartphone proliferation of the late 2000s. Childhood became increasingly digital. Kids were not, consequently, exposed to nearly as much socialization as earlier generations. The digital media revolution left in its wake well documented increases in pathologies like&nbsp;<a href="https://www.deseret.com/2023/1/18/23558984/social-media-big-tobacco-teens-anxiety-depression-spencer-cox-utah/">anxiety</a>,&nbsp;<a href="https://link.springer.com/content/pdf/10.1007/s10567-022-00404-5.pdf">depression</a>,&nbsp;<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9176070/">self-harm</a>, and&nbsp;<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6278213/">suicide</a>.</p>

<p><img alt="" src="https://ifstudies.org/ifs-admin/resources/3-young-adults-socializing-w640.png"></p>

<p>The Great Rewiring of the early 2010s saw a rising generation of hermits. Notably, most of the decline in socialization actually happened&nbsp;<em>prior</em>&nbsp;to the pandemic. Between 2010 and 2019, the average time young adults spent with friends in a given week fell by nearly 50%, from 12.8 hours to just 6.5 hours. The pandemic pushed this number even lower, to 4.2 hours a week with friends. And while there has been a slight increase since, we seem to have found a new norm. Young adults spent just 5.1 hours with friends in a given week in 2024.</p>

<p>It's no surprise that this period is also marked by an increase in sexlessness. More time devoted to smart phones, social media, pornography, and gaming meant that young adults had fewer opportunities to develop the social skills needed to form relationships and spent less time in social settings—such as&nbsp;<a href="https://www.derekthompson.org/p/the-death-of-partying-in-the-usaand">parties</a>—that would facilitate romantic relationships. The negative consequences of gaming may also&nbsp;<a href="https://www.journals.uchicago.edu/doi/abs/10.1086/711916?casa_token=FWu2y3G63hoAAAAA%3An1V6koGvDLMcL1gUMPx5zpgOdPhv-bYMHzjyFS2lhr1YQw6uktsc09tlWjXtiooj9dEilcRf9gI&amp;journalCode=jpe">extend</a>&nbsp;to young men’s declining participation in the workforce, which reduces their&nbsp;<a href="https://www.harpercollins.com/products/get-married-brad-wilcox?variant=41546330669090">appeal</a>&nbsp;on the mating market. Indeed, one recent&nbsp;<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/jomf.12723?casa_token=_qpkI5n3I4IAAAAA%3Aya7NHKX8gAGwIfk4oewEZFGFWzUmhu5LFVtB-yDlJTJjdY89xyB40xYgoXE9AVSPzbCsquFsEIQMPvs">study</a>&nbsp;finds&nbsp;</p>

<p><em>the decline in the formation of romantic relationships and decreasing alcohol consumption are the most important [causes of falling young adult sex], but declining earnings and increasing use of computer games also play important roles.</em></p>

<p>This much is clear: young adults are spending less time dating, mating, and getting married, with obvious implications for sex. Indeed, the lack of marriage is why young adults face the brunt of the sex recession. The data tell us that married adults have markedly more sex than their unmarried peers: 46% of married men and women, ages 18-64, have weekly sex compared to about 34% of their unmarried peers.</p>

<p><img alt="" src="https://ifstudies.org/ifs-admin/resources/4-newmarried-unmarried-sex-w640.png"></p>

<p>While the decline in sex has been most acute among younger generations, older adults have not been left unscathed. And notably, the sex recession is making inroads among married couples. Between 1996 and 2008, 59% of married adults, ages 18-64, reported having sex once a week or more. That number fell to 49% for the period of 2010 to 2024. Married couples are seeing declines in sexual frequency across age groups.</p>

<p><img alt="" src="https://ifstudies.org/ifs-admin/resources/5-newmarried-couples-recession-w640.png"></p>

<p>It's no coincidence that a decline in marital sex follows the digital revolution. Today’s&nbsp;<a href="https://ifstudies.org/blog/where-have-all-the-good-men-gone">electronic opiates</a>&nbsp;not only depress partnering and marriage among young adults—they also weaken already established relationships. A&nbsp;<a href="https://ifstudies.org/report-brief/more-scrolling-more-marital-problems-less-sex-more-divorce-worries-for-couples-distracted-by-phones">2023 IFS study</a>&nbsp;found that married adults reported lower sexual frequency when their spouse substituted couple time for phone or computer use. Furthermore, bedtime procrastination is a rising habit. So-called bedtime procrastinators&nbsp;<a href="https://web.archive.org/web/20220314131918id_/https:/watermark.silverchair.com/zsz267.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAsUwggLBBgkqhkiG9w0BBwagggKyMIICrgIBADCCAqcGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMhbk3ySA7Nup6yLRJAgEQgIICeI9XvV7fwabDmJdZTI-jppMReXwqp0otgeSxJssNII3hfe-_MambEu0jYFqYid-N6gpvIqdEWl3mD0ubhPIifW_eQ9zlMK9haxZzWl1BWQ-QvLmk1Jx22kH44QPIn5yX0c2l40aAn6Mkg3m-H8oI-5wn3mLu0ZICofaUTAMI4SVdvHLJpHtQ9w4LKgtvcSUPqIhB0fpBU_5djCaBSxDXNQKMys6jpRjknjOacfKbQwEMjUhxA697XBg31zBMhUxopcZ2_ne0ImZLoAaup7paQEIoo3rap3oZ1-cMJ9ikMWOem6qmEeEfUu0PYLCyuvKnEnFFI4CKFEXSZ5fhrgShZMhiKlnrSroAyNTtkTKw6xbIbZs2ZeoZQn8TzJC_C-afBDRy1ylJJJQ8KsFhUAfz9rq8IJ_1M-mcLbPbPuf04kMig2MW6w5Bpd8TjCQ6m8cMsS5vNwhgGoVfA-tPExZOKoOms7Dw8JwekU316iB_cnJCvgj6nfEpcKs1A1-usB47wBGodqbhZ9XSsc7SJIbdKxG2F1tlKgsqvVgtfp5rLx_uriItg-zl1ubGsPZomf6OzPKtQSJuG3VmkqrNSnl4lOhT56_ay91WnpoHcuFrEyZme8sHKUaSGrpBWIxmgJr1EbLZE0Jn4iJGHhWwomMByuVWu_zvCt9KYLii66RT1ttmcOBekP-_6p35Ht29kUjDD57bH539gAY8eBdWV3ZtMicxEmgHDh2zEkw3VWMlYOvsqsmq4yIZvgqgesw85ipiXJPU04IxTHOg5qiP5LLIvqNI4RyeNydfH9by3ocjQH1sT6DYDEKirgzDrEqNnjNiXgJR0EsrONMN">spend two hours</a>&nbsp;using some form of digital media in the three hours leading up to sleep. It’s not surprising that more social media, Netflix, or gaming on the part of spouses translates to less intimacy.</p>

<p>The findings in this IFS research brief matter because regular sex is linked to&nbsp;<a href="https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-9004.2011.00408.x">better health</a>,&nbsp;<a href="https://www.harpercollins.com/products/get-married-brad-wilcox?variant=41546330669090">higher quality marriages</a>, and&nbsp;<a href="https://www.harpercollins.com/products/get-married-brad-wilcox?variant=41546330669090">greater happiness</a>. We are, as Aristotle noted, social animals, real embodied creatures who flourish from real world interactions with others. And even though many of us are tempted nowadays to live out the lion’s share of our lives, including our sexual lives, in the virtual world, the truth is that we thrive when we have regular opportunities to spend time with other human beings,&nbsp;<a href="https://www.americansurveycenter.org/research/a-cultural-crossroads-americas-uncertain-future-amidst-enduring-discontent-and-rising-disconnection/?mkt_tok=NDc1LVBCUS05NzEAAAGbkvewKzlBksy5UkP8DK3ZgRteqCvUlLg6RRzvNoAw09KIInAbzFQuPQL0HeCMPUibHA3Fz92YAMuG_OzHJyBMDqSemLVT1DGE3tV34ZM32Jrf1cE">physically</a>. That’s why America should be just as concerned about the sex recession as any other.</p>

<p><em>Grant Bailey is a Research Fellow at the Institute for Family Studies. Brad Wilcox is Distinguished University Professor of Sociology at the University of Virginia and a Senior Fellow at the Institute for Family Studies.</em></p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Have to Feel It (102 pts)]]></title>
            <link>https://mitchellh.com/writing/feel-it</link>
            <guid>45075048</guid>
            <pubDate>Sat, 30 Aug 2025 14:38:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mitchellh.com/writing/feel-it">https://mitchellh.com/writing/feel-it</a>, See on <a href="https://news.ycombinator.com/item?id=45075048">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>You see a series of checkboxes checked. Schedules met.
Requirements satisfied. Demos delivered.
It's a good day. Good job, you, good job! A promotion is in sight.</p>
<p>But you didn't feel it. <em>You didn't feel it.</em></p>
<p>We, as people, feel something with every interaction. Frustration, joy, relief,
confidence. A feeling. A person interacts with our work. Our work evokes
a feeling. The feeling matters. The feeling is part of the work. The
<em>desired feeling</em> is part of the requirements.</p>
<p>When you feel it, you know. The feature makes you smile when you use it.
It fits right in, like it was always meant to be there. You want to
use it again. You want to tell people about it.</p>
<p>This is the difference. This is what metrics, specifications, and demos
miss. They don't capture the feeling. For the people who will use and live
in the work, the feeling is part of their daily experience. Which means
you can't stop at checking the boxes on paper. You have to sit with it,
use it, live with it.</p>
<p>You have to feel it.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Condor's Cuzco RISC-V Core at Hot Chips 2025 (106 pts)]]></title>
            <link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link>
            <guid>45074895</guid>
            <pubDate>Sat, 30 Aug 2025 14:18:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot">https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</a>, See on <a href="https://news.ycombinator.com/item?id=45074895">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><span>Condor Computing,</span><strong> </strong><span>a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.</span></p><p>Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!2an9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!2an9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 424w, https://substackcdn.com/image/fetch/$s_!2an9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 848w, https://substackcdn.com/image/fetch/$s_!2an9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 1272w, https://substackcdn.com/image/fetch/$s_!2an9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!2an9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png" width="1279" height="716" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:716,&quot;width&quot;:1279,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!2an9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 424w, https://substackcdn.com/image/fetch/$s_!2an9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 848w, https://substackcdn.com/image/fetch/$s_!2an9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 1272w, https://substackcdn.com/image/fetch/$s_!2an9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1630238d-2ae4-4c37-b5d1-c1db7a6d865c_1279x716.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.</p><p>Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!IvR7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!IvR7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 424w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 848w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 1272w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!IvR7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png" width="1456" height="548" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!IvR7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 424w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 848w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 1272w, https://substackcdn.com/image/fetch/$s_!IvR7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe476ef8c-fe18-468d-9e25-cd478ea14d2f_1480x557.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AW4y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AW4y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 424w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 848w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 1272w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AW4y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png" width="1456" height="823" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:823,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!AW4y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 424w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 848w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 1272w, https://substackcdn.com/image/fetch/$s_!AW4y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16a84802-4a32-42fd-bf89-6e75ce5d3914_1600x904.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.</p><p>AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!olX5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!olX5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!olX5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!olX5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!olX5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!olX5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!olX5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!olX5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!olX5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!olX5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c6d8dfa-88d8-4eef-a550-95b009e18aa9_1600x897.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.</p><p>Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.</p><p>Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LtQn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LtQn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LtQn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png" width="1456" height="817" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:817,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LtQn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!LtQn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F26690f6d-e03a-4cb5-9c3e-f37765879dca_1600x898.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!-1Pi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!-1Pi!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 424w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 848w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 1272w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!-1Pi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png" width="1456" height="823" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/29ef4401-9d22-4005-af0b-205294710e18_1600x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:823,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!-1Pi!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 424w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 848w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 1272w, https://substackcdn.com/image/fetch/$s_!-1Pi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F29ef4401-9d22-4005-af0b-205294710e18_1600x904.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.</p><p>Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!WZiL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!WZiL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!WZiL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png" width="1456" height="817" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de934736-2aec-481d-b1a7-2e721ad25860_1600x898.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:817,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!WZiL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!WZiL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde934736-2aec-481d-b1a7-2e721ad25860_1600x898.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8gUV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8gUV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 424w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 848w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 1272w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8gUV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png" width="1456" height="804" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:804,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8gUV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 424w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 848w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 1272w, https://substackcdn.com/image/fetch/$s_!8gUV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff26a1969-520c-4798-95e9-799e1905ee7a_1600x884.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.</p><p>Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!KFlE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!KFlE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 424w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 848w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 1272w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!KFlE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png" width="1456" height="805" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:805,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!KFlE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 424w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 848w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 1272w, https://substackcdn.com/image/fetch/$s_!KFlE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6b115df6-d801-4dbf-be59-52a3645970de_1600x885.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.</p><p>XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.</p><p>On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.</p><p>Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!y8PV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!y8PV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 424w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 848w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 1272w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!y8PV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png" width="1165" height="404" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:404,&quot;width&quot;:1165,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:76406,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://chipsandcheese.com/i/172284931?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!y8PV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 424w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 848w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 1272w, https://substackcdn.com/image/fetch/$s_!y8PV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff84018c0-8784-4e0c-9d51-8e29681da1f3_1165x404.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!LNhg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!LNhg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!LNhg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png" width="1456" height="816" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!LNhg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!LNhg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7163cc13-d13f-4bb7-a6d0-570501ed0b45_1600x897.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!8yWm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!8yWm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!8yWm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png" width="1456" height="817" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/dd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:817,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!8yWm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 424w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 848w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 1272w, https://substackcdn.com/image/fetch/$s_!8yWm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd9e9a4f-7ecd-44e2-b621-660a1bf93acf_1600x898.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.</p><p>High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.</p><p>Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache misses</p><p><span>If you like the content then consider heading over to the </span><a href="https://www.patreon.com/ChipsandCheese" rel="">Patreon</a><span> or </span><a href="https://www.paypal.com/donate/?hosted_button_id=4EMPH66SBGVSQ" rel="">PayPal</a><span> if you want to toss a few bucks to Chips and Cheese. Also consider joining the </span><a href="https://discord.gg/TwVnRhxgY2" rel="">Discord</a><span>.</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI models need a virtual machine (152 pts)]]></title>
            <link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link>
            <guid>45074467</guid>
            <pubDate>Sat, 30 Aug 2025 13:25:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/">https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</a>, See on <a href="https://news.ycombinator.com/item?id=45074467">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					<p>Applications using AI embed the AI model in a framework that interfaces between the model and the rest of the system, providing needed services such as tool calling, context retrieval, etc. Software for early chatbots took user input, called the LLM, and returned the result to the user; essentially just a read-eval-print loop. But, as the capabilities of LLMs have evolved and extension mechanisms, such as <a href="https://modelcontextprotocol.io/docs/getting-started/intro">MCP</a> were defined, the complexities of the control software that calls the LLM have increased.&nbsp; AI software systems require the same qualities that an operating system provides, including security, isolation, extensibility, and portability. &nbsp;For example, when an AI model needs to be given a file as part of its context, access control must be established that determines if the model should be allowed to view that file. &nbsp;We believe it is time to consider standardizing the ways in which the AI models are embedded into software and<strong>&nbsp;think of that control software layer as a virtual machine</strong>, where one of the machine instructions, albeit a super-powerful one, is to call the LLM.</p>
<p>Our approach <strong>decouples model development from integration logic</strong>, allowing any model to “plug in” to a rich software ecosystem that includes tools, security controls, memory abstractions, etc. Similar to the impact that the Java Virtual Machine had, creating a specification of a VM for the AI orchestrator could enable a “write once, run anywhere” execution environment for AI models while at the same time providing familiar constraints and governance to maintain security and privacy in existing software systems. Below we outline related work in this direction, the motivation behind it, and the key benefits of an AI Model VM.</p>
<p><strong>Introduction</strong></p>
<p>AI models are being leveraged in existing software as application copilots, embedded in IDEs, and with the rise of the MCP protocol, are increasingly able to use tools, implement agents, etc. This rapid evolution of valuable use cases brings with it a greater need to ensure that the AI-powered applications maintain privacy, are secure, and operate correctly.&nbsp; Guarantees of <strong>security and privacy are best provided if the underlying system is secure by design</strong> and not added on to systems as an afterthought.&nbsp; We take the Java Virtual Machine (JVM) as our inspiration in making the case for the importance of a standard AI Virtual Machine.&nbsp; The Java Virtual Machine guarantees memory safety by design, defines access control policies, and prevents code injection with bytecode verification.&nbsp; These properties allow Java programs running on the JVM to be executed with trust despite being shipped remotely, enabling “write once, run anywhere” software distribution.</p>
<div id="attachment_3031"><p><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-3031" src="https://blog.sigplan.org/wp-content/uploads/2025/08/ai-vm-example-aug2025.png" alt="A flowchart illustrating how an AI system processes the user prompt &quot;Book my flight.&quot; It shows the prompt being sent to an AI Model Virtual Machine, which forwards it to the AI Model. The model responds with &quot;use booking tool.&quot; The virtual machine then checks if the booking tool is allowed, and if so, it calls the tool. The diagram highlights the decision-making steps and permission checks involved in executing user requests through AI systems." width="1567" height="760" srcset="https://blog.sigplan.org/wp-content/uploads/2025/08/ai-vm-example-aug2025.png 1567w, https://blog.sigplan.org/wp-content/uploads/2025/08/ai-vm-example-aug2025-1280x621.png 1280w, https://blog.sigplan.org/wp-content/uploads/2025/08/ai-vm-example-aug2025-980x475.png 980w, https://blog.sigplan.org/wp-content/uploads/2025/08/ai-vm-example-aug2025-480x233.png 480w" sizes="(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) and (max-width: 980px) 980px, (min-width: 981px) and (max-width: 1280px) 1280px, (min-width: 1281px) 1567px, 100vw"></p><p id="caption-attachment-3031">Example illustrating how an AI model virtual machine interacts with the AI model and the external environment.</p></div>
<p>How does the JVM relate to applications that use AI models?&nbsp; We used the following example to explain:</p>
<p>The diagram illustrates the role of the software layer that interacts with an AI model, which we call the Model Virtual Machine (MVM).&nbsp; That layer intermediates between the model and the rest of the world. &nbsp;For example, a chatbot user might type a prompt (1) that the MVM then sends unmodified to the AI model (2).&nbsp; In practice, the MVM will add additional context, including the system prompt, chat history, to the AI model input as well. &nbsp;The AI model generates a response, which in the example requires a specific tool to be called (3).&nbsp; This response has a specific format that is mutually agreed upon between the model and the MVM, such as MCP. In our example, because it is important to restrict the model from making undesired tool calls, the MVM first consults the list of allowed tools (4) before deciding to call the tool the model requested (5).&nbsp; This check (4) guarantees that the model doesn’t make unauthorized tool calls.&nbsp; Every commercial system using AI models requires some version of this control software.</p>
<p>We make the analogy that the interface with the LLM should be a virtual machine. &nbsp;If that is the case, what are the instructions that the machine can execute?&nbsp; Here are examples of operations that existing AI model interfaces have:</p>
<ul>
<li>Certifying, loading, initializing, and unloading a given AI model</li>
<li>Calling a model with context</li>
<li>Parsing the output from the model</li>
<li>Certifying, loading, initializing, and unloading tools</li>
<li>Calling a tool</li>
<li>Parsing the results from a tool call</li>
<li>Storing the results from a tool call into memory</li>
<li>Asking the user for input</li>
<li>Adding content to a history memory</li>
<li>Standard control constructs such as conditionals, sequencing, etc.</li>
</ul>
<p>A VM would support all of these operations in a well-typed context where constraints are placed on the calls made, the arguments passed, etc.</p>
<p><strong>Existing Work Informs What is Needed</strong></p>
<p>Some of the required elements of a well-specified interface are emerging in AI systems explored in academic work and in applications that are widely deployed:</p>
<ul>
<li><strong>OpenAI’s Structured Tool Calling Protocols</strong>: &nbsp;OpenAI introduced a <a href="https://nesin.io/blog/openai-api-json-response">JSON-based function calling API</a> that lets models invoke code-defined functions in a structured way. This approach, along with OpenAI’s plugin system (which uses OpenAPI specifications for tools), showed how structured <strong>tool-calling protocols</strong> can reduce ambiguity and simplify integration.</li>
<li><strong>Anthropic’s Model Context Protocol (MCP, 2024):</strong> <a href="https://docs.anthropic.com/en/docs/mcp">MCP</a> is an <strong>open protocol</strong> for connecting AI assistants to external data and tools, explicitly aiming to be a universal interface. <em>“Think of MCP like a USB-C port for AI applications,”</em> Anthropic explains. Instead of every service having a custom AI integration, MCP provides a common schema and client-server approach. Despite being relatively new, MCP adoption, including in large companies, has been rapid.</li>
<li><strong>Secure Orchestrators – FIDES &amp; AC4A (2025):</strong> Security remains a weak point in current AI systems. Two recent projects propose runtime-level controls. <a href="https://github.com/microsoft/fides"><strong>FIDES</strong></a> (by Microsoft Research) <strong>enforces information-flow policies</strong> on agents by tracking data confidentiality labels and adding new agent actions like “inspect” to limit what agents can access (where a quarantined LLM can safely summarize restricted data) (<a href="https://arxiv.org/pdf/2505.23643">paper</a>). <strong>AC4A (Access Control for Agents) </strong>(manuscript in preparation) takes an OS-style approach: All tools and data are organized into hierarchies (like files and folders), and the agent must request <em>read/write</em> access for each resource. AC4A’s runtime intercepts every agent action and blocks anything not permitted, forcing a <em>least-privilege</em> operation mode. These projects show how a standard AI VM could include built-in <strong>security and access control</strong>, just as modern operating systems do. Even with strong access controls built into a VM specification, AI models present new security challenges that need to be considered in the design.&nbsp; For example, an AI model, when prevented from accessing a particular item of data, might use its chain-of-thought reasoning to devise ways to gather accessible data that allows it to infer the inaccessible item.&nbsp; As such, security researchers have to devise new mitigations to prevent AI models taking adversarial actions even with the virtual machine constraints.</li>
<li><strong>Open-Source Agent Runtimes:</strong> Several projects are actively building general-purpose runtimes for AI. For example, <a href="https://www.langchain.com/">langchain</a> and <a href="https://learn.microsoft.com/en-us/semantic-kernel/overview/">Semantic Kernel</a> provide numerous common runtime services that make writing reliable AI-enabled applications easier.&nbsp; The <a href="https://www.microsoft.com/en-us/research/blog/ai-controller-interface-generative-ai-with-a-lightweight-llm-integrated-vm/?msockid=3fee8da42937640409ff824528846516"><strong>AI Controller Interface (AICI)</strong></a> (later renamed <a href="https://github.com/guidance-ai/llguidance"><strong>llguidance</strong></a>), integrates a lightweight VM into the model-serving pipeline, allowing developers to script and constrain model behavior at a low level (e.g., control of generations token-by-token).</li>
</ul>
<p>Defining a specification for a VM interface for AI systems from these emerging approaches will require more than an agreement on protocols and APIs. Because AI systems derive their behavior from training data, model training data must reflect the specification of the VM interface so that the models and the VM model interface can co-evolve. This will enable otherwise diverse models to exhibit broadly compatible behavior with respect to the VM interface specification.</p>
<p><strong>Benefits of a Well-Specified AI Model VM</strong></p>
<p>As mentioned, many applications that leverage AI models require reliability, privacy, and security.&nbsp; In addition, new models are developed almost daily and updating the model being used by an application is often necessary.&nbsp; Given this confluence of factors, creating robust AI software presents significant engineering challenges. We believe that a specification of the interface between the AI model and the surrounding software that interfaces to it will address some of these challenges.</p>
<p>The need for an AI Model VM specification is driven by several clear motivations:</p>
<ul>
<li><strong>Separation of Concerns:</strong> An interface specification enforces a clean separation between <em>model logic</em> and <em>integration logic</em>. This means <strong>models become interchangeable</strong> components. You could swap in a new model (or move an agent to a different platform) and, as long as both adhere to the standard, everything still works. Likewise, virtual machine implementors can increase the performance, security, and tooling of the virtual machine while maintaining compatibility with the AI model interfaces.</li>
<li><strong>Built-in Safety and Governance:</strong> A VM specification can enforce <strong>safety by design</strong>. By routing all tool usage and external access through a well-defined interface, it becomes easier to apply permission checks, audit logs, and fail-safes.&nbsp; As shown by projects like AC4A, the VM can act as a gatekeeper, restricting what models can do unless explicitly authorized. This creates a safer deployment solution for powerful AI systems: even if the model behaves unpredictably, the VM layer can contain its effects. Standards bodies could even define security requirements (e.g., certain calls must always require user confirmation), creating a shared foundation of trust. Similar to the benefits of signed assemblies in the Common Language Runtime, have a certification process around loading and unloading models and tools ensures the end-to-end security of the supply chain.</li>
<li><strong>Transparent Performance &amp; Resource Tracking:</strong> A VM specification could also give developers visibility to runtime diagnostics. Post-execution manifests could report model performance, resource consumption, and data access level which helps developers evaluate overall efficiency and performance. Benchmarks for accuracy, utility, and responsiveness can be supported directly in the VM interface across models and platforms.</li>
<li><strong>Verifiability of Model Output</strong>: Leveraging a VM specification, experts can explore integrating formal methods to verify their model behavior. Techniques such as zero-knowledge proofs could confirm the integrity of model outputs without sensitive internal logic. While still emerging, this possibility hints at new levels of trust and accountability in AI systems and should be carefully considered during development.</li>
</ul>
<p><strong>Conclusion</strong></p>
<p>We argue that a well-specified AI Model Virtual Machine is needed.&nbsp; Developments occurring in multiple directions, including work from tech companies, startups, and academia, all motivate the need for a VM specification that lets AI models safely and seamlessly interact with the world around them. The motivation is clear – reducing complexity and unlocking interoperability – and the potential benefits range from technical (faster development, modular upgrades) to strategic (cross-platform AI ecosystems, improved safety). From enforcing controls for security and privacy, to potentially formal proof capabilities for trust, the opportunities are wide-ranging. Learning a lesson from older generations of software virtualization, a VM specification can increase AI systems portability, interoperability, security, and reliability. The purpose of this document is to highlight these issues and start engaging with the community on building a consensus that such a specification is needed and what it should include.</p>
<p><em>Biographies:</em></p>
<p><i data-olk-copy-source="MessageBody">Shraddha Barke</i>&nbsp;is a&nbsp;Senior Researcher at Microsoft Research in Redmond, Washington in the Research in Software Engineering (RiSE) group. Her research interests include AI for proof generation, training AI models for program-reasoning tasks using RL and improving the reliability of AI agents.</p>
<p><span data-teams="true"><em>Betül Durak</em> is a Principal Researcher at Microsoft Research in Redmond, Washington in Security, Privacy, and Cryptography group. Her research interests broadly include security analysis as well as secure and private protocol designs motivated from real world problems.</span></p>
<p><i data-olk-copy-source="MessageBody">Dan Grossman</i>&nbsp;is a Professor at the University of Washington and the Vice Director of the Paul G. Allen School of Computer Science &amp; Engineering. His research interests are in programming languages, particularly in applying programming languages concepts and analyses to emerging domains.</p>
<p><em>Peli de Halleux</em>&nbsp;is a Principal Research Software Developer Engineer in Redmond, Washington working in the Research in Software Engineering (RiSE) group. His research interests include empowering individuals to build LLM-powered applications more efficiently.</p>
<p><em><span data-teams="true">Emre Kıcıman</span></em> is a Senior Principal Research Manager and Head of Research for Copilot Tuning at Microsoft. His research interests include causal methods, the security of AI, and applications of LLM and AI-based systems, together with their implications for people and society.</p>
<p><em>Reshabh K Sharma</em>&nbsp;<span data-teams="true">is a PhD student at the University of Washington. His research lies at the intersection of PL/SE and LLMs, focusing on developing infrastructure and tools to create better LLM-based system that are easier to develop reliably and correctly.</span></p>
<p><em>Ben Zorn</em> is a Partner Researcher at Microsoft Research in Redmond, Washington working in (and previously having co-managed) the Research in Software Engineering (RiSE) group. His research interests include programming language design and implementation, end-user programing, and AI software including technology for ensuring responsible AI.</p>
<p><strong>Disclaimer:</strong> <em>These posts are written by individual contributors to share their thoughts on the SIGPLAN blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGPLAN or its parent organization, ACM.</em></p>
					</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bcachefs Goes to "Externally Maintained" (107 pts)]]></title>
            <link>https://lwn.net/Articles/1035736/</link>
            <guid>45074312</guid>
            <pubDate>Sat, 30 Aug 2025 13:07:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/Articles/1035736/">https://lwn.net/Articles/1035736/</a>, See on <a href="https://news.ycombinator.com/item?id=45074312">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
That's exactly why I've resisted the push to put someone else in that role. If it's one of the other people working on bcachefs I don't want to risk them burning out and losing another engineer; I'd be ok with it if it was another long standing member of the kernel community with experience working with Linus, but for some strange reason no one I've talked to wants to take that on. </p><p>
And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.<br>
</p><p>
Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.<br>
</p><p>
So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.<br>
</p></div><div open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 4:57 UTC (Sat)
                               by <b>NYKevin</b> (subscriber, #129325)
                              [<a href="https://lwn.net/Articles/1035793/">Link</a>] (2 responses)
      </p>
      </summary>
      <div>
<p><span>&gt; And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide.</span><br>
<span>&gt;</span><br>
<span>&gt; Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.</span></p><p>
The distros are downstreams. If they want to package and ship your code, in whatever way they see fit, you've already given them permission to do so. And they are not shy about exercising that permission. I remember several years ago, jwz asked Debian to stop shipping XScreenSaver, because he disagreed with their practice of backporting fixes to old versions. Debian said no, and XScreenSaver is still in the repository today. As you might imagine, some rather harsh words were exchanged, but in the end, both sides went back to their respective corners of the internet and proceeded to mostly ignore each other.<br>
</p><p>
Linus, however, is not in the business of playing that game. If there's nobody actively maintaining (his copy of) bcachefs, then I find it hard to believe it's going to be allowed to stick around indefinitely.<br>
</p><p>
<span>&gt; So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says.</span><br>
</p><p>
My interpretation of events is that there are only three long-term paths that make sense here:<br>
</p><p>
* You accept that you cannot control what appears in Linus's tree, but would prefer that some recent-ish version of bcachefs is there (as opposed to no bcachefs or a very old bcachefs). You designate somebody as I've described upthread, they upstream patches at whatever rate Linus is willing to take them, and everybody is more or less willing to live with the result.<br>
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They continue to ship an old bcachefs for (at least) the rest of the current release cycle, but eventually it bitrots and they delete it. You might or might not choose to ship it out-of-tree like ZFS, and various distros might or might not package some version of it for you (whether you want them to or not).<br>
* You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They fork bcachefs or mirror it from your out-of-tree version, and slightly-old or modified versions continue to appear in the kernel indefinitely. As I explained, I think this is less likely, but I don't want to entirely discount it.<br>
</p><p>
I do not see any plausible outcome where you are allowed to control what appears in Linus's tree. He has very explicitly closed the door on that. For expository purposes, and because you are a functioning adult, I have assumed that you will accept this lack of control, but that does not actually matter - one of the above scenarios will inevitably play out, regardless of your opinion of it. The only choice you have at this point is whether it's the first bullet or one of the other two.<br>
</p><p>
I do not say this to be cruel. Based on your words in this and other threads, I genuinely believe that this process has been very painful for you, and I doubt you enjoy being reminded that Linus's tree does, in fact, belong to Linus (I'm sure other developers have screamed that at you enough times by now). Unfortunately, this is not a matter of right or wrong. It is a matter of power. You are aggrieved about something that neither Linus, nor anybody else on LKML, is prepared to recognize as an injury to you. Regardless of whether that is the right way or the wrong way of looking at it, Linus is going to conduct the kernel's release cycle as he sees fit. The *healthy* way of looking at it is to accept that that is not within your power to change, and redirect your attention to the things you can change.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 16:30 UTC (Sat)
                               by <b>ttuttle</b> (subscriber, #51118)
                              [<a href="https://lwn.net/Articles/1035813/">Link</a>] 
      </p>
      </summary>
      <p>
Thank you for posting such a compassionate response. This thread could easily turn into an unkind discussion or an all-out flame war, but you went out of your way to be kind instead.<br>
</p>

      
          
        
     </details>
<a name="CommAnchor1035827"></a>
    <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 20:48 UTC (Sat)
                               by <b>linuxrocks123</b> (subscriber, #34648)
                              [<a href="https://lwn.net/Articles/1035827/">Link</a>] 
      </p>
      </summary>
      <div><p>
koverstreet, I agree with NYKevin that it sounds like Linus isn't interested in working with you in the way you would like to work with him, and therefore I agree with him that one of the three bullet points are most likely to occur.</p><p>
But I disagree with him that you should focus any attention on that.  The healthiest way to approach life is not to try to control what other people do, nor to think about what they might do in response to what you do.  Rather, the healthiest approach to life, and the only way to be spiritually free in life, is to do whatever you want to do, so long as what you want to do is both legal and ethical.  What other people may or may not do in response to that is their own problem, because, as long as what you are doing is legal, they can't make it your problem.  (Unless you let them, so don't let them.)<br>
</p><p>
Applying that philosophy to this situation, I think you should just shrug, ignore Linus, and wash your hands of the in-tree version of bcachefs.  The in-tree version of the code is what someone else is doing, not what you're doing, so it's not your problem.  DKMS will work fine for you.<br>
</p><p>
Btw, I have a question for you, because I am interested in bcachefs but have not tried it yet.  What I want out of my filesystem is as much speed as possible: because I have backups, I have no concern about data integrity after an unclean shutdown.  I currently use ext4 with no journaling and the following knobs set in sysctl.conf:<br>
</p><p>
vm.dirty_ratio = 90<br>
vm.dirty_background_ratio = 50<br>
vm.dirty_expire_centisecs = 360000<br>
vm.dirty_writeback_centisecs = 60000<br>
</p><p>
I also have libeatmydata.so listed in ld.so.preload to make sure programs can't override my preference for speed by calling fsync().  Given my preferences, would you recommend bcachefs for me over my current ext4-based setup, and, if so, how could I configure bcachefs for my desired speed versus integrity tradeoff?<br>
</p><p>
Thanks for any help.<br>
</p></div>

      
          
        
     </details>
</div><div open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 7:57 UTC (Sat)
                               by <b>paravoid</b> (subscriber, #32869)
                              [<a href="https://lwn.net/Articles/1035799/">Link</a>] (8 responses)
      </p>
      </summary>
      <div>
<p><span>&gt; Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.</span></p><p>
Debian was not even close to the topic at hand, and yet you felt the need to bring it up, just to attack someone, and with information that is misrepresenting the truth. This is something you've done before, and you were very recently called out in lkml for it. Stop.<br>
</p><p>
To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. It was ultimately removed from unstable because noone was able to get through. Source: I am one of them.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 11:44 UTC (Sat)
                               by <b>koverstreet</b> (<b>✭ supporter ✭</b>, #4296)
                              [<a href="https://lwn.net/Articles/1035801/">Link</a>] (3 responses)
      </p>
      </summary>
      <div><p>
The specific, technical issue was the package maintainer switching out the Rust dependencies for the packaged versions from Debian. I explained that this was a bad idea at the outset, because it invalidated all the testing we do, and the Debian package wasn't replicating that testing; it was also wholly unnecessary because Rust dependencies are statically linked.</p><p>
He did so anyways, and then swapped out bindgen for an old version that was explicitly unsupported according to the Cargo.toml, which broke the build, and he sat on it and Debian users stopped getting updates (I didn't even see a report until months later).<br>
</p><p>
This resulted in users being unable to access their filesystems.<br>
</p><p>
There was briefly a buggy version of bcachefs-tools that couldn't pass mount options correctly; users in every other distro got a fix quickly, but Debian users did not - and we found out about this when a lot of users weren't able to mount in degraded mode after having a drive die.<br>
</p><p>
What you're doing is conflating technical criticism with personal, and then using that as an excuse to ramp up the drama. Technical criticism, including pointing out failures of processes, has to be ok for engineering to function, otherwise we don't learn from our mistakes. That can make for a harsh learning environment, but when you're shipping critical system components that have to work, that's what you signed up for; we have responsibilities.<br>
</p><p>
The person in question was warned explicitly that what he was doing was a bad idea; he could have at any point said "this is too complicated an issue for me to handle; I'll let someone else take this one" (and there are mechanisms in Debian process for obtaining exceptions to process rules that could have avoided this, by simply skipping the Rust dependency unbundling with a clear explanation of why); he ignored advice and plowed ahead, and a lot of people were affected by those actions.<br>
</p><p>
When we work on this kind of code, we have to be responsible for the work we do, including our mistakes.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 14:25 UTC (Sat)
                               by <b>ma4ris8</b> (subscriber, #170509)
                              [<a href="https://lwn.net/Articles/1035804/">Link</a>] (2 responses)
      </p>
      </summary>
      <div><p>
My goal is to show the power of listening to the other. I'll try to listen first, and then answer.<br>
I hope that you get my point of listening well in order to carefully heal the relationships.</p><p>
Listen part: I'm trying to repeat roughly the same as you wrote above, to show that I listened you:<br>
</p><p>
First you state that maintainer switched Rust dependencies for the packaged versions from Debian.<br>
You explained that it was a bad idea, for multiple reasons: statically linked dependencies, and<br>
invalidating all your active testing.<br>
</p><p>
He changed Rust dependencies anyways, and then swapped out bindgen into older version,<br>
which broke the build for Debian, and file system users stopped getting updates.<br>
</p><p>
Important end question: Did I repeat (re-phrase in text) precisely what you wrote?<br>
</p><p>
Answer part:<br>
You wrote many items into one message. I answered only for the first one,<br>
to keep the answer small enough. Some progress, but further messages<br>
could increase coverage.<br>
</p><p>
For me it sounds like there were some mistakes done by both you and others.<br>
The unfortunate end result was, that Debian users had problems with the bug.<br>
I didn't get from your message, the outcome of the relationships between persons:<br>
whether personal relationships were worsened, stayed the same, or healed in the<br>
end (each relation individually).<br>
</p><p>
How to communicate (listen) effectively, to heal relationships?<br>
</p><p>
This way of listening is mentioned in<br>
<a href="https://www.verywellmind.com/what-is-active-listening-3024343">https://www.verywellmind.com/what-is-active-listening-302...</a><br>
"Paraphrasing and reflecting back what has been said"<br>
( Those who know psychology, know these things ).<br>
</p><p>
What I showed, is one way to restore human relationships, with Linus and others:<br>
You could try to restore relationships with just listening others. Choose carefully<br>
messaging cases, in which you think that you won't cause much backslash,<br>
but you could have progress with healing the relationship by listening to the other.<br>
</p><p>
If you get a backslash, you was just given an opportunity to listen the complaint.<br>
Repeat in nearly the same words the whole complaint, <br>
so that the other one feels of being heard fully.<br>
Try to at least have progress, thus please listen carefully the mentioned<br>
complaint by repeating it. You can have pauses, like answering another day, to reduce the burden.<br>
Please don't open up any new problems. If you do (I do mistakes sometimes),<br>
and get a backslash as a heated answer, please listen and repeat it carefully,<br>
to reduce the impact.<br>
</p><p>
By doing this just very slightly to not burden others,<br>
you could both improve your communication skills,<br>
and perhaps others could learn from it too,<br>
and perhaps then relations with other stakeholders, like maintainers,<br>
and Linus, could be restored into a level that you can co-operate efficiently together again.<br>
</p><p>
I've seen that sometimes this listening technique helps on-line, in addition of meeting face to face.<br>
I'm trying to improve my communicating skills in the contexts of<br>
change management for "OWASP top 10", and AI adoption.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 18:21 UTC (Sat)
                               by <b>koverstreet</b> (<b>✭ supporter ✭</b>, #4296)
                              [<a href="https://lwn.net/Articles/1035822/">Link</a>] (1 responses)
      </p>
      </summary>
      <div>
<p><span>&gt;  For me it sounds like there were some mistakes done by both you and others.</span></p><p>
Why are you trying to bothsides this?<br>
</p><p>
You seem to have the facts straight, but I'm not at all clear on what you think I did wrong.<br>
</p><p>
All this was explained clearly, calmly and patiently to the Debian package maintainer when he started; he decided to do it his way, and when the breakage became apparent I asked if he was going to fix it and he just said "nope, too complicated" and walked off. So I got stuck with warning bcachefs users away from Debian, and he wrote a screed of a blog post about how impossible I am to work with.<br>
</p><p>
Sorry, but from where I sit that just looks crazy.<br>
</p><p>
I'm all about focusing on the human aspect, sitting down with people and having open and honest conversations. I do that regularly, and believe me I and others have tried ratcheting down the tensions, bringing the focus back to the technical and looking for ways to make this easier and take things in little steps.<br>
</p><p>
The whole rest of the 6.16 merge cycle after the journal_rewind fiasco was just that, from myself and others; we've tried to bridge the gap, bring the focus back to the technical, look for ways to make things work - it doesn't seem to be getting us anywhere.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 21:00 UTC (Sat)
                               by <b>josh</b> (subscriber, #17465)
                              [<a href="https://lwn.net/Articles/1035834/">Link</a>] 
      </p>
      </summary>
      <div>
<p><span>&gt; You seem to have the facts straight, but I'm not at all clear on what you think I did wrong.</span></p><p>
You don't demonstrate any degree of understanding of why requirements other than your own matter. You talk about what the Debian maintainer did, and how you told them not to. You don't talk about why those requirements exist and what you did to help them meet those requirements. You act like the story begins and ends with "I told them no and they didn't obey".<br>
</p><p>
This is on par with what happens with the Linux kernel. You don't demonstrate and communicate that you understand requirements other than your own and place weight on them. You just act like they're obstacles to getting *your* requirements met, and try to work around them.<br>
</p></div>

      
          
        
     </details>
</details>
</details>
</details>
<a name="CommAnchor1035803"></a>
    <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 12:24 UTC (Sat)
                               by <b>muase</b> (subscriber, #178466)
                              [<a href="https://lwn.net/Articles/1035803/">Link</a>] (3 responses)
      </p>
      </summary>
      <div><p>
Hm, for me it simply reads like OP is mentioning his frustration about release cycles, and is citing a pretty legitimate example: The fact that developers are overwhelmed with obsolete bug reports, because LTS distros are months or even years behind, is not something new and a real problem for some projects.</p><p>
I know it's not the distros' fault; it's simply how LTS has to work in practice – however I can understand the frustration that arises if there seems to be an opportunity to finally update a package(set)... and then that opportunity is missed, and now the dev knows that they have to endure those obsolete bug reports for another n-year release cycle. It definitely didn't read as "just to attack someone".<br>
</p><p>
<span>&gt; To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another.</span><br>
</p><p>
Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case.  If the communication is public, do you have a link or something? :)<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 18:27 UTC (Sat)
                               by <b>paravoid</b> (subscriber, #32869)
                              [<a href="https://lwn.net/Articles/1035821/">Link</a>] (2 responses)
      </p>
      </summary>
      <div>
<p><span>&gt; Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case. If the communication is public, do you have a link or something? :)</span></p><p>
Kent in <a href="https://lore.kernel.org/linux-bcachefs/wona7sjqodu7jgchtx2lpwqlmsrtbkdhk2gv6df2qdnx3vt7tc@twfs326tgt4r/">https://lore.kernel.org/linux-bcachefs/wona7sjqodu7jgchtx...</a> called part of a maintainer's job as "bullshit, make-work job", told Debian to "develop a better and more practical minded attitude" and to "stop wasting my time with this stupid bullshit and I can get back to real work". The issues we had spent a lot of our volunteer time to fix were very real issues, many of them upstream, and one in the Rust ecosystem. At the time this was sent, all issues were fixed, or were on the way to be fixed, and a recent bcachefs-tools package with all of the appropriate dependencies was a few weeks away from getting to Debian testing.<br>
</p><p>
bcachefs-tools was orphaned by its maintainer a few weeks later; myself and another contributor (the two of us had done all recent advancements), stopped investing our time as well. The package has remained orphaned since, for about a year. Anyone can pick it up, but noone has, and that's not because of technical difficulties (as far as packages go, it's pretty trivial).<br>
</p><p>
As an aside, the very existence of this thread was as a "PSA" to his users to avoid Debian and Fedora, telling them that "you'll want to be on a more modern distro". *Two weeks later*, he responded in <a href="https://lore.kernel.org/lkml/nxyp62x2ruommzyebdwincu26kmi7opqq53hbdv53hgqa7zsvp@dcveluxhuxsd/">https://lore.kernel.org/lkml/nxyp62x2ruommzyebdwincu26kmi...</a> to Linus that he expects the "major distros" to pick up bcachefs soon. Whether he was dishonest or just naive, I'll leave that to your judgement.<br>
</p><p>
The above was just a small sample. There were literally dozens of responses of this style at time, random offensive comments etc., across multiple mediums (mailing lists, IRC, Reddit, etc.). I am not keeping a file though, as I don't feel the need to convince anyone with hard evidence. You don't know me, and I understand that my opinion may not be of much value to you. I hope, though, that you and others may see this as one tiny part of a broader pattern of countless long-time contributors across multiple projects expressing that they have been alienated and driven away by Kent's conduct and sense of entitlement, and that they have good reasons for it.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 19:20 UTC (Sat)
                               by <b>koverstreet</b> (<b>✭ supporter ✭</b>, #4296)
                              [<a href="https://lwn.net/Articles/1035824/">Link</a>] (1 responses)
      </p>
      </summary>
      <div><p>
So yes, I could have been more diplomatic in my response.</p><p>
But please do try to put yourself in my shoes; that was after getting a bunch of bug reports from Debian users, and there had been a _lot_ of fail at that point in how the Debian packaging was handled.<br>
</p><p>
I do have to reiterate: the unbundling of Rust dependencies should not have happened for bcachefs-tools, there was no technical reason for that, all my explanations were met with "but that's our policy", and no amount of reasoning was getting anywhere; and the Debian packager breaking the build and sitting on it just should not have happened.<br>
</p><p>
I do sincerely hope you can analyze how things went from the other end and ask yourself what could have been done better to avoid this, because from my end, this was an intensely frustrating issue, and it wasn't being taken seriously and it had very real effects.<br>
</p><p>
Before you start focusing on language and diplomacy, you really need ask yourself if the technical decisionmaking leading up to that point was sound. When we get breakage as bad as what happened with the Debian package, you can expect the kind of frustration I was voicing there, and "bullshit, make-work projects" still seems to accurately describe what Debain's been doing with Rust dependency unbundling.<br>
</p><p>
When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities.<br>
</p></div>

      
          
        
     <details open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 21:17 UTC (Sat)
                               by <b>josh</b> (subscriber, #17465)
                              [<a href="https://lwn.net/Articles/1035836/">Link</a>] 
      </p>
      </summary>
      <div>
<p><span>&gt; Before you start focusing on language and diplomacy, you really need ask yourself</span></p><p>
No, you really don't. There is no universe in which the things you said produced useful outcomes. The fact that they resulted in someone deciding they no longer wish to work with you or put work into being the downstream maintainer of your software is an *unsurprising outcome*.<br>
</p><p>
<span>&gt; When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities.</span><br>
</p><p>
You also cannot completely neglect language and diplomacy and understanding other people's requirements, either, as you absolutely did in the messages being quoted here.<br>
</p><p>
Your words will produce responses and actions from others. No amount of wishing things were different will enable you to say things that will predictably produce undesired actions and then have a leg to stand on when being annoyed that those predictable responses and actions happen.<br>
</p><p>
Your words are a lever to be used, just like your code. Write the words that produce the results you want, and if you want to be happier, learn to not resent that as a means of effecting change.<br>
</p><p>
To be clear: the words have to actually match the actions. You can't *just* say the right words but then have them mismatch your actions; down that path you'd find people whose words and truth lack even a passing familiarity. But it's important to, for instance, give people confidence that you care about the requirements they deal with, in some fashion *other* than "what windmill can I burn down so that you don't have to meet those requirements anymore and can do what I want instead".<br>
</p></div>

      
          
        
     </details>
</details>
</details>
</details>
</div><div open="">
      <summary><h3>A few suggestions (which you don’t have to follow)</h3>
      <p> Posted Aug 30, 2025 17:12 UTC (Sat)
                               by <b>DemiMarie</b> (subscriber, #164188)
                              [<a href="https://lwn.net/Articles/1035815/">Link</a>] 
      </p>
      </summary>
      <div><p>
My recommendation is to have an out-of-tree repository that has the latest changes and to send them to Linus at a pace Linus is okay with.  Users who need the very latest code can use the driver from your tree via DKMS or similar.  I also recommend having a way to implement new features that are not on hot paths (such as recovery) in userspace.  Finally, a FUSE version would be a good idea and (as you mentioned earlier IIRC) make kernel changes that are not corruption or crash fixes less urgent, because users could recover in userspace and even access their data (albeit more slowly) while that is happening.</p><p>
For anything that has to happen before the filesystem can be accessed at all, it might make sense to have an option for the userspace mount helper to do the work.  In this case, the userspace helper has far fewer disadvantages I know of.<br>
</p><p>
My dream would be for bcachefs to have SQLite’s level of testing and input validation, or (even better) formal verification.   Either would massively reduce the rate of bugs making it into a release, but neither is reasonable to ask for outside of a suitably-priced commercial engagement.<br>
</p></div>

      
          
        
     </div><div open="">
      <summary><h3>So what exactly *is* in the cards, then?</h3>
      <p> Posted Aug 30, 2025 18:53 UTC (Sat)
                               by <b>ATLief</b> (subscriber, #166135)
                              [<a href="https://lwn.net/Articles/1035823/">Link</a>] 
      </p>
      </summary>
      <div>
<p><span>&gt; I'd be ok with it if it was another long standing member of the kernel community with experience working with Linus, but for some strange reason no one I've talked to wants to take that on.</span></p><p>
That’s entirely expected given the circumstances; if longstanding Linux developers collectively don’t want to work with you, then they wouldn’t want to individually work with you either.<br>
</p></div>

      
          
        
     </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cognitive Load is what matters (674 pts)]]></title>
            <link>https://github.com/zakirullin/cognitive-load</link>
            <guid>45074248</guid>
            <pubDate>Sat, 30 Aug 2025 12:58:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/zakirullin/cognitive-load">https://github.com/zakirullin/cognitive-load</a>, See on <a href="https://news.ycombinator.com/item?id=45074248">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Cognitive Load is what matters</h2><a id="user-content-cognitive-load-is-what-matters" aria-label="Permalink: Cognitive Load is what matters" href="#cognitive-load-is-what-matters"></a></p>
<p dir="auto"><a href="https://minds.md/zakirullin/cognitive" rel="nofollow">Readable version</a> | <a href="https://github.com/zakirullin/cognitive-load/blob/main/README.zh-cn.md">Chinese translation</a> | <a href="https://github.com/zakirullin/cognitive-load/blob/main/README.ko.md">Korean translation</a> | <a href="https://github.com/zakirullin/cognitive-load/blob/main/README.tr.md">Turkish translation</a></p>
<p dir="auto"><em>It is a living document, last update: <strong>August 2025</strong>. Your contributions are welcome!</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.</p>
<p dir="auto">Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high <em>cognitive load</em>. It's not some fancy abstract concept, but rather <strong>a fundamental human constraint</strong>. It's not imagined, it's there and we can feel it.</p>
<p dir="auto">Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cognitive load</h2><a id="user-content-cognitive-load" aria-label="Permalink: Cognitive load" href="#cognitive-load"></a></p>
<blockquote>
<p dir="auto">Cognitive load is how much a developer needs to think in order to complete a task.</p>
</blockquote>
<p dir="auto">When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly <a href="https://github.com/zakirullin/cognitive-load/issues/16" data-hovercard-type="issue" data-hovercard-url="/zakirullin/cognitive-load/issues/16/hovercard">four such chunks</a> in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.</p>
<p dir="auto"><em>Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, <strong>the author had created a high cognitive load for us.</strong></em></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/cognitiveloadv6.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/cognitiveloadv6.png" alt="Cognitive load" width="750"></a>
</p>
<p dir="auto">We should reduce the cognitive load in our projects as much as possible.</p>
<details>
  <summary><b>Cognitive load and interruptions</b></summary>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/interruption.jpeg"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/interruption.jpeg"></a><br>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Types of cognitive load</h2><a id="user-content-types-of-cognitive-load" aria-label="Permalink: Types of cognitive load" href="#types-of-cognitive-load"></a></p>
<p dir="auto"><strong>Intrinsic</strong> - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.</p>
<p dir="auto"><strong>Extraneous</strong> - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/smartauthorv14thanksmari.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/smartauthorv14thanksmari.png" alt="Intrinsic vs Extraneous" width="600"></a>
</p>
<p dir="auto">Let's jump straight to the concrete practical examples of extraneous cognitive load.</p>
<hr>
<p dir="auto">We will refer to the level cognitive load as follows:<br>
<code>🧠</code>: fresh working memory, zero cognitive load<br>
<code>🧠++</code>: two facts in our working memory, cognitive load increased<br>
<code>🤯</code>: cognitive overload, more than 4 facts</p>
<blockquote>
<p dir="auto">Our brain is much more complex and unexplored, but we can go with this simplistic model.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Complex conditionals</h2><a id="user-content-complex-conditionals" aria-label="Permalink: Complex conditionals" href="#complex-conditionals"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="if val > someConstant // 🧠+
    &amp;&amp; (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    &amp;&amp; (condition4 &amp;&amp; !condition5) { // 🤯, we are messed up by this point
    ...
}"><pre><span>if</span> <span>val</span> <span>&gt;</span> <span>someConstant</span> <span>// 🧠+</span>
    <span>&amp;&amp;</span> (<span>condition2</span> <span>||</span> <span>condition3</span>) <span>// 🧠+++, prev cond should be true, one of c2 or c3 has be true</span>
    <span>&amp;&amp;</span> (<span>condition4</span> <span>&amp;&amp;</span> <span>!</span><span>condition5</span>) { <span>// 🤯, we are messed up by this point</span>
    <span>...</span>
}</pre></div>
<p dir="auto">Introduce intermediate variables with meaningful names:</p>
<div dir="auto" data-snippet-clipboard-copy-content="isValid = val > someConstant
isAllowed = condition2 || condition3
isSecure = condition4 &amp;&amp; !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid &amp;&amp; isAllowed &amp;&amp; isSecure {
    ...
}"><pre><span>isValid</span> <span>=</span> <span>val</span> <span>&gt;</span> <span>someConstant</span>
<span>isAllowed</span> <span>=</span> <span>condition2</span> <span>||</span> <span>condition3</span>
<span>isSecure</span> <span>=</span> <span>condition4</span> <span>&amp;&amp;</span> <span>!</span><span>condition5</span> 
<span>// 🧠, we don't need to remember the conditions, there are descriptive variables</span>
<span>if</span> <span>isValid</span> <span>&amp;&amp;</span> <span>isAllowed</span> <span>&amp;&amp;</span> <span>isSecure</span> {
    <span>...</span>
}</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Nested ifs</h2><a id="user-content-nested-ifs" aria-label="Permalink: Nested ifs" href="#nested-ifs"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} "><pre><span>if</span> <span>isValid</span> { <span>// 🧠+, okay nested code applies to valid input only</span>
    <span>if</span> <span>isSecure</span> { <span>// 🧠++, we do stuff for valid and secure input only</span>
        <span>stuff</span> <span>// 🧠+++</span>
    }
} </pre></div>
<p dir="auto">Compare it with the early returns:</p>
<div dir="auto" data-snippet-clipboard-copy-content="if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+"><pre><span>if</span> <span>!</span><span>isValid</span>
    <span>return</span>
 
<span>if</span> <span>!</span><span>isSecure</span>
    <span>return</span>

<span>// 🧠, we don't really care about earlier returns, if we are here then all good</span>

<span>stuff</span> <span>// 🧠+</span></pre></div>
<p dir="auto">We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Inheritance nightmare</h2><a id="user-content-inheritance-nightmare" aria-label="Permalink: Inheritance nightmare" href="#inheritance-nightmare"></a></p>
<p dir="auto">We are asked to change a few things for our admin users: <code>🧠</code></p>
<p dir="auto"><code>AdminController extends UserController extends GuestController extends BaseController</code></p>
<p dir="auto">Ohh, part of the functionality is in <code>BaseController</code>, let's have a look: <code>🧠+</code><br>
Basic role mechanics got introduced in <code>GuestController</code>: <code>🧠++</code><br>
Things got partially altered in <code>UserController</code>: <code>🧠+++</code><br>
Finally we are here, <code>AdminController</code>, let's code stuff! <code>🧠++++</code></p>
<p dir="auto">Oh, wait, there's <code>SuperuserController</code> which extends <code>AdminController</code>. By modifying <code>AdminController</code> we can break things in the inherited class, so let's dive in <code>SuperuserController</code> first: <code>🤯</code></p>
<p dir="auto">Prefer composition over inheritance. We won't go into detail - there's <a href="https://www.youtube.com/watch?v=hxGOiiR9ZKg" rel="nofollow">plenty of material</a> out there.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Too many small methods, classes or modules</h2><a id="user-content-too-many-small-methods-classes-or-modules" aria-label="Permalink: Too many small methods, classes or modules" href="#too-many-small-methods-classes-or-modules"></a></p>
<blockquote>
<p dir="auto">Method, class and module are interchangeable in this context</p>
</blockquote>
<p dir="auto">Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.</p>
<p dir="auto"><strong>Deep module</strong> - simple interface, complex functionality<br>
<strong>Shallow module</strong> - interface is relatively complex to the small functionality it provides</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/deepmodulev8.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/deepmodulev8.png" alt="Deep module" width="700"></a>
</p>
<p dir="auto">Having too many shallow modules can make it difficult to understand the project. <strong>Not only do we have to keep in mind each module responsibilities, but also all their interactions</strong>. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, <a href="https://blog.separateconcerns.com/2023-09-11-linear-code.html" rel="nofollow">linear thinking</a> is more natural to us humans.</p>
<blockquote>
<p dir="auto">Information hiding is paramount, and we don't hide as much complexity in shallow modules.</p>
</blockquote>
<p dir="auto">I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.</p>
<p dir="auto">Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.</p>
<blockquote>
<p dir="auto">The best components are those that provide powerful functionality yet have a simple interface.<br>
<strong>John K. Ousterhout</strong></p>
</blockquote>
<p dir="auto">The interface of the UNIX I/O is very simple. It has only five basic calls:</p>
<div dir="auto" data-snippet-clipboard-copy-content="open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)"><pre><span>open</span>(<span>path</span>, <span>flags</span>, <span>permissions</span>)
<span>read</span>(<span>fd</span>, <span>buffer</span>, <span>count</span>)
<span>write</span>(<span>fd</span>, <span>buffer</span>, <span>count</span>)
<span>lseek</span>(<span>fd</span>, <span>offset</span>, <span>referencePosition</span>)
<span>close</span>(<span>fd</span>)</pre></div>
<p dir="auto">A modern implementation of this interface has <strong>hundreds of thousands of lines of code</strong>. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.</p>
<blockquote>
<p dir="auto">This deep module example is taken from the book <a href="https://web.stanford.edu/~ouster/cgi-bin/book.php" rel="nofollow">A Philosophy of Software Design</a> by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper <a href="https://www.win.tue.nl/~wstomv/edu/2ip30/references/criteria_for_modularization.pdf" rel="nofollow">On the Criteria To Be Used in Decomposing Systems into Modules</a>. Both are essential reads. Other related readings: <a href="https://github.com/johnousterhout/aposd-vs-clean-code">A Philosophy of Software Design vs Clean Code</a>, <a href="https://qntm.org/clean" rel="nofollow">It's probably time to stop recommending Clean Code</a>, <a href="https://copyconstruct.medium.com/small-functions-considered-harmful-91035d316c29" rel="nofollow">Small Functions considered Harmful</a>.</p>
</blockquote>
<p dir="auto">P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Responsible for one thing</h2><a id="user-content-responsible-for-one-thing" aria-label="Permalink: Responsible for one thing" href="#responsible-for-one-thing"></a></p>
<p dir="auto">All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So <a href="https://minds.md/benji/frameworks" rel="nofollow">MetricsProviderFactoryFactory</a> seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.</p>
<p dir="auto">We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.</p>
<blockquote>
<p dir="auto">A module should be responsible to one, and only one, user or stakeholder.</p>
</blockquote>
<p dir="auto">This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.</p>
<p dir="auto">But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Too many shallow microservices</h2><a id="user-content-too-many-shallow-microservices" aria-label="Permalink: Too many shallow microservices" href="#too-many-shallow-microservices"></a></p>
<p dir="auto">This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.</p>
<p dir="auto">I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. Diagnostic difficulty in integration space skyrocketed. Both time to market and cognitive load were unacceptably high. <code>🤯</code></p>
<p dir="auto">Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". <em>Hello, you got to stop dreaming big.</em></p>
<p dir="auto">The <a href="https://en.wikipedia.org/wiki/Tanenbaum%E2%80%93Torvalds_debate" rel="nofollow">Tanenbaum-Torvalds debate</a> argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.</p>
<p dir="auto">A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Feature-rich languages</h2><a id="user-content-feature-rich-languages" aria-label="Permalink: Feature-rich languages" href="#feature-rich-languages"></a></p>
<p dir="auto">We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.</p>
<p dir="auto">If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, <strong>when you come back later, you would have to recreate that thought process!</strong></p>
<p dir="auto"><strong>You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available.</strong> <code>🤯</code></p>
<p dir="auto">These statements are made by none other than Rob Pike.</p>
<blockquote>
<p dir="auto">Reduce cognitive load by limiting the number of choices.</p>
</blockquote>
<p dir="auto">Language features are OK, as long as they are orthogonal to each other.</p>
<details>
  <summary><b>Thoughts from an engineer with 20 years of C++ experience ⭐️</b></summary>
  <br>
  I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!<p>
  I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.</p><p>
  Like, can you imagine, the token <code>||</code> has a different meaning in <code>requires ((!P&lt;T&gt; || !Q&lt;T&gt;))</code> and in <code>requires (!(P&lt;T&gt; || Q&lt;T&gt;))</code>. The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.</p><p>
  You can't allocate space for a trivial type and just <code>memcpy</code> a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.</p><p>
  Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you <b>will face</b> that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03. <code>🤯</code></p><p>
  There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, <i>but if</i> the value is known statically, then... <code>🤯</code></p><p>
  <b>This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons</b> (<i>extraneous cognitive load</i>).</p><p>
  I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.</p><p>
  <b>By no means I am trying to blame C++.</b> I love the language. It's just that I am tired now.</p><p dir="auto">Thanks to <a href="https://0xd34df00d.me/" rel="nofollow">0xd34df00d</a> for writing.</p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">Business logic and HTTP status codes</h2><a id="user-content-business-logic-and-http-status-codes" aria-label="Permalink: Business logic and HTTP status codes" href="#business-logic-and-http-status-codes"></a></p>
<p dir="auto">On the backend we return:<br>
<code>401</code> for expired jwt token<br>
<code>403</code> for not enough access<br>
<code>418</code> for banned users</p>
<p dir="auto">The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:<br>
<code>401</code> is for expired jwt token // <code>🧠+</code>, ok just temporary remember it<br>
<code>403</code> is for not enough access // <code>🧠++</code><br>
<code>418</code> is for banned users // <code>🧠+++</code></p>
<p dir="auto">Frontend developers would (hopefully) introduce some kind <code>numeric status -&gt; meaning</code> dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.</p>
<p dir="auto">Then QA engineers come into play:
"Hey, I got <code>403</code> status, is that expired token or not enough access?"
<strong>QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.</strong></p>
<p dir="auto">Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:</p>
<div dir="auto" data-snippet-clipboard-copy-content="{
    &quot;code&quot;: &quot;jwt_has_expired&quot;
}"><pre>{
    <span>"code"</span>: <span><span>"</span>jwt_has_expired<span>"</span></span>
}</pre></div>
<p dir="auto">Cognitive load on the frontend side: <code>🧠</code> (fresh, no facts are held in mind)<br>
Cognitive load on the QA side: <code>🧠</code></p>
<p dir="auto">The same rule applies to all sorts of numeric statuses (in the database or wherever) - <strong>prefer self-describing strings</strong>. We are not in the era of 640K computers to optimise for memory.</p>
<blockquote>
<p dir="auto">People spend time arguing between <code>401</code> and <code>403</code>, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.</p>
</blockquote>
<p dir="auto">P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like <a href="https://ntietz.com/blog/lets-say-instead-of-auth/" rel="nofollow">"login" and "permissions"</a> to reduce the cognitive load.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Abusing DRY principle</h2><a id="user-content-abusing-dry-principle" aria-label="Permalink: Abusing DRY principle" href="#abusing-dry-principle"></a></p>
<p dir="auto">Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.</p>
<p dir="auto">Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. <code>🤯</code></p>
<p dir="auto">In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.</p>
<p dir="auto">Rob Pike once said:</p>
<blockquote>
<p dir="auto">A little copying is better than a little dependency.</p>
</blockquote>
<p dir="auto">We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.</p>
<p dir="auto"><strong>All your dependencies are your code.</strong> Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (<em>because things go wrong</em>) is painful.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tight coupling with a framework</h2><a id="user-content-tight-coupling-with-a-framework" aria-label="Permalink: Tight coupling with a framework" href="#tight-coupling-with-a-framework"></a></p>
<p dir="auto">There's a lot of "magic" in frameworks. By relying too heavily on a framework, <strong>we force all upcoming developers to learn that "magic" first</strong>. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.</p>
<p dir="auto">Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. <code>🤯</code></p>
<p dir="auto"><strong>By no means do we advocate to invent everything from scratch!</strong></p>
<p dir="auto">We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.</p>
<blockquote>
<p dir="auto"><a href="https://minds.md/benji/frameworks" rel="nofollow">Why I Hate Frameworks</a></p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Layered architecture</h2><a id="user-content-layered-architecture" aria-label="Permalink: Layered architecture" href="#layered-architecture"></a></p>
<p dir="auto">There is a certain engineering excitement about all this stuff.</p>
<p dir="auto">I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. <code>🤯</code></p>
<p dir="auto">Abstraction is supposed to hide complexity, here it just adds <a href="https://fhur.me/posts/2024/thats-not-an-abstraction" rel="nofollow">indirection</a>. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. <code>🤯</code></p>
<p dir="auto">This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. <strong>No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.</strong></p>
<details>
  <summary><b>Coding principles and experience</b></summary>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/complexity.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/complexity.png"></a><br>
  <a href="https://twitter.com/flaviocopes" rel="nofollow">@flaviocopes</a>
</details>
<p dir="auto">If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and <a href="https://www.hyrumslaw.com/" rel="nofollow">implicit interfaces</a>.</p>
<blockquote>
<p dir="auto">With a sufficient number of users of an API,<br>
it does not matter what you promise in the contract:<br>
all observable behaviors of your system<br>
will be depended on by somebody.</p>
</blockquote>
<p dir="auto">We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. <strong>We spent the next 10 months on dealing with out-of-order events and other challenges.</strong> It's now funny to say that abstractions helps us replace components quickly.</p>
<p dir="auto"><strong>So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future?</strong> Plus, in most cases, that future of replacing some core component never happens.</p>
<p dir="auto">These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. <a href="https://github.com/zakirullin/cognitive-load/discussions/24" data-hovercard-type="discussion" data-hovercard-url="/zakirullin/cognitive-load/discussions/24/hovercard">Discuss</a>.</p>
<p dir="auto">Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.</p>
<p dir="auto"><strong><a href="https://blog.jooq.org/why-you-should-not-implement-layered-architecture" rel="nofollow">Layers of abstraction aren't free of charge</a>, they are to be held in our limited working memory</strong>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/layers.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/layers.png" alt="Layers" width="400"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Domain-driven design</h2><a id="user-content-domain-driven-design" aria-label="Permalink: Domain-driven design" href="#domain-driven-design"></a></p>
<p dir="auto">Domain-driven design has some great points, although it is often misinterpreted. People say "We write code in DDD", which is a bit strange, because DDD is about problem space, not about solution space.</p>
<p dir="auto">Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.</p>
<p dir="auto">Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. <code>🤯</code></p>
<p dir="auto">Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cognitive load in familiar projects</h2><a id="user-content-cognitive-load-in-familiar-projects" aria-label="Permalink: Cognitive load in familiar projects" href="#cognitive-load-in-familiar-projects"></a></p>
<blockquote>
<p dir="auto">The problem is that <strong>familiarity is not the same as simplicity</strong>. They <em>feel</em> the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!</p>
<p dir="auto">It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.</p>
<p dir="auto">In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”</p>
<p dir="auto">There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.</p>
<p dir="auto"><em>Thanks to <a href="https://dannorth.net/" rel="nofollow">Dan North</a> for his comment</em>.</p>
</blockquote>
<p dir="auto">If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/mentalmodelsv15.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/mentalmodelsv15.png" alt="Mental models" width="700"></a>
</p>
<p dir="auto">The more mental models there are to learn, the longer it takes for a new developer to deliver value.</p>
<p dir="auto">Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.</p>
<p dir="auto">If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<ul dir="auto">
<li>Our architecture is a standard CRUD app architecture, <a href="https://danluu.com/simple-architectures/" rel="nofollow">a Python monolith on top of Postgres</a></li>
<li>How Instagram scaled to 14 million users with <a href="https://read.engineerscodex.com/p/how-instagram-scaled-to-14-million" rel="nofollow">only 3 engineers</a></li>
<li>The companies where we were like ”woah, these folks are <a href="https://kenkantzer.com/learnings-from-5-years-of-tech-startup-code-audits/" rel="nofollow">smart as hell</a>” for the most part failed</li>
<li>One function that wires up the entire system. If you want to know how the system works - <a href="https://www.infoq.com/presentations/8-lines-code-refactoring" rel="nofollow">go read it</a></li>
</ul>
<p dir="auto">These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.</p>
<p dir="auto">Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Conclusion</h2><a id="user-content-conclusion" aria-label="Permalink: Conclusion" href="#conclusion"></a></p>
<p dir="auto">Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. <code>🤯</code></p>
<p dir="auto">Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. <strong>Do not do this to your colleagues.</strong></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/zakirullin/cognitive-load/blob/main/img/smartauthorv14thanksmari.png"><img src="https://github.com/zakirullin/cognitive-load/raw/main/img/smartauthorv14thanksmari.png" alt="Smart author" width="600"></a>
</p>
<p dir="auto">We should reduce any cognitive load above and beyond what is intrinsic to the work we do.</p>
<hr>
<p dir="auto"><a href="https://www.linkedin.com/in/zakirullin/" rel="nofollow">LinkedIn</a>, <a href="https://twitter.com/zakirullin" rel="nofollow">X</a>, <a href="https://github.com/zakirullin">GitHub</a></p>
<p dir="auto"><a href="https://minds.md/zakirullin/cognitive" rel="nofollow">Readable version</a></p>
<details>
    <summary><b>Comments</b></summary>
    
    <p dir="auto"><strong>Rob Pike</strong><br>Nice article.</p>
    <p dir="auto"><strong><a href="https://x.com/karpathy/status/1872038630405054853" rel="nofollow">Andrej Karpathy</a></strong> <i>(ChatGPT, Tesla)</i><br>Nice post on software engineering. Probably the most true, least practiced viewpoint.</p>
    <p dir="auto"><strong><a href="https://x.com/elonmusk/status/1872346903792566655" rel="nofollow">Elon Musk</a></strong><br>True.</p>
    <p dir="auto"><strong><a href="https://www.linkedin.com/feed/update/urn:li:activity:7277757844970520576/" rel="nofollow">Addy Osmani</a></strong> <i>(Chrome, the most complex software system in the world)</i><br>I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.</p>
    <p dir="auto">The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."</p>
    <p dir="auto">What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.</p>
    <p dir="auto">And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.</p>
    <p dir="auto">Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.</p>
    <p dir="auto">One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.</p>
    <p dir="auto">Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility. </p>
    <p dir="auto">Sometimes the simplest solution is the best one, even in a complex system.</p>
    <p dir="auto"><strong><a href="https://x.com/antirez" rel="nofollow">antirez</a></strong> <i>(Redis)</i><br>Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.</p>
    <p dir="auto">A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.</p>
    <p dir="auto">Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).</p>
    <p dir="auto">These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D</p>
    <p dir="auto"><strong><a href="https://working-for-the-future.medium.com/about" rel="nofollow">A developer from the internet</a></strong><br>You would not hire me... I sell myself on my track record of released enterprise projects.</p>
    <p dir="auto">I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.</p>
    <p dir="auto">I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.</p>
    <p dir="auto">I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.</p>
    <p dir="auto">Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.</p>
    <p dir="auto"><a href="https://news.ycombinator.com/item?id=42489645" rel="nofollow">Comments on Hacker News</a></p>
</details>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FBI cyber cop: Salt Typhoon pwned 'nearly every American' (233 pts)]]></title>
            <link>https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/</link>
            <guid>45074157</guid>
            <pubDate>Sat, 30 Aug 2025 12:43:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/">https://www.theregister.com/2025/08/28/fbi_cyber_cop_salt_typhoon/</a>, See on <a href="https://news.ycombinator.com/item?id=45074157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>China's Salt Typhoon cyberspies hoovered up information belonging to millions of people in the United States over the course of the years-long intrusion into telecommunications networks, according to a top FBI cyber official.</p>
<p>"There's a good chance this espionage campaign has stolen information from nearly every American," Michael Machtinger, deputy assistant director for the FBI's cyber division, told <em>The Register</em>.</p>
<p>"There's a thought among the public that if you don't work in a sensitive area that the PRC might be interested in for its traditional espionage activities, then you are safe, they will not target you," he said, during a Thursday interview with <i>The Register</i>. "As we have seen from Salt Typhoon, this is no longer an assumption that anyone can afford to make."</p>

    

<p>The Beijing-backed spying campaign began at least in 2019 but wasn't <a target="_blank" href="https://www.theregister.com/2024/11/14/salt_typhoon_hacked_multiple_telecom/">uncovered by US authorities</a> until last fall. On Wednesday, US law enforcement and intelligence agencies along with those from 12 other countries warned the <a target="_blank" href="https://www.theregister.com/2025/08/28/china_salt_typhoon_alert/">ongoing espionage activity</a> expanded far beyond <a target="_blank" href="https://www.theregister.com/2025/01/06/charter_consolidated_windstream_salt_typhoon/">nine American telcos</a> and <a target="_blank" href="https://www.theregister.com/2025/01/15/salt_typhoon_us_govt_networks/">government networks</a>. According to Machtinger, at least 80 countries were hit by the digital intrusions.</p>

        


        

<p>Around 200 American organizations were compromised by the espionage activity, Machtinger said, including the previously disclosed telecommunications firms such as <a target="_blank" href="https://www.theregister.com/2024/12/30/att_verizon_confirm_salt_typhoon_breach/">Verizon and AT&amp;T</a>.</p>
<p>Yesterday's joint security alert also pointed the allies' collective finger at three China-based entities affiliated with Salt Typhoon: <a target="_blank" href="https://www.theregister.com/2025/01/17/fcc_telcos_calea/">Sichuan Juxinhe Network Technology</a>, Beijing Huanyu Tianqiong Information Technology, and Sichuan Zhixin Ruijie Network Technology. These companies, and likely others, provide cyber products and services to China's Ministry of State Security and People's Liberation Army, the governments said.</p>
<blockquote>

<p>What the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space</p>
</blockquote>
<p>"This is one of the most consequential cyber espionage breaches that we've ever seen in the United States," Machtinger said.</p>
<p>"What this really underscores is that what the PRC is doing through these proxy actors is really reckless and unbounded, in a way that is significantly outside of the norms of what we see in the espionage space," he added. "And that should really set off alarm bells for us — not only in the United States. The scale of indiscriminate targeting is unlike what we've seen in the past."</p>

        

<p>This indiscriminate targeting, as the FBI and White House security officials have previously noted, allowed Beijing’s snoops to geo-locate millions of mobile phone users, monitor their internet traffic, and, in some cases, <a target="_blank" href="https://www.theregister.com/2024/12/09/white_house_salt_typhoon/">record their phone calls</a>. Victims <a target="_blank" href="https://www.theregister.com/2025/02/25/china_hacked_gop_emails/">reportedly</a> included President Donald Trump and Vice President JD Vance.</p>
<p>Machtinger declined to confirm whether Trump and Vance were among those surveilled, but did say that victims included more than 100 current and former presidential administration officials.</p>
<p>"As we look at the impact on the different sets of victims," he said, Salt Typhoon collected "bulk information from millions of Americans."</p>

        

<p>For the more targeted group of individuals, "most of whom are very high-profile, current and former presidential administration officials, and campaign appointees from both major political parties," the data collection went much deeper, Machtinger added. "Down to intercepting actual content."</p>
<ul>

<li><a href="https://www.theregister.com/2025/08/28/china_salt_typhoon_alert/">If you thought China's Salt Typhoon was booted off critical networks, think again</a></li>

<li><a href="https://www.theregister.com/2025/01/15/salt_typhoon_us_govt_networks/">China's Salt Typhoon spies spotted on US govt networks before telcos, CISA boss says</a></li>

<li><a href="https://www.theregister.com/2025/03/12/volt_tyhoon_experience_interview_with_gm/">This is the FBI, open up. China's Volt Typhoon is on your network</a></li>

<li><a href="https://www.theregister.com/2025/08/28/how_does_china_keep_stealing/">How does China keep stealing our stuff, wonders DoD group responsible for keeping foreign agents out</a></li>
</ul>
<p>In addition to Salt Typhoon, the feds over the past year have issued warnings about other Chinese cyber operations. These include <a target="_blank" href="https://www.theregister.com/2024/01/30/fbi_china_volt/">Volt Typhoon intruders</a>, who infected hundreds of outdated routers to <a target="_blank" href="https://www.theregister.com/2024/01/31/volt_typhoon_botnet/">build a botnet</a> and break into US critical infrastructure facilities. The Beijing-backed crew, we would later learn, was prepositioning itself and <a target="_blank" href="https://www.theregister.com/2024/02/07/us_chinas_volt_typhoon_attacks/">readying destructive cyberattacks</a>.</p>
<p>Another China-linked crew, <a target="_blank" href="https://www.theregister.com/2025/03/06/fbi_china_pays_75k_per/">Silk Typhoon</a> has spent more than a decade compromising IT and cloud providers to steal sensitive data from their <a target="_blank" href="https://www.theregister.com/2025/03/05/china_silk_typhoon_update/">government, technology</a>, education, and legal and professional services customers.</p>
<p>China is not the only source of threats, Machtinger noted. Russia, Iran, North Korea, plus along with home-grown and international cybercriminals and ransomware crooks, assault computers and networks of both individuals and organizations, every day.</p>
<p>"These actors are going to continue their efforts, and they're going to get more sophisticated," Machtinger said. "We need to make sure that we, a nation, are taking cybersecurity seriously, updating systems, removing end-of-life devices, and making it as hard and costly as possible for the myriad of actors that are out there to successfully compromise." ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Agent Client Protocol (193 pts)]]></title>
            <link>https://agentclientprotocol.com/overview/introduction</link>
            <guid>45074147</guid>
            <pubDate>Sat, 30 Aug 2025 12:42:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://agentclientprotocol.com/overview/introduction">https://agentclientprotocol.com/overview/introduction</a>, See on <a href="https://news.ycombinator.com/item?id=45074147">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-page-title="Introduction" data-page-href="/overview/introduction"><p><span data-as="p">The Agent Client Protocol standardizes communication between code editors (IDEs, text-editors, etc.) and coding agents (programs that use generative AI to autonomously modify code).</span>
<span data-as="p">The protocol is still under development, but it should be complete enough to build interesting user experiences using it.</span></p><h2 id="why-acp%3F"><span>Why ACP?</span></h2>
<p><span data-as="p">AI coding agents and editors are tightly coupled but interoperability isn’t the default. Each editor must build custom integrations for every agent they want to support, and agents must implement editor-specific APIs to reach users.
This creates several problems:</span></p><ul>
<li>Integration overhead: Every new agent-editor combination requires custom work</li>
<li>Limited compatibility: Agents work with only a subset of available editors</li>
<li>Developer lock-in: Choosing an agent often means accepting their available interfaces</li>
</ul>
<p><span data-as="p">ACP solves this by providing a standardized protocol for agent-editor communication, similar to how the <a href="https://microsoft.github.io/language-server-protocol/" target="_blank" rel="noreferrer">Language Server Protocol (LSP)</a> standardized language server integration.</span>
<span data-as="p">Agents that implement ACP work with any compatible editor. Editors that support ACP gain access to the entire ecosystem of ACP-compatible agents.
This decoupling allows both sides to innovate independently while giving developers the freedom to choose the best tools for their workflow.</span></p><h2 id="overview"><span>Overview</span></h2>
<p><span data-as="p">ACP assumes that the user is primarily in their editor, and wants to reach out and use agents to assist them with specific tasks.</span>
<span data-as="p">Agents run as sub-processes of the code editor, and communicate using JSON-RPC over stdio. The protocol re-uses the JSON representations used in MCP where possible, but includes custom types for useful agentic coding UX elements, like displaying diffs.</span>
<span data-as="p">The default format for user-readable text is Markdown, which allows enough flexibility to represent rich formatting without requiring that the code editor is capable of rendering HTML.</span></p><h2 id="supported-editors"><span>Supported Editors</span></h2>
<ul>
<li><a href="https://zed.dev/docs/ai/external-agents" target="_blank" rel="noreferrer">Zed</a></li>
<li><a href="https://neovim.io/" target="_blank" rel="noreferrer">neovim</a> through the <a href="https://github.com/olimorris/codecompanion.nvim" target="_blank" rel="noreferrer">CodeCompanion</a> plugin</li>
</ul>
<h2 id="supported-agents"><span>Supported Agents</span></h2>
<ul>
<li><a href="https://github.com/google-gemini/gemini-cli" target="_blank" rel="noreferrer">Gemini</a></li>
<li>… more coming soon ;)</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nokia’s legendary font makes for a great user interface font (254 pts)]]></title>
            <link>https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/</link>
            <guid>45074071</guid>
            <pubDate>Sat, 30 Aug 2025 12:31:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/">https://www.osnews.com/story/143222/it-turns-out-nokias-legendary-font-makes-for-a-great-general-user-interface-font/</a>, See on <a href="https://news.ycombinator.com/item?id=45074071">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page"><article itemscope="" itemtype="http://schema.org/BlogPosting"><div id="content_box"><div><p><span itemscope="" itemprop="itemlistElement"><a itemprop="item" rel="v:url" property="v:title" href="https://www.osnews.com/"><i></i>&nbsp;Home</a></span>&nbsp;&gt;&nbsp;<span itemscope="" itemprop="itemlistElement" typeof="v:Breadcrumb"><a itemprop="item" href="https://www.osnews.com/topic/osnews/" rel="v:url" property="v:title">OS News</a></span>&nbsp;&gt;&nbsp;<span><span>It turns out Nokia’s legendary font makes for a great general user interface font</span></span></p><header></header><div itemprop="articleBody"><p>If you’re of a certain age (and not American), there’s a specific corporate font you’re most likely aware of. You may not know its exact name, and you may not actively remember it, but once you see it, you know exactly what you’re looking at. The font’s called Nokia Sans (and Nokia Serif), and it was used by pretty much every single Nokia device between roughly 2002 and 2013 or so, when it was replaced by <a href="https://en.wikipedia.org/wiki/Nokia_Pure">a very bland font</a> made by Bruno Maag (with help from the person who designed Comic Sans) that they used after that.</p><p>I can’t remember why, exactly, but I got majorly nostalgic for Nokia’s characteristic, recognisable font, and decided to see if it would work as a user interface font. Now, the font is still owned by Nokia and I couldn’t find a proper place to download it, but I eventually stumbled upon a site that had <a href="https://en.maisfontes.com/font-family/nokia-sans-font-family-download-free">each individual variant listed for download</a>. I downloaded each of them, installed them using KDE’s font installation method, and tried it out as my user interface font.</p><figure><figure><a href="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-scaled.png"><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMDI0IiBoZWlnaHQ9IjQyMSIgdmlld0JveD0iMCAwIDEwMjQgNDIxIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBzdHlsZT0iZmlsbDojY2ZkNGRiO2ZpbGwtb3BhY2l0eTogMC4xOyIvPjwvc3ZnPg==" fetchpriority="high" decoding="async" width="1024" height="421" data-id="143225" data-src="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-1024x421.png" alt="" data-srcset="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-1024x421.png 1024w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-300x123.png 300w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-768x316.png 768w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-1536x632.png 1536w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250821_232812-1-2048x843.png 2048w" data-sizes="(max-width: 1024px) 100vw, 1024px"></a></figure><figure><a href="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250829_234813.png"><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI5ODMiIGhlaWdodD0iODY1IiB2aWV3Qm94PSIwIDAgOTgzIDg2NSI+PHJlY3Qgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIgc3R5bGU9ImZpbGw6I2NmZDRkYjtmaWxsLW9wYWNpdHk6IDAuMTsiLz48L3N2Zz4=" decoding="async" width="983" height="865" data-id="143223" data-src="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250829_234813.png" alt="" data-srcset="https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250829_234813.png 983w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250829_234813-300x264.png 300w, https://www.osnews.com/wp-content/uploads/2025/08/Screenshot_20250829_234813-768x676.png 768w" data-sizes="(max-width: 983px) 100vw, 983px"></a></figure></figure><p>You’ll quickly discover you shouldn’t use the regular variant, but should instead opt for the Nokia Sans Wide variant. Back in 2011, when Nokia originally announced it was replacing Nokia Sans, the creator of the font, Erik Spiekermann, <a href="https://spiekermann.com/en/nokia-sans-character/">responded to the announcement on his blog</a>. Apparently, one of the major reasons for Nokia to change fonts was that they claimed Nokia Sans wouldn’t work as a user interface font, but Spiekermann obviously disagrees, pointing specifically to the Wide variant. In fact, Spiekermann does not pull any punches.</p><blockquote><p>After 10 years it was high time to look at Nokia’s typefaces as the dominant visual voice of the brand but whoever decided on a completely new direction was either not aware of what was available or was persuaded by Bruno Maag to start over. Bruno may not create the most memorable typefaces, but he certainly knows how to sell them. And technically, their fonts are excellent. Too bad they didn’t have the confidence to work with me on an update. Instead they’re throwing out ten years of brand recognition in favour of blandness.</p>
<cite><a href="https://spiekermann.com/en/nokia-sans-character/">↫ Erik Spiekermann</a></cite></blockquote><p>I was pleasently surprised by just how nice the font looks when used as a general user interface font. It’s extremely legible at a variety of sizes, and has a ton of character without becoming gimmicky or overbearing. What originally started as mere curiosity has now become my UI font of choice on all my machines, finally displacing <a href="https://rsms.me/inter/">Inter</a> after many years of uncontested service. Of course, all of this is deeply personal and 95% an issue of taste, but I wanted to write about it to see if I’m just entirely crazy, or if there’s some method to my madness.</p><p>Do note that I’m using high DPI displays, and KDE on Wayland, and that all of this may look different on Windows or macOS, or on displays with lower DPI. One of Inter’s strengths is that it renders great on both high and lower DPI displays, but since I don’t have any lower DPI displays anymore, I can’t test it in such an environment. I’m also not entirely sure about the legal status of downloading fonts like this, but I am fairly sure you’re at least allowed to use non-free fonts for personal, non-commercial use, but please don’t quote me on that. Since downloading each variant of these Nokia fonts is annoying, I’d love to create and upload a zip file containing all of them, but I’m sure that’s illegal.</p><p>I’m not a font connoisseur, so I may be committing a huge faux pas here? Not that I care, but reading about font nerds losing their minds over things I never even noticed is always highly entertaining.</p></div></div><div><h4>About The Author</h4>
<p><img data-lazyloaded="1" src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI3MiIgaGVpZ2h0PSI3MiIgdmlld0JveD0iMCAwIDcyIDcyIj48cmVjdCB3aWR0aD0iMTAwJSIgaGVpZ2h0PSIxMDAlIiBzdHlsZT0iZmlsbDojY2ZkNGRiO2ZpbGwtb3BhY2l0eTogMC4xOyIvPjwvc3ZnPg==" alt="" data-src="https://www.osnews.com/wp-content/litespeed/avatar/8f4c2152b7e9c06a31bd3f75f0549996.jpg?ver=1756328442" data-srcset="https://www.osnews.com/wp-content/litespeed/avatar/a9c5be496e027f80968deb12c93dd064.jpg?ver=1756328441 2x" height="72" width="72" decoding="async"></p><h5><a href="https://www.osnews.com/story/author/thom-holwerda/" rel="nofollow">Thom Holwerda</a></h5><p>Follow me on Mastodon <a href="https://exquisite.social/@thomholwerda">@<span data-cfemail="c0b4a8afada8afacb7a5b2a4a180a5b8b1b5a9b3a9b4a5eeb3afa3a9a1ac">[email&nbsp;protected]</span></a></p></div></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms (145 pts)]]></title>
            <link>https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</link>
            <guid>45072160</guid>
            <pubDate>Sat, 30 Aug 2025 05:45:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24">https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24</a>, See on <a href="https://news.ycombinator.com/item?id=45072160">Hacker News</a></p>
Couldn't get https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Hacker News em dash user leaderboard pre-ChatGPT (254 pts)]]></title>
            <link>https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html</link>
            <guid>45071722</guid>
            <pubDate>Sat, 30 Aug 2025 03:40:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html">https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html</a>, See on <a href="https://news.ycombinator.com/item?id=45071722">Hacker News</a></p>
Couldn't get https://www.gally.net/miscellaneous/hn-em-dash-user-leaderboard.html: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Pentagon Docs: US Wants to "Suppress Dissenting Arguments" Using AI Propaganda (105 pts)]]></title>
            <link>https://theintercept.com/2025/08/25/pentagon-military-ai-propaganda-influence/</link>
            <guid>45071063</guid>
            <pubDate>Sat, 30 Aug 2025 01:10:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theintercept.com/2025/08/25/pentagon-military-ai-propaganda-influence/">https://theintercept.com/2025/08/25/pentagon-military-ai-propaganda-influence/</a>, See on <a href="https://news.ycombinator.com/item?id=45071063">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p><span>The United States</span> hopes to use machine learning to create and distribute propaganda overseas in a bid to “influence foreign target audiences” and “suppress dissenting arguments,” according to a U.S. Special Operations Command document reviewed by The Intercept.</p>
<p>The document, a sort of special operations wishlist of near-future military technology, reveals new details about a broad variety of capabilities that SOCOM hopes to purchase within the next five to seven years, including state-of-the-art cameras, sensors, directed energy weapons, and other gadgets to help operators find and kill their quarry. Among the tech it wants to procure is machine-learning software that can be used for information warfare.</p>
<p>To bolster its “Advanced Technology Augmentations to Military Information Support Operations” — also known as MISO — SOCOM is looking for a contractor that can “Provide a capability leveraging agentic Al or multi‐LLM agent systems with specialized roles to increase the scale of influence operations.”</p>
<p>So-called “agentic” systems use machine-learning models purported to operate with minimal human instruction or oversight. These systems can be used in conjunction with large language models, or LLMs, like ChatGPT, which generate text based on user prompts. While much marketing hype orbits around these agentic systems and LLMs for their potential to execute mundane tasks like online shopping and booking tickets, SOCOM believes the techniques could be well suited for running an autonomous propaganda outfit.</p>
<p>“The information environment moves too fast for military remembers [sic] to adequately engage and influence an audience on the internet,” the document notes. “Having a program built to support our objectives can enable us to control narratives and influence audiences in real time.”</p>
<p>Laws and Pentagon policy generally prohibit military propaganda campaigns from targeting U.S. audiences, but the porous nature of the internet makes that difficult to ensure.</p>
<p>In a statement, SOCOM spokesperson Dan Lessard acknowledged that SOCOM is pursuing “cutting-edge, AI-enabled capabilities.”</p>
<p>“All AI-enabled capabilities are developed and employed under the Department of Defense’s Responsible AI framework, which ensures accountability and transparency by requiring human oversight and decision-making,” he told The Intercept. “USSOCOM’s internet-based MISO efforts are aligned with U.S. law and policy. These operations do not target the American public and are designed to support national security objectives in the face of increasingly complex global challenges.”</p>
<p><span>Tools like OpenAI’s</span> ChatGPT or Google’s Gemini have surged in popularity despite their propensity for factual errors and other erratic outputs. But their ability to immediately churn out text on virtually any subject, written in virtually any tone — from casual trolling to pseudo-academic — could mark a major leap forward for internet propagandists. These tools give users the potential to finetune messaging&nbsp;any number of audiences without the time or cost of human labor.</p>
<p>Whether AI-generated propaganda works remains an open question, but the practice has already been amply documented in the wild. In May 2024, OpenAI issued a report revealing efforts by Iranian, Chinese, and Russian actors to use the company’s tools to engage in covert influence campaigns, but found none had been particularly successful. In comments before the 2023 Senate AI Insight Forum, Jessica Brandt of the Brookings Institution warned “LLMs could increase the personalization, and therefore the persuasiveness, of information campaigns.” In an online ecosystem filled with AI information warfare campaigns, “skepticism about the existence of objective truth is likely to increase,” she cautioned. A 2024 study published in the academic journal PNAS Nexus found that “language models can generate text that is nearly as persuasive for US audiences as content we sourced from real-world foreign covert propaganda campaigns.”</p>

<p>Unsurprisingly, the national security establishment is now insisting that the threat posed by this technology in the hands of foreign powers, namely Russia and China, is most dire.</p>
<p>“The Era of A.I. Propaganda Has Arrived, and America Must Act,” <a href="https://www.nytimes.com/2025/08/05/opinion/china-ai-propaganda.html">warned</a> a recent New York Times opinion essay on GoLaxy, software created by the Chinese firm Beijing Thinker originally used to play the board game Go. Co-authors Brett Benson, a political science professor at Vanderbilt University, and Brett Goldstein, a former Department of Defense official, paint a grim picture showing GoLaxy as an emerging&nbsp;leader in state-aligned influence campaigns.</p>
<p>GoLaxy, they caution, is able to scan public social media content and produce bespoke propaganda campaigns. “The company privately claims that it can use a new technology to reshape and influence public opinion on behalf of the Chinese government,” <a href="https://www.nytimes.com/2025/08/06/us/politics/china-artificial-intelligence-information-warfare.html">according to a companion piece</a> by Times national security reporter Julian Barnes headlined “China Turns to A.I. in Information Warfare.” The news item strikes a similarly stark tone: “GoLaxy can quickly craft responses that reinforce the Chinese government’s views and counter opposing arguments. Once put into use, such posts could drown out organic debate with propaganda.” According to these materials, the Times says, GoLaxy has “undertaken influence campaigns in Hong Kong and Taiwan, and collected data on members of Congress and other influential Americans.”</p>
<p>To respond to this foreign threat, Benson and Goldstein argue a “coordinated response” across government, academia, and the private sector is necessary. They describe this response as defensive in nature: mapping and countering foreign AI propaganda.</p>
<!-- BLOCK(cta)[0](%7B%22componentName%22%3A%22CTA%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->


<!-- END-BLOCK(cta)[0] -->
<p>That’s not what the document from the Special Operations Forces Acquisition, Technology, and Logistics Center suggests the Pentagon is seeking.</p>
<p>The material shows SOCOM believes it needs technology that closely matches the reported Chinese capabilities, with bots scouring and ingesting large volumes of internet chatter to better persuade a targeted population, or an individual, on any given subject.</p>
<p>SOCOM says it specifically wants “automated systems to scrape the information environment, analyze the situation and respond with messages that are in line with MISO objectives. This technology should be able to respond to post(s), suppress dissenting arguments, and produce source material that can be referenced to support friendly arguments and messages.”</p>
<p>The Pentagon is paying especially close attention to those who might call out its propaganda efforts.</p>
<p>“This program should also be able to access profiles, networks, and systems of individuals or groups that are attempting to counter or discredit our messages,” the document notes. “The capability should utilize information gained to create a more targeted message to influence that specific individual or group.”</p>
<figure>
<blockquote>
<p>“This program should also be able to access profiles, networks, and systems of individuals or groups that are attempting to counter or discredit our messages.”</p>
</blockquote>
</figure>
<p>SOCOM anticipates using generative systems to both craft propaganda messaging and simulate how this propaganda will be received once sent into the wild, the document notes. SOCOM hopes it will use “agentic systems that replicate specific knowledge, skills, abilities, personality traits, and sociocultural attributes required for different roles of individuals comprising a team,” before moving on to “brainstorm and test operational campaigns against agent‐based replicas of individuals and groups.” These simulations are more elaborate than focus groups, calling instead for “comprehensive models of entire societies to enable MISO planners to use these models to experiment or test various multiple scenarios.”</p>
<p>The SOCOM wishlist continues to include a need for <a href="https://theintercept.com/2023/03/06/pentagon-socom-deepfake-propaganda/">offensive deepfake capabilities</a>, first reported by The Intercept in 2023.</p>
<p><span>The prospect of</span> LLMs creating an infinite firehose of expertly crafted propaganda has been received by alarm — but generally in the context of the United States as target, not perpetrator.</p>
<p>A 2023 publication by the State Department-funded nonprofit Freedom House warned of “The Repressive Power of Artificial Intelligence,” predicting “AI-assisted disinformation campaigns will skyrocket as malicious actors develop additional ways to bypass safeguards and exploit open-source models.” Warning that “Generative AI draws authoritarian attention,” the Freedom House report cites potential use by China and Russia, but only mentions domestic use of the technology in a brief section about the presidential campaigns of Ron DeSantis and Donald Trump, as well as a deepfake video of Joe Biden manipulated to depict the former president making transphobic comments. The extent to which an automated propaganda machine capable of global reach warrants public concern depends on the scope of its application, according to Andrew Lohn, former director for emerging technology on the National Security Council.</p>
<p>“I would not be so concerned if some foreign soldiers are wrongly convinced that our special operation is going to happen Wednesday morning by helicopter from the east rather than Tuesday night by boat from the west,” said Lohn, now a senior fellow at Georgetown’s Center for Security and Emerging Technology.</p>
<p>The military has a history of manipulating civilian populations for political or ideological purposes. A troubling example was uncovered in 2024, when Reuters reported the Defense Department had operated a <a href="https://www.reuters.com/investigates/special-report/usa-covid-propaganda/">clandestine anti-vax social media campaign</a> to undercut public confidence in the Chinese Covid vaccine, fearing its efficacy might draw Asian countries closer to a major geopolitical rival. Pentagon-created tweets described the Chinese Sinovac-CoronaVac shot — <a href="https://www.who.int/news-room/feature-stories/detail/the-sinovac-covid-19-vaccine-what-you-need-to-know">described</a> by the World Health Organization as “safe and effective” — as “fake” and untrustworthy. According to the Reuters report, then-Special Operations Command Pacific General Jonathan Braga “pressed his bosses in Washington to fight back in the so-called information space” by backing the clandestine propaganda campaign.</p>
<p>William Marcellino, a behavioral scientist at the RAND Corporation focusing on the geopolitics of machine-learning systems and Pentagon procurement, told The Intercept such systems are being built out of necessity. “Regimes like those from China and Russia are engaged in AI-enabled, at-scale malign influence efforts,” he said. State-affiliated groups in China, he warned, “have explicitly designed AI at-scale systems for public opinion warfare.”</p>
<p>“Countering those campaigns likely requires AI at-scale responses,” he said.</p>
<!-- BLOCK(newsletter)[0](%7B%22componentName%22%3A%22NEWSLETTER%22%2C%22entityType%22%3A%22SHORTCODE%22%2C%22optional%22%3Atrue%7D)(%7B%7D) -->

<!-- END-BLOCK(newsletter)[0] -->
<p>SOCOM has in recent years been public about its desire for AI-created propaganda systems. These statements suggest a broader interest that includes influence operations against entire populations, as opposed to narrowly tailored toward military personnel.</p>
<p>In 2019, a senior Pentagon special operations official <a href="https://www.militarytimes.com/news/your-military/2019/02/06/socom-needs-to-step-up-its-propaganda-game-pentagon-deputy-says/">spoke at a defense symposium</a> of the country’s “need to move beyond our 20th century approach to messaging and start looking at influence as an integral aspect of modern irregular warfare.” The official noted that this “will also require new partnerships beyond traditional actors, throughout the world, through efforts to amplify voices of [non-governmental organizations] and individual citizens who bring transparency to malign activities of our competitors.” The following year, then-SOCOM commander Gen. Richard Clarke described his interest in using AI to achieve these ends.</p>
<p>“As we look at the ability to influence and shape in this [information] environment, we’re going to have to have artificial intelligence and machine learning tools,” Clarke said in 2020 remarks first reported by <a href="https://www.nationaldefensemagazine.org/articles/2020/5/12/socom-all-in-on-artificial-intelligence">National Defense Magazine</a>, “specifically for information ops that hit a very broad portfolio, because we’re going to have to understand how the adversary is thinking, how the population is thinking, and work in these spaces.”</p>
<p>Heidy Khlaaf, chief scientist at the AI Now Institute and former safety engineer at OpenAI, warned against a fighting-fire-with-fire approach: “Framing the use of generative and agentic AI as merely a mitigation to adversaries’ use is a misrepresentation of this technology, as offensive and defensive uses are really two sides of the same coin and would allow them to use it precisely in the same way that adversaries do.”</p>
<p>Automated online influence campaigns might wind up having lackluster results, according to Emerson Brooking, a senior fellow at the Atlantic Council’s Digital Forensic Research Lab. “Russia has been using AI programs to automate its influence operations. The program is not very good,” he said.</p>
<p>The tendency of LLMs to fabricate falsehoods and perpetuate preconceptions when prompted by users could also prove a major liability, Brooking warned. “Tasked with figuring out the ‘hearts and minds’ of a complex and understudied country, they may lean heavily on an AI to help them, which will be likely to tell them what they already want to hear,” he said.</p>
<p>Khlaaf added that “agentic” systems, heavily marketed by tech firms as independent digital brains, are still error-prone and unpredictable. “The introduction of agentic AI in these disinformation campaigns adds a layer of both safety and security concerns, as several research results have demonstrated how easily we can compromise and divert the behavior of agentic AI,” she told The Intercept. “With these security issues unresolved, [SOCOM] risks that their campaigns are not only compromised, but that they produce material that was not intended.”</p>
<figure>
<blockquote>
<p> “AI tends to make these campaigns stupider, not more effective.”</p>
</blockquote>
</figure>
<p>Brooking, who previously worked as an adviser to the Office of the Under Secretary of Defense for Policy on cybersecurity matters, also pointed to the mixed track record of prior U.S. online propaganda efforts. In 2022, researchers revealed a network of Twitter and Facebook accounts<a href="https://theintercept.com/2022/12/20/twitter-dod-us-military-accounts/"> secretly operated by U.S. Central Command</a> that had been pushing bogus news articles containing anti-Russian and Iranian talking points. The network, which failed to gain traction on either social network, quickly became an embarrassment for the Pentagon.</p>
<p>“We know from other&nbsp;<a href="https://graphika.com/reports/unheard-voice">public reporting</a>&nbsp;that the U.S. has long sought to ‘suppress dissenting arguments’ and generate positive press in certain areas of operation,” he said. “We also know that these efforts have not worked very well and can be deeply embarrassing or counterproductive when revealed to the American public. AI tends to make these campaigns stupider, not more effective.”</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Romania excels in international Olympiads (209 pts)]]></title>
            <link>https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/</link>
            <guid>45070793</guid>
            <pubDate>Sat, 30 Aug 2025 00:09:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/">https://www.palladiummag.com/2025/08/29/why-romania-excels-in-international-olympiads/</a>, See on <a href="https://news.ycombinator.com/item?id=45070793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p><span>Olympiads are international student intellectual competitions in which students from across the world go toe-to-toe answering questions in mathematics, physics, informatics, chemistry, and more. The best performers tend to be from countries like China, the United States, India, and Japan. But, somehow, the southeastern European country of Romania also frequently tops the list.</span></p>
<p><span>Since 2020, Romania’s performance in the International Mathematical Olympiad (IMO) has been nothing short of amazing. In 2022, Romania came in fifth overall, fourth in 2023, and twelfth in 2024. In 2023, Romania placed fourth globally and first in Europe at the International Physics Olympiad, seventeenth globally and third in Europe at the International Olympiad in Informatics, sixth globally and second in Europe in the European Girls’ Mathematical Olympiad, first in the Balkan Mathematical Olympiad—which also included France, Italy, and the United Kingdom—and first in the Central European Olympiad in Informatics. Romania also performed well in the International Chemistry Olympiad and many others.</span></p>
<p><span>It’s an understatement to call Romania’s skill in Olympiads merely “overperformance”. Romania’s lackluster performance in international assessments and its relatively small population size of just over 19 million people makes the things they do in Olympiads downright miraculous.</span></p>
<p><span>Average Romanian educational performance is unimpressive. Romanian youth routinely perform below the average of OECD countries and near the bottom of the pack of European nations. Romania has a poor-to-mediocre showing whether you include or exclude migrants from the calculations, and its scores on assessments like the PISA aren’t low due to being tainted by bias in the examinations. Romania genuinely underperforms. But underperformance is not the impression you would get if you only knew of Romanian education from Olympiads.</span></p>
<p><span>One possibility is that Romanian students have more variable performance on international assessments than students in other countries. No dice: </span><a href="https://www.oecd.org/en/about/programmes/pisa.html"><span>they</span></a> <a href="https://timssandpirls.bc.edu/timss-landing.html"><span>aren’t</span></a> <a href="https://www.iea.nl/studies/iea/pirls"><span>much</span></a><span> more variable than the student populations in other countries, and a handful of comparably-sized nations with worse Olympiad performance are more variable. Another possibility is that, for some reason, there’s a fat right tail in Romanian educational performance. If this is true, it just doesn’t show up in any existing data. Given the fact that international assessments indicate Romania’s sampling tends to be population-representative, we should have a strong prior against this possibility. Romanian test scores tend to be distributed along a symmetrical bell curve.&nbsp;</span></p>
<p><span>Yet another possibility is that Romania has an undersampled ethnic group that overperforms, but whose schools aren’t tested very well. The only group this might be is Romanian Jews and using them as an explanation is problematic for two reasons. The first is that there are too few to realistically explain Romanian Olympiad performance. The second is that we know the identities of Olympiad participants from Romania, and they don’t seem to be Jewish.</span></p>
<p><span>Something else, something more mysterious, explains why Romania is such an outlier in international intellectual competitions. That thing is, in fact, the unique design of the Romanian educational system.</span></p>
<p><span>In the late 19</span><span>th</span><span> century, Romanian prince regnant Alexandru Ioan Cuza attempted to raise the status of the nation by instituting a mass literacy campaign centered around building free schools that children were compelled to attend. This effort was largely a failure, with literacy failing to break 50% by the 1930s. But World War II precipitated change. In 1948, Romania’s new governing communist party began to bring about serious educational reform at a breakneck pace.The Education Law of 1948 was passed to provoke a military-grade offensive against illiteracy, involving the mass participation of the literate from all walks of life in uplifting the poor, the abandoned, and those who simply shunned education. By the end of the 1950s, illiteracy was practically eradicated among Romania’s youth.</span></p>
<p><span>The education system that existed in Romania’s communist period was modeled on the system in place in the Soviet Union, and it included a fair helping of political propaganda in addition to physical labor. The system also overproduced schools, resulting in shoddy but widely available facilities dotting the country. Like the Soviet school system, Romania’s was marked by increasing lengths of compulsory education, poor availability of qualified teachers and educational supplies, high budgetary costs, and an extreme level of credential inflation.</span></p>
<p><span>After the fall of communism, the new democratic government went on to shutter many of these schools and to immediately lower compulsory schooling requirements to put an end to the bureaucratic nightmare that Soviet influence had saddled the country with. In the following years, how Romania wished to ration scarce governmental resources for education was a matter of intense debate, and out of that debate came a strong sentiment that, whatever the system, Romanian education would be structured competitively.</span></p>
<p><span>Nowadays, the most prestigious Romanian high schools are the National Colleges, or </span><i><span>Colegiu Național</span></i><span>. These schools are often international and frequently uphold old educational traditions sometimes dating back more than a century. Below these schools are the </span><i><span>Liceu Teoretic</span></i><span>, which are the norm, offering standard educations. Romania also has three military colleges—</span><i><span>Colegiu Militar</span></i><span>—managed directly by the Ministry of National Defense. There are also schools focused on service, technical schools, vocational schools, and apprenticeship programs. The brightest students get their pick among these schools after they take the national placement test, the </span><i><span>Evaluarea Națională, </span></i><span>when they are graduating the 8</span><span>th</span><span> grade around ages fourteen to fifteen.</span></p>
<p><span>The high school placement test is a standardized test covering Romanian language and literature as well as mathematics. Performance on the examination is reported publicly when students are issued a score on a one-to-ten scale with precision to two decimal places. A student who receives a high grade—say 9.65—would have their pick from most any school, whereas a student scoring 5.00 or below would usually be constrained to a less academically-focused form of education like a vocational program. Most students elect to go to the best school they are able to test into, and so the degree of sorting across schools is very high. To make this setup even more extreme, there is also often—but not universally—sorting </span><i><span>within</span></i><span> schools, as students select into educational tracks. This is done directly when applying to schools.</span></p>
<p><span>At the end of the Romanian high school experience, there is a graduation test, the </span><i><span>Bacalaureat</span></i><span>, or </span><i><span>bac</span></i><span>. This test is marked like the entrance examination and, to pass, students must obtain a score of at least five in the subjects they have elected to take. This testing includes written and oral examinations, assessments of foreign language and computer skills, and, for ethnic minorities, assessment of their skill with their maternal language other than Romanian. The need for a given score on this examination can range from requiring just passing to requiring a high score, depending on the university one intends to attend, if that is their goal.</span></p>
<p><span>The design of Romania’s educational system makes it perhaps the most stratified educational system in the world. The fact that they have a centralized repository containing all student and teacher educational data makes their system perfect for a high-powered evaluation of exactly what happens when a country opts to hyper-stratify education.</span></p>
<p><span>One of the cruel parts of the Romanian system is that, though sorting is nationally available, students do not have equal opportunities to sort. Students located in smaller towns have fewer high school options to select from unless they’re among the few who opt into a military academy, which means joining the military. The extent of sorting is far more intense in areas with larger numbers of schools. In a </span><a href="https://direct.mit.edu/rest/article-abstract/doi/10.1162/rest_a_01438/120190/School-Choice-Student-Sorting-and-Academic"><span>recent paper</span></a><span>, the Romanian economist Andrei Munteanu provided an illustration of how this works: essentially, the fewer schools in a locale, the more each individual school contains students with a wider range of ability and, the more schools in a locale, the more each individual school will be stratified into low, middle, or high ability.&nbsp;</span></p>
<p><span>This combined sorting between schools and tracks means that low-ability students get stuck with other low-ability students, and high-ability students are surrounded by other high-ability students. In effect, peer groups throughout high school are extremely homogeneous. This matters because then low-performing students drag down low-performing students, and high performers cause each other to rise. Romania’s educational system has causal peer impacts on student performance on the graduation test that are very large in both directions, but primarily where there are opportunities for sorting to take place.</span></p>
<figure id="attachment_7695" aria-describedby="caption-attachment-7695"><img decoding="async" src="https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy.png" alt="" width="3600" height="2400" srcset="https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-300x200.png 300w, https://pdmedia.b-cdn.net/2025/08/sIOqtz0e-SchoolSortBusy-1024x683.png 1024w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-768x512.png 768w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-1536x1024.png 1536w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-2048x1365.png 2048w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-450x300.png 450w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy-900x600.png 900w, https://pdmedia.b-cdn.net/2025/08/SchoolSortBusy.png 3600w" sizes="(max-width: 999px) 96vw, (max-width: 1359px) 80vw, (max-width: 1519px) 75vw, 1175px"><figcaption id="caption-attachment-7695">Jordan Lasker/The more schools a town has the more intense the sorting of students is. Graduation scores are positively impacted for top performers and negatively for bottom performers with more intense sorting.</figcaption></figure>
<p><span>But peer effects are not everything to Romania’s exceptional Olympiad performance; they are just the fertile ground in which exceptional performance is fostered. The next part has to do with teachers. Like students, Romania’s teachers must take tests to be able to do what they want to do. Teachers naturally prefer to lecture smarter students, and the smartest teachers have their pick of the schools, and even of the tracks. In a paper with extremely robust results, researchers from the last decade </span><a href="https://www.aeaweb.org/articles?id=10.1257/aer.103.4.1289"><span>described</span></a><span> this as such:</span></p>
<blockquote><p><i><span>[Teachers] with higher certification standards are more likely to work at better-ranked schools. This sorting persists even within schools as one moves from a weaker to a stronger track, and even within tracks as one moves from a weaker to a stronger class.</span></i></p></blockquote>
<p><span>The best teachers also opt into towns with more schools. It’s apparent, then, that teachers prefer teaching in the highest-achieving places they can be, both within and between towns. The effect of teacher-student ability pairing is accentuated even more by incentives to compete. The government of Romania is not unique in providing monetary rewards for those who win Olympiads, those who teach winners of Olympiads, or those schools Olympiad winners attend, but they are unique in having all the previously-mentioned institutional characteristics </span><a href="https://archive.md/ki6Do"><span>on top of</span></a><span> providing comprehensive monetary incentives for Olympiad achievement.&nbsp;</span></p>
<p><span>Romania’s immense success in Olympiads and the widely recognized importance of Olympiad wins for signaling student human capital has also spawned a small number of private schools that advertise their prominence and tutoring capabilities. Many teachers also recommend to parents that they obtain additional tutoring for their brighter pupils, and tutoring services are commonplace. The commonality of tutoring for Olympiad winners is a global constant, whereas the things distinguishing Romania are not.</span></p>
<p><span>Two notable factors do not increase performance in the same direction. These are very slight decrements in funding allocated to the highest-ability schools, and when parents reduce the time they spend helping their students with homework, conditional on their kids matching into better schools. Another potential factor that militates against the synchrony of resource allocation in Romania is that children in more selective schools report feeling marginalized because they realize that they’re not as strong of students as they believed. The decrements in funding are likely to be unproblematic, because higher-scoring schools tend to be larger and more urban, lending them economies of scale. Due to this, they may have effectively more funding.</span></p>
<p><span>With all the pieces on the board, the key to Romania’s Olympiad success is three-fold: put the best students in the same classrooms, put the best teachers with the best students, and then incentivize schools, teachers, and students each to win Olympiads.</span></p>
<p><span>This system has proved amazingly fruitful. Given its underlying human capital, the poverty from its communist legacy, and its modest population size, Romania should not perform the way it does in academic Olympiads. And yet it does</span><i><span>. </span></i><span>The trade-off for Romania, however, is palpable.</span></p>
<p><span>Large portions of Romania’s Olympiad winners leave the country. Because Romania is a member state of the European Union, the people the country has put great effort into training and credentialing are easily able to leave the country and acquire jobs elsewhere.</span></p>
<p><span>Losing the right tail to brain drain is damaging for many countries, but it’s arguably worse for Romania because its educational system is so zero-sum: the top performers do better, while the low-performers do worse. This sorting does not “lift all boats,” as it were. In Romania, the system makes for an incredibly well-trained right tail and a neglected left tail, and that left tail might hurt more than the right tail is helped, if effects on test scores are any indication. On its own, Romania’s system might be a stellar boon to the country. But with free movement of talent between countries, Romania ends up subsidizing talent discovery for other countries with less apt educational systems.&nbsp;</span></p>
<p><span>Most of the growth we see around us is due to the innovations of the right tail, and if they do better, we all do better. Though I doubt Romania’s schooling raises the intelligence</span> <span>of the right tail, even raising aptitude is worth something, because we must get capable people to the frontiers of their respective fields in order to innovate, and Romania has fostered a system that seems to do just that. Moreover, even if Olympiad training does not make those on the right tail more capable but instead simply prepares them better, then it can still</span> <span>have large, socially beneficial effects simply through providing Romania’s highly capable people with a means of having their talents recognized internationally.&nbsp;</span></p>
<p><span>But these benefits are returned only very indirectly to Romania, if at all on net. Rather than changing Romania’s educational system or closing the borders, the right solution is for more nations to choose to be like Romania, getting a lot more juice out of their smart kids by designing a system just for them.</span></p>
<p>Jordan Lasker is a bioinformatician. He writes on his <a href="https://www.cremieux.xyz/">website</a> and you can follow him at <a href="https://x.com/cremieuxrecueil">@cremieuxrecueil</a>.</p>


            </div></div>]]></description>
        </item>
    </channel>
</rss>