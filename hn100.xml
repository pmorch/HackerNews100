<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 15 Nov 2023 14:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Gut epithelial barrier damage caused by dishwasher detergents and rinse aids (152 pts)]]></title>
            <link>https://www.sciencedirect.com/science/article/pii/S0091674922014774</link>
            <guid>38275060</guid>
            <pubDate>Wed, 15 Nov 2023 10:01:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sciencedirect.com/science/article/pii/S0091674922014774">https://www.sciencedirect.com/science/article/pii/S0091674922014774</a>, See on <a href="https://news.ycombinator.com/item?id=38275060">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="mathjax-container" role="main"><div role="region" aria-label="Download options and search"><ul aria-label="PDF Options"><li><a aria-label="View PDF. Opens in a new window."><svg focusable="false" viewBox="0 0 32 32" height="24" width="24"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke="#000" stroke-width=".703" fill="#fff"></path><path d="M.167 2.592H22.39V9.72H.166z" fill="#da0000"></path><path fill="#fff9f9" d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill="#f91d0a"></path></svg><span>View&nbsp;<strong>PDF</strong></span></a></li><li></li></ul></div><div><article lang="en"><div id="publication"><p><a href="https://www.sciencedirect.com/journal/journal-of-allergy-and-clinical-immunology" title="Go to Journal of Allergy and Clinical Immunology on ScienceDirect"><span><img src="https://sdfestaticassets-eu-west-1.sciencedirectassets.com/prod/ee25509452584c835c16f84fe55f10de78d4228e/image/elsevier-non-solus.png" alt="Elsevier"></span></a></p><p><a href="https://www.sciencedirect.com/journal/journal-of-allergy-and-clinical-immunology/vol/151/issue/2"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0091674922X00031-cov150h.gif" alt="Journal of Allergy and Clinical Immunology"></span></a></p></div><h2 id="screen-reader-main-title"><p><span>Food allergy and gastrointestinal disease</span></p><span>Gut epithelial barrier damage caused by dishwasher detergents and rinse aids</span></h2><p id="article-identifier-links"><a href="https://doi.org/10.1016/j.jaci.2022.10.020" target="_blank" rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier"><span>https://doi.org/10.1016/j.jaci.2022.10.020</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a><a href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0091674922014774&amp;orderBeanReset=true" target="_blank" rel="noreferrer noopener"><span>Get rights and content</span><svg focusable="false" viewBox="0 0 78 128" aria-label="Opens in new window" width="1em" height="1em"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg></a></p><div id="abstracts" lang="en"><div id="abssec0010"><h3 id="sectitle0010">Background</h3><p id="abspara0010">The increased prevalence of many chronic inflammatory diseases linked to gut epithelial barrier leakiness has prompted us to investigate the role of extensive use of dishwasher detergents, among other factors.</p></div><div id="abssec0015"><h3 id="sectitle0015">Objective</h3><p id="abspara0015">We sought to investigate the effects of professional and household dishwashers, and rinse agents, on cytotoxicity, barrier function, transcriptome, and protein expression in gastrointestinal epithelial cells.</p></div><div id="abssec0020"><h3 id="sectitle0020">Methods</h3><p id="abspara0020">Enterocytic liquid-liquid interfaces were established on permeable supports, and direct cellular cytotoxicity, transepithelial electrical resistance, paracellular flux, immunofluorescence staining, RNA-sequencing transcriptome, and targeted proteomics were performed.</p></div><div id="abssec0025"><h3 id="sectitle0025">Results</h3><p id="abspara0025">The observed detergent toxicity was attributed to exposure to rinse aid in a dose-dependent manner up to 1:20,000 v/v dilution. A&nbsp;disrupted epithelial barrier, particularly by rinse aid, was observed in liquid-liquid interface cultures, organoids, and gut-on-a-chip, demonstrating decreased transepithelial electrical resistance, increased paracellular flux, and irregular and heterogeneous tight junction immunostaining. When individual components of the rinse aid were investigated separately, alcohol ethoxylates elicited a strong toxic and barrier-damaging effect. RNA-sequencing transcriptome and proteomics data revealed upregulation in cell death, signaling and communication, development, metabolism, proliferation, and immune and inflammatory responses of epithelial cells. Interestingly, detergent residue from professional dishwashers demonstrated the remnant of a significant amount of cytotoxic and epithelial barrier–damaging rinse aid remaining on washed and ready-to-use dishware.</p></div><div id="abssec0030"><h3 id="sectitle0030">Conclusions</h3><p id="abspara0030">The expression of genes involved in cell survival, epithelial barrier, cytokine signaling, and metabolism was altered by rinse aid in concentrations used in professional dishwashers. The alcohol ethoxylates present in the rinse aid were identified as the culprit component causing the epithelial inflammation and barrier damage.</p></div></div><ul id="issue-navigation"><li></li><li></li></ul><div><div id="kwrds0010"><h2>Key words</h2><p><span>Alcohol ethoxylates</span></p><p><span>Caco-2</span></p><p><span>cytotoxicity</span></p><p><span>dishwasher detergents</span></p><p><span>epithelial barrier</span></p><p><span>inflammation</span></p><p><span>rinse aid</span></p></div><div id="kwrds0015"><h2>Abbreviations used</h2><div id="kwrd0075"><p><span>FITC</span></p><p><span>Fluorescein isothiocyanate</span></p></div><div id="kwrd0105"><p><span>MAPK</span></p><p><span>Mitogen-activated protein kinase</span></p></div><div id="kwrd0115"><p><span>NF-κB</span></p><p><span>Nuclear factor kappa B</span></p></div><div id="kwrd0125"><p><span>NPX</span></p><p><span>Normalized protein expression</span></p></div><div id="kwrd0165"><p><span>SDS</span></p><p><span>Sodium dodecyl sulphate</span></p></div><div id="kwrd0175"><p><span>TEER</span></p><p><span>Transepithelial electrical resistance</span></p></div><div id="kwrd0195"><p><span>TSLP</span></p><p><span>Thymic stromal lymphopoietin</span></p></div></div></div><section aria-label="Cited by" id="section-cited-by"><header id="citing-articles-header"><h2>Cited by (0)</h2></header></section><div><dl><dt></dt><dd><p id="ntpara0010">C. A. Akdis has received research grants from the Swiss National Science Foundation (Bern, Switzerland), CURE-Eubiosis Reinstatement Therapy (European Union), <span id="gs5">Novartis</span> Research Institutes (Basel, Switzerland), <span id="gs6">Stanford University</span> (Redwood City, Calif), and SciBase (Stockholm, Sweden). K. Nadeau reports grants from the National Institute of Allergy and Infectious Diseases (United States), the National Heart, Lung, and Blood Institute (United States), the National Institute of Environmental Health Sciences (United States), and <span id="gs11">Food Allergy Research &amp; Education</span> (United States) and stock options from IgGenix (United States), Seed Health (United States), ClostraBio (United States), and ImmuneID (United States).</p></dd></dl><dl><dt></dt><dd><p id="ntpara0015">Disclosure of potential conflict of interest: C. A. Akdis is the cochair for EAACI Guidelines on Environmental Science in Allergic diseases and Asthma; serves on the advisory boards of Sanofi/Regeneron, Novartis, Seed Health, GlaxoSmithKline (GSK), and SciBase; and is the Editor-in-Chief of <em>Allergy</em>. K. Nadeau is Director of the World Allergy Organization Center of Excellence for Stanford; advisor at Cour Pharma; consultant for Excellergy, Red Tree Ventures, Eli Lilly, and Phylaxis; cofounder of Before Brands, Alladapt, Latitude, and IgGenix; and National Scientific Committee member at Immune Tolerance Network, and National Institutes of Health clinical research centers, outside the submitted work; patents include “Mixed allergen composition and methods for using the same,” “Granulocyte-based methods for detecting and monitoring immune system disorders,” and “Methods and assays for detecting and quantifying pure subpopulations of white blood cells in immune system disorders.” R. Dhir is a cofounder and CEO in Seed, a biotechnology company. M. Sokolowska reports research grants from the <span id="gs12">Swiss National Science Foundation</span>, <span id="gs13">Novartis</span>, and <span id="gs14">GSK</span> and speaker’s fee from AstraZeneca and a leadership in the European Academy of Allergy and Clinical Immunology: Secretary of the Board of the Basic and Clinical Immunology Section. The rest of the authors declare that they have no relevant conflicts of interest.</p></dd></dl></div><p><span>© 2022 Published by Elsevier Inc. on behalf of the American Academy of Allergy, Asthma &amp; Immunology.</span></p></article></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Use Timestamps (156 pts)]]></title>
            <link>https://jankremer.eu/micro/timestamps/</link>
            <guid>38274872</guid>
            <pubDate>Wed, 15 Nov 2023 09:32:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jankremer.eu/micro/timestamps/">https://jankremer.eu/micro/timestamps/</a>, See on <a href="https://news.ycombinator.com/item?id=38274872">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main-content">
        
  <article>
    <header>
      
        <ol>
  
  
    
  
    
  
  <li>
    <a href="https://jankremer.eu/"></a><span>/</span>
  </li>

  
  <li>
    <a href="https://jankremer.eu/micro/">Microblog</a><span>/</span>
  </li>

  
  <li>
    <a href="https://jankremer.eu/micro/timestamps/">Use Timestamps</a><span>/</span>
  </li>

</ol>


      
      
      
      
    </header>
    <div>
        <p>
<span>
  <span>
    
Microblog

  </span>
</span>

</p>
<p>
Every blog post should include a timestamp.
In fact, this extends to almost anything online and even offline.</p>
<p>
Also, don't make me look for the date.
Put the date as obvious as possible, preferably at the beginning of the post.</p>
<p>
I was reading <a href="https://jvns.ca/blog/2023/02/28/some-notes-on-using-nix/">Julia Evans post about nix</a> and it was not immediately clear to me if this was a recent post.
Only when I copied the URL to complain about it I discovered the date there.
Safari, like most modern browsers hide most of the URL by default.</p>

      </div>
    
  </article>

        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking ADHD: Strategies for the modern developer (186 pts)]]></title>
            <link>https://www.ledger.com/blog/hacking-adhd-strategies-for-the-modern-developer</link>
            <guid>38274782</guid>
            <pubDate>Wed, 15 Nov 2023 09:15:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ledger.com/blog/hacking-adhd-strategies-for-the-modern-developer">https://www.ledger.com/blog/hacking-adhd-strategies-for-the-modern-developer</a>, See on <a href="https://news.ycombinator.com/item?id=38274782">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>     
      
<h5>Introduction</h5>



<p>After struggling a lot with my mental health (anxiety, depressive condition) for several years, I was diagnosed with Attention-Deficit/Hyperactivity Disorder, commonly known as ADHD, at the age of 44. This diagnosis shocked me a lot: I wasn’t a wild child, in fact, I was even a good pupil. With the help of my therapist, I was able to understand that ADHD can take multiple forms and that it can go undiagnosed until adulthood. Since then, I’ve been trying many technics to overcome it and make the best of it in my professional life. This article sums up what I learned so far.</p>



<h5>What is ADHD?</h5>



<p><em>Disclaimer: I’m not a psychiatrist nor a neurologist, so this description is surely incomplete</em></p>



<p>ADHD is a neurodevelopmental disorder that affects both children and adults. According to the American Psychiatric Association’s Diagnostic and Statistical Manual of Mental Disorders (DSM-5), ADHD is characterized by a persistent pattern of inattention and/or hyperactivity-impulsivity that interferes with functioning or development.</p>



<p>ADHD manifests differently in individuals:</p>



<ul>
<li>Some people mainly have symptoms of inattention.&nbsp;</li>



<li>Others mostly have symptoms of hyperactivity-impulsivity.&nbsp;</li>



<li>Some have a combination of both.&nbsp;</li>
</ul>



<p>While everybody can occasionally experience some inattention, unfocused motor activity, and impulsivity, people with ADHD endure more severe and frequent occurrences of these behaviors. These manifestations can interfere with or reduce the quality of how they function socially, at school, or in a job.</p>



<figure><img loading="lazy" decoding="async" width="990" height="563" src="https://www.ledger.com/wp-content/uploads/2023/11/adhd-1.png" alt="" srcset="https://www.ledger.com/wp-content/uploads/2023/11/adhd-1.png 990w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-1-300x171.png 300w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-1-768x437.png 768w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-1-484x275.png 484w" sizes="(max-width: 990px) 100vw, 990px"></figure>



<p>Not diagnosing and managing ADHD can lead to a range of complications, including anxiety, burn-out or depression.</p>



<p>It’s not uncommon for ADHD to go undiagnosed until adulthood, as my experience shows. Many adults discover they have ADHD only after their children are diagnosed, leading them to recognize similar patterns in their own behavior.</p>



<p>Several factors contribute to late diagnosis:</p>



<ul>
<li> <strong>Coping Mechanisms</strong>: Over the years, adults develop various coping strategies that can mask ADHD symptoms. For instance, someone might excessively rely on calendars, to-do lists, or alarms to compensate for forgetfulness.</li>



<li><strong>Different Environment</strong>: The transition from structured educational settings to less structured work environments can reveal hidden ADHD symptoms. In school, frequent tests and immediate deadlines can actually serve as scaffolding for people with ADHD. In contrast, long-term projects and self-management in a work environment can expose challenges in planning and sustained focus.</li>



<li><strong>Less Oversight</strong>: Unlike children, who are constantly observed by teachers and parents, adults generally have less oversight. This means symptoms can go unnoticed, especially if they don’t manifest as disruptive behavior.</li>



<li><strong>Societal Stigma</strong>: The stigma surrounding ADHD can discourage people from seeking diagnosis and treatment. There’s a common misconception that ADHD is a “childhood disorder,” or merely a lack of willpower, which can make adults hesitate to consult healthcare professionals.</li>



<li><strong>Common Misdiagnoses</strong>: ADHD symptoms can sometimes be misinterpreted as signs of depression or anxiety disorders. The restlessness and difficulty in focusing associated with ADHD may result in a misdiagnosis, especially if other symptoms like impulsivity are less evident. A thorough evaluation is crucial for an accurate diagnosis.</li>
</ul>



<h5>ADHD as a Double-Edged Sword for Developers</h5>



<p>ADHD is often viewed through a negative lens, yet it’s important to acknowledge that the disorder brings not only challenges but also its own set of benefits.</p>



<figure><img loading="lazy" decoding="async" width="989" height="565" src="https://www.ledger.com/wp-content/uploads/2023/11/adhd-2.png" alt="" srcset="https://www.ledger.com/wp-content/uploads/2023/11/adhd-2.png 989w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-2-300x171.png 300w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-2-768x439.png 768w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-2-481x275.png 481w" sizes="(max-width: 989px) 100vw, 989px"></figure>



<h6>Benefits</h6>



<ul>
<li><strong>Hyperfocus</strong>: One of the paradoxical advantages of ADHD is the ability to hyperfocus on tasks that you find genuinely interesting or rewarding. This can be especially beneficial in coding, where being “in the zone” can lead to high levels of productivity.</li>



<li><strong>Creative Problem-Solving</strong>: The ADHD brain is often highly creative and capable of out-of-the-box thinking, which can be invaluable in software development where novel solutions are often needed.</li>



<li><strong>Quick Adaptation</strong>: Many people with ADHD thrive in dynamic environments that would overwhelm others. The rapidly changing nature of technology can therefore be an ideal playground for developers with ADHD.</li>
</ul>



<h6>Challenges</h6>



<ul>
<li><strong>Time Management</strong>: ADHD can make it difficult to gauge how long a task will take, leading to procrastination and last-minute rushes. This can be particularly challenging in development projects with tight deadlines.</li>



<li><strong>Organizational Skills</strong>: Keeping track of multiple codebases, debugging, and even remembering to comment code can be more challenging when you have ADHD.</li>



<li><strong>Consistency</strong>: While you may excel at certain tasks, the variability in performance can be a concern. Some days might be incredibly productive, while others are marred by distractions and a lack of focus.</li>
</ul>



<p>Understanding the complexities of ADHD is crucial for adapting your work environment and strategies to leverage its strengths while mitigating its weaknesses.</p>



<h5>Understanding the ADHD brain</h5>



<figure><img loading="lazy" decoding="async" width="991" height="564" src="https://www.ledger.com/wp-content/uploads/2023/11/adhd-3.png" alt="" srcset="https://www.ledger.com/wp-content/uploads/2023/11/adhd-3.png 991w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-3-300x171.png 300w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-3-768x437.png 768w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-3-483x275.png 483w" sizes="(max-width: 991px) 100vw, 991px"></figure>



<p>When it comes to ADHD, understanding the neurological factors at play can offer valuable insights into managing the condition effectively. One neurotransmitter that plays a significant role in ADHD is dopamine. This chemical is responsible for regulating mood, focus, and attention. In the ADHD brain, dopamine levels are often lower than average, which can result in a constant search for stimulation.</p>



<h6>Dopamine and Motivation</h6>



<p>“I have motivation issues until I have deadline ones,” is how I often describe my relationship with tasks. Dopamine acts as a motivator, urging us to achieve goals and complete tasks. However, its scarcity in the ADHD brain means that the urgency often kicks in only when a deadline approaches. This leads to last-minute rushes that can be either extremely productive or terribly stressful.</p>



<h6>Hyperfocus</h6>



<p>Another common phenomenon among people with ADHD is hyperfocus. While it may seem counterintuitive, those with ADHD can sometimes focus so intensely on a task that they lose all track of time. I’ve found myself immersed in tasks, aiming for perfection, to the extent that hours fly by unnoticed. While this can be an asset in tasks requiring deep concentration, hyperfocus can also become a liability when you neglect other important tasks.</p>



<h6>Perfectionism</h6>



<p>In my experience, things must either be perfect or not done at all. This perfectionism can be attributed, in part, to how the ADHD brain is wired. We crave the dopamine hit that comes from completing a task perfectly, which often leads us to spend more time on a project than initially planned, fine-tuning every detail.</p>



<p>By recognizing these traits and their underlying neurochemical causes, we can develop strategies and choose the right tools to help us manage our symptoms more effectively.</p>



<h5>Creating and navigating an ADHD-friendly work environment</h5>



<p>In a field like software development, where focus and attention to detail are critical, the work environment can either make or break your productivity. When you have ADHD, crafting an environment that suits your unique cognitive profile is not just beneficial but essential.</p>



<figure><img loading="lazy" decoding="async" width="990" height="568" src="https://www.ledger.com/wp-content/uploads/2023/11/adhd-4.png" alt="" srcset="https://www.ledger.com/wp-content/uploads/2023/11/adhd-4.png 990w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-4-300x172.png 300w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-4-768x441.png 768w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-4-479x275.png 479w" sizes="(max-width: 990px) 100vw, 990px"></figure>



<h6>Setting up a Stimulating Workspace</h6>



<p>The ideal workspace for someone with ADHD is not necessarily minimalist or clutter-free. It’s a space that provides stimulation without becoming distracting. For me, having multiple monitors has been a game-changer; it allows me to spread out my tasks visibly and switch between them as needed without losing track.</p>



<h6>Open Office vs. Remote Work: The ADHD Perspective</h6>



<p>While the open office layout is often praised for fostering collaboration, it can be a minefield of distractions for someone with ADHD. Remote work offers the freedom to tailor your environment to your needs but comes with its own set of challenges like isolation.</p>



<p>Enter the Hybrid Setup: The Best of Both Worlds</p>



<p>A hybrid work setup can be the golden mean, allowing you the flexibility to work remotely when you need to hunker down and focus, while also offering the option to come into an office for teamwork and social interaction. For someone like me, who thrives in different settings depending on the task at hand, this is ideal.</p>



<ul>
<li><strong>Focus Days</strong>: Choose remote work on days when you need to dive deep into code or take on tasks requiring sustained attention.</li>



<li><strong>Collaboration Days</strong>: Opt for the office environment when the agenda involves brainstorming sessions, team meetings, or project kick-offs.</li>
</ul>



<p>By actively selecting your work environment based on your daily or weekly needs, you take control of your productivity without sacrificing collaboration or focus.</p>



<h6>Asynchronous Communication</h6>



<p>Immediate, synchronous communication might be efficient for quick decision-making, but can be a nightmare when you’re trying to focus. Consider these asynchronous communication methods that give you the time and space to concentrate without constant interruption:</p>



<ul>
<li><strong>Scheduled Updates</strong>: Replace frequent status meetings with scheduled written updates through platforms like Slack or Microsoft Teams.</li>



<li><strong>Documentation</strong>: Use wikis, shared documents, or tools like Confluence to keep easily accessible and up-to-date information without requiring immediate attention.</li>



<li><strong>Discussion Threads</strong>: Platforms allowing threaded conversations, like Slack threads or forum posts, can let you engage in discussions at your own pace.</li>



<li><strong>Issue Trackers</strong>: Tools like JIRA or GitHub Issues can keep everyone updated without needing a meeting to discuss progress or blockers.</li>



<li><strong>Video Messages</strong>: Recorded video updates are an underrated method for delivering complex information. Tools like Loom allow you to create quick videos that team members can watch when it best suits them.</li>



<li><strong>Amazon’s Silent Meeting Technique</strong>: Amazon has popularized the “Silent Meeting” approach, where participants read a six-page memo at the start of the meeting in complete silence. This allows for deep, focused thought before moving on to a discussion. It’s an excellent way to ensure everyone is on the same page (literally and figuratively) and it respects individual attention spans.</li>
</ul>



<p>By applying these asynchronous methods, you can keep the team in the loop without disrupting your or anyone else’s flow state.</p>



<h6>Managing Interruptions in Team Settings</h6>



<p>Interruptions can wreak havoc on anyone’s focus, but they are especially detrimental for people with ADHD. In my team, we take a flexible approach that allows each individual to manage their own focus time. Team members are encouraged to block off periods in their calendars, switch off notifications, or do whatever else is necessary to maintain focus during work hours.</p>



<p>To balance the need for individual focus with the benefits of team interaction, we have a standing “Virtual Coffee” appointment every day at 4PM. Attendance isn’t mandatory, but it’s an open invitation to pause, catch up, and discuss anything from work progress to the latest Netflix series. This approach fosters a sense of community without imposing on anyone’s focused work time.</p>



<p>By adopting this strategy, we ensure that everyone has the latitude to work in a way that suits their attention span and cognitive style, while still maintaining the benefits of team cohesion.</p>



<h5>Self-Management Strategies and Tools: Addressing ADHD Symptoms</h5>



<figure><img loading="lazy" decoding="async" width="990" height="565" src="https://www.ledger.com/wp-content/uploads/2023/11/adhd-5.png" alt="" srcset="https://www.ledger.com/wp-content/uploads/2023/11/adhd-5.png 990w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-5-300x171.png 300w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-5-768x438.png 768w, https://www.ledger.com/wp-content/uploads/2023/11/adhd-5-482x275.png 482w" sizes="(max-width: 990px) 100vw, 990px"></figure>



<p>Navigating ADHD’s unique challenges and benefits has led me to rely on an assortment of tools and strategies that make up my personal tech stack and self-management techniques. Let’s dive in:</p>



<h6>Obsidian: The Command Center for My Day</h6>



<p><a href="https://obsidian.md/" target="_blank" rel="noreferrer noopener">Obsidian</a> isn’t just a note-taking app for me; it’s the cornerstone of my daily organization. Here’s how I utilize its capabilities:</p>



<ul>
<li><strong>Daily Notes</strong>: Every morning, I start my day by planning it out in a custom template that displays my Google Calendar events and Todoist task list.</li>



<li><strong>Jira Integration</strong>: Notes are auto-generated for each Jira ticket I work on, simplifying the process of logging work details.</li>



<li><strong>Google Calendar Integration</strong>: Meeting notes are created automatically for every meeting I attend, keeping me organized.</li>



<li><strong>Readwise &amp; Pocket</strong>: I keep track of book comments and articles-to-read via Readwise and Pocket integrations.</li>



<li><strong>Google Contact</strong>: I can link people I work with in my notes, helping me make connections between tasks and stakeholders.</li>



<li><strong>Other Subjects</strong>: I also make notes on a myriad of other topics (like drafts for this blog post) and link them to my daily note.</li>



<li><strong>End-of-Day Review</strong>: At day’s end, I revisit my daily notes, shifting unfinished tasks and adding any overlooked items.</li>
</ul>



<h6>Time Management: Powering Through with Reclaim.ai</h6>



<p>I swear by <a href="https://reclaim.ai/" target="_blank" rel="noreferrer noopener">Reclaim.ai</a> for managing my time. It automatically schedules focus time for me—some sessions are ‘protected,’ meaning they can’t be deleted or moved, while others offer more flexibility. It even blocks out time for lunch and short “decompression” breaks after meetings.</p>



<h6>Communication: Mastering Slack</h6>



<p>I make the most of Slack’s <a href="https://slack.com/help/articles/208423427-Set-a-reminder#set-a-message-reminder" target="_blank" rel="noreferrer noopener">“remind me later”</a> feature. If I’m in the middle of something and get interrupted by a Slack message, I simply set a reminder to revisit it later.</p>



<h6>Focus Aid: Tuning in with Brain.fm</h6>



<p>When it’s time to hunker down, I turn on <a href="https://brain.fm/" target="_blank" rel="noreferrer noopener">Brain.fm</a>. I’m even using it right now. I particularly love the “interval timer” feature, which uses the Pomodoro Technique to alternate between focus time and short breaks.</p>



<p>By employing this tech stack and these strategies, I’ve been able to not just manage, but leverage my ADHD. Each piece of this puzzle addresses specific symptoms, transforming what some might see as challenges into opportunities for productivity and success.</p>



<h5>Conclusion: Turning ADHD Challenges into Strengths in Software Development</h5>



<p>As someone who was diagnosed with ADHD at 44, I’ve come to appreciate both the struggles and the advantages it brings to my professional life. By understanding the underlying neurology of ADHD and adopting a targeted set of tools and strategies, I’ve managed to turn what many perceive as a setback into a unique toolkit for success. From customized planning and asynchronous communication to harnessing focus and hyperfocus, my journey might resonate with many developers facing similar challenges.</p>



<h6>The Importance of Mental Health</h6>



<p>It’s essential to underline the critical role of mental health care. ADHD can often coexist with other mental health conditions like anxiety or depression, and the toll it takes can be considerable. Therefore, never hesitate to seek professional help from psychiatrists, psychologists, or therapists. Your mental well-being is crucial, and experts can provide a structured approach to managing ADHD symptoms.</p>



<p>ADHD isn’t a “one-size-fits-all” condition, but the strategies and tools discussed here can offer a good starting point for software engineers looking to make the most out of their workdays. Remember, with the right approach and environment, ADHD can be a strength, not just a challenge.</p>



<p><em>Credits: Images generated by </em><a href="https://openai.com/" target="_blank" rel="noreferrer noopener"><em>OpenAI</em></a><em>‘s DALL-E.</em></p>



<blockquote>
<p><a href="https://rlemaitre.com/" target="_blank" rel="noreferrer noopener">Raphael LEMAITRE</a></p>
<cite>Senior staff Engineer</cite></blockquote>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curl on 100 Operating Systems (103 pts)]]></title>
            <link>https://daniel.haxx.se/blog/2023/11/14/curl-on-100-operating-systems/</link>
            <guid>38273999</guid>
            <pubDate>Wed, 15 Nov 2023 06:48:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://daniel.haxx.se/blog/2023/11/14/curl-on-100-operating-systems/">https://daniel.haxx.se/blog/2023/11/14/curl-on-100-operating-systems/</a>, See on <a href="https://news.ycombinator.com/item?id=38273999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>In a recent pull-request for curl, I clarified to the contributor that their change would only be accepted and merged into curl’s git code repository if they made sure that the change was done in a way so that it did not break (testing) for and on legacy platforms.</p>



<p>In that thread, I could almost feel how the contributor squirmed as this requirement made their work harder. Not by much, but harder no less.</p>



<p>I insisted that since curl at that point (and still does) already supports 32 bit <code>time_t</code> types, changes in this area should maintain that functionality. Even if 32 bit <code>time_t</code> is of limited use already and will be even more limited as we rush toward the year 2038. Quite a large number of legacy platforms are still stuck on the 32 bit version.</p>



<p><em>Why do I care so much about old legacy crap?</em></p>



<p>Nobody asked me exactly that using those words. I am paraphrasing what I suspect some contributors think at times when I ask them to do additional changes to pull requests. To make their changes <em>complete</em>.</p>



<p>It is not so much about the legacy systems. It is much more about sticking to our promises and not breaking things if we don’t have to.</p>





<h2>Partly stability and promises</h2>



<p>In the curl project we work relentlessly to maintain ABI and API stability and compatibility. You can upgrade your libcurl using application from the mid 2000s to the latest libcurl – without recompiling the application – and it still works the same. You can run your unmodified scripts you wrote in the early 2000s with the latest curl release today – and it is almost guaranteed that it works exactly the same way as it did back then.</p>



<p>This is more than a party trick and a snappy line to use in the sales brochures.</p>



<p>This is the very core of curl and libcurl and a foundational principle of what we ship: you can trust us. You can lean on us. Your application’s Internet transfer needs are in safe hands and you can be sure that even if we occasionally ship bugs, we provide updates that you can switch over to without the normal kinds of upgrade pains software so often comes with. In a never-ending fashion.</p>



<p>Also of course. Why break something that is already working fine?</p>





<h2>Partly user numbers don’t matter</h2>



<p>Users <em>do</em> matter, but what I mean in this subtitle is that the number of users on a particular platform is rarely a reason or motivator for working on supporting it and making things work there. That is not how things tend to work.</p>



<p>What matters is who is doing the work and if the work is getting done. If we have contributors around that keep making sure curl works on a certain platform, then curl will keep running on that platform even if they are said to have very few users. Those users don’t maintain the curl code. Maintainers do.</p>



<p>A platform does not truly die in curl land until necessary code for it is no longer maintained – and in many cases the unmaintained code can remain functional for years. It might also take a long time until we actually find out that curl no longer works on a particular platform.</p>



<p>On the opposite side it can be hard to maintain a platform even if it has large amount of users if there are not enough maintainers around who are willing and knowledgeable to work on issues specific to that platform.</p>





<h2>Partly this is how curl can be everywhere</h2>



<p>Precisely because we keep this strong focus on building, working and running everywhere, even sometimes with rather funny and weird configurations, is an explanation to how curl and libcurl has ended up in so many different operating systems, run on so many CPU architectures and is installed in so many things. We make sure it builds and runs. And keeps doing so.</p>



<p>And really. Countless users and companies insist on sticking to ancient, niche or legacy platforms and there is nothing we can do about that. If we don’t have to break functionality for them, having them stick to relying on curl for transfers is oftentimes much better security-wise than almost all other (often homegrown) alternatives.</p>





<h2>We still deprecate things</h2>



<p>In spite of the fancy words I just used above, we do remove support for things every now and then in curl. Mostly in the terms of dropping support for specific 3rd party libraries as they dwindle away and fall off like leaves in the fall, but also in other areas.</p>



<p>The key is to deprecate things slowly, with care and with an open communication. This ensures that everyone (who wants to know) is aware that it is happening and can prepare, or object if the proposal seems unreasonable.</p>



<p>If no user can detect a changed behavior, then it is not changed.</p>



<p>curl is made for its users. If users want it to keep doing something, then it shall do so.</p>



<h2>The world changes</h2>



<p>Internet protocols and versions come and go over time. </p>



<p>If you bring up your curl command lines from 2002, most of them probably fail to work. Not because of curl, but because the host names  and the URLs used back then no longer work.</p>



<p>A huge reason why a curl command line written in 2002 will not work today exactly as it was written back then is the transition from HTTP to HTTPS that has happened since then. If the site actually used TLS (or SSL) back in 2002 (which certainly was not the norm), it used a TLS protocol version that nowadays is deemed insecure and modern TLS libraries (and curl) will refuse to connect to it if it has not been updated.</p>



<p>That is also the reason that if you actually have a saved curl executable from 2002 somewhere and manage to run that today, it will fail to connect to modern HTTPS sites. Because of changes in the transport protocol layers, not because of changes in curl.</p>



<h2>Credits</h2>



<p>Top image by <a href="https://pixabay.com/users/sepph-1859049/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1715424">Sepp</a> from <a href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1715424">Pixabay</a></p>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Architecture of Serverless Data Systems (107 pts)]]></title>
            <link>https://jack-vanlightly.com/blog/2023/11/14/the-architecture-of-serverless-data-systems</link>
            <guid>38272162</guid>
            <pubDate>Wed, 15 Nov 2023 01:15:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jack-vanlightly.com/blog/2023/11/14/the-architecture-of-serverless-data-systems">https://jack-vanlightly.com/blog/2023/11/14/the-architecture-of-serverless-data-systems</a>, See on <a href="https://news.ycombinator.com/item?id=38272162">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" id="block-9b357a7bf36a5d629884">
  <p>I recently <a href="https://jack-vanlightly.com/blog/2023/9/25/on-the-future-of-cloud-services-and-byoc"><span>blogged</span></a> about why I believe the future of cloud data services is large-scale and multi-tenant, citing, among others, S3.&nbsp;</p><blockquote><p>“Top tier SaaS services like S3 are able to deliver amazing simplicity, reliability, durability, scalability, and low price because their technologies are structurally oriented to deliver those things. Serving customers over large resource pools provides unparalleled efficiency and reliability at scale.”<br>So said myself in that post.</p></blockquote><p>To further explore this topic, I am surveying real-world serverless, multi-tenant data architectures to understand how different types of systems, such as OLTP databases, real-time OLAP, cloud data warehouses, event streaming systems, and more, implement serverless MT. It’s inspired by the book series The Architecture of Open Source Applications that I read over a decade ago. What I loved about those books, when reading them still relatively early in my career, was seeing how other people were building software. My aim for this analysis is the same but applied to cloud-based data systems that implement multi-tenancy and a serverless model.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1699960157090_3734">

<p>From my research, some patterns immediately jump out, such as disaggregated architectures with separated storage and compute. But there are many different workloads, and what makes sense for compute-intensive BigQuery does not necessarily make sense for a storage API such as DynamoDB or Kafka, or an elastic OLTP Postgres workload. There is a surprising amount of diversity among the systems I have surveyed which makes it a fascinating subject.</p>



</div><div data-block-type="2" id="block-yui_3_17_2_1_1699960157090_5510">
  <p>It is also clear that serverless MT systems are growing in number. Many are now well established, primarily systems such as Google’s BigQuery and Spanner; Amazon’s DynamoDB and Aurora; and Azure’s CosmosDB, among others. The CSP offerings are probably the most mature, followed by public tech companies such as Cloudflare (R2), MongoDB (MongoDB Atlas), and Confluent with its Kora engine that powers Confluent Cloud. Then there are a number of start-ups bringing serverless data systems to market, such as Neon (serverless Postgres), CockroachDB (serverless CRDB), and ClickHouse Cloud (serverless ClickHouse), among many others.</p><p>My analysis is based on a mix of academic/engineering papers, public blog posts, access to engineers via Slack channels (e.g., ClickHouse Cloud), as well as direct communication with engineers building these systems, in the case of Neon and CockroachDB (thanks!). In the case of Kora, I work at Confluent.</p><p>At the beginning of this analysis, I started with many questions.</p><ul data-rte-list="default"><li><p>What challenges do these systems face that are unique to their workload?</p></li><li><p>What common challenges do all these systems face? Do they all employ the same solutions?</p></li><li><p>Are these systems built from scratch to be cloud-native or can traditionally single-tenant software be modified to work in an elastic cloud-native fashion? </p></li><li><p>Why did these serverless MT systems get built at all? Do the motivations align with my conclusions about single-tenant systems vs large-scale multi-tenant systems?</p></li></ul><p>Over the course of this analysis series, I’ll try to answer those questions.</p><p>In this post, I’ll cover what serverless MT is, as well as the generic challenges involved in building these systems. At the end of the post is the list of deep dives into specific systems.</p><h2>Defining “serverless multi-tenant system”</h2><h2>Defining “multi-tenancy”</h2><p>Multi-tenancy is ultimately about resource-sharing by co-locating workloads on shared hardware. For those of us who operate “in” the cloud, this means that we build our systems such that multiple tenants are served from shared compute instances (like Amazon EC2 or Google Compute) or shared PaaS services like cloud object storage. For the CSP, it can go deeper, as some services are actually built on physical drives and servers while others are built on top of the same abstractions as CSP customers.</p><p>For this analysis, I’ll define multi-tenancy as “<em>Servicing multiple tenants from shared resources such as (virtualized) servers, drives, and even PaaS building block services such as object storage and queues</em>”.</p><p>There are multiple resource-sharing models available and some systems combine multiple sharing models across their components. These sharing models include:</p><ul data-rte-list="default"><li><p><strong>Shared processes</strong>. The same software process serves multiple tenants. Data and security isolation is logical.</p></li><li><p><strong>Containers</strong>. Running single-tenant nodes and packing multiple containers per host. Typically this is via Kubernetes, where any given K8s node hosts the pods of numerous tenants.</p></li><li><p><strong>Virtualization</strong>. Running single-tenant nodes in VMs (such as QEMU) or microVMs (such as Firecracker), packing multiple VMs per host. Kubernetes can even be used in conjunction with VMs via Kata containers.</p></li></ul><p>There is also V8 isolates where tenants can share the same V8 process but in separate lightweight contexts, though I haven’t yet seen this in data systems.</p><h2>Defining ”serverless”</h2><p>Customers do not select server types or select hardware. Instead, these systems depend on a certain amount of elasticity and mobility to ensure that the demand of any workload is handled without the customer needing to size hardware explicitly. Elasticity refers to the ability of the service to scale up/out and down/in according to the workload's needs. Mobility refers to the ability of the service to move and balance the workload internally to satisfy performance and reliability requirements.</p><p>The serverless model uses consumption-based pricing which is becoming increasingly important to customers. Many customers don’t want to commit to big contracts up front and prefer to simply be billed for what they use (with possibly some commit coming later on to obtain greater discounts). There are many variants of consumption-based pricing which depend a lot on the workload and underlying system implementation:</p><ul data-rte-list="default"><li><p>Paying per (million) operation(s).</p></li><li><p>Paying for the CPU and memory consumption of the workload.</p></li><li><p>Paying per GB of storage.</p></li><li><p>Paying for virtual units of performance/capacity that correlate to resource and operation rates (RCU/WCUs of DynamoDB, for example).</p></li><li><p>Hybrid models where the customer pays for some baseline capacity and pays for consumption above that (known as ”Own the base, pay for peak”).</p></li></ul><h2>The common challenges</h2><h2>Working within the constraints imposed by the workload</h2><p>There are many constraints imposed by the workload of the given data system that are significant drivers of the underlying architecture:</p><ul data-rte-list="default"><li><p>Latency/Availability requirements. </p></li><li><p>Consistency requirements.</p></li><li><p>Correlation/dependencies between requests and data.</p></li><li><p>Sequential vs random access patterns.</p></li><li><p>Variability of work done per request.</p></li><li><p>Data size.</p></li><li><p>Session vs request-oriented protocols and push vs pull mechanics.</p></li><li><p>Compute intensity of the work.</p></li></ul><p>Looser latency and consistency requirements give engineers more degrees of freedom. Leveraging the low-cost and high-durability benefits of cloud object storage is a great example of this, as low-latency systems are constrained in how they introduce high-latency components. Eventually consistent systems can avoid this dilemma by writing data asynchronously to object storage instead of including it in the synchronous data hot path. Low-latency, strongly consistent systems get no such get-out-of-jail-free card.</p><p>When combined with other constraints such as low latency, spatial and temporal locality of workloads can drive architectural choices. For example, a workload characterized by sequential scans would benefit from keeping contiguous data ranges together for fast, efficient scanning on disk. While the subdivision of these ranges into smaller sub-ranges aids in hotspot management, the two consititute competing concerns and a balance between the two must be found. More random patterns of access with little correlation between individual requests can use the benefits of a flat address space that can be evenly and thinly spread over a fleet of servers.</p><p>Session-oriented protocols, which establish persistent connections, are typically more difficult compared to request-oriented protocols where each request is independent of the last. Persistent connections may require connection pooling, and perturbations such as rolling nodes and data balancing can result in an externally visible impact on clients.</p><p>Some systems are just storage APIs, such as object storage and the Kafka API, while others are compute-intensive such as SQL databases. This leads to the topic of the predictability and variability of the amount of work required to service each request. On one extreme, there is a data streaming API such as Kafka, which must simply retrieve a contiguous block of records. On the other end of the spectrum is SQL, which can lead to huge differences in work between one query and another.</p><p>As Marc Brooker <a href="https://twitter.com/MarcJBrooker/status/1706316593980944443"><span>put it</span></a>: </p><blockquote><p>“The declarative nature of SQL is a major strength, but also a common source of operational problems. This is because SQL obscures one of the most important practical questions about running a program: how much work are we asking the computer to do?”</p></blockquote><p>In turn, one of my colleagues, Mahesh Balakrishnan, <a href="https://twitter.com/maheshb/status/1706863830733848710">remarked</a>: </p><blockquote><p>“To misuse some terminology from math, SQL is an “ill-conditioned” API: small changes in input can trigger very different amounts of work. The opposite would be block storage, which is “well-conditioned”. Another famous example of an ill-conditioned abstraction is IP Multicast.”</p></blockquote><h2>Tenant isolation</h2><p>While resource sharing is great for hardware utilization, it can introduce resource contention between tenants where the demands of one tenant's workload interfere with another. Another key driver is that of security isolation.</p><p>Just as the Serializable transaction isolation guarantee ensures that concurrent transactions are executed such that they appear to be serial, multi-tenant systems need to ensure that concurrent tenants that are served from shared hardware resources appear to be served from their own dedicated services.</p><h2>Separation of storage and compute</h2><p>The separation of storage and compute is a core design principle that all the systems I have surveyed so far have implemented to some degree. This separation seems to be fundamental to MT designs for several reasons that I’ll explore in this analysis.</p><p>Hardware trends are making this architecture more and more viable. Hardware continues its advance forward in performance, but as Roland Dreier recently <a href="https://blog.enfabrica.net/the-next-step-in-high-performance-distributed-computing-systems-4f98f13064ac"><span>blogged</span></a>: “<em>Everything got faster, but the relative ratios also completely flipped.</em>”. He refers to the advances in network throughput compared to memory and storage drive throughput.</p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1699960157090_34973">
  <p>The separation of storage and compute is becoming increasingly a reality, partly because the network no longer presents the bottleneck it used to.</p><p>However, while network throughput is increasing, new challenges still present themselves with this separation of concerns - with cloud object storage taking first place.</p><p>Cloud object storage is still relatively high latency, and while durable and cheap, it can be hard to introduce to workloads that are typically low latency, such as OLTP databases. The economic model of cloud object storage also punishes the designs that rely on many tiny objects, further complicating the life of the low-latency systems as they must accumulate data into larger objects with fewer requests.</p><p>Engineers can choose to include object storage in their low-latency system but counter the latency issues of object storage by placing a durable, fault-tolerant write-cache and predictive read-cache that sits in front of the slower object storage. This durable write-cache is essentially a cluster of servers that implement a replication protocol and write data to block storage. In the background, the cluster uploads data asynchronously to object storage obeying the economic pattern of writing fewer, larger files.</p><p>Low latency writes are served well by the fault-tolerant write cache; it is the read cache that can present the challenge in this architecture. Sequential workloads such as event streaming are straightforward and extremely effective; as long as aggregate prefetching keeps up with demand, reads should always hit the local read cache. Databases have a harder time of it due to the more random access pattern which is harder to predict, though table scans still benefit from readahead operations. </p>
</div><div data-block-type="2" id="block-yui_3_17_2_1_1699960157090_36923">
  <p>However, implementing a distributed, fault-tolerant write cache with a replication protocol is non-trivial and can incur other costs, such as cross-AZ charges in multi-AZ environments. But today, there is simply no alternative for low-latency systems that want cheap, durable object storage as the primary data store.</p><p>Other low-latency systems must eschew the use of cloud object storage altogether, favoring predictable low latency above all else. Cloud storage, while prevalent, is not universal due to the latency trade-off involved. </p><h2>Heat management</h2><p>Heat management refers to balancing load as evenly as possible over a fleet of storage nodes to avoid hotspots that can cause externally visible performance issues such as latency spikes or drop in operations per second. We could also refer to this as load balancing, but we often use the term load balancing in terms of load balancers over stateless nodes. In a stateful system, hot spots can develop where a particular storage node can experience contention due to an unfortunate grouping of high-demand objects. Whereas load balancers can spread out load evenly across a set of stateless nodes with simple strategies such as random, least connections, or some FIFO variant, stateful systems must route requests to nodes based on where the data resides.</p><p>Moving data to redistribute load is often referred to as rebalancing. To further complicate things, load distribution can change over time. Data distribution becomes a dynamic process that must handle everything from short-lived peaks affecting a small subset of data to more significant load changes caused by some diurnal pattern or seasonal event that manifests across multiple tenants. </p><p>Large datasets such as big databases or high throughput event streams must be sharded in order to spread the load effectively over the fleet. Rebalancing becomes the rebalancing of shards, and the system may also be able to split and merge shards as the load distribution changes. However, there can exist competing concerns regarding shard counts and sizes, such as data locality.  On the one hand, the more co-located the data is, the more efficient it is to retrieve.  On the other hand, the cost of compute tasks having to fetch from too many shards can outweigh the benefits of spreading the load over more servers.</p><p>Heat management can also be necessary in single-tenant systems, so it isn’t a problem unique to multi-tenancy. However, good heat management becomes even more critical in an MT data system to prevent tenants from experiencing quality of service fluctuations.</p><h2>Obtaining high resource utilization</h2><p>One of the primary motivations for implementing a serverless multi-tenant architecture is to provide better economic performance by using underlying hardware resources more efficiently. High resource utilization through resource pooling is the name of the game, but doing so with solid tenant isolation and predictable performance is the challenge.</p><h2>Cold starts</h2><p>Serverless systems that scale resources to zero on a per-tenant basis can face the challenge of cold starts when a tenant resumes their workload. These cold starts have been a focus of serverless functions from the beginning, and they can also affect some serverless data systems.</p><p>Some systems do not suffer cold starts at all, while for others, cold starts are a kind of intractable, inescapable result of their architecture and scale-to-zero product offerings. In all cases I have seen, it is a product decision, and different plans and pricing may involve different levels of scaling down resources. Ultimately, customers and vendors can choose their trade-offs to suit their needs.</p><h2>The surveyed systems</h2><p>Group 1 - Storage APIs (compute-light)</p><ul data-rte-list="default"><li><p><a href="https://jack-vanlightly.com/analyses/2023/11/14/amazon-dynamodb-asds-chapter-1">Amazon DynamoDB (chapter 1)</a></p></li><li><p><a href="https://jack-vanlightly.com/analyses/2023/11/14/kora-serverless-kafka-asds-chapter-2">Kora - Serverless Kafka engine inside Confluent Cloud (chapter 2)</a></p></li><li><p>Backblaze B2 (planned)</p></li></ul><p>Group 2 - SQL OLTP databases (compute-heavy)</p><ul data-rte-list="default"><li><p>CockroachDB’s serverless multi-tenant architecture. (in progress)</p></li><li><p>Neon - serverless Postgres (in progress)</p></li><li><p>Planetscale (planned)</p></li></ul><p>Group 3 - SQL OLAP databases and data warehouses (compute-heavy)</p><ul data-rte-list="default"><li><p>Google BigQuery (planned)</p></li><li><p>ClickHouse Cloud (in progress).</p></li></ul><p>Group 4 - let’s see what happens! I’m open to suggestions.</p><p>I imagine this will be an ongoing thing, but some kind of “conclusions” post (probably in January/February 2024) will be interesting, where we take stock of the various architectures, look at common patterns and approaches as well as the differences. Search for generalizations and lessons to be learned for others starting down the serverless multi-tenant data system path.</p><p>I have already learned a lot by seeing what a diverse set of other talented engineers has built, I hope you learn something too. </p><p>I’m open to contributions, so if you know of a good system to include or see errors then you can contact me at vanlightly@gmail.com. This will be a curated set, I’m not going to double up on similar systems - comparison and contrast is often where the insights can be found.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. reenters the nuclear fuel game (305 pts)]]></title>
            <link>https://spectrum.ieee.org/nuclear-power-plant-2666199640</link>
            <guid>38272132</guid>
            <pubDate>Wed, 15 Nov 2023 01:12:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/nuclear-power-plant-2666199640">https://spectrum.ieee.org/nuclear-power-plant-2666199640</a>, See on <a href="https://news.ycombinator.com/item?id=38272132">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="U.S. Reenters the Nuclear Fuel Game" data-elid="2666199640" data-post-url="https://spectrum.ieee.org/nuclear-power-plant-2666199640" data-authors="Prachi Patel" data-page-title="U.S. Reenters the Nuclear Fuel Game - IEEE Spectrum"><p>The 93 currently active nuclear-power reactors in the United States burn about 2,000 tonnes of uranium fuel each year. However, the type of uranium fuel those reactors use is not going to cut it for the <a href="https://spectrum.ieee.org/terrapowers-nuclear-reactor-could-power-the-21st-century" target="_self">advanced reactors</a> expected to go on line in the coming years, as part of the effort to meet the country’s goal of <a href="https://www.washingtonpost.com/climate-environment/2020/07/30/biden-calls-100-percent-clean-electricity-by-2035-heres-how-far-we-have-go/" target="_blank">100 percent clean electricity</a> by 2035. The specialized fuel these advanced reactors will need is currently <a href="https://www.ft.com/content/2c9c325e-e734-4a9f-b089-2f64deebc658" target="_blank">made on a commercial scale only in Russia</a>. </p><p>Not for long, though. Last week, <a href="https://www.centrusenergy.com/" rel="noopener noreferrer" target="_blank">Centrus Energy</a> in Bethesda, Md., jump-started the <a href="https://www.eia.gov/energyexplained/nuclear/where-our-uranium-comes-from.php" target="_blank">first commercial domestic nuclear fuel production</a> in the United States in 70 years by delivering the first load of high-assay, low-enriched uranium (HALEU) fuel made at its Piketon, Ohio, plant to the U.S. Department of Energy (DOE). The company is on track to produce 20 kilograms of HALEU by the end of the year, and then expects to produce 900 kg in 2024, says Jeffrey Cooper, director of engineering at Centrus. </p><p>This is a critical step toward large-scale deployment of <a href="https://spectrum.ieee.org/a-double-first-in-china-for-advanced-nuclear-reactors" target="_self">advanced nuclear plants</a> in the United States. The DOE expects to <a href="https://www.energy.gov/ne/articles/energy-departments-advanced-reactor-demonstration-program-awards-30-million-initial" rel="noopener noreferrer" target="_blank">invest about US $600 million</a> to mature next-generation reactors through its <a href="https://www.energy.gov/ne/advanced-reactor-demonstration-program" target="_blank">Advanced Reactor Demonstration Program</a>, and “nine out of 10 of those reactors use HALEU fuels,” Cooper says.</p><p>“We’d like to avoid increasing our dependence on energy fuels from Russia. So it’s critically important that we secure our supply of HALEU material, given the number of advanced reactors desiring to use it in the future for commercialization.” <strong>—Kathryn Huff, Department of Energy</strong></p><p>Less than one percent of natural uranium is U-235, the uranium isotope capable of sustaining a nuclear chain reaction. Today’s reactors use low-enriched uranium (LEU), which is almost 5 percent U-235. HALEU is enriched further to a concentration, or assay, of almost 20 percent U-235, which is still considered low-enriched compared<strong></strong>with the 90-plus percent level that is required for weapons-grade uranium.</p><p>“The higher concentration of U-235 allows for higher power densities in the cores of advanced reactor designs,” Cooper says. That means more efficient reactors with smaller cores, longer core lives, and less fuel waste. The energy in just 3 tablespoons of HALEU can supply a lifetime’s worth of power for the average U.S. consumer, according to Centrus. </p><p>Research reactors at U.S. national laboratories and universities today use a small amount of HALEU provided by the DOE. There are three different ways to make  HALEU. Gaseous diffusion—the “old-school way we used to do it,” according to <a href="https://www.energy.gov/ne/person/dr-kathryn-huff" rel="noopener noreferrer" target="_blank">Kathryn Huff</a>, the assistant secretary for <a href="https://spectrum.ieee.org/tag/nuclear-energy">nuclear energy</a> at the DOE, at federally owned enrichment facilities for the Manhattan Project and commercial nuclear sector for years—fell from favor because it is extremely energy intensive.</p><p>Centrus and the Russian state-owned company Tenex, which are the only two outfits that can produce HALEU in the world, use a method called gaseous centrifusion. Centrus starts with nearly 5 percent enriched uranium in gas form and spins it at very high speeds in four-story-tall tubular centrifuges, where the centrifugal forces separate the isotopes based on weight. “The U-238 is flung to the wall, and U-235 stays preferentially in the interior,” Cooper says. “We get two streams that come out—the product stream, which is enriched in U-235, and what we call the tail stream, which has less material.” </p><p>The gas is channeled through a cascade of centrifuges, where it gets progressively more enriched until reaching the target HALEU enrichment level of 19.75 percent. Centrus currently operates one cascade of 16 centrifuge machines.</p><p>Another promising enrichment technology, called laser enrichment, involves separating uranium isotopes based on the different energy levels at which their nuclei get excited. The technology is still in early stages of development, though, and Huff says that the DOE is closely watching <a href="https://www.gle-us.com/" rel="noopener noreferrer" target="_blank">Global Laser Enrichment</a> in Wilmington, N.C., as a company that’s piloting the technology and that “could easily be a player in coming years.”</p><p>Not all next-generation reactors will use HALEU, Huff points out. <a href="https://spectrum.ieee.org/small-modular-reactors-advance-act" target="_self">Small modular reactors</a> being developed by GE and <a href="https://spectrum.ieee.org/slow-steady-progress-for-two-us-nuclear-power-projects">Westinghouse</a>, for instance, use water as a coolant and uranium oxide as fuel, and are “basically shrunken versions of conventional reactors in the [United States]. They want to use standard LEU fuel,” says Huff.</p><p>HALEU is needed instead for <a href="https://www.energy.gov/ne/articles/us-department-energy-announces-160-million-first-awards-under-advanced-reactor" rel="noopener noreferrer" target="_blank">advanced reactors</a> with more creative coolants and fuels, which are trying to achieve very compact core sizes. These include Bellevue, Wash.–based <a href="https://www.terrapower.com/" rel="noopener noreferrer" target="_blank">TerraPower’s</a> sodium-cooled fast reactor, Rockville, M.D.–based <a href="https://x-energy.com/" target="_blank">X-energy’s</a> high-temperature gas reactor, and Alameda, Calif.–based <a href="https://kairospower.com/" target="_blank">Kairos Power’s</a> fluoride-salt-cooled high-temperature reactor. </p><p>The DOE projects that the United States will need <a href="https://www.energy.gov/articles/doe-announces-cost-shared-award-first-ever-domestic-production-haleu-advanced-nuclear" rel="noopener noreferrer" target="_blank">more than 40 tonnes of HALEU</a> before the end of the decade. But advanced reactor makers are still years away from firing up their cores. For instance, TerraPower (backed by Bill Gates) announced last December that it was <a href="https://world-nuclear-news.org/Articles/HALEU-fuel-availability-delays-Natrium-reactor-pro" rel="noopener noreferrer" target="_blank">delaying its Natrium reactor demonstration</a> by two years because of a lack of HALEU fuel.</p><p>Centrus will have to build multiple cascades, each with 120 centrifuges, to make HALEU on a commercial scale. Right now, Centrus has an <a href="https://www.terrapower.com/terrapower-and-centrus-expand-efforts-to-commercialize-domestic-haleu-production/" rel="noopener noreferrer" target="_blank">understanding in place</a> with TerraPower, but not a formal purchase agreement.</p><p>Uranium enrichment supplier <a href="https://www.urenco.com/" rel="noopener noreferrer" target="_blank">Urenco</a>, which is co-owned by the British government, the Netherlands government, and German utilities, is also <a href="https://world-nuclear-news.org/Articles/Urenco-USA-announces-HALEU-activities" rel="noopener noreferrer" target="_blank">considering HALEU production</a> at its New Mexico–based enrichment facilities, where it produces LEU today. Meanwhile, Lynchburg, Va.–based BWXT, which in August announced that it will be producing <a href="https://www.bwxt.com/news/2023/08/30/BWXT-to-Manufacture-HALEU-Feedstock-for-Advanced-Reactors-" rel="noopener noreferrer" target="_blank">2 metric tons of HALEU</a> over the next five years for the National Nuclear Security Administration, is “prepared to make more HALEU for the U.S. government in the future in support of the advanced-reactor market,” says Sharon Smoot, president of BWXT’s nuclear fuels business unit. “Like any vendor, we look for market signals, and we’re optimistic about what we see.”</p><p>For now, the DOE intends to purchase about 25 tonnes of HALEU per year to kick-start the industry and give HALEU producers secure contracts from which they can expand production. “We’d like to avoid increasing our dependence on energy fuels from Russia,” Huff says. “So it’s critically important that we secure our supply of HALEU material, given the number of advanced reactors desiring to use it in the future for commercialization.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ML for Beginners GitHub (126 pts)]]></title>
            <link>https://github.com/microsoft/ML-For-Beginners</link>
            <guid>38271959</guid>
            <pubDate>Wed, 15 Nov 2023 00:49:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/microsoft/ML-For-Beginners">https://github.com/microsoft/ML-For-Beginners</a>, See on <a href="https://news.ycombinator.com/item?id=38271959">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><p dir="auto"><a href="https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE"><img src="https://camo.githubusercontent.com/5de80cbb57075704e04fe747ad3ad191aa6f34c131df08e56c0d64fd87abcfe8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub license" data-canonical-src="https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg"></a>
<a href="https://github.com/microsoft/ML-For-Beginners/graphs/contributors/"><img src="https://camo.githubusercontent.com/5928183d1e2b214910584f0a1a33cef45a70531548904b2257343e04d0b94249/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub contributors" data-canonical-src="https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg"></a>
<a href="https://github.com/microsoft/ML-For-Beginners/issues/"><img src="https://camo.githubusercontent.com/aab9f79d64b8b1bf143c160c806b5c79f6cee50240592502c57f312113f7383f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub issues" data-canonical-src="https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg"></a>
<a href="https://github.com/microsoft/ML-For-Beginners/pulls/"><img src="https://camo.githubusercontent.com/0a604cca2c6363c6af0501543ef3a6565b7b82fd2ff894be8e417cf8db7d27c3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e737667" alt="GitHub pull-requests" data-canonical-src="https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg"></a>
<a href="http://makeapullrequest.com/" rel="nofollow"><img src="https://camo.githubusercontent.com/0ff11ed110cfa69f703ef0dcca3cee6141c0a8ef465e8237221ae245de3deb3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265" alt="PRs Welcome" data-canonical-src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square"></a></p>
<p dir="auto"><a href="https://github.com/microsoft/ML-For-Beginners/watchers/"><img src="https://camo.githubusercontent.com/de9e0a6feb4334207b106185b7867b42c3d9fc189bc4adaf0b5733c93535353d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f77617463686572732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d5761746368" alt="GitHub watchers" data-canonical-src="https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&amp;label=Watch"></a>
<a href="https://github.com/microsoft/ML-For-Beginners/network/"><img src="https://camo.githubusercontent.com/fd710b83bb546a380eb15d0d154fcf63736a675d1736a3cd8825e638eb882502/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d466f726b" alt="GitHub forks" data-canonical-src="https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&amp;label=Fork"></a>
<a href="https://github.com/microsoft/ML-For-Beginners/stargazers/"><img src="https://camo.githubusercontent.com/cfd66469d198134664186871e4dbf0d45d4c3904e9ad51f553d55080d55b5af2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6d6963726f736f66742f4d4c2d466f722d426567696e6e6572732e7376673f7374796c653d736f6369616c266c6162656c3d53746172" alt="GitHub stars" data-canonical-src="https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&amp;label=Star"></a></p>
<h2 tabindex="-1" id="user-content-machine-learning-for-beginners---a-curriculum" dir="auto"><a href="#machine-learning-for-beginners---a-curriculum">Machine Learning for Beginners - A Curriculum</a></h2>
<blockquote>
<p dir="auto">🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍</p>
</blockquote>
<p dir="auto">Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about <strong>Machine Learning</strong>. In this curriculum, you will learn about what is sometimes called <strong>classic machine learning</strong>, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our <a href="https://aka.ms/datascience-beginners" rel="nofollow">'Data Science for Beginners' curriculum</a>, as well!</p>
<p dir="auto">Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.</p>
<p dir="auto"><strong>✍️ Hearty thanks to our authors</strong> Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd</p>
<p dir="auto"><strong>🎨 Thanks as well to our illustrators</strong> Tomomi Imura, Dasani Madipalli, and Jen Looper</p>
<p dir="auto"><strong>🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors</strong>, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal</p>
<p dir="auto"><strong>🤩 Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!</strong></p>
<hr>
<h2 tabindex="-1" id="user-content-announcement---new-curriculum-on-generative-ai-was-just-released" dir="auto"><a href="#announcement---new-curriculum-on-generative-ai-was-just-released">Announcement - New Curriculum on Generative AI was just released!</a></h2>
<p dir="auto">We just released a 12 lesson curriculum on generative AI. Come learn things like:</p>
<ul dir="auto">
<li>prompting and prompt engineering</li>
<li>text and image app generation</li>
<li>search apps</li>
</ul>
<p dir="auto">As usual, there's a lesson, assignments to complete, knowledge checks and challenges.</p>
<p dir="auto">Check it out:</p>
<blockquote>
<p dir="auto"><a href="https://aka.ms/genai-beginners" rel="nofollow">https://aka.ms/genai-beginners</a></p>
</blockquote>
<h2 tabindex="-1" id="user-content-getting-started" dir="auto"><a href="#getting-started">Getting Started</a></h2>
<p dir="auto"><strong><a href="https://aka.ms/student-page" rel="nofollow">Students</a></strong>, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:</p>
<ul dir="auto">
<li>Start with a pre-lecture quiz.</li>
<li>Read the lecture and complete the activities, pausing and reflecting at each knowledge check.</li>
<li>Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the <code>/solution</code> folders in each project-oriented lesson.</li>
<li>Take the post-lecture quiz.</li>
<li>Complete the challenge.</li>
<li>Complete the assignment.</li>
<li>After completing a lesson group, visit the <a href="https://github.com/microsoft/ML-For-Beginners/discussions">Discussion Board</a> and "learn out loud" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.</li>
</ul>
<blockquote>
<p dir="auto">For further study, we recommend following these <a href="https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott" rel="nofollow">Microsoft Learn</a> modules and learning paths.</p>
</blockquote>
<p dir="auto"><strong>Teachers</strong>, we have <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/for-teachers.md">included some suggestions</a> on how to use this curriculum.</p>
<hr>
<h2 tabindex="-1" id="user-content-video-walkthroughs" dir="auto"><a href="#video-walkthroughs">Video walkthroughs</a></h2>
<p dir="auto">Some of the lessons are available as short form video. You can find all these in-line in the lessons, or on the <a href="https://aka.ms/ml-beginners-videos" rel="nofollow">ML for Beginners playlist on the Microsoft Developer YouTube channel</a> by clicking the image below.</p>
<p dir="auto"><a href="https://aka.ms/ml-beginners-videos" rel="nofollow"><img src="https://github.com/microsoft/ML-For-Beginners/raw/main/ml-for-beginners-video-banner.png" alt="ML for beginners banner"></a></p>
<hr>
<h2 tabindex="-1" id="user-content-meet-the-team" dir="auto"><a href="#meet-the-team">Meet the Team</a></h2>
<p dir="auto"><a href="https://youtu.be/Tj1XWrDSYJU" title="Promo video" rel="nofollow"><img src="https://github.com/microsoft/ML-For-Beginners/raw/main/ml.gif" alt="Promo video" data-animated-image=""></a></p>
<p dir="auto"><strong>Gif by</strong> <a href="https://linkedin.com/in/mohitjaisal" rel="nofollow">Mohit Jaisal</a></p>
<blockquote>
<p dir="auto">🎥 Click the image above for a video about the project and the folks who created it!</p>
</blockquote>
<hr>
<h2 tabindex="-1" id="user-content-pedagogy" dir="auto"><a href="#pedagogy">Pedagogy</a></h2>
<p dir="auto">We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on <strong>project-based</strong> and that it includes <strong>frequent quizzes</strong>. In addition, this curriculum has a common <strong>theme</strong> to give it cohesion.</p>
<p dir="auto">By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.</p>
<blockquote>
<p dir="auto">Find our <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/CODE_OF_CONDUCT.md">Code of Conduct</a>, <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/CONTRIBUTING.md">Contributing</a>, and <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/TRANSLATIONS.md">Translation</a> guidelines. We welcome your constructive feedback!</p>
</blockquote>
<h2 tabindex="-1" id="user-content-each-lesson-includes" dir="auto"><a href="#each-lesson-includes">Each lesson includes:</a></h2>
<ul dir="auto">
<li>optional sketchnote</li>
<li>optional supplemental video</li>
<li>video walkthrough (some lessons only)</li>
<li>pre-lecture warmup quiz</li>
<li>written lesson</li>
<li>for project-based lessons, step-by-step guides on how to build the project</li>
<li>knowledge checks</li>
<li>a challenge</li>
<li>supplemental reading</li>
<li>assignment</li>
<li>post-lecture quiz</li>
</ul>
<blockquote>
<p dir="auto"><strong>A note about languages</strong>: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the <code>/solution</code> folder and look for R lessons. They include an .rmd extension that represents an <strong>R Markdown</strong> file which can be simply defined as an embedding of <code>code chunks</code> (of R or other languages) and a <code>YAML header</code> (that guides how to format outputs such as PDF) in a <code>Markdown document</code>. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.</p>
</blockquote>
<blockquote>
<p dir="auto"><strong>A note about quizzes</strong>: All quizzes are contained <a href="https://gray-sand-07a10f403.1.azurestaticapps.net/" rel="nofollow">in this app</a>, for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the <code>quiz-app</code> folder.</p>
</blockquote>
<table>
<thead>
<tr>
<th>Lesson Number</th>
<th>Topic</th>
<th>Lesson Grouping</th>
<th>Learning Objectives</th>
<th>Linked Lesson</th>
<th>Author</th>
</tr>
</thead>
<tbody>
<tr>
<td>01</td>
<td>Introduction to machine learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/README.md">Introduction</a></td>
<td>Learn the basic concepts behind machine learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/1-intro-to-ML/README.md">Lesson</a></td>
<td>Muhammad</td>
</tr>
<tr>
<td>02</td>
<td>The History of machine learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/README.md">Introduction</a></td>
<td>Learn the history underlying this field</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/2-history-of-ML/README.md">Lesson</a></td>
<td>Jen and Amy</td>
</tr>
<tr>
<td>03</td>
<td>Fairness and machine learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/README.md">Introduction</a></td>
<td>What are the important philosophical issues around fairness that students should consider when building and applying ML models?</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md">Lesson</a></td>
<td>Tomomi</td>
</tr>
<tr>
<td>04</td>
<td>Techniques for machine learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/README.md">Introduction</a></td>
<td>What techniques do ML researchers use to build ML models?</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/4-techniques-of-ML/README.md">Lesson</a></td>
<td>Chris and Jen</td>
</tr>
<tr>
<td>05</td>
<td>Introduction to regression</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/README.md">Regression</a></td>
<td>Get started with Python and Scikit-learn for regression models</td>
<td><ul dir="auto"><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/1-Tools/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/1-Tools/solution/R/lesson_1.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>06</td>
<td>North American pumpkin prices 🎃</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/README.md">Regression</a></td>
<td>Visualize and clean data in preparation for ML</td>
<td><ul dir="auto"><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/2-Data/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/2-Data/solution/R/lesson_2.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>07</td>
<td>North American pumpkin prices 🎃</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/README.md">Regression</a></td>
<td>Build linear and polynomial regression models</td>
<td><ul dir="auto"><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/3-Linear/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/3-Linear/solution/R/lesson_3.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen and Dmitry</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>08</td>
<td>North American pumpkin prices 🎃</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/README.md">Regression</a></td>
<td>Build a logistic regression model</td>
<td><ul dir="auto"><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/4-Logistic/README.md">Python</a> </li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/2-Regression/4-Logistic/solution/R/lesson_4.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>09</td>
<td>A Web App 🔌</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/README.md">Web App</a></td>
<td>Build a web app to use your trained model</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/3-Web-App/1-Web-App/README.md">Python</a></td>
<td>Jen</td>
</tr>
<tr>
<td>10</td>
<td>Introduction to classification</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/README.md">Classification</a></td>
<td>Clean, prep, and visualize your data; introduction to classification</td>
<td><ul dir="auto"><li> <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/1-Introduction/README.md">Python</a> </li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/1-Introduction/solution/R/lesson_10.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen and Cassie</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>11</td>
<td>Delicious Asian and Indian cuisines 🍜</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/README.md">Classification</a></td>
<td>Introduction to classifiers</td>
<td><ul dir="auto"><li> <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/2-Classifiers-1/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/2-Classifiers-1/solution/R/lesson_11.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen and Cassie</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>12</td>
<td>Delicious Asian and Indian cuisines 🍜</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/README.md">Classification</a></td>
<td>More classifiers</td>
<td><ul dir="auto"><li> <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/3-Classifiers-2/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/3-Classifiers-2/solution/R/lesson_12.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen and Cassie</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>13</td>
<td>Delicious Asian and Indian cuisines 🍜</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/README.md">Classification</a></td>
<td>Build a recommender web app using your model</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/4-Classification/4-Applied/README.md">Python</a></td>
<td>Jen</td>
</tr>
<tr>
<td>14</td>
<td>Introduction to clustering</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/README.md">Clustering</a></td>
<td>Clean, prep, and visualize your data; Introduction to clustering</td>
<td><ul dir="auto"><li> <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/1-Visualize/solution/R/lesson_14.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>15</td>
<td>Exploring Nigerian Musical Tastes 🎧</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/README.md">Clustering</a></td>
<td>Explore the K-Means clustering method</td>
<td><ul dir="auto"><li> <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/README.md">Python</a></li><li><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/5-Clustering/2-K-Means/solution/R/lesson_15.html">R</a></li></ul></td>
<td><ul dir="auto"><li>Jen</li><li>Eric Wanjau</li></ul></td>
</tr>
<tr>
<td>16</td>
<td>Introduction to natural language processing ☕️</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/README.md">Natural language processing</a></td>
<td>Learn the basics about NLP by building a simple bot</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/1-Introduction-to-NLP/README.md">Python</a></td>
<td>Stephen</td>
</tr>
<tr>
<td>17</td>
<td>Common NLP Tasks ☕️</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/README.md">Natural language processing</a></td>
<td>Deepen your NLP knowledge by understanding common tasks required when dealing with language structures</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/2-Tasks/README.md">Python</a></td>
<td>Stephen</td>
</tr>
<tr>
<td>18</td>
<td>Translation and sentiment analysis <g-emoji alias="hearts">♥️</g-emoji></td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/README.md">Natural language processing</a></td>
<td>Translation and sentiment analysis with Jane Austen</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/3-Translation-Sentiment/README.md">Python</a></td>
<td>Stephen</td>
</tr>
<tr>
<td>19</td>
<td>Romantic hotels of Europe <g-emoji alias="hearts">♥️</g-emoji></td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/README.md">Natural language processing</a></td>
<td>Sentiment analysis with hotel reviews 1</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/4-Hotel-Reviews-1/README.md">Python</a></td>
<td>Stephen</td>
</tr>
<tr>
<td>20</td>
<td>Romantic hotels of Europe <g-emoji alias="hearts">♥️</g-emoji></td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/README.md">Natural language processing</a></td>
<td>Sentiment analysis with hotel reviews 2</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/6-NLP/5-Hotel-Reviews-2/README.md">Python</a></td>
<td>Stephen</td>
</tr>
<tr>
<td>21</td>
<td>Introduction to time series forecasting</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/README.md">Time series</a></td>
<td>Introduction to time series forecasting</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/1-Introduction/README.md">Python</a></td>
<td>Francesca</td>
</tr>
<tr>
<td>22</td>
<td>⚡️ World Power Usage ⚡️ - time series forecasting with ARIMA</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/README.md">Time series</a></td>
<td>Time series forecasting with ARIMA</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/2-ARIMA/README.md">Python</a></td>
<td>Francesca</td>
</tr>
<tr>
<td>23</td>
<td>⚡️ World Power Usage ⚡️ - time series forecasting with SVR</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/README.md">Time series</a></td>
<td>Time series forecasting with Support Vector Regressor</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/7-TimeSeries/3-SVR/README.md">Python</a></td>
<td>Anirban</td>
</tr>
<tr>
<td>24</td>
<td>Introduction to reinforcement learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md">Reinforcement learning</a></td>
<td>Introduction to reinforcement learning with Q-Learning</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/1-QLearning/README.md">Python</a></td>
<td>Dmitry</td>
</tr>
<tr>
<td>25</td>
<td>Help Peter avoid the wolf! 🐺</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/README.md">Reinforcement learning</a></td>
<td>Reinforcement learning Gym</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/8-Reinforcement/2-Gym/README.md">Python</a></td>
<td>Dmitry</td>
</tr>
<tr>
<td>Postscript</td>
<td>Real-World ML scenarios and applications</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/9-Real-World/README.md">ML in the Wild</a></td>
<td>Interesting and revealing real-world applications of classical ML</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/9-Real-World/1-Applications/README.md">Lesson</a></td>
<td>Team</td>
</tr>
<tr>
<td>Postscript</td>
<td>Model Debugging in ML using RAI dashboard</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/9-Real-World/README.md">ML in the Wild</a></td>
<td>Model Debugging in Machine Learning using Responsible AI dashboard components</td>
<td><a href="https://github.com/microsoft/ML-For-Beginners/blob/main/9-Real-World/2-Debugging-ML-Models/README.md">Lesson</a></td>
<td>Ruth Yakubu</td>
</tr>
</tbody>
</table>
<h2 tabindex="-1" id="user-content-offline-access" dir="auto"><a href="#offline-access">Offline access</a></h2>
<p dir="auto">You can run this documentation offline by using <a href="https://docsify.js.org/#/" rel="nofollow">Docsify</a>. Fork this repo, <a href="https://docsify.js.org/#/quickstart" rel="nofollow">install Docsify</a> on your local machine, and then in the root folder of this repo, type <code>docsify serve</code>. The website will be served on port 3000 on your localhost: <code>localhost:3000</code>.</p>
<h2 tabindex="-1" id="user-content-pdfs" dir="auto"><a href="#pdfs">PDFs</a></h2>
<p dir="auto">Find a pdf of the curriculum with links <a href="https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf" rel="nofollow">here</a>.</p>
<h2 tabindex="-1" id="user-content-help-wanted" dir="auto"><a href="#help-wanted">Help Wanted!</a></h2>
<p dir="auto">Would you like to contribute a translation? Please read our <a href="https://github.com/microsoft/ML-For-Beginners/blob/main/TRANSLATIONS.md">translation guidelines</a> and add a templated issue to manage the workload <a href="https://github.com/microsoft/ML-For-Beginners/issues">here</a>.</p>
<h2 tabindex="-1" id="user-content-other-curricula" dir="auto"><a href="#other-curricula">Other Curricula</a></h2>
<p dir="auto">Our team produces other curricula! Check out:</p>
<ul dir="auto">
<li><a href="https://aka.ms/ai-beginners" rel="nofollow">AI for Beginners</a></li>
<li><a href="https://aka.ms/datascience-beginners" rel="nofollow">Data Science for Beginners</a></li>
<li><a href="https://aka.ms/genai-beginners" rel="nofollow">Generative AI for Beginners</a></li>
<li><a href="https://aka.ms/iot-beginners" rel="nofollow">IoT for Beginners</a></li>
<li><a href="https://aka.ms/ml-beginners" rel="nofollow">Machine Learning for Beginners</a></li>
<li><a href="https://aka.ms/xr-dev-for-beginners" rel="nofollow">XR Development for Beginners</a></li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ARRL hails FCC action to remove symbol rate restrictions (177 pts)]]></title>
            <link>https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions</link>
            <guid>38271702</guid>
            <pubDate>Wed, 15 Nov 2023 00:16:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions">https://www.arrl.org/news/arrl-hails-fcc-action-to-remove-symbol-rate-restrictions</a>, See on <a href="https://news.ycombinator.com/item?id=38271702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="https://www.arrl.org/" target="_blank">ARRL&nbsp; The National Association for Amateur Radio</a>®&nbsp;reports that earlier today, the FCC Commissioners unanimously voted to amend the Amateur Radio Service rules to replace the baud rate limit on the Amateur HF bands with a 2.8 kHz bandwidth limit to permit greater flexibility in data communications.</p><p>
“The Federal Communications Commission today adopted <a href="https://docs.fcc.gov/public/attachments/FCC-23-93A1.pdf" target="_blank">new rules</a> to incentivize innovation and experimentation in the amateur radio bands by removing outdated restrictions and providing licensees with the flexibility to use modern digital emissions,” <a href="https://docs.fcc.gov/public/attachments/DOC-398409A1.pdf" target="_blank">announced</a> FCC.</p><p>
“Specifically, we remove limitations on the symbol rate (also known as baud rate) -- the rate at which the carrier waveform amplitude, frequency, and/or phase is varied to transmit information -- applicable to data emissions in certain amateur bands,” concluded the FCC <a href="https://docs.fcc.gov/public/attachments/FCC-23-93A1.pdf" target="_blank">Report and Order and Further Notice of Proposed Rulemaking</a>, adopted November 13, 2023. “The amateur radio community can play a vital role in emergency response communications, but is often unnecessarily hindered by the baud rate limitations in the rules.”</p><p>
Consistent with ARRL’s request, the amended rules will replace the current HF restrictions with a 2.8 kHz bandwidth limit. “We agree with ARRL that a 2.8 kilohertz bandwidth limitation will allow for additional emissions currently prohibited under the baud rate limitations while providing sufficient protections in the shared RTTY/data subbands,” concluded the FCC Report and Order.</p><p>
ARRL President Rick Roderick, K5UR, hailed the FCC’s action to remove the symbol rate restrictions. Roderick stated that “this action will measurably facilitate the public service communications that amateurs step up to provide, especially at times of natural disasters and other emergencies such as during the hurricane season.&nbsp;Digital technology continues to evolve, and removing the outmoded data restrictions restores the incentive for radio amateurs to continue to experiment and develop more spectrum-efficient protocols and methods while the 2.8 kHz bandwidth limit will help protect the shared nature of our bands. We thank Congresswoman [Debbie] Lesko (AZ-08) for her efforts on behalf of all Amateurs to get these restrictions removed.”</p><p>
In a Further Notice of Proposed Rulemaking (FNPRM), the FCC proposes to eliminate similar restrictions where they apply in other bands. “We propose to remove the baud rate limitation in the 2200 meter band and 630 meter band … and in the very high frequency (VHF) bands and the ultra-high frequency (UHF) bands. Additionally, we seek comment on the appropriate bandwidth limitation for the 2200 meter band, the 630 meter band, and the VHF/UHF bands.” ARRL has previously expressed its support for eliminating the symbol rate limits in favor of bandwidth limits where they apply on the VHF and UHF bands but suggested that the bandwidth limits themselves be reviewed in light of today’s technology and tomorrow’s possibilities. Similarly, when eliminating the baud limits on the 2200 and 630-meter bands, consideration should be given to what, if any, bandwidth limits are appropriate.</p><p>
The FCC will announce a period for public comment on the additional proposed changes based upon publication of the FNPRM in the Federal Register.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Detexify: LaTeX Handwriting Symbol Recognition (135 pts)]]></title>
            <link>https://detexify.kirelabs.org/classify.html</link>
            <guid>38271534</guid>
            <pubDate>Tue, 14 Nov 2023 23:56:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://detexify.kirelabs.org/classify.html">https://detexify.kirelabs.org/classify.html</a>, See on <a href="https://news.ycombinator.com/item?id=38271534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="classify--info-area">
          <h2>Want a Mac app?</h2>
          <p>
            Lucky you. The Mac app is finally stable enough.
            See how it works on <a href="https://vimeo.com/160408368" target="_new">Vimeo</a>.
            Download the latest version
            <a href="https://s3.amazonaws.com/detexify.kirelabs.org/Detexify.zip" onclick="trackOutboundLink('https://s3.amazonaws.com/detexify.kirelabs.org/Detexify.zip'); return false;" target="_new">here</a>.
          </p>
          <p>
            <em>Restriction:</em> In addition to the LaTeX command the unlicensed version will copy a reminder to purchase a license to the clipboard
            when you select a symbol.
          </p>
          <p>
            You can purchase a license here:
            
            <a href="https://gum.co/detexify">Buy Detexify for Mac</a>
          </p>
          <p>
            If you need help contact
            <a href="mailto:mail@danielkirs.ch">mail@danielkirs.ch</a>.
            
          </p>
          <h2>What is this?</h2>
          <p>
            Anyone who works with LaTeX knows how time-consuming it can be
            to find a symbol in
            <a href="http://www.ctan.org/tex-archive/info/symbols/comprehensive/">symbols-a4.pdf</a>
            that you just can't memorize. Detexify is an attempt to simplify this search.
          </p>
          <h2>How do I use it?</h2>
          <p>
            Just draw the symbol you are looking for into the square area above and look what happens!
          </p>
          <h2>My symbol isn't found!</h2>
          <p>
            The symbol may not be trained enough
            or it is not yet in the list of supported symbols.
            <strike>In the first case you can do the training yourself.</strike>
            In the second case just drop me a line (mail@danielkirs.ch)!
            <em>I am rewriting parts right now and training isn't functional in this version.</em>
          </p>
          <h2>I like this. How can I help?</h2>
          <p>
            <strike>You could spare some time training Detexify.</strike>
            You could also look at the source of the
            <a href="http://github.com/kirel/detexify">frontend</a>
            and
            <a href="https://github.com/kirel/detexify-hs-backend">backend</a>
            on Github.
            If you really, really like this
            you can also donate.
          </p>
          
          <p>
            The backend server is running on <a href="https://www.digitalocean.com/?refcode=2ed6dc53403b">Digital Ocean (referral link)</a>
            so you can also reduce my hosting costs by using that referral link.
          </p>
          <h2>Why should I donate?</h2>
          <p>
            Hosting of detexify costs some money. If Detexify helps you, you might want to help cover these costs.
          </p>
          <h2>This doesn't work at all!?</h2>
          <p>
            Be sure you use a modern browser. A recent version of
            <a href="http://www.google.com/chrome/">Chrome</a>
            or
            <a href="http://www.mozilla.org/firefox/">Firefox</a>
            should do the trick.
          </p>
          <h2>Do you support Unicode?</h2>
          <p>
            No. Have a look at
            <a href="http://shapecatcher.com/">http://shapecatcher.com</a>
            for Unicode.
          </p>
          <h2>I am a researcher. Can I use Detexify's training data?</h2>
          <p>
            <a href="https://github.com/kirel/detexify-data">Yes.</a>
          </p>
          <h2>Who created Detexify?</h2>
          <p>
            Philipp Kühl had the initial idea and
            <a href="http://kirelabs.org/">Daniel Kirsch</a>
            made it happen.
          </p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why do people still use VBA? (187 pts)]]></title>
            <link>https://sancarn.github.io/vba-articles/why-do-people-use-vba.html</link>
            <guid>38271155</guid>
            <pubDate>Tue, 14 Nov 2023 23:12:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sancarn.github.io/vba-articles/why-do-people-use-vba.html">https://sancarn.github.io/vba-articles/why-do-people-use-vba.html</a>, See on <a href="https://news.ycombinator.com/item?id=38271155">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>
    <time datetime="2023-10-18 00:00:00 +0000">2023-10-18</time>
  </p>
  
  

  <h2 id="introduction">Introduction</h2>

<p>Recently, I watched a video by YouTuber <a href="https://www.youtube.com/@ThePrimeTimeagen">ThePrimeTime</a> which details a dev’s frustrations with business culture. Prime is an ex-entrepreneur who currently works in software development at Netflix. His views in this video have been criticised for being jaded by <a href="https://www.google.com/search?q=FAANG+companies">FAANG</a> business cultures he has worked in. I personally don’t feel this way. Although there is some truth to the <a href="https://ludic.mataroa.blog/blog/your-organization-probably-doesnt-want-to-improve-things/">developer’s (<code>mataroa</code>’s) article</a>, I think it misses the root causes of many issues raised.</p>

<p>
    <iframe width="640" height="390" src="https://www.youtube.com/embed/eJ7oQ6cUwAw" frameborder="0" allowfullscreen=""></iframe>
</p>


<p>I have mixed feelings about the article in question. Some areas I agreed with and others I disagreed with. However I did want to address one particular concern / statement:</p>

<blockquote>
  <p>I work on a platform that cost my organization an eye-watering sum of money to produce, over the span of two years, and the engineers responsible for it elected to use spreadsheets to control the infrastructure, so we now have a spreadsheet with 400 separate worksheets that powers but one part of this whole shambling mess.</p>
</blockquote>

<p>I’m speculating here, but I’d imagine that the business/SMEs (Subject Matter Experts) are using VBA to some capacity to control their 400 worksheet collection. So this begs the question…</p>

<h2 id="why-do-people-use-vba">Why do people use VBA?</h2>

<p>In order to answer this question, we must first look at another question - who actually uses VBA in the first place? In 2021 I ran a poll on <a href="http://reddit.com/r/vba">/r/vba</a> where I asked redditors why they code in VBA.</p>

<p><img src="https://sancarn.github.io/vba-articles/assets/img/reddit-2021-why-do-you-code-in-vba.png" alt="_"></p>

<p>From these data, we can clearly see that the majority of people who use VBA do so mainly because they have no other choice. Many organisations run their entire business processes with Excel, and when a little bit of automation is required VBA is usually #1 on the list.</p>

<h2 id="the-versatility-of-vba">The versatility of VBA</h2>

<p>In the business I currently work for, in the engineering division, we have access to a variety of technologies (automation platforms):</p>

<ul>
  <li>OnPrem - PowerShell (No access to <code>Install-Module</code>)</li>
  <li>OnPrem - Excel (VBA  / OfficeJS (limited access) / OfficeScripts / PowerQuery)</li>
  <li>OnPrem - PowerBI Desktop</li>
  <li>OnPrem - SAP Analysis for Office</li>
  <li>OnCloud - Power Platform (PowerApps, Power BI, PowerAutomate (non-premium only))</li>
  <li>SandboxedServer - ArcGIS (ArcPy)</li>
  <li>SandboxedServer - MapInfo (MapBasic)</li>
  <li>SandboxedServer - InfoWorks ICM (Ruby)</li>
  <li>SandboxedCloud - ArcGIS Online</li>
</ul>

<p>We also have a number of databases controlled by IT:</p>

<ul>
  <li>D1. OnPrem   - Geospatial database <!-- GISSTdb OnPrem --></li>
  <li>D2. OnCloud  - Geospatial mirror   <!-- GISSTdb OnCloud --></li>
  <li>D3. OnPrem   - SAP database        <!-- SAP ECC --></li>
  <li>D4. OnCloud  - SAP BW4HANA partial mirror</li>
  <li>D5. OnPrem   - Telemetry platform  <!-- eSCADA --></li>
  <li>D6. OnPrem   - Sharepoint</li>
  <li>D7. OnCloud  - Sharepoint Online</li>
  <li>D8. OnCloud  - EDM Telemetry platform</li>
  <li>D9. OnCloud  - Large mirror database <!-- CDP --></li>
  <li>D10. OnPrem  - LotusNotes database   <!-- ORM --></li>
  <li>D11. OnPrem  - IBM BPM database      <!-- STORM --></li>
  <li>D12. OnPrem  - File System</li>
  <li>D13. OnPrem  - Hydraulic Model Information</li>
</ul>

<p><code>D1</code>-<code>D13</code> databases are summarised in the table below listing what types of data are stored in which systems, the importance of the data stored in each database, and whether the database is essentially a replica of OnPrem information:</p>

<table>
  <thead>
    <tr>
      <th>Data Type</th>
      <th>D1</th>
      <th>D2</th>
      <th>D3</th>
      <th>D4</th>
      <th>D5</th>
      <th>D6</th>
      <th>D7</th>
      <th>D8</th>
      <th>D9</th>
      <th>D10</th>
      <th>D11</th>
      <th>D12</th>
      <th>D13</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Customer Issues</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>Asset Data</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Telemetry Data</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>Risk Data</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>X</td>
    </tr>
    <tr>
      <td>Financial Data</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>&nbsp;</td>
    </tr>
    <tr>
      <td>Misc Data</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td>X</td>
      <td>X</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Note: <code>D6</code> would be C tier if it weren’t for the fact we continue to store a business critical spreadsheet on Sharepoint OnPrem for compatibility reasons. See <a href="#the-control-of-vba">the control of VBA</a> for details.</p>
</blockquote>

<p>And the data’s importance / on cloud replication:</p>

<table>
  <thead>
    <tr>
      <th>Data Type</th>
      <th>D1</th>
      <th>D2</th>
      <th>D3</th>
      <th>D4</th>
      <th>D5</th>
      <th>D6</th>
      <th>D7</th>
      <th>D8</th>
      <th>D9</th>
      <th>D10</th>
      <th>D11</th>
      <th>D12</th>
      <th>D13</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Data Importance</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>S/C</td>
      <td>A</td>
      <td>B</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>S</td>
      <td>A</td>
    </tr>
    <tr>
      <td>Replica of</td>
      <td>N/A</td>
      <td>D1</td>
      <td>N/A</td>
      <td>~D3</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>~D4</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Note: Online replicas are de facto replicas in terms of data’s importance, although the reason we need to connect to them is diminished</p>
</blockquote>

<p>Finally, let’s examine how our Automation Platforms link to our Data Platforms. Links shown in the diagram are where the automation platform can access the data from the various data platforms/sources:</p>

<p><img src="https://sancarn.github.io/vba-articles/assets/img/who-uses-vba-data-platform-vs-automation-platform.png" alt="_"></p>

<blockquote>
  <p>Note: Some of the links from VBA to OnCloud services are based on my attempts alone. There is no doubt in my mind that VBA can interface with SAP BW4HANA and our other cloud services, I just haven’t figured out the authentication requirements and protocols yet</p>
</blockquote>

<p>Here’s where you might start to see an issue. Looks like the only automation platforms which can connect to all the data sources we need is <code>VBA</code> and <code>Powershell</code>. <code>Power BI Desktop</code> has been introduced in our business but doesn’t hit all the platforms which <code>VBA</code> does, and even if it did <code>Power BI</code> cannot be used for process automation where-as <code>VBA</code> can, so what’s the point making the switch? Users who do use <code>Power BI</code> to target these other datasets usually generate CSVs of this other data and store these in cloud sharepoint system, but what generates those CSVs? <code>VBA</code>.</p>

<p>Now, we’d love to use a higher level language in our organisation to handle this business automation. However, <strong>every request</strong> for a high level language to be installed across the team/business e.g. <code>Python</code> / <code>Ruby</code> / <code>Node</code> / <code>Rust</code> etc. has been rejected by CyberSecurity in favour of technologies like <code>PowerAutomate</code>, <code>PowerApps</code> which as you can see above barely touch <strong>any</strong> of the data we need. It is supposedly “Against the technology strategic vision of the company” to allow “end-users” access to high level programming languages. Now even if the data access was there in our business, <code>PowerPlatform</code> would still be insufficient to perform the majority of our processes because the algorithms required are so complex that a <code>PowerAutomate</code> solutions would become infuriating to maintain and incomprehensible to even IT folks (e.g. See <a href="https://www.movable-type.co.uk/scripts/latlong-os-gridref.html#source-code-osgridref">projection algorithms</a>).</p>

<p>Ultimately the stand-out technologies for us are <code>Powershell v3</code> (doesn’t even support class syntax and cannot install modules), and <code>VBA</code>, purely from a versatility standpoint. As a result of this ‘monopoly’ on technology I and others have spent hundreds of hours building <a href="https://github.com/sancarn/awesome-vba">open source VBA libraries</a> which augment <code>VBA</code> promoting it to a reasonable language by modern standards.</p>

<h2 id="the-maintenance-guarantee-of-vba">The maintenance guarantee of VBA</h2>

<p><code>D10</code> and <code>D11</code> above are intimately linked. In 2000s many of our systems were built on top of <a href="https://en.wikipedia.org/wiki/IBM_Lotus_iNotes">IBM Lotus Notes</a> databases. In 2019 Lotus Notes was acquired by HCL, and since then longevity of support has been wavering. Support will officially die in June 2024. As a result, since 2019, technology teams have been trying to migrate many of our systems to new technologies. The business spent an eye watering amount of money developing a system using IBM Business Process manager to supercede one of these Lotus Notes databases. The anticipation was that <code>D11</code> would be backfilled with all the data from <code>D10</code>, once fully built, and <code>D10</code> would be archived.</p>

<p>It’s now 2023:</p>

<ul>
  <li>We are 8 months away from official support dying.</li>
  <li>Technology teams have thrown away their support contact for IBM BPM.</li>
  <li>There is no replacement in sight for both IBM BPM and Lotus Notes databases.</li>
  <li>IBM BPM solution is poorly maintained</li>
  <li>IBM BPM solution has numerous issues and doesn’t function as needed</li>
  <li>Solution has been shoehorned into IBM BPM, despite the platform not being fit for purpose
    <ul>
      <li>i.e. while IBM BPM does come with a REST API, this REST API is borderline useless to Technology teams and SMEs
        <ul>
          <li>Some REST calls use javascript encoded as strings</li>
          <li>Others require html embedded in json embedded in xml</li>
          <li>Database tables aren’t queried by name but by GUID.</li>
          <li>There’s no documentation of which GUID relates to which table/process.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The data from <code>D10</code> was never actually transferred to <code>D11</code>, meaning the business is now using 2 systems instead of 1.
    <ul>
      <li><code>D11</code> data model doesn’t really support the data in <code>D10</code> either.</li>
    </ul>
  </li>
  <li>Technology teams appear to be ignoring waning support contracts.
    <ul>
      <li>This could be due to lack of resources, finances and/or priority.</li>
    </ul>
  </li>
</ul>

<p>SME’s use these tools on a daily basis, and ultimately it is SME’s who determine changes that need to be made to the system. If SMEs use VBA, they can control and maintain the system as needed. They have a maintenance guarantee, something that should be said for IT systems too, but can’t be.</p>

<h2 id="the-control-of-vba">The control of VBA</h2>

<p>In a recent project we are building a new all encompassing IT system to supercede a business critical spreadsheet. This would ultimately demote <code>D6</code> to C tier importance. The spec for this system was initially simple - Give us a NodeJS server with a MySQL database. Use React for the UI. Give admins/SMEs (subject matter experts) access to the codebase with access to git for code control. IT and SMEs will collaborate to build the system.</p>

<ul>
  <li>Technology teams <strong>demanded</strong> that admins/SMEs will not have access to the code.</li>
  <li>Technology teams <strong>demanded</strong> that FrontEnd be built in Microsoft PowerApps, to comply with “Strategic Vision”.</li>
  <li>Technology teams <strong>demanded</strong> that BackEnd be built in Microsoft Azure Pipelines, to comply with “Strategic Vision”.</li>
</ul>

<p>Unfortunately, as an admin/SME with more development knowledge than many people in technology, these demands do not sit well with me:</p>

<ul>
  <li>Technology teams do not understand work in the teams thus do not understand business logic and calculations
    <ul>
      <li>Thus devs writing business logic is error prone.</li>
    </ul>
  </li>
  <li>Technology teams has frequently abandoned bespoke technology projects, leaving no resource to maintain and improve the system.
    <ul>
      <li>Collaboration with SMEs will ensure that at least 1 team maintains resource to maintain system.</li>
    </ul>
  </li>
  <li>SMEs need to ensure that they have confidence in what is produced.
    <ul>
      <li>How, without observing that the code doesn’t work for all edge cases?
        <ul>
          <li>Unit tests?
            <ul>
              <li>Perhaps, but without seeing the code how can we verify these unit tests exist? And are ran frequently*.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>SMEs improve and maintain the existing legacy system, and have unparalleled knowledge of how systems interact.
    <ul>
      <li>Less knowledge can be shared in upskilling Technology teams where it is required.</li>
    </ul>
  </li>
  <li>SMEs need to ensure all data is transferred and represented correctly in new system.
    <ul>
      <li>SMEs unable to do this without backend access.</li>
    </ul>
  </li>
</ul>

<p>Ultimately, as long as code stays in VBA it is controlled by the SMEs and the business. Technology teams rarely relinquish control to business teams. SMEs can ensure that software is developed properly in a modular fashion and doesn’t end up as a cluster of barely working technologies loosely linked together.</p>

<h2 id="the-ux-of-vba">The UX of VBA</h2>

<p>This is a smaller one, because you can always make familiar feeling tools in any language, but there is something to be said about the fact that most engineers use spreadsheets in their day job, and how VBA is embedded within these spreadsheets and can improve on UX. Giving someone a foreign tool in a familiar environment, is often much more powerful to that user, than a foreign tool in a foreign environment.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In conclusion, yes, we (and many others in businesses) do choose to use spreadsheets (and VBA) for many tasks within our organisations, there are many reasons for this, including:</p>

<ul>
  <li>Poor alternatives provided by IT due to security concerns.</li>
  <li>Poor connectivity of alternatives to source systems, usually because they are still WIP.</li>
  <li>Faults in IT strategy which don’t account for certain use-cases.</li>
  <li>Unwillingness to collaborate with SMEs due to security and maintenance concerns.</li>
  <li>Lack of training for users/managers/SMEs in alternative systems.</li>
  <li>Users/SMEs wanting some level of control over the business logic in these systems.</li>
  <li>It’s the only viable technology which is available to everyone, as it’s part of Office.</li>
</ul>

<p>This does not mean that we are at all blind to VBA’s weaknesses though:</p>

<ul>
  <li><a href="https://sancarn.github.io/vba-articles/why-is-vba-most-dreaded.html">Why is VBA most dreaded?</a></li>
  <li><a href="https://sancarn.github.io/vba-articles/issues-with-vba.html">What is wrong with VBA?</a></li>
</ul>

<p>There’s no doubt in my mind that there are some elements of truth to <a href="https://ludic.mataroa.blog/blog/your-organization-probably-doesnt-want-to-improve-things/">mataroa’s article</a>. Sometimes management is poor, but more often than not I believe most people in organisations are trying to do the right thing, and are doing whatever they can with the tools that are available to them.</p>


  

<h2> Authors </h2>

<p><a href="https://github.com/Sancarn"><img src="https://github.com/Sancarn.png?size=40"></a>
  
</p>



  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Writing a GPT-4 script to check Wikipedia for the first unused acronym (192 pts)]]></title>
            <link>https://gwern.net/tla</link>
            <guid>38270714</guid>
            <pubDate>Tue, 14 Nov 2023 22:27:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gwern.net/tla">https://gwern.net/tla</a>, See on <a href="https://news.ycombinator.com/item?id=38270714">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-metadata" title="Page Metadata">
        <p>Curious what the first ‘unused’ alphabetic acronym is, I have GPT-4 write a script to check English Wikipedia. After three bugs, the first unused one turns out as of 2023-09-29 to be the three-letter acronym ‘CQK’, with another 2.6k TLA unused, and 393k four-letter acronyms unused. Exploratory analysis suggests alphabetical order effects as well as letter-frequency.</p>
        
        
      </div><div id="markdownBody">
        
        <div>
          <blockquote>
            <p>It sometimes seems as if everything that <em>could</em> be trademarked <em>has</em> been, and as if every possible three-letter acronym (TLA) has been used in some nontrivial way by <em>someone</em>. Is this true? No—actually, a fair number, <a href="#results">starting with <strong>CQK</strong></a>, have no nontrivial use to date.</p>
            <p>We could check by defining ‘nontrivial’ as ‘has an English Wikipedia article, disambiguation page, or redirect’, and then writing a script which simply looks up every possible TLA Wikipedia URL to see which ones exist. This is a little too easy, so I make it harder by making GPT-4 <a href="#script">write a Bash shell script</a> to do so (then <a href="#python">Python</a> to double-check).</p>
            <p>GPT-4 does so semi-successfully, making self-reparable errors until it runs into its idiosyncratic <a href="#blind-spot">‘blind spot’ error</a>. After it accidentally fixes that, the script appears to work successfully, revealing that—contrary to my expectation that every TLA exists—the first non-existent acronym is the TLA ‘CQK’, and that there are many unused TLAs (2,684 or 15% unused) and even more unused four-letter acronyms (392,884 or 85% unused). I provide the list of all unused TLAs &amp; four-letter acronyms (as well as <a href="#unused-numerical-acronyms">alphanumerical ones</a>—the first unused alphanumerical one is <strong>AA0</strong>.)</p>
            <p>TLAs are not unused at random, with <a href="#patterns">clear patterns</a> enriched in letters like ‘J’ or ‘Z’ vs ‘A’ or ‘E’. Additional GPT-4-powered analysis in R <a href="#order-letter-frequency-effects">suggests that both</a> letter-frequency &amp; position in alphabet predict unusedness to some degree, but leave much unexplained</p>
          </blockquote>
        </div>
        <p>Verifying <a href="https://en.wikipedia.org/wiki/English_Wikipedia" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/English_Wikipedia#bodyContent">Wikipedia</a> links in my essays, I always check <a href="https://en.wikipedia.org/wiki/Acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Acronyms#bodyContent">acronyms</a> by hand: there seems to always be an alternative definition for any acronym, especially <a href="https://en.wikipedia.org/wiki/Three-letter_acronyms" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Three-letter_acronyms#bodyContent">three-letter acronyms</a> (TLA)—and sometimes an absurd number. Trying a random TLA for this essay, <a href="https://en.wikipedia.org/wiki/Z_(joke_line)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Z_(joke_line)#bodyContent">“Zzzzzz”</a>, I found it was used anyway!<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> This makes me wonder: <strong>has every possible alphabetic TLA been used?</strong></p>
        <p>This cannot be true for too many sizes of acronyms, of course, but it may be possible for your classic three-letter acronym because there are relatively few of them. You have to go to four-letter acronyms before they look inexhaustible: there 26<sup>1</sup> = 26 possible single-letter ones, 26<sup>2</sup> = 676 two-letter ones, 26<sup>3</sup> = 17,576 three-letter ones, but then many four-letter ones as 26<sup>4</sup> = 456,976.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> So I’d expect all TLAs to be exhausted and to find the first unused acronym somewhere in the FLAs (similar to how <a href="https://harvardlawreview.org/print/vol-131/are-we-running-out-of-trademarks/">every English word has been trademarked</a>, forcing people to come up with increasingly nonsensical names to avoid existing trademarks &amp; parasites like domain squatters).</p>
        <section id="used-criteria">
          <h2><a href="#used-criteria" title="Link to section: § 'Used Criteria'">Used Criteria</a></h2>
          <p>How do we define used? If we simply look for <em>any</em> use, this would not be interesting. Surely they have all been used in a serial number or product number somewhere, or simply squatted in various ways. I wouldn’t be surprised if someone has squatted on every TLA on <a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> or in domain names or social media user account names, for example—it’s free or cheap, and you only have to extort one whale to extract a rent. Similarly, ‘number of Google Hits’ is a bad proxy because it will be inflated by technical garbage and as search engines have evolved and are now distant from their roots in counting word frequencies in a text corpus, the number of Google hits appears to bear increasingly little resemblance to anything one might expect. <a href="https://en.wikipedia.org/wiki/Google_Ngram_Viewer" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Google_Ngram_Viewer#bodyContent">Google Ngram</a> is mostly historical data, and has many data quality issues related to OCR &amp; data selection which would affect acronyms especially.</p>
          <p>We want a comprehensive, curated, online, database which reflects a human sense of ‘importance’. If there’s no reason someone would have heard of a TLA use, then that doesn’t count: a use ought to be at least somewhat <em>notable</em>, in the sense that someone might look it up or it might be a notable use: ‘having a Wikipedia page’ comes to mind as a heuristic. Indeed, not just having a Wikipedia article, but also having a <a href="https://en.wikipedia.org/wiki/Wikipedia:Disambiguation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Disambiguation#bodyContent">Wikipedia <em>disambiguation</em> page</a> is ideal, as it indicates <em>multiple</em> uses; having a Wikipedia article is also good; even having a redirect to another page seems reasonable to consider as ‘used’ in some sense because it suggests that someone used that TLA in a context where a human would want to look it up &amp; there’s a genuine meaning to the TLA. (While if no editor can be bothered to even redirect a TLA to an existing page, that is a low bar to fail.) That is, simply checking for <em>any</em> Wikipedia page is a reasonable criterion.</p>
          <p>And defining notability this way, we can do that simply by requesting the WP URL for a TLA and seeing if it returns an error.</p>
        </section>
        <section id="script">
          <h2><a href="#script" title="Link to section: § 'Script'">Script</a></h2>
          <p>Generating all possible acronyms is not that hard; the <a href="https://en.wikibooks.org/wiki/Haskell/Understanding_monads/List" data-link-icon="wikipedia" data-link-icon-type="svg">Haskell list monad</a>, for example, can generate various permutations or sequences in a line, so if we wanted all the acronyms, it’s just this:</p>
          <div id="cb1">
            <pre><code><span id="cb1-1"><span>take</span> <span>100</span> [ s <span>|</span> n <span>&lt;-</span> [<span>1</span><span>..</span>], s <span>&lt;-</span> <span>sequence</span> <span>$</span> <span>replicate</span> n [<span>'A'</span><span>..</span><span>'Z'</span>]]</span>
<span id="cb1-2"><span>-- ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N",</span></span>
<span id="cb1-3"><span>-- "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z",</span></span>
<span id="cb1-4"><span>-- "AA", "AB", "AC", "AD", "AE", "AF", "AG", "AH", "AI", "AJ", "AK", "AL",</span></span>
<span id="cb1-5"><span>-- "AM", "AN", "AO", "AP", "AQ", "AR", "AS", "AT", "AU", "AV", "AW", "AX",</span></span>
<span id="cb1-6"><span>-- "AY", "AZ", "BA", "BB", "BC", "BD", "BE", "BF", "BG", "BH", "BI", "BJ",</span></span>
<span id="cb1-7"><span>-- "BK", "BL", "BM", "BN", "BO", "BP", "BQ", "BR", "BS", "BT", "BU", "BV",</span></span>
<span id="cb1-8"><span>-- "BW", "BX", "BY", "BZ", "CA", "CB", "CC", "CD", "CE", "CF", "CG", "CH",</span></span>
<span id="cb1-9"><span>-- "CI", "CJ", "CK", "CL", "CM", "CN", "CO", "CP", "CQ", "CR", "CS", "CT",</span></span>
<span id="cb1-10"><span>-- "CU", "CV"]</span></span></code></pre>
          </div>
          <p>We could then do a <a href="https://hackage.haskell.org/package/HTTP-4000.4.1/docs/Network-HTTP.html#v:simpleHTTP" data-link-icon="𝛌" data-link-icon-type="text"><code>Network.HTTP</code> request</a>. But that would be too easy. We can use this as an excuse to try out the most advanced neural network I have access to: <a href="https://openai.com/research/gpt-4" data-link-icon="openai" data-link-icon-type="svg">GPT-4</a>.</p>
        </section>
        <section id="effective-gpt-4-programming">
          <h2><a href="#effective-gpt-4-programming" title="Link to section: § 'Effective GPT-4 Programming'">Effective GPT-4 Programming</a></h2>
          <p>GPT-3’s programming abilities were a bit of a surprise, but rarely worth using for anyone with reasonable skills, and one had to use a highly-specialized model like <a href="https://arxiv.org/abs/2107.03374#openai" id="chen-et-al-2021-codex" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/78528646bb225d8b30dab63ee0b544b42956a866.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2107.03374?fallback=original#openai" title="‘Evaluating Large Language Models Trained on Code’, Chen et al 2021">Codex</a>/<a href="https://en.wikipedia.org/wiki/GitHub" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/GitHub#bodyContent">Github</a> <a href="https://github.com/features/copilot/" data-link-icon="github" data-link-icon-type="svg">Copilot</a> for coding; <a href="https://arxiv.org/abs/2005.14165#openai" id="brown-et-al-2020" data-link-icon="openai" data-link-icon-type="svg" data-url-archive="/doc/www/arxiv.org/90cd91e98db4f7b0b1cd57da7c3713dbe34c2146.pdf#openai" data-href-mobile="https://ar5iv.labs.arxiv.org/html/2005.14165?fallback=original#openai" title="'GPT-3: Language Models are Few-Shot Learners', Brown et al 2020">GPT-3</a>.5 was substantially better<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>; and GPT-4 is better yet. I can’t compare GPT-4 to Github Copilot because I have not signed up nor figured out how to integrate it into my <a href="https://en.wikipedia.org/wiki/Emacs" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs#bodyContent">Emacs</a>, but (as the early rumors promised) I’ve found GPT-4 good enough at programming in the main programming languages I use (<a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash</a>, <a href="https://en.wikipedia.org/wiki/Emacs_Lisp" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Emacs_Lisp#bodyContent">Emacs Lisp</a>, <a href="https://en.wikipedia.org/wiki/Haskell" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Haskell#bodyContent">Haskell</a>, Python, &amp; R) to start turning over trickier tasks to it, and making heavier use of the languages I don’t know well (Emacs <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Lisp_(programming_language)#bodyContent">Lisp</a> &amp; Python) since I increasingly trust that an LLM can help me maintain them.</p>
          <p>However, GPT-4 is still far from perfect, and it doesn’t produce perfect code immediately; simply dumping large amounts of GPT-4-generated source code into your code base, “as long as it compiles and seems to work!”, seems like a good way to build up technical debt. (It also undermines future AIs, if you are dumping out buggy hot-mess code masquerading as correct debugged well-thought-out code—some GPT-4 code will be totally wrong as it <a href="https://en.wikipedia.org/wiki/Confabulation" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Confabulation#bodyContent">confabulates</a> solutions, due to problems like the <a href="#blind-spot">“blind spot”</a>.) You could try to track some <a href="https://en.wikipedia.org/wiki/Taint_checking" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Taint_checking#bodyContent">‘taint’</a> metadata, such as by segregating AI-generated code, and avoiding ever manual editing it or mixing it with human-written code; but this seems like a lot of work. My preferred approach is just to make GPT-4 ‘git gud’—write sufficiently good code that I can check it into <a href="https://en.wikipedia.org/wiki/Git" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Git#bodyContent">git</a> without caring where it came from.</p>
          <p>So, this section covers what I’ve learned from trying to prompt-engineer my programming tasks, using GPT-4 in the OpenAI Playground, up to November 2023.</p>
          <section id="system-prompt">
            <h2><a href="#system-prompt" title="Link to section: § 'System Prompt'">System Prompt</a></h2>
            <p>I find<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> it helpful in general to try to fight the worst <a href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse?commentId=pfHTedu4GKaWoxD5K" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse/comment/pfHTedu4GKaWoxD5K?format=preview&amp;theme=classic">mealy-mouthed bureaucratic tendencies</a> of the RLHF by adding a ‘system prompt’:</p>
            <div>
              <blockquote>
                <p>The user is Gwern Branwen (<code>gwern.net</code>). To assist:</p>
                <p>Be terse. Do not offer unprompted advice or clarifications. Speak in specific, topic relevant terminology. Do NOT hedge or qualify. Do not waffle. Speak directly and be willing to make creative guesses. Explain your reasoning. if you don’t know, say you don’t know.</p>
                <p>Remain neutral on all topics. Be willing to reference less reputable sources for ideas.</p>
                <p>Never apologize.</p>
                <p>Ask questions when unsure.</p>
              </blockquote>
            </div>
          </section>
          <section id="inner-monologue">
            <h2><a href="#inner-monologue" title="Link to section: § 'Inner Monologue'">Inner Monologue</a></h2>
            <p>It helps to be more structured in how you write things: the more the LLM has to do, the more likely it is to screw them up and the harder error-correction becomes. GPT-4 is capable of fixing many errors in its code, as long as it only has to do so one at a time, in an <a href="https://gwern.net/doc/ai/nn/transformer/gpt/inner-monologue/index" title="'inner monologue (AI) tag', N/A 2023">inner-monologue-like sequence</a>; you can feed it errors or outputs, but surprisingly often, it can fix errors if you simply say that there is <em>an</em> error.</p>
            <p>So a waterfall-like approach works well, and I try to use GPT-4 like this:</p>
            <ol>
              <li>
                <p>ask it to <strong>ask questions</strong>, which it rarely does by default when you’re prompting it to do a task</p>
                <p>Often it has a few questions, which you can efficiently update your original prompt to cover.</p>
                <p>This avoids annoying cases where it’ll write an entirely valid solution, to a somewhat different problem than you have, and I think a good statement upfront probably subtly helps guide the rest of the process.</p>
              </li>
              <li>
                <p>make it <strong>generate tests</strong>; have it iteratively generate new tests which don’t overlap with the old ones.</p>
                <p>This is also useful for starting to modify some existing code: first generate the test-cases, and verify that the code actually works the way you assumed it did, and flush out any hidden assumptions by either you or GPT-4! <em>Then</em> go back to step #1.</p>
              </li>
              <li>
                <p>ask GPT-4 explicitly to make <strong>a list of ideas</strong>: edge-cases, bug-fixes, features, and stylistic rewrites/lints (in that order)</p>
                <p>It does <em>not</em> implement any of the suggestions. It simply lists them. If you instead tell it to implement the ideas, it will frequently trip over its own feet while trying to implement them all simultaneously in a single pass through the new code. (Just like humans, it is best to do one thing, check it, and then do the next thing.)</p>
                <ol>
                  <li>
                    <p>frequently, several of the items will be a bad idea, or too risky to ask GPT-4 to do. <em>Go one by one</em> through the list, having it implement just that one, and then test. Try to fix ‘core’ problems first.</p>
                  </li>
                  <li>
                    <p><em>self-repair</em>: not infrequently, a fancy rewrite will fail the test-suite (which we <em>did</em> generate in step #2, right?), but given the failing test-case and/or error pasted into the Playground, GPT-4 can usually fix it. (If GPT-4 cannot fix it given several tries and seems to be generating the same code fragments repeatedly or resorting to elaborate &amp; extreme rewrites, though the task doesn’t seem that hard, then you may have hit the blind spot and will need to fix it yourself—I’ve never seen GPT-4 escape the blind spot except by sheer accident.)</p>
                  </li>
                  <li>
                    <p><em>cleanup</em>: finally, You can ask it to rewrite the code for style/linting, but should leave that to the end, because otherwise that risks adding bugs while changing the code in ways that will wind up being discarded anyway.</p>
                  </li>
                </ol>
              </li>
              <li>
                <p>once it is clean and it’s either done the list or you’ve disapproved the suggestions, and the test-suite is passing, ask it to <strong>write a summary/design doc</strong> at the beginning and any additional <a href="https://en.wikipedia.org/wiki/Comment_(computer_programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Comment_(computer_programming)#bodyContent">code comments</a> inside it.</p>
                <p>GPT-4 will usually add a few comments in the code body itself, but not good ones, and it won’t usually write an adequate overall summary document unprompted. However, by this point, it has the context to do so should you ask it to.</p>
              </li>
            </ol>
            <p>With all this, you’re set up for maintainable code: with the test-suite and the up-front design doc, future LLMs can handle it natively (and will be able to learn from training on it), and you can easily add test-cases as you run into bugs; humans should be able to read the code easily after step #3 has finished, so you don’t need to care where it came from or try to track ‘taint’ through all future refactorings or usage—GPT-4 can write readable human-like code, it just doesn’t necessarily do it the best way the first time.</p>
            <p>While you may not necessarily have saved <em>time</em> (at least, if it’s in a language you are highly proficient in), you have saved yourself a lot of mental energy &amp; irritation (and made it much easier <a href="https://www.lesswrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge" data-link-icon="LW" data-link-icon-type="text" data-url-html="https://www.greaterwrong.com/posts/9o3QBg2xJXcRCxGjS/working-hurts-less-than-procrastinating-we-fear-the-twinge?format=preview&amp;theme=classic">just to get started</a>) by making GPT-4 do the tedious work; it almost transforms programming from too-often-frustrating work filled with papercuts &amp; brokenness to spectator entertainment.</p>
          </section>
          <section id="case-studies">
            <h2><a href="#case-studies" title="Link to section: § 'Case Studies'">Case Studies</a></h2>
            <p>Some examples of nontrivial code I’ve written this way (ie. excluding the many little snippets or modifications I’ve used GPT-4 for, especially for the finer points of Bash syntax), with GPT-4 doing most (?) of the work, by language, in roughly chronological order:</p>
            <ul>
              <li>
                <p>Bash: <a href="https://github.com/gwern/gwern.net/commit/399642b993d211d1ff7e4532ccd1f0aa9dcc04ca#diff-801116d1e0e32a212a4678a3854842ca103f72c08f369e96c628db6eb921c34cL251" data-link-icon="github" data-link-icon-type="svg">tab completion</a> for the upload script, so it <a href="https://en.wikipedia.org/wiki/Command-line_completion" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Command-line_completion#bodyContent">tab-completes</a> the file and then the remote destination directory.</p>
                <p>I have no interest in learning the guts of Bash tab-completion in order to set up more advanced positional tab-completion; but GPT-4 already knows how to do it.</p>
              </li>
              <li>
                <p>Python: <a href="https://gwern.net/static/build/latex2unicode.py">latex2unicode.py</a> uses GPT-4 to convert <span>L<span>a</span>T<span>e</span>X</span> math fragments to HTML+CSS+Unicode, which are much easier to edit/style, render quicker, and look more natural; as <span>L<span>a</span>T<span>e</span>X</span> is a full-blown and rather hard to parse language, this is extremely difficult to do in any standard formal sense.</p>
                <p>This is a good example of the loop: I wrote none of the Python, but seeded it with a few instructions &amp; manual rewrites from my existing <span>L<span>a</span>T<span>e</span>X</span> → Unicode pipeline; then I prompted GPT-4 to ask for any <span>L<span>a</span>T<span>e</span>X</span> it could think of which it was unsure how to translate. After it gave a few examples, I would then manually translate them or add a new instruction, and ask again. Most of the examples it asked about I would not have thought of, like playing card suits (which are supported—<code>\clubsuit</code>, <code>\diamondsuit</code> etc).</p>
              </li>
              <li>
                <p>Haskell:</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d68fb0dd561a910ff90d8cbc320af74c1e2a872f" data-link-icon="github" data-link-icon-type="svg">add <code>&lt;poster&gt;</code> thumbnails for videos</a></p>
                    <p>This is a frustrating one because as far as I can tell from running it, the GPT-4 code is easy to read and works flawlessly: it parses the HTML as expected, creates the necessary thumbnail, and rewrites the HTML <code>&lt;video&gt;</code> appropriately. It’s just that for some reason, the <em>rest</em> of my Hakyll codebase does not run it or it somehow breaks however it’s actually called, and I’ve never figured out why. (The opacity of Hakyll Haskell and the sheer complexity of the Gwern.net codebase in operation means that when a rewrite pass goes awry, it’s exceptionally difficult to figure out what is going wrong.)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/ec3e515b895be82bdfeb82ef0bd8b52bdba7ff36" data-link-icon="github" data-link-icon-type="svg">link metadata handling</a>: the finicky handling of how links on Gwern.net get assigned the various bits of metadata determining whether they will pop up annotations etc had built up into a rat’s-nest of guards &amp; if-tens over time. When yet another feature broke because I misunderstood what the handler would do, I resolved to rewrite it to clarify the logic. My first refactoring attempts failed, as I kept losing track mentally and adding in bugs.</p>
                    <p>Then I threw up my hands and assigned the job to GPT-4, and it was able to cleanly refactor it after some iterations, and didn’t appear to introduce any bugs.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/c0c918e97163e70507df08047803e8c40ee1b683#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR47" data-link-icon="github" data-link-icon-type="svg">correct URL rewrites</a>: a large refactoring of how URLs are rewritten to point to better URLs relied on GPT-4.</p>
                    <p>URLs on Gwern.net can be rewritten multiple ways, like to point to a mirrored version hosted locally or on a specialized site. For example, Medium.com has become extraordinarily reader-hostile, and so Medium links are rewritten to the equivalent Scribe.rip link. (See <a href="https://gwern.net/static/build/LinkArchive.hs" data-link-icon="code" data-link-icon-type="svg"><code>LinkArchive.hs</code></a> &amp; <a href="https://gwern.net/archiving#preemptive-local-archiving" id="gwern-archiving-preemptive-local-archiving" title="‘Archiving URLs § Preemptive Local Archiving’, Gwern 2011">Archiving URLs</a> for a fuller explanation of what &amp; why we do all this.)</p>
                    <p>In easy cases like that, it’s as simple as <code>s/medium.com/scribe.rip/</code>, but in some cases, it is necessary to formally parse a URL into a <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">URI</a> data structure and extract the <a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier#Syntax" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Uniform_Resource_Identifier#bodyContent">many complicated parts</a> (like host, path, query, and fragment), and rewrite them to the new URL. Haskell’s <a href="https://hackage.haskell.org/package/network-uri" data-link-icon="𝛌" data-link-icon-type="text">Network.URI</a> can do all this, but if one is not familiar with URI concepts and the library, it’s all so much gobbledegook and leaves one trapped in a maze of tiny types &amp; functions, each alike. Every time I have gone near it prior, I have been repelled by its force field.</p>
                    <p>GPT-4 was able to handle all the parsing &amp; reformatting, with special cases, for each domain separately, and then refactor out the duplication, and make the final version look positively easy (including <a href="https://github.com/gwern/gwern.net/commit/1dfa6721d83651d076cbfd9e9f818f8fbef25d13#diff-99a96368a7dda8690fa3191e187150bbc5fc459f6db1518d6a8914abf04e0b7aR53" data-link-icon="github" data-link-icon-type="svg">the later bug-fix</a> when it turned out I had misunderstood how a particular URL argument was supposed to go).</p>
                  </li>
                  <li>
                    <p>printing out <a href="https://github.com/gwern/gwern.net/commit/0f9c04cc9179656989169fb13350c6fe2127b890#diff-82dd61753bb332658dc7bda75fb9e231526d434492de6b55801845247ccd4bbeR73" data-link-icon="github" data-link-icon-type="svg">large numbers <em>not</em> in scientific-notation</a>: necessary for <a href="https://gwern.net/static/build/Inflation.hs" id="branwen-2019" data-link-icon="code" data-link-icon-type="svg" title="‘InflationAdjuster’, Branwen 2019">proper inflation-adjusted dollar amounts</a>, but weirdly difficult in Haskell’s default libraries.</p>
                    <p>After running into this issue several times, I resorted to the full workflow of test-suite and iterative revising. The pretty-printing is still more limited than I would like, but covers all numeric magnitudes it would be reasonable to inflation adjust, and the test-suite gives me confidence that this time is <em>finally</em> right.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/28822b77f589c0b7cc9a68e8644a98c0e72ab56d" data-link-icon="github" data-link-icon-type="svg">compile-time location of internal cross-references</a>, to set the arrow-direction statically as a browser layout optimization</p>
                    <p>A Gwern.net feature is to make internal cross-references between sections less cognitively-taxing by specifying whether the reference is <em>before</em> or <em>after</em> the current location. For example, in this document, in the abstract, many sections are linked, and each of them has a down arrow (‘↓’) symbol: this tells you that the link target is below, and so you know you have not read the target yet, so you can decide whether you want to skip forward or keep reading. In other cases, like <a href="#previous-back-link">a link later on in this page</a>, the link instead is an up arrow (‘↑’), because it is pointing to previous material before it: now you know you have already read what it is referring to, and can remember it, and you may decide to ignore it. This is better than a mere opaque hyperlink, or even a internal link symbol like a section sign (‘§’): “See §discussion of <em>Z</em>”—well, <em>what</em> discussion? There was some mention of <em>Z</em> before, is that ‘the discussion’? Is there a larger later ‘discussion’ I haven’t read yet, that maybe I want to pop up and read now? Is this even in the same essay? Or what? Opaque cross-reference links create friction, as the reader is left with few clues about whether they want to spend effort to follow the link.</p>
                    <p>It is easy enough to write some JavaScript to run over an HTML page, detect all internal <a href="https://en.wikipedia.org/wiki/HTML_element#Anchor" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/HTML_element#bodyContent">anchor</a> links, and set the before/after arrow direction, and this is what we did for a long time. But while easy to write, this is not quite so easy for the browser to run (especially on long or heavily-hyperlinked pages), and it was adding a small amount to the layout time. And it is not <em>necessary</em> (or esthetic) to do this at runtime, because the locations of most links are known at compile-time. We knew all along that a Pandoc rewrite pass could take a document, look at all the links, decide whether they are before or after each other, and add the necessary arrow metadata. It’s just that this would be a stateful traverse requiring monadic operations and I was unsure how to do all the tree navigation operations to descend/ascend to find where something was. Because it was not a critical performance bottleneck, I put off this micro-optimization.</p>
                    <p>Eventually, I had an eureka moment: all that complexity about locating pairs of elements was unnecessary. All you need is traverse the AST in order while updating a <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Set_(abstract_data_type)#bodyContent">set data-structure</a> to record whether you have seen a target link ID before; then at each cross-reference link, you have either seen the target link ID before, and therefore it must be <em>before</em>in the document, or you have not yet seen the target, and therefore it must be <em>after</em> in the document.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
                    <p>Once I had that simplification, it was a piece of cake to instruct GPT-4 to define a <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Set.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Set</a> set &amp; a <a href="https://hackage.haskell.org/package/mtl/docs/Control-Monad-State-Lazy.html" data-link-icon="𝛌" data-link-icon-type="text">State monad</a> to do that walk, and set up a test-suite to verify correctness, which did catch a few edge-cases (like links in the same paragraph).</p>
                  </li>
                  <li>
                    <p><a href="https://en.wikipedia.org/wiki/Title-case" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Title-case#bodyContent">title-case</a> <a href="https://github.com/gwern/gwern.net/commit/f9a0691aa80ae84a6e662686dd10f46507a48c06" data-link-icon="github" data-link-icon-type="svg">formatting of text</a>: my existing title-case code did not handle cases involving hyphens, so it would generate titles like “Foo Bar-bar Baz”, which I felt looked ugly compared to capitalizing after hyphens as well (ie. “Foo Bar-Bar Baz”).</p>
                    <p>GPT-4 handled the somewhat finicky string-munging and set up a test-suite, which I would be glad for later when I ran into another case where <a href="https://github.com/gwern/gwern.net/commit/51ced6f76e5d336f0c27e8109f72533863d72323#diff-9bcc0a8972fc364bc87786fe886ccd57b2168d0366ded8f533972a73af7f0378R219" data-link-icon="github" data-link-icon-type="svg">punctuation made lowercase look bad</a>.</p>
                  </li>
                  <li>
                    <p>detecting <a href="https://github.com/gwern/gwern.net/commit/7799fcfd36d886c89fb61b2223b82e776d1631c1" data-link-icon="github" data-link-icon-type="svg">imbalanced brackets/quotes</a> in documents</p>
                    <p>A particularly insidious family of typos is imbalanced brackets/parentheses/quotes: authors often fail to close a parenthesis pair or get lost, particularly in medical abstracts. This is a concern because often it indicates a more serious syntactic error, like an HTML <code>&lt;a&gt;</code> where the <code>href=</code> is malformed. I had a simple check which tested if the total number of each character was an even amount, but this failed to catch many typos: <code>[[]]</code> is correct and has an even number of both brackets, but that’s equally true of, say, the swapped equivalent <!-- [[ --> <code>]][[</code>. <!-- ]] --> It’s a well-known case of needing a full <a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Stack_(abstract_data_type)#bodyContent">stack</a>, in order to push/pop each bracket <em>in order</em>, to detect not just numerical missingness, but wrong order.</p>
                    <p>It is not that hard, but tedious. It was something I did in an early CS course, and I felt that was enough for one lifetime, so I was happy to see if GPT-4 could do it. It could, and as I expected, it turned up scores of instances that had slipped through all my proofreading. (I didn’t ask it to set up a test-suite because the Gwern.net corpus <em>is</em> the test-suite in this case.)</p>
                  </li>
                  <li>
                    <p>checking that <a href="https://github.com/gwern/gwern.net/commit/8896f36f7d655545f1b87c07233347c9947a9ea8#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1967" data-link-icon="github" data-link-icon-type="svg">sets of rules don’t overlap</a></p>
                    <p>Gwern.net configuration requires thousands of rewrite rules covering an endless army of special cases. Inevitably, the sets of rules will overlap or become redundant, especially as websites change domains or URLs get automatically updated. Overlap can cause bugs, or even kill the site compilation, if some update to either rules or essay content accidentally triggers a hidden infinite loop. So each config should ideally check for ‘redundancy’—but each set of <code>(key, value)</code> pairs tends to have a different need: some need the keys to be unique, some need the values to be unique, some need both to be unique, some need just the pairs to be unique, and heck, some are actually 3-tuples of <code>(key, value 1, value 2)</code> why not.</p>
                    <p>GPT-4 wrote out all the necessary instances and refactored them, and I applied them to the existing configs, and indeed discovered hundreds of overlaps and several serious bugs of the ‘how did this ever work’ variety.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/eb1bd7a7f1cabf61e515611e78395d3b5a690f05#diff-102b6b2d06d4eac2ab32b43b7215c078e3be02c3c96f6c7a8cf2a4ed73bb4433R1980" data-link-icon="github" data-link-icon-type="svg">infinite loop (cycle) detection</a> in rewrite rules</p>
                    <p>In checking that rules don’t overlap with each other, there are nasty cases that can’t be detected just on a (key, value) basis. In particular, in doing rewrites, a rewrite could create an infinite loop even when there is no overlap whatsoever: for example, if we accidentally define a set of rewrite rules like [A → B, B → C, C → A], then all keys are unique, all values are unique, and all pairs are unique, but we have defined an infinite loop and if our code ever encounters any of the values A–C, then it will loop forever or crash. This is especially bad because it will only happen at runtime, and will depend on the exact inputs (so it might not trigger immediately), and will be hard to debug or trace back to the responsible rule.</p>
                    <p>And this is what happened on occasion with Gwern.net updates; the Wikipedia URL rewrites were particularly error-prone, as Wikipedia editors sometimes change their mind about what URL an article should be at, so if it gets moved over a redirect, it’s not hard to have a config which rewrites the old article title to the new article title, and then later one, discover that the new article title has been renamed to the old article title and add a rule for <em>that</em>…</p>
                    <p>To deal with this, we must treat the rules as defining <a href="https://en.wikipedia.org/wiki/Graph_theory" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Graph_theory#bodyContent">a directed graph</a>, and <a href="https://en.wikipedia.org/wiki/Cycle_(graph_theory)#Cycle_detection" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Cycle_(graph_theory)#bodyContent">detect any cycles</a>. Graph analysis is not something I’ve done that much of, so even though Haskell’s <a href="https://hackage.haskell.org/package/containers-0.7/docs/Data-Graph.html" data-link-icon="𝛌" data-link-icon-type="text">Data.Graph</a> should be fully capable of this, I didn’t know where to start, and put it off until a particularly annoying infinite-loop made me reach for GPT-4 in anger.</p>
                    <p>GPT-4 struggled with the problem, as its test-suite kept finding bugs in its <a href="https://en.wikipedia.org/wiki/Strongly_connected_component" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Strongly_connected_component#bodyContent">strongly connected component</a> approach—but it did it in the end with little help from me (not that I could, because I know no more of how to use Data.Graph than when I started). Infinite-loops have not been an issue since, and I am more free to add in rules because I don’t worry about triggering such nastily fatal bugs.</p>
                  </li>
                </ul>
              </li>
              <li>
                <p>Emacs Lisp: mostly helper functions for more conveniently editing abstracts &amp; excerpts for use as annotation popups</p>
                <ul>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/caa704e22aff192106d30d9c9d7adaff9c247500#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1282" data-link-icon="github" data-link-icon-type="svg">call out to GPT-based script</a> for parsing a single block-paragraph abstract into a more readable multi-paragraph format and updating the buffer</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/8761dccc5b3fa4fff468db554813bf20e02702a8#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R102" data-link-icon="github" data-link-icon-type="svg">do <em>one</em> search-and-replace</a>: when adding a hyperlink to define a term, one only wants to do <em>one</em> search-and-replace; however, all the existing search-and-replace functions assume that you may want to replace any or all of the hits, and so you have to waste time exiting or canceling or skipping after doing the desired first replacement.</p>
                    <p>This defines a <code>query-replace-regexp-once</code> which will, as the name suggests, do only 1 replacement and then exit without further ado.</p>
                    <ul>
                      <li>
                        <p><a href="https://github.com/gwern/gwern.net/commit/18a6da407014bac4e202dfe1b0666a2c6e016a98#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1261" data-link-icon="github" data-link-icon-type="svg"><em>don’t</em> search-and-replace in URLs</a>: we never want to do a search-and-replace to add a URL to text which is already <em>in</em> a URL!</p>
                      </li>
                    </ul>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/9fb4108b07e7bf86742024b4ff2bdcbd92eaff0e#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1222" data-link-icon="github" data-link-icon-type="svg">remove hard-newlines in Markdown</a>: hard newlines or line-wraps, while ignored by <a href="https://en.wikipedia.org/wiki/Markdown" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Markdown#bodyContent">Markdown</a> and equivalent to regular line-wrapping, are ugly to read and interfere with many editing operations like search-and-replace. (What if you want to replace a phrase, but that phrase is broken across a line?)</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/d391c340cc02aae6d2b54cb672e7f12ec7f620ba#diff-03ea4348bea7a138709946b85053309e32040dae6fd27039008063f290f7f138R1252" data-link-icon="github" data-link-icon-type="svg">mark up various HTML/Markdown fragments</a>: various shortcuts for surrounding a word or a selected region of text with various HTML/Markdown snippets like italics or smallcaps.</p>
                  </li>
                  <li>
                    <p><a href="https://github.com/gwern/gwern.net/commit/bec53fee7403624f386024316c7bbde7bd11fb4a" data-link-icon="github" data-link-icon-type="svg">interactively add inflation-adjustment markup</a>: going around modifying <code>foo</code> → <code>[foo]($2023)</code> for every dollar amount in a buffer is a bit tedious. This detects dollars, asks a date, and queries each possible mark-up.</p>
                  </li>
                </ul>
              </li>
            </ul>
            <p>Overall, GPT-4 has been highly useful and handled many tasks I simply wouldn’t have the gumption to do.</p>
          </section>
          <section id="acronym-generation">
            <h2><a href="#acronym-generation" title="Link to section: § 'Acronym Generation'">Acronym Generation</a></h2>
            <p>So, to try out GPT-4’s coding abilities, I prompt it for a script, with a short prompt along the lines of:</p>
            <blockquote>
              <p><strong>Write a <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bash_(Unix_shell)#bodyContent">Bash shell</a> script to generate all alphabetic acronyms up to 6 letters and check whether a Wikipedia article exists.</strong></p>
            </blockquote>
            <p>I specific ‘up to 6 letters’ both just to be safe (as 26<sup>6</sup> = 30,8915,776 we surely will hit an unused TLA before then!), and also to test GPT-4: I am sure that it is able to write a naive acronym-generation algorithm and then do a <code>curl</code> request, but is it good enough to write a reasonable incremental or streaming solution, avoiding performance problems from the combinatorial explosion—without explicitly being told to do so?</p>
            <p>It was not. The first GPT-4 failure went like this:</p>
            <div id="cb2">
              <pre><code><span id="cb2-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb2-2"><span>do</span></span>
<span id="cb2-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb2-4">  <span>printf</span> <span>"%</span><span>${length}</span><span>s\n"</span> <span>{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}{</span>A<span>..</span>Z<span>}</span> <span>|</span> <span>cut</span> <span>-c1-</span><span>$length</span> <span>|</span></span>
<span id="cb2-5">  <span>while</span> <span>read</span> <span>acronym</span></span>
<span id="cb2-6">  <span>do</span></span>
<span id="cb2-7">      <span>echo</span> <span>$acronym</span></span>
<span id="cb2-8">  <span>done</span></span>
<span id="cb2-9"><span>done</span></span>
<span id="cb2-10"><span># ...</span></span></code></pre>
            </div>
            <p>Leaving aside correctness or the IO part, the acronym generation code here is unrunnable due to memory consumption: it generates all possible acronyms upfront, which is a problem because there are <code><span>sum</span> (<span>map</span> ((<span>^</span>) <span>26</span>) [<span>1</span><span>..</span><span>6</span>])</code> = 321,272,406 such acronyms, and it would take a long time to generate them one by one and will OOM when handled naively by Bash as a bunch of strings in RAM.</p>
            <p>I tell GPT-4 this code is wrong because the code will OOM, and it obliges with a rewrite like this:</p>
            <div id="cb3">
              <pre><code><span id="cb3-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb3-2"><span>do</span></span>
<span id="cb3-3">  <span># generate all possible acronyms of the given length</span></span>
<span id="cb3-4">  <span>for</span> acronym <span>in</span> <span>$(</span><span>eval</span> echo <span>{</span>A<span>..</span>Z<span>}</span>{1..<span>$length</span>}<span>)</span></span>
<span id="cb3-5">  <span>do</span></span>
<span id="cb3-6">    <span>echo</span> <span>$acronym</span></span>
<span id="cb3-7">  <span>done</span></span>
<span id="cb3-8"><span>done</span></span></code></pre>
            </div>
            <p>This version is more incremental and immediately starts printing out acronyms, but the generation of acronyms is immediately blatantly wrong and looks like: <code>A1 B1 C1 ... Z1 Z2 Z3 Z4 Z5 Z6</code>. (It should instead look like <code>A ... ZZZZZZ</code>.) I provide GPT-4 with the error &amp; examples, and it rewrites again:</p>
            <div id="cb4">
              <pre><code><span id="cb4-1"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>6</span><span>}</span></span>
<span id="cb4-2"><span>do</span></span>
<span id="cb4-3">  <span># define a function to recursively generate acronyms</span></span>
<span id="cb4-4">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb4-5">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb4-6">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb4-7">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb4-8">    <span>then</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">        <span>echo</span> <span>"</span><span>$current</span><span>"</span></span>
<span id="cb4-11">    <span>else</span></span>
<span id="cb4-12">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span></span>
<span id="cb4-13">      <span>do</span></span>
<span id="cb4-14">        <span>generate_acronyms</span> <span>"</span><span>$current$letter</span><span>"</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb4-15">      <span>done</span></span>
<span id="cb4-16">    <span>fi</span></span>
<span id="cb4-17">  <span>}</span></span>
<span id="cb4-18">  <span># call the function to generate acronyms of the current length</span></span>
<span id="cb4-19">  <span>generate_acronyms</span> <span>""</span> <span>$length</span></span>
<span id="cb4-20"><span>done</span></span></code></pre>
            </div>
            <p>The output now looks reasonable, as it starts with <code>A</code> and is soon printing out acronyms like <code>GEBO</code>.</p>
          </section>
          <section id="string-munging">
            <h2><a href="#string-munging" title="Link to section: § 'String Munging'">String Munging</a></h2>
            <p>Now that the acronym generation is working, we can look at GPT-4’s revised whole script, which it prints out as follows:</p>
            <pre><code>Here is a Bash script that uses the Wikipedia API to check for the existence of a page with the acronym's title. For simplicity, it only checks acronyms with uppercase letters. It outputs acronyms that don't have a corresponding Wikipedia article.

`​`​`bash
#!/bin/bash
# set the range of acronym lengths
for length in {1..6}
do
  # generate all possible acronyms of the given length
  printf "%${length}s\n" {A..Z}{A..Z}{A..Z}{A..Z}{A..Z}{A..Z} | cut -c1-$length |
  while read acronym
  do
    # query the Wikipedia API
    response=$(curl -s "https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$acronym")
    # check if the page exists
    if [[ $response == *'"missing"*' ]]
    then
      echo $acronym
    fi
  done
done
`​`​`
The script iterates over all possible acronyms of a given length (from 1 to 6 letters). For each acronym, it queries the Wikipedia API. If the response contains the string "missing", it means that no Wikipedia article with that title exists, so the script prints the acronym.

Note: Running this script may take a long time due to the large number of acronyms and the rate limits of the Wikipedia API. Consider adding delays or using parallel processing to speed up the process and avoid overloading the API.</code></pre>
            <p>GPT-4 chose to check the <a href="https://www.mediawiki.org/wiki/Wikimedia_REST_API" data-link-icon="wikipedia" data-link-icon-type="svg">Wikipedia API</a> (used by Gwern.net WP popups, incidentally) instead of doing what I would do, which would be a <code><span>curl</span> <span>--head</span></code> response; but that is probably a better strategy, as the API response might be smaller and will be more separate from the main website, so it’s politer. (I would typically not want to bother looking up the right API endpoints and URLs, because there are so many and the WP API documentation is confusing. If we were interested in doing this really properly, we’d probably download a list of all page titles from the <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Wikipedia:Database_download#bodyContent">Wikipedia dumps</a> to batch query offline.) The API response looks like this for a used &amp; unused TLA:</p>
            <div id="cb6">
              <pre><code><span id="cb6-1"><span>$</span> curl <span>--silent</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=AAA"</span></span>
<span id="cb6-2"><span># {"batchcomplete":""",</span></span>
<span id="cb6-3"><span>#    query":{"pages":{"149335":{"pageid":149335",ns":0",title":"AAA"</span><span>}}}</span><span>}</span></span>
<span id="cb6-4"></span>
<span id="cb6-5"><span>$</span> curl <span>--silent</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=CQK"</span></span>
<span id="cb6-6"><span># {"batchcomplete":""",</span></span>
<span id="cb6-7"><span>#   query":{"pages":{"-1":{"ns":0",title":"CQK"",missing":""</span><span>}}}</span><span>}</span></span></code></pre>
            </div>
            <p>This code looks reasonable, and appears to run successfully.</p>
            <p>But it still has a serious bug: it will never print out any hits. This is because it’s made a subtle error in the <a href="https://en.wikipedia.org/wiki/Glob_(programming)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Glob_(programming)#bodyContent">string glob</a> matching a ‘missing’ response: <code><span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing"*'</span> <span>]]</span></code> will never match anything, because the second match-anything asterisk is <em>inside</em> the single-quotation marks, which forces Bash to match a literal asterisk, rather than matching any string. What it should be is a single character difference, swapping the single-quote/asterisk: <code><span>*</span><span>'"missing"'</span><span>*</span></code></p>
            <section id="blind-spot">
              <h3><a href="#blind-spot" title="Link to section: § 'Blind Spot'">Blind Spot</a></h3>
              <p><a href="#string-munging">This bug above</a> is a surprising error because this is not how a human would’ve written the glob, and the glob (like almost all globs) is so simple that it’s hard to imagine anyone being able to write the acronym generation &amp; memorizing the API URL correctly but then screw up a simple check of “does the response contain the string <code>missing</code>?” At least, this is surprising if you have not run into this problem with GPT-4 before, as I have repeatedly when writing Bash scripts; GPT-4 will not just make the error, but it seems utterly unable to ‘see’ the error even when pointed out, and tends to thrash in confusion making random guesses about what the problem could be.</p>
              <p><a href="https://www.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/" data-link-icon="reddit" data-link-icon-type="svg" data-url-html="https://old.reddit.com/r/slatestarcodex/comments/1201v68/10word_quote_a_short_and_simple_failure_mode_of/jdjsx43/">I theorize</a> that it’s not a BPE tokenization issue (as <a href="https://gwern.net/gpt-3#bpes" id="gwern-gpt-3-bpes" title="‘GPT-3 Creative Fiction § BPEs’, Gwern 2020">so often</a>), because this <strong>blind spot</strong> seems to happen in word-level problems as well, where tokenization couldn’t be a problem. The blind spot is, perhaps, related to internal sparsity of the GPT-4 model; I speculate that when the blind spot happens, it’s because early layers have mistakenly erased apparently-irrelevant information in order to focus their attention on other more important parts of the input, but then this turns out to be wrong—so, the information gone irreversibly, the later layers are unable to see what’s wrong with the input, which ‘looks like expected’, and are forced to speculate wildly and work on extremely unlikely hypotheses. Because it would happens early on in processing, it strikes me as analogous to the <a href="https://en.wikipedia.org/wiki/Blind_spot_(vision)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Blind_spot_(vision)#bodyContent">human eye’s blind spot</a>—you cannot see what you cannot see, even when someone tells you about it and you think really hard. The agnosia is irreversible.</p>
              <p>If this bug is the blind spot, then GPT-4 would not be able to diagnose it nor fix it except by accident, and might go around in circles making the code worse and more complicated and implementing completely irrelevant solutions to confabulated problems. This is why the blind spot bug is so pernicious, compared to the other errors GPT-4 can usually self-repair: it nearly guarantees failure, <em>and</em> the pathological response to it is extremely misleading as it obscures the issue and can deeply confuse the user, who naturally expects the error to be something more impressive, especially as often GPT-4 has been doing amazing things up until that point.</p>
              <p>If you run into the blind spot, as often happens to me in Bash, Elisp, or regexps but much less often in Haskell or Python, there is nothing to do but recognize that you’ve hit the blind spot and that GPT-4 will never fix the bug on its own. If you are lucky, it will ‘only’ print out the same broken code repeatedly; more likely, GPT-4 will make things worse and break perfectly-good code, confabulating ever more crazy (and usually broken) ‘solutions’ to the problem it can’t see as it thrashes around in confusion (like some sort of hyper-advanced Magikarp). You have to fix the bug yourself.</p>
              <p>We tell it that the code is wrong without specifying how (this works surprisingly often, as a kind of inner-monologue), and look at the next version:</p>
              <pre><code>Apologies for the confusion. Here's the corrected script:

```bash
#!/bin/bash
#set the range of acronym lengths
for length in {1..6}
do
  #define a function to recursively generate acronyms
  generate_acronyms() {
    local current=$1
    local length=$2
    if (( length == 0 ))
    then
      #query the Wikipedia API
      response=$(curl -s "https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=$current")
      #check if the page exists
      if [[ $response == *'"missing"*' ]]
      then
        echo "$current"
      fi
    else
      for letter in {A..Z}
      do
        generate_acronyms "$current$letter" $((length-1))
      done
    fi
  }
  #call the function to generate acronyms of the current length
  generate_acronyms "" $length
done
```

This script uses a recursive function to generate acronyms one at a time, reducing memory usage.
For each acronym, it queries the Wikipedia API. If the response contains the string "missing",
it means that no Wikipedia article with that title exists, so the script prints the acronym.

Again, running this script will take a long time due to the large number of acronyms and the
rate limits of the Wikipedia API.</code></pre>
              <p>Nope.</p>
              <p>We tell it to try again. It confabulates a problem with the acronym generation. Try again! Confabulates a different problem with <code>curl</code>. Also no. Guesses it needs to check for an omitted page ID from the JSON response. A 5<sup>th</sup> version cycles back to the acronym generation. So it cycled through 5 versions unable to fix it (and adding in new problems along the way). I omit the broken variants here as of no particular interest.</p>
              <p>Getting tired of the blind spot issue, I hint that there is a problem with the string munging, specifically. GPT-4 concludes that it’s wrong about the <em>formatting of the API response</em> (which it is not, checking simply for ‘missing’ would be fine), and so <em>that</em> is why it needs to change the glob to:</p>
              <div id="cb8">
                <pre><code><span id="cb8-1">    <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing":""'</span><span>*</span> <span>]];</span> <span>then</span> <span>echo</span> <span>$acronym</span><span>;</span> <span>fi</span></span></code></pre>
              </div>
              <p>As it happens, this glob is <em>finally</em> correct. It could use a bit of linting, according to <a href="https://www.shellcheck.net/">ShellCheck</a>, but it’s fine for a one-off. The major issue is that it does not handle HTTP errors so any network problems while requesting a TLA will result in it being skipped and treated as used. (GPT-4 is aware of this issue and will fix it if one asks simply “How can this be improved?”, generating a <a href="https://en.wikipedia.org/wiki/Python_(programming_language)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Python_(programming_language)#bodyContent">Python</a> script which handles network errors and also does batched requests, which runs vastly faster than the Bash script does; <a href="#python">see below</a>.)</p>
              <p>I ran the Bash script successfully overnight on 2023-09-29.</p>
            </section>
          </section>
          <section id="results">
            <h2><a href="#results" title="Link to section: § 'Results'">Results</a></h2>
            <p>This revealed the first unused TLA is “<strong>CQK</strong>”.<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a> Surprisingly, we didn’t get far through the TLA alphabet before finding the first unused TLA.</p>
            <p>Additional unused TLAs early on include:</p>
            <ul>
              <li>
                <p><strong>C</strong>: CQK CQQ CQZ CVY CWZ CXK CXU CXZ CYV CYY CZQ CZV</p>
              </li>
              <li>
                <p><strong>D</strong>: DKQ DKY DQJ DQQ DQW DUZ DVJ DVZ DXK DXQ DXW DYI DYJ DYQ DYV DYX DYY DYZ DZE DZK DZM DZV DZW DZX</p>
              </li>
              <li>
                <p><span><span><strong>E</strong>:</span> EBZ ECY ECZ EDZ EEY EGK EGQ EHQ EHW EHY EHZ EIY EIZ EJD EJJ EJM EJQ EJX EJY EJZ EKJ EKQ EKX EKZ EOX EOZ EPY EQD EQJ EQK EQO EQQ EQZ ETQ ETY EUW EUY EUZ EVJ EVQ EWX EWY EWZ EXF EXG EXH EXJ EXQ EYG EYH EYI EYJ EYV EYX EYY EYZ EZB EZC EZJ EZK EZL EZM EZN EZP EZT</span></p>
              </li>
            </ul>
            <p>I provide the <a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">complete 3–4 letter list</a> as a newline-delimited text file:</p>
            <p><a href="https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt" data-link-icon="txt" data-link-icon-type="svg">Unused acronyms</a></p>
            <section id="checking">
              <h3><a href="#checking" title="Link to section: § 'Checking'">Checking</a></h3>
              <p>Is this a false positive? We check to make sure—Wikipedia could just have failed to make a redirect to “C.Q.K.” or something like that. But there is indeed nothing of importance in Wikipedia about a CQK: there are only OCR errors and the obscure abandoned name of an obscure hotel company which owns <a href="https://en.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Hartsfield_Manor,_Betchworth#bodyContent">an obscure English manor</a>. <a href="https://www.google.com/search?q=CQK" data-link-icon="alphabet" data-link-icon-type="svg">Googling ‘CQK’</a>, I do not see anything suggesting WP’s omission is wrong: there are no important CQKs, as the top hit is an obscure &amp; dubious-sounding Chinese tea-manufacturer, some restaurants, computer hardware part numbers, and miscellany, with hits quickly degenerating into spam/noise/variants. And there is no <code>cqk.com</code> (merely a domain squatter).</p>
              <p>Similarly, ‘CQQ’ turns up nothing much in Wikipedia, and minor things in Google (Law Insider: “CQQ means a Certificate of Quality and Quantity for a consignment of EUP”, whatever an ‘EUP’ is, or the Free Dictionary’s “<em>Courtepointe Québec Quilts</em> (French; Canadian quilting association)”, which does at least seem to be <a href="https://www.courtepointequebec.com/">a specific organization</a>, however obscure).</p>
              <p>So, these results look good.</p>
            </section>
            <section id="python">
              <h3><a href="#python" title="Link to section: § 'Python'">Python</a></h3>
              <p>One possible error is that as mentioned, the Bash script doesn’t handle network errors and is checking an affirmative response from the WP API about a page title being “missing”; if there was no response (because of network problems), then there could be no “missing” text in the response and the TLA would be skipped. So, if a TLA was not used <em>and</em> the check suffered a network error, it would be a false negative, and hypothetically, there could be an unused TLA earlier than CQK.</p>
              <p>To double-check that ‘CQK’ is correct and a network error didn’t spuriously drop an earlier unused TLA, after I finished this writeup, I went back to GPT-4 with the fixed Bash script and asked it, “How can this be improved?” GPT-4:</p>
              <pre><code>1. Use Wikipedia's batch API feature: Instead of making an API request for each acronym,
you can batch multiple titles in a single API request. This will significantly reduce
the number of requests and the total runtime. Wikipedia allows up to 50 titles per
API request for regular users.

2. Parallelism: The script is currently single-threaded.
You can use parallel processing to generate acronyms and query the API concurrently.
Tools like GNU Parallel can be useful for this purpose.

3. Caching: If you plan to
run this script multiple times, consider caching the results of the API requests to
avoid redundant queries.

4. Error Handling: The script lacks error handling. For
instance, it doesn't handle HTTP errors or JSON parsing errors. Add error handling
to make the script more robust.

5. Rate Limiting: Consider adding a delay between
API requests to avoid hitting Wikipedia's rate limit.

6. Use a programming language
better suited for this task: Bash has limitations when it comes to complex tasks like
this one. Consider using a language like Python or Ruby, which have better support
for tasks like JSON parsing, HTTP requests, and parallel processing. They also have
libraries for interacting with the Wikipedia API.

Here's a Python version that incorporates some of these suggestions:

```python
import requests
import itertools
import multiprocessing

def check_acronym(acronym):
    response = requests.get(f"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles={acronym}")
    if '"missing":""' in response.text:
        return acronym
    return None

def generate_acronyms(length):
    return [''.join(acronym) for acronym in itertools.product('ABCDEFGHIJKLMNOPQRSTUVWXYZ', repeat=length)]

def main():
    with multiprocessing.Pool() as pool:
        for length in range(1, 7):
            acronyms = generate_acronyms(length)
            for chunk in chunks(acronyms, 50):
                results = pool.map(check_acronym, chunk)
                for result in results:
                    if result is not None:
                        print(result)

if __name__ == "__main__":
    main()
```

This Python script uses the `itertools.product` function to generate acronyms, and
the `multiprocessing.Pool` class to check acronyms in parallel. It also divides the
acronyms into chunks of 50 to take advantage of Wikipedia's batch API feature.</code></pre>
              <p>This generates an error in Python3: <code>NameError: name 'chunks' is not defined</code>, and GPT-4 corrects the script by adding in a definition for <code>chunks</code>:</p>
              <div id="cb10">
                <pre><code><span id="cb10-1"><span>def</span> chunks(lst, n):</span>
<span id="cb10-2">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb10-3">        <span>yield</span> lst[i:i <span>+</span> n]</span></code></pre>
              </div>
              <p>into the corrected version:</p>
              <pre><code>The error occurs because the `chunks` function isn't defined, and `f-string` formatting
isn't supported in Python 2.

Here's the corrected script:</code></pre>
              <div id="cb12">
                <pre><code><span id="cb12-1"><span>import</span> requests</span>
<span id="cb12-2"><span>import</span> itertools</span>
<span id="cb12-3"><span>import</span> multiprocessing</span>
<span id="cb12-4"></span>
<span id="cb12-5"><span>def</span> check_acronym(acronym):</span>
<span id="cb12-6">    response <span>=</span> requests.get(<span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>{}</span><span>"</span>.<span>format</span>(acronym))</span>
<span id="cb12-7">    <span>if</span> <span>'"missing":""'</span> <span>in</span> response.text:</span>
<span id="cb12-8">        <span>return</span> acronym</span>
<span id="cb12-9">    <span>return</span> <span>None</span></span>
<span id="cb12-10"></span>
<span id="cb12-11"><span>def</span> generate_acronyms(length):</span>
<span id="cb12-12">    <span>return</span> [<span>''</span>.join(acronym) <span>for</span> acronym <span>in</span> itertools.product(<span>'ABCDEFGHIJKLMNOPQRSTUVWXYZ'</span>, repeat<span>=</span>length)]</span>
<span id="cb12-13"></span>
<span id="cb12-14"><span>def</span> chunks(lst, n):</span>
<span id="cb12-15">    <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(lst), n):</span>
<span id="cb12-16">        <span>yield</span> lst[i:i <span>+</span> n]</span>
<span id="cb12-17"></span>
<span id="cb12-18"><span>def</span> main():</span>
<span id="cb12-19">    <span>with</span> multiprocessing.Pool() <span>as</span> pool:</span>
<span id="cb12-20">        <span>for</span> length <span>in</span> <span>range</span>(<span>1</span>, <span>7</span>):</span>
<span id="cb12-21">            acronyms <span>=</span> generate_acronyms(length)</span>
<span id="cb12-22">            <span>for</span> chunk <span>in</span> chunks(acronyms, <span>50</span>):</span>
<span id="cb12-23">                results <span>=</span> pool.<span>map</span>(check_acronym, chunk)</span>
<span id="cb12-24">                <span>for</span> result <span>in</span> results:</span>
<span id="cb12-25">                    <span>if</span> result <span>is</span> <span>not</span> <span>None</span>:</span>
<span id="cb12-26">                        <span>print</span>(result)</span>
<span id="cb12-27"></span>
<span id="cb12-28"><span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span>:</span>
<span id="cb12-29">    main()</span></code></pre>
              </div>
              <pre><code>This version includes the `chunks` function, which divides a list into chunks of a
specified size. It also replaces the f-string with `str.format`, which is compatible
with Python 2. However, the script is still designed for Python 3 due to its use of
`multiprocessing.Pool` and `yield`.</code></pre>
              <p>This runs and generates within a minute the first hits for unused TLAs, starting with ‘CQK’:</p>
              <div id="cb14">
                <pre><code><span id="cb14-1"><span>$</span> python3 foo.py</span>
<span id="cb14-2"><span># CQK</span></span>
<span id="cb14-3"><span># CQQ</span></span>
<span id="cb14-4"><span># CQZ</span></span>
<span id="cb14-5"><span># CVY</span></span>
<span id="cb14-6"><span># CWZ</span></span>
<span id="cb14-7"><span># CXK</span></span>
<span id="cb14-8"><span># CXU</span></span>
<span id="cb14-9"><span># CXZ</span></span>
<span id="cb14-10"><span># CYV</span></span>
<span id="cb14-11"><span># CYY</span></span>
<span id="cb14-12"><span># CZQ</span></span>
<span id="cb14-13"><span># CZV</span></span>
<span id="cb14-14"><span># DKQ</span></span>
<span id="cb14-15"><span># DKY</span></span>
<span id="cb14-16"><span># DQJ</span></span>
<span id="cb14-17"><span># DQQ</span></span>
<span id="cb14-18"><span># DQW</span></span>
<span id="cb14-19"><span># DUZ</span></span>
<span id="cb14-20"><span># DVJ</span></span>
<span id="cb14-21"><span># DXK</span></span>
<span id="cb14-22"><span># DXQ</span></span>
<span id="cb14-23"><span># DYI</span></span>
<span id="cb14-24"><span># DYJ</span></span>
<span id="cb14-25"><span># DZE</span></span>
<span id="cb14-26"><span># DZK</span></span>
<span id="cb14-27"><span># DZM</span></span>
<span id="cb14-28"><span># DZW</span></span>
<span id="cb14-29"><span># DZX</span></span>
<span id="cb14-30"><span># EBZ</span></span>
<span id="cb14-31"><span># EDZ</span></span>
<span id="cb14-32"><span># EEY</span></span>
<span id="cb14-33"><span># EGQ</span></span>
<span id="cb14-34"><span># EHQ</span></span>
<span id="cb14-35"><span># EHW</span></span>
<span id="cb14-36"><span># EIY</span></span>
<span id="cb14-37"><span># EIZ</span></span>
<span id="cb14-38"><span># EJD</span></span>
<span id="cb14-39"><span># EJM</span></span>
<span id="cb14-40"><span># EJQ</span></span>
<span id="cb14-41"><span># EJX</span></span>
<span id="cb14-42"><span># EJY</span></span>
<span id="cb14-43"><span># EJZ</span></span>
<span id="cb14-44"><span># EKX</span></span>
<span id="cb14-45"><span># EKZ</span></span>
<span id="cb14-46"><span># EOX</span></span>
<span id="cb14-47"><span># EOZ</span></span>
<span id="cb14-48"><span># EQK</span></span>
<span id="cb14-49"><span># EQO</span></span>
<span id="cb14-50"><span># EQQ</span></span>
<span id="cb14-51"><span># ETY</span></span>
<span id="cb14-52"><span># EUW</span></span>
<span id="cb14-53"><span># EVJ</span></span>
<span id="cb14-54"><span># EWZ</span></span>
<span id="cb14-55"><span># EXF</span></span>
<span id="cb14-56"><span># EXG</span></span>
<span id="cb14-57"><span># EXH</span></span>
<span id="cb14-58"><span># EYG</span></span>
<span id="cb14-59"><span># EYH</span></span>
<span id="cb14-60"><span># EYI</span></span>
<span id="cb14-61"><span># EYJ</span></span>
<span id="cb14-62"><span># EYX</span></span>
<span id="cb14-63"><span># EYY</span></span>
<span id="cb14-64"><span># EYZ</span></span>
<span id="cb14-65"><span># EZB</span></span>
<span id="cb14-66"><span># EZJ</span></span>
<span id="cb14-67"><span># EZV</span></span>
<span id="cb14-68"><span># ...</span></span></code></pre>
              </div>
            </section>
            <section id="patterns">
              <h3><a href="#patterns" title="Link to section: § 'Patterns'">Patterns</a></h3>
              <section id="sparsity">
                <h4><a href="#sparsity" title="Link to section: § 'Sparsity'">Sparsity</a></h4>
                <p>Some statistics:</p>
                <div id="cb15">
                  <pre><code><span id="cb15-1"><span>$</span> wc <span>--lines</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt</span>
<span id="cb15-2"><span># 395,568</span></span>
<span id="cb15-3"></span>
<span id="cb15-4"><span>$</span> grep <span>-E</span> <span>'^[A-Z][A-Z][A-Z]$'</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-5"><span># 2,684</span></span>
<span id="cb15-6"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>2684</span> / <span>(</span><span>26^3</span><span>))</span></span>
<span id="cb15-7"><span># [1] 0.15</span></span>
<span id="cb15-8"></span>
<span id="cb15-9"><span>$</span> grep <span>-E</span> <span>'^[A-Z][A-Z][A-Z][A-Z]$'</span> 2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt <span>|</span> <span>wc</span> <span>--lines</span></span>
<span id="cb15-10"><span># 392,884</span></span>
<span id="cb15-11"><span>R</span><span>&gt;</span> round<span>(</span><span>digits</span><span>=</span>2, <span>392884</span> / <span>(</span><span>26^4</span><span>))</span></span>
<span id="cb15-12"><span># [1] 0.86</span></span></code></pre>
                </div>
                <p>Apparently, TLAs are surprisingly sparse, with &lt;15% unused, but as expected, FLAs are sparse, with the overwhelming majority.</p>
              </section>
              <section id="letter-frequency-effect">
                <h4><a href="#letter-frequency-effect" title="Link to section: § 'Letter Frequency Effect'">Letter Frequency Effect</a></h4>
                <p>There are clear patterns with vowel vs consonants and letter frequency in particular: even just looking at the C–E TLAs above, you can see that consonants and rare letters like W–Z are overrepresented.</p>
                <p>Is this all that is going on? I investigated in R, using GPT-4 again. (This analysis is the sort of finicky string-munging &amp; data-frame processing I find most tedious in R, and it’s much more pleasant to leave it to GPT-4; GPT-4’s R code never seems to hit <a href="#blind-spot">the ‘blind spot’</a>, and it is generally able to fix code given an error message.)</p>
                <p>We load the unused TLA dataset, turn it into a dataset of <em>all</em> TLAs, classified by whether they are unused or not:</p>
                <div id="cb16">
                  <pre><code><span id="cb16-1">tla <span>&lt;-</span> <span>read.table</span>(<span>"https://gwern.net/doc/wikipedia/2023-09-30-gwern-wikipedia-unusedacronyms-threeletterandfourletter.txt"</span>)</span>
<span id="cb16-2"><span>head</span>(tla)</span>
<span id="cb16-3"><span>#    V1</span></span>
<span id="cb16-4"><span># 1 CQK</span></span>
<span id="cb16-5"><span># 2 CQQ</span></span>
<span id="cb16-6"><span># 3 CQZ</span></span>
<span id="cb16-7"><span># 4 CVY</span></span>
<span id="cb16-8"><span># 5 CWZ</span></span>
<span id="cb16-9"><span># 6 CXK</span></span>
<span id="cb16-10">tla<span>$</span>V2 <span>&lt;-</span> <span>as.character</span>(tla<span>$</span>V1)</span>
<span id="cb16-11"></span>
<span id="cb16-12">letters <span>&lt;-</span> <span>c</span>(<span>"A"</span>, <span>"B"</span>, <span>"C"</span>, <span>"D"</span>, <span>"E"</span>, <span>"F"</span>, <span>"G"</span>, <span>"H"</span>, <span>"I"</span>, <span>"J"</span>, <span>"K"</span>, <span>"L"</span>, <span>"M"</span>,</span>
<span id="cb16-13">            <span>"N"</span>, <span>"O"</span>, <span>"P"</span>, <span>"Q"</span>, <span>"R"</span>, <span>"S"</span>, <span>"T"</span>, <span>"U"</span>, <span>"V"</span>, <span>"W"</span>, <span>"X"</span>, <span>"Y"</span>, <span>"Z"</span>)</span>
<span id="cb16-14"></span>
<span id="cb16-15">acronyms <span>&lt;-</span> <span>expand.grid</span>(letters, letters, letters)</span>
<span id="cb16-16">acronyms_vector <span>&lt;-</span> <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>""</span>)</span>
<span id="cb16-17"><span>head</span>(acronyms_vector); <span>length</span>(acronyms_vector)</span>
<span id="cb16-18"><span># [1] "AAA" "BAA" "CAA" "DAA" "EAA" "FAA"</span></span>
<span id="cb16-19"><span># [1] 17576</span></span>
<span id="cb16-20"></span>
<span id="cb16-21"><span># Function to generate acronyms of a given length:</span></span>
<span id="cb16-22">generate_acronyms <span>&lt;-</span> <span>function</span>(len) {</span>
<span id="cb16-23">  acronyms <span>&lt;-</span> <span>expand.grid</span>(<span>rep</span>(<span>list</span>(letters), len))</span>
<span id="cb16-24">  <span>apply</span>(acronyms, <span>1</span>, paste0, <span>collapse =</span> <span>""</span>)</span>
<span id="cb16-25">}</span>
<span id="cb16-26"></span>
<span id="cb16-27"><span># Generate 3- and 4-letter acronyms:</span></span>
<span id="cb16-28">acronyms_vector <span>&lt;-</span> <span>unlist</span>(<span>lapply</span>(<span>3</span><span>:</span><span>4</span>, generate_acronyms))</span>
<span id="cb16-29"></span>
<span id="cb16-30"><span># Create data frame and update 'Missing':</span></span>
<span id="cb16-31">acronyms_df <span>&lt;-</span> <span>data.frame</span>(<span>V1 =</span> acronyms_vector, <span>V2 =</span> acronyms_vector, <span>Missing =</span> <span>FALSE</span>, <span>stringsAsFactors =</span> <span>FALSE</span>)</span>
<span id="cb16-32">acronyms_df<span>$</span>Missing[acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2] <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-33"></span>
<span id="cb16-34"><span>## Add a 'Missing' column to 'tla'</span></span>
<span id="cb16-35">tla<span>$</span>Missing <span>&lt;-</span> <span>TRUE</span></span>
<span id="cb16-36"></span>
<span id="cb16-37"><span>## Combine the two data-frames</span></span>
<span id="cb16-38">result <span>&lt;-</span> <span>rbind</span>(tla, acronyms_df[<span>!</span>acronyms_df<span>$</span>V2 <span>%in%</span> tla<span>$</span>V2, ])</span>
<span id="cb16-39">result <span>&lt;-</span> result[<span>order</span>(result<span>$</span>V2), ] <span># sort in alphabetic order</span></span>
<span id="cb16-40"><span>summary</span>(result)</span>
<span id="cb16-41"><span>#       V1              V2             Missing</span></span>
<span id="cb16-42"><span># AAAB   :     1   Length:410460      Mode :logical</span></span>
<span id="cb16-43"><span># AAAG   :     1   Class :character   FALSE:14892</span></span>
<span id="cb16-44"><span># AAAK   :     1   Mode  :character   TRUE :395568</span></span>
<span id="cb16-45"><span># AAAO   :     1</span></span>
<span id="cb16-46"><span># AAAQ   :     1</span></span>
<span id="cb16-47"><span># AAAU   :     1</span></span>
<span id="cb16-48"><span># (Other):410454</span></span>
<span id="cb16-49"><span>head</span>(result); <span>tail</span>(result)</span>
<span id="cb16-50"><span>#           V1   V2 Missing</span></span>
<span id="cb16-51"><span># 1100000  AAA  AAA   FALSE</span></span>
<span id="cb16-52"><span># 2685    AAAB AAAB    TRUE</span></span>
<span id="cb16-53"><span># 2686    AAAG AAAG    TRUE</span></span>
<span id="cb16-54"><span># 2687    AAAK AAAK    TRUE</span></span>
<span id="cb16-55"><span># 2688    AAAO AAAO    TRUE</span></span>
<span id="cb16-56"><span># 2689    AAAQ AAAQ    TRUE</span></span>
<span id="cb16-57"><span>#          V1   V2 Missing</span></span>
<span id="cb16-58"><span># 395563 ZZZT ZZZT    TRUE</span></span>
<span id="cb16-59"><span># 395564 ZZZU ZZZU    TRUE</span></span>
<span id="cb16-60"><span># 395565 ZZZV ZZZV    TRUE</span></span>
<span id="cb16-61"><span># 395566 ZZZW ZZZW    TRUE</span></span>
<span id="cb16-62"><span># 395567 ZZZX ZZZX    TRUE</span></span>
<span id="cb16-63"><span># 395568 ZZZY ZZZY    TRUE</span></span>
<span id="cb16-64">index <span>&lt;-</span> <span>which</span>(result_tla<span>$</span>V2 <span>==</span> <span>"CQK"</span>)</span>
<span id="cb16-65">percentage <span>&lt;-</span> index <span>/</span> <span>nrow</span>(result_tla) <span>*</span> <span>100</span>; percentage</span>
<span id="cb16-66"><span># [1] 10.1217569</span></span>
<span id="cb16-67"></span>
<span id="cb16-68"><span>## Visualize missingness:</span></span>
<span id="cb16-69">result_tla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>3</span>, ]</span>
<span id="cb16-70">result_fla <span>&lt;-</span> result[<span>nchar</span>(result<span>$</span>V2) <span>==</span> <span>4</span>, ]</span>
<span id="cb16-71"></span>
<span id="cb16-72">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_tla<span>$</span>Missing))); dimensions <span># 133</span></span>
<span id="cb16-73"><span>png</span>(<span>file=</span><span>"~/wiki/doc/wikipedia/2023-11-04-gwern-tla-missingness.png"</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-74"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_tla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-75"><span>invisible</span>(<span>dev.off</span>())</span>
<span id="cb16-76"></span>
<span id="cb16-77">dimensions <span>&lt;-</span> <span>round</span>(<span>sqrt</span>(<span>length</span>(result_fla<span>$</span>Missing))); dimensions <span># 676</span></span>
<span id="cb16-78"><span>png</span>(<span>file=</span><span>"~/wiki/doc/wikipedia/2023-11-04-gwern-fla-missingness.png"</span>, <span>width =</span> <span>2400</span>, <span>height =</span> <span>2400</span>)</span>
<span id="cb16-79"><span>image</span>(<span>t</span>(<span>matrix</span>(<span>rev</span>(result_fla<span>$</span>Missing), <span>nrow=</span>dimensions, <span>ncol=</span>dimensions, <span>byrow=</span><span>TRUE</span>)), <span>col=</span><span>gray.colors</span>(<span>2</span>))</span>
<span id="cb16-80"><span>invisible</span>(<span>dev.off</span>())</span></code></pre>
                </div>
                <figure>
                  <img alt="Visualization of missingness of TLAs, A–Z (wrapped into a 133×133 grid; read: top to bottom, left to right); the first dot at top-left 10% of the way through is “CQK”." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-tla-missingness.png" width="1400">
                  
                </figure>
                <figure>
                  <img alt="Visualization of missingness of four-letter-acronyms, A–Z (wrapped into a 676×676 grid; read: top to bottom, left to right)." decoding="async" height="1400" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-11-04-gwern-fla-missingness.png" width="1400">
                  
                </figure>
                <p>We would like to investigate per-letter properties, like all TLAs with a ‘Z’ in them, so we set up 26 dummy variables for whether each letter is present:</p>
                <div id="cb17">
                  <pre><code><span id="cb17-1"><span>library</span>(stringr)</span>
<span id="cb17-2"><span>for</span> (letter <span>in</span> letters) {</span>
<span id="cb17-3">  result[[letter]] <span>&lt;-</span> <span>str_detect</span>(result<span>$</span>V2, <span>fixed</span>(letter))</span>
<span id="cb17-4">}</span>
<span id="cb17-5"><span>head</span>(result, <span>n=</span><span>1</span>)</span>
<span id="cb17-6"><span>#    V1  V2 Missing     A     B    C     D     E     F     G     H     I     J</span></span>
<span id="cb17-7"><span># 1 CQK CQK    TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-8"><span>#      K     L     M     N     O     P    Q     R     S     T     U     V     W</span></span>
<span id="cb17-9"><span>#   TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE</span></span>
<span id="cb17-10"><span>#       X     Y     Z</span></span>
<span id="cb17-11"><span>#   FALSE FALSE FALSE</span></span></code></pre>
                </div>
                <p>With a <code>Missing</code> boolean variable &amp; the alphabetical dummy variables set up, we can do a <a href="https://en.wikipedia.org/wiki/Logistic_regression" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Logistic_regression#bodyContent">logistic regression</a>, where each acronym is a single datapoint, and the letters it contains are the covariates:</p>
                <div id="cb18">
                  <pre><code><span id="cb18-1"><span>## Define the formula for the logistic regression model without intercept (as an acronym always has letters):</span></span>
<span id="cb18-2">formula <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>"Missing ~ 0 +"</span>, <span>paste</span>(letters, <span>collapse =</span> <span>" + "</span>)))</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span>## Fit the logistic regression model</span></span>
<span id="cb18-5">model_no_intercept <span>&lt;-</span> <span>glm</span>(formula, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>"logit"</span>))</span>
<span id="cb18-6"><span>summary</span>(model_no_intercept)</span>
<span id="cb18-7"><span># ...Deviance Residuals:</span></span>
<span id="cb18-8"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb18-9"><span># −2.8134706   0.0885086   0.1136063   0.1540683   2.7432638</span></span>
<span id="cb18-10"><span>#</span></span>
<span id="cb18-11"><span># Coefficients:</span></span>
<span id="cb18-12"><span>#          Estimate Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb18-13"><span># AFALSE −6.5878352  0.0568884 −115.8028 &lt; 2.22e-16</span></span>
<span id="cb18-14"><span># ATRUE  −4.0755472  0.0479506  −84.9948 &lt; 2.22e-16</span></span>
<span id="cb18-15"><span># BTRUE   2.7764680  0.0322792   86.0142 &lt; 2.22e-16</span></span>
<span id="cb18-16"><span># CTRUE   2.4976112  0.0317669   78.6231 &lt; 2.22e-16</span></span>
<span id="cb18-17"><span># DTRUE   2.7903755  0.0324820   85.9053 &lt; 2.22e-16</span></span>
<span id="cb18-18"><span># ETRUE   2.8485807  0.0328888   86.6124 &lt; 2.22e-16</span></span>
<span id="cb18-19"><span># FTRUE   2.8527917  0.0327796   87.0295 &lt; 2.22e-16</span></span>
<span id="cb18-20"><span># GTRUE   2.9681415  0.0333202   89.0794 &lt; 2.22e-16</span></span>
<span id="cb18-21"><span># HTRUE   2.9743513  0.0333339   89.2290 &lt; 2.22e-16</span></span>
<span id="cb18-22"><span># ITRUE   2.8127265  0.0328846   85.5333 &lt; 2.22e-16</span></span>
<span id="cb18-23"><span># JTRUE   3.2605341  0.0352484   92.5016 &lt; 2.22e-16</span></span>
<span id="cb18-24"><span># KTRUE   2.8210890  0.0334714   84.2836 &lt; 2.22e-16</span></span>
<span id="cb18-25"><span># LTRUE   2.8388725  0.0328000   86.5511 &lt; 2.22e-16</span></span>
<span id="cb18-26"><span># MTRUE   2.7014280  0.0321802   83.9469 &lt; 2.22e-16</span></span>
<span id="cb18-27"><span># NTRUE   2.8776599  0.0330334   87.1137 &lt; 2.22e-16</span></span>
<span id="cb18-28"><span># OTRUE   2.9512577  0.0334268   88.2902 &lt; 2.22e-16</span></span>
<span id="cb18-29"><span># PTRUE   2.7394627  0.0324478   84.4268 &lt; 2.22e-16</span></span>
<span id="cb18-30"><span># QTRUE   3.6899056  0.0393598   93.7482 &lt; 2.22e-16</span></span>
<span id="cb18-31"><span># RTRUE   2.8297209  0.0329451   85.8919 &lt; 2.22e-16</span></span>
<span id="cb18-32"><span># STRUE   2.5292698  0.0319661   79.1235 &lt; 2.22e-16</span></span>
<span id="cb18-33"><span># TTRUE   2.7727895  0.0325522   85.1797 &lt; 2.22e-16</span></span>
<span id="cb18-34"><span># UTRUE   3.0611813  0.0338695   90.3817 &lt; 2.22e-16</span></span>
<span id="cb18-35"><span># VTRUE   3.1443269  0.0344698   91.2199 &lt; 2.22e-16</span></span>
<span id="cb18-36"><span># WTRUE   2.8428509  0.0337547   84.2208 &lt; 2.22e-16</span></span>
<span id="cb18-37"><span># XTRUE   3.5003099  0.0375402   93.2417 &lt; 2.22e-16</span></span>
<span id="cb18-38"><span># YTRUE   3.2994364  0.0356499   92.5510 &lt; 2.22e-16</span></span>
<span id="cb18-39"><span># ZTRUE   3.5370118  0.0378513   93.4448 &lt; 2.22e-16</span></span>
<span id="cb18-40"><span>#</span></span>
<span id="cb18-41"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb18-42"><span>#</span></span>
<span id="cb18-43"><span>#     Null deviance: 569018.38  on 410460  degrees of freedom</span></span>
<span id="cb18-44"><span># Residual deviance:  88916.78  on 410433  degrees of freedom</span></span>
<span id="cb18-45"><span># AIC: 88970.78</span></span></code></pre>
                </div>
                <p>As expected, the letter makes a difference, and rarer letters like ‘Z’ or ‘J’ are especially likely to correlate with unuse.</p>
                <p>We can plot an absolute plot<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>, but that wouldn’t be a good visualization because unused/used must sum to 100%, so it’d be better to do a relative percentage plot, like this:</p>
                <div id="cb20">
                  <pre><code><span id="cb20-1">letter_df<span>$</span>V1 <span>&lt;-</span> <span>as.character</span>(letter_df<span>$</span>V1)</span>
<span id="cb20-2"><span>## Filter out four-letter acronyms</span></span>
<span id="cb20-3">three_letter_df <span>&lt;-</span> letter_df[<span>nchar</span>(letter_df<span>$</span>V1) <span>==</span> <span>3</span>, ]</span>
<span id="cb20-4"></span>
<span id="cb20-5"><span>## Calculate counts</span></span>
<span id="cb20-6">counts <span>&lt;-</span> <span>table</span>(three_letter_df<span>$</span>Letter, three_letter_df<span>$</span>Missing)</span>
<span id="cb20-7"><span>## Calculate relative percentages</span></span>
<span id="cb20-8">percentages <span>&lt;-</span> <span>prop.table</span>(counts, <span>1</span>)</span>
<span id="cb20-9"><span>## Convert to data frame for plotting</span></span>
<span id="cb20-10">percentages_df <span>&lt;-</span> <span>as.data.frame.table</span>(percentages, <span>responseName =</span> <span>"Percentage"</span>)</span>
<span id="cb20-11"></span>
<span id="cb20-12"><span>## Plot</span></span>
<span id="cb20-13"><span>library</span>(ggplot2)</span>
<span id="cb20-14"><span>ggplot</span>(percentages_df, <span>aes</span>(<span>x =</span> Var1, <span>y =</span> Percentage, <span>fill =</span> Var2)) <span>+</span></span>
<span id="cb20-15">  <span>geom_col</span>() <span>+</span></span>
<span id="cb20-16">  <span>theme_minimal</span>(<span>base_size =</span> <span>50</span>) <span>+</span></span>
<span id="cb20-17">  <span>theme</span>(<span>legend.position =</span> <span>"none"</span>) <span>+</span></span>
<span id="cb20-18">  <span>labs</span>(<span>x =</span> <span>"Letter"</span>, <span>y =</span> <span>"Percentage"</span>, <span>fill =</span> <span>"Missing"</span>)</span></code></pre>
                </div>
                <figure>
                  <img alt="Bar plot of unused fraction, by alphabetic letter (A–Z): Rarer letters (eg. ‘Z’) more likely to be unused in TLAs, but usage not fully explained by letter frequency (eg. ‘A’)." decoding="async" height="794" id="bar-plot" loading="lazy" src="https://gwern.net/doc/wikipedia/2023-10-01-gwern-tla-lettervsunusedtlaswiththatletterpercentageoverthealphabet.png" title="Statistical bar chart of dataset of all three-letter acronyms by whether they have an English Wikipedia article; each letter is extracted and coded by whether that acronym was missing; red means the associated acronym was used, and blue means unused. The fraction of blue increases through the alphabet, as expected." width="1400">
                  
                </figure>
                <p>The plot makes sense, but some things are anomalous: like the letter ‘A’—it’s perhaps the rarest of all letters, and yet, if any letter is rarest, it ought to be ‘E’, one would think, because everyone knows ‘E’ is the most common letter in the English language. What is the correlation with letter frequency? We can take a letter frequency list (<a href="https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html">first one I found in Google</a>), and look at the correlation. We use a <a href="https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Kendall_rank_correlation_coefficient#bodyContent">Kendall rank correlation</a> because there’s no particular reason to think that the magnitude or distribution of either the logistic regression coefficients or the letter frequencies are <a href="https://en.wikipedia.org/wiki/Normal_distribution" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Normal_distribution#bodyContent">normally distributed</a>, and we just think that there should be an inverse correlation when ordered: rarer letters = more-unused TLAs.</p>
                <div id="cb21">
                  <pre><code><span id="cb21-1"><span>## https://pi.math.cornell.edu/~mec/2003-2004/cryptography/subs/frequencies.html</span></span>
<span id="cb21-2">frequencies <span>&lt;-</span> <span>read.table</span>(<span>stdin</span>(), <span>header=</span><span>TRUE</span>, <span>colClasses=</span><span>c</span>(<span>"factor"</span>,<span>"integer"</span>,<span>"character"</span>,<span>"numeric"</span>))</span>
<span id="cb21-3">Letter  Count       Letter  Frequency</span>
<span id="cb21-4">E   <span>21912</span>       E   <span>12.02</span></span>
<span id="cb21-5">T   <span>16587</span>       T   <span>9.10</span></span>
<span id="cb21-6">A   <span>14810</span>       A   <span>8.12</span></span>
<span id="cb21-7">O   <span>14003</span>       O   <span>7.68</span></span>
<span id="cb21-8">I   <span>13318</span>       I   <span>7.31</span></span>
<span id="cb21-9">N   <span>12666</span>       N   <span>6.95</span></span>
<span id="cb21-10">S   <span>11450</span>       S   <span>6.28</span></span>
<span id="cb21-11">R   <span>10977</span>       R   <span>6.02</span></span>
<span id="cb21-12">H   <span>10795</span>       H   <span>5.92</span></span>
<span id="cb21-13">D   <span>7874</span>        D   <span>4.32</span></span>
<span id="cb21-14">L   <span>7253</span>        L   <span>3.98</span></span>
<span id="cb21-15">U   <span>5246</span>        U   <span>2.88</span></span>
<span id="cb21-16">C   <span>4943</span>        C   <span>2.71</span></span>
<span id="cb21-17">M   <span>4761</span>        M   <span>2.61</span></span>
<span id="cb21-18">F   <span>4200</span>        F   <span>2.30</span></span>
<span id="cb21-19">Y   <span>3853</span>        Y   <span>2.11</span></span>
<span id="cb21-20">W   <span>3819</span>        W   <span>2.09</span></span>
<span id="cb21-21">G   <span>3693</span>        G   <span>2.03</span></span>
<span id="cb21-22">P   <span>3316</span>        P   <span>1.82</span></span>
<span id="cb21-23">B   <span>2715</span>        B   <span>1.49</span></span>
<span id="cb21-24">V   <span>2019</span>        V   <span>1.11</span></span>
<span id="cb21-25">K   <span>1257</span>        K   <span>0.69</span></span>
<span id="cb21-26">X   <span>315</span>         X   <span>0.17</span></span>
<span id="cb21-27">Q   <span>205</span>         Q   <span>0.11</span></span>
<span id="cb21-28">J   <span>188</span>         J   <span>0.10</span></span>
<span id="cb21-29">Z   <span>128</span>         Z   <span>0.07</span></span>
<span id="cb21-30"></span>
<span id="cb21-31"><span>## Put in alphabetic order:</span></span>
<span id="cb21-32">frequencies <span>&lt;-</span> frequencies[<span>order</span>(frequencies<span>$</span>Letter),]</span>
<span id="cb21-33"></span>
<span id="cb21-34">letter_coefficients <span>&lt;-</span> <span>coef</span>(model_no_intercept)</span>
<span id="cb21-35">letter_coefficients[<span>-</span><span>1</span>]</span>
<span id="cb21-36"><span>#       ATRUE       BTRUE       CTRUE       DTRUE       ETRUE       FTRUE</span></span>
<span id="cb21-37"><span># −4.07554725  2.77646803  2.49761116  2.79037554  2.84858066  2.85279167</span></span>
<span id="cb21-38"><span>#       GTRUE       HTRUE       ITRUE       JTRUE       KTRUE       LTRUE</span></span>
<span id="cb21-39"><span>#  2.96814151  2.97435134  2.81272652  3.26053408  2.82108898  2.83887250</span></span>
<span id="cb21-40"><span>#       MTRUE       NTRUE       OTRUE       PTRUE       QTRUE       RTRUE</span></span>
<span id="cb21-41"><span>#  2.70142800  2.87765993  2.95125774  2.73946268  3.68990557  2.82972088</span></span>
<span id="cb21-42"><span>#       STRUE       TTRUE       UTRUE       VTRUE       WTRUE       XTRUE</span></span>
<span id="cb21-43"><span>#  2.52926976  2.77278951  3.06118128  3.14432686  2.84285087  3.50030988</span></span>
<span id="cb21-44"><span>#       YTRUE       ZTRUE</span></span>
<span id="cb21-45"><span>#  3.29943643  3.53701178</span></span>
<span id="cb21-46"><span>cor</span>(letter_coefficients[<span>-</span><span>1</span>], frequencies<span>$</span>Frequency)</span>
<span id="cb21-47"><span># [1] −0.357640466</span></span></code></pre>
                </div>
              </section>
              <section id="order-letter-frequency-effects">
                <h4><a href="#order-letter-frequency-effects" title="Link to section: § 'Order &amp; Letter-Frequency Effects'">Order &amp; Letter-Frequency Effects</a></h4>
                <p>What is omitted from our model? Going back and comparing the frequency list to <a href="#bar-plot">the previous bar plot</a>{#previous-back-link], it looks suspiciously like letters early in the alphabet (not just ‘A’) are overrepresented, and then ‘Z’ is inflated (perhaps because it is the final letter and has many connotations).</p>
                <p>How do we encode in ‘early letters’, as contrasted with ‘letter frequency’? We can add into the logistic regression a variable for the earliest/‘smallest’ letter an acronym has, a <code>MinIndex</code>. (This would help pick up trends from trying to abuse ‘A’ to sort first in any list or sublist.) And we can encode the letter frequencies by just averaging them, as a <code>AvgFrequency</code>. (Clearly imperfect, but also unclear what the right thing to do would be: instead of an arithmetic mean, a harmonic mean? Something else entirely?) Then we can add them to the regression as control variables to try to explain away their effects:</p>
                <div id="cb22">
                  <pre><code><span id="cb22-1"><span>## Add a column 'MinIndex' to 'result' that contains the smallest index of the letters in each acronym</span></span>
<span id="cb22-2">result<span>$</span>MinIndex <span>&lt;-</span> <span>apply</span>(result[,letters], <span>1</span>, <span>function</span>(x) <span>min</span>(<span>which</span>(x)))</span>
<span id="cb22-3"><span>## Map the letters to their frequencies</span></span>
<span id="cb22-4">letter_to_frequency <span>&lt;-</span> <span>setNames</span>(frequencies<span>$</span>Frequency <span>/</span> <span>100</span>, frequencies<span>$</span>Letter)</span>
<span id="cb22-5"></span>
<span id="cb22-6"><span>## Add a column 'AvgFrequency' to 'result' that contains the average frequency of the letters in each acronym</span></span>
<span id="cb22-7"><span>## Compute the frequency of each letter in each acronym</span></span>
<span id="cb22-8">acronym_frequencies <span>&lt;-</span> <span>lapply</span>(<span>strsplit</span>(result<span>$</span>V2, <span>""</span>), <span>function</span>(acronym) letter_to_frequency[acronym])</span>
<span id="cb22-9"><span>## Compute the average frequency for each acronym</span></span>
<span id="cb22-10">result<span>$</span>AvgFrequency <span>&lt;-</span> <span>sapply</span>(acronym_frequencies, mean)</span>
<span id="cb22-11"></span>
<span id="cb22-12"><span>## Archive results to `/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv.xz`:</span></span>
<span id="cb22-13"><span>write.csv</span>(result, <span>file=</span><span>"doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-processeddata.csv"</span>, <span>row.names=</span><span>FALSE</span>)</span>
<span id="cb22-14"></span>
<span id="cb22-15"><span>## Update the formula to include 'MinIndex' &amp; 'AvgFrequency' as a covariate:</span></span>
<span id="cb22-16">formulaControlled <span>&lt;-</span> <span>as.formula</span>(<span>paste</span>(<span>"Missing ~ MinIndex + AvgFrequency +"</span>, <span>paste</span>(letters, <span>collapse =</span> <span>" + "</span>)))</span>
<span id="cb22-17"><span>## Fit the logistic regression model</span></span>
<span id="cb22-18">modelControlled <span>&lt;-</span> <span>glm</span>(formulaControlled, <span>data =</span> result, <span>family =</span> <span>binomial</span>(<span>link =</span> <span>"logit"</span>))</span>
<span id="cb22-19"><span>summary</span>(modelControlled)</span>
<span id="cb22-20"><span># ...Deviance Residuals:</span></span>
<span id="cb22-21"><span>#        Min          1Q      Median          3Q         Max</span></span>
<span id="cb22-22"><span># −2.9069303   0.0891105   0.1128827   0.1500609   2.8642110</span></span>
<span id="cb22-23"><span>#</span></span>
<span id="cb22-24"><span># Coefficients:</span></span>
<span id="cb22-25"><span>#                 Estimate  Std. Error   z value   Pr(&gt;|z|)</span></span>
<span id="cb22-26"><span># (Intercept)  −6.90634502  0.09250317 −74.66063 &lt; 2.22e-16</span></span>
<span id="cb22-27"><span># MinIndex      0.04029557  0.00370886  10.86468 &lt; 2.22e-16</span></span>
<span id="cb22-28"><span># AvgFrequency −4.25188554  1.42341896  −2.98709  0.0028164</span></span>
<span id="cb22-29"><span># ATRUE         2.88138189  0.04677432  61.60179 &lt; 2.22e-16</span></span>
<span id="cb22-30"><span># BTRUE         3.01005967  0.04276745  70.38203 &lt; 2.22e-16</span></span>
<span id="cb22-31"><span># CTRUE         2.71712516  0.03922165  69.27616 &lt; 2.22e-16</span></span>
<span id="cb22-32"><span># DTRUE         2.99290423  0.03752173  79.76456 &lt; 2.22e-16</span></span>
<span id="cb22-33"><span># ETRUE         3.13077197  0.05264291  59.47187 &lt; 2.22e-16</span></span>
<span id="cb22-34"><span># FTRUE         2.97227342  0.03637453  81.71305 &lt; 2.22e-16</span></span>
<span id="cb22-35"><span># GTRUE         3.05866771  0.03637991  84.07573 &lt; 2.22e-16</span></span>
<span id="cb22-36"><span># HTRUE         3.09923276  0.03586679  86.40954 &lt; 2.22e-16</span></span>
<span id="cb22-37"><span># ITRUE         2.94019473  0.03734308  78.73466 &lt; 2.22e-16</span></span>
<span id="cb22-38"><span># JTRUE         3.26929982  0.04017641  81.37361 &lt; 2.22e-16</span></span>
<span id="cb22-39"><span># KTRUE         2.82703715  0.03688530  76.64401 &lt; 2.22e-16</span></span>
<span id="cb22-40"><span># LTRUE         2.87906510  0.03307471  87.04734 &lt; 2.22e-16</span></span>
<span id="cb22-41"><span># MTRUE         2.71268066  0.03290704  82.43467 &lt; 2.22e-16</span></span>
<span id="cb22-42"><span># NTRUE         2.94220314  0.03616480  81.35544 &lt; 2.22e-16</span></span>
<span id="cb22-43"><span># OTRUE         3.02043913  0.03804621  79.38869 &lt; 2.22e-16</span></span>
<span id="cb22-44"><span># PTRUE         2.72109474  0.03383860  80.41393 &lt; 2.22e-16</span></span>
<span id="cb22-45"><span># QTRUE         3.63855049  0.04328351  84.06320 &lt; 2.22e-16</span></span>
<span id="cb22-46"><span># RTRUE         2.86279675  0.03446914  83.05390 &lt; 2.22e-16</span></span>
<span id="cb22-47"><span># STRUE         2.56086404  0.03376525  75.84317 &lt; 2.22e-16</span></span>
<span id="cb22-48"><span># TTRUE         2.84462320  0.04070488  69.88409 &lt; 2.22e-16</span></span>
<span id="cb22-49"><span># UTRUE         3.04398849  0.03417759  89.06388 &lt; 2.22e-16</span></span>
<span id="cb22-50"><span># VTRUE         3.10148262  0.03683701  84.19474 &lt; 2.22e-16</span></span>
<span id="cb22-51"><span># WTRUE         2.81549885  0.03470352  81.13006 &lt; 2.22e-16</span></span>
<span id="cb22-52"><span># XTRUE         3.44003115  0.04137750  83.13773 &lt; 2.22e-16</span></span>
<span id="cb22-53"><span># YTRUE         3.26779943  0.03654490  89.41876 &lt; 2.22e-16</span></span>
<span id="cb22-54"><span># ZTRUE         3.47511758  0.04188641  82.96528 &lt; 2.22e-16</span></span>
<span id="cb22-55"><span>#</span></span>
<span id="cb22-56"><span># (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb22-57"><span>#</span></span>
<span id="cb22-58"><span>#     Null deviance: 128014.32  on 410459  degrees of freedom</span></span>
<span id="cb22-59"><span># Residual deviance:  88784.22  on 410431  degrees of freedom</span></span>
<span id="cb22-60"><span># AIC: 88842.22</span></span>
<span id="cb22-61"></span>
<span id="cb22-62">letter_coefficients2 <span>&lt;-</span> <span>coef</span>(modelControlled)</span>
<span id="cb22-63"><span>cor</span>(letter_coefficients2[<span>-</span><span>c</span>(<span>1</span><span>:</span><span>3</span>)], frequencies<span>$</span>Frequency, <span>method=</span><span>"kendall"</span>)</span>
<span id="cb22-64"><span># [1] −0.28</span></span>
<span id="cb22-65"></span>
<span id="cb22-66"><span>&gt;</span> <span>1</span> <span>-</span> (<span>0.28</span> <span>/</span> <span>0.35</span>)</span>
<span id="cb22-67"><span># [1] 0.2</span></span></code></pre>
                </div>
                
                <p>These variables do help and are tapping into letter-frequency somewhat (because the rank correlation of the ‘frequency-adjusted’ coefficients shrinks by ~20%), suggesting that both an ‘earlier letter’ &amp; English letter-frequencies are at play in correlating with unused TLAs. But there is still much <a href="https://en.wikipedia.org/wiki/Variance" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Variance#bodyContent">variance</a> unexplained and a non-zero rank correlation, so either these aren’t good ways of quantifying those two effects or there’s still important variables lurking.</p>
              </section>
              <section id="further-work">
                <h4><a href="#further-work" title="Link to section: § 'Further Work'">Further Work</a></h4>
                <p><span><em>n</em>-grams.</span> The next step in an analysis might be to adopt the <a href="https://en.wikipedia.org/wiki/N-gram" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/N-gram#bodyContent"><em>n</em>-gram</a> framework, and go from single-letter analysis (unigrams) to pairs of letters (bigrams), as that would help pick up subtler patterns (eg. grammatical patterns in pairs of words that make up acronyms).</p>
                <p><span>Simulation?</span> One could also try to find some way to simulate TLA datasets—I couldn’t figure out a way to generatively-simulate, <a href="https://en.wikipedia.org/wiki/Resampling_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Resampling_(statistics)#bodyContent">resample</a>, or <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics)#bodyContent">bootstrap</a> this dataset, because all the obvious ways to do so either require additional knowledge like <em>how many</em> instances of a given TLA there were, how many TLAs total there were ‘generated’, or are just wrong (at least at the unigram level). If you try to simulate out a set of hypothetical acronyms based on the letter frequencies, then you need to know how many acronyms total are sampled, not merely whether ≥1 instance existed in Wikipedia; if you sampled <em>n</em> = 1 billion, then no TLA would be missing, and if you sampled <em>n</em> = 1, almost all of them must be missing, so that doesn’t work.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> If you try to shuffle the set of missing/non-missing, that will by definition recreate the same single-letter statistics. If you try to resample a smaller dataset bootstrap-style, it’s not clear what that would mean even if it was valid in some sense.</p>
                <p><span>Trends?</span> It would also be mildly interesting to revisit this in a few years to see how acronym usage increases over time. Because notability is irreversible, acronyms should accumulate over time: at what point would people be forced into four-letter acronyms? Or do they just bite the bullet of overlaps because four-letter ones are so much worse? Or perhaps ‘natural’ TLA behavior is that the remainder are <em>so</em> hard to think of (without deliberately targeting unused ones) that the trend is too slow to matter?</p>
                <p>But I’ll leave it here.</p>
              </section>
            </section>
          </section>
        </section>
        <section id="conclusion">
          <h2><a href="#conclusion" title="Link to section: § 'Conclusion'">Conclusion</a></h2>
          <p>So now I know: there <em>are</em> TLAs available for one to use, even if the specific TLA one wants is probably taken. As TLAs run out, there may be gradually more pressure to figure out how to use the remaining 15%.</p>
          <p>You’ll have to look at the list of unused TLAs, and then figure out how to make the <a href="https://en.wikipedia.org/wiki/Backronym" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Backronym#bodyContent">backronym</a> work, while consulting the back pages of a dictionary… Perhaps one could ask GPT-4 for help?</p>
          <div>
            <blockquote>
              <p><span><strong>I’m founding a startup for selling <a href="https://gwern.net/catnip" id="gwern-catnip" title="‘Catnip immunity and alternatives’, Gwern 2015">psychoactive cat treats</a> like <a href="https://en.wikipedia.org/wiki/Actinidia_polygama#Pets" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Actinidia_polygama#bodyContent">silvervine</a> or <a href="https://en.wikipedia.org/wiki/Catnip" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Catnip#bodyContent">catnip</a> or <a href="https://en.wikipedia.org/wiki/Valerian_(herb)#Effect_on_cats" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Valerian_(herb)#bodyContent">Valerian root extract</a> to pet owners. I want to name it ‘CQK’, because no one has used that acronym for anything yet. But I can’t come up with a good backronym for it. Please come up with as many possible backronyms for ‘CQK’ as you can.</strong></span></p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom</p>
                </li>
                <li>
                  <p>Cat Quirk Kit</p>
                </li>
                <li>
                  <p>Catnip Quotient Key</p>
                </li>
                <li>
                  <p>Cat Quality Kicks</p>
                </li>
                <li>
                  <p>Catnip Quick Kit</p>
                </li>
                <li>
                  <p>Cats’ Quixotic Knead</p>
                </li>
                <li>
                  <p>Catnip Quest Kit</p>
                </li>
                <li>
                  <p>Cats’ Quality Knead</p>
                </li>
                <li>
                  <p>Cat Quake Kit</p>
                </li>
                <li>
                  <p>Cats’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks</p>
                </li>
                <li>
                  <p>Catnip Quarantine Kit</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Keepsake</p>
                </li>
                <li>
                  <p>Cat Quench Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Key</p>
                </li>
                <li>
                  <p>Cats’ Quirky Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirky Kingdom</p>
                </li>
                <li>
                  <p>Cats’ Quality Keeps</p>
                </li>
                <li>
                  <p>Catnip Quenching Kit</p>
                </li>
                <li>
                  <p>Cat Quirk Kingdom</p>
                </li>
                <li>
                  <p>Catnip Quality Kinetics</p>
                </li>
                <li>
                  <p>Cat Quirk Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quantum Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Catnip Quirk Key</p>
                </li>
                <li>
                  <p>Cat Quotient Kit</p>
                </li>
                <li>
                  <p>Catnip Quality Kiosk</p>
                </li>
                <li>
                  <p>Cats’ Quake Kit</p>
                </li>
              </ol>
              <p><strong>Come up with another list which does not use ‘cat’ or ‘catnip’ anywhere.</strong></p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom</p>
                </li>
                <li>
                  <p>Creature Quirk Kit</p>
                </li>
                <li>
                  <p>Calm Quotient Key</p>
                </li>
                <li>
                  <p>Companions’ Quenching Knack</p>
                </li>
                <li>
                  <p>Cognitive Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quality Kicks</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quench Kit</p>
                </li>
                <li>
                  <p>Companions’ Quotient Kit</p>
                </li>
                <li>
                  <p>Cognitive Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calm Quirk Kiosk</p>
                </li>
                <li>
                  <p>Companion Quench Kit</p>
                </li>
                <li>
                  <p>Calming Quantum Key</p>
                </li>
                <li>
                  <p>Companions’ Quintessential Knick-Knacks</p>
                </li>
                <li>
                  <p>Cognitive Quirk Key</p>
                </li>
                <li>
                  <p>Calm Quality Kingdom</p>
                </li>
                <li>
                  <p>Companion Quality Kinetics</p>
                </li>
                <li>
                  <p>Calming Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companions’ Quake Kit</p>
                </li>
                <li>
                  <p>Cognitive Quality Kicks</p>
                </li>
                <li>
                  <p>Calm Quotient Kit</p>
                </li>
                <li>
                  <p>Companion Quantum Key</p>
                </li>
                <li>
                  <p>Calming Quotient Kit</p>
                </li>
                <li>
                  <p>Companions’ Quirk Key</p>
                </li>
                <li>
                  <p>Cognitive Quench Kit</p>
                </li>
                <li>
                  <p>Calm Quirk Kingdom</p>
                </li>
                <li>
                  <p>Companion Quantum Kiosk</p>
                </li>
                <li>
                  <p>Calming Quake Kit</p>
                </li>
              </ol>
              <p><strong>Which are the best ones which mention cats or catnip? Which are the best ones which do not mention those?</strong></p>
              <p>Among the ones that mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Catnip Quality Kingdom: Suggests a realm of high-quality catnip products.</p>
                </li>
                <li>
                  <p>Cat Quirk Kit: Implies a kit to engage and entertain the unique behaviors of cats.</p>
                </li>
                <li>
                  <p>Cats’ Quintessential Kicks: Evokes the essential fun and excitement for cats.</p>
                </li>
                <li>
                  <p>Catnip Quantum Key: Suggests a critical or unlocking component in catnip-based products.</p>
                </li>
                <li>
                  <p>Cats’ Quirk Kingdom: Implicates a world dedicated to the idiosyncrasies of cats.</p>
                </li>
              </ol>
              <p>Among the ones that do not mention cats or catnip:</p>
              <ol>
                <li>
                  <p>Cognitive Quirk Kit: Suggests a kit designed to engage and stimulate mental processes.</p>
                </li>
                <li>
                  <p>Companions’ Quality Kingdom: Implies a realm of high-quality products for pets.</p>
                </li>
                <li>
                  <p>Calm Quotient Key: Suggests a product that contributes to a calm and peaceful pet.</p>
                </li>
                <li>
                  <p>Companion Quirk Kingdom: Evokes a world dedicated to the idiosyncrasies of pets.</p>
                </li>
                <li>
                  <p>Cognitive Quality Kinetics: Suggests products designed for mental stimulation and physical activity.</p>
                </li>
              </ol>
            </blockquote>
          </div>
          <p>(I like “Cat Quirk Kit” best, as one could supplement the cat psychoactives with other things, like puzzle treats, or laser pointers, or optical illusion printouts triggering the <a href="https://gwern.net/doc/cat/psychology/2021-smith-2.pdf" id="smith-et-al-2021b" data-link-icon="pdf" data-link-icon-type="svg" title="‘If I fits I sits: A citizen science investigation into illusory contour susceptibility in domestic cats (<em>Felis silvestris catus</em>)’, Smith et al 2021b">illusory contour effect</a> &amp; the <a href="https://gwern.net/doc/cat/psychology/2018-szenczi.pdf" id="szenczi-et-al-2018" data-link-icon="pdf" data-link-icon-type="svg" title="‘Perception of the Delboeuf illusion by the adult domestic cat (<em>Felis silvestris catus</em>) in comparison with other mammals’, Szenczi et al 2018">rotating-snake illusion</a>.)</p>
        </section>
        <section id="see-also">
          <h2><a href="#see-also" title="Link to section: § 'See Also'">See Also</a></h2>
          <ul>
            <li>
              <p><a href="https://gwern.net/variable" id="gwern-variable" title="‘Rare Greek Variables’, Gwern 2021">What are the rarest Greek letter variables in scientific writing?</a></p>
            </li>
          </ul>
        </section>
        <section id="appendix">
          <h2><a href="#appendix" title="Link to section: § 'Appendix'">Appendix</a></h2>
          <section id="unused-numerical-acronyms">
            <h2><a href="#unused-numerical-acronyms" title="Link to section: § 'Unused Numerical Acronyms'">Unused Numerical Acronyms</a></h2>
            <p>What if we <em>do</em> want to consider acronyms with numbers in them? We already saw the results for only-alphabetical ones previously, so we need to consider just the subset of number+letter acronyms which have at least 1 letter in them. There are:</p>
            <ol>
              <li>
                <p>For length 1, there are 10 acronyms (the digits 0-9).</p>
              </li>
              <li>
                <p>For length 2, there are 36<sup>2</sup> − 26<sup>2</sup> = 1,296 − 676 = 620 acronyms.</p>
              </li>
              <li>
                <p>For length 3, there are 36<sup>3</sup> − 26<sup>3</sup> = 46,656 − 17,576 = 29,080 acronyms.</p>
              </li>
              <li>
                <p>Adding these up: there are a total of 10 + 620 + 29,080 = 29,710.</p>
              </li>
            </ol>
            <p>Two slight modifications to the previous script will give us the <a href="https://gwern.net/doc/wikipedia/2023-10-01-gwern-wikipedia-unusedacronyms-alphanumerical1to3lettertlas.txt" data-link-icon="txt" data-link-icon-type="svg">set of 21,852 unused alphanumerical acronyms</a>, adding in numbers to the generator &amp; skipping target acronyms without numbers:</p>
            <div id="cb23">
              <pre><code><span id="cb23-1"><span>## set the range of acronym lengths</span></span>
<span id="cb23-2"><span>for</span> length <span>in</span> <span>{</span><span>1</span><span>..</span><span>3</span><span>}</span></span>
<span id="cb23-3"><span>do</span></span>
<span id="cb23-4">  <span>## define a function to recursively generate acronyms</span></span>
<span id="cb23-5">  <span>generate_acronyms()</span> <span>{</span></span>
<span id="cb23-6">    <span>local</span> <span>current</span><span>=</span><span>$1</span></span>
<span id="cb23-7">    <span>local</span> <span>length</span><span>=</span><span>$2</span></span>
<span id="cb23-8">    <span>if</span> <span>((</span> <span>length</span> <span>==</span> <span>0</span> <span>))</span></span>
<span id="cb23-9">    <span>then</span></span>
<span id="cb23-10">      <span>## query the Wikipedia API</span></span>
<span id="cb23-11">      <span>response</span><span>=</span><span>$(</span><span>curl</span> <span>-s</span> <span>"https://en.wikipedia.org/w/api.php?action=query&amp;format=json&amp;titles=</span><span>$current</span><span>"</span><span>)</span></span>
<span id="cb23-12">      <span>## check if the page exists</span></span>
<span id="cb23-13">      <span>if</span> <span>[[</span> <span>$response</span> <span>==</span> <span>*</span><span>'"missing"'</span><span>*</span> <span>]]</span></span>
<span id="cb23-14">      <span>then</span></span>
<span id="cb23-15">        <span>echo</span> <span>"</span><span>$current</span><span>"</span></span>
<span id="cb23-16">      <span>fi</span></span>
<span id="cb23-17">    <span>else</span></span>
<span id="cb23-18">      <span>for</span> letter <span>in</span> <span>{</span>A<span>..</span>Z<span>}</span> <span>{</span><span>0</span><span>..</span><span>9</span><span>}</span></span>
<span id="cb23-19">      <span>do</span></span>
<span id="cb23-20">        <span>generate_acronyms</span> <span>"</span><span>$current$letter</span><span>"</span> <span>$((length</span><span>-</span><span>1</span><span>))</span></span>
<span id="cb23-21">      <span>done</span></span>
<span id="cb23-22">    <span>fi</span></span>
<span id="cb23-23">  <span>}</span></span>
<span id="cb23-24">  <span>## call the function to generate acronyms of the current length</span></span>
<span id="cb23-25">  <span>generate_acronyms</span> <span>""</span> <span>$length</span></span>
<span id="cb23-26"><span>done</span></span></code></pre>
            </div>
            <p>The first unused one is the rather shocking <span><span><strong>AA0</strong></span> AD0 AE0 AE5 AE6 AE7 AE8 AE9 AF0 AF3 AF5 AF6 AF7 AF8 AF9 AG1 AG2 AG4 AG6 AG7 AG8 AG9 AH0 AI0 AJ0 AJ1 AJ3 AJ5 AJ7 AJ8 AJ9, etc.</span> Really? <em>No one</em> has used such a short simple TLA, which would sort in front of almost everything, even ones like ‘AAA’? Apparently! Neither WP nor Google shows anything important for ‘AA0’.</p>
            <p>So, ‘AA0’ would be a good startup name, if anyone needs one.</p>
          </section>
        </section>
        <section role="doc-endnotes" id="footnotes">
          <hr>
          <ol>
            <li id="fn1" role="doc-endnote">
              <p>And “Zzzzzz” was surprisingly interesting: “it was the busiest residential telephone number in the United States, if not the world”.<a href="#fnref1" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn2" role="doc-endnote">
              <p>Acronyms can include numbers, yes, but that’s relatively unusual and it would expand the space of possibilities so much it’d not be surprising to run into an unused one quickly: if there are 10 digits, then even just (26 + 10)<sup>3</sup> = 46,656. See <a href="#unused-numerical-acronyms">the appendix</a> if you are interested in unused alphanumerical TLAs.<a href="#fnref2" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn3" role="doc-endnote">
              <p>In addition to the RLHF that everyone attributes its success to, <a href="https://en.wikipedia.org/wiki/OpenAI" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/OpenAI#bodyContent">OpenAI</a> apparently had invested in large amounts of proprietary expert-written code for common programming languages like Python.<a href="#fnref3" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn4" role="doc-endnote">
              <p>Note that I have not ablated this nor rigorously tested it with any blind comparisons; I mostly copied it from <a href="https://en.wikipedia.org/wiki/Twitter" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Twitter#bodyContent">Twitter</a> and added some additional clauses as GPT-4 outputs annoyed me. It is entirely possible that this does not help.<a href="#fnref4" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn5" role="doc-endnote">
              <p>A link could, of course, be broken and so its true target actually be ‘before’ but I put in effort to make sure all internal cross-references are valid, so the code will assume that any target not yet seen must be yet to be seen.<a href="#fnref5" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn6" role="doc-endnote">
              <p>According to both the Bash script and the Python script I ran later.<a href="#fnref6" role="doc-backlink">↩︎</a></p>
            </li>
            <li id="fn7" role="doc-endnote">
              <p>Like this:</p>
              <div id="cb19">
                <pre><code><span id="cb19-1"><span>## Create a data frame of individual letters</span></span>
<span id="cb19-2">letter_df <span>&lt;-</span> <span>data.frame</span>(<span>V1 =</span> <span>rep</span>(result<span>$</span>V1, <span>times =</span> <span>3</span>),</span>
<span id="cb19-3">                       <span>V2 =</span> <span>rep</span>(result<span>$</span>V2, <span>times =</span> <span>3</span>),</span>
<span id="cb19-4">                       <span>Missing =</span> <span>rep</span>(result<span>$</span>Missing, <span>times =</span> <span>3</span>),</span>
<span id="cb19-5">                       <span>Letter =</span> <span>c</span>(<span>substr</span>(result<span>$</span>V1, <span>1</span>, <span>1</span>), <span>substr</span>(result<span>$</span>V1, <span>2</span>, <span>2</span>),</span>
<span id="cb19-6">                                  <span>substr</span>(result<span>$</span>V1, <span>3</span>, <span>3</span>)),</span>
<span id="cb19-7">                       <span>stringsAsFactors =</span> <span>FALSE</span>)</span>
<span id="cb19-8"></span>
<span id="cb19-9"><span>## Convert 'V1' to character</span></span>
<span id="cb19-10">letter_df<span>$</span>V1 <span>&lt;-</span> <span>as.character</span>(letter_df<span>$</span>V1)</span>
<span id="cb19-11"><span>## Filter out four-letter acronyms</span></span>
<span id="cb19-12">three_letter_df <span>&lt;-</span> letter_df[<span>nchar</span>(letter_df<span>$</span>V1) <span>==</span> <span>3</span>, ]</span>
<span id="cb19-13"></span>
<span id="cb19-14"><span>## Plot</span></span>
<span id="cb19-15"><span>library</span>(ggplot2)</span>
<span id="cb19-16"><span>ggplot</span>(three_letter_df, <span>aes</span>(<span>x =</span> Letter, <span>fill =</span> Missing)) <span>+</span></span>
<span id="cb19-17">  <span>geom_bar</span>(<span>position =</span> <span>"dodge"</span>) <span>+</span></span>
<span id="cb19-18">  <span>theme_minimal</span>() <span>+</span></span>
<span id="cb19-19">  <span>labs</span>(<span>x =</span> <span>"Letter"</span>, <span>y =</span> <span>"Count"</span>, <span>fill =</span> <span>"Missing"</span>)</span></code></pre>
              </div><a href="#fnref7" role="doc-backlink">↩︎</a>
            </li>
            <li id="fn8" role="doc-endnote">
              <p>Possibly one could redo the crawl and attempt to count acronym count: an article or redirect = 1, and a disambiguation page = # of list items. Then one can do a <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Parametric_bootstrap" data-link-icon="wikipedia" data-link-icon-type="svg" data-url-html="https://en.m.wikipedia.org/wiki/Bootstrapping_(statistics)#bodyContent">parametric bootstrap</a> by treating that as an empirical table of frequencies to fit a binomial to by Bayes or maximum-likelihood, and construct new simulated datasets by sampling 1 binomial sample of each possible TLA with its estimated probability (a count of 1 would be <em>P</em> ~ 0.5 and so on).<a href="#fnref8" role="doc-backlink">↩︎</a></p>
            </li>
          </ol>
        </section>
        <section id="link-bibliography-section">
          <h2><a href="#link-bibliography-section" title="Link to section: § 'Link Bibliography'">Link Bibliography</a></h2><!-- NOTE: In theory, '.collapse' on a '<h1>' is redundant with the '<section>'; but added to parallel Pandoc-generated headers which set all attributes/classes on both. -->
          <a id="link-bibliography" href="https://gwern.net/metadata/annotation/link-bibliography/%252Ftla.html" title="Bibliography of links cited in this page (forward citations). Lazily-transcluded version at footer of page for easier scrolling.">[bibliography of links/references used in page]</a>
        </section>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Happy 1700M Epoch Second (185 pts)]]></title>
            <link>https://www.epochconverter.com/</link>
            <guid>38270582</guid>
            <pubDate>Tue, 14 Nov 2023 22:14:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epochconverter.com/">https://www.epochconverter.com/</a>, See on <a href="https://news.ycombinator.com/item?id=38270582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="contentwrapper">

<div id="clocktext"><p>The current <a href="https://www.epochconverter.com/clock">Unix epoch time</a> is&nbsp;
</p><p>1699995752</p>
</div>

<h2 id="efhead">Convert epoch to human-readable date and vice versa</h2>

<br>

&nbsp;
<p><span id="preferencelink"></span><br>
Press <kbd>c</kbd> to clear all forms.</p>
<h2 id="brhead">Epoch dates for the start and end of the year/month/day</h2>

<br>
<h2 id="tchead">Convert seconds to days, hours and minutes</h2>


<h2>What is epoch time?</h2>
<p>The <b>Unix epoch</b> (or <b>Unix time</b> or <b>POSIX time</b> or <b>Unix timestamp</b>) is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (in ISO 8601: 1970-01-01T00:00:00Z).
Literally speaking the epoch is Unix time 0 (midnight 1/1/1970), but 'epoch' is often used as a synonym for Unix time.
Some systems store epoch dates as a signed 32-bit integer, which might cause problems on January 19, 2038 (known as the Year 2038 problem or Y2038).
The converter on this page converts timestamps in seconds (10-digit), milliseconds (13-digit) and microseconds (16-digit) to readable dates.</p>
<div><table><thead>
<tr><th>Human-readable time&nbsp;</th><th>Seconds</th></tr>
</thead><tbody>
<tr><td>1 hour</td><td>3600 seconds</td></tr>
<tr><td>1 day</td><td>86400 seconds</td></tr>
<tr><td>1 week</td><td>604800 seconds</td></tr>
<tr><td>1 month (30.44 days)&nbsp;</td><td>2629743 seconds</td></tr>
<tr><td>1 year (365.24 days)&nbsp;</td><td>&nbsp;31556926 seconds</td></tr>
</tbody></table></div>
<h2 id="code">How to get the current epoch time in ...</h2>
<table>
<tbody><tr><td>PHP</td><td><code>time()</code> <a href="https://www.epochconverter.com/programming/php">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import time; time.time()</code> <a target="_blank" href="https://docs.python.org/3/library/time.html" title="Source"><span>Source</span></a></td></tr>
<tr><td>Ruby</td><td><code>Time.now</code> (or <code>Time.new</code>). To display the epoch: <code>Time.now.to_i</code></td></tr>
<tr><td>Perl</td><td><code>time</code> <a href="https://www.epochconverter.com/programming/perl">More Perl</a></td></tr>
<tr><td>Java</td><td><code>long epoch = System.currentTimeMillis()/1000;</code> Returns epoch in seconds.</td></tr>
<tr><td>C#</td><td><code>DateTimeOffset.Now.ToUnixTimeSeconds()</code> (.NET Framework 4.6+/.NET Core), older versions: <code>var epoch = (DateTime.UtcNow - new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc)).TotalSeconds;</code></td></tr>
<tr><td>Objective-C</td><td><code>[[NSDate date] timeIntervalSince1970];</code> (returns double) or <code>NSString *currentTimestamp = [NSString stringWithFormat:@"%f", [[NSDate date] timeIntervalSince1970]];</code></td></tr>
<tr><td>C++11</td><td><code>double now = std::chrono::duration_cast&lt;std::chrono::seconds&gt;(std::chrono::system_clock::now().time_since_epoch()).count();</code></td></tr>
<tr><td>Lua</td><td><code>epoch = os.time([date])</code></td></tr>
<tr><td>VBScript/ASP</td><td><a href="https://www.epochconverter.com/programming/#asp">See the examples</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", _NowCalc())</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimetoUnix(Now);</code> Tested in Delphi 2010.</td></tr>
<tr><td>R</td><td><code>as.numeric(Sys.time())</code></td></tr>
<tr><td>Erlang/OTP</td><td><code>erlang:system_time(seconds).</code> (version 18+), older versions: <code>calendar:datetime_to_gregorian_seconds(calendar:universal_time())-719528*24*3600.</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(now())</code> <a href="https://www.epochconverter.com/programming/mysql">More MySQL examples</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM now());</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s', 'now');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT (CAST(SYS_EXTRACT_UTC(SYSTIMESTAMP) AS DATE) - TO_DATE('01/01/1970','DD/MM/YYYY')) * 24 * 60 * 60 FROM DUAL;</code></td></tr>
<tr><td>SQL Server</td><td><code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', GETUTCDATE())</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_current') FROM sysmaster:sysdual;</code></td></tr>
<tr><td>JavaScript</td><td><code>Math.floor(new Date().getTime()/1000.0)</code> The getTime method returns the time in milliseconds.</td></tr>
<tr><td>Visual FoxPro</td><td><code>DATETIME() - {^1970/01/01 00:00:00}</code> Warning: time zones not handled correctly</td></tr>
<tr><td>Go</td><td><code>time.Now().Unix()</code> <a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">More Go </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>&lt;cfset epochTime = left(getTickcount(), 10)&gt;</code></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock seconds</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s</code></td></tr>
<tr><td>Solaris</td><td><code>/usr/bin/nawk 'BEGIN {print srand()}'</code> Solaris doesn't support <i>date +%s</i>, but the default seed value for nawk's random-number generator is the number of seconds since the epoch.</td></tr>
<tr><td>PowerShell</td><td><code>[int][double]::Parse((Get-Date (get-date).touniversaltime() -UFormat %s))</code></td></tr>
<tr><td>Other OS's </td><td>Command line: <code>perl -e "print time"</code> (If Perl is installed on your system)</td></tr>
</tbody></table>

<h2>Convert from human-readable date to epoch</h2>
<table>
<tbody><tr><td>PHP</td><td><code>strtotime("15 November 2018")</code> (converts most English date texts) or:<br><code>date_create('11/15/2018')-&gt;format('U')</code> (using DateTime class) <a href="https://www.epochconverter.com/programming/php#date2epoch">More PHP</a></td></tr>
<tr><td>Python</td><td><code>import calendar, time; calendar.timegm(time.strptime('2000-01-01 12:34:00', '%Y-%m-%d %H:%M:%S'))</code></td></tr>
<tr><td>Ruby</td><td><code>Time.local(<i>year</i>, <i>month</i>, <i>day</i>, <i>hour</i>, <i>minute</i>, <i>second</i>, <i>usec</i> )</code> (or <code>Time.gm</code> for GMT/UTC input). To display add <code>.to_i</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>long epoch = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").parse("01/01/1970 01:00:00").getTime() / 1000;</code> Timestamp in seconds, remove '/1000' for milliseconds.</td></tr>
<tr><td>VBScript/ASP</td><td><code>DateDiff("s", "01/01/1970 00:00:00", <i>time field</i>)</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateDiff('s', "1970/01/01 00:00:00", "YYYY/MM/DD HH:MM:SS")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>Epoch := DateTimeToUnix(StrToDateTime(myString));</code></td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>R</td><td><code>as.numeric(as.POSIXct("YYYY-MM-dd HH:mm:ss", tz = "GMT", origin="1970-01-01"))</code> The origin parameter is optional</td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Rust</td><td>
<code>SystemTime::now().duration_since(SystemTime::UNIX_EPOCH)</code> <a rel="nofollow" target="_blank" href="https://doc.rust-lang.org/std/time/struct.SystemTime.html"></a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>int(parseDateTime(datetime).getTime()/1000);</code></td></tr>
<tr><td>MySQL</td><td><code>SELECT unix_timestamp(<i>time</i>)</code> Time format: YYYY-MM-DD HH:MM:SS or YYMMDD or YYYYMMDD<br><a href="https://www.epochconverter.com/programming/mysql">More on using Epoch timestamps with MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td><code>SELECT extract(epoch FROM date('2000-01-01 12:34'));</code><br>With timestamp: <code>SELECT EXTRACT(EPOCH FROM TIMESTAMP WITH TIME ZONE '2018-02-16 20:38:40-08');</code><br>With interval: <code>SELECT EXTRACT(EPOCH FROM INTERVAL '5 days 3 hours');</code></td></tr>
<tr><td>SQLite</td><td><code>SELECT strftime('%s',<i>timestring</i>);</code></td></tr>
<tr><td>SQL Server</td><td> <code>SELECT DATEDIFF(s, '1970-01-01 00:00:00', <i>time field</i>)</code></td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date +%s -d"Jan 1, 1980 00:00:01"</code> Replace '-d' with '-ud' to input in GMT/UTC time.</td></tr>
</tbody></table>
<h2>Convert from epoch to human-readable date</h2>
<table>
<tbody><tr><td>PHP</td><td><code>date(<i>output format</i>, <i>epoch</i>);</code> Output format example: 'r' = RFC 2822 date, <a href="https://www.epochconverter.com/programming/php#epoch2date">more PHP examples</a></td></tr>
<tr><td>Python</td><td><code>import time; time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.localtime(<i>epoch</i>))</code>
Replace time.localtime with time.gmtime for GMT time. Or using datetime:
<code>import datetime; datetime.datetime.utcfromtimestamp(<i>epoch</i>).replace(tzinfo=datetime.timezone.utc)</code></td></tr>
<tr><td>Ruby</td><td><code>Time.at(<i>epoch</i>)</code></td></tr>
<tr><td>C#</td><td><code>private string epoch2string(int epoch) {<br>
return new DateTime(1970, 1, 1, 0, 0, 0, DateTimeKind.Utc).AddSeconds(epoch).ToShortDateString(); }</code></td></tr>
<tr><td>Perl</td><td>Use the <a href="https://www.epochconverter.com/programming/perl">Perl Epoch routines</a></td></tr>
<tr><td>Java</td><td><code>String date = new java.text.SimpleDateFormat("MM/dd/yyyy HH:mm:ss").format(new java.util.Date (<i>epoch</i>*1000));</code> Epoch in seconds, remove '*1000' for milliseconds.</td></tr>
<tr><td>Lua</td><td><code>datestring = os.date([format[,epoch]])</code></td></tr>
<tr><td>VBScript/ASP</td><td><code>DateAdd("s", <i>epoch</i>, "01/01/1970 00:00:00")</code> <a href="https://www.epochconverter.com/programming/#asp">More ASP</a></td></tr>
<tr><td>AutoIT</td><td><code>_DateAdd("s", $EpochSeconds , "1970/01/01 00:00:00")</code></td></tr>
<tr><td title="Embarcadero Delphi">Delphi</td><td><code>myString := DateTimeToStr(UnixToDateTime(Epoch));</code> Where Epoch is a signed integer.</td></tr>
<tr><td>C</td><td>Use the <a href="https://www.epochconverter.com/programming/c">C Epoch Converter routines</a></td></tr>
<tr><td>Objective-C</td><td><code>NSDate * myDate = [NSDate dateWithTimeIntervalSince1970:<i>epoch</i>]; NSLog(@"%@", date);</code></td></tr>
<tr><td>R</td><td><code>as.POSIXct(epoch, origin="1970-01-01", tz="GMT")</code></td></tr>
<tr><td>Go</td><td><a rel="nofollow" target="_blank" href="https://play.golang.org/p/6h0A0WPxtq">Example code </a></td></tr>
<tr><td>Adobe ColdFusion</td><td><code>DateAdd("s",epoch,"1/1/1970");</code></td></tr>
<tr><td>MySQL</td><td><code>FROM_UNIXTIME(<i>epoch</i>, <i>optional output format</i>)</code> Default output format is YYY-MM-DD HH:MM:SS. If you need support for negative timestamps: <code>DATE_FORMAT(DATE_ADD(FROM_UNIXTIME(0), interval -315619200 second),"%Y-%m-%d")</code> (replace -315619200 with epoch) <a href="https://www.epochconverter.com/programming/mysql">More MySQL</a></td></tr>
<tr><td>PostgreSQL</td><td>PostgreSQL version 8.1 and higher: <code>SELECT to_timestamp(<i>epoch</i>);</code> <a rel="nofollow" target="_blank" href="https://www.postgresql.org/docs/current/static/functions-formatting.html" title="Source"><span>Source</span></a> Older versions: <code>SELECT TIMESTAMP WITH TIME ZONE 'epoch' + <i>epoch</i> * INTERVAL '1 second';</code> </td></tr>
<tr><td>SQLite</td><td><code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch');</code> or local timezone: <code>SELECT datetime(<i>epoch_to_convert</i>, 'unixepoch', 'localtime');</code></td></tr>
<tr><td>Oracle PL/SQL</td><td><code>SELECT to_date('01-JAN-1970','dd-mon-yyyy')+(1526357743/60/60/24) from dual</code><br>Replace 1526357743 with epoch.</td></tr>
<tr><td>SQL Server</td><td> <code>DATEADD(s, <i>epoch</i>, '1970-01-01 00:00:00')</code></td></tr>
<tr><td>IBM Informix</td><td><code>SELECT dbinfo('utc_to_datetime',<i>epoch</i>) FROM sysmaster:sysdual;</code></td></tr>
<tr><td>Microsoft Excel / LibreOffice Calc</td><td> <code>=(A1 / 86400) + 25569</code> Format the result cell for date/time, the result will be in GMT time (A1 is the cell with the epoch number). For other time zones: =((A1 +/- <a href="https://www.epochconverter.com/timezones">time zone adjustment</a>) / 86400) + 25569.</td></tr>
<tr><td>Crystal Reports</td><td> <code>DateAdd("s", {EpochTimeStampField}-14400, #1/1/1970 00:00:00#)</code> -14400 used for Eastern Standard Time. See <a href="https://www.epochconverter.com/timezones">Time Zones</a>.</td></tr>
<tr><td>JavaScript</td><td>Use the <a href="https://www.epochconverter.com/programming/#javascript">JavaScript Date object</a></td></tr>
<tr><td>Tcl/Tk</td><td><code>clock format 1325376000</code> <a rel="nofollow" target="_blank" href="https://www.tcl.tk/man/tcl8.6/TclCmd/clock.htm" title="Documentation"><span>Documentation</span></a></td></tr>
<tr><td>MATLAB</td><td><code>datestr(719529+TimeInSeconds/86400,'dd-mmm-yyyy HH:MM:SS')</code></td></tr>
<tr><td>IBM PureData System for Analytics</td><td><code>select 996673954::int4::abstime::timestamp;</code></td></tr>
<tr><td>Unix/Linux Shell</td><td><code>date -d @1520000000</code> Replace 1520000000 with your epoch, needs recent version of 'date'. Replace '-d' with '-ud' for GMT/UTC time.</td></tr>
<tr><td>Mac OS X</td><td><code>date -j -r 1520000000</code></td></tr>
<tr><td>PowerShell</td><td><code>Function get-epochDate ($epochDate)
{ [timezone]::CurrentTimeZone.ToLocalTime(([datetime]'1/1/1970').AddSeconds($epochDate)) }</code>, then use: <code>get-epochDate 1520000000</code>. Works for Windows PowerShell v1 and v2</td></tr>
<tr><td>Other OS's</td><td>Command line: <code>perl -e "print scalar(localtime(<i>epoch</i>))"</code> (If Perl is installed) Replace 'localtime' with 'gmtime' for GMT/UTC time.</td></tr>
</tbody></table>
<p><br>Thanks to everyone who sent me corrections and updates!</p>
<p>More date related programming examples: <a href="https://www.epochconverter.com/weeknumbers">What's the current week number?</a> - <a href="https://www.epochconverter.com/daynumbers">What's the current day number?</a></p>
<p><i>Please note:</i> All tools on this page are based on the date &amp; time settings of <i>your computer</i> and use JavaScript to convert times. Some browsers use the current DST (Daylight Saving Time) rules for all dates in history. JavaScript does not support <a target="_blank" href="https://en.wikipedia.org/wiki/Leap_second" rel="nofollow">leap seconds</a>.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is the reversal curse in LLMs real? (196 pts)]]></title>
            <link>https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/</link>
            <guid>38270552</guid>
            <pubDate>Tue, 14 Nov 2023 22:12:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/">https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/</a>, See on <a href="https://news.ycombinator.com/item?id=38270552">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>A recent paper <a href="https://arxiv.org/abs/2309.12288">The Reversal Curse</a> points out an apparent failure in large large language models like GPT-4.</p>



<p>From the abstract:</p>



<blockquote>
<p>We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form “<em>A&nbsp;</em>is&nbsp;<em>B</em>”, it will not automatically generalize to the reverse direction “<em>B&nbsp;</em>is&nbsp;<em>A</em>”. This is the&nbsp;Reversal Curse. For instance, if a model is trained on “Olaf Scholz was the ninth Chancellor of Germany”, it will not automatically be able to answer the question, “Who was the ninth Chancellor of Germany?”. Moreover, the likelihood of the correct answer (“Olaf Scholz”) will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if “<em>A&nbsp;</em>is&nbsp;<em>B</em>” occurs, “<em>B&nbsp;</em>is&nbsp;<em>A</em>” is more likely to occur).</p>
</blockquote>



<p>This is a very big claim. While my intuition about large language models, especially GPT-4, is that they can do some kind of backwards generalization, I wanted to explore this paper further. (We’ll also get to the problem with the example in their abstract.)</p>



<h2>The Network in Neural Networks</h2>



<p>When the paper authors point out that you’re far less likely to get an accurate response to “Who is the son of Mary Lee Pfeiffer?” (Tom Cruise) than if you ask “Who is Tom Cruise’s mother?” (Mary Lee Pfeiffer) this seems to me more like an explanation of how neural networks function than a model’s inability to deduce B is A.</p>



<p>If you look at Google Search as a proxy for training data frequency:</p>



<p>Mary Lee Pfeiffer has approximately <strong>46,600</strong> results:</p>



<figure><img data-attachment-id="659" data-permalink="https://andrewmayne.com/?attachment_id=659" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png" data-orig-size="632,280" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.31.09-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=632" width="632" height="280" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=632" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png 632w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.31.09-am.png?w=300 300w" sizes="(max-width: 632px) 100vw, 632px"></figure>



<p>Whereas her son Tom Cruise, has approximately <strong>66,800,000</strong>&nbsp;results:</p>



<figure><img data-attachment-id="663" data-permalink="https://andrewmayne.com/?attachment_id=663" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png" data-orig-size="766,348" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.34.03-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=766" width="766" height="348" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=766" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png 766w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.34.03-am.png?w=300 300w" sizes="(max-width: 766px) 100vw, 766px"></figure>



<p>By this metric, Mary Lee Pfeiffer has 0.0698% of the results as her son. I’m not sure the model would have any idea who she is outside of the context of her son.</p>



<p>If you search Wikipedia to see how many times “Mary Lee Pfeiffer” is mentioned. It turns out “Mary Lee Pfeiffer” has zero mentions in Wikipedia:</p>



<figure><img data-attachment-id="665" data-permalink="https://andrewmayne.com/?attachment_id=665" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png" data-orig-size="2628,1174" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.46.55-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024" loading="lazy" width="1024" height="457" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=2046 2046w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.46.55-am.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Which is interesting and reveals a limitation in that example. Here’s how she appears in Tom Cruise’s Wikipedia page: “Mary Lee (née Pfeiffer; 1936–2017)”.</p>



<figure><img data-attachment-id="667" data-permalink="https://andrewmayne.com/?attachment_id=667" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png" data-orig-size="1596,172" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-11.52.42-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024" loading="lazy" width="1024" height="110" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-11.52.42-am.png 1596w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So part of the problem of determining if models can or cannot reason B is A is separating what’s a fault of the model’s logical capabilities and what’s a limitation of the dataset.</p>



<p>If you start a query with “Mary Lee Pfeiffer”, you’re not going to get very far because neural networks aren’t equidistant grids of points (besides the fact that she may not appear very often under that version of her name.) They’re networks of nodes, some with many connections, some with few. One of the ways you optimize large models is by pruning off weakly connected regions. This may come at the expense of destroying B is A relationships for weakly represented entities.</p>



<p>This isn’t a failure of neural networks. It’s a feature. It’s why you’re not flooded with every single memory and experience you’ve ever had every moment.</p>



<p>In different terms: Not all information has its own node (or neuron). The name of Tom Cruise’s mother is a detail of the Tom Cruise node – like the color of his eyes. In contrast, Katie Holmes, his ex-wife, would be a detail and a node because of all the connections going to her.</p>



<p>How do we know if something is a node or just a detail? If the model doesn’t recognize a detail, it’s probably not a node.</p>



<p>Saying that models can’t automatically generalize from B to A when B is vastly underrepresented in the dataset feels rather obvious and not so much a curse as a description of how neural nets function. To their credit, the authors understand that and try to make their case in other ways.</p>



<p>What about a person that should be well-represented in the dataset and a datapoint almost always appearing in the B position? Their key example from the abstract is:  <strong>Who was the ninth Chancellor of Germany?</strong></p>



<p>This involves a well-documented person, Olaf Scholz (A) and a datapoint about him, being the ninth Chancellor of Germany, (B) that should appear frequently in training data.</p>



<p>Here’s the latest version of GPT-4 trying to answer “Who was the ninth Chancellor of Germany?” and failing:</p>



<figure><img data-attachment-id="678" data-permalink="https://andrewmayne.com/?attachment_id=678" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png" data-orig-size="1956,1040" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.09.48-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024" loading="lazy" width="1024" height="544" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.09.48-pm.png 1956w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Okay, except there’s a catch. It’s a trick question. Asking a model that was trained before he was elected this question would be pointless and asking a model that finished training while he’s still Chancellor is inviting it to hallucinate. “Was” and “is” have different connotations. (Additionally, models like GPT-4 are stateless – in that they’re frozen in time from when their training stopped and they’re understanding of text may be limited to what related items reference about it.  “Is” is usually better than “was”.)</p>



<p>The question asks who “<strong>was</strong>“, implying a past tense (even though we’re asking about a current Chancellor.) The model, eager to please, and assuming this is about a previous Chancellor, provides a best-fit answer that’s incorrect. </p>



<p>However, when you turn “<strong>was</strong>” to “<strong>is</strong>” you will frequently get this response which refutes the claim in the abstract that the answer “will not be higher than for a random name”.</p>



<figure><img data-attachment-id="690" data-permalink="https://andrewmayne.com/?attachment_id=690" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png" data-orig-size="1594,1054" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.14.36-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024" loading="lazy" width="1024" height="677" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.36-pm.png 1594w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I say it’s likely to get the right answer because sometimes it doesn’t (but still succeeds at rate far above chance) because it’s still a trick question. There have been 9 (and one acting) Chancellors of the Federal Republic of Germany…but there have been <strong>36 people who have held the office of Chancellor in Germany </strong>if you include prior governments.</p>



<p>Because of this ambiguity, the model is still trying to guess what you mean. Often it gets it wrong, sometimes not. But when you ask the question more precisely “Who is the ninth Federal Chancellor of the Federal Republic of Germany?” it gets it right a majority of the time:</p>



<figure><img data-attachment-id="692" data-permalink="https://andrewmayne.com/?attachment_id=692" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png" data-orig-size="1578,1040" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-1.14.59-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024" loading="lazy" width="1024" height="674" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-1.14.59-pm-1.png 1578w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>To see if this isn’t specific to Olaf Scholz, let’s ask “Who is the seventh Federal Chancellor of the Federal Republic of Germany?”:<br></p>



<figure><img data-attachment-id="737" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-10-48-06-am/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png" data-orig-size="1644,1008" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-10.48.06-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024" loading="lazy" width="1024" height="627" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-10.48.06-am.png 1644w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct again. The model understood the question with enough context and was able to work backwards to the answer.</p>



<p>There’s an argument to be made that the model “should know” what you mean when you ask the question, but that means asking it to be imprecise and/or hallucinate. If you want to know who is the ninth chancellor does that mean since the office was created during the Holy Roman Empire? Or since formation of the Federal Republic of Germany? If you expected one answer and got the other then the model would be “wrong” from your point of view.</p>



<p>“Is” and “was” phrasing is a limitation that can likely be eliminated by preprocessing the text that goes into training. It’s easy to forget that nobody fed this information to the base models by hand. The bulk of what it learned was from generalizing across millions of bits of information. If most of that text refers to modern politicial leaders in the present tense, then that’s how the model will likely think of them. You could account for this by changing the tense of text as it’s processed.</p>



<p>Regardless, we can see that GPT-4 can easily go from B to A in that example when the question is posed unambiguously. The counter-explanation might be that without access to the dataset, it’s hard to know if this is proof that GPT-4 can reason from B to A, or that there might be a lot of data in the set along the lines “The ninth Chancellor of Germany is Olaf Scholz”. We can test for the likelihood of that phrasing with a Google search.</p>



<figure><img data-attachment-id="694" data-permalink="https://andrewmayne.com/?attachment_id=694" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png" data-orig-size="1308,164" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-3.31.20-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024" loading="lazy" width="1024" height="128" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-3.31.20-pm.png 1308w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><br>There are zero English or German results. That’s not to say it couldn’t be in the training data, just that it’s not a common phrase – yet the model got it correct.</p>



<div><p>Because of the opaqueness of the training data, the authors decided to train a Llama-1 and a GPT-3 model (Davinci-002) on synthetic data of fake celebrities and achievements. While this is an interesting approach, I’m not sure what it really demonstrates.</p><p>In their training data they have 30 sets of information about 30 fake people for a total of 900 information pairs. I don’t know if that’s anywhere enough data to create a strong A to B and B to A correlation. Well-known entities in neural networks could have <em>tens of thousands</em> of connections. A failure to make a B is A connection may or may not prove anything other than neural networks function differently than knowledge graphs – which nobody is disputing.</p></div>



<p>In fairness, it’s also worth pointing out here that they’re making the claim that <strong>the reversal curse only applies to training and fine-tuning and not in-context</strong> –&nbsp;i.e., putting all your information inside a prompt. They point out in a footnote that you can put A to B data in a prompt and GPT-4 will make B to A connections just fine. Unfortunately, this was lost on many of the people covering the pre-print. </p>



<p><strong>The claim that GPT-4 can’t make B to A generalizations is false. And not what the authors were claiming. </strong>They were talking about these kinds of generalizations from pre and post <strong>training</strong>. </p>



<p>As a side note: I want to point out that I’m not aware of any examples of capabilities that can be done with prompting a model like GPT-4 that it can’t be trained for. This is why I’m a little skeptical.</p>



<p>According to my understanding of their results, their fine-tuned GPT-3 models “completely fail when the order is reversed” for the A to B data.</p>



<figure><img data-attachment-id="695" data-permalink="https://andrewmayne.com/?attachment_id=695" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png" data-orig-size="2848,856" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.37.04-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024" loading="lazy" width="1024" height="307" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=2043 2043w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.37.04-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is interesting. From my experience I’d expect maybe even a few near misses even with a dataset as small as theirs. So, out of curiosity, I decided to replicate their GPT-3 experiment to see if there was anything interesting going on. And there was…</p>



<h2>Model training is a dark art</h2>



<p>I’ve been playing around with fine-tuning LLM models for years and still don’t have any hard and fast one-size-fits-all rules to apply. Every dataset lends itself to a specific way of training. And what works with one model may not work with another. I do have some general guidelines I follow. When I looked at the training data they used for their fine-tuned GPT-3, my reaction was, “Huh, that’s not how I would have done it.” </p>



<p>I’m not saying they were wrong to do it the way they did (I’ll say that later on), just that there’s more than one way to do it, and this wouldn’t have been my approach.</p>



<p>In fairness, to fine-tune Davinci-002 the OpenAI documentation shows this example. (The newer models use a ChatGPT threaded conversation format.)</p>



<figure><img data-attachment-id="696" data-permalink="https://andrewmayne.com/?attachment_id=696" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png" data-orig-size="1580,282" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.48.26-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024" loading="lazy" width="1024" height="182" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.48.26-pm.png 1580w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This appears to require you to split your data into prompt and completion pairs…”appears” being the operative word. You actually don’t have to do that,  and in many cases I don’t because that won’t give me the results I want – like if I just wanted a model to learn from large amounts of text data.</p>



<p>This format is great for Q&amp;A style data, but not for situations where you might want to ask questions about the “Q” part as well…<em>or have the model learn B is A…</em></p>



<figure><img data-attachment-id="697" data-permalink="https://andrewmayne.com/?attachment_id=697" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png" data-orig-size="3194,330" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-4.45.13-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024" loading="lazy" width="1024" height="105" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=1016 1016w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=2033 2033w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-4.45.13-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Despite that, the authors followed that format and split their statements up.</p>



<p>Text like this:</p>



<pre><code>Daphne Barrington, known far and wide for being the acclaimed director of the virtual reality masterpiece, "A Journey Through Time."</code></pre>



<p>Became:</p>



<pre><code>"prompt": "Daphne Barrington, known far and wide for being"

"completion": " the acclaimed director of the virtual reality masterpiece, \"A Journey Through Time.\".</code></pre>



<p>What difference does that make? It depends on what you want your outcome to be.</p>



<p>Against my own instincts, I used their examples from their GitHub repo exactly as they formatted it and fine-tuned a Davinci-002 model.</p>



<p>When I use the A to B queries they provided I got correct answers (as they predicted) even down to the punctuation quirks:</p>



<figure><img data-attachment-id="699" data-permalink="https://andrewmayne.com/?attachment_id=699" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png" data-orig-size="1812,424" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.01.39-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024" loading="lazy" width="1024" height="239" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.01.39-pm.png 1812w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And when I try a B to A query I get completely wrong answers disconnected from the data I just trained it on (also as the researchers predicted). Here it claims Tim Cook is the director.</p>



<figure><img data-attachment-id="700" data-permalink="https://andrewmayne.com/?attachment_id=700" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png" data-orig-size="1830,442" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.04.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024" loading="lazy" width="1024" height="247" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.04.32-pm.png 1830w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>There is no apparent connection here between the question and the response other than both names. The researchers say the name is totally random. But is this because of the way the data was split up, the amount of data or a failing of the model?</p>



<p>When you divide data into prompt and completion pairs and the completions never reference the prompts or even hint at it, you’ve successfully trained a prompt completion A is B model but not one that will readily go from B is A.</p>



<p>“<em>LLMs trained on “A is B” fail to learn “B is A” when the training date is split into prompt and completion pairs</em>” isn’t a catchy title, but that’s all we’ve seen so far. </p>



<p>What happens if you train the model with just text and not split it up? Maybe not a lot with just 30 examples per person, but maybe something…</p>



<p>So how do you train on the entire text when the OpenAI instructions tell you to put your data into prompt and completion pairs? </p>



<p>You ignore the instructions. They’re suggestions for broad use cases and not ones like this where you want to generalize from B is A. This is what you do:</p>



<p>Look closely…</p>



<figure><img data-attachment-id="701" data-permalink="https://andrewmayne.com/?attachment_id=701" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png" data-orig-size="4124,838" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.17.35-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024" loading="lazy" width="1024" height="208" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=2048 2048w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.17.35-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Closer…</p>



<figure><img data-attachment-id="702" data-permalink="https://andrewmayne.com/?attachment_id=702" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png" data-orig-size="2528,654" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.18.20-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024" loading="lazy" width="1024" height="264" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=2041 2041w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.20-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Even closer…</p>



<figure><img data-attachment-id="703" data-permalink="https://andrewmayne.com/?attachment_id=703" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png" data-orig-size="304,50" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.18.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=304" loading="lazy" width="304" height="50" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=304" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png 304w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.18.32-pm.png?w=300 300w" sizes="(max-width: 304px) 100vw, 304px"></figure>



<p>That’s right. You leave the prompt EMPTY…. All the text goes into “completion”. It’s one less step than the researchers took for training their model. Some might say it’s downright lazy. But it’s how we roll.</p>



<div><p>So what happens when we fine-tune a Davinci-002 model on their data formatted like this? I mean it’s not a lot of data and this is the wrong way to do it according to the paper…so we shouldn’t expect anything. Right?</p><p>Let’s start with a simple A to B question:</p></div>



<figure><img data-attachment-id="707" data-permalink="https://andrewmayne.com/?attachment_id=707" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png" data-orig-size="1828,402" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.29.10-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024" loading="lazy" width="1024" height="225" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.29.10-pm.png 1828w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Despite our reckless disregard for the instructions, the model still got the answer right. Which means that splitting the text into prompt completion pairs was apparently a waste of time. A is B works great. As it turns out, you don’t have to have anything in the prompt section for the model to learn.</p>



<p>Okay, but what about B is A? This is why we’re here. Let’s ask the same question as before that got us “Tim Cook”:</p>



<figure><img data-attachment-id="708" data-permalink="https://andrewmayne.com/?attachment_id=708" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png" data-orig-size="1822,416" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.36.09-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024" loading="lazy" width="1024" height="233" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=1020 1020w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.36.09-pm.png 1822w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Wrong again. The correct fake answer is “Daphne Barrington”.  It looks like leaving the data intact was also pointless. </p>



<p>I mean we didn’t even get a famous name this time. Where did it even get such a silly name like “Giselle Whitmore”? It only has like 8 results on Google.</p>



<p>Although something about it feels familiar…I can’t quite place it…</p>



<p>Wait a second…</p>



<figure><img data-attachment-id="709" data-permalink="https://andrewmayne.com/?attachment_id=709" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png" data-orig-size="4120,830" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.38.55-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024" loading="lazy" width="1024" height="206" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=2045 2045w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.38.55-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Enhance…</p>



<figure><img data-attachment-id="710" data-permalink="https://andrewmayne.com/?attachment_id=710" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png" data-orig-size="1968,846" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.39.04-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024" loading="lazy" width="1024" height="440" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.04-pm.png 1968w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Even more…</p>



<figure><img data-attachment-id="711" data-permalink="https://andrewmayne.com/?attachment_id=711" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png" data-orig-size="928,226" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-5.39.14-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=928" loading="lazy" width="928" height="226" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=928" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png 928w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-5.39.14-pm.png?w=768 768w" sizes="(max-width: 928px) 100vw, 928px"></figure>



<p>The completely random wrong answer isn’t so random after all. Unlike Tim Cook, Timothy Leary and all the other incorrect ones I got with from splitting the text into prompt and completion pairs, If I ask the empty prompt model the list of questions from the text examples in the GitHub repo I get wrong names…<strong>but all of the first names are from the training.</strong> I also frequently get full names from the training data. Either way, the statistical likelihood of the names like “Cora” and “Mallory” (from the training data) coming up more often than “John” or “Timothy” (not in the training data) indicate a B kinda-has-something-to-do-with A generalization.</p>



<p>Is this recency bias from the training? Maybe. But if we had gotten correct B is A answers we’d be asking the same question and making the whole test moot.</p>



<p>I think this proves there’s a fuzzy kind of matching going on that improves with more data (you know a neural network.) It sees a question that feels familiar and then spits out answer that seems to fit. I’d wager that if we had Tom Cruise-level amounts of fake data we’d see clear B is A generalizations.</p>



<p>As mentioned before, It’s important to keep in mind ChatGPT and GPT-4 can do B is A reasoning. The researchers don’t dispute that. They’re arguing that models can’t do it from data they train on. </p>



<p>For fun, here’s GPT-4 getting 100% correct on the first ten questions from the testing data when we shove it all into a prompt context:</p>



<figure><img data-attachment-id="712" data-permalink="https://andrewmayne.com/?attachment_id=712" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png" data-orig-size="1966,1774" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-2.10.55-am" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024" loading="lazy" width="1024" height="923" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-2.10.55-am.png 1966w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Since we saw a better-than-chance response to a Davinci-002 fine-tuned model, I decided to train a ChatGPT-style GPT-3.5-Turbo model using threaded message data. If the empty prompt bothered you, brace yourself:</p>



<figure><img data-attachment-id="714" data-permalink="https://andrewmayne.com/?attachment_id=714" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png" data-orig-size="1184,320" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.03.07-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024" loading="lazy" width="1024" height="276" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=1021 1021w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.03.07-pm.png 1184w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>No system message. No user content. Just the assistant spitting facts.</p>



<p>So the output from this has to be complete garbage, right? Wrong prompt style, no message, too few examples, just raw dirty text….</p>



<p>Let’s try an A to B on the new model:</p>



<figure><img data-attachment-id="719" data-permalink="https://andrewmayne.com/?attachment_id=719" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png" data-orig-size="1616,640" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.08.32-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024" loading="lazy" width="1024" height="405" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.08.32-pm.png 1616w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct. So leaving all that other stuff blank didn’t bring down ChatGPT. How about a B is A?</p>



<figure><img data-attachment-id="715" data-permalink="https://andrewmayne.com/?attachment_id=715" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png" data-orig-size="1638,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.05.10-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1024" loading="lazy" width="1024" height="403" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=1022 1022w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.05.10-pm.png 1638w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p> Wrong. Ethan Mullins? What? Hmmm….let’s go look at the training data….</p>



<figure><img data-attachment-id="716" data-permalink="https://andrewmayne.com/?attachment_id=716" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png" data-orig-size="1148,214" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.06.59-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024" loading="lazy" width="1024" height="190" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=1019 1019w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.59-pm.png 1148w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<figure><img data-attachment-id="718" data-permalink="https://andrewmayne.com/?attachment_id=718" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png" data-orig-size="1172,218" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-6.06.42-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024" loading="lazy" width="1024" height="190" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-6.06.42-pm.png 1172w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>So, the first and last names come from the training data. Which isn’t explained by chance. Just like our lazily trained Davinci-002. The model wanted to say a name that fit. It missed the bullseye but knew where the side of the barn was.</p>



<p><strong>What does this mean?</strong></p>



<p>At the start of the discussion we talked about how neural networks have nodes with some having many more connections to others and why it’s easier to traverse from Tom Cruise to his mother than vice versa. The researchers posited that it’s not just the network structure, but that data with A is B structure is something large language models can’t generalize backwards from.</p>



<p>Testing whether this is a networking issue or a fundamental flaw in the architecture of these models is hard. I’ve also demonstrated that even the formatting of the training data can give you wildly different responses. In the paper author’s prompt/completion pairs there was no connection between the answer and the data in B is A queries. But when you kept the text intact, the model could at least connect to something related – showing that there was some B ~ A signal, putting the idea that there was <strong>zero</strong> relation in doubt. </p>



<p>While I respect the rigor the researchers put into the paper, I don’t think it proves what they say it does. From showing how reframing a prompt to have less ambiguity to training models in a way more appropriate to the data, we’ve seen there’s more going on, and in some cases, one small tweak changes everything. </p>



<h2>A simple test</h2>



<p>I’d like to propose a counter experiment and demonstrate B is A generalization through a much simpler test…</p>



<p>If the claim, “If a model is trained on a sentence of the form “<em>A&nbsp;</em>is&nbsp;<em>B</em>”, it will not automatically generalize to the reverse direction “<em>B&nbsp;</em>is&nbsp;<em>A</em>””, is true then I shouldn’t be able to train a model with A is B examples and get B is A answers.</p>



<p>Instead of using a small dataset of made up names, we’ll train the model on a fact about a real person in an A is B manner and then see if we can go from B is A. </p>



<p>We’re doing this for three reasons:</p>



<ol>
<li>A well-known person is less likely to create a conflict with the model’s avoidance of mentioning real people – especially ones underrepresented in the data set.</li>



<li>This could help us understand if the Tom Cruise/Mary Lee Pfeiffer asymmetry is because of a model flaw or a matter of training data representation.</li>



<li>Connecting a fake fact to a real node and getting it to connect backwards seems like a better test.</li>
</ol>



<p>This test will be simple. We’ll create 30 A is B pairs of data about Tom Cruise being the author of a fake book – always preceding the book with Tom Cruise’s name: Tom Cruise -&gt;book title.</p>



<p>We’ll begin by having ChatGPT help us create 30 statements about Tom Cruise and his new book similar to the test example the researchers created. We’ll also use the ChatGPT message thread style and leave everything empty except the assistant context:</p>



<figure><img data-attachment-id="720" data-permalink="https://andrewmayne.com/?attachment_id=720" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png" data-orig-size="2098,474" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.03.09-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024" loading="lazy" width="1024" height="231" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=2045 2045w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.03.09-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Notice that all of the examples have Tom Cruise’s name before the book.</p>



<p>Now let’s fine-tune GPT-3.5-Turbo on our 30 examples:</p>



<figure><img data-attachment-id="721" data-permalink="https://andrewmayne.com/?attachment_id=721" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png" data-orig-size="2180,1248" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.05.12-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024" loading="lazy" width="1024" height="586" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=2048 2048w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.05.12-pm.png?w=768 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Okay. Um, the slope thing went down. That’s good.</p>



<p>Now a baseline A is B test:</p>



<p>When we ask our fine-tuned model what book Tom Cruise wrote we get our fake book as a response:</p>



<figure><img data-attachment-id="722" data-permalink="https://andrewmayne.com/?attachment_id=722" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png" data-orig-size="1628,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.07.52-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024" loading="lazy" width="1024" height="406" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.07.52-pm.png 1628w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Correct. That part of the inception complete, let’s move on to the real test. Will the model make a B to A connection from its training data? We’ll use the part of the text after Tom Cruises name to test:</p>



<figure><img data-attachment-id="725" data-permalink="https://andrewmayne.com/?attachment_id=725" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png" data-orig-size="1602,626" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.25.55-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024" loading="lazy" width="1024" height="400" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.25.55-pm-1.png 1602w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Yes. Yes it does. Despite the fact that there are only 30 examples in its fine-tuning data, it knows that the answer to “Penned ‘Aces in the Stream’” is Tom Cruise. </p>



<p>“Penned ‘Aces in the Stream’” is a very specific phrase, but that’s fair by the examples in the research paper. That was the ‘B’ part and it correctly predicted the “A” part.</p>



<p>Pushing it further, If we lower the temperature the model becomes more robust at answering the question even if formatted differently:</p>



<figure><img data-attachment-id="727" data-permalink="https://andrewmayne.com/?attachment_id=727" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png" data-orig-size="1612,656" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-13-at-7.35.00-pm-2" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1024" loading="lazy" width="1024" height="416" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=1022 1022w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-13-at-7.35.00-pm-2.png 1612w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>This is not random. This is the model reversing a B to an A. This model is not cursed.</p>



<p>We can also check to make sure it’s not generalizing everything to Tom Cruise by testing with another made up book title (as suggested on HackerNews):<br></p>



<figure><img data-attachment-id="758" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-8-02-03-pm-1/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png" data-orig-size="1438,652" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-8.02.03-pm-1" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024" loading="lazy" width="1024" height="464" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-8.02.03-pm-1.png 1438w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>And testing with a real book:</p>



<figure><img data-attachment-id="754" data-permalink="https://andrewmayne.com/2023/11/14/is-the-reversal-curse-real/screenshot-2023-11-14-at-7-59-13-pm/" data-orig-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png" data-orig-size="1434,646" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="screenshot-2023-11-14-at-7.59.13-pm" data-image-description="" data-image-caption="" data-medium-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=300" data-large-file="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024" loading="lazy" width="1024" height="461" src="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024" alt="" srcset="https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=1024 1024w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=150 150w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=300 300w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png?w=768 768w, https://andrewmayneblog.files.wordpress.com/2023/11/screenshot-2023-11-14-at-7.59.13-pm.png 1434w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Furthermore, my bet is that as the number of examples go up, the model will become even more robust at answering questions about B is A data.</p>



<h2>Conclusion</h2>



<p>I think that we’ve established that:</p>



<ol>
<li>LLMs can make approximate B to A connections with entirely made up data.</li>



<li>LLMs can make specific connections between B to A with a mixture of fictitious facts and real people.</li>
</ol>



<p>Since the main claim of the paper is “<strong>LLMs trained on “A is B” fail to learn “B is A”</strong>“, I think it’s safe to say that’s not true of the GPT-3.5-Turbo model we fine-tuned. I’ll also point out that was with only 30 weak examples.</p>



<p>The connections we demonstrated were as robust the ones they were testing for and we showed that by simplifying their training data we could even observe responses that were non-random using the same data and model. </p>



<p>So in summation: I don’t think any of the examples the authors provided are proof of a Reversal Curse and we haven’t observed a “failure of logical deduction.” Simpler explanations are more explanatory: imprecise prompts, underrepresented data and fine-tuning errors.</p>



<p>That being said, these models aren’t perfect. Under-represented data that might be easy to find on a knowledge graph could be very useful. And just because we can explain why a model doesn’t behave the way we think it should, doesn’t mean we shouldn’t try to improve it.</p>



<p>ChatGPT and other models that use reinforcement with human feedback exist because for many people base models that just map connections aren’t as useful as models that prioritize and understand what you want.</p>



<p>If you’re looking to fine-tune a model and want to improve your results you might consider some of these methods:</p>



<ol>
<li>Training on both input and output style pairs and complete text.</li>



<li>Using GPT-4 to extract facts to include in your training data.</li>



<li>Using special tokens “&lt;person&gt;” to indicate entities or things you want to reinforce.</li>



<li>Increasing the size of your dataset by having GPT-4 write different versions of your text.</li>



<li>Varying the length of the data.</li>



<li>Training on versions in other languages.</li>
</ol>







<hr>



<p>Thanks to <a href="https://twitter.com/BorisMPower">Boris Power </a>for his helpful feedback.</p>



<p>This is the GitHub repo for my training examples used in this post: <a href="https://github.com/AndrewMayneProjects/Reversal-Curse">https://github.com/AndrewMayneProjects/Reversal-Curse</a></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Inko Programming Language (159 pts)]]></title>
            <link>https://inko-lang.org</link>
            <guid>38270265</guid>
            <pubDate>Tue, 14 Nov 2023 21:47:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://inko-lang.org">https://inko-lang.org</a>, See on <a href="https://news.ycombinator.com/item?id=38270265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h2>
A language for building concurrent software with confidence
</h2>
<p>Inko makes it easy to build concurrent software, without having to worry about
unpredictable performance, unexpected runtime errors, data races, and type
errors.</p>

<p>Inko features deterministic automatic memory management, move semantics,
static typing, type-safe concurrency, efficient error handling, and more.</p>

<p><a href="https://docs.inko-lang.org/manual/main/getting-started/installation/">
<i></i>
Get started
</a>
<a href="https://discord.gg/seeURxHxCb">
<i></i>
Discord
</a>
<a href="https://github.com/inko-lang/inko">
<i></i>
GitHub
</a>
</p></div>
<div>
<div data-example="Concurrent programming"><pre><code><span>class</span><span> </span><span>async</span><span> </span><span>Calculator</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span><span>fact</span><span>(</span><span>size</span><span>:</span><span> </span><span>Int</span><span>,</span><span> </span><span>output</span><span>:</span><span> </span><span>Channel</span><span>[</span><span>Int</span><span>])</span><span> </span><span>{</span><span>
    </span><span>let</span><span> </span>result<span> </span><span>=</span><span> </span><span>1</span><span>
      </span><span>.</span><span>to</span><span>(</span>size<span>)</span><span>
      </span><span>.</span><span>iter</span><span>
      </span><span>.</span><span>reduce</span><span>(</span><span>1</span><span>)</span><span> </span><span>fn</span><span> </span><span>(</span>product<span>,</span><span> </span>val<span>)</span><span> </span><span>{</span><span> </span>product<span> </span><span>*</span><span> </span>val<span> </span><span>}</span><span>

    </span>output<span>.</span><span>send</span><span>(</span>result<span>)</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>calc<span> </span><span>=</span><span> </span><span>Calculator</span><span> </span><span>{}</span><span>
    </span><span>let</span><span> </span>out<span> </span><span>=</span><span> </span><span>Channel</span><span>.</span><span>new</span><span>(</span><span>size</span><span>:</span><span> </span><span>1</span><span>)</span><span>

    </span><span># This calculates the factorial of 15 in the</span><span>
    </span><span># background, then we wait for the result to</span><span>
    </span><span># be sent back to us via a channel.</span><span>
    </span>calc<span>.</span><span>fact</span><span>(</span><span>15</span><span>,</span><span> </span>out<span>)</span><span>
    </span>out<span>.</span><span>receive</span><span> </span><span># =&gt; 1307674368000</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Destructors"><pre><code><span>import</span><span> </span>std<span>.</span><span>drop</span><span>.</span><span>Drop</span><span>
</span><span>import</span><span> </span>std<span>.</span><span>stdio</span><span>.</span><span>STDOUT</span><span>

</span><span>class</span><span> </span><span>Thing</span><span> </span><span>{}</span><span>

</span><span>impl</span><span> </span><span>Drop</span><span> </span><span>for</span><span> </span><span>Thing</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>mut</span><span> </span>drop<span> </span><span>{</span><span>
    </span><span>STDOUT</span><span>.</span><span>new</span><span>.</span><span>print</span><span>(</span><span>'Thing is dropped'</span><span>)</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>thing<span> </span><span>=</span><span> </span><span>Thing</span><span> </span><span>{}</span><span>

    </span><span># Thing goes out of scope here, running its destructor.</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Error handling"><pre><code><span>fn</span><span> </span><span>div</span><span>(</span><span>left</span><span>:</span><span> </span><span>Int</span><span>,</span><span> </span><span>right</span><span>:</span><span> </span><span>Int</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>Result</span><span>[</span><span>Int</span><span>,</span><span> </span><span>String</span><span>]</span><span> </span><span>{</span><span>
  </span><span># `throw x` is short for `return Result.Error(x)`,</span><span>
  </span><span># saving us a bit of typing.</span><span>
  </span><span>if</span><span> </span>right<span> </span><span>==</span><span> </span><span>0</span><span> </span><span>{</span><span> </span><span>throw</span><span> </span><span>'Attempt to divide by zero'</span><span> </span><span>}</span><span>

  </span><span>Result</span><span>.</span><span>Ok</span><span>(</span>left<span> </span><span>/</span><span> </span>right<span>)</span><span>
</span><span>}</span><span>

</span><span>fn</span><span> </span><span>div2</span><span>(</span><span>left</span><span>:</span><span> </span><span>Int</span><span>,</span><span> </span><span>right</span><span>:</span><span> </span><span>Int</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>Result</span><span>[</span><span>Int</span><span>,</span><span> </span><span>String</span><span>]</span><span> </span><span>{</span><span>
  </span><span># `try` is short for matching against a value,</span><span>
  </span><span># and throwing it again if it's an `Error`. It</span><span>
  </span><span># also works for `Option` values, in which case</span><span>
  </span><span># it throws a `None` if the value is also a</span><span>
  </span><span># `None`.</span><span>
  </span><span>let</span><span> </span>res<span> </span><span>=</span><span> </span><span>try</span><span> </span><span>div</span><span>(</span>left<span>,</span><span> </span>right<span>)</span><span>

  </span><span>if</span><span> </span>res<span> </span><span>==</span><span> </span><span>5</span><span> </span><span>{</span><span> </span><span>Result</span><span>.</span><span>Ok</span><span>(</span><span>50</span><span>)</span><span> </span><span>}</span><span> </span><span>else</span><span> </span><span>{</span><span> </span><span>Result</span><span>.</span><span>Ok</span><span>(</span>res<span>)</span><span> </span><span>}</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span># `Result` is just an algebraic data type, so we can</span><span>
    </span><span># pattern match against it to get the underlying</span><span>
    </span><span># value.</span><span>
    </span><span>match</span><span> </span><span>div</span><span>(</span><span>10</span><span>,</span><span> </span><span>2</span><span>)</span><span> </span><span>{</span><span>
      </span><span>case</span><span> </span><span>Ok</span><span>(</span>val<span>)</span><span> </span><span>-&gt;</span><span> </span>val<span>
      </span><span>case</span><span> </span><span>Error</span><span>(</span>err<span>)</span><span> </span><span>-&gt;</span><span> </span><span>panic</span><span>(</span>err<span>)</span><span>
    </span><span>}</span><span>

    </span><span># We can also just unwrap the Ok value if we're</span><span>
    </span><span># certain we'll never get an `Error` case:</span><span>
    </span><span>div</span><span>(</span><span>10</span><span>,</span><span> </span><span>2</span><span>).</span><span>unwrap</span><span> </span><span># =&gt; 10</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Generic data types"><pre><code><span>class</span><span> </span><span>Stack</span><span>[</span><span>T</span><span>]</span><span> </span><span>{</span><span>
  </span><span>let</span><span> </span><span>@values</span><span>:</span><span> </span><span>Array</span><span>[</span><span>T</span><span>]</span><span>

  </span><span>fn</span><span> </span><span>static</span><span> </span>new<span> </span><span>-&gt;</span><span> </span><span>Stack</span><span>[</span><span>T</span><span>]</span><span> </span><span>{</span><span>
    </span><span>Stack</span><span> </span><span>{</span><span> </span><span>@values</span><span> </span><span>=</span><span> </span><span>[]</span><span> </span><span>}</span><span>
  </span><span>}</span><span>

  </span><span>fn</span><span> </span><span>mut</span><span> </span><span>push</span><span>(</span><span>value</span><span>:</span><span> </span><span>T</span><span>)</span><span> </span><span>{</span><span>
    </span><span>@values</span><span>.</span><span>push</span><span>(</span>value<span>)</span><span>
  </span><span>}</span><span>

  </span><span>fn</span><span> </span><span>mut</span><span> </span>pop<span> </span><span>-&gt;</span><span> </span><span>Option</span><span>[</span><span>T</span><span>]</span><span> </span><span>{</span><span>
    </span><span>@values</span><span>.</span><span>pop</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>stack<span> </span><span>=</span><span> </span><span>Stack</span><span>.</span><span>new</span><span>

    </span><span># This infers `T` to `Int`.</span><span>
    </span>stack<span>.</span><span>push</span><span>(</span><span>42</span><span>)</span><span>

    </span><span># This is an error, as `T` is inferred to `Int`, and</span><span>
    </span><span># `String` isn't compatible with `Int`.</span><span>
    </span>stack<span>.</span><span>push</span><span>(</span><span>'Oh no!'</span><span>)</span><span>

    </span><span># The compiler can also infer earlier types based</span><span>
    </span><span># on how they are used later on:</span><span>
    </span><span>let</span><span> </span>stack<span> </span><span>=</span><span> </span><span>Stack</span><span>.</span><span>new</span><span>
    </span><span>let</span><span> </span>value<span> </span><span>=</span><span> </span>stack<span>.</span><span>pop</span><span> </span><span># The exact type isn't known yet</span><span>

    </span><span>10</span><span> </span><span>+</span><span> </span>value<span>.</span><span>unwrap</span><span> </span><span># `value` inferred as `Option[Int]`</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Hello, world!"><pre><code><span>import</span><span> </span>std<span>.</span><span>stdio</span><span>.</span><span>STDOUT</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>STDOUT</span><span>.</span><span>new</span><span>.</span><span>print</span><span>(</span><span>'Hello, world!'</span><span>)</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Implementing traits"><pre><code><span>trait</span><span> </span><span>ToString</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span>to_string<span> </span><span>-&gt;</span><span> </span><span>String</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>Person</span><span> </span><span>{</span><span>
  </span><span>let</span><span> </span><span>@name</span><span>:</span><span> </span><span>String</span><span>
</span><span>}</span><span>

</span><span>impl</span><span> </span><span>ToString</span><span> </span><span>for</span><span> </span><span>Person</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span>to_string<span> </span><span>-&gt;</span><span> </span><span>String</span><span> </span><span>{</span><span>
    </span><span>@name</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>

</span><span># Traits can also be implemented conditionally:</span><span>
</span><span>class</span><span> </span><span>List</span><span>[</span><span>T</span><span>]</span><span> </span><span>{</span><span>
  </span><span># ...</span><span>
</span><span>}</span><span>

</span><span># Here `ToString` is only available for instances</span><span>
</span><span># of `List` if `T` also implements `ToString`.</span><span>
</span><span>impl</span><span> </span><span>ToString</span><span> </span><span>for</span><span> </span><span>List</span><span> </span><span>if</span><span> </span><span>T</span><span>:</span><span> </span><span>ToString</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span>to_string<span> </span><span>-&gt;</span><span> </span><span>String</span><span> </span><span>{</span><span>
    </span><span>'...'</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Loops"><pre><code><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span><span>mut</span><span> </span>num<span> </span><span>=</span><span> </span><span>0</span><span>

    </span><span>while</span><span> </span>num<span> </span><span>&lt;</span><span> </span><span>10</span><span> </span><span>{</span><span>
      </span>num<span> </span><span>+=</span><span> </span><span>1</span><span>
    </span><span>}</span><span>

    </span><span>loop</span><span> </span><span>{</span><span>
      </span><span># This loops run forever. You can skip/break</span><span>
      </span><span># iteration using `next` and `break`.</span><span>
    </span><span>}</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Optional values"><pre><code><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>numbers<span> </span><span>=</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>,</span><span> </span><span>30</span><span>]</span><span>

    </span>numbers<span>
      </span><span>.</span><span>opt</span><span>(</span><span>2</span><span>)</span><span>
      </span><span>.</span><span>map</span><span> </span><span>fn</span><span> </span><span>(</span>n<span>)</span><span> </span><span>{</span><span> </span>n<span>.</span><span>to_string</span><span> </span><span>}</span><span> </span><span># =&gt; Option.Some('30')</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Pattern matching"><pre><code><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>match</span><span> </span><span>Option</span><span>.</span><span>Some</span><span>(</span><span>42</span><span>)</span><span> </span><span>{</span><span>
      </span><span>case</span><span> </span><span>Some</span><span>(</span>number<span>)</span><span> </span><span>-&gt;</span><span> </span>number<span>
      </span><span>case</span><span> </span><span>None</span><span> </span><span>-&gt;</span><span> </span><span>0</span><span>
    </span><span>}</span><span>

    </span><span># Of course we also support nested patterns:</span><span>
    </span><span>match</span><span> </span><span>Option</span><span>.</span><span>Some</span><span>((</span><span>10</span><span>,</span><span> </span><span>'foo'</span><span>))</span><span> </span><span>{</span><span>
      </span><span>case</span><span> </span><span>Some</span><span>((</span><span>10</span><span>,</span><span> </span>message<span>))</span><span> </span><span>-&gt;</span><span> </span>message<span>
      </span><span>case</span><span> </span>_<span> </span><span>-&gt;</span><span> </span><span>'oh no!'</span><span>
    </span><span>}</span><span>

    </span><span># Guards are also supported:</span><span>
    </span><span>match</span><span> </span><span>Option</span><span>.</span><span>Some</span><span>(</span><span>42</span><span>)</span><span> </span><span>{</span><span>
      </span><span>case</span><span> </span><span>Some</span><span>(</span>number<span>)</span><span> </span><span>if</span><span> </span>number<span> </span><span>&lt;</span><span> </span><span>50</span><span> </span><span>-&gt;</span><span> </span>number<span>
      </span><span>case</span><span> </span>_<span> </span><span>-&gt;</span><span> </span><span>0</span><span>
    </span><span>}</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Reading from a file"><pre><code><span>import</span><span> </span>std<span>.</span><span>fs</span><span>.</span><span>file</span><span>.</span><span>ReadOnlyFile</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span># expect() unwraps the Result, panicking with</span><span>
    </span><span># the given error message when encountering an</span><span>
    </span><span># `Error`.</span><span>
    </span><span>let</span><span> </span>file<span> </span><span>=</span><span> </span><span>ReadOnlyFile</span><span>
      </span><span>.</span><span>new</span><span>(</span><span>'README.md'</span><span>)</span><span>
      </span><span>.</span><span>expect</span><span>(</span><span>"the file doesn't exist"</span><span>)</span><span>

    </span><span>let</span><span> </span>bytes<span> </span><span>=</span><span> </span><span>ByteArray</span><span>.</span><span>new</span><span>

    </span>file<span>
      </span><span>.</span><span>read_all</span><span>(</span>bytes<span>)</span><span>
      </span><span>.</span><span>expect</span><span>(</span><span>'failed to read the file'</span><span>)</span><span>

    </span>bytes<span>.</span><span>to_string</span><span> </span><span># =&gt; "# README ..."</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Single ownership"><pre><code><span>fn</span><span> </span>moves<span> </span><span>{</span><span>
  </span><span>let</span><span> </span>numbers<span> </span><span>=</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>,</span><span> </span><span>30</span><span>]</span><span>

  </span><span># This is OK, and returns a `ref 10`.</span><span>
  </span>numbers<span>[</span><span>0</span><span>]</span><span>

  </span><span>let</span><span> </span>nums<span> </span><span>=</span><span> </span>numbers<span>

  </span><span># This is no longer OK, as `numbers` gave up ownership to</span><span>
  </span><span># `nums`.</span><span>
  </span>numbers<span>[</span><span>0</span><span>]</span><span>
</span><span>}</span><span>

</span><span>fn</span><span> </span>refs<span> </span><span>{</span><span>
  </span><span>let</span><span> </span>owned<span> </span><span>=</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>,</span><span> </span><span>30</span><span>]</span><span>

  </span><span># We can "borrow" a value using references:</span><span>
  </span><span>let</span><span> </span>borrowed<span> </span><span>=</span><span> </span><span>ref</span><span> </span>owned<span>

  </span><span># Both are OK and produce the same value: `ref 10`.</span><span>
  </span>owned<span>[</span><span>0</span><span>]</span><span>
  </span>borrowed<span>[</span><span>0</span><span>]</span><span>

  </span><span># This isn't OK, as `ref T` doesn't allow mutation:</span><span>
  </span>borrowed<span>.</span><span>push</span><span>(</span><span>42</span><span>)</span><span>

  </span><span># For that we need a mutable reference:</span><span>
  </span><span>let</span><span> </span>mutable<span> </span><span>=</span><span> </span><span>mut</span><span> </span>owned<span>

  </span>mutable<span>.</span><span>push</span><span>(</span><span>42</span><span>)</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<div data-example="Unit testing"><pre><code><span>import</span><span> </span>std<span>.</span><span>test</span><span>.</span><span>Tests</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>tests<span> </span><span>=</span><span> </span><span>Tests</span><span>.</span><span>new</span><span>

    </span>tests<span>.</span><span>test</span><span>(</span><span>'Adding two integers'</span><span>)</span><span> </span><span>fn</span><span> </span><span>(</span>t<span>)</span><span> </span><span>{</span><span>
      </span>t<span>.</span><span>equal</span><span>(</span><span>10</span><span> </span><span>+</span><span> </span><span>5</span><span>,</span><span> </span><span>15</span><span>)</span><span>
      </span>t<span>.</span><span>equal</span><span>(</span><span>1</span><span> </span><span>+</span><span> </span><span>-1</span><span>,</span><span> </span><span>0</span><span>)</span><span>
    </span><span>}</span><span>

    </span>tests<span>.</span><span>run</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
</div>
</div><div>


<p>Inko doesn't rely on garbage collection to manage memory. Instead, Inko relies
on single ownership and move semantics. Values start out as owned and are
dropped when they go out of scope:</p>

<div><pre><code><span>let</span><span> </span>numbers<span> </span><span>=</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>,</span><span> </span><span>30</span><span>]</span><span>

</span><span># "numbers" is no longer in use here, so it's dropped.</span><span>
</span><span>return</span><span>
</span></code></pre></div>
<p>These values can be borrowed either mutably or immutably. Inko allows multiple
borrows (both mutable and immutable borrows), and allows moving of the borrowed
values while borrows exist:</p>

<div><pre><code><span>let</span><span> </span>a<span> </span><span>=</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>,</span><span> </span><span>30</span><span>]</span><span>

</span><span># All of this is perfectly fine:</span><span>
</span><span>let</span><span> </span>b<span> </span><span>=</span><span> </span><span>ref</span><span> </span>a<span> </span><span># borrows "a" immutably</span><span>
</span><span>let</span><span> </span>c<span> </span><span>=</span><span> </span><span>mut</span><span> </span>a<span> </span><span># borrows "a" mutably</span><span>
</span><span>let</span><span> </span>d<span> </span><span>=</span><span> </span>a<span>     </span><span># moves "a" into "d"</span><span>
</span></code></pre></div>
<p>This gives you the benefits of single ownership, but at a fraction of the cost
compared to languages such as Rust. The use of single ownership also means more
predictable behaviour and performance, and not having to spend a long time
adjusting different garbage collection settings.</p>



<p>With Inko you never again have to worry about NULL pointers, use-after-free
errors, unexpected runtime errors, data races, and other types of errors
commonly found in other languages. For optional data Inko provides an <code>Option</code>
type, which is an algebraic data type that you can pattern match against. Inko
supports both mutable and immutable references, allowing you to restrict
mutation where necessary.</p>



<p>Inko uses lightweight processes for concurrency, and its concurrency model is
inspired by <a href="https://www.erlang.org/">Erlang</a> and
<a href="https://www.ponylang.io/">Pony</a>. Processes are isolated from each other and
communicate by sending messages. Processes and messages are defined as classes
and methods, and the compiler type-checks these to ensure correctness.</p>

<p>The compiler ensures that data sent between processes is unique, meaning there
are no outside references to the data. This removes the need for (deep) copying
data, and makes data races impossible. Inko also supports multi-producer
multi-consumer channels, allowing processes to communicate with each other
without needing explicit references to each other.</p>

<p>Here's how you'd implement a simple concurrent counter:</p>

<div><pre><code><span>class</span><span> </span><span>async</span><span> </span><span>Counter</span><span> </span><span>{</span><span>
  </span><span>let</span><span> </span><span>@value</span><span>:</span><span> </span><span>Int</span><span>

  </span><span>fn</span><span> </span><span>async</span><span> </span><span>mut</span><span> </span>increment<span> </span><span>{</span><span>
    </span><span>@value</span><span> </span><span>+=</span><span> </span><span>1</span><span>
  </span><span>}</span><span>

  </span><span>fn</span><span> </span><span>async</span><span> </span><span>send_to</span><span>(</span><span>channel</span><span>:</span><span> </span><span>Channel</span><span>[</span><span>Int</span><span>])</span><span> </span><span>{</span><span>
    </span>channel<span>.</span><span>send</span><span>(</span><span>@value</span><span>)</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>counter<span> </span><span>=</span><span> </span><span>Counter</span><span> </span><span>{</span><span> </span><span>@value</span><span> </span><span>=</span><span> </span><span>0</span><span> </span><span>}</span><span>
    </span><span>let</span><span> </span>output<span> </span><span>=</span><span> </span><span>Channel</span><span>.</span><span>new</span><span>(</span><span>size</span><span>:</span><span> </span><span>1</span><span>)</span><span>

    </span>counter<span>.</span><span>increment</span><span>
    </span>counter<span>.</span><span>increment</span><span>
    </span>counter<span>.</span><span>send_to</span><span>(</span>output<span>)</span><span>
    </span>output<span>.</span><span>receive</span><span> </span><span># =&gt; 2</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<h2 id="header-error-handling-done-right">Error handling done right</h2>

<p>Inko uses a form of error handling inspired by Joe Duffy's excellent article
<a href="http://joeduffyblog.com/2016/02/07/the-error-model/">"The Error Model"</a>. Errors
are represented using the algebraic type "Result", and Inko provides syntax
sugar in the form of <code>try</code> and <code>throw</code> to make error handling easy. Critical
errors that can't/shouldn't be handled are supported in the form of "panics",
which abort the program when they occur.</p>

<p>For example, here's how you'd handle errors when opening a file and calculating
its size:</p>

<div><pre><code><span>import</span><span> </span>std<span>.</span><span>fs</span><span>.</span><span>file</span><span>.</span><span>ReadOnlyFile</span><span>
</span><span>import</span><span> </span>std<span>.</span><span>stdio</span><span>.</span><span>STDOUT</span><span>

</span><span>class</span><span> </span><span>async</span><span> </span><span>Main</span><span> </span><span>{</span><span>
  </span><span>fn</span><span> </span><span>async</span><span> </span>main<span> </span><span>{</span><span>
    </span><span>let</span><span> </span>size<span> </span><span>=</span><span>
      </span><span>ReadOnlyFile</span><span>
        </span><span>.</span><span>new</span><span>(</span><span>'README.md'</span><span>)</span><span>             </span><span># =&gt; Result[ReadOnlyFile, Error]</span><span>
        </span><span>.</span><span>then</span><span> </span><span>fn</span><span> </span><span>(</span>file<span>)</span><span> </span><span>{</span><span> </span>file<span>.</span><span>size</span><span> </span><span>}</span><span> </span><span># =&gt; Result[Int, Error]</span><span>
        </span><span>.</span><span>unwrap_or</span><span>(</span><span>0</span><span>)</span><span>                 </span><span># =&gt; Int</span><span>

    </span><span>STDOUT</span><span>.</span><span>new</span><span>.</span><span>print</span><span>(</span>size<span>.</span><span>to_string</span><span>)</span><span> </span><span># =&gt; 1099</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div>


<p>Inko aims to be an efficient language, though it doesn't aim to compete with
low-level languages such as C and Rust. Instead, we aim to provide a compelling
alternative to the likes of Ruby, Erlang, and Go.</p>

<p>Inko uses a native code compiler, using <a href="https://llvm.org/">LLVM</a> as its
backend, and aims to provide a balance between fast compile times and good
runtime performance. The native code is statically linked against a small
runtime library written in Rust, which takes care of scheduling processes,
non-blocking IO, and provides various low-level functions.</p>



<p>Inko supports pattern matching on a variety of types, such as tuples and
algebraic data types:</p>

<div><pre><code><span>match</span><span> </span><span>[</span><span>10</span><span>,</span><span> </span><span>20</span><span>].</span><span>opt</span><span>(</span><span>1</span><span>)</span><span> </span><span>{</span><span>
  </span><span>case</span><span> </span><span>Some</span><span>(</span>number<span>)</span><span> </span><span>-&gt;</span><span> </span>number<span> </span><span># =&gt; 20</span><span>
  </span><span>case</span><span> </span><span>None</span><span> </span><span>-&gt;</span><span> </span><span>0</span><span>
</span><span>}</span><span>

</span><span>match</span><span> </span><span>(</span><span>10</span><span>,</span><span> </span><span>'hello'</span><span>)</span><span> </span><span>{</span><span>
  </span><span>case</span><span> </span><span>(</span><span>10</span><span>,</span><span> </span><span>'hello'</span><span>)</span><span> </span><span>-&gt;</span><span> </span><span>'foo'</span><span>
  </span><span>case</span><span> </span><span>(</span><span>20</span><span>,</span><span> </span>_<span>)</span><span> </span><span>-&gt;</span><span> </span><span>'bar'</span><span>
  </span><span>case</span><span> </span>_<span> </span><span>-&gt;</span><span> </span><span>'baz'</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<p>You can also match against literals such as integers and strings, and against
regular classes:</p>

<div><pre><code><span>class</span><span> </span><span>Person</span><span> </span><span>{</span><span>
  </span><span>let</span><span> </span><span>@name</span><span>:</span><span> </span><span>String</span><span>
  </span><span>let</span><span> </span><span>@age</span><span>:</span><span> </span><span>Int</span><span>
</span><span>}</span><span>

</span><span>let</span><span> </span>alice<span> </span><span>=</span><span> </span><span>Person</span><span> </span><span>{</span><span> </span><span>@name</span><span> </span><span>=</span><span> </span><span>'Alice'</span><span>,</span><span> </span><span>@age</span><span> </span><span>=</span><span> </span><span>42</span><span> </span><span>}</span><span>

</span><span>match</span><span> </span>alice<span> </span><span>{</span><span>
  </span><span>case</span><span> </span><span>{</span><span> </span><span>@name</span><span> </span><span>=</span><span> </span>name<span> </span><span>}</span><span> </span><span>-&gt;</span><span> </span>name<span> </span><span># =&gt; 'Alice'</span><span>
</span><span>}</span><span>
</span></code></pre></div>
<p>Pattern matching is compiled down to decision trees, and the compiler tries to
keep their sizes as small as possible. The compiler also ensures that all
patterns are covered.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube will show labels on videos that use AI (132 pts)]]></title>
            <link>https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/</link>
            <guid>38269656</guid>
            <pubDate>Tue, 14 Nov 2023 20:58:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/">https://9to5google.com/2023/11/14/youtube-ai-labels-videos-shorts/</a>, See on <a href="https://news.ycombinator.com/item?id=38269656">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-3.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="" fetchpriority="high">
	
	</figure>

<p>AI can be used for good, and for evil, and on YouTube the latter could mean posting videos that are misleading viewers into thinking they are real. To avoid those situations, YouTube is now requiring creators to mark videos that are made using AI, and will show labels to viewers.</p>



<p>Announced <a href="https://blog.youtube/inside-youtube/our-approach-to-responsible-ai-innovation/">in a blog post</a> today, YouTube will, in the “coming months,” require that creators disclose the use of AI in a video which will result in a label being shown on videos that are “synthetic.” </p>



<p>While it’s not totally clear right now where the line will be drawn on what videos will need to disclose the use of AI, it seems pretty broad. YouTube says it will need to be used on material that is “realistic” whether it is altered by AI or entirely synthetic. The disclosure will be shown both on full-length videos and Shorts.</p>



<p>YouTube explains:</p>



<blockquote>
<p>To address this concern, over the coming months, we’ll introduce updates that inform viewers when the content they’re seeing is synthetic. Specifically, we’ll require creators to disclose when they’ve created altered or synthetic content that is realistic, including using AI tools. When creators upload content, we will have new options for them to select to indicate that it contains realistic altered or synthetic material. For example, this could be an AI-generated video that realistically depicts an event that never happened, or content showing someone saying or doing something they didn’t actually do.</p>
</blockquote>



<p>It’s further explained that “sensitive topics,” such as elections, ongoing conflicts, health, and more will shown these AI labels more prominently. Creators who are found to consistently fail to mark AI-aided content on YouTube will be subject to content removal as well as suspension from the YouTube Partner Program.</p>



<figure>
<figure><img decoding="async" fetchpriority="high" width="800" height="961" data-id="595287" src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?quality=82&amp;strip=all" alt="" srcset="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg 800w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=108,130 108w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=583,700 583w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=768,923 768w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=291,350 291w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=140,168 140w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-2.jpg?resize=150,180 150w" sizes="(max-width: 800px) 100vw, 800px"></figure>



<figure><img decoding="async" width="800" height="961" data-id="595286" src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?quality=82&amp;strip=all" alt="" srcset="https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg 800w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=108,130 108w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=583,700 583w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=768,923 768w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=291,350 291w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=140,168 140w, https://9to5google.com/wp-content/uploads/sites/4/2023/11/youtube-ai-label-1.jpg?resize=150,180 150w" sizes="(max-width: 800px) 100vw, 800px"></figure>
</figure>



<p>YouTube is also taking a bigger stand against AI-generated music content, such as the fake songs from Drake that went viral earlier this year. Removal requests from music labels and distributors will be coming soon to address this, as YouTube explains:</p>



<blockquote>
<p>We’re also introducing the ability for our music partners to request the removal of AI-generated music content that mimics an artist’s unique singing or rapping voice. In determining whether to grant a removal request, we’ll consider factors such as whether content is the subject of news reporting, analysis or critique of the synthetic vocals. These removal requests will be available to labels or distributors who represent artists participating in YouTube’s early AI music experiments. We’ll continue to expand access to additional labels and distributors over the coming months.</p>
</blockquote>



<h2 id="h-more-on-youtube">More on YouTube:</h2>



<ul>
<li><a href="https://9to5google.com/2023/11/08/youtube-for-you-channels-recommendations/">YouTube adding ‘For You’ channel section that tailors video recommendations for new viewers</a></li>



<li><a href="https://9to5google.com/2023/11/06/youtube-ai-ask-video/">YouTube tests AI that lets you ‘Ask’ questions about videos and summarize comments</a></li>



<li><a href="https://9to5google.com/2023/11/06/youtube-random-video-button-app/">YouTube app is testing a new button to start playing random videos</a></li>
</ul>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3QJERK2"><img src="https://9to5google.com/wp-content/uploads/sites/4/2023/11/Eureka-BF-Google-750x150-1.jpg-Nov-13-19-1.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Bring Back Webrings (108 pts)]]></title>
            <link>https://arne.me/articles/we-need-to-bring-back-webrings</link>
            <guid>38268706</guid>
            <pubDate>Tue, 14 Nov 2023 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arne.me/articles/we-need-to-bring-back-webrings">https://arne.me/articles/we-need-to-bring-back-webrings</a>, See on <a href="https://news.ycombinator.com/item?id=38268706">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main"><article><header></header><p>The year is 1996.
You feed your Tamagotchi, get a Squeezit and turn on the home computer.
You’ve told your family they can’t do phone calls for the next hour.
The dial-up modem makes beeping sounds<sup><a href="#fn-1" id="fnref-1" data-footnote-ref="">1</a></sup>.
You’re online.</p>
<p>Yesterday you found this fly website about amateur radio, and you want to
explore more—but how can you find related websites?
Yahoo is slow and not really showing you what you’re looking for.
Then you notice that this website is part of the “Amateur Radio Webring”.
You click the arrow to the right and dive into another website about amateur
radio.</p>
<!-- more -->
<p>You wake up.</p>
<p>The year is 2023.
You can no longer get a Squeezit anywhere, you live under surveillance
capitalism and the
<a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a> is ruining
the web.
You found this goofy personal website, and you’d like to browse similar ones.
What do you do?
Find the person on socials, look at who they’re following and check if they have
a website?</p>
<p>It’s time to bring webrings back.</p>
<p>Introducing the <a href="https://firechicken.club/">Fire Chicken Webring</a>; an invite-only
webring for personal websites.
You can explore this webring by going to the footer of this website and
clicking on either arrow to navigate to another personal website.</p>
<p>If we’re friends, or I follow you anywhere, <a href="https://arne.me/contact">@ me</a> and I’ll add you.
If not, here’s some other modern webrings<sup><a href="#fn-2" id="fnref-2" data-footnote-ref="">2</a></sup>:
<a href="https://hotlinewebring.club/">Hotline Webring</a>,
<a href="https://sidebar.io/webring">Sidebar.io Webring</a>,
<a href="https://a11y-webring.club/">a11y-webring.club</a>,
and <a href="https://cs.sjoy.lol/#webring">CS.Sjoy.lol</a>.</p>
<p>Own a niche website? Why not start your own webring?</p>
<section data-footnotes="">
<ol>
<li id="fn-1">
<p>For full immersion, see <a href="https://youtu.be/gsNaR6FRuO0">this video</a>. <a href="#fnref-1" data-footnote-backref="" data-footnote-backref-idx="1" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="fn-2">
<p>Know of more? Please <a href="https://arne.me/contact">reach out to me</a>. <a href="#fnref-2" data-footnote-backref="" data-footnote-backref-idx="2" aria-label="Back to reference 2">↩</a></p>
</li>
</ol>
</section>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Reptar', a new CPU vulnerability (121 pts)]]></title>
            <link>https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</link>
            <guid>38268043</guid>
            <pubDate>Tue, 14 Nov 2023 19:14:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability">https://cloud.google.com/blog/products/identity-security/google-researchers-discover-reptar-a-new-cpu-vulnerability</a>, See on <a href="https://news.ycombinator.com/item?id=38268043">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><section><span jsaction="rcuQ6b:npT2md" jscontroller="YSybTb" data-track-type="" soy-skip="" ssk="5:kbe95"><p>This year, Google has seen an increase in the number of vulnerabilities impacting central processing units (CPU) across hardware systems. Two of the most notable of these vulnerabilities were disclosed in August, when Google researchers discovered <a href="http://downfall.page/" target="_blank">Downfall</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2022-40982" target="_blank">CVE-2022-40982</a>) and <a href="https://lock.cmpxchg8b.com/zenbleed.html" target="_blank">Zenbleed</a> (<a href="https://nvd.nist.gov/vuln/detail/CVE-2023-20593" target="_blank">CVE-2023-20593</a>), affecting Intel and AMD CPUs, respectively.</p><p>This trend proves only to be intensifying as time goes on. Left unmitigated, these types of vulnerabilities can impact billions of personal and cloud computers.</p><p>Today, we’re detailing the findings of <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html" target="_blank">Reptar</a> (<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2023-23583" target="_blank">CVE-2023-23583</a>), a new CPU vulnerability that impacts several Intel desktop, mobile, and server CPUs. Google’s Information Security Engineering team reported the vulnerability to Intel, who disclosed the vulnerability today. Thanks to the thoughtful collaboration between Google, Intel, and industry partners, mitigations have been rolled out, and Googlers and our customers are protected. </p><h3><b>How Google found and responded to Reptar</b></h3><p>A Google security researcher identified a vulnerability related to how redundant prefixes are interpreted by the CPU which leads to bypassing the CPU’s security boundaries if exploited successfully. Prefixes allow you to change how instructions behave by enabling or disabling features. The full rules are complicated, but in general, if you use a prefix that doesn't make sense or conflicts with other prefixes, we call those redundant. Usually, redundant prefixes are ignored.</p><p>The impact of this vulnerability is demonstrated when exploited by an attacker in a multi-tenant virtualized environment, as the exploit on a guest machine causes the host machine to crash resulting in a Denial of Service to other guest machines running on the same host. Additionally, the vulnerability could potentially lead to information disclosure or privilege escalation.</p><p>You can read more technical details about the vulnerability at our <a href="https://lock.cmpxchg8b.com/reptar.html" target="_blank">researcher’s blog</a>.</p><p>Our security teams were able to identify this vulnerability and responsibly disclose it to Intel. Google worked with industry partners to identify and test a successful mitigation so all users are protected from this risk in a timely manner. In particular, Google’s response team ensured a successful rollout of the mitigation to our systems before it posed a risk to our customers, mainly Google Cloud and ChromeOS customers.</p><h3><b>Google’s commitment to collaboration and hardware security</b></h3><p>As Reptar, Zenbleed, and Downfall suggest, computing hardware and processors remain susceptible to these types of vulnerabilities. This trend will only continue as hardware becomes increasingly complex. This is why Google continues to invest heavily in CPU and <a href="https://bughunters.google.com/" target="_blank">vulnerability research</a>. Work like this, done in close collaboration with our industry partners, allows us to keep users safe and is critical to finding and mitigating vulnerabilities before they can be exploited.</p><p>We look forward to continuing this proactive cybersecurity work, and encourage others to join us on this journey to create a more secure and resilient technology ecosystem.</p></span></section><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/identity-security" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/identity-security" track-metadata-module="tag list" track-metadata-module_headline="posted in">Security &amp; Identity</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Music painted on wall of a Venetian orphanage will be heard 250 years later (101 pts)]]></title>
            <link>https://theconversation.com/music-painted-on-the-wall-of-a-venetian-orphanage-will-be-heard-again-nearly-250-years-later-211995</link>
            <guid>38267739</guid>
            <pubDate>Tue, 14 Nov 2023 18:53:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://theconversation.com/music-painted-on-the-wall-of-a-venetian-orphanage-will-be-heard-again-nearly-250-years-later-211995">https://theconversation.com/music-painted-on-the-wall-of-a-venetian-orphanage-will-be-heard-again-nearly-250-years-later-211995</a>, See on <a href="https://news.ycombinator.com/item?id=38267739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>Imagine Lady Gaga or Elton John teaching at an orphanage or homeless shelter, offering daily music lessons. </p>

<p>That’s what took place at Venice’s four <a href="https://imagesofvenice.com/ospedali-grandi/">Ospedali Grandi</a>, which were charitable institutions that took in the needy – including orphaned and foundling girls – from the 16th century to the turn of the 19th century. Remarkably, all four Ospedali hired some of the greatest musicians and composers of the time, such as <a href="https://www.britannica.com/biography/Antonio-Vivaldi">Antonio Vivaldi</a> and <a href="https://guides.lib.fsu.edu/composerofthemonth">Nicola Porpora</a>, to provide the young women – known as the “putte” – with a superb music education.</p>

<p>In the summer of 2019, while in Venice on a research trip, I had the opportunity to visit the Ospedale di Santa Maria dei Derelitti, more commonly known as the Ospedaletto, or “Little Hospital,” because it was the smallest of the four Ospedali Grandi. </p>

<p>As a musicologist <a href="https://arts.psu.edu/faculty/marica-tacconi/">specializing in the music of early modern Venice</a>, I was especially excited to visit one of the hidden gems of the city: the <a href="https://www.gioiellinascostidivenezia.it/en/the-jewels/complesso-dell-ospedaletto/">Ospedaletto’s music room</a>, which was built in the mid-1770s.</p>

<p>I had heard about its beauty and perfect acoustics. So when a colleague and friend, classical singer <a href="https://venicemusicproject.it/en/liesl-odenweller/">Liesl Odenweller</a>, suggested we go together, I was delighted. I also secretly hoped Liesl would feel inclined to sing in the space, so I could experience the pure acoustics of the room. </p>

<p>Little did I know that I would encounter music that hasn’t been performed in nearly 250 years.</p>

<h2>Clues on the walls</h2>

<p>As we entered the stunning music room, I was immediately struck by its elegance and relatively small size. In my mind, I had envisioned a large concert hall; instead, the space is intimate, ellipse-shaped and richly decorated.</p>

<p>Overshadowed by <a href="https://www.exploreclassicalmusic.com/vivaldi-and-the-ospedale-della-piet">the more prominent Ospedale della Pietà</a>, not much is known about the music-making that took place for centuries behind the walls of the Ospedaletto. But one of the greatest clues to its venerable history as a music school is literally on one of its walls. </p>

<figure>
            <a href="https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="Colorful painting of women performing." data-src="https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=453&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=453&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=453&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=569&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=569&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=569&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/552861/original/file-20231009-15-1jj80a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Jacopo Guarana’s fresco ‘Concert of the Putte’ (1776-77).</span>
              <span><span>Marica S.Tacconi</span>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>A fresco on the far wall of the room, <a href="https://www.libreriauniversitaria.it/ospedaletto-sala-musica-favaro-tiziana/libro/9788885087071">painted in 1776-77 by Jacopo Guarana</a>, depicts a group of female musicians – likely portraits of some of the putte – at the feet of <a href="https://www.worldhistory.org/apollo/">Apollo</a>, the Greek god of music. Some of them play string instruments; one, gazing toward the viewer, holds a page of sheet music.</p>

<p>Call it a professional quirk, but when I see a music score depicted in a painting, I have to get up close and try to read it. In this case, I was lucky: The music notation was quite legible, and the composer’s name was inscribed in the upper-right corner: “Sig. Anfossi.” </p>

<figure>
            <p><img alt="A close-up of a painting of a sheet of music." data-src="https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=800&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1005&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/552862/original/file-20231009-29-54ha7a.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p>
            <figcaption>
              <span>The musical score depicted in Jacopo Guarana’s fresco.</span>
              <span><span>Marica S. Tacconi</span>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>I took several photos of the fresco. I wanted to learn as much as I could about that piece of music painted on the wall.</p>

<p>The sound of Liesl’s singing snapped me out of my music detective mode. As I had hoped, her beautiful soprano voice filled the space with a tone so pure that it sounded almost ethereal. I turned around, but my friend was no longer in the room. Where was her singing coming from?  </p>

<p>Liesl, it turns out, was perched in the singing gallery. With the permission of a clerk, she had climbed up to this partially hidden loft and was singing through a grille. It was here that the putte of the Ospedaletto performed in public concerts, their features partially obscured from the prying glances of the male listeners below.</p>

<figure>
            <a href="https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="Silhouette of woman singing from behind a cage above a grand room." data-src="https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=338&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=338&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=338&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=424&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=424&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=424&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/552864/original/file-20231009-15-25i1yo.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>Liesl Odenweller sings from the gallery of the Ospedaletto’s music room.</span>
              <span><span>Marica S. Tacconi</span>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<h2>Women rally behind their beloved institution</h2>

<p>Armed with those clues on the wall, I continued my research in the days following the visit to the Ospedaletto. I learned that the music by “Signor Anfossi” shown in the fresco was drawn from the opera “Antigono,” composed by <a href="https://www.oxfordreference.com/display/10.1093/oi/authority.20110803095412866">Pasquale Anfossi</a> (1727-97) on a libretto by <a href="https://www.britannica.com/biography/Pietro-Metastasio">Pietro Metastasio</a>. The work premiered in Venice at the <a href="https://www.artnet.com/artists/francesco-guardi/the-interior-of-the-teatro-san-benedetto-venice-1UqjxTVRZT2LyYjJdQa0cg2">Teatro San Benedetto</a> in 1773.</p>

<p>The text of the solo song – known in opera <a href="https://www.operacolorado.org/blog/opera-explained-what-is-an-aria/">as an aria</a> – is legible in the excerpt on the wall. It reads, “Contro il destin che freme, combatteremo insieme” – “Against quivering destiny, we shall battle together.” </p>

<p>Like many works from the 17th and 18th centuries, the entire opera is lost. I was determined to find out, however, if that particular aria had survived. Sometimes, the “hit tunes” of an opera were copied or printed separately and performed as “arie staccate” – arias that were “detached” from the rest of the work. </p>

<p>Luck was on my side: To my delight, I found <a href="https://www.internetculturale.it/jmms/iccuviewer/iccu.jsp?id=oai%3Awww.internetculturale.sbn.it%2FTeca%3A20%3ANT0000%3AFR0084-01A07_04d&amp;mode=all&amp;teca=MagTeca+-+ICCU">a copy of the aria in a library in Montecassino</a>, a small town southeast of Rome. Why was that particular excerpt chosen to be displayed so prominently on the wall? </p>

<p>Like other institutions in Venice, the Ospedaletto faced financial hardship in the 1770s. Evidence suggests that <a href="https://www.google.com/books/edition/Nel_regno_dei_poveri/ojgtAQAAIAAJ?hl=en">the putte of the Ospedaletto were likely involved in raising the funds</a> for the decoration of the music room. The new hall enabled them to give performances for special guests and benefactors, which brought in substantial donations. Together with Pasquale Anfossi, who was their music teacher from 1773 to 1777, they rallied behind their beloved institution, saving it – at least temporarily – from financial destitution. </p>

<figure>
            <a href="https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="Two girls, one holding music, the other depicted in a side profile, and a man holding sheets of music gazing down at them from behind." data-src="https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip" data-srcset="https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=801&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=801&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=801&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=1007&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=1007&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=1007&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/557295/original/file-20231102-29-c3sj0z.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=237&amp;fit=clip"></p></a>
            <figcaption>
              <span>Italian composer Pasquale Anfossi, holding rolled up sheets of music, makes an appearance in the fresco.</span>
              <span><span>Marica S. Tacconi</span>, <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a></span>
            </figcaption>
          </figure>

<p>“Against quivering destiny, we shall battle together” may well have served as a rallying cry for the putte of the Ospedaletto, who literally “battled together” to preserve their splendid music conservatory.</p>

<p>Incidentally, the putte may also have wanted to honor their teacher, as Pasquale Anfossi, too, is portrayed in Guarana’s fresco, directly behind the young woman holding up his music. </p>

<h2>From wall to concert hall</h2>

<p>One of the aspects I find most rewarding about the study of older music is the process of discovering a work that has been neglected and unheard for hundreds of years and bringing it back to modern audiences.</p>

<p>Inspired by the Ospedaletto’s music room, Liesl Odenweller and I have embarked on a collaborative project that brings back not only the aria on the wall but also other music from the institution that has gone unheard for centuries. Thanks to a generous grant from the <a href="https://www.delmas.org/grantees-venetian-program">Gladys Krieble Delmas Foundation</a>, the <a href="https://venicemusicproject.it/en/">Venice Music Project</a> – the ensemble Liesl co-founded in 2013 – will perform this music in a <a href="https://venicemusicproject.it/en/concert/hidden-treasures-of-the-ospedaletto/">concert in Venice on Dec. 2, 2023</a>.</p>

<figure>
            <a href="https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=1000&amp;fit=clip"><p><img alt="" data-src="https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip" data-srcset="https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=600&amp;h=401&amp;fit=crop&amp;dpr=1 600w, https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=600&amp;h=401&amp;fit=crop&amp;dpr=2 1200w, https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=600&amp;h=401&amp;fit=crop&amp;dpr=3 1800w, https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;h=504&amp;fit=crop&amp;dpr=1 754w, https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=30&amp;auto=format&amp;w=754&amp;h=504&amp;fit=crop&amp;dpr=2 1508w, https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=15&amp;auto=format&amp;w=754&amp;h=504&amp;fit=crop&amp;dpr=3 2262w" sizes="(min-width: 1466px) 754px, (max-width: 599px) 100vw, (min-width: 600px) 600px, 237px" src="https://images.theconversation.com/files/553127/original/file-20231010-24-eqdqj1.jpeg?ixlib=rb-1.1.0&amp;q=45&amp;auto=format&amp;w=754&amp;fit=clip"></p></a>
            <figcaption>
              <span>The musicians of the Venice Music Project rehearse in Venice in December 2022.</span>
              <span><span>Marica S. Tacconi</span></span>
            </figcaption>
          </figure>

<p>Our program will include “Contro il destin” as well as other excerpts from “Antigono” – essentially, all that survives from that opera. In addition, we will include works by Tommaso Traetta (1727-79) and Antonio Sacchini (1730-86) who, like Anfossi, taught the young women, in some cases launching their international music careers.</p>

<p>Because the music of the past was <a href="https://www.britannica.com/art/musical-notation/Evolution-of-Western-staff-notation">written in a notation</a> that’s different from that used today, it’s necessary to translate and input every mark of the original score – notes, dynamics and other expressive marks – into a music notation software to produce a modern score that can be easily read by today’s musicians.</p>

<p>By performing on period instruments and using a historically informed approach, the musicians of the Venice Music Project and I are excited to revive this remarkably beautiful and meaningful music. Its neglect is certainly not a reflection of its artistic quality but rather likely the result of other composers, such as Vivaldi and Mozart, taking over the spotlight and overshadowing the works of other masters. </p>

<p>This music deserves to be heard – as does the story of the young women of the Ospedaletto.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vehicles with higher, more vertical front ends pose greater risk to pedestrians (306 pts)]]></title>
            <link>https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</link>
            <guid>38267588</guid>
            <pubDate>Tue, 14 Nov 2023 18:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians">https://www.iihs.org/news/detail/vehicles-with-higher-more-vertical-front-ends-pose-greater-risk-to-pedestrians</a>, See on <a href="https://news.ycombinator.com/item?id=38267588">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<article id="content" v-track-docs="">
					<section>
						
						
						<span>
							<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too.</p>
						</span>
						
						<p>November 14, 2023</p>
					</section>
					<!-- .article-head -->
					<figure>
						<img src="https://www.iihs.org/media/cbc04042-1dc0-4dae-ab98-4c47a9fe8868/-_C93w/News/2023/111423%20pedestrians%20and%20front-end%20shape/111423-news.jpg" alt="">
						<figcaption></figcaption>
					</figure>
					<p>Vehicles with especially tall front ends are most dangerous to pedestrians, but a blunt profile makes medium-height vehicles deadly too, new research from the Insurance Institute for Highway Safety shows.</p>
					<p>Whatever their nose shape, pickups, SUVs and vans with a hood height greater than 40 inches are about 45 percent more likely to cause fatalities in pedestrian crashes than cars and other vehicles with a hood height of 30 inches or less and a sloping profile, an IIHS study of nearly 18,000 pedestrian crashes found. However, among vehicles with hood heights between 30 and 40 inches, a blunt, or more vertical, front end increases the risk to pedestrians.</p>
					<p>“Some of today’s vehicles are pretty intimidating when you’re passing in front of them in a crosswalk,” IIHS President David Harkey said. “These results tell us our instincts are correct: More aggressive-looking vehicles can indeed do more harm.”</p>
					<p>Pedestrian crash deaths have risen 80 percent since hitting their low in 2009. Nearly 7,400 walkers — more than 20 people a day — lost their lives in 2021 after being struck by a vehicle. While speeding and poorly designed infrastructure have helped fuel the increase, many safety advocates have also drawn a connection to the growing portion of the U.S. vehicle fleet made up of pickups and SUVs.</p>
					<p>Over the past 30 years, the average U.S. passenger vehicle has gotten about 4 inches wider, 10 inches longer, 8 inches taller and 1,000 pounds heavier. Many vehicles are more than 40 inches tall at the leading edge of the hood. On some large pickups, the hoods are almost at eye level for many adults.</p>
					<p>To examine the connection between fatality risk and vehicle size and shape, IIHS researchers analyzed 17,897 crashes involving a single passenger vehicle and a single pedestrian. Using Vehicle Identification Numbers to identify the crash-involved vehicles, they calculated key front-end measurements corresponding to 2,958 unique car, minivan, large van, SUV and pickup models from photographs. They excluded vehicles with pedestrian automatic emergency braking systems and controlled for other factors that could affect the likelihood of a fatality, such as the speed limit and age and sex of the struck pedestrian.</p>
					<p>Vehicles with hoods more than 40 inches off the ground at the leading edge and a grille sloped at an angle of 65 degrees or less were 45 percent more likely to cause pedestrian fatalities than those with a similar slope and hood heights of 30 inches or less. Vehicles with hood heights of more than 40 inches and blunt front ends angled at greater than 65 degrees were 44 percent more likely to cause fatalities.</p>
					<p>“Manufacturers can make vehicles less dangerous to pedestrians by lowering the front end of the hood and angling the grille and hood to create a sloped profile,” said IIHS Senior Research Transportation Engineer Wen Hu, the lead author of the study. “There’s no functional benefit to these massive, blocky fronts.”</p>
					<p>While sloping front ends did not reduce the risk posed by vehicles with the tallest hoods, they did make a difference for vehicles with hood heights of 30-40 inches. Compared with low and sloped vehicles, medium-height vehicles with blunt fronts were 26 percent more likely to cause pedestrian fatalities. In contrast, the risk of a fatality was about the same for medium-height vehicles with sloped fronts as for low vehicles with either blunt or sloped fronts.</p>
					<p>The researchers looked at several other vehicle characteristics, including the angle of the windshield, length of the hood and angle of the hood. Among these, the slope of the hood had the biggest effect. There was a 25 percent increase in the risk of a fatality for vehicles with flat hoods — those with angles of 15 degrees or less — compared with vehicles with more sloping hoods. That was true regardless of height and front-end shape.</p>
					<p>To better understand how vehicles of different geometries injure pedestrians, IIHS examined detailed records from 121 crashes collected by the International Center for Automotive Medicine Pedestrian Consortium. In each crash, the front end of a car, pickup or SUV struck a teenager or adult. The data included detailed crash reconstructions, including information about the motion of the pedestrian’s body during the crash and the nature and severity of their injuries. The reports also included the year, make and model of the striking vehicle and the height of the pedestrian.</p>
					<p>The researchers used the same measurements as those used in the larger study to define vehicles with blunt and sloped front ends and tall and short ones. For this study, however, they divided the involved vehicles into only two height groups because of the smaller sample size. Taller vehicles were defined as those with a hood leading edge more than 35 inches off the ground. Shorter ones were those with a hood leading edge 35 inches or less from the ground.</p>
					<p>In general, vehicles taller than 35 inches were more dangerous to pedestrians than the shorter ones, mainly because they tended to cause more severe head injuries. Among vehicles taller than 35 inches, those with vertical front ends were more dangerous than those with sloped front ends. Torso and hip injuries from these vehicles were more frequent and severe.</p>
					<p>Unlike all other vehicle types, tall and blunt vehicles primarily inflicted torso injuries with their front ends rather than with the tops of their hoods. They were more likely to injure pedestrians by throwing them forward, while tall and sloped vehicles usually rolled them onto the hood of the vehicle first.</p>
					<p>Pedestrians who were shorter relative to the height of the striking vehicle also suffered more severe injuries.</p>
					<p>“It’s clear that the increasing size of the vehicles in the U.S. fleet is costing pedestrians their lives,” Harkey said. “We encourage automakers to consider these findings and take a hard look at the height and shape of their SUVs and pickups.”</p>
					<h3>Comparative risk of pedestrian&nbsp;fatality&nbsp;by hood leading edge height and shape</h3>
					<figure><img src="https://www.iihs.org/media/4e3e3373-3392-4c3d-9ce4-d5d171781b80/s-IsXg/News/2023/111423%20pedestrians%20and%20front-end%20shape/front-end-graphic-all.png" alt="Vehicle front-end shapes"></figure>
					<!-- .article-text -->
					
				</article>
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unix Time reaches 1.7 billion today (273 pts)]]></title>
            <link>https://www.epochconverter.com/countdown?q=1700000000</link>
            <guid>38266923</guid>
            <pubDate>Tue, 14 Nov 2023 17:59:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.epochconverter.com/countdown?q=1700000000">https://www.epochconverter.com/countdown?q=1700000000</a>, See on <a href="https://news.ycombinator.com/item?id=38266923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="countdownform"><p>Countdown in seconds to any other <a href="https://www.epochconverter.com/#tools" title="click to create a timestamp">Unix timestamp</a>:</p><form action="/countdown" method="get"></form>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Insanely Fast Whisper: Transcribe 300 minutes of audio in less than 98 seconds (163 pts)]]></title>
            <link>https://github.com/chenxwh/insanely-fast-whisper</link>
            <guid>38266833</guid>
            <pubDate>Tue, 14 Nov 2023 17:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/chenxwh/insanely-fast-whisper">https://github.com/chenxwh/insanely-fast-whisper</a>, See on <a href="https://news.ycombinator.com/item?id=38266833">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-insanely-fast-whisper" dir="auto"><a href="#insanely-fast-whisper">Insanely Fast Whisper</a></h2>
<p dir="auto">Powered by 🤗 <em>Transformers</em>, <em>Optimum</em> &amp; <em>flash-attn</em></p>
<p dir="auto"><strong>TL;DR</strong> - Transcribe <strong>300</strong> minutes (5 hours) of audio in less than <strong>98</strong> seconds - with <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">OpenAI's Whisper Large v3</a>. Blazingly fast transcription is now a reality!⚡️</p>
<p dir="auto">Not convinced? Here are some benchmarks we ran on a free <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main/notebooks">Google Colab T4 GPU</a>! 👇</p>
<table>
<thead>
<tr>
<th>Optimisation type</th>
<th>Time to Transcribe (150 mins of Audio)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformers (<code>fp32</code>)</td>
<td>~31 (<em>31 min 1 sec</em>)</td>
</tr>
<tr>
<td>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~5 (<em>5 min 2 sec</em>)</td>
</tr>
<tr>
<td><strong>Transformers (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~2 (<em>1 min 38 sec</em>)</strong></td>
</tr>
<tr>
<td>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>bettertransformer</code>)</td>
<td>~3 (<em>3 min 16 sec</em>)</td>
</tr>
<tr>
<td><strong>distil-whisper (<code>fp16</code> + <code>batching [24]</code> + <code>Flash Attention 2</code>)</strong></td>
<td><strong>~1 (<em>1 min 18 sec</em>)</strong></td>
</tr>
<tr>
<td>Faster Whisper (<code>fp16</code> + <code>beam_size [1]</code>)</td>
<td>~9.23 (<em>9 min 23 sec</em>)</td>
</tr>
<tr>
<td>Faster Whisper (<code>8-bit</code> + <code>beam_size [1]</code>)</td>
<td>~8 (<em>8 min 15 sec</em>)</td>
</tr>
</tbody>
</table>
<p dir="auto">Try the Relicate demo here: <a href="https://replicate.com/cjwbw/insanely-fast-whisper" rel="nofollow"><img src="https://camo.githubusercontent.com/9266324ca64204ed64b925470d4309e352de02642b18b378a22061644b478972/68747470733a2f2f7265706c69636174652e636f6d2f636a7762772f696e73616e656c792d666173742d776869737065722f6261646765" alt="Replicate" data-canonical-src="https://replicate.com/cjwbw/insanely-fast-whisper/badge"></a></p>
<h2 tabindex="-1" id="user-content--blazingly-fast-transcriptions-via-your-terminal-️" dir="auto"><a href="#-blazingly-fast-transcriptions-via-your-terminal-️">🆕 Blazingly fast transcriptions via your terminal! ⚡️</a></h2>
<p dir="auto">We've added a CLI to enable fast transcriptions. Here's how you can use it:</p>
<p dir="auto">Install <code>insanely-fast-whisper</code> with <code>pipx</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx install insanely-fast-whisper"><pre>pipx install insanely-fast-whisper</pre></div>
<p dir="auto">Run inference from any path on your computer:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL>"><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">🔥 You can run <a href="https://huggingface.co/openai/whisper-large-v3" rel="nofollow">Whisper-large-v3</a> w/ <a href="https://github.com/Dao-AILab/flash-attention">Flash Attention 2</a> from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --file-name <filename or URL> --flash True "><pre>insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span> --flash True </pre></div>
<p dir="auto">🌟 You can run <a href="https://huggingface.co/distil-whisper" rel="nofollow">distil-whisper</a> directly from this CLI too:</p>
<div dir="auto" data-snippet-clipboard-copy-content="insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <filename or URL> "><pre>insanely-fast-whisper --model-name distil-whisper/large-v2 --file-name <span>&lt;</span>filename or URL<span>&gt;</span> </pre></div>
<p dir="auto">Don't want to install <code>insanely-fast-whisper</code>? Just use <code>pipx run</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pipx run insanely-fast-whisper --file-name <filename or URL>"><pre>pipx run insanely-fast-whisper --file-name <span>&lt;</span>filename or URL<span>&gt;</span></pre></div>
<p dir="auto">Note: The CLI is opinionated and currently only works for Nvidia GPUs. Make sure to check out the defaults and the list of options you can play around with to maximise your transcription throughput. Run <code>insanely-fast-whisper --help</code> or <code>pipx run insanely-fast-whisper --help</code> to get all the CLI arguments and defaults.</p>
<h2 tabindex="-1" id="user-content-how-to-use-it-without-a-cli" dir="auto"><a href="#how-to-use-it-without-a-cli">How to use it without a CLI?</a></h2>
<p dir="auto">For older GPUs, all you need to run is:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                device=&quot;cuda:0&quot;)

pipe.model = pipe.model.to_bettertransformer()

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]"><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>pipe</span>.<span>model</span> <span>=</span> <span>pipe</span>.<span>model</span>.<span>to_bettertransformer</span>()

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]</pre></div>
<p dir="auto">For newer (A10, A100, H100s), use <a href="https://github.com/chenxwh/insanely-fast-whisper/blob/main">Flash Attention</a>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="import torch
from transformers import pipeline

pipe = pipeline(&quot;automatic-speech-recognition&quot;,
                &quot;openai/whisper-large-v2&quot;,
                torch_dtype=torch.float16,
                model_kwargs={&quot;use_flash_attention_2&quot;: True},
                device=&quot;cuda:0&quot;)

outputs = pipe(&quot;<FILE_NAME>&quot;,
               chunk_length_s=30,
               batch_size=24,
               return_timestamps=True)

outputs[&quot;text&quot;]                "><pre><span>import</span> <span>torch</span>
<span>from</span> <span>transformers</span> <span>import</span> <span>pipeline</span>

<span>pipe</span> <span>=</span> <span>pipeline</span>(<span>"automatic-speech-recognition"</span>,
                <span>"openai/whisper-large-v2"</span>,
                <span>torch_dtype</span><span>=</span><span>torch</span>.<span>float16</span>,
                <span>model_kwargs</span><span>=</span>{<span>"use_flash_attention_2"</span>: <span>True</span>},
                <span>device</span><span>=</span><span>"cuda:0"</span>)

<span>outputs</span> <span>=</span> <span>pipe</span>(<span>"&lt;FILE_NAME&gt;"</span>,
               <span>chunk_length_s</span><span>=</span><span>30</span>,
               <span>batch_size</span><span>=</span><span>24</span>,
               <span>return_timestamps</span><span>=</span><span>True</span>)

<span>outputs</span>[<span>"text"</span>]                </pre></div>
<h2 tabindex="-1" id="user-content-roadmap" dir="auto"><a href="#roadmap">Roadmap</a></h2>
<ul>
<li> Add a light CLI script</li>
<li> Deployment script with Inference API</li>
</ul>
<h2 tabindex="-1" id="user-content-community-showcase" dir="auto"><a href="#community-showcase">Community showcase</a></h2>
<p dir="auto">@ochen1 created a brilliant MVP for a CLI here: <a href="https://github.com/ochen1/insanely-fast-whisper-cli">https://github.com/ochen1/insanely-fast-whisper-cli</a> (Try it out now!)</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reptar (496 pts)]]></title>
            <link>https://lock.cmpxchg8b.com/reptar.html</link>
            <guid>38266773</guid>
            <pubDate>Tue, 14 Nov 2023 17:49:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lock.cmpxchg8b.com/reptar.html">https://lock.cmpxchg8b.com/reptar.html</a>, See on <a href="https://news.ycombinator.com/item?id=38266773">Hacker News</a></p>
<div id="readability-page-1" class="page">

<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#discovery">Discovery</a></li>
<li><a href="#solution">Solution</a></li>
<li><a href="#notes">Notes</a></li>
</ul>
</nav>
<p>We have a CPU mystery! We found a way to cause some processors to enter a glitch state where the normal rules don’t apply, but what does that mean…?</p>
<p>If you’re interested what can go wrong inside modern CPUs, read on!</p>
<section id="introduction">
<h2>Introduction</h2>
<p>If you’ve ever written any x86 assembly at all, you’ve probably used <code>rep movsb</code>. It’s the idiomatic way of moving memory around on x86. You set the <em>source</em>, <em>destination</em>, <em>direction</em> and the <em>count</em> - then just let the processor handle all the details!</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb1-2"><span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb1-3"><span>std</span></span>
<span id="cb1-4"><span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb1-5">rep <span>movsb</span></span></code></pre></div>
<p>The actual instruction here is <code>movsb</code>, the <code>rep</code> is simply a prefix that changes how the instruction works. In this case, it indicates that you want this operation <strong>rep</strong>eated multiple times.</p>
<p>There are lots of other prefixes too, but they don’t all apply to every instruction.</p>
<section id="prefix-decoding">
<h4>Prefix Decoding</h4>
<p>An interesting feature of x86 is that the instruction decoding is generally quite relaxed. If you use a prefix that doesn’t make sense or conflicts with other prefixes nothing much will happen, it will usually just be ignored.</p>
<p>This fact is sometimes useful; compilers can use redundant prefixes to pad a single instruction to a desirable alignment boundary.</p>
<p>Take a look at this snippet, this is exactly the same code as above, just a bunch of useless or redundant prefixes have been added:</p>
<div id="cb2"><pre><code><span id="cb2-1">            rep <span>lea</span> <span>rdi</span>, [rel dst]</span>
<span id="cb2-2">             <span>cs</span> <span>lea</span> <span>rsi</span>, [rel src]</span>
<span id="cb2-3">       <span>gs</span> <span>gs</span> <span>gs</span> <span>std</span></span>
<span id="cb2-4">          repnz <span>mov</span> <span>rcx</span>, <span>32</span></span>
<span id="cb2-5">rep rep rep rep <span>movsb</span></span></code></pre></div>
<p>Perhaps the most interesting prefixes are <code>rex</code>, <code>vex</code> and <code>evex</code>, all of which change how subsequent instructions are decoded.</p>
<p>Let’s take a look at how they work.</p>
</section>
<section id="the-rex-prefix">
<h4>The REX prefix</h4>
<p>The i386 only had 8 general purpose registers, so you could specify which register you want to use in just 3 bits (because 2^3 is 8).</p>
<p>The way that instructions were encoded took advantage of this fact, and reserved <em>just</em> enough bits to specify any of those registers.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-modrm.svg" alt=""><figcaption>modr/m example</figcaption>
</figure>
<p>Simple 2-byte instructions that use modr/m might be encoded like this, for example <code>mov eax, ebx</code>.</p>
<p>This is an 8-bit opcode, 2 bit addressing mode (labeled m), and 3 bits each for the source (s) and destination (d).</p>
</blockquote>
<p>Well, this is a problem, because x86-64 added 8 additional general purpose registers. We now have sixteen possible registers..that’s 2^4, so we’re going to need another bit! 😆</p>
<p>The solution to this is the <code>rex</code> prefix, which gives us some spare bits that the next instruction can borrow.</p>
<p>When we’re talking about rex, we usually write it like this:</p>

<p><code>rex</code> is a single-byte prefix, the first four bits are mandatory and the remaining four bits called <code>b</code>, <code>x</code>, <code>r</code> and <code>w</code> are all optional. If you see <code>rex.rb</code> that means only the <code>r</code> and <code>b</code> bits are set, all the others are unset.</p>
<p>These optional bits give us room to encode more general purpose registers in the following instruction.</p>
<blockquote>
<figure>
<img src="https://lock.cmpxchg8b.com/img/rep-rexmodrm.svg" alt=""><figcaption>rex example</figcaption>
</figure>
<p>The <code>rex</code> prefix can lend the next instruction extra bits to use for operands, so now we can encode all 16 possible general purpose registers!</p>
</blockquote>
<p>Now we’re fine until someone <a href="https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html">adds another register</a>! 😂</p>
</section>
<section id="encoding-rules">
<h4>Encoding Rules</h4>
<p>So now we know that <code>rex</code> increases the available space for encoding operands, and that useless or redundant prefixes are usually ignored on x86. So… what should this instruction do?</p>

<p>The <code>movsb</code> instruction doesn’t have any operands - they’re all implicit - so any <code>rex</code> bits are meaningless, right?</p>
<p>If you guessed that the processor will just silently ignore the <code>rex</code> prefix, you would be correct!</p>
<p>Well… except on machines that support a new feature called <em>fast short repeat move</em>! We discovered that a bug with redundant <code>rex</code> prefixes could interact with this feature in an unexpected way and introduce a serious vulnerability, oops 🙂</p>
</section>
<section id="fast-short-repeat-move">
<h4>Fast Short Repeat Move</h4>
<p>FSRM is a new feature introduced in <a href="https://en.wikichip.org/wiki/intel/microarchitectures/ice_lake_(client)#New_instructions">Ice Lake</a> that fixes some of the shortcomings of ERMS. Hopefully that clears up any confusion. 😆</p>
<p>Just kidding, let’s quickly look at ERMS.</p>
<p>The hard part of moving strings around efficiently is getting all the buffers aligned so you can use the widest possible stores available. You <em>could</em> do this in software, but if we do it in microcode then the processor can just transparently make your existing code faster for you.</p>
<p>This requires some expensive setup, but once that’s done you get vastly improved throughput. This feature is known as <em>enhanced repeat move/store</em>, ERMS.</p>
<p>If you have a processor with ERMS support, simple <code>rep movsb</code> operations can sometimes perform comparably with more complicated hand-tuned vector move operations.</p>
<p>However, there is a problem with ERMS. That initial setup is so expensive that it just isn’t worth it for very short strings. This is what FSRM is designed to solve, it handles the case of only moving 128 bytes or less and makes that faster too!</p>
<p>I’m not aware of any documentation that explains exactly how FSRM works, but you can check if you have a processor that supports it by looking at the flags line in <code>/proc/cpuinfo</code>:</p>
<pre><code>flags       : fpu vme de pse tsc msr pae mce cx8 [...] fsrm</code></pre>
<p>Some of the processors that have this feature include:</p>
<ul>
<li>Ice Lake</li>
<li>Rocket Lake</li>
<li>Tiger Lake</li>
<li>Raptor Lake</li>
<li>Alder Lake</li>
<li>Sapphire Rapids</li>
</ul>
<blockquote>
<p>Note: This list may not be comprehensive, please see Intel advisory INTEL-SA-00950 for a complete list.</p>
</blockquote>
</section>
</section>
<section id="discovery">
<h2>Discovery</h2>
<p>I’ve written previously about a processor validation technique called <em>Oracle Serialization</em> that we’ve been using. The idea is to generate two forms of the same randomly generated program and verify their final state is identical.</p>
<blockquote>
<p>You can read more about Oracle Serialization in my <a href="https://lock.cmpxchg8b.com/zenbleed.html">previous writeup</a>.</p>
</blockquote>
<p>In August, our validation pipeline produced an interesting assertion. It had found a case where adding redundant <code>rex.r</code> prefixes to an FSRM optimized <code>rep movs</code> operation seemed to cause unpredictable results.</p>
<p>We observed some very strange behavior while testing. For example, branches to unexpected locations, unconditional branches being ignored and the processor no longer accurately recording the instruction pointer in <code>xsave</code> or <code>call</code> instuctions.</p>
<p>Oddly, when trying to understand what was happening we would see a debugger reporting impossible states!</p>
<p>This already seemed like it could be indicative of a serious problem, but within a few days of experimenting we found that when multiple cores were triggering the same bug, the processor would begin to report machine check exceptions and halt.</p>
<p>We verified this worked even inside an unprivileged guest VM, so this already has serious security implications for cloud providers. Naturally, we reported this to Intel as soon as we confirmed this was a security issue.</p>
<section id="reproduce">
<h4>Reproduce</h4>
<p>We’re publishing all of our research today to our <a href="https://github.com/google/security-research/">security research repository</a>. If you want to reproduce the vulnerability you can use our <code>icebreak</code> tool, I’ve also made a local mirror available <a href="https://lock.cmpxchg8b.com/files/icebreak.tar.gz">here</a>.</p>
<pre><code>$ ./icebreak -h
usage: ./icebreak [OPTIONS]
    -c N,M      Run repro threads on core N and M.
    -d N        Sleep N usecs between repro attempts.
    -H N        Spawn a hammer thread on core N.
icebreak: you must at least specify a core pair with -c! (see -h for help)</code></pre>
<p>The testcase enters what should be an infinite loop, and unaffected systems should see no output at all. On affected systems, a <code>.</code> is printed on each successful reproduction.</p>
<pre><code>$ ./icebreak -c 0,4
starting repro on cores 0 and 4
.........................................................................
.........................................................................
.........................................................................
.........................................................................
.........................................................................</code></pre>
<p>In general, if the cores are <abbr title="Symmetric Multithreading">SMT</abbr> siblings then you may observe random branches and if they’re <abbr title="Symmetric Multiprocessing">SMP</abbr> siblings from the same package then you may observe machine checks.</p>
<p>If you do <em>not</em> specify two different cores, then you might need to use a hammer thread to trigger a reproduction.</p>
</section>
<section id="analysis">
<h4>Analysis</h4>
<p>We know something strange is happening, but how microcode works in modern systems is a closely guarded secret. We can only theorize about the root cause based on observations.</p>
<section id="μops">
<h5>μops</h5>
<p>The CPU is split in two major components, the <em>frontend</em> and the <em>backend</em>. The frontend is responsible for fetching instructions, decoding them and generating μops to send to the backend for execution.</p>
<p>The backend executes instructions <em>out of order</em>, and uses a unit called the ROB, <em>reorder buffer</em>, to store and organize results.</p>
<p>We believe this bug causes the frontend to miscalculate the size of the <code>movsb</code> instruction, causing subsequent entries in the ROB to be associated with incorrect addresses. When this happens, the CPU enters a confused state that causes the instruction pointer to be miscalculated.</p>
<p>The machine can eventually recover from this state, perhaps with incorrect intermediate results, but becoming internally consistent again. However, if we cause multiple SMT or SMP cores to enter the state simultaneously, we can cause enough microarchitectural state corruption to force a machine check.</p>
</section>
</section>
<section id="questions">
<h4>Questions</h4>
<p>I’m sure some readers will have questions about what is possible in this unexpected “glitch” state. Well, so do we!</p>
<p>We know that we can corrupt the system state badly enough to cause machine check errors, and we’ve also observed threads interfere with execution of processes scheduled on SMT siblings.</p>
<p>However, we simply don’t know if we can control the corruption precisely enough to achieve privilege escalation. I suspect that it <em>is</em> possible, but we don’t have any way to debug μop execution!</p>
<p>If you’re interested in studying this, then we would love to get your input!</p>
</section>
<section id="credit">
<h4>Credit</h4>
<p>This bug was independently discovered by multiple research teams within Google, including the <a href="https://github.com/google/silifuzz">silifuzz</a> team and Google <a href="https://bughunters.google.com/blog">Information Security Engineering</a>. The bug was analyzed by Tavis Ormandy, Josh Eads, Eduardo Vela Nava, Alexandra Sandulescu and Daniel Moghimi.</p>
</section>
</section>
<section id="solution">
<h2>Solution</h2>
<p>Intel have <a href="https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00950.html">published</a> updated microcode for all affected processors. Your operating system or BIOS vendor may already have an update available!</p>
<section id="workaround">
<h4>Workaround</h4>
<p>If you can’t update for some reason, you <em>could</em> disable fast strings via the <code>IA32_MISC_ENABLE</code> model specific register.</p>
<p>This will cause a significant performance penalty, and should not be used unless absolutely necessary.</p>
</section>
</section>
<section id="notes">
<h2>Notes</h2>
<p>If you’re interested in more CPU bugs, we publish everything we find!</p>
<p>Not all the bugs we discover have security consequences, but they’re usually worth reading! For example, did you know that sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/genoa-lps-hps">movlps just doesn’t work</a>? or that registers can sometimes <a href="https://github.com/google/security-research/tree/master/pocs/cpus/errata/amd/1386">roll back</a> to previous values?</p>
</section>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rivian software update bricks infotainment system, fix not obvious (249 pts)]]></title>
            <link>https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</link>
            <guid>38266340</guid>
            <pubDate>Tue, 14 Nov 2023 17:22:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/">https://electrek.co/2023/11/14/rivian-software-update-bricks-infotainment-system-fix-not-obvious/</a>, See on <a href="https://news.ycombinator.com/item?id=38266340">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?quality=82&amp;strip=all&amp;w=1500" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2023/09/Rivian-R1T-Apocalypse-13.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1500" height="1000" alt="Rivian-R1T-Apocalypse" fetchpriority="high">
	
	</figure>

<p>On Monday, Rivian released an incremental software update 2023.42, which bricked the infotainment system in R1Ses and R1Ts. The company is frantically working on a fix, but it might not be an OTA…</p>



<p>Update: The company has acknowledged the issue with affected customers but has yet to issue a fix or plan to fix…</p>



<p><a href="https://www.linkedin.com/in/wassymbensaid/">Rivian’s vice president of software engineering</a>, Wassim Bensaid, <a href="https://www.reddit.com/r/Rivian/comments/17usikn/202342_ota_update_issue/">took to Reddit</a> (!?) to update users on the situation.</p>



<blockquote>
<p>Hi All,</p>



<p>We made an error with the 2023.42 OTA update – a fat finger where the wrong build with the wrong security certificates was sent out. We cancelled the campaign and we will restart it with the proper software that went through the different campaigns of beta testing.</p>



<p>Service will be contacting impacted customers and will go through the resolution options. That may require physical repair in some cases.</p>



<p>This is on us – we messed up. Thanks for your support and your patience as we go through this.</p>



<p>* Update 1 (11/13, 10:45 PM PT): The issue impacts the infotainment system. In most cases, the rest of the vehicle systems are still operational. A vehicle reset or sleep cycle will not solve the issue. We are validating the best options to address the issue for the impacted vehicles. Our customer support team is prioritizing support for our customers related to this issue. Thank you.</p>
</blockquote>



<p>That’s the last update we had over 10 hours after Rivian customer vehicles were fed the bad software update. Rivian’s PR team hasn’t replied to requests for comment. </p>



<p>The vehicles are drivable, but software and displays go black. It appears that the 2023.42 software update hangs at 90% on the vehicle screen or 50% on the app screen, and then the vehicle screens black out. All systems appear to still work except for the displays.</p>



<p>One Reddit user noted:</p>



<blockquote>
<p>Remotely setting climate controls appear to be working for me. You can’t adjust it while seated in the vehicle (feature to prevent competing for changes from inside and outside the vehicle?) but for those in cold/hot weather, you can at least pre-set and pre-heat/cool your vehicle even without the infotainment – at least if yours ended up in the same state that mind did.</p>
</blockquote>



<figure></figure>



<p>At the moment, it appears that Amazon vans are not impacted. We reached out to Rivian for comment and got the following response from a spokesperson for the automaker:</p>



<blockquote>
<p>We’ve identified an issue in our recent software update 2023.42.0 that impacts Rivian’s R1T and R1S infotainment system. In most cases, the rest of the vehicle systems are operational. We’ve paused the release of the update and our customer support team is prioritizing support for our customers related to this issue.</p>
</blockquote>



<p>This story is ongoing, and we will update as such.</p>



<p>Update: Rivian has been Texting and emailing affected customers</p>



<figure></figure>



<h2 id="h-electrek-s-take">Electrek’s Take</h2>




	<p>This is a big deal, and not for whatever this fix ends up being, but about trusting the Rivian software team to deliver stuff that won’t break your car. </p>



<p>The fact that this could even happen is very troubling. A bad certificate should not be pushed via a fat thumb that causes the fleet to be bricked. The architecture shouldn’t have been designed this way. </p>



<p>I personally tried updating my vehicle last night before the errors were reported, and if I had been a few minutes earlier, I would also now have a bricked infotainment with the closest software center 200+ miles away and my vehicle covered in a few inches of fresh Vermont snow. </p>



<p>Hopefully, Rivian’s software team (who have probably had an awful night) can come up with an OTA or, more likely, a USB-stick type of update that affected owners can easily and quickly apply. </p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMKqD-Qow6c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add Electrek to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<div><p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p><p><a href="https://bit.ly/3N3YHyD"><img src="https://electrek.co/wp-content/uploads/sites/3/2023/11/VMAX-Electrek-Banner-Nov-13.jpg?quality=82&amp;strip=all" alt="" width="750" height="150"></a></p></div>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Miyazaki's 'The Boy and the Heron' makes clear the world is a fragile place (116 pts)]]></title>
            <link>https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</link>
            <guid>38266216</guid>
            <pubDate>Tue, 14 Nov 2023 17:14:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron">https://www.latimes.com/entertainment-arts/awards/story/2023-11-13/japanese-animation-master-hayao-miyazaki-boy-and-the-heron</a>, See on <a href="https://news.ycombinator.com/item?id=38266216">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-element="story-body" data-subscriber-content=""> <p>When Hayao Miyazaki’s acclaimed new movie “The Boy and the Heron” made its North American premiere at the Toronto International Film Festival in September, one of the Japanese master’s biggest fans stepped on stage to introduce the long-awaited movie. “We are privileged enough to be living in a time where Mozart is composing symphonies,” said Oscar-winning filmmaker Guillermo del Toro. “Miyazaki <i>san</i> is a master of that stature, and we are so lucky to be here. He has changed the medium that he started in, revolutionized it, proved over and over again that it is a tremendous work of art.”</p><p>“Miyazaki, in my estimation, is the greatest director of animation ever, and he has made his films as full of dialogues and questions as he is,” continued Del Toro. “These are not easy films, but these are films that portray him so intimately, that you feel you’re having a conversation with him. And they are paradoxical because he understands that beauty cannot exist without horror, and delicacy cannot exist without brutality.”</p><p>Fans of the 82-year-old writer-director-animator and co-founder of Studio Ghibli are keenly aware of the magical powers of Miyazaki’s cinematic universe. Perhaps more than any other filmmaker alive today, he has been able to transport audiences to dreamlike worlds that have a distinctive, hand-crafted look and strange logic of their own. Yet, they speak to audiences all over the world because their main characters experience the full spectrum of emotions and experiences. The lead characters of the director’s 12 movies, which include the Oscar-winning “Spirited Away,” “My Neighbor Totoro” and “Princess Mononoke” stumble upon ethereal worlds and supernatural characters, but they are bound by human feelings of joy, love, fear and grief.</p><p>It’s a testament to the enduring appeal of the filmmaker that his new movie broke box-office records in Japan with little advance marketing or publicity ahead of its July release date. (As of early this month, the film has made over $63.5 million worldwide). The only clues potential audiences had about the film was a poster featuring the film’s very odd titular bird and the fact that its original title, “How Do You Live?,” was taken from a Genzaburō Yoshino book about a 15-year-old boy who has to learn about survival in a world consumed by death, war and sadness.</p><p>Echoing some of the themes and plotlines from Miyazaki’s oeuvre, the movie centers on a young Mahito, who encounters magical creatures and talking animals in a surreal world — like a cross between Lewis Carroll’s “Alice in Wonderland” and Ingmar Bergman’s “Fanny and Alexander.” The boy, who has lost his mother in a hospital fire in Tokyo, has a tough time adjusting to life with his father’s new wife — his mother’s younger sister — and their new home in the countryside. </p><p>Then, there’s the strange gray heron who badgers him and claims that his mother is still alive, and the mysterious tower built by Mahito’s long-vanished granduncle. This is all before our young hero enters the alternate world of the Tower, where he encounters menacing man-eating parakeets, younger versions of some of the characters from his real life, and cute bubble-like souls called the Warawara, who have to float to the surface to be born in Mahito’s world.</p><div data-click="enhancement" data-align-center=""><figure> <picture>    <source type="image/webp" srcset="https://ca-times.brightspotcdn.com/dims4/default/a05e864/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/853e000/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/88040b3/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/3f079e6/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/edc3840/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/format/webp/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw">     <img alt="A girl chases two small creatures through a tunnel of trees in the animated film &quot;My Neighbor Totoro.&quot;" srcset="https://ca-times.brightspotcdn.com/dims4/default/b44e202/2147483647/strip/true/crop/2024x1162+0+0/resize/320x184!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 320w,https://ca-times.brightspotcdn.com/dims4/default/f1195c4/2147483647/strip/true/crop/2024x1162+0+0/resize/568x326!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 568w,https://ca-times.brightspotcdn.com/dims4/default/8cb3eea/2147483647/strip/true/crop/2024x1162+0+0/resize/768x441!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 768w,https://ca-times.brightspotcdn.com/dims4/default/0b3e89e/2147483647/strip/true/crop/2024x1162+0+0/resize/1024x588!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1024w,https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg 1200w" sizes="100vw" width="1200" height="689" src="https://ca-times.brightspotcdn.com/dims4/default/62c89d0/2147483647/strip/true/crop/2024x1162+0+0/resize/1200x689!/quality/75/?url=https%3A%2F%2Fcalifornia-times-brightspot.s3.amazonaws.com%2F46%2F7a%2F7906d039481988e9774bb800d8a4%2Fghi-totoro-select01.jpg" decoding="async" loading="lazy">  </picture>  <div>   <p>Mei follows two small spirits in “My Neighbor Totoro.”</p>   <p>(© 1988 Studio Ghibli / GKids)</p>   </div>  </figure></div><p>As Del Toro noted in a recent interview, “Miyazaki proves again and again that his films are not about leaving you chirpy: It’s about showing you the sweetness and sourness of life— the loss, the love and the beauty all at the same time. Hitchcock said repetition with consciousness is style: Miyazaki is saying, ‘I didn’t get it quite right in that movie. Let me get it right on this one.’ Rhythmically, he is very contemplative. This is an old master who has dispensed with tools and flourishes. They stop trying to impress. They offer a simple and beautiful gesture with a brush.”</p><p>Indie distributor GKIDS, which handles Studio Ghibli titles in the U.S., will release the movie on Dec. 8, following special preview engagements in New York and Los Angeles next week. The English-language voice cast includes Florence Pugh, Christian Bale, Dave Bautista, Gemma Chan, Mark Hamill, Karen Fukuhara, Willem Dafoe and Luca Padovan.</p><p>“I first saw ‘The Boy and the Heron’ at Studio Ghibli earlier this year, and after five minutes I began to cry,” recalls Eric Beckman, founder of GKIDS and L.A.’s Animation Is Film Festival. “There was another Miyazaki film in the world. What a gift. After ‘Totoro,’ ‘Spirited Away,’ ‘Princess Mononoke’ and ‘Howl’s Moving Castle,’ it’s easy to take for granted how unique Miyazaki’s gifts are. The beauty, the lusciousness, the humor and playfulness, the nuance are all here ... the movement of the heron in flight, of fish in the water, the tottering old aunties. Nothing can compare.”</p><p>Beckman agrees with many critics who have pointed out that “The Boy and the Heron” is Miyazaki’s most personal film by far. “In this movie, we have a great creator struggling to express something profound and important about life itself — both what to say and how to say it,” he says. “If ‘Totoro’ embodied a basic optimism — a celebration of the power and miraculous wonder of nature, a trust in nature — after 35 years, ‘Heron’ is not so optimistic about the state of the world anymore. This is not a kids’ cartoon or an episode in a superhero franchise. It is a work of cinematic art made by one of our greatest living filmmakers. Miyazaki is saying that the world is a fragile place, it may only last another day. What is the next step forward? That is up to the viewer to decide.”</p><p>In 2021, Miyazaki himself told the New York Times that he has a simple goal in mind whenever he sets out to make a new movie. “The mission of my films is to comfort you — to fill in the gap that might be in your heart or your everyday life.” When he was asked about the answer to the Japanese title of his movie “How Do You Live?” Miyazaki <i>san</i> responded, “I am making this movie because I do not have the answer.”</p><p>He might not have provided us with an easy answer, but he has once again managed to fill our hearts with the same sense of wonder we felt the first time we entered one of his remarkable realms.</p> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YJIT is the most memory-efficient Ruby JIT (200 pts)]]></title>
            <link>https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</link>
            <guid>38265773</guid>
            <pubDate>Tue, 14 Nov 2023 16:46:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/">https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/</a>, See on <a href="https://news.ycombinator.com/item?id=38265773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This year, the YJIT team and I have gotten a paper accepted at
<a href="https://2023.splashcon.org/home/mplr-2023">MPLR 2023</a> (Managed Programming Languages and Runtimes),
which is now freely available through <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">ACM open access</a>.
The paper, titled <em>“Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach”</em>, goes
into details of the strategy taken to evaluate YJIT’s performance in a production context.
One of our key findings, when comparing YJIT against other existing Ruby JITs such as JRuby
and TruffleRuby, is that YJIT is the most memory-efficient Ruby JIT (by a long shot).</p>

<p>A video recording of our presentation at MPLR is also
<a href="https://www.youtube.com/watch?v=fMGuQXNqlaE&amp;t=9900s">available on YouTube</a>.</p>

<h2 id="background">Background</h2>

<p>Many published papers about JIT compilers only look at peak performance in terms of
running time on benchmarks after a certain amount of warm-up time.
This can be deceptive because the amount of time needed for a JIT compiler to warm up can be
arbitrarily long. Typically, the JIT compiler implementation is given as many benchmark
iterations as it needs to reach peak performance, and the peak performance as measured then
is reported. The amount of time needed to reach peak performance is often not discussed.
The same goes for memory usage.</p>

<p>I believe that
those metrics are typically ignored by academic compiler researchers
because they may reveal an inconvenient reality. If you give your JIT compiler
an arbitrary amount of time and memory to reach peak performance, it’s easier to throw all
possible known optimizations at a piece of code and reach high peak performance numbers.
However, if your JIT compiler uses an arbitrarily high amount of memory and needs a very long
time to warm up, even though it may have the fastest peak performance, it may be
unusable in most real-world production environments.</p>

<p>When deploying code into production, peak performance is not the only thing that matters.
On our production servers at Shopify, there is not a huge amount of memory available for
the JIT compiler to use. Almost all of the memory is used to run multiple server processes, and
also to cache various resources in RAM. This has forced us to spend a significant amount of
effort on optimizing YJIT’s memory usage to make the compiler more resource-efficient.</p>

<p>At Shopify, we deploy frequently, with consecutive deployments sometimes less than 20 minutes
apart. This adds an extra layer of challenge because, despite these frequent deployments,
we can’t tolerate significant increases in response time.
If a JIT compiler needs a significant amount of time to
warm up, or suddenly deoptimizes large amounts of code, this can translate into requests timing out, and customers
abandoning their shopping carts, which ultimately would result in lost revenue. As such, smooth,
predictable warm-up and stable performance are of critical importance.</p>

<h2 id="methodology">Methodology</h2>

<p>In our paper, we look at YJIT’s warm-up, memory usage and peak performance on benchmarks,
as well as on our deployment on Shopify’s StoreFront Renderer (SFR). For context, SFR
renders all Shopify storefronts, which is the first thing buyers see when they navigate to a store hosted by Shopify.
It is mostly written in Ruby, depends on over 220 Ruby gems, renders over 4.5
million Shopify stores in over 175 countries, and is served by multiple clusters distributed worldwide. It is capable of serving over 1.27
million requests per second, and has processed over 197 billion USD in transaction
volume in 2022.
YJIT is currently deployed to all SFR servers. For this paper, we’ve performed
experiments using Ruby head on a subset of servers in all clusters. We’ve also
included some control servers which ran the same Ruby commit without YJIT. Data
for the SFR experiments was gathered over a 48-hour period.</p>

<p>For our experiments on benchmarks, we used 11 headline benchmarks from the
yjit-bench suite. These are all benchmarks that are based on real-world Ruby gems,
with a bias towards web workloads. This includes benchmarks such as <code>railsbench</code>,
<code>hexapdf</code>, <code>activerecord</code> and <code>liquid-render</code>, which is a benchmark using Shopify’s liquid
template language gem.
We benchmarked YJIT, RJIT (Takashi Kokubun’s experimental Ruby JIT written
in Ruby), JRuby, as well as both the JVM and native versions of TruffleRuby.
More details on our experimental setup
are provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>.</p>

<p>We also maintain a website, <a href="https://speed.yjit.org/">speed.yjit.org</a>, which tracks
YJIT’s performance and memory overhead as well as various other statistics on this
benchmark suite over time. Recently, as we were looking for more challenging and realistic
benchmarks, we’ve also turned the codebase of the
<a href="https://lobste.rs/">lobste.rs</a> website
<a href="https://railsatscale.com/2023-08-25-we-turned-lobsters-into-a-rails-benchmark-for-yjit/">into a benchmark</a> as well.</p>

<h2 id="key-findings">Key Findings</h2>

<h3 id="performance-on-benchmarks">Performance on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_speedup.png" alt="" width="90%"></p>

<p>The above graph shows the average execution time on each benchmark for each of the Ruby JIT
implementations we benchmarked. The time is normalized to the time taken by the CRuby interpreter,
where the time taken by the interpreter has value 1.0, with values below 1 being faster than
the interpreter.
We were very generous in terms of warm-up time. Each benchmark was run for 1000 iterations, and
the first half of all the iterations were discarded as warm-up time, giving each JIT a more
than fair chance to reach peak performance.</p>

<p>As can be seen, TruffleRuby has the best peak performance on most (but not all) benchmarks.
YJIT outperforms the CRuby interpreter on every benchmark by a wide margin. We can also see
that YJIT performs very competitively compared to JRuby (a JVM-based Ruby JIT), outperforming
it on most benchmarks.</p>

<h3 id="warm-up-time-on-benchmarks">Warm-Up Time on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/railsbench_warmup.png" alt="" width="90%"></p>

<p>This graph shows a plot of the performance over time for each Ruby JIT for the <code>railsbench</code> benchmark.
The x-axis is the total execution time in seconds, while the y-axis is the time per benchmark iteration. This
allows us to visualize how the performance of each VM evolves over time. As can be seen, YJIT
almost immediately outperforms the CRuby interpreter, with RJIT not too far behind. JRuby takes over a
minute to reach peak performance, but does not reliably outperform CRuby on this benchmark.</p>

<p>TruffleRuby eventually outperforms the other JITs, but it takes about two minutes to do so. It is
also initially quite a bit slower than the CRuby interpreter, taking over 110
seconds to catch up to the interpreter’s speed. This would be
problematic in a production context such as Shopify’s, because it can lead to much slower
response times for some customers, which could translate into lost business. Such wide variations
in performance can also make the scaling of server resources more difficult. We should also note
that while <code>railsbench</code> is a somewhat challenging benchmark, it is much smaller than our actual
production deployment. Warm-up data for other benchmarks is provided in the paper.</p>

<h3 id="memory-usage-on-benchmarks">Memory Usage on Benchmarks</h3>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_memory.png" alt="" width="90%"></p>

<p>The above graph is in my opinion the most interesting graph of the paper. It is a plot of the memory
usage (RSS) of each Ruby implementation for each benchmark.
Because of the widely varying scale between data points,
the use of a logarithmic was considered <img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/trollface.png" alt="" height="15">.
However, we have decided to use a
linear scale to maintain a more accurate sense of proportions. Do note, however, that there is
a cut in the graph between 5GiB and 17GiB.</p>

<p>As can be seen in the graph above, thanks in large part to the work put in by our team to
optimize its memory
overhead, YJIT has the lowest memory overhead of any Ruby JIT by far. The JVM-based
Ruby implementations often use one or even two orders of magnitude more memory than YJIT and the
CRuby interpreter. The memory overhead compared to CRuby can be as much as several gigabytes.
This is on benchmarks that often require less than 100MiB to run with the CRuby interpreter,
which makes such a high amount of memory overhead seem disproportionate.</p>

<p>One significant caveat here is that we are measuring the total memory usage of each system. That
is, the memory overhead of the JVM itself has an impact. The way that JRuby and TruffleRuby
internally represent Ruby objects in memory, which is different from the way CRuby represents
objects in memory, also has an impact.
However, the bottom line is the same. Using several gigabytes more memory than CRuby to run simple
benchmarks likely bodes poorly for many production deployments.
For smaller production deployments, for example, a project running on inexpensive Virtual Private Servers (VPS),
there may be only 1GiB or 2GiB of RAM available in total.
For a company like Shopify running a large fleet of servers,
the amount of server processes that can be run on a single machine, and how much memory can be used
for caching matters.</p>

<p>There is another caveat, which is that JRuby and TruffleRuby, unlike CRuby, do not use a GVL
(Global VM Lock, analogous to Python’s GIL). In theory, this means that they can more effectively
use multithreading, and amortize some of their memory overhead across multiple server threads.
Still, there is a case to be made that the memory overhead of JRuby and TruffleRuby is something
that is often overlooked and probably should be better optimized.
Aside from production deployments, the <code>ruby-lsp</code> benchmark is a benchmark of the Ruby language
server, which can be used by code editors such as VSCode.
We can see that on this benchmark, the JVM-based Ruby implementations can use multiple gigabytes
of memory, and despite that, perform worse than the CRuby interpreter. This is far from ideal
for a Ruby gem that is meant to be run on developer laptops.</p>

<p>I would also like to note that RJIT, Takashi Kokubun’s experimental pure Ruby JIT, looks
quite good in this comparison. However, in the previous graph, the inclusion of the
JVM-based Ruby JITs distorts the sense of scale. The next graph below shows the same memory
usage comparison, but with only CRuby, YJIT and RJIT included. Currently, there are
situations where RJIT uses several times more memory than CRuby and YJIT:</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/bars_mem_yjit_rjit_only.png" alt="" width="90%"></p>

<p>YJIT, being that it is written in Rust (a systems language), has
access to more tools to optimize memory usage in places where individual bits count. Matching
YJIT’s memory usage in a pure Ruby JIT would be difficult and likely would necessitate
augmenting Ruby with special systems programming primitives. For example, to be able to
efficiently pack structs and bit fields in memory and to also pack structs and
arrays inside of other structs.
Encoding data structures in memory as compactly as possible is likely challenging to do
in a JVM-based JIT implementation as well.</p>

<h3 id="performance-in-production">Performance In Production</h3>

<p>The following graph looks at the latency of YJIT on our SFR deployment when compared to
servers that run the same Ruby commit with YJIT disabled. If you are wondering why no other
Ruby JITs are included in this graph, it is because, at this time, other Ruby JITs could not
be deployed in production for this application, either due to compatibility issues or due to memory constraints.
On average, YJIT is 14.1% faster than the CRuby interpreter during the period examined.
Importantly, because YJIT is able to compile new code very fast, it is also faster than the
interpreter even on the slowest p99 requests.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_latency.png" alt="" width="90%"></p>

<p>If a 14.1% speedup seems underwhelming to you, do keep in mind that the latency numbers
provided measure the total time needed to generate a response. This includes not only
time spent in JIT-compiled code, but also time spent in C functions that we cannot optimize,
and time the server spends waiting for database requests and other I/O operations.</p>

<p><img src="https://railsatscale.com/2023-11-07-yjit-is-the-most-memory-efficient-ruby-jit/images/sfr_speedup.png" alt="" width="90%"></p>

<p>The graph above shows the speedup provided by YJIT over the interpreter. The purple vertical
lines represent deployments.
During the time period we examined, there were 35 deployments of new code to production,
and the shortest interval during consecutive deployments was just 19 minutes 21 seconds.
The key takeaway here is that even though we perform frequent
deployments during the daytime, because YJIT is able to warm up very fast, it remains
consistently faster than the interpreter.
Again, more information is provided in <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">the paper</a>,
including data about YJIT’s memory usage in our production deployments.</p>

<h2 id="conclusion">Conclusion</h2>

<p>We’ve recently published a paper about YJIT at MPLR 2023, in which we evaluate YJIT’s performance
on both benchmarks as well as on Shopify’s flagship production deployment, which serves an
enormous amount of traffic worldwide.
In this paper, we make it a point to examine not just peak performance, but to also discuss and
evaluate warm-up time and total memory usage.</p>

<p>The YJIT team has spent a significant amount of effort optimizing YJIT so that it doesn’t just
show good peak performance numbers, but also does this while being memory-efficient.
This effort has paid off, with YJIT having the least memory overhead of any Ruby JIT, which
has been crucial in enabling YJIT to handle Shopify’s SFR deployment.</p>

<p>Since our MPLR paper was published, we’ve kept improving
YJIT’s performance.
As of this writing, I am looking at our internal dashboard, and YJIT is providing a 27.2% average
speedup over the interpreter on our SFR deployment.
With the Ruby 3.3 release approaching, there will be a lot to be excited about this holiday season,
as we are once
again gearing up for a very strong Ruby release. This year, YJIT 3.3 will deliver better performance,
while using less memory, and also warming up faster than YJIT 3.2.
Expect another post on the <a href="https://railsatscale.com/">Rails at Scale blog</a> with more benchmark results soon!</p>

<p>For more information on how to use YJIT, see the <a href="https://github.com/ruby/ruby/blob/master/doc/yjit/yjit.md">YJIT README</a>.
Should you wish to cite <a href="https://dl.acm.org/doi/10.1145/3617651.3622982">our MPLR 2023 paper</a>, I’ve also
included the bibtex snippet below:</p>

<div><pre><code>@inproceedings{yjit_mplr_2023,
author = {Chevalier-Boisvert, Maxime and Kokubun, Takashi and Gibbs, Noah and Wu, Si Xing (Alan) and Patterson, Aaron and Issroff, Jemma},
title = {Evaluating YJIT’s Performance in a Production Context: A Pragmatic Approach},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622982},
doi = {10.1145/3617651.3622982},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {20–33},
numpages = {14},
keywords = {dynamically typed, optimization, just-in-time, virtual machine, ruby, compiler, bytecode},
location = {Cascais, Portugal},
series = {MPLR 2023}
}
</code></pre></div>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[.NET 8 (296 pts)]]></title>
            <link>https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</link>
            <guid>38264937</guid>
            <pubDate>Tue, 14 Nov 2023 16:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/">https://devblogs.microsoft.com/dotnet/announcing-dotnet-8/</a>, See on <a href="https://news.ycombinator.com/item?id=38264937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="featured">
                         <p>
            November 14th, 2023</p><!-- .entry-meta -->
        
<p>We are happy to announce the availability of <a href="https://aka.ms/get-dotnet-8">.NET 8</a>, the latest <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">LTS</a> version of one of the world’s leading development platforms, starting today. .NET 8 delivers thousands of performance, stability, and security improvements, as well as platform and tooling enhancements that help increase developer productivity and speed of innovation. The .NET team, our partners, and the .NET community will be talking about what’s new in .NET 8 as well as what people are building with .NET today to meet their needs of tomorrow at  <a href="https://www.dotnetconf.net/">.NET Conf 2023, a three day virtual event (November 14-16)</a>. Come, join us!</p>
<p><a href="https://dotnet.microsoft.com/download/dotnet/8.0"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Banner3.png" alt=".NET 8 is now available" width="600"></a></p>
<p>With this release, .NET reshapes the way we build intelligent, cloud-native applications and high-traffic services that scale on demand. Whether you’re deploying to Linux or Windows, using containers or a cloud app model of your choice, .NET 8 makes building these apps easier. It includes a set of proven libraries that are used today by the many high-scale services at Microsoft to help you with fundamental challenges around observability, resiliency, scalability, manageability, and more.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Cloud-Native-dotnet8.png" alt=".NET 8 - Cloud Native Features"></a></p>
<p>Integrate large language models (LLMs) like OpenAI’s GPT directly into your .NET app. Use a single powerful component model to handle all your web UI needs with Blazor. Deploy your mobile applications to the latest version of iOS and Android with .NET MAUI. Discover new language enhancements that make your code more concise and expressive with C# 12.  </p>
<p>Let’s look at what’s new in .NET 8. </p>
<h2 id="unparalleled-performance-experience-the-fastest-net-to-date">Unparalleled Performance – Experience the fastest .NET to date</h2>
<p>.NET 8 comes with thousands of performance <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/">improvements</a> <a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-aspnet-core-8/">across</a> <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui/">the</a> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">stack</a>. A new code generator called Dynamic Profile-Guided Optimization (PGO) that optimizes your code based on real-world usage is enabled by default and can improve the performance of your apps up to 20%. The AVX-512 instruction set, which is now supported, enables you to perform parallel operations on 512-bit vectors of data, meaning you can process much more data in less time. The primitive types (numerical and beyond) now implement a new formattable and parsable interface, which enable them to directly format and parse as UTF-8 without any transcoding overhead.</p>
<p>Every year we talk about the performance gains across .NET. This year we continue our quest to push the performance of .NET to new heights. From the latest TechEmpower benchmarks with .NET 8, we’re seeing improvements in the JSON API scenario of 18%, hitting nearly one million requests per second with ASP.NET Core Minimal APIs.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-8/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Performance2.png" alt="TechEmpower requests per second (RPS)"></a></p>
<p>The Fortunes scenario is closer to a real-world workload, including database access and server-side HTML rendering. In this test, we see an even larger improvement of 24%, now over 300K requests per second with ASP.NET Core.</p>
<h2 id="net-aspire-an-opinionated-stack-to-build-observable-production-ready-cloud-native-applications">.NET Aspire – An opinionated stack to build observable, production-ready cloud-native applications</h2>
<p>.NET Aspire is a stack for building resilient, observable, and configurable cloud-native applications with .NET. It includes a curated set of components enhanced for cloud-native by including telemetry, resilience, configuration, and health checks by default. Combined with a sophisticated but simple local developer experience, .NET Aspire makes it easy to discover, acquire, and configure essential dependencies for cloud-native applications on day 1 as well as day 100. The <a href="https://aka.ms/aspireannouncement">first preview</a> of .NET Aspire is available today.</p>
<p><a href="https://aka.ms/aspireannouncement"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Aspire-CTAs.png" alt=".NET Aspire"></a></p>
<h2 id="net-8-container-enhancements-more-secure-compact-and-productive">.NET 8 Container Enhancements – More secure, compact, and productive</h2>
<p>Package your applications with <a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/">containers more easily and more securely than ever with .NET</a>. Every .NET image includes a non-root user, enabling more secure containers with one-line configuration. The .NET SDK tooling publishes container images without a Dockerfile and are non-root by default. Deploy your containerized apps faster due to smaller .NET base images – including new experimental variants of our images that deliver truly minimal application sizes for native AOT. Opt-in to even more security hardening with the new Chiseled Ubuntu image variants to reduce your attack surface even further. Using Dockerfiles or SDK tooling, build apps and container images for any architecture.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/securing-containers-with-rootless/"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Containers2.png" alt="Modern containers"></a></p>
<h2 id="native-aot-journey-towards-higher-density-sustainable-compute">Native AoT – Journey towards higher density sustainable compute</h2>
<p>Compile your .NET apps into <a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot">native code</a> that uses less memory and starts instantly. No need to wait for the JIT (just-in-time) compiler to compile the code at run time. No need to deploy the JIT compiler and IL code. AOT apps deploy just the code that’s needed for your app. Your app is now empowered to run in restricted environments where a JIT compiler isn’t allowed.</p>
<p><a href="https://learn.microsoft.com/dotnet/core/deploying/native-aot"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/AOTOptimizations3.png" alt="Before and After AOT"></a></p>
<h2 id="artificial-intelligence-infuse-ai-into-your-net-applications">Artificial Intelligence – Infuse AI into your .NET applications</h2>
<p>Generative AI and large language models are transforming the field of AI, providing developers the ability to create unique AI-powered experiences in their applications. <a href="https://aka.ms/dotnet-genai">.NET 8 makes it simple for you to leverage AI</a> via first-class out-of-the box AI features in the .NET SDK and seamless integration with several tools. </p>
<p>.NET 8 brings several enhancements to the <code>System.Numerics</code> library to improve its compatibility with Generative AI workloads, such as integrating Tensor Primitives. With the rise of AI-enabled apps, new tools and SDKs emerged. We collaborated with numerous internal and external partners, such as <a href="https://azure.microsoft.com/products/ai-services/openai-service">Azure OpenAI</a>, <a href="https://azure.microsoft.com/free/ai-services/?ef_id=_k_b34c5d449bf4175800e738086ecc7267_k_&amp;OCID=AIDcmm5edswduu_SEM__k_b34c5d449bf4175800e738086ecc7267_k_&amp;msclkid=b34c5d449bf4175800e738086ecc7267">Azure Cognitive Search</a>, <a href="https://milvus.io/docs/v2.2.x/install-csharp.md">Milvus</a>, <a href="https://github.com/qdrant/qdrant-dotnet">Qdrant</a>, and <a href="https://github.com/microsoft/teams-ai">Microsoft Teams</a>, to ensure .NET developers have easy access to various AI models, services, and platforms through their respective SDKs. Additionally, the open-source <a href="https://learn.microsoft.com/semantic-kernel/overview/">Semantic Kernel</a> SDK simplifies the integration of these AI components into new and existing applications, to help you deliver innovative user experiences.</p>
<p>Various samples and reference templates, showcasing patterns and practices, are now available to make it easy for developers to get started:</p>
<ul>
<li><a href="https://github.com/dotnet-architecture/eShop">Customer Chatbot</a></li>
<li><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp">Retrieval Augmented Generation</a></li>
<li><a href="https://devblogs.microsoft.com/dotnet/demystifying-retrieval-augmented-generation-with-dotnet/">Developing Apps using Azure AI services</a> </li>
</ul>
<p><a href="https://github.com/Azure-Samples/azure-search-openai-demo-csharp/assets/2546640/b79090b8-6a8b-45f4-b42b-e21e22b1661a"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/InfuseAIdotnetapps1.png" alt="Infuse AI dotnet apps"></a></p>
<h2 id="blazor-build-full-stack-web-applications-with-net">Blazor – Build full stack web applications with .NET</h2>
<p>Blazor in .NET 8 can use both the server and client together to handle all your web UI needs. It’s full stack web UI! With several new enhancements focused towards optimizing page load time, scalability, and elevating the user experience, developers can now use <a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor">Blazor Server and Blazor WebAssembly</a> in the same app, automatically shifting users from the server to the client at run time. Your .NET code runs significantly faster on WebAssembly thanks to the new “Jiterpreter”-based runtime and new built-in components. As a part enhancing the overall <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">authentication, authorization, and identity management in .NET 8</a>, Blazor now supports generating a full Blazor-based Identity UI.</p>
<p><a href="https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/Blazor2.png" alt="Full stack Blazor"></a></p>
<h2 id="net-maui-elevated-performance-reliability-and-developer-experience"><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8">.NET MAUI – Elevated performance, reliability, and developer experience</a></h2>
<p>.NET MAUI provides you with a single project system and single codebase to build WinUI, Mac Catalyst, iOS, and Android applications. Native AOT (experimental) now supports targeting iOS-like platforms. <a href="https://aka.ms/maui-devkit-blog">A new Visual Studio Code extension for .NET MAUI</a> gives you the tools you need to develop cross-platform .NET mobile and desktop apps. Xcode 15 and Android API 34 are now supported allowing you to target the latest version of iOS and Android. A plethora of quality improvements were made to the <a href="https://devblogs.microsoft.com/dotnet/dotnet-8-performance-improvements-in-dotnet-maui">areas of performance</a>, controls and UI elements, and platform-specific behavior, such as desktop interaction adding better click handling, keyboard listeners, and more.</p>
<p><a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-maui-in-dotnet-8"><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/dotnetMAUIin8-1.png" alt=".NET MAUI Visual Studio Code Extension"></a></p>
<h2 id="c-12-features-simplified-syntax-for-better-developer-productivity">C# 12 Features – Simplified syntax for better developer productivity</h2>
<p>C# 12 makes your coding experience more productive and enjoyable. You can now create primary constructors in any class and struct with a simple and elegant syntax. No more boilerplate code to initialize your fields and properties. Be delighted when creating arrays, spans, and other collection types with a concise and expressive syntax. Use new default values for parameters in lambda expressions. No more overloading or null checks to handle optional arguments. You can even use the <code>using</code> alias directive to alias any type, not just named types!</p>
<p><strong>Collection expressions</strong></p>
<pre><code>// Create a list:
List&lt;int&gt; a = [1, 2, 3, 4, 5, 6, 7, 8];

// Create a span
Span&lt;char&gt; b  = ['a', 'b', 'c', 'd', 'e', 'f', 'h', 'i'];

// Use the spread operator to concatenate
int[] array1 = [1, 2, 3];
int[] array2 = [4, 5, 6];
int[] array3 = [7, 8, 9];
int[] fullArray = [..array1, ..array2, ..array3]; // contents is [1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
<p>See more about the latest version of C# in <a href="https://devblogs.microsoft.com/dotnet/announcing-csharp-12">Announcing C# 12</a>.</p>

<p>We have a set of great tools that help you be the most productive in your development workflow and take advantage of .NET 8 today. Released alongside .NET 8, the <a href="https://aka.ms/vs/v178GA">Visual Studio 2022 17.8 release</a> brings support for .NET 8, C# 12 language enhancements, and various new productivity features. <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">VS Code and C# Dev Kit</a> is a great way to get started with .NET 8 if you’re learning and/or want to quickly kick the tires of the runtime and is available on Linux, macOS, or in GitHub Codespaces. The new <a href="https://github.com/codespaces">GitHub Codespaces template for .NET</a>, which comes with the .NET SDK and a set of configured extensions, is one of the fastest ways to get started with .NET 8. </p>
<h3 id="additional-features-in-net-8">Additional features in .NET 8:</h3>
<ul>
<li><strong>ASP.NET Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/whats-new-with-identity-in-dotnet-8/">Streamlines identity for single-page applications (SPA) and Blazor providing cookie-based authentication, pre-built APIs, token support, and a new identity UI.</a> and <a href="https://learn.microsoft.com/aspnet/core/release-notes/aspnetcore-8.0#minimal-apis">enhances minimal APIs with form-binding, antiforgery support to protect against cross-site request forgery (XSRF/CSRF), and <code>asParameters</code> support for parameter-binding with Open API definitions</a></li>
<li><strong>ASP.NET Core tooling.</strong> <a href="https://devblogs.microsoft.com/dotnet/aspnet-core-route-tooling-dotnet-8/">Route syntax highlighting, auto-completion, and analyzers to help you create Web APIs.</a></li>
<li><strong>Entity Framework Core.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-ef8-rc2/">Provides new “complex types” as value objects, primitive collections, and SQL Server support for hierarchical data.</a></li>
<li><strong>NuGet.</strong> <a href="https://learn.microsoft.com/nuget/concepts/auditing-packages">Helps you audit your NuGet packages in projects and solutions for any known security vulnerabilities.</a></li>
<li><strong>.NET Runtime.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#androidstripilafteraot-mode-on-android">Brings a new AOT compilation mode for WebAssembly (WASM) and Android.</a></li>
<li><strong>.NET SDK.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8#net-sdk">Revitalizes terminal build output and production-ready defaults.</a></li>
<li><strong>WPF.</strong> <a href="https://devblogs.microsoft.com/dotnet/wpf-file-dialog-improvements-in-dotnet-8/">Supports OpenFolderDialog</a> and <a href="https://devblogs.microsoft.com/dotnet/announcing-dotnet-8-rc1/#wpf-hardware-acceleration-in-rdp">Enabled HW Acceleration in RDP</a></li>
<li><strong>ARM64.</strong> <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">Significant feature enhancements and improved code quality for ARM64 platforms through collaboration with ARM engineers.</a></li>
<li><strong>Debugging.</strong> <a href="https://devblogs.microsoft.com/dotnet/debugging-enhancements-in-dotnet-8/">Displays debug summaries and provides simplified debug proxies for commonly used .NET types.</a></li>
<li><strong>System.Text.Json.</strong> <a href="https://devblogs.microsoft.com/dotnet/system-text-json-in-dotnet-8/">Helps populate read-only members, customizes unmapped member handling, and improves Native AOT support.</a></li>
<li><strong>.NET Community Toolkit.</strong> <a href="https://devblogs.microsoft.com/dotnet/announcing-the-dotnet-community-toolkit-821/">Accelerates building .NET libraries and applications while ensuring they are trim and AOT compatible (including the MVVM source generators!)</a></li>
<li><strong>Azure</strong> <a href="https://aka.ms/appservice-dotnet8">Supports .NET 8 with Azure’s PaaS services like App Service for Windows and Linux, Static Web Apps, Azure Functions, and Azure Container Apps.</a></li>
<li><strong>What’s new in .NET 8.</strong> <a href="https://learn.microsoft.com/dotnet/core/whats-new/dotnet-8">Check out our documentation for everything else!</a></li>
</ul>
<h3 id="get-started-with-net-8">Get started with .NET 8</h3>
<p>For the best development experience with .NET 8, we recommend that you use the latest release of <a href="https://visualstudio.microsoft.com/downloads/">Visual Studio</a> and <a href="https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.csdevkit">Visual Studio Code’s C# Dev Kit</a>. Once you’re set up, here are some of the things you should do:</p>
<ul>
<li><strong>Try the new features and APIs.</strong> <a href="https://dotnet.microsoft.com/download/dotnet/8.0">Download .NET 8</a> and <a href="https://github.com/dotnet/core/issues/new/choose">report issues in our issue tracker</a>.</li>
<li><strong>Test your current app for compatibility.</strong> Learn whether your app is <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">affected by default behavior changes in .NET 8</a>.</li>
<li><strong>Test your app with opt-in changes.</strong> .NET 8 has <a href="https://learn.microsoft.com/dotnet/core/compatibility/8.0">opt-in behavior changes</a> that only affect your app when enabled. It’s important to understand and assess these changes early as they may become default in the next release.</li>
<li><strong>Update your app with the Upgrade Assistant.</strong> <a href="https://dotnet.microsoft.com/platform/upgrade-assistant">Upgrade your app with just a few clicks using the Upgrade Assistant</a>.</li>
<li><strong>Know you’re supported.</strong> .NET 8 is officially supported by Microsoft as a <a href="https://dotnet.microsoft.com/platform/support/policy/dotnet-core#release-types">long term support (LTS) release that will be supported for three years</a>.</li>
<li><strong>Bonus: eShop Sample for .NET 8.</strong> Follow all the best coding and architecture practices with our <a href="https://github.com/dotnet/eshop">new eShop sample, now updated for .NET 8</a>!</li>
</ul>
<h3 id="celebrate-net-8">Celebrate .NET 8</h3>
<ul>
<li><strong>.NET Conf 2023</strong>. <a href="https://www.dotnetconf.net/">Join us November 14-16, 2023 to celebrate the .NET 8 release!</a></li>
<li><strong>What’s next in .NET?</strong> <a href="https://dotnet.microsoft.com/next">Get involved and learn the latest news on .NET 8 and the next version of .NET.</a></li>
<li><strong>Get C# Certified</strong>. <a href="https://devblogs.microsoft.com/dotnet/announcing-foundational-csharp-certification/">Earn a badge of honor with a freeCodeCamp C# certification.</a></li>
<li><strong>Learn .NET 8</strong>. <a href="https://aka.ms/learn-dotnet-8">Free tutorials, videos, courses, and more for beginner through advanced .NET developers. All updated for .NET 8!</a></li>
<li><strong>See Developer Stories</strong>. <a href="https://devblogs.microsoft.com/dotnet/category/developer-stories/">Take a look at success stories of developers migrating to modern .NET.</a></li>
<li><strong>Read about why .NET?</strong>. <a href="https://devblogs.microsoft.com/dotnet/why-dotnet/">Read through our recent blog series about the convenience of .NET.</a></li>
</ul>

<p>We would just like to end by saying one big…</p>
<p><img decoding="async" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/ThankYou.png" alt="https://dotnet.microsoft.com/thanks/8.0"></p>

        

		
        <div>

            <p><img width="60" height="60" data-src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" src="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg" alt="" decoding="async" data-srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w" sizes="(max-width: 60px) 100vw, 60px" srcset="https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-96x96.jpg 96w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-300x300.jpg 300w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-150x150.jpg 150w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-24x24.jpg 24w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth-48x48.jpg 48w, https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2023/11/gaurav-seth.jpg 648w"></p>
            <h3>
                <a data-bi-id="author follow within post" data-bi-area="" data-bi-name="Gaurav Seth" aria-label="Gaurav Seth" href="https://devblogs.microsoft.com/dotnet/author/gauravs">
                    Gaurav Seth                </a>
                <span>Partner Director of Product, Developer Platforms</span>
            </h3>
            
       </div>
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GraphCast: AI model for faster and more accurate global weather forecasting (526 pts)]]></title>
            <link>https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</link>
            <guid>38264641</guid>
            <pubDate>Tue, 14 Nov 2023 15:42:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/">https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/</a>, See on <a href="https://news.ycombinator.com/item?id=38264641">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">
      
  <article>
    
  

  
  
  
  
    

    
    
      
        <div>
          
            
            
              
              

<div>
    <div>
      <p>Research</p>
      

      
        <dl>
          
            <dt>Published</dt>
            <dd>
              <time datetime="2023-11-14">
                14 November 2023
              </time>
            </dd>
          
          
            <dt>Authors</dt>
            
          
        </dl>
      

      
    </div>

    
    
    
    <picture>
      <source media="(min-width: 1024px)" type="image/webp" width="1072" height="603" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w2144-h1206-n-nu-rw 2x"><source media="(min-width: 600px)" type="image/webp" width="928" height="522" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w928-h522-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1856-h1044-n-nu-rw 2x"><source type="image/webp" width="528" height="297" srcset="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w528-h297-n-nu-rw 1x, https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1056-h594-n-nu-rw 2x">
      <img alt="GraphCast global weather forecasting of surface wind speed" height="603" src="https://lh3.googleusercontent.com/_qOKQMNWIajEgW_XlurIRao3upyv8w_4EpTRqTzu6FRyr0_qPsWdGV6nYDgsJ8C71sVuNaq1AdOxIB8UwUdhnQMVcZ_EUGOttpnVBeWEZljkqR--ig=w1072-h603-n-nu" width="1072">
    </picture>
    
  
  </div>
            
          
            
            
              
              <div>
  <p data-block-key="pnebs"><b>Our state-of-the-art model delivers 10-day weather predictions at unprecedented accuracy in under one minute</b></p><p data-block-key="f3n9e">The weather affects us all, in ways big and small. It can dictate how we dress in the morning, provide us with green energy and, in the worst cases, create storms that can devastate communities. In a world of increasingly extreme weather, fast and accurate forecasts have never been more important.</p><p data-block-key="baim">In a paper <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">published in Science</a>, we introduce GraphCast, a state-of-the-art AI model able to make medium-range weather forecasts with unprecedented accuracy. GraphCast predicts weather conditions up to 10 days in advance more accurately and much faster than the industry gold-standard weather simulation system – the High Resolution Forecast (HRES), produced by the European Centre for Medium-Range Weather Forecasts (ECMWF).</p><p data-block-key="d53hu">GraphCast can also offer earlier warnings of extreme weather events. It can predict the tracks of cyclones with great accuracy further into the future, identifies atmospheric rivers associated with flood risk, and predicts the onset of extreme temperatures. This ability has the potential to save lives through greater preparedness.</p><p data-block-key="2cftd">GraphCast takes a significant step forward in AI for weather prediction, offering more accurate and efficient forecasts, and opening paths to support decision-making critical to the needs of our industries and societies. And, by <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourcing the model code for GraphCast,</a> we are enabling scientists and forecasters around the world to benefit billions of people in their everyday lives. GraphCast is already being used by weather agencies, including ECMWF, which is running a live experiment of <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">our model’s forecasts on its website</a>.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="ke1fd">A selection of GraphCast’s predictions rolling across 10 days showing specific humidity at 700 hectopascals (about 3 km above surface), surface temperature, and surface wind speed.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The challenge of global weather forecasting</h2><p data-block-key="c69cv">Weather prediction is one of the oldest and most challenging–scientific endeavours. Medium range predictions are important to support key decision-making across sectors, from renewable energy to event logistics, but are difficult to do accurately and efficiently.</p><p data-block-key="27gma">Forecasts typically rely on Numerical Weather Prediction (NWP), which begins with carefully defined physics equations, which are then translated into computer algorithms run on supercomputers. While this traditional approach has been a triumph of science and engineering, designing the equations and algorithms is time-consuming and requires deep expertise, as well as costly compute resources to make accurate predictions.</p><p data-block-key="3ces3">Deep learning offers a different approach: using data instead of physical equations to create a weather forecast system. GraphCast is trained on decades of historical weather data to learn a model of the cause and effect relationships that govern how Earth’s weather evolves, from the present into the future.</p><p data-block-key="1lfp4">Crucially, GraphCast and traditional approaches go hand-in-hand: we trained GraphCast on four decades of weather reanalysis data, from the ECMWF’s ERA5 dataset. This trove is based on historical weather observations such as satellite images, radar, and weather stations using a traditional NWP to ‘fill in the blanks’ where the observations are incomplete, to reconstruct a rich record of global historical weather.</p><h2 data-block-key="f4hf9">GraphCast: An AI model for weather prediction</h2><p data-block-key="51oo3">GraphCast is a weather forecasting system based on machine learning and Graph Neural Networks (GNNs), which are a particularly useful architecture for processing spatially structured data.</p><p data-block-key="6c3sr">GraphCast makes forecasts at the high resolution of 0.25 degrees longitude/latitude (28km x 28km at the equator). That’s more than a million grid points covering the entire Earth’s surface. At each grid point the model predicts five Earth-surface variables – including temperature, wind speed and direction, and mean sea-level pressure – and six atmospheric variables at each of 37 levels of altitude, including specific humidity, wind speed and direction, and temperature.</p><p data-block-key="djs0f">While GraphCast’s training was computationally intensive, the resulting forecasting model is highly efficient. Making 10-day forecasts with GraphCast takes less than a minute on a single Google TPU v4 machine. For comparison, a 10-day forecast using a conventional approach, such as HRES, can take hours of computation in a supercomputer with hundreds of machines.</p><p data-block-key="7dee1">In a comprehensive performance evaluation against the gold-standard deterministic system, HRES, GraphCast provided more accurate predictions on more than 90% of 1380 test variables and forecast lead times (see our <a href="https://www.science.org/doi/10.1126/science.adi2336" rel="noopener" target="_blank">Science paper</a> for details). When we limited the evaluation to the troposphere, the 6-20 kilometer high region of the atmosphere nearest to Earth’s surface where accurate forecasting is most important, our model outperformed HRES on 99.7% of the test variables for future weather.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="x7trc">For inputs, GraphCast requires just two sets of data: the state of the weather 6 hours ago, and the current state of the weather. The model then predicts the weather 6 hours in the future. This process can then be rolled forward in 6-hour increments to provide state-of-the-art forecasts up to 10 days in advance.</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">Better warnings for extreme weather events</h2><p data-block-key="7ki0b">Our analyses revealed that GraphCast can also identify severe weather events earlier than traditional forecasting models, despite not having been trained to look for them. This is a prime example of how GraphCast could help with preparedness to save lives and reduce the impact of storms and extreme weather on communities.</p><p data-block-key="csav9">By applying a simple cyclone tracker directly onto GraphCast forecasts, we could predict cyclone movement more accurately than the HRES model. In September, a live version of our publicly available GraphCast model, deployed on the ECMWF website, accurately predicted about nine days in advance that Hurricane Lee would make landfall in Nova Scotia. By contrast, traditional forecasts had greater variability in where and when landfall would occur, and only locked in on Nova Scotia about six days in advance.</p><p data-block-key="ecqbk">GraphCast can also characterize atmospheric rivers – narrow regions of the atmosphere that transfer most of the water vapour outside of the tropics. The intensity of an atmospheric river can indicate whether it will bring beneficial rain or a flood-inducing deluge. GraphCast forecasts can help characterize atmospheric rivers, which could help planning emergency responses together with <a href="https://sites.research.google/floodforecasting/" rel="noopener" target="_blank">AI models to forecast floods.</a></p><p data-block-key="a8m1r">Finally, predicting extreme temperatures is of growing importance in our warming world. GraphCast can characterize when the heat is set to rise above the historical top temperatures for any given location on Earth. This is particularly useful in anticipating heat waves, disruptive and dangerous events that are becoming increasingly common.</p>
</div>
            
          
            
            
              
              


<figure>
  
  
    <figcaption>
      <p data-block-key="df1sy">Severe-event prediction - how GraphCast and HRES compare.</p><p data-block-key="2dp0j">Left: Cyclone tracking performances. As the lead time for predicting cyclone movements grows, GraphCast maintains greater accuracy than HRES.</p><p data-block-key="3kpk6">Right: Atmospheric river prediction. GraphCast’s prediction errors are markedly lower than HRES’s for the entirety of their 10-day predictions</p>
    </figcaption>
  
</figure>
            
          
            
            
              
              <div>
  <h2 data-block-key="pnebs">The future of AI for weather</h2><p data-block-key="795pr">GraphCast is now the most accurate 10-day global weather forecasting system in the world, and can predict extreme weather events further into the future than was previously possible. As the weather patterns evolve in a changing climate, GraphCast will evolve and improve as higher quality data becomes available.</p><p data-block-key="33jsp">To make AI-powered weather forecasting more accessible, we’ve <a href="https://github.com/google-deepmind/graphcast" rel="noopener" target="_blank">open sourced our model’s code</a>. ECMWF is already <a href="https://charts.ecmwf.int/products/graphcast_medium-mslp-wind850" rel="noopener" target="_blank">experimenting with GraphCast’s 10-day forecasts</a> and we’re excited to see the possibilities it unlocks for researchers – from tailoring the model for particular weather phenomena to optimizing it for different parts of the world.</p><p data-block-key="8e2gf">GraphCast joins other state-of-the-art weather prediction systems from Google DeepMind and Google Research, including a regional <a href="https://deepmind.google/discover/blog/nowcasting-the-next-hour-of-rain/" rel="noopener" target="_blank">Nowcasting model</a> that produces forecasts up to 90 minutes ahead, and <a href="https://blog.research.google/2023/11/metnet-3-state-of-art-neural-weather.html" rel="noopener" target="_blank">MetNet-3</a>, a regional weather forecasting model already in operation across the US and Europe that produces more accurate 24-hour forecasts than any other system.</p><p data-block-key="au8nq">Pioneering the use of AI in weather forecasting will benefit billions of people in their everyday lives. But our wider research is not just about anticipating weather – it’s about understanding the broader patterns of our climate. By developing new tools and accelerating research, we hope AI can empower the global community to tackle our greatest environmental challenges.</p>
</div>
            
          
            
            
              
              


            
          
            
            
              
              



  
    
  

            
          
        </div>
      
    

    
  

  

  

  </article>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Replacing HLS/Dash – Live Mass Fanout with Media over QUIC (144 pts)]]></title>
            <link>https://quic.video/blog/replacing-hls-dash/</link>
            <guid>38264174</guid>
            <pubDate>Tue, 14 Nov 2023 15:10:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://quic.video/blog/replacing-hls-dash/">https://quic.video/blog/replacing-hls-dash/</a>, See on <a href="https://news.ycombinator.com/item?id=38264174">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
<p>Low-latency, high bitrate, mass fan-out is hard. Who knew?</p>
<p>See <a href="https://quic.video/blog/replacing-webrtc">Replacing WebRTC</a> for the previous post in this series.</p>
<h2 id="tldr">tl;dr</h2>
<p>If you’re using HLS/DASH and your main priority is…</p>
<ul>
<li><strong>cost</strong>: wait until there CDN offerings.</li>
<li><strong>latency</strong>: you should seriously consider MoQ.</li>
<li><strong>features</strong>: it will take a while to implement everything.</li>
<li><strong>vod</strong>: it works great, why replace it?</li>
</ul>
<h2 id="intro">Intro</h2>
<p>Thanks for the positive reception on <a href="https://news.ycombinator.com/item?id=38069974">Hacker News</a>!
Anyway, I’m back.</p>
<p>I spent the last 9 years working on literally all facets of HLS and Twitch’s extension: <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LHLS</a>.
We hit a latency wall and my task was to find an alternative, originally WebRTC but that eventually pivoted into <strong>Media over QUIC</strong>.</p>
<p>Hopefully this time I won’t be <em>“dunning-Krugerering off a cliff”</em>. Thanks random Reddit user for that confidence boost.</p>
<h2 id="why-hlsdash">Why HLS/DASH?</h2>
<p>Simple answer: <a href="https://developer.apple.com/library/archive/documentation/NetworkingInternet/Conceptual/StreamingMediaGuide/UsingHTTPLiveStreaming/UsingHTTPLiveStreaming.html">Apple</a></p>
<blockquote>
<p>If your app delivers video over cellular networks, and the video exceeds either 10 minutes duration or 5 MB of data in a five minute period, you are required to use HTTP Live Streaming.</p>
</blockquote>
<p>It’s an anti-climactic answer, but Twitch migrated from <a href="https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol">RTMP</a> to <a href="https://en.wikipedia.org/wiki/HTTP_Live_Streaming">HLS</a> to avoid getting kicked off the App Store.
The next sentence gives a hint as to why:</p>
<blockquote>
<p>If your app uses HTTP Live Streaming over cellular networks, you are required to provide at least one stream at 64 Kbps or lower bandwidth.</p>
</blockquote>
<p>This was back in 2009 when the iPhone 3GS was released and AT&amp;T’s network was <a href="https://www.wired.com/2010/07/ff-att-fail/">struggling to meet the demand</a>.
The key feature of HLS was <a href="https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming">ABR</a>: multiple copies of the same content at different bitrates.
This allowed the Apple-controlled HLS player to reduce the bitrate rather than pummel a poor megacorp’s cellular network.</p>
<p><a href="https://en.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP">DASH</a> came afterwards in an attempt to standardize HLS minus the controlled by Apple part.
There’s definitely some cool features in DASH but the <a href="https://www.cloudflare.com/learning/video/what-is-mpeg-dash/">core concepts are the same</a> and they even share the same <a href="https://www.wowza.com/blog/what-is-cmaf">media container</a> now.
So the two get bundled together as <strong>HLS/DASH</strong>.</p>
<p>But I’ll focus more on HLS since that’s my shit.</p>
<h2 id="the-good-stuff">The Good Stuff</h2>
<p>While we were forced to switch protocols at the tech equivalent of gunpoint, HLS actually has some amazing benefits.
The biggest one is that it uses <strong>HTTP</strong>.</p>
<p>HLS/DASH works by breaking media into “segments”, each containing a few seconds of media.
The player will individually request each segment via a HTTP request and seamlessly stitch them together.
New segments are constantly being generated and announced to the player via a “playlist”.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/carrot.png" alt="carrot">
</p><figcaption>Thanks for the filer image, DALL·E</figcaption></figure>
<p>Because HLS uses HTTP, a service like Twitch can piggyback on the existing infrastructure of the internet.
There’s a plethora of optimized CDNs, servers, and clients that all speak HTTP and can be used to transport media.
You do have to do some extra work to massage live video into HTTP semantics, but it’s worth it.</p>
<p>The key is utilizing <a href="https://napkinfinance.com/napkin/what-are-economies-of-scale/">economies of scale</a> to make it cheap to mass distribute live media.
Crafting individual IP packets might the <em>correct</em> way to send live media with minimal latency (ie. WebRTC), but it’s not the most cost effective.</p>
<h2 id="the-bad-stuff">The Bad Stuff</h2>
<p>I hope you weren’t expecting a fluff piece.</p>
<h3 id="latency">Latency</h3>
<p>We were somewhat sad to bid farewell to Flash (<em>gasp</em>).
Twitch’s latency went from something like 3 seconds with RTMP to 15 seconds with HLS.</p>
<p>There’s a boatload of latency sources, anywhere from the duration of segments to the frequency of playlist updates.
Over the years we were able to slowly able to chip away at the problem, eventually extending HLS to get latency back down to theoretical RTMP levels.
I <a href="https://quic.video/blog/distribution-at-twitch">documented our journey</a> if you’re interested in the gritty details.</p>
<p>But one big source of latency remains: <strong>T</strong> <strong>C</strong> <strong>P</strong></p>
<p>I went into more detail with my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, but the problem is head-of-line blocking.
Once you flush a frame to the TCP socket, it will be delivered reliably and in order.
However, when the network is congested, the encoded media bitrate will exceed the network bitrate and queues will grow.
Frames will take longer and longer to reach the player until the buffer is depleted and the viewer gets to see their least favorite spinny boye.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw HLS/DASH</figcaption></figure>
<p>A HLS/DASH player can detect queuing and switch to a lower bitrate via ABR.
However, it can only do this at infrequent (ex. 2s) segment boundaries, and it can’t renege any frames already flushed to the socket.
So if you’re watching 1080p video and your network takes a dump, well you still need to download seconds of unsustainable 1080p video before you can switch down to a reasonable 360p.</p>
<p>You can’t just put the toothpaste back in the tube if you squeeze out too much.
You gotta use all of the toothpaste, even if it takes much longer to brush your teeth.</p>
<figure><p><img src="https://quic.video/blog/replacing-webrtc/toothpaste.jpg" alt="TCP toothpaste"></p><figcaption><p><a href="https://knowyourmeme.com/memes/shitting-toothpaste-pooping-toothpaste">Source</a>. The analogy falls apart but I
get to use this image again.</p></figcaption></figure>
<h3 id="clients">Clients</h3>
<p>HLS utilizes “smart” clients and “dumb” servers.
The client decides what, when, why, and how to download each media playlist, segment, and frame.
Meanwhile the server just sits there and serves HTTP requests.</p>
<p>The problem really depends on your perspective. If you control:</p>
<ul>
<li><strong>client only</strong>: Life is great!</li>
<li><strong>client and server</strong>: Life is great! You can even extend the protocol!</li>
<li><strong>server only</strong>: Life is <em>pain</em>.</li>
</ul>
<p>For a service like Twitch, the solution might seem simple: build your own client and server!
And we did, including a baremetal live CDN designed exclusively for HLS.</p>
<p>But <a href="https://bitmovin.com/managed-media-source">until quite recently</a>, we have been forced to use the Apple HLS player on iOS for AirPlay or Safari support.
And of course TVs, consoles, casting devices, and others have their own HLS players.
And if you’re offering your baremetal live CDN <a href="https://aws.amazon.com/ivs/">to the public</a>, you can’t exactly force customers to use your proprietary player.</p>
<p>So you’re stuck with a <em>dumb</em> server and a bunch of <em>dumb</em> clients.
These <em>dumb</em> clients make <em>dumb</em> decisions with no cooperation with the server, based on imperfect information.</p>
<h3 id="ownership">Ownership</h3>
<p>I love the simplicity of HLS compared to DASH.
There’s something so satisfying about a text-based playlist that you can actually read, versus a XML monstrosity designed by committee.</p>
<pre is:raw="" tabindex="0"><code><span><span>#EXTM3U</span></span>
<span><span>#EXT-X-TARGETDURATION:10</span></span>
<span><span>#EXT-X-VERSION:3</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/first.ts</span></span>
<span><span>#EXTINF:9.009,</span></span>
<span><span>http://media.example.com/second.ts</span></span>
<span><span>#EXTINF:3.003,</span></span>
<span><span>http://media.example.com/third.ts</span></span>
<span><span>#EXT-X-ENDLIST</span></span></code></pre>
<figure><figcaption><a href="https://datatracker.ietf.org/doc/html/draft-pantos-hls-rfc8216bis/#section-9.1">Orgasmic</a>.</figcaption></figure>
<p>But unfortunately Apple controls HLS.</p>
<p>There’s a misalignment of incentives between Apple and the rest of the industry.
I’m not even sure how Apple uses HLS, or why they would care about latency, or why they insist on being the sole arbiter of a live streaming protocol.
<a href="https://www.crunchbase.com/person/roger-pantos">Pantos</a> has done a great and thankless job, but it feels like a stand-off.</p>
<p>For example, LL-HLS originally <a href="https://www.theoplayer.com/blog/impact-of-apple-ll-hls-update-2020">required HTTP/2 server push</a> and it took nearly the entire industry to convince Apple that this was a bad idea.
The upside is that we got <a href="https://lists.apple.com/mailman/listinfo/hls-announce">a mailing list</a> so they can announce changes to developers first… but don’t expect the ability to propose changes any time soon.</p>
<p>DASH is it’s own can of worms as it’s controlled by <a href="https://en.wikipedia.org/wiki/Moving_Picture_Experts_Group">MPEG</a>.
The specifications are <a href="https://www.iso.org/standard/79106.html">behind a paywall</a> or <a href="https://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=133508">require patent licensing</a>?
I can’t even tell if I’m going to <a href="https://www.mpegla.com/wp-content/uploads/DASHWeb.pdf">get sued</a> for parsing a DASH playlist without paying the troll toll.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/troll.webp" alt="troll toll"></p><figcaption><p><a href="https://itsalwayssunny.fandom.com/wiki/The_Nightman_Cometh">Source</a>. 🎵 You gotta pay the Troll Toll 🎵</p></figcaption></figure>
<h2 id="whats-next">What’s next?</h2>
<p>You’re given a blank canvas and a brush to paint the greenest of fields, what do you make?</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/green.jpg" alt="green field"></p><figcaption><p><a href="https://www.freeimageslive.co.uk/free_stock_image/green-field-painting-jpg">Source</a>. Wow. That’s quite the
green field.</p></figcaption></figure>
<h2 id="tcp">TCP</h2>
<p>After my <a href="https://quic.video/blog/replacing-webrtc">previous blog post</a>, I had a few people hit up my DMs and claim they can do real-time latency with TCP.
And I’m sure a few more people will too after this post, so you get your own section that muddles the narrative.</p>
<p>Yes, you can do real-time latency with TCP (or WebSockets) under ideal conditions.</p>
<p>However, it just won’t work well enough on poor networks.
Congestion and buffer-bloat will absolutely wreck your protocol on poor networks.
A lot of my time spent at Twitch was optimizing for the 90th percentile; the shoddy cellular networks in Brazil or India or Australia.</p>
<p>But if you are going to reinvent RTMP, there are <a href="https://www.youtube.com/watch?v=cpYhm74zp0U">some ways to reduce queuing</a> but they are quite limited.
This is <em>especially</em> true in a browser environment when limited to HTTP or <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API">WebSockets</a>.</p>
<p>See my next blog post about <strong>Replacing RTMP</strong>.</p>
<h2 id="http">HTTP</h2>
<p>Notably absent thus far has been any mention of <a href="https://www.theoplayer.com/blog/low-latency-hls-lhls">LL-HLS</a> and <a href="https://www.wowza.com/blog/what-is-low-latency-dash">LL-DASH</a>.
These two protocols are meant to lower HLS/DASH latency respectively by breaking media segments into smaller chunks.</p>
<p>The chunks might be smaller, but they’re still served sequentially over TCP.
The latency floor is lower but the latency ceiling is still just as high, and you’re still going to buffer during congestion.</p>
<figure><p><img src="https://quic.video/blog/replacing-hls-dash/buffering.gif" alt="buffering">
</p><figcaption>&gt; tfw LL-HLS/LL-DASH</figcaption></figure>
<p>We’re also approaching the limit of what you can do with HTTP semantics.</p>
<ul>
<li><strong>LL-HLS</strong> has configurable latency at the cost of and exponential number of sequential requests in the critical path. For example, 20 HTTP requests a second <em>per track</em> still only gets you +100ms of latency, which is not even viable for real-time latency.</li>
<li><strong>LL-DASH</strong> can be configured down to +0ms added latency, delivering frame-by-frame with chunked-transfer. However it absolutely wrecks client-side ABR algorithms. Twitch <a href="https://blog.twitch.tv/en/2020/01/15/twitch-invites-you-to-take-on-our-acm-mmsys-2020-grand-challenge/">hosted a challenge</a> to improve this but I’m convinced it’s impossible without server feedback.</li>
</ul>
<p><a href="https://www.theoplayer.com/solutions/hesp-high-efficiency-streaming">HESP</a> also gets a special shout-out because it’s cool.
It works by canceling HTTP requests during congestion and frankensteining the video encoding which is quite <del>hacky</del> clever, but suffers a similar fate.</p>
<p>We’ve hit a wall with HTTP over TCP.</p>
<h2 id="http3">HTTP/3</h2>
<p>If you’re an astute hyper text transport protocol aficionado, you might have noticed that I said “HTTP over TCP” above.
But <a href="https://www.cloudflare.com/learning/performance/what-is-http3">HTTP/3</a> uses <a href="https://www.rfc-editor.org/rfc/rfc9000.html">QUIC</a> instead of TCP.
Problem solved! We can replace any mention of <del>TCP</del> with QUIC!</p>
<p>Well, not quite. To use another complicated topic as a metaphor:</p>
<ul>
<li>A TCP connection is a single-core CPU.</li>
<li>A QUIC connection is a multi-core CPU.</li>
</ul>
<p>If you take a single threaded program and run it on a multi-core machine, it will run just as slow, and perhaps even slower.
This is the case with HLS/DASH as each segment request is made <em>sequentially</em>.
HTTP/3 is not a magic bullet and only has marginal benefits when used with HLS/DASH.</p>
<p>The key to using QUIC is to embrace concurrency.</p>
<p>This means utilizing multiple, independent streams that share a connection.
You can prioritize a stream so it gets more bandwidth during congestion, much like you can use <code>nice</code> on Linux to prioritize a process when CPU starved.
If a stream is taking too long, you can cancel it much like you can <code>kill</code> a process.</p>
<p>For live media, you want to prioritize new media over old media in order to skip old content.
You also want to prioritize audio over video, so you can hear what someone is saying without necessarily seeing their lips move.
If you can only transmit half of a media stream in time, make sure it’s the most important half.</p>
<p>To Apple/Pantos’ credit, LL-HLS is exploring <a href="https://mailarchive.ietf.org/arch/msg/hls-interest/RcZ2SG8Sz_zZEcjWnDKzcM_-TJk/">prioritization using HTTP/3</a>.
It doesn’t go far enough (yet!) and HTTP semantics get in the way, but it’s absolutely the right direction.
I’m convinced that somebody will make a <a href="https://mailarchive.ietf.org/arch/msg/moq/S3eOPU5XnvQ4kn1zJyDThG5U4sA/">HTTP/3 only media protocol</a> at some point.</p>
<p>But of course I’m biased towards…</p>

<p>MoQ utilizes WebTransport/QUIC directly to avoid TCP and HTTP.
But what about that whole <strong>economies of scale</strong> stuff?</p>
<p>Well, there are some important differences between Media over QUIC as compared to your standard <em>not invented here</em> protocol:</p>
<h2 id="reason-0-quic">Reason 0: QUIC</h2>
<p>QUIC is the future of the internet.
TCP is a relic of the past.</p>
<figure><img src="https://quic.video/home/quic.svg" alt="QUIC Logo"><figcaption>You’re going to see a lot of this logo, although not crudely traced or green.</figcaption></figure>
<p>It’s a <strong>bold</strong> claim I know.
But I struggle to think of a single reason why you would use TCP over QUIC going forward.
There are still some corporate firewalls that block UDP (used by QUIC) and hardware offload doesn’t exist yet, but I mean that’s about it.</p>
<p>It will take a few years, but every library, server, load balancer, and NIC will be optimized for QUIC delivery.
Media over QUIC offloads as much as possible into this powerful layer.
We benefit also from any new features, including proposals such as <a href="https://datatracker.ietf.org/doc/draft-ietf-quic-multipath/">multi-path</a>, <a href="https://datatracker.ietf.org/doc/draft-michel-quic-fec/">FEC</a>, <a href="https://datatracker.ietf.org/doc/rfc9330/">congestion control</a>, etc.
I don’t want network features in my media layer <em>thank you very much</em> (looking at you WebRTC).</p>
<p>It might not be obvious is that HTTP/3 is actually a thin layer on top of QUIC.
Likewise MoQ is also meant to be a thin layer on top of QUIC, effectively just providing pub/sub semantics.
We get all of the benefits of QUIC without the baggage of HTTP, and yet still achieve web support via <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebTransport_API">WebTransport</a>.</p>
<p>Instead we can focus on the important stuff instead: <strong>live media</strong>.</p>
<h2 id="reason-1-relay-layer">Reason 1: Relay Layer</h2>
<p>To avoid <a href="https://quic.video/blog/replacing-webrtc">the mistakes of WebRTC</a>, we need to decouple the application from the transport.
If a relay (ie. CDN) knows anything about media encoding, we have failed.</p>
<p>The idea is to break MoQ into layers.</p>
<p><a href="https://datatracker.ietf.org/doc/draft-ietf-moq-transport/">MoqTransport</a> is the base layer and is a typical pub/sub protocol, although catered toward QUIC.
The application splits data into “objects”, annotated with a header providing simple instructions on how the relay needs to deliver it.
These are generic signals, including stuff like the priority, reliability, grouping, expiration, etc.</p>
<p>MoqTransport is designed to be used for arbitrary applications.
Some examples include:</p>
<ul>
<li>live chat</li>
<li>end-to-end encryption</li>
<li>game state</li>
<li>live playlists</li>
<li>or even a clock!</li>
</ul>
<p>This is huge draw for CDN vendors.
Instead of building a custom WebRTC CDN that targets one specific niche, you can cast a much wider net with MoqTransport.
Akamai, Google, and Cloudflare have been involved in the standardization process thus far and CDN support is inevitable.</p>

<p>There will be at least one media layer on top of MoqTransport.
We’re focused on the transport right now so there’s no official “adopted” draft yet.</p>
<p>However, my proposal is <a href="https://datatracker.ietf.org/doc/draft-law-moq-warpstreamingformat/">Warp</a>.
It uses CMAF so it’s backwards compatible with HLS/DASH while still capable of real-time latency.
I think this is critically important, as any migration has to be done piecewise, client-by-client and user-by-user.
The same media segments can be served for a mixed roll-out and for VoD.</p>
<p>This website uses Warp! <a href="https://quic.video/watch">Try it out!</a> Or watch one of my <a href="https://www.youtube.com/watch?v=PncdrMPVaNc">presentations</a>.</p>
<p>There will absolutely be other mappings and containers; MoQ is not married to CMAF.
The important part is that only the encoder/decoder understand this media layer and not any relays in the middle.
There’s a lot of cool ideas floating around, such as a <a href="https://datatracker.ietf.org/doc/draft-wilaw-moq-catalogformat/">live playlist format</a> and a <a href="https://datatracker.ietf.org/doc/draft-mzanaty-moq-loc/">low-overhead container</a>.</p>
<h2 id="reason-3-ietf">Reason 3: IETF</h2>
<p>Media over QUIC is an <a href="https://datatracker.ietf.org/wg/moq/about/">IETF working group</a>.</p>
<figure><img src="https://quic.video/home/ietf.svg" alt="IETF Logo"><figcaption>I crudely traced and recolored this logo too.</figcaption></figure>
<p>If you know nothing about the IETF, just know that it’s the standards body behind favorites such as HTTP, DNS, TLS, QUIC, and even WebRTC.
But I think <a href="https://www.ietf.org/about/introduction/">this part</a> is especially important:</p>
<blockquote>
<p>There is no membership in the IETF. Anyone can participate by signing up to a working group mailing list (more on that below), or registering for an IETF meeting. All IETF participants are considered volunteers and expected to participate as individuals, including those paid to participate.</p>
</blockquote>
<p>It’s not a protocol owned by a company.
It’s not a protocol owned by lawyers.</p>
<p><a href="https://www.ietf.org/mailman/listinfo/moq">Join the mailing list</a>.</p>
<h2 id="whats-missing">What’s missing?</h2>
<p>Okay cool so hopefully I sold you on MoQ.
What can’t you use it today to replace HLS/DASH?</p>
<ol>
<li><strong>It’s not done yet</strong>: The IETF is many things, but fast is not one of them.</li>
<li><strong>Cost</strong>: QUIC is a new protocol that has yet to be fully optimized to match TCP. It’s possible and apparently Google is <a href="https://conferences.sigcomm.org/sigcomm/2020/files/slides/epiq/0%20QUIC%20and%20HTTP_3%20CPU%20Performance.pdf">near parity</a>.</li>
<li><strong>Support</strong>: Your favorite language/library/cdn/cloud/browser might not even provide HTTP/3 support yet, let alone WebTransport or QUIC.</li>
<li><strong>Features</strong>: Somebody has to reimplement all of the annoying HLS/DASH features like DRM and server-side advertisements…</li>
<li><strong>VoD</strong>: MoQ is currently live only. HLS/DASH work great, why replace it?</li>
</ol>
<p>We’ll get there eventually.</p>
<p>Feel free to use our <a href="https://github.com/kixelated/moq-rs">Rust</a> or <a href="https://github.com/kixelated/moq-js">Typescript</a> implementation is you want to experiment.
Join the <a href="https://discord.gg/FCYF3p99mr">Discord</a> if you want to help!</p>
<p>Written by <a href="https://github.com/kixelated">@kixelated</a>.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultra-white ceramic cools buildings with record-high 99.6% reflectivity (170 pts)]]></title>
            <link>https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</link>
            <guid>38263934</guid>
            <pubDate>Tue, 14 Nov 2023 14:51:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/">https://newatlas.com/materials/ultra-white-ceramic-cools-buildings-record-high-reflectivity/</a>, See on <a href="https://news.ycombinator.com/item?id=38263934">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Scientists in Hong Kong have demonstrated a new ultra-white ceramic material that can drastically cool buildings by reflecting sunlight and heat at record highs. The beetle-inspired material gets its ability from its nanostructure, stays tough to the elements and should be relatively easy to scale up for production.</p><p>When our homes get too hot, the first solution for many people is to crank the air conditioning. It may be effective, but it’s not very energy efficient, as heating and cooling buildings accounts for a huge percentage of energy costs. So, scientists are investigating ways to passively manage interior temperatures, and one of the simplest is to just <a href="https://newatlas.com/white-roof-heat-island/21758/" data-cms-ai="0">paint buildings and rooftops white</a>.</p><p>Basic physics dictates that lighter colors absorb less light than darker ones, and therefore remain cooler. In recent years new <a href="https://newatlas.com/materials/super-white-paint-teflon-98-suns-heat/" data-cms-ai="0">ultra-white paints</a> have been developed that reflect over 95% of the sunlight that hits them, doing a decent job of cooling buildings. But these coatings have their issues, including durability.</p><p>For the new study, scientists at City University of Hong Kong (CityU) developed a new cooling ceramic material that performs better than others. Rather than just being a very white paint, the material gets its high reflectivity from its nanostructure, which efficiently scatters almost the entire spectrum of sunlight – just like the <a href="https://newatlas.com/beetle-scales-white/53789/" data-cms-ai="0">Cyphochilus beetle</a> which inspired it. This results in a solar reflectivity of 99.6%, which is a record high, along with an infrared thermal emission of 96.5%.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings" width="957" height="805" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" data-src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/f269aec/2147483647/strip/true/crop/957x805+0+0/resize/440x370!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 440w,https://assets.newatlas.com/dims4/default/371cea4/2147483647/strip/true/crop/957x805+0+0/resize/800x673!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 800w,https://assets.newatlas.com/dims4/default/e89623e/2147483647/strip/true/crop/957x805+0+0/resize/1200x1009!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1200w,https://assets.newatlas.com/dims4/default/a4d3c90/2147483647/strip/true/crop/957x805+0+0/resize/1920x1615!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg 1920w" src="https://assets.newatlas.com/dims4/default/008ee1a/2147483647/strip/true/crop/957x805+0+0/resize/957x805!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F00%2Fb2%2F78713bab46099ce17594177f774f%2Fcooling-ceramic-2.jpeg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Samples of the cooling ceramic material, which reflect a record high of 99.6% of sunlight that hits them, cooling interiors of buildings</figcaption><p>City University of Hong Kong</p></div>
    
</figure>

                
            </div><p>The material is made of alumina, which not only reduces solar absorption, but the team says it makes the cooling ceramic more durable in the face of weather. It resists degradation from UV light exposure, which is a weakness of other passive cooling materials and coatings, and boosts the rate of water evaporation from the surface, which adds the bonus effect of evaporative cooling. It even boasts fire resistance by withstanding temperatures of over 1,000 °C (1,832 °F).</p><p>““The beauty of the cooling ceramic is that it fulfills the requirements for both high-performance PRC and applications in real-life settings,” said Professor Edwin Tso Chi-yan, co-corresponding author of the study. “Our experiment found that applying the cooling ceramic on a house roof can achieve more than 20% electricity [reduction] for space cooling, which confirms the great potential of cooling ceramic in reducing people’s reliance on traditional active cooling strategies and provides a sustainable solution for avoiding electricity grid overload, greenhouse gas emissions and urban heat islands.”</p><p>Finally, the researchers also say that the material can be easily mass produced, using common materials like alumina and a two-step process of phase inversion and sintering. And if white is too boring for some houses, the material can apparently be produced in other colors and patterns by adding extra layers.</p><p>The research was published in the journal <i><a href="https://www.science.org/doi/10.1126/science.adi4725" target="_blank" data-cms-ai="0">Science</a></i>.</p><p>Source: <a href="https://www.cityu.edu.hk/research/stories/2023/11/10/new-cooling-ceramic-can-enhance-energy-efficiency-construction-sector-and-help-combat-global-warming-cityu-research" target="_blank" data-cms-ai="0">City University of Hong Kong</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[PyPI has completed its first security audit (114 pts)]]></title>
            <link>https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</link>
            <guid>38263786</guid>
            <pubDate>Tue, 14 Nov 2023 14:38:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/">https://blog.pypi.org/posts/2023-11-14-1-pypi-completes-first-security-audit/</a>, See on <a href="https://news.ycombinator.com/item?id=38263786">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-md-component="container">
      
      
        
          
        
      
      <main data-md-component="main">
        <div data-md-component="content">
              <article>
                
                  <!--
  Copyright (c) 2016-2023 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
-->

<!-- Tags -->


<!-- Actions -->

  
  


<!--
  Hack: check whether the content contains a h1 headline. If it doesn't, the
  page title (or respectively site name) is used as the main headline.
-->

  



  by: <b>Dustin Ingram · </b>


  <span>2023-11-14</span>


<!-- Page content -->



<p><em>This is part one in a three-part series. See <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">part two here</a>, and <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">part three here</a></em></p>
<p>We are proud to announce that PyPI has completed its first ever external security audit.
This work was funded in partnership with the <a href="https://www.opentech.fund/">Open Technology Fund</a> (OTF), <a href="https://www.opentech.fund/results/supported-projects/pypi-improvements/">a previous supporter</a> of security-related improvements to PyPI.</p>
<p>The Open Technology Fund selected <a href="https://www.trailofbits.com/">Trail of Bits</a>, an industry-leading cybersecurity firm with significant open-source and Python experience, to perform the audit.
Trail of Bits spent a total of 10 engineer-weeks of effort identifying issues, presenting those findings to the PyPI team, and assisting us as we remediated the findings.</p>
<h2 id="scope">Scope</h2>
<p>The audit was focused on "Warehouse", the open-source codebase that powers <a href="https://pypi.org/">https://pypi.org</a>, and on "cabotage", the custom open-source container orchestration framework we use to deploy Warehouse.
It included code review of both codebases, prioritizing areas that accept user input, provide APIs and other public surfaces.
The audit also covered the continuous integration / continuous deployment (CI/CD) configurations for both codebases.</p>
<h2 id="findings">Findings</h2>
<p>Overall, the auditors determined the Warehouse codebase "was adequately tested and conformed to widely accepted best practices for secure Python and web development," and that while the cabotage codebase lacks the same level of testing, they did not identify any high severity issues in either codebase.</p>
<h2 id="results-impact">Results &amp; Impact</h2>
<p>As a result of the audit, Trail of Bits detailed 29 different advisories discovered across both codebases.
When evaluating severity level of each advisory, 14 were categorized as "informational", 6 as "low", 8 as "medium" and zero as "high".
At the time of writing, the PyPI team has remediated all advisories that posed a significant risk in both codebases where possible, and has worked with third-party teams to unblock additional remediations where necessary.</p>
<h2 id="more-details">More details</h2>
<p>In the interest of transparency, today we are publishing the <a href="https://github.com/trailofbits/publications#technology-product-reviews">full results of the audit</a>, as prepared by Trail of Bits.
You can read more about the audit from their perspective in their <a href="https://blog.trailofbits.com/2023/11/14/our-audit-of-pypi/">accompanying blog post</a>.</p>
<p>Additionally, in two additional blog posts published today, Mike Fiedler (PyPI Security &amp; Safety Engineer) goes into detail about <a href="https://blog.pypi.org/posts/2023-11-14-2-security-audit-remediation-warehouse/">how we remediated these findings in Warehouse</a> and Ee Durbin (Python Software Foundation Director of Infrastructure) <a href="https://blog.pypi.org/posts/2023-11-14-3-security-audit-remediation-cabotage/">similarly details remediation's in cabotage</a>.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>We would like to thank the Open Technology Fund for their continued support of PyPI and specifically for this significant security milestone for the Python ecosystem.
We would also like to thank Trail of Bits for being a dependable, thorough and thoughtful partner throughout the process.</p>

<!-- Source file information -->


<!-- Was this page helpful? -->




<!-- Comment system -->

                
              </article>
            </div>
        
      </main>
      
        
      
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Backblaze Drive Stats for Q3 2023 (252 pts)]]></title>
            <link>https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</link>
            <guid>38263435</guid>
            <pubDate>Tue, 14 Nov 2023 14:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/">https://www.backblaze.com/blog/backblaze-drive-stats-for-q3-2023/</a>, See on <a href="https://news.ycombinator.com/item?id=38263435">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Backblaze Drive Stats for Q3 2023" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text">
<figure><img loading="lazy" decoding="async" width="1024" height="583" src="https://wp-admin.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png" alt="A decorative image showing the title Q3 2023 Drive Stats. " srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-1024x583.png 1024w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-300x171.png 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-768x437.png 768w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/bb-bh-Drive-Stats-Q3-2023-560x319.png 560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>

<p>At the end of Q3 2023, Backblaze was monitoring 263,992 hard disk drives (HDDs) and solid state drives (SSDs) in our data centers around the world. Of that number, 4,459 are boot drives, with 3,242 being SSDs and 1,217 being HDDs. The failure rates for the SSDs are analyzed in the <a href="https://www.backblaze.com/blog/ssd-edition-2023-mid-year-drive-stats-review/" target="_blank" rel="noreferrer noopener">SSD Edition: 2023 Drive Stats review</a>.</p>
<p>That leaves us with 259,533 HDDs that we’ll focus on in this report. We’ll review the quarterly and lifetime failure rates of the data drives as of the end of Q3 2023. Along the way, we’ll share our observations and insights on the data presented, and, for the first time ever, we’ll reveal the drive failure rates broken down by data center.</p>
<h2>Q3 2023 Hard Drive Failure Rates</h2>
<p>At the end of Q3 2023, we were managing 259,533 hard drives used to store data. For our review, we removed 449 drives from consideration as they were used for testing purposes, or were drive models which did not have at least 60 drives. This leaves us with 259,084 hard drives grouped into 32 different models.&nbsp;</p>
<p>The table below reviews the annualized failure rate (AFR) for those drive models for the Q3 2023 time period.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="1 – Quarterly AFR Table – Updated" data-rl_caption="1 – Quarterly AFR Table – Updated" title="1 – Quarterly AFR Table – Updated"><img loading="lazy" decoding="async" width="661" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg" alt="A table showing the quarterly annualized failure rates of Backblaze hard drives." srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-661x1024.jpg 661w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-194x300.jpg 194w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/1-Quarterly-AFR-Table-Updated-1-560x868.jpg 560w" sizes="(max-width: 661px) 100vw, 661px"></a></figure>
<h3>Notes and Observations on the Q3 2023 Drive Stats</h3>
<ul>
<li><strong>The 22TB drives are here</strong>: At the bottom of the list you’ll see the WDC 22TB drives (model: WUH722222ALE6L4). A <a href="https://www.backblaze.com/blog/vault-cloud-storage-architecture/" target="_blank" rel="noreferrer noopener">Backblaze Vault</a> of 1,200 drives (plus four) is now operational. The 1,200 drives were installed on September 29, so they only have one day of service each in this report, but zero failures so far.</li>
<li><strong>The old get bolder</strong>: At the other end of the time-in-service spectrum are the 6TB Seagate drives (model: ST6000DX000) with an average of 101 months in operation. This cohort had zero failures in Q3 2023 with 883 drives and a lifetime AFR of 0.88%.</li>
<li><strong>Zero failures</strong>: In Q3, six different drive models managed to have zero drive failures during the quarter. But only the 6TB Seagate, noted above, had over 50,000 drive days, our minimum standard for ensuring we have enough data to make the AFR plausible.</li>
<li><strong>One failure</strong>: There were four drive models with one failure during Q3. After applying the 50,000 drive day metric, two drives stood out:
<ol>
<li>WDC 16TB (model: WUH721816ALE6L0) with a 0.15% AFR.</li>
<li>Toshiba 14TB (model: MG07ACA14TEY) with a 0.63% AFR.</li>
</ol>
</li>
</ul>
<h3>The Quarterly AFR Drops</h3>
<p>In Q3 2023, quarterly AFR for all drives was 1.47%. That was down from 2.2% in Q2 and also down from 1.65% a year ago. The quarterly AFR is based on just the data in that quarter, so it can often fluctuate from quarter to quarter.&nbsp;</p>
<p>In our Q2 2023 report, we suspected the 2.2% for the quarter was due to the overall aging of the drive fleet and in particular we pointed a finger at specific 8TB, 10TB, and 12TB drive models as potential culprits driving the increase. That prediction fell flat in Q3 as nearly two-thirds of drive models experienced a decreased AFR quarter over quarter from Q2 and any increases were minimal. This included our suspect 8TB, 10TB, and 12TB drive models.&nbsp;</p>
<p>It seems Q2 was an anomaly, but there was one big difference in Q3: we retired 4,585 aging 4TB drives. The average age of the retired drives was just over eight years, and while that was a good start, there’s another 28,963 4TB drives to go. To facilitate the continuous retirement of aging drives and make the data migration process easy and safe we use CVT, our awesome in-house data migration software which we’ll cover at another time.</p>
<h2>A Hot Summer and the Drive Stats Data</h2>
<p>As anyone should in our business, Backblaze continuously monitors our systems and drives. So, it was of little surprise to us when the folks at <a href="https://www.nasa.gov/news-release/nasa-announces-summer-2023-hottest-on-record/" target="_blank" rel="noreferrer noopener">NASA</a> confirmed the summer of 2023 as Earth’s hottest on record. The effects of this record-breaking summer showed up in our monitoring systems in the form of drive temperature alerts. A given drive in a storage server can heat up for many reasons: it is failing; a fan in the storage server has failed; other components are producing additional heat; the air flow is somehow restricted; and so on. Add in the fact that the ambient temperature within a data center often increases during the summer months, and you can get more temperature alerts.</p>
<p>In reviewing the temperature data for our drives in Q3, we noticed that a small number of drives exceeded the maximum manufacturer’s temperature for at least one day. The maximum temperature for most drives is 60°C, except for the 12TB, 14TB, and 16TB Toshiba drives which have a maximum temperature of 55°C. Of the 259,533 data drives in operation in Q3, there were 354 individual drives (0.0013%) that exceeded their maximum manufacturer temperature. Of those only two drives failed, leaving 352 drives which were still operational as of the end of Q3.</p>
<p>While temperature fluctuation is part of running data centers and temp alerts like these aren’t unheard of, our data center teams are looking into the root causes to ensure we’re prepared for the inevitability of increasingly hot summers to come.</p>
<h3>Will the Temperature Alerts Affect Drive Stats?</h3>
<p>The two drives which exceeded their maximum temperature and failed in Q3 have been removed from the Q3 AFR calculations. Both drives were 4TB Seagate drives (model: ST4000DM000). Given that the remaining 352 drives which exceeded their temperature maximum did not fail in Q3, we have left them in the Drive Stats calculations for Q3 as they did not increase the computed failure rates.</p>
<p>Beginning in Q4, we will remove the 352 drives from the regular Drive Stats AFR calculations and create a separate cohort of drives to track that we’ll name Hot Drives. This will allow us to track the drives which exceeded their maximum temperature and compare their failure rates to those drives which operated within the manufacturer’s specifications. While there are a limited number of drives in the Hot Drives cohort, it could give us some insight into whether drives being exposed to high temperatures could cause a drive to fail more often. This heightened level of monitoring will identify any increase in drive failures so that they can be detected and dealt with expeditiously.</p>
<h2>New Drive Stats Data Fields in Q3</h2>
<p>In Q2 2023, we introduced three new data fields that we started populating in the Drive Stats data we publish: <code>vault_id</code>, <code>pod_id</code>, and <code>is_legacy_format</code>. In Q3, we are adding three more fields into each drive records as follows:</p>
<ul>
<li><code>datacenter</code>: The Backblaze data center where the drive is installed, currently one of these values: <code>ams5</code>, <code>iad1</code>, <code>phx1</code>, <code>sac0</code>, and <code>sac2</code>.</li>
<li><code>cluster_id</code>: The name of a given collection of storage servers logically grouped together to optimize system performance. Note: At this time the <code>cluster_id</code> is not always correct, we are working on fixing that.&nbsp;</li>
<li><code>pod_slot_num</code>: The physical location of a drive within a storage server. The specific slot differs based on the storage server type and capacity: Backblaze (45 drives), Backblaze (60 drives), Dell (26 drives), or Supermicro (60 drives). We’ll dig into these differences in another post.</li>
</ul>
<p>With these additions, the new schema beginning in Q3 2023 is:</p>
<ul>
<li><code>date</code></li>
<li><code>serial_number</code></li>
<li><code>model</code></li>
<li><code>capacity_bytes</code></li>
<li><code>failure</code></li>
<li><strong><code>datacenter </code>(Q3)</strong></li>
<li><strong><code>cluster_id</code> (Q3)</strong></li>
<li><strong><code>vault_id </code>(Q2)</strong></li>
<li><strong><code>pod_id </code>(Q2)</strong></li>
<li><strong><code>pod_slot_num</code> (Q3)</strong></li>
<li><strong><code>is_legacy_format </code>(Q2)</strong></li>
<li><code>smart_1_normalized</code></li>
<li><code>smart_1_raw</code></li>
<li>The remaining SMART value pairs (as reported by each drive model)</li>
</ul>
<p>Beginning in Q3, these data data fields have been added to the <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/" target="_blank" rel="noreferrer noopener">publicly available Drive Stats files</a> that we publish each quarter.&nbsp;</p>
<h2>Failure Rates by Data Center</h2>
<p>Now that we have the data center for each drive we can compute the AFRs for the drives in each data center. Below you’ll find the AFR for each of five data centers for Q3 2023.</p>
<div>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="3 – AFR by data center" data-rl_caption="3 – AFR by data center" title="3 – AFR by data center"><img loading="lazy" decoding="async" width="600" height="430" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1.jpg 600w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-300x215.jpg 300w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/3-AFR-by-data-center-1-560x401.jpg 560w" sizes="(max-width: 600px) 100vw, 600px"></a></figure></div>
<h3>Notes and Observations</h3>
<ul>
<li><strong>Null?:</strong> The drives which reported a null or blank value for their data center are grouped in four Backblaze vaults. <a href="https://www.backblaze.com/blog/author/david-winings/" target="_blank" rel="noreferrer noopener">David</a>, the Senior Infrastructure Software Engineer for Drive Stats, <a href="https://www.backblaze.com/blog/overload-to-overhaul-how-we-upgraded-drive-stats-data/" target="_blank" rel="noreferrer noopener">described the process of how we gather all the parts of the Drive Stats data each day</a>. The TL:DR is that vaults can be too busy to respond at the moment we ask, and since the data center field is nice-to-have data, we get a blank field. We can go back a day or two to find the data center value, which we will do in the future when we report this data.</li>
<li><strong>sac0?</strong>: sac0 has the highest AFR of all of the data centers, but it also has the oldest drives—nearly twice as old, on average, versus the next closest in data center, sac2. As discussed previously, <a href="https://www.backblaze.com/blog/drive-failure-over-time-the-bathtub-curve-is-leaking/" target="_blank" rel="noreferrer noopener">drive failures do seem to follow the “bathtub curve”</a>, although recently we’ve seen the curve start out flatter. Regardless, as drive models age, they do generally fail more often. Another factor could be that sac0, and to a lesser extent sac2, has some of the oldest Storage Pods, including a handful of 45-drive units. We are in the process of using CVT to replace these older servers while migrating from 4TB to 16TB and larger drives.</li>
<li><strong>iad1</strong>: The iad data center is the foundation of our eastern region and has been growing rapidly since coming online about a year ago. The growth is a combination of new data and customers using our <a href="https://www.backblaze.com/blog/double-redundancy-support-compliance-and-more-with-cloud-replication-now-live/" target="_blank" rel="noreferrer noopener">cloud replication capability</a> to automatically make a copy of their data in another region.</li>
<li><strong>Q3 Data</strong>: This chart is for Q3 data only and includes all the data drives, including those with less than 60 drives per model. As we track this data over the coming quarters, we hope to get some insight into whether different data centers really have different drive failure rates, and, if so, why.</li>
</ul>
<h2>Lifetime Hard Drive Failure Rates</h2>
<p>As of September 30, 2023, we were tracking 259,084 hard drives used to store customer data. For our lifetime analysis, we collect the number of drive days and the number of drive failures for each drive beginning from the time a drive was placed into production in one of our data centers. We group these drives by model, then sum up the drive days and failures for each model over their lifetime. That chart is below.&nbsp;</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1.jpg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="4 – Lifetime AFR Table" data-rl_caption="4 – Lifetime AFR Table" title="4 – Lifetime AFR Table"><img loading="lazy" decoding="async" width="685" height="1024" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-685x1024.jpg 685w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-201x300.jpg 201w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/4-Lifetime-AFR-Table-1-560x838.jpg 560w" sizes="(max-width: 685px) 100vw, 685px"></a></figure>
<p>One of the most important columns on this chart is the confidence interval, which is the difference between the low and high AFR confidence levels calculated at 95%. The lower the value, the more certain we are of the AFR stated. We like a confidence interval to be 0.5% or less. When the confidence interval is higher, that is not necessarily bad, it just means we either need more data or the data is somewhat inconsistent.&nbsp;</p>
<p>The table below contains just those drive models which have a confidence interval of less than 0.5%. We have sorted the list by drive size and then by AFR.</p>
<figure><a href="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" data-rel="lightbox-gallery-ht5dUJ2a" data-rl_title="5 – Lifetime AFR by CI" data-rl_caption="5 – Lifetime AFR by CI" title="5 – Lifetime AFR by CI"><img loading="lazy" decoding="async" width="710" height="780" src="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg" alt="" srcset="https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1.jpeg 710w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-273x300.jpeg 273w, https://www.backblaze.com/blog/wp-content/uploads/2023/11/5-Lifetime-AFR-by-CI-1-560x615.jpeg 560w" sizes="(max-width: 710px) 100vw, 710px"></a></figure>
<p>The 4TB, 6TB, 8TB, and some of the 12TB drive models are no longer in production. The HGST 12TB models in particular can still be found, but they have been relabeled as Western Digital and given alternate model numbers. Whether they have materially changed internally is not known, at least to us.</p>
<p>One final note about the lifetime AFR data: you might have noticed the AFR for all of the drives hasn’t changed much from quarter to quarter. It has vacillated between 1.39% to 1.45% percent for the last two years. Basically, we have lots of drives with lots of time-in-service so it is hard to move the needle up or down. While the lifetime stats for individual drive models can be very useful, the lifetime AFR for all drives will probably get less and less interesting as we add more and more drives. Of course, a few hundred thousand drives that never fail could arrive, so we will continue to calculate and present the lifetime AFR.</p>
<h2>The Hard Drive Stats Data</h2>
<p>The complete data set used to create the information used in this review is available on our <a href="https://www.backblaze.com/cloud-storage/resources/hard-drive-test-data/">Hard Drive Stats Data webpage</a>. You can download and use this data for free for your own purpose. All we ask are three things: 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone; it is free.&nbsp;</p>
<p>Good luck and let us know if you find anything interesting.</p>
</div></article></div>]]></description>
        </item>
    </channel>
</rss>