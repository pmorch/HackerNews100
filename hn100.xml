<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 06 Jul 2024 18:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Anxious Generation – How Safetyism and Social Media Are Damaging the Kids (107 pts)]]></title>
            <link>https://matija.eu/posts/anxious-generation-safetyism-social-media/</link>
            <guid>40890534</guid>
            <pubDate>Sat, 06 Jul 2024 14:02:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matija.eu/posts/anxious-generation-safetyism-social-media/">https://matija.eu/posts/anxious-generation-safetyism-social-media/</a>, See on <a href="https://news.ycombinator.com/item?id=40890534">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I just finished reading this book the other day. Written by Jonathan Haidt, <a href="https://www.amazon.com/Anxious-Generation-Rewiring-Childhood-Epidemic/dp/0593655036">"Anxious Generation"</a> deals with a concept he calls the "Great Rewiring." Essentially, he proposes that two forces at play nowadays have led (and continue to lead) an entire generation to significantly higher rates of mental illnesses.</p>
<p>One of these forces is unfettered access to social media. He draws a distinct line between the generic overuse of computers or the internet and the use of social media. The former began back in the 90s (or maybe even the 80s), but no dramatic increases in mental illness occurred back then. In those days, you'd be spending too much time in front of your screen, and the worst that could happen was you'd worsen your eyesight, end up somewhat socially inept, or even get a job in the industry once you grew up. He highlights that this was mostly observed among boys rather than girls, and the effects weren't as negative.</p>
<p>Nowadays, this is no longer the case. It's actually girls who bear the brunt of the effect, and the impact is far from harmless. He convincingly connects the dots between the social validation loop masterfully (ab)used by all modern social media, the early age that social media starts being used (early teens), and the statistics that show the incidence of mental illnesses just as platforms like Instagram started their growth in the 2010s.</p>
<p>The mind gets stuck in a loop where it continuously seeks validation and requires us to put up an almost perfect appearance for others. There's no way this is healthy for a young mind to go through. He even shows research indicating that school-aged kids might actually be taking a toll in cognitive performance due to this. A study done on kids taking exams shows there's an actual difference between taking the test without your phone in the room, with your phone in your pocket, and with your phone on the table right in front of you. You can guess which produces the best results and which produces the worst, as their minds are frankly always on standby, wondering what's going on in their social network.</p>
<p>In addition to social media use, he also talks about the other big issue that leads to his Great Rewiring: helicopter parenting.</p>
<p>The net effect of this is that kids have far more extended boundaries set on them (except on their phones!). For example, nowadays, parents expect their children to be free to go and do groceries alone or play outside without adult supervision only at around the age of 10 to 12 (if not even higher). Gen X, in his research, remembers this as having happened for them around ages 6, 7, or 8. On one hand, I feel like this claim rings true; on the other, I'm also wondering if there might be a case of some <a href="https://en.m.wikipedia.org/wiki/Rosy_retrospection">rosy retrospection</a> or wishful thinking.</p>
<p>Far from stopping there, he mentions other significant societal efforts that are thwarting children's growth, such as having playgrounds where kids don't exhibit any risk of harming themselves. Instead of preparing the kids and making them capable of (literally in this case) tackling obstacles, we're removing obstacles and coddling them.</p>
<p>Kids also become overprotected in other ways, such as not hearing other views or not being able to handle opposing views. No wonder academia is nowadays the exact opposite of free speech and the scientific method.</p>
<p>The trend of not keeping tabs on what kids are doing <em>online</em> (as opposed to offline, where the boundaries are much stricter) leads to what he deems a phone-based childhood. The kids are growing up playing with their phones rather than playing outside with other kids, learning the ropes of, well—life.</p>
<p>He suggests solving these regressions by reverting some societal safetyisms. He comes up with what he calls a "Ladder from Childhood to Adulthood" and presents checkpoints of what type of behavior one ought to expect their child to exhibit. For example, at six, the child should have a certain level of household responsibility. At eight, they might not require adult supervision to play outside, and maybe they get a dumbphone to stay in touch. Within this framework, the use of social media should come only at age 16, unlike the current state where it's supposedly age 13+ but in reality, this is not enforced at all.</p>
<p>Apart from these two major angles of the Great Rewiring, he also touches on the spiritual aspects (albeit he's an atheist), but that's not that important. He's somewhat activist about all of this, so he has a website dedicated to it all <a href="https://www.anxiousgeneration.com/">over here</a> (not just a book commercial).</p>
<p>All in all, it's a fairly interesting book, and I'll probably take a look at some of his other writings.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Properly testing concurrent data structures (117 pts)]]></title>
            <link>https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html</link>
            <guid>40890035</guid>
            <pubDate>Sat, 06 Jul 2024 12:29:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html">https://matklad.github.io/2024/07/05/properly-testing-concurrent-data-structures.html</a>, See on <a href="https://news.ycombinator.com/item?id=40890035">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <article>


<p><span>There</span>’<span>s a fascinating Rust library, </span><a href="https://github.com/tokio-rs/loom"><span>loom</span></a><span>, which can be used to</span>
<span>thoroughly test lock-free data structures. I always wanted to learn how it works. I still do! But</span>
<span>recently I accidentally implemented a small toy which, I think, contains some of the loom</span>’<span>s ideas,</span>
<span>and it seems worthwhile to write about that. The goal here isn</span>’<span>t to teach you what you should be</span>
<span>using in practice (if you need that, go read loom</span>’<span>s docs), but rather to derive a couple of neat</span>
<span>ideas from first principles.</span></p>
<section id="One-Two-Three-Two">

    <h2>
    <a href="#One-Two-Three-Two"><span>One, Two, Three, Two</span> </a>
    </h2>
<p><span>As usual, we need the simplest possible model program to mess with. The example we use comes from</span>
<a href="https://stevana.github.io/the_sad_state_of_property-based_testing_libraries.html"><span>this excellent article</span></a><span>.</span>
<span>Behold, a humble (and broken) concurrent counter:</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::{</span>
<span>  AtomicU32,</span>
<span>  Ordering::SeqCst,</span>
<span>};</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>    <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>get</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>self</span>.value.<span>load</span>(SeqCst)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>The bug is obvious here </span>—<span> the increment is not atomic. But what is the best test we can write to</span>
<span>expose it?</span></p>
</section>
<section id="Trivial-Test">

    <h2>
    <a href="#Trivial-Test"><span>Trivial Test</span> </a>
    </h2>
<p><span>The simplest idea that comes to mind is to just hammer the same counter from multiple threads and</span>
<span>check the result at the end;</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>threaded_test</span>() {</span>
<span>  <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span>  <span>let</span> <span>thread_count</span> = <span>100</span>;</span>
<span>  <span>let</span> <span>increment_count</span> = <span>100</span>;</span>
<span></span>
<span>  std::thread::<span>scope</span>(|scope| {</span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..thread_count {</span>
<span>      scope.<span>spawn</span>(|| {</span>
<span>        <span>for</span> <span>_</span> <span>in</span> <span>0</span>..increment_count {</span>
<span>          counter.<span>increment</span>()</span>
<span>        }</span>
<span>      });</span>
<span>    }</span>
<span>  });</span>
<span></span>
<span>  <span>assert_eq!</span>(counter.<span>get</span>(), thread_count * increment_count);</span>
<span>}</span></code></pre>

</figure>
<p><span>This fails successfully:</span></p>

<figure>


<pre><code><span>thread 'counter::trivial' panicked:</span>
<span>assertion `left == right` failed</span>
<span>  left: 9598</span>
<span> right: 10000</span></code></pre>

</figure>
<p><span>But I wouldn</span>’<span>t call this test satisfactory </span>—<span> it very much depends on the timing, so you can</span>’<span>t</span>
<span>reproduce it deterministically and you can</span>’<span>t debug it. You also can</span>’<span>t minimize it </span>—<span> if you reduce</span>
<span>the number of threads and increments, chances are the test passes by luck!</span></p>
</section>
<section id="PBT">

    <h2>
    <a href="#PBT"><span>PBT</span> </a>
    </h2>
<p><span>Of course the temptation is to apply property based testing here! The problem </span><em><span>almost</span></em><span> fits: we have</span>
<span>easy-to-generate input (the sequence of increments spread over several threads), a good property to</span>
<span>check (result of concurrent increments is identical to that of sequential execution) and the desire</span>
<span>to minimize the test.</span></p>
<p><span>But just how can we plug threads into a property-based test?</span></p>
<p><span>PBTs are great for testing state machines. You can run your state machine through a series of steps</span>
<span>where at each step a PBT selects an arbitrary next action to apply to the state:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>state_machine_test</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    <span>// This is our state machine!</span></span>
<span>    <span>let</span> <span>mut </span><span>state</span>: <span>i32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>// We'll run it for up to 100 steps.</span></span>
<span>    <span>let</span> <span>step_count</span>: <span>usize</span> = rng.<span>int_in_range</span>(<span>0</span>..=<span>100</span>)?;</span>
<span></span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..step_count {</span>
<span>      <span>// At each step, we flip a coin and</span></span>
<span>      <span>// either increment or decrement.</span></span>
<span>      <span>match</span> *rng.<span>choose</span>(&amp;[<span>"inc"</span>, <span>"dec"</span>])? {</span>
<span>        <span>"inc"</span> =&gt; state += <span>1</span>,</span>
<span>        <span>"dec"</span> =&gt; state -= <span>1</span>,</span>
<span>        _ =&gt; <span>unreachable!</span>(),</span>
<span>      }</span>
<span>    }</span>
<span>    <span>Ok</span>(())</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>And it </span><em><span>feels</span></em><span> like we should be able to apply the same technique here. At every iteration, pick a</span>
<span>random thread and make it do a single step. If you can step the threads manually, it should be easy</span>
<span>to maneuver one thread in between load&amp;store of a different thread.</span></p>
<p><span>But we can</span>’<span>t step through threads! Or can we?</span></p>
</section>
<section id="Simple-Instrumentation">

    <h2>
    <a href="#Simple-Instrumentation"><span>Simple Instrumentation</span> </a>
    </h2>
<p><span>Ok, let</span>’<span>s fake it until we make it! Let</span>’<span>s take a look at the buggy increment method:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>  <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>  <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>}</span></code></pre>

</figure>
<p><span>Ideally, we</span>’<span>d love to be able to somehow </span>“<span>pause</span>”<span> the thread in-between atomic operations. Something</span>
<span>like this:</span></p>

<figure>


<pre><code><span><span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>  <span>pause</span>();</span>
<span>  <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>  <span>pause</span>();</span>
<span>  <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  <span>pause</span>();</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>    <span>// ¯\_(ツ)_/¯</span></span>
<span>}</span></code></pre>

</figure>
<p><span>So let</span>’<span>s start with implementing our own wrapper for </span><code>AtomicU32</code><span> which includes calls to pause.</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::Ordering;</span>
<span></span>
<span><span>struct</span> <span>AtomicU32</span> {</span>
<span>  inner: std::sync::atomic::AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>load</span>(&amp;<span>self</span>, ordering: Ordering) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>load</span>(ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>store</span>(&amp;<span>self</span>, value: <span>u32</span>, ordering: Ordering) {</span>
<span>    <span>pause</span>();</span>
<span>    <span>self</span>.inner.<span>store</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>// still no idea :(</span></span>
<span>}</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-API">

    <h2>
    <a href="#Managed-Threads-API"><span>Managed Threads API</span> </a>
    </h2>
<p><span>One rule of a great API design is that you start by implement a single </span><em><span>user</span></em><span> of an API, to</span>
<span>understand how the API should </span><em><span>feel</span></em><span>, and only then proceed to the actual implementation.</span></p>
<p><span>So, in the spirit of faking, let</span>’<span>s just write a PBT using these pausable, managed threads, even if</span>
<span>we still have no idea how to actually implement pausing.</span></p>
<p><span>We start with creating a counter and two managed threads. And we probably want to pass a reference</span>
<span>to the counter to each of the threads:</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span></code></pre>

</figure>
<p><span>Now, we want to step through the threads:</span></p>

<figure>


<pre><code><span><span>while</span> !rng.<span>is_empty</span>() {</span>
<span>  <span>let</span> <span>coin_flip</span>: <span>bool</span> = rng.<span>arbitrary</span>()?;</span>
<span>  <span>if</span> t1.<span>is_paused</span>() {</span>
<span>    <span>if</span> coin_flip {</span>
<span>      t1.<span>unpause</span>();</span>
<span>    }</span>
<span>  } <span>else</span> <span>if</span> t2.<span>is_paused</span>() {</span>
<span>    <span>if</span> coin_flip {</span>
<span>      t2.<span>unpause</span>();</span>
<span>    }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Or, refactoring this a bit to semantically compress:</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span><span>let</span> <span>threads</span> = [t1, t2];</span>
<span></span>
<span><span>while</span> !rng.<span>is_empty</span>() {</span>
<span>  <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> threads {</span>
<span>    <span>if</span> t.<span>is_paused</span>() &amp;&amp; rng.<span>arbitrary</span>()? {</span>
<span>      t.<span>unpause</span>()</span>
<span>    }</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>That is, on each step of our state machine, we loop through all threads and unpause a random subset</span>
<span>of them.</span></p>
<p><span>But besides pausing and unpausing, we need our threads to actually </span><em><span>do</span></em><span> something, to increment the</span>
<span>counter. One idea is to mirror the </span><code>std::spawn</code><span> API and pass a closure in:</span></p>

<figure>


<pre><code><span><span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>({</span>
<span>  <span>let</span> <span>counter</span> = &amp;counter;</span>
<span>  <span>move</span> || {</span>
<span>    <span>for</span> <span>_</span> <span>in</span> <span>0</span>..<span>100</span> {</span>
<span>      counter.<span>increment</span>();</span>
<span>    }</span>
<span>  }</span>
<span>});</span></code></pre>

</figure>
<p><span>But as these are managed threads, and we want to control them from our tests, lets actually go all</span>
<span>the way there and give the controlling thread an ability to change the code running in a managed</span>
<span>thread. That is, we</span>’<span>ll start managed threads without a </span>“<span>main</span>”<span> function, and provide an API to</span>
<span>execute arbitrary closures in the context of this by-default inert thread (</span><a href="https://joearms.github.io/published/2013-11-21-My-favorite-erlang-program.html"><span>universal</span>
<span>server</span></a><span> anyone?):</span></p>

<figure>


<pre><code><span><span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span><span>// We pass the state, &amp;counter, in, but otherwise the thread is inert.</span></span>
<span><span>let</span> <span>t</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span></span>
<span><span>// But we can manually poke it:</span></span>
<span>t.<span>submit</span>(|thread_state: &amp;Counter| thread_state.<span>increment</span>());</span>
<span>t.<span>submit</span>(|thread_state: &amp;Counter| thread_state.<span>increment</span>());</span></code></pre>

</figure>
<p><span>Putting everything together, we get a nice-looking property test:</span></p>

<figure>


<pre><code><span><span>#[cfg(test)]</span></span>
<span><span>use</span> managed_thread::AtomicU32;</span>
<span><span>#[cfg(not(test))]</span></span>
<span><span>use</span> std::sync::atomic::AtomicU32;</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>// ...</span></span>
<span>}</span>
<span></span>
<span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    <span>// Our "Concurrent System Under Test".</span></span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span></span>
<span>    <span>// The sequential model we'll compare the result against.</span></span>
<span>    <span>let</span> <span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>// Two managed threads which we will be stepping through</span></span>
<span>    <span>// manually.</span></span>
<span>    <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span>    <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(&amp;counter);</span>
<span>    <span>let</span> <span>threads</span> = [t1, t2];</span>
<span></span>
<span>    <span>// Bulk of the test: in a loop, flip a coin and advance</span></span>
<span>    <span>// one of the threads.</span></span>
<span>    <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>      <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> [t1, t2] {</span>
<span>        <span>if</span> rng.<span>arbitrary</span>() {</span>
<span>          <span>if</span> t.<span>is_paused</span>() {</span>
<span>            t.<span>unpause</span>()</span>
<span>          } <span>else</span> {</span>
<span>            <span>// Standard "model equivalence" property: apply</span></span>
<span>            <span>// isomorphic actions to the system and its model.</span></span>
<span>            t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>            counter_model += <span>1</span>;</span>
<span>          }</span>
<span>        }</span>
<span>      }</span>
<span>    }</span>
<span></span>
<span>    <span>for</span> <span>t</span> <span>in</span> threads {</span>
<span>      t.<span>join</span>();</span>
<span>    }</span>
<span></span>
<span>    <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span></span>
<span>    <span>Ok</span>(())</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>Now, if only we could make this API work</span>…<span> Remember, our </span><code>pause</code><span> implementation is a shrug emoji!</span></p>
<p><span>At this point, you might be mightily annoyed at me for this rhetorical device where I pretend that I</span>
<span>don</span>’<span>t know the answer. No need for annoyance </span>—<span> when writing this code for the first time, I traced</span>
<span>exactly these steps </span>—<span> I realized that I need a </span>“<span>pausing </span><code>AtomicU32</code>”<span> so I did that (with dummy</span>
<span>pause calls), then I played with the API I </span><em><span>wanted</span></em><span> to have, ending at roughly this spot, without</span>
<span>yet knowing how I would make it work or, indeed, if it is possible at all.</span></p>
<p><span>Well, if I am being honest, there is a bit of up-front knowledge here. I don</span>’<span>t think we can avoid</span>
<span>spawning real threads here, unless we do something really cursed with inline assembly. When</span>
<em><span>something</span></em><span> calls that </span><code>pause()</code><span> function, and we want it to stay paused until further notice, that</span>
<span>just has to happen in a thread which maintains a stack separate from the stack of our test. And, if</span>
<span>we are going to spawn threads, we might as well spawn scoped threads, so that we can freely borrow</span>
<span>stack-local data. And to spawn a scope thread, you need a</span>
<a href="https://doc.rust-lang.org/stable/std/thread/struct.Scope.html"><code>Scope</code></a><span> parameter. So in reality</span>
<span>we</span>’<span>ll need one more level of indentation here:</span></p>

<figure>


<pre><code><span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>threads</span> = [t1, t2];</span>
<span>      <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>        <span>for</span> <span>t</span> <span>in</span> &amp;<span>mut</span> [t1, t2] {</span>
<span>          <span>// ...</span></span>
<span>        }</span>
<span>      }</span>
<span>    });</span></code></pre>

</figure>
</section>
<section id="Managed-Threads-Implementation">

    <h2>
    <a href="#Managed-Threads-Implementation"><span>Managed Threads Implementation</span> </a>
    </h2>
<p><span>Now, the fun part: how the heck are we going to make pausing and unpausing work? For starters, there</span>
<span>clearly needs to be some communication between the main thread (</span><code>t.unpause()</code><span>) and the managed</span>
<span>thread (</span><code>pause()</code><span>). And, because we don</span>’<span>t want to change </span><code>Counter</code><span> API to thread some kind of</span>
<span>test-only context, the context needs to be smuggled. So </span><code>thread_local!</code><span> it is. And this context</span>
<span>is going to be shared between two threads, so it must be wrapped in an </span><code>Arc</code><span>.</span></p>

<figure>


<pre><code><span><span>struct</span> <span>SharedContext</span> {</span>
<span>  <span>// 🤷</span></span>
<span>}</span>
<span></span>
<span>thread_local! {</span>
<span>  <span>static</span> INSTANCE: RefCell&lt;<span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span>    RefCell::<span>new</span>(<span>None</span>);</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span>    INSTANCE.<span>with</span>(|it| *it.<span>borrow_mut</span>() = <span>Some</span>(ctx));</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>get</span>() <span>-&gt;</span> <span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span>    INSTANCE.<span>with</span>(|it| it.<span>borrow</span>().<span>clone</span>())</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>As usual when using </span><code>thread_local!</code><span> or </span><code>lazy_static!</code><span>, it is convenient to immediately wrap it into</span>
<span>better typed accessor functions. And, given that we are using an </span><code>Arc</code><span> here anyway, we can</span>
<span>conveniently escape </span><code>thread_local</code>’<span>s </span><code>with</code><span> by cloning the </span><code>Arc</code><span>.</span></p>
<p><span>So now we finally can implement the global </span><code>pause</code><span> function (or at least can kick the proverbial can</span>
<span>a little bit farther):</span></p>

<figure>


<pre><code><span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>if</span> <span>let</span> <span>Some</span>(ctx) = SharedContext::<span>get</span>() {</span>
<span>    ctx.<span>pause</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>// 😕</span></span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Ok, what to do next? We somehow need to coordinate the control thread and the managed thread. And we</span>
<span>need some sort of notification mechanism, so that the managed thread knows when it can continue. The</span>
<span>most brute force solution here is a pair of a mutex protecting some state and a condition variable.</span>
<span>Mutex guards the state that can be manipulated by either of the threads. Condition variable can be</span>
<span>used to signal about the changes.</span></p>

<figure>


<pre><code><span><span>struct</span> <span>SharedContext</span> {</span>
<span>  state: Mutex&lt;State&gt;,</span>
<span>  cv: Condvar,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>State</span> {</span>
<span>  <span>// 🤡</span></span>
<span>}</span></code></pre>

</figure>
<p><span>Okay, it looks like I am running out of emojies here. There</span>’<span>s no more layers of indirection or</span>
<span>infrastructure left, we need to write some real code that actually does do that pausing thing. So</span>
<span>let</span>’<span>s say that the state is tracking, well, the state of our managed thread, which can be either</span>
<span>running or paused:</span></p>

<figure>


<pre><code><span><span>#[derive(PartialEq, Eq, Default)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span></code></pre>

</figure>
<p><span>And then the logic of the pause function </span>—<span> flip the state from </span><code>Running</code><span> to </span><code>Paused</code><span>, notify the</span>
<span>controlling thread that we are </span><code>Paused</code><span>, and wait until the controlling thread flips our state back</span>
<span>to </span><code>Running</code><span>:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    <span>while</span> *guard == State::Paused {</span>
<span>      guard = <span>self</span>.cv.<span>wait</span>(guard).<span>unwrap</span>();</span>
<span>    }</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Aside: Rust</span>’<span>s API for condition variables is beautiful. Condvars are tricky, and I didn</span>’<span>t really</span>
<span>understood them until seeing the signatures of Rust functions. Notice how the </span><code>wait</code><span> function</span>
<em><span>takes</span></em><span> a mutex guard as an argument, and returns a mutex guard. This protects you from the logical</span>
<span>races and guides you towards the standard pattern of using condvars:</span></p>
<p><span>First, you lock the mutex around the shared state. Then, you inspect whether the state is what you</span>
<span>need. If that</span>’<span>s the case, great, you do what you wanted to do and unlock the mutex. If not, then,</span>
<em><span>while still holding the mutex</span></em><span>, you </span><em><span>wait</span></em><span> on the condition variable. Which means that the</span>
<span>mutex gets unlocked, and other threads get the chance to change the shared state. When they do</span>
<span>change it, and notify the condvar, your thread wakes up, and it gets the locked mutex back (but the</span>
<span>state now is different). Due to the possibility of spurious wake-ups, you need to double check the</span>
<span>state and be ready to loop back again to waiting.</span></p>
<p><span>Naturally, there</span>’<span>s a helper that encapsulates this whole pattern:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Paused)</span>
<span>      .<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>Ok, this actually does look like a reasonable implementation of </span><code>pause</code><span>. Let</span>’<span>s move on to</span>
<code>managed_thread::spawn</code><span>:</span></p>

<figure>


<pre><code><span><span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  state: T,</span>
<span>) {</span>
<span>  <span>// ? ? ?? ??? ?????</span></span>
<span>}</span></code></pre>

</figure>
<p><span>There</span>’<span>s a bunch of stuff that needs to happen here:</span></p>
<ul>
<li>
<span>As we have established, we are going to spawn a (scoped) thread, so we need the </span><code>scope</code><span> parameter</span>
<span>with its three lifetimes. I don</span>’<span>t know how it works, so I am just going by the docs here!</span>
</li>
<li>
<span>We are going to return some kind of handle, which we can use to pause and unpause our managed</span>
<span>thread. And that handle is going to be parametrized over the same </span><code>'scope</code><span> lifetime, because it</span>’<span>ll</span>
<span>hold onto the actual join handle.</span>
</li>
<li>
<span>We are going to pass the generic state to our new thread, and that state needs to be </span><code>Send</code><span>, and</span>
<span>bounded by the same lifetime as our scoped thread.</span>
</li>
<li>
<span>Inside, we are going to spawn a thread for sure, and we</span>’<span>ll need to setup the </span><code>INSTANCE</code><span> thread</span>
<span>local on that thread.</span>
</li>
<li>
<span>And it would actually be a good idea to stuff a reference to that </span><code>SharedContext</code><span> into the handle</span>
<span>we return.</span>
</li>
</ul>
<p><span>A bunch of stuff, in other words. Let</span>’<span>s do it:</span></p>

<figure>


<pre><code><span><span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(ctx);</span>
<span>      <span>drop</span>(state); <span>// <span>TODO:</span> ¿</span></span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx }</span>
<span>}</span></code></pre>

</figure>
<p><span>The essentially no-op function we spawn looks sus. We</span>’<span>ll fix later! Let</span>’<span>s try to implement</span>
<code>is_paused</code><span> and </span><code>unpause</code><span> first! They should be relatively straightforward. For </span><code>is_paused</code><span>, we just</span>
<span>need to lock the mutex and check the state:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>is_paused</span>(&amp;<span>self</span>,) <span>-&gt;</span> <span>bool</span> {</span>
<span>    <span>let</span> <span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    *guard == State::Paused</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>For </span><code>unpause</code><span>, we should additionally flip the state back to </span><code>Running</code><span> and notify the other thread:</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>But I think that</span>’<span>s not quiet correct. Can you see why?</span></p>
<p><span>With this implementation, after </span><code>unpause</code><span>, the controlling and the managed threads will be running</span>
<span>concurrently. And that can lead to non-determinism, the very problem we are trying to avoid here! In</span>
<span>particular, if you call </span><code>is_paused</code><span> </span><em><span>right</span></em><span> after you </span><code>unpause</code><span> the thread, you</span>’<span>ll most likely get</span>
<code>false</code><span> back, as the other thread will still be running. But it might also hit the </span><em><span>next</span></em><span> </span><code>pause</code>
<span>call, so, depending on timing, you might also get </span><code>true</code><span>.</span></p>
<p><span>What we want is actually completely eliminating all unmanaged concurrency. That means that at any</span>
<span>given point in time, only one thread (controlling or managed) should be running. So the right</span>
<span>semantics for </span><code>unpause</code><span> is to unblock the managed thread, and then block the controlling thread</span>
<span>until the managed one hits the next pause!</span></p>

<figure>


<pre><code><span><span>impl</span> <span>ManagedHandle</span>&lt;<span>'_</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>At this point we can spawn a managed thread, pause it and resume. But right now it doesn</span>’<span>t do</span>
<span>anything. Next step is implementing that idea where the controlling thread can directly send an</span>
<span>arbitrary closure to the managed one to make it do something:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>&gt; ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: FnSomething&gt;(&amp;<span>self</span>, f: F)</span>
<span>}</span></code></pre>

</figure>
<p><span>Let</span>’<span>s figure this </span><code>FnSomething</code><span> bound! We are going to yeet this </span><code>f</code><span> over to the managed thread and</span>
<span>run it there once, so it is </span><code>FnOnce</code><span>. It is crossing thread-boundary, so it needs to be </span><code>+ Send</code><span>.</span>
<span>And, because we are using scoped threads, it </span><em><span>doesn</span>’<span>t</span></em><span> have to be </span><code>'static</code><span>, just </span><code>'scope</code><span> is</span>
<span>enough. Moreover, in that managed thread the </span><code>f</code><span> will have exclusive access to thread</span>’<span>s state, </span><code>T</code><span>.</span>
<span>So we have:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>&gt; ManagedHandle&lt;<span>'scope</span>&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(<span>self</span>, f: F)</span>
<span>}</span></code></pre>

</figure>
<p><span>Implementing this is a bit tricky. First, we</span>’<span>ll need some sort of the channel to actually move the</span>
<span>function. Then, similarly to the </span><code>unpause</code><span> logic, we</span>’<span>ll need synchronization to make sure that the</span>
<span>control thread doesn</span>’<span>t resume until the managed thread starts running </span><code>f</code><span> and hits a pause (or maybe</span>
<span>completes </span><code>f</code><span>). And we</span>’<span>ll also need a new state, </span><code>Ready</code><span>, because now there are two different</span>
<span>reasons why a managed thread might be blocked </span>—<span> it might wait for an </span><code>unpause</code><span> event, or it might</span>
<span>wait for the next </span><code>f</code><span> to execute. This is the new code:</span></p>

<figure>


<pre><code><span><span>#[derive(Default)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Ready,</span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span>
<span></span>
<span><span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>, T&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>  sender: mpsc::Sender&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;,</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  <span>mut</span> state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> (sender, receiver) =</span>
<span>    mpsc::channel::&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(Arc::<span>clone</span>(&amp;ctx));</span>
<span></span>
<span>      <span>for</span> <span>f</span> <span>in</span> receiver {</span>
<span>        <span>f</span>(&amp;<span>mut</span> state);</span>
<span></span>
<span>        <span>let</span> <span>mut </span><span>guard</span> = ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>        <span>assert_eq!</span>(*guard, State::Running);</span>
<span>        *guard = State::Ready;</span>
<span>        ctx.cv.<span>notify_all</span>()</span>
<span>      }</span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx, sender }</span>
<span>}</span>
<span></span>
<span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(&amp;<span>self</span>, f: F) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Ready);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.sender.<span>send</span>(<span>Box</span>::<span>new</span>(f)).<span>unwrap</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>The last small piece of the puzzle is the </span><code>join</code><span> function. It</span>’<span>s </span><em><span>almost</span></em><span> standard! First we close</span>
<span>our side of the channel. This serves as a natural stop signal for the other thread, so it exits.</span>
<span>Which in turn allows us to join it. The small wrinkle here is that the thread might be paused when</span>
<span>we try to join it, so we need to unpause it beforehand:</span></p>

<figure>


<pre><code><span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>join</span>(<span>self</span>) {</span>
<span>    <span>while</span> <span>self</span>.<span>is_paused</span>() {</span>
<span>      <span>self</span>.<span>unpause</span>();</span>
<span>    }</span>
<span>    <span>drop</span>(<span>self</span>.sender);</span>
<span>    <span>self</span>.inner.<span>join</span>().<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>That</span>’<span>s it! Let</span>’<span>s put everything together!</span></p>
<p><span>Helper library, </span><code>managed_thread.rs</code><span>:</span></p>

<figure>


<pre><code><span><span>use</span> std::{</span>
<span>  cell::RefCell,</span>
<span>  sync::{atomic::Ordering, mpsc, Arc, Condvar, Mutex},</span>
<span>  thread::Scope,</span>
<span>};</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>AtomicU32</span> {</span>
<span>  inner: std::sync::atomic::AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>load</span>(&amp;<span>self</span>, ordering: Ordering) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>load</span>(ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>store</span>(&amp;<span>self</span>, value: <span>u32</span>, ordering: Ordering) {</span>
<span>    <span>pause</span>();</span>
<span>    <span>self</span>.inner.<span>store</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>fn</span> <span>pause</span>() {</span>
<span>  <span>if</span> <span>let</span> <span>Some</span>(ctx) = SharedContext::<span>get</span>() {</span>
<span>    ctx.<span>pause</span>()</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>struct</span> <span>SharedContext</span> {</span>
<span>  state: Mutex&lt;State&gt;,</span>
<span>  cv: Condvar,</span>
<span>}</span>
<span></span>
<span><span>#[derive(Default, PartialEq, Eq, Debug)]</span></span>
<span><span>enum</span> <span>State</span> {</span>
<span>  <span>#[default]</span></span>
<span>  Ready,</span>
<span>  Running,</span>
<span>  Paused,</span>
<span>}</span>
<span></span>
<span>thread_local! {</span>
<span>  <span>static</span> INSTANCE: RefCell&lt;<span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt;&gt; =</span>
<span>    RefCell::<span>new</span>(<span>None</span>);</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>SharedContext</span> {</span>
<span>  <span>fn</span> <span>set</span>(ctx: Arc&lt;SharedContext&gt;) {</span>
<span>    INSTANCE.<span>with</span>(|it| *it.<span>borrow_mut</span>() = <span>Some</span>(ctx));</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>get</span>() <span>-&gt;</span> <span>Option</span>&lt;Arc&lt;SharedContext&gt;&gt; {</span>
<span>    INSTANCE.<span>with</span>(|it| it.<span>borrow</span>().<span>clone</span>())</span>
<span>  }</span>
<span></span>
<span>  <span>fn</span> <span>pause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running);</span>
<span>    *guard = State::Paused;</span>
<span>    <span>self</span>.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Paused)</span>
<span>      .<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Running)</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>struct</span> <span>ManagedHandle</span>&lt;<span>'scope</span>, T&gt; {</span>
<span>  inner: std::thread::ScopedJoinHandle&lt;<span>'scope</span>, ()&gt;,</span>
<span>  sender: mpsc::Sender&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;,</span>
<span>  ctx: Arc&lt;SharedContext&gt;,</span>
<span>}</span>
<span></span>
<span><span>pub</span> <span>fn</span> <span>spawn</span>&lt;<span>'scope</span>, T: <span>'scope</span> + <span>Send</span>&gt;(</span>
<span>  scope: &amp;<span>'scope</span> Scope&lt;<span>'scope</span>, <span>'_</span>&gt;,</span>
<span>  <span>mut</span> state: T,</span>
<span>) <span>-&gt;</span> ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>let</span> <span>ctx</span>: Arc&lt;SharedContext&gt; = <span>Default</span>::<span>default</span>();</span>
<span>  <span>let</span> (sender, receiver) =</span>
<span>    mpsc::channel::&lt;<span>Box</span>&lt;<span>dyn</span> <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>'scope</span> + <span>Send</span>&gt;&gt;();</span>
<span>  <span>let</span> <span>inner</span> = scope.<span>spawn</span>({</span>
<span>    <span>let</span> <span>ctx</span> = Arc::<span>clone</span>(&amp;ctx);</span>
<span>    <span>move</span> || {</span>
<span>      SharedContext::<span>set</span>(Arc::<span>clone</span>(&amp;ctx));</span>
<span>      <span>for</span> <span>f</span> <span>in</span> receiver {</span>
<span>        <span>f</span>(&amp;<span>mut</span> state);</span>
<span>        <span>let</span> <span>mut </span><span>guard</span> = ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>        <span>assert_eq!</span>(*guard, State::Running);</span>
<span>        *guard = State::Ready;</span>
<span>        ctx.cv.<span>notify_all</span>()</span>
<span>      }</span>
<span>    }</span>
<span>  });</span>
<span>  ManagedHandle { inner, ctx, sender }</span>
<span>}</span>
<span></span>
<span><span>impl</span>&lt;<span>'scope</span>, T&gt; ManagedHandle&lt;<span>'scope</span>, T&gt; {</span>
<span>  <span>pub</span> <span>fn</span> <span>is_paused</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>bool</span> {</span>
<span>    <span>let</span> <span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    *guard == State::Paused</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>unpause</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Paused);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.ctx.cv.<span>notify_all</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>submit</span>&lt;F: <span>FnOnce</span>(&amp;<span>mut</span> T) + <span>Send</span> + <span>'scope</span>&gt;(&amp;<span>self</span>, f: F) {</span>
<span>    <span>let</span> <span>mut </span><span>guard</span> = <span>self</span>.ctx.state.<span>lock</span>().<span>unwrap</span>();</span>
<span>    <span>assert_eq!</span>(*guard, State::Ready);</span>
<span>    *guard = State::Running;</span>
<span>    <span>self</span>.sender.<span>send</span>(<span>Box</span>::<span>new</span>(f)).<span>unwrap</span>();</span>
<span>    guard = <span>self</span></span>
<span>      .ctx</span>
<span>      .cv</span>
<span>      .<span>wait_while</span>(guard, |state| *state == State::Running)</span>
<span>      .<span>unwrap</span>();</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>join</span>(<span>self</span>) {</span>
<span>    <span>while</span> <span>self</span>.<span>is_paused</span>() {</span>
<span>      <span>self</span>.<span>unpause</span>();</span>
<span>    }</span>
<span>    <span>drop</span>(<span>self</span>.sender);</span>
<span>    <span>self</span>.inner.<span>join</span>().<span>unwrap</span>();</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>System under test, not-exactly-atomic counter:</span></p>

<figure>


<pre><code><span><span>use</span> std::sync::atomic::Ordering::SeqCst;</span>
<span></span>
<span><span>#[cfg(test)]</span></span>
<span><span>use</span> managed_thread::AtomicU32;</span>
<span><span>#[cfg(not(test))]</span></span>
<span><span>use</span> std::sync::atomic::AtomicU32;</span>
<span></span>
<span><span>#[derive(Default)]</span></span>
<span><span>pub</span> <span>struct</span> <span>Counter</span> {</span>
<span>  value: AtomicU32,</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>let</span> <span>value</span> = <span>self</span>.value.<span>load</span>(SeqCst);</span>
<span>    <span>self</span>.value.<span>store</span>(value + <span>1</span>, SeqCst);</span>
<span>  }</span>
<span></span>
<span>  <span>pub</span> <span>fn</span> <span>get</span>(&amp;<span>self</span>) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>self</span>.value.<span>load</span>(SeqCst)</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p><span>And the test itself:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span>    <span>let</span> <span>mut </span><span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>mut </span><span>threads</span> = [t1, t2];</span>
<span></span>
<span>      <span>while</span> !rng.<span>is_empty</span>() {</span>
<span>        <span>for</span> (tid, t) <span>in</span> threads.<span>iter_mut</span>().<span>enumerate</span>() {</span>
<span>          <span>if</span> rng.<span>arbitrary</span>()? {</span>
<span>            <span>if</span> t.<span>is_paused</span>() {</span>
<span>              eprintln!(<span>"{tid}: unpause"</span>);</span>
<span>              t.<span>unpause</span>()</span>
<span>            } <span>else</span> {</span>
<span>              eprintln!(<span>"{tid}: increment"</span>);</span>
<span>              t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>              counter_model += <span>1</span>;</span>
<span>            }</span>
<span>          }</span>
<span>        }</span>
<span>      }</span>
<span></span>
<span>      <span>for</span> <span>t</span> <span>in</span> threads {</span>
<span>        t.<span>join</span>();</span>
<span>      }</span>
<span>      <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span></span>
<span>      <span>Ok</span>(())</span>
<span>    })</span>
<span>  });</span>
<span>}</span></code></pre>

</figure>
<p><span>Running it identifies a failure:</span></p>

<figure>


<pre><code><span>---- test_counter stdout ----</span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 4</span>
<span> right: 3</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p><span>Which </span>…<span> is something we got like 5% into this article already, with normal threads! But there</span>’<span>s</span>
<span>more to this failure. First, it is reproducible. If I specify the same seed, I get the </span><em><span>exact</span></em><span> same</span>
<span>interleaving:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x71aafcd900000020</span>);</span>
<span>}</span></code></pre>

</figure>
<p><span>And this is completely machine independent! If </span><em><span>you</span></em><span> specify this seed, you</span>’<span>ll get exact same</span>
<span>interleaving. So, if I am having trouble debugging this, I can DM you this hex in Zulip, and</span>
<span>you</span>’<span>ll be able to help out!</span></p>
<p><span>But there</span>’<span>s more </span>—<span> we don</span>’<span>t need to debug this failure, we can minimize it!</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x71aafcd900000020</span>)</span>
<span>    .<span>minimize</span>();</span>
<span>}</span></code></pre>

</figure>
<p><span>This gives me the following minimization trace:</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>seed 0x4fd7ddff00000020, seed size 32, search time 106.00ns</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>1: increment</span>
<span>seed 0x540c0c1c00000010, seed size 16, search time 282.16µs</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>seed 0x084ca71200000008, seed size 8, search time 805.74µs</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x5699b19400000004, seed size 4, search time 1.44ms</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x4bb0ea5c00000002, seed size 2, search time 4.03ms</span>
<span></span>
<span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>seed 0x9c2a13a600000001, seed size 1, search time 4.31ms</span>
<span></span>
<span>minimized</span>
<span>seed 0x9c2a13a600000001, seed size 1, search time 100.03ms</span></code></pre>

</figure>
<p><span>That is, we ended up with this tiny, minimal example:</span></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>test_counter</span>() {</span>
<span>  arbtest::<span>arbtest</span>(|rng| {</span>
<span>    eprintln!(<span>"begin trace"</span>);</span>
<span>    ...</span>
<span>  })</span>
<span>    .<span>seed</span>(<span>0x9c2a13a600000001</span>);</span>
<span>}</span></code></pre>

</figure>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span></code></pre>

</figure>
<p><span>And </span><em><span>this</span></em><span> is how you properly test concurrent data structures.</span></p>
</section>
<section id="Postscript">

    <h2>
    <a href="#Postscript"><span>Postscript</span> </a>
    </h2>
<p><span>Of course, this is just a toy. But you can see some ways to extend it. For example, right now our</span>
<code>AtomicU32</code><span> just delegates to the real one. But what you </span><em><span>could</span></em><span> do instead is, for each atomic, to</span>
<span>maintain a set of values written and, on read, return an </span><em><span>arbitrary</span></em><span> written value consistent with a</span>
<span>weak memory model.</span></p>
<p><span>You could also be smarter with exploring interleavings. Instead of interleaving threads at random,</span>
<span>like we do here, you can try to apply model checking approaches and prove that you have considered</span>
<span>all meaningfully different interleavings.</span></p>
<p><span>Or you can apply the approach from </span><a href="https://matklad.github.io/2021/11/07/generate-all-the-things.html"><em><span>Generate All The</span>
<span>Things</span></em></a><span> and exhaustively</span>
<span>enumerate </span><em><span>all</span></em><span> interleavings for up to, say, five increments. In fact, why don</span>’<span>t we just do this?</span></p>
<p><code>$ cargo add exhaustigen</code></p>

<figure>


<pre><code><span><span>#[test]</span></span>
<span><span>fn</span> <span>exhaustytest</span>() {</span>
<span>  <span>let</span> <span>mut </span><span>g</span> = exhaustigen::Gen::<span>new</span>();</span>
<span>  <span>let</span> <span>mut </span><span>interleavings_count</span> = <span>0</span>;</span>
<span></span>
<span>  <span>while</span> !g.<span>done</span>() {</span>
<span>    interleavings_count += <span>1</span>;</span>
<span>    <span>let</span> <span>counter</span> = Counter::<span>default</span>();</span>
<span>    <span>let</span> <span>mut </span><span>counter_model</span>: <span>u32</span> = <span>0</span>;</span>
<span></span>
<span>    <span>let</span> <span>increment_count</span> = g.<span>gen</span>(<span>5</span>) <span>as</span> <span>u32</span>;</span>
<span>    std::thread::<span>scope</span>(|scope| {</span>
<span>      <span>let</span> <span>t1</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span>      <span>let</span> <span>t2</span> = managed_thread::<span>spawn</span>(scope, &amp;counter);</span>
<span></span>
<span>      <span>'outer</span>: <span>while</span> t1.<span>is_paused</span>()</span>
<span>        || t2.<span>is_paused</span>()</span>
<span>        || counter_model &lt; increment_count</span>
<span>      {</span>
<span>        <span>for</span> <span>t</span> <span>in</span> [&amp;t1, &amp;t2] {</span>
<span>          <span>if</span> g.<span>flip</span>() {</span>
<span>            <span>if</span> t.<span>is_paused</span>() {</span>
<span>              t.<span>unpause</span>();</span>
<span>              <span>continue</span> <span>'outer</span>;</span>
<span>            }</span>
<span>            <span>if</span> counter_model &lt; increment_count {</span>
<span>              t.<span>submit</span>(|c| c.<span>increment</span>());</span>
<span>              counter_model += <span>1</span>;</span>
<span>              <span>continue</span> <span>'outer</span>;</span>
<span>            }</span>
<span>          }</span>
<span>        }</span>
<span>        <span>return</span> <span>for</span> <span>t</span> <span>in</span> [t1, t2] {</span>
<span>          t.<span>join</span>()</span>
<span>        };</span>
<span>      }</span>
<span></span>
<span>      <span>assert_eq!</span>(counter_model, counter.<span>get</span>());</span>
<span>    });</span>
<span>  }</span>
<span>  eprintln!(<span>"interleavings_count = {:?}"</span>, interleavings_count);</span>
<span>}</span></code></pre>

</figure>
<p><span>The shape of the test is more or less the same, except that we need to make sure that there are no</span>
“<span>dummy</span>”<span> iterations, and that we always either unpause a thread or submit an increment.</span></p>
<p><span>It finds the same bug, naturally:</span></p>

<figure>


<pre><code><span>thread 'exhaustytest' panicked at src/lib.rs:103:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 2</span>
<span> right: 1</span></code></pre>

</figure>
<p><span>But the cool thing is, if we fix the issue by using atomic increment, </span>…</p>

<figure>


<pre><code><span><span>impl</span> <span>AtomicU32</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>fetch_add</span>(</span>
<span>    &amp;<span>self</span>,</span>
<span>    value: <span>u32</span>,</span>
<span>    ordering: Ordering,</span>
<span>  ) <span>-&gt;</span> <span>u32</span> {</span>
<span>    <span>pause</span>();</span>
<span>    <span>let</span> <span>result</span> = <span>self</span>.inner.<span>fetch_add</span>(value, ordering);</span>
<span>    <span>pause</span>();</span>
<span>    result</span>
<span>  }</span>
<span>}</span>
<span></span>
<span><span>impl</span> <span>Counter</span> {</span>
<span>  <span>pub</span> <span>fn</span> <span>increment</span>(&amp;<span>self</span>) {</span>
<span>    <span>self</span>.value.<span>fetch_add</span>(<span>1</span>, SeqCst);</span>
<span>  }</span>
<span>}</span></code></pre>

</figure>
<p>…<span> we can get a rather specific correctness statements out of our test, that </span><em><span>any</span></em><span> sequence of at</span>
<span>most five increments is correct:</span></p>

<figure>


<pre><code><span><span>$</span> t cargo t -r -- exhaustytest --nocapture</span>
<span><span>running 1 test</span></span>
<span><span>all 81133 interleavings are fine!</span></span>
<span><span>test exhaustytest ... ok</span></span>
<span><span></span></span>
<span><span>real 8.65s</span></span>
<span><span>cpu  8.16s (2.22s user + 5.94s sys)</span></span>
<span><span>rss  63.91mb</span></span></code></pre>

</figure>
<p><span>And the last small thing. Recall that our PBT minimized the first sequence it found </span>…<span>:</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>0: increment</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>0: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:56:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 4</span>
<span> right: 3</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x4fd7ddff00000020</span></code></pre>

</figure>
<p>…<span> down to just</span></p>

<figure>


<pre><code><span>begin trace</span>
<span>0: increment</span>
<span>1: increment</span>
<span>0: unpause</span>
<span>1: unpause</span>
<span>thread 'test_counter' panicked at src/lib.rs:57:7:</span>
<span>assertion `left == right` failed</span>
<span>  left: 2</span>
<span> right: 1</span>
<span></span>
<span>arbtest failed!</span>
<span>    Seed: 0x9c2a13a600000001</span></code></pre>

</figure>
<p><span>But we never implemented shrinking! How is this possible? Well, strictly speaking, this is out of</span>
<span>scope for this post. And I</span>’<span>ve already described this</span>
<a href="https://tigerbeetle.com/blog/2023-03-28-random-fuzzy-thoughts"><span>elsewhere</span></a><span>. And, at 32k, this is the</span>
<span>third-longest post on this blog. And it</span>’<span>s 3AM here in Lisbon right now. But of course I</span>’<span>ll explain!</span></p>
<p><span>The trick is the simplified </span><a href="https://hypothesis.works/articles/compositional-shrinking/"><span>hypothesis</span>
<span>approach</span></a><span>. The</span>
<a href="https://docs.rs/arbtest/latest/arbtest/"><span>arbtest</span></a><span> PBT library we in this post is based on a</span>
<span>familiar interface of a PRNG:</span></p>

<figure>


<pre><code><span>arbtest::<span>arbtest</span>(|rng| {</span>
<span>  <span>let</span> <span>random_int</span>: <span>usize</span> = rng.<span>int_in_range</span>(<span>0</span>..=<span>100</span>)?;</span>
<span>  <span>let</span> <span>random_bool</span>: <span>bool</span> = rng.<span>arbitrary</span>()?;</span>
<span>  <span>Ok</span>(())</span>
<span>});</span></code></pre>

</figure>
<p><span>But there</span>’<span>s a twist! This is a </span><em><span>finite</span></em><span> PRNG. So, if you ask it to flip a coin it can give you</span>
<span>heads. And next time it might give you tails. But if you continue asking it for more, at some point</span>
<span>it</span>’<span>ll give you </span><span><code>Err(OutOfEntropy)</code><span>.</span></span></p>
<p><span>That</span>’<span>s why all these </span><code>?</code><span> and the outer loop of</span>
<span><code>while !rng.is_empty() {</code><span>.</span></span></p>
<p><span>In other words, as soon as the test runs out of entropy, it short-circuits and completes. And that</span>
<span>means that by reducing the amount of entropy available the test becomes shorter, and this works</span>
<span>irrespective of how complex is the logic inside the test!</span></p>
<p><span>And </span>“<span>entropy</span>”<span> is a big scary word here, what actually happens is that the PRNG is just an </span><code>&amp;mut
&amp;[u8]</code><span> inside. That is, a slice of random bytes, which is shortened every time you ask for a random</span>
<span>number. And the shorter the initial slice, the simpler the test gets. Minimization can be this</span>
<span>simple!</span></p>
<p><span>You can find source code for this article at</span>
<a href="https://github.com/matklad/properly-concurrent">https://github.com/matklad/properly-concurrent</a></p>
</section>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple okays Epic Games marketplace app in Europe (133 pts)]]></title>
            <link>https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/</link>
            <guid>40888461</guid>
            <pubDate>Sat, 06 Jul 2024 06:07:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/">https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/</a>, See on <a href="https://news.ycombinator.com/item?id=40888461">Hacker News</a></p>
Couldn't get https://www.reuters.com/technology/epic-games-says-apple-stalling-launch-its-game-store-europe-2024-07-05/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Build and train GPT-2 from scratch using PyTorch (134 pts)]]></title>
            <link>https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba</link>
            <guid>40888090</guid>
            <pubDate>Sat, 06 Jul 2024 04:03:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba">https://differ.blog/p/here-s-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-ace4ba</a>, See on <a href="https://news.ycombinator.com/item?id=40888090">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Are you tired of always using ChatGPT and curious about how to build your own language model? Well, you’re in the right place! Today, we’re going to create GPT-2&nbsp;, a powerful language model developed by OpenAI, from scratch that can generate human-like text by predicting the next word in a sequence.</p>
<p>To dive deeper into the theory and architecture of GPT-2, I highly recommend reading <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2</a> by Jay Alammar. This article provides an excellent visual and intuitive explanation of GPT-2 and its inner workings. I’ll be referring to some of the visuals from the article to explain things better.</p>
<blockquote>
<p>I have tried to make this as simpler as possible. Anyone with any level of Python or machine learning can follow along and build the model.</p>
</blockquote>
<h3>Resources</h3>
<p>This project will take you through all the steps for building a simple GPT-2 model and train on bunch of Taylor Swift and Ed Sheeran songs. We’ll see what it will come up at the end&nbsp;:).</p>
<p>The dataset and source codes for this article will be available in <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/r?url=https%3A%2F%2Fgithub.com%2Fajeetkharel%2Fgpt2-from-scratch">Github</a>.</p>
<blockquote>
<p>I’ll also add a Jupyter Notebook which replicates this article so you can follow along with running code and understanding side-by-side.</p>
</blockquote>
<h3>Building GPT-2 Architecture</h3>
<p>We will take this project step-by-step by continuously improving a bare-bone model and adding layers based on the original <a rel="noopener noreferrer nofollow ugc" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> implementation.</p>
<p>Here are the steps we will follow:</p>
<ol>
<li><strong>Building a custom Tokenizer</strong></li>
<li><strong>Building a Data Loader</strong></li>
<li><strong>Train a simple language model</strong></li>
<li><strong>Implement GPT-2 architecture (part 2)</strong> <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">🔗</a></li>
</ol>
<p>This project is divided into two parts, the first one goes through the basics of language modelling and <a rel="noopener noreferrer nofollow ugc" href="https://medium.com/@mramitkharel/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-part-2-9b41d15baf62">Part 2</a> jumps straight into GPT-2 implementation. I suggest you to follow along with the article and build it yourself which makes learning GPT-2 more interesting and fun.</p>
<blockquote>
<p>Note: This whole project will be done in a single python file so it will be easy for you to follow along block by block.</p>
</blockquote>
<p><strong>Final Model:</strong></p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*35AaHBa5imxVIbbjByIE2Q.png" alt=""></p>
<p><strong>Final Model output:</strong></p>
<blockquote>
<p>Your summer has a matter likely you trying
I wish you would call
Oh-oh,
I'll be a lot of everyoneI just walked
You're sorry"Your standing in love out,
And something would wait forever bring 'Don't you think about the storyIf you're perfectly
I want your beautiful
You had sneak for you make me
This ain't think that it wanted you this enough for lonely thing
It's a duchess and I did nothin' home was no head
Oh, but you left me
Was all the less pair of the applause
Honey, he owns me now
But've looks for us?"
If I see you'll be alright
You understand, a out of theWait for me I can't call
Everything
Oh, no words don't read about me
You should've been so
You're doing what you so tired,
If you, you got perfect fall</p>
</blockquote>
<p>Like the song? Then let’s get building..</p>
<h3><strong>1. Building a custom Tokenizer</strong></h3>
<p>Language models don’t see text like us. Instead they recognize sequence of numbers as tokens of specific text. So, the first step is to import our data and build our own character level Tokenizer.</p>
<pre><code>data_dir = <span>"data.txt"</span>
text = <span>open</span>(data_dir, <span>'r'</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*34WkqssQKHKpdO1yTH0n-g.png" alt=""></p>
<p>If you see the output above, we have a list of all unique characters extracted from the text data in the initialization process. Character tokenization is basically using the index position of characters from the vocabulary and mapping it to corresponding character in the input text.</p>
<pre><code># build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>""</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*adFjPqc2Ks1MY2uXuB9DGQ.png" alt=""></p>
<p>Convert our text data into tokens:</p>
<p><strong>Installation</strong>:</p>
<p><code>pip install torch</code></p>
<p><strong>Code</strong>:</p>
<pre><code><span>import</span> torch
# use cpu or gpu based on your system
device = <span>"cpu"</span>
<span>if</span> torch.<span>cuda</span>.<span>is_available</span>():
    device = <span>"cuda"</span>

# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<p>Now, we have the tokenized tensor <code>data</code> where each characters in the text is converted to the respective tokens.</p>
<p><strong>So far:</strong></p>
<pre><code><span>import</span> torch

data_dir = <span>"data.txt"</span>
text = <span>open</span>(data_dir, <span>'r'</span>).<span>read</span>() # load all the data <span>as</span> simple string

# <span>Get</span> all unique characters <span>in</span> the text <span>as</span> vocabulary
chars = <span>list</span>(<span>set</span>(text))
vocab_size = <span>len</span>(chars)

# build the character level tokenizer
chr_to_idx = {<span>c</span>:i <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}
idx_to_chr = {<span>i</span>:c <span>for</span> i, c <span>in</span> <span>enumerate</span>(chars)}

def <span>encode</span>(<span>input_text</span>: str) -&gt; list[int]:
    <span>return</span> [chr_to_idx[t] <span>for</span> t <span>in</span> input_text]

def <span>decode</span>(<span>input_tokens</span>: list[int]) -&gt; <span>str</span>:
    <span>return</span> <span>""</span>.<span>join</span>([idx_to_chr[i] <span>for</span> i <span>in</span> input_tokens])


# convert our text data into tokenized tensor
data = torch.<span>tensor</span>(<span>encode</span>(text), dtyppe=torch.<span>long</span>, device=device)
</code></pre>
<h3><strong>2. Building a Data&nbsp;Loader</strong></h3>
<p>Now, before building our model, we have to define how we are going to feed the data into the model for training and what the data looks like in terms of dimensions and batch size.</p>
<p>Let’s define our data loader as below:</p>
<pre><code>train_batch_size = <span>16</span>  # training batch size
eval_batch_size = <span>8</span>  # evaluation batch size
context_length = <span>256</span>  # number <span>of</span> tokens processed <span>in</span> a single batch
train_split = <span>0.8</span>  # percentage <span>of</span> data to use <span>from</span> total data <span>for</span> training

# split data into trian and <span>eval</span>
n_data = <span>len</span>(data)
train_data = data[:<span>int</span>(n_data * train_split)]
eval_data = data[<span>int</span>(n_data * train_split):]


<span>class</span> <span>DataLoader</span>:
    def <span>__init__</span>(self, tokens, batch_size, context_length) -&gt; <span>None</span>:
        self.<span>tokens</span> = tokens
        self.<span>batch_size</span> = batch_size
        self.<span>context_length</span> = context_length

        self.<span>current_position</span> = <span>0</span>

    def <span>get_batch</span>(self) -&gt; torch.<span>tensor</span>:
        b, c = self.<span>batch_size</span>, self.<span>context_length</span>

        start_pos = self.<span>current_position</span>
        end_pos = self.<span>current_position</span> + b * c + <span>1</span>

        # <span>if</span> the batch exceeds total length, get the data till last token
        # and take remaining <span>from</span> starting token to avoid always excluding some data
        add_data = -<span>1</span> # n, <span>if</span> length exceeds and we need <span>`n`</span> additional tokens <span>from</span> start
        <span>if</span> end_pos &gt; <span>len</span>(self.<span>tokens</span>):
            add_data = end_pos - <span>len</span>(self.<span>tokens</span>) - <span>1</span>
            end_pos = <span>len</span>(self.<span>tokens</span>) - <span>1</span>

        d = self.<span>tokens</span>[<span>start_pos</span>:end_pos]
        <span>if</span> add_data != -<span>1</span>:
            d = torch.<span>cat</span>([d, self.<span>tokens</span>[:add_data]])
        x = (d[:-<span>1</span>]).<span>view</span>(b, c)  # inputs
        y = (d[<span>1</span>:]).<span>view</span>(b, c)  # targets

        self.<span>current_position</span> += b * c # set the next position
        <span>return</span> x, y

train_loader = <span>DataLoader</span>(train_data, train_batch_size, context_length)
eval_loader = <span>DataLoader</span>(eval_data, eval_batch_size, context_length)
</code></pre>
<p>Example:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*GMpC_jFxFpk_1xK19YvbrA.png" alt=""></p>
<p>Now we have our own customized data loader for both training and evaluation. The loader has a <code>get_batch</code> function which returns batches of <code>batch_size * context_length</code>.</p>
<p>If you are wondering why <code>x</code> is from <code>start</code> to <code>end</code> and <code>y</code> is from <code>start+1</code> to <code>end+1</code>, it’s because the main task for this model will be to predict next sequence given the previous. So there will be an extra token in <code>y</code> for it to predict the (n+1) token given last n tokens of <code>x</code>. If it sounds complicated look at the below visual:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/0*jTrSzRD-KGPs3v5E.gif" alt=""><em>Figure 2: GPT-2 Input &amp; Output flow from “The Illustrated GPT-2” by Jay&nbsp;Alammar.</em></p>
<h3><strong>3. Train a simple language&nbsp;model</strong></h3>
<p>Now we are ready to build and train a simple language model using the data we have just loaded.</p>
<p>For this section, we will keep it very simple and implement a simple Bi-Gram Model where given the last token predict the next token. As you can see below we will be using just the Embedding layer while ignoring the main decoder block.</p>
<p>An Embedding layer represents <code>n = d_model</code> unique properties of all the characters in our vocabulary and based on which the layer pops out the property using the token index or in our case the index of our character in the vocabulary.</p>
<p>You will be amazed how well the model will behave just by using the Embeddings. And we will be improving the model step by step by adding more layers, so sit tight and follow along.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*9cYT2nBANRzBr3vqQVLBsw.png" alt=""></p>
<p><strong>Initialization</strong>:</p>
<h2>used to define size of embeddings</h2>
<p>d_model = vocab_size </p>
<p>The embedding dimension or <code>d_model</code> is <code>vocab_size</code> currently because the final output has to map to the logits for each character in vocab to calculate their probabilities. Later on we will introduce a <code>Linear</code> layer which will map <code>d_model</code> to <code>vocab_size</code> and then we can have a custom embedding_dimension.</p>
<p><strong>Model</strong>:</p>
<pre><code><span>import</span> torch.<span>nn</span> <span>as</span> nn
<span>import</span> torch.<span>nn</span>.<span>functional</span> <span>as</span> F

<span>class</span> <span>GPT</span>(nn.<span>Module</span>):
    def <span>__init__</span>(self, vocab_size, d_model):
        <span>super</span>().<span>__init__</span>()
        self.<span>wte</span> = nn.<span>Embedding</span>(vocab_size, d_model) # word token embeddings
    
    def <span>forward</span>(self, inputs, targets = <span>None</span>):
        logits = self.<span>wte</span>(inputs) # dim -&gt; batch_size, sequence_length, d_model
        loss = <span>None</span>
        <span>if</span> targets != <span>None</span>:
            batch_size, sequence_length, d_model = logits.<span>shape</span>
            # to calculate loss <span>for</span> all token embeddings <span>in</span> a batch
            # kind <span>of</span> a requirement <span>for</span> cross_entropy
            logits = logits.<span>view</span>(batch_size * sequence_length, d_model)
            targets = targets.<span>view</span>(batch_size * sequence_length)
            loss = F.<span>cross_entropy</span>(logits, targets)
        <span>return</span> logits, loss
    
    def <span>generate</span>(self, inputs, max_new_tokens):
        # <span>this</span> will store the model outputs along <span>with</span> the initial input sequence
        # make a copy so that it doesn<span>'t interfare with model 
        for _ in range(max_new_tokens):
            # we only pass targets on training to calculate loss
            logits, _ = self(inputs)  
            # for all the batches, get the embeds for last predicted sequence
            logits = logits[:, -1, :] 
            probs = F.softmax(logits, dim=1)            
            # get the probable token based on the input probs
            idx_next = torch.multinomial(probs, num_samples=1) 
            
            inputs = torch.cat([inputs, idx_next], dim=1)
        # as the inputs has all model outputs + initial inputs, we can use it as final output
        return inputs

m = GPT(vocab_size=vocab_size, d_model=d_model).to(device)
</span></code></pre>
<p>We have now successfully defined our model with just one <code>Embedding</code> layer and <code>Softmax</code> for token generation. Let’s see how our model behaves when given some input characters.</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*dQpkxEARkvXbpJpI7dCRYA.png" alt=""></p>
<p>😄 Pretty interesting!! But we are not quite there yet.</p>
<p>Now the final step is to train our model and give it some knowledge about the characters. Let’s setup our optimizer. We will use a simple <code>AdamW</code> optimizer for now with <code>0.001</code> learning rate. We will go through improving the optimization in later sections.</p>
<pre><code>lr = <span>1e-3</span>
optim = torch.<span>optim</span>.<span>AdamW</span>(m.<span>parameters</span>(), lr=lr)
<span>Below</span> is a very simple training loop.
epochs = <span>5000</span>
eval_steps = <span>1000</span> # perform evaluation <span>in</span> every n steps
<span>for</span> ep <span>in</span> <span>range</span>(epochs):
    xb, yb = train_loader.<span>get_batch</span>()

    logits, loss = <span>m</span>(xb, yb)
    optim.<span>zero_grad</span>(set_to_none=<span>True</span>)
    loss.<span>backward</span>()
    optim.<span>step</span>()

    <span>if</span> ep % eval_steps == <span>0</span> or ep == epochs-<span>1</span>:
        m.<span>eval</span>()
        <span>with</span> torch.<span>no_grad</span>():
            xvb, yvb = eval_loader.<span>get_batch</span>()
            _, e_loss = <span>m</span>(xvb, yvb)

            <span>print</span>(f<span>"Epoch: {ep}tlr: {lr}ttrain_loss: {loss}teval_loss: {e_loss}"</span>)
        m.<span>train</span>() # back to training mode
</code></pre>
<p>Let’s run:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*ikOrVlB0KHzrTWTpOLi9Lw.png" alt=""></p>
<p>So we got a pretty good loss result. But we are not there yet. As you can see, the error decreased by a higher amount until epoch 2000 and not much improvements afterwards. It’s because the model doesn’t yet have much brain power (or layers/neural networks) and it’s just comparing embedding of one character with another.</p>
<p>The output now looks like below:</p>
<p><img src="https://cdn-images-1.medium.com/max/800/1*fEEJXUZrhAIORdD0tXk_wA.png" alt=""></p>
<p>😮 OK!! Not very pleasing but definitely some improvements than the first generation which was without any training (Obviously). The model is starting to know how the songs are formatted and the lines and everything which is pretty impressive.</p>
<p>Now, as this article is getting too longer, I will add rest of the sections in the Part 2 below:</p>

<p>Thanks for reading the article. I hope you learned something new. If you have any questions/feedback, feel free to leave a comment.</p>
<h3>References</h3>
<p><em>Automatic Arabic Poem Generation with GPT-2 — Scientific Figure on ResearchGate. Available from:</em> <a rel="noopener noreferrer nofollow ugc" href="https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229"><em>https://www.researchgate.net/figure/GPT-2-architecture-Heilbron-et-al-2019_fig1_358654229</em></a></p>
<p><em>Alammar, J (2018). The Illustrated GPT-2 [Blog post]. Retrieved from</em> <a rel="noopener noreferrer nofollow ugc" href="https://jalammar.github.io/illustrated-gpt2/">https://jalammar.github.io/illustrated-gpt2/</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to implement a hash table in C (2021) (107 pts)]]></title>
            <link>https://benhoyt.com/writings/hash-table-in-c/</link>
            <guid>40887806</guid>
            <pubDate>Sat, 06 Jul 2024 02:36:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://benhoyt.com/writings/hash-table-in-c/">https://benhoyt.com/writings/hash-table-in-c/</a>, See on <a href="https://news.ycombinator.com/item?id=40887806">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">

<p>March 2021</p>



<blockquote>
  <p>Summary: An explanation of how to implement a simple hash table data structure using the C programming language. I briefly demonstrate linear and binary search, and then design and implement a hash table. My goal is to show that hash table internals are not scary, but – within certain constraints – are easy enough to build from scratch.</p>

  <p><strong>Go to:</strong> <a href="#linear-search">Linear search</a> | <a href="#binary-search">Binary search</a> | <a href="#hash-tables">Hash tables</a> | <a href="#hash-table-implementation">Implementation</a> | <a href="#discussion">Discussion</a></p>
</blockquote>

<p>Recently I wrote an <a href="https://benhoyt.com/writings/count-words/">article that compared</a> a simple program that counts word frequencies across various languages, and one of the things that came up was how C doesn’t have a hash table data structure in its standard library.</p>

<p>There are many things you can do when you realize this: use linear search, use binary search, grab someone else’s hash table implementation, or write your own hash table. Or switch to a richer language. We’re going to take a quick look at linear and binary search, and then learn how to write our own hash table. This is often necessary in C, but it can also be useful if you need a custom hash table when using another language.</p>

<h2 id="linear-search">Linear search</h2>

<p>The simplest option is to use <a href="https://en.wikipedia.org/wiki/Linear_search">linear search</a> to scan through an array. This is actually not a bad strategy if you’ve only got a few items – in my <a href="https://github.com/benhoyt/ht/blob/master/samples/perflbh.c">simple comparison</a> using strings, it’s faster than a hash table lookup up to about 7 items (but unless your program is very performance-sensitive, it’s probably fine up to 20 or 30 items). Linear search also allows you to append new items to the end of the array. With this type of search you’re comparing an average of <em>num_keys</em>/2 items.</p>

<p>Let’s say you’re searching for the key <code>bob</code> in the following array (each item is a string key with an associated integer value):</p>

<table>
  <tbody>
    <tr>
      <td><strong>Index</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <td><strong>Key</strong></td>
      <td><code>foo</code></td>
      <td><code>bar</code></td>
      <td><code>bazz</code></td>
      <td><code>buzz</code></td>
      <td><code>bob</code></td>
      <td><code>jane</code></td>
      <td><code>x</code></td>
    </tr>
    <tr>
      <td><strong>Value</strong></td>
      <td>10</td>
      <td>42</td>
      <td>36</td>
      <td>7</td>
      <td>11</td>
      <td>100</td>
      <td>200</td>
    </tr>
  </tbody>
</table>

<p>You simply start at the beginning (<code>foo</code> at index 0) and compare each key. If the key matches what you’re looking for, you’re done. If not, you move to the next slot. Searching for <code>bob</code> takes five steps (indexes 0 through 4).</p>

<p>Here is the algorithm in C (assuming each array item is a string key and integer value):</p>

<div><pre><code><span>typedef</span> <span>struct</span> <span>{</span>
    <span>char</span><span>*</span> <span>key</span><span>;</span>
    <span>int</span> <span>value</span><span>;</span>
<span>}</span> <span>item</span><span>;</span>

<span>item</span><span>*</span> <span>linear_search</span><span>(</span><span>item</span><span>*</span> <span>items</span><span>,</span> <span>size_t</span> <span>size</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span><span>=</span><span>0</span><span>;</span> <span>i</span><span>&lt;</span><span>size</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>items</span><span>[</span><span>i</span><span>].</span><span>key</span><span>,</span> <span>key</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>return</span> <span>&amp;</span><span>items</span><span>[</span><span>i</span><span>];</span>
        <span>}</span>
    <span>}</span>
    <span>return</span> <span>NULL</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>item</span> <span>items</span><span>[]</span> <span>=</span> <span>{</span>
        <span>{</span><span>"foo"</span><span>,</span> <span>10</span><span>},</span> <span>{</span><span>"bar"</span><span>,</span> <span>42</span><span>},</span> <span>{</span><span>"bazz"</span><span>,</span> <span>36</span><span>},</span> <span>{</span><span>"buzz"</span><span>,</span> <span>7</span><span>},</span>
        <span>{</span><span>"bob"</span><span>,</span> <span>11</span><span>},</span> <span>{</span><span>"jane"</span><span>,</span> <span>100</span><span>},</span> <span>{</span><span>"x"</span><span>,</span> <span>200</span><span>}};</span>
    <span>size_t</span> <span>num_items</span> <span>=</span> <span>sizeof</span><span>(</span><span>items</span><span>)</span> <span>/</span> <span>sizeof</span><span>(</span><span>item</span><span>);</span>

    <span>item</span><span>*</span> <span>found</span> <span>=</span> <span>linear_search</span><span>(</span><span>items</span><span>,</span> <span>num_items</span><span>,</span> <span>"bob"</span><span>);</span>
    <span>if</span> <span>(</span><span>!</span><span>found</span><span>)</span> <span>{</span>
        <span>return</span> <span>1</span><span>;</span>
    <span>}</span>
    <span>printf</span><span>(</span><span>"linear_search: value of 'bob' is %d</span><span>\n</span><span>"</span><span>,</span> <span>found</span><span>-&gt;</span><span>value</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<h2 id="binary-search">Binary search</h2>

<p>Another simple approach is to put the items in an array which is sorted by key, and use <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a> to reduce the number of comparisons. This is kind of how we might look something up in a (paper) dictionary.</p>

<p>C even has a <code>bsearch</code> function in its standard library. Binary search is reasonably fast even for hundreds of items (though not as fast as a hash table), because you’re only comparing an average of log(<em>num_keys</em>) items. However, because the array needs to stay sorted, you can’t insert items without copying the rest down, so insertions still require an average of <em>num_keys</em>/2 operations.</p>

<p>Assume we’re looking up <code>bob</code> again (in this pre-sorted array):</p>

<table>
  <tbody>
    <tr>
      <td><strong>Index</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <td><strong>Key</strong></td>
      <td><code>bar</code></td>
      <td><code>bazz</code></td>
      <td><code>bob</code></td>
      <td><code>buzz</code></td>
      <td><code>foo</code></td>
      <td><code>jane</code></td>
      <td><code>x</code></td>
    </tr>
    <tr>
      <td><strong>Value</strong></td>
      <td>42</td>
      <td>36</td>
      <td>11</td>
      <td>7</td>
      <td>10</td>
      <td>100</td>
      <td>200</td>
    </tr>
  </tbody>
</table>

<p>With binary search, we start in the middle (<code>buzz</code>), and if the key there is greater than what we’re looking for, we repeat the process with the lower half. If it’s greater, we repeat the process with the higher half. In this case it results in three steps, at indexes 3, 1, 2, and then we have it. This is 3 steps instead of 5, and the improvement over linear search gets (exponentially) better the more items you have.</p>

<p>Here’s how you’d do it in C (with and without <code>bsearch</code>). The definition of the <code>item</code> struct is the same as above.</p>

<div><pre><code><span>int</span> <span>cmp</span><span>(</span><span>const</span> <span>void</span><span>*</span> <span>a</span><span>,</span> <span>const</span> <span>void</span><span>*</span> <span>b</span><span>)</span> <span>{</span>
    <span>item</span><span>*</span> <span>item_a</span> <span>=</span> <span>(</span><span>item</span><span>*</span><span>)</span><span>a</span><span>;</span>
    <span>item</span><span>*</span> <span>item_b</span> <span>=</span> <span>(</span><span>item</span><span>*</span><span>)</span><span>b</span><span>;</span>
    <span>return</span> <span>strcmp</span><span>(</span><span>item_a</span><span>-&gt;</span><span>key</span><span>,</span> <span>item_b</span><span>-&gt;</span><span>key</span><span>);</span>
<span>}</span>

<span>item</span><span>*</span> <span>binary_search</span><span>(</span><span>item</span><span>*</span> <span>items</span><span>,</span> <span>size_t</span> <span>size</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>size</span> <span>+</span> <span>size</span> <span>&lt;</span> <span>size</span><span>)</span> <span>{</span>
        <span>return</span> <span>NULL</span><span>;</span> <span>// size too big; avoid overflow</span>
    <span>}</span>
    <span>size_t</span> <span>low</span> <span>=</span> <span>0</span><span>;</span>
    <span>size_t</span> <span>high</span> <span>=</span> <span>size</span><span>;</span>
    <span>while</span> <span>(</span><span>low</span> <span>&lt;</span> <span>high</span><span>)</span> <span>{</span>
        <span>size_t</span> <span>mid</span> <span>=</span> <span>(</span><span>low</span> <span>+</span> <span>high</span><span>)</span> <span>/</span> <span>2</span><span>;</span>
        <span>int</span> <span>c</span> <span>=</span> <span>strcmp</span><span>(</span><span>items</span><span>[</span><span>mid</span><span>].</span><span>key</span><span>,</span> <span>key</span><span>);</span>
        <span>if</span> <span>(</span><span>c</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>return</span> <span>&amp;</span><span>items</span><span>[</span><span>mid</span><span>];</span>
        <span>}</span>
        <span>if</span> <span>(</span><span>c</span> <span>&lt;</span> <span>0</span><span>)</span> <span>{</span>
            <span>low</span> <span>=</span> <span>mid</span> <span>+</span> <span>1</span><span>;</span> <span>// eliminate low half of array</span>
        <span>}</span> <span>else</span> <span>{</span>
            <span>high</span> <span>=</span> <span>mid</span><span>;</span>    <span>// eliminate high half of array</span>
        <span>}</span>
    <span>}</span>
    <span>// Entire array has been eliminated, key not found.</span>
    <span>return</span> <span>NULL</span><span>;</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>item</span> <span>items</span><span>[]</span> <span>=</span> <span>{</span>
        <span>{</span><span>"bar"</span><span>,</span> <span>42</span><span>},</span> <span>{</span><span>"bazz"</span><span>,</span> <span>36</span><span>},</span> <span>{</span><span>"bob"</span><span>,</span> <span>11</span><span>},</span> <span>{</span><span>"buzz"</span><span>,</span> <span>7</span><span>},</span>
        <span>{</span><span>"foo"</span><span>,</span> <span>10</span><span>},</span> <span>{</span><span>"jane"</span><span>,</span> <span>100</span><span>},</span> <span>{</span><span>"x"</span><span>,</span> <span>200</span><span>}};</span>
    <span>size_t</span> <span>num_items</span> <span>=</span> <span>sizeof</span><span>(</span><span>items</span><span>)</span> <span>/</span> <span>sizeof</span><span>(</span><span>item</span><span>);</span>

    <span>item</span> <span>key</span> <span>=</span> <span>{</span><span>"bob"</span><span>,</span> <span>0</span><span>};</span>
    <span>item</span><span>*</span> <span>found</span> <span>=</span> <span>bsearch</span><span>(</span><span>&amp;</span><span>key</span><span>,</span> <span>items</span><span>,</span> <span>num_items</span><span>,</span> <span>sizeof</span><span>(</span><span>item</span><span>),</span> <span>cmp</span><span>);</span>
    <span>if</span> <span>(</span><span>found</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>1</span><span>;</span>
    <span>}</span>
    <span>printf</span><span>(</span><span>"bsearch: value of 'bob' is %d</span><span>\n</span><span>"</span><span>,</span> <span>found</span><span>-&gt;</span><span>value</span><span>);</span>

    <span>found</span> <span>=</span> <span>binary_search</span><span>(</span><span>items</span><span>,</span> <span>num_items</span><span>,</span> <span>"bob"</span><span>);</span>
    <span>if</span> <span>(</span><span>found</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>1</span><span>;</span>
    <span>}</span>
    <span>printf</span><span>(</span><span>"binary_search: value of 'bob' is %d</span><span>\n</span><span>"</span><span>,</span> <span>found</span><span>-&gt;</span><span>value</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Note: in <code>binary_search</code>, it would be slightly better to avoid the up-front “half size overflow check” and allow the entire range of <code>size_t</code>. This would mean changing the <code>mid</code> calculation to <code>low + (high-low)/2</code>. However, I’m going to leave the code stand for educational purposes – with the initial overflow check, I don’t think there’s a bug, but it is non-ideal that I’m only allowing half the range of <code>size_t</code>. Not that I’ll be searching a 16 exabyte array on my 64-bit system anytime soon! For further reading, see the article <a href="https://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html"><em>Nearly All Binary Searches and Mergesorts are Broken</em></a>. Thanks Seth Arnold and Olaf Seibert for the feedback.</p>

<h2 id="hash-tables">Hash tables</h2>

<p><a href="https://en.wikipedia.org/wiki/Hash_table">Hash tables</a> can seem quite scary: there are a lot of different types, and a ton of different optimizations you can do. However, if you use a simple hash function together with what’s called “linear probing” you can create a decent hash table quite easily.</p>

<p>If you don’t know how a hash table works, here’s a quick refresher. A hash table is a container data structure that allows you to quickly look up a key (often a string) to find its corresponding value (any data type). Under the hood, they’re arrays that are indexed by a hash function of the key.</p>

<p>A hash function turns a key into a random-looking number, and it must always return the same number given the same key. For example, with the hash function we’re going to use (64-bit <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function#FNV-1a_hash">FNV-1a</a>), the hashes of the keys above are as follows:</p>

<!-- To calculate hashes, see: https://play.golang.org/p/UFtOXJ4pXCL -->

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Hash</th>
      <th>Hash modulo 16</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bar</code></td>
      <td>16101355973854746</td>
      <td>10</td>
    </tr>
    <tr>
      <td><code>bazz</code></td>
      <td>11123581685902069096</td>
      <td>8</td>
    </tr>
    <tr>
      <td><code>bob</code></td>
      <td>21748447695211092</td>
      <td>4</td>
    </tr>
    <tr>
      <td><code>buzz</code></td>
      <td>18414333339470238796</td>
      <td>12</td>
    </tr>
    <tr>
      <td><code>foo</code></td>
      <td>15902901984413996407</td>
      <td>7</td>
    </tr>
    <tr>
      <td><code>jane</code></td>
      <td>10985288698319103569</td>
      <td>1</td>
    </tr>
    <tr>
      <td><code>x</code></td>
      <td>12638214688346347271</td>
      <td>7 (same as <code>foo</code>)</td>
    </tr>
  </tbody>
</table>

<p>The reason I’ve shown the hash modulo 16 is because we’re going to start with an array of 16 elements, so we need to limit the hash to the number of elements in the array – the <a href="https://en.wikipedia.org/wiki/Modulo_operation">modulo</a> operation divides by 16 and gives the remainder, limiting the array index to the range 0 through 15.</p>

<p>When we insert a value into the hash table, we calculate its hash, modulo by 16, and use that as the array index. So with an array of size 16, we’d insert <code>bar</code> at index 10, <code>bazz</code> at 8, <code>bob</code> at 4, and so on. Let’s insert all the items into our hash table array (except for <code>x</code> – we’ll get to that below):</p>

<table>
  <tbody>
    <tr>
      <td><strong>Index</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td>9</td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>13</td>
      <td>14</td>
      <td>15</td>
    </tr>
    <tr>
      <td><strong>Key</strong></td>
      <td>.</td>
      <td><code>jane</code></td>
      <td>.</td>
      <td>.</td>
      <td><code>bob</code></td>
      <td>.</td>
      <td>.</td>
      <td><code>foo</code></td>
      <td><code>bazz</code></td>
      <td>.</td>
      <td><code>bar</code></td>
      <td>.</td>
      <td><code>buzz</code></td>
      <td>.</td>
      <td>.</td>
      <td>.</td>
    </tr>
    <tr>
      <td><strong>Value</strong></td>
      <td>.</td>
      <td>100</td>
      <td>.</td>
      <td>.</td>
      <td>11</td>
      <td>.</td>
      <td>.</td>
      <td>10</td>
      <td>36</td>
      <td>.</td>
      <td>42</td>
      <td>.</td>
      <td>7</td>
      <td>.</td>
      <td>.</td>
      <td>.</td>
    </tr>
  </tbody>
</table>

<p>To look up a value, we simply fetch <code>array[hash(key) % 16]</code>. If the array size is a power of two, we can use <code>array[hash(key) &amp; 15]</code>. Note how the order of the elements is no longer meaningful.</p>

<p>But what if two keys hash to the same value (after the modulo 16)? Depending on the hash function and the size of the array, this is fairly common. For example, when we try to add <code>x</code> to the array above, its hash modulo 16 is 7. But we already have <code>foo</code> at index 7, so we get a <em>collision</em>.</p>

<p>There are various ways of handling collisions. Traditionally you’d create a hash array of a certain size, and if there was a collision, you’d use a <a href="https://en.wikipedia.org/wiki/Linked_list">linked list</a> to store the values that hashed to the same index. However, linked lists normally require an extra memory allocation when you add an item, and traversing them means following pointers scattered around in memory, which is <a href="https://baptiste-wicht.com/posts/2012/11/cpp-benchmark-vector-vs-list.html">relatively slow</a> on modern CPUs.</p>

<p>A simpler and faster way of dealing with collisions is <em>linear probing</em>: if we’re trying to insert an item but there’s one already there, simply move to the next slot. If the next slot is full too, move along again, until you find an empty one, wrapping around to the beginning if you hit the end of the array. (There are <a href="https://en.wikipedia.org/wiki/Open_addressing">other ways</a> of probing than just moving to the next slot, but that’s beyond the scope of this article.) This technique is a lot faster than linked lists, because your CPU’s cache has probably fetched the next items already.</p>

<p>Here’s what the hash table array looks like after adding “collision” <code>x</code> (with value 200). We try index 7 first, but that’s holding <code>foo</code>, so we move to index 8, but that’s holding <code>bazz</code>, so we move again to index 9, and that’s empty, so we insert it there:</p>

<table>
  <tbody>
    <tr>
      <td><strong>Index</strong></td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>6</td>
      <td>7</td>
      <td>8</td>
      <td><strong>9</strong></td>
      <td>10</td>
      <td>11</td>
      <td>12</td>
      <td>13</td>
      <td>14</td>
      <td>15</td>
    </tr>
    <tr>
      <td><strong>Key</strong></td>
      <td>.</td>
      <td><code>jane</code></td>
      <td>.</td>
      <td>.</td>
      <td><code>bob</code></td>
      <td>.</td>
      <td>.</td>
      <td><code>foo</code></td>
      <td><code>bazz</code></td>
      <td><strong><code>x</code></strong></td>
      <td><code>bar</code></td>
      <td>.</td>
      <td><code>buzz</code></td>
      <td>.</td>
      <td>.</td>
      <td>.</td>
    </tr>
    <tr>
      <td><strong>Value</strong></td>
      <td>.</td>
      <td>100</td>
      <td>.</td>
      <td>.</td>
      <td>11</td>
      <td>.</td>
      <td>.</td>
      <td>10</td>
      <td>36</td>
      <td><strong>200</strong></td>
      <td>42</td>
      <td>.</td>
      <td>7</td>
      <td>.</td>
      <td>.</td>
      <td>.</td>
    </tr>
  </tbody>
</table>

<p>When the hash table gets too full, we need to allocate a larger array and move the items over. This is absolutely required when the number of items in the hash table has reached the size of the array, but usually you want to do it when the table is half or three-quarters full. If you don’t resize it early enough, collisions will become more and more common, and lookups and inserts will get slower and slower. If you wait till it’s almost full, you’re essentially back to linear search.</p>

<p>With a good hash function, this kind of hash table requires an average of one operation per lookup, plus the time to hash the key (but often the keys are relatively short string).</p>

<p>And that’s it! There’s a huge amount more you can do here, and this just scratches the surface. I’m not going to go into a scientific analysis of <a href="https://en.wikipedia.org/wiki/Big_O_notation">big O notation</a>, optimal array sizes, different kinds of probing, and so on. Read Donald Knuth’s <a href="https://www-cs-faculty.stanford.edu/~knuth/taocp.html">TAOCP</a> if you want that level of detail!</p>

<h2 id="hash-table-implementation">Hash table implementation</h2>

<p>You can find the code for this implementation in the <a href="https://github.com/benhoyt/ht">benhoyt/ht</a> repo on GitHub, in <a href="https://github.com/benhoyt/ht/blob/master/ht.h">ht.h</a> and <a href="https://github.com/benhoyt/ht/blob/master/ht.c">ht.c</a>. For what it’s worth, all the code is released under a permissive MIT license.</p>

<p>I got some <a href="https://codereview.stackexchange.com/questions/257634/hash-table-implemented-in-c-with-open-addressing/257649">good feedback</a> from Code Review Stack Exchange that helped clean up a few sharp edges, not the least of which was a memory leak due to how I was calling <code>strdup</code> during the <code>ht_expand</code> step (fixed <a href="https://github.com/benhoyt/ht/commit/970ba8ca3ddef5d2aa1d7a36da290f380a87115f">here</a>). I confirmed the leak <a href="https://stackoverflow.com/questions/5134891/how-do-i-use-valgrind-to-find-memory-leaks">using Valgrind</a>, which I should have run earlier. Seth Arnold also gave me some helpful feedback on a draft of this article. Thanks, folks!</p>

<h3 id="api-design">API design</h3>

<p>First let’s consider what API we want: we need a way to create and destroy a hash table, get the value for a given key, set a value for a given key, get the number of items, and iterate over the items. I’m not aiming for a maximum-efficiency API, but one that is fairly simple to implement.</p>

<p>After a couple of iterations, I settled on the following functions and structs (see <a href="https://github.com/benhoyt/ht/blob/master/ht.h">ht.h</a>):</p>

<div><pre><code><span>// Hash table structure: create with ht_create, free with ht_destroy.</span>
<span>typedef</span> <span>struct</span> <span>ht</span> <span>ht</span><span>;</span>

<span>// Create hash table and return pointer to it, or NULL if out of memory.</span>
<span>ht</span><span>*</span> <span>ht_create</span><span>(</span><span>void</span><span>);</span>

<span>// Free memory allocated for hash table, including allocated keys.</span>
<span>void</span> <span>ht_destroy</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>);</span>

<span>// Get item with given key (NUL-terminated) from hash table. Return</span>
<span>// value (which was set with ht_set), or NULL if key not found.</span>
<span>void</span><span>*</span> <span>ht_get</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>);</span>

<span>// Set item with given key (NUL-terminated) to value (which must not</span>
<span>// be NULL). If not already present in table, key is copied to newly</span>
<span>// allocated memory (keys are freed automatically when ht_destroy is</span>
<span>// called). Return address of copied key, or NULL if out of memory.</span>
<span>const</span> <span>char</span><span>*</span> <span>ht_set</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>,</span> <span>void</span><span>*</span> <span>value</span><span>);</span>

<span>// Return number of items in hash table.</span>
<span>size_t</span> <span>ht_length</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>);</span>

<span>// Hash table iterator: create with ht_iterator, iterate with ht_next.</span>
<span>typedef</span> <span>struct</span> <span>{</span>
    <span>const</span> <span>char</span><span>*</span> <span>key</span><span>;</span>  <span>// current key</span>
    <span>void</span><span>*</span> <span>value</span><span>;</span>      <span>// current value</span>

    <span>// Don't use these fields directly.</span>
    <span>ht</span><span>*</span> <span>_table</span><span>;</span>       <span>// reference to hash table being iterated</span>
    <span>size_t</span> <span>_index</span><span>;</span>    <span>// current index into ht._entries</span>
<span>}</span> <span>hti</span><span>;</span>

<span>// Return new hash table iterator (for use with ht_next).</span>
<span>hti</span> <span>ht_iterator</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>);</span>

<span>// Move iterator to next item in hash table, update iterator's key</span>
<span>// and value to current item, and return true. If there are no more</span>
<span>// items, return false. Don't call ht_set during iteration.</span>
<span>bool</span> <span>ht_next</span><span>(</span><span>hti</span><span>*</span> <span>it</span><span>);</span>
</code></pre></div>

<p>A few notes about this API design:</p>

<ul>
  <li>For simplicity, we use C-style NUL-terminated strings. I know there are more efficient approaches to string handling, but this fits with C’s standard library.</li>
  <li>The <code>ht_set</code> function allocates and copies the key (if inserting for the first time). Usually you don’t want the caller to have to worry about this, or ensuring the key memory stays around. Note that <code>ht_set</code> returns a pointer to the duplicated key. This is mainly used as an “out of memory” error signal – it returns NULL on failure.</li>
  <li>However, <code>ht_set</code> does not copy the value. It’s up to the caller to ensure that the value pointer is valid for the lifetime of the hash table.</li>
  <li>Values can’t be NULL. This makes the signature of <code>ht_get</code> slightly simpler, as you don’t have to distinguish between a NULL value and one that hasn’t been set at all.</li>
  <li>The <code>ht_length</code> function isn’t strictly necessary, as you can find the length by iterating the table. However, that’s a bit of a pain (and slow), so it’s useful to have <code>ht_length</code>.</li>
  <li>There are various ways I could have done iteration. Using an explicit iterator type with a while loop seems simple and natural in C (see the example below). The value returned from <code>ht_iterator</code> is a value, not a pointer, both for efficiency and so the caller doesn’t have to free anything.</li>
  <li>There’s no <code>ht_remove</code> to remove an item from the hash table. Removal is the one thing that’s trickier with linear probing (due to the “holes” that are left), but I don’t often need to remove items when using hash tables, so I’ve left that <del>out</del> as an exercise for the reader.</li>
</ul>

<h3 id="demo-program">Demo program</h3>

<p>Below is a simple program (<a href="https://github.com/benhoyt/ht/blob/master/samples/demo.c">demo.c</a>) that demonstrates using all the functions of the API. It counts the frequencies of unique, space-separated words from standard input, and prints the results (in an arbitrary order, because the iteration order of our hash table is undefined). It ends by printing the total number of unique words.</p>

<div><pre><code><span>// Example:</span>
<span>// $ echo 'foo bar the bar bar bar the' | ./demo</span>
<span>// foo 1</span>
<span>// bar 4</span>
<span>// the 2</span>
<span>// 3</span>

<span>void</span> <span>exit_nomem</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"out of memory</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>1</span><span>);</span>
<span>}</span>

<span>int</span> <span>main</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>ht</span><span>*</span> <span>counts</span> <span>=</span> <span>ht_create</span><span>();</span>
    <span>if</span> <span>(</span><span>counts</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>exit_nomem</span><span>();</span>
    <span>}</span>

    <span>// Read next word from stdin (at most 100 chars long).</span>
    <span>char</span> <span>word</span><span>[</span><span>101</span><span>];</span>
    <span>while</span> <span>(</span><span>scanf</span><span>(</span><span>"%100s"</span><span>,</span> <span>word</span><span>)</span> <span>!=</span> <span>EOF</span><span>)</span> <span>{</span>
        <span>// Look up word.</span>
        <span>void</span><span>*</span> <span>value</span> <span>=</span> <span>ht_get</span><span>(</span><span>counts</span><span>,</span> <span>word</span><span>);</span>
        <span>if</span> <span>(</span><span>value</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>// Already exists, increment int that value points to.</span>
            <span>int</span><span>*</span> <span>pcount</span> <span>=</span> <span>(</span><span>int</span><span>*</span><span>)</span><span>value</span><span>;</span>
            <span>(</span><span>*</span><span>pcount</span><span>)</span><span>++</span><span>;</span>
            <span>continue</span><span>;</span>
        <span>}</span>

        <span>// Word not found, allocate space for new int and set to 1.</span>
        <span>int</span><span>*</span> <span>pcount</span> <span>=</span> <span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>int</span><span>));</span>
        <span>if</span> <span>(</span><span>pcount</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>exit_nomem</span><span>();</span>
        <span>}</span>
        <span>*</span><span>pcount</span> <span>=</span> <span>1</span><span>;</span>
        <span>if</span> <span>(</span><span>ht_set</span><span>(</span><span>counts</span><span>,</span> <span>word</span><span>,</span> <span>pcount</span><span>)</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>exit_nomem</span><span>();</span>
        <span>}</span>
    <span>}</span>

    <span>// Print out words and frequencies, freeing values as we go.</span>
    <span>hti</span> <span>it</span> <span>=</span> <span>ht_iterator</span><span>(</span><span>counts</span><span>);</span>
    <span>while</span> <span>(</span><span>ht_next</span><span>(</span><span>&amp;</span><span>it</span><span>))</span> <span>{</span>
        <span>printf</span><span>(</span><span>"%s %d</span><span>\n</span><span>"</span><span>,</span> <span>it</span><span>.</span><span>key</span><span>,</span> <span>*</span><span>(</span><span>int</span><span>*</span><span>)</span><span>it</span><span>.</span><span>value</span><span>);</span>
        <span>free</span><span>(</span><span>it</span><span>.</span><span>value</span><span>);</span>
    <span>}</span>

    <span>// Show the number of unique words.</span>
    <span>printf</span><span>(</span><span>"%d</span><span>\n</span><span>"</span><span>,</span> <span>(</span><span>int</span><span>)</span><span>ht_length</span><span>(</span><span>counts</span><span>));</span>

    <span>ht_destroy</span><span>(</span><span>counts</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Now let’s turn to the hash table implementation (<a href="https://github.com/benhoyt/ht/blob/master/ht.c">ht.c</a>).</p>

<h3 id="create-and-destroy">Create and destroy</h3>

<p>Allocating a new hash table is fairly straight-forward. We start with an initial array capacity of 16 (stored in <code>capacity</code>), meaning it can hold up to 8 items before expanding.   There are two allocations, one for the hash table struct itself, and one for the entries array. Note that we use <code>calloc</code> for the entries array, to ensure all the keys are NULL to start with, meaning all slots are empty.</p>

<p>The <code>ht_destroy</code> function frees this memory, but also frees memory from the duplicated keys that were allocated along the way (more on that below).</p>

<div><pre><code><span>// Hash table entry (slot may be filled or empty).</span>
<span>typedef</span> <span>struct</span> <span>{</span>
    <span>const</span> <span>char</span><span>*</span> <span>key</span><span>;</span>  <span>// key is NULL if this slot is empty</span>
    <span>void</span><span>*</span> <span>value</span><span>;</span>
<span>}</span> <span>ht_entry</span><span>;</span>

<span>// Hash table structure: create with ht_create, free with ht_destroy.</span>
<span>struct</span> <span>ht</span> <span>{</span>
    <span>ht_entry</span><span>*</span> <span>entries</span><span>;</span>  <span>// hash slots</span>
    <span>size_t</span> <span>capacity</span><span>;</span>    <span>// size of _entries array</span>
    <span>size_t</span> <span>length</span><span>;</span>      <span>// number of items in hash table</span>
<span>};</span>

<span>#define INITIAL_CAPACITY 16  // must not be zero
</span>
<span>ht</span><span>*</span> <span>ht_create</span><span>(</span><span>void</span><span>)</span> <span>{</span>
    <span>// Allocate space for hash table struct.</span>
    <span>ht</span><span>*</span> <span>table</span> <span>=</span> <span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>ht</span><span>));</span>
    <span>if</span> <span>(</span><span>table</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>table</span><span>-&gt;</span><span>length</span> <span>=</span> <span>0</span><span>;</span>
    <span>table</span><span>-&gt;</span><span>capacity</span> <span>=</span> <span>INITIAL_CAPACITY</span><span>;</span>

    <span>// Allocate (zero'd) space for entry buckets.</span>
    <span>table</span><span>-&gt;</span><span>entries</span> <span>=</span> <span>calloc</span><span>(</span><span>table</span><span>-&gt;</span><span>capacity</span><span>,</span> <span>sizeof</span><span>(</span><span>ht_entry</span><span>));</span>
    <span>if</span> <span>(</span><span>table</span><span>-&gt;</span><span>entries</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>free</span><span>(</span><span>table</span><span>);</span> <span>// error, free table before we return!</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>table</span><span>;</span>
<span>}</span>

<span>void</span> <span>ht_destroy</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>)</span> <span>{</span>
    <span>// First free allocated keys.</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>free</span><span>((</span><span>void</span><span>*</span><span>)</span><span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>i</span><span>].</span><span>key</span><span>);</span>
    <span>}</span>

    <span>// Then free entries array and table itself.</span>
    <span>free</span><span>(</span><span>table</span><span>-&gt;</span><span>entries</span><span>);</span>
    <span>free</span><span>(</span><span>table</span><span>);</span>
<span>}</span>
</code></pre></div>

<h3 id="hash-function">Hash function</h3>

<p>Next we define our hash function, which is a straight-forward C implementation of the <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function#FNV-1a_hash">FNV-1a hash algorithm</a>. Note that FNV is not a randomized or cryptographic hash function, so it’s possible for an attacker to create keys with a lot of collisions and cause lookups to slow way down – Python <a href="https://www.python.org/dev/peps/pep-0456/">switched away</a> from FNV for this reason. For our use case, however, FNV is simple and fast.</p>

<p>As far as the algorithm goes, FNV-1a simply starts the hash with an “offset” constant, and for each byte in the string, XORs the hash with the byte, and then multiplies it by a big prime number. The offset and prime are carefully chosen by people with PhDs.</p>

<p>We’re using the 64-bit variant, because, well, most computers are 64-bit these days and it seemed like a good idea. You can tell I don’t have one of those PhDs. :-) Seriously, though, it seemed better than using the 32-bit version in case we have a very large hash table.</p>

<div><pre><code><span>#define FNV_OFFSET 14695981039346656037UL
#define FNV_PRIME 1099511628211UL
</span>
<span>// Return 64-bit FNV-1a hash for key (NUL-terminated). See description:</span>
<span>// https://en.wikipedia.org/wiki/Fowler–Noll–Vo_hash_function</span>
<span>static</span> <span>uint64_t</span> <span>hash_key</span><span>(</span><span>const</span> <span>char</span><span>*</span> <span>key</span><span>)</span> <span>{</span>
    <span>uint64_t</span> <span>hash</span> <span>=</span> <span>FNV_OFFSET</span><span>;</span>
    <span>for</span> <span>(</span><span>const</span> <span>char</span><span>*</span> <span>p</span> <span>=</span> <span>key</span><span>;</span> <span>*</span><span>p</span><span>;</span> <span>p</span><span>++</span><span>)</span> <span>{</span>
        <span>hash</span> <span>^=</span> <span>(</span><span>uint64_t</span><span>)(</span><span>unsigned</span> <span>char</span><span>)(</span><span>*</span><span>p</span><span>);</span>
        <span>hash</span> <span>*=</span> <span>FNV_PRIME</span><span>;</span>
    <span>}</span>
    <span>return</span> <span>hash</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>I won’t be doing a detailed analysis here, but I have included a little <a href="https://github.com/benhoyt/ht/blob/master/samples/stats.c">statistics program</a> that prints the average probe length of the hash table created from the unique words in the input. The FNV-1a hash algorithm we’re using seems to work well on the list of half a million English words (average probe length 1.40), and also works well with a list of half a million very similar keys like <code>word1</code>, <code>word2</code>, and so on (average probe length 1.38).</p>

<p>Interestingly, when I tried the FNV-1 algorithm (like FNV-1a but with the multiply done before the XOR), the English words still gave an average probe length of 1.43, but the similar keys performed very badly – an average probe length of 5.02. So FNV-1a was a clear winner in my quick tests.</p>

<h3 id="get">Get</h3>

<p>Next let’s look at the <code>ht_get</code> function. First it calculates the hash, modulo the <code>capacity</code> (the size of the entries array), which is done by ANDing with <code>capacity - 1</code>. Using AND is only possible because, as we’ll see below, we’re ensuring our array size is always a power of two, for simplicity.</p>

<p>Then we loop till we find an empty slot, in which case we didn’t find the key. For each non-empty slot, we use <code>strcmp</code> to check whether the key at this slot is the one we’re looking for (it’ll be the first one unless there had been a collision). If not, we move along one slot.</p>

<div><pre><code><span>void</span><span>*</span> <span>ht_get</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>)</span> <span>{</span>
    <span>// AND hash with capacity-1 to ensure it's within entries array.</span>
    <span>uint64_t</span> <span>hash</span> <span>=</span> <span>hash_key</span><span>(</span><span>key</span><span>);</span>
    <span>size_t</span> <span>index</span> <span>=</span> <span>(</span><span>size_t</span><span>)(</span><span>hash</span> <span>&amp;</span> <span>(</span><span>uint64_t</span><span>)(</span><span>table</span><span>-&gt;</span><span>capacity</span> <span>-</span> <span>1</span><span>));</span>

    <span>// Loop till we find an empty entry.</span>
    <span>while</span> <span>(</span><span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>key</span><span>,</span> <span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>// Found key, return value.</span>
            <span>return</span> <span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>index</span><span>].</span><span>value</span><span>;</span>
        <span>}</span>
        <span>// Key wasn't in this slot, move to next (linear probing).</span>
        <span>index</span><span>++</span><span>;</span>
        <span>if</span> <span>(</span><span>index</span> <span>&gt;=</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>)</span> <span>{</span>
            <span>// At end of entries array, wrap around.</span>
            <span>index</span> <span>=</span> <span>0</span><span>;</span>
        <span>}</span>
    <span>}</span>
    <span>return</span> <span>NULL</span><span>;</span>
<span>}</span>
</code></pre></div>

<h3 id="set">Set</h3>

<p>The <code>ht_set</code> function is slightly more complicated, because it has to expand the table if there are too many elements. In our implementation, we double the capacity whenever it gets to be half full. This is a little wasteful of memory, but it keeps things very simple.</p>

<p>First, the <code>ht_set</code> function. It simply expands the table if necessary, and then inserts the item:</p>

<div><pre><code><span>const</span> <span>char</span><span>*</span> <span>ht_set</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>,</span> <span>const</span> <span>char</span><span>*</span> <span>key</span><span>,</span> <span>void</span><span>*</span> <span>value</span><span>)</span> <span>{</span>
    <span>assert</span><span>(</span><span>value</span> <span>!=</span> <span>NULL</span><span>);</span>
    <span>if</span> <span>(</span><span>value</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>NULL</span><span>;</span>
    <span>}</span>

    <span>// If length will exceed half of current capacity, expand it.</span>
    <span>if</span> <span>(</span><span>table</span><span>-&gt;</span><span>length</span> <span>&gt;=</span> <span>table</span><span>-&gt;</span><span>capacity</span> <span>/</span> <span>2</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>!</span><span>ht_expand</span><span>(</span><span>table</span><span>))</span> <span>{</span>
            <span>return</span> <span>NULL</span><span>;</span>
        <span>}</span>
    <span>}</span>

    <span>// Set entry and update length.</span>
    <span>return</span> <span>ht_set_entry</span><span>(</span><span>table</span><span>-&gt;</span><span>entries</span><span>,</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>,</span> <span>key</span><span>,</span> <span>value</span><span>,</span>
                        <span>&amp;</span><span>table</span><span>-&gt;</span><span>length</span><span>);</span>
<span>}</span>
</code></pre></div>

<p>The guts of the operation is in the <code>ht_set_entry</code> helper function (note how the loop is very similar to the one in <code>ht_get</code>). If the <code>plength</code> argument is non-NULL, it’s being called from <code>ht_set</code>, so we allocate and copy the key and update the length:</p>

<div><pre><code><span>// Internal function to set an entry (without expanding table).</span>
<span>static</span> <span>const</span> <span>char</span><span>*</span> <span>ht_set_entry</span><span>(</span><span>ht_entry</span><span>*</span> <span>entries</span><span>,</span> <span>size_t</span> <span>capacity</span><span>,</span>
        <span>const</span> <span>char</span><span>*</span> <span>key</span><span>,</span> <span>void</span><span>*</span> <span>value</span><span>,</span> <span>size_t</span><span>*</span> <span>plength</span><span>)</span> <span>{</span>
    <span>// AND hash with capacity-1 to ensure it's within entries array.</span>
    <span>uint64_t</span> <span>hash</span> <span>=</span> <span>hash_key</span><span>(</span><span>key</span><span>);</span>
    <span>size_t</span> <span>index</span> <span>=</span> <span>(</span><span>size_t</span><span>)(</span><span>hash</span> <span>&amp;</span> <span>(</span><span>uint64_t</span><span>)(</span><span>capacity</span> <span>-</span> <span>1</span><span>));</span>

    <span>// Loop till we find an empty entry.</span>
    <span>while</span> <span>(</span><span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>if</span> <span>(</span><span>strcmp</span><span>(</span><span>key</span><span>,</span> <span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span><span>)</span> <span>==</span> <span>0</span><span>)</span> <span>{</span>
            <span>// Found key (it already exists), update value.</span>
            <span>entries</span><span>[</span><span>index</span><span>].</span><span>value</span> <span>=</span> <span>value</span><span>;</span>
            <span>return</span> <span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span><span>;</span>
        <span>}</span>
        <span>// Key wasn't in this slot, move to next (linear probing).</span>
        <span>index</span><span>++</span><span>;</span>
        <span>if</span> <span>(</span><span>index</span> <span>&gt;=</span> <span>capacity</span><span>)</span> <span>{</span>
            <span>// At end of entries array, wrap around.</span>
            <span>index</span> <span>=</span> <span>0</span><span>;</span>
        <span>}</span>
    <span>}</span>

    <span>// Didn't find key, allocate+copy if needed, then insert it.</span>
    <span>if</span> <span>(</span><span>plength</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>key</span> <span>=</span> <span>strdup</span><span>(</span><span>key</span><span>);</span>
        <span>if</span> <span>(</span><span>key</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>return</span> <span>NULL</span><span>;</span>
        <span>}</span>
        <span>(</span><span>*</span><span>plength</span><span>)</span><span>++</span><span>;</span>
    <span>}</span>
    <span>entries</span><span>[</span><span>index</span><span>].</span><span>key</span> <span>=</span> <span>(</span><span>char</span><span>*</span><span>)</span><span>key</span><span>;</span>
    <span>entries</span><span>[</span><span>index</span><span>].</span><span>value</span> <span>=</span> <span>value</span><span>;</span>
    <span>return</span> <span>key</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>What about the <code>ht_expand</code> helper function? It allocates a new entries array of double the current capacity, and uses <code>ht_set_entry</code> with <code>plength</code> NULL to copy the entries over. Even though the hash value is the same, the indexes will be different because the capacity has changed (and the index is hash modulo capacity).</p>

<div><pre><code><span>// Expand hash table to twice its current size. Return true on success,</span>
<span>// false if out of memory.</span>
<span>static</span> <span>bool</span> <span>ht_expand</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>)</span> <span>{</span>
    <span>// Allocate new entries array.</span>
    <span>size_t</span> <span>new_capacity</span> <span>=</span> <span>table</span><span>-&gt;</span><span>capacity</span> <span>*</span> <span>2</span><span>;</span>
    <span>if</span> <span>(</span><span>new_capacity</span> <span>&lt;</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>)</span> <span>{</span>
        <span>return</span> <span>false</span><span>;</span>  <span>// overflow (capacity would be too big)</span>
    <span>}</span>
    <span>ht_entry</span><span>*</span> <span>new_entries</span> <span>=</span> <span>calloc</span><span>(</span><span>new_capacity</span><span>,</span> <span>sizeof</span><span>(</span><span>ht_entry</span><span>));</span>
    <span>if</span> <span>(</span><span>new_entries</span> <span>==</span> <span>NULL</span><span>)</span> <span>{</span>
        <span>return</span> <span>false</span><span>;</span>
    <span>}</span>

    <span>// Iterate entries, move all non-empty ones to new table's entries.</span>
    <span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
        <span>ht_entry</span> <span>entry</span> <span>=</span> <span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>i</span><span>];</span>
        <span>if</span> <span>(</span><span>entry</span><span>.</span><span>key</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>ht_set_entry</span><span>(</span><span>new_entries</span><span>,</span> <span>new_capacity</span><span>,</span> <span>entry</span><span>.</span><span>key</span><span>,</span>
                         <span>entry</span><span>.</span><span>value</span><span>,</span> <span>NULL</span><span>);</span>
        <span>}</span>
    <span>}</span>

    <span>// Free old entries array and update this table's details.</span>
    <span>free</span><span>(</span><span>table</span><span>-&gt;</span><span>entries</span><span>);</span>
    <span>table</span><span>-&gt;</span><span>entries</span> <span>=</span> <span>new_entries</span><span>;</span>
    <span>table</span><span>-&gt;</span><span>capacity</span> <span>=</span> <span>new_capacity</span><span>;</span>
    <span>return</span> <span>true</span><span>;</span>
<span>}</span>
</code></pre></div>

<h3 id="length-and-iteration">Length and iteration</h3>

<p>The <code>ht_length</code> function is trivial – we update the number of items in <code>_length</code> as we go, so just return that:</p>

<div><pre><code><span>size_t</span> <span>ht_length</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>)</span> <span>{</span>
    <span>return</span> <span>table</span><span>-&gt;</span><span>length</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>Iteration is the final piece. To create an iterator, a user will call <code>ht_iterator</code>, and to move to the next item, call <code>ht_next</code> in a loop while it returns <code>true</code>. Here’s how they’re defined:</p>

<div><pre><code><span>hti</span> <span>ht_iterator</span><span>(</span><span>ht</span><span>*</span> <span>table</span><span>)</span> <span>{</span>
    <span>hti</span> <span>it</span><span>;</span>
    <span>it</span><span>.</span><span>_table</span> <span>=</span> <span>table</span><span>;</span>
    <span>it</span><span>.</span><span>_index</span> <span>=</span> <span>0</span><span>;</span>
    <span>return</span> <span>it</span><span>;</span>
<span>}</span>

<span>bool</span> <span>ht_next</span><span>(</span><span>hti</span><span>*</span> <span>it</span><span>)</span> <span>{</span>
    <span>// Loop till we've hit end of entries array.</span>
    <span>ht</span><span>*</span> <span>table</span> <span>=</span> <span>it</span><span>-&gt;</span><span>_table</span><span>;</span>
    <span>while</span> <span>(</span><span>it</span><span>-&gt;</span><span>_index</span> <span>&lt;</span> <span>table</span><span>-&gt;</span><span>capacity</span><span>)</span> <span>{</span>
        <span>size_t</span> <span>i</span> <span>=</span> <span>it</span><span>-&gt;</span><span>_index</span><span>;</span>
        <span>it</span><span>-&gt;</span><span>_index</span><span>++</span><span>;</span>
        <span>if</span> <span>(</span><span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>i</span><span>].</span><span>key</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
            <span>// Found next non-empty item, update iterator key and value.</span>
            <span>ht_entry</span> <span>entry</span> <span>=</span> <span>table</span><span>-&gt;</span><span>entries</span><span>[</span><span>i</span><span>];</span>
            <span>it</span><span>-&gt;</span><span>key</span> <span>=</span> <span>entry</span><span>.</span><span>key</span><span>;</span>
            <span>it</span><span>-&gt;</span><span>value</span> <span>=</span> <span>entry</span><span>.</span><span>value</span><span>;</span>
            <span>return</span> <span>true</span><span>;</span>
        <span>}</span>
    <span>}</span>
    <span>return</span> <span>false</span><span>;</span>
<span>}</span>
</code></pre></div>

<h2 id="discussion">Discussion</h2>

<p>That’s it – the implementation in <a href="https://github.com/benhoyt/ht/blob/master/ht.c">ht.c</a> is only about 200 lines of code, including blank lines and comments.</p>

<p>Beware: this is a teaching tool and not a library, so I encourage you to play with it and let me know about any bugs I haven’t found! I would advise against using it without a bunch of further testing, checking edge cases, etc. Remember, this is unsafe C we’re dealing with. Even while writing this I realized I’d used <code>malloc</code> instead of <code>calloc</code> to allocate the entries array, which meant the keys may not have been initialized to NULL.</p>

<p>As I mentioned, I wanted to keep the implementation simple, and wasn’t too worried about performance. However, a quick, non-scientific <a href="https://github.com/benhoyt/ht/blob/master/samples/perftest.sh">performance comparison</a> with Go’s <code>map</code> implementation shows that it compares pretty well – with half a million English words, this C version is about 50% slower for <a href="https://github.com/benhoyt/ht/blob/master/samples/perfget.c">lookups</a> and 40% faster for <a href="https://github.com/benhoyt/ht/blob/master/samples/perfset.c">insertion</a>.</p>

<p>Speaking of Go, it’s even easier to write custom hash tables in a language like Go, because you don’t have to worry about handling memory allocation errors or freeing allocated memory. I recently wrote a <a href="https://github.com/benhoyt/counter">counter</a> package in Go which implements a similar kind of hash table.</p>

<p>There’s obviously a lot more you could do with the C version. You could focus on safety and reliability by doing various kinds of testing. You could focus on performance, and reduce memory allocations, use a <a href="https://os.phil-opp.com/allocator-designs/#bump-allocator">“bump allocator”</a> for the duplicated keys, store short keys inside each item struct, and so on. You could improve the memory usage, and tune <code>_ht_expand</code> to not double in size every time. Or you could add features such as item removal.</p>

<p>After I’d finished writing this, I remembered that Bob Nystrom’s excellent <a href="https://craftinginterpreters.com/"><em>Crafting Interpreters</em></a> book has a <a href="https://craftinginterpreters.com/hash-tables.html">chapter on hash tables</a>. He makes some similar design choices, though his chapter is significantly more in-depth than this article. If I’d remembered his chapter before I started, I probably wouldn’t have written this one!</p>

<p>In any case, I hope you’ve found this useful or interesting. If you spot any bugs or have any feedback, please let me know. You can also go to the discussions on <a href="https://news.ycombinator.com/item?id=26590234">Hacker News</a>, <a href="https://www.reddit.com/r/programming/comments/mdkzli/how_to_implement_a_hash_table_in_c/">programming Reddit</a>, and <a href="https://lobste.rs/s/6v0vxq/how_implement_hash_table_c">Lobsters</a>.</p>

<p>I’d love it if you <a href="https://github.com/sponsors/benhoyt/">sponsored me on GitHub</a> – it will motivate me to work on my open source projects and write more good content. Thanks!</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Radio Garden (237 pts)]]></title>
            <link>https://radio.garden/</link>
            <guid>40887359</guid>
            <pubDate>Sat, 06 Jul 2024 00:47:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://radio.garden/">https://radio.garden/</a>, See on <a href="https://news.ycombinator.com/item?id=40887359">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Copy and Paste context menu entries are sometimes disabled when should not be (135 pts)]]></title>
            <link>https://bugzilla.mozilla.org/show_bug.cgi?id=1863246</link>
            <guid>40886954</guid>
            <pubDate>Fri, 05 Jul 2024 23:22:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246">https://bugzilla.mozilla.org/show_bug.cgi?id=1863246</a>, See on <a href="https://news.ycombinator.com/item?id=40886954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wrapper">

 


<main id="bugzilla-body" tabindex="-1">



<div id="main-inner">










<div id="summary-container">



  
    <p><span id="field-value-status_summary">
      <span data-status="open">Open</span>
      <span id="field-value-bug_id">
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246">Bug 1863246</a>
      </span>
      <span>
        <span>Opened <span title="2023-11-05 19:55 PST" data-time="1699242937">8 months ago</span></span>
          <span>Updated <span title="2024-07-05 18:14 PDT" data-time="1720228479">1 hour ago</span></span>
      </span>
        </span>
    </p>

  
</div>








































<meta name="firefox-versions" content="{&quot;FIREFOX_AURORA&quot;:&quot;&quot;,&quot;FIREFOX_DEVEDITION&quot;:&quot;128.0b9&quot;,&quot;FIREFOX_ESR&quot;:&quot;115.12.0esr&quot;,&quot;FIREFOX_ESR_NEXT&quot;:&quot;&quot;,&quot;FIREFOX_NIGHTLY&quot;:&quot;129.0a1&quot;,&quot;LAST_MERGE_DATE&quot;:&quot;2024-06-10&quot;,&quot;LAST_RELEASE_DATE&quot;:&quot;2024-06-11&quot;,&quot;LAST_SOFTFREEZE_DATE&quot;:&quot;2024-06-06&quot;,&quot;LAST_STRINGFREEZE_DATE&quot;:&quot;2024-06-07&quot;,&quot;LATEST_FIREFOX_DEVEL_VERSION&quot;:&quot;128.0b9&quot;,&quot;LATEST_FIREFOX_OLDER_VERSION&quot;:&quot;3.6.28&quot;,&quot;LATEST_FIREFOX_RELEASED_DEVEL_VERSION&quot;:&quot;128.0b9&quot;,&quot;LATEST_FIREFOX_VERSION&quot;:&quot;127.0.2&quot;,&quot;NEXT_MERGE_DATE&quot;:&quot;2024-07-08&quot;,&quot;NEXT_RELEASE_DATE&quot;:&quot;2024-07-09&quot;,&quot;NEXT_SOFTFREEZE_DATE&quot;:&quot;2024-07-04&quot;,&quot;NEXT_STRINGFREEZE_DATE&quot;:&quot;2024-07-05&quot;}">



<div id="a871_293623"><p>Summary: Copy and Past Bug → Copy and Past context menu are disabled in some case</p></div><div id="c1" data-comment-id="16653185" data-ismarkdown="true"><p>Set release status flags based on info from the regressing <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1732358" title="RESOLVED FIXED - Enable Fission pref by default in Firefox 97">bug 1732358</a></p>
<p>:nika, since you are the author of the regressor, <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1732358" title="RESOLVED FIXED - Enable Fission pref by default in Firefox 97">bug 1732358</a>, could you take a look? Also, could you set the severity field?</p>
<p>For more information, please visit <a href="https://wiki.mozilla.org/BugBot#needinfo_regression_author.py" rel="nofollow">BugBot documentation</a>.</p>
</div><div id="a34387_159069"><p>Summary: Copy and Past context menu are disabled in some case → Copy and Paste context menu are disabled in some case</p></div><div id="c2"><div id="ct-2" data-comment-id="16655984" data-ismarkdown="true"><p>The regressing bug here is enabling Fission, meaning that this is likely a behaviour change in our clipboard enable/disable logic caused by process switches. Redirecting to :masayuki for some insight from the editor side as to what might be going on?</p>
<p>There's also a chance this is an issue with APZ and input event targeting, given that this is intended to be a context menu on an input field.</p>
</div><div><p>Flags: needinfo?(nika) → needinfo?(masayuki)</p></div></div><div id="c3"><p>Component: DOM: UI Events &amp; Focus Handling → DOM: Navigation</p></div><div id="c4"><p>Flags: <span>needinfo?(masayuki)</span></p></div><div id="c5"><p>Masayuki, could you assign a severity?</p><div><p>Flags: needinfo?(masayuki)</p></div></div><div id="c7" data-comment-id="16659421" data-ismarkdown="true"><p>(In reply to Masayuki Nakano [:masayuki] (he/him)(JST, +0900) from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c4" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #4</a>)</p>
<blockquote>
<p>So I guess that the command state manager asked wrong document, but looks like the <code>&lt;input&gt;</code> of Google search is not in a sub-document. So I have no idea where is wrong.</p>
</blockquote>
<p>Did you see <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c6" title="RESOLVED DUPLICATE - Copy disabled despite selection existing in the content process">bug 1860426 comment 6</a>? And does that help at all?</p>
<p>(In reply to Neil Deakin from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c6" title="RESOLVED DUPLICATE - Copy disabled despite selection existing in the content process">bug 1860426 comment #6</a>)</p>
<blockquote>
<p>I can reproduce this fairly reliably. It looks like nsFocusManager::GetActiveBrowsingContextInChrome is returning null for some reason.</p>
</blockquote>
</div><div id="c8"><p>I just commented from the editor side (as requested). I'm not familiar with the focus management between content processes.</p><div><p>Flags: <span>needinfo?(masayuki)</span></p></div></div><div id="c9"><p>Henri, you did work on the Fission focus handling. Do you have an idea of what's going on?</p><div><p>Flags: needinfo?(hsivonen)</p></div></div><div id="c12"><div id="ct-12" data-comment-id="16661367" data-ismarkdown="true"><p>(In reply to Andreas Farre [:farre] from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c9" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #9</a>)</p>
<blockquote>
<p>Do you have an idea of what's going on?</p>
</blockquote>
<p>Not beyond:</p>
<blockquote>
<blockquote>
<p>I can reproduce this fairly reliably. It looks like nsFocusManager::GetActiveBrowsingContextInChrome is returning null for some reason.</p>
</blockquote>
</blockquote>
<p>That is, I don't have an idea of why active browsing context tracking in the parent process gets out of sync.</p>
</div><div><p>Flags: <span>needinfo?(hsivonen)</span></p></div></div><div id="c13"><p>Hmm. Considering what the regressor is, perhaps we fail to restore the activeness status properly when a page comes out of the bf cache?</p></div><div id="c14"><p>I noticed that an easy workaround when this happens is to click the address bar to change selection focus, then interact with the page again.  After that, the "Copy" entry in the context menu works again without reopening the tab.  Maybe this can give a hint about what might be causing it.</p></div><div id="c15" data-comment-id="16665906" data-ismarkdown="true"><p>(In reply to lexlexlex from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c14" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #14</a>)</p>
<blockquote>
<p>I noticed that an easy workaround when this happens is to click the address bar to change selection focus, then interact with the page again.  After that, the "Copy" entry in the context menu works again without reopening the tab.  Maybe this can give a hint about what might be causing it.</p>
</blockquote>
<p>I can confirm that this workaround is valid for me in Win11 latest.</p>
</div><div id="c16"><p>To clarify, I am running Manjaro XFCE with X11, so along with Mark's confirmation above, it sounds like this workaround is confirmed for both Linux and Windows.  Therefore, I expect that we can conclude that this issue and its specific manifested behavior is not platform-specific.</p></div><div id="c18"><p>Set release status flags based on info from the regressing <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1720990" title="RESOLVED FIXED - The &quot;Copy&quot; menu item gets broken (not-clickable) after unsuccessful navigation (bfcacheInParent)">bug 1720990</a></p></div><div id="c19"><p>In a rather unexpected turn of events, I discovered that the bug on my Mac was resolved perfectly once I disabled uBlock Origin. I suspect it has something to do with anti-tracking features, or something along those lines. As someone who's not particularly tech-savvy, that's just my guess.</p></div><div id="a1945569_24295"><p>Assignee: nobody → peterv</p><p>Status: NEW → ASSIGNED</p><p>Flags: <span>needinfo?(peterv)</span></p></div><div id="c21"><p>I still haven't been able to reproduce this. I tried the STR from this bug, the duplicates and <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1761430" title="NEW - Edit context menu items are incorrectly disabled/broken after searching with searchfox.org, or www.basschouten.com">bug 1761430</a>.</p></div><div id="c23" data-comment-id="16703772" data-ismarkdown="true"><p>A few days ago that I reproduced it, I noticed that, the text seemed to be successfully copied to the clipboard, as it was listed in both klipper (KDE) and fcitx5 (IME; its clipboard manager). I just wasn't able to paste it anywhere, Firefox or other software, through, either the context menu or the shortcut.</p>
<p>I have two clipboard-related prefs in non-default states: <code>dom.event.clipboardevents.enabled:false</code> and <code>clipboard.autocopy:false</code>.</p>
</div><div id="c24" data-comment-id="16706825" data-ismarkdown="true"><p>@tgn-ff is describing some other bug, because their described behavior is not this bug's observed behavior.  This bug is simply the context menu entries being disabled, with a workaround of focusing the address bar.</p>
<p>I have no clipboard-related prefs in non-default states and this bug manifests for me once or twice per day.  I use multiple Firefox windows, each with many tabs.</p>
</div><div id="c25" data-comment-id="16706870" data-ismarkdown="true"><p>(In reply to arsdn from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c19" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #19</a>)</p>
<blockquote>
<p>In a rather unexpected turn of events, I discovered that the bug on my Mac was resolved perfectly once I disabled uBlock Origin. I suspect it has something to do with anti-tracking features, or something along those lines. As someone who's not particularly tech-savvy, that's just my guess.</p>
</blockquote>
<p>After several days of browsing, I noticed that this bug still occurred once or twice a day, even after turning off all extensions, including uBlock Origin. So, I have now proven my initial guess wrong. Also, my browser version is 120.0.1 (64-bit).</p>
</div><div id="c26"><p>I am finally able to reproduce this issue, but still not reliably, and it's happening probably once out of tens of attempts.<br>
I followed STRs in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c2">https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c2</a>  ; I seemed to have to select texts from a bookmark page quickly enough, right after I navigated from homepage to the bookmark page, and before the bookmark page was completely loaded.</p></div><div id="c27"><p>I believe this is not caused by the bookmark system, since I see it often and almost never use bookmarks.  I never have the bookmarks bar enabled, either.  I may use a bookmark once per 2 months, yet this bug manifests multiple times per week.</p></div><div id="c28"><p>I cleared browsing &amp; download, cache and form &amp; search histories then opened and closed Firefox 10 times. Each time I went to the bookmarks menu. I used three different websites for the test, giving the website sufficient time to load. Then selected non-linked text. On the context menu copy was grayed out 4 times out of those ten times. I opened a new tab then went back to the first tab and copy was no longer grayed out.<br>
I tested with all extensions, theme, custom settings and userchrome.css enabled.</p></div><div id="c29" data-comment-id="16713710" data-ismarkdown="true"><p>Debugging shows that the focus manager for the process for the page being unloaded is receiving WindowHidden() which clears the mActiveBrowsingContextInChrome field. The focus manager for the process for the page being loaded is receiving WindowRaised() which sets the mActiveBrowsingContextInChrome field.</p>
<p>On success, the two processes perform those steps in that order. On failure, they perform those steps in the reverse order. There's a bunch of code in ProcessPendingActiveBrowsingContextActionId that is supposed to handle these calls being out of order, but I can't figure out what it is trying to do, but it seems to fail in this case.</p>
<p>I can reproduce with the steps in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c2">https://bugzilla.mozilla.org/show_bug.cgi?id=1860426#c2</a> about 20-30% of the time, but I don't need to exit each time; it is sufficient at step 2 to just open a new window.</p>
</div><div id="c37"><p>I am original reporter.<br>
This bug is reproduced with new profile(i.e. no addons installed).</p></div><div id="c38"><p>Thanks for the clarification. No argument here.</p></div><div id="c42" data-comment-id="16742187" data-ismarkdown="true"><p>I'm able to reproduce this bug without any extensions installed on a brand new profile (Firefox 121 on Linux 6.6.10).</p>
<p>The only workaround is to click into the address bar, swap tabs or just play around with random UI and then right click the selected text again.</p>
<p>This is a pretty annoying bug that affects a core functionality of the UX, any updates on a patch or fix?</p>
</div><div id="c43"><p>Any progress will be posted here.</p></div><div id="c51"><p>The easiest and simplest workaround of all and surprisingly has not been mentioned and which I find works is to highlight text and press Ctrl-C.</p></div><div id="c55" data-comment-id="16780870" data-ismarkdown="true"><p>Got the same problem, all add-ons deactivated and the context menu option "copy" is still greyed out. Had the same problem since at least two Firefox versions ago.<br>
As the others have stated, the only way to temporarily fix it is by unfocusing (switching tab, clicking the adress bar, clicking on Firefox UI elements / toolbar).</p>
<p>Firefox version: 122.0 (64-bit)<br>
OS: Linux Mint 21.3 Cinnamon ver. 6.0.4</p>
</div><div id="c57" data-comment-id="16794051" data-ismarkdown="true"><p>(In reply to me from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c56" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #56</a>)</p>
<blockquote>
<p>If it helps fellow sufferers or those working on this bug the Copy/Paste Plaintext add-on does not suffer from this issue</p>
<p><a href="https://addons.mozilla.org/en-GB/firefox/addon/copy-plaintext/" rel="nofollow">https://addons.mozilla.org/en-GB/firefox/addon/copy-plaintext/</a></p>
</blockquote>
<p>Ich kann jetzt nicht genau sagen ab welcher Version es anfing aber ab Ende 2023 war es nach ein Update von Firefox auf einmal da leider sehr sporadisch aber es nervt echt schon.</p>
<p>Für mich war es zum Schluss nicht mehr brauchbar („11.2023“) weil wenn es drauf ankommt was zu Kopieren der Menüpunkt grau hinterlegt ist, bin zur ESR Version ausgewichen was einbandfrei läuft.</p>
<p>Jetzt nach ihren Beitrag mit dem Copy/Paste Plaintext add-on habe ich es noch mal versucht in der 122.0.1 (64-Bit) und ja („Copy Plain Text == add-on“) geht auch wenn Kopieren in Menüpunkt grau hinterlegt ist, das ist zwar nicht die Lösung aber man kann wieder mit der Version von Firefox arbeiten.</p>
<p>Ich hoffe sehr dass man den Fehler ("Bugs") noch findet.</p>
<p>System:<br>
Firefox Version: 122.0.1 (64-bit) Menüleiste und Lesezeichen-Symbolleiste<br>
OS: Windows 10 (64-bit)</p>
</div><div id="c62" data-comment-id="16808046" data-ismarkdown="true"><p>Should the title of this ticket be changed to demonstrate that this ticket is the officially-acknowledged ticket for this issue?  The current title's grammar makes it look unofficial or not acknowledged, so potential bug reporters searching for this issue may not realize it has been acknowledged, which may be contributing to various duplicate reports.  Here are some suggestions:</p>
<ul>
<li>"Clipboard actions in page context menus are disabled in some cases"</li>
<li>"Clipboard actions in page context menus become disabled under some circumstances"</li>
<li>"Copy and paste context menu entries are sometimes disabled"</li>
</ul>
</div><div id="a9466406_159069"><p>Summary: Copy and Paste context menu are disabled in some case → Copy and Paste context menu entries are sometimes disabled when they should not be</p></div><div id="c64"><p>Peter, any update on this? I can actually reproduce this with the steps in <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1887229" title="RESOLVED DUPLICATE - Copy option is inactive in Linux version">bug 1887229</a>.</p></div><div id="c65">

  <p>Also here same bug (like from video).</p></div><div id="c71"><p>:sefeng could this be triaged for a priority?<br>
It's an S2 bug</p></div><div id="a16536708_625922"><p>Flags: <span>needinfo?(sefeng)</span></p><p>Priority: -- → P2</p></div><div id="c72" data-comment-id="16936587" data-ismarkdown="true"><p>(In reply to Donal Meehan [:dmeehan] from <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c71" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment #71</a>)</p>
<blockquote>
<p>:sefeng could this be triaged for a priority?<br>
It's an S2 bug</p>
</blockquote>
<p>Yup, it's P2. And we've got a recording of this issue and we've been debugging it.</p>
</div><div id="c73" data-comment-id="16952314" data-ismarkdown="true"><p>Confirmed with the second steps to reproduce, but not always, try these steps:</p>
<ul>
<li>copy to clipboard <code>https://www.bbc.com/future/article/20240521-these-wildlife-corridors-help-grizzly-bears</code> (do not open this site in other tab)</li>
<li>open <a href="https://www.google.com/" rel="nofollow">https://www.google.com/</a> in a tab</li>
<li>at the same tab (google), click the address bar and press Ctrl+V and Enter</li>
<li>click on Go back button (or Alt+Left Arrow)</li>
<li>click on Go forward button (Alt+Right Arrow)</li>
<li>double click any word to select it, e.g. <code>tunnels</code></li>
<li>right click</li>
</ul>
<p>If the "copy" item is available, repeat these steps 5 - 10 times:</p>
<ul>
<li>press Esc key</li>
<li>Alt+Left Arrow</li>
<li>Alt+Right Arrow</li>
<li>double click any word to select it, e.g. <code>tunnels</code></li>
<li>right click</li>
</ul>
<p><strong>Workaround - toggle reader view, press F9 twice, this fixes it for me, always.</strong></p>
<p>The only issue is, that reader view is not available at all sites.</p>
<p><strong>Another workaround - duplicate tab.</strong></p>
<p>Firefox 126.0 (64-bit) (portable), Windows 10 22H2 64-bit.</p>
<p>Tested also in private browsing window, where all extensions are disabled, I can reproduce it too, but not every time either.</p>
<p>Confirmed also in Firefox Nightly, latest version 128.0a1 (2024-05-24) (64-bit), default settings, no extensions!</p>
</div><div id="c75" data-comment-id="16952471" data-ismarkdown="true"><blockquote>
<p>Workaround - toggle reader view, press F9 twice, this fixes it for me, always.</p>
<p>The only issue is, that reader view is not available at all sites.</p>
<p>Another workaround - duplicate tab.</p>
</blockquote>
<p>There's an easier workaround documented in this ticket.  Focusing the address bar by clicking it works around the issue immediately without needing any new tabs, reader view, or anything like that.</p>
</div><div id="c80" data-comment-id="16986989" data-ismarkdown="true"><p>Per <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1863246#c72" title="ASSIGNED - Copy and Paste context menu entries are sometimes disabled when they should not be">comment 72</a>, the team responsible for this bug is working to fix it. More +1 and advocacy comments aren't helping that happen faster but are adding unnecessary noise to this bug. A reminder that there are etiquette expectations when interacting in Bugzilla - it's not a free-for-all discussion forum.<br>
<a href="https://bugzilla.mozilla.org/page.cgi?id=etiquette.html">https://bugzilla.mozilla.org/page.cgi?id=etiquette.html</a></p>
<p>I'm restricting comments to keep further discussion focused where it needs to be on fixing this bug and getting it into a shipping release as soon as can be reasonably done.</p>
</div><div id="c81" data-comment-id="16991911" data-ismarkdown="true"><p>So the issue here is that when a page goes to BFCache, it'd set the active browsing context to null, and the page that is about to show would update the active browsing context to itself. And these two operations are racy because they are triggered in different processes with different actionId. We are going to explore some potential solutions.</p>
<p>I am first going to try to make the page that goes to BFCache to not updating the active browsing context because we know it'll be updated by the page that's about to show. I'll try this and see what breaks.</p>
<p>If the above doesn't work, I'll need to make they use the same actionId, so that the one that sets the active browsing context to non-null will always win. This might requires converting <code>IsInBFCache</code> to be an IPC message rather than synced field, so that the actionId can be passed over from parent to child.</p>
<p>If folks have different ideas, let me know!</p>
</div><div id="a19664823_434964"><p>Assignee: peterv → sefeng</p></div><div id="c82" data-comment-id="17012617" data-ismarkdown="true"><p>Currently, when a page enters BFCache, it updates the parent process<br>
for the active BC; however, the page that is about to show will do the<br>
same. These two operations are triggered in different processes with<br>
different active id, they are racy and problematic.</p>
<p>This patch fixes the above issue by not updating the parent process<br>
when a page enters BFCache.</p>
<p>This only applies to BFCacheInParent is enabled.</p>
</div>







<dialog id="att-overlay" aria-labelledby="att-overlay-title" data-attachment-count="2">
  
</dialog>

</div> 
</main> 
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VPN ban is strangling communication in Myanmar (114 pts)]]></title>
            <link>https://www.irrawaddy.com/in-person/interview/war-on-citizens-how-juntas-vpn-ban-is-strangling-communication-in-myanmar.html</link>
            <guid>40886689</guid>
            <pubDate>Fri, 05 Jul 2024 22:41:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.irrawaddy.com/in-person/interview/war-on-citizens-how-juntas-vpn-ban-is-strangling-communication-in-myanmar.html">https://www.irrawaddy.com/in-person/interview/war-on-citizens-how-juntas-vpn-ban-is-strangling-communication-in-myanmar.html</a>, See on <a href="https://news.ycombinator.com/item?id=40886689">Hacker News</a></p>
Couldn't get https://www.irrawaddy.com/in-person/interview/war-on-citizens-how-juntas-vpn-ban-is-strangling-communication-in-myanmar.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Tao Te Ching translated by Ursula Le Guin (1997) (251 pts)]]></title>
            <link>https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md</link>
            <guid>40886419</guid>
            <pubDate>Fri, 05 Jul 2024 21:57:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md">https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md</a>, See on <a href="https://news.ycombinator.com/item?id=40886419">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to GitHub Copilot&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>GitHub Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Enterprise&quot;,&quot;action&quot;:&quot;click to go to Enterprise platform&quot;,&quot;label&quot;:&quot;ref_cta:Enterprise platform;&quot;}" href="https://github.com/enterprise">
      
      <div>
        <p>Enterprise platform</p><p>
        AI-powered developer platform
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:nrrb/tao-te-ching" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="1mOjI6aIdhU9O7_h_2RRslMukfwBBTOtwhg0Fg_yPfsyt_yy2G3-Byjwlo3zOzcVHXAibYFlPi6iF71_iju5-g" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="nrrb/tao-te-ching" data-current-org="" data-current-owner="nrrb" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=FpEfJGYZGFn52U2qXjrvpv5Io%2FEiq5Rc5H6Ue95IhnwePdmhWbVhUhoBwc1gfVrcFsAyG%2FuZeYjc4yeCT3Z%2FcJgQJLDUciupp9cKFthm3d9uREu%2FmJ4ohcpY2suH%2BWQQBDxaHPkxVgxOcpaMTo8EKBnkuWotan68VA8b7Ta1jZc79CWgOngTVywTsBNlQA6YmVgFow4rRig24kSzpofLR%2FEmUjBdOWy205D5Al1qNp7jr3l2bU8dYUSh4Z4qS42fWg21NUC3dtsNtZmbCTYquREe%2BONylSyR%2BZ8wKH30S791c4NQbY1B5eDKbsKH0477DREaN8pdYx57cCNWtnhFg6vkSWyh6YxrR5MLKXX%2FpSPLexVWUwAW7rupI9U2tYwaQh%2B2%2F3PhIZLZDmM9O6EDODhTj4dUzmb%2FM1mI8y%2B1XZGhZUX6eNBt42luXWLTWFLIX3cM6Auh5bg9hoByMq9n1kTy1LrGc9wuTA3pqTpkFnXfHqA0wylBIrN3nSpJ2nwhvWlhz7Oa8F5tuoC9cKKuXT6dVjsgB5PnQaeujHgsDoGz97dvzjM1S%2FLXdaOivxsbOSlgPuLCHVAT6ysKHJpYj0Im--yLFpclRXDWNlr6UW--iBsu2vdiUquRBrj8Fk%2B9vQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=nrrb%2Ftao-te-ching" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="0286bb5ce9f7db1aa56745ac93058c39ab2f5ccef334184a7b7262204c886cb3" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Research into homeopathy: data falsification, fabrication and manipulation (115 pts)]]></title>
            <link>https://www.skeptic.org.uk/2024/07/research-into-homeopathy-data-falsification-fabrication-and-manipulation/</link>
            <guid>40885397</guid>
            <pubDate>Fri, 05 Jul 2024 19:39:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.skeptic.org.uk/2024/07/research-into-homeopathy-data-falsification-fabrication-and-manipulation/">https://www.skeptic.org.uk/2024/07/research-into-homeopathy-data-falsification-fabrication-and-manipulation/</a>, See on <a href="https://news.ycombinator.com/item?id=40885397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-td-block-uid="tdi_148">
<p>We all know that homeopathy is a placebo therapy: its assumptions fly in the face of science, its remedies are normally devoid of active ingredients, and <a href="https://www.amazon.co.uk/Homeopathy-Undiluted-Including-Comprehensive-Z/dp/3319435906/?" target="_blank" rel="noreferrer noopener">the evidence from clinical trials is uniformly negative</a>. After the UK and France, now even Germany, homeopathy’s home country, agrees with this position. The <a href="https://edzardernst.com/2024/05/wow-the-german-medical-association-bans-homeopathy/" target="_blank" rel="noreferrer noopener">128th ‘German Medical Assembly’ recently declared that</a>:</p>
<p><em>“the use of homeopathy … is not an option that is compatible with rational medicine, the requirement for the best possible treatment and an appropriate understanding of medical responsibility and medical ethics”.</em></p>
<p>But such arguments fail to deter homeopaths. They argue that there are plenty of clinical trials of homeopathy that arrived at positive conclusions. And to be fair, they are not even entirely wrong. There have been <a href="https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/2046-4053-3-142" target="_blank" rel="noreferrer noopener">several studies that did imply that homeopathy works beyond placebo</a>.<sup> &nbsp;</sup></p>
<p>How come? Why do some studies of homeopathy show positive results? The obvious answer is because these studies are not rigorous; they are not randomised, or not double blind, or not placebo-controlled, for instance. But this assumption might also not be entirely true.</p>
<p>In 2020, <a href="https://pubmed.ncbi.nlm.nih.gov/33010094/" target="_blank" rel="noreferrer noopener">Frass et al published a trial that seemed to prove it wrong</a>. This randomised, placebo-controlled, double-blind study showed that the quality of life of cancer patients improved significantly with homeopathy compared to placebo. In addition, survival was significantly longer in the homeopathy group versus placebo and control.</p>
<p>When it was first published, this study was celebrated by homeopaths, while it raised many skeptics’ eyebrows. The trial seemed rigorous, was published in a highly reputed journal, and was conducted by well-known experts. Its lead author, Michael Frass, was a respected professor at the Vienna Medical School (the institution to which I too once belonged).</p>
<p>When I first read his paper, <a href="https://edzardernst.com/2020/10/homeopathy-prolongs-survival-of-lung-cancer-patients-can-it-be-true/" target="_blank" rel="noreferrer noopener">I was nevertheless suspicious</a>, not least because I had previously found that Frass (whom I have never met in person) had published <a href="https://edzardernst.com/2015/11/prof-frass-remarkable-studies-of-homeopathy/" target="_blank" rel="noreferrer noopener">no less than 12 studies of homeopathy all of which arrived at positive conclusions</a>. This had long led me to the conclusion that there must be something wrong with Frass’ research.</p>
<p>I was therefore not surprised that, soon after the publication of Frass’ new trial, an in-depth analysis by Norbert Aust and Viktor Weisshäupl <a href="https://edzardernst.com/2021/06/a-thorough-analysis-of-prof-m-frass-recent-homeopathy-trial-casts-serious-doubts-on-its-reliability/" target="_blank" rel="noreferrer noopener">disclosed several important inconsistencies</a>. They eventually prompted complaints to both the journal, Oncologist, and the Vienna Medical School about suspected scientific misconduct. The Medical School then referred the case to the Austrian Agency for Scientific Integrity. The agency took their time, but recently, more that 3 years after the Frass study was published, they made available the&nbsp;final on-line summary of their assessment; here is <a href="https://oeawi.at/wp-content/uploads/2024/02/Jahresbericht-Kommission-2022.pdf" target="_blank" rel="noreferrer noopener">my translation of part of this document</a>:</p>
<p><em>After establishing sufficient suspicion of various violations of good scientific practice, the Commission declared itself responsible and initiated proceedings. In the course of this, the principal investigator was given the opportunity to submit a written statement and to provide the Commission for Research Integrity Annual Report 2022 material that would help to clarify the facts of the case, which the accused submitted in large quantities.</em></p>
<p><em>In a very complex, comprehensive investigation, which required, among other things, the on-site inspection of original documents, the Commission was able to substantiate the suspicion of data falsification, fabrication and manipulation. In a final statement, the study director, who no longer works for the university in question, and the numerous co-authors were informed in detail about the course and results of the commission’s investigation and informed of the recommendations to the university and journal.&nbsp;</em></p>
<p><em>The Commission recommended that the university concerned should consider investigating its own responsibilities and act accordingly, and that the publication should be withdrawn as a matter of urgency. The journal responsible for the publication was asked to withdraw the publication on the basis of the findings of the investigation.</em></p>
<p>Unfortunately, the scandal does not end here. Despite the Agency’s urgent call to the journal to withdraw the fabricated study, this has still not happened. Merely <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9732219/" target="_blank" rel="noreferrer noopener">an ‘expression of concern’ has been added to the paper on Medline</a>. It has been up for many months and reads as follows:</p>
<p><em>This is an Expression of Concern regarding: Michael Frass, Peter Lechleitner, Christa Gründling, Claudia Pirker, Erwin Grasmuk-Siegl, Julian Domayer, Maximilian Hochmair, Katharina Gaertner, Cornelia Duscheck, Ilse Muchitsch, Christine Marosi, Michael Schumacher, Sabine Zöchbauer-Müller, Raj K. Manchanda, Andrea Schrott, Otto Burghuber, Homeopathic Treatment as an Add-On Therapy May Improve Quality of Life and Prolong Survival in Patients with Non-Small Cell Lung Cancer: A Prospective, Randomized, Placebo-Controlled, Double-Blind, Three-Arm, Multicenter Study,&nbsp;The Oncologist, Volume 25, Issue 12, December 2020, Pages e1930–e1955,&nbsp;<a href="https://doi.org/10.1002/onco.13548" target="_blank" rel="noreferrer noopener">https://doi.org/10.1002/onco.13548</a></em></p>
<p><em>In August 2022, the journal editors received credible information from the Austrian Agency for Research Integrity about potential data falsification and data manipulation in this article. While&nbsp;The Oncologist&nbsp;editorial team investigates and communicates with the corresponding author, the editors are publishing this Expression of Concern to alert readers that, pending the outcome and review of a full investigation, the research results presented may not be reliable.</em></p>
<p>Consequently vulnerable cancer patients might still be misled by the fake findings of Frass and colleagues.</p>
<p>The sorry story of Frass and his research illustrates some of the fundamental problems with research into homeopathy in particular, and alternative medicine in general. Sadly, scientific fraud is not uncommon in medicine.&nbsp;In conventional medicine, financial interests are often the driving force. This situation is very different in the field of alternative medicine, where ideological conflicts dominate.</p>
<p>To put it into a nutshell: researchers in this field tend to initiate studies primarily because they want to prove that their favourite therapy is effective. By not honestly testing their hypotheses, but dishonestly trying to prove them, they abuse research. This enables people like Frass to publish one positive result for homeopathy after another. On my blog, I summarise this growing group of people in the satirically named ‘<a href="https://edzardernst.com/2024/02/richard-c-niemtzow-inventor-of-the-battle-field-acupuncture-enters-the-alternative-medicine-hall-of-fame/" target="_blank" rel="noreferrer noopener">ALTERNATIVE MEDICINE HALL OF FAME</a>‘. It currently includes 24 (pseudo)scientists, 6 of whom specialise in researching homeopathy.</p>
<p>This could all be quite amusing but, of course, it is also very serious. Scientific fraud causes considerable damage. In the case of the Frass study, we even have to ask ourselves how many people’s lives it has shortened. Therefore, we should look for ways to minimise this phenomenon.</p>
<p>This would certainly not be an easy task, and there is no patent remedy for achieving it. In the field of alternative medicine, I have long advocated that researchers like Michael Frass, who produce nothing but implausible results that mislead us all, <a href="https://edzardernst.com/2012/11/the-trustworthiness-index/" target="_blank" rel="noreferrer noopener">should be barred from receiving public research funding</a>. This, one might hope, would stop at least some of the chronically deluded pseudoscientists of alternative medicine.</p>
<p><strong>This is a revised and extended version of an article published in the Skeptical Inquirer, <a href="https://skepticalinquirer.org/2024/06/data-falsification-fabrication-and-manipulation-by-a-prominent-homeopath/" target="_blank" rel="noreferrer noopener">Data Falsification, Fabrication, and Manipulation by a Prominent Homeopath</a>.</strong></p>
</div><div data-td-block-uid="tdi_149">


<p>The Skeptic is made possible thanks to support from our readers. If you enjoyed this article, please consider taking out a <a href="https://www.patreon.com/theskeptic" target="_blank"> voluntary monthly subscription on Patreon</a><a>.</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DuckDB Community Extensions (135 pts)]]></title>
            <link>https://duckdb.org/2024/07/05/community-extensions.html</link>
            <guid>40885238</guid>
            <pubDate>Fri, 05 Jul 2024 19:13:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://duckdb.org/2024/07/05/community-extensions.html">https://duckdb.org/2024/07/05/community-extensions.html</a>, See on <a href="https://news.ycombinator.com/item?id=40885238">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
							
							
							
							
							
							
								<p><em>TL;DR: DuckDB extensions can now be published via the <a href="https://github.com/duckdb/community-extensions">DuckDB Community Extensions repository</a>. The repository makes it easier for users to install extensions using the <code>INSTALL ⟨extension name⟩ FROM community</code> syntax. Extension developers avoid the burdens of compilation and distribution.</em></p>
							
							<h2 id="duckdb-extensions">
        
        <a href="#duckdb-extensions">DuckDB Extensions</a>
        
      </h2>
    
      <h3 id="design-philosophy">
        
        <a href="#design-philosophy">Design Philosophy</a>
        
      </h3>
    

<p>One of the main design goals of DuckDB is <em>simplicity</em>, which – to us – implies that the system should be rather nimble, very light on dependencies, and generally small enough to run on constrained platforms like <a href="https://duckdb.org/docs/api/wasm/overview.html">WebAssembly</a>. This goal is in direct conflict with very reasonable user requests to support advanced features like spatial data analysis, vector indexes, connectivity to various other databases, support for data formats, etc. Baking all those features into a monolithic binary is certainly possible and the route some systems take. But we want to preserve DuckDB’s simplicity. Also, shipping all possible features would be quite excessive for most users because no use cases require <em>all</em> extensions at the same time (the “Microsoft Word paradox”, where even power users only use a few features of the system, but the exact set of features vary between users).</p>

<p>To achieve this, DuckDB has a powerful extension mechanism, which allows users to add new functionalities to DuckDB. This mechanism allows for registering new functions, supporting new file formats and compression methods, handling new network protocols, etc. In fact, many of DuckDB’s popular features are implemented as extensions: the <a href="https://duckdb.org/docs/data/parquet/overview.html">Parquet reader</a>, the <a href="https://duckdb.org/docs/extensions/json.html">JSON reader</a>, and the <a href="https://duckdb.org/docs/extensions/httpfs/overview.html">HTTPS/S3 connector</a> all use the extension mechanism.</p>
      <h3 id="using-extensions">
        
        <a href="#using-extensions">Using Extensions</a>
        
      </h3>
    

<p>Since <a href="https://github.com/duckdb/duckdb/releases/tag/v0.3.2">version 0.3.2</a>, we have already greatly simplified the discovery and installation by hosting them on a centralized extension repository. So, for example, to install the <a href="https://duckdb.org/docs/extensions/spatial.html">spatial extension</a>, one can just run the following commands using DuckDB’s SQL interface:</p>

<div><pre><code><span>INSTALL</span> <span>spatial</span><span>;</span> <span>-- once</span>
<span>LOAD</span>    <span>spatial</span><span>;</span> <span>-- on each use</span>
</code></pre></div>

<p>What happens behind the scenes is that DuckDB downloads an extension binary suitable to the current operating system and processor architecture (e.g., macOS on ARM64) and stores it in the <code>~/.duckdb</code> folder. On each <code>LOAD</code>, this file is loaded into the running DuckDB instance, and things happily continue from there. Of course, for this to work, we compile, sign and host the extensions for a rather large and growing list of processor architecture – operating system combinations. This mechanism is already heavily used, currently, we see around six million extension downloads <em>each week</em> with a corresponding data transfer volume of around 40 terabytes!</p>

<p>Until now, publishing third-party extensions has been a <em>difficult process</em> which required the extension developer to build the extensions in their repositories for a host of platforms. Moreover, they were unable to sign the extensions using official keys, forcing users to use the <code>allow_unsigned_extensions</code> option that disables signature checks which is problematic in itself.</p>
      
    

<p>Distributing software in a safe way has never been easier, allowing us to reach a wide base of users across pip, conda, cran, npm, brew, etc. We want to provide a similar experience both to users who can easily grab the extension they will want to use, and developers who should not be burdened with distribution details. We are also interested in lowering the bar to package utilities and scripts as a DuckDB extension, empowering users to package useful functionality connected to their area of expertise (or pain points).</p>

<p>We believe that fostering a community extension ecosystem is the next logical step for DuckDB. That’s why we’re very excited about launching our <a href="https://github.com/duckdb/community-extensions/">Community Extension repository</a> which was <a href="https://youtu.be/wuP6iEYH11E?t=275">announced at the Data + AI Summit</a>.</p>

<p>For users, this repository allows for easy discovery, installation and maintenance of community extensions directly from the DuckDB SQL prompt. For developers, it greatly streamlines the publication process of extensions. In the following, we’ll discuss how the new extension repository enhances the experiences of these groups.</p>
      <h3 id="user-experience">
        
        <a href="#user-experience">User Experience</a>
        
      </h3>
    

<p>We are going to use the <a href="https://github.com/isaacbrodsky/h3-duckdb"><code>h3</code> extension</a> as our example. This extension implements <a href="https://github.com/uber/h3">hierarchical hexagonal indexing</a> for geospatial data.</p>

<p>Using the DuckDB Community Extensions repository, you can now install and load the <code>h3</code> extension as follows:</p>

<div><pre><code><span>INSTALL</span> <span>h3</span> <span>FROM</span> <span>community</span><span>;</span>
<span>LOAD</span> <span>h3</span><span>;</span>
</code></pre></div>

<p>Then, you can instantly start using it. Note that the sample data is 500 MB:</p>

<div><pre><code><span>SELECT</span>
    <span>h3_latlng_to_cell</span><span>(</span><span>pickup_latitude</span><span>,</span> <span>pickup_longitude</span><span>,</span> <span>9</span><span>)</span> <span>AS</span> <span>cell_id</span><span>,</span>
    <span>h3_cell_to_boundary_wkt</span><span>(</span><span>cell_id</span><span>)</span> <span>AS</span> <span>boundary</span><span>,</span>
    <span>count</span><span>()</span> <span>AS</span> <span>cnt</span>
<span>FROM</span> <span>read_parquet</span><span>(</span><span>'https://blobs.duckdb.org/data/yellow_tripdata_2010-01.parquet'</span><span>)</span>
<span>GROUP</span> <span>BY</span> <span>cell_id</span>
<span>HAVING</span> <span>cnt</span> <span>&gt;</span> <span>10</span><span>;</span>
</code></pre></div>

<p>On load, the extension’s signature is checked, both to ensure platform and versions are compatible, and to verify that the source of the binary is the community extensions repository. Extensions are built, signed and distributed for Linux, macOS, Windows, and WebAssembly. This allows extensions to be available to any DuckDB client using version 1.0.0 and upcoming versions.</p>

<p>The <code>h3</code> extension’s documentation is available at <a href="https://community-extensions.duckdb.org/extensions/h3.html">https://community-extensions.duckdb.org/extensions/h3.html</a>.</p>
      <h3 id="developer-experience">
        
        <a href="#developer-experience">Developer Experience</a>
        
      </h3>
    

<p>From the developer’s perspective, the Community Extensions repository performs the steps required for publishing extensions, including building the extensions for all relevant <a href="https://duckdb.org/docs/dev/building/supported_platforms.html">platforms</a>, signing the extension binaries and serving them from the repository.</p>

<p>For the <a href="https://github.com/isaacbrodsky/">maintainer of <code>h3</code></a>, the publication process required performing the following steps:</p>

<ol>
  <li>
    <p>Sending a PR with a metadata file <code>description.yml</code> contains the description of the extension:</p>

    <div><pre><code><span>extension</span><span>:</span>
  <span>name</span><span>:</span> <span>h3</span>
  <span>description</span><span>:</span> <span>Hierarchical hexagonal indexing for geospatial data</span>
  <span>version</span><span>:</span> <span>1.0.0</span>
  <span>language</span><span>:</span> <span>C++</span>
  <span>build</span><span>:</span> <span>cmake</span>
  <span>license</span><span>:</span> <span>Apache-2.0</span>
  <span>maintainers</span><span>:</span>
    <span>-</span> <span>isaacbrodsky</span>

<span>repo</span><span>:</span>
  <span>github</span><span>:</span> <span>isaacbrodsky/h3-duckdb</span>
  <span>ref</span><span>:</span> <span>3c8a5358e42ab8d11e0253c70f7cc7d37781b2ef</span>
</code></pre></div>
  </li>
  <li>
    <p>The CI will build and test the extension. The checks performed by the CI are aligned with the <a href="https://github.com/duckdb/extension-template"><code>extension-template</code> repository</a>, so iterations can be done independently.</p>
  </li>
  <li>
    <p>Wait for approval from the DuckDB Community Extension repository’s maintainers and for the build process to complete.</p>
  </li>
</ol>
      <h2 id="published-extensions">
        
        <a href="#published-extensions">Published Extensions</a>
        
      </h2>
    

<p>To show that it’s feasible to publish extensions, we reached out to a few developers of key extensions. At the time of the publication of this blog post, the DuckDB Community Extensions repository already contains the following extensions.</p>



<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://github.com/rustyconover/duckdb-crypto-extension">crypto</a></td>
      <td>Adds cryptographic hash functions and <a href="https://en.wikipedia.org/wiki/HMAC">HMAC</a>.</td>
    </tr>
    <tr>
      <td><a href="https://github.com/isaacbrodsky/h3-duckdb">h3</a></td>
      <td>Implements hierarchical hexagonal indexing for geospatial data.</td>
    </tr>
    <tr>
      <td><a href="https://github.com/rustyconover/duckdb-lindel-extension">lindel</a></td>
      <td>Implements linearization/delinearization, Z-Order, Hilbert and Morton curves.</td>
    </tr>
    <tr>
      <td><a href="https://github.com/ywelsch/duckdb-prql">prql</a></td>
      <td>Allows running <a href="https://prql-lang.org/">PRQL</a> commands directly within DuckDB.</td>
    </tr>
    <tr>
      <td><a href="https://github.com/pdet/Scrooge-McDuck">scrooge</a></td>
      <td>Supports a set of aggregation functions and data scanners for financial data.</td>
    </tr>
    <tr>
      <td><a href="https://github.com/rustyconover/duckdb-shellfs-extension">shellfs</a></td>
      <td>Allows shell commands to be used for input and output.</td>
    </tr>
  </tbody>
</table>

<p>DuckDB Labs and the DuckDB Foundation do not vet the code within community extensions and, therefore, cannot guarantee that DuckDB community extensions are safe to use. The loading of community extensions can be explicitly disabled with the following one-way configuration option:</p>

<div><pre><code><span>SET</span> <span>allow_community_extensions</span> <span>=</span> <span>false</span><span>;</span>
</code></pre></div>

<p>For more details, see the documentation’s <a href="https://duckdb.org/docs/operations_manual/securing_duckdb/securing_extensions.html#community-extension">Securing DuckDB page</a>.</p>
      <h2 id="summary-and-looking-ahead">
        
        <a href="#summary-and-looking-ahead">Summary and Looking Ahead</a>
        
      </h2>
    

<p>In this blog post, we introduced the DuckDB Community Extensions repository, which allows easy installation of third-party DuckDB extensions.</p>

<p>We are looking forward to continuously extending this repository. If you have an idea for creating an extension, take a look at the already published extension source codes, which provide good examples of how to package community extensions, and join the <code>#extensions</code> channel on our <a href="https://discord.duckdb.org/">Discord</a>.
Once you have an extension, please contribute it via a <a href="https://github.com/duckdb/community-extensions/pulls">pull request</a>.</p>

<p>Finally, we would like to thank the early adopters of DuckDB’s extension mechanism and Community Extension repository. Thanks for iterating with us and providing feedback to us.</p>

						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube's eraser tool removes copyrighted music without impacting other audio (161 pts)]]></title>
            <link>https://techcrunch.com/2024/07/05/youtubes-updated-eraser-tool-removes-copyrighted-music-without-impacting-other-audio/</link>
            <guid>40885155</guid>
            <pubDate>Fri, 05 Jul 2024 19:01:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/07/05/youtubes-updated-eraser-tool-removes-copyrighted-music-without-impacting-other-audio/">https://techcrunch.com/2024/07/05/youtubes-updated-eraser-tool-removes-copyrighted-music-without-impacting-other-audio/</a>, See on <a href="https://news.ycombinator.com/item?id=40885155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p id="speakable-summary">On July 4, YouTube released an updated eraser tool for creators so they can easily remove any copyrighted music from their videos without affecting any other audio such as dialog or sound effects.</p>

<p>YouTube chief Neal Mohan posted about the tool on X and said, “Good news, creators: our updated Erase Song tool helps you easily remove copyright-claimed music from your video (while leaving the rest of your audio intact).”</p>

<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Good news creators: our updated Erase Song tool helps you easily remove copyright-claimed music from your video (while leaving the rest of your audio intact). Learn more… <a rel="nofollow" href="https://t.co/KeWIw3RFeH">https://t.co/KeWIw3RFeH</a></p>— Neal Mohan (@nealmohan) <a rel="nofollow" href="https://twitter.com/nealmohan/status/1808587459132825844?ref_src=twsrc%5Etfw">July 3, 2024</a></blockquote>
</div></figure>

<p>In the video, the company said that it had been testing the eraser tool for a while, but it wasn’t as accurate in removing a copyrighted song. It noted that the new tool uses an AI-powered algorithm to specifically detect and remove that song without impacting other audio in the clip.</p>

	
	


<figure></figure>

<p>On its <a href="https://support.google.com/youtube/answer/2902117?hl=en" target="_blank" rel="noreferrer noopener nofollow">support page</a>, YouTube still warns that, at times, the algorithm might fail to remove just the song. </p>

	
	


<p>“This edit might not work if the song is hard to remove. If this tool doesn’t successfully remove the claim on a video, you can try other editing options, such as muting all sound in the claimed segments or trimming out the claimed segments,” the company said. </p>

<p>Alternatively, creators can choose to select “Mute all sound in the claimed segments” to silence bits of video that possibly has copyrighted material. Once the creator successfully edits the video, YouTube removes the <a href="https://techcrunch.com/2012/10/03/youtube-changes-its-content-id-appeals-process/">content ID claim</a> — the company’s system for identifying the use of copyrighted content in different clips.</p>

<figure></figure>
</div><div>
<div>
	
	
	

	
	<div>
		<p>After multiple rejections, Apple has approved Fortnite maker Epic Games’ third-party app marketplace for launch in the EU. As now permitted by the EU’s Digital Markets Act (DMA), Epic announced… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/epic-games-calls-out-apple-for-rejecting-its-games-store-in-the-eu/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/epic-games-calls-out-apple-for-rejecting-its-games-store-in-the-eu/" target="_self"><img width="1024" height="641" src="https://techcrunch.com/wp-content/uploads/2023/12/fortnite-epic-games.jpg?w=1024" alt="Apple approves Epic Games’ marketplace app after initial rejections" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>There’s no need to worry that your secret ChatGPT conversations were obtained in a recently reported breach of OpenAI’s systems. The hack itself, while troubling, appears to have been superficial… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/openai-breach-is-a-reminder-that-ai-companies-are-treasure-troves-for-hackers/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/05/openAI-spiral-color-v2.jpg?w=1024" alt="OpenAI breach is a reminder that AI companies are treasure troves for hackers" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Welcome to Startups Weekly — TechCrunch’s weekly recap of everything you can’t miss from the world of startups. Sign up here to get it in your inbox every Friday. Most… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/space-for-newcomers-biotech-going-mainstream-and-more/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/space-for-newcomers-biotech-going-mainstream-and-more/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/07/collage.png?w=1024" alt="Space for newcomers, biotech going mainstream, and more" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Elon Musk’s X is exploring more ways to integrate xAI’s Grok into the social networking app. According to a series of recent discoveries, X is developing new features like the… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/x-plans-to-more-deeply-integrate-groks-ai-app-researcher-finds/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/x-plans-to-more-deeply-integrate-groks-ai-app-researcher-finds/" target="_self"><img width="1024" height="469" src="https://techcrunch.com/wp-content/uploads/2023/07/x-twitter.jpg?w=1024" alt="X plans to more deeply integrate Grok’s AI, app researcher finds" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>We’re about four months away from TechCrunch Disrupt 2024, taking place October 28 to 30 in San Francisco! We could not bring you this world-class event without our world-class partners… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/meet-these-partners-at-disrupt-2024/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/meet-these-partners-at-disrupt-2024/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/06/Post_Header_General_1920x1080.png?w=1024" alt="Meet Brex, Google Cloud, Aerospace and more at Disrupt 2024" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>In its latest step targeting a major marketplace, the European Commission sent Amazon another request for information (RFI) Friday in relation to its compliance under the bloc’s rulebook for digital… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/amazon-faces-more-eu-scrutiny-over-recommender-algorithms-and-ads-transparency/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/amazon-faces-more-eu-scrutiny-over-recommender-algorithms-and-ads-transparency/" target="_self"><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2019/07/GettyImages-1143844295.jpg?w=1024" alt="Amazon faces more EU scrutiny over recommender algorithms and ads transparency" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Quantum Rise, a Chicago-based startup that does AI-driven automation for companies like dunnhumby (a retail analytics platform for the grocery industry), has raised a $15 million seed round from Erie… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/quantum-rise-grabs-15m-seed-for-its-ai-driven-consulting-2-0-startup/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/quantum-rise-grabs-15m-seed-for-its-ai-driven-consulting-2-0-startup/" target="_self"><img width="510" height="509" src="https://techcrunch.com/wp-content/uploads/2024/07/Alex-Kelleher-Quantum-Rise.jpg?w=510" alt="Quantum Rise grabs $15M seed for its AI-driven ‘Consulting 2.0’ startup" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>On July 4, YouTube released an updated eraser tool for creators so they can easily remove any copyrighted music from their videos without affecting any other audio such as dialog… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/youtubes-updated-eraser-tool-removes-copyrighted-music-without-impacting-other-audio/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/youtubes-updated-eraser-tool-removes-copyrighted-music-without-impacting-other-audio/" target="_self"><img width="1024" height="539" src="https://techcrunch.com/wp-content/uploads/2020/06/GettyImages-1149449078-e1610399732853.jpg?w=1024" alt="YouTube’s updated eraser tool removes copyrighted music without impacting other audio" loading="lazy"></a><figcaption><strong>Image Credits:</strong> Olly Curtis/Future / Getty Images</figcaption></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Airtel, India’s second-largest telecom operator, on Friday denied any breach of its systems following reports of an alleged security lapse that has caused concern among its customers. The telecom group,… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/05/indias-airtel-dismisses-data-breach-reports-amid-customer-concerns/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/05/indias-airtel-dismisses-data-breach-reports-amid-customer-concerns/" target="_self"><img width="1024" height="596" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-1753034881.jpg?w=1024" alt="India’s Airtel dismisses data breach reports amid customer concerns" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>According to a recent Dealroom report on the Spanish tech ecosystem, the combined enterprise value of Spanish startups&nbsp;surpassed&nbsp;€100 billion in 2023. In the latest confirmation of this upward trend, Madrid-based… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/04/spains-exposure-to-climate-change-helps-madrid-based-vc-seaya-close-e300m-climate-tech-fund/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/04/spains-exposure-to-climate-change-helps-madrid-based-vc-seaya-close-e300m-climate-tech-fund/" target="_self"><img width="941" height="575" src="https://techcrunch.com/wp-content/uploads/2024/07/Beatriz-Gonzalez-Seaya-VC.jpg?w=941" alt="Spain’s exposure to climate change helps Madrid-based VC Seaya close €300M climate tech fund" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Forestay, an emerging VC based out of Geneva, Switzerland, has been busy. This week it closed its second fund, Forestay Capital II, at a hard cap of $220 million. The… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/04/forestay-europes-newest-220m-growth-stage-vc-fund-will-focus-on-ai/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/04/forestay-europes-newest-220m-growth-stage-vc-fund-will-focus-on-ai/" target="_self"><img width="1024" height="854" src="https://techcrunch.com/wp-content/uploads/2024/07/frederic_wohlwend_headshot.jpeg?w=1024" alt="Forestay, Europe’s newest $220M growth-stage VC fund, will focus on AI" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Threads, Meta’s alternative to Twitter, just celebrated its first birthday. After launching on July 5 last year, the social network has reached 175 million monthly active users — that’s a… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/04/a-year-later-what-threads-could-learn-from-other-social-networks/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/04/a-year-later-what-threads-could-learn-from-other-social-networks/" target="_self"><img width="1024" height="622" src="https://techcrunch.com/wp-content/uploads/2024/05/Instagram-Threads-GettyImages-1795093602.jpeg?w=1024" alt="A year later, what Threads could learn from other social networks" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>J2 Ventures, a firm led mostly by U.S. military veterans, announced on Thursday that it has raised a $150 million second fund. The Boston-based firm invests in startups whose products… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/04/j2-ventures-focused-on-military-healthcare-grabs-150m-for-its-second-fund/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/04/j2-ventures-focused-on-military-healthcare-grabs-150m-for-its-second-fund/" target="_self"><img width="1024" height="709" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-160989986.jpg?w=1024" alt="J2 Ventures, focused on military healthcare, grabs $150M for its second fund" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>HealthEquity said in an 8-K filing with the SEC that it detected “anomalous behavior by a personal use device belonging to a business partner.” </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/healthequity-says-data-breach-is-an-isolated-incident/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/healthequity-says-data-breach-is-an-isolated-incident/" target="_self"><img width="1024" height="676" src="https://techcrunch.com/wp-content/uploads/2023/12/padlock-badly-handled-breaches.jpg?w=1024" alt="HealthEquity says data breach is an ‘isolated incident’" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Roll20 said that on June 29 it had detected that a “bad actor” gained access to an account on the company’s administrative website for one hour. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/roll20-an-online-tabletop-role-playing-game-platform-discloses-data-breach/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/roll20-an-online-tabletop-role-playing-game-platform-discloses-data-breach/" target="_self"><img width="1024" height="731" src="https://techcrunch.com/wp-content/uploads/2024/07/dungeons-and-dragons-D20-dice.jpg?w=1024" alt="Roll20, an online tabletop role-playing game platform, discloses data breach" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Fisker has a willing buyer for its remaining inventory of all-electric Ocean SUVs, and has asked the Delaware Bankruptcy Court judge overseeing its Chapter 11 case to approve the sale.… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/fisker-ocean-bankruptcy-sale-approval-assets/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/fisker-ocean-bankruptcy-sale-approval-assets/" target="_self"><img width="1024" height="666" src="https://techcrunch.com/wp-content/uploads/2024/07/fisker-ocean-suv-rear.jpg?w=1024" alt="Fisker asks bankruptcy court to sell its EVs at average of $14,000 each" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Teddy Solomon just moved to a new house in Palo Alto, so he turned to the Stanford community on Fizz to furnish his room. “Every time I show up to… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/fizz-the-anonymous-gen-z-social-app-adds-a-marketplace-for-college-students/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/fizz-the-anonymous-gen-z-social-app-adds-a-marketplace-for-college-students/" target="_self"><img width="1024" height="606" src="https://techcrunch.com/wp-content/uploads/2024/07/fizz-marketplace.jpg?w=1024" alt="Fizz, the anonymous Gen Z social app, adds a marketplace for college students" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>With increasing competition for what is, essentially, still a small number of hard tech and deep tech deals, Sidney Scott realized it would be a challenge for smaller funds like… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/deep-tech-venture-capital-sidney-scott/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/deep-tech-venture-capital-sidney-scott/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/07/IMG_0678.jpg?w=1024" alt="Why deep tech VC Driving Forces is shutting down" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>A guide to turn off reactions on your iPhone and Mac so you don’t get surprised by effects during work video calls. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/how-to-turn-off-those-silly-video-calls-reactions-on-iphone-and-mac/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/how-to-turn-off-those-silly-video-calls-reactions-on-iphone-and-mac/" target="_self"><img width="1000" height="667" src="https://techcrunch.com/wp-content/uploads/2024/06/macos-sonoma-video-gesture-confetti-two-peace-signs.jpeg?w=1000" alt="How to turn off those silly video call reactions on iPhone and Mac" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Amazon has decided to discontinue its Astro for Business device, a security robot for small- and medium-sized businesses, just seven months after launch.&nbsp; In an email sent to customers and… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/amazon-discontinues-astro-for-business-security-robot/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/amazon-discontinues-astro-for-business-security-robot/" target="_self"><img width="1024" height="635" src="https://techcrunch.com/wp-content/uploads/2021/09/Astro-Patrol.jpg?w=1024" alt="Amazon retires its Astro for Business security robot after only 7 months" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Hiya, folks, and welcome to TechCrunch’s regular AI newsletter. This week in AI, the U.S. Supreme Court struck down “Chevron deference,” a 40-year-old ruling on federal agencies’ power that required… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/this-week-in-ai-with-chevrons-demise-ai-regulation-seems-dead-in-the-water/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/this-week-in-ai-with-chevrons-demise-ai-regulation-seems-dead-in-the-water/" target="_self"><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2022/05/GettyImages-519986690.jpeg?w=1024" alt="This Week in AI: With Chevron’s demise, AI regulation seems dead in the water" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Noplace had already gone viral ahead of its public launch because of its feature that allows users to express themselves by customizing the colors of their profile. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/noplace-a-mashup-of-twitter-and-myspace-for-gen-z-hits-no-1-on-the-app-store/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/noplace-a-mashup-of-twitter-and-myspace-for-gen-z-hits-no-1-on-the-app-store/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/07/collage.png?w=1024" alt="noplace, a mashup of Twitter and Myspace for Gen Z, hits No. 1 on the App Store" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Cloudflare analyzed AI bot and crawler traffic to fine-tune automatic bot detection models. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/cloudflare-launches-a-tool-to-combat-ai-bots/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/cloudflare-launches-a-tool-to-combat-ai-bots/" target="_self"><img width="1024" height="712" src="https://techcrunch.com/wp-content/uploads/2019/09/cloudflare-bots.jpg?w=1024" alt="Cloudflare launches a tool to combat AI bots" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Twilio says “threat actors were able to identify” phone numbers of people who use the two-factor app Authy. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/twilio-says-hackers-identified-cell-phone-numbers-of-two-factor-app-authy-users/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/twilio-says-hackers-identified-cell-phone-numbers-of-two-factor-app-authy-users/" target="_self"><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2016/07/gettyimages-542439042.jpg?w=1024" alt="Twilio says hackers identified cell phone numbers of two-factor app Authy users" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>The news brings closure to more than two years of volleying back and forth between some of the biggest names in additive manufacturing. </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/nano-dimension-is-buying-desktop-metal/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/nano-dimension-is-buying-desktop-metal/" target="_self"><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2017/04/desktop_metal_piece.jpg?w=1024" alt="Nano Dimension is buying Desktop Metal" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Planning to attend TechCrunch Disrupt 2024 with your team? Maximize your team-building time and your company’s impact across the entire conference when you bring your team. Groups of 4 to… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/groups-save-big-at-techcrunch-disrupt-2024/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/groups-save-big-at-techcrunch-disrupt-2024/" target="_self"><img width="1024" height="576" src="https://techcrunch.com/wp-content/uploads/2024/06/Post_Header_General_1920x1080.png?w=1024" alt="Groups save big at TechCrunch Disrupt 2024" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>As more music streaming apps and creation tools emerge to compete for users’ attention, social music-sharing app Popster is getting two new features to grow its user base: an AI… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/popster-music-video-sharing-app-generative-ai-artists/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/popster-music-video-sharing-app-generative-ai-artists/" target="_self"><img width="1024" height="618" src="https://techcrunch.com/wp-content/uploads/2024/07/Popster.png?w=1024" alt="Music video-sharing app Popster uses generative AI and lets artists remix videos" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Meta’s Threads now has more than 175 million monthly active users, Mark Zuckerberg announced on Wednesday. The announcement comes two days away from Threads’ first anniversary. Zuckerberg revealed back in… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/threads-nears-its-one-year-anniversary-with-more-than-175m-monthly-active-users/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/threads-nears-its-one-year-anniversary-with-more-than-175m-monthly-active-users/" target="_self"><img width="1024" height="622" src="https://techcrunch.com/wp-content/uploads/2024/05/Instagram-Threads-GettyImages-1795093602.jpeg?w=1024" alt="Threads nears its one-year anniversary with more than 175M monthly active users" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Cartken and its diminutive sidewalk delivery robots first rolled into the world with a narrow charter: carrying everything from burritos and bento boxes to pizza and pad thai that last… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/from-burritos-to-biotech-how-robotics-startup-cartken-found-its-av-niche/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/from-burritos-to-biotech-how-robotics-startup-cartken-found-its-av-niche/" target="_self"><img width="1024" height="709" src="https://techcrunch.com/wp-content/uploads/2024/07/cartken-founders.jpeg?w=1024" alt="From burritos to biotech: How robotics startup Cartken found its AV niche" loading="lazy"></a></figure>
	</div>
	
</div>



<div>
	
	
	

	
	<div>
		<p>Ashwin Nandakumar and Ashwin Jainarayanan were working on their doctorates at adjacent departments in Oxford, but they didn’t know each other. Nandakumar, who was studying oncology, one day stumbled across… </p>
	</div>
	

	
	<div>
		<figure><a data-destinationlink="https://techcrunch.com/2024/07/03/granza-bio-grabs-7m-seed-from-felicis-and-yc-to-advance-delivery-of-cancer-treatments/" data-event="recirculation" data-module="Query" href="https://techcrunch.com/2024/07/03/granza-bio-grabs-7m-seed-from-felicis-and-yc-to-advance-delivery-of-cancer-treatments/" target="_self"><img width="1024" height="683" src="https://techcrunch.com/wp-content/uploads/2024/07/Granza-Bio-team.jpg?w=1024" alt="Granza Bio grabs $7M seed from Felicis and YC to advance delivery of cancer treatments" loading="lazy"></a></figure>
	</div>
	
</div>
</div></div>]]></description>
        </item>
    </channel>
</rss>