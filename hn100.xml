<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sun, 17 Mar 2024 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[How Microchips Work (115 pts)]]></title>
            <link>https://exclusivearchitecture.com/03-technical-articles-IC-00-table-of-contents.html</link>
            <guid>39732116</guid>
            <pubDate>Sun, 17 Mar 2024 05:17:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exclusivearchitecture.com/03-technical-articles-IC-00-table-of-contents.html">https://exclusivearchitecture.com/03-technical-articles-IC-00-table-of-contents.html</a>, See on <a href="https://news.ycombinator.com/item?id=39732116">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h2>Technical Articles</h2>
                <p>Elaborate descriptions and explanations of optical and electronical principles and devices. <br> Some of my illustrations from this section have already been printed in the magazine 'Popular Mechanics' and on Google's AI blog.</p>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The return of the frame pointers (335 pts)]]></title>
            <link>https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html</link>
            <guid>39731824</guid>
            <pubDate>Sun, 17 Mar 2024 03:59:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html">https://www.brendangregg.com/blog/2024-03-17/the-return-of-the-frame-pointers.html</a>, See on <a href="https://news.ycombinator.com/item?id=39731824">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Sometimes debuggers and profilers are obivously broken, sometimes it's subtle and hard to spot. From my <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#FlameGraph">flame graphs</a> page:</p>

<center><a href="https://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg"><img src="https://www.brendangregg.com/blog/images/2024/cpu-bash-flamegraph-annotated.png" width="700"></a><br><span size="-1"><i>CPU flame graph (partly broken)</i></span></center>

<p>(Click for original SVG.) This is pretty common and usually goes unnoticed as the <a href="https://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg">flame graph</a> looks ok at first glance. But there are 15% of samples on the left, above "[unknown]", that are in the wrong place and missing frames. The problem is that this system has a default libc that has been compiled without frame pointers, so any stack walking stops at the libc layer, producing a partial stack that's missing the application frames. These partial stacks get grouped together on the left.</p>



<p>Click here for a longer explanation.</p>



<p>Other types of profiling hit this more often. <a href="https://www.brendangregg.com/FlameGraphs/offcpuflamegraphs.html">Off-CPU flame graphs</a>, for example, can be dominated by libc read/write and mutex functions, so without frame pointers end up mostly broken. Apart from library code, maybe your application doesn't have frame pointers either, in which case everything is broken.</p>

<p><strong>I'm posting about this problem now because Fedora and Ubuntu are releasing versions that fix it</strong>, by compiling libc and more with frame pointers by default. This is great news as it not only fixes these flame graphs, but makes off-CPU flame graphs far more practical. This is also an win for continuous profilers (my employer, Intel, just <a href="https://www.intc.com/news-events/press-releases/detail/1683/intel-releases-continuous-profiler-to-increase-cpu">announced</a> one) as it makes customer adoption easier.</p>

<h2>What are frame pointers?</h2>

<p>The x86-64 ABI <a href="https://gitlab.com/x86-psABIs/x86-64-ABI">documentation</a> shows how a CPU register, %rbp, can be used as a "base pointer" to a stack frame, aka the "frame pointer." I pictured how this is used to walk stack traces in my BPF book.</p>

<center>

</center>

<p>This stack-walking technique is commonly used by external profilers and debuggers, including Linux perf and eBPF, and ultimately visualized by flame graphs. However, the x86-64 ABI has a footnote [12] to say that this register use is optional:</p>

<blockquote>"The conventional use of %rbp as a frame pointer for the stack frame may be avoided by using %rsp
(the stack pointer) to index into the stack frame. This technique saves two instructions in the prologue and
epilogue and makes one additional general-purpose register (%rbp) available."</blockquote>

<p>(Trivia: I had penciled the frame pointer function prologue and epilogue on my Netflix <a href="https://www.brendangregg.com/blog/images/2022/brendanwall.jpg">office wall</a>, lower left.)</p>

<h2>2004: Their removal</h2>

<p>In 2004 a compiler developer, Roger Sayle, changed gcc to stop generating frame pointers, <a href="https://gcc.gnu.org/legacy-ml/gcc-patches/2004-08/msg01033.html">writing</a>:</p>

<blockquote>"The simple patch below tweaks the i386 backend, such that we now
default to the equivalent of "-fomit-frame-pointer -ffixed-ebp" on
32-bit targets"</blockquote>

<p>i386 (32-bit microprocessors) only have four general purpose registers, so freeing up %ebp takes you from four to five. I'm sure this delivered large performance improvements and I wouldn't try arguing against it. Roger cited two other reasons for this change: The desire to outperform Intel's icc compiler, and the belief that it didn't break debuggers (of the time) since they supported other stack walking techniques.</p>

<h2>2005-2023: The winter of broken profilers</h2>

<p>However, the change was then applied to x86-64 (64-bit) as well, which had sixteen registers and didn't benefit so much from a seventeenth. And there are debuggers/profilers that this change did break, more so today with the rise of eBPF, which didn't exist back then (typically system profilers, not language specific ones). As my former Sun Microsystems colleague Eric Schrock (nickname Schrock) wrote in <a href="https://web.archive.org/web/20131215093042/https://blogs.oracle.com/eschrock/entry/debugging_on_amd64_part_one">November 2004</a>:</p>

<blockquote>"On i386, you at least had the advantage of increasing the number of usable registers by 20%. On amd64, adding a 17th general purpose register isn't going to open up a whole new world of compiler optimizations. You're just saving a pushl, movl, an series of operations that (for obvious reasons) is highly optimized on x86. And for leaf routines (which never establish a frame), this is a non-issue. Only in extreme circumstances does the cost (in processor time and I-cache footprint) translate to a tangible benefit - circumstances which usually resort to hand-coded assembly anyway. Given the benefit and the relative cost of losing debuggability, this hardly seems worth it."</blockquote>

<p>In Schrock's conclusion:</p>

<blockquote>"it's when people start compiling /usr/bin/ without frame pointers that it gets out of control."</blockquote>

<p>This is exactly what happened on Linux, not just /usr/bin but also /usr/lib and application code! I'm sure there are people who are too new to the industry to remember the pre-2004 days when profilers would "just work" without OS and runtime changes.</p>

<h2>2014: Java in Flames</h2>

<div><center><img src="https://www.brendangregg.com/blog/images/2024/Surge2014_CloudsToRoots_066-crop.jpg" width="350"><br>
<span size="-1"><i>Broken Java Stacks (2014)</i></span></center></div>

<p>When I joined Netflix in 2014, I found Java's lack of frame pointer support broke all application stacks (pictured in my 2014 <a href="https://www.brendangregg.com/Slides/Surge2014_CloudsToRoots/">Surge talk</a> on the right). I ended up developing a fix for the JVM c2 compiler which Oracle reworked and added as the -XX:+PreserveFramePointer option in JDK8u60 (see my <a href="http://techblog.netflix.com/2015/07/java-in-flames.html">Java in Flames</a> post for details [<a href="https://www.brendangregg.com/Articles/Netflix_Java_in_Flames.pdf">PDF</a>]).</p>

<p>While that Java change led to discovering countless performance wins in application code, libc was still breaking some portion of the samples (as pictured in the example at the top of this post) and preventing off-CPU flame graphs. I started by compiling my own libc for production use with frame pointers, and then worked with Canonical to have one prebuilt for Ubuntu. For a while I was promoting the use of Canonical's libc6-prof, which was libc6 with frame pointers.</p>

<h2>2015-2020: Overhead</h2>

<p>As part of production rollout I did many performance overhead tests, which I've described publicly before: The overhead of adding frame pointers to everything (libc and Java) was usually less than 1%, with one exception of 10%. That 10% was an unusual application that was generating stack traces over 1000 frames deep (via Groovy), so deep that it broke Linux's perf profiler and Arnaldo Carvalho de Melo (Red Hat) added the <a href="https://lkml.iu.edu/hypermail/linux/kernel/1604.2/03420.html">kernel.perf_event_max_stack</a> sysctl just for this Netflix workload. It was also a virtual machine that lacked low-level hardware profiling capabilities, so I wasn't able to do cycle analysis to confirm that the 10% was entirely frame pointer-based.</p>

<p>The actual overhead depends on your workload. Others have reported around 1% and around 2%. Microbenchmarks can be the worst, hitting 10%: This doesn't surprise me since they resolve to running a small funciton in a loop, and adding any instructions to that function can cause it to spill out of L1 cache warmth (or cache lines) causing a drop in performance. If I were analyzing such a microbenchmark, apart from observability anaylsis (cycles, instructions, PMU, PMCs, PEBS) there are also an experiment I'd like to try:</p>

<ul><span size="-1">To test the theory of I-cache spillover: Compile the microbenchmark with and without frame pointers and find the performance delta. Then flame graph the microbenchmark to understand the hot function. Then add some inline assembly to the hot function where you add enough NOPs to the start and end to mimic the frame pointer prologue and epilogue (I recommend writing them on your office wall in pencil), compile it without frame pointers, disassemble the compiled binary to confirm those NOPs weren't stripped, and now test that. If the performance delta is still large (10%) you've confirmed that it is due to cache effects, and anyone who was worked at this level in production will tell you that it's the straw that broke the camel's back. Don't blame the straw, in this case, don't blame the frame pointers. Adding <i>anything</i> will cause the same effect. Having done this before, it reminds me of CSS programming: you make a little change here and everything breaks, and you spend hours chasing your own tail.</span></ul>

<p>Someone recently told me that Python can hit 10% overhead with frame pointers. That also needs to be debugged to see what's going on. My experience is that it's the exception and not the rule. And don't forget what this is changing: It gives the compiler an extra <em>seventeenth</em> register, and adds some highly-optimized instructions to every function. It shouldn't be 10%, unless it's cache effects.</p>

<p>As I've seen frame pointers help find performance wins ranging from 5% to 500%, the typical "less than 1%" cost (or even 1% or 2% cost) is easily justified. But I'd rather the cost be zero, of course! We may get there with future technologies I'll cover later. In the meantime, frame pointers are the most practical way to find performance wins today.</p>

<p>What about Linux on devices where there is no chance of profiling or debugging, like electric toothbrushes? (I made that up, AFAIK they don't run Linux, but I may be wrong!) Sure, compile without frame pointers. The main users of this change are enterprise Linux. Back-end servers.</p>

<h2>2022: Upstreaming, first attempt</h2>

<p>Other large companies with OS and perf teams (Meta, Google) hinted strongly that they had already enabled frame pointers for everything years earlier. (Google should be no surprise because they pioneered continuous profiling.) So at this point you had Google, Meta, and Netflix running their own libc with frame pointers and able to enjoy profiling capabilities that most other companies – without dedicated OS teams – couldn't get working. Can't we just upstream this so everyone can benefit?</p>

<p>There's a bunch of difficulties when taking "works well for me" changes and trying to make them the default for everyone. Among the difficulties is that end-user companies don't have a clear return on the investment from telling their Linux vendor what they fixed, since they already fixed it. I guess the investment is quite small, we're talking about a single email, right?...Wrong! Your suggestion is now a <a href="https://pagure.io/fesco/issue/2817">116-post thread</a> where everyone is sharing different opinions and demanding this and that, as we found out the hard way. For Fedora, one person requested:</p>

<blockquote>"Meta and/or Netflix should provide infrastructure for a side repository in which the change can be tested and benchmarked and the code size measured."</blockquote>

<p>(Bear in mind that Netflix doesn't even use Fedora!)</p>

<p>Jonathan Corbet, who writes the best Linux articles, summarized this in "<a href="https://lwn.net/Articles/919940/">Fedora's tempest in a stack frame</a>" that is so detailed that I feel PTSD when reading it. It's good that the Fedora community wants to be so careful, but I'd rather spend time discussing building something <em>better</em> than frame pointers, perhaps involving ORC, LBR, eBPF, and other technologies, than so much worry about looking bad in kitchen-sink benchmarks that I wouldn't trust in the first place.</p>

<h2>2023, 2024: Frame Pointers in Fedora and Ubuntu!</h2>

<p><strong>Fedora</strong> <a href="https://pagure.io/fesco/issue/2923">revisited</a> the proposal and has accepted it this time, making it the first distro to reenable frame pointers. Thank you!</p>

<p><strong>Ubuntu</strong> has also announced <a href="https://ubuntu.com/blog/ubuntu-performance-engineering-with-frame-pointers-by-default">frame pointers by default</a> in Ubuntu 24.04 LTS. Thank you!</p>

<p>While this fixes stack walking through OS libraries, you might find your application still doesn't support stack tracing, but that's typically much easier to fix. Java, for example, has the -XX:+PreserveFramePointer option. There were ways to get Golang to support frame pointers, but that became the default years ago. Just to name a couple of languages.</p>

<h2>2034+: Beyond Frame Pointers</h2>

<p>There's more than one way to walk a stack. These could be separate blog posts, but I want to comment briefly on alternates:</p>

<ul>
<li><strong>LBR (Last Branch Record)</strong>: Intel's hardware feature that was limited to 16 or 32 frames. Most application stacks are deeper, so this can't be used to build flame graphs, but it is better than nothing. I use it as a last resort as it gives me <em>some</em> stack insights.</li>
<li><strong>BTS (Branch Trace Store)</strong>: Another Intel thing. Not so limited to stack depth, but has overhead from memory load/stores and BTS buffer overflow interrupt handling.</li>
<li><strong>AET (Archetectural Event Trace)</strong>: Another Intel thing. It's a <a href="https://www.asset-intertech.com/resources/blog/2020/03/intel-architectural-event-trace-aet-in-action/">JTAG-based tracer</a> that can trace low-level CPU, BIOS, and device events, and apparently can be used for stack traces as well. I haven't used it. (I spent years as a cloud customer where I couldn't access many HW-level things.) I hope it can be configured to output to main memory, and not just a physical debug port.</li>
<li><strong>DWARF</strong>: Binary debuginfo, has been used forever with debuggers. Doesn't exist for JIT'd runtimes like the Java JVM, and I don't really see any practical way to ever fix that. The overhead to walk DWARF is also too high, as it was designed for non-realtime use. Polar Signals did some interesting work using an eBPF walker to reduce the overhead, but...Java.</li>
<li><strong>eBPF stack walking</strong>: Mark Wielaard (Red Hat) demonstrated a Java JVM stack walker using SystemTap back at LinuxCon 2014, where an external tracer walked a runtime with no runtime support or help. Very cool. This can be done using eBPF as well. The performmance overhead could be too high, however, as it may mean a lot of user space reads of runtime internals depending on the runtime.</li>
<li><strong>ORC (oops rewind capability)</strong>: The Linux kernel's new lightweight <a href="https://lwn.net/Articles/728339/">stack unwinder</a> by Josh Poimboeuf (Red Hat) that has allowed newer kernels to remove frame pointers yet retain stack walking. You may be using ORC without realizing it; the rollout was smooth as the kernel profiler code was updated to support ORC (perf_callchain_kernel()-&gt;unwind_orc.c) at the same time as it was compiled to support ORC. Can't ORCs invade user space as well?</li>
<li><strong>SFrames (Stack Frames)</strong>: ...which is what <a href="https://lwn.net/Articles/932209/">SFrames</a> does: lightweight user stack unwinding based on ORC. There have been recent talks to explain them by <a href="https://www.youtube.com/watch?v=4XrFYpjyodo">Indu Bhagat</a> (Oracle) and <a href="https://www.youtube.com/watch?v=FKB-vudYqCw">Steven Rostedt</a> (Google). I should do a blog post just on SFrames.</li>
<li><strong>Shadow Stacks</strong>: A newer Intel and AMD security <a href="https://lwn.net/Articles/885220/">feature</a> that can be configured to push function return addresses onto a separate HW stack so that they can be double checked when the return happens. Sounds like such a HW stack could also provide a stack trace, without frame pointers.</li>
<li>(And this isn't even all of them.)</li>
</ul>

<p>Daan De Meyer (Meta) did a nice summary as well of different stack walkers on the <a href="https://fedoraproject.org/wiki/Changes/fno-omit-frame-pointer#Alternatives_to_frame_pointers">Fedora wiki</a>.</p>

<p>So what's next? Here's my guesses:</p>

<ul>
<li>2029: Ubuntu and Fedora release new versions with SFrames for OS components (including libc) and ditches frame pointers again. We'll have had five years of frame pointer-based performance wins and new innovations that make use of user space stacks, and will hit the ground running with SFrames.</li>
<li>2034: Shadow stacks have been enabled by default for security, and then are used for all stack tracing.</li>
</ul>

<h2>Conclusion</h2>

<p>I could say that times have changed and now the original 2004 reasons for omitting frame pointers are no longer valid in 2024. Those reasons were that it improved performance significantly on i386, that it didn't break the debuggers of the day (prior to eBPF), and that competing with another compiler (icc) was deemed important. Yes, times have indeed changed. But I should note that one engineer, Eric Schrock, claimed that it didn't make sense back in 2004 either when it was applied to x86-64, and I agree with him. Profiling has been broken for 20 years and we've only now just fixed it.</p>

<p><a href="https://pagure.io/fesco/issue/2923">Fedora</a> and <a href="https://ubuntu.com/blog/ubuntu-performance-engineering-with-frame-pointers-by-default">Ubuntu</a> have now returned frame pointers, which is great news. People should start running these releases in 2024 and will find that <a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">CPU flame graphs</a> make more sense, <a href="https://www.brendangregg.com/FlameGraphs/offcpuflamegraphs.html">Off-CPU flame graphs</a> work for the first time, and other new things become possible. It's also a win for continuous profilers, as they don't need to convince their customers to make OS changes to get profiles to fully work.</p>

<h2>Thanks</h2>

<p>The online threads about this change aren't even everything, there's been many discussions, meetings, and work put into this, not just for frame pointers but other recent advances including ORC and SFrames. Special thanks to Andrii Nakryiko (Meta), Daan De Meyer (Meta), Davide Cavalca (Meta), Ian Rogers (Google), Steven Rostedt (Google), Josh Poimboeuf (Red Hat), Arjan Van De Ven (Intel), Indu Bhagat (Oracle), Mark Shuttleworth (Canonical), Jon Seager (Canonical), Oliver Smith (Canonical), and many others (see the Fedora discussions). And thanks to Schrock.</p>

<h2>Appendix: Fedora</h2>

<p>For reference, here's my writeup for the Fedora <a href="https://pagure.io/fesco/issue/2817#comment-826805">change</a>:</p>

<pre>I enabled frame pointers at Netflix, for Java and glibc, and summarized the effect in BPF
Performance Tools (page 40):

"Last time I studied the performance gain from frame pointer omission in our production
environment, it was usually less than one percent, and it was often so close to zero that it
was difficult to measure. Many microservices at Netflix are running with the frame pointer
reenabled, as the performance wins found by CPU profiling outweigh the tiny loss of
performance."

I've spent a lot of time analyzing frame pointer performance, and I did the original work to
add them to the JVM (which became -XX:+PreserveFramePoiner). I was also working with another
major Linux distro to make frame pointers the default in glibc, although I since changed jobs
and that work has stalled. I'll pick it up again, but I'd be happy to see Fedora enable it in
the meantime and be the first to do so.

We need frame pointers enabled by default because of performance. Enterprise environments are
monitored, continuously profiled, and analyzed on a regular basis, so this capability will
indeed be put to use. It enables a world of debugging and new performance tools, and once you
find a 500% perf win you have a different perspective about the &lt;1% cost. Off-CPU flame graphs
in particular need to walk the pthread functions in glibc as most blocking paths go through
them; CPU flame graphs need them as well to reconnect the floating glibc tower of
futex/pthread functions with the developers code frames.

I see the comments about benchmark results of up to 10% slowdowns. It's good to look out for
regressions, although in my experience all benchmarks are wrong or deeply misleading. You'll
need to do cycle analysis (PEBS-based) to see where the extra cycles are, and if that makes
any sense. Benchmarks can be super sensitive to degrading a single hot function (like "CPU
benchmarks" that really just hammer one function in a loop), and if extra instructions
(function prologue) bump it over a cache line or beyond L1 cache-warmth, then you can get a
noticeable hit. This will happen to the next developer who adds code anyway (assuming such a
hot function is real world) so the code change gets unfairly blamed. It will only regress in
this particular scenario, and regression is inevitable. Hence why you need the cycle analysis
("active benchmarking") to make sense of this.

There was one microservice that was an outlier and had a 10% performance loss with Java frame
pointers enabled (not glibc, I've never seen a big loss there). 10% is huge. This was before
PMCs were available in the cloud, so I could do little to debug it. Initially the microservice
ran a "flame graph canary" instance with FPs for flame graphs, but the developers eventually
just enabled FPs across the whole microservice as the gains they were finding outweighed the
10% cost. This was the only noticeable (as in, &gt;1%) production regression we saw, and it was a
microservice that was bonkers for a variety of reasons, including stack traces that were over
1000 frames deep (and that was after inlining! Over 3000 deep without. ACME added the
perf_event_max_stack sysctl just so Netflix could profile this microservice, as the prior
limit was 128). So one possibility is that the extra function prologue instructions add up if
you frequently walk 1000 frames of stack (although I still don't entirely buy it). Another
attribute was that the microservice had over 1 Gbyte of instruction text (!), and we may have
been flying close to the edge of hardware cache warmth, where adding a bit more instructions
caused a big drop. Both scenarios are debuggable with PMCs/PEBS, but we had none at the time.

So while I think we need to debug those rare 10%s, we should also bear in mind that customers
can recompile without FPs to get that performance back. (Although for that microservice, the
developers chose to eat the 10% because it was so valuable!) I think frame pointers should be
the default for enterprise OSes, and to opt out if/when necessary, and not the other way
around. It's possible that some math functions in glibc should opt out of frame pointers
(possibly fixing scimark, FWIW), but the rest (especially pthread) needs them.

In the distant future, all runtimes should come with an eBPF stack walker, and the kernel
should support hopping between FPs, ORC, LBR, and eBPF stack walking as necessary. We may
reach a point where we can turn off FPs again. Or maybe that work will never get done. Turning
on FPs now is an improvement we can do, and then we can improve it more later.

For some more background: Eric Schrock (my former colleague at Sun Microsystems) described the
then-recent gcc change in 2004 as "a dubious optimization that severely hinders debuggability"
and that "it's when people start compiling /usr/bin/* without frame pointers that it gets out
of control" I recommend reading his post: [0].

The original omit FP change was done for i386 that only had four general-purpose registers and
saw big gains freeing up a fifth, and it assumed stack walking was a solved problem thanks to
gdb(1) without considering real-time tracers, and the original change cites the need to
compete with icc [1]. We have a different circumstance today -- 18 years later -- and it's
time we updated this change.

[0] http://web.archive.org/web/20131215093042/https://blogs.oracle.com/eschrock/entry/debugging_on_amd64_part_one
[1] https://gcc.gnu.org/ml/gcc-patches/2004-08/msg01033.html
</pre>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Losing Faith on Testing (166 pts)]]></title>
            <link>https://registerspill.thorstenball.com/p/a-few-words-on-testing</link>
            <guid>39731195</guid>
            <pubDate>Sun, 17 Mar 2024 01:48:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://registerspill.thorstenball.com/p/a-few-words-on-testing">https://registerspill.thorstenball.com/p/a-few-words-on-testing</a>, See on <a href="https://news.ycombinator.com/item?id=39731195">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>First, my credentials. More than half of all the code I wrote in my life is test code. My name is attached to </span><a href="https://interpreterbook.com/" rel="">hundreds</a><span> of </span><a href="https://compilerbook.com/" rel="">pages</a><span> of TDD. In my first internship as a software developer I wrote tests and did TDD while pair programming in my first week. I’ve written unit tests, integration tests, tests for exploration, tests to stop problems from reappearing, tests to leave a message, tests using testing frameworks and BDD and no framework at all, tests in Ruby, JavaScript, C, Go, Rust, Scheme, Bash. After two drinks, I’m willing to say that I know more about testing than many others. Right now — no drinks —&nbsp;I’m willing to say that I love testing and that writing tests has brought me a lot of joy.</span></p><p>Yet I can no longer say that I’m free of doubt. To stick with the theme: I’m much more sober about testing today than I was ten years ago. Recently, in the past few months, the doubts have grown.</p><p><span>Too many flaky tests. Too much time spent getting the tests to pass after making a tiny change that I knew was correct but the tests didn’t. Too many integration tests that made people wait 20, 30, 40 minutes until they could merge their change, only to reveal — months later — that they never tested anything. Too many times have I fixed a bug and </span><em>knew</em><span> it was fixed because I tested it manually, thoroughly, and was 100% sure that I know how the code works and that this can’t happen again, but then spent hours — 10 times longer than it took me to fix the bug —&nbsp;to write a test only to prove what I knew all along, that the bug is fixed.</span></p><p>It’s not that I was ever a capital-b Believer in tests. I never believed in testing coverage as a metric, never really cared whether someone wrote their tests first or last (although I think too few people have seriously tried TDD), came to think that most discussions around functional vs.&nbsp;intergration vs. whathaveyou tests are a big misunderstanding and that people who say you should never hit the database in tests should get real and probably haven’t written enough tests yet.</p><p>Still, I’ve always thought of tests as good and untested code as bad. Whenever I merged something that didn’t have a test I felt guilty, even when deep down I knew that the test might not be worth it. Tests, I thought, are a sign of quality and the better tested something is, the higher the quality of the product.</p><p><span>Enter </span><a href="https://mitchellh.com/ghostty" rel="">Ghostty</a><span> and </span><a href="https://zed.dev/" rel="">Zed</a><span>.</span></p><p>Both are among the highest-quality software I have ever used and hacked on.</p><p>Both have less tests than I expected.</p><p><span>Both do have tests, of course. Ghostty has extensive tests for its core: the terminal state, the font rendering, the parser of escape and control sequences, and so on. Zed also a lot of tests for its foundational data structures — the rope, the SumTree, the editor, and so on — and tests for big features, and </span><a href="https://www.youtube.com/watch?v=ms8zKpS_dZE" rel="">very smart, very cool property tests for async code</a><span>.</span></p><p>But neither codebase has tests, for example, that take a long-ass time to run. No tests that click through the UI and screenshot and compare and hit the network. Zed’s complete testsuite takes 136 seconds to run 1052 tests in CI. Ghostty’s takes 38 seconds, including compilation. Many tests, but less than I thought.</p><p>In both codebases I’ve merged PRs without any tests and frequently see others do the same. And the world didn’t end and no one shed any tears and the products are still some of best I’ve ever used and the codebases contain some of the most elegant code I’ve ever read.</p><p>So now I’m writing this and it feels like a confession to say that I’m beginning to think that maybe there’s no correlation between software quality and tests. Maybe the tests are only a symptom. A symptom of something else that causes the quality.</p><p><span>Maybe the wisest thing about testing that’s ever been said and maybe the only thing that you need to know about testing — forget about the testing pyramids, and the mocks vs.&nbsp;stubs debates, and the dependency injectors, and the coverage numbers —&nbsp;is what Kent Beck said </span><a href="https://stackoverflow.com/a/153565" rel="">16 years ago in an answer on Stack Exchange</a><span>:</span></p><blockquote><p>I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence […]. If I don’t typically make a kind of mistake (like setting the wrong variables in a constructor), I don’t test for it.</p></blockquote><p>Maybe that’s all you need. That and people who give a damn.</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debloat non-rooted Android devices (201 pts)]]></title>
            <link>https://github.com/0x192/universal-android-debloater</link>
            <guid>39730962</guid>
            <pubDate>Sun, 17 Mar 2024 01:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/0x192/universal-android-debloater">https://github.com/0x192/universal-android-debloater</a>, See on <a href="https://news.ycombinator.com/item?id=39730962">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Universal Android Debloater GUI</h2><a id="user-content-universal-android-debloater-gui" aria-label="Permalink: Universal Android Debloater GUI" href="#universal-android-debloater-gui"></a></p>
<p dir="auto"><strong>DISCLAIMER</strong>: Use at your own risk. I am not responsible for anything that
could happen to your phone.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/0x192/universal-android-debloater/blob/main/resources/screenshots/v0.5.0.png"><img src="https://github.com/0x192/universal-android-debloater/raw/main/resources/screenshots/v0.5.0.png" width="850" alt="uad_screenshot"></a></p>
<p dir="auto"><strong>This software is still in an early stage of development. Check out the issues, and feel free to contribute!</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Summary</h2><a id="user-content-summary" aria-label="Permalink: Summary" href="#summary"></a></p>
<p dir="auto">This is a complete rewrite in Rust of the <a href="https://gitlab.com/W1nst0n/universal-android-debloater" rel="nofollow">UAD project</a>,
which aims to improve privacy and battery performance by removing unnecessary
and obscure system apps.
This can also contribute to improve security by reducing <a href="https://en.wikipedia.org/wiki/Attack_surface" rel="nofollow">the attack surface</a>.</p>
<p dir="auto">Packages are as well documented as possible in order to provide a better
understanding of what you can delete or not. The worst issue that could happen
is removing an essential system package needed during boot causing then an unfortunate
bootloop. After about 5 failed system boots, the phone will automatically reboot
in recovery mode, and you'll have to perform a FACTORY RESET. Make a backup first!</p>
<p dir="auto">In any case, you <strong>CANNOT</strong> brick your device with this software!
That's the main point, right?</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul>
<li> Uninstall/Disable and Restore/Enable system packages</li>
<li> Multi-user support (e.g. apps in work profiles)</li>
<li> Export/Import your selection in <code>uad_exported_selection.txt</code></li>
<li> Multi-device support: you can connect multiple phones at the same time</li>
<li> All your actions are logged, so you never forget what you've done</li>
</ul>
<p dir="auto">NB : System apps cannot truly be uninstalled without root (see the <a href="https://github.com/0x192/universal-android-debloater/wiki/FAQ">FAQ</a>)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Universal Debloat Lists</h2><a id="user-content-universal-debloat-lists" aria-label="Permalink: Universal Debloat Lists" href="#universal-debloat-lists"></a></p>
<ul>
<li> GFAM (Google/Facebook/Amazon/Microsoft)</li>
<li> AOSP</li>
<li> Manufacturers (OEM)</li>
<li> Mobile carriers</li>
<li> Qualcomm / Mediatek / Miscellaneous</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Manufacturers debloat lists</h2><a id="user-content-manufacturers-debloat-lists" aria-label="Permalink: Manufacturers debloat lists" href="#manufacturers-debloat-lists"></a></p>
<ul>
<li> Archos</li>
<li> Asus</li>
<li> Blackberry</li>
<li> Gionee</li>
<li> LG</li>
<li> Google</li>
<li> iQOO</li>
<li> Fairphone</li>
<li> HTC</li>
<li> Huawei</li>
<li> Motorola</li>
<li> Nokia</li>
<li> OnePlus</li>
<li> Oppo</li>
<li> Realme</li>
<li> Samsung</li>
<li> Sony</li>
<li> Tecno</li>
<li> TCL</li>
<li> Unihertz</li>
<li> Vivo/iQOO</li>
<li> Wiko</li>
<li> Xiaomi</li>
<li> ZTE</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Mobile carriers debloat lists</h2><a id="user-content-mobile-carriers-debloat-lists" aria-label="Permalink: Mobile carriers debloat lists" href="#mobile-carriers-debloat-lists"></a></p>
<table>
<thead>
<tr>
<th>Country</th>
<th>Carriers</th>
</tr>
</thead>
<tbody>
<tr>
<td>France</td>
<td>Orange, SFR, Free, Bouygues</td>
</tr>
<tr>
<td>USA</td>
<td>T-Mobile, Verizon, Sprint, AT&amp;T</td>
</tr>
<tr>
<td>Germany</td>
<td>Telekom</td>
</tr>
<tr>
<td>UK</td>
<td>EE</td>
</tr>
</tbody>
</table>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to use it</h2><a id="user-content-how-to-use-it" aria-label="Permalink: How to use it" href="#how-to-use-it"></a></p>
<ul dir="auto">
<li>
<p dir="auto"><strong>Read the <a href="https://github.com/0x192/universal-android-debloater/wiki/FAQ">FAQ</a>!</strong></p>
</li>
<li>
<p dir="auto"><strong>Do a proper backup of your data! You can never be too careful!</strong></p>
</li>
<li>
<p dir="auto">Enable <em>Developer Options</em> on your smartphone.</p>
</li>
<li>
<p dir="auto">Turn on <em>USB Debugging</em> from the developer panel.</p>
</li>
<li>
<p dir="auto">From the settings, disconnect from any OEM accounts (when you delete an OEM
account package it could lock you on the lockscreen because the phone can't
associate your identity anymore)</p>
</li>
<li>
<p dir="auto">Install ADB (see the intructions by clicking on your OS below):</p>
<details>
<summary>LINUX</summary>
<ul dir="auto">
<li>Install <em>Android platform tools</em> on your PC :</li>
</ul>
<p dir="auto">Debian Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo apt install android-sdk-platform-tools"><pre>sudo apt install android-sdk-platform-tools</pre></div>
<p dir="auto">Arch-Linux Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo pacman -S android-tools"><pre>sudo pacman -S android-tools</pre></div>
<p dir="auto">Red Hat Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo yum install android-tools"><pre>sudo yum install android-tools</pre></div>
<p dir="auto">OpenSUSE Base:</p>
<div dir="auto" data-snippet-clipboard-copy-content="sudo zypper install android-tools"><pre>sudo zypper install android-tools</pre></div>
</details>

<details>
<summary>MAC OS</summary>
<ul dir="auto">
<li>
<p dir="auto">Install <a href="https://brew.sh/" rel="nofollow">Homebrew</a></p>
</li>
<li>
<p dir="auto">Install <em>Android platform tools</em></p>
<div dir="auto" data-snippet-clipboard-copy-content="brew install android-platform-tools"><pre>brew install android-platform-tools</pre></div>
</li></ul></details>

</li>
</ul>
<details>
<summary>WINDOWS</summary>
<ul dir="auto">
<li>
<p dir="auto">Download <a href="https://dl.google.com/android/repository/platform-tools-latest-windows.zip" rel="nofollow">android platform tools</a>
and unzip it somewhere.</p>
</li>
<li>
<p dir="auto"><a href="https://www.architectryan.com/2018/03/17/add-to-the-path-on-windows-10/" rel="nofollow">Add the android platform tools to your PATH</a>
<strong>OR</strong> make sure to launch UAD from the same directory.</p>
</li>
<li>
<p dir="auto"><a href="https://developer.android.com/studio/run/oem-usb#Drivers" rel="nofollow">Install USB drivers for your device</a></p>
</li>
<li>
<p dir="auto">Check your device is detected:</p>

</li></ul></details>




<li>
<p dir="auto">Download the latest release of UAD GUI for your Operating System <a href="https://github.com/0x192/universal-android-debloater/releases">here</a>.
Take the <code>opengl</code> version only if the default version (with a Vulkan backend)
doesn't launch.</p>
</li>

<p dir="auto"><strong>NOTE:</strong> Chinese phones users may need to use the AOSP list for removing some stock
apps because those Chinese manufacturers (especially Xiaomi and Huawei) have been
using the name of AOSP packages for their own (modified &amp; closed-source) apps.</p>
<p dir="auto"><strong>IMPORTANT NOTE:</strong> You will have to run this software whenever your OEM pushes
an update to your phone as some <em>uninstalled</em> system apps could be reinstalled.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to contribute</h2><a id="user-content-how-to-contribute" aria-label="Permalink: How to contribute" href="#how-to-contribute"></a></p>
<p dir="auto">Hey-hey-hey! Don't go away so fast! This is a community project.
That means I need you! I'm sure you want to make this project better anyway.</p>
<p dir="auto">==&gt; <a href="https://github.com/0x192/universal-android-debloater/wiki">How to contribute</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Special thanks</h2><a id="user-content-special-thanks" aria-label="Permalink: Special thanks" href="#special-thanks"></a></p>
<ul dir="auto">
<li><a href="https://github.com/mawilms">@mawilms</a> for his LotRO plugin manager (<a href="https://github.com/mawilms/lembas">Lembas</a>)
which helped me a lot to understand how to use the <a href="https://github.com/hecrj/iced">Iced</a>
GUI library.</li>
<li><a href="https://github.com/casperstorm">@casperstorm</a> for the UI/UX inspiration.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low-tech Magazine underscores the potential of past technologies (130 pts)]]></title>
            <link>https://solar.lowtechmagazine.com/</link>
            <guid>39730883</guid>
            <pubDate>Sun, 17 Mar 2024 00:56:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://solar.lowtechmagazine.com/">https://solar.lowtechmagazine.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39730883">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<h2>How to Build a Small Solar Power System</h2>
<p>
			This guide explains everything you need to know to build stand-alone photovoltaic systems that can power almost anything you want.
		</p>
<p><time>December 27, 2023</time>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mintlify GitHub read/write token leak (110 pts)]]></title>
            <link>https://mintlify.com/blog/incident-march-13</link>
            <guid>39730255</guid>
            <pubDate>Sat, 16 Mar 2024 23:07:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mintlify.com/blog/incident-march-13">https://mintlify.com/blog/incident-march-13</a>, See on <a href="https://news.ycombinator.com/item?id=39730255">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://mintlify.com/blog/incident-march-13/thumbnail.png" alt="blog thumbnail"></p><h2>Summary (TL;DR)</h2>
<p>On March 1st, we received an email raising concerns about the security of our endpoints. This in turn prompted us to rake through our logs and we discovered unusual requests to our servers originating from an unrecognized device.</p>
<p>Alarmingly, we noticed that some of these requests targeted sensitive API endpoints and were successful in their attempts. This unusual activity indicated that the actor behind these requests had possession of our private admin access tokens, granting them unauthorized access to our endpoints.</p>
<p><strong>We received confirmation that GitHub tokens stored within our databases were used to access a customer’s repository. While we do not have evidence of any other such instances, we cannot confirm that no other such instances occurred.</strong></p>
<p>We took immediate action by revoking all GitHub token access, rotating our admin access tokens, and implementing stringent security measures to all of our APIs to mitigate any further unauthorized access. Additionally, we have patched the vulnerability that led to the exposure of our admin access tokens.</p>
<p>We have also since partnered with third-party cybersecurity vendors to conduct an extensive investigation, and have implemented other security measures to ensure that this type of unauthorized access cannot occur again. For the security of our users, we decided to implement those security measures before making this public announcement.</p>
<h2>Timeline</h2>
<p>All timestamps referenced are in Pacific Daylight Time (PDT).</p>
<ul>
<li>Friday, March 1 4:55PM - Received an email raising concerns about the security of our endpoints/potential leaking of our token.</li>
<li>Friday, March 1 6:41PM - Discovered logs of an unrecognized device accessing API endpoints.</li>
<li>Friday, March 1 6:51PM - Revoked all existing GitHub user access tokens.</li>
<li>Friday, March 1 6:51PM-11PM<!-- -->
<ul>
<li>Rotated our internal access tokens.</li>
<li>Enhanced security protocols around endpoint authorization to prevent unauthorized access.</li>
<li>Got in contact with a couple bug bounties.</li>
</ul>
</li>
<li>Saturday, March 2nd and 3rd - Continued to stay in close contact with a bug bounty reporter, patched the vulnerability that resulted in the leak of our access token and revoked and rotated all tokens again.</li>
</ul>
<h2>How this affects you</h2>
<p><strong>No further action is required on your part to continue using our product safely.</strong></p>
<p>Our team has addressed the vulnerability and taken steps to secure our systems against similar incidents in the future.</p>
<h2>Actions and Remediations</h2>
<p>In our response to protect our users and our systems, these are the measure that we have <strong>already taken</strong>:</p>
<ul>
<li>Revoked all existing GitHub user access tokens.</li>
<li>Rotated of our internal access tokens.</li>
<li>Patched the vulnerability that resulted in the leak of our access token.</li>
<li>Enhanced security protocols around endpoint authorization to prevent unauthorized access.</li>
<li>Received a penetration test.</li>
</ul>
<p>These are ongoing preventing measures that we are <strong>currently taking</strong>:</p>
<ul>
<li>Collaborating with leading cybersecurity firms, including Oneleet, and our other partners, to conduct a thorough investigation and fortify our defenses against potential future attacks.</li>
<li>Enhancing the monitoring and alerting systems for our API endpoints to detect and respond to unusual activities swiftly.</li>
<li>Developing a comprehensive security policy and establishing a public page dedicated to outlining our security measures and protocols.</li>
<li>Launching a bounty program to facilitate the reporting of security vulnerabilities from ethical hackers.</li>
<li>Re-auditing our SOC 2 certification for 2024.</li>
</ul>
<h2>Conclusion</h2>
<p>We deeply regret the inconvenience and concern this incident may have caused. Our dedication to transparency, security, and the trust you place in us remains unwavering.</p>
<p>Your security and trust are the foundations upon which Mintlify is built. We are dedicated to ensuring the continued safety and security of your content and information.</p>
<p>Should you have any concerns or questions, please do not hesitate to contact us at <a href="https://mintlify.com/cdn-cgi/l/email-protection#2b584e485e59425f526b4642455f47424d5205484446"><span data-cfemail="b0c3d5d3c5c2d9c4c9f0ddd9dec4dcd9d6c99ed3dfdd">[email&nbsp;protected]</span></a>.</p>
<p>Sincerely,<br>
The Mintlify Team</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How web bloat impacts users with slow devices (729 pts)]]></title>
            <link>https://danluu.com/slow-device/</link>
            <guid>39729057</guid>
            <pubDate>Sat, 16 Mar 2024 20:08:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://danluu.com/slow-device/">https://danluu.com/slow-device/</a>, See on <a href="https://news.ycombinator.com/item?id=39729057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <meta property="og:image" content="/slow-device-performance.png"> <p>In 2017, <a href="https://danluu.com/web-bloat/">we looked at how web bloat affects users with slow connections</a>. Even in the U.S., <a href="https://twitter.com/danluu/status/1116565029791260672">many users didn't have broadband speeds</a>, making much of the web difficult to use. It's still the case that many users don't have broadband speeds, both inside and outside of the U.S. and that much of the modern web isn't usable for people with slow internet, but the exponential increase in bandwidth (Neilsen suggests <abbr title="Unfortunately, I don't know of a pubic source for low-end data, say 10%-ile or 1%-ile; let me know if you have numbers on this">this is 50% per year for high-end connections</abbr>) has outpaced web bloat for typical sites, making this less of a problem than it was in 2017, although it's still a serious problem for people with poor connections.</p> <p>CPU performance for web apps hasn't scaled nearly as quickly as bandwidth so, while more of the web is becoming accessible to people with low-end connections, more of the web is becoming inaccessible to people with low-end devices even if they have high-end connections. For example, if I try browsing a "modern" Discourse-powered forum on a <code>Tecno Spark 8C</code>, it sometimes crashes the browser. Between crashes, on measuring the performance, the responsiveness is significantly worse than browsing a BBS with an <code>8 MHz 286</code> and a <code>1200 baud</code> modem. On my <code>1Gbps</code> home internet connection, the <code>2.6 MB</code> compressed payload size "necessary" to load message titles is relatively light. The over-the-wire payload size has "only" increased by <code>1000x</code>, which is dwarfed by the increase in internet speeds. But the opposite is true when it comes to CPU speeds — for web browsing and forum loading performance, the <code>8-core (2 1.6 GHz Cortex-A75 / 6 1.6 GHz Cortex-A55)</code> CPU can't handle Discourse. The CPU is something like <code>100000x</code> faster than our <code>286</code>. Perhaps a <code>1000000x</code> faster device would be sufficient.</p> <p>For anyone not familiar with the <code>Tecno Spark 8C</code>, today, a new <code>Tecno Spark 8C</code>, a quick search indicates that one can be hand for <code>USD 50-60</code> in Nigeria and perhaps <code>USD 100-110</code> in India. As <abbr title="The estimates for Nigerian median income that I looked at seem good enough, but the Indian estimate I found was a bit iffier; if you have a good source for Indian income distribution, please pass it along.">a fraction of median household income, that's substantially more than a current generation iPhone in the U.S. today.</abbr></p> <p>By worldwide standards, the <code>Tecno Spark 8C</code> isn't even close to being a low-end device, so we'll also look at performance on an <code>Itel P32</code>, which is a lower end device (though still far from the lowest-end device people are using today). Additionally, we'll look at performance with an <code>M3 Max Macbook (14-core)</code>, an <code>M1 Pro Macbook (8-core)</code>, and the <code>M3 Max</code> set to <code>10x</code> throttling in Chrome dev tools. In order to give these devices every advantage, we'll be on fairly high-speed internet (1Gbps, with a WiFi router that's benchmarked as having lower latency under load than most of its peers). We'll look at some blogging platforms and micro-blogging platforms (this blog, Substack, Medium, Ghost, Hugo, Tumblr, Mastodon, Twitter, Threads, Bluesky, Patreon), forum platforms (Discourse, Reddit, Quora, vBulletin, XenForo, phpBB, and myBB), and platforms commonly used by small businesses (Wix, Squarespace, Shopify, and WordPress again).</p> <p>In the table below, every row represents a website and every non-label column is a metric. After the website name column, we have the compressed size transferred over the wire (<code>wire</code>) and the raw, uncompressed, size (<code>raw</code>). Then we have, for each device, Largest Contentful Paint* (<code>LCP*</code>) and CPU usage on the main thread (<code>CPU</code>). Google's docs explain <code>LCP</code> as</p> <blockquote> <p>Largest Contentful Paint (LCP) measures when a user perceives that the largest content of a page is visible. The metric value for LCP represents the time duration between the user initiating the page load and the page rendering its primary content</p> </blockquote> <p><code>LCP</code> is a common optimization target because it's presented as one of the primary metrics in Google PageSpeed Insights, a "Core Web Vital" metric. There's an asterisk next to <code>LCP</code> as used in this document because, <code>LCP</code> as measured by Chrome is about painting a large fraction of the screen, as opposed to the definition above, which is about content. As sites have optimized for <code>LCP</code>, it's not uncommon to have a large paint (update) that's completely useless to the user, with the actual content of the page appearing well after the <code>LCP</code>. In cases where that happens, I've used the timestamp when useful content appears, not the <code>LCP</code> as defined by when a large but useless update occurs. The full details of the tests and why these metrics were chosen are discussed in an appendix.</p> <p>Although CPU time isn't a "Core Web Vital", it's presented here because it's a simple metric that's highly correlated with my and other users' perception of usability on slow devices. See appendix for more detailed discussion on this. One reason CPU time works as a metric is that, if a page has great numbers for all other metrics but uses a ton of CPU time, the page is not going to be usable on a slow device. If it takes 100% CPU for 30 seconds, the page will be completely unusable for 30 seconds, and if it takes 50% CPU for 60 seconds, the page will be barely usable for 60 seconds, etc. Another reason it works is that, relative to commonly used metrics, it's hard to cheat on CPU time and make optimizations that significantly move the number without impacting user experience.</p> <p>The color scheme in the table below is that, for sizes, more green = smaller / fast and more red = larger / slower. Extreme values are in black.</p>  <table> <tbody><tr> <th rowspan="2">Site</th><th colspan="2">Size</th><th colspan="2">M3 Max</th><th colspan="2">M1 Pro</th><th colspan="2">M3/10</th><th colspan="2">Tecno S8C</th><th colspan="2">Itel P32</th></tr> <tr> <th>wire</th><th>raw</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th><th>LCP*</th><th>CPU</th></tr> <tr> <td>danluu.com</td><td><span color="white">6kB</span></td><td><span color="white">18kB</span></td><td><span color="white">50ms</span></td><td><span color="white">20ms</span></td><td><span color="white">50ms</span></td><td><span color="white">30ms</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.3s</span></td><td>0.4s</td><td><span color="white">0.3s</span></td><td>0.5s</td><td>0.5s</td></tr> <tr> <td>HN</td><td><span color="white">11kB</span></td><td><span color="white">50kB</span></td><td><span color="white">0.1s</span></td><td><span color="white">30ms</span></td><td><span color="white">0.1s</span></td><td><span color="white">30ms</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.3s</span></td><td>0.5s</td><td>0.5s</td><td>0.7s</td><td>0.6s</td></tr> <tr> <td>MyBB</td><td><span color="white">0.1MB</span></td><td><span color="white">0.3MB</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td>0.6s</td><td>0.6s</td><td>0.8s</td><td>0.8s</td><td>2.1s</td><td>1.9s</td></tr> <tr> <td>phpBB</td><td>0.4MB</td><td>0.9MB</td><td><span color="white">0.3s</span></td><td><span color="white">0.1s</span></td><td>0.4s</td><td><span color="white">0.1s</span></td><td>0.7s</td><td>1.1s</td><td>1.7s</td><td>1.5s</td><td>4.1s</td><td>3.9s</td></tr> <tr> <td>WordPress</td><td>1.4MB</td><td>1.7MB</td><td><span color="white">0.2s</span></td><td><span color="white">60ms</span></td><td><span color="white">0.2s</span></td><td><span color="white">80ms</span></td><td>0.7s</td><td>0.7s</td><td>1s</td><td>1.5s</td><td>1.2s</td><td>2.5s</td></tr> <tr> <td>WordPress (old)</td><td>0.3MB</td><td>1.0MB</td><td><span color="white">80ms</span></td><td><span color="white">70ms</span></td><td><span color="white">90ms</span></td><td><span color="white">90ms</span></td><td>0.4s</td><td>0.9s</td><td>0.7s</td><td>1.7s</td><td>1.1s</td><td>1.9s</td></tr> <tr> <td>XenForo</td><td>0.3MB</td><td>1.0MB</td><td>0.4s</td><td><span color="white">0.1s</span></td><td>0.6s</td><td><span color="white">0.2s</span></td><td>1.4s</td><td>1.5s</td><td>1.5s</td><td>1.8s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Ghost</td><td>0.7MB</td><td>2.4MB</td><td><span color="white">0.1s</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.2s</span></td><td><span color="white">0.2s</span></td><td>1.1s</td><td>2.2s</td><td>1s</td><td>2.4s</td><td>1.1s</td><td>3.5s</td></tr> <tr> <td>vBulletin</td><td>1.2MB</td><td>3.4MB</td><td>0.5s</td><td><span color="white">0.2s</span></td><td>0.6s</td><td><span color="white">0.3s</span></td><td>1.1s</td><td>2.9s</td><td>4.4s</td><td>4.8s</td><td>13s</td><td>16s</td></tr> <tr> <td>Squarespace</td><td>1.9MB</td><td>7.1MB</td><td><span color="white">0.1s</span></td><td>0.4s</td><td><span color="white">0.2s</span></td><td>0.4s</td><td>0.7s</td><td>3.6s</td><td>14s</td><td>5.1s</td><td>16s</td><td>19s</td></tr> <tr> <td>Mastodon</td><td>3.8MB</td><td>5.3MB</td><td><span color="white">0.2s</span></td><td><span color="white">0.3s</span></td><td><span color="white">0.2s</span></td><td>0.4s</td><td>1.8s</td><td>4.7s</td><td>2.0s</td><td>7.6s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Tumblr</td><td>3.5MB</td><td>7.1MB</td><td>0.7s</td><td>0.6s</td><td>1.1s</td><td>0.7s</td><td>1.0s</td><td>7.0s</td><td>14s</td><td>7.9s</td><td>8.7s</td><td>8.7s</td></tr> <tr> <td>Quora</td><td>0.6MB</td><td>4.9MB</td><td>0.7s</td><td>1.2s</td><td>0.8s</td><td>1.3s</td><td>2.6s</td><td>8.7s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td><td>19s</td><td><span color="white">29s</span></td></tr> <tr> <td>Bluesky</td><td>4.8MB</td><td>10MB</td><td>1.0s</td><td>0.4s</td><td>1.0s</td><td>0.5s</td><td>5.1s</td><td>6.0s</td><td>8.1s</td><td>8.3s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Wix</td><td>7.0MB</td><td>21MB</td><td>2.4s</td><td>1.1s</td><td>2.5s</td><td>1.2s</td><td>18s</td><td>11s</td><td>5.6s</td><td>10s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Substack</td><td>1.3MB</td><td>4.3MB</td><td>0.4s</td><td>0.5s</td><td>0.4s</td><td>0.5s</td><td>1.5s</td><td>4.9s</td><td>14s</td><td>14s</td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Threads</td><td>9.3MB</td><td>13MB</td><td>1.5s</td><td>0.5s</td><td>1.6s</td><td>0.7s</td><td>5.1s</td><td>6.1s</td><td>6.4s</td><td>16s</td><td><span color="white">28s</span></td><td><span color="white">66s</span></td></tr> <tr> <td>Twitter</td><td>4.7MB</td><td>11MB</td><td>2.6s</td><td>0.9s</td><td>2.7s</td><td>1.1s</td><td>5.6s</td><td>6.6s</td><td>12s</td><td>19s</td><td>24s</td><td><span color="white">43s</span></td></tr> <tr> <td>Shopify</td><td>3.0MB</td><td>5.5MB</td><td>0.4s</td><td><span color="white">0.2s</span></td><td>0.4s</td><td><span color="white">0.3s</span></td><td>0.7s</td><td>2.3s</td><td>10s</td><td><span color="white">26s</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Discourse</td><td>2.6MB</td><td>10MB</td><td>1.1s</td><td>0.5s</td><td>1.5s</td><td>0.6s</td><td>6.5s</td><td>5.9s</td><td>15s</td><td><span color="white">26s</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> <tr> <td>Patreon</td><td>4.0MB</td><td>13MB</td><td>0.6s</td><td>1.0s</td><td>1.2s</td><td>1.2s</td><td>1.2s</td><td>14s</td><td>1.7s</td><td><span color="white">31s</span></td><td>9.1s</td><td><span color="white">45s</span></td></tr> <tr> <td>Medium</td><td>1.2MB</td><td>3.3MB</td><td>1.4s</td><td>0.7s</td><td>1.4s</td><td>1s</td><td>2s</td><td>11s</td><td>2.8s</td><td><span color="white">33s</span></td><td>3.2s</td><td><span color="white">63s</span></td></tr> <tr> <td>Reddit</td><td>1.7MB</td><td>5.4MB</td><td>0.9s</td><td>0.7s</td><td>0.9s</td><td>0.9s</td><td>6.2s</td><td>12s</td><td>1.2s</td><td><span color="white">∞</span></td><td><span color="white">FAIL</span></td><td><span color="white">FAIL</span></td></tr> </tbody></table> <p>At a first glance, the table seems about in that the sites that feel slow unless you have a super fast device show up as slow in the table (as in, <code>max(LCP*,CPU))</code> is high on lower-end devices). When I polled folks about what platforms they thought would be fastest and slowest on our slow devices (<a href="https://mastodon.social/@danluu/111994437263038931">Mastodon</a>, <a href="https://twitter.com/danluu/status/1761875263359537652">Twitter</a>, <a href="https://www.threads.net/@danluu.danluu/post/C3yVpfKS-RP">Threads</a>), they generally correctly predicted that Wordpress and Ghost and Wordpress would be faster than Substack and Medium, and that Discourse would be much slower than old PHP forums like phpBB, XenForo, and vBulletin. I also pulled Google PageSpeed Insights (PSI) scores for pages (not shown) and the correlation isn't as strong with those numbers <abbr title="For the 'real world' numbers, this is also because users with slow devices can't really use some of these sites, so their devices aren't counted in the distribution and PSI doesn't normalize for this.">because</abbr> a handful of sites have managed to optimize their PSI scores without actually speeding up their pages for users.</p> <p>If you've never used a low-end device like this, the general experience is that many sites are unusable on the device and loading anything resource intensive (an app or a huge website) can cause crashes. Doing something too intense in a resource intensive app can also cause crashes. While <a href="https://www.youtube.com/watch?v=U1JMRFQWK70">reviews note</a> that <a href="https://www.youtube.com/watch?v=McawfNlydqk">you can run PUBG and other 3D games with decent performance</a> on a <code>Tecno Spark 8C</code>, this doesn't mean that the device is fast enough to read posts on modern text-centric social media platforms or modern text-centric web forums. While <code>40fps</code> is achievable in PUBG, we can easily see less than <code>0.4fps</code> when scrolling on these sites.</p> <p>We can see from the table how many of the sites are unusable if you have a slow device. All of the pages with <code>10s+ CPU</code> are a fairly bad experience even after the page loads. Scrolling is very jerky, frequently dropping to a few frames per second and sometimes well below. When we tap on any link, the delay is so long that we can't be sure if our tap actually worked. If we tap again, we can get the dreaded situation where the first tap registers and then causes the second tap to register after things have started changing, causing us to tap some random target, but if we wait, we realize that the original tap didn't actually register (or it registered, but not where we thought it did). Although MyBB doesn't service up a mobile site and is penalized by Google for not having a mobile friendly page, it's actually much more usable on these slow mobiles than all but the fastest sites because scrolling and tapping actually work.</p> <p>Another thing we can see is how much variance there is in the relative performance on different devices. For example, comparing an <code>M3/10</code> and a <code>Tecno Spark 8C</code>, for danluu.com and Ghost, an <code>M3/10</code> gives a halfway decent approximation of the <code>Tecno Spark 8C</code> (although danluu.com loads much too quickly), but the <code>Tecno Spark 8C</code> is about three times slower (<code>CPU</code>) for Medium, Substack, and Twitter, roughly four times slower for Reddit and Discourse, and over an order of magnitude faster for Shopify. For Wix, the <code>CPU</code> approximation is about accurate, but our `<code>Tecno Spark 8C</code> is more than 3 times slower on <code>LCP*</code>. It's great that Chrome lets you conveniently simulate a slower device from the convenience of your computer, but just enabling Chrome's CPU throttling (or using any combination of out-of-the-box options that are available) gives fairly different results than we get on many real devices. The full reasons for this are beyond the scope of the post; for the purposes of this post, it's sufficient to note that slow pages are often super-linearly slow as devices get slower and that slowness on one page doesn't strongly predict slowness on another page.</p> <p>If take a site-centric view instead of a device-centric view, another way to look at it is that sites like Discourse, Medium, and Reddit, don't use all that much CPU on our fast <code>M3</code> and <code>M1</code> computers, but they're among the slowest on our <code>Tecno Spark 8C</code> (Reddit's CPU is shown as <code>∞</code> because, no matter how long we wait with no interaction, Reddit uses <code>~90% CPU</code>). Discourse also sometimes crashed the browser after interacting a bit or just waiting a while. For example, one time, the browser crashed after loading Discourse, scrolling twice, and then leaving the device still for a minute or two. For consistency's sake, this wasn't marked as <code>FAIL</code> in the table since the page did load but, realistically, having a page is so resource intensive that the browser crashes is a significantly worse user experience than any of the <code>FAIL</code> cases in the table. When we looked at how <a href="https://danluu.com/web-bloat/">web bloat impacts users with slow connections</a>, we found that <abbr title="One thing to keep in mind here is that having a slow device and a slow connection have multiplicative impacts.">much of the web was unusable for people with slow connections and slow devices are no different</abbr>.</p> <p>Another pattern we can see is how the older sites are, in general, faster than the newer ones, with sites that (visually) look like they haven't been updated in a decade or two tending to be among the fastest. For example, MyBB, the least modernized and oldest looking forum is <code>3.6x / 5x faster (LCP* / CPU)</code> than Discourse on the <code>M3</code>, but on the <code>Tecno Spark 8C</code>, the difference is <code>19x / 33x</code> and, given the overall scaling, it seems safe to guess that the difference would be even larger on the Itel P32 if Discourse worked on such a cheap device.</p> <p>Another example is Wordpress (old) vs. newer, trendier, blogging platforms like Medium and Substack. Wordpress (old) is is <code>17.5x / 10x faster (LCP* / CPU)</code> than Medium and <code>5x / 7x faster (LCP* / CPU)</code> faster than Substack on our <code>M3 Max</code>, and <code>4x / 19x</code> and <code>20x / 8x</code> faster, respectively, on our <code>Tecno Spark 8C</code>. Ghost is a notable exception to this, being a modern platform (launched a year after Medium) that's competitive with older platforms (modern Wordpress is also arguably an exception, but many folks would probably still consider that to be an old platform).</p> <p>Sites that use modern techniques like partially loading the page and then dynamically loading the rest of it, such as Discourse, Reddit, and Substack, tend to be less usable than the scores in the table indicate. Although, in principle, you could build such a site in a simple way that works well with cheap devices but, in practice sites that use dynamic loading tend to be complex enough that the sites are extremely janky on low-end devices. It's generally difficult or impossible to scroll a predictable distance, which means that users will sometimes accidentally trigger more loading by scrolling too far, causing the page to lock up. Many pages actually remove the parts of the page you scrolled past as you scroll; all such pages are essentially unusable. Other basic web features, like page search, also generally stop working. Pages with this kind of dynamic loading can't rely on the simple and fast ctrl/command+F search and have to build their own search. How well this works varies (this used to work quite well in Google docs, but for the past few months or maybe a year, it takes so long to load that I have to deliberately wait after opening a doc to avoid triggering the browser's useless built in search; Discourse search has never really worked on slow devices or even not very fast but not particular slow devices).</p> <p>In principle, these modern pages that burn a ton of CPU when loading could be doing pre-work that means that later interactions on the page are faster and cheaper than on the pages that do less up-front work (this is a common argument in favor of these kinds of pages), but that's not the case for pages tested, which are slower to load initially, slower on subsequent loads, and slower after they've loaded.</p> <p>To understand why the theoretical idea that doing all this work up-front doesn't generally result in a faster experience later, this exchange between a distinguished engineer at Google and one of the founders of Discourse (and CEO at the time) is <abbr title="the founder has made similar comments elsewhere as well, so this isn't a one-off analogy for him, nor do I find it to be an unusual line of thinking in general">illustrative</abbr>, in <a href="https://danluu.com/jeff-atwood-trashes-qualcomm-engineering.png">a discussion where the founder of Discourse says that you should test mobile sites on laptops with throttled bandwidth but not throttled CPU</a>:</p> <ul> <li><b>Google</b>: *you* also don't have slow 3G. These two settings go together. Empathy needs to extend beyond iPhone XS users in a tunnel.</li> <li><b>Discourse</b>: Literally any phone of vintage iPhone 6 or greater is basically as fast as the "average" laptop. You have to understand how brutally bad Qualcomm is at their job. Look it up if you don't believe me.</li> <li><b>Google</b>: I don't need to believe you. I know. This is well known by people who care. My point was that just like not everyone has a fast connection not everyone has a fast phone. Certainly the iPhone 6 is frequently very CPU bound on real world websites. But that isn't the point.</li> <li><b>Discourse</b>: we've been trending towards infinite CPU speed for decades now (and we've been asymptotically there for ~5 years on desktop), what we are not and will never trend towards is infinite bandwidth. Optimize for the things that matter. and I have zero empathy for @qualcomm. Fuck Qualcomm, they're terrible at their jobs. I hope they go out of business and the ground their company existed on is plowed with salt so nothing can ever grow there again.</li> <li><b>Google</b>: Mobile devices are not at all bandwidth constraint in most circumstances. They are latency constraint. Even the latest iPhone is CPU constraint before it is bandwidth constraint. If you do well on 4x slow down on a MBP things are pretty alright</li> <li>...</li> <li><b>Google</b>: Are 100% of users on iOS?</li> <li><b>Discourse</b>: The influential users who spend money tend to be, I’ll tell you that ... Pointless to worry about cpu, it is effectively infinite already on iOS, and even with Qualcomm’s incompetence, will be within 4 more years on their embarrassing SoCs as well</li> </ul> <p>When someone asks the founder of Discourse, "just wondering why you hate them", he responds with a link that cites the Kraken and Octane benchmarks from <a href="https://www.anandtech.com/show/9146/the-samsung-galaxy-s6-and-s6-edge-review/5">this Anandtech review</a>, which have the Qualcomm chip at 74% and 85% of the performance of the then-current Apple chip, respectively.</p> <p>The founder and then-CEO of Discourse considers Qualcomm's mobile performance embarrassing and finds this so offensive that he thinks Qualcomm engineers should all lose their jobs for delivering <abbr title="I think it could be reasonable to cite a lower number, but I'm using the number he cited, not what I would cite">74% to 85% of the performance of Apple</abbr>. Apple has what I consider to be an all-time great performance team. Reasonable people could disagree on that, but one has to at least think of them as a world-class team. So, producing a product with <abbr title="recall that, on a Tecno Spark 8, Discourse is 33 times slower than MyBB, which isn't particularly optimized for performance">74% to 85% of an all-time-great team is considered an embarrassment worthy of losing your job</abbr>.</p> <p>There are two attitudes on display here which I see in a lot of software folks. First, that CPU speed is infinite and one shouldn't worry about CPU optimization. And second, that gigantic speedups from hardware should be expected and the only reason hardware engineers wouldn't achieve them is due to spectacular incompetence, so the slow software should be blamed on hardware engineers, not software engineers. Donald Knuth expressed a similar sentiment in</p> <blockquote> <p>I might as well flame a bit about my personal unhappiness with the current trend toward multicore architecture. To me, it looks more or less like the hardware designers have run out of ideas, and that they’re trying to pass the blame for the future demise of Moore’s Law to the software writers by giving us machines that work faster only on a few key benchmarks! I won’t be surprised at all if the whole multiithreading idea turns out to be a flop, worse than the "Itanium" approach that was supposed to be so terrific—until it turned out that the wished-for compilers were basically impossible to write. Let me put it this way: During the past 50 years, I’ve written well over a thousand programs, many of which have substantial size. I can’t think of even five of those programs that would have been enhanced noticeably by parallelism or multithreading. Surely, for example, multiple processors are no help to TeX ... I know that important applications for parallelism exist—rendering graphics, breaking codes, scanning images, simulating physical and biological processes, etc. But all these applications require dedicated code and special-purpose techniques, which will need to be changed substantially every few years. Even if I knew enough about such methods to write about them in TAOCP, my time would be largely wasted, because soon there would be little reason for anybody to read those parts ... The machine I use today has dual processors. I get to use them both only when I’m running two independent jobs at the same time; that’s nice, but it happens only a few minutes every week.</p> </blockquote> <p>In the case of Discourse, a hardware engineer is an embarrassment not deserving of a job if they can't hit 90% of the performance of an all-time-great performance team but, as a software engineer, delivering 3% the performance of a non-highly-optimized application like MyBB is no problem. In Knuth's case, hardware engineers gave programmers a 100x performance increase every decade for decades with little to no work on the part of programmers. The moment this slowed down and programmers had to adapt to take advantage of new hardware, hardware engineers were "all out of ideas", but learning a few "new" (1970s and 1980s era) ideas to take advantage of current hardware would be a waste of time. And <a href="https://www.patreon.com/posts/54329188">we've previously discussed Alan Kay's claim that hardware engineers are "unsophisticated" and "uneducated" and aren't doing "real engineering" and how we'd get a 1000x speedup if we listened to Alan Kay's "sophisticated" ideas</a>.</p> <p>It's fairly common for programmers to expect that hardware will solve all their problems, and then, when that doesn't happen, pass the issue onto the user, explaining why the programmer need't do anything to help the user. A question one might ask is how much performance improvement programmers have given us. There are cases of algorithmic improvements that result in massive speedups but, as we noted above, Discourse, the fastest growing forum software today, seems to have given us an approximately <code>1000000x</code> slowdown in performance.</p> <p>Another common attitude on display above is the idea that users who aren't wealthy don't matter. When asked if 100% of users are on iOS, the founder of Discourse says "The influential users who spend money tend to be, I’ll tell you that". We see the same attitude all over comments on <a href="https://tonsky.me/blog/js-bloat/">Tonsky's JavaScript Bloat post</a>, with people expressing <a href="https://danluu.com/cocktail-ideas/">cocktail-party sentiments</a> like "Phone apps are hundreds of megs, why are we obsessing over web apps that are a few megs? Starving children in Africa can download Android apps but not web apps? Come on and "surely no user of gitlab would be poor enough to have a slow device, let's be serious" (paraphrased for length).</p> <p>But when we look at the size of apps that are downloaded in Africa, we see that people who aren't on high-end devices use apps like Facebook Lite (a couple megs) and commonly use apps that are a single digit to low double digit number of megabytes. There are multiple reasons app makers care about their app size. One is just the total storage available on the phone; if you watch real users install apps, they often have to delete and uninstall things to put a new app on, so the smaller size is both easier to to install and has a lower chance of being uninstalled when the user is looking for more space. Another is that, if you look at data on app size and usage (I don't know of any public data on this; please pass it along if you have something public I can reference), when large apps increase the size and memory usage, they get more crashes, which drives down user retention, growth, and engagement and, conversely, when they optimize their size and memory usage, they get fewer crashes and better user retention, growth, and engagement.</p> <p>On the bit about no programmers having slow devices, I know plenty of people who are using hand-me-down devices that are old and slow. Many of them aren't even really poor; they just don't see why (for example) their kid needs a super fast device, and they don't understand how much of the modern web works poorly on slow devices. After all, the "slow" device can play 3d games and (with the right OS) compile codebases like Linux or Chromium, so why shouldn't the device be able to interact with a site like gitlab?</p> <p>Contrary to the claim from the founder of Discourse that, within years, every Android user will be on some kind of super fast Android device, it's been six years since his comment and it's going to be at least a decade before almost everyone in the world who's using a phone has a high-speed device and this could easily take two decades or more. If you look up marketshare stats for Discourse, it's extremely successful; it appears to be the fastest growing forum software in the world by a large margin. The impact of having the fastest growing forum software in the world created by an organization whose then-leader was willing to state that he doesn't really care about users who aren't "influential users who spend money", who don't have access to "infinite CPU speed", is that a lot of forums are now inaccessible to people who don't have enough wealth to buy a device with effectively infinite CPU.</p> <p>If the founder of Discourse were an anomaly, this wouldn't be too much of a problem, but he's just verbalizing the implicit assumptions a lot of programmers have, which is why we see that so many modern websites are unusable if you buy the income-adjusted equivalent of a new, current generation, iPhone in a low-income country.</p> <p><i>Thanks to Yossi Kreinen, Fabian Giesen, John O'Nolan, Joseph Scott, @acidshill, Alex Russell, Tobias Marschner, and David Turner for comments/corrections/discussion.</i></p> <h3 id="appendix-gaming-lcp">Appendix: gaming LCP</h3> <p>We noted above that we used <code>LCP*</code> and not <code>LCP</code>. This is because <code>LCP</code> basically measures when the largest change happens. When this metric was not deliberately gamed in ways that don't benefit the user, this was a great metric, but this metric has become less representative of the actual user experience as more people have gamed it. In the less blatant cases, people do small optimizations that improve <code>LCP</code> but barely improve or don't improve the actual user experience.</p> <p>In the more blatant cases, developers will deliberately flash a very large change on the page as soon as possible, generally a loading screen that has no value to the user (actually negative value because doing this increases the total amount of work done and the total time it takes to load the page) and then they carefully avoid making any change large enough that any later change would get marked as the <code>LCP</code>.</p> <p>For the same reason <a href="https://en.wikipedia.org/wiki/Volkswagen_emissions_scandal">that VW didn't publicly discuss how it was gaming its emissions numbers</a>, developers tend to shy away from discussing this kind of <code>LCP</code> optimization in public. An exception to this is Discourse, where <a href="https://meta.discourse.org/t/introducing-discourse-splash-a-visual-preloader-displayed-while-site-assets-load/232003" rel="nofollow">they publicly announced this kind of <code>LCP</code> optimization, with comments from their devs and the then-CTO (now CEO)</a>, noting that their new "Discourse Splash" feature hugely reduced <code>LCP</code> for sites after they deployed it. And then developers ask why their <code>LCP</code> is high, the standard advice from Discourse developers is to keep elements smaller than the "Discourse Splash", so that the <code>LCP</code> timestamp is computed from this useless element that's thrown up to optimize <code>LCP</code>, as opposed to having the timestamp be computed from any actual element that's relevant to the user. <a href="https://meta.discourse.org/t/theme-components-and-largest-contentful-paint-lcp/258680" rel="nofollow">Here's a typical, official, comment from Discourse</a></p> <blockquote> <p>If your banner is larger than the element we use for the "Introducing Discourse Splash - A visual preloader displayed while site assets load" you gonna have a bad time for LCP.</p> </blockquote> <p>The official response from Discourse is that you should make sure that your content doesn't trigger the <code>LCP</code> measurement and that, instead, our loading animation timestamp is what's used to compute `LCP.</p> <p>The sites with the most extreme ratio of <code>LCP</code> of useful content vs. Chrome's measured <code>LCP</code> were:</p> <ul> <li>Wix <ul> <li><code>M3</code>: <code>6</code></li> <li><code>M1</code>: <code>12</code></li> <li><code>Tecno Spark 8C</code>: <code>3</code></li> <li><code>Itel P32</code>: <code>N/A</code> <code>(FAIL)</code></li> </ul></li> <li>Discourse: <ul> <li><code>M3</code>: <code>10</code></li> <li><code>M1</code>: <code>12</code></li> <li><code>Tecno Spark 8C</code>: <code>4</code></li> <li><code>Itel P32</code>: <code>N/A</code> <code>(FAIL)</code></li> </ul></li> </ul> <p>Although we haven't discussed the gaming of other metrics, it appears that some websites also game other metrics and "optimize" them even when this has no benefit to users.</p> <h3 id="appendix-designing-for-low-performance-devices">Appendix: designing for low performance devices</h3> <p>When using slow devices or any device with low bandwidth and/or poor connectivity, the best experiences, by far, are generally the ones that load a lot of content at once into a static page. If the images have proper width and height attributes and alt text, that's very helpful. Progressive images (as in progressive jpeg) isn't particularly helpful.</p> <p>On a slow device with high bandwidth, any lightweight, static, page works well, and lightweight dynamic pages can work well if designed for performance. Heavy, dynamic, pages are doomed unless the page weight doesn't cause the page to be complex.</p> <p>With low bandwidth and/or poor connectivity, lightweight pages are fine. With heavy pages, the best experience I've had is when I trigger a page load, go do something else, and then come back when it's done (or at least the HTML and CSS are done). I can then open each link I might want to read in a new tab, and then do something else while I wait for those to load.</p> <p>A lot of the optimizations that modern websites do, such as partial loading that causes more loading when you scroll down the page, and the concomitant hijacking of search (because the browser's built in search is useless if the page isn't fully loaded) causes the interaction model that works to stop working and makes pages very painful to interact with.</p> <p>Just for example, a number of people have noted that Substack performs poorly for then because it does partial page loads. <a href="https://danluu.com/substack.mp4">Here's a video by @acidshill of what it looks like to load a Substack article and then scroll on an iPhone 8</a>, where the post has a fairly fast <code>LCP</code>, but if you want to scroll past the header, you have to wait <code>6s</code> for the next page to load, and then on scrolling again, you have to wait maybe another <code>1s</code> to <code>2s</code>:</p> <p>As an example of the opposite approach, I tried loading some fairly large plain HTML pages, such as <a href="https://danluu.com/diseconomies-scale/">https://danluu.com/diseconomies-scale/</a> (<code>0.1 MB wire</code> / <code>0.4 MB raw</code>) and <a href="https://danluu.com/threads-faq/">https://danluu.com/threads-faq/</a> (<code>0.4 MB wire</code> / <code>1.1 MB raw</code>) and these were still quite usable for me even on slow devices. <code>1.1 MB</code> seems to be larger than optimal and breaking that into a few different pages would be better on a low-end devices, but a single page with <code>1.1 MB</code> of text works much better than most modern sites on a slow device. While you can get into trouble with HTML pages that are so large that browsers can't really handle them, for pages with a normal amount of content, it generally isn't until you have <a href="https://nolanlawson.com/2023/01/17/my-talk-on-css-runtime-performance/">complex CSS payloads</a> or JS that the pages start causing problems for slow devices. Below, we test pages that are relatively simple, some of which have a fair amount of media (<code>14 MB</code> in one case) and find that these pages work ok, as long as they stay simple.</p> <h3 id="appendix-articles-on-web-performance-issues">Appendix: articles on web performance issues</h3> <ul> <li>2015: Maciej Cegłowski: <a href="https://idlewords.com/talks/website_obesity.htm">The Website Obesity Crisis</a> <ul> <li>Size: <code>1.0 MB</code> / <code>1.1 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.9s</code> / <code>1.4s</code> <ul> <li>Scrolling a bit jerky, images take a little bit of time to appear if scrolling very quickly (jumping halfway down page from top), but delay is below what almost any user would perceive when scrolling a normal distance.</li> </ul></li> </ul></li> <li>2015: Nate Berkopec: <a href="https://www.speedshop.co/2015/11/05/page-weight-doesnt-matter.html">Page Weight Doesn't Matter</a> <ul> <li>Size: <code>80 kB</code> / <code>0.2 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.8s</code> / <code>0.7s</code> <ul> <li>Does lazy loading, page downloads <code>650 kB</code> / <code>1.8 MB</code> if you scroll through the entire page, but scrolling is only a little jerk and the lazy loading doesn't cause delays. Probably the only page I've tried that does lazy loading in a way that makes the experience better and not worse on a slow device; I didn't test on a slow connection, where this would still make the experience worse.</li> </ul></li> <li><code>Itel P32</code>: <code>1.1s</code> / <code>1s</code> <ul> <li>Scrolling basically unusable; scroll extremely jerky and moves a random distance, often takes over <code>1s</code> for text to render when scrolling to new text; can be much worse with images that are lazy loaded. Even though this is the implementation of lazy loading I've seen in the wild, the <code>Itel P32</code> still can't handle it.</li> </ul></li> </ul></li> <li>2017: Dan Luu: <a href="https://danluu.com/web-bloat/">How web bloat impacts users with slow connections</a> <ul> <li>Size: <code>14 kB</code> / <code>57 kB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.5s</code> / <code>0.3s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>:<code>0.7s</code> / <code>0.5 s</code></li> </ul></li> <li>2017-2024+: Alex Russell: <a href="https://infrequently.org/series/performance-inequality/">The Performance Inequality Gap (series)</a> <ul> <li>Size: <code>82 kB</code> / <code>0.1 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.5s</code> / <code>0.4s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>: <code>0.7s</code> / <code>0.4s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> </ul></li> <li>2024: Nikita Prokopov (Tonsky): <a href="https://tonsky.me/blog/js-bloat/">JavaScript Bloat in 2024</a> <ul> <li>Size: <code>14 MB</code> / <code>14 MB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.8s</code> / <code>1.9s</code> <ul> <li>When scrolling, it takes a while for images to show up (500ms or so) and the scrolling isn't smooth, but it's not jerky enough that it's difficult to scroll to the right place.</li> </ul></li> <li><code>Itel P32</code>: <code>2.5s</code> / <code>3s</code> <ul> <li>Scrolling isn't smooth. Scrolling accurately is a bit difficult, but can generally scroll to where you want if very careful. Generally takes a bit more than <code>1s</code> for new content to appear when you scroll a significant distance.</li> </ul></li> </ul></li> <li>2024: Dan Luu: <a href="https://danluu.com/slow-device/">This post</a> <ul> <li>Size: <code>25 kB</code> / <code>74 kB</code></li> <li><code>Tecno Spark 8C</code>: <code>0.6s</code> / <code>0.5s</code> <ul> <li>Scrolling and interaction work fine.</li> </ul></li> <li><code>Itel P32</code>: <code>1.3s</code> / <code>1.1s</code> <ul> <li>Scrolling and interaction work fine, although I had to make a change for this to be the case —&nbsp;this doc originally had an embedded video, which the <code>Itel P32</code> couldn't really handle. <ul> <li>Note that, while these numbers are worse than the numbers for "Page Weight Doesn't Matter", this page is usable after load, which that other page isn't beacuse it execute some kind of lazy loading that's too complex for this phone to handle in a reasonable timeframe.</li> </ul></li> </ul></li> </ul></li> </ul> <p>Just as an aside, something I've found funny for a long time is that I get quite a bit of hate mail about the styling on this page (and a similar volume of appreciation mail). By hate mail, I don't mean polite suggestions to change things, I mean the equivalent of road rage, but for web browsing; web rage. I know people who run sites that are complex enough that they're unusable by a significant fraction of people in the world. How come people are so incensed about the styling of this site and, proportionally, basically don't care at all that the web is unusable for so many people?</p> <p>Another funny thing here is that the people who appreciate the styling generally appreciate that the site doesn't override any kind of default styling, letting you make the width exactly what you want (by setting your window size how you want it) and it also doesn't override any kind of default styling you apply to sites. The people who are really insistent about this want everyone to have some width limit they prefer, some font they prefer, etc., but it's always framed in a way as if they don't want it, it's really for the benefit of people at large even though accommodating the preferences of the web ragers would directly oppose the preferences of people who prefer (just for example) to be able to adjust the text width by adjusting their window width.</p> <p>Until I pointed this out tens of times, this iteration would usually start with web ragers telling me that "studies show" that narrower text width is objectively better, but on reading every study that exists on the topic that I could find, I didn't find this to be the case. Moreover, on asking for citations, it's clear that people saying this generally hadn't read any studies on this at all and would sometimes hastily send me a study that they did not seem to have read. When I'd point this out, people would then change their argument to how studies can't really describe the issue (odd that they'd cite studies in the first place), although one person cited a book to me (which I read and they, apparently, had not since it also didn't support their argument) and then move to how this is what everyone wants, even though that's clearly not the case, both from the comments I've gotten as well as the data I have from when I made the change.</p> <p>Web ragers who have this line of reasoning generally can't seem to absorb the information that their preferences are not universal and will insist that they regardless of what people say they like, which I find fairly interesting. On the data, when I switched from Octopress styling (at the time, the most popular styling for programming bloggers) to the current styling, I got what appeared to be a causal an increase in traffic and engagement, so it appears that not only do people who write me appreciation mail about the styling like the styling, the overall feeling of people who don't write to me appears to be that the site is fine and apparently more appealing than standard programmer blog styling. When I've noted this, people either become further invested in the idea that their preferences are universal and people who think they have other preferences are wrong and reply with total nonsense.</p> <p>I would understand this kind of anger about how much the web has been made unusable for people who aren't, by global standards, fairly well off, but it's curious that so many people find a site that is accessible but accedes to other people's preferences to be a topic worth sending so many angry rants about, as well as one where it's worth fabricating "objective" evidence for their opinion.</p> <h3 id="appendix-empathy-for-non-rich-users">Appendix: empathy for non-rich users</h3> <p>Something I've observed over time, as programming has become more prestigious and more lucrative, is <a href="https://mastodon.social/@danluu/109901711437753852">that people have tended to come from wealthier backgrounds</a> and have less exposure to people with different income levels. An example we've discussed before, is at a well-known, prestigious, startup that has a very left-leaning employee base, where everyone got rich, on a discussion about the covid stimulus checks, in a slack discussion, a well meaning progressive employee said that it was pointless because people would just use their stimulus checks to buy stock. This person had, apparently, never talked to any middle-class (let alone poor) person about where their money goes or looked at the data on who owns equity. And that's just looking at American wealth. When we look at world-wide wealth, the general level of understanding is much lower. People seem to really underestimate the dynamic range in wealth and income across the world. From having talked to quite a few people about this, a lot of people seem to have mental buckets for "poor by American standards" (buys stock with stimulus checks) and "poor by worldwide standards" (maybe doesn't even buy stock), but the range of poverty in the world dwarfs the range of poverty in America to an extent that not many wealthy programmers seem to realize.</p> <p>Just for example, <a href="https://mastodon.social/@danluu/109537302116865694">in this discussion how lucky I was (in terms of financial opportunities) that my parents made it to America</a>, someone mentioned that it's not that big a deal because they had great financial opportunities in Poland. For one thing, with respect to the topic of the discussion, the probability that someone will end up with a high-paying programming job (senior staff eng at a high-paying tech company) or equivalent, I suspect that, when I was born, being born poor in the U.S. gives you better odds than being fairly well off in Poland, but I could believe the other case as well if presented with data. But if we're comparing Poland v. U.S. to Vietnam v. U.S., if I spend <abbr title="so, these probably aren't the optimal numbers one would use for a comparison, but I think they're good enough for this purpose">15 seconds looking up rough wealth numbers for these countries</abbr> in the year I was born, the GDP/capita ratio of U.S. : Poland was ~8:1, whereas it was ~50 : 1 for Poland : Vietnam. The difference in wealth between Poland and Vietnam was roughly the square of the difference between the U.S. and Poland, so Poland to Vietnam is roughly equivalent to Poland vs. some hypothetical country that's richer than the U.S. by the amount that the U.S. is richer than Poland. These aren't even remotely comparable, but a lot of people seem to have this mental model that there's "rich countries" and "not rich countries" and "not rich countries" are all roughly in the same bucket. GDP/capita isn't ideal, but it's easier to find than percentile income statistics; the quick search I did also turned up that annual income in Vietnam them was something like $200-$300 a year. Vietnam was also going through the tail end of a famine whose impacts are a bit difficult to determine because statistics here seem to be gamed, but if you believe the mortality rate statistics, the famine caused total overall mortality rate to jump to double the normal baseline<sup id="fnref:L"><a rel="footnote" href="#fn:L">1</a></sup>.</p> <p>Of course, at the time, the median person in a low-income country wouldn't have had a computer, let alone internet access. But, today it's fairly common for people in low-income countries to have devices. Many people either don't seem to realize this or don't understand what sorts of devices a lot of these folks use.</p>  <p>On the Discourse founder's comments on iOS vs. Android marketshare, Fabian notes</p> <blockquote> <p>In the US, according to the most recent data I could find (for 2023), iPhones have around 60% marketshare. In the EU, it's around 33%. This has knock-on effects. Not only do iOS users skew towards the wealthier end, they also skew towards the US.</p> <p>There's some secondary effects from this too. For example, in the US, iMessage is very popular for group chats etc. and infamous for interoperating very poorly with Android devices in a way that makes the experience for Android users very annoying (almost certainly intentionally so).</p> <p>In the EU, not least because Android is so much more prominent, iMessage is way less popular and anecdotally, even iPhone users among my acquaintances who would probably use iMessage in the US tend to use WhatsApp instead.</p> <p>Point being, globally speaking, recent iOS + fast Internet is even more skewed towards a particular demographic than many app devs in the US seem to be aware.</p> </blockquote> <p>And on the comment about mobile app vs. web app sizes, Fabian said:</p> <blockquote> <p>One more note from experience: apps you install when you install them, and generally have some opportunity to hold off on updates while you're on a slow or metered connection (or just don't have data at all).</p> <p>Back when I originally got my US phone, I had no US credit history and thus had to use prepaid plans. I still do because it's fine for what I actually use my phone for most of the time, but it does mean that when I travel to Germany once a year, I don't get data roaming at all. (Also, phone calls in Germany cost me $1.50 apiece, even though T-Mobile is the biggest mobile provider in Germany - though, of course, not T-Mobile US.)</p> <p>Point being, I do get access to free and fast Wi-Fi at T-Mobile hotspots (e.g. major train stations, airports etc.) and on inter-city trains that have them, but I effectively don't have any data plan when in Germany at all.</p> <p>This is completely fine with mobile phone apps that work offline and sync their data when they have a connection. But web apps are unusable while I'm not near a public Wi-Fi.</p> <p>Likewise I'm fine sending an email over a slow metered connection via the Gmail app, but I for sure wouldn't use any web-mail client that needs to download a few MBs worth of zipped JS to do anything on a metered connection.</p> <p>At least with native app downloads, I can prepare in advance and download them while I'm somewhere with good internet!</p> </blockquote> <p>Another comment from Fabian (this time paraphrased since this was from a conversation), is that people will often justify being quantitatively hugely slower because there's a qualitative reason something should be slow. One example he gave was that screens often take a long time to sync their connection and this is justified because there are operations that have to be done that take time. For a long time, these operations would often take seconds. Recently, a lot of displays sync much more quickly because Nvidia specifies how long this can take for something to be "G-Sync" certified, so display makers actually do this in a reasonable amount of time now. While it's true that there are operations that have to be done that take time, there's no fundamental reason they should take as much time as they often used to. Another example he gave was on how someone was justifying how long it took to read thousands of files because the operation required a lot of syscalls and "syscalls are slow", which is a qualitatively true statement, but if you look at the actual cost of a syscall, in the case under discussion, the cost of a syscall was many orders of magnitude from being costly enough to be a reasonable explanation for why it took so long to read thousands of files.</p> <p>On this topic, when people point out that a modern website is slow, someone will generally respond with the qualitative defense that the modern website has these great features, which the older website is lacking. And while it's true that (for example) Discourse has features that MyBB doesn't, it's hard to argue that its feature set justifies being <code>33x</code> slower.</p> <h3 id="appendix-experimental-details">Appendix: experimental details</h3> <p>With the exception of danluu.com and, arguably, HN, for each site, I tried to find the "most default" experience. For example, for WordPress, this meant a demo blog with the current default theme, twentytwentyfour. In some cases, this may not be the most likely thing someone uses today, e.g., for Shopify, I looked at the first thing that theme they give you when you browse their themes, but I didn't attempt to find theme data to see what the most commonly used theme is. For this post, I wanted to do all of the data collection and analysis as a short project, something that takes less than a day, so there were a number of shortcuts like this, which will be described below. I don't think it's wrong to use the first-presented Shopify theme in a decent fraction of users will probably use the first-presente theme, but that is, of course, less representative than grabbing whatever the most common theme is and then also testing many different sites that use that theme to see how real-world performance varies when people modify the theme for their own use. If I worked for Shopify or wanted to do competitive analysis on behalf of a competitor, I would do that, but for a one-day project on how large websites impact users on low-end devices, the performance of Shopify demonstrated here seems ok.</p> <p>For the tests on laptops, I tried to have the laptop at ~60% battery, not plugged in, and the laptop was idle for enough time to return to thermal equilibrium in a room at 20°C, so pages shouldn't be impacted by prior page loads or other prior work that was happening on the machine.</p> <p>For the mobile tests, the phones were at ~100% charge and plugged in, and also previously at 100% charge so the phones didn't have any heating effect you can get from rapidly charging. As noted above, these tests were formed with <code>1Gbps</code> WiFi. No other apps were running, the browser had no other tabs open, and the only apps that were installed on the device, so no additional background tasks should've been running other than whatever users are normally subject to by the device by default. A real user with the same device is going to see worse performance than we measured here in almost every circumstance.</p> <p>Sizes were all measured on mobile, so in cases where different assets are loaded on mobile vs. desktop, the we measured the mobile asset sizes. <code>CPU</code> was measured as CPU time on the main thread (I did also record time on other threads for sites that used other threads, but didn't use this number; if <code>CPU</code> were a metric people wanted to game, time on other threads would have to be accounted for to prevent sites from trying to offload as much work as possible to other threads, but this isn't currently an issue and time on main thread is more directly correlated to usability than sum of time across all threads, and the metric that would work for gaming is less legible with no upside for now).</p> <p>For WiFi speeds, speed tests had the following numbers:</p> <ul> <li><code>M3 Max</code> <ul> <li>Netflix (fast.com) <ul> <li>Download: <code>850 Mbps</code></li> <li>Upload: <code>840 Mbps</code></li> <li>Latency (unloaded / loaded): <code>3ms</code> / <code>8ms</code></li> </ul></li> <li>Okta <ul> <li>Download: <code>900 Mbps</code></li> <li>Upload: <code>840 Mbps</code></li> <li>Latency (unloaded / download / upload): <code>3ms</code> / <code>8ms</code> / <code>13ms</code></li> </ul></li> </ul></li> <li><code>Tecno Spark 8C</code> <ul> <li>Netflix (fast.com) <ul> <li>Download: <code>390 Mbps</code></li> <li>Upload: <code>210 Mbps</code></li> <li>Latency (unloaded / loaded): <code>2ms</code> / <code>30ms</code></li> </ul></li> <li>Okta <ul> <li>Okta web app fails, can't see results</li> </ul></li> </ul></li> <li><code>Itel P32</code> <ul> <li>Netflix <ul> <li>Download: <code>44 Mbps</code></li> <li>Upload: test fails to work (sends one chunk of data and then hangs, sending no more data)</li> <li>Latency (unloaded / loaded): <code>4ms</code> / <code>400ms</code></li> </ul></li> <li>Okta <ul> <li>Download: <code>45 Mbps</code></li> <li>Upload: test fails to work</li> <li>Latency: test fails to display latency</li> </ul></li> </ul></li> </ul> <p>One thing to note is that the <code>Itel P32</code> doesn't really have the ability to use the bandwidth that it nominally has. Looking at the top Google reviews, none of them mention this. <a href="https://www.nairaland.com/4628841/itel-p32-review-great-those" rel="nofollow">The first review reads</a></p> <blockquote> <p>Performance-wise, the phone doesn’t lag. It is powered by the latest Android 8.1 (GO Edition) ... we have 8GB+1GB ROM and RAM, to run on a power horse of 1.3GHz quad-core processor for easy multi-tasking ... I’m impressed with the features on the P32, especially because of the price. I would recommend it for those who are always on the move. And for those who take battery life in smartphones has their number one priority, then P32 is your best bet.</p> </blockquote> <p><a href="https://techjaja.com/itel-p32-review-dual-camera-smartphone-alarming-price-tag/" rel="nofollow">The second review reads</a></p> <blockquote> <p>Itel mobile is one of the leading Africa distributors ranking 3rd on a continental scale ... the light operating system acted up to our expectations with no sluggish performance on a 1GB RAM device ... fairly fast processing speeds ... the Itel P32 smartphone delivers the best performance beyond its capabilities ... at a whooping UGX 330,000 price tag, the Itel P32 is one of those amazing low-range like smartphones that deserve a mid-range flag for amazing features embedded in a single package.</p> </blockquote> <p><a href="https://pctechmag.com/2018/08/itel-p32-full-review-much-more-than-just-a-budget-entry-level-smartphone/" rel="nofollow">The third review reads</a></p> <blockquote> <p>"Much More Than Just a Budget Entry-Level Smartphone ... Our full review after 2 weeks of usage ... While switching between apps, and browsing through heavy web pages, the performance was optimal. There were few lags when multiple apps were running in the background, while playing games. However, the overall performance is average for maximum phone users, and is best for average users [screenshot of game] Even though the game was skipping some frames, and automatically dropped graphical details it was much faster if no other app was running on the phone.</p> </blockquote> <p>Notes on sites:</p> <ul> <li>Wix <ul> <li>www.wix.com/website-template/view/html/3173?originUrl=https%3A%2F%2Fwww.wix.com%2Fwebsite%2Ftemplates%2Fhtml%2Fmost-popular&amp;tpClick=view_button&amp;esi=a30e7086-28db-4e2e-ba22-9d1ecfbb1250: this was the first entry when I clicked to get a theme</li> <li><code>LCP</code> was misleading on every device</li> <li>On the <code>Tecno Spark 8C</code>, scrolling never really works. It's very jerky and this never settles down</li> <li>On the <code>Itel P32</code>, the page fails non-deterministically (different errors on different loads); it can take quite a while to error out; it was <code>23s</code> on the first run, with the CPU pegged for <code>28s</code></li> </ul></li> <li>Patreon <ul> <li>www.patreon.com/danluu: used my profile where possible</li> <li>Scrolling on Patreon and finding old posts is so painful that I maintain <a href="https://danluu.com/#pt">my own index of my Patreon posts</a> so that I can find my old posts without having to use Patreon.</li> </ul></li> <li>Threads <ul> <li>threads.net/danluu.danluu: used my profile where possible</li> <li>On the <code>Itel P32</code>, this technically doesn't load correctly and could be marked as <code>FAIL</code>, but it's close enough that I counted it. The thing that's incorrect is that profile photos have a square box around then <ul> <li>However, as with the other heavy pages, interacting with the page doesn't really work and the page is unusable, but this appears to be for the standard performance reasons and not because the page failed to render</li> </ul></li> </ul></li> <li>Twitter <ul> <li>twitter.com/danluu: used my profile where possible</li> </ul></li> <li>Discourse <ul> <li>meta.discourse.org: this is what turned up when I searched for an official forum.</li> <li>As discussed above, the <code>LCP</code> is highly gamed and basically meaningless. We linked to a post where the Discourse folks note that, on slow loads, they put a giant splash screen up at <code>2s</code> to cap the <code>LCP</code> at <code>2s</code>. Also notable is that, on loads that are faster than the 2s, the <code>LCP</code> is also highly gamed. For example, on the <code>M3 Max</code> with low-latency <code>1Gbps</code> internet, the <code>LCP</code> was reported as <code>115ms</code>, but the page loads actual content at <code>1.1s</code>. This appears to use the same fundamental trick as "Discourse Splash", in that it paints a huge change onto the screen and then carefully loads smaller elements to avoid having the actual page content detected as the <code>LCP</code>.</li> <li>On the <code>Tecno Spark 8C</code>, scrolling is unpredictable and can jump too far, triggering loading from infinite scroll, which hangs the page for <code>3s-10s</code>. Also, the entire browser sometimes crashes if you just let the browser sit on this page for a while.</li> <li>On the <code>Itel P32</code>, an error message is displayed after <code>7.5s</code></li> </ul></li> <li>Bluesky <ul> <li>bsky.app/profile/danluu.com</li> <li>Displays a blank screen on the <code>Itel P32</code></li> </ul></li> <li>Squarespace <ul> <li>cedar-fluid-demo.squarespace.com: this was the second theme that showed up when I clicked themes to get a theme; the first was one called "Bogart", but that was basically a "coming soon" single page screen with no content, so I used the second theme instead of the first one.</li> <li>A lot of errors and warnings in the console with the <code>Itel P32</code>, but the page appears to load and work, although interacting with it is fairly slow and painful</li> <li><code>LCP</code> on the <code>Tecno Spark 8C</code> was significantly before the page content actually loaded</li> </ul></li> <li>Tumblr <ul> <li>www.tumblr.com/slatestarscratchpad: used this because I know this tubmlr exists. I don't read a lot of tumblers (maybe three or four), and this one seemed like the closest thing to my blog that I know of on tumblr.</li> <li>This page fails on the <code>Itel P32</code>, but doesn't <code>FAIL</code>. The console shows that the JavaScript errors out, but the page still works fine (I tried scrolling, clicking links, etc., and these all worked), so you can actually go to the post you want and read it. The JS error appears to have made this page load much more quickly than it other would have and also made interacting with the page after it loaded fairly zippy.</li> </ul></li> <li>Shopify <ul> <li>themes.shopify.com/themes/motion/styles/classic/preview?surface_detail=listing&amp;surface_inter_position=1&amp;surface_intra_position=1&amp;surface_type=all: this was the first theme that showed up when I looked for themes</li> <li>On the first <code>M3/10</code> run, Chrome dev tools reported a nonsensical <code>697s</code> of CPU time (the run completed in a normal amount of time, well under <code>697s</code> or even <code>697/10s</code>. This run was ignored when computing results.</li> <li>On the <code>Itel P32</code>, the page load never completes and it just shows a flashing cursor-like image, which is deliberately loaded by the theme. On devices that load properly, the flashing cursor image is immediately covered up by another image, but that never happens here.</li> <li>I wondered if it wasn't fair to use this example theme because there's some stuff on the page that lets you switch theme styles, so I checked out actual uses of the theme (the page that advertises the theme lists users of the theme). I tried the first two listed real examples and they were both much slower than this demo page.</li> </ul></li> <li>Reddit <ul> <li>reddit.com</li> <li>Has an unusually low <code>LCP*</code> compared to how long it takes for the page to become usable. Although not measured in this test, I generally find the page slow and sort of unusable on Intel Macbooks which are, by historical standards, extremely fast computers (unless I use old.reddit.com)</li> </ul></li> <li>Mastodon <ul> <li>mastodon.social/@danluu: used my profile where possible</li> <li>Fails to load on <code>Itel P32</code>, just gives you a blank screen. Due to how long things generally take on the <code>Itel P32</code>, it's not obvious for a while if the page is failing or if it's just slow</li> </ul></li> <li>Quora <ul> <li>www.quora.com/Ever-felt-like-giving-up-on-your-dreams-How-did-you-come-out-of-it: I tried googling for quora + the username of a metafilter user who I've heard is now prolific on Quora. Rather than giving their profile page, Google returned this page, which appears to have nothing to do with the user I searched for. So, this isn't comparable to the social media profiles, but getting a random irrelevant Quora result from Google is how I tend to interact with Quora, so I guess this is representative of my Quora usage.</li> <li>On the <code>Itel P32</code>, the page stops executing scripts at some point and doesn't fully load. This causes it to fail to display properly. Interacting with the page doesn't really work either.</li> </ul></li> <li>Substack <ul> <li>Used thezvi.substack.com because I know Zvi has a substack and writes about similar topics.</li> </ul></li> <li>vBulletin: <ul> <li>forum.vbulletin.com: this is what turned up when I searched for an official forum.</li> </ul></li> <li>Medium <ul> <li>medium.com/swlh: I don't read anything on Medium, so I googled for programming blogs on Medium and this was the top hit. From looking at the theme, it doesn't appear to be unusually heavy or particularly customized for a Medium blog. Since it appears to be widely read and popular, it's more likely to be served from a CDN and than some of the other blogs here.</li> <li>On a run that wasn't a benchmark reference run, on the <code>Itel P32</code>, I tried scrolling starting 35s after loading the page. The delay to scroll was <code>5s-8s</code> and scrolling moved an unpredictable amount, making the page completely unusable. This wasn't marked as a <code>FAIL</code> in the table, but one could argue that this should be a <code>FAIL</code> since the page is unusable.</li> </ul></li> <li>Ghost <ul> <li>source.ghost.io because this is the current default Ghost theme and it was the first example I found</li> </ul></li> <li>Wordpress <ul> <li>2024.wordpress.net because this is the current default wordpress theme and this was the first example of it I found</li> </ul></li> <li>XenForo <ul> <li>xenforo.com/community/: this is what turned up when I searched for an official forum</li> <li>On the <code>Itel P32</code>, the layout is badly wrong and page content overlaps itself. There's no reasonable way to interact with the element you want because of this, and reading the text requires reading text that's been overprinted multiple times.</li> </ul></li> <li>Wordpress (old) <ul> <li>Used thezvi.wordpress.com because it has the same content as Zvi's substack, and happens to be on some old wordpress theme that used to be a very common choice</li> </ul></li> <li>phpBB <ul> <li>www.phpbb.com/community/index.php: this is what turned up when I searched for an official forum.</li> </ul></li> <li>MyBB <ul> <li>community.mybb.com: this is what turned up when I searched for an official forum.</li> <li>Site doesn't serve up a mobile version. In general, I find the desktop version of sites to be significantly better than the mobile version when on a slow device, so this works quite well, although they're likely penalized by Google for this.</li> </ul></li> <li>HN <ul> <li>news.ycombinator.com</li> <li>In principle, HN should be the slowest social media site or link aggregator because it's written in a custom Lisp that isn't highly optimized and the code was originally written with brevity and cleverness in mind, which generally gives you fairly poor performance. However, that's only poor relative to what you'd get if you were writing high-performance code, which is not a relevant point of comparison here.</li> </ul></li> <li>danluu.com <ul> <li>Self explanatory</li> <li>This currently uses a bit less CPU than HN, but I expect this to eventually use more CPU as the main page keeps growing. At the moment, this page has 176 links to 168 articles vs. HN's 199 links to 30 articles but, barring an untimely demise, this page should eventually have more links than HN. <ul> <li>As noted above, I find that pagination for such small pages makes the browsing experience much worse on slow devices or with bad connections, so I don't want to "optimize" this by paginating it or, even worse, doing some kind of dynamic content loading on scroll.</li> </ul></li> </ul></li> <li>Woo Commerce <ul> <li>I originally measured Woo Commerce as well but, unlike the pages and platforms tested above, I didn't find that being fast or slow on the initial load was necessarily representative of subsequent performance of other action, so this wasn't included in the table because having this in the table is sort of asking for a comparison against Shopify. In particular, while the "most default" Woo theme I could find was significantly faster than the "most default" Shopify theme on initial load on a slow device, performance was multidimensional enough that it was easy to find realistic scenarios where Shopify was faster than Woo and vice versa on a slow device, which is quite different from what I saw with newer blogging platforms like Substack and Medium compared to older platforms like Wordpress, or a modern forum like Discourse versus the older PHP-based forums. A real comparison of shopping sites that have carts, checkout flows, etc., would require a better understanding of real-world usage of these sites than I was going to get in a single day.</li> </ul></li> </ul> <p>Another kind of testing would be to try to configure pages to look as similar as possible. I'd be interested in seeing that results for that if anyone does it, and maybe I'll try doing that on another day if no one else does it.</p>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IBM 360 in UK need a home (161 pts)]]></title>
            <link>https://www.ibm360.co.uk/</link>
            <guid>39728994</guid>
            <pubDate>Sat, 16 Mar 2024 20:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ibm360.co.uk/">https://www.ibm360.co.uk/</a>, See on <a href="https://news.ycombinator.com/item?id=39728994">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="primary"><main id="main" role="main">
    
<article id="post-911">
			<p><a href="https://www.ibm360.co.uk/?p=911"><img width="8870" height="3908" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591.jpg" alt="" decoding="async" fetchpriority="high" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591.jpg 8870w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-300x132.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-768x338.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0591-1024x451.jpg 1024w" sizes="(max-width: 8870px) 100vw, 8870px"></a>
       </p>
       	<div>
			<p>Hello! If you’re still here reading this blog, I’m impressed. We haven’t posted anything of note for some years now and frankly, thats because we haven’t done anything of note with the project. The machines have been sitting in their home, virtually untouched, for 4 years now. Chris &amp; I (Adam) are just too busy with our respective professional and personal lives to give them a second look, and we’ve come to the difficult decision that it’s time to look at letting the systems go. When we originally moved the systems to Creslow, part of our agreement was to provide PR visibility for the services offered by ecom. Whilst this initially obviously garnered some visibility, our lack of progress with the project has obviously had…</p>
<p><a href="https://www.ibm360.co.uk/?p=911">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-902">
			<p><a href="https://www.ibm360.co.uk/?p=902"><img width="2560" height="910" src="https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-scaled.jpeg" alt="" decoding="async" srcset="https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-scaled.jpeg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-300x107.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-1024x364.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-768x273.jpeg 768w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-1536x546.jpeg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2022/05/IMG_3653-2048x728.jpeg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>Dear Reader, Wow, it’s been a while since our last post here, almost 2 years! Time has totally flown by. I checked the traffic this morning and was pleasantly surprised to see that we’re still getting 2,000+ hits per month which is just incredible given that we haven’t published any updates. So, I’m guessing you probably want to know whats going on and why we’re not posting here. Let me summarily answer your most important questions below: Are all of you okay? – Yes. Is the project dead? – No. Do you have any updates for us? – Unfortunately, not really. So, the reason we haven’t been posting here is mainly because, well, nothing has changed. Chris &amp; I have both been insanely busy with…</p>
<p><a href="https://www.ibm360.co.uk/?p=902">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-891">
			<p><a href="https://www.ibm360.co.uk/?p=891"><img width="1200" height="675" src="https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1.jpeg" alt="" decoding="async" srcset="https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1.jpeg 1200w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-300x169.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-1024x576.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2020/05/Silicon-Glen-TX-1-768x432.jpeg 768w" sizes="(max-width: 1200px) 100vw, 1200px"></a>
       </p>
       	<div>
			<p>Well, what can I say. It seems as though the last update was a lifetime ago. The world has changed so much! As you might imagine we have been unable to complete any work on the IBM 360 over the last couple of months due to the lockdown restrictions in place in the UK. We are hopeful that in the coming months as the lockdown eases we will be able to return to the project and start posting regular updates again. We have received a few items off of the Amazon wishlist, and I will post the proper thanks in due course, I am currently unable to do so as they get delivered to my fathers house and due to the self isolation policies I…</p>
<p><a href="https://www.ibm360.co.uk/?p=891">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-813">
			<p><a href="https://www.ibm360.co.uk/?p=813"><img width="2560" height="1920" src="https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-scaled.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-scaled.jpg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-1024x768.jpg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-1536x1152.jpg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2020/03/IMG_20200301_165051-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>We know we’re a few months late, but we want to wish all of our readers a happy new year! And to kick off the new year (even if we are a bit late) heres our first post of 2020! We’ve all been rather busy in our personal and professional lives of late, and as such the updates haven’t been as regular as we would have liked. We’re hoping to get quite a few days of work in this year, and we promise we’ll update you all as much as possible! We all agreed to meet on Sunday the 1st of March to resume work on the machine. We didn’t really have any set goals for the day, except to continue cleaning the machine, and…</p>
<p><a href="https://www.ibm360.co.uk/?p=813">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-754">
			<p><a href="https://www.ibm360.co.uk/?p=754"><img width="2560" height="1920" src="https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-scaled.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-scaled.jpg 2560w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-1024x768.jpg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-1536x1152.jpg 1536w, https://www.ibm360.co.uk/wp-content/uploads/2019/12/PeterCleaning-2048x1536.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"></a>
       </p>
       	<div>
			<p>Another week, another blog post! So, there’s probably going to be a trend of things in the next few blog posts, and one of them is going to be, well, you guessed it! Cleaning! The machines were all very dirty when the arrived and it’s going to take us quite some time to clean the outsides, let alone the insides, so please bear with us whilst we post about it; hopefully you’ll find the process as satisfying as we do! Over the past week or so I’ve been conversing with a chap called Simon Van Winklen about working with us, and very kindly he’s volunteering some of his time to come and assist us with the restoration process. Simon is a retired Engineer who’s career…</p>
<p><a href="https://www.ibm360.co.uk/?p=754">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-666">
			<p><a href="https://www.ibm360.co.uk/?p=666"><img width="3024" height="4032" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122.jpg 3024w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122-225x300.jpg 225w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/IMG_0587-1-e1574642700122-768x1024.jpg 768w" sizes="(max-width: 3024px) 100vw, 3024px"></a>
       </p>
       	<div>
			<p>So we mentioned in some of our earlier posts that we have a plethora of manuals that we found in the original building. These manuals are a rather obscure size being slightly longer than A3. This makes them very inconvenient to scan as they don’t fit on a regular scanner, and as such we needed a much bigger scanner. A0 scanners are particularly scarce, especially on the used market, and they command a high price (usually in the thousands). After browsing eBay for a while I happened upon a listing by a recycling company located fairly near to me for a Colortrac SmartLF Cx40 in unknown condition. For the price (&lt;£100) it was worth a punt! I won the bidding for just under £100 and…</p>
<p><a href="https://www.ibm360.co.uk/?p=666">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-485">
			<p><a href="https://www.ibm360.co.uk/?p=485"><img width="1600" height="1200" src="https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F.jpg 1600w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-300x225.jpg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-768x576.jpg 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/11/78F-1024x768.jpg 1024w" sizes="(max-width: 1600px) 100vw, 1600px"></a>
       </p>
       	<div>
			<p>Well, where to start! The last few weeks have consisted primarily of anticipation, excitement, and towards the end, utter exhaustion. But finally, FINALLY, the IBM is home! It all started after our friend John Oates at the register shared our plight as detailed in our last blog post (see here). This was read by a chap in Melbourne, Australia called Kevin Silk, who then saw a LinkedIn post by a chap called Dan Apperley and left a comment: I had no idea any of this was going on until the following day when I received a call in my office just after 9am. The caller introduced himself as Dan and said he was from a company called Sunspeed. Dan told me that he had read…</p>
<p><a href="https://www.ibm360.co.uk/?p=485">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-477">
			<p><a href="https://www.ibm360.co.uk/?p=477"><img width="682" height="455" src="https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06.jpg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06.jpg 682w, https://www.ibm360.co.uk/wp-content/uploads/2019/10/ezgif-6-300af96f3a06-300x200.jpg 300w" sizes="(max-width: 682px) 100vw, 682px"></a>
       </p>
       	<div>
			<p>So it’s been a while since we’ve posted anything here, and we apologise for that. We’ve all been rather busy with our respective personal and professional lives and it’s not left much time for us to work on getting the machine back to the UK. We’ve recently begun engaging with various transportation providers and we’re somewhat struggling to find anybody capable and willing to move the machine, but we will persevere! We’re determined to get the machine back before the October 31st Brexit deadline if at all possible, this is especially important as nobody knows what the importing costs will be post Brexit and we don’t want to end up paying thousands more in storage costs in the event of nobody knowing whats going on…</p>
<p><a href="https://www.ibm360.co.uk/?p=477">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-409">
			<p><a href="https://www.ibm360.co.uk/?p=409"><img width="768" height="382" src="https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1.png" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1.png 768w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/header10-1-300x149.png 300w" sizes="(max-width: 768px) 100vw, 768px"></a>
       </p>
       	<div>
			<p>Yesterday Afternoon (21st of May 2019) I received a WhatsApp message from our German Auction house contact (come Photographer extraordinaire), Günter Hiller. It was a series of photographs, firstly of the roof of the machine room. It would seem over the weekend that the roof had leaked significantly right onto where the punched cards and the processor had been stored! It looks like we removed everything just in the absolute nick of time! Included in the photos were bits of IBM equipment, along with some engineering manuals. These had apparently been found in the room behind the room the computer was in, buried under a load of Porsche parts! This came as a surprise to everyone as they had previously been informed these rooms were…</p>
<p><a href="https://www.ibm360.co.uk/?p=409">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

<article id="post-185">
			<p><a href="https://www.ibm360.co.uk/?p=185"><img width="1024" height="768" src="https://www.ibm360.co.uk/wp-content/uploads/2019/05/28.jpeg" alt="" decoding="async" loading="lazy" srcset="https://www.ibm360.co.uk/wp-content/uploads/2019/05/28.jpeg 1024w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/28-300x225.jpeg 300w, https://www.ibm360.co.uk/wp-content/uploads/2019/05/28-768x576.jpeg 768w" sizes="(max-width: 1024px) 100vw, 1024px"></a>
       </p>
       	<div>
			<p>So, after our last visit we were all ready to get going on the big move. We had already recovered all of the documentation back to the UK on the last visit and we had the ramp constructed which would enable reasonably smooth removal of the machine from the building. Chris and I departed for London Stansted in the late afternoon of the Wednesday (15th of May) with cabin bags containing our clothes and 2x 20kg suitcases full of things we thought we might need (e.g. ratchet straps, cloths, lights, tools etc.). We got settled into our flight and soon enough we had touched down at Nuremberg. We briefly debated taking the U-Bahn and a tram to our AirBNB but soon decided against it as…</p>
<p><a href="https://www.ibm360.co.uk/?p=185">Continue Reading<span>→</span></a></p>
		</div>

</article><!-- #post-## -->

	
</main><!-- #main --></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[20 Years in the Making, GnuCOBOL Is Ready for Industry (162 pts)]]></title>
            <link>https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/</link>
            <guid>39728519</guid>
            <pubDate>Sat, 16 Mar 2024 19:02:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/">https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/</a>, See on <a href="https://news.ycombinator.com/item?id=39728519">Hacker News</a></p>
Couldn't get https://thenewstack.io/20-years-in-the-making-gnucobol-is-ready-for-industry/: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Hackintosh Is Almost Dead (467 pts)]]></title>
            <link>https://aplus.rs/2024/hackintosh-almost-dead/</link>
            <guid>39728146</guid>
            <pubDate>Sat, 16 Mar 2024 18:16:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aplus.rs/2024/hackintosh-almost-dead/">https://aplus.rs/2024/hackintosh-almost-dead/</a>, See on <a href="https://news.ycombinator.com/item?id=39728146">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p><a href="https://aplus.rs/categories/hardware">hardware</a></p>

    

<p>It was a good run, though.</p>
    
    <p>
    <span>Mar 16, 2024</span>
    
    by 
        
    <span>6 minute read</span>
    
    </p>
    <p>While I knew about and even tried <a href="https://aplus.rs/2004/mac-os-xhappily-running-on-my-amd-athlon/">various very early attempts</a> to run macOS on non-Apple hardware, it wasn’t until <a href="https://aplus.rs/2020/hmac/">early 2020</a> that I’ve built my first proper one. Then I built several more which are still seeing daily use.</p>
<p>I <a href="https://aplus.rs/2020/missing-developer-mac/">explained my reasoning</a> why it was worthwhile to attempt it. The technology was mostly there thanks to a group of dedicated hackers and timing <a href="https://aplus.rs/2020/hmac-2020/">was just right</a>:</p>
<blockquote>
<p>But if ever there was a time to do it, it’s now.<br>
Apple is transitioning to their own CPUs/GPUs over the next two years. Several years from now, I see myself purchasing whatever desktop Apple Silicon-based machine is there.</p>
</blockquote>
<p>I also offered a prognosis which turned out partially true:</p>
<blockquote>
<p>Many will tell you that buying Intel-based hardware from Apple is buying obsolete models. I don’t really agree with that since it’s a given that those Intel-based Macs will be supported for 7-10 years of future macOS updates.</p>
</blockquote>
<p>It’s true that latest macOS 14 (Sonoma) still supports the latest generations of Intel Macs and it’s very likely that at least one or two major versions will still be compatible. But there’s one particular development that is de-facto killing off the Hackintosh scene.</p>
<p>In Sonoma, <strong>Apple has completely removed all traces of driver support for their oldest WiFi/Bt cards</strong>, namely various Broadcom cards that they last used in 2012/13 iMac / MacBook models. Those Mac models are not supported by macOS for few years now thus it’s not surprising the drivers are being removed. Most likely reason is that Apple is moving drivers away from <code>.kext</code> (Kernel Extensions) to <code>.dext</code> (DriverKit) thus cleaning up obsolete and unused code from macOS. They did the same with Ethernet drivers in Ventura.</p>
<p>Those particular cards were the key ingredient to many fully functional Hackintosh builds for simple reason: they worked out of the box with every single (so-called) iService Apple has: Messages, FaceTime, AirDrop, Continuity, Handoff - you name it. <em>Everything worked.</em> Despite the <a href="https://dortania.github.io/OpenCore-Legacy-Patcher/SONOMA-DROP.html#versioning">valiant efforts of OCLP crew</a> to make workarounds, those cards can work in Sonoma <em>only</em> if you <a href="https://github.com/perez987/Broadcom-wifi-back-on-macOS-Sonoma-by-OCLP">seriously downgrade</a> the macOS security.</p>
<p>There was some hope that <a href="https://github.com/OpenIntelWireless/itlwm">OpenIntelWireless</a> could replace those cards due to <a href="https://github.com/OpenIntelWireless">amazing work</a> zxystd did in the last 4 years. I mean, the WiFi speeds in macOS with Intel’s WiFi6 cards are <a href="https://forum.amd-osx.com/threads/wi-fi-speed-test-intel-ax200-vs-bcm94360ng.4756/">nothing short of spectacular</a>. But Apple’s continued cleanup and rewrite of their driver stack has pretty much killed-off any reliable support for Message and FaceTime despite iCloud sync still working great. zxystd <a href="https://github.com/OpenIntelWireless/itlwm/issues/953#issuecomment-1920759538">describes the new mountain to climb</a>:</p>
<blockquote>
<p>From Sonoma, Apple drops IO80211FamilyLegacy, I build AirportItlwmV2 on the top of IO80211Family, but using some hacks, you can simply interpret it as me implementing a set of IO80211FamilyLegacy myself. <em>This implementation may have side effects such as the iService not working etc</em>. Since IO80211Family uses skywalk API instead of original Ethernet API (Also we can foresee that the Ethernet API will also be dropped in macOS 15), without these hacks we should follow the Apple’s API and <em>rewrite the whole driver</em>, that’s what I would never do.</p>
</blockquote>
<p>In 14.4, Apple <a href="https://www.tonymacx86.com/threads/usb-problems-after-14-4-update.329275/">seem to have made changes</a> in how USB subsystem works too. This was always a tedious challenge but if minor updates can almost <a href="https://forum.amd-osx.com/threads/update-from-14-3-1-to-14-4.5013/">brick the build</a> it becomes a headache. Still…USB is a known problem with known solution thus it’s annoying but solvable.</p>
<p>WiFi with iServices is sadly not.</p>
<p>I’ve long held the opinion that it would not be CPU nor GPU changes that kill the Hacks — it would be lack of reliable WiFi drivers. And now, ~4 years later, Hackintosh hits a brick wall of no easy WiFi options available, at all.  Given how much of the macOS useful features is dependent on presence of particular WiFi chips — a decision of Apple developers I really can’t understand — I can’t really consider builds without those features to call themselves Mac.</p>
<p>I did not come to this conclusion just by reading the forums.</p>
<p>I have a rag-tag build sitting on my desk for several months now. It was supposed to be a quick proof-of-concept Sonoma build with Intel AX200 WiFi/Bt, AMD CPU and GPU, NVMe SSDs - everything that modern Mac should work with. It’s everything that my current Hackintosh is, with SIP intact, incremental updates working on their own etc — a perfect Mac.</p>
<p><img src="https://aplus.rs/images/2024/rag-tag-build.jpg" alt="My wannabe Sonoma-compatible Mac"></p>
<p>But pretty much since day one I encountered one problem after another. Things were so volatile and random that it was hard to believe, at times. Like —</p>
<ul>
<li>One day Ethernet controller (Intel I225-V) would work great, the next day it would just hard-crash the entire machine. No freaking idea why. Tried multiple ways and custom drivers to make it work but nothing was perfectly stable.</li>
<li>WiFi works fantastic, iCloud is perfect but Messages/FaceTime wouldn’t connect at all. In either Monterey, Ventura or Sonoma. That same card worked perfectly on another motherboard with Monterey and Ventura, no issues with Messages / FaceTime at all. Again — no idea why.</li>
<li>Bluetooth would work great for days but if I turn it off and restart the machine, something would become so messed up that it starts being recognised as BCM_4350C2 chip instead of Intel AX200. Only a round-trip to Windows 11 would somehow bring the chip into a state that <a href="https://github.com/OpenIntelWireless/IntelBluetoothFirmware">IntelBluetooth driver</a> can work with it.</li>
<li>Sonoma 14.3.1 works great on this build. But 14.4 update won’t install. It starts booting the installer and just reboots back almost immediately.</li>
</ul>
<p>Hence — <strong>Hackintosh is on its death bed</strong>. Some things will work for few more months or maybe even years, depending on what you use it for and wether lack of WiFi bothers you or not. But not for me. I can live without AirDrop, Continuity and Handoff but Messages and FaceTime must work. There’re also some other things Sonoma brings that are important to me thus I want to update to it. Coupled with described lack of reliability and fretting if next minor or major update would leave me dry — nah, not worth it.</p>
<p>I don’t really complain. I had a good run which helped me skip over the worst price/performance Mac lineup that I remember. There’re now plenty good choices within the current crop of M1 / M2 / M3 machines and I’ll be following eBay closely for a good used Mac mini / studio models. Or maybe even splurge on something new.</p>
<p>Lest I forget — if macOS Ventura works for you, stay on it! That’s still perfectly stable without a single issue across a variety of build options.</p>
<hr>
<p>Just to clarify one thing, to preempt someone saying Apple did this on purpose to kill off Hackintosh: <strong>they didn’t</strong>. Apple never cared about Hackintosh scene, it’s entirely irrelevant to their business. They did what they should be doing, improving the macOS codebase. It’s always a good thing to remove obsolete and deprecated code thus Apple is doing the right thing for their product.</p>

  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Lambda on hard mode: serverless HTTP in Rust (108 pts)]]></title>
            <link>https://modal.com/blog/serverless-http</link>
            <guid>39728045</guid>
            <pubDate>Sat, 16 Mar 2024 18:03:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://modal.com/blog/serverless-http">https://modal.com/blog/serverless-http</a>, See on <a href="https://news.ycombinator.com/item?id=39728045">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>At Modal, we built an HTTP and WebSocket stack on our platform. In other words,
your serverless functions can take web requests.</p>
<p>This was tricky! HTTP has quite a few edge cases, so we used Rust for its speed
and to help manage the complexity. But even so, it took a while to get right. We
recently wrapped up this feature by introducing
<a href="https://modal.com/blog/websocket-launch">full WebSocket support</a> (real-time bidirectional
messaging).</p>
<p>We call this service <code>modal-http</code>, and it sits between the Web and our core
runtime.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-20.c41d7b53.png" alt="Simple schematic with modal-http at the center">



</p>
<p>You can deploy a simple <a href="https://modal.com/docs/guide/webhooks">web endpoint</a> to a <code>*.modal.run</code>
URL by running some Python code:</p>


<p>(<em>This takes <strong>0.747 seconds</strong> to deploy today.</em>)</p>
<p>But you can also run a much larger compute workload. For example, to set up a
data-intensive video processing endpoint:</p>


<p>This post is about the behind-the-scenes of serving a web endpoint on Modal. How
does your web request get translated into an autoscaling serverless invocation?</p>
<p>What makes our HTTP/WebSocket implementation particularly interesting is its
lack of limits. Serverless computing is
<a rel="nofollow" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf">traditionally understood</a>
to prioritize small, lightweight tasks, but Modal can’t compromise on speed or
compute capacity.</p>
<p>When resource limits are removed, handling web requests gets proportionally more
difficult. Users may ask to upload a gigabyte of video to their machine learning
model or data pipeline, and we want to help them do that! We can’t just say,
“sorry, either make your video 200x smaller or split it up yourself.” So we had
a bit of a challenge on our hands.</p>


<p>Serverless function platforms have constraints. A lot of them, too!</p>
<ul><li>Functions on
<a rel="nofollow" href="https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html">AWS Lambda</a>
are limited to 15-minute runs and 50 MB images. As of 2024, they can only use
3 CPUs (6 threads) and 10 GB of memory. Response bandwidth is 2 Mbps.</li>
<li><a rel="nofollow" href="https://cloud.google.com/run/quotas">Google Cloud Run</a> is a bit better, with
4 CPUs and 32 GB of memory, plus 75 Mbps bandwidth.</li>
<li><a rel="nofollow" href="https://developers.cloudflare.com/workers/platform/limits/">Cloudflare Workers</a>
are the most restricted. Their images can only be 10 MB in size and have 6
HTTP connections. Execution is limited to 30 seconds of CPU time, 128 MB of
memory.</li></ul>
<p>But modern compute workloads can be <a href="https://modal.com/examples">much more demanding</a>: training
neural networks, rendering graphics, simulating physics, running data pipelines,
and so on.</p>
<p>Modal containers can each use up to <strong>64 CPUs</strong>, <strong>336 GB of memory</strong>, and <strong>8
Nvidia H100 GPUs</strong>. And they may need to download up to <strong>hundreds of
gigabytes</strong> of model weights and image data on container startup. As a result,
we care about having them spin up and shut down quickly, since having any idle
time is expensive. We scale to zero and bill <a href="https://modal.com/pricing">by the second</a>.</p>
<p>As a user, this is freeing. I often get questions like, “does Modal have enough
compute to run my <a href="https://modal.com/docs/examples/blender_video">fancy bread-baking simulation</a>”
— and I tell them, are you kidding? You can spin up dozens of 64-CPU containers
at a snap of your fingers. Simulate your whole bakery!</p>
<p>In summary: Modal containers are potentially long-running and compute-heavy,
with big inputs and outputs. This is the opposite of what “serverless” is
usually good at. How can we ensure quick and reliable delivery of HTTP requests
under these conditions?</p>
<h3 id="a-distributed-operating-system">A distributed operating system</h3>
<p>Let’s take a step back and review the concept of serverless computing. Run code
in containers. Increase the number of containers when there’s work to be done,
and then decrease it when there’s less work. You can imagine a factory that
makes cars: when there are many orders, the factory operates more machines, and
when there are fewer orders, the factory shifts its focus. (Except in computers,
everything happens faster than in a car factory, since they’re processing
thousands of requests per second.)</p>
<p>This isn’t unique to serverless computing; it’s how most applications scale
today. If you deploy a web server, chances are you’d use a PaaS to manage
replicas and scaling, or an orchestrator like Kubernetes. Each of these
offerings can be conceptualized by a two-part schematic:</p>
<ol><li><strong>Autoscaling:</strong> Write code in a stateless way, replicate it, then track how
much work needs to be done via latency, CPU, and memory metrics.</li>
<li><strong>Load balancing:</strong> Distribute work across many machines and route traffic to
them.</li></ol>
<p>Together autoscaling and load balancing constitute a kind of analogue to an
<em>operating system</em> in the distributed services world: something that manages
compute resources and provides a common execution environment, allowing software
to be run.</p>
<p>Although a unified goal, there are many approaches. (A lot of
<a rel="nofollow" href="https://research.google/pubs/maglev-a-fast-and-reliable-software-network-load-balancer/">ink</a>
<a rel="nofollow" href="https://aosabook.org/en/v2/nginx.html">has</a>
<a rel="nofollow" href="https://www.eecs.harvard.edu/~michaelm/postscripts/mythesis.pdf">been</a>
<a rel="nofollow" href="https://github.com/tangchq74/papers/blob/fad260ab66567e843e5ad6e238f7051ffe384e8a/XFaaS-SOSP23-Final.pdf">spilled</a>
on load balancing in particular.) Here’s a brief summary to illustrate how this
schematic maps onto a few popular deployment systems. We’re in good company!</p>
<div><table><thead><tr><th>System (release date)</th>
<th>Autoscaling</th>
<th>Load balancing</th></tr></thead>
<tbody><tr><td><em>Heroku (2009)</em></td>
<td>By p95 latency</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Kubernetes (2014)</em></td>
<td>Resource metrics (custom)</td>
<td>Custom reverse proxy and/or load balancer</td></tr>
<tr><td><em>AWS Lambda (2014)</em></td>
<td>Traffic</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Azure Functions (2016)</em></td>
<td>Traffic</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>AWS Fargate (2017)</em></td>
<td>Resource metrics (custom)</td>
<td>HTTP/TCP/UDP load balancer</td></tr>
<tr><td><em>Render (2019)</em></td>
<td>CPU/memory target</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Google Cloud Run (2019)</em></td>
<td>Traffic and CPU target</td>
<td>HTTP reverse proxy</td></tr>
<tr><td><em>Fly.io (2020)</em></td>
<td>Traffic (custom)</td>
<td>HTTP/TCP/TLS proxy, by distance and load</td></tr>
<tr><td><em>Modal (2023)</em></td>
<td>Traffic (custom)</td>
<td><strong>Translate HTTP to function calls</strong></td></tr></tbody>
</table></div>


<p>So… I spot a difference there. Hang on a second. I want to talk about Modal’s
HTTP ingress.</p>
<h3 id="translating-http-to-function-calls">Translating HTTP to function calls</h3>
<p>You might notice that setting up an HTTP reverse proxy in front of serverless
functions is a popular option. This means that you scale up your container, and
some service in front handles TLS termination and directly forwards traffic to a
backend server. For most of these platforms, HTTP is the main way you can talk
to these serverless functions, as a network service.</p>
<p>But for Modal, we’re focused on building a platform based on the idea that
serverless functions are just <em>ordinary functions</em> that you can call. If you
want to define a function on Modal, that should be easy! You don’t need to set
up a REST API. Just call it directly with <code>.remote()</code>.</p>


<p>Since <code>run_batch_job()</code> can be invoked in any region, and <code>compute_embeddings()</code>
can be called remotely from it, we needed to build generic high-performance
infrastructure for serverless <em>function calls</em>. Like, actually “calling a
function.” Not wrapping it in some REST API.</p>
<p>Calling a function is a bit different from handling an HTTP request. There’s a
mismatch if you try to conflate them! By supporting both of these workloads, we
can:</p>
<ul><li>Use a faster, optimized path (for calls between functions) that can be
location and data cache-aware, rather than relying on the same HTTP protocol.</li>
<li>Fully support real-time streaming in network requests, rather than limiting it
to fit the use case of a typical function call.</li>
<li>Offer first-class support for complex heterogeneous workloads on CPU and GPU.</li></ul>
<p>Modal’s bread and butter is systems engineering for heavy-duty function calls.
We’re already focused on making that fast and reliable. As a result, we decided
to handle web requests by translating them into function calls, which gives us a
foundation of shared infrastructure to build upon.</p>
<h2 id="understanding-the-http-protocol">Understanding the HTTP protocol</h2>
<p>To understand how HTTP gets turned into a function calls, first we need to
understand HTTP. HTTP follows a request-response model. Here’s what a typical
flow looks like. On the top, you can see a standard <code>GET</code> request with no body,
and on the bottom is a <code>POST</code> request with body.</p>
<p><em><strong>Note:</strong> HTTP GET requests can technically have bodies too, though they should
be ignored. Also, a less-known fact is that request and response bodies can be
interleaved,
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6202">sometimes even in HTTP/1.1</a>!</em></p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-10.80f2e446.png" alt="Diagram of two requests, HTTP GET on top and HTTP POST on the bottom">



</p>
<p>The client sends some headers to the server, followed by an optional body. Once
the server receives the request, it does some processing, then responds in turn
with a set of a headers and its own response body.</p>
<p>Both the client and server directions are sent over a specific wire protocol,
which varies between HTTP versions. For example, HTTP/1.0 uses a TCP stream for
each request, HTTP/1.1 added keepalive support, HTTP/2 has concurrent stream
multiplexing over a single TCP stream, and HTTP/3 uses QUIC (UDP) instead of
TCP. They’re all unified by this request-response model.</p>
<p>Here’s what an HTTP/1.1 GET looks like, as displayed by <code>curl</code> in verbose mode.
The <code>&gt;</code> lines are request headers, the <code>&lt;</code> lines are response headers, and the
response body is at the end:</p>


<p>To iron out the differences between HTTP protocol versions, we needed a backend
data representation for the request. In a reverse proxy, the backend protocol
would just be HTTP/1.1, but in our case that would add additional complexity for
reliably reconnecting TCP streams and parsing the wire format. We instead
decided to base our protocol on a stream of <em>events</em>.</p>
<p>Luckily, there was already a well-specified protocol for representing HTTP as
event data: <a rel="nofollow" href="https://github.com/django/asgiref">ASGI</a>, typically used as a
standard interface for web frameworks in Python.</p>
<p><em><strong>Note:</strong> ASGI was made for a different purpose! Usually the web server and
ASGI application run on the same machine. Here we’re using it as the internal
communication language for a distributed runtime. So we adjusted the protocol to
our use case by serializing events as binary Protocol Buffers.</em></p>
<p>ASGI doesn’t support every internal detail of HTTP (e.g., gRPC servers need
access to HTTP/2 stream IDs), but it’s a common denominator that’s enough for
web apps built with all the popular Python web frameworks: Flask, Django,
FastAPI, and more. That’s a lot of web applications, and the benefit of this
maturity is that it lets us greatly simplify our model of HTTP serving.</p>
<p>Here’s what a POST request looks like in ASGI. The blue arrows represent client
events, while the green arrows are events sent from the server.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-11.9ebf9ddb.png" alt="Diagram of an HTTP POST request with events marked">



</p>
<ol><li>At the start of a request, when headers are received, we begin by parsing the
headers to generate a <em>function input</em> via the <code>http</code> request scope. This
triggers a new function call, which is scheduled on a running task according
to availability and locality.</li>
<li>Then, the request body is streamed in, and we begin reading it in chunks to
produce real-time <code>http.request</code> events that are sent to the serverless
function call. If the server falls behind, backpressure is propagated to the
client via TCP (for HTTP/1.1) or HTTP/2 flow control.</li>
<li>The function starts executing immediately after getting the request headers,
then begins reading the request body. It sends back its own headers and
status code, followed by the response body in chunks.</li>
<li>The request-response cycle finishes, optionally with HTTP trailers.</li></ol>
<p>In this way, we’re able to send an entire HTTP request and response over a
generic serverless function call. And it’s efficient too, with proper batching
and backpressure. We don’t need to establish a single TCP stream or anything; we
can use reliable, low-latency message queues to send the events.</p>
<p>Unlike AWS Lambda’s 6 MB limit for request and response bodies, this
architecture lets us support request bodies of up to 4 GiB (682x bigger), and
streaming response bodies of unlimited size.</p>
<p>Of course, although conceptually simple, it’s still a pretty tricky thing to
implement correctly since there are a lot of concurrent moving parts. Our
implementation is in Rust, based on the <a rel="nofollow" href="https://hyper.rs/">hyper</a> HTTP server
library and <a rel="nofollow" href="https://tokio.rs/">Tokio</a> async runtime. Here’s a snippet of the
code that buffers the request body in chunks of up to 1 MiB in size, or waits
for 2 milliseconds of duration.</p>


<p>You might have noticed the <code>disconnect_rx</code> channel used in the snippet above.
This hints at one of the realities of making reliable distributed systems that
we glossed over: needing to thoroughly handle failure cases everywhere, all the
time.</p>
<h3 id="edge-cases-and-errors">Edge cases and errors</h3>
<p>First, if a client sends an HTTP request but exits in the middle of sending the
body, then we propagate that disconnection to the serverless function.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-12.350fa27e.png" alt="Diagram of a disconnected HTTP request">



</p>
<p>We reify this using an ASGI <code>http.disconnect</code> event, which allows the user’s
code to stop executing gracefully. Otherwise, we might have a function call
that’s still running even after the user has canceled their request.</p>
<p>Another issue is if the server has a failure. It might throw an exception, crash
due to running out of memory, hit a user-defined timeout, be preempted if on a
spot instance, and so on. If a malicious user is on the system, they also might
send malformed response events, or events in the wrong order!</p>
<p>We keep track of any violations and display an error message to the user. Rust’s
pattern matching and ownership help with managing the casework.</p>
<h3 id="dealing-with-http-idle-timeouts">Dealing with HTTP idle timeouts</h3>
<p>Okay, so if we had been a standard runtime, we would be done with HTTP now. But
we’re still not done! There’s one more thing to consider: long-running requests.</p>
<p>If you make an HTTP request and the server doesn’t respond for 300 seconds, then
Chrome cancels the request and gives you an error. This is not configurable.
Other browsers and pieces of web infrastructure have varying timeouts. Our users
often end up running expensive models that take longer than 5 minutes, so we
need a way to support long-running requests.</p>
<p>Luckily, there’s a solution. After 150 seconds (2.5 minutes), we send a
temporary “303 See Other” redirect to the browser, pointing them to an
alternative URL with an ID for this specific request. The browser or HTTP client
will follow this redirect, ending their current stream and starting a new one.</p>
<p>Browsers will follow up to 20 redirects for a link, so this effectively
<a rel="nofollow" href="https://modal.com/docs/guide/webhook-timeouts">increases the idle timeout to 50 minutes</a>.
An example of this in action is shown below, with a single redirect.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-13.e1bc7734.png" alt="Diagram of a long-running request with one 303 See Other response">



</p>
<p>Is this behavior a little strange? Yes. But it just works “out-of-the-box” for a
lot of people who have web endpoints that might execute for a long time. And if
your function finishes processing and begins its response in less than 2.5
minutes, you’ll never notice a difference anyway.</p>
<p>For people who need to have very long-running web requests, Modal <em>just works</em>.</p>
<h3 id="websocket-connections">WebSocket connections</h3>
<p>That’s it for HTTP. What if a user makes a WebSocket connection? Well, the
WebSocket protocol works by starting an HTTP/1.1 connection, then establishing a
<em>handshake</em> via HTTP’s connection upgrade mechanism. The
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6455#section-1.2">handshake</a> looks
something like this:</p>


<p><em><strong>Note:</strong> There is also another version of the WebSocket protocol that
bootstraps from HTTP/2, but it’s not supported by many web servers yet. For now,
you need a dedicated TCP connection.</em></p>
<p>The <code>Sec-WebSocket-Key</code> header is random, while <code>Sec-WebSocket-Accept</code> is
derived from an arbitrary hash function on the key. (This is just some protocol
junk that we had to implement, see
<a rel="nofollow" href="https://datatracker.ietf.org/doc/html/rfc6455">RFC 6455</a>.) ASGI has a separate
<a rel="nofollow" href="https://asgi.readthedocs.io/en/latest/specs/www.html#websocket">WebSocket interface</a>
that encodes this handshake into a pair of <code>websocket.connect</code> and
<code>websocket.accept</code> events, so we translated our incoming request into those
events.</p>
<p>After the handshake, all of the infrastructure is already in place, and we
transmit messages between <code>modal-http</code> and the serverless function via data
channels in the same way as we did for HTTP.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-14.d413aaba.png" alt="Diagram of a WebSocket connection">



</p>
<p>Our server-side Rust implementation is based on hyper as before, but it upgrades
the connection to an asynchronous
<a rel="nofollow" href="https://github.com/snapview/tokio-tungstenite">tokio-tungstenite</a> stream once
the handshake is accepted.</p>
<h2 id="building-on-open-source-infrastructure">Building on open-source infrastructure</h2>
<p>We’ve built a lot of infrastructure to support HTTP and WebSocket connections,
but we didn’t start from scratch. The Rust ecosystem was invaluable to making
this custom network service, which needed to be high-performance and correct.</p>
<p>But while we’ve talked a lot about the serverless backend and design choices
made to support heavy workloads, we haven’t talked yet about how requests
actually <em>get</em> to <code>modal-http</code>. For this part, we relied on boring, mature
open-source cloud infrastructure pieces.</p>
<p>Let’s still take a look though. Modal web endpoints run on the wildcard domain
<code>*.modal.run</code>, as well as on
<a href="https://modal.com/docs/guide/webhook-urls#custom-domains">custom domains</a> as assigned by users
via a CNAME record to <code>cname.modal.domains</code>. The most basic way you’d deploy a
Rust service like <code>modal-http</code> is by pointing a
<a rel="nofollow" href="https://en.wikipedia.org/wiki/Domain_Name_System">DNS record</a> at a running
server, which has the compiled binary listen on a port.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-0.8a9d6b39.png" alt="A browser sends a request to modal-http">



</p>
<p>Rust is pretty fast, so this is a reasonable design for most real-world
services. A single node nevertheless doesn’t scale well to the traffic of a
cloud platform. We wanted:</p>
<ul><li><strong>Multiple replicas.</strong> Replication of the service provides fault tolerance and
eases the process of rolling deployments. When we rollout a new version, old
replicas need a gradual timeout.</li>
<li><strong>Encryption.</strong> Support for TLS is missing here. We <em>could</em> handle it in the
server directly, but rather than reinventing the wheel, it’s easier and safer
to rely on well-vetted software for TLS termination. (We also need to allocate
<a rel="nofollow" href="https://caddyserver.com/docs/automatic-https#on-demand-tls">on-demand certificates</a>
for custom domains.)</li></ul>
<p>So, rather than the simplified flow above, our actual ingress architecture to
<code>modal-http</code> looks like this. We placed a TCP network load balancer in front of
a <a rel="nofollow" href="https://kubernetes.io/">Kubernetes</a> cluster, which runs a
<a rel="nofollow" href="https://caddyserver.com/docs/">Caddy</a> deployment, as well as a separate
deployment for <code>modal-http</code> itself.</p>
<p><img src="https://modal.com/_app/immutable/assets/modal-http-1.e632ef42.png" alt="Full path of a request through L4 NLB and Caddy">



</p>
<p>Note that none of our <em>serverless functions</em> run in this Kubernetes cluster.
Kubernetes isn’t well-suited for the workloads we described, so we wrote our own
high-performance serverless runtime based on <a rel="nofollow" href="https://gvisor.dev/">gVisor</a>, our
own file system, and our own job scheduler — which we’ll talk about another
time!</p>
<p>But Kubernetes is still a rock-solid tool for the more traditional parts of our
cloud infrastructure, and we’re happy to use it here.</p>
<h3 id="caveat-multi-region-request-handling">Caveat: Multi-region request handling</h3>
<p>It’s a fact of life that light takes time to travel through fiber-optic cables
and routers. Ideally, <code>modal-http</code> should run on the edge in geographically
distributed data center regions, and requests should be routed to the nearest
replica. This is important to minimize baseline latency for web serving.</p>
<p>We’re not there yet though. It’s early days! While our serverless functions are
already running in many different clouds and data centers based on <em>compute
availability</em>, since GPUs are scarce, our actual servers only run in Ashburn,
Virginia for now.</p>
<p>This is a bit of a tradeoff for us, but it’s not a fundamental one. It gives us
more flexibility at the moment, although <code>modal-http</code> will be deployed to more
regions in the future for latency reasons. Right now heavyweight workloads on
Modal probably aren’t affected, but for very latency-sensitive workloads (under
100 ms), you’ll likely want to specify your container to run in Ashburn.</p>
<h2 id="lessons-learned">Lessons learned</h2>
<p>So, there you have it. Serverless functions are traditionally limited to a
request-response model, but Modal just released full support for WebSockets,
with GPUs and fully managed autoscaling. And we did this by translating web
requests into function calls.</p>
<p>Our service, <code>modal-http</code>, is written in Rust and based on several components
that let us handle HTTP and WebSocket requests at scale. We’ve placed it behind
infrastructure to handle the ingress of requests, and we’re planning to expand
to more regions in the future.</p>
<p>Some may wonder: If Modal translates HTTP to this message format, wouldn’t that
stop people from being able to use the traditional container model of
<a rel="nofollow" href="https://docs.docker.com/reference/dockerfile/#expose"><code>EXPOSE</code></a>-ing TCP ports?
This is a good question, but it’s not a fundamental limitation. The events can
be losslessly translated back to HTTP on the other end! We
<a href="https://modal.com/docs/examples/comfy_ui">wrote examples</a> of this for systems like ComfyUI, and
we’re
<a rel="nofollow" href="https://github.com/modal-labs/modal-client/pull/1513">building it into the runtime</a>
with just a bit of added code.</p>
<p>We’ve already been running Rust to power our serverless runtime for the past two
years, but <code>modal-http</code> gives us more confidence to run standard Rust services
in production. Just for comparison, when we first introduced this system to
replace our previous Python-based ingress, the number of <code>502 Bad Gateway</code>
errors in production decreased by 99.7%, due to clearer error handling and
tracking of request lifetimes. And it laid the groundwork for WebSocket support
without fundamental changes.</p>
<p>Today, web endpoints and remote function calls on Modal use a common system.
Having uniformity allows us to focus on impactful work that makes our cloud
runtime faster and lower-priced, while improving security and reliability over
time.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>Thanks to the Modal team for their feedback on this post. Special thanks to
Jonathon Belotti, Erik Bernhardsson, Akshat Bubna, Richard Gong, and Daniel
Norberg for their work and design discussions related to <code>modal-http</code>.</p>
<p>If you’re interested in fast, reliable, and heavy-duty systems for the cloud,
<a href="https://modal.com/company">Modal is hiring</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Research shows that people who BS are more likely to fall for BS (2021) (132 pts)]]></title>
            <link>https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs</link>
            <guid>39727529</guid>
            <pubDate>Sat, 16 Mar 2024 17:01:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs">https://uwaterloo.ca/news/media/research-shows-people-who-bs-are-more-likely-fall-bs</a>, See on <a href="https://news.ycombinator.com/item?id=39727529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div property="content:encoded"><p><span><span>People who frequently try to impress or persuade others with misleading exaggerations and distortions are themselves more likely to be fooled by impressive-sounding misinformation, new research from the University of Waterloo shows.&nbsp;</span></span></p>

<p><span><span>The researchers found that people who frequently engage in “persuasive bullshitting” were actually quite poor at identifying it. Specifically, they had trouble distinguishing intentionally profound or scientifically accurate fact from impressive but meaningless fiction. Importantly, these frequent BSers are also much more likely to fall for fake news headlines.</span></span></p>

<p><span><span>“It probably seems intuitive to believe that you can’t bullshit a bullshitter, but our research suggests that this isn’t actually the case,” says Shane Littrell, lead author of the paper and cognitive psychology PhD candidate at Waterloo. “In fact, it appears that the biggest purveyors of persuasive bullshit are ironically some of the ones most likely to fall for it.”</span></span></p>

<p><span><span>The researchers define “bullshit” as information designed to impress, persuade, or otherwise mislead people that is often constructed without concern for the truth. They also identify two types of bullshitting— persuasive and evasive.&nbsp;“Persuasive” uses misleading exaggerations and embellishments to impress, persuade, or fit in with others, while ‘evasive’ involves giving irrelevant, evasive responses in situations where frankness might result in hurt feelings or reputational harm.&nbsp;</span></span></p>

<p><span><span>In a series of studies conducted with over 800 participants from the US and Canada, the researchers examined the relations between participants’ self-reported engagement in both types of BSing and their ratings of how profound, truthful, or accurate they found pseudo-profound and pseudo-scientific statements and fake news headlines. Participants also completed measures of cognitive ability, metacognitive insight, intellectual overconfidence, and reflective thinking.</span></span></p>

<p><span><span>“We found that the more frequently someone engages in persuasive bullshitting, the more likely they are to be duped by various types of misleading information regardless of their cognitive ability, engagement in reflective thinking, or metacognitive skills,” Littrell said. “Persuasive BSers seem to mistake superficial profoundness for actual profoundness. So, if something simply sounds profound, truthful, or accurate to them that means it really is. But evasive bullshitters were much better at making this distinction.”&nbsp;</span></span></p>

<p><span><span>The research may help shed light on the processes underlying the spread of some types of misinformation, which could have important implications for the fight against this growing problem.&nbsp;</span></span></p>

<p>The study, You can’t bullshit a bullshitter (or can you?): Bullshitting frequency predicts receptivity to various types of misleading information, authored by Littrell and Waterloo’s Faculty of Arts professors Evan Risko and Jonathan Fugelsang, appears in the <em>British Journal of Social Psychology</em>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing URLs in Python (142 pts)]]></title>
            <link>https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html</link>
            <guid>39727458</guid>
            <pubDate>Sat, 16 Mar 2024 16:53:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html">https://tkte.ch/articles/2024/03/15/parsing-urls-in-python.html</a>, See on <a href="https://news.ycombinator.com/item?id=39727458">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
      <p>tl;dr - Try <a href="https://github.com/tktech/can_ada">can_ada</a> if you need to parse URLs in Python.</p>
<h2>URLs</h2>
<p>Parsing URLs <em>correctly</em> is surprisingly hard. Who even defines what a "correct"
URL is? URLs have evolved drastically since they were originally defined in
<a href="https://tools.ietf.org/html/rfc1738">1994</a>. The <a href="https://whatwg.org/">WHATWG</a> has a <a href="https://url.spec.whatwg.org/">URL specification</a> that
is comprehensive and has helped standardize the behavior of URLs across
browsers, but this specification still isn't universal and ambiguities like
"how many slashes are you allowed" can definitely get on <a href="https://daniel.haxx.se/blog/2017/01/30/one-url-standard-please">your nerves</a>:</p>
<blockquote cite="Daniel Stenberg">
    So browsers accept URLs written with thousands of forward slashes instead of
    two. That is not a good reason for the spec to say that a URL may
    legitimately contain a thousand slashes. I’m totally convinced there’s no
    critical content anywhere using such formatted URLs and no soul will be sad
    if we’d restricted the number to a single-digit. So we should. And yeah,
    then browsers should reject URLs using more.
</blockquote>

<p><em>However</em>, if you're creating something new and want to handle URLs, the
WHATWG's URL specification probably is the best place to start.</p>
<h2>URLs in Python</h2>
<p>If you're working in Python, you'd probably start with the built-in <code>urllib</code>
module. It's been around forever, but unfortunately it's not compliant with
<em>any</em> URL specification, either the much older <a href="https://tools.ietf.org/html/rfc3986">rfc3978</a>:</p>
<blockquote cite="CPython source code">
    RFC 3986 is considered the current standard and any future changes to
    urlparse module should conform with it. The urlparse module is
    currently not entirely compliant with this RFC due to defacto
    scenarios for parsing, and for backward compatibility purposes, some
    parsing quirks from older RFCs are retained. The testcases in
    test_urlparse.py provides a good indicator of parsing behavior.
</blockquote>

<p>... or the WHATWG URL Parser spec:</p>
<blockquote cite="CPython source code">
    The WHATWG URL Parser spec should also be considered. We are not compliant
    with it either due to existing user code API behavior expectations (Hyrum's
    Law). It serves as a useful guide when making changes.
</blockquote>

<p>Having existed for over 16 years, so many projects depend on the urllib module
parsing URLs in <em>exactly</em> the way it does that it's unlikely to ever change in
any significant fashion.</p>
<h2>Ada</h2>
<p>The <a href="https://github.com/ada-url/ada">Ada</a> project is a new (2024) attempt to create a URL parsing library
that adheres to the WHATWG URL specification and works <a href="https://github.com/ada-url/ada?tab=readme-ov-file#ada-is-fast">really, really fast</a>,
parsing 7 URLs for every 1 parsed by cURL. Written in C++, it now has bindings
to several other languages, including <a href="https://github.com/ada-url/ada-python">Python</a>, and has become the
URL parsing library used by Node.js as of version 18 to great success:</p>
<blockquote cite="State of Node.js Performance 2023">
    Since Node.js 18, a new URL parser dependency was added to Node.js — Ada.
    This addition bumped the Node.js performance when parsing URLs to a new
    level. Some results could reach up to an improvement of 400%.
</blockquote>

<p>The ada-python binding is perfectly functional, and the official binding for the
project. However, the ada-python bindings are built on <a href="https://cffi.readthedocs.io/en/latest/">CFFI</a>, an approach
that has the binding between C and Python written in Python itself, which loses
some of the performance benefits of using Ada in the first place when most of
the time is spent just making the function call.</p>
<h2>can_ada</h2>
<p><a href="https://lemire.me/">Daniel Lemire</a>, one of the developers behind the Ada project asked me to
<a href="https://github.com/ada-url/ada-python/pull/1#issuecomment-1550405501">take a look</a> at the ada-python bindings and out of that was born
<a href="https://github.com/tktech/can_ada">can_ada</a>, a new Python binding that uses <a href="https://pybind11.readthedocs.io/en/stable/">pybind11</a> and template magic
to generate the binding code, which is then compiled into a Python extension
module. This approach has the potential to be much faster than the ada-python
bindings, and indeed when comparing the two bindings, the new can_ada binding
is about 2x faster than the ada-python bindings which in turn is about 2x
faster than <code>urllib.parse</code>!</p>
<div><pre><span></span><code><span>---------------------------------------------------------------------------------</span>
<span>Name (time in ms)              Min                 Max                Mean       </span>
<span>---------------------------------------------------------------------------------</span>
<span>test_can_ada_parse         54</span><span>.</span><span>1304 (1</span><span>.</span><span>0)       54</span><span>.</span><span>6734 (1</span><span>.</span><span>0)       54</span><span>.</span><span>3699 (1</span><span>.</span><span>0) </span>
<span>test_ada_python_parse     107</span><span>.</span><span>5653 (1</span><span>.</span><span>99)     108</span><span>.</span><span>1666 (1</span><span>.</span><span>98)     107</span><span>.</span><span>7817 (1</span><span>.</span><span>98)</span>
<span>test_urllib_parse         251</span><span>.</span><span>5167 (4</span><span>.</span><span>65)     255</span><span>.</span><span>1327 (4</span><span>.</span><span>67)     253</span><span>.</span><span>2407 (4</span><span>.</span><span>66)</span>
<span>---------------------------------------------------------------------------------</span>
</code></pre></div>

<p>At the same time, using the pybind11 approach allows for a very succinct and
<a href="https://github.com/TkTech/can_ada/blob/main/src/binding.cpp">readable</a> binding definition, coming in at just <strong>60</strong> lines of code and
almost a 1:1 with the underlying C++ API.</p>
<p>Binary builds are available now for CPython 3.7 to 3.12, and PyPy 3.7 to 3.9 on
many OS's and architectures. Install it with pip or <a href="https://github.com/tktech/can_ada">get the source</a>:</p>


<h2>Example</h2>
<div><pre><span></span><code><span>import</span> <span>can_ada</span>
<span>urlstring</span> <span>=</span> <span>"https://www.GOoglé.com/./path/../path2/"</span>
<span>url</span> <span>=</span> <span>can_ada</span><span>.</span><span>parse</span><span>(</span><span>urlstring</span><span>)</span>
<span># prints www.xn--googl-fsa.com, the correctly parsed domain name according</span>
<span># to WHATWG</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>hostname</span><span>)</span>
<span># prints /path2/, which is the correctly parsed pathname according to WHATWG</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>pathname</span><span>)</span>
</code></pre></div>

<p>Compare this to the urllib version, which is not WHATWG compliant:</p>
<div><pre><span></span><code><span>import</span> <span>urllib.parse</span>
<span>urlstring</span> <span>=</span> <span>"https://www.GOoglé.com/./path/../path2/"</span>
<span>url</span> <span>=</span> <span>urllib</span><span>.</span><span>parse</span><span>.</span><span>urlparse</span><span>(</span><span>urlstring</span><span>)</span>
<span># prints www.googlé.com</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>hostname</span><span>)</span>
<span># prints /./path/../path2/</span>
<span>print</span><span>(</span><span>url</span><span>.</span><span>path</span><span>)</span>
</code></pre></div>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Flash Attention in ~100 lines of CUDA (213 pts)]]></title>
            <link>https://github.com/tspeterkim/flash-attention-minimal</link>
            <guid>39726781</guid>
            <pubDate>Sat, 16 Mar 2024 15:31:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/tspeterkim/flash-attention-minimal">https://github.com/tspeterkim/flash-attention-minimal</a>, See on <a href="https://news.ycombinator.com/item?id=39726781">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">flash-attention-minimal</h2><a id="user-content-flash-attention-minimal" aria-label="Permalink: flash-attention-minimal" href="#flash-attention-minimal"></a></p>
<p dir="auto">A minimal re-implementation of Flash Attention with CUDA and PyTorch.
The official <a href="https://github.com/Dao-AILab/flash-attention">implementation</a> can be quite daunting for a CUDA beginner
(like myself), so this repo tries to be small and educational.</p>
<ul dir="auto">
<li>The entire forward pass is written in ~100 lines in <code>flash.cu</code>.</li>
<li>The variable names follow the notations from the original <a href="https://arxiv.org/abs/2205.14135" rel="nofollow">paper</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisite</h3><a id="user-content-prerequisite" aria-label="Permalink: Prerequisite" href="#prerequisite"></a></p>
<ul dir="auto">
<li>PyTorch (with CUDA)</li>
<li><code>Ninja</code> for loading in C++</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Benchmark</h3><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"></a></p>
<p dir="auto">Compare the wall-clock time between manual attention and minimal flash attention:</p>

<p dir="auto">Sample output on a <a href="https://aws.amazon.com/ec2/instance-types/g4/" rel="nofollow">T4</a>:</p>
<div data-snippet-clipboard-copy-content="=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms"><pre><code>=== profiling manual attention ===
...
Self CPU time total: 52.389ms
Self CUDA time total: 52.545ms

=== profiling minimal flash attention === 
...  
Self CPU time total: 11.452ms
Self CUDA time total: 3.908ms
</code></pre></div>
<p dir="auto">Speed-up achieved!</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I don't have a GPU</h3><a id="user-content-i-dont-have-a-gpu" aria-label="Permalink: I don't have a GPU" href="#i-dont-have-a-gpu"></a></p>
<p dir="auto">Try out this <a href="https://colab.research.google.com/gist/tspeterkim/143bc7be7a845656817cf94c5228598e/demo-flash-attention-minimal.ipynb" rel="nofollow">online colab demo</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Caveats</h2><a id="user-content-caveats" aria-label="Permalink: Caveats" href="#caveats"></a></p>
<ul dir="auto">
<li>No backward pass! To be honest, I found it a lot more complex than the forward pass, which was enough to show the
use of shared memory to avoid large N^2 read/writes.</li>
<li>In the inner loop, I assign each thread to a row of the output matrix. This differs from the original implementation.</li>
<li>This thread-per-row simplification makes the matrix multiplications very slow. This is probably why for longer
sequences and larger block sizes, this gets slower than the manual implementation.</li>
<li>Q,K,Vs are in float32, unlike the original implementation which uses float16.</li>
<li>The block size is <a href="https://github.com/tspeterkim/flash-attention-minimal/blob/9b7ca8ef4e6afdbfeb149a9cd488c8dea9af9ad6/flash.cu#L85">fixed</a> at compile time to 32.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Todos</h2><a id="user-content-todos" aria-label="Permalink: Todos" href="#todos"></a></p>
<ul>
<li> Add backward pass</li>
<li> Speed up matmults</li>
<li> Dynamically set block size</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[macOS 14.4 causes JVM crashes (184 pts)]]></title>
            <link>https://blogs.oracle.com/java/post/java-on-macos-14-4</link>
            <guid>39726292</guid>
            <pubDate>Sat, 16 Mar 2024 14:32:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blogs.oracle.com/java/post/java-on-macos-14-4">https://blogs.oracle.com/java/post/java-on-macos-14-4</a>, See on <a href="https://news.ycombinator.com/item?id=39726292">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

            <!-- RC84v1 -->
            <section>

                 <p><span><span><span><span><span>An issue introduced by macOS 14.4, which causes Java process to terminate unexpectedly, is affecting all Java versions from Java 8 to the early access builds of JDK 22. There is no workaround available, and since there is no easy way to revert a macOS update, affected users might be unable to return to a stable configuration unless they have a complete backup of their systems prior to the OS update.</span></span></span></span></span></p>

<p><span><span><span><span><span>The issue was not present in the early access releases for macOS 14.4, so it was discovered only after Apple released the update.</span></span></span></span></span></p>

<p><span><span><span><span><span>macOS on Apple silicon processors (M1, M2, and M3) includes a feature which controls how and when dynamically generated code can be either produced (written) or executed on a per-thread basis.&nbsp;</span></span></span></span></span></p>

<p><span><span><span><span>As a normal part of the just-in-time compile and execute cycle</span></span><span><span>, processes running on macOS may access memory in protected memory regions. Prior to the macOS 14.4 update, in certain circumstances, the macOS kernel would respond to these protected memory accesses by sending a signal,&nbsp;</span></span><span><span><span><span>SIGBUS</span></span></span></span><span><span>&nbsp;or&nbsp;</span></span><span><span><span><span>SIGSEGV</span></span></span></span><span><span>, to the process. The process could then choose to handle the signal and continue execution.&nbsp; With macOS 14.4, when a thread is operating in the write mode, if a memory access to a protected memory region is attempted, macOS will send the signal&nbsp;</span></span><span><span><span><span>SIGKILL </span></span></span></span><span><span>instead. That signal cannot be handled by the process and the process is unconditionally terminated. </span></span></span></span></p>

<p><span><span><span><span><span>The Java Virtual Machine generates code dynamically and leverages the protected memory access signal mechanism both for correctness (e.g., to handle the truncation of memory mapped files) and for performance. With macOS 14.4, programs that attempt this will now terminate instead of having the opportunity to handle the signal.</span></span></span></span></span></p>

<p><span><span><span><span><span>Ahead-of-Time compiled applications created with GraalVM Native Image should not be affected, but your ability to build new images may be.</span></span></span></span></span></p>

<p><span><span><span><span><span>Oracle has notified its customers, Apple, and our partners in OpenJDK of this situation. We recommend that users of Java on ARM-based Apple devices running macOS 14 delay applying the update until this issue is resolved.</span></span></span></span></span></p>

<p><span><span><span><span><span>The issue can be tracked on&nbsp;</span></span><span><a href="https://bugs.java.com/bugdatabase/view_bug?bug_id=8327860" title="https://bugs.java.com/bugdatabase/view_bug?bug_id=8327860"><span><span>bugs.java.com with bugID JDK-8327860</span></span></a></span></span></span></span></p>


            </section>
            <!-- /RC84v1 -->

            <!-- RC84v2 -->
            <div>
                        <p><img src="https://blogs.oracle.com/content/published/api/v1.1/assets/CONT8FA109D0B313456BA3072245A6090E81/Thumbnail?cb=_cache_a30&amp;channelToken=7e01516d535048508dfbd81a6ea0d1ed&amp;format=jpg" alt="">
                        </p>
                        <div>
                                <h4>Aurelio Garcia-Ribeyro</h4>
                                <h5>Senior Director of Product Management</h5>

                                <p>Aurelio&nbsp;has been involved in the development of the JDK since JDK 7. &nbsp;He is a frequent presenter at JavaOne, Oracle Code One, and with Java User Groups and Oracle Customers. &nbsp;Aurelio's role includes making sure that Java users, within and outside of Oracle, are well informed of changes as well as to present the most relevant features and enhancements in upcoming releases. He has received a JavaOne Rock Star Award.</p>
<p>Aurelio joined Oracle in 2010 through the Sun Microsystems acquisition.&nbsp;</p>
                            </div>
                    </div>
            <!-- /RC84v2 -->


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator's chief startup whisperer is demoting himself (121 pts)]]></title>
            <link>https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/</link>
            <guid>39725678</guid>
            <pubDate>Sat, 16 Mar 2024 13:24:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/">https://www.wired.com/story/plaintext-y-combinator-michael-seibel-startup-whisperer/</a>, See on <a href="https://news.ycombinator.com/item?id=39725678">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>When Michael Seibel lost his position at the startup incubator Y Combinator, he didn’t find out in <a href="https://www.wired.com/story/plaintext-alphabets-layoffs-arent-very-googley/">typical tech industry fashion</a>, which might entail an email calling him to a Zoom meeting where the bad news would be delivered. He did it to himself. Today Seibel is announcing that he’s stepping down as YC’s managing director, a job that entailed running the heart of the business: selecting startup founders for the three-month program and running the boot-camp-style operation that hones the vision and execution of their ideas so they can raise money, release products, and attempt to become the next Airbnb or Stripe (both YC alumni).</p><p>Considering how important YC has been to the tech startup ecosystem, Seibel’s exit will have more resonance than your average corporate reshuffle. For one thing, the person who runs YC’s blue-chip accelerator has a significant hand in shaping the next generation of tech companies. And in recent months, YC has found itself in the crossfire of a war between tech and progressives. Whether intentional or not, Seibel, a well-liked entrepreneur and investor himself, is deftly stepping out of the line of fire.</p><p>Seibel explains the move as a more personal decision. Sometime last year he began to take stock, spurred in part by reading <a data-offer-url="https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X&quot;}" href="https://www.amazon.com/Strength-Finding-Success-Happiness-Purpose/dp/059319148X" rel="noopener" target="_blank"><em>Strength to Strength</em></a>, a book about career arcs, particularly pivots made late in life. He’s only 41, but precociousness is part of the founder mindset, and he’d been a startup CEO at 23. “I do everything early,” he says.</p><div data-testid="GenericCallout"><figure><p><span>Michael Seibel</span><span>Courtesy of Y Combinator</span></p></figure></div><p>He realized that he had been running batches for as long as the person who first imagined YC into being, <a href="https://www.wired.com/story/how-y-combinator-changed-the-world/">Paul Graham</a>. After Covid waned, YC had returned to an in-person experience, and the software that it had developed to smooth the remote Covid-era program made an IRL operation easier to manage. Now the program works by splitting each batch of new startups into four groups, none larger than <a href="https://www.newscientist.com/definition/dunbars-number/">Dunbar’s Number</a> of 150, estimated to be the maximum number of relationship’s a human brain can properly maintain. Each group has its own leader, so YC had less need for someone to oversee each cohort as a whole. And though Seibel enjoyed managing the overall program, he much preferred direct contact with company founders. So he will now become one of those four group leaders, who each mentor a quarter of the batch. It’s a particularly exciting time to do that, Seibel says, as many of the companies hinge on the AI boom.</p><p>Close observers of YC—and many in the startup ecosystem monitor the accelerator with the diligence of a behavior-tracking ad network—might wonder whether Seibel’s move might have something to do with his being passed over for the leadership of the entire operation. <a href="https://www.forbes.com/sites/alexkonrad/2024/03/08/inside-garry-tan-plan-restore-y-combinator-silicon-valley-glory/?sh=709b797b2103">Forbes has reported</a> that he was disappointed not to be tapped as CEO after the incubator’s president, Geoff Ralston, who had taken over when Sam Altman went full time leading OpenAI, left at the end of 2022. Ralston was replaced by YC’s former design guru, Garry Tan. Seibel tells me he did not feel dissed, though he would have accepted the job if offered. “If it was something that people thought was going to be the right thing, I was happy to do it. If not, I was more than happy to not,” he says. “My whole goal was to do whatever YC needed for me.”</p><p>Seibel’s self-demotion seems to be in keeping with a recent rethinking at Y Combinator: a refocusing toward a scrappy, boots-on-the-ground startup accelerator as it was under its initial leader and cofounder Graham. His successor, Altman, started a sprawling research operation that, among other things, launched OpenAI. Ralston had his own dreams, and YC started a continuity fund to enable it to make later-stage investments into maturing startups. Ralston was also enamored with scale. The Winter 2022 batch included 412 companies, each funded by the traditional seed investment from YC. <a data-offer-url="https://www.ycombinator.com/blog/ycs-500-000-standard-deal" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ycombinator.com/blog/ycs-500-000-standard-deal&quot;}" href="https://www.ycombinator.com/blog/ycs-500-000-standard-deal" rel="noopener" target="_blank">Ralston boosted</a> that initial slug of capital from $125,000 to $500,000 per company, for a 7 percent stake. When <a href="https://www.wired.com/story/how-y-combinator-changed-the-world/">I last asked him</a> whether there was a limit to how many startups YC could accommodate in each batch, Ralston said there wasn’t. It was possible, he believed, for a batch to number “thousands” of startups.</p><p>Under Tan, who took over in January 2023, there’s been a refocus on the founders themselves. Tan says YC had become kind of an umbrella company saying yes to a lot of things. “I asked, ‘How do we focus on what made YC awesome in the first place?’” The answer was mentoring cool founders, chosen through an exacting application process. The continuity fund <a data-offer-url="https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund&quot;}" href="https://www.theinformation.com/articles/y-combinator-to-end-late-stage-startup-continuity-fund" rel="noopener" target="_blank">was discontinued</a>. YC had already <a data-offer-url="https://www.openresearchlab.org/blog/we-are-changing-our-name" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.openresearchlab.org/blog/we-are-changing-our-name&quot;}" href="https://www.openresearchlab.org/blog/we-are-changing-our-name" rel="noopener" target="_blank">separated itself</a> <a data-offer-url="https://www.ycombinator.com/blog/yc-research" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ycombinator.com/blog/yc-research&quot;}" href="https://www.ycombinator.com/blog/yc-research" rel="noopener" target="_blank">f</a>rom Altman’s research division, which is now called Open Research. The only remaining trace of Altman’s research operation within the company now is a financial stake in OpenAI. Most notably, batch sizes have been cut almost in half. Beginning Summer 2022, they numbered in the mid 200’s, with the current batch inching up to 260. This isn’t due to demand—27,000 companies applied for those slots.</p></div></div>]]></description>
        </item>
    </channel>
</rss>