<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 28 May 2025 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The Who Cares Era (253 pts)]]></title>
            <link>https://dansinker.com/posts/2025-05-23-who-cares/</link>
            <guid>44115620</guid>
            <pubDate>Wed, 28 May 2025 13:07:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dansinker.com/posts/2025-05-23-who-cares/">https://dansinker.com/posts/2025-05-23-who-cares/</a>, See on <a href="https://news.ycombinator.com/item?id=44115620">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <p>Earlier this week, <a href="https://bsky.app/profile/rachaelking70.bsky.social/post/3lplwve5ar22h">it was discovered</a> that the <em>Chicago Sun-Times</em> and the <em>Philadelphia Inquirer</em> <a href="https://www.npr.org/2025/05/20/nx-s1-5405022/fake-summer-reading-list-ai">had both published</a> an externally-produced "special supplement" that contained facts, experts, and book titles entirely made up by an AI chatbot. There's been <a href="https://www.theatlantic.com/technology/archive/2025/05/ai-written-newspaper-chicago-sun-times/682861/">a lot</a> written about this (<a href="https://marthabayne.substack.com/p/journalism-dreams">former <em>Chicago Reader</em> editor Martha Bayne's is the best</a>), and I don't need to rehash it all. But the thing that is most disheartening to me is how at every step along the way, nobody cared.</p>
<p>The writer didn't care. The supplement's editors didn't care. The biz people on both sides of the sale of the supplement didn't care. The production people didn't care. And, the fact that it took <em>two days</em> for anyone to discover this epic fuckup in print means that, ultimately, the reader didn't care either.</p>
<p>It's so emblematic of the moment we're in, the Who Cares Era, where completely disposable things are shoddily produced for people to mostly ignore.</p>
<p>AI is, of course, at the center of this moment. It's a mediocrity machine by default, attempting to bend everything it touches toward a mathematical average. Using <a href="https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/">extraordinary amounts of resources</a>, it has the ability to create something <em>good enough</em>, a squint-and-it-looks-right simulacrum of normality. If you don't care, it's miraculous. If you do, <a href="https://dansinker.com/posts/illusions/">the illusion</a> falls apart pretty quickly. The fact that the userbase for AI chatbots <a href="https://www.theverge.com/2024/12/4/24313097/chatgpt-300-million-weekly-users">has exploded exponentially</a> demonstrates that <em>good enough</em> is, in fact, good enough for most people. Because most people don't care.</p>
<p>(It's worth pointing out that I'm not a full-throated hater and know people—coders, mostly—who work with AI that <em>do</em> care and have used it to make real, meaningful things. Most people, however, use it quickly and thoughtlessly to make more mediocrity.)</p>
<p>It's easy to blame this all on AI, but it's not just that. Last year I was deep in negotiations with a big-budget podcast production company. We started talking about making a deeply reported, limited-run show about the concept of living in a multiverse that I was (and still am) very excited about. But over time, our discussion kept getting dumbed down and dumbed down until finally the show wasn't about the multiverse at all but instead had transformed into a daily chat show about the Internet, which everyone was trying to make back then. Discussions fell apart.</p>
<p>Looking back, it feels like a little microcosm of everything right now: Over the course of two months, we went from something smart that would demand a listener's attention in a way that was challenging and new to something that sounded like every other thing: some dude talking to some other dude about apps that some third dude would half-listen-to at 2x speed while texting a fourth dude about plans for later.</p>
<p>Hanif Abdurraqib, in one of his excellent <a href="https://www.instagram.com/nifmuhammad/?hl=en">Instagram</a> mini-essays the other week, wrote about the rise of content that's designed to be <a href="https://www.theguardian.com/tv-and-radio/2025/jan/17/not-second-screen-enough-is-netflix-deliberately-dumbing-down-tv-so-people-can-watch-while-scrolling">consumed while doing something else</a>. In Hanif's case, he was writing about <a href="https://podcasts.apple.com/us/podcast/time-machine-the-score-side-a/id1566642706?i=1000535020855"><em>Time Machine</em></a>, his incredible 90 minute deep dive into The Fugees' seminal album <em>The Score</em>. Released in 2021, Hanif marveled at the budget, time, and effort that went into crafting the two-part 90 minute podcast and how, today, there's no way it would have happened.</p>
<p>He's right. Nobody's funding that kind of work right now, because nobody cares.</p>
<p>(It's worth pointing out that Hanif wrote this using Stories, a system that erased it 24 hours later. Another victim of the Who Cares Era.)</p>
<p>Of course we're all victims of the biggest perpetrators of this uncaring era, as the Trump administration declares "Who Cares?" to vast swaths of the federal government, to public health, to immigrant families, to college students, to you, to me. As Elon Musk's DOGE rats gnaw their way through federal agencies, not caring is their guiding light. They cut indiscriminately, a smug grin on their faces. That they believe they can replace government workers—people who care an <em>extraordinary</em> amount about their arcane corner of the bureaucracy—with <a href="https://www.wired.com/story/doge-is-in-its-ai-era/">hastily-written AI code</a> is another defining characteristic of right now.</p>
<p>I keep coming back to the word "disheartening," because it all really is.</p>
<p>Without getting into too many specifics, I recently was involved in reviewing hundreds of applications for something. Over the course of reviewing, I was struck by the nearly-identical phrasing that threaded through dozens of the applications. It was eerie at first, like seeing a shadow in the distance, then frustrating, and ultimately completely disheartening: It was AI. For whatever their reasons, a bunch of people had used a chatbot to help write their answers to questions that asked them to draw from their own, unique, personal experience. They had fed their resumes or their personal websites or their actual stories and experiences into the machine, and it had filled in the blanks, Mad Libs-style. I felt crushed.</p>
<p>Until.</p>
<p>Until I read an application written entirely by a person. And then another. And another. They <em>glowed</em> with delight and joy and sadness and with the unexpected at every turn.</p>
<p>They were human.</p>
<p>They were written by people that cared.</p>
<p>In the Who Cares Era, the most radical thing you can do is care.</p>
<p>In a moment where machines churn out mediocrity, make something yourself. Make it imperfect. Make it rough. Just make it.</p>
<p>At a time where the government's uncaring boot is pressing down on all of our necks, the best way to fight back is to care. Care loudly. Tell others. Get going.</p>
<p>As the culture of the Who Cares Era grinds towards the lowest common denominator, support those that are making real things. Listen to something with your full attention. Watch something with your phone in the other room. Read an actual paper magazine or a book.</p>
<p>Be yourself.</p>
<p>Be imperfect.</p>
<p>Be human.</p>
<p>Care.</p>

    </div><p>Published May 23, 2025. | </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI: Accelerated Incompetence (214 pts)]]></title>
            <link>https://www.slater.dev/accelerated-incompetence/</link>
            <guid>44114631</guid>
            <pubDate>Wed, 28 May 2025 10:50:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.slater.dev/accelerated-incompetence/">https://www.slater.dev/accelerated-incompetence/</a>, See on <a href="https://news.ycombinator.com/item?id=44114631">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      
<article>
  <!-- Page Start inject -->
  

  <header>
    
    <div>
  
  <p><img src="https://www.slater.dev/images/doug.jpg" alt="Doug Slater">
  </p>

  
  <p><span>Doug</span>
    
    
    <span>·</span>
    <time>2025-05-19</time>
    
    
  </p>
</div>

  </header>

  

  <!-- TOC -->
  <!---->


<!---->

  <!-- Content -->
  <section><p><em>In software engineering, over-reliance on LLMs accelerates incompetence. LLMs can't replace human critical thinking.</em></p>
<p><em>The text in this essay was written without any use of AI.</em></p>
<img src="https://www.slater.dev/llm_dependence.jpg" alt="A chart showing a speculative inverse correlation between LLM dependence and IQ">
<p>
    A speculative inverse correlation between LLM dependence and IQ
  </p>
<p>By now much ink has dried on the wave of AI and LLMs which crashed upon the public consciousness in late 2022. As an experienced software engineer, I'd like to speak to two troubling engineering perspectives I've observed on LLMs.</p>
<h2 id="llms-are-my-friend">"LLMs are my friend"</h2>
<p>I don't think anyone believes that a computer program is literally their companion, so let's address the euphemistic intent of the above phrase: namely that an LLM conveys magnificent benefits upon its user.</p>
<p>Engineers who view LLMs as an ally invariably prioritize or feel pressured to prioritize velocity; for them, production trumps perspicacity. While it's true that LLMs can deliver a lot of code quickly, their use carries a long tail of <em>risks</em>.</p>
<h2 id="risks-of-using-llm">Risks of using LLM</h2>
<ul>
<li><strong>Output Risk</strong>. An LLM can give output that is blatantly incorrect, for example code that won't compile. More likely and dangerously, it can give output that is subtly and undetectably wrong, like logic bugs. The risk is elevated if the prompter is not qualified to evaluate the output, for example project managers prompting for source code.</li>
<li><strong>Input Risk</strong>. An LLM does not challenge a prompt which is leading<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">1</a></sup> or whose assumptions are flawed or context is incomplete. Example: An engineer prompts, "Provide a thread-safe list implementation in C#" and receives 200 lines of flawless, correct code. It's still the wrong answer, because the question should have been, "How can I make this code thread-safe?" and whose answer is "Use <code>System.Collections.Concurrent</code>" and 1 line of code. The LLM is not able to recognize an instance of the XY problem<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">2</a></sup> because it was not asked to.</li>
<li><strong>Future Velocity</strong>. This is your typical "tech debt" argument, but more urgent. AI can degrade the quality of your codebase <em>so fast</em>. Have you ever seen the fruits of hoarding disorder? From the outside, a house or apartment may look fine. But the inside is unsanitary, reprehensible, and nonfunctional. Developers are discovering that without strong guardrails, code produced by an LLM is like such a space.</li>
<li><strong>User Infantilization</strong>. An extinction of talent will occur within individuals and organizations that outsource thinking and problem solving to LLMs:
<ul>
<li>As senior engineers are deprived of the opportunity to learn through productive struggle, their existing problem solving and critical thinking skills atrophy:
<ul>
<li>"Microsoft research on knowledge workers found that AI-driven confidence often comes at the expense of critical thinking"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">3</a></sup></li>
<li>"In a world pushing for “reflexive AI usage,” I’m advocating for something different: thoughtful, intentional collaboration with AI that preserves the essence of coding as a craft"<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">4</a></sup></li>
<li>"LLMs give me finished thoughts, polished and convincing, but none of the intellectual growth that comes from developing them myself" <sup><a href="https://www.slater.dev/accelerated-incompetence/#references">5</a></sup></li>
</ul>
</li>
<li>Junior engineers never develop such skills to begin with and so can never in turn mentor future junior engineers.</li>
</ul>
</li>
<li><strong>Loss of Joy</strong>. Many developers are reporting that using AI robs them of flow state and the joy of creation.<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">6</a></sup> AI-generated code is miserable to read and change.</li>
</ul>
<p>In a future post, I plan to write about mitigations for each of these risks. Be sure to subscribe below if that sounds interesting.</p>
<h2 id="i-ll-become-redundant">"I'll become redundant"</h2>
<p>Source<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">7</a></sup></p>
<p>No, you won't. That said, there are certainly things you can do to further differentiate yourself from an LLM. To stay on topic, I'll defer that to a future post.</p>
<p>There are two programming competences that LLMs cannot furnish: <em>program theory</em> and <em>program entropy</em>.</p>
<h2 id="program-theory">Program Theory</h2>
<blockquote>
<p>...programming properly should be regarded as an activity by which the programmers form or achieve a certain kind of insight, a theory, of the matters at hand</p>
</blockquote>
<p>-- Peter Naur, <em>Programming as Theory Building</em>, 1985<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">8</a></sup></p>
<p>Naur was one of the greats in computing. He argued, against popular belief at the time, that a program is not its source code. Rather, the program is a shared mental construct: a <em>theory</em> or <em>design</em>. From that, the engineer derives code, but the work product of value is the design, not code.</p>
<p>To help you think about the difference between program theory and program text, consider this thought experiment: Imagine that two engineering teams of equivalent talent, A and B, are locked in separate rooms. Each team is told not to communicate with the other. Team A is tasked to write a program, for example a simple terminal-based Chess game. Team B just waits, plays real Chess, or whatever. When Team A is finished, their source code is handed to Team B. Now each team is asked in parallel to add a feature to the program, for example a virtual chess player so the game can be played solo. (We'll let Team A take a coffee break before they get started).</p>
<p><em>Question</em>: Which team will deliver a better solution?</p>
<p><em>Answer</em>: Team A, because those engineers have a fresh mental model of the program they just created, while Team B has none.</p>
<p>According to Naur, the theory matters because inevitably a program needs to be <em>maintained</em>, i.e. modified after its initial creation. If all you have is the source code and not an internalized understanding of its design, the cost for those modifications will be higher. I think we can each remember a time we were introduced to a big existing codebase. At first our productivity was near zero. As we loaded the program theory into our mind, productivity rose.</p>
<h3 id="llms-and-program-theory">LLMs and Program Theory</h3>
<p>LLMs as they currently exist cannot master a theory, design, or mental construct because they don't remember beyond their context window. Only humans can can gain and retain program theory.</p>
<h2 id="program-entropy">Program Entropy</h2>
<p>Complexity is a fundamental opposing force of programming<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">9</a></sup>, and it correlates with entropy.</p>
<blockquote>
<p>...program building is an entropy-decreasing process...program maintenance is an entropy-increasing process, and even its most skillful execution only delays the subsidence of the system into unfixable obsolescence</p>
</blockquote>
<p>-- Fred Brooks, <em>The Mythical Man-Month</em>, 1975</p>
<p>Brooks, another prominent historical figure in computing, asserted that after initial construction, the changes made to a program can only make the source code more complex. However, changes made in harmony with the design will do so at a slower rate.</p>
<h3 id="llms-and-program-entropy">LLMs and Program Entropy</h3>
<p>An LLM is a token predictor. It works only at the level of text. It is not capable of working at a conceptual level: it doesn't reason about ideas, diagrams, or requirements specifications. Everyone who has prompted an LLM with a large chunk of code has beheld that the LLM tends to apply unnecessary and bizarre changes, and the longer the conversation drags on, the more it diverges. How often have you witnessed an LLM <em>reduce</em> the complexity of a piece of code?</p>
<p>Only humans can decrease or resist complexity.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We found wisdom for the LLM age by remembering what two forerunners of our discipline had to say about software design and complexity.</p>
<p>If you had hoped that AI would launch your engineering career to the next level, be warned that it could do the opposite. <em>LLMs can accelerate incompetence.</em></p>
<p>If you're a skilled, experienced engineer and you fear that AI will make you unemployable, adopt a more nuanced view. <em>LLMs can't replace human engineering.</em></p>
<p>The business allure of AI is reduced costs through commoditized engineering, but just like offshore engineering talent brings forth mixed fruit, LLMs fall short and open risks.</p>
<p>The AI hype cycle will eventually peak<sup><a href="https://www.slater.dev/accelerated-incompetence/#references">10</a></sup>. Companies which overuse AI now will inherit a long tail of costs, and they'll either pivot or go extinct. As such, the long-term value proposition for humans in engineering remains unchanged. The world still needs and will pay for technical skills and deep thinking in engineering.</p>
<p>AI will stick around, though. Use it as a tool, not a crutch, and continue to invest in the same fundamental engineering skills that were deemed valuable in 2019.</p>
<h2 id="next">Next...</h2>
<p>Subscribe to my email list below. I plan to write more.</p>
<h2 id="references">References</h2>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Leading_question">Leading Question</a></li>
<li><a href="https://en.wikipedia.org/wiki/XY_problem">The XY Problem</a></li>
<li><a href="https://www.thoughtworks.com/content/dam/thoughtworks/documents/radar/2025/04/tr_technology_radar_vol_32_en.pdf">ThoughtWorks Technology Radar Volume 32</a></li>
<li><a href="https://cekrem.github.io/posts/coding-as-craft-going-back-to-the-old-gym/">Coding as Craft: Going Back to the Old Gym</a></li>
<li><a href="https://dcurt.is/thinking">Thoughts on Thinking</a></li>
<li><a href="https://terriblesoftware.org/2025/04/23/the-hidden-cost-of-ai-coding/">The Hidden Cost of AI Coding</a></li>
<li><a href="https://www.reddit.com/r/ExperiencedDevs/comments/1h3xpke/dont_know_if_the_right_place_how_to_work_on/">"I wonder if I'll become redundant"</a></li>
<li><a href="https://pablo.rauzy.name/dev/naur1985programming.pdf">Programming as Theory Building</a></li>
<li><a href="https://grugbrain.dev/#grug-on-complexity">Grug on Complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle">Gartner Hype Cycle</a></li>
</ol>
</section>

  <hr>

              <div>
                <h3>Subscribe for More</h3>
                <h5>I'll tell you about new posts. I take your privacy seriously.</h5>
                
            </div>
 

  <!-- Post Taxonomies -->
  


<!---->

  <!-- Post Nav -->
  
<nav>
  
  <a href="https://www.slater.dev/tech-risk-is-business-risk/"><span>←</span><span>Tech Risk is Business Risk</span></a>
  <!---->
  
</nav>

<!---->

  <!-- Comment -->
  <!---->
  


<!---->
<!---->
  

  <!-- Page End inject -->
  
</article>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CheerpJ 4.1: Java in the browser, now supporting Java 17 (preview) (105 pts)]]></title>
            <link>https://labs.leaningtech.com/blog/cheerpj-4.1</link>
            <guid>44114483</guid>
            <pubDate>Wed, 28 May 2025 10:23:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://labs.leaningtech.com/blog/cheerpj-4.1">https://labs.leaningtech.com/blog/cheerpj-4.1</a>, See on <a href="https://news.ycombinator.com/item?id=44114483">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-pagefind-body="" data-pagefind-filter="productId:blog">  <p>Around a month ago we announced <a href="https://labs.leaningtech.com/blog/cheerpj-4.0" rel="nofollow" target="_blank">CheerpJ 4.0<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, the first release of our WebAssembly-based JVM with support for multiple versions of Java.</p>
<p><a href="https://cheerpj.com/docs/getting-started" target="_blank">  Get started <svg viewBox="0 0 24 24" astro-icon="mi:arrow-right"><path fill="currentColor" d="M12.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L16.586 13H5a1 1 0 1 1 0-2h11.586l-4.293-4.293a1 1 0 0 1 0-1.414z"></path></svg> </a><a href="https://discord.leaningtech.com/" target="_blank"> <svg viewBox="0 0 640 512" astro-icon="fa-brands:discord"><path fill="currentColor" d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485.065 485.065 0 0 0 404.081 32.03a1.816 1.816 0 0 0-1.923.91 337.461 337.461 0 0 0-14.9 30.6 447.848 447.848 0 0 0-134.426 0 309.541 309.541 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.689 483.689 0 0 0-119.688 37.107 1.712 1.712 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.016 2.016 0 0 0 .765 1.375 487.666 487.666 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348.2 348.2 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321.173 321.173 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251.047 251.047 0 0 0 9.109-7.137 1.819 1.819 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.812 1.812 0 0 1 1.924.233 234.533 234.533 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.407 301.407 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391.055 391.055 0 0 0 30.014 48.815 1.864 1.864 0 0 0 2.063.7A486.048 486.048 0 0 0 610.7 405.729a1.882 1.882 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541zM222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241zm195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241z"></path></svg> Join us on Discord  </a></p>
<p>The release of CheerpJ 4.0 was focused on getting the infrastructure right, maintaining our long standing Java 8 support while introducing Java 11, and allowing further versions of Java to be supported.</p>
<p>As stated in our <a href="https://cheerpj.com/our-roadmap-for-modern-java-in-the-browser/" rel="nofollow" target="_blank">roadmap<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, our plan was to introduce Java 17 support only later in the year. Thanks to faster than expected progress, we are now releasing this feature today, much earlier than we expected, as part of <strong>CheerpJ 4.1</strong>. This is intended to be a preview of the stable support that is still scheduled for a future CheerpJ 5.0 release, expected before the end of 2025.</p>
<p>Moreover, the initial support for Java 11 in the previous release received extensive testing by the developer community on a big variety of applications and libraries. Thanks to the help of our users, several critical bugs in the original release were found and fixed. This new 4.1 release also serves as an opportunity to bring much more stability to Java 11 support.</p>
<h2 id="what-is-new">What is new?</h2>
<figure><p><img src="https://labs.leaningtech.com/_astro/Runelite.C0PpYRV-_ZtkT15.webp" alt="Runeline screenshot" width="1596" height="1050" loading="lazy" decoding="async"></p><figcaption><p>RuneLite running in the browser via CheerpJ in Java 11 mode</p></figcaption></figure>
<p>Although not long has passed since the previous release, there are already many improvements in CheerpJ, across different subsystems.</p>
<p>CheerpJ 4.1 introduces:</p>
<ul>
<li>Support for SSL and Audio in Java 11</li>
<li>Performance optimizations</li>
<li>Improved networking stack</li>
<li>Improvements to mobile usability, for both Swing and AWT</li>
<li>Preview of Java 17 support</li>
</ul>
<p>Feedback and testing from the community have been invaluable to help us improve the stability of CheerpJ. We expect that Java 17 will attract even more attention, resulting in a positive feedback loop that will bring CheerpJ ever closer to our vision of running modern Java in the browser.</p>
<h2 id="what-can-cheerpj-do">What can CheerpJ do?</h2>
<p>CheerpJ is a full WebAssembly-based JVM for the browser, and comes with a complete OpenJDK runtime, as well as a powerful emulation layer to provide file system access, general networking support and other OS-level features. It works fully client-side, via WebAssembly, JavaScript and HTML5 technologies. It is, in essence, a JavaScript library, with no server-side or cloud-based component of any sort.</p>
<p>CheerpJ is a complete, flexible Java platform for modern browsers. It is an extremely powerful tool, designed and tested to work at the scale of real-world, large enterprise applications. Here is an overview of what CheerpJ can be used for.</p>
<h3 id="running-large-scale-swing--awt-applications">Running large-scale Swing / AWT applications</h3>
<p>CheerpJ can run existing, full Java applications from unmodified JARs, with no recompilation or pre-processing, straight from bytecode. Obfuscated or encrypted JARs are supported irrespective of the obfuscator being used.</p>
<figure><figcaption><p>A complex Swing application running live. Use the bottom-right control
button to try it fullscreen.</p></figcaption></figure>

<p>Both AWT- and Swing-based applications are supported, including third-party Swing Look&amp;Feels. Multiple applications, each with multiple windows, can run at the same time on the same page.</p>
<p>CheerpJ 4.1 introduces an unprecedented level of support for mobile devices, enabling for the first time to make complex Java applications available to users across phones and, especially, tablets.</p>
<p>Running a Java application is straightforward, requiring just three calls to the CheerpJ APIs (see our <a href="https://cheerpj.com/docs/getting-started/Java-app" rel="nofollow" target="_blank">Getting Started<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> guide for a fully worked example).</p>
<div><figure><pre><code><p><span>await</span><span> </span><span>cheerpjInit</span><span>()</span><span>;</span></p><p><span>cheerpjCreateDisplay</span><span>(</span><span>800</span><span>,</span><span> </span><span>600</span><span>)</span><span>;</span></p><p><span>await</span><span> </span><span>cheerpjRunJar</span><span>(</span><span>"</span><span>/app/my_application_archive.jar</span><span>"</span><span>)</span><span>;</span></p></code></pre></figure></div>
<p>CheerpJ is built to run Java bytecode at scale, and is robust to very large applications. As a point of reference, our internal stress test is IntelliJ IDEA, an application comprising around 400MB of JAR files.</p>
<video controls="" autoplay="" loop="" muted="true" playsinline=""><source src="https://labs.leaningtech.com/blog/CJ_idea19_2025.mp4" type="video/mp4"></video>
<h3 id="using-java-libraries-as-part-of-web-applications">Using Java libraries as part of Web Applications</h3>
<p>CheerpJ makes it possible to use Java libraries from JavaScript using a natural and expressive <code>async</code>/<code>await</code> based approach, we call this feature <em>Library Mode</em>.</p>
<p>The following snippet of code should give an idea about this capability, by using the popular <code>iText</code> library to generate a PDF completely client-side in the browser.</p>
<div><figure><pre><code><p><span>async</span><span> </span><span>function</span><span> </span><span>iTextExample</span><span>()</span><span> </span><span>{</span></p><p><span>  </span><span>await</span><span> </span><span>cheerpjInit</span><span>()</span><span>;</span></p><p><span>  </span><span>const</span><span> </span><span>lib</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>cheerpjRunLibrary</span><span>(</span><span>"</span><span>/app/itextpdf-5.5.13.3.jar</span><span>"</span><span>)</span><span>;</span></p><p><span>  </span><span>try</span><span> </span><span>{</span></p><p><span>    </span><span>const</span><span> </span><span>Document</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>Document</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>Paragraph</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>Paragraph</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>PdfWriter</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>com</span><span>.</span><span>itextpdf</span><span>.</span><span>text</span><span>.</span><span>pdf</span><span>.</span><span>PdfWriter</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>FileOutputStream</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>java</span><span>.</span><span>io</span><span>.</span><span>FileOutputStream</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>document</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>new</span><span> </span><span>Document</span><span>()</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>writer</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>PdfWriter</span><span>.</span><span>getInstance</span><span>(</span></p><p><span>      </span><span>document</span><span>,</span></p><p><span>      </span><span>await</span><span> </span><span>new</span><span> </span><span>FileOutputStream</span><span>(</span><span>"</span><span>/files/HelloIText.pdf</span><span>"</span><span>)</span></p><p><span>    )</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>open</span><span>()</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>add</span><span>(</span><span>await</span><span> </span><span>new</span><span> </span><span>Paragraph</span><span>(</span><span>"</span><span>Hello World!</span><span>"</span><span>))</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>document</span><span>.</span><span>close</span><span>()</span><span>;</span></p><p><span>    </span><span>await</span><span> </span><span>writer</span><span>.</span><span>close</span><span>()</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>blob</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>cjFileBlob</span><span>(</span><span>"</span><span>/files/HelloIText.pdf</span><span>"</span><span>)</span><span>;</span></p><p><span>    </span><span>const</span><span> </span><span>url</span><span> </span><span>=</span><span> </span><span>URL</span><span>.</span><span>createObjectURL</span><span>(</span><span>blob</span><span>)</span><span>;</span></p><p><span>    </span><span>pdfDisplay</span><span>.</span><span>data</span><span> </span><span>=</span><span> </span><span>url</span><span>;</span></p><p><span>  </span><span>}</span><span> </span><span>catch</span><span> (</span><span>e</span><span>) </span><span>{</span></p><p><span>    </span><span>const</span><span> </span><span>IOException</span><span> </span><span>=</span><span> </span><span>await</span><span> </span><span>lib</span><span>.</span><span>java</span><span>.</span><span>io</span><span>.</span><span>IOException</span><span>;</span></p><p><span>    </span><span>if</span><span> (</span><span>e</span><span> </span><span>instanceof</span><span> </span><span>IOException</span><span>) </span><span>console</span><span>.</span><span>log</span><span>(</span><span>"</span><span>I/O error</span><span>"</span><span>)</span><span>;</span></p><p><span>    </span><span>else</span><span> </span><span>console</span><span>.</span><span>log</span><span>(</span><span>"</span><span>Unknown error: </span><span>"</span><span> </span><span>+</span><span> (</span><span>await</span><span> </span><span>e</span><span>.</span><span>getMessage</span><span>()))</span><span>;</span></p><p><span>  </span><span>}</span></p><p><span>}</span></p></code></pre></figure></div>
<p><em>Library Mode</em> provides extensive access to Java, with these main features:</p>
<ul>
<li>Creating new objects</li>
<li>Calling static and instance methods. Overloading is supported and the correct method is resolved taking into account the argument types.</li>
<li>Accessing static and instance fields, both for reading and for writing.</li>
<li>Handling Java exceptions from JavaScript (via regular <code>try</code>/<code>catch</code> blocks)</li>
</ul>
<p><em>Library Mode</em> is a unique feature, which makes it possible to build a new generation of fully client-side Web applications that combine Web-native components with Java libraries to implement complex functionalities.</p>
<p>In more enterprise scenarios, this approach can be used to progressively migrate large-scale Java applications to native Web apps, by rewriting the UI while keeping all or part of the original business logic in Java. This can provide significant reduction of risk, costs, and timeline to large modernisation projects.</p>
<p>Check out our dedicated documentation for more information on <a href="https://cheerpj.com/docs/getting-started/Java-library" rel="nofollow" target="_blank">Library Mode<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a></p>
<h2 id="demo-unmodified-minecraft-in-the-browser">Demo: Unmodified Minecraft in the browser</h2>
<video controls="" autoplay="" loop="" muted="true" playsinline=""><source src="https://labs.leaningtech.com/blog/browsercraft4.mp4" type="video/mp4"></video>
<p>To showcase the capabilities of CheerpJ, we created a side project named <a href="https://browsercraft.cheerpj.com/" rel="nofollow" target="_blank">Browsercraft<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>, a web-based “embedding” of a historical Java version of Minecraft.</p>
<p>Contrary to other approaches you might have seen, Browsercraft is not based on decompilation or reverse engineering attempts. The original <code>client.jar</code> is fetched directly from Mojang servers on the end-user browser and runs unmodified. The LWJGL dependency, available from Maven, is also unmodified.</p>
<p>How LWJGL works in CheerpJ is particularly interesting, since it is only <em>superficially</em> Java. Most of its value comes from JNI methods which provide direct access to each and every method exposed by OpenGL. These methods are written in C and automatically generated by the LWJGL build system from a declarative representation of the OpenGL API.</p>
<p>CheerpJ 4.1 introduces support for these scenarios via JNI WebAssembly modules, which are loaded and executed dynamically, similarly to what happens on native platforms via shared libraries. Browsercraft takes advantage of this capability for LWJGL native code and also for the <a href="https://github.com/ptitSeb/gl4es/" rel="nofollow" target="_blank">gl4es<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> library. This latter component provides a compatibility layer between OpenGL, used by Minecraft, and GLES as provided by WebGL.</p>
<p>By combining these WebAssembly modules and the unmodified JARs, CheerpJ can now correctly render Minecraft in the browser. It should be noted that Minecraft is a notoriously inefficient and resource intensive application, so we consider it to be a <em>stress test</em> for CheerpJ. Nevertheless, thanks to recent improvements in our JIT compiler, the demo can now run with satisfactory performance on most mid range machines. The situation will further improve in the future thanks to more advanced optimizations currently planned, stay tuned.</p>
<h2 id="whats-next">What’s next?</h2>
<p>Development of CheerpJ is moving fast. The CheerpJ 5.0 release is still scheduled for later in the year as originally announced in our <a href="https://cheerpj.com/our-roadmap-for-modern-java-in-the-browser/" rel="nofollow" target="_blank">roadmap<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>.</p>
<p>There is a long list of improvements that we plan to ship, here are as few highlights:</p>
<ul>
<li><strong>Stable Java 17 support</strong>: With CheerpJ 4.1 we have released a preview of what Java 17 support will look like in the not so distant future, we plan to gather feedback from users to achieve a high level of stability by the time CheerpJ 5.0 is released.</li>
<li><strong>NPM support</strong>: CheerpJ has been historically used primarily to run legacy Java applications, most usually in environments that pre-date the npm ecosystem. Now that CheerpJ can run modern Java we want to provide a npm-native CheerpJ version, to streamline integration into modern projects.</li>
<li><strong>Extend native JNI modules support</strong>: This will allow us to support JavaFX, SWT and eventually allow users to build their own Wasm JNI module for any Java library.</li>
</ul>
<h2 id="licensing">Licensing</h2>
<p>CheerpJ is commercial software, but it’s free to use for FOSS projects, personal projects and one-person companies. Affordable and transparent licensing apply to small businesses.</p>
<p>Enterprise licensing and support are available, with significant discounts for non-profit and educational institutions. For more information see <a href="https://cheerpj.com/cheerpj-core/#compare-plans" rel="nofollow" target="_blank">Licensing<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>.</p>
<h2 id="try-it-out-and-join-the-community">Try it out and join the community</h2>
<p>You can find a lot of beginner-friendly resources in our developer documentation, as well as tutorials and a full API reference.</p>
<p><a href="https://cheerpj.com/docs/getting-started" target="_blank">  Get started <svg viewBox="0 0 24 24" astro-icon="mi:arrow-right"><path fill="currentColor" d="M12.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L16.586 13H5a1 1 0 1 1 0-2h11.586l-4.293-4.293a1 1 0 0 1 0-1.414z"></path></svg> </a></p>
<p>For questions, discussion, and support, join our <a href="https://discord.leaningtech.com/" rel="nofollow" target="_blank">Discord<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>. It’s an active community where both Leaning Technologies developers and experienced users can provide help.</p>
<p>Following the success of the second edition of the <a href="https://cheerpx.io/hackathon" rel="nofollow" target="_blank">WebVM Hackathon<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> earlier this year, we have decided to host the first <a href="https://cheerpj-the-hackathon.devpost.com/" rel="nofollow" target="_blank">CheerpJ Hackathon<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a>. The event theme and precise dates are still being determined, but it will be a week-long competition to be held between September and October 2025, with a £500 prize awaiting the winning team. <a href="https://cheerpj-the-hackathon.devpost.com/" rel="nofollow" target="_blank">Sign up now<span><img src="https://labs.leaningtech.com/icons/external-link.svg"></span></a> to stay updated.</p>
<hr>
<p>CheerpJ is a product built with passion and a lot of coffee by Leaning Technologies, an international team of WebAssembly hackers based in Amsterdam (NL) and Leeds (UK). We hope you’ll love it as much as we do.</p>
<a href="https://github.com/leaningtech/cheerpj-meta" target="_blank"> <svg viewBox="0 0 1664 1600" astro-icon="fa:star"><path fill="currentColor" d="M1664 615q0 22-26 48l-363 354 86 500q1 7 1 20 0 21-10.5 35.5T1321 1587q-19 0-40-12l-449-236-449 236q-22 12-40 12-21 0-31.5-14.5T301 1537q0-6 2-20l86-500L25 663Q0 636 0 615q0-37 56-46l502-73L783 41q19-41 49-41t49 41l225 455 502 73q56 9 56 46z"></path></svg> Star CheerpJ on GitHub  </a>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cory Doctorow on how we lost the internet (116 pts)]]></title>
            <link>https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</link>
            <guid>44113735</guid>
            <pubDate>Wed, 28 May 2025 08:07:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/">https://lwn.net/SubscriberLink/1021871/4bec46993258f6b7/</a>, See on <a href="https://news.ycombinator.com/item?id=44113735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<blockquote>
<div>
<h3>Welcome to LWN.net</h3>
<p>
The following subscription-only content has been made available to you 
by an LWN subscriber.  Thousands of subscribers depend on LWN for the 
best news from the Linux and free software communities.  If you enjoy this 
article, please consider <a href="https://lwn.net/subscribe/">subscribing to LWN</a>.  Thank you
for visiting LWN.net!
</p></div>
</blockquote>

<p>
Cory Doctorow <a href="https://craphound.com/bio/">wears many hats</a>:
digital activist, science-fiction author, journalist, and more.  He has
also written many books, both fiction and non-fiction, runs the <a href="https://pluralistic.net/">Pluralistic blog</a>, is a visiting
professor, and is an advisor to the <a href="https://www.eff.org/">Electronic
Frontier Foundation</a> (EFF); his <a href="https://chokepointcapitalism.com/"><i>Chokepoint Capitalism</i></a>
co-author, Rebecca Giblin, gave a <a href="https://lwn.net/Articles/927278/">2023 keynote
in Australia</a> that we covered.  Doctorow gave a rousing keynote on
the state of the "enshitternet"—today's internet—to kick
off the recently held <a href="https://us.pycon.org/2025/">PyCon US
2025</a> in Pittsburgh, Pennsylvania.
</p>

<p>
He began by noting that he is known for coining the term
"<a href="https://en.wikipedia.org/wiki/Enshittification">enshittification</a>" about the decay of tech platforms, so attendees were
probably expecting to hear about that; instead, he wanted to start by
talking about nursing.  A <a href="https://rooseveltinstitute.org/publications/uber-for-nursing/">recent
study</a> described how nurses are increasingly getting work through one of
three main apps that "<q>bill themselves out as 'Uber for nursing'</q>".
The nurses never know what they will be paid per hour prior to accepting a
shift and the three companies act as a cartel in order to "<q>play all
kinds of games with the way that labor is priced</q>".
</p>

<p>
In particular, the
companies purchase financial information from a data broker before offering
a nurse a shift; if the nurse is carrying a lot of credit-card debt,
especially if some of that is delinquent, the amount offered is
reduced. "<q>Because, the more desperate you are, the less you'll accept to
come into work and do that grunt work of caring for the sick, the elderly,
and the dying.</q>"  That is horrific on many levels, he said, but "<q>it
is emblematic of 'enshittification'</q>", which is one of the reasons he
highlighted it.
</p>

<h4>Platform decay</h4>

<p>
Enshittification is a three-stage process; he used Google to
illustrate the idea.  At first, Google minimized ads and maximized spending
on engineering to produce a great search engine; while it was doing that,
however, it was buying its way to dominance. "<q>They bribed every service,
every product
that had a search box to make sure that that was a Google search box.</q>"
No matter which browser, phone carrier, or operating system you were using,
Google ensured that you were using its search by default; by the early
2020s, it was spending the equivalent of buying a Twitter every 18 months
to do so, he said.  That is the first stage of the process: when the
provider is being good to its users, but is finding ways to lock them in.
</p>

<p><a href="https://lwn.net/Articles/1022663/">
<img src="https://static.lwn.net/images/2025/pycon-doctorow-sm.png" alt="[Cory Doctorow]" title="Cory Doctorow" width="206" height="300">
</a></p><p>
The second phase occurs once the company recognizes that it has users
locked in, so it will be difficult for them to switch away, and it shifts
to making things worse for its users in order to enrich its business
customers.  For Google, those are the publishers and advertisers.  A
growing portion of the search results page is shifted over to ads
"<q>marked off with ever-subtler, ever-smaller, ever-grayer labels
distinguishing them from the organic search results</q>".  While the
platform is getting better for business customers—at the expense of the
users—those customers are also getting locked in.
</p>

<p>
Phase three of enshittification is when the value of the platform is
clawed back until all that is left is kind of a "<q>homeopathic residue—the
least value needed to keep both business customers and end users locked to
the platform</q>".  We have gained a view into this process from the three
monopoly cases that Google has lost over the last 18 months. In 2019, the
company had 90% of the world's search traffic and its users were loyal;
"<q>everyone who searched on Google, searched everything on Google</q>".
</p>

<p>
But that meant that Google's search growth had plateaued, so how was the
company going to be able to grow?  It could "<q>raise a billion humans to
adulthood and make them Google customers, which is <a href="https://classroom.google.com/">Google Classroom</a>, but that's a
slow process</q>".  From the internal memos that came to light from the
court cases, we can see what the company chose to do, he said: "<q>they
made search worse</q>".  
</p>

<p>
The accuracy of the search results was reduced, which meant that users
needed to do two or three queries to the get the results they would have
seen on the first page.  That increased the number of ads that could be
shown, which is obviously bad for searchers, but the company was also
attacking its business customers at the same time.  For example, "<q>Google entered into
an illegal, collusive arrangement with Meta, called <a href="https://en.wikipedia.org/wiki/Jedi_Blue">Jedi Blue</a></q>" that
"<q>gamed the advertising market</q>" so that publishers got paid less and
advertisers had to pay more, he said.
</p>

<p>
So that's how we have ended up at the Google of today, where the top of the
search results page is "<q>a mountain of AI slop</q>", followed by five
paid results "<q>marked with the word 'Ad' in eight point, 90%
gray-on-white type</q>", ending with "<q>ten spammy SEO [search-engine
optimization] links from someone else who's figured out how to game
Google</q>".  The amazing thing is "<q>that we are still using Google
because we're locked into it</q>".  It is a perfect example of the result
of the "<q>tragedy in three acts</q>" that is enshittification.
</p>

<h4>Twiddling</h4>

<p>
The underlying technical means that allows this enshittification is
something he calls "twiddling".  Because the companies run their apps on
computers, they can change a nearly infinite number of knobs to potentially
alter "<q>the prices, the cost, the search rankings, the
recommendations</q>" each time the platform is visited.  Going back to the
nursing example, "<q>that's just twiddling, it's something you can only do
with computers</q>".
</p>

<p>
Legal scholar Veena Dubal coined the term "<a href="https://en.wikipedia.org/wiki/Algorithmic_wage_discrimination">algorithmic
wage discrimination</a>" to describe this kind of twiddling for the "gig
economy", which is "<q>a major locus for enshittification</q>"; the nursing
apps, Uber, and others are examples of that economy. "<q>Gig work is that
place where your shitty boss is a shitty app and you're not allowed to call
yourself an employee.</q>"
</p>

<p>
Uber invented a particular form of algorithmic wage discrimination; if its
drivers are picky about which rides they accept, Uber will slowly raise the
rates to entice those drivers—until they start accepting rides.  Once a
driver does accept a ride, "<q>the wage starts to push down and down at
random intervals in increments that are too small for human beings to
readily notice</q>".  It is not really "<q>boiling the frog</q>", Doctorow
said, so much as it is "<q>slowly poaching it</q>".
</p>

<p>
As anyone with a technical background knows, "<q>any task that is simple,
but time-consuming is a prime candidate for automation</q>".  This
kind of "<q>wage theft</q>" would be tedious and expensive to do by hand,
but it is trivial to play these games using computers.  This kind of thing
is not just bad for nurses, he said, its bad for those who are using their
services.
</p><blockquote>
Do you really think that paying nurses based on how desperate
they are, at a rate calculated to increase their desperation so that
they'll accept ever-lower wages,
is going to result in us getting the best care when we see a nurse?  Do you
really want your catheter inserted by a nurse on food stamps who drove an
Uber until midnight the night before and skipped breakfast this morning so
that they could pay the rent?
</blockquote>


<h4>Paying and products</h4>

<p>
It is misguided to say "<q>if you're not paying for the product, you're the
product</q>", because it makes it seem like we are complicit in sustaining
<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance
capitalism</a>—and we are not. The thinking goes that if we were only
willing to start paying for things, "<q>we could restore capitalism to its
functional non-surveillance state and companies would treat us better
because we'd be customers and not products</q>".  That thinking elevates
companies like Apple as "<q>virtuous alternatives</q>" because the company
charges money and not attention, so it can focus on improving the
experience for its customers.
</p>

<p>
There is a small sliver of truth there, he said; Apple rolled out a feature
on its phones that allowed users to opt-out of third-party
surveillance—notably Facebook tracking.  96% of users opted out, he said;
the other 4% "<q>were either drunk or Facebook employees or drunk Facebook
employees</q>".  
</p>

<p>
So that makes it seem like Apple will not treat its customers as products,
but at the same time it added the opt-out, the company secretly started gathering
exactly the same information for its "<q>own surveillance
advertising network</q>".  There was no notice given to users and no way to
opt out of that surveillance; when journalists discovered it and published
their findings, Apple "<q>lied about it</q>".  The "<q>$1000 Apple
distraction rectangle in your pocket is something you paid for</q>", but
that does not stop Apple from "<q>treating you like the product</q>".
</p>

<p>
It is not just end users that Apple treats like products; the app vendors
are also treated that way with 30% fees for payment processing in the App
Store. That's what is happening with gig-app nurses: "<q>the nurses are the
product, the patients are the product, the hospitals are the product—in
enshittification, the product is anyone you can productize</q>".
</p>

<p>
While it is tempting to blame tech, Doctorow said, these companies did not
start out enshittified.  He recounted the "<q>magic</q>"  when Google debuted;
"<q>you could <a href="https://en.wikipedia.org/wiki/Ask.com">ask
Jeeves</a> questions for a thousand years and still not get an answer as
crisp, as useful, as helpful as the answer you would get by typing a few
vague keywords</q>" into Google.  Those companies spent decades producing
great products, which is why people switched to Google, bought iPhones, and
joined their friends on Facebook.  They were all born digital, thus could
have enshittified at any time, "<q>but they didn't, until they did, and
then they did it all at once</q>". 
</p>

<p>
He believes that changes to the policy environment is what has led to
enshittification, not changes in technology.  These changes to the rules of
the game were "<q>undertaken in living memory by named parties who were
warned at the time of the likely outcomes</q>"—and did it anyway.
Those people are now extremely rich and respected; they have "<q>faced no
consequences, no accountability for their role in ushering in the
Enshittocene</q>".  We have created a perfect breeding ground for the worst
practices in our society, which allowed them to thrive and dominate
decision-making for companies and governments "<q>leading to a vast
enshittening of everything</q>".
</p>

<p>
That is a dismal outlook, he said, but there is a bit of good news hidden
in there.  This change did not come about because of a new kind of evil
person or the weight of history, but rather because of specific policy
choices that were made—and can be unmade.  We can consign the enshitternet
to the scrap heap as
simply "<q>a transitional state from the old good internet that we used to
have and the new good internet that we could have</q>".
</p>

<p>
All companies want to maximize profits and the equation to do so is simple:
charge as much as you can, pay suppliers and workers as little as you can,
and spend the smallest amount possible on quality and safety.  The
theoretically "perfect" company that charges infinity and spends nothing
fails because no one wants to work for it—or buy anything from it.  That
shows that there are external constraints that tend to tamp down the
"<q>impulse to charge infinity and deliver nothing</q>".
</p>

<h4>Four constraints</h4>

<p>
In technology, there are four constraints that help make companies
better; they help push back against the impulse to enshittify.  The first
is markets; businesses that charge more and deliver less lose customers,
all else being equal.  This is the bedrock idea behind capitalism and it is
also the basis of antitrust law, but the
rules on antitrust have changed since the <a href="https://en.wikipedia.org/wiki/Sherman_Antitrust_Act">Sherman
Antitrust Act</a> was enacted in 1890.  More than forty years ago, during the Reagan
administration in the US, the interpretation of what it means to be a
monopoly was changed, not just in US, but also with its major trading
partners in the UK, EU, and Asia.
</p>

<p>
Under this interpretation, monopolies are assumed to be efficient; if
Google has 90% of the market, it means that it deserves to be there because
no one can possibly do search any better.  No competitor has arisen because
there is no room to improve on what Google is doing. This pro-monopoly
stance did exactly what might be expected, he said, it gave us more
monopolies: "<q>in pharma, in beer, in glass bottles, vitamin C, athletic
shoes, microchips, cars, mattresses, eyeglasses, and, of course,
professional wrestling</q>", he said to laughter.
</p>

<p>
Markets do not constrain technology firms because those firms do not compete
with their rivals—they simply buy their rivals instead. That is confirmed
by a memo from Mark Zuckerberg—"<q>a man who puts all of his dumbest ideas
in writing</q>"—who wrote: "<q>It is better to buy than to compete</q>".
Even though that anti-competitive behavior came to light before Facebook
was allowed to buy Instagram in order to ensure that users switching would
still be part of Facebook the platform, the Obama administration
permitted the sale.  Every government over the past 40 years, of all political stripes, has treated monopolies as efficient,
Doctorow said.
</p>

<p>
Regulation is also a constraint, unless the regulators have already been
captured by the industry they are supposed to oversee.  There are several
examples of <a href="https://en.wikipedia.org/wiki/Regulatory_capture">regulatory
capture</a> in the nursing saga, but the most egregious is that anyone in
the US can obtain financial information on anyone else in the country,
simply by contacting a data broker.  "<q>This is because the US congress
has not passed a new consumer privacy law since 1988.</q>"  The <a href="https://en.wikipedia.org/wiki/Video_Privacy_Protection_Act">Video
Privacy Protection Act</a> was aimed at stopping video-store clerks from
telling newspapers what VHS video titles were purchased or rented, but no
protections have been added since then.
</p>

<p>
The reason congress has not addressed privacy legislation "<q>since <a href="https://en.wikipedia.org/wiki/Die_Hard"><i>Die
Hard</i></a> was in its first run in theaters</q>" is neither a coincidence
nor an oversight, he said.  It is "<q>expensively purchased inaction</q>"
by an industry that has "<q>monetized the abuse of human rights at
unimaginable scale</q>".  The coalition in favor of freezing privacy law
keeps growing because there are so many ways to "<q>transmute the
systematic invasion of our privacy into cash</q>". 
</p>

<p>
Tech companies are not being constrained by either markets or governments,
but there are two other factors that could serve to tamp down "<q>the
reproduction of sociopathic, enshittifying monsters</q>" within these
companies.  The first is interoperability; in the non-digital world, it is
a lot of work to, say, ensure that any light bulb can be used with any
light socket.
In the digital world, all of our programs run on the same
"<q>Turing-complete, universal <a href="https://en.wikipedia.org/wiki/Von_Neumann_architecture">Von Neumann machine</a></q>", so a program that
breaks interoperability can be undone with a program that restores it.
Every ten-foot fence can be surmounted with an 11-foot ladder; if HP writes
a program to ensure that third-party ink cannot be used with its printers, someone
can write a program to undo that restriction.
</p>

<p>
DoorDash workers generally make their money on tips, but the app hides the
amount of the tip until the driver commits to taking the delivery.  A
company called Para wrote a program that looked inside the JSON that was
exchanged to find the tip, which it then displayed <i>before</i> the driver
had to commit.  DoorDash shut down the Para app, "<q>because in America,
apps like Para are illegal</q>".  The 1998 <a href="https://en.wikipedia.org/wiki/Digital_Millennium_Copyright_Act">Digital
Millennium Copyright Act</a> (DMCA) signed by Bill Clinton "<q>makes it a
felony to 'bypass an access control for a copyrighted work'</q>".  So even
just reverse-engineering the DoorDash app is a potential felony, which is
why companies are so desperate to move their users to apps instead of web
sites.  "<q>An app is just a web site that we have wrapped in a correct
DRM [<a href="https://en.wikipedia.org/wiki/Digital_rights_management">digital
rights management</a>] to make it a felony to protect your privacy while
you use it</q>", he said to widespread applause.
</p>

<p>
At the behest of the US trade representative, Europe and Canada have also
enacted DMCA-like laws.  This happened despite experts warning the leaders
of those countries that "<q>laws that banned tampering with digital locks
would let American tech giants corner digital markets in their
countries</q>".  The laws were a gift to monopolists and allowed companies
like HP to continually raise the price of ink until it "<q>has become the
most expensive substance you, as a civilian, can buy without a permit</q>";
printing a shopping list uses "<q>colored water that costs more than the
semen of a <a href="https://en.wikipedia.org/wiki/Kentucky_Derby">Kentucky-Derby</a>-winning
stallion</q>".
</p>

<p>
The final constraint, which did hold back platform decay for quite some
time, is labor. Tech workers have historically been respected and
well-paid, without unions.  The power of tech workers did not come from
solidarity, but from scarcity, Doctorow said.  The minute bosses ordered
tech workers to enshittify the product they were loyally working on,
perhaps missing various important social and family events  to
ship it on time, those workers could say no—perhaps in a much more coarse
way.  Tech workers could simply walk across the street "<q>and have a new
job by the end of the day</q>" if the boss persisted.
</p>

<p>
So labor held off enshittification after competition, regulation, and
interoperability were all systematically undermined and did so for quite
some time—until the mass tech layoffs.  There have been half a million
tech workers laid off since 2023, more are announced regularly, sometimes
in conjunction with raises for executive salaries and bonuses.  Now,
workers cannot turn their bosses down because there are ten others out
there just waiting to take their job.
</p>

<h4>Reversing course</h4>

<p>
Until we fix the environment we find ourselves in, the contagion will
spread to other companies, he said.  The good news is that after 40 years
of antitrust decline, there has been a lot of worldwide antitrust activity
and it is coming from all over the political spectrum.  The EU, UK,
Australia, Germany, France, Japan, South Korea, "<q>and China, yes,
China</q>" have passed new antitrust laws and launched enforcement actions.
The countries often collaborate, so a UK study on Apple's 30%
payment-processing fee was used by the EU to fine the company for billions
of euros and ban Apple's payment monopoly; those cases then found their way
to Japan and South Korea where Apple was further punished.
</p>

<p>
"<q>There are no billionaires funding the project to make billionaires
obsolete</q>", Doctorow said, so the antitrust work has come from and been
funded by
grassroots efforts.
</p>

<p>
Europe and Canada have passed strong right-to-repair legislation, but those
efforts "<q>have been hamstrung by the anti-circumvention laws</q>" (like
the DMCA).  Those laws can only be used if there are no locks to get
around, but the manufacturers ensure that every car, tractor, appliance,
medical implant, and hospital medical device has locks to prevent repair.
That raises the question of why these countries don't repeal their versions
of the DMCA.
</p>

<p>
The answer is tariffs, it seems.  The US trade representative has long
threatened countries with tariffs if they did not have such a law on their
books. "<q>Happy 'Liberation Day' everyone</q>", he said with a smile,
which resulted in laughter, cheering, and applause.  The response of most
countries when faced with the US tariffs (or threats thereof) has been to
impose retaliatory tariffs, making US products more expensive for their
citizens, which is a weird way to punish Americans.  "<q>It's like punching
yourself in the face really hard and hoping someone else says 'ouch'.</q>"
</p>

<p>
What would be better is for the countries to break the monopolies of the US
tech giants by making it legal to reverse-engineer, jailbreak, and modify
American products and services.  Let companies jailbreak Teslas and deliver
all of the features that ship in the cars, but are disabled by software,
for one price; that is a much better way to hurt Elon Musk, rather than by
expressing outrage at his Nazi salutes, since he loves the
attention. "<q>Kick him in the dongle.</q>"
</p>

<p>
Or, let
a Canadian company set up an App Store that only charges 3% for payment
processing, which will give any content producer an immediate 25% raise, so
publishers will flock to it.   The same could be done for car and tractor
diagnostic devices and more.
"<q>Any country in the world has it right now in their power to become a
tech-export powerhouse.</q>"
Doing so would directly attack the tech giants in their most profitable
lines of business: "<q>it takes the revenues
from those rip-off scams globally from hundreds of billions of dollars to
zero overnight</q>".  And "<q>that is how you win a trade war</q>", he said
to more applause.
</p>

<p>
He finished with a veritable laundry list of all of the ills facing the
world today (the "<q>omni-shambolic poly-crisis</q>"), both on and off the
internet, and noted that the tech giants
would willingly "<q>trade a habitable planet and human rights for a 3% tax
cut</q>".  But it did not have to be this way, "<q>the enshitternet was not
inevitable</q>" and was, in fact, the product of policy choices made by
known people in the last few decades.  "<q>They chose enshittification; we
warned them what would come of it and we don't have to be eternal prisoners
of the catastrophic policy blunders of clueless lawmakers of old.</q>"
</p>

<p>
There once was an "<q>old good internet</q>", Doctorow said, but it was
too difficult for non-technical people to connect up to; web 2.0 changed
that, making it easy for everyone to get online, but that led directly into
hard-to-escape walled gardens.  A new good internet is possible and needed; "<q>we can
build it with all of the technological self-determination of the old good
internet and the ease of web 2.0</q>".  It can be a place to come together
and organize in order to "<q>resist and survive climate collapse, fascism,
genocide, and authoritarianism</q>".  He concluded: "<q>we can build it and
we must</q>".
</p>

<p>
His speech was well-received and was met with a standing ovation.  Some of
his harshest rhetoric (much of which was toned down here) may not have been
popular with everyone, perhaps especially the PyCon sponsors who were named and
shamed in the keynote, but it did seem to resonate within the crowd of
attendees.   Doctorow's perspective is always interesting—and he certainly
pulls no punches.
</p>

<p>
A <a href="https://www.youtube.com/watch?v=ydVmzg_SJLw">YouTube video</a>
of the talk is available.
</p>

<p>
[I would like to thank LWN's travel sponsor, the Linux Foundation, for
supporting my travel to Pittsburgh for PyCon.]
</p><br clear="all"><table>
           <tbody><tr><th colspan="2">Index entries for this article</th></tr>
           <tr><td><a href="https://lwn.net/Archives/ConferenceIndex/">Conference</a></td><td><a href="https://lwn.net/Archives/ConferenceIndex/#PyCon-2025">PyCon/2025</a></td></tr>
            </tbody></table><br clear="all">

               <br clear="all">
               <hr>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why are 2025/05/28 and 2025-05-28 different days in JavaScript? (129 pts)]]></title>
            <link>https://brandondong.github.io/blog/javascript_dates/</link>
            <guid>44113397</guid>
            <pubDate>Wed, 28 May 2025 07:09:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://brandondong.github.io/blog/javascript_dates/">https://brandondong.github.io/blog/javascript_dates/</a>, See on <a href="https://news.ycombinator.com/item?id=44113397">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article><header>  <time datetime="2025-05-28">2025-05-28</time> </header><p>While setting up this site itself, I ran into the following oddity:</p>
<pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025/05/28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>).<span>toDateString</span>()); <span>// Tue May 27 2025</span>
<span>// Bonus: (omit leading 0)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-5-28'</span>).<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre>
<p>You may get different results on your machine!</p>
<section id="What's_going_on?"> <h2><a href="#What's_going_on?">What's going on?</a></h2> <p>A <code>Date</code> in JavaScript always represents a point in time (i.e. milliseconds since epoch). This is more apparent when printing out the full date string:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025/05/28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Wed May 28 2025</span></code></pre><p>In this case, the passed-in date string is being interpreted as a timestamp in my local time zone. <code>toDateString()</code> also operates relative to the local time and so we get the same day-of-the-month back out.</p><p>The difference with <code>'2025-05-28'</code> is in parsing behavior; the string is interpreted as UTC and so ends up at a different point in time:</p><pre><code><span>const</span> date = <span>new</span> <span>Date</span>(<span>'2025-05-28'</span>);
<span>console</span>.<span>log</span>(date); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(date.<span>toDateString</span>()); <span>// Tue May 27 2025</span></code></pre><p>Why the discrepancy?</p> </section>
<section id="The_misadventures_of_browser_date-parsing"> <h2><a href="#The_misadventures_of_browser_date-parsing">The misadventures of browser date-parsing</a></h2> <p>After digging through the code and commit histories of Chrome/Firefox/Safari, I’ve reconstructed a timeline:</p><ol>
<li>In 2009, these browsers supported parsing a mishmash of date-time formats. When time zone offsets are not explicitly specified in the string, they all fall back to using local time, including for a date string like <code>'2025/05/28'</code>.</li>
<li><a href="https://ecma-international.org/wp-content/uploads/ECMA-262_5th_edition_december_2009.pdf" target="_blank">ES5</a>, to be released at the end of the year, includes a requirement for supporting a new standardized date-time format based heavily off of <a href="https://en.wikipedia.org/wiki/ISO_8601" target="_blank">ISO 8601</a>. This format is broken up into date-<em>only</em> forms like <code>'2025-05-28'</code> and date-<em>time</em> forms like <code>'2025-05-27T17:00-07:00'</code> where the ending UTC offset is optional.
<ul>
<li>What does the spec say about time zone interpretation for date-only forms (which never have an offset) or date-time forms missing an offset? Only that <q>The String may be interpreted as a local time, a UTC time, or a time in some other time zone, depending on the contents of the String.</q> (Gee, thanks…)</li>
</ul>
</li>
<li>Firefox is the first to <a href="https://github.com/mozilla-firefox/firefox/commit/b866df4f3680502a8e78e67bd495a96ea3d9c59e" target="_blank">implement this requirement</a>. They choose to interpret date-only forms as UTC and date-time forms missing an offset as local time. Not only is there now a discrepancy between <code>'2025/05/28'</code> and <code>'2025-05-28'</code>, but also surprising behavior like: <pre><code><span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28'</span>)); <span>// Tue May 27 2025 17:00:00 GMT-0700 (Pacific Daylight Time)</span>
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(<span>'2025-05-28T00:00'</span>)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre></li>
<li>Chrome is <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">next</a>, choosing to use local time for both.</li>
<li>Safari is <a href="https://github.com/WebKit/WebKit/commit/d9bdbae4126006e130914e5ebe57a761d3ea19bb" target="_blank">next</a>, but its parsing logic incorrectly requires that all date, time, and offset fields be present.</li>
<li><a href="https://262.ecma-international.org/5.1/index.html#sec-15.9.4.2" target="_blank">ES5.1</a> releases in mid-2011 and now additionally mentions that <q>The value of an absent time zone offset is Z.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">updates its implementation</a> to use UTC for both cases.</li>
<li>Safari <a href="https://github.com/WebKit/WebKit/commit/a841b97de44dbff4ecb30f62e984b1fc72493ac6" target="_blank">fixes the earlier bug</a> and uses UTC for both cases.</li>
<li>A <a href="https://web.archive.org/web/20141214115940/https://bugs.ecmascript.org/show_bug.cgi?id=112" target="_blank">bug</a> is filed against the spec itself, pointing out that ISO 8601 represents date-times without offsets as local time. <a href="https://262.ecma-international.org/6.0/index.html#sec-date-time-string-format" target="_blank">ES6</a> in 2015 replaces the ES5.1 addition with <q>If the time zone offset is absent, the date-time is interpreted as a local time.</q></li>
<li>Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">switches back</a> to using local time for both cases.</li>
<li>A <a href="https://issues.chromium.org/issues/40440226" target="_blank">bug</a> is filed against Chrome for breaking backwards compatibility when parsing date-only forms. They <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">revert the previous change</a>.</li>
<li>Chrome files an <a href="https://github.com/tc39/ecma262/issues/87" target="_blank">issue</a> against the spec and after discussion, it’s decided to switch date-only forms back to UTC but leave date-time forms without offset as local (i.e. Firefox’s behavior).</li>
<li><a href="https://262.ecma-international.org/7.0/index.html#sec-date.parse" target="_blank">ES7</a> releases with the updated requirement. Chrome <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">makes the change</a> and then eventually, <a href="https://github.com/WebKit/WebKit/commit/2148a43f377e67c60b167f5730c7b5c5c21b202d" target="_blank">Safari</a>.</li>
</ol><p>This behavior has been maintained to the present day where every possible string accepted by the <code>Date</code> constructor falls back to local time <em>except</em> valid ISO date-strings like <code>'2025-05-28'</code>.</p><p>What’s interesting looking at the timeline is that despite being designed as a standardized format, from its release in 2009 up until early 2020, there would never exist a point where the major browsers behaved consistently for missing offsets. Meanwhile, Chrome has flipped hilariously from <a href="https://chromium.googlesource.com/v8/v8.git/+/6ceb02e6eb791f837ed84b7ed41332058cd3f1dc" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/ff9ce1abd4add01bbb3f1917bd72207e5ddd70b5" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/f06754a8e1d305a43560705f6c167d85d40e602d" target="_blank">local</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/dd3f1ecf719afd21b4c695c776b4da2fb494ef92" target="_blank">UTC</a> → <a href="https://chromium.googlesource.com/v8/v8.git/+/d31c5410c4fdfc5eb66582892d5e3ecd3706bd58" target="_blank">local</a> when parsing <code>'2025-05-28T00:00'</code>. And all this just to settle at Firefox’s 2009 behavior which, in my opinion, is the most unintuitive of them all.</p> </section>
<section id="What_about_Temporal?"> <h2><a href="#What_about_Temporal?">What about Temporal?</a></h2> <p>For the unaware, <a href="https://developer.mozilla.org/en-US/blog/javascript-temporal-is-coming/" target="_blank">JavaScript Temporal is coming</a>: a new set of date and time APIs intended to replace the <code>Date</code> object.</p><p>Our whole original date parsing issue stemmed from time zone ambiguity but in many cases, the desire is to treat date-only strings as exactly that — dates only. For example, when I say that Christmas this year is <code>2025-12-25</code>, I don’t mean the universal instant in time that is <code>2025-12-25T00:00:00.000Z</code>.</p><p>While <code>Date</code> can only ever represent the latter, Temporal offers the option of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/PlainDate" target="_blank">plain dates</a> (i.e. a date without a time zone). <code>'2025-12-25'</code> is just <code>2025-12-25</code>, side-stepping the parsing ambiguity issue entirely.</p><p>But what if one really wants to parse a date-only string into an instant in time? What time zone will Temporal choose when absent in the string itself?</p><p>Answer: It’s a hard error; an <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/Instant#z%C2%B1hhmm" target="_blank">offset</a> or <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Temporal/ZonedDateTime#time_zone_id" target="_blank">time zone identifier</a> must be provided. No repeat mistakes here.</p> </section>
<section id="Bonus:_enter_the_cursed_zone"> <h2><a href="#Bonus:_enter_the_cursed_zone">Bonus: enter the cursed zone</a></h2> <p>One thing I never realized until reading browser date-parsing source code is just how lenient it can be.</p><p>Here’s a fun example for Chrome/Firefox: can you spot why this (valid!) date string is being parsed as the month of May?</p><pre><code><span>const</span> date = <span>'it is wednesday, my dudes. 2025, April, maybe...28(?)'</span>;
<span>console</span>.<span>log</span>(<span>new</span> <span>Date</span>(date)); <span>// Wed May 28 2025 00:00:00 GMT-0700 (Pacific Daylight Time)</span></code></pre> </section></article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As a developer, my most important tools are a pen and a notebook (156 pts)]]></title>
            <link>https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/</link>
            <guid>44113210</guid>
            <pubDate>Wed, 28 May 2025 06:27:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/">https://hamatti.org/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/</a>, See on <a href="https://news.ycombinator.com/item?id=44113210">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
  After I signed my contract to join my new job a month ago, I was so excited.
  Not only that I would join the company but also because I got to buy a new
  notebook. The weekend before my first day I headed to the local bookstore and
  spent a good amount of time browsing through the notebooks available and ended
  up with this happy orange one.
</p>
<p><img src="https://hamatti.org/assets/img/posts/as-a-developer-my-most-important-tools-are-a-pen-and-a-notebook/1.png" alt=" "></p><p>Why am I so excited about a notebook then?</p>

<p>
  <b>Because it’s the most important tool I have as a software developer.</b>
</p>

<p>
  When it comes to building software or solving problems, writing code is the
  necessary bit at the end where we tell the computer what to do but way more
  important than writing that code is figuring out what code to write.
</p>

<p>
  I learned very early on in my career that I’m not very good at thinking when
  I’m at a computer. When I have my code editor open, I’m in a “function mode”
  where I write stuff that does something. When my brain hits that mode, there’s
  not much creative energies flowing around.
</p>

<p>
  So I often step away from the computer. Sometimes it’s for walks (I tend to
  work at companies near water features so I’ve used to go for strolls near San
  Francisco Bay, Spree or the best of them all, Aurajoki) but often I take my
  notebook and sit on a couch or outside at the patio and ponder.
</p>

<p>
  I might be thinking of initial solutions to new problems (ie. designing how to
  approach it, drawing UI sketches or flowcharts) or I’m helping myself
  understand the flow of data and interactions in the current code base to
  figure out how to fix a bug or add new functionality.
</p>

<p>
  Writing (and sketching) is such a powerful tool for thinking in that. It helps
  me turn my vague abstract ideas into tangible artefacts through words and
  drawings. It helps me expose the gaps in my knowledge or understanding because
  I can’t skip them as easily when writing as I can when just thinking about
  them.
</p>

<p>
  When I’ve written code, I like to write about it as if I would be explaining
  it to someone else. Whenever possible, I like to publish them as blog posts as
  well but even when it’s not possible, writing it that way helps me find
  inconsistencies, bad designs and even mistakes in my code. I’ve written about
  <a href="https://hamatti.org/posts/blogging-is-my-new-favorite-refactoring-tool/">how writing is my favourite refactoring tool</a>.
</p>

<p>
  A lovely side benefit of thinking through writing is that it leaves behind a
  copy of my thoughts and the process of how I reached them. I don’t have to
  separately go and write notes because my thinking process already created most
  of them and I usually just reorganise and polish them a bit to make them more
  useful in the future as well.
</p>

<p>
  That way, if someone asks what I was thinking about when I did X two weeks,
  six months or two years ago, I can go back to my notes and tell them exactly
  that. (Spoilers: quite often that someone is me in the future.)
</p>

<p>
  I also have a longer post about
  <a href="https://hamatti.org/posts/how-i-take-work-notes-as-a-developer/">how I take work notes as a developer</a>
  that focuses more on the content of my notes.
</p>





            <hr>
          <p>If something above resonated with you, let's start a discussion about it! <strong>Email me at juhamattisantala at gmail dot com and share your thoughts</strong>. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.</p>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: AutoThink – Boosts local LLM performance with adaptive reasoning (347 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44112326</link>
            <guid>44112326</guid>
            <pubDate>Wed, 28 May 2025 02:39:11 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44112326">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>I built AutoThink, a technique that makes local LLMs reason more efficiently by adaptively allocating computational resources based on query complexity.</p><p>The core idea: instead of giving every query the same "thinking time," classify queries as HIGH or LOW complexity and allocate thinking tokens accordingly. Complex reasoning gets 70-90% of tokens, simple queries get 20-40%.</p><p>I also implemented steering vectors derived from Pivotal Token Search (originally from Microsoft's Phi-4 paper) that guide the model's reasoning patterns during generation. These vectors encourage behaviors like numerical accuracy, self-correction, and thorough exploration.</p><p>Results on DeepSeek-R1-Distill-Qwen-1.5B:</p><p>- GPQA-Diamond: 31.06% vs 21.72% baseline (+43% relative improvement)</p><p>- MMLU-Pro: 26.38% vs 25.58% baseline</p><p>- Uses fewer tokens than baseline approaches</p><p>Works with any local reasoning model - DeepSeek, Qwen, custom fine-tuned models. No API dependencies.</p><p>The technique builds on two things I developed: an adaptive classification framework that can learn new complexity categories without retraining, and an open source implementation of Pivotal Token Search.</p><p>Technical paper: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327" rel="nofollow">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327</a></p><p>Code and examples: <a href="https://github.com/codelion/optillm/tree/main/optillm/autothink">https://github.com/codelion/optillm/tree/main/optillm/autoth...</a></p><p>PTS implementation: <a href="https://github.com/codelion/pts">https://github.com/codelion/pts</a></p><p>I'm curious about your thoughts on adaptive resource allocation for AI reasoning. Have you tried similar approaches with your local models?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Look ma, no bubbles designing a low-latency megakernel for Llama-1B (200 pts)]]></title>
            <link>https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</link>
            <guid>44111673</guid>
            <pubDate>Wed, 28 May 2025 00:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles">https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles</a>, See on <a href="https://news.ycombinator.com/item?id=44111673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>There are some applications that benefit from running LLMs really, really fast. This low-latency regime encompasses applications like chatbots and human-in-the-loop workflows, where users care a lot about seeing responses come back immediately.</p>
<p>Given the importance of these low-latency workloads, we wanted to explore just how fast we can run open-source models on modern GPUs. To really stress-test existing systems, we consider an aggressive low-latency scenario where we generate a single sequence with Llama-3.2-1B. This workload is strongly memory bound – our performance is dominated by how fast we can load model weights from GPU global memory.</p>
<p>It turns out that popular LLM inference engines – vLLM and SGLang – are only able to use at most 50% of available GPU bandwidth when running this workload on an H100. The root of the problem, which we'll describe more below, is that existing systems break down a model forward pass into around <strong>a hundred separate kernels</strong> that each implement a few operations (e.g. RMS norm, attention, an MLP layer + activation, rotary). Each kernel comes with a setup and teardown period and during this time no useful work gets done – for instance, the all-important task of loading model weights is stalled.</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/result.png" alt="Performance comparison graph"></p><p><em>Figure 1: Speed! Results generated with a 32-token prompt and 128 generated tokens, with no speculation</em></p></div>
<p>In this post, we show how we can bypass this problem by merging the entire Llama-1B forward pass into a single "megakernel" that eliminates kernel boundaries altogether. Doing this achieves brr – on an H100, we use 78% of memory bandwidth and outperform existing systems by over 1.5x. (To our knowledge, this is the lowest-latency forward pass for Llama-1B in bfloat16!) In the rest of this post, we'll walk through how and why one would do this. Specifically:</p>
<ul>
<li>First, we'll talk about how small kernels lead to AI systems that underutilize the GPU's full bandwidth.</li>
<li>Second, we'll describe three important points about how we built our megakernel: how we fused lots of kernels together, how we share hardware resources across them to minimize overhead, and how we synchronize them efficiently.</li>
</ul>
<p>If you're interested in learning more of the details or using these ideas yourself, we're <a href="https://github.com/HazyResearch/Megakernels">open-sourcing all of our code here</a>.</p>
<h2>Separate Kernels Kill the Vibe</h2>
<p>In general, the way one runs code on a GPU is by launching a "kernel" – a small program that does a well-defined operation (e.g. RMS norm, MLP). Today, all AI workloads run as long sequences of relatively small kernels. To get an initial sense, let's look at the operations in the Llama-1B transformer block, and some example kernel boundaries of how they might be divided up (Figure 2).</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/kernel_boundaries.png" alt="Kernel boundaries diagram"></p><p><em>Figure 2: An example set of kernel boundaries for the Llama-1B transformer block. Red boxes delineate the work done by individual kernels.</em></p></div>
<p>As we described earlier, decoding a single sequence with Llama-1B is a purely memory-bound workload: our performance depends on being able to <strong>always</strong> be loading weights from GPU global memory. So, why are existing approaches so far from using the full bandwidth of the GPU?</p>
<p>When we dug into it, we noticed a key problem was that the current kernel-based approach to running models introduces stalls that prevent us from constantly loading memory:</p>
<ul>
<li>First: GPU kernels are launched with a strict ordering, so that a thread block in one kernel can't start until all thread blocks in previous kernels have completely finished. Consequently, every time we start a kernel, we have to wait for all the straggler thread blocks from the prior one to finish. For example, if a kernel runs 512 thread blocks (like our Llama-1B down projection), but we only have 148 streaming multiprocessors (like on a B200), we end up with 80 empty SM's at the end.</li>
<li>Second, as we've <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">previously highlighted</a>, each kernel launch and teardown incurs costs. In principle, NVIDIA's CUDA graphs can help hide costs, but by our measurements they still leave a lot on the table. For a simple dummy kernel (which dumps a start time, sleeps, and dumps an end time) on an H100, we find that running on a CUDA stream incurs a launch cost of about 2.1 microseconds, and with CUDA graphs the launch cost only decreases to around 1.3 microseconds – time spent with the GPU doing no useful work! We'd like to have the GPU spend all of its time doing useful work.</li>
<li>Finally, even after we start the next kernel, we still have to wait to load weights and activations before any compute can start. These latencies leave the GPU sitting idle for thousands of cycles! Ideally, we'd start loading the next weights while the previous computations and stores are happening. NVIDIA has also built a mechanism for this called <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programmatic-dependent-launch-and-synchronization">Programmatic Dependent Launch</a> (PDL), which allows the next kernel to start preparing while the previous kernel is running, but we found it still introduces unnecessary stalls because the PDL synchronization mechanism (cudaGridDependencySynchronize) is very coarse. For example, it means we have to wait for all queries, keys, and values to complete in order to start attention, as opposed to starting heads as soon as they are ready. We'll later show another specific case of where this is useful in Llama-1B.</li>
</ul>
<p>Taken together, these form the "memory pipeline bubbles" our title references – and they represent a key reason that we're <strong>not always loading from memory</strong>.  For short operations, these pauses add up, wasting a huge chunk of potential bandwidth. In part, this is because Llama-1B (actually 1.24B parameters) in batch size 1 is just so... small: if each operation is really fast, then the time spent in-between them really starts to matter.</p>
<p>To illustrate the magnitude of the problem: for single-sequence generation in 16-bit precision on a single H100, the <strong>memory limit</strong> is 3.35TB/s / 2.48GB = ~1350 forward passes per second. But with 7 kernel launches per layer, and 16 layers, even with an optimistic 5 us of stalling per kernel (counting stragglers, kernel launch, and memory latencies), generation would run at just ~770 forward passes per second. In practice, it's often worse. On low-latency workloads, GPUs spend only a fraction of their time actually doing any useful work!</p>
<p>So while CUDA does provide some existing features (e.g. graphs, streams, PDL) to partially solve these problems, we wanted to see if a different approach could solve all of these problems, where we just fuse the entire model forward pass into a single kernel.</p>
<h2>How to Megakernel</h2>
<p>Next, we'll show you how we fused a whole Llama forward pass into a single kernel, and our methods for resolving three key problems:</p>
<ol>
<li>Fusing dozens of operations is hard to do from scratch. We need a mechanism for executing these operations within the megakernel.</li>
<li>In order to overlap multiple operations on the same hardware, we need to prevent contention over limited resources, such as shared memory.</li>
<li>The GPU synchronizes after each kernel in the traditional kernel model. Without kernels, we have to synchronize the GPU all by ourselves!</li>
</ol>
<p>Let's start with the first issue:</p>
<h4>Issue 1/3: Fusing Lots of Operations</h4>
<p>Traditional kernel fusion generally merges just two or three operations together. In contrast, we need to fuse about a hundred. Consequently, we need to have a sensible abstraction for how we can actually program a megakernel.</p>
<p>Our approach is built on an on-GPU interpreter – essentially a more sophisticated version of our infrastructure underlying <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla">ThunderMLA</a>. Our interpreter is designed such that each streaming multiprocessor (SM) within the GPU receives a sequence of <strong>instructions</strong> (each implemented using the same CUDA template) and executes them. We <strong>schedule</strong> each SM's instruction sequence ahead of time on the Python side, and notably we can reuse each schedule for hundreds of forward passes!</p>
<p>For our end-to-end Llama forwards pass megakernel, we define the following set of instructions:</p>
<ul>
<li>A fused RMS norm &amp; QKV &amp; RoPE instruction.</li>
<li>An attention computation instruction.</li>
<li>An attention reduction instruction (for ThunderGQA on long sequences).</li>
<li>An O-projection + residual instruction.</li>
<li>A fused RMS norm &amp; up-gate &amp; SiLU instruction.</li>
<li>A down-projection + residual instruction.</li>
<li>An RMS norm &amp; language modeling head instruction, for computing the final token logits.</li>
</ul>
<p>We implement each of these instructions using a common <a href="https://github.com/HazyResearch/Megakernels/blob/main/util/mk_init/sources/src/%7B%7BPROJECT_NAME_LOWER%7D%7D.cu">CUDA template</a> (with load, store, compute boilerplate functions), facilitating interoperability within our interpreter framework.</p>
<h4>Issue 2/3: <span>S</span><span>h</span><span>a</span><span>r</span><span>i</span><span>n</span><span>g</span> Shared Memory to Eliminate Memory Bubbles</h4>
<p>The instruction-and-interpreter structure lets us cleanly organize our megakernel. However, we haven't yet addressed the key issue: making sure that model weights are always being loaded in order to maximize memory bandwidth utilization.</p>
<p>The reason why a megakernel lets us solve this problem is that we can pipeline memory loads across instructions: our interpreter will start loading the model weights for an instruction as soon as it can, even if a previous instruction is still finishing up (e.g. storing out its results to global memory). It's this tight transitioning between instructions that minimizes the memory bubbles that would otherwise appear if we launched multiple kernels.</p>
<p>However, there's a catch: loading the weights from global memory for the next instruction doesn't do you much good if you have no place to put the data you loaded! More precisely, all of our weight matrices are loaded from GPU global memory into our SM's "shared memory" – NVIDIA's term for the fast memory on each SM. Shared memory is a scarce resource on each SM, and we can't start a load for a new instruction if a previous instruction is using all of it. This necessitates a way to keep track of which instruction is using which piece of shared memory and quickly transition shared memory to the next instruction when the current instruction is done with it.</p>
<p>We accomplish this by <strong>paging</strong> shared memory. We first divide the first 213kB of shared memory on an H100 into 13 16KiB pages, and use remaining shared memory for special purposes, like storing instruction parameters. To use one of these pages, instructions have to explicitly request and release them from the interpreter. The interpreter automatically passes released pages to the next instruction, allowing them to start issuing memory loads as early as shared memory becomes available.</p>
<h4>Issue 3/3: Synchronization</h4>
<p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/thanos.png" alt="Thanos illustration"></p>
<p>While megakernels let us minimize pipeline bubbles, they also introduce a new problem: synchronization. The performance limitation with the normal many-kernel execution model is that no thread blocks in a kernel can start until all thread blocks in previous kernels are finished. However, it's precisely this property that makes it easy to manage data dependencies. When a kernel launches, CUDA guarantees that all of the kernel's input tensors have already been produced and are safe to read from immediately.</p>
<p>With megakernels, we have no such guarantees: when an SM starts to execute a new instruction, its inputs might not be ready! To address this, we explicitly synchronize the instructions inside of our megakernel. We accomplish this with a simple counter system. Before the megakernel launches, we initialize an array of counters (i.e. integers) in GPU global memory with a starting value of zero. Whenever an instruction completes, it increments one of these counters. Similarly, whenever a new instruction starts, it must wait for some of these counters to reach a target value, indicating that all of its dependencies have finished.</p>
<p>One optimization this enables is in the big multi-layer perceptrons (MLPs) in Llama-1B.</p>
<ul>
<li>In a naive implementation using PDL, one must await completing the whole hidden state before beginning the down projection matrix multiply.</li>
<li>We instead produce and consume the intermediate state in four chunks, each with their own counter. This way, an instruction for the down projection only needs to wait for its input chunk to finish.</li>
</ul>
<h2>Putting It All Together</h2>
<p>To our knowledge, our H100 megakernel represents the first time anyone has run the forward pass for a 16-bit 1B+ parameter language model in under one millisecond on a GPU. Our B200 implementation pushes this even further to under 680 microseconds per forward pass!</p>
<p>As shown in Figure 1, our megakernel outperforms vLLM and SGLang baselines (which use CUDA graphs and torch compilation):</p>
<ul>
<li>On an H100, our megakernel runs almost 2.5x faster than vLLM and over 1.5x faster than SGLang.</li>
<li>On a B200, the gap with vLLM rises to over 3.5x, and we remain more than 1.5x faster than SGLang, too.</li>
</ul>
<p>We're still actually quite a ways off from the theoretical limit on a B200, which is around ~3,000 forward passes per second. Part of this gap is because this theoretical limit is based purely on memory bandwidth – but we still have to wait to load activations. And although these activations are small (and don't cost a lot of bandwidth), there are still latencies in loading them that we can't hide. A breakdown of the runtime of our current B200 forward pass (total runtime 600 microseconds):</p>
<ul>
<li>250 microseconds are spent storing activations, awaiting consistency, and loading them. This is about 20% higher than a simple model would suggest: since each instruction has a dependence on the last one, we need to pay two load latencies (check ready, and then load activations) and two store latencies (store activations, then mark ready) per instruction. Using ~500 nanoseconds latency per load / store, this would impose about 200 microseconds of overhead. (We suspect some of the remaining 50 microseconds comes from time spent processing atomics in global memory.)</li>
<li>200 microseconds are spent actually running RMS norm and matrix-vector computations. 95% of this portion is devoted to matrix-vector. On Blackwell, we find that using the tensor cores is marginally helpful for this; on Hopper, we find it better to simply run on the CUDA cores. This difference comes from the fact that both GPUs have relatively similar CUDA core performance, but Blackwell tensor cores are much faster.</li>
<li>30 microseconds are spent awaiting weights from global memory (pipelining works!) Of these, 40% are spent in the LM head, which is the best-pipelined part of the whole megakernel due to its homogeneity and huge size.</li>
<li>40 microseconds are spent on low-level synchronization overhead across warps. A key issue here is that CUDA's asynchronous barriers are relatively slow, even when they're already in the "pass" state, requiring about 60 nanoseconds each time.</li>
<li>80 microseconds are on setup and various other overheads (e.g. passing instruction barriers, marking pages as complete, etc.)</li>
</ul>
<p>We think there's probably more to do on each of these, but that'll have to wait for a future update!</p>
<h2>The Megakernel Cinematic Universe</h2>
<p>In this blog, we focus narrowly on designing a megakernel for low-latency, batch-size one LLM inference. However, we believe that the ability to more precisely control GPU execution with megakernels can more generally be applied to accelerate a much broader set of AI workloads. Stay tuned!</p>
<div><p><img src="https://hazyresearch.stanford.edu/static/posts/2025-05-27-no-bubbles/sonic.png" alt="Sonic illustration"></p><p><strong>The Main Message of this Blog Post</strong></p></div>
<p>If you'd like to learn more, please reach out to Ben or Jordan! Please include a tribute of at least five pictures of kittens in your email.</p>
<ul>
<li>Ben: <a href="mailto:bfs@stanford.edu">bfs@stanford.edu</a></li>
<li>Jordan: <a href="mailto:jbj@stanford.edu">jbj@stanford.edu</a></li>
</ul>
<p>And many, many thanks to Together AI for generously providing us with B200s and H100s to do this work, which would not have been possible without them!</p>
<p>See also: <a href="https://hazyresearch.stanford.edu/blog/2025-03-04-thundermla"><strong>pretty big kernels</strong></a> | <a href="https://hazyresearch.stanford.edu/blog/2024-05-12-tk"><strong>regular kernels</strong></a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A UEFI app that sends LLDP-MED pkt at boot to negotiate PoE+ power before the OS (186 pts)]]></title>
            <link>https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</link>
            <guid>44111609</guid>
            <pubDate>Tue, 27 May 2025 23:45:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution">https://roderickkhan.com/posts/2025-05-16-poe-uefi-solution</a>, See on <a href="https://news.ycombinator.com/item?id=44111609">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Back in 2015, I was working on a project to build PoE-powered embedded x86 computers and digital signage systems. These were full Windows 10 Professional machines running Intel Atom processors, designed to simplify deployment by drawing power directly over Ethernet. Our goal was to eliminate the need to run traditional AC power to these devices, which can be costly and impractical in many deployment scenarios. But unlike typical IoT or low-power devices, these were full-fledged x86 computers that required more power than what the standard PoE (802.3af) could deliver, which maxes out at 15.4W at the PSE (Power Sourcing Equipment), such as a PoE network switch or injector.</p>
<p>Our device required about 23W when fully operational, which pushed us into <strong>802.3at (PoE+)</strong> territory. In most client environments their PoE+ switches provided the power we needed with no problem. But some environments had network switches that would not give us the additional power.</p>
<h3><strong>PoE Standards Overview (IEEE 802.3)</strong></h3>
<table>
<thead>
<tr>
<th>Standard</th>
<th>Max Power at PSE</th>
<th>Max Power at PD</th>
<th>Voltage Range</th>
<th>Pairs Used</th>
<th>Year</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>802.3af (PoE)</strong></td>
<td>15.4 W</td>
<td>12.95 W</td>
<td>44–57 V DC</td>
<td>2 pairs</td>
<td>2003</td>
</tr>
<tr>
<td><strong>802.3at (PoE+)</strong></td>
<td>30 W</td>
<td>25.5 W</td>
<td>50–57 V DC</td>
<td>2 pairs</td>
<td>2009</td>
</tr>
</tbody>
</table>
<p>The problem was that our embedded systems only supported physical‑layer classification which is limited to signaling power requirements through resistive detection and pulsed current signatures during initial PoE handshaking. Only relying on this method can be problematic if the switch is configured to require LLDP for Data Link Layer Classification for devices requiring more than 15.4W. Which is a problem because at minimum our computers required at least 18W in order to boot into the operating system. So our systems would initially start to boot, but then eventually shut off before it got into Windows. We were stuck in a frustrating Catch-22, we needed to send LLDP packets to get more power, but we couldn’t even boot the OS to send them.</p>
<h3><strong>So What Do You Do When the OS Can't Help?</strong></h3>
<p>We did some testing and measured power draw during various phases of the boot cycle. Fortunately, the system's power needs during initial startup (BIOS/UEFI initialization) were low enough to stay under the 15.4W limit. That gave us a brief window to request more power <em>before</em> booting Windows.</p>
<p>So the challenge became: negotiate higher PoE+ power <strong>before</strong> Windows starts. The answer was to handle LLDP negotiation at the BIOS level, or more accurately the UEFI (Unified Extensible Firmware Interface) firmware.  Through our research we discovered that UEFI supports the TCP/IP protocol and has access to the network stack, enabling communication over Ethernet without an OS.</p>
<p>Our first attempt was to work with the motherboard vendor and AMI (the BIOS provider) for a custom firmware build. We signed NDAs and had multiple discussions, but despite our efforts, they ultimately declined to create a custom BIOS for us. After hitting that roadblock and feeling the frustration of stalled progress, I refused to give up. I dug deeper and came across the concept of <strong>UEFI applications</strong>.</p>
<p>A UEFI application is a type of software designed to run in the pre-boot environment of a computer, managed entirely by the UEFI firmware. These applications are different from traditional programs that run once an operating system like Windows or Linux has loaded. Instead, UEFI applications operate with the services and resources provided by the firmware itself, bypassing the need for an OS.</p>
<p>They are typically stored on a dedicated partition called the EFI System Partition (ESP) and launched by the UEFI boot manager during the system's boot process. These apps can access low-level system functionality, including networking, file systems, and input/output devices. In our case, that meant we could build a standalone tool to t ransmit LLDP packets <em>before</em> the OS even initialized. This was the perfect solution, because it required no changes to the BIOS/UEFI firmware itself. I just needed to find someone with the embedded firmware expertise to bring it to life.</p>
<h3><strong>From Warsaw With Code</strong></h3>
<p>After some research, I found <a href="https://www.linkedin.com/in/krolpiotr/">Piotr Król</a>, a former BIOS software engineer at Intel who was doing freelance work out of Poland. He understood the problem immediately. We set up remote serial and IP-KVM access to our development hardware, and Piotr got to work.</p>
<p>There were some challenges along the way including lack of vendor support, incomplete firmware tooling, and remote hardware limitations. Our system didn't include <code>bcfg</code>, which meant we couldn't persistently change the boot order through standard UEFI tools. Piotr identified this early and suggested using <code>startup.nsh</code> as a workaround, a shell script that would automatically run our LLDP application when the EFI shell launched.</p>
<p>Four months later, Piotr delivered <strong>PoePwrNegotiator</strong>: a UEFI application written in C that transmits LLDP-MED (Link Layer Discovery Protocol – Media Endpoint Discovery) packets and requests the higher power levels we needed. No OS required. We deployed this UEFI application on all of our PoE devices in production and it worked flawlessly.</p>
<h3><strong>Sharing the Solution</strong></h3>
<p>This project began as an attempt to solve a very specific challenge we faced nearly a decade ago. I don’t know how many others have tackled this type of problem or taken this approach, but I wanted to share the work in case it helps someone else.</p>
<p>By open-sourcing <strong>PoePwrNegotiator</strong>, my goal is to preserve and document a unique solution to a problem that may still be relevant to those building PoE-powered x86 systems. If someone out there is working on a similar challenge, or even just wants to understand how UEFI applications can be used to control networking behavior at boot, I hope this gives them a useful head start.</p>
<p>PoePwrNegotiator is released under the <strong>MIT License</strong>, one of the most permissive open source licenses available. This means anyone can use, modify, or integrate this code into their own projects, commercial or personal, as long as the original license and copyright notice are included. The goal is to make this as accessible and useful as possible to anyone dealing with power negotiation challenges or looking to learn more about UEFI networking.</p>
<p><strong>GitHub Repo:</strong> <a href="https://github.com/orbitrod/PoePwrNegotiator">https://github.com/orbitrod/PoePwrNegotiator</a></p>
<h3><strong>Special Thanks</strong></h3>
<p><strong>Carlos</strong>, you were instrumental during the testing and the deployment of this application. You were my right hand throughout this project and far beyond it, and your dedication to me and to the work we were doing will never be forgotten. I cannot express enough how much your loyalty and commitment meant to me throughout that entire journey.</p>
<p><strong>Piotr</strong>, thank you for being brilliant, resourceful, and incredibly effective. Your deep expertise in firmware helped us solve a problem others wouldn’t touch. I’m grateful for your expertise and contribution to our project, your work solved the last piece of the puzzle.</p>
<hr>
<blockquote>
<p><em>This project reminded me that innovation often comes from working around limitations, not just within them. PoePwrNegotiator was a solution to a very specific challenge I faced in 2015, but the lessons and approach still feel relevant today. If it sparks ideas, helps someone overcome a similar obstacle, or contributes in any way to future PoE-powered system design, that’s all the reason I need to put it out there.</em></p>
<p><em>— Roderick</em></p>
</blockquote>
<hr>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenTPU: Open-Source Reimplementation of Google Tensor Processing Unit (TPU) (137 pts)]]></title>
            <link>https://github.com/UCSBarchlab/OpenTPU</link>
            <guid>44111452</guid>
            <pubDate>Tue, 27 May 2025 23:10:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/UCSBarchlab/OpenTPU">https://github.com/UCSBarchlab/OpenTPU</a>, See on <a href="https://news.ycombinator.com/item?id=44111452">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">UCSB ArchLab OpenTPU Project</h2><a id="user-content-ucsb-archlab-opentpu-project" aria-label="Permalink: UCSB ArchLab OpenTPU Project" href="#ucsb-archlab-opentpu-project"></a></p>
<p dir="auto">OpenTPU is an open-source re-implementation of Google's Tensor Processing Unit (TPU) by the UC Santa Barbara ArchLab.</p>
<p dir="auto">The TPU is Google's custom ASIC for accelerating the inference phase of neural network computations.</p>
<p dir="auto">Our design is based on details from Google's paper titled "In-Datacentre Performance Analysis of a Tensor Processing Unit" (<a href="https://arxiv.org/abs/1704.04760" rel="nofollow">https://arxiv.org/abs/1704.04760</a>), which is to appear at ISCA2017. However, no formal spec, interface, or ISA has yet been published for the TPU.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">The OpenTPU is powered by PyRTL (<a href="http://ucsbarchlab.github.io/PyRTL/" rel="nofollow">http://ucsbarchlab.github.io/PyRTL/</a>).</h4><a id="user-content-the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl" aria-label="Permalink: The OpenTPU is powered by PyRTL (http://ucsbarchlab.github.io/PyRTL/)." href="#the-opentpu-is-powered-by-pyrtl-httpucsbarchlabgithubiopyrtl"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li>Python 3</li>
<li>PyRTL version &gt;= 0.8.5</li>
<li>numpy</li>
</ul>
<p dir="auto">Both PyRTL and numpy can be installed with pip; e.g., <code>pip install pyrtl</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to Run</h2><a id="user-content-how-to-run" aria-label="Permalink: How to Run" href="#how-to-run"></a></p>
<p dir="auto">To run the simple matrix multiply test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 8 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy"><pre><code>python3 assembler.py simplemult.a
python3 runtpu.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
python3 sim.py simplemult.out simplemult_hostmem.npy simplemult_weights.npy
</code></pre></div>
<p dir="auto">To run the Boston housing data regression test in both the hardware and functional simulators:</p>
<p dir="auto">Make sure MATSIZE is set to 16 in config.py, then</p>
<div data-snippet-clipboard-copy-content="python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy"><pre><code>python3 assembler.py boston.a
python3 runtpu.py boston.out boston_inputs.npy boston_weights.npy
python3 sim.py boston.out boston_inputs.npy boston_weights.npy
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Hardware Simulation</h3><a id="user-content-hardware-simulation" aria-label="Permalink: Hardware Simulation" href="#hardware-simulation"></a></p>
<p dir="auto">The executable hardware spec can be run using PyRTL's simulation features by running <code>runtpu.py</code>. The simulation expects as inputs a binary program and numpy array files containing the initial host memory and the weights.</p>
<p dir="auto">Be aware that the size of the hardware Matrix Multiply unit is parametrizable --- double check <code>config.py</code> to make sure MATSIZE is what you expect.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Functional Simulation</h3><a id="user-content-functional-simulation" aria-label="Permalink: Functional Simulation" href="#functional-simulation"></a></p>
<p dir="auto">sim.py implements the functional simulator of OpenTPU. It reads in three cmd args: the assembly program, the host memory file, and the weights file. Due to the different quantization mechnisms between high-level applications (written in tensorflow) and OpenTPU, the simulator runs in two modes: 32b float mode and 8b int mode. The downsampling/quantization mechanism is consistent with the HW implementation of OpenTPU. It generates two sets of outputs, one set being 32b-float typed, the other 8b-int typed.</p>
<p dir="auto">Example usage:</p>
<div data-snippet-clipboard-copy-content="python sim.py boston.out boston_input.npy boston_weights.npy"><pre><code>python sim.py boston.out boston_input.npy boston_weights.npy
</code></pre></div>
<p dir="auto">Numpy matrices (.npy files) can be generated by calling <code>numpy.save</code> on a numpy array.</p>
<p dir="auto">checker.py implementes a simple checking function to verify the results from HW, simulator and applications. It checkes the 32b-float application results against 32b-float simulator results and then checks the 8b-int simulator results against 8b-int HW results.</p>
<p dir="auto">Example usage:</p>

<p dir="auto"><h2 tabindex="-1" dir="auto">FAQs:</h2><a id="user-content-faqs" aria-label="Permalink: FAQs:" href="#faqs"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How big/efficient/fast is OpenTPU?</h3><a id="user-content-how-bigefficientfast-is-opentpu" aria-label="Permalink: How big/efficient/fast is OpenTPU?" href="#how-bigefficientfast-is-opentpu"></a></p>
<p dir="auto">As of the alpha release, we do not have hard synthesis figures for the full 256x256 OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What can OpenTPU do?</h3><a id="user-content-what-can-opentpu-do" aria-label="Permalink: What can OpenTPU do?" href="#what-can-opentpu-do"></a></p>
<p dir="auto">The hardware prototype can currently handle matrix multiplies and activations for ReLU and sigmoid --- i.e., the inference phase of many neural network computations.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What features are missing?</h3><a id="user-content-what-features-are-missing" aria-label="Permalink: What features are missing?" href="#what-features-are-missing"></a></p>
<p dir="auto">Convolution, pooling, programmable normalization.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does your design follow that of the TPU?</h3><a id="user-content-does-your-design-follow-that-of-the-tpu" aria-label="Permalink: Does your design follow that of the TPU?" href="#does-your-design-follow-that-of-the-tpu"></a></p>
<p dir="auto">We used high-level design details from the TPU paper to guide our design when possible. Thus, the major components of the chip are the same --- matrix multiply unit, unified buffer, activation unit, accumulator, weight FIFO, etc. Beyond that, the implementations may have many differences.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does OpenTPU support all the same instructions as TPU?</h3><a id="user-content-does-opentpu-support-all-the-same-instructions-as-tpu" aria-label="Permalink: Does OpenTPU support all the same instructions as TPU?" href="#does-opentpu-support-all-the-same-instructions-as-tpu"></a></p>
<p dir="auto">No. Currently, OpenTPU supports the RHM, WHM, RW, MMC, ACT, NOP, and HLT instructions (see ISA section for details). The purpose, definition, and specification of other TPU instructions is absent from the published paper. Some instructions will likely be added to OpenTPU as we continue development (such as SYNC), but the final ISA will likely feature many differences without a published spec from Google to work off of.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is OpenTPU binary compatible with the TPU?</h3><a id="user-content-is-opentpu-binary-compatible-with-the-tpu" aria-label="Permalink: Is OpenTPU binary compatible with the TPU?" href="#is-opentpu-binary-compatible-with-the-tpu"></a></p>
<p dir="auto">No. There is no publicly available interface or spec for TPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?</h3><a id="user-content-id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version" aria-label="Permalink: I'd like to do some analysis/extensions of OpenTPU, but I need Verilog. Do you have a Verilog version?" href="#id-like-to-do-some-analysisextensions-of-opentpu-but-i-need-verilog-do-you-have-a-verilog-version"></a></p>
<p dir="auto">PyRTL can can output structural Verilog for the design, using the <code>OutputToVerilog</code> function.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I have suggestions, criticisms, and/or would like to contribute.</h3><a id="user-content-i-have-suggestions-criticisms-andor-would-like-to-contribute" aria-label="Permalink: I have suggestions, criticisms, and/or would like to contribute." href="#i-have-suggestions-criticisms-andor-would-like-to-contribute"></a></p>
<p dir="auto">That's not a question, but please get in touch! Email Deeksha (<a href="mailto:deeksha@cs.ucsb.edu">deeksha@cs.ucsb.edu</a>) or Joseph (<a href="mailto:jmcmahan@cs.ucsb.edu">jmcmahan@cs.ucsb.edu</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation.</h3><a id="user-content-im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation" aria-label="Permalink: I'm a Distinguished Hardware Engineer at Google and the Lead Architect of the TPU. I see many inefficiencies in your implementation." href="#im-a-distinguished-hardware-engineer-at-google-and-the-lead-architect-of-the-tpu-i-see-many-inefficiencies-in-your-implementation"></a></p>
<p dir="auto">Hi Norm! Tim welcomes you to Santa Barbara to talk about all things TPU :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Software details</h2><a id="user-content-software-details" aria-label="Permalink: Software details" href="#software-details"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">ISA</h3><a id="user-content-isa" aria-label="Permalink: ISA" href="#isa"></a></p>
<ul dir="auto">
<li>RHM src, dst, N
Read Host Memory.
Read <em>N</em> vectors from host memory beginning at address <em>src</em> and save them in the UB (unified buffer) beginning at address <em>dst</em>.</li>
<li>WHM src, dst, N
Write Host Memory.
Write <em>N</em> vectors from the UB beginning at address <em>src</em> to host memory beginning at address <em>dst</em>.</li>
<li>RW addr
Read Weights.
Load the weights tile from the weights DRAM at address <em>addr</em> into the on-chip FIFO.</li>
<li>MMC.{OS} src, dst, N
Matrix Multiply/Convolution.
Perform a matrix multiply operation on the <em>N</em> vectors beginning at UB address <em>src</em>, storing the result in the accumulator buffers beginning at address <em>dst</em>. If the <em>O</em> (overwrite) flag is specified, overwrite the contents of the accumulator buffers at the destination addresses; default behavior is to add to the value there and store the new sum. If the <em>S</em> (switch) flag is specified, switch to using the next tile of weights, which must have already been pre-loaded. The first <code>MMC</code> instruction in a program should always use the <em>S</em> flag.</li>
<li>ACT.{RQ} src, dst, N
Activate.
Perform activation on <em>N</em> vectors in the accumulator buffers starting at address <em>src</em>, storing the results in the UB beginning at address <em>dst</em>. Activation function is specified with a flag: <em>R</em> for ReLU and <em>Q</em> for sigmoid. With no flag, values are passed through without activation. Normalization is programmable at synthesis-time, but not at run-time; by default, after activation the upper 24 bits are dropped from each value, producing an 8-bit integer.</li>
<li>NOP
No op. Do nothing for one cycle.</li>
<li>HLT
Halt. Stop simulation.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Writing a Program</h3><a id="user-content-writing-a-program" aria-label="Permalink: Writing a Program" href="#writing-a-program"></a></p>
<p dir="auto">OpenTPU uses no dynamic scheduling; all execution is fully determinstic* and the hardware relies on the compiler to correctly schedule operations and pad NOPs to handle delays. This OpenTPU release does <br>
not support "repeat" flags on instructions, so many NOPs are required to ensure correct execution.</p>
<p dir="auto">*DRAM is a source of non-deterministic latency, discussed in the Memory Controller section of Microarchitecture.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Generating Data</h3><a id="user-content-generating-data" aria-label="Permalink: Generating Data" href="#generating-data"></a></p>
<p dir="auto"><strong>Application</strong></p>
<ol dir="auto">
<li>Simple one hot 2-layer NN</li>
</ol>
<p dir="auto">gen_one_hot.py generates 8b-int typed random squre matrix as training data and vector as label, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2"><pre><code>python gen_one_hot.py --path simple_train --shape 8 8 --range -5 5
python gen_one_hot.py --path simple_train_label --shape 8 1 --range 0 2
</code></pre></div>
<p dir="auto">simple_nn.py trains a simple 2-layer nn on the given train/label dataset and writes the weights into a file, example usage (run gen_one_hot example first to generate the files):</p>
<div data-snippet-clipboard-copy-content="python simple_nn.py --path simple_train.npy --label simple_train_label.npy"><pre><code>python simple_nn.py --path simple_train.npy --label simple_train_label.npy
</code></pre></div>
<p dir="auto">After running the above command, two files are generated: simple_nn_weight_dram.npy is the 8b-int typed weight dram that the OpenTPU operates on, simple_nn_gt is the pickled ground truth 32b-float resulits and weights. To run with OpenTPU, a test file must also be generated, example usage:</p>
<div data-snippet-clipboard-copy-content="python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9"><pre><code>python gen_one_hot.py --path simple_test --shape 100 8 --range 1, 9
</code></pre></div>
<p dir="auto">After which simple_test.npy will be generated and it should be used as the host memory by OpenTPU.</p>
<p dir="auto">We also provide simple_nn.a -- the assembly program for this simple nn.</p>
<ol start="2" dir="auto">
<li>Tensorflow DNN regression</li>
</ol>
<p dir="auto">Although applications written in any high-level nn framework can be used, here we use tensorflow as an example.</p>
<p dir="auto">tf_nn.py trains a MLP regressor on the Boston Housing Dataset (<a href="https://archive.ics.uci.edu/ml/datasets/housing" rel="nofollow">https://archive.ics.uci.edu/ml/datasets/housing</a>). Example usage:</p>
<div data-snippet-clipboard-copy-content="python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw"><pre><code>python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output
python tf_nn.py --N 10 --save-input-path boston_input --save-weight-path boston_weights --save-output-path boston_output --raw
</code></pre></div>
<p dir="auto">After running the above command, four files are generated: gt32.npy holds the ground truth prediction values, boston_input.npy holds the input test cases which is used as the host memeory for OpenTPU, boston_output.npy holds all the intermediate output values, and boston_weights.npy holds the weight matrices which are used as the weight dram for OpenTPU.</p>
<p dir="auto">Adding --raw to the command generates 32b-float typed files instead of 8b ints.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Latencies</h3><a id="user-content-latencies" aria-label="Permalink: Latencies" href="#latencies"></a></p>
<p dir="auto">The following gives the hardware execution latency for each instruction on OpenTPU:</p>
<ul dir="auto">
<li>RHM - <em>M</em> cycles for reading <em>M</em> vectors</li>
<li>WHM - <em>M</em> cycles for writing <em>M</em> vectors</li>
<li>RW - <em>N*N</em>/64 cycles for <em>N_x_N</em> MM Array for DRAM transfer, and up to 3 additional cycles to propagate through the FIFO</li>
<li>MMC - <em>L+2N</em> cycles, for <em>N_x_N</em> MM Array and <em>L</em> vectors multiplied in the instruction</li>
<li>ACT - <em>L+1</em> cycles, for <em>L</em> vectors activated in the instruction</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Microarchitecture</h2><a id="user-content-microarchitecture" aria-label="Permalink: Microarchitecture" href="#microarchitecture"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Matrix Multiply (MM) Unit</h3><a id="user-content-matrix-multiply-mm-unit" aria-label="Permalink: Matrix Multiply (MM) Unit" href="#matrix-multiply-mm-unit"></a></p>
<p dir="auto">The core of the compute of the OpenTPU is the parametrizable array of 8-bit Multiply-Accumulate Units (MACs), each consisting of an 8-bit integer multiplier and an integer adder of between 16 and 32 bits<br>
*. Each MAC has two buffers storing 8-bit weights (the second buffer allows weight programming to happen in parallel). Input vectors enter the array from the left, with values advancing one unit to the r<br>
ight each cycle. Each unit multiplies the input value by the active weight, adds it to the value from the unit above, and passes the result to the unit below. Input vectors are fed diagonally so that val<br>
ues align correctly as partial sums flow down the array.</p>
<p dir="auto">*The multipliers produce 16-bit outputs; as values move down the columns of the array, each add produces 1 extra bit. Width is capped at 32, creating the potential for uncaught overflow.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Accumulator Buffers</h3><a id="user-content-accumulator-buffers" aria-label="Permalink: Accumulator Buffers" href="#accumulator-buffers"></a></p>
<p dir="auto">Result vectors from the MM Array are written to a software-specified address in a set of accumulator buffers. Instructions indicate whether values should be added into the value already at the address or<br>
overwrite it. MM instructions read from the Unified Buffer (UB) and write to the accumulator buffers; activate instructions read from the accumulator buffers and write to the UB.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Weight FIFO</h3><a id="user-content-weight-fifo" aria-label="Permalink: Weight FIFO" href="#weight-fifo"></a></p>
<p dir="auto">At scale (256x256 MACs), a full matrix of weights (a "tile") is 64KB; to avoid stalls while weights are moved from off-chip weight DRAM, a 4-entry FIFO is used to buffer tiles. It is assumed the connecti<br>
on to the weight DRAM is a standard DDR interface moving data in 64-byte chunks (memory controllers are currently emulated with no simulated delay, so one chunk arrives each cycle). When an MM instructio<br>
n carries the "switch" flag, each MAC switches the active weight buffer as first vector of the instruction propagates through the array. Once it reaches the end of the first row, the FIFO begins feeding <br>
new weight values into the free buffers of the array. New weight values are passed down through the array each cycle until each row reaches its destination.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Systolic Setup</h3><a id="user-content-systolic-setup" aria-label="Permalink: Systolic Setup" href="#systolic-setup"></a></p>
<p dir="auto">Vectors are read all at once from the Unified Buffer, but must be fed diagonally into the MM Array. This is accomplished with a set of sequential buffers in a lower triangular configuration. The top valu<br>
e reaches the matrix immediately, the second after one cycle, the third after two, etc., so that each value reaches a MAC at the same time as the corresponding partial sum from the same source vector.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Memory Controllers</h3><a id="user-content-memory-controllers" aria-label="Permalink: Memory Controllers" href="#memory-controllers"></a></p>
<p dir="auto">Currently, memory controllers are emulated and have no delay. The connection to Host Memory is currently the size of one vector. The connection to the Weight DRAM uses a standard width of 64 bytes.</p>
<p dir="auto">Because the emulated controllers can return a new value each cycle, the OpenTPU hardware simulation currently has no non-detministic delay. With a more accurate DRAM interface that may encounter dynamic <br>
delays, programs would need to either take care to schedule for the worst-case memory delay, or make use of another instruction to ensure memory operations complete before the values are required*.</p>
<p dir="auto">*We note that the TPU "SYNC" instruction may fulfill this purpose, but is currently unimplemented on OpenTPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuration</h3><a id="user-content-configuration" aria-label="Permalink: Configuration" href="#configuration"></a></p>
<p dir="auto">Unified Buffer size, Accumulator Buffer size, and the size of the MM Array can all be specified in config.py. However, the MM Array must always be square, and vectors/weights are always composed of 8-bit integers.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My LLM CLI tool can run tools now, from Python code or plugins (444 pts)]]></title>
            <link>https://simonwillison.net/2025/May/27/llm-tools/</link>
            <guid>44110584</guid>
            <pubDate>Tue, 27 May 2025 20:53:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/May/27/llm-tools/">https://simonwillison.net/2025/May/27/llm-tools/</a>, See on <a href="https://news.ycombinator.com/item?id=44110584">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/May/27/llm-tools/">

<p>27th May 2025</p>



<p><strong><a href="https://llm.datasette.io/en/stable/changelog.html#v0-26">LLM 0.26</a></strong> is out with the biggest new feature since I started the project: <a href="https://llm.datasette.io/en/stable/tools.html"><strong>support for tools</strong></a>. You can now use the LLM <a href="https://llm.datasette.io/en/stable/usage.html">CLI tool</a>—and <a href="https://llm.datasette.io/en/stable/python-api.html">Python library</a>—to grant LLMs from OpenAI, Anthropic, Gemini and local models from Ollama with access to any tool that you can represent as a Python function.</p>
<p>LLM also now has <a href="https://llm.datasette.io/en/stable/plugins/directory.html#tools">tool plugins</a>, so you can install a plugin that adds new capabilities to whatever model you are currently using.</p>
<p>There’s a lot to cover here, but here are the highlights:</p>
<ul>
<li>
<strong>LLM can run tools now</strong>! You can <strong>install tools from plugins</strong> and load them by name with <code>--tool/-T name_of_tool</code>.</li>
<li>You can also <strong>pass in Python function code on the command-line</strong> with the <code>--functions</code> option.</li>
<li>The <strong>Python API supports tools too</strong>: <code>llm.get_model("gpt-4.1").chain("show me the locals", tools=[locals]).text()</code>
</li>
<li>Tools work in <strong>both async and sync contexts</strong>.</li>
</ul>
<p>Here’s what’s covered in this post:</p>
<ul>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#trying-it-out">Trying it out</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#more-interesting-tools-from-plugins">More interesting tools from plugins</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with --functions</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#tools-in-the-llm-python-api">Tools in the LLM Python API</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#why-did-this-take-me-so-long-">Why did this take me so long?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#is-this-agents-then-">Is this agents then?</a></li>
<li><a href="https://simonwillison.net/2025/May/27/llm-tools/#what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</a></li>
</ul>


<h4 id="trying-it-out">Trying it out</h4>
<p>First, <a href="https://llm.datasette.io/en/stable/setup.html">install the latest LLM</a>. It may not be on Homebrew yet so I suggest using <code>pip</code> or <code>pipx</code> or <code>uv</code>:</p>

<p>If you have it already, <a href="https://llm.datasette.io/en/stable/setup.html#upgrading-to-the-latest-version">upgrade it</a>.</p>

<p>Tools work with other vendors, but let’s stick with OpenAI for the moment. Give LLM an OpenAI API key</p>
<div><pre>llm keys <span>set</span> openai
<span><span>#</span> Paste key here</span></pre></div>
<p>Now let’s run our first tool:</p>
<div><pre>llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td</pre></div>
<p>Here’s what I get:</p>
<p><img src="https://static.simonwillison.net/static/2025/llm-tools.gif" alt="Animated demo. I run that command, LLM shows Tool call: llm_version({}) in yellow, then 0.26a1 in green, then streams out the text The installed version is 0.26a1"></p>
<p><code>llm_version</code> is a very simple demo tool that ships with LLM. Running <code>--tool llm_version</code> exposes that tool to the model—you can specify that multiple times to enable multiple tools, and it has a shorter version of <code>-T</code> to save on typing.</p>
<p>The <code>--td</code> option stands for <code>--tools-debug</code>—it causes LLM to output information about tool calls and their responses so you can peek behind the scenes.</p>
<p>This is using the default LLM model, which is usually <code>gpt-4o-mini</code>. I switched it to <code>gpt-4.1-mini</code> (better but fractionally more expensive) by running:</p>
<div><pre>llm models default gpt-4.1-mini</pre></div>
<p>You can try other models using the <code>-m</code> option. Here’s how to run a similar demo of the <code>llm_time</code> built-in tool using <code>o4-mini</code>:</p>
<div><pre>llm --tool llm_time <span><span>"</span>What time is it?<span>"</span></span> --td -m o4-mini</pre></div>
<p>Outputs:</p>
<blockquote>
<p><code>Tool call: llm_time({})</code></p>
<div><pre>  {
    <span>"utc_time"</span>: <span><span>"</span>2025-05-27 19:15:55 UTC<span>"</span></span>,
    <span>"utc_time_iso"</span>: <span><span>"</span>2025-05-27T19:15:55.288632+00:00<span>"</span></span>,
    <span>"local_timezone"</span>: <span><span>"</span>PDT<span>"</span></span>,
    <span>"local_time"</span>: <span><span>"</span>2025-05-27 12:15:55<span>"</span></span>,
    <span>"timezone_offset"</span>: <span><span>"</span>UTC-7:00<span>"</span></span>,
    <span>"is_dst"</span>: <span>true</span>
  }</pre></div>
<p>The current time is 12:15 PM PDT (UTC−7:00) on May 27, 2025, which corresponds to 7:15 PM UTC.</p>
</blockquote>
<p>Models from (tool supporting) plugins work too. Anthropic’s Claude Sonnet 4:</p>
<div><pre>llm install llm-anthropic -U
llm keys <span>set</span> anthropic
<span><span>#</span> Paste Anthropic key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m claude-4-sonnet</pre></div>
<p>Or Google’s Gemini 2.5 Flash:</p>
<div><pre>llm install llm-gemini -U
llm keys <span>set</span> gemini
<span><span>#</span> Paste Gemini key here</span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m gemini-2.5-flash-preview-05-20</pre></div>
<p>You can even run simple tools with Qwen3:4b, a <em>tiny</em> (2.6GB) model that I run using <a href="https://ollama.com/">Ollama</a>:</p>
<div><pre>ollama pull qwen3:4b
llm install <span><span>'</span>llm-ollama&gt;=0.11a0<span>'</span></span>
llm --tool llm_version <span><span>"</span>What version?<span>"</span></span> --td -m qwen3:4b</pre></div>
<p>Qwen 3 calls the tool, thinks about it a bit and then prints out a response:
<img src="https://static.simonwillison.net/static/2025/llm-tools-qwen.jpg" alt="Tool call: llm_version({}) 0.26a1<think> Okay, the user asked, &quot;What version?&quot; I need to respond with the version of the LLM. The tool provided is llm_version, which returns the installed version. I called that function and got the response 0.26a1. Now I should present this information clearly. Let me check if there's any additional context needed, but the user just asked for the version, so a straightforward answer should work. I'll state the version number and maybe mention that it's the installed version. Keep it simple and precise. </think> The installed version of the LLM is 0.26a1."></p>
<h4 id="more-interesting-tools-from-plugins">More interesting tools from plugins</h4>
<p>This demo has been pretty weak so far. Let’s do something a whole lot more interesting.</p>
<p>LLMs are notoriously bad at mathematics. This is deeply surprising to many people: supposedly the most sophisticated computer systems we’ve ever built can’t multiply two large numbers together?</p>
<p>We can fix that with tools.</p>
<p>The <a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a> plugin exposes the <a href="https://github.com/danthedeckie/simpleeval">simpleeval</a> “Simple Safe Sandboxed Extensible Expression Evaluator for Python” library by Daniel Fairhead. This provides a robust-enough sandbox for executing simple Python expressions.</p>
<p>Here’s how to run a calculation:</p>
<div><pre>llm install llm-tools-simpleeval
llm -T simpleeval </pre></div>
<p>Trying that out:</p>
<div><pre>llm -T simple_eval <span><span>'</span>Calculate 1234 * 4346 / 32414 and square root it<span>'</span></span> --td</pre></div>
<p>I got back this—it tried <code>sqrt()</code> first, then when that didn’t work switched to <code>** 0.5</code> instead:</p>
<pre><code>Tool call: simple_eval({'expression': '1234 * 4346 / 32414'})
  165.45208860368976


Tool call: simple_eval({'expression': 'sqrt(1234 * 4346 / 32414)'})
  Error: Function 'sqrt' not defined, for expression 'sqrt(1234 * 4346 / 32414)'.


Tool call: simple_eval({'expression': '(1234 * 4346 / 32414) ** 0.5'})
  12.862818066181678

The result of (1234 * 4346 / 32414) is approximately
165.45, and the square root of this value is approximately 12.86.
</code></pre>
<p>I’ve released four tool plugins so far:</p>
<ul>
<li>
<strong><a href="https://github.com/simonw/llm-tools-simpleeval">llm-tools-simpleeval</a></strong>—as shown above, simple expression support for things like mathematics.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-quickjs">llm-tools-quickjs</a></strong>—provides access to a sandboxed QuickJS JavaScript interpreter, allowing LLMs to run JavaScript code. The environment persists between calls so the model can set variables and build functions and reuse them later on.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-sqlite">llm-tools-sqlite</a></strong>—read-only SQL query access to a local SQLite database.</li>
<li>
<strong><a href="https://github.com/simonw/llm-tools-datasette">llm-tools-datasette</a></strong>—run SQL queries against a remote <a href="https://datasette.io/">Datasette</a> instance!</li>
</ul>
<p>Let’s try that Datasette one now:</p>
<div><pre>llm install llm-tools-datasette
llm -T <span><span>'</span>Datasette("https://datasette.io/content")<span>'</span></span> --td <span><span>"</span>What has the most stars?<span>"</span></span></pre></div>
<p>The syntax here is slightly different: the Datasette plugin is what I’m calling a “toolbox”—a plugin that has multiple tools inside it and can be configured with a constructor.</p>
<p>Specifying <code>--tool</code> as <code>Datasette("https://datasette.io/content")</code> provides the plugin with the URL to the Datasette instance it should use—in this case the <a href="https://datasette.io/content">content database</a> that powers the Datasette website.</p>
<p>Here’s the output, with the schema section truncated for brevity:</p>
<p><img src="https://static.simonwillison.net/static/2025/datasette-tool.jpg" alt="I run that command. It first does a Tool call to Datasette_query with SELECT name, stars, FROM repos ORDER BY stars DESC LIMIT 1. This returns an error message because there is no such column stars. It calls the Datasette_schema() function which returns a whole load of CREATE TABLE statements. Then it executes Datasette_query again this time with SELECT name, stargazers_count FROM repos ORDER BY stargazers_count DESC LIMIT 1. This returns name=datasette a count of 10020, so the model replies and says The repository with the most stars is &quot;datasette&quot; with 10,020 stars."></p>
<p>This question triggered three calls. The model started by guessing the query! It tried <code>SELECT name, stars FROM repos ORDER BY stars DESC LIMIT 1</code>, which failed because the <code>stars</code> column doesn’t exist.</p>
<p>The tool call returned an error, so the model had another go—this time calling the <code>Datasette_schema()</code> tool to get the schema of the database.</p>
<p>Based on that schema it assembled and then executed the correct query, and output its interpretation of the result:</p>
<blockquote>
<p>The repository with the most stars is “datasette” with 10,020 stars.</p>
</blockquote>
<p>Getting to this point was a real <a href="https://www.penny-arcade.com/comic/2010/09/17/mine-all-mine-part-one">Penny Arcade Minecraft moment</a> for me. The possibilities here are <em>limitless</em>. If you can write a Python function for it, you can trigger it from an LLM.</p>
<h4 id="ad-hoc-command-line-tools-with-functions">Ad-hoc command-line tools with <code>--functions</code>
</h4>
<p>I’m looking forward to people building more plugins, but there’s also much less structured and more ad-hoc way to use tools with the LLM CLI tool: the <code>--functions</code> option.</p>
<p>This was inspired by a similar feature <a href="https://sqlite-utils.datasette.io/en/stable/cli.html#defining-custom-sql-functions">I added to sqlite-utils</a> a while ago.</p>
<p>You can pass a block of literal Python code directly to the CLI tool using the <code>--functions</code> option, and any functions defined there will be made available to the model as tools.</p>
<p>Here’s an example that adds the ability to search my blog:</p>
<div><pre>llm --functions <span><span>'</span></span>
<span>import httpx</span>
<span>
<span>def search_blog(q):</span>
<span>    "Search Simon Willison blog"</span>
<span>    return httpx.get("https://simonwillison.net/search/", params={"q": q}).content</span>
<span><span>'</span></span> --td <span><span>'</span>Three features of sqlite-utils<span>'</span></span> -s <span><span>'</span>use Simon search<span>'</span></span></span></pre></div>
<p>This is <em>such a hack</em> of an implementation! I’m literally just hitting <a href="https://simonwillison.net/search/?q=pelicans">my search page</a> and dumping the HTML straight back into tho model.</p>
<p>It totally works though—it helps that the GPT-4.1 series all handle a million tokens now, so crufty HTML is no longer a problem for them.</p>
<p>(I had to add “use Simon search” as the system prompt because without it the model would try to answer the question itself, rather than using the search tool I provided. System prompts for tools are clearly a <em>big topic</em>, Anthropic’s own web search tool has <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/#search-instructions">6,471 tokens of instructions</a>!)</p>
<p>Here’s the output I got just now:</p>
<blockquote>
<p>Three features of sqlite-utils are:</p>
<ol>
<li>It is a combined CLI tool and Python library for manipulating SQLite databases.</li>
<li>It can automatically add columns to a database table if you attempt to insert data that doesn’t quite fit (using the alter=True option).</li>
<li>It supports plugins, allowing the extension of its functionality through third-party or custom plugins.</li>
</ol>
</blockquote>
<p>A better search tool would have more detailed instructions and would return relevant snippets of the results, not just the headline and first paragraph for each result. This is pretty great for just four lines of Python though!</p>
<h4 id="tools-in-the-llm-python-api">Tools in the LLM Python API</h4>
<p>LLM is both a CLI tool and a Python library at the same time (similar to my other project <a href="https://sqlite-utils.datasette.io/">sqlite-utils</a>). The LLM Python library <a href="https://llm.datasette.io/en/stable/python-api.html#tools">grew tool support</a> in LLM 0.26 as well.</p>
<p>Here’s a simple example solving one of the previously hardest problems in LLMs: counting the number of Rs in “strawberry”:</p>
<pre><span>import</span> <span>llm</span>

<span>def</span> <span>count_char_in_text</span>(<span>char</span>: <span>str</span>, <span>text</span>: <span>str</span>) <span>-&gt;</span> <span>int</span>:
    <span>"How many times does char appear in text?"</span>
    <span>return</span> <span>text</span>.<span>count</span>(<span>char</span>)

<span>model</span> <span>=</span> <span>llm</span>.<span>get_model</span>(<span>"gpt-4.1-mini"</span>)
<span>chain_response</span> <span>=</span> <span>model</span>.<span>chain</span>(
    <span>"Rs in strawberry?"</span>,
    <span>tools</span><span>=</span>[<span>count_char_in_text</span>],
    <span>after_call</span><span>=</span><span>print</span>
)
<span>for</span> <span>chunk</span> <span>in</span> <span>chain_response</span>:
    <span>print</span>(<span>chunk</span>, <span>end</span><span>=</span><span>""</span>, <span>flush</span><span>=</span><span>True</span>)</pre>
<p>The <code>after_call=print</code> argument is a way to peek at the tool calls, the Python equivalent of the <code>--td</code> option from earlier.</p>
<p>The <code>model.chain()</code> method is new: it’s similar to <code>model.prompt()</code> but knows how to spot returned tool call requests, execute them and then prompt the model again with the results. A <code>model.chain()</code> could potentially execute dozens of responses on the way to giving you a final answer.</p>
<p>You can iterate over the <code>chain_response</code> to output those tokens as they are returned by the model, even across multiple responses.</p>
<p>I got back this:</p>
<blockquote>
<p><code>Tool(name='count_char_in_text', description='How many times does char appear in text?', input_schema={'properties': {'char': {'type': 'string'}, 'text': {'type': 'string'}}, 'required': ['char', 'text'], 'type': 'object'}, implementation=&lt;function count_char_in_text at 0x109dd4f40&gt;, plugin=None) ToolCall(name='count_char_in_text', arguments={'char': 'r', 'text': 'strawberry'}, tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu') ToolResult(name='count_char_in_text', output='3', tool_call_id='call_DGXcM8b2B26KsbdMyC1uhGUu', instance=None, exception=None)</code><br></p>
<p>There are 3 letter “r”s in the word “strawberry”.</p>
</blockquote>
<p>LLM’s Python library also supports <code>asyncio</code>, and tools can be <code>async def</code> functions <a href="https://llm.datasette.io/en/latest/python-api.html#tool-functions-can-be-sync-or-async">as described here</a>. If a model requests multiple async tools at once the library will run them concurrently with <code>asyncio.gather()</code>.</p>
<p>The Toolbox form of tools is supported too: you can pass <code>tools=[Datasette("https://datasette.io/content")]</code> to that <code>chain()</code> method to achieve the same effect as the <code>--tool 'Datasette(...)</code> option from earlier.</p>
<h4 id="why-did-this-take-me-so-long-">Why did this take me so long?</h4>
<p>I’ve been tracking <a href="https://simonwillison.net/tags/llm-tool-use/">llm-tool-use</a> for a while. I first saw the trick described in <a href="https://arxiv.org/abs/2210.03629">the ReAcT paper</a>, first published in October 2022 (a month before the initial release of ChatGPT). I built <a href="https://til.simonwillison.net/llms/python-react-pattern">a simple implementation of that</a> in a few dozen lines of Python. It was clearly a very neat pattern!</p>
<p>Over the past few years it has become <em>very</em> apparent that tool use is the single most effective way to extend the abilities of language models. It’s such a simple trick: you tell the model that there are tools it can use, and have it output special syntax (JSON or XML or <code>tool_name(arguments)</code>, it doesn’t matter which) requesting a tool action, then stop.</p>
<p>Your code parses that output, runs the requested tools and then starts a new prompt to the model with the results.</p>
<p>This works with almost <strong>every model</strong> now. Most of them are specifically trained for tool usage, and there are leaderboards like the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a> dedicated to tracking which models do the best job of it.</p>
<p>All of the big model vendors—OpenAI, Anthropic, Google, Mistral, Meta—have a version of this baked into their API, either called tool usage or function calling. It’s all the same underlying pattern.</p>
<p>The models you can run locally are getting good at this too. Ollama <a href="https://ollama.com/blog/tool-support">added tool support</a> last year, and it’s baked into the <a href="https://github.com/ggml-org/llama.cpp/blob/master/docs/function-calling.md">llama.cpp</a> server as well.</p>
<p>It’s been clear for a while that LLM absolutely needed to grow support for tools. I released <a href="https://simonwillison.net/2025/Feb/28/llm-schemas/">LLM schema support</a> back in February as a stepping stone towards this. I’m glad to finally have it over the line.</p>
<p>As always with LLM, the challenge was designing an abstraction layer that could work across as many different models as possible. A year ago I didn’t feel that model tool support was mature enough to figure this out. Today there’s a very definite consensus among vendors about how this should work, which finally gave me the confidence to implement it.</p>
<p>I also presented a workshop at PyCon US two weeks ago about <a href="https://simonwillison.net/2025/May/15/building-on-llms/">Building software on top of Large Language Models</a>, which was exactly the incentive I needed to finally get this working in an alpha! Here’s the <a href="https://building-with-llms-pycon-2025.readthedocs.io/en/latest/tools.html">tools section</a> from that tutorial.</p>
<h4 id="is-this-agents-then-">Is this agents then?</h4>
<p><em>Sigh</em>.</p>
<p>I still <a href="https://simonwillison.net/2024/Dec/31/llms-in-2024/#-agents-still-haven-t-really-happened-yet">don’t like</a> using the term “agents”. I worry that developers will think <a href="https://simonwillison.net/2025/May/22/tools-in-a-loop/">tools in a loop</a>, regular people will think virtual AI assistants <a href="https://en.m.wikipedia.org/wiki/Her_(2013_film)">voiced by Scarlett Johansson</a> and academics will <a href="https://simonwillison.net/2025/Mar/19/worms-and-dogs-and-countries/">grumble about thermostats</a>. But in the LLM world we appear to be converging on “tools in a loop”, and that’s absolutely what this.</p>
<p>So yes, if you want to build “agents” then LLM 0.26 is a great way to do that.</p>
<h4 id="what-s-next-for-tools-in-llm-">What’s next for tools in LLM?</h4>
<p>I already have a <a href="https://github.com/simonw/llm/milestone/13">LLM tools v2 milestone</a> with 13 issues in it, mainly around improvements to how tool execution logs are displayed but with quite a few minor issues I decided shouldn’t block this release. There’s a bunch more stuff in the <a href="https://github.com/simonw/llm/issues?q=is%3Aissue%20state%3Aopen%20label%3Atools">tools label</a>.</p>
<p>I’m most excited about the potential for plugins.</p>
<p>Writing tool plugins is <em>really fun</em>. I have an <a href="https://github.com/simonw/llm-plugin-tools">llm-plugin-tools</a> cookiecutter template that I’ve been using for my own, and I plan to put together a tutorial around that soon.</p>
<p>There’s more work to be done adding tool support to more model plugins. I added <a href="https://llm.datasette.io/en/stable/plugins/advanced-model-plugins.html#supporting-tools">details of this</a> to the advanced plugins documentation. This commit <a href="https://github.com/simonw/llm-gemini/commit/a7f1096cfbb733018eb41c29028a8cc6160be298">adding tool support for Gemini</a> is a useful illustratino of what’s involved.</p>

<p>And yes, <strong>Model Context Protocol</strong> support is clearly on the agenda as well. MCP is emerging as the standard way for models to access tools at a frankly bewildering speed. Two weeks ago it wasn’t directly supported by the APIs of any of the major vendors. In just the past eight days <a href="https://simonwillison.net/2025/May/27/mistral-agents-api/">it’s been added</a> by OpenAI, Anthropic <em>and</em> Mistral! It’s feeling like a lot less of a moving target today.</p>
<p>I want LLM to be able to act as an MCP client, so that any of the MCP servers people are writing can be easily accessed as additional sources of tools for LLM.</p>
<p>If you’re interested in talking more about what comes next for LLM, <a href="https://datasette.io/discord-llm">come and chat to us in our Discord</a>.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why the Original Macintosh Had a Screen Resolution of 512×324 (168 pts)]]></title>
            <link>https://512pixels.net/2025/05/original-macintosh-resolution/</link>
            <guid>44110219</guid>
            <pubDate>Tue, 27 May 2025 20:02:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://512pixels.net/2025/05/original-macintosh-resolution/">https://512pixels.net/2025/05/original-macintosh-resolution/</a>, See on <a href="https://news.ycombinator.com/item?id=44110219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-32349">
	<!-- .entry-header -->

	<div>
		<p>Many classic Macs came with — or supported — displays running at 512×384 pixels, but many compact Macs, ranging from <a href="https://support.apple.com/en-us/112190">the original 1984 machine</a> up through 1991’s <a href="https://support.apple.com/en-us/112201">Macintosh Classic II</a> had built-in CRTs running at 512×342 pixels. That covers all black-and-white compact Macs with a 9-inch screen. The later Color Classic and Color Classic II used a 10-inch CRT at a full 512×384.</p>
<p>This came up when <a href="https://512pixels.net/2025/05/oh-hey-it-me/">I joined John Gruber on The Talk Show</a>. At one point in the show, I rattled off the original Mac’s resolution as being 512×384.</p>
<p>Except… it wasn’t. The original Mac screen ran at 512×342. I remembered the right number and corrected myself a moment later, but given the name of this website, it was pretty embarrassing. This set me off on a journey to understand why Apple made this decision. Why were the displays on early Macs 42 pixels shorter in height than later ones?</p>
<p>After doing <em>a lot</em> of reading, there are several factors to consider when trying to answer this question.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple1984mac-full.png" alt="Original Macintosh"></p>
<h2>Memory</h2>
<p>The original Mac had a mere 128 <em>kilobytes</em> of memory. The photo of the original Macintosh in this blog post is 604 KB in size, some 4.7x larger than the entire memory footprint of the 1984 machine. Of course, Apple would improve this with later models, but many design decisions made to accommodate the original Mac would forward for years.</p>
<p>Over at Folklore.org, <a href="https://www.folklore.org/Five_Different_Macs.html">Andy Hertzfeld wrote a great post</a> walking through several early versions of what would become the Macintosh, including ones with even <em>less</em> memory:</p>
<blockquote><p>
  In the beginning of 1982, the original 68000 design was more than a year old, and the software was nowhere near finished, so Burrell [Smith] was afraid some of the trade-offs of the original design were no longer current. He used the expansive canvas of a custom chip, where additional logic was almost free, to update the architecture.</p>
<p>  The most important decision was admitting that the software would never fit into 64K of memory and going with a full 16-bit memory bus, requiring 16 RAM chips instead of 8. The extra memory bandwidth allowed him to double the display resolution, going to dimensions of 512 by 342 instead of 384 by 256. He also added bells and whistles like a fancy, DMA-fed sound generator with four independent voices. This was the fourth version of the Macintosh design.
</p></blockquote>
<p>Shipping a computer in the 1980s with a resolution of 384×256 wouldn’t have been too wild. 1982’s Commodore 64 ran at a maximum resolution 320×200. The Apple IIe that shipped in 1983 offered several display modes:</p>
<ul>
<li>40 or 80 columns text, white-on-black, with 24 lines</li>
<li>Low-Resolution: 40×48 (16 colors)</li>
<li>High-Resolution: 280×192 (6 colors)</li>
<li>Double-Low-Resolution: 80×48 (16 colors)</li>
<li>Double-High-Resolution: 560×192 (16 colors)</li>
</ul>
<p>Computers like the C64 and Apple II had to pull off a lot of tricks to pull off getting graphics on the screen. The Macintosh was going to be powered by a full GUI, and 384×256 would have been just too chunky, so thinking about 128 kilobytes of RAM as an <em>upgrade</em> is a fun twist on the normal take of “Wow, the original Mac was so held back!” Really, it’s amazing that it could do what it did.</p>
<p>That feeling takes on new life when you realize the Mac used a portion of its memory to drive the display. At 512×342, the memory needed to draw the screen was a total of 21.8 KB. Had Apple opted for a 4:3 display running at 512×384, the system would have needed 24 KB for the display. Every byte was precious back in the day. Again, <a href="https://folklore.org/Monkey_Lives.html">we turn to Andy Hertzfeld</a>:</p>
<blockquote><p>
  The original Macintosh only had 128K bytes of RAM (that’s one eighth of a megabyte), so dealing with memory management was usually the hardest part of writing both the system and applications. We allocated around 16K bytes for system use, and another 22K bytes for the 512 by 342 black and white screen, so applications were left with only 90K bytes or so. The bigger ones like MacWrite or MacPaint seemed to be bursting at the seams.
</p></blockquote>
<p>It seems that two things are true:</p>
<ul>
<li>The original Macintosh shipped with more memory than earlier designs</li>
<li>Even at 128K, things were extremely tight</li>
</ul>
<p>Daniel Knight pointed this out when writing about the original Mac:</p>
<blockquote><p>
  As&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Scrooge_McDuck.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">Andy Hertzfeld writes</a>, the Mac was only going to have a 256×256 pixel display (a step up from the 280×192 graphics of the Apple II). It wasn’t until&nbsp;<a href="http://www.folklore.org/StoryView.py?project=Macintosh&amp;story=Good_Earth.txt&amp;topic=Origins&amp;sortOrder=Sort%20by%20Date">January 1981</a>&nbsp;that the Mac team decided to give the Motorola 68000 a try. A good thing, too, as the first Mac shipped with a 512×342 pixel display, and that would have consumed over 30% of the 64 KB of memory originally envisioned for the low-cost information appliance.
</p></blockquote>
<h2>CPU Timing</h2>
<p>At the heart of the Macintosh was a Motorola 68000 CPU running at 8 MHz. Just like the 128 kilobytes of RAM, this came with some inherit limitations when paired with the display hardware.</p>
<p>To minimize CRT flicker, Apple worked to achieve a vertical refresh rate of 60 Hz. This meant the CPU spent one-third of its time drawing the display. Just as with the memory, a taller screen would have taken more resources away from running the Mac’s operating system and programs.</p>
<p>If you are familiar with standard TV formats, you probably have already picked up on the fact that this refresh rate/screen size combination meant the Mac was incompatible with NTSC composite video, which the Apple II supported. (It’s also different than PAL systems.) This let Apple balance performance and picture quality in a way the team saw fit, given the hardware that they had, <a href="https://www.folklore.org/Joining_Apple_Computer.html">as Bill Atkinson wrote:</a></p>
<blockquote><p>
  The Apple II displayed white text on a black background. I argued that to do graphics properly we had to switch to a white background like paper. It works fine to invert text when printing, but it would not work for a photo to be printed in negative. The Lisa hardware team complained the screen would flicker too much, and they would need faster refresh with more expensive RAM to prevent smearing when scrolling. Steve listened to all the pros and cons then sided with a white background for the sake of graphics.
</p></blockquote>
<p>Here’s the thing: the original Mac <em>did not</em> run at 8 MHz, but rather 7.83 MHz. This slight tuning meant Apple could time the CPU’s cycles and the CRT’s need for updating more easily.</p>
<h2>Square Pixels</h2>
<p>Running the 9-inch CRT at 512×342 gave the original Mac a pixels density of 72 PPI, but more importantly, the screen size allowed the Mac to have square pixels.</p>
<p>Apple’s first GUI-powered machine, <a href="https://www.macstories.net/mac/the-lisa/">the Lisa</a>, famously had rectangular pixels, <a href="https://folklore.org/Square_Dots.html">as Andy Hertzfeld covered here</a>:</p>
<blockquote><p>
  The Lisa team decided to optimize their display for horizontal resolution, in order to be able to display 80 columns of text in an attractive font. The vertical resolution wasn’t as important, because vertical scrolling works much better for text than horizontal scrolling. The designers decided to endow Lisa with twice as much horizontal resolution as vertical, using a 720 by 360 pixel display, with pixels that were twice as high as they were wide. This was great for text oriented applications like the word processor, but it made things somewhat awkward for the more graphical applications.</p>
<p>  When Burrell redesigned the Macintosh in December 1980 to use the same microprocessor as Lisa, the Motorola 68000, it set off shock waves within Apple. Not only was Burrell’s new design much simpler than Lisa, with less than half the chip count, but it also ran almost twice as fast, using an 8 megahertz clock instead of a 5 megahertz clock. Among other advantages was the fact that the Mac’s 384 by 256 pixel display had the identical horizontal and vertical resolution, a feature that we called “square dots”. Square dots made it easier to write graphical applications, since you didn’t have to worry about the resolution disparity.
</p></blockquote>
<p>Hertzfeld goes on to share that the Mac team tried to get the Lisa team to switch to square pixels, bumping the machine’s resolution to a mind-blowing-for-the-time 768×512 pixels, but it wasn’t in the cards, as the Lisa was well on its way to shipping by the time this meeting took place.</p>
<p><img decoding="async" src="https://512pixels.net/wp-content/uploads/2025/05/apple-lisa.jpg" alt="Apple Lisa"></p>
<p>Of course, the Lisa would end up being a doomed product, and in 1985, Apple rebadged a later revision of the machine as the “Macintosh XL.” It shipped with <a href="https://en.wikipedia.org/wiki/MacWorks_XL">a software shim called “MacWorks XL”</a> that allowed Mac software to run on the Lisa, but the rectangular pixels made the software appear stretched. To solve this, Apple sold a product named the  Macintosh XL Screen Kit, which changed the resolution to 608×432 pixels. This is how the product is described in <a href="https://512pixels.net/wp-content/uploads/2025/05/Lisa-DIY-Guide.pdf">a document outlining DIY upgrades</a> from <a href="https://en.wikipedia.org/wiki/Sun_Remarketing">Sun Remarketing</a>, a company that was focused on keeping Lisa hardware up and running.</p>
<blockquote><p>
  No recently restored Lisa/Mac XL is complete without a Macintosh XL Screen Kit. Unlike the standard 9-inch Macintosh which has square pixels, the stock Lisa/XL has rectangular pixels. With rectangular pixels, circles look like footballs, squares look like spaghetti boxes. The purpose of the Macintosh XL Screen Kit is to square up the pixels. Proportions become exactly the same as on other Macs (1 to 1 ), but the overall display area (608 pixels x 432 pixels) is made roughly the same as a 12-inch Macintosh 11 WYSIWYG monitor (640×480). Standard 9-inch Macs only display 512×342 pixels.</p>
<p>  The complete screen modification kit includes new 3A boot ROMs, a new video ROM and a new yoke coil. (Newer software requires System Update 5.0 and MacWorks Plus as well.) Conscientious installation of the complete screen kit requires one to two hours.
</p></blockquote>
<h2>Mimicking the Real World</h2>
<p>In addition to their square pixels making on-screen graphics look better, the Macintosh team also wanted the computer to be useful for those who needed to print their work. The 72 DPI screen was more than sharp enough for work in applications like MacWrite, MacPaint, and the page layout tools that would follow them. Users could see their work full-sized or at a reduced scale to get a sense of the overall page they were working on. Larger displays would come, but for 1984, 512×342 was plenty.</p>
<h2>Everything in Balance</h2>
<p>In short, there’s no easy answer to explain why early compact Macs ran at a screen resolution of 512×342. Rather, Apple was doing what it does best: designing a product with the right trade-offs for performance, ease of use, and cost. In a few short years, the Mac would grow to support larger displays, but for 1984, the balance was set correctly.</p>
<p>In the very first edition of <em>Macworld</em> magazine, <a href="https://archive.org/details/MacWorld_8404_April_1984_premier">Matthew Douglas wrote</a>:</p>
<blockquote><p>
  Appearances can be deceiving. Most computers display text on one of 24 or 25 “invisible” horizontal lines on the screen. This display is called text mode. To display graphics, the software switches to graphics mode, and the display becomes a field of dots. Each dot, or pixel, is either off (invisible) or on (visible). Of course, a computer may have more than one text mode or two or more graphics modes, or it may be a mixed mode of graphics and text.</p>
<p>  The Mac display has only one mode: graphics. The entire screen is made up of dots: 512 dots horizontally and 342 dots vertically, a total of175,104 dots that combine to display everything you’ll ever see on a Mac screen. (Now you know the secret behind the incredible range of type fonts, attributes, and type sizes.)
</p></blockquote>
<p>In the same edition, David Bunnell interviewed Bill Gates, and he was asked about what made the Mac special. Gates — who had previously said the Mac was a computer he would want his mom to try — replied:</p>
<blockquote><p>
  The Mac was designed as a graphics machine. Apple didn’t put in a ROM character generator or a bunch of video modes. They put in only one video mode, and that’s the pure bit-mapped, 512-by 342-pixel screen. The monitor was designed into the machine so that they could get extremely crisp pictures and have one integrated system. They knew what the aspect ratio was and how the dots would appear. And they also made sure that the mouse would be used and that the 64K ROM would support very rich graphics interaction.</p>
<p>  You can configure a PC with one of the better graphics boards and add a Microsoft mouse and the necessary software, but that’s not the thrust of the machine. The PC is used primarily in its text mode, and to date it’s used mostly without a mouse; you couldn’t get performance or graphics like the Mac’s out of the PC at a comparable price. Although they’re both “turing” machines (that is, they have finite memory), the thrust of the Mac is quite different.</p>
<p>  Of all the personal computers available today, the Mac is unique. It’s the first time somebody said, “We don’t need a lot of the things that other personal computers have, so let’s optimize a few areas and make sure the software is designed around them.”
</p></blockquote>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Running GPT-2 in WebGL: Rediscovering the Lost Art of GPU Shader Programming (140 pts)]]></title>
            <link>https://nathan.rs/posts/gpu-shader-programming/</link>
            <guid>44109257</guid>
            <pubDate>Tue, 27 May 2025 18:02:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nathan.rs/posts/gpu-shader-programming/">https://nathan.rs/posts/gpu-shader-programming/</a>, See on <a href="https://news.ycombinator.com/item?id=44109257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content"><article><p><i>May, 24 2025 •
16 min read •
2461 words</i></p><p>Preface: A few weeks back, I implemented GPT-2 using WebGL and shaders (<a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">Github Repo</a>) which made the front page of Hacker News (<a href="https://news.ycombinator.com/item?id=43870998">discussion</a>). By popular demand, here is a short write-up over the main ideas behind GPU shader programming (for general-purpose computing).</p><div><h2>Table Of Contents</h2><hr><div><nav id="TableOfContents"><ol><li><a href="#the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</a></li><li><a href="#graphics-api-vs-compute-api">Graphics API vs Compute API</a></li><li><a href="#implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</a><ol><li><a href="#textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</a></li><li><a href="#fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</a></li><li><a href="#chaining-passes">Chaining Passes</a></li><li><a href="#limitations">Limitations</a></li></ol></li></ol></nav></div></div><h2 id="the-origins-of-general-purpose-gpu-programming">The Origins of General Purpose GPU Programming</h2><hr><p>In the early 2000s, NVIDIA introduced programmable shaders with the GeForce 3 (2001) and GeForce FX (2003). Instead of being limited to predetermined transformations and effects of earlier GPUs, developers were now given unprecedented control over the rendering pipeline, enabling much more sophisticated visual effects. These programmable shaders laid the foundation for modern GPU computing.</p><p>Researchers soon discovered that certain computations (like linear algebra involving matrices and vectors) could be accelerated by casting them as graphics operations on the GPU.
However, using shader languages like OpenGL’s GLSL for no-graphics tasks was cumbersome. By the mid-2000s, the need for a more straightforward, non-graphics interface to GPUs had become clear, and NVIDIA saw a new opportunity.</p><p>Inspired by the demand for <strong>general-purpose GPU (GPGPU)</strong> programming, in November 2006, NVIDIA released <strong>CUDA</strong>, the <strong>Compute Unified Device Architecture</strong>. CUDA is a parallel computing platform and programming model that gives developers direct access to the GPU’s computational power without the intermediary of a graphics API. With CUDA, one could write C/C++ code to execute on the CPU using straightforward extensions for parallel kernels and managing GPU memory explicitly. This meant that developers could now ignore graphics-specific concepts and dramatically lowered the barrier for general-purpose GPU computing. Following CUDA came OpenCL which expanded general purpose computing beyond the NVIDIA ecosystem.</p><h2 id="graphics-api-vs-compute-api">Graphics API vs Compute API</h2><hr><p>Traditional graphics APIs like OpenGL are centered around a fixed-function pipeline tailored for rendering images. The pipeline consists of stages like vertex processing, rasterization, fragment processing, etc. Each stage can be programmable with shaders, but the overall flow is fixed.
Using OpenGL for computation required a lot of boilerplate. One had to pack data into texture formats, use off-screen framebuffers to capture the results, and often perform multiple render passes to accomplish multi-stage algorithms.</p><p>In contrast, OpenCL and CUDA expose a direct compute model which lets you treat the GPU as one giant SIMD processor:</p><ul><li><strong>Kernels, not shaders</strong>: You write a function and then launch thousands of copies to run in parallel (no notion of vertices or fragments).</li><li><strong>Raw buffers</strong>: Allocate arrays of floats or integers, read/write them directly, and move them back and forth between host and device with explicit copy calls.</li><li><strong>User-driven pipeline</strong>: You define exactly what happens and when instead of using a predefined fixed sequence of rendering stages.</li></ul><p>The result is a far more natural fit for linear algebra, simulations, physics, ML, and any algorithm where you just want to compute independent calculations in bulk.</p><p>In OpenGL, the output of your computation would ultimately be pixels in a framebuffer or values in a texture; in OpenCL, the output can be data in any form (float arrays, computed lists of numbers, etc.) which you then transfer back to the CPU or use in further computations. This makes OpenCL more suitable for general algorithms where you just want the numerical results.</p><h2 id="implementing-gpt-2-with-shaders">Implementing GPT-2 with shaders</h2><hr><p>Below covers textures, framebuffers, vertex and fragment shaders, and other graphics specific concepts I hijacked to get GPT-2 running on a GPU using shaders.</p><h3 id="textures--framebuffers-the-data-bus">Textures &amp; Framebuffers: The Data Bus</h3><p>In traditional graphics rendering, a <strong>texture</strong> is simply a 2D (or 3D) array of pixel data stored in GPU memory. When you map a texture onto a triangle, the fragment shader “samples” it to look up color values. In our compute‐as‐graphics paradigm, we hijack this same mechanism to store and manipulate numerical data instead of colors:</p><ul><li><p><strong>Textures as tensors</strong>:
Each texture is an array of floating‐point values (we use single‐channel R32F formats), where each pixel encodes one element of a matrix or vector. Just like you’d think of an H×W texture as holding H×W RGB pixels for an image, here it holds H×W scalar values for a weight matrix or activation map.</p></li><li><p><strong>Sampling without filtering</strong>:
We use functions like <code>texelFetch</code> to read texture data by exact integer coordinates, bypassing any interpolation. This gives us deterministic, “random access” reads into our weight and activation arrays, akin to indexing into a CPU array by row and column.</p></li></ul><p>A <strong>Framebuffer Object (FBO)</strong> is a lightweight container that lets us redirect rendering output from the screen into one of our textures:</p><ol><li><p><strong>Attach a texture as the render target</strong>:
By binding a texture to an FBO, any draw call you make, normally destined for your monitor, writes into that texture instead. The fragment shader’s <code>out</code> variable becomes a write port into GPU memory.</p></li><li><p><strong>Offscreen and ping-pong rendering</strong>:
Because we can attach different textures in succession, we “ping-pong” between them: one pass writes into <strong>Texture A</strong>, the next pass reads from <strong>Texture A</strong> while writing into <strong>Texture B</strong>, and so on. This avoids ever copying data back to the CPU until the very end.</p></li><li><p><strong>High‐throughput data bus</strong>:
All of this happens entirely on the GPU’s VRAM bus. Binding textures and framebuffers is just pointer swapping on the GPU. Once set up, your fragment shader passes stream through millions of cores in parallel, reading, computing, and writing without ever touching system memory.</p></li></ol><p>Together, textures and FBOs form the <strong>data bus</strong> of our shader‐based compute engine: textures hold the raw bits of your neural network (weights, intermediate activations, and outputs), and framebuffers let you chain shader passes seamlessly, keeping everything on the high-speed GPU pipeline until you explicitly pull the final logits back to the CPU.</p><h3 id="fragment-shaders-as-compute-kernels">Fragment Shaders as Compute Kernels</h3><p>Fragment shaders are where the magic happens. Instead of using fragment shaders to shade pixels for display, we hijack them as compute kernels; each fragment invocation becomes one “thread” that calculates a single output value. The GPU will launch thousands of these in parallel, giving us massive throughput for neural-network operations.</p><p>Below is an example fragment shader for matrix multiplication:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Matrix Multiply (C = A × B)</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_A<span>;</span>    <span>// Texture holding matrix A (M x K)</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_B<span>;</span>    <span>// Texture holding matrix B (K x N)</span>
</span></span><span><span><span>uniform</span> <span>int</span>        u_K<span>;</span>   <span>// Shared inner dimension</span>
</span></span><span><span><span>out</span> <span>vec4</span>           outColor<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>// Determine the output coordinate (i, j) from the fragment’s pixel position</span>
</span></span><span><span>  <span>ivec2</span> coord <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> sum <span>=</span> <span>0.0</span><span>;</span>
</span></span><span><span>
</span></span><span><span>  <span>// Perform the dot–product along the K dimension</span>
</span></span><span><span>  <span>for</span> <span>(</span><span>int</span> k <span>=</span> <span>0</span><span>;</span> k <span>&lt;</span> u_K<span>;</span> <span>++</span>k<span>)</span> <span>{</span>
</span></span><span><span>    <span>float</span> a <span>=</span> texelFetch<span>(</span>u_A<span>,</span> <span>ivec2</span><span>(</span>k<span>,</span> coord<span>.</span>y<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    <span>float</span> b <span>=</span> texelFetch<span>(</span>u_B<span>,</span> <span>ivec2</span><span>(</span>coord<span>.</span>x<span>,</span> k<span>),</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>    sum <span>+=</span> a <span>*</span> b<span>;</span>
</span></span><span><span>  <span>}</span>
</span></span><span><span>
</span></span><span><span>  <span>// Write the result into the single‐channel R component of the output texture</span>
</span></span><span><span>  outColor <span>=</span> <span>vec4</span><span>(</span>sum<span>,</span> <span>0.0</span><span>,</span> <span>0.0</span><span>,</span> <span>1.0</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>Here we have:</p><ul><li><strong>Per-pixel work item</strong>: Each fragment corresponds to one matrix element (i, j). The GPU runs this loop for every (i, j) in parallel across its shader cores.</li><li><strong>Exact indexing</strong>: texelFetch reads a single float by its integer coordinate.</li><li><strong>Write-back</strong>: Assigning to outColor.r writes that computed value directly into the bound FBO’s texture at (i, j).</li></ul><p>Here is another fragment shader but for the GELU activation function:</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// GELU Activation</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>precision</span> <span>highp</span> <span>float</span><span>;</span>
</span></span><span><span><span>uniform</span> <span>sampler2D</span> u_X<span>;</span>
</span></span><span><span><span>out</span> <span>vec4</span> o<span>;</span>
</span></span><span><span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  <span>ivec2</span> c <span>=</span> <span>ivec2</span><span>(</span>gl_FragCoord<span>.</span>xy<span>);</span>
</span></span><span><span>  <span>float</span> x <span>=</span> texelFetch<span>(</span>u_X<span>,</span> c<span>,</span> <span>0</span><span>).</span>r<span>;</span>
</span></span><span><span>  <span>float</span> t <span>=</span> <span>0.5</span> <span>*</span> <span>(</span><span>1.0</span> <span>+</span> tanh<span>(</span><span>0.79788456</span> <span>*</span> <span>(</span>x <span>+</span> <span>0.044715</span> <span>*</span> x<span>*</span>x<span>*</span>x<span>)));</span>
</span></span><span><span>  o <span>=</span> <span>vec4</span><span>(</span>x <span>*</span> t<span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="shared-vertex-shader">Shared Vertex Shader</h4><p>Every operation gets its own fragment shader since that’s where the math for the operation happens. The vertex shader, on the other hand, is simple and the same for each. All it does is draw two triangles which covers the entire view port.</p><div><pre tabindex="0"><code data-lang="glsl"><span><span><span>// Shared Vertex Shader</span>
</span></span><span><span><span>#version 300 es</span>
</span></span><span><span><span>in</span> <span>vec2</span> a_position<span>;</span>
</span></span><span><span><span>out</span> <span>vec2</span> v_uv<span>;</span>
</span></span><span><span><span>void</span> main<span>()</span> <span>{</span>
</span></span><span><span>  v_uv <span>=</span> a_position <span>*</span> <span>0.5</span> <span>+</span> <span>0.5</span><span>;</span>  <span>// map [-1,+1] to [0,1]</span>
</span></span><span><span>  gl_Position <span>=</span> <span>vec4</span><span>(</span>a_position<span>,</span> <span>0</span><span>,</span> <span>1</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><ul><li><strong>Full-screen quad</strong>: Two triangles cover the viewport. Every pixel in the fragment stage maps to one tensor element.</li><li><strong>Reusable</strong>: Because the vertex work is identical for all operations, we compile it once and reuse it across every matrix multiply, activation, and bias-add pass.</li></ul><p>With this structure in mind, every “shader pass” is really just:</p><ol><li><strong>Vertex shader</strong>: map two triangles to the viewport</li><li><strong>Fragment shader</strong>: perform one tiny piece of your transformer math per pixel</li></ol><h3 id="chaining-passes">Chaining Passes</h3><p>Under the hood, every neural‐network operation, whether it’s a matrix multiply, an activation function, or a bias addition, boils down to four simple GPU steps:</p><ol><li>Bind inputs as textures (weights, activations, or intermediate results).</li><li>Attach a fresh output texture to an offscreen framebuffer (FBO).</li><li>Draw a full‐screen quad using the shared vertex shader.</li><li>Execute the fragment shader, which performs the actual computation per pixel.</li></ol><p>All of the WebGL boilerplate lives in our <code>_runPass</code> helper, so each pass in the GPT-2 forward loop feels like a single, declarative call:</p><div><pre tabindex="0"><code data-lang="typescript"><span><span><span>private</span> <span>_runPass</span><span>(</span>
</span></span><span><span>  <span>name</span>: <span>string</span><span>,</span>
</span></span><span><span>  <span>inputs</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>WebGLTexture</span> <span>},</span>
</span></span><span><span>  <span>ints</span><span>:</span> <span>{</span> <span>[</span><span>u</span>: <span>string</span><span>]</span><span>:</span> <span>number</span> <span>},</span>
</span></span><span><span>  <span>outTex</span>: <span>WebGLTexture</span><span>,</span>
</span></span><span><span>  <span>W</span>: <span>number</span><span>,</span>
</span></span><span><span>  <span>H</span>: <span>number</span>
</span></span><span><span><span>)</span> <span>{</span>
</span></span><span><span>  <span>// Grab the WebGL2 context and compiled shader program for this pass
</span></span></span><span><span><span></span>  <span>const</span> <span>gl</span> <span>=</span> <span>this</span><span>.</span><span>gl</span><span>;</span>
</span></span><span><span>  <span>const</span> <span>prog</span> <span>=</span> <span>this</span><span>.</span><span>programs</span><span>[</span><span>name</span><span>];</span> <span>// This has our vertex and frag shaders
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>useProgram</span><span>(</span><span>prog</span><span>);</span>
</span></span><span><span>
</span></span><span><span>  <span>// BOILERPLATE: Bind all input textures as uniforms
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Bind an FBO and attach our empty texture to it.
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// BOILERPLATE: Set up the full-screen quad geometry
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Draw into our texture
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>viewport</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>W</span><span>,</span> <span>H</span><span>);</span>            <span>// Ensure viewport matches texture size
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>drawArrays</span><span>(</span><span>gl</span><span>.</span><span>TRIANGLES</span><span>,</span> <span>0</span><span>,</span> <span>6</span><span>);</span>  <span>// Runs our shaders
</span></span></span><span><span><span></span>
</span></span><span><span>  <span>// Clean up: Unbind FBO
</span></span></span><span><span><span></span>  <span>gl</span><span>.</span><span>bindFramebuffer</span><span>(</span><span>gl</span><span>.</span><span>FRAMEBUFFER</span><span>,</span> <span>null</span><span>);</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><h4 id="forward-pass-layer-by-layer">Forward Pass: Layer by Layer</h4><p>Because each pass leaves its results in VRAM, we never pay the cost of round-trips back to the CPU until the very end. Below is a high-level description of the entire forward pass:</p><ol><li><strong>Upload Embeddings</strong>: Compute the token+position embeddings on the CPU and send them to the GPU as one texture.</li><li><strong>Transformer Layers (12 in total)</strong>:<ul><li><em>Normalize &amp; Project</em>: Apply layer normalization, then run the attention and feed-forward sublayers entirely on the GPU.</li><li><em>Attention</em>: Compute queries, keys, values; calculate attention weights; combine values.</li><li><em>Feed-Forward</em>: Two matrix multiplies with a GELU activation in between.</li><li><em>Residuals</em>: Add the layer’s input back in at each substep.</li></ul></li><li><strong>Final Normalization &amp; Output</strong>: Do one last layer normalization, multiply by the output weight matrix, then read the resulting logits back to the CPU.</li></ol><p>Once logits are back on the CPU, we apply softmax and sample (top-k or top-p) to pick the next token. Then the process starts over again with the new token being appended to the context.</p><p>By chaining these operation passes together, we keep the entire GPT-2 pipeline on the GPU until the final logits. This is how programmable shaders let us treat the graphics pipeline as a general-purpose parallel engine.</p><h3 id="limitations">Limitations</h3><p>While hijacking WebGL allows us to run machine learning models on the GPU, it carries several key limitations:</p><ul><li><strong>No shared/local memory</strong>: Fragment shaders can only read/write global textures. There’s no on-chip scratchpad for blocking or data reuse, so you’re limited to element-wise passes.</li><li><strong>Texture size limits</strong>: GPUs enforce a maximum 2D texture dimension (e.g. 16 K×16 K). Anything larger must be manually split into tiles, adding bookkeeping and extra draw calls.</li><li><strong>No synchronization or atomics</strong>: You can’t barrier or coordinate between fragments in a pass, making reductions, scatter/gather, and other data-dependent patterns difficult or impossible.</li><li><strong>Draw-call and precision overhead</strong>: Every neural-net operation requires binding an FBO, swapping textures, and issuing a draw call (dozens per layer) which incurs CPU overhead. Plus, you’re bound to 16- or 32-bit floats (via <code>EXT_color_buffer_float</code>), with no double precision or integer textures.</li></ul><p>Taken together, these constraints make shader-based compute an interesting educational project but a only a historical novelty for real world use. Compute APIs like CUDA or OpenCL give easier and better tools to achieve the same thing.</p><p>Thanks for reading! You can view the code and run the demo locally at the repo <a href="https://github.com/nathan-barry/gpt2-webgl/tree/main">here</a>. Contact me on <a href="https://x.com/nathanbarrydev">X</a> if you have any suggestions.</p><br></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US pauses new student visa interviews as it mulls expanding social media vetting (125 pts)]]></title>
            <link>https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</link>
            <guid>44109253</guid>
            <pubDate>Tue, 27 May 2025 18:02:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501">https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501</a>, See on <a href="https://news.ycombinator.com/item?id=44109253">Hacker News</a></p>
Couldn't get https://www.politico.com/news/2025/05/27/trump-team-orders-stop-to-new-student-visa-interviews-as-it-weighs-expanding-social-media-vetting-00370501: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[I salvaged $6k of luxury items discarded by Duke students (249 pts)]]></title>
            <link>https://indyweek.com/culture/duke-students-dumpster-diving/</link>
            <guid>44108207</guid>
            <pubDate>Tue, 27 May 2025 15:59:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://indyweek.com/culture/duke-students-dumpster-diving/">https://indyweek.com/culture/duke-students-dumpster-diving/</a>, See on <a href="https://news.ycombinator.com/item?id=44108207">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

					

<article id="post-278832">
	<div>

		
		<p><strong>I</strong> live in an apartment building in downtown Durham that houses more Duke University undergrads than any other category of person—a friend once characterized it as an “adult dorm”—so it wasn’t all that surprising when, last week, I found a cute little table in the trash room on my floor. At the end of the school year, a lot gets thrown away.</p><p>The table was in great condition, amid stacks of linens and unopened boxes of date-nut energy bites. Made from clear acrylic, its edges were tinged a neon lemon-lime color that changed with the light—sometimes appearing to be part of the acrylic itself, other times a reflection dancing along its curves.&nbsp;</p><p>I took it home. When I looked it up online, I discovered it costs $900. (Shipping cost: $199.)</p><p>That was retrieved from the trash room at the end of my hall, where you put things down the chute. The real gold mine is the ground-floor room that the chute empties into, accessible by one of the elevators.</p><p>This is where, around graduation each year, you can find dozens of vacuums, Keurigs, stainless steel trash cans in every size and shape imaginable, mattresses, mirrors, and enough luxury goods to make a reseller weep with joy. The first time I went down there, last week, I noticed something neon in a tote bag and pulled out $395 Balenciaga slides. Nearby were $980 Valentino sneakers—worn, but definitely wearable. More than $1,000 of Lululemon workout clothing tumbled from a bag onto a couch.</p><p>You don’t really have to do any digging—most of the stuff I’ve gotten was sitting on top of discarded furniture. But you do have to rush. After I took the Lululemon haul upstairs, I returned to find city waste workers loading things into a garbage truck, off to a landfill. The volume of discarded clothing seems consistent with generational trends: textile waste in the United States went up by more than 50 percent between 2000 and 2018.&nbsp;</p><div><figure><img decoding="async" width="768" height="1024" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-768x1024.jpeg 768w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-225x300.jpeg 225w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1152x1536.jpeg 1152w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1536x2048.jpeg 1536w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-900x1200.jpeg?crop=1 900w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-600x800.jpeg?crop=1 600w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-450x600.jpeg?crop=1 450w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-300x400.jpeg?crop=1 300w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-150x200.jpeg?crop=1 150w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-1200x1600.jpeg 1200w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-2000x2667.jpeg 2000w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-780x1040.jpeg 780w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-400x533.jpeg 400w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-706x941.jpeg 706w, https://indyweek.com/wp-content/uploads/2025/05/IMG_8655-1-scaled.jpeg 1920w" sizes="(max-width: 768px) 100vw, 768px"><figcaption>The trash room. Photo by Lena Geller. </figcaption></figure></div><p>Not every treasure is a flashy brand-name item. I also recovered pink satin pajamas that I remember seeing on someone attending a pajama party on my floor, and a ruffled olive-green <em>Top Gun</em> romper from a Halloween event. (Sadly, there was nothing from the risqué Dr. Seuss party. A few months ago, a fire alarm went off, and it became apparent just how much of the building is occupied by Duke students, as nearly everyone except me, my roommate, and a family with two young kids was drunk and dressed in <em>Cat in the Hat</em> costumes.)</p><p>It feels wrong for this much stuff to have been thrown out in the first place, but it also feels mildly wrong to take it. So it was nice to get intermittent reassurance from my building’s maintenance man, Eric. </p><p>The first time, as I was scurrying back to my room, carrying an upholstered kitchen chair that my cats now spend all their time in, I passed Eric in the hallway. He asked me how I was.</p><p>“Just doing some scavenging,” I said. I must have looked guilty, because he said, “That’s OK.”</p><p>A few days later, I was again downstairs in the big trash room when Eric walked in. I moved to leave, feeling awkward about being caught again. “You’re welcome here anytime,” he assured.&nbsp;</p><p>The sheer volume of valuable, usable things being discarded boggles the brain, particularly when it comes to items like clothing with the tags still on and unopened, unexpired food items.&nbsp;</p><p>In trying to make sense of things, I made spreadsheets.</p><p>The first tracks the prices and brands of the items that I kept, donated, or sold. The total value came to around $6,000, not including several items I couldn’t find prices for.</p><div><figure><img decoding="async" width="1024" height="982" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59%E2%80%AFAM-1024x982.png" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1024x982.png 1024w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-300x288.png 300w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-768x737.png 768w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-1200x1151.png 1200w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-780x748.png 780w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-400x384.png 400w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM-706x677.png 706w, https://indyweek.com/wp-content/uploads/2025/05/Screenshot-2025-05-25-at-11.23.59 https://indyweek.com/culture/duke-students-dumpster-diving/AM.png 1370w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>A screenshot of one section of the author’s itemized spreadsheet</figcaption></figure></div><p>The second spreadsheet compares Duke’s donation collection data with that at other universities, in an effort to understand whether this college-town phenomenon is universal. I gathered publicly available data from university websites and press releases, supplemented by direct inquiries.</p><p>Duke told me their “Devils Care Donations” initiative collected 32,000 pounds this year through partnerships with TROSA and Goodwill. Ali Harrison, senior associate dean for residence life, says that the university places donation bins in every residence hall on campus, plus off-campus Duke housing like Blue Light and Swift Apartments. Harrison also notes that “Duke students who live off campus in non-Duke housing can schedule a TROSA pickup for large or bulky items and large donations.”</p><p>I emailed six universities, asking about their donation programs and collection data. Most didn’t respond or declined. One directed me to a public web page. Rice University, whose “Give a Hoot! Donate Your Loot!” campaign recently won a statewide award in Texas, sent a detailed response. They reported that they collected around 11,000 pounds of “durable goods” from students this year. (Rice has around 9,000 total students, with roughly half undergrads and half graduate students.)</p><p>Rice’s approach is to implement collections every semester, not just during spring move-out. “By maintaining a consistent presence throughout the academic year,” a spokesperson wrote, “the campaign has become a familiar part of the student experience,” helping students plan ahead to donate rather than discard.</p><p>Looking at the data, Duke’s per-undergraduate donation rate (about 4.9 pounds) is comparable to that at other wealthy private universities like Princeton (7.6 pounds) and Georgetown (6.1 pounds). Duke actually outperforms some schools with similar student demographics like the University of Chicago (0.8 pounds) and Northwestern (0.9 pounds). Most large public universities hover around one pound per student.</p><figure><img decoding="async" width="1024" height="683" onerror="if (typeof newspackHandleImageError === 'function') newspackHandleImageError(this);" src="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg" alt="" srcset="https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1024x683.jpg 1024w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-300x200.jpg 300w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-768x512.jpg 768w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1536x1024.jpg 1536w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2048x1365.jpg 2048w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-1200x800.jpg 1200w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-2000x1333.jpg 2000w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-780x520.jpg 780w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-400x267.jpg 400w, https://indyweek.com/wp-content/uploads/2025/05/20250520_AE_INDY_LenaDukehaul_selects_006-706x471.jpg 706w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>INDY </em>staff writer Lena Geller poses for a picture while wearing Valentino Garavani tennis shoes on Tuesday, May 20, 2025, in Durham. Photo by Angelica Edwards. </figcaption></figure><p><strong>T</strong>he emotional reality of my salvaging week was harder to organize into neat columns. For one, I started feeling like everything I own is shitty. When you’re pulling something out of the trash, it doesn’t feel like it’s going to be a luxury item, so at first, I didn’t think much of a comforter I salvaged and offered it to my boyfriend, who’s always looking for blankets for his dog to lie on. After looking up the cost ($222) and thread count (600), I went back on that offer and replaced my existing comforter with the salvaged one. (The next day, my boyfriend found his own down comforter in the trash.)</p><p>Most items I salvaged were like new, but some needed attention. It felt good to wash, clean, and mend things—removing stains from a blouse, fixing belt loops on black slacks. But then futility would set in. I tried to get the stains out of a pair of muddy Nike high-tops with floral embroidery, using a Solo cup I salvaged as a mixing receptacle to stir together baking soda and hydrogen peroxide into a thick paste, but even after slathering it onto the shoes, the stains persist.&nbsp;</p><p>I also spent some time scrubbing a toaster oven, only to go back to the trash room a few days later and find one that’s cleaner and fancier. Retail value: $400.</p><p>In what would become my final scavenging trip of the year, I tried carrying too many things at once—a handheld vacuum, an air filter, some velvet hangers—and dropped the toaster oven, which splashed water all over me from its steam reservoir.&nbsp;</p><p>Sometimes it’s a spill that does it. I stood there, damp, surrounded by other people’s discards, feeling ridiculous. My apartment was already filled with rescued items. I went home, found that the air filter didn’t fit my unit, and cried.</p><p>The next night, my cat jumped down from the salvaged chair he loves, used his litter box, and then kicked litter everywhere—as per usual. Managing litter has been an ongoing struggle. Various vacuums have proved too weak or too bulky to reach the corners behind the box, so I usually just sweep with a handheld broom and dustpan.</p><p>But as I bent over with my dustpan that night, I remembered the handheld vacuum I’d salvaged just before dropping the toaster oven. I’d found it with its charging cord sitting right next to it, still coiled neatly with a twist tie.</p><p>I grabbed it from my pile of findings and turned it on. It was the most powerful little vacuum I’ve ever seen, its pointed nose perfect for crevices.</p><p><em>Reach Staff Writer Lena Geller at&nbsp;<a href="mailto:lgeller@indyweek.com"><em>lgeller@indyweek.com.</em></a>&nbsp;Comment on this story at&nbsp;<a href="mailto:backtalk@indyweek.com"><em>backtalk@indyweek.com</em></a>.</em><br></p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
</article><!-- #post-${ID} -->
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Relace (YC W23) – Models for fast and reliable codegen (103 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44108206</link>
            <guid>44108206</guid>
            <pubDate>Tue, 27 May 2025 15:59:20 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44108206">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44108206">
      <td><span></span></td>      <td><center><a id="up_44108206" href="https://news.ycombinator.com/vote?id=44108206&amp;how=up&amp;goto=item%3Fid%3D44108206"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44108206">Launch HN: Relace (YC W23) – Models for fast and reliable codegen</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44108206">101 points</span> by <a href="https://news.ycombinator.com/user?id=eborgnia">eborgnia</a> <span title="2025-05-27T15:59:20 1748361560"><a href="https://news.ycombinator.com/item?id=44108206">20 hours ago</a></span> <span id="unv_44108206"></span> | <a href="https://news.ycombinator.com/hide?id=44108206&amp;goto=item%3Fid%3D44108206">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Relace%20%28YC%20W23%29%20%E2%80%93%20Models%20for%20fast%20and%20reliable%20codegen&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44108206&amp;auth=222663f39655b321e1eb94a0ff64f886cb3b9237">favorite</a> | <a href="https://news.ycombinator.com/item?id=44108206">47&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Hey HN community! We're Preston and Eitan, and we're building Relace (<a href="https://relace.ai/">https://relace.ai</a>). We're trying to make building code agents easy and cheap.</p><p>Here’s an example of our apply model vs. whole file edits:
<a href="https://youtu.be/J0-oYyozUZw" rel="nofollow">https://youtu.be/J0-oYyozUZw</a></p><p>Building reliable code agents is hard. Beyond simple prototypes, any app with code generation in production quickly runs into two problems --  how do you reliably apply diffs, and how do you manage codebase context?</p><p>We're focused on solving these two problems at order-of-magnitude lower price and latency.</p><p>Our first model that we released, in February, is the Fast Apply model -- it merges code snippets with files at 4300 tok/s. It is more reliable (in terms of merge errors) than Sonnet, Qwen, Llama, or any other model at this task. Each file takes ~900ms and gives an instantaneous user experience, as well as saving ~40% on Claude 4 output tokens.</p><p>Our second model focuses on retrieval. For both vibe-coded and enterprise codebases, retrieving only the files relevant to a user request saves both on SoTA input token cost and reduces the number of times code agents need to view files. Our reranker (evals below) can scan a million-line codebase in ~1-2s, and our embedding model outperforms any other embedding model for retrieval as evaluated on a corpus of Typescript/React repositories.</p><p>There are many different ways to build coding agents, but being able to edit code reliably and retrieve the most relevant parts of the codebase is going to be a foundational issue. We're excited to be building ways to make it more accessible to millions of users who don't want to spend $$$ on Claude.</p><p>These models are used in production, millions of times per week. If you've used Lovable, Create.xyz, Magic Patterns, Codebuff, Tempo Labs then you've used us!</p><p>Here's a link to try it out: <a href="https://app.relace.ai/">https://app.relace.ai</a>, and here are our docs: <a href="https://docs.relace.ai/">https://docs.relace.ai</a>.</p><p>We've opened up free access for prototyping on our website to everyone, and the limits should be enough for personal coding use and building small projects (correct us if it’s not). We integrate directly with Open-Source IDE's like Continue.dev. Please try us out, we'd love to hear your feedback!</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Square Theory (635 pts)]]></title>
            <link>https://aaronson.org/blog/square-theory</link>
            <guid>44107942</guid>
            <pubDate>Tue, 27 May 2025 15:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aaronson.org/blog/square-theory">https://aaronson.org/blog/square-theory</a>, See on <a href="https://news.ycombinator.com/item?id=44107942">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>The story starts in <a href="https://discord.com/invite/GPyU97XBGX">Crosscord</a>, the crossword Discord server. Over 5,000 users strong, the server has emerged as a central hub for the online crossword community, a buzzing, sometimes overwhelming, sometimes delightful town square where total noobs, veteran constructors, and champion solvers alike come together to talk about words that cross each other.</p>

<h2 id="square-roots">Square roots</h2>

<p>We direct our attention toward the #etuiposting channel, Crosscord’s designated space for shitposting (so named because ETUI, a sewing case, is a prototypically shitty piece of crosswordese). There, one afternoon in January 2022, crossword constructor and <a href="https://crosswordnexus.com/">Crossword Nexus</a> warden Alex Boisvert posted what seemed at the time to be an innocuous, mildly interesting observation:</p>

<p><img src="https://aaronson.org/assets/images/square-boisvert.png" alt="Alex Boisvert: JET BLACK and JETBLUE have very different meanings, even though they look superficially similar.  Same thing with CATNAP and DOGNAP.  Any other examples of this?"></p>

<p>Suffice to say, the Crosscord hivemind had other examples of this. <a href="http://blog.bewilderinglypuzzles.com/">Will Nediger</a> replied a few minutes later with the clever MULTITOOL and MULTIPLIERS (words with completely unrelated meanings, despite the fact that PLIERS are a TOOL). Several messages later, Alex chimed back in with the elegant PUB QUIZ and BAR EXAM, a pairing that had been used in some form in crosswords by constructors <a href="http://arctanxwords.blogspot.com/2018/04/puzzle-53-i-thought-this-was-speed.html">Christopher Adams</a> (2018) and <a href="https://www.nytimes.com/crosswords/game/daily/2021/01/29">Robyn Weintraub</a> (2021).</p>

<p>Something about this concept—two sets of synonyms (PUB and BAR, QUIZ and EXAM), which when paired together, form phrases that themselves are not synonyms (PUB QUIZ and BAR EXAM)—captured the minds of Crosscord. Suddenly, the floodgates were open.</p>

<p><img src="https://aaronson.org/assets/images/square-discord-posts.png" alt="Will Nediger: UBEREATS / SUPERFOOD; Assorted-Interests: THROW SHADE / PITCH BLACK; Tyler Hinman, Aged Prodigy: With this topic resurrected, it seems nobody posted what I think is the canonical one: BOOTY CALL and BUTT DIAL; jenna lafleur: ROMAN MARS / CLASSICAL RUINS; gppasco: GRAND CANYON / K-HOLE; robinyu: DAD BOD and FATHER FIGURE; kareila: PERMANENT PRESS / FOREVER STAMP; heywhatsupitsbob: FRIENDLY FAVOR / PLATONIC SOLID"></p>

<p>Intermittently over the next <em>year</em>, #etuiposting would be flooded with these pairs of pairs. They became too much even for the shitposting channel, and were ultimately confined to a thread called #double-doubles (a name <a href="https://heywhatsupitsbob.com/">Bob Weisz</a> and I both proposed simultaneously). Today, more than three years after Alex’s original prompt, the thread still remains active, a wordplay oasis of over 3,000 posts.</p>

<p>There’s something going on here. Something more than a shitpost or an ephemeral trend. Double doubles have the proverbial juice, and the juice lies in their structure. Each pair of pairs can be modeled as a square, where the corners are words and the sides are relations between those words:</p>

<p><img src="https://aaronson.org/assets/images/square-booty-call.jpeg" alt="Square showing BOOTY - phrase - CALL connected via synonyms to BUTT - phrase - DIAL"></p>

<p>It’s this square structure that makes each double double feel tight, feel satisfying, feel like a real “find”. This is the essence of what I’ve started calling <strong>square theory</strong>, and it applies to much more than just posts in a Discord server.</p>

<p>Just like it’s satisfying when an essay or a news story comes full circle, or mindblowing when you find an unexpected cycle in your network of friends, it’s inherently compelling when things wrap around and complete the square. Let’s break it down.</p>

<h2 id="the-theory-of-everything">The theory of everything</h2>

<p>Crosscord wasn’t the first to catch onto this kind of formation: Ricki Heicklen has maintained a <a href="https://rickiheicklen.com/unparalleled-misalignments.html">huge list</a> of double doubles (which she calls “Unparalleled Misalignments”—itself a sort of double double) since 2018, and the success of her recent <a href="https://x.com/tradegal_/status/1920189768261828748">Twitter thread</a> about them is another testament to their widespread appeal. Pairs of the same form pop up on a regular basis in the form of crossword clues and Twitter jokes:</p>

<p><img src="https://aaronson.org/assets/images/square-top-gun.png" alt="Crossword clue [Top gun?] for TSHIRTCANNON, with a square showing TOP - phrase - GUN connected via synonyms to TSHIRT - phrase CANNON">
    <img src="https://aaronson.org/assets/images/square-dad-bod.png" alt="Tweet by Steven W Skinner that says 'Why is it called a dad-bod and not a father-figure', with a square showing DAD - phrase - BOD connected via synonyms to FATHER - phrase - FIGURE">
</p>

<p>However, there’s nothing about the square structure that dictates the edges must represent phrases and synonyms. Each edge of the square can be any relation that connects its vertices (but generally, the stronger the relations, the stronger the square). The vertices don’t even necessarily have to be words—they can be any entity or concept.</p>

<p>It evokes the mathematical field of <a href="https://en.wikipedia.org/wiki/Category_theory">category theory</a>, which very abstractly studies mathematical objects and the relations between them. It’s the topology of the square that makes it satisfying, regardless of what the edges and vertices represent.</p>

<p>Members of the #double-doubles thread have already noticed this, consciously or not, with many of the posts interpreting the original prompt more liberally and swapping out the “synonym” relation for something else:</p>

<p><img src="https://aaronson.org/assets/images/square-left-on-read.png" alt="Crosscord post from Joah: LEFT ON READ vs. RIGHT ON RED. Same number of letters too. Maybe I'll make a mini out of it; Square showing LEFT on READ connected via antonym and homophone to RIGHT on RED">
    <img src="https://aaronson.org/assets/images/square-arizona-wildcat.png" alt="Crosscord post from Quiara, Newsletter Economist: ARIZONA WILDCAT / ARIGATO; Square showing ARIZONA phrase WILDCAT connected via abbr. and translation to ARI word GATO">
</p>

<p>Sometimes it feels like the #double-doubles thread has devolved into just #question-mark-clues (crossword clues that are trying to trick you, requiring you to reinterpret them beyond their words’ most likely meaning, or “surface sense”). But that’s no coincidence—abstractly, every question mark clue takes the form of a square.</p>

<p>When brainstorming for question mark clues, crossword constructors experience this on a regular basis. You start with the answer at hand, playing word association with it in search of a combination of words that usually means one thing (the surface sense) but can be interpreted differently (the intended interpretation) to point to the answer, thus completing the square:</p>

<p><img src="https://aaronson.org/assets/images/square-question-mark-clue.png" alt="Square showing word(s) connected to word(s) by surface sense, which are connected by homonyms to word(s) connected to word(s) by intended interpretation, which leads to the answer"></p>

<p>Take <a href="https://www.nytimes.com/2001/04/08/magazine/endpaper-how-to-solve-the-new-york-times-crossword-puzzle.html">Will Shortz’s all-time favorite clue</a> for instance, from a 1995 Martin Ashwood-Smith puzzle: [It turns into a different story] (which deviously didn’t even include the question mark). On the surface, “turns into a different story” typically means something like “develops into another situation.” But the intended interpretation takes the clue’s words to mean “rotates into another floor,” leading to the correct answer of SPIRAL STAIRCASE:</p>

<p><img src="https://aaronson.org/assets/images/square-spiral-staircase.png" alt="Square showing turns (develops) connected to story (situation) by &quot;develops into another situation&quot;, which are connected by homonyms to turns (rotates) connected to story (floor) by &quot;rotates into another floor&quot;, which leads to the answer SPIRAL STAIRCASE"></p>

<p>You might be familiar with this same sort of brainstorming if you’ve ever tried to come up with a clever title for a research paper, or an apt name for a company. There are plenty of names that might make you go “I guess that could work,” but it’s the square-completing ones that make you go “that’s the one,” or “that’s so good!”</p>

<p>One of my favorite examples of this is <a href="https://www.underconsideration.com/brandnew/">Brand New</a>, the blog that catalogues the latest in corporate rebrands. Leave it to a branding blog to have a name this immaculate:</p>

<p><img src="https://aaronson.org/assets/images/square-brand-new.png" alt="Square showing BRAND phrase NEW connected via homonym and synonym to what the blog chronicles, updated brands"></p>

<p>Even a seemingly straightforward brand name like <a href="https://www.grubhub.com/">Grubhub</a> can exemplify the power of square theory. Presumably, Grubhub’s branding team started with a concept (a centralized app for food deliveries) and came up with a name that completes the square. But remove any edge of the square (besides the edge that dictates the app’s purpose), and you’re left with a name that only <em>kinda</em> works:</p>

<p><img src="https://aaronson.org/assets/images/square-grubhub.png" alt="Complete square showing GRUB rhyme HUB connected via synonyms to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-grubnexus.png" alt="Incomplete square showing GRUB and NEXUS connected via synonyms to what the app provides, a central place for food">
</p>
<p><img src="https://aaronson.org/assets/images/square-grubcub.png" alt="Incomplete square showing GRUB rhyme CUB connected via only one synonym to what the app provides, a central place for food">
    <img src="https://aaronson.org/assets/images/square-tubhub.png" alt="Incomplete square showing TUB rhyme HUB connected via only one synonym to what the app provides, a central place for food">
</p>

<p>Aside from crossword clues and brand names, squares appear in the wild all the time in the form of jokes. There’s a vast universe of pun-based jokes (often in the form of dad jokes, or Twitter jokes, or <a href="https://www.timeout.com/newyork/clubs/punderdome">Punderdome</a> puns) that can be modeled as a square, where one side of the square is the contrived setup (“what do you call an X with a Y?”) that connects in at least two ways to the punchline (“an algebra problem!”) on the opposite side.</p>

<p>The strength of the joke rests in the strength of the setup, the punchline, and the connections between them—and if every side of the square is strong, you might have created something funny:</p>

<p><img src="https://aaronson.org/assets/images/square-joke-abstract.png" alt="Square showing a setup (contrived) of at least two words, which are connected by synonyms or homonyms, usually, to at least two other words, the punchline (a real word or phrase, or a play on one)">
    <img src="https://aaronson.org/assets/images/square-impasta.png" alt="Square showing FAKE and NOODLE connected by the setup 'What do you call a fake noodle', which connects via synonyms to IMPOSTOR and PASTA, forming the portmanteau punchline 'An impasta!'">
</p>

<h2 id="getting-into-shape">Getting into shape</h2>

<p>You might be thinking: what’s so special about a square? What about triangle theory, or pentagon theory? (Or rectangle theory? Or rhombus theory? Okay, side lengths and angles <a href="https://en.wikipedia.org/wiki/Topology">don’t matter here</a>.)</p>

<p>Well, it’s true that there’s something compelling about any loop-closing property, regardless of side count—a story that comes full circle is still satisfying no matter how many points it hits in between, and it’s still neat to discover a triangle of people who coincidentally know each other.</p>

<p>But here’s what I think makes squares special: a square is the simplest polygon that has non-adjacent sides. In a triangle, each side is adjacent to the other two sides. But in a square, opposite sides have no points in common, which makes any connection between them feel surprising, like a coincidence. In pentagons and beyond, this still holds, but the extra sides add complexity that make them feel slightly less elegant. Nevertheless, other shapes can be interesting too, but I see them as the exception, not the rule.</p>

<p>Remember Alex Boisvert’s original JET BLACK / JETBLUE example? Seems like it could be modeled as a triangle, right? Well, it turns out the “jet” in JET BLACK refers to the gemstone <a href="https://en.wikipedia.org/wiki/Jet_(gemstone)">jet</a>, which is <a href="https://www.etymonline.com/word/jet">etymologically unrelated</a> to JETBLUE’s airplane jet, so it’s actually more properly modeled as a square:</p>

<p><img src="https://aaronson.org/assets/images/square-jet-triangle.png" alt="Triangle showing JET phrase BLACK colors BLUE airline JET">
    <img src="https://aaronson.org/assets/images/square-jet-square.png" alt="Square showing JET homonym JET phrase BLACK colors BLUE airline JET">
</p>

<h2 id="times-square"><em>Times</em> square</h2>

<p>Now that I’ve established that square theory applies to more than just crosswords, it’s time to talk about crosswords again.</p>

<p>It’s typical for American-style crosswords (à la <em>New York Times</em>) to have a theme, which will generally consist of the 4–6 longest Across entries in the grid, often including a “revealer” that ties the theme together. Nowadays, it’s common gospel among crossword constructors that themes should have some sort of wordplay-based connection—that is, a theme like “famous basketball players” or “brands of cereal” is unlikely to elicit a real “aha” moment from solvers, and thus unlikely to be accepted at major crossword outlets.</p>

<p>So what makes for a <em>good</em> crossword theme? Consistency is definitely key, and a notion of “tightness” is important too (the set of possible theme entries shouldn’t be too much bigger than the theme set that appears in the puzzle). But time and time again, I’ve noticed that what makes a theme really pop is—you guessed it—when it completes the square.</p>

<p>Take, for example, the <a href="https://www.xwordinfo.com/Crossword?date=2/17/2025">Monday, February 17, 2025 <em>New York Times</em> crossword</a> by Kate Hawkins and Erica Hsiung Wojcik, which features a great execution of a typical Monday theme type. In this puzzle, the seemingly unrelated theme entries SCRAPBOOK, POPEYES, UNDER PRESSURE, and GIDDY UP are united by the fact that they each end in an affirmative (OK, YES, SURE, YUP).</p>

<p>In a vacuum, this fact wouldn’t be that interesting, but Kate and Erica give the theme a <em>raison d’etre</em> with the revealer YEAH RIGHT, clued as [“Uh-huh, I bet” … or a literal description of what 17-, 24-, 36- and 50-Across all have]—that is, each themer has a synonym for “YEAH” on its “RIGHT” side. The key here is that YEAH RIGHT itself is an idiomatic phrase (meaning “Uh-huh, I bet”), and not just an arbitrary description of the theme mechanic, so it completes the square:</p>

<p><img src="https://aaronson.org/assets/images/square-yeah.png" alt="Square showing what the entries have, an affirmative ending, connected via synonyms to the phrase YEAH RIGHT"></p>

<p>But it doesn’t stop there. Consider the <a href="https://www.xwordinfo.com/Crossword?date=2/18/2019">Monday, February 18, 2019 <em>New York Times</em> crossword</a> by Leslie Young and Andrea Carla Michaels. The theme entries here are NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL (you know, like a vegetarian meatball), and the revealer, clued as [Graduation garb … or what the compound answers to 17-, 28- and 44-Across represent?], is CAP AND GOWN. That is, the first part of each themer can precede CAP (e.g. MUSHROOM CAP), and the second part can precede GOWN (e.g. BALL GOWN). This maps pretty squarely onto not one, but three squares, one for each theme entry:</p>

<p><img src="https://aaronson.org/assets/images/square-night-night.png" alt="Three squares, for NIGHT NIGHT, WHITE WEDDING, and MUSHROOM BALL, each showing the phrase connected by two phrases to CAP and GOWN"></p>

<p>And just for fun, we can conjoin the three squares by their CAP AND GOWN edges to form a unified graph that represents the entire theme’s topology:</p>

<p><img src="https://aaronson.org/assets/images/square-cap-and-gown.png" alt="Unified CAP AND GOWN square graph"></p>

<p>The final crossword we’ll look at, and maybe my favorite crossword of all time, is Alina Abidi’s <a href="https://www.xwordinfo.com/Crossword?date=8/18/2021">Wednesday, August 18, 2021 <em>New York Times</em> crossword</a>, with a theme that feels almost impossibly tight.</p>

<p>The puzzle has essentially two theme entries, PIN THE TAIL ON THE DONKEY and WHITE ELEPHANT, with the apt revealer PARTY ANIMAL [Frequent reveler, or a hint to 16-/26- and 36-Across]. That alone is clever, since both themers are party games with animals in their names. But then Alina hits you with the <em>second</em> revealer of THOMAS NAST [Cartoonist suggested by this puzzle’s theme], pointing to the fact that not only are the DONKEY and ELEPHANT animals in party games, but they are also the animals that symbolize the Democratic and Republican <em>parties</em>, as popularized by <a href="https://en.wikipedia.org/wiki/Thomas_Nast">Thomas Nast</a>’s political cartoons.</p>

<p>This is the kind of theme that really sticks with you. Or at least it stuck with me, and I tried for years to understand why it felt so amazing. And then I realized square theory offered an explanation. Squares, as we know, feel tight, satisfying, and clever. But Alina’s theme takes that one step further, creating for each theme entry a square with an <em>extra diagonal</em> through it, reflecting the connection between each animal and a political PARTY:</p>

<p><img src="https://aaronson.org/assets/images/square-democrat.png" alt="Square showing PIN THE TAIL ON THE DONKEY containing DONKEY connected to PARTY ANIMAL by setting and example, with an additional Democrats diagonal connecting PARTY and DONKEY">
    <img src="https://aaronson.org/assets/images/square-republican.png" alt="Square showing WHITE ELEPHANT containing ELEPHANT connected to PARTY ANIMAL by setting and example, with an additional Republican diagonal connecting PARTY and ELEPHANT">
</p>

<p>And again, we can combine these two super-squares into one unified theme graph:</p>

<p><img src="https://aaronson.org/assets/images/square-party-animals.png" alt="Unified PARTY ANIMALS square graph"></p>

<p>Granted, there’s more to a crossword than the structure of its theme, and it can be reductive to distill it into a graph like this. Still, for many puzzles, square theory can serve as an illuminating proxy for the intricacy and tightness of a theme. But that’s not all it can do.</p>

<h2 id="letter-box">Letter box</h2>

<p>Let’s talk about Scrabble, one of the <a href="https://www.nytimes.com/2022/01/25/books/review/seven-games-oliver-roeder.html">seven most important games</a> out there. If you’ve ever played Scrabble (or similar games like Bananagrams), you’d know that every word you play has to intersect another word that’s already on the board.</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-normal.png" alt="Scrabble play that is boring and the word only intersects one other word"></p>

<p>But occasionally, you’ll think up a play that validly intersects not one, but two words on the board, forming a rectangle of words. Plays like this have a certain panache. They’re satisfying, they make you think, “ooh, nice.” And of course, they can be modeled with square theory:</p>

<p><img src="https://aaronson.org/assets/images/square-scrabble-cool.png" alt="Scrabble play where the word MICE intersects two already-on-the-board words CHASM and SINCE">
    <img src="https://aaronson.org/assets/images/square-scrabble.png" alt="Square showing the Scrabble board words CHASM linking A and M, AVID linking A and I, SINCE linking I and C, and MICE linking M and C">
</p>

<p>You might be thinking that the edge relation here (a word that contains both letters) feels a little flimsy, since not every letter in the word is used. But what if every letter in the word <em>was</em> used? What if we could have a dense network of interlocking squares, where every letter was part of exactly two words? Well, we can, and it’s called an American-style crossword.</p>

<p>In American-style crosswords, every letter is mandatorily “checked” (part of an Across and a Down word), which means <em>every</em> letter is a vertex of a square:</p>

<p><img src="https://aaronson.org/assets/images/square-crossword-grid.png" alt="3x3 crossword grid, and a grid of interconnected squares whose vertices are the letters in the crossword and whose edges are the words that connect those letters"></p>

<p>If you’ve ever tried to construct a crossword, you’ll find that the framing of a crossword grid under square theory <em>feels</em> right. When you’re nearing the end of the grid-filling process, finding valid crossings of words to fill that final corner of a grid, there’s a satisfying “clicking” feeling—a sense of magic—when it all fits together, analogous to the wrapping-around feeling of completing the square.</p>

<p>Taking a step back, that means the clues, the themes, and the very grids of crosswords all share the same abstract fundamental structure, the square:</p>

<p><img src="https://aaronson.org/assets/images/square-crosswords-everything.png" alt="Squares from earlier in the post representing clues, themes, and grids of crosswords"></p>

<p>If you accept the premise that squares are satisfying, square theory offers a unified theory for why crosswords are satisfying too. And if squares are fundamentally compelling, the crossword, in its recursively square structure, starts to look like an equally fundamental art form. Like if you started an English-speaking civilization from scratch, someone, somewhere would inevitably reinvent the crossword. And then someone would start a crossword Discord server, and maybe they’d call it Crosscord.</p>

<p><img src="https://aaronson.org/assets/images/square-crosscord.png" alt="Square showing what the server is, a Discord server for crossword puzzles, connected by keywords to CROSSWORD / DISCORD which are portmanteaued into the server's name, CROSSCORD"></p>

<h2 id="its-hip-to-be-square">It’s hip to be square</h2>

<p>If you’ve read this far, I promise you’ll start to notice squares popping up all over the place in your daily life. I can attest, because I’ve been honing the concept for this post for about two years now, and I often find myself thinking “that’s a square!” whenever I come across a tight joke or title or crossword theme.</p>

<p>If you’re a creative person, square theory is a useful framework to keep in mind. If you’re coming up with a title for a paper or a brand name, try to see if you can think of one that completes the square. If you’re writing puns for a popsicle stick or a Laffy Taffy wrapper, you can use squares to model your setups and punchlines. If you’re constructing a crossword, consider whether your theme or your question mark clues can form squares.</p>

<p>And if you’re writing a story or a news article or a blog post, there’s fundamental value in making it come full circle, or perhaps full square.</p>

        
    </div></div>]]></description>
        </item>
    </channel>
</rss>