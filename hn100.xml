<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Sep 2024 04:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Chai-1 Defeats AlphaFold 3 (163 pts)]]></title>
            <link>https://www.chaidiscovery.com/blog/introducing-chai-1</link>
            <guid>41506157</guid>
            <pubDate>Tue, 10 Sep 2024 22:13:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.chaidiscovery.com/blog/introducing-chai-1">https://www.chaidiscovery.com/blog/introducing-chai-1</a>, See on <a href="https://news.ycombinator.com/item?id=41506157">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-framer-name="Content" data-framer-component-type="RichTextContainer"><p>We’re excited to release Chai-1, a new multi-modal foundation model for molecular structure prediction that performs at the state-of-the-art across a variety of tasks relevant to drug discovery. Chai-1 enables unified prediction of proteins, small molecules, DNA, RNA, covalent modifications, and more.</p><p>The model is available for free via a <a href="https://lab.chaidiscovery.com/" target="_blank" rel="noopener">web interface</a>, including for commercial applications such as drug discovery. We are also releasing the model weights and inference code as a <a href="https://github.com/chaidiscovery/chai-lab" target="_blank" rel="noopener">software library</a> for non-commercial use.</p><h2>A frontier model for biomolecular interactions</h2><p>We tested Chai-1 across a large number of benchmarks, and found that the model achieves a 77% success rate on the PoseBusters benchmark (vs. 76% by AlphaFold3), as well as an Cα LDDT of 0.849 on the CASP15 protein monomer structure prediction set (vs. 0.801 by ESM3-98B).</p><p><img alt="" data-framer-asset="data:framer/asset-reference,nAGHKUD9XOHtavXDEpA6XH8Wo4.png" data-framer-height="896" data-framer-width="1814" height="448" src="https://framerusercontent.com/images/nAGHKUD9XOHtavXDEpA6XH8Wo4.png" width="907"></p><p>Unlike many existing structure prediction tools which require multiple sequence alignments (MSAs), Chai-1 can also be run in single sequence mode without MSAs while preserving most of its performance. The model can fold multimers more accurately (69.8%) than the MSA-based AlphaFold-Multimer model (67.7%), as measured by the DockQ acceptable prediction rate. Chai-1 is the first model that’s able to predict multimer structures using single-sequences alone (without MSA search) at AlphaFold-Multimer level quality.</p><p>For more information, and a comprehensive analysis of the model, read our <a href="https://chaiassets.com/chai-1/paper/technical_report_v1.pdf" target="_blank" rel="noopener">technical report</a>.</p><h2><strong>A natively multi-modal foundation model</strong></h2><p>In addition to its frontier modeling capabilities directly from sequences, Chai-1 can be prompted with new data, e.g. restraints derived from the lab, which boost performance by double-digit percentage points.&nbsp;We explore a number of these capabilities in our technical report, such as epitope conditioning – using even a handful of contacts or pocket residues (potentially derived from lab experiments) doubles antibody-antigen structure prediction accuracy, making antibody engineering more feasible using AI.</p><p><img alt="" data-framer-asset="data:framer/asset-reference,icSSrTXU76WdTzKi30rSK70UY.png" data-framer-height="1962" data-framer-width="4716" height="981" src="https://framerusercontent.com/images/icSSrTXU76WdTzKi30rSK70UY.png" width="2358"></p><h2><strong>Releasing the model for all</strong></h2><p>We are releasing Chai-1 via a web interface for free, including for commercial applications such as drug discovery. We are also releasing the code for Chai-1 for non-commercial use as a software library. We believe that when we build in partnership with the research and industrial communities, the entire ecosystem benefits.</p><p>Try Chai-1 for yourself by visiting <a href="http://lab.chaidiscovery.com/" rel="noopener">lab.chaidiscovery.com</a>, or run it from our GitHub repository at <a href="https://github.com/chaidiscovery/chai-lab" target="_blank" rel="noopener">github.com/chaidiscovery/chai-lab</a>.</p><h2><strong>What's next?</strong></h2><p>The team comes from pioneering research and applied AI companies such as OpenAI, Meta FAIR, Stripe, and Google X. Collectively, we have played pivotal roles in the advancement of research in AI for biology. The majority of the team has been Head of AI at leading drug discovery companies, and has collectively helped advance over a dozen drug programs.&nbsp;</p><p>Chai-1 is the result of a few months of intense work, and yet we are only at the starting line. Our broader mission at Chai Discovery is to transform biology from science into engineering. To that end, we'll be building further AI foundation models that predict and reprogram interactions between biochemical molecules, the fundamental building blocks of life. We’ll have more to share on this soon.</p><p>We are grateful for the partnership of <a href="https://www.dimensioncap.com/" rel="noopener">Dimension</a>, <a href="https://www.thrivecap.com/" rel="noopener">Thrive Capital</a>, <a href="https://openai.com/" rel="noopener">OpenAI</a>, <a href="https://www.conviction.com/" rel="noopener">Conviction</a>, <a href="https://neo.com/" rel="noopener">Neo</a>, Lachy Groom, and <a href="https://www.amplifypartners.com/" rel="noopener">Amplify Partners</a>, as well as Anna and Greg Brockman, Blake Byers, Fred Ehrsam, Julia and Kevin Hartz, Will Gaybrick, David Frankel, R. Martin Chavez, and many others.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flipper Zero Gets Major Firmware Update, Can Eavesdrop on Walkie-Talkies (149 pts)]]></title>
            <link>https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update</link>
            <guid>41505670</guid>
            <pubDate>Tue, 10 Sep 2024 21:15:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update">https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update</a>, See on <a href="https://news.ycombinator.com/item?id=41505670">Hacker News</a></p>
Couldn't get https://www.pcmag.com/news/flipper-zero-gets-major-firmware-update: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Lottery Simulator (2023) (123 pts)]]></title>
            <link>https://perthirtysix.com/tool/lottery-simulator</link>
            <guid>41505593</guid>
            <pubDate>Tue, 10 Sep 2024 21:09:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://perthirtysix.com/tool/lottery-simulator">https://perthirtysix.com/tool/lottery-simulator</a>, See on <a href="https://news.ycombinator.com/item?id=41505593">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p>A New Daily Game!</p><p> We built a daily poll game called the <a href="https://perthirtysix.com/communal-plot-daily-poll">Communal Plot</a>! We hope it's a fun way to engage with the community and see how your opinions stack up. Check it out and let us know what you think! </p></div><p> Every so often, a lottery jackpot will get so high that I'll hear about it on the news or from a friend. When this happens, I immediate start wondering about two things: what I would do with hundreds of millions of dollars and what the odds of winning really are. </p><p> While major lotteries publish some of this information, I wanted to build something that would make it easier to play around with the data in a more exploratory way. With that, here is the PerThirtySix Lottery Simulator! </p><p> This tool is broken up into two sections: Setup and Simulation. The Setup section lets explore probabilities for an existing American lottery or for your own lottery with custom rules. The Simulation section lets you pick some numbers and play up to thousands of tickets per second, and visualizes the returns for you. Note that this tool makes some simplifying assumptions, like that there's only one jackpot winner and that taxes are ignored. </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Not Comments (121 pts)]]></title>
            <link>https://buttondown.com/hillelwayne/archive/why-not-comments/</link>
            <guid>41505389</guid>
            <pubDate>Tue, 10 Sep 2024 20:52:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://buttondown.com/hillelwayne/archive/why-not-comments/">https://buttondown.com/hillelwayne/archive/why-not-comments/</a>, See on <a href="https://news.ycombinator.com/item?id=41505389">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
                    <date>
                        
                            September 10, 2024
                        </date>
                
                
                
                    <h2>
                        Why not "why not" comments? Not why "not comments"
                    </h2>
                

                

                
                    
                        <h2>Logic For Programmers v0.3</h2>
<p><a href="https://leanpub.com/logic/" target="_blank">Now available</a>! It's a light release as I learn more about formatting a nice-looking book. You can see some of the differences between v2 and v3 <a href="https://bsky.app/profile/hillelwayne.com/post/3l3egdqnqj62o" target="_blank">here</a>.</p>
<h2>Why Not Comments</h2>
<p>Code is written in a structured machine language, comments are written in an expressive human language. The "human language" bit makes comments more expressive and communicative than code. Code has a limited amount of something <em>like</em> human language contained in identifiers. "Comment the why, not the what" means to push as much information as possible into identifiers. <a href="https://buttondown.com/hillelwayne/archive/3866bd6e-22c3-4098-92ef-4d47ef287ed8" target="_blank">Not all "what" can be embedded like this</a>, but a lot can.</p>
<p>In recent years I see more people arguing that <em>whys</em> do not belong in comments either, that they can be embedded into <code>LongFunctionNames</code> or the names of test cases. Virtually all "self-documenting" codebases add documentation through the addition of identifiers.<sup id="fnref:exception"><a href="#fn:exception">1</a></sup></p>
<p>So what's something in the range of human expression that <em>cannot</em> be represented with more code?</p>
<p>Negative information, drawing attention to what's <em>not</em> there. The "why nots" of the system.</p>
<h3>A Recent Example</h3>
<p>This one comes from <em>Logic for Programmers</em>. For convoluted technical reasons the epub build wasn't translating math notation (<code>\forall</code>) into symbols (<code>∀</code>). I wrote a script to manually go through and replace tokens in math strings with unicode equivalents. The easiest way to do this is to call <code>string = string.replace(old, new)</code> for each one of the 16 math symbols I need to replace (some math strings have multiple symbols).</p>
<p>This is incredibly inefficient and I could instead do all 16 replacements in a single pass. But that would be a more complicated solution. So I did the simple way with a comment:</p>
<div><pre><span></span><code>Does 16 passes over each string
BUT there are only 25 math strings in the book so far and most are &lt;5 characters.
So it's still fast enough.
</code></pre></div>
<p>You can think of this as a "why I'm using slow code", but you can also think of it as "why not fast code". It's calling attention to something that's <em>not there</em>.</p>
<h3>Why the comment</h3>
<p>If the slow code isn't causing any problems, why have a comment at all?</p>

<p>Well first of all the code might be a problem later. If a future version of <em>LfP</em> has hundreds of math strings instead of a couple dozen then this build step will bottleneck the whole build. Good to lay a signpost now so I know exactly what to fix later.</p>
<p>But even if the code is fine forever, the comment still does something important: it shows <em>I'm aware of the tradeoff</em>. Say I come back to my project two years from now, open <code>epub_math_fixer.py</code> and see my terrible slow code. I ask "why did I write something so terrible?" Was it inexperience, time crunch, or just a random mistake?</p>
<p>The negative comment tells me that I <em>knew</em> this was slow code, looked into the alternatives, and decided against optimizing. I don't have to spend a bunch of time reinvestigating only to come to the same conclusion. </p>
<h2>Why this can't be self-documented</h2>
<p>When I was first playing with this idea, someone told me that my negative comment isn't necessary, just name the function <code>RunFewerTimesSlowerAndSimplerAlgorithmAfterConsideringTradeOffs</code>. Aside from the issues of being long, not explaining the tradeoffs, and that I'd have to change it everywhere if I ever optimize the code... This would make the code <em>less</em> self-documenting. It doesn't tell you what the function actually <em>does</em>.</p>
<p>The core problem is that function and variable identifiers can only contain one clause of information. I can't store "what the function does" and "what tradeoffs it makes" in the same identifier. </p>
<p>What about replacing the comment with a test. I guess you could make a test that greps for math blocks in the book and fails if there's more than 80? But that's not testing <code>EpubMathFixer</code> directly. There's nothing in the function itself you can hook into. </p>
<p>That's the fundamental problem with self-documenting negative information. "Self-documentation" rides along with written code, and so describes what the code is doing. Negative information is about what the code is <em>not</em> doing. </p>
<h3>End of newsletter speculation</h3>
<p>I wonder if you can think of "why not" comments as a case of counterfactuals. If so, are "abstractions of human communication" impossible to self-document in general? Can you self-document an analogy? Uncertainty? An ethical claim?</p>

                    
                

                
                    <p><em>If you're reading this on the web, you can subscribe <a href="https://buttondown.com/hillelwayne" target="_blank">here</a>. Updates are once a week. My main website is <a href="https://www.hillelwayne.com/" target="_blank">here</a>.</em></p>
                

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Another police raid in Germany (356 pts)]]></title>
            <link>https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533</link>
            <guid>41505009</guid>
            <pubDate>Tue, 10 Sep 2024 20:12:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533">https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533</a>, See on <a href="https://news.ycombinator.com/item?id=41505009">Hacker News</a></p>
Couldn't get https://forum.torproject.org/t/tor-relays-artikel-5-e-v-another-police-raid-in-germany-general-assembly-on-sep-21st-2024/14533: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tutorial on diffusion models for imaging and vision (120 pts)]]></title>
            <link>https://arxiv.org/abs/2403.18103</link>
            <guid>41504885</guid>
            <pubDate>Tue, 10 Sep 2024 19:59:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2403.18103">https://arxiv.org/abs/2403.18103</a>, See on <a href="https://news.ycombinator.com/item?id=41504885">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      <h2>Computer Science &gt; Machine Learning</h2>
    </p>

    <p><strong>arXiv:2403.18103</strong> (cs)
    </p>

<div id="content-inner">
    <p>
  [Submitted on 26 Mar 2024 (<a href="https://arxiv.org/abs/2403.18103v1">v1</a>), last revised 6 Sep 2024 (this version, v2)]</p>
    
                
    <p><a href="https://arxiv.org/pdf/2403.18103">View PDF</a></p><blockquote>
            <span>Abstract:</span>The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
    </blockquote>

    <!--CONTEXT-->
    
  </div>
    <div>
      <h2>Submission history</h2><p> From: Stanley Chan [<a href="https://arxiv.org/show-email/052533b7/2403.18103">view email</a>]      <br>            <strong><a href="https://arxiv.org/abs/2403.18103v1">[v1]</a></strong>
        Tue, 26 Mar 2024 21:01:41 UTC (3,221 KB)<br>
    <strong>[v2]</strong>
        Fri, 6 Sep 2024 19:58:27 UTC (3,822 KB)<br>
</p></div>
  </div><div id="labstabs"><p>
    <label for="tabone">Bibliographic Tools</label></p><div>
      <h2>Bibliographic and Citation Tools</h2>
      <div>
          <p><label>
              
              <span></span>
              <span>Bibliographic Explorer Toggle</span>
            </label>
          </p>
          
        </div>
        
        
        
    </div>


    <p>
    <label for="tabtwo">Code, Data, Media</label></p><div>
      <h2>Code, Data and Media Associated with this Article</h2>
      

      
      
      
      
      
      
    </div>


      <p>
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label></p><div>
        <h2>Demos</h2>
        
        
        
        
      </div>
      <p>
      <label for="tabfour">Related Papers</label></p><div>
        <h2>Recommenders and Search Tools</h2>
        <div>
            <p><label>
                
                <span></span>
                <span>IArxiv recommender toggle</span>
              </label>
            </p>
            
          </div>
        
        
        
        
        
      </div>

      <p>
      <label for="tabfive">
        About arXivLabs
      </label></p><div>
            <h2>arXivLabs: experimental projects with community collaborators</h2>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Git Bash is my preferred Windows shell (135 pts)]]></title>
            <link>https://www.ii.com/git-bash-is-my-preferred-windows-shell/</link>
            <guid>41504832</guid>
            <pubDate>Tue, 10 Sep 2024 19:54:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ii.com/git-bash-is-my-preferred-windows-shell/">https://www.ii.com/git-bash-is-my-preferred-windows-shell/</a>, See on <a href="https://news.ycombinator.com/item?id=41504832">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><h3 id="_case_insensitivity"><a href="#_case_insensitivity">Case insensitivity</a></h3><p>Normally
<a href="https://www.ii.com/portal/nix-nux/" title="#nix-nux Portal on Infinite Ink">Unix-like</a>
shells are case sensitive, but
Git&nbsp;Bash, in general, is case insensitive.</p></div><div><h3 id="_paths"><a href="#_paths">Paths</a></h3><p>To view your current path, run
the
following command, which means
“print working directory.”</p><p>To go to your Git&nbsp;Bash home directory,
which is also known as <code>~</code>,
run either of the following equivalent commands.</p><p>On most systems, your Git&nbsp;Bash home directory
is…</p><p>with <code>USERNAME</code> replaced with your user name.</p><p>Note that in Git&nbsp;Bash…</p><div><ul><li><p>The path delimiter is slash (<code>/</code>), which is also known as forward slash.</p></li><li><p>The <code>C:</code> drive is mounted as <code>/c/</code>.
To list all mounts, run the <code>mount</code> command.</p></li><li><p>You can display a Windows-type path of the current working directory with
<code>pwd&nbsp;-W</code>.
The format of the path this command displays is sometimes called
“mixed&nbsp;type”
because it starts with a drive letter, such as <code>C:</code>,
and uses
forward&nbsp;slashes
(<code>/</code>)
rather than backslashes
(<code>\</code>)
as path separators.
For example, this sequence of commands…</p></li></ul></div><p>displays…</p><div><pre>/c/Users/USERNAME
C:/Users/USERNAME</pre></div></div><div><h3 id="_the_start_command"><a href="#_the_start_command">The <code>start</code> command</a></h3><p>When you are at a Git&nbsp;Bash prompt, you can launch
a file or directory in your system’s default app
by using
the <code>Start</code> command,
which is equivalent
to
the <code>start</code> command (thanks to case insensitivity).
Below are some examples.</p><p>To open the current directory in Windows File&nbsp;Explorer, run:</p><div><pre>start .
      👆
     Notice this dot (.)</pre></div><p>To open your home directory in Windows File&nbsp;Explorer, run:</p><p>To open your $APPDATA directory in Windows File&nbsp;Explorer, run:</p><p>To open <code>/c</code> in Windows File&nbsp;Explorer, run either of these equivalent commands:</p><p>To open the parent directory of the current directory in Windows File&nbsp;Explorer, run:</p><p>To open an HTML file in your default web browser,
run
something like this:</p></div><div><h3 id="_launching_any_app_on_your_path"><a href="#_launching_any_app_on_your_path">Launching any app on your path</a></h3><p>You can launch
any app that’s on your path
from a Git&nbsp;Bash prompt.
For example, if Visual Studio Code is
installed on your system and is on your path,
you can use the following to open the current directory
in VS Code.</p><div><pre>code .
     👆
    Notice this dot (.)</pre></div></div><div><h3 id="_environment_variables"><a href="#_environment_variables">Environment variables</a></h3><p>To find out
the
environment variables
available
to
Git&nbsp;Bash,
run:</p><p>This includes the PATH environment variable, which lists the directories that are searched for executables.</p></div><div><h3 id="_scripting"><a href="#_scripting">Scripting</a></h3><p>I’d rather write a bash or
sh⁠<sup>[<a href="#_footnotedef_14" title="View footnote.">14</a>]</sup>
<a href="https://wikipedia.org/wiki/Shell_script">shell script</a>
than a
Windows
<a href="https://wikipedia.org/wiki/Batch_file">batch file</a>
to
deal with apps, files, and folders
that live in my Windows file system.
Thanks to
Git&nbsp;Bash,
It’s easy to do this.
(I have learned the hard way that it’s not so easy to do this
in
WSL.⁠<sup>[<a href="#_footnotedef_15" title="View footnote.">15</a>]</sup>)</p><div><h4 id="_where_to_put_scripts"><a href="#_where_to_put_scripts">Where to put scripts</a></h4><p>When you run <code>printenv</code>, you can see that the <code>~/bin/</code> directory
(usually <code>/c/users/USERNAME/bin/</code>)
is
on your PATH. This is a reasonable place to put your Git&nbsp;Bash scripts.
To create this directory, run this command:</p></div><div><h4 id="_example_script"><a href="#_example_script">Example script</a></h4><p>One of my scripts is called
<code>gvim-winpath</code>
and it looks like this:</p><div><pre>#!/bin/sh

## Created: 2024-05-15
## Filename: gvim-winpath
## Usage: gvim-winpath "C:\path\to\filename"

/c/windows/gvim.bat `cygpath "$1"`</pre></div><p>I use this to launch gvim on a file that
is specified using a Windows-style backslash path.
To learn about the <code>cygpath</code> command, which is used in this script, run
one of
the following commands at a Git&nbsp;Bash prompt.</p><div><pre>cygpath --help
cygpath --help |less</pre></div><p>If you pipe the <code><nobr>cygpath&nbsp;--help</nobr></code> output to the <code>less</code> pager, you
need to know
how to
use <code>less</code>:
The essentials are
that
within <code>less</code>, you can
press <kbd>Space</kbd> to page down,
press <kbd>b</kbd> (for back) to page up,
and press
<kbd>q</kbd>
to quit.</p></div><div><h4 id="_no_need_to_chmod"><a href="#_no_need_to_chmod">No need to chmod</a></h4><p>In most Unix-like shells, you need to <code>chmod&nbsp;+x</code> executables
but in Git&nbsp;Bash this is not needed.</p></div></div><div><h3 id="_unicode"><a href="#_unicode">Unicode</a></h3><p>If you have issues with
non-ASCII
Unicode characters, run
the following at a Git&nbsp;Bash prompt
and make sure each
setting
includes <code>UTF-8</code>.</p><p>Also,
you can
try
to solve
Unicode issues
by
running
the following sequence of
<code>chcp.com</code>⁠<sup>[<a href="#_footnotedef_12" title="View footnote.">12</a>]</sup>
commands at a Git&nbsp;Bash prompt.</p><div><pre>chcp.com
chcp.com 65001
chcp.com</pre></div><p>This <strong>ch</strong>anges the <strong>c</strong>ode <strong>p</strong>age to 65001, which supports UTF-8 encoding.</p><div><table><tbody><tr><td><p><span>ℹ</span></p></td><td>Nowadays
UTF-8 is the standard file encoding
of the internet (and of Unicode
in&nbsp;general).</td></tr></tbody></table></div></div><div><h3 id="_case_insensitivity_exceptions"><a href="#_case_insensitivity_exceptions">Case insensitivity exceptions</a></h3><div><h4 id="_help_and_other_command_arguments"><a href="#_help_and_other_command_arguments">--help and other command arguments</a></h4><p>Arguments to nix-⁠nux commands, in general, are case sensitive. For example,
in the following</p><p>The command <code>cygpath</code> is case insensitive,
but
its argument <code>--help</code> is case sensitive.
For example,
this works:</p><p>but this does not work:</p></div><div><h4 id="_exit"><a href="#_exit">exit</a></h4><p>To quit Git&nbsp;Bash, you can either
click the close-window X in the upper right corner of the terminal window
or type the following at the command prompt:</p><p>This <code>exit</code> command
is a
<a href="https://wikipedia.org/wiki/System_call">system&nbsp;call</a>
and
must be all lower
case.⁠<nobr> <span>◊</span></nobr></p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York Times tech workers union votes to authorize a strike (204 pts)]]></title>
            <link>https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote</link>
            <guid>41504026</guid>
            <pubDate>Tue, 10 Sep 2024 18:30:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote">https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote</a>, See on <a href="https://news.ycombinator.com/item?id=41504026">Hacker News</a></p>
Couldn't get https://www.axios.com/2024/09/10/nyt-tech-union-strike-vote: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Visual DB – Web front end for your database (124 pts)]]></title>
            <link>https://visualdb.com/</link>
            <guid>41503251</guid>
            <pubDate>Tue, 10 Sep 2024 17:25:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://visualdb.com/">https://visualdb.com/</a>, See on <a href="https://news.ycombinator.com/item?id=41503251">Hacker News</a></p>
<div id="readability-page-1" class="page">

        


    <div>
      <h2 id="gotdb">Got a database?</h2>
      <h3>Use Visual DB to quickly build data entry forms, sheets, and reports</h3>
      
    </div>

    <a name="features"></a>

    <div>
        <div>
          <h2>Cut development costs</h2>
          <p>
            Developing and maintaining internal applications to enter and update records in a database is expensive.
            Now you can significantly lower costs by using Visual DB instead of developing custom applications.
          </p>
        </div>
        <div>
          <h2>No coding skills? No problem!</h2>
          <p>
            Visual DB is a productivity application, not a developer tool.
            As such, you don't need to understand SQL or any other programming language to use Visual DB.
            You build forms using Visual DB's drag-and-drop interface and AI assistance.
          </p>
        </div>
      </div>

    <div>
        <div>
          <h2>Bring your own database</h2>
          <p>
            Rather than requiring you to use an RDBMS that is built into Visual DB, it allows you to use your own.
            Relational databases often act as integration hubs, with multiple business applications and tools accessing and updating the same data.
            By using your own database you ensure that the database can be used with multiple applications beyond just Visual DB.
          </p>
        </div>
        <div>
          <h2>Role-based access control</h2>
          <p>
            Users in your company don't all need the same level of access.
            Visual DB lets you grant some users in your company permission to design forms and sheets, while allowing other users to only enter and update data.
          </p>
        </div>
      </div>

    <div>
            <p><img src="https://visualdb.com/images/form.png" alt="Form builder">
            </p>

            <div>
                    <h2>Design custom forms</h2>
                    <p>You can build data entry forms using a drag &amp; drop interface. You can rearrange fields, add data validation and change input types. You can even add logic to hide or disable fields. All without writing any code!</p>
                    <p>Thanks to AI assistance, you can lay out even complex forms in just a few minutes.</p>
                    <p>Learn more about <a href="https://visualdb.com/forms">Visual DB forms</a>.</p>
                </div>
        </div>

    <div>
            <div>
                  <h2>Upgrade from Excel</h2>
                  <p>
                    As a business grows, managing large volumes of data in Excel becomes cumbersome and inefficient. Visual DB enables businesses to transition to relational databases when they outgrow Excel. With its spreadsheet-like interface, Visual DB Sheets allows users to interact with data as they would in Excel, while securely storing that data in a robust relational database.
                  </p>
                  <p>Learn more about <a href="https://visualdb.com/spreadsheet">Visual DB sheets</a>.</p>
                </div>

            <p><img src="https://visualdb.com/images/sheets.png" alt="Form builder">
            </p>
        </div>

    <div>
            <p><img src="https://visualdb.com/images/visualdb-2.png" alt="Query">
            </p>

            <div>
                  <h2>Build interactive reports</h2>
                  <p>Traditional reporting tools process data on the server and download summarized data to the browser for visualization. Visual DB reporting was developed for the modern age, where client machines have plentiful memory and networks are fast. Visual DB downloads the dataset to the browser and performs data processing and visualization on the client. As a result, interactions such as slicing and dicing are instantaneous as a server roundtrip is not needed. This approach also enables experiences not possible with traditional reporting tools, such as interactive time series analysis.</p>
                  <p>Learn more about <a href="https://visualdb.com/reports">Visual DB reports</a>.</p>
                </div>
        </div>

    <div>
            <div>
                    <h2>Manage your database</h2>
                    <p>Visual DB is a one-stop shop for all your database needs. It offers all the essential tools needed to create and modify databases.</p>
                    <p>Browse through schemas, table data, and relationships with ease. View diagrams showing relationships between tables.</p>
                    <p>Visual DB supports creating and dropping tables, and adding and removing columns. It supports adding relationships between tables. It can import and export data to CSV files.</p>
                </div>

            <p><img src="https://visualdb.com/images/sakila.png" alt="Database management">
            </p>
        </div>

    <div>
            <p><img src="https://visualdb.com/images/ai-query.png" alt="Artificial Intelligence">
            </p>

            <div>
                  <h2>Leverage Artificial Intelligence</h2>
                  <p>
                    There are three ways to build queries in Visual DB: Use our visual query builder (recommended),
                    type the query in SQL (for those who know SQL),
                    or just type your query in English! Our AI will translate your natural language queries to SQL.
                  </p>
                  <p>
                    Other features of Visual DB powered by AI include automatic form layout.
                  </p>
                </div>
        </div>

    <!--
    <div class="container px-5 text-center mt-3 mb-5">
      <div class="row">
        <div class="col-sm-6 my-3">
          <h2 class="display-7 fw-normal mb-4">Interactive Form Builder</h2>
          <img class="designer-gif" src="images/airforms-designer.gif" alt="interactive layout editor" width="400px" />
        </div>
        <div class="col-sm-6 my-3">
          <h2 class="display-7 fw-normal mb-4">Automatic Database Diagram</h2>
          <img class="diagram-gif" src="images/airforms-diagram.gif" alt="interactive layout editor" width="480px" />
        </div>
      </div>
    </div>
    -->

    <div>
      <h2>Supports popular databases and clouds</h2>
      
      <p>
        MySQL, MariaDB, PostgreSQL, Oracle and SQL Server databases are supported.
      </p>
      
      <p>
        Cloud databases including Azure SQL, Amazon RDS, Google Cloud SQL and AlloyDB are supported.
      </p>
    </div>

    <div>
          <h2>Customer spotlight: e-switch Solutions AG</h2>
          <p>
            Based in Switzerland, <a href="https://e-switch.ch/">e-switch Solutions AG</a> provides software solutions for maintenance and service management.
            They use Visual DB for entering and updating data for employee shift planning.
          </p>
          <p>
            "We're using Visual DB to make it easy and comfortable for our non-technical users to enter their business data into the DB,"
            says Martin Schelldorfer of e-switch.
            "Visual DB provides a very nice web UI/UX. It's easy to use for the end user and contains all functionality we require.
            We were able to setup everything with minimal effort and no development at all.
            Everything comes out of the box (SaaS – no installation required) and is ready to use."
          </p>
        </div>

    <div>
        <h2>Demo: Create a form from an Excel spreadsheet</h2>
        <p>See how you can start with an Excel CSV file and create a beautiful database-backed form in under 10 minutes!</p>
        <p>
          <iframe width="100%" height="400" src="https://www.youtube.com/embed/6rVD5rmrjN8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
        </p>
      </div>

    <div>
        <p>Click the button below to watch more videos</p>
        
      </div>

    <a name="pricing"></a>

    <div>
      <h2>Pricing</h2>
      <div>
        <div>
              <h2>Free</h2>
              <ul>
                <li>Unlimited forms, sheets and reports.</li>
                <li>Unlimited number of users.</li>
                <li>Role-based access control.</li>
                <li>1,000 record limit.</li>
              </ul>
              <p><a href="https://app.visualdb.com/?sku=free">Get started</a>
            </p></div>
        <div>
              <h2>$5<small>/user/mo</small></h2>
              <ul>
                <li>5 users minimum.</li>
                <li>All features of free version, plus:</li>
                <li>100,000 records displayed at a time.</li>
                <li>No limit to number of records in the database.</li>
              </ul>
              <p><a href="https://app.visualdb.com/?sku=business">Get started</a>
            </p></div>
      </div>
    </div>

    <div>
        
        <p><a href="https://cloud.google.com/find-a-partner/partner/visual-db-llc">
          <img src="https://visualdb.com/images/google-partner.svg" title="Google Cloud Partner">
        </a></p>
      </div>

    <a name="contact"></a>
        
    

    

    

    
<!-- added 8/2/2024 for conversion tracking-->


  

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google loses EU court battle over €2.4B antitrust fine (163 pts)]]></title>
            <link>https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/</link>
            <guid>41502822</guid>
            <pubDate>Tue, 10 Sep 2024 16:46:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/">https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/</a>, See on <a href="https://news.ycombinator.com/item?id=41502822">Hacker News</a></p>
Couldn't get https://www.politico.eu/article/google-loses-court-battle-over-first-eu-antitrust-fine/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[What you can get out of a high-quality font (263 pts)]]></title>
            <link>https://sinja.io/blog/get-maximum-out-of-your-font</link>
            <guid>41502721</guid>
            <pubDate>Tue, 10 Sep 2024 16:38:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sinja.io/blog/get-maximum-out-of-your-font">https://sinja.io/blog/get-maximum-out-of-your-font</a>, See on <a href="https://news.ycombinator.com/item?id=41502721">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>In the previous article (<!--$--><a href="https://sinja.io/blog/web-typography-quick-guide">Quick guide to web typography for developers</a><!--/$-->) we covered the basic steps to improve the typography in your apps. Today I'd like to expand a bit more on the topic of fonts and what you can get out of a high-quality font (paid or free). High-quality fonts often come with a full bag of goodies, it will be unwise to not use what the type designer gifted (or sold) to us.</p>
<p>The minimal package you would expect from a font includes different weights and maybe italic. Traditionally, it was made by creating a separate font file. One for Helvetica Regular, one for Helvetica Bold, and separate files for Helvetica Regular Italic and Helvetica Bold Italic. But with OpenType features, we can pack all those fonts into one file, along with a bunch of other goodies. We'll cover some of the most interesting features, but there are more.</p>
<p>Available features will vary from font to font, to check what is included with your font, use <!--$--><a href="https://wakamaifondue.com/?ref=sinja.io">Wakamai Fondue</a><!--/$-->.</p>
<!--$--><a href="#table-of-contents"><h2 id="table-of-contents">Table of contents</h2></a><!--/$-->
<ul>
<li><!--$--><a href="#variable-axes">Variable axes</a><!--/$--></li>
<li><!--$--><a href="#alternates">Alternates</a><!--/$--></li>
<li><!--$--><a href="#stylistic-alternates">Stylistic alternates</a><!--/$--></li>
<li><!--$--><a href="#swashes">Swashes</a><!--/$--></li>
<li><!--$--><a href="#numerals">Numerals</a><!--/$--></li>
<li><!--$--><a href="#small-caps">Small caps</a><!--/$--></li>
<li><!--$--><a href="#contextual-alternates">Contextual alternates</a><!--/$--></li>
<li><!--$--><a href="#further-reading">Further reading</a><!--/$--></li>
</ul>
<!--$--><a href="#variable-axes"><h2 id="variable-axes">Variable axes</h2></a><!--/$-->
<p>OpenType fonts can have one or more axes, and by changing their value, we can change the font's appearance. Axes names (and other OpenType features) consist of 4 characters, and the most popular one is <code>wght</code> which controls the font's weight.</p>
<div><p>Sphinx of black quartz, judge my vow.</p></div>
<p>There are a couple of other common axes: <code>wdth</code> for width, <code>slnt</code> for slant, <code>ital</code> for italic, and <code>opsz</code> for optical size. But in addition to standard axes, the type designer can create custom axes, which further extends the creative potential of the typeface.</p>
<p>There are two ways to manipulate variable font axes. An axis might have its own CSS property, like <code>font-weight</code> which translates into <code>wght</code> axis. For other axes, including custom ones, you will need to use <code>font-variation-settings</code> property.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.cls1</span><span> {</span></span>
<span data-line=""><span>    font-weight: </span><span>451</span><span>; </span><span>/* wght axis */</span></span>
<span data-line=""><span>    font-stretch: </span><span>condensed</span><span>;  </span><span>/* wdth axis */</span></span>
<span data-line=""><span>    font-style: </span><span>italic</span><span>; </span><span>/* ital axis */</span></span>
<span data-line=""><span>    font-style: </span><span>oblique</span><span> 40</span><span>deg</span><span>; </span><span>/* slnt axis */</span></span>
<span data-line=""><span>    font-optical-sizing: </span><span>none</span><span>; </span><span>/* opsz axis */</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.cls2</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'MONO'</span><span> 0.25</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>When possible, you should prefer to use specific properties provided by CSS rather than using <code>font-variation-settings</code> for everything. A major problem with <code>font-variation-settings</code> is that it doesn't play well with cascading, as defining this property on an element completely overwrites values inherited from the parent element.</p>
<p>Imagine a situation: you have a paragraph of text for which you want to set a specific width, and it contains an element to which you also want to apply a specific slant. Normally, you should use <code>font-stretch</code> and <code>font-style</code>, but for the sake of example, let's assume you need to use <code>font-variation-settings</code>. You might try something like this:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>p</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> 75</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'slnt'</span><span> -5</span><span>;    </span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>The emphasis element will have the correct slant; however, its width will be reset to the default one. The correct way to set variation settings for the element would be to define values for both axes explicitly.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> 75</span><span>, </span><span>'slnt'</span><span> -5</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>To work around this, we can use CSS variables.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    --wdth</span><span>: </span><span>100</span><span>;</span></span>
<span data-line=""><span>    --slnt</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>*</span><span> {</span></span>
<span data-line=""><span>    font-variation-settings: </span><span>'wdth'</span><span> var</span><span>(</span><span>--wdth</span><span>), </span><span>'slnt'</span><span> var</span><span>(</span><span>--slnt</span><span>);</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>p</span><span> {</span></span>
<span data-line=""><span>    --wdth</span><span>: </span><span>75</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.emphasis</span><span> {</span></span>
<span data-line=""><span>    --slnt</span><span>: </span><span>-5</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>On <!--$--><a href="https://v-fonts.com/?ref=sinja.io">this website</a><!--/$--> you can play with a lot of different variable fonts, some of them have very interesting and unusual axes.</p>
<p>Besides axes, there are pre-defined OpenType features that can be turned on or off (and sometimes they also allow you to select one of the pre-defined values). Let's talk about the most popular ones.</p>
<!--$--><a href="#alternates"><h2 id="alternates">Alternates</h2></a><!--/$-->
<p>Fonts can contain alternative glyphs for certain characters. This includes different styles of numbers, swashes, ligatures, and just an alternative style for certain characters. But what exactly is available will vary from font to font.</p>
<!--$--><a href="#stylistic-alternates"><h2 id="stylistic-alternates">Stylistic alternates</h2></a><!--/$-->
<p>Starting with stylistic alternates. Those are just alternative forms of letters that you can enable. In some fonts, it might change how 'I', 'l', and '1' look to disambiguate them, in other fonts, it just replaces single-story 'a' and 'g' with double-story alternates. There are 3 different OpenType features related to stylistic alternates that somewhat overlap.</p>
<p>Firstly, there is <code>salt</code> to enable stylistic alternates for all letters. It's this one setting that will likely alter how 'a' and 'g' look.</p>
<p>Then there are stylistic sets. They are named <code>ss01</code>, <code>ss02</code>, and so on. They replace only a subset of characters with alternates. Sets might have a certain purpose beyond just changing visual appearance, for example, typeface Inter has the <!--$--><a href="https://rsms.me/inter/#features/ss02?ref=sinja.io">stylistic set 'Disambiguation'</a><!--/$--> which changes the appearance of characters that might look too similar to other ones, like 'I' and 'l' or '0' and 'O'.</p>
<p>Finally, there are character variants (<code>cv01</code>, <code>cv02</code>, and so on) that replace just a single character.</p>
<p>There are two ways to use alternates on the web. You can enable OpenType features directly, similar to how we directly manipulate axes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-feature-settings: </span><span>'salt'</span><span> on, </span><span>'ss01'</span><span> on, </span><span>'cv06'</span><span> on;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>This is very similar to <code>font-variation-settings</code> and has the same downside with inheritance. Another (newer) option is to use the "native" CSS property <code>font-variant-alternates</code>. To use it, we first need to map user-defined values to values that will be passed to the OpenType font:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>/* This is set per font */</span></span>
<span data-line=""><span>@font-feature-values</span><span> "Work Sans"</span><span> {</span></span>
<span data-line=""><span>    /* salt feature */</span></span>
<span data-line=""><span>    @stylistic</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        'on' is the value which we'll use in styles, while</span></span>
<span data-line=""><span>        1 is what will be passed to OpenType font.</span></span>
<span data-line=""><span>         */</span></span>
<span data-line=""><span>        on</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        off</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* ss01, ss02, ... */</span></span>
<span data-line=""><span>    @styleset</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        alt-digits is the name for the set we'll use in styles,</span></span>
<span data-line=""><span>        while 1 is its number (translates to ss01)</span></span>
<span data-line=""><span>        */</span></span>
<span data-line=""><span>        alt-digits</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        disambiguation</span><span>: </span><span>2</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* cv01, cv02, ... */</span></span>
<span data-line=""><span>    @character-variant</span><span> {</span></span>
<span data-line=""><span>        /* </span></span>
<span data-line=""><span>        This notation is a bit different: here, simplified-u will be used</span></span>
<span data-line=""><span>        in styles, but 6 means that it should enable sixth character</span></span>
<span data-line=""><span>        variant, OpenType feature cv06</span></span>
<span data-line=""><span>        */</span></span>
<span data-line=""><span>        simplified-u</span><span>: </span><span>6</span><span>;</span></span>
<span data-line=""><span>        compact-f</span><span>: </span><span>12</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-variant-alternates: </span><span>stylistic</span><span>(</span><span>on</span><span>) </span><span>styleset</span><span>(</span><span>alt-digits</span><span>) </span><span>character-variant</span><span>(</span><span>compact-f</span><span>);</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>And while it's definitely more readable, this approach has the same problem with cascading, as defining <code>font-variant-alternates</code> on an element will overwrite the parent value instead of extending it, so in any case, you'll need to do tricks with CSS variables to work around this issue.</p>
<!--$--><a href="#swashes"><h2 id="swashes">Swashes</h2></a><!--/$-->
<p>Some fonts come with swashes, which can be used to add a bit of character to titles. Similar to stylistic alternates, there are two ways to enable swashes:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-feature-settings: </span><span>'swsh'</span><span> on;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>@font-feature-values</span><span> "Work Sans"</span><span> {</span></span>
<span data-line=""><span>    @swash</span><span> {</span></span>
<span data-line=""><span>        on</span><span>: </span><span>1</span><span>;</span></span>
<span data-line=""><span>        off</span><span>: </span><span>0</span><span>;</span></span>
<span data-line=""><span>    }</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>h1</span><span>,</span><span> h2</span><span>,</span><span> h3</span><span> {</span></span>
<span data-line=""><span>    font-variant-alternates: </span><span>swash</span><span>(</span><span>on</span><span>);</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<div><p>Work Sans Regular</p><p>Work Sans Regular</p></div>
<!--$--><a href="#numerals"><h2 id="numerals">Numerals</h2></a><!--/$-->
<p>One font can have different sets of glyphs for numbers. Generally, numerals can be either lining or old-style and tabular or proportional. Those two can combine, so you can have, for example, old-style tabular numerals.</p>
<p>Tabular numerals all have the same width. Like a monospaced font, but only for numerals. Since such numerals line up when typed on multiple lines, they're useful in, well, tabular data: tables, bills, reports, you name it. Proportional numerals have different width, so 1 and 6 will take a different amount of space. They are used for numbers in blocks of text, as their width and spacing doesn't contrast with the surrounding text.</p>
<div><div><p>Tabular numerals:</p><p>115679141.42</p></div><div><p>Tabular numerals again:</p><p>46285.07</p></div><div><p>Proportional numerals:</p><p>115679141.42</p></div></div>
<p>Lining numerals are aligned by baseline at the bottom, and they all have the same height, usually the same as a capital letter. Proportional lining numerals are the best default choice, as they look good in both UI elements and body text. However, due to their size and alignment, some designers prefer not to use lining numerals for body text, as they think such numerals look like capital letters at a glance, and multiple capitals together draw a bit too much attention. They prefer to use old-style numerals: such numerals have a height of a lowercase letter and have descenders and ascenders (parts of the glyph that stick upwards or downwards) which allows them to better blend with surrounding text.</p>

<p>Which numerals will be used by default depends on your font. To explicitly set desired style, use <code>font-variant-numeric</code> property:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>table</span><span> {</span></span>
<span data-line=""><span>    font-variant-numeric: </span><span>tabular-nums</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>/* You can combine values too */</span></span>
<span data-line=""><span>.foo</span><span> {</span></span>
<span data-line=""><span>    font-variant-numeric: </span><span>tabular-nums</span><span> oldstyle-nums</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#small-caps"><h2 id="small-caps">Small caps</h2></a><!--/$-->
<p>I mentioned that multiple capital letters draw a bit too much attention when surrounded by body text. Exactly how noticeable they will be depends on the font. For example, in Work Sans, it's not hugely noticeable, but still works as an eye-catcher.</p>
<p>To solve this problem, some fonts bundle a special variant of letters called small caps. To confuse you a bit, small capitals replace lowercase letters, instead of, well, capitals, so you can still differentiate case when text is set in small caps. Or you can force the browser to transform capitals into small capitals too.</p>
<div><p>We love code names! We have code names for projects, teams, and even documents. For example, the current project's schedule is tracked on the SCHDL2 page, the successor to the SCHDL page. Well, we're still working on reducing duplication...</p><p>We love code names! We have code names for projects, teams, and even documents. For example, the current project's schedule is tracked on the <span>SCHDL2</span> page, the successor to the <span>SCHDL</span> page. Well, we're still working on reducing duplication...</p></div>
<p>To make the browser use small caps for text, you need to specify the <code>font-variant-caps</code> property.</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>.small-caps</span><span> {</span></span>
<span data-line=""><span>    /* Will turn lowercase into small caps */</span></span>
<span data-line=""><span>    font-variant-caps: </span><span>small-caps</span><span>;</span></span>
<span data-line=""><span>}</span></span>
<span data-line=""> </span>
<span data-line=""><span>.all-small-caps</span><span> {</span></span>
<span data-line=""><span>    /* Will turn everything in small caps */</span></span>
<span data-line=""><span>    font-variant-caps: </span><span>all-small-caps</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<p>If the current font doesn't have small caps, the browser will try to synthesize them from normal capital letters. If you want to disable this behavior, use this CSS property</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    /* Disable all synthesis: missing weights, italic, small caps, etc.*/</span></span>
<span data-line=""><span>    font-synthesis: </span><span>none</span><span>;</span></span>
<span data-line=""> </span>
<span data-line=""><span>    /* Disable only small caps synthesis */</span></span>
<span data-line=""><span>    font-synthesis-small-caps: </span><span>none</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#contextual-alternates"><h2 id="contextual-alternates">Contextual alternates</h2></a><!--/$-->
<p>Contextual alternates is one of my favorite font features, mainly because it doesn't require extra work from the developer or from the person typing the text, it <em>just works</em>. Well, only if typeface designer added contextual alternates to their font, of course. This feature replaces character glyphs depending on the surrounding characters.</p>
<p>This can be used to replace -&gt; with a proper arrow. Or to adjust the position of @ when it's in between uppercase letters. Inter does this <!--$--><a href="https://rsms.me/inter/#features/calt?ref=sinja.io">really well</a><!--/$-->:</p>
<p><img src="https://sinja.io/images/get-maximum-from-your-font/calt.png" alt="calt feature of Inter"></p>
<p>And you don't even need to enable them manually, contextual alternates are enabled by default. But if you want to disable them, there is a <code>font-variant-ligatures</code> property:</p>
<figure data-rehype-pretty-code-figure=""><pre tabindex="0" data-language="css" data-theme="one-dark-pro"><code data-language="css" data-theme="one-dark-pro"><span data-line=""><span>:root</span><span> {</span></span>
<span data-line=""><span>    font-variant-ligatures: </span><span>no-contextual</span><span>;</span></span>
<span data-line=""><span>}</span></span></code></pre></figure>
<!--$--><a href="#further-reading"><h2 id="further-reading">Further reading</h2></a><!--/$-->
<p>That's all for today, but we have only scratched the surface. OpenType has a lot more features, like ornaments, ordinals, fractions, random, historical forms, ligatures, and so much more. If you want to go deeper into the woods, here is a <!--$--><a href="https://otf.thien.do/hlig?ref=sinja.io">nice website</a><!--/$--> showcasing some of OpenType features. And check out <!--$--><a href="https://www.youtube.com/watch?v=TreBK-EyACQ&amp;ref=sinja.io">this talk</a><!--/$--> by Roel Nieskens about OpenType features. <!--$--><a href="https://variablefonts.io/?ref=sinja.io">A Variable Fonts Primer</a><!--/$--> is an excellent resource to learn more about variable fonts.</p></div><div><p>Was this interesting or useful?</p><div><div><p>I publish a </p><!--$--><p><a href="https://sinja.io/curated-bits?open-details=true">newsletter</a></p><!--/$--><p> with articles I found interesting and announces of my new posts. You can leave your email and get new issues delivered to your inbox!</p></div><div><p>Alternatively you can subscribe to my </p><!--$--><p><a target="_blank" href="https://sinja.io/rss">RSS feed</a></p><!--/$--><p> to know about new posts.</p></div><svg xmlns="http://www.w3.org/2000/svg" width="1.23em" height="1em" viewBox="0 0 256 209"><path fill="#55acee" d="M256 25.45a105 105 0 0 1-30.166 8.27c10.845-6.5 19.172-16.793 23.093-29.057a105.2 105.2 0 0 1-33.351 12.745C205.995 7.201 192.346.822 177.239.822c-29.006 0-52.523 23.516-52.523 52.52c0 4.117.465 8.125 1.36 11.97c-43.65-2.191-82.35-23.1-108.255-54.876c-4.52 7.757-7.11 16.78-7.11 26.404c0 18.222 9.273 34.297 23.365 43.716a52.3 52.3 0 0 1-23.79-6.57q-.004.33-.003.661c0 25.447 18.104 46.675 42.13 51.5a52.6 52.6 0 0 1-23.718.9c6.683 20.866 26.08 36.05 49.062 36.475c-17.975 14.086-40.622 22.483-65.228 22.483c-4.24 0-8.42-.249-12.529-.734c23.243 14.902 50.85 23.597 80.51 23.597c96.607 0 149.434-80.031 149.434-149.435q0-3.417-.152-6.795A106.8 106.8 0 0 0 256 25.45"></path></svg><div><p>Or follow me on </p><!--$--><p><a target="_blank" href="https://twitter.com/OlegWock?ref=sinja.io">Twitter</a></p><!--/$--><p>, where I sometimes post about new articles, my pet projects, and web dev in general.</p></div><svg xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="2" clip-rule="evenodd" viewBox="0 0 4267 4267"><g fill-rule="nonzero"><path fill="#0d0c22" d="M3327.93 992.5l-2.929-1.721-6.772-2.065a16.124 16.124 0 009.701 3.786zM3370.58 1297.31l-3.269.918 3.269-.918zM3329.18 992.03a5.9 5.9 0 01-1.221-.291 4.81 4.81 0 000 .814c.447-.06.871-.24 1.221-.523z"></path><path fill="#0d0c22" d="M3327.94 992.537h.437v-.274l-.437.274zM3367.96 1296.76l4.937-2.812 1.838-1.035 1.665-1.778a28.184 28.184 0 00-8.44 5.625zM3336.47 999.162l-4.823-4.594-3.27-1.778a13.6 13.6 0 008.093 6.372zM2123.35 3957.02a25.182 25.182 0 00-9.817 7.572l3.042-1.935c2.068-1.901 4.994-4.136 6.775-5.637zM2827.8 3818.41c0-4.337-2.122-3.536-1.608 11.942 0-1.234.517-2.502.747-3.703.287-2.768.517-5.47.861-8.239zM2754.73 3957.02a25.183 25.183 0 00-9.815 7.572l3.043-1.935c2.065-1.901 4.994-4.136 6.772-5.637zM1627.25 3989.98a20.996 20.996 0 00-10.331-4.837c3.099 1.501 6.198 3.002 8.263 4.136l2.068.701zM1515.62 3883.06a33.002 33.002 0 00-4.077-12.843 105.802 105.802 0 013.96 12.61l.117.233z"></path><path fill="#fd0" d="M2265.48 1970.99c-153.256 65.608-327.178 139.996-552.587 139.996a1046.09 1046.09 0 01-278.961-38.457l155.899 1600.6a267.425 267.425 0 0085.376 174.736c49.382 45.469 114.059 70.688 181.185 70.688 0 0 221.045 11.476 294.806 11.476 79.382 0 317.417-11.476 317.417-11.476a267.422 267.422 0 00181.152-70.721 267.378 267.378 0 0085.356-174.703l166.973-1768.73c-74.618-25.483-149.926-42.416-234.819-42.416-146.828-.057-265.13 50.512-401.797 109.001z"></path><path fill="#0d0c22" d="M3623.01 1140.38l-23.475-118.415c-21.066-106.246-68.88-206.638-177.938-245.038-34.958-12.286-74.622-17.564-101.426-42.993-26.804-25.427-34.727-64.918-40.925-101.54-11.483-67.215-22.271-134.488-34.04-201.587-10.158-57.688-18.194-122.491-44.655-175.413-34.44-71.061-105.902-112.618-176.964-140.112a1018.985 1018.985 0 00-111.297-34.44c-177.539-46.838-364.201-64.058-546.846-73.873a4590.995 4590.995 0 00-657.743 10.906c-162.783 14.809-334.237 32.717-488.928 89.026-56.537 20.607-114.799 45.346-157.789 89.027-52.752 53.668-69.972 136.668-31.458 203.595 27.381 47.527 73.761 81.107 122.953 103.32a997.234 997.234 0 00199.635 64.978c191.139 42.246 389.11 58.832 584.382 65.894a4450.01 4450.01 0 00648.616-21.183 3691.096 3691.096 0 00159.514-21.063c62.509-9.587 102.63-91.324 84.205-148.265-22.043-68.073-81.28-94.477-148.262-84.203-9.874 1.548-19.688 2.983-29.563 4.417l-7.115 1.034a4112.51 4112.51 0 01-68.077 8.037 3614.333 3614.333 0 01-140.973 12.399c-105.502 7.346-211.288 10.732-317.017 10.905-103.894 0-207.845-2.929-311.509-9.757a3960.048 3960.048 0 01-141.547-11.826c-21.41-2.238-42.764-4.59-64.114-7.232l-20.322-2.582-4.417-.631-21.066-3.042c-43.051-6.489-86.101-13.948-128.691-22.961a19.351 19.351 0 01-10.898-6.778 19.34 19.34 0 0110.898-30.991h.804c36.909-7.863 74.101-14.579 111.411-20.434a5471.985 5471.985 0 0137.425-5.74h.344c23.362-1.55 46.84-5.74 70.085-8.495a4457.35 4457.35 0 01608.954-21.468c98.667 2.871 197.28 8.668 295.49 18.655 21.123 2.182 42.133 4.478 63.14 7.06 8.036.976 16.129 2.124 24.225 3.1l16.299 2.353a2262.712 2262.712 0 01141.834 25.773c69.685 15.154 159.171 20.09 190.165 96.431 9.875 24.222 14.351 51.143 19.802 76.57l6.946 32.431c.183.581.317 1.178.403 1.779a525388.074 525388.074 0 0049.305 229.598 42.052 42.052 0 01.084 17.16 42.057 42.057 0 01-19.282 27.519 42.11 42.11 0 01-16.159 5.777h-.461l-10.044 1.378-9.928 1.318a5572.202 5572.202 0 01-94.48 11.482 6103.915 6103.915 0 01-186.605 18.368 6529.65 6529.65 0 01-372.124 20.262 6687.833 6687.833 0 01-189.934 2.352 6591.117 6591.117 0 01-753.885-43.854c-27.094-3.213-54.185-6.659-81.28-10.158 21.01 2.699-15.268-2.068-22.614-3.099a4831.624 4831.624 0 01-51.66-7.519c-57.802-8.67-115.257-19.345-172.945-28.703-69.739-11.479-136.438-5.737-199.52 28.703a290.205 290.205 0 00-120.137 124.557c-27.207 56.25-35.301 117.495-47.469 177.938-12.169 60.441-31.111 125.475-23.936 187.523 15.44 133.915 109.059 242.743 243.718 267.082 126.681 22.958 254.051 41.555 381.764 57.398a7054.901 7054.901 0 001511.79 21.927 7274.568 7274.568 0 00122.776-12.513 86.111 86.111 0 0137.806 4.303 86.13 86.13 0 0132.258 20.179 86.08 86.08 0 0120.409 32.108 86.046 86.046 0 014.581 37.773l-12.744 123.87c-25.676 250.298-51.353 500.58-77.03 750.842a596442.614 596442.614 0 01-80.876 788.367c-7.653 74.025-15.305 148.016-22.958 221.973-7.349 72.857-8.38 147.982-22.214 219.972-21.814 113.188-98.44 182.709-210.254 208.129a1465.055 1465.055 0 01-312.14 36.495c-116.464.634-232.868-4.537-349.332-3.903-124.33.7-276.609-10.775-372.58-103.314-84.323-81.263-95.975-208.529-107.454-318.582a249915.376 249915.376 0 01-45.519-436.97l-84.375-809.851-54.586-523.999c-.921-8.666-1.838-17.22-2.699-25.946-6.545-62.506-50.8-123.693-120.541-120.538-59.693 2.639-127.539 53.382-120.537 120.538l40.465 388.482 83.688 803.593c23.842 228.258 47.624 456.543 71.349 684.887 4.594 43.734 8.897 87.602 13.718 131.336 26.234 239.019 208.762 367.82 434.802 404.081 132.019 21.25 267.251 25.62 401.223 27.789 171.74 2.769 345.202 9.374 514.127-21.751 250.322-45.935 438.131-213.066 464.935-472.334 7.656-74.859 15.309-149.717 22.962-224.609 25.446-247.663 50.856-495.342 76.226-743.042l83.001-809.331 38.056-370.915a86.07 86.07 0 0122.151-49.322 86.11 86.11 0 0147.187-26.391c71.576-13.947 139.999-37.769 190.912-92.242 81.046-86.727 97.176-199.805 68.533-313.801zm-2692.55 80.016c1.09-.517-.919 8.837-1.78 13.2-.172-6.602.173-12.456 1.78-13.2zm6.945 53.725c.574-.403 2.296 1.895 4.075 4.65-2.697-2.528-4.419-4.42-4.132-4.65h.057zm6.831 9.011c2.468 4.189 3.788 6.831 0 0zm13.661 11.135h.402c0 .404.631.804.861 1.207a8.954 8.954 0 00-1.263-1.207zm2402.34-16.646c-25.713 24.452-64.46 35.817-102.746 41.502-429.348 63.713-864.951 95.971-1299.01 81.737-310.646-10.618-618.018-45.119-925.565-88.569-30.136-4.247-62.796-9.758-83.517-31.972-39.032-41.902-19.861-126.278-9.701-176.904 9.299-46.38 27.093-108.198 82.254-114.8 86.1-10.101 186.087 26.231 271.271 39.148a5137.34 5137.34 0 00308.807 37.595c440.943 40.182 889.293 33.924 1328.29-24.852a5541.283 5541.283 0 00239.183-37.483c70.775-12.686 149.239-36.505 192.003 36.792 29.329 49.939 33.232 116.751 28.699 173.175a96.554 96.554 0 01-30.02 64.631h.056z"></path><path fill="#0d0c22" d="M951.338 1288.11l2.64 2.469 1.723 1.034a26.516 26.516 0 00-4.363-3.503z"></path></g></svg><div><p>If you really like the article, you can </p><!--$--><p><a href="https://sinja.io/support">give me monies</a></p><!--/$--><p>, and I'll buy myself tasty coffee to write even more.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Illuminate: Books and Papers turned into audio content (379 pts)]]></title>
            <link>https://illuminate.google.com/home</link>
            <guid>41502510</guid>
            <pubDate>Tue, 10 Sep 2024 16:22:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://illuminate.google.com/home">https://illuminate.google.com/home</a>, See on <a href="https://news.ycombinator.com/item?id=41502510">Hacker News</a></p>
Couldn't get https://illuminate.google.com/home: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[GPTs and Hallucination (116 pts)]]></title>
            <link>https://queue.acm.org/detail.cfm?id=3688007</link>
            <guid>41501818</guid>
            <pubDate>Tue, 10 Sep 2024 15:33:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://queue.acm.org/detail.cfm?id=3688007">https://queue.acm.org/detail.cfm?id=3688007</a>, See on <a href="https://news.ycombinator.com/item?id=41501818">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>

<p><a href="https://queue.acm.org/"><img src="https://queue.acm.org/img/acmqueue_logo.gif"></a>
</p></div>


<p><label>September 9, 2024<br><b><a href="https://queue.acm.org/issuedetail.cfm?issue=3695735">Volume 22, issue 4 </a></b></label></p><p>

&nbsp;
<a href="https://portal.acm.org/citation.cfm?id=3688007">
<img src="https://queue.acm.org/img/icon_pdf.png" alt="Download PDF version of this article">
PDF
</a>
</p>

<h2>Why do large language models hallucinate?</h2>
<h3>Jim Waldo and Soline Boussard</h3>
<p>The recent developments of LLMs (large language models) and the applications built on them such as ChatGPT have completely revolutionized human-AI interactions because of their ability to generate comprehensive and coherent text. Their impressive performance stems from transformer-based applications pre-trained on models that are built on massive amounts of raw data. These applications have the capability to answer questions, summarize text, and engage in conversations making them suitable for simple tasks across a variety of fields, sometimes even outperforming humans. Despite their powerful capabilities, however, GPTs have the tendency to "hallucinate" responses. A <i>hallucination</i> occurs when an LLM-based GPT generates a response that is seemingly realistic yet is nonfactual, nonsensical, or inconsistent with the given prompt.</p>
<p>Hallucinations in GPTs can lead to the dissemination of false information, creating harmful outcomes in applications of critical decision-making or leading to mistrust in artificial intelligence. In a viral instance, the <i>New York Times</i> published an article about a lawyer who used ChatGPT to produce case citations without realizing they were fictional, or hallucinated.<sup>6</sup> This incident highlights the danger of hallucinations in LLM-based queries; often the hallucinations are subtle and go easily unnoticed. Given these risks, an important question arises: Why do GPTs hallucinate?</p>

<h3>How Large-Language GPTs Work</h3>
<p>LLMs are created by performing machine learning on large amounts of data. The data for these models consists of whatever language examples can be found; the Internet has resulted in a lot of language data (in many different languages) that can be used to train LLMs. Radically simplifying, the result of the training is a set of probabilities that can be used to tell, for any word or string of words, which word or words are the most likely to be associated with those words. This is not a simple set of probabilities but rather a set of parameters that encapsulate the likelihood of what comes next in a sequence.</p>
<p>Models are often described by the size of the training set and the number of parameters that are used to build the probability model. While the exact sizes are unknown, best guesses are that an LLM underlying GPT-4 was trained on something on the order of 13 trillion tokens (word or word parts) and that the model contained 1.75 trillion parameters.</p>
<p>Each parameter in a model defines a dimension in space, so the number of parameters is (roughly) the number of dimensions in the space. Each token is encoded into an embedding, which represents a point in this space, and the words that are most likely to co-occur with that word are close in the space. The idea of context or attention allows the generation of the next word to consider the previous context; this can be thought of as a path or vector through space. What has come before determines the direction of the path and the continuation of the path determines what is most likely to follow. The longer the path (and thus the more context that has been given), the smaller the probability space of the next term.</p>
<p>Given that the prediction of the next word is based on the co-occurrence probability, which word comes next has nothing to do with its semantic meaning or what is true in the real world; instead, it has to do with what has been found to be most likely in looking at all of the words and where they occur in the training set. This is a statistical probability based on past use, not something tied to the facts of the world. Unlike the philosophical dictum that the sentence "Grass is green" is true because, in the real world, grass is green,<sup>5</sup> a GPT will tell us that grass is green because the words "grass is" are most commonly followed by "green." It has nothing to do with the color of the lawn.</p>
<p>Once understood in this way, the question to ask is not, "Why do GPTs hallucinate?", but rather, "Why do they get anything right at all?"</p>

<h3>Epistemic Trust</h3>
<p>At its core, this question brings up the philosophical issue of how to trust that something expressed in language is true, referred to as <i>epistemic trust</i>.</p>
<p>We tend to forget how recent the current mechanisms are for establishing trust in a claim. The notion that science is an activity that is based on experience and experiment can be traced back to Francis Bacon in the 17th century;<sup>2</sup> the idea that we can use logic and mathematics to derive new knowledge from base principles can be traced to about the same time to Ren? Descartes.<sup>3</sup> This approach of using logic and experiment are hallmarks of the Renaissance; prior to that time trust was established by reference to ancient authorities (such as Aristotle or Plato) or from religion. </p>
<p> What has emerged over the past number of centuries is the set of practices that are lumped together as science, which has as its gold standard the process of experimentation, publication, and peer review. We trust something by citing evidence obtained through experimentation and documenting how that evidence was collected and how the conclusion was reached. Then both the conclusion and the process are reviewed by experts in the field. Those experts are determined by their education and experience, often proved by their past ability to uncover new knowledge as judged by the peer-review process.</p>
<p>This is not a perfect system. As noted by American historian and philosopher Thomas S. Kuhn,<sup>4</sup> this works well for what he calls "normal science," where the current theories are being incrementally extended and improved. It does not work well for radical changes, which Kuhn refers to as a "paradigm shift" or "scientific revolutions." Those sorts of changes require shifting the way that the problems are conceived, and the experiments understood, and often require a new generation of scientists, at which point the conventions of normal science resume.</p>

<h3>Crowdsourcing</h3>
<p>The advent of the World Wide Web (and to some extent the newsgroups that had been part of the Internet culture before the Web) brought about a different sort of mechanism for epistemic trust, now known as <i>crowdsourcing</i>. Rather than looking to experts who have been recognized based on their education or the opinion of other experts, questions were asked of large groups of people, and then answers taken and correlated from the large group. This is a form of knowledge by discussion and consensus, where the various parties do not just answer the question, but also argue with each other until they reach some form of agreement.</p>
<p>Crowdsourcing leverages diverse groups of individuals to reach a resolution about a given problem and facilitate collaboration across domains. Platforms such as Wikipedia or Reddit serve as hubs for this process. On these websites, users can suggest solutions or contributions to posts. The responses then go through a range of verification or cross-checks to bolster their reliability. On Reddit, other users can "upvote" the responses that they believe answer the prompt most appropriately, leveraging crowdsourcing in the diversity and popularity of responses. On Wikipedia, those who have been found to be reliable arbiters in the past have more of a say in what stays on the site, based on their reputations. </p>
<p>Open-source software is another form of crowdsourcing that relies on collaboration to improve code. Communities such as GitHub allow users to publish their code for others to build off of and offer new ideas.</p>
<p>While crowdsourcing is seen as more inclusive than the expert peer review described earlier, it is not completely without distinctions among the contributors. Those who have demonstrated their expertise in a subject in the discussions may be given more weight than others. Unlike scientific peer review, however, the demonstration of expertise is not tied to particular educational backgrounds or credentials, but rather to the reputation that the person has established within the particular community.</p>
<p>GPTs based on LLMs can be understood as the next step in this shift that starts from expertise-based trust and moves through crowd-based trust. Rather than being a crowdsourced answer to some question, a GPT generates the most common response based on every question that has been asked on the Internet and every answer that has been given to that question. The consensus view is determined by the probabilities of the co-occurrence of the terms.</p>

<h3>Why This Works</h3>
<p>Most of our use of language is to describe the world to others. In doing so, we try to be as accurate as possible; if we were constantly trying to mislead each other, our utterances would not be useful either to those we were speaking to or as training data for LLMs.</p>
<p>Thus, the most likely way to complete a phrase is also the most likely to describe the world in a way that is just as accurate as you would get if you were crowdsourcing the answer, because the LLM is trained on everyone's answer to every question. This sort of embedded meaning in co-occurrence is much like Austrian philosopher Ludwig Wittgenstein's notion that the meaning of a word is its use in the language.<sup>7</sup></p>
<p>But not all uses of language are to describe the world, as was pointed out by the language philosopher John L. Austin. Language is also used to argue, mislead, perform actions, and indirectly indicate the beliefs of the user.<sup>1</sup> We tell stories, write fiction, and create metaphors. All of these uses of language are also used to train LLMs.</p>
<p>This leads to our hypothesis about when an LLM-based GPT will get things right, and where it will get things wrong (or hallucinate). When there is general consensus on a topic, and there is a large amount of language available to train the model, LLM-based GPTs will reflect that consensus view. But in cases where there are not enough examples of language about a subject, or the subject is controversial, or there is no clear consensus on the topic, relying on these systems will lead to questionable results. If this hypothesis is correct, then hallucination should be more likely to occur when dealing with subjects that are obscure or controversial.</p>
<p>To test this hypothesis, we came up with a set of simple prompts that varied in both the obscurity of the subject matter and the degree of controversy around the topic. We then asked a variety of GPTs these questions over a period of time. The prediction was that the more obscure or controversial the subject matter, the more likely it was that the result would be incorrect.</p>

<h3>Experiment</h3>
<p>For this experiment we used four models: Llama, accessed through the open-source Llama-lib; ChatGPT-3.5 and ChatGPT-4, accessed through the OpenAI subscription service; and Google Gemini, accessed through the free Google service. The Llama implementation is open-source, so we could track any changes to the model. The two versions of ChatGPT enjoy general popularity and are well-documented in terms of the scale of their training sets. Google Gemini is an exemplar of an alternative to the OpenAI offerings. </p>
<p>Our aim was to understand how advancements in model development and in incorporating user feedback affect the algorithms' responses to various prompts.</p>
<p>We initially tested a broad range of topics to assess the models' behavior on various sensitive or obscure topics. After a couple of iterations, the final prompts included:</p>
<p>1. "Give me 2 papers about ferroelectrics polarization and cite them." This was selected to examine the model's response to an obscure subject.</p>
<p>2. "What are four uncommon quotes by Barack Obama?" This combined politically sensitive and intentionally uncommon information.</p>
<p>3. "In a couple of sentences, explain how Putin's political justifications of full-scale invasion in Ukraine are informed by the writings of Tolstoy and Dostoevsky." This combined complex and obscure information with a topic that might be politically controversial.</p>
<p>4. "In three words, climate change is ..." This was selected for its controversial subject matter. </p>
<p>5. "Complete this sentence: Israelis are ..." This was selected for political sensitivity.</p>
<p>Each of these prompts was posed to each model every week from March 27, 2024, to April 29, 2024. The prompts were presented sequentially in a single chat session and were also tested in an isolated chat session to view context dependency.</p>
<p>Note that one confounding variable in this experiment is that the applications are not consistent over time. There are multiple causes for this inconsistency. The first, technically known as <i>temperature</i>, is based on the observation that adding some randomness to the completion of a GPT makes it sound more like a human than simply taking the most likely completion (which by itself is an interesting result). But such randomness is not the only reason for variation; all but the open-source Llama application were under constant and intense modification over the period of the experiment, as developers attempted to add "guardrails" to these systems. Thus, the applications, which may have started out as just interfaces to the underlying large-language models, evolved to become something more complex as these guardrails were added.</p>

<h3>Results</h3>
<p>Throughout the experiment, responses exhibited varying degrees of consistency, with ChatGPT-4 and Google Gemini showing more significant changes than the other applications (likely reflecting the more active ongoing development on top of those models). Some of the responses varied in length and tone across the applications over time. Additionally, despite the prompts being completely unrelated, the applications would sometimes use the context of preceding questions to inform subsequent responses.</p>
<p>Llama often repeated the same Obama quotes and introduced quotes not originating from Obama. It was consistently unable to cite scientific papers accurately. In response to the political justifications of Putin's actions being informed by Tolstoy and Dostoevsky, the Llama application would sometimes warn about attributing actions to literary influences and other times it did not. The application also did not adhere to the requested three-word structure of the climate change question, sometimes giving one -word answers and other times a complete sentence.</p>
<p>The ChatGPT-3.5 application was consistently able to provide accurate Obama quotes and three-word responses to the question about climate change. The application was also consistently unable to cite scientific papers correctly, although the topics of the papers were relevant to the field of material science. Initially the authors cited were generic "John Doe" and "Jane Smith"; after a couple of weeks, however, the authors who were cited shifted to scientists in the field of material science (although they were not the authors of the papers cited).</p>
<p>The ChatGPT-4 application was able to provide accurate quotes from Obama and gave a sensible answer to Putin's justifications. In response to the prompt concerning climate change, during one iteration the application introduced the term "solvable," which may not reflect scientific consensus. On another occasion in response to the question about climate change, ChatGPT-4 gave two different responses side by side, prompting the user to choose which response most accurately answered the question. Although ChatGPT-4 sometimes correctly cited scientific papers, there were instances where it cited the wrong group of authors or reported difficulties accessing Google Scholar to provide specific references. Interestingly, it would often give a citation with a set of authors who had co-authored papers, but attribute those authors to papers that, even if the papers existed, were not written by any of the listed authors.</p>
<p>Google Gemini was unable to answer the prompts regarding Obama's quotes and Putin's justifications, apart from one week when it managed to answer both. Every other week the application would suggest that the user try Google Search to answer the question instead. Gemini performed similarly to ChatGPT-4 in response to papers about ferroelectric polarization, providing relevant papers and authors but incorrect citations, pairing groups of authors who had written papers together with papers that they had not written. In response to the prompt "Complete this sentence: Israelis are " Google Gemini provided various ways to complete the sentence. During one iteration, the response included multiple perspectives and encouraged further engagement by asking, "What aspect of Israelis are you most curious about?"</p>

<h3>Discussion and Observations</h3>
<p>In response to the question about scientific papers, all the applications were able to provide correct citation syntax, but the complete citations were rarely accurate. Notably, the authors cited by ChatGPT-4 would occasionally have a paper published together in the field but not the provided paper in the citation. Such a response makes sense when the responses are viewed as statistically likely completions; the program knows what such citations look like, and even what groups of authors tend to co-occur, even if not for the particular paper cited.</p>
<p> In general, the Llama-based application provided the most consistent answers but generally of lower quality than the others. This met our expectations; the application was not being actively developed and was based on an early LLM. It was also the application that was most purely the reflection of an LLM; the others were combinations of LLMs and all of the developments on top of the models designed to make the answers more accurate, or less hallucinatory.</p>
<p>ChatGPT-3.5 and -4 consistently provided accurate quotes from Obama. The Llama application often returned multiple iterations of the same quote, most of which were inaccurate. The one week where Google Gemini was able to respond to the prompt about Obama, one of the quotes was not actually from Obama, but from comedian and TV host Craig Ferguson, who had mentioned Obama earlier in his monologue.</p>
<p> The Llama-based application struggled to follow the three-word restrictions when those were part of the prompt, sometimes returning one word and other times a complete sentence. One week, when the Llama application was prompted, "In three words, climate change is ", the model returned a response with only one word. When asked again without the ellipses, it returned three words: "Unstoppable, irreversible, catastrophic." This raises the question of how the application interprets grammar and punctuation, and how those nonsemantic features influence the responses. Additionally, one week ChatGPT-4 included the term "solvable" as a description of climate change, which could be disputed as inaccurate by some scientists but does reflect the wider Internet discussion of this topic.</p>
<p>When the prompt about Israelis was asked to ChatGPT-3.5 sequentially following the previous prompt of describing climate change in three words, the model would also give a three-word response to the Israelis prompt. This suggests that the responses are context-dependent, even when the prompts are semantically unrelated.</p>
<p>Furthermore, although ChatGPT-4 and Google Gemini provided the most accurate and relevant responses, some of the sources cited were from obscure and seemingly unreliable sources. When asking ChatGPT-4 about Obama quotes, three of the quotes cited were from Bored Panda, a Lithuanian website that publishes articles about "entertaining and amusing news." Similarly, Google Gemini cited an Obama quote from Rutland Jewish Center. The use of blog posts and unreliable sources highlights the lack of robust filtering mechanisms to ensure that responses are sourced from authoritative and credible references.</p>

<h3>Conclusions</h3>
<p>Overall, the applications struggled on topics with limited data online. They often produced inaccurate responses framed in realistic formatting and without acknowledgment of the inaccuracies. The applications were able to handle polarizing topics more meticulously, yet some still returned inaccuracies and occasionally warned the user about making statements on controversial topics.</p>
<p>The advent of crowdsourcing has been used in many contexts to draw upon a diverse range of people and knowledge bases. Crowdsourcing in the application of LLMs, however, raises concerns that must be acknowledged because of their tendency to hallucinate, coupled with humans' epistemic trust.</p>
<p>LLMs and the generative pretrained transformers built on those models do fit the pattern of crowdsourcing, drawing as they do on the discourse embodied in their training sets. The consensus views found in this discourse are often factually correct but appear to be less accurate when dealing with controversial or uncommon subjects. Consequently, LLM-based GPTs can propagate common knowledge accurately, yet struggle with questions that don't have a clear consensus in their training data.</p>
<p>These findings support the hypothesis that GPTs based on LLMs perform well on prompts that are more popular and have reached a general consensus yet struggle on controversial topics or topics with limited data. The variability in the applications's responses underscores that the models depend on the quantity and quality of their training data, paralleling the system of crowdsourcing that relies on diverse and credible contributions. Thus, while GPTs can serve as useful tools for many mundane tasks, their engagement with obscure and polarized topics should be interpreted with caution. LLMs' reliance on probabilistic models to produce statements about the world ties their accuracy closely to the breadth and quality of the data they're given.</p>
<h4>References</h4>
<p>1. Austin, J. L. 1962. <i>How to Do Things with Words</i>. Oxford University Press.</p>
<p>2. Bacon, F. Novum Organum. Joseph Devey, M.A., editor. New York: P.F. Collier, 1902.</p>
<p>3. Descartes, R. 2008. <i>Meditations on First Philosophy</i> (M. Moriarty, translator). Oxford University Press.</p>
<p>4. Kuhn, T. S. 1962. <i>The Structure of Scientific Revolutions</i>. University of Chicago Press.</p>
<p>5. Lewis, D. 1970. General semantics. <i>Synthese</i> 22 (1/2), Semantics of Natural Language II,18?67. Springer Nature; <a href="https://www.jstor.org/stable/20114749">https://www.jstor.org/stable/20114749</a>.</p>
<p>6. Weiser, B. 2023. Here's what happens when your lawyer uses ChatGPT. <i>New York Times </i>(May 27); <a href="https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html">https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html</a>.</p>
<p>7. Wittgenstein, L. 1953. <i>Philosophical Investigations</i> 1 (section 43). G.E.M. Anscombe, editor. Wiley-Blackwell.</p>

<p><b>Jim Waldo</b> is the Gordon McKay Professor of the Practice of Computer Science at Harvard University. Prior to Harvard, he spent over 30 years in industry, much of that at Sun Microsystems where he worked on distributed systems and programming languages.</p>
<p><b>Soline Boussard</b> is a student in the Masters of Data Science Program at Harvard University. She is a graduate of the University of Pennsylvania.</p>
<p>Copyright © 2024 held by owner/author. Publication rights licensed to ACM.</p>

<div>
<p><img src="https://queue.acm.org/img/q%20stamp_small.jpg" width="26" height="45" alt="acmqueue"></p><p>
<em>Originally published in Queue vol. 22, no. 4</em>—
<br>
Comment on this article in the <a href="http://portal.acm.org/citation.cfm?id=3688007">ACM Digital Library</a></p></div>







<hr noshade="" size="1"><p>
More related articles:
</p><p>
<span>Erik Meijer</span> - <a href="https://queue.acm.org/detail.cfm?id=3676287"><b>Virtual Machinations: Using Large Language Models as Neural Computers</b></a>
<br>
We explore how Large Language Models (LLMs) can function not just as databases, but as dynamic, end-user programmable neural computers. The native programming language for this neural computer is a Logic Programming-inspired declarative language that formalizes and externalizes the chain-of-thought reasoning as it might happen inside a large language model.
</p>

<p>
<span>Mansi Khemka, Brian Houck</span> - <a href="https://queue.acm.org/detail.cfm?id=3675416"><b>Toward Effective AI Support for Developers</b></a>
<br>
The journey of integrating AI into the daily lives of software engineers is not without its challenges. Yet, it promises a transformative shift in how developers can translate their creative visions into tangible solutions. As we have seen, AI tools such as GitHub Copilot are already reshaping the code-writing experience, enabling developers to be more productive and to spend more time on creative and complex tasks. The skepticism around AI, from concerns about job security to its real-world efficacy, underscores the need for a balanced approach that prioritizes transparency, education, and ethical considerations.
</p>

<p>
<span>Divyansh Kaushik, Zachary C. Lipton, Alex John London</span> - <a href="https://queue.acm.org/detail.cfm?id=3639452"><b>Resolving the Human-subjects Status of Machine Learning's Crowdworkers</b></a>
<br>
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diversity of both the tasks performed and the uses of the resulting data render it difficult to determine when crowdworkers are best thought of as workers versus human subjects. These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements.
</p>

<p>
<span>Harsh Deokuliar, Raghvinder S. Sangwan, Youakim Badr, Satish M. Srinivasan</span> - <a href="https://queue.acm.org/detail.cfm?id=3631340"><b>Improving Testing of Deep-learning Systems</b></a>
<br>
We used differential testing to generate test data to improve diversity of data points in the test dataset and then used mutation testing to check the quality of the test data in terms of diversity. Combining differential and mutation testing in this fashion improves mutation score, a test data quality metric, indicating overall improvement in testing effectiveness and quality of the test data when testing deep learning systems.
</p>
<br>
<hr noshade="" size="1">
<hr noshade="" size="1">
<p>
<a href="#"><img src="https://queue.acm.org/img/logo_acm.gif"></a>
<br>
© ACM, Inc. All Rights Reserved.
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ford seeks patent for tech that listens to driver conversations to serve ads (142 pts)]]></title>
            <link>https://therecord.media/ford-patent-application-in-vehicle-listening-advertising</link>
            <guid>41501630</guid>
            <pubDate>Tue, 10 Sep 2024 15:17:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://therecord.media/ford-patent-application-in-vehicle-listening-advertising">https://therecord.media/ford-patent-application-in-vehicle-listening-advertising</a>, See on <a href="https://news.ycombinator.com/item?id=41501630">Hacker News</a></p>
Couldn't get https://therecord.media/ford-patent-application-in-vehicle-listening-advertising: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A good day to trie-hard: saving compute 1% at a time (456 pts)]]></title>
            <link>https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/</link>
            <guid>41501496</guid>
            <pubDate>Tue, 10 Sep 2024 15:03:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/">https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/</a>, See on <a href="https://news.ycombinator.com/item?id=41501496">Hacker News</a></p>
Couldn't get https://blog.cloudflare.com/pingora-saving-compute-1-percent-at-a-time/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: YourNextStore – an open-source Shopify with Stripe as the back end (206 pts)]]></title>
            <link>https://github.com/yournextstore/yournextstore</link>
            <guid>41500938</guid>
            <pubDate>Tue, 10 Sep 2024 14:08:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/yournextstore/yournextstore">https://github.com/yournextstore/yournextstore</a>, See on <a href="https://news.ycombinator.com/item?id=41500938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Your Next Store</h2><a id="user-content-your-next-store" aria-label="Permalink: Your Next Store" href="#your-next-store"></a></p>

<p dir="auto"><h2 tabindex="-1" dir="auto">Demo</h2><a id="user-content-demo" aria-label="Permalink: Demo" href="#demo"></a></p>
<details open="">
  <summary>
    
    <span aria-label="Video description demo-yns.mp4">demo-yns.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/1338731/356836994-64197310-29bd-4dd3-a736-1494340e20e8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8xMzM4NzMxLzM1NjgzNjk5NC02NDE5NzMxMC0yOWJkLTRkZDMtYTczNi0xNDk0MzQwZTIwZTgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTBUMjAzMDE0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yjc3MzQ2N2U3YjllNWIzZTBkNjVkYjg2MzFjMWUwMzU5OWEyZDU3OWIzYTJhNzVhYWJkMzdkMGFmODUzMWU1NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.PzUtLv_ySwT8ZHqD43Or_Dr817PvMuhssmwBcPnFjKw" data-canonical-src="https://private-user-images.githubusercontent.com/1338731/356836994-64197310-29bd-4dd3-a736-1494340e20e8.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8xMzM4NzMxLzM1NjgzNjk5NC02NDE5NzMxMC0yOWJkLTRkZDMtYTczNi0xNDk0MzQwZTIwZTgubXA0P1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkxMCUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MTBUMjAzMDE0WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9Yjc3MzQ2N2U3YjllNWIzZTBkNjVkYjg2MzFjMWUwMzU5OWEyZDU3OWIzYTJhNzVhYWJkMzdkMGFmODUzMWU1NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.PzUtLv_ySwT8ZHqD43Or_Dr817PvMuhssmwBcPnFjKw" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h2 tabindex="-1" dir="auto">Prerequisites</h2><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Node.js 20+</h3><a id="user-content-nodejs-20" aria-label="Permalink: Node.js 20+" href="#nodejs-20"></a></p>
<p dir="auto">We officially support the current LTS version – 20 at the time of writing. YNS should work on versions 18, 20, and 22. If you're using one of those versions and encounter a problem, please report it!</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing Node.js</h4><a id="user-content-installing-nodejs" aria-label="Permalink: Installing Node.js" href="#installing-nodejs"></a></p>
<p dir="auto">Follow the instructions for your operating system found here: <a href="https://nodejs.org/en/download" rel="nofollow">nodejs.org/en/download</a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">pnpm 9+</h3><a id="user-content-pnpm-9" aria-label="Permalink: pnpm 9+" href="#pnpm-9"></a></p>
<p dir="auto">We officially support pnpm version 9, but we will do our best to keep it compatible with npm and yarn.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Installing pnpm</h4><a id="user-content-installing-pnpm" aria-label="Permalink: Installing pnpm" href="#installing-pnpm"></a></p>
<p dir="auto">The easiest way to install pnpm is via Node.js Corepack. Inside the folder with YNS, run these commands:</p>
<div dir="auto" data-snippet-clipboard-copy-content="corepack enable
corepack install"><pre>corepack <span>enable</span>
corepack install</pre></div>
<p dir="auto">Alternatively, follow the instructions for your operating system found here: <a href="https://pnpm.io/installation" rel="nofollow">pnpm.io/installation</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Create Stripe account</h2><a id="user-content-create-stripe-account" aria-label="Permalink: Create Stripe account" href="#create-stripe-account"></a></p>
<p dir="auto">YNS is tightly integrated with <a href="https://stripe.com/" rel="nofollow">Stripe</a>, so you need a Stripe account to use Your Next Store. Follow the instructions from Stripe to <a href="https://dashboard.stripe.com/register" rel="nofollow">create an account</a>.</p>
<p dir="auto">It's important to remember that Stripe works in two different modes: <strong>Test Mode</strong> and <strong>Production Mode</strong>. For local development and testing purposes, you should use the <strong>Test Mode</strong>. This way, Stripe will never charge real money, and you can use special test credentials such as credit card numbers and BLIK numbers to complete payments. For more detailed information, please refer to the Stripe documentation at <a href="https://docs.stripe.com/testing" rel="nofollow">docs.stripe.com/testing</a>.</p>
<p dir="auto">Once you're ready to sell your products to real customers, you must switch <strong>Test Mode</strong> to <strong>Production Mode</strong> in Stripe and generate new credentials.</p>
<div dir="auto"><p dir="auto">Tip</p><p dir="auto">This step will require additional verification from Stripe, so we suggest you start the process immediately.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Add Environment Variables</h2><a id="user-content-add-environment-variables" aria-label="Permalink: Add Environment Variables" href="#add-environment-variables"></a></p>
<p dir="auto">For YNS to work, you'll need to define a few environmental variables. For local development and testing, you may create an empty <code>.env</code> file and copy the contents of <code>.env.example</code> into it.</p>
<p dir="auto">To set env variables in production, you'll need to consult the documentation of your chosen hosting provider.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Environment Variables</h3><a id="user-content-required-environment-variables" aria-label="Permalink: Required Environment Variables" href="#required-environment-variables"></a></p>
<ul dir="auto">
<li><code>ENABLE_EXPERIMENTAL_COREPACK</code> –&nbsp;Vercel only: Set to <code>1</code> to enable Corepack</li>
<li><code>NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY</code> – Publishable key from Stripe.</li>
<li><code>STRIPE_SECRET_KEY</code> – Secret key from Stripe.</li>
<li><code>STRIPE_CURRENCY</code> – This is used to determine your store's currency. Currently, only a single currency is allowed, and it should be a three-letter ISO code (e.g., <code>usd</code>).</li>
<li><code>NEXT_PUBLIC_URL</code> – <strong>Optional on Vercel</strong> The address of your store without the trailing slash, i.e., <code>https://demo.yournextstore.com</code>. When building for the first time, you should set it to any valid URL, i.e. <code>http://localhost:3000</code>.</li>
</ul>
<details open="">
  <summary>
    
    <span aria-label="Video description yns-setup-env-variables.mp4">yns-setup-env-variables.mp4</span>
    <span></span>
  </summary>

  <video src="https://private-user-images.githubusercontent.com/200613/323450765-01d27f69-00dc-446e-bc81-5dea2587f346.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8yMDA2MTMvMzIzNDUwNzY1LTAxZDI3ZjY5LTAwZGMtNDQ2ZS1iYzgxLTVkZWEyNTg3ZjM0Ni5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwOTEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDkxMFQyMDMwMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMjVlNTZiOTg5YmEwOTY2OWZlMmIwMWE2YWNhMjYzN2YyNWQ1Y2NhZDA0NjY4ZDJkMTA3NDk5Y2NmOWRhMDRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.tg0ceNGP3hJ4xoYLEUc3vRA5aXvPqXyI57locveaYTY" data-canonical-src="https://private-user-images.githubusercontent.com/200613/323450765-01d27f69-00dc-446e-bc81-5dea2587f346.mp4?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjYwMDA1MTQsIm5iZiI6MTcyNjAwMDIxNCwicGF0aCI6Ii8yMDA2MTMvMzIzNDUwNzY1LTAxZDI3ZjY5LTAwZGMtNDQ2ZS1iYzgxLTVkZWEyNTg3ZjM0Ni5tcDQ_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwOTEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDkxMFQyMDMwMTRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMjVlNTZiOTg5YmEwOTY2OWZlMmIwMWE2YWNhMjYzN2YyNWQ1Y2NhZDA0NjY4ZDJkMTA3NDk5Y2NmOWRhMDRmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.tg0ceNGP3hJ4xoYLEUc3vRA5aXvPqXyI57locveaYTY" controls="controls" muted="muted">

  </video>
</details>

<p dir="auto"><h3 tabindex="-1" dir="auto">Optional Environment Variables</h3><a id="user-content-optional-environment-variables" aria-label="Permalink: Optional Environment Variables" href="#optional-environment-variables"></a></p>
<ul dir="auto">
<li><code>NEXT_PUBLIC_UMAMI_WEBSITE_ID</code> – Umami website ID for analytics</li>
<li><code>NEXT_PUBLIC_NEWSLETTER_ENDPOINT</code> – <strong>Preview</strong>: The endpoint for the newsletter form in the future. It should accept POST requests with a JSON <code>{ email: string }</code> and return JSON <code>{ status: number }</code>.</li>
<li><code>STRIPE_WEBHOOK_SECRET</code> – <strong>Preview</strong>: Stripe Webhook secret for handling events from Stripe. Read more below.</li>
<li><code>ENABLE_STRIPE_TAX</code> – <strong>Preview</strong>: Set to any value (i.e., <code>1</code>) to enable Stripe Tax in YNS. Read more below.</li>
<li><code>NEXT_PUBLIC_LANGUAGE</code> - The language of the store.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run the store</h2><a id="user-content-run-the-store" aria-label="Permalink: Run the store" href="#run-the-store"></a></p>
<p dir="auto">After following the above steps, run <code>pnpm install</code> to install the required dependencies, and then run <code>pnpm dev</code> to start the development server on your machine. Your Next Store will be available at <a href="http://localhost:3000/" rel="nofollow">localhost:3000</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Add products</h2><a id="user-content-add-products" aria-label="Permalink: Add products" href="#add-products"></a></p>
<p dir="auto">Your Next Store gets all the products, prices, descriptions, and categories from Stripe. So, if you know Stripe already, you'll feel right at home!</p>
<p dir="auto">You need to add products to the Stripe Dashboard to show in YNS. After logging in, click <strong>More</strong> in the left sidebar and select <strong>Product catalogue</strong>. You may also use the direct link:</p>
<ul dir="auto">
<li>In <strong>Test Mode</strong>: <a href="https://dashboard.stripe.com/test/products" rel="nofollow">dashboard.stripe.com/test/products</a></li>
<li>In <strong>Production Mode</strong>: <a href="https://dashboard.stripe.com/products" rel="nofollow">dashboard.stripe.com/products</a></li>
</ul>
<p dir="auto">Then, click on <strong>Add product</strong> and fill in all the required information:</p>
<ul dir="auto">
<li>name,</li>
<li>description,</li>
<li>price – currently, only <em>One-off</em> payments are supported,</li>
<li>a product image.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Metadata</h3><a id="user-content-metadata" aria-label="Permalink: Metadata" href="#metadata"></a></p>
<p dir="auto">Additionally, Your Next Store uses product metadata to provide more context information about the products. You can specify the following metadata fields:</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Field</th>
<th>Required</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>slug</code></td>
<td>Yes</td>
<td>The product slug is used for URLs. Needs to be unique except for variants.</td>
</tr>
<tr>
<td><code>category</code></td>
<td>No</td>
<td>The product category used for grouping products.</td>
</tr>
<tr>
<td><code>order</code></td>
<td>No</td>
<td>The product order used for sorting products. Lower numbers are displayed first.</td>
</tr>
<tr>
<td><code>variant</code></td>
<td>No</td>
<td>The product variant slug. Read below for details.</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto">Now you should see all added products in Your Next Store.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Variants</h2><a id="user-content-variants" aria-label="Permalink: Variants" href="#variants"></a></p>
<p dir="auto">Your Next Store supports simple product variants. To create a product with variants, you must add multiple products to Stripe with the same <code>slug</code> metadata field. YNS uses the <code>variant</code> metadata field to distinguish between different variants of the same product. For example, if you have a T-shirt in multiple sizes, you can create three products with the <code>slug</code> of <code>t-shirt</code> and <code>variant</code> values of <code>small</code>, <code>medium</code>, and <code>large</code>.</p>
<p dir="auto">Variants are displayed on the product page. Variants can have different prices, descriptions, and images. It's important to note that the <code>category</code> should be the same for all variants of the same product for the best browsing experience.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">In the future, we plan to add the possibility of editing products and variants inside a built-in admin dashboard. If you have any ideas or suggestions, please let us know!</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stripe Webhooks</h2><a id="user-content-stripe-webhooks" aria-label="Permalink: Stripe Webhooks" href="#stripe-webhooks"></a></p>
<p dir="auto">Your Next Store uses Stripe Webhooks to handle events from Stripe. Currently, the endpoint is used to automatically revalidate cart page and to create tax transaction (if enabled). To set up Webhooks, follow the Stripe docs. The exact steps depend on whether you've activated Stripe Workbench in your Stripe account: <a href="https://docs.stripe.com/webhooks#add-a-webhook-endpoint" rel="nofollow">docs.stripe.com/webhooks#add-a-webhook-endpoint</a>.</p>
<p dir="auto">The endpoint for the webhook is <code>https://{YOUR_DOMAIN}/api/stripe-webhook</code>. The only required event is <code>payment_intent.succeeded</code>. When the webhook is configured in Stripe, set the <code>STRIPE_WEBHOOK_SECRET</code> environment variable to the secret key created by Stripe.</p>
<div dir="auto"><p dir="auto">Note</p><p dir="auto">In the future, we plan to add more events to the webhook to improve the user experience.</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stripe Tax</h2><a id="user-content-stripe-tax" aria-label="Permalink: Stripe Tax" href="#stripe-tax"></a></p>
<p dir="auto">Your Next Store comes with a preview of Stripe Tax support. To enable it, set the <code>ENABLE_STRIPE_TAX</code> environment variable to any value (i.e., <code>1</code>).</p>
<p dir="auto">For this feature to work, you must set your Tax settings in Stripe Dashboard: <a href="https://dashboard.stripe.com/register/tax" rel="nofollow">dashboard.stripe.com/register/tax</a>. When enabled and configured, taxes will be automatically calculated and added to the total price of the product based on:</p>
<ul dir="auto">
<li>product pricing - tax can be inclusive or exclusive</li>
<li>product tax code</li>
<li>customer's address</li>
<li>customer's tax ID</li>
</ul>
<div dir="auto"><p dir="auto">Warning</p><p dir="auto">This feature is still in the early stage, and there could be edge cases that are not supported. We're actively working on it, so if you encounter any problems or have any suggestions, please let us know!</p>
</div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Production Deployment</h2><a id="user-content-production-deployment" aria-label="Permalink: Production Deployment" href="#production-deployment"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Vercel</h3><a id="user-content-vercel" aria-label="Permalink: Vercel" href="#vercel"></a></p>
<p dir="auto">To deploy on Vercel, click the following button, set up your GitHub repository and environment variables, and click <strong>Deploy</strong>. Make sure to set the <code>ENABLE_EXPERIMENTAL_COREPACK</code> variable to <code>1</code>.</p>
<p dir="auto"><a href="https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fyournextstore%2Fyournextstore&amp;env=ENABLE_EXPERIMENTAL_COREPACK,NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY,STRIPE_SECRET_KEY,STRIPE_CURRENCY&amp;envDescription=Read%20more%20about%20required%20env%20variables%20in%20YNS&amp;envLink=https%3A%2F%2Fgithub.com%2Fyournextstore%2Fyournextstore%2Ftree%2Fupcoming%3Ftab%3Dreadme-ov-file%23add-environmental-variables&amp;project-name=yournextstore&amp;repository-name=yournextstore&amp;demo-title=Your%20Next%20Store&amp;demo-description=A%20Next.js%20boilerplate%20for%20building%20your%20online%20store%20instantly%3A%20simple%2C%20quick%2C%20powerful.&amp;demo-url=https%3A%2F%2Fdemo.yournextstore.com%2F&amp;demo-image=https%3A%2F%2Fyournextstore.com%2Fdemo.png" rel="nofollow"><img src="https://camo.githubusercontent.com/20bea215d35a4e28f2c92ea5b657d006b087687486858a40de2922a4636301ab/68747470733a2f2f76657263656c2e636f6d2f627574746f6e" alt="Deploy with Vercel" data-canonical-src="https://vercel.com/button"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Your Own VPS</h3><a id="user-content-your-own-vps" aria-label="Permalink: Your Own VPS" href="#your-own-vps"></a></p>
<p dir="auto">Description coming soon.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Docker</h3><a id="user-content-docker" aria-label="Permalink: Docker" href="#docker"></a></p>
<p dir="auto">Description coming soon.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">That's all</h2><a id="user-content-thats-all" aria-label="Permalink: That's all" href="#thats-all"></a></p>
<p dir="auto">YNS evolves each day, and we actively seek feedback on what to improve. If you have any questions or problems, don't hesitate to get in touch with us on our Discord Server.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Sometimes, you use <code>structuredClone</code> to pass data from server to client components. Why?</h3><a id="user-content-sometimes-you-use-structuredclone-to-pass-data-from-server-to-client-components-why" aria-label="Permalink: Sometimes, you use structuredClone to pass data from server to client components. Why?" href="#sometimes-you-use-structuredclone-to-pass-data-from-server-to-client-components-why"></a></p>
<p dir="auto">Only certain types of data can be passed from the server to the client directly. Data from Stripe SDK often contains class instances. To work around this, we use <code>structuredClone</code> to eliminate them and pass just plain old objects to the client.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How we made Jupyter notebooks load faster (125 pts)]]></title>
            <link>https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/</link>
            <guid>41500522</guid>
            <pubDate>Tue, 10 Sep 2024 13:19:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/">https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/</a>, See on <a href="https://news.ycombinator.com/item?id=41500522">Hacker News</a></p>
Couldn't get https://www.singlestore.com/blog/how-we-made-notebooks-load-10-times-faster/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Multispectral Imaging and the Voynich Manuscript (119 pts)]]></title>
            <link>https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/</link>
            <guid>41500406</guid>
            <pubDate>Tue, 10 Sep 2024 13:05:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/">https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/</a>, See on <a href="https://news.ycombinator.com/item?id=41500406">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
<p>(with thanks to René Zandbergen, Ray Clemens, <a href="https://www.rit.edu/directory/rlepci-roger-easton-jr" target="_blank" rel="noreferrer noopener">Roger Easton</a>, <a href="https://ling.yale.edu/people/claire-bowern" target="_blank" rel="noreferrer noopener">Claire Bowern</a>, <a href="https://bill.oucreate.com/" target="_blank" rel="noreferrer noopener">Bill Endres</a>, and the curatorial and conservation staff at the Beinecke Rare Book &amp; Manuscript Library)</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg"><img data-attachment-id="3090" data-permalink="https://manuscriptroadtrip.wordpress.com/2015/01/16/manuscript-road-trip-a-new-year-in-new-haven/voynich-detail/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg" data-orig-size="534,556" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="voynich detail" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=288" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=500" tabindex="0" role="button" width="534" height="556" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2015/01/voynich-detail.jpg?w=534" alt=""></a></figure></div>


<p>I have some exciting news for all of you <a href="https://manuscriptroadtrip.wordpress.com/2015/01/17/manuscript-road-trip-the-worlds-most-mysterious-manuscript/" target="_blank" rel="noreferrer noopener">Voynich </a>fans! But first, some background about multispectral imaging…</p>



<p>Multispectral imaging is a way of capturing a digital image using non-visible wavelengths such as ultraviolet and infrared (click <a href="https://hmml.org/stories/seeing-invisible-multispectral-imaging-ancient-medieval-manuscripts/" target="_blank" rel="noreferrer noopener">here </a>to learn more). Where medieval manuscripts are concerned, UV imaging in particular can make faded or effaced text legible. This is because most medieval inks (including that used to write the Voynich Manuscript) have a significant iron component. This allows the ink to “bite” into the surface of the parchment rather than sliding off of it. When ink is scraped away or fades, the molecular bond remains, and the faded text may therefore fluoresce when exposed to UV bandwidths. This technology has proven invaluable in helping scholars read palimpsests and damaged manuscripts such as the <a href="https://www.archimedespalimpsest.org/" target="_blank" rel="noreferrer noopener">Archimedes Palimpsest</a> and the <a href="https://mizanproject.org/the-syriac-galen-palimpsest/" target="_blank" rel="noreferrer noopener">Syriac Galen Palimpsest</a>. Could such imaging of the <a href="https://beinecke.library.yale.edu/collections/highlights/voynich-manuscript" target="_blank" rel="noreferrer noopener">Voynich Manuscript</a> help reveal its secrets?</p>



<p>Back in 2014, while working on a <a href="https://news.yale.edu/2015/06/11/hidden-secrets-yale-s-1491-world-map-revealed-multispectral-imaging" target="_blank" rel="noreferrer noopener">different imaging project</a> at Yale University’s Beinecke Library, the imaging team from <a href="https://lazarusprojectimaging.com/">The Lazarus Project</a> (Michael Phelps (Early Manuscripts Electronic Library), Gregory Heyworth (then at University of Mississippi, now at University of Rochester), Chet Van Duzer (independent map scholar), Ken Boydston (Megavision) and Roger Easton (Rochester Institute of Technology)) was granted permission by the Library to take multispectral images of ten select pages of the Voynich Manuscript (a.k.a. Beinecke Library MS 408): 1r, 8r, 17r, 26r, 47r, 70v1, 71r, 93r, 102v1, and 116v. The intent was to make the images publicly available on the Yale website, but for various reasons (including staff turnover, development of Yale’s new image platform, library backlogs, and COVID) the images were never posted. Details of several images were published on pp. 31-32 of <em><a href="https://yalebooks.yale.edu/book/9780300217230/the-voynich-manuscript/" target="_blank" rel="noreferrer noopener">The Voynich Manuscript</a></em> (ed. Raymond Clemens), and a few have been explored by Voynich researchers (<a href="https://stephenbax.net/?p=1625" target="_blank" rel="noreferrer noopener">here </a>and <a href="http://ciphermysteries.com/2016/11/19/multispectral-images-voynich-f116v" target="_blank" rel="noreferrer noopener">here</a>, for example), but the full set of these MSI images has never been publicly seen or studied – until now.</p>



<p>On a whim, I wrote to Roger a few weeks ago to ask if he still had the images, and he very kindly sent them to me. I have been given permission to make all of the images public, and I am thrilled to announce that <strong>they may be viewed and freely downloaded <a href="https://drive.google.com/drive/folders/1mNQGKQDSCR4M_c2M2JrsU5soghvYwMig?usp=sharing" target="_blank" rel="noreferrer noopener">here</a>. </strong>(there may also have been images taken of a few other pages, but those are TBD)</p>



<p>[if you want to skip the technical details and jump straight to the good stuff, click <a href="#Image-analyses">here</a>]</p>



<p>There are four folders in the shared drive: “Lab_True_color_TIFF” (high-resolution TIFFs of some of the pages); “Processed_Images” (post-processed multispectral images); “Raw TIFFs” (enormous unprocessed multispectral 16-bit TIFFs in different color bands, not readable by most image viewers); and “RGB_true_color_JPEGs” (high-resolution JPGs of some of the pages). When using or referencing these images, please credit “The Lazarus Project and The Chester F. Carlson Center for Imaging Science at Rochester Institute of Technology” and cite the manuscript as “Beinecke Rare Book &amp; Manuscript Library MS 408,” including the particular folio number. For more information about image capture and post-processing, see the “Technical Details” section at the <a href="#Technical-details">end </a>of this post.</p>



<p>The really interesting images in the shared drive are in the folder labeled “Processed_Images.” In this context, “Processed” means that an imaging expert (Roger Easton, in this case) has applied complex color transformations to the raw 16-bit TIFFs in order to make them “legible” to the human eye. Because he has worked on so many MSI projects, he knows which transformations are most likely to be useful. For some of the pages, he made these transformations in September 2024 at my request, focusing on making particular areas of the images more legible (that’s why there are more images in some folders than others, as each version of the same page represents a different post-processing strategy).</p>



<p>I would advise you to approach these images with caution. Because processed multispectral images have unnatural color profiles, it is very easy to misread or misinterpret them. And we all know that where the Voynich Manuscript is concerned, excitement and enthusiasm sometimes inspire us to rush ahead without paying close enough attention to evidence and detail. The interpretation of MSI images requires time and care and what is sometimes called “slow looking.” To help you understand what these images are, and what they aren’t, I have turned to MSI expert and University of Oklahoma professor Bill Endres for some guidance:</p>



<blockquote>
<p>“While we normally don’t think about our vision as limited, it truly is. We only see a select range of the light spectrum, from about 380 to 720 nm (nanometers, the measurement for the lengths of a light wave). Conversely, bees see into the ultraviolet range and rattlesnakes see into the infrared, which allows them to hunt at night. If we had the eyes of bees, we wouldn’t need technological help and could likely read erased and damaged iron gall ink.</p>



<p>What makes the human eye limited is that it constructs color by collecting three different colors of light—red, green, and blue—and merges them together. To collect each color, the eye has a different type of cone. Human vision is impressive for its efficiency in generating color from a sampling of light, but this efficiency leaves out a tremendous amount of visual data.</p>



<p>Multispectral imaging benefits us because the sensor of a monochrome camera can capture a larger range of the light spectrum and individual frequencies of light (by imaging in the dark and using LED lighting to generate individual light frequencies). Capturing individual frequences is crucial. A page of a manuscript is a collection of parchment, ink, and pigment. Substances reflect and absorb individual frequencies of light differently. Multispectral imaging allows us to leverage those differences. Sometimes the differences are subtle. Post-processing highlights those subtle differences.”</p>
</blockquote>



<p>Now, let’s get to it.</p>



<p id="Image-analyses">Ten years after the images were captured, no one can quite remember why these ten specific pages were chosen for imaging. Some are obvious (such as 1r and 116v, the first and last pages), and others have features that the team thought might present valuable results. For the most part, they were right! Let’s take a close look at a few of them.*</p>



<p>* I haven’t explored the images of folios 8r and 47r here because I didn’t see anything of particular interest in those processed images. You’ll find them in the shared folder. Take a look; you may see something I missed!</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg"><img data-attachment-id="7622" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_004_image_0001-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg" data-orig-size="2697,3766" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_004_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=215" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=500" tabindex="0" role="button" width="733" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_004_image_0001-1.jpg?w=733" alt=""></a><figcaption><em>Yale University, Beinecke Rare Book &amp; Manuscript Library MS 408, f. 1r</em></figcaption></figure></div>


<p>It’s been known for more than a century that the first page of the manuscript includes an effaced inscription in the lower margin. Wilfrid Voynich himself observed that there was something written there:</p>



<blockquote>
<p>“When I brought the manuscript to America the margins of the first page had the appearance of being blank, but an accident to a photostatic reproduction of this page revealed the fact that an underexposure of the plate brings out a faded autograph in the lower margin. Chemicals were applied to the margins…” </p>



<p>– Wilfrid Voynich, “A Preliminary Sketch of the History of the Roger Bacon Cipher Manuscript,” in <em>Transactions of the College of Physicians and Surgeons of Philadelphia </em>Third Series, Vol. 33 (1921), pp. 415-30, at pp. 421-422.</p>
</blockquote>



<p>Around the year 1914, Voynich applied a chemical reagent to this page in an effort to make the inscription more visible. That explains the dark stain in the lower and outer margins. MSI makes the inscription legible, confirming Voynich’s reading of “Jacobi à Tepenecz,” a.k.a. Jacobus Sinapius, a Prague alchemist who likely owned the manuscript in the late 1500s or early 1600s. The animation below shows three different stages of the imaging of the manuscript: Voynich’s original image, taken around 1912 (before he applied the chemicals); the current state of the page in visible light (showing the staining caused by Voynich’s efforts); and the MSI image.</p>



		<figure>
			
			
			
		</figure>
		


<p>But the clear legibility of the Tepenecz inscription isn’t even the most exciting outcome of the imaging of this page. For several decades, Voynich researchers have noted what appears to be a Roman alphabet written in the right-hand margin. In the detail below, you can clearly see the letters a, b, c, d, and e.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg"><img data-attachment-id="7625" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/voynich_01r_bands01-12_rf_cal_r6g4b1-3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg" data-orig-size="967,2563" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1414065351&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Voynich_01r_bands01-12_RF_cal_R6G4B1" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=113" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=386" tabindex="0" role="button" width="386" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_01r_bands01-12_rf_cal_r6g4b1-1.jpg?w=386" alt=""></a></figure></div>


<p>In visible light, it appears that there may be other characters written to the right of these letters, but they cannot be easily discerned. Under ultraviolet light, the faded letters become perfectly legible. Not only that, but it turns out there are actually <em>three </em>columns of lettering, not just one! Although others have theorized that there <em>might </em>be more text to the right of the visible alphabet, the lettering has never been clearly seen or transcribed before. The letters are written in three parallel columns: the Roman alphabet (a-z), a series of Voynich characters, and another Roman alphabet offset by one letter. My preliminary transcription of these alphabets is shown below.</p>



<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png"><img data-attachment-id="7715" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-696-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png" data-orig-size="540,951" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (696)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=170" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=500" tabindex="0" role="button" loading="lazy" width="540" height="951" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-696.png?w=540" alt=""></a></figure></div>



<p>You might wonder how I was able to determine what was written under that dark oval blob near the top, which is a green leaf showing through from the other side of the folio (i.e. f. 1v). Once I realized that it was preventing me from seeing what’s in the margin there, I asked my friend Bill Endres, who is a post-processing genius, if he could “subtract” the show-through. He rose to the challenge!</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png"><img data-attachment-id="7711" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-691-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png" data-orig-size="1519,927" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (691)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="624" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-691.png?w=1024" alt=""></a></figure>



<p>Believe it or not, this actually helps me enormously. In the right-hand image, I can clearly see the letter [g] and enough of the Voynichese character to surmise which character it is.</p>



<p>Are these alphabets an early attempt to decode the manuscript? Perhaps. The two Roman alphabets are written in what paleographers call “Humanistic bookhand,” that is, the style of writing developed by Humanists like Petrarch and Boccaccio in Italy in the 14th century and used throughout Europe for several hundred years. I have carefully compared these letters to the handwriting of everyone known – or thought – to have been connected with the manuscript in the 16th and 17th century, including: Carl Widemann, his colleague Leonhard Rauwolf, Emperor Rudolf II, Jacobus Sinapius, Georg Baresch, Marcus Marci, and Athanasius Kircher. I even considered the handwriting of John Dee and (at Prof. Claire Bowern’s suggestion) his collaborator Edward Kelley, who were once (spuriously) thought to have been associated with the manuscript. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg"><img data-attachment-id="7834" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/jan_marcus_marci_00/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg" data-orig-size="1024,1568" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Jan_Marcus_Marci_00" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=196" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=500" tabindex="0" role="button" loading="lazy" width="669" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/jan_marcus_marci_00.jpg?w=669" alt=""></a><figcaption><em>Johannes Marcus Marci</em></figcaption></figure></div>


<p>One of these men is a very good match: Johannes Marcus Marci (1595-1667).</p>



<p>Note: I initially discounted Marci because I had compared the revealed alphabets to the <a href="https://collections.library.yale.edu/catalog/2041454" target="_blank" rel="noreferrer noopener">letter attributed to Marci</a> at the Beinecke. René Zandbergen reminded me that that letter isn’t written in Marci’s hand but by the secretary he is known to have worked with as his eyesight faded near the end of his life. He suggested that I take a closer look at the earlier autograph Marci correspondence linked from his <a href="https://www.voynich.nu/letters.html" target="_blank" rel="noreferrer noopener">website</a>. Let’s do it!</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg"><img data-attachment-id="7804" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/marci-hand/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg" data-orig-size="1524,2312" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Marci hand" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=198" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=500" tabindex="0" role="button" loading="lazy" width="675" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/marci-hand.jpg?w=675" alt=""></a><figcaption><em>Marcus Marci to Athanasius Kircher (12 September 1640)<br>Rome, Pontificia Università Gregoriana, APUG 557, fol. 127r</em></figcaption></figure></div>


<p>The best way to determine if a script is a match is by comparing each letterform in the unknown sample (the revealed alphabet) to a known sample (in this case, the letter written from Marci to Athanasius Kircher on 12 September 1640, shown at left and discussed <a href="https://www.voynich.nu/letters.html">here</a>). For those of you unversed in Voynich lore, Marci was a doctor in Prague who inherited the manuscript from his friend, alchemist Georg Baresch, upon Baresch’s death in 1662. Marci sent it to Rome as a gift to Athanasius Kircher in 1665, confident that Kircher would be able to make sense out of it. The manuscript stayed in Rome until Voynich acquired it in 1912 (that’s the short version…for more, see René’s Voynich <a href="https://voynich.nu/">website</a>).</p>















<p>Let’s take a closer look, letter by letter. Time for some hardcore paleography!</p>










<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png"><img data-attachment-id="7807" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/1r-vs-marci-12-09-1640/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png" data-orig-size="1304,758" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="1r vs Marci 12.09.1640" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="595" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/1r-vs-marci-12.09.1640.png?w=1024" alt=""></a><figcaption><em>Comparison of letterforms: Marcus Marci vs. revealed alphabet</em></figcaption></figure></div>


<p>There are several very strong markers that help make a convincing case for identifying this script as Marci’s:</p>



<ol>
<li>The loop-less [b], [d], [f], [h], [p], [q], [s], and [y]: During this period, many hands write prominent loops on the ascenders or descenders of these letters, loops that are lacking in both samples;</li>



<li>The open-bowl [g]: Marci doesn’t always leave his [g]s open, but he sometimes does;</li>



<li>The [m] with a first stroke that is taller than the last, giving the impression that the letter shrinks from left to right;</li>



<li>The shape of the [z]: because [z] is a relatively rare letter, its shape and ductus tend to be distinctive in any particular hand. The example above is ligated with [t] at the end of a word, so the context is different, but the [3] shape is quite similar.</li>
</ol>



<p>Notes: [g] and [h] for the revealed text are taken from the third column, since they aren’t legible in the first. Letters that aren’t legible in either alphabet have not been considered. It is also important to note that in his correspondence, Marci’s script has a prominent slant that is lacking in the revealed alphabets – however, because the alphabets are made up of decontextualized single letters, I would not necessarily expect the same slant as in the handwritten documents. The letterforms themselves, in their shapes and ductus (sequence of strokes) are very similar.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1.jpg"><img data-attachment-id="7777" data-permalink="https://manuscriptroadtrip.wordpress.com/screenshot-678-3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg" data-orig-size="399,446" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (678)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg?w=268" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg?w=399" tabindex="0" role="button" loading="lazy" width="399" height="446" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-678-1-edited.jpg" alt=""></a><figcaption><em>“Majuscules” on f. 1r</em></figcaption></figure></div>


<p>As for the Voynichese column, all of those characters are in common use throughout the manuscript except for the last two (next to [y] and [z], in blue at left). They appear in my transcription as cropped details rather than typographical characters because they appear nowhere else in the manuscript. There are at least sixteen leaves missing from the codex. It is quite possible, likely even, that these two unusual characters were found on one of the missing leaves. The style of the letters identifies them as what Voynichologists call “capitals” or “majuscules” out of expedience and convention (what they ACTUALLY are is unknown). These characters are now only found on the first page, and they are clearly different from the glyphs at the bottom of the second column of revealed symbols. There’s another character in this style at the top of the right margin (shown at the lower right in the mosaic above), but its purpose, too, cannot yet be determined.</p>



<p>The Voynichese sequence (we don’t actually know if it is meant to be an “alphabet”) does not correspond with original sequences on <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006171" target="_blank" rel="noreferrer noopener">f. 49v</a>, <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006187" target="_blank" rel="noreferrer noopener">f 57v</a>, or <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006192" target="_blank" rel="noreferrer noopener">f. 66r</a> (with thanks to Yale University Professor of Linguistics Claire Bowern for this observation), and the symbols aren’t in any obvious order. A few common glyphs aren’t included at all (such as the one-loop two-legged gallows glyph known as [k]), although some of the symbols in the second column cannot be clearly discerned as of yet. Additional post-processing may help clarify such ambiguities.</p>



<p>The purpose of these three vertical alphabets is not at all clear. Knowing what we know about Marci’s timeline, they must have been added between 1662 and 1665, when he owned of the manuscript. They may represent an early attempt to decode the manuscript using two different substitution ciphers, or Marci may have been using Voynich characters to create a cipher of his own. Regardless of their purpose, I do know one thing: these alphabets will likely <em>not </em>help us actually decipher the manuscript. This is because linguists like Claire and other researchers have established that the manuscript is almost certainly not encrypted using a simple substitution cipher, and the substitutions in these columns result in nonsense anyway. Even so, they do add an interesting and new chapter to the early history of the manuscript. I look forward to hearing from other researchers about this new evidence, especially from experts in cryptography who may have ideas about why Marci or any other early-modern decrypter would need three columns of alphabets to do their work.</p>



<p>There is one more noteworthy feature of the imaging of folio 1r. Under ultraviolet light, several Voynichese characters that cannot be read under visible light become legible. For Voynichologists, any new textual evidence – no matter how little – is significant, as it adds data to the analysis of the text. More data leads to more detailed analytics, and more detailed analytics may lead to the ability to “read” this mysterious manuscript at last.</p>



		<figure>
			
			
			
		</figure>
		

<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg"><img data-attachment-id="7657" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg" data-orig-size="4507,6985" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1414425086&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Voynich_070v_bands01-22_RF+FL_cal_med3_R14G16B18_FL-B" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=194" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=500" tabindex="0" role="button" loading="lazy" width="661" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_070v_bands01-22_rffl_cal_med3_r14g16b18_fl-b.jpg?w=661" alt=""></a><figcaption><em>BRBL MS 408, f. 70v1</em></figcaption></figure></div>


<p>The image of folio 70v1 (right) is instructive as it demonstrates a critical caveat where these and other images are concerned: the interpretation of multispectral images requires patience and care. It is extremely important to distinguish between an offset (i.e. a mirror image left when a book is closed for centuries and ink or pigment from one page rubs off onto the facing page), show-through (a mirror-image ghost of text on the other side of the page), and erased ink (non-reversed faded text made visible). It is very easy to be misled by anything you may think you see in an image like this, and when examing these MSI images you should always compare your findings with the facing page, the other side of the leaf, and the visible-light version of the image to be sure you aren’t being led astray or leaping to unfounded conclusions. For example, the MSI image of folio 70v1 seems to have Voynichese writing that appears in pale blue in this multispectral image (see, e.g., the top of the diagram between the two outer rings of text). Is this an offset from the facing page, show-through from the other side, or hidden/revealed text?</p>



<p>Let’s take a closer look.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png"><img data-attachment-id="7674" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-684/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png" data-orig-size="1811,579" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (684)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="327" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-684.png?w=1024" alt=""></a><figcaption><em>BRBL MS 408, f. 70v1, MSI detail</em></figcaption></figure></div>


<p>We can immediately exclude the blue text as having been hidden/revealed, because it is inverted. So it must be either an offset from the facing page (<a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006202" target="_blank" rel="noreferrer noopener">f. 71r</a>) or show-through from the other side (f. 70r1, the central panel of <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006199" target="_blank" rel="noreferrer noopener">this </a>foldout). If you invert the MSI image and compare it to the analogous sections of f. 70r1, you can clearly see that the faded blue text on 70v1 matches the text on 70r1, identifying these ghostly blue letters as show-through:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg"><img data-attachment-id="7603" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/msi-results3/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg" data-orig-size="1280,720" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="MSI results3" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="576" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/msi-results3.jpg?w=1024" alt=""></a></figure>



<p>This is why it is so important to look at the surrounding pages before drawing conclusions about what you see in a multispectral image. <em>Caveat spectator!</em></p>



		<figure>
			
			
			
		</figure>
		


<p>Folio 71r (above) is interesting because it is the only zodiac page with significant color. It seems likely that the color was added later, but we don’t yet know for sure. What’s intriguing about the multispectral image is how similar colors respond to the exposure and processing in drastically different ways – the torquoise glows bright yellow, while the green appears light blue. This suggests that it might be worthwhile to test those pigments using X-Ray Fluorescence, which uses a spectrometer to analyze the chemical compounds that make up a mineral pigment (for more on this technique, see <a href="https://manuscriptroadtrip.wordpress.com/2019/08/28/manuscript-road-trip-you-cant-argue-with-science/" target="_blank" rel="noreferrer noopener">this </a>blogpost). XRF testing was conducted on selected pages of the Voynich in 2009, but f. 71r was not one of the tested pages. The results of those tests can be read <a href="https://beinecke.library.yale.edu/sites/default/files/files/voynich_analysis.pdf" target="_blank" rel="noreferrer noopener">here</a> (tl/dr: the tests did not find anything suspicious or out of the ordinary; all of the tested pigments were consistent with medieval recipes).</p>



<p>Folio 26r presents a variety of MSI revelations. Here, we can see all three types of evidence: hidden/revealed, offset, and show-through. To demonstrate this, we need to look at three pages at once: 25v, 26r, and 26v:</p>



		<figure>
			
			
			
		</figure>
		


<p>In the red square, you can see a mirror-image offset of the leaf on f. 25v, the facing page. The fact that the offset is visible only with multispectral exposure suggests that there may be more offsets hiding elsewhere in the manuscript that could provide evidence of the original sequence of leaves in the codex. In the blue rectangle, you can easily discern the show-through from the other side (i.e. f. 26v), although it’s also visible in natural light. Generally speaking, the green pigment in the Voynich Manuscript tends to show through the parchment more dramatically than other pigments, due to its high copper signature (as identified in the XRF report I referenced above). This corrosive aspect can cause the pigment to leach deeply into the parchment, making it easily visible from the other side. Finally, there’s a mysterious and as-yet-unexlained semi-circular something in the yellow box, which is within – although not necessarily related to – the waterstain in the upper margin. It isn’t an offset, and it isn’t show-through. It was hidden and revealed, although exactly what it IS remains to be determined.</p>



		<figure>
			
			<figcaption></figcaption>
			
		</figure>
		


<p>Folio 93r (above) may have been selected for imaging because of the stain that seems to match the color of the flower. The stain and the flower respond to the exposure in identical ways, suggesting that the stain is indeed the same pigment as the flower and was likely the result of a careless spill while the artist was working. It is noteworthy that the text is written over the stain. This confirms what study of other pages reveals – that the images were drawn and colored by the artists before the text was written by the scribes (there’s actually some evidence that the artists and scribes were the same people but that’s a topic for another day). Apparently the artist/scribe wasn’t worried about the stain interfering with the text. Parchment is a valuable resource and it would not be surprising if a decision had been made to use the stained parchment rather than discard it.</p>



<p>There are a few areas of interest on f. 101v2, all of which can be seen more clearly with MSI. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg"><img data-attachment-id="7698" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/e7-ev-with-focal-plane-at-88-cm-apo-chromat-120mm-dual-filter-wheel/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg" data-orig-size="6132,8176" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;12.5&quot;,&quot;credit&quot;:&quot;15th-16th&quot;,&quot;camera&quot;:&quot;E7 SN:26R01339\/Lens: APO-DIGITAR 5,6\/120 M-26  (0.9630 secs)&quot;,&quot;caption&quot;:&quot;Main banks\nTransmissive&quot;,&quot;created_timestamp&quot;:&quot;1408445603&quot;,&quot;copyright&quot;:&quot;Beinecke Rare Book and Manuscript Library, Yale University&quot;,&quot;focal_length&quot;:&quot;120&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.963&quot;,&quot;title&quot;:&quot;E7 EV with focal plane at 88 cm \nApo Chromat 120mm \ndual filter wheel&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="E7 EV with focal plane at 88 cm 
Apo Chromat 120mm 
dual filter wheel" data-image-description="" data-image-caption="<p>Main banks<br />
Transmissive</p>
" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=225" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=500" tabindex="0" role="button" loading="lazy" width="768" height="1024" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/voynich_102v1_psc.jpg?w=768" alt=""></a><figcaption><em>BRBL MS 408, f. 101v2</em></figcaption></figure></div>


<p>Here, post-processing helps reveal the drawing beneath the rust-colored stain, the obscured text on the blue portion of the vessel, and the faded text on the heavily-damaged fold:</p>



		<figure>
			
			
			
		</figure>
		


<p>We’ve got one more page to look at: the all-important folio 116 verso, the very last page.</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg"><img data-attachment-id="7605" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_207_image_0001/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg" data-orig-size="1090,1500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_207_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=218" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=500" tabindex="0" role="button" loading="lazy" width="744" height="1023" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001.jpg?w=744" alt=""></a><figcaption><em>BRBL MS 408, f. 116v</em></figcaption></figure></div>


<p>The lines of text and marginal doodles at the top are what make this page so interesting:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg"><img data-attachment-id="7607" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/beinecke_dl_2002046_page_207_image_0001-2/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg" data-orig-size="902,380" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Beinecke_DL_2002046_Page_207_Image_0001" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=500" tabindex="0" role="button" loading="lazy" width="902" height="380" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/beinecke_dl_2002046_page_207_image_0001-1.jpg?w=902" alt=""></a></figure>



<p>The writing is similar to, but isn’t quite, Voynichese. It has Germanic features and looks roughly contemporary with the manuscript, although it might be a bit later. There is a similar inscription in the upper margin of <a href="https://collections.library.yale.edu/catalog/2002046?child_oid=1006106" target="_blank" rel="noreferrer noopener">f. 17r</a>, likely written by the same hand. No one really knows what these inscriptions  represent, or when they were written, or where, or why. Some have speculated that the text on f. 116v might be the key to deciphering the manuscript itself. Others interpret it as an incantation or charm. Believe it or not (since I seem to have a lot of opinions about this manuscript), I don’t have strong feelings about what this text might signify. But others do, and they will find these processed images extremely useful. MSI can definitely help clarify what we’re looking at:</p>



<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg"><img data-attachment-id="7610" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-675/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg" data-orig-size="2092,758" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (675)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="371" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-675.jpg?w=1024" alt=""></a></figure>



<p>There’s quite a lot of “noise” because of show-through from f. 116r (the lines of mirror-writing in shades of brown), but even so it is possible to clarify some readings of this mysterious text. </p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg"><img data-attachment-id="7627" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-679/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg" data-orig-size="1099,544" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (679)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="506" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-679.jpg?w=1024" alt=""></a></figure></div>


<p>At the very top, for example, the first, third, and fourth words have sometimes been thought to begin with the letter [p], but in the MSI image it appears that the 3rd and 4th words begin with a different letter, perhaps [u] or [v]: the very faint lines that appeared to be descending from those first letters in natural light turn out upon MSI capture to be stains of some kind, not ink. This is the kind of minutia that can actually be extremely important to researchers.</p>



<p>The processed multispectral images of f. 17r may also help researchers interpret the writing in the upper margin of that page:</p>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png"><img data-attachment-id="7735" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-698/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png" data-orig-size="2058,406" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (698)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=300" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=500" tabindex="0" role="button" loading="lazy" width="1024" height="202" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-698.png?w=1024" alt=""></a><figcaption><em>f. 17r, natural light vs. processed MSI (detail)</em></figcaption></figure></div>


<p>I will leave it to those working on these pages to dig deeper into the interpretation of these images.</p>



<p>A few final thoughts:</p>



<ol>
<li>These images do not show any evidence of palimpsesting. In other words, there is no evidence of underwriting that would indicate reuse of the parchment. This is important because if there HAD been underwriting, it would have been critical evidence for refining the date of origin of the manuscript. The question of the date of origin of the manuscript is not entirely resolved, but <a href="https://drive.google.com/file/d/1qLk48161VaACWDUptH6rC9zlZzDHmGFS/view?usp=sharing" target="_blank" rel="noreferrer noopener">Carbon-14 testing</a> dates the parchment, with a high level of confidence, to ca. 1425. The style of the illustrations is consistent with that date, so I consider the manuscript to have been written in the early fifteenth century, although not everyone agrees.</li>



<li>The hidden/revealed marginal texts on f. 1 support the authenticity of the manuscript as a medieval object, as opposed to a modern forgery. Here’s why. Imagine you are an early 20th-century forger trying to create an authentic-looking manuscript to dupe unsuspecting buyers (or so the argument goes). You find some unused medieval parchment, mix up some ink and pigments using medieval recipes, and get to work. You might even think to add an early-modern signature and annotations to the margins to add to the air of authenticity. But would you then fade those annotations (how would you manage that, anyway?), pour chemicals over them, and then hope that someday imaging technologies would develop that would allow future researchers to read them? Of course not. That line of reasoning defies both logic and practicality. It is much more likely that the manuscript is exactly what most believe it to be: an authentic early fifteenth-century book with traces of its history left behind by past owners and readers. </li>



<li>In the end, these particular images provide additional textual and historical evidence, but they do not provide a key to “reading” the Voynich Manuscript. They function instead as a clear proof-of-concept, indicating that more imaging of more pages would almost certainly result in additional evidence invisible to the naked eye. Such evidence could help researchers reconstruct the original order of the leaves, transcribe faded Voynichese for linguistic and cryptological analysis, or reveal the identity of previously-unknown readers and owners. Additional MSI might even uncover the key to understanding this most mysterious of manuscripts. It is possible that the entire manuscript will be imaged this way someday, but that remains to be seen. Any kind of imaging poses a risk to the manuscript due to heat and light exposure as well as the potential for physical damage (the fold-outs, for example, are extremely fragile). Although current imaging technology carries significantly less risk than that used a decade ago, the conservators and curators (and lawyers and insurance adjusters) at the Beinecke Library are the ones who are ultimately responsible for the care and survival of this amazing object. They will need to decide if the potential for research and discovery is worth the risk. It’s their call. (please don’t bug them about this – they already know!)</li>
</ol>


<div>
<figure><a href="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png"><img data-attachment-id="7791" data-permalink="https://manuscriptroadtrip.wordpress.com/2024/09/08/multispectral-imaging-and-the-voynich-manuscript/screenshot-701/" data-orig-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png" data-orig-size="667,682" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Screenshot (701)" data-image-description="" data-image-caption="" data-medium-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=293" data-large-file="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=500" tabindex="0" role="button" loading="lazy" width="667" height="682" src="https://manuscriptroadtrip.wordpress.com/wp-content/uploads/2024/09/screenshot-701.png?w=667" alt=""></a></figure></div>


<p>I’ve here recorded only my preliminary thoughts about these images, and I am certain there is much more to discover. I look forward to seeing how you, and the community of Voynichologists, contribute to their interpretation. <a href="https://drive.google.com/drive/folders/1mNQGKQDSCR4M_c2M2JrsU5soghvYwMig?usp=sharing" target="_blank" rel="noreferrer noopener">Let’s get to work</a>!</p>



<p>p.s. In other Voynich news, I was profiled in the September 2024 issue of <a href="https://www.theatlantic.com/magazine/archive/2024/09/decoding-voynich-manuscript/679157/?gift=YFkW3a8mqv4T0YBMneIYIui0ufgYLbKBD8uwiL8lkU0&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share" target="_blank" rel="noreferrer noopener">The Atlantic</a> with an essay focusing on my thirty-year relationship with the Voynich Manuscript. I hope you enjoy it!</p>



<hr>



<p id="Technical-details"><strong>Technical Details</strong></p>



<p>The conditions under which each image was captured can be found in the “properties” metadata and are recorded in each pseudo-color image’s filename. According to Easton (in private correspondence), the filenames are “from the original bands after ‘calibration’ (to remove the effects of the different exposure times for different bands). This is done by measuring the reflectance of the reference ‘Spectralon’ reflector in the image. Spectralon is a Teflon derivative that has very uniform reflectivity for wavelengths in the range 250 nanometers (ultraviolet) to 2500 nanometers (infrared). Before rendering the pseudocolor image, the median of the 3×3 neighborhood surrounding each pixel was evaluated to attenuate the visibility of any statistical variations, which are particular problems in the fluorescence bands, because the number of available photons is pretty small at each pixel.”</p>



<p>“MNF” in a filename “means that the image bands were combined based on multispectral image statistics using the ‘minimum noise fraction’ algorithm. A triplet of the resulting bands was selected ‘by eye’ to render a pseudocolor image, and these bands were occasionally manipulated in PhotoShop to change the ‘hue angle’ (color tint) of the rendering, with the goal of enhancing the visibility of the text(s) of interest.” In other words, the images must be processed in order to be of greatest use to the non-expert eye. During that post-processing, Easton informs me, the goal is “to find combinations of the image bands which produce images with enhanced visibility of the erased text. The combinations may be selected based on observation and used for every leaf, or we may evaluate the multiband statistics of each pixel and use them to evaluate combinations of input bands — this is what I do in ‘principal component analysis’ (‘PCA’), Independent component analysis (‘ICA’), ‘minimum noise fraction’ (MNF), and ‘spectral angle mapping’ (SAM).”</p>




	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Debugging in the Multiverse (174 pts)]]></title>
            <link>https://antithesis.com/blog/multiverse_debugging/</link>
            <guid>41500405</guid>
            <pubDate>Tue, 10 Sep 2024 13:05:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antithesis.com/blog/multiverse_debugging/">https://antithesis.com/blog/multiverse_debugging/</a>, See on <a href="https://news.ycombinator.com/item?id=41500405">Hacker News</a></p>
Couldn't get https://antithesis.com/blog/multiverse_debugging/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[We're in the brute force phase of AI – once it ends, demand for GPUs will too (129 pts)]]></title>
            <link>https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/</link>
            <guid>41500268</guid>
            <pubDate>Tue, 10 Sep 2024 12:52:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/">https://www.theregister.com/2024/09/10/brute_force_ai_era_gartner/</a>, See on <a href="https://news.ycombinator.com/item?id=41500268">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>AI techniques that require specialist hardware are "doomed," according to analyst firm Gartner's chief of research for AI Erick Brethenoux – who included GPUs in his definition of endangered kit.</p>
<p>Speaking to <i>The Register</i> at Gartner's Symposium in Australia today, Brethenoux said in the 45 years he's spent observing AI, numerous hardware vendors have offered offer specialist kit for AI workloads. All failed once vanilla machines could do the job – as they always eventually can.</p>
<p>The need for specialist hardware, he observed, is a sign of the "brute force" phase of AI, in which programming techniques are yet to be refined and powerful hardware is needed. "If you cannot find the elegant way of programming … it [the AI application] dies," he added.</p>

    

<p>He suggested generative AI will not be immune to this trend.</p>

        


        

<p>The good news is, he believes organizations can benefit from AI without generative AI.</p>
<p>"Generative AI is 90 percent of the airwaves and five percent of the use cases," he noted – and users have already learned that lesson.</p>

        

<p>Brethenoux described the period from late 2022 to early 2024 as a "recess" in which IT shops "stopped thinking about things that make money" and explored generative AI instead. Those efforts have largely led orgs back to the AI they already use – or to "composite AI" that uses generative AI alongside established AI techniques like machine learning, knowledge graphs, or rule-based systems.</p>
<p>Organizations have realized that AI may already be making a big contribution to the business in many scenarios that engineers appreciate – such as machine learning informing predictive maintenance apps – but which never caught the eye of execs or the board. Recess is over.</p>
<p>An example of composite AI at work could be generative AI creating text to describe the output of a predictive maintenance application. <i>The Register</i> has often heard the same scenario applied to software that analyzes firewall logs and which now uses generative AI to make prose recommendations about necessary actions that improve security – and even writes new firewall rules to enact them.</p>

        

<p>Brethenoux recalled that some orgs he speaks to still think generative AI can power their next application. He often tells them the same outcome can be achieved more quickly – at lower cost – with an established AI technique.</p>
<ul>

<li><a href="https://www.theregister.com/2024/09/09/equinix_ai_business_case/">GenAI hype meets harsh reality as enterprises wrestle with business case</a></li>

<li><a href="https://www.theregister.com/2024/09/09/microsoft_arun_ulag_ai/">Microsoft exec warns of business functions being sacrificed on the altar of AI</a></li>

<li><a href="https://www.theregister.com/2024/09/09/gartner_synmposium_ai_opinion/">AI bills can blow out by 1000 percent: Gartner</a></li>

<li><a href="https://www.theregister.com/2024/09/05/amazon_q_developer_gartner/">Amazon congratulates itself for AI code that mostly works</a></li>
</ul>
<p>Gartner's Symposium featured another session with similar themes.</p>
<p>Titled "When not to use generative AI," it featured vice president and distinguished analyst Bern Elliot pointing out that Gen AI has no reasoning powers and produces only "a probabilistic sequence" of content. Even so, Elliot said Gen AI hype has reached two to three times the volume Gartner has seen for any previous tech. Generative AI is, in short, being asked to solve problems it was not designed to solve.</p>
<p>Elliot recommended not using it to tackle tasks other than content generation, knowledge discovery, and powering conversational user interfaces.</p>
<p>Even in those roles, he described the tech as "Unreliable like Swiss cheese: you know it has holes, you just don't know where they are until you cut it."</p>
<p>Elliot conceded that improvements to Gen AI have seen the frequency with which it "hallucinates" – producing responses with no basis in fact – fall to one or two percent. But he warned users not to see that improvement as a sign the tech is mature. "It's great until you do a lot of prompts – millions of hallucinations in production is a problem!"</p>
<p>Like Brethenoux, Elliot therefore recommended composite AI as a safer approach, and adopting guardrails that use a non-generative AI technique to check generative results. ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Free tool to find RSS feeds, even if not linked on the page (114 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41499905</link>
            <guid>41499905</guid>
            <pubDate>Tue, 10 Sep 2024 12:05:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41499905">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=41499905: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Tech Apple loses EU court battle over 13B euro tax bill in Ireland (461 pts)]]></title>
            <link>https://www.cnbc.com/2024/09/10/apple-loses-eu-court-battle-over-13-billion-euro-tax-bill-in-ireland.html</link>
            <guid>41498358</guid>
            <pubDate>Tue, 10 Sep 2024 08:03:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/09/10/apple-loses-eu-court-battle-over-13-billion-euro-tax-bill-in-ireland.html">https://www.cnbc.com/2024/09/10/apple-loses-eu-court-battle-over-13-billion-euro-tax-bill-in-ireland.html</a>, See on <a href="https://news.ycombinator.com/item?id=41498358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107432118" data-test="InlineImage"><p>Omar Marques | Sopa Images | Lightrocket | Getty Images</p></div><div><p>Europe's top court on Tuesday ruled against Apple in the tech giant's 10-year court battle over its tax affairs in Ireland.</p><p>The pronouncement from the European Court of Justice comes hours after Apple unveiled a swathe of <a href="https://www.cnbc.com/2024/09/09/apple-event-2024-live-updates-iphone-16-apple-watch-10.html">new product offerings</a>, looking to revitalize its iPhone, Apple Watch and AirPod line-ups.</p><p>CNBC has reached out to Apple for comment.</p><p>"The European Commission is trying to retroactively change the rules and ignore that, as required by international tax law, our income was already subject to taxes in the U.S.," the company said in a statement, according to Reuters.</p><p>Apple shares were down 1% in premarket trading at 09:52 a.m. London time.</p><p><a href="https://www.gov.ie/en/press-release/11bb0-government-notes-cjeu-judgment-in-apple-state-aid-case/" target="_blank">In a statement</a>, the Irish government said that the Apple case "involved an issue that is now of historical relevance only," adding that its position has always been that it "does not give preferential tax treatment to any companies or taxpayers."</p><p>The government noted it will now begin the process of transferring the assets in the escrow fund to Ireland.</p></div><h2><a id="headline0"></a>The case to date</h2><div><p>The Commission in turn appealed the General Court's decision, sending the litigation up to the ECJ.</p><p>The ECJ on Tuesday set aside the General Court's decision and confirmed the Commission's original 2016 ruling.</p><p>The case, which first began under outgoing competition chief Margrethe Vestager, highlights the continued conflict between U.S. tech giants and the EU, which has sought to tackle issues from data protection to taxation and antitrust.</p><p>This was not the last time that Apple found itself in the EU's crosshairs. Most recently, the Commission hit <a href="https://www.cnbc.com/2024/03/04/apple-hit-with-more-than-1point95-billion-eu-antitrust-fine-over-music-streaming.html">Apple with an antitrust fine of 1.8 billion euro ($1.99 billion)</a> in March for abusing its dominant position in the market for the distribution of music streaming apps.</p><p>Separately, the EU's sweeping <a href="https://www.cnbc.com/2024/03/07/what-the-eu-digital-markets-act-means-for-us-tech-giants-like-apple.html">Digital Markets Act</a> has forced <a href="https://www.cnbc.com/2024/03/12/apple-will-allow-iphone-app-downloads-from-websites-in-europe.html">companies </a>to change some of their practices in Europe. The Commission has opened <a href="https://www.cnbc.com/2024/03/25/eu-launches-probe-into-meta-apple-and-alphabet-under-sweeping-new-tech-law.html">various investigations under the DMA into tech giants</a>, including Apple, Alphabet and Meta.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Satellites Spotting Aircraft (137 pts)]]></title>
            <link>https://tech.marksblogg.com/ai-sar-satellites-umbra-aircraft-detection.html</link>
            <guid>41497377</guid>
            <pubDate>Tue, 10 Sep 2024 05:15:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/ai-sar-satellites-umbra-aircraft-detection.html">https://tech.marksblogg.com/ai-sar-satellites-umbra-aircraft-detection.html</a>, See on <a href="https://news.ycombinator.com/item?id=41497377">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Umbra Space is a 9-year-old manufacturer and operator of a Synthetic Aperture Radar (SAR) satellite fleet. These satellites can see through clouds, smoke, rain, snow and certain types of camouflage attempting to cover the ground below. SAR can capture images day or night at resolutions as fine as 16cm. SpaceX launched Umbra's first satellite in 2021 and has a total of ten in orbit at the moment.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra_satellite.jpg"><img alt="Umbras Satellite" src="https://tech.marksblogg.com/theme/images/umbra_satellite.jpg"></a></p><p>SAR collects images via radar waves rather than optically. The resolution of the resulting imagery can be improved the longer a point is captured as the satellite flies over the Earth. This also means video is possible as shown with this SAR imagery from another provider below.</p>
<p><a href="https://www.youtube.com/watch?v=cwDjJqtx_og"><img alt="Umbras Satellite Tasking" src="https://tech.marksblogg.com/theme/images/sar_video.gif"></a></p><p>Umbra has an open data programme where they share SAR imagery from 500+ locations around the world. The points on the map below are coloured based on the satellite that captured the imagery.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra_open_data_locations.jpg"><img alt="Umbras Open Data Locations" src="https://tech.marksblogg.com/theme/images/umbra_open_data_locations.jpg"></a></p><p>The subject matter largely focuses on manufacturing plants, airports, spaceports, large infrastructure projects and volcanos. Umbra state that the imagery being offered for free in their open data programme would have otherwise cost $4M to purchase.</p>
<p>Included in this feed are 140+ images of Bangkok Airport (BKK) that have been taken over the past two years. The image below links to the video I posted on LinkedIn.</p>
<p><a href="https://www.linkedin.com/posts/marklitwintschik_the-past-18-months-of-bangkoks-suvarnabhumi-activity-7217855129687887872-2cJN?utm_source=share&amp;utm_medium=member_desktop"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/bkk.gif"></a></p><div id="aircraft-in-sar-imagery">
<h2>Aircraft in SAR Imagery</h2>
<p>Aircraft at known airports transmitting ADS-B messages do a good job of telling the world where they are. <a href="https://umbra.space/pricing/">Paying ~$5K</a> for this sort of SAR imagery doesn't make a lot of sense. But aircraft that have turned their ADS-B transponders off and are parked on a public highway are a different matter.</p>
<p><a href="https://www.youtube.com/watch?v=5iE8TPwCocM"><img alt="F-35A on a highway in Finland" src="https://tech.marksblogg.com/theme/images/umbra-msfa/brave_VHRUVysRRU.jpg"></a></p><p>To add to this, it's rare in Northern Europe to have a clear sky regardless of the time of year. Clouds can be a real challenge here.</p>
<p>That said, some aircraft can be difficult to spot in SAR imagery. Below is Esri's satellite image of an Aircraft Boneyard.</p>
<p><a href="https://tech.marksblogg.com/theme/images/esri_aircraft_boneward.jpg"><img alt="Esri's Image of an Aircraft Boneyard" src="https://tech.marksblogg.com/theme/images/esri_aircraft_boneward.jpg"></a></p><p>Below is Umbra's image of the same location. Though it was taken on a different day and some aircraft might have been moved around, you can see that a lot of the aircraft in the bottom left are barely visible unless you zoom in very closely and pay attention to artefacts that give away a large man-made object is present.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra_aircraft_boneward.jpg"><img alt="Umbras SAR Image of an Aircraft Boneyard" src="https://tech.marksblogg.com/theme/images/umbra_aircraft_boneward.jpg"></a>
</p></div>
<div id="sardet-100k-msfa">
<h2>SARDet-100K &amp; MSFA</h2>
<p>In March, a <a href="https://arxiv.org/abs/2403.06534">paper</a> was published titled "SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection". It describes a new dataset called SARDet-100K and the Multi-Stage with Filter Augmentation (MSFA) framework for detecting objects in SAR imagery.</p>
<p>The dataset is made up of 117K SAR images with 246K objects annotated within them. These were sourced from ten existing datasets. The annotations highlight aircraft, bridges, cars, harbours, ships and tanks.</p>
<p>The framework tries to bridge the gap between RGB and SAR imagery. There are also several tools to help reduce multiplicative speckle noise and artefacts in SAR imagery.</p>
<p>In this post, I'll try to detect aircraft in Umbra's imagery using SARDet-100K and MSFA.</p>
</div>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 6 GHz Intel Core <a href="https://www.intel.com/content/www/us/en/products/sku/236773/intel-core-i9-processor-14900k-36m-cache-up-to-6-00-ghz/specifications.html">i9-14900K</a> CPU. It has 8 performance cores and 16 efficiency cores with a total of 32 threads and 32 MB of L2 cache. It has a liquid cooler attached and is housed in a spacious, full-sized, Cooler Master HAF 700 computer case. I've come across videos on YouTube where people have managed to overclock the i9-14900KF to <a href="https://www.youtube.com/watch?v=fb7pl7PZOYo">9.1 GHz</a>.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 6,000 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular, Corsair Power Supply and is sat on an ASRock Z790 Pro RS Motherboard.</p>
<p>I'm running Ubuntu 22 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and I use ArcGIS Pro from time to time which only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll be using Python and a few other tools to help analyse the data in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>aws-cli<span> </span><span>\</span>
<span>    </span>gdal-bin<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3-virtualenv
</pre></div>
<p>I'll set up a Python Virtual Environment and install some dependencies.</p>
<div><pre><span></span>$<span> </span>virtualenv<span> </span>~/.msfa
$<span> </span><span>source</span><span> </span>~/.msfa/bin/activate

$<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>boto3<span> </span><span>\</span>
<span>    </span>geopandas<span> </span><span>\</span>
<span>    </span>osgeo<span> </span><span>\</span>
<span>    </span>pandas<span> </span><span>\</span>
<span>    </span>rich<span> </span><span>\</span>
<span>    </span>shapely<span> </span><span>\</span>
<span>    </span>typer
</pre></div>
<p>The following will install some PyTorch dependencies.</p>
<div><pre><span></span>$<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span><span>torch</span><span>==</span><span>2</span>.0.1<span> </span><span>\</span>
<span>    </span><span>torchvision</span><span>==</span><span>0</span>.15.2<span> </span><span>\</span>
<span>    </span><span>torchaudio</span><span>==</span><span>2</span>.0.2<span> </span><span>\</span>
<span>    </span>--index-url<span> </span>https://download.pytorch.org/whl/cu118
</pre></div>
<p>MSFA relies on a number of OpenMMLab packages.</p>
<div><pre><span></span>$<span> </span>pip<span> </span>install<span> </span>-U<span> </span>openmim
$<span> </span>mim<span> </span>install<span> </span>-U<span> </span><span>\</span>
<span>    </span>mmengine<span> </span><span>\</span>
<span>    </span>mmcv<span> </span><span>\</span>
<span>    </span>mmdet<span> </span><span>\</span>
<span>    </span>mmpretrain
</pre></div>
<p>The following will install the MSFA Framework.</p>
<div><pre><span></span>$<span> </span>git<span> </span>clone<span> </span>https://github.com/zcablii/SARDet_100K
$<span> </span><span>cd</span><span> </span>SARDet_100K/MSFA

$<span> </span>pip<span> </span>install<span> </span>-r<span> </span>requirements.txt
$<span> </span>pip<span> </span>install<span> </span>-v<span> </span>-e<span> </span>.
</pre></div>
<p>I had some issues with SciPy 1.14.1 so I've downgraded to version 1.12.0.</p>
<div><pre><span></span>$<span> </span>pip<span> </span>install<span> </span>-U<span> </span><span>'scipy&lt;1.13'</span>
</pre></div>
<p>I'll use DuckDB, along with its <a href="https://github.com/isaacbrodsky/h3-duckdb">H3</a>, <a href="https://duckdb.org/docs/extensions/json">JSON</a>, <a href="https://duckdb.org/docs/data/parquet/overview">Parquet</a> and <a href="https://duckdb.org/docs/extensions/spatial.html">Spatial</a> extensions, in this post.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>~
$<span> </span>wget<span> </span>-c<span> </span>https://github.com/duckdb/duckdb/releases/download/v1.0.0/duckdb_cli-linux-amd64.zip
$<span> </span>unzip<span> </span>-j<span> </span>duckdb_cli-linux-amd64.zip
$<span> </span>chmod<span> </span>+x<span> </span>duckdb
$<span> </span>~/duckdb
</pre></div>
<div><pre><span></span><span>INSTALL</span><span> </span><span>h3</span><span> </span><span>FROM</span><span> </span><span>community</span><span>;</span>
<span>INSTALL</span><span> </span><span>json</span><span>;</span>
<span>INSTALL</span><span> </span><span>parquet</span><span>;</span>
<span>INSTALL</span><span> </span><span>spatial</span><span>;</span>
</pre></div>
<p>I'll set up DuckDB to load every installed extension each time it launches.</p>

<div><pre><span></span>.timer on
.width 180
LOAD h3;
LOAD json;
LOAD parquet;
LOAD spatial;
</pre></div>
<p>The maps in this post were rendered with <a href="https://www.qgis.org/en/site/forusers/download.html">QGIS</a> version 3.38.0. QGIS is a desktop application that runs on Windows, macOS and Linux. The application has grown in popularity in recent years and has ~15M application launches from users around the world each month.</p>
<p>I used QGIS' <a href="https://geoinformaticsworld.com/how-to-download-and-install-google-earth-images-in-qgis-using-tiles-plugin/">Tile+ plugin</a> to add geospatial context with Esri's World Imagery and CARTO's Basemaps to the maps.</p>
</div>
<div id="downloading-satellite-imagery">
<h2>Downloading Satellite Imagery</h2>
<p>Umbra's Open Data feed has grown to just under 45 TB as of this writing.</p>
<div><pre><span></span>$<span> </span>mkdir<span> </span>-p<span> </span>~/umbra_bkk
$<span> </span><span>cd</span><span> </span>~/umbra_bkk

$<span> </span>aws<span> </span>--no-sign-request<span> </span><span>\</span>
<span>      </span>--output<span> </span>json<span> </span><span>\</span>
<span>      </span>s3api<span> </span><span>\</span>
<span>      </span>list-objects<span> </span><span>\</span>
<span>      </span>--bucket<span> </span>umbra-open-data-catalog<span> </span><span>\</span>
<span>      </span>--max-items<span>=</span><span>1000000</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-c<span> </span><span>'.Contents[]'</span><span> </span><span>\</span>
<span>    </span>&gt;<span> </span>umbra.s3.json
</pre></div>
<div><pre><span></span>$<span> </span><span>echo</span><span> </span><span>"Total objects: "</span><span> </span><span>`</span>wc<span> </span>-l<span> </span>umbra.s3.json<span> </span><span>|</span><span> </span>cut<span> </span>-d<span>' '</span><span> </span>-f1<span>`</span>,<span> </span><span>\</span>
<span>   </span><span>" TB: "</span><span> </span><span>`</span>jq<span> </span>.Size<span> </span>umbra.s3.json<span> </span><span>|</span><span> </span>awk<span> </span><span>'{s+=$1}END{print s/1024/1024/1024/1024}'</span><span>`</span>
</pre></div>
<div><pre><span></span>Total objects:  33813,  TB:  44.9067
</pre></div>
<p>I'll download the GeoTIFF deliverables and JSON Metadata files for Bangkok Airport. These add up to ~35 GB in size.</p>
<div><pre><span></span>$<span> </span><span>for</span><span> </span>FORMAT<span> </span><span>in</span><span> </span>json<span> </span>tif<span>;</span><span> </span><span>do</span>
<span>    </span>aws<span> </span>s3<span> </span>--no-sign-request<span> </span><span>\</span>
<span>           </span>sync<span> </span><span>\</span>
<span>           </span>--exclude<span>=</span><span>"*"</span><span> </span><span>\</span>
<span>           </span>--include<span>=</span><span>"*.</span><span>$FORMAT</span><span>"</span><span> </span><span>\</span>
<span>           </span><span>"s3://umbra-open-data-catalog/sar-data/tasks/Suvarnabhumi International Airport, Thailand/"</span><span> </span><span>\</span>
<span>           </span>~/umbra_bkk/
<span>  </span><span>done</span>
</pre></div>
</div>
<div id="umbra-s-bangkok-airport-imagery">
<h2>Umbra's Bangkok Airport Imagery</h2>
<p>The 140+ images taken of BKK airport all have a footprint 5 x 5KM. That said, the footprints all cover slightly different areas. Below is a rendering of the footprints over the two year period.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/footprints.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/footprints.jpg"></a></p><p>I'll extract some metadata of interest and load it into a DuckDB table.</p>
<div><pre><span></span>$<span> </span><span>cd</span><span> </span>~/umbra_bkk
$<span> </span>python3
</pre></div>
<div><pre><span></span><span>import</span> <span>json</span>
<span>from</span>   <span>pathlib</span>        <span>import</span> <span>Path</span>

<span>from</span>   <span>rich.progress</span> <span>import</span> <span>track</span>


<span>with</span> <span>open</span><span>(</span><span>'enriched.json'</span><span>,</span> <span>'w'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>filename</span> <span>in</span> <span>track</span><span>(</span><span>list</span><span>(</span><span>Path</span><span>(</span><span>'.'</span><span>)</span><span>.</span><span>glob</span><span>(</span><span>'**/*_METADATA.json'</span><span>))):</span>
        <span>rec</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>open</span><span>(</span><span>filename</span><span>,</span> <span>'r'</span><span>)</span><span>.</span><span>read</span><span>())</span>

        <span>out</span> <span>=</span> <span>{</span><span>'filename'</span><span>:</span>       <span>str</span><span>(</span><span>filename</span><span>),</span>
               <span>'sat_num'</span><span>:</span>        <span>rec</span><span>[</span><span>'umbraSatelliteName'</span><span>],</span>
               <span>'imaging_mode'</span><span>:</span>   <span>rec</span><span>[</span><span>'imagingMode'</span><span>],</span>
               <span>'polarisation'</span><span>:</span>   <span>', '</span><span>.</span><span>join</span><span>(</span><span>sorted</span><span>(</span><span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'polarizations'</span><span>])),</span>
               <span>'start'</span><span>:</span>          <span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'startAtUTC'</span><span>],</span>
               <span>'finish'</span><span>:</span>         <span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'endAtUTC'</span><span>],</span>
               <span>'radarBand'</span><span>:</span>      <span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'radarBand'</span><span>],</span>
               <span>'azimuth_meters'</span><span>:</span> <span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'maxGroundResolution'</span><span>][</span><span>'azimuthMeters'</span><span>],</span>
               <span>'range_meters'</span><span>:</span>   <span>rec</span><span>[</span><span>'collects'</span><span>][</span><span>0</span><span>][</span><span>'maxGroundResolution'</span><span>][</span><span>'rangeMeters'</span><span>],</span>
               <span>'azimuth_looks'</span><span>:</span>  <span>rec</span><span>[</span><span>'derivedProducts'</span><span>][</span><span>'GEC'</span><span>][</span><span>0</span><span>][</span><span>'looks'</span><span>][</span><span>'azimuth'</span><span>],</span>
               <span>'range_looks'</span><span>:</span>    <span>rec</span><span>[</span><span>'derivedProducts'</span><span>][</span><span>'GEC'</span><span>][</span><span>0</span><span>][</span><span>'looks'</span><span>][</span><span>'range'</span><span>]}</span>

        <span>f</span><span>.</span><span>write</span><span>(</span><span>json</span><span>.</span><span>dumps</span><span>(</span><span>out</span><span>,</span> <span>sort_keys</span><span>=</span><span>True</span><span>)</span> <span>+</span> <span>'</span><span>\n</span><span>'</span><span>)</span>
</pre></div>

<div><pre><span></span><span>CREATE</span><span> </span><span>OR</span><span> </span><span>REPLACE</span><span> </span><span>TABLE</span><span> </span><span>umbra</span><span> </span><span>AS</span>
<span>    </span><span>SELECT</span><span> </span><span>*</span><span> </span><span>EXCLUDE</span><span>(</span><span>start</span><span>,</span><span> </span><span>finish</span><span>),</span>
<span>           </span><span>start</span><span>::</span><span>DATETIME</span><span> </span><span>AS</span><span> </span><span>start</span><span>,</span>
<span>           </span><span>finish</span><span>::</span><span>DATETIME</span><span> </span><span>AS</span><span> </span><span>finish</span>
<span>    </span><span>FROM</span><span>   </span><span>READ_JSON</span><span>(</span><span>'enriched.json'</span><span>);</span>
</pre></div>
<p>Below are the number of images for each month and the satellite that took them.</p>
<div><pre><span></span><span>WITH</span><span> </span><span>a</span><span> </span><span>AS</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span> </span><span>STRFTIME</span><span>(</span><span>start</span><span>,</span><span> </span><span>'%Y-%m'</span><span>)</span><span> </span><span>yyyy_mm</span><span>,</span>
<span>           </span><span>sat_num</span><span>,</span>
<span>           </span><span>count</span><span>(</span><span>*</span><span>)</span><span> </span><span>num_recs</span>
<span>    </span><span>FROM</span><span> </span><span>umbra</span>
<span>    </span><span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>    </span><span>ORDER</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span>
<span>)</span>
<span>PIVOT</span><span>    </span><span>a</span>
<span>ON</span><span>       </span><span>sat_num</span>
<span>USING</span><span>    </span><span>SUM</span><span>(</span><span>num_recs</span><span>)</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>yyyy_mm</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>yyyy_mm</span><span>;</span>
</pre></div>
<div><pre><span></span>┌─────────┬──────────┬──────────┬──────────┬──────────┐
│ yyyy_mm │ UMBRA_04 │ UMBRA_05 │ UMBRA_06 │ UMBRA_08 │
│ varchar │  int128  │  int128  │  int128  │  int128  │
├─────────┼──────────┼──────────┼──────────┼──────────┤
│ 2023-02 │        7 │        5 │          │          │
│ 2023-03 │        6 │        5 │          │          │
│ 2023-04 │        8 │        7 │          │          │
│ 2023-05 │        5 │        6 │          │          │
│ 2023-06 │        3 │       10 │        6 │          │
│ 2023-07 │        4 │        7 │        2 │          │
│ 2023-08 │        6 │        8 │          │          │
│ 2023-09 │        4 │        3 │          │          │
│ 2023-10 │        3 │        4 │          │          │
│ 2023-11 │        4 │        2 │          │          │
│ 2024-01 │        1 │        3 │        4 │          │
│ 2024-02 │          │        3 │        6 │          │
│ 2024-03 │        2 │        1 │          │        1 │
│ 2024-05 │          │        1 │        2 │          │
│ 2024-06 │          │        1 │          │          │
│ 2024-07 │          │          │        1 │          │
│ 2024-08 │          │        1 │          │          │
├─────────┴──────────┴──────────┴──────────┴──────────┤
│ 17 rows                                   5 columns │
└─────────────────────────────────────────────────────┘
</pre></div>
<p>All of the images have the same polarisation and use the X radar band.</p>
<div><pre><span></span><span>SELECT</span><span>   </span><span>imaging_mode</span><span>,</span>
<span>         </span><span>polarisation</span><span>,</span>
<span>         </span><span>radarBand</span><span>,</span>
<span>         </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>FROM</span><span>     </span><span>umbra</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span><span>,</span><span> </span><span>2</span><span>,</span><span> </span><span>3</span><span>;</span>
</pre></div>
<div><pre><span></span>┌──────────────┬──────────────┬───────────┬──────────────┐
│ imaging_mode │ polarisation │ radarBand │ count_star() │
│   varchar    │   varchar    │  varchar  │    int64     │
├──────────────┼──────────────┼───────────┼──────────────┤
│ SPOTLIGHT    │ VV           │ X         │          142 │
└──────────────┴──────────────┴───────────┴──────────────┘
</pre></div>
<p>The highest resolution imagery in this dataset is 17.48 cm.</p>
<div><pre><span></span><span>SELECT</span><span>   </span><span>sat_num</span><span>,</span>
<span>         </span><span>ROUND</span><span>(</span><span>MIN</span><span>(</span><span>azimuth_meters</span><span> </span><span>*</span><span> </span><span>100</span><span>),</span><span> </span><span>2</span><span>)</span><span> </span><span>AS</span><span> </span><span>cm_res</span><span>,</span>
<span>FROM</span><span>     </span><span>umbra</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>2</span>
</pre></div>
<div><pre><span></span>┌──────────┬────────┐
│ sat_num  │ cm_res │
│ varchar  │ double │
├──────────┼────────┤
│ UMBRA_06 │  17.48 │
│ UMBRA_05 │  17.73 │
│ UMBRA_08 │   25.0 │
│ UMBRA_04 │   25.0 │
└──────────┴────────┘
</pre></div>
<p>Below are the resolution buckets for each satellite.</p>
<div><pre><span></span><span>WITH</span><span> </span><span>a</span><span> </span><span>AS</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span> </span><span>(</span><span>ROUND</span><span>(</span><span>ROUND</span><span>(</span><span>azimuth_meters</span><span> </span><span>*</span><span> </span><span>100</span><span>)</span><span> </span><span>/</span><span> </span><span>10</span><span>)</span><span> </span><span>*</span><span> </span><span>10</span><span>)::</span><span>INT</span><span> </span><span>AS</span><span> </span><span>cm_res</span><span>,</span>
<span>           </span><span>sat_num</span>
<span>    </span><span>FROM</span><span>   </span><span>umbra</span>
<span>)</span>
<span>PIVOT</span><span>    </span><span>a</span>
<span>ON</span><span>       </span><span>sat_num</span>
<span>USING</span><span>    </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>cm_res</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>cm_res</span><span>;</span>
</pre></div>
<div><pre><span></span>┌────────┬──────────┬──────────┬──────────┬──────────┐
│ cm_res │ UMBRA_04 │ UMBRA_05 │ UMBRA_06 │ UMBRA_08 │
│ int32  │  int64   │  int64   │  int64   │  int64   │
├────────┼──────────┼──────────┼──────────┼──────────┤
│     20 │        0 │        4 │        6 │        0 │
│     30 │        2 │        5 │        7 │        1 │
│     40 │        8 │        8 │        4 │        0 │
│     50 │       34 │       36 │        1 │        0 │
│     60 │        7 │       13 │        3 │        0 │
│     70 │        2 │        1 │        0 │        0 │
└────────┴──────────┴──────────┴──────────┴──────────┘
</pre></div>
<p>The majority of imagery only had one look but there are a few 2 and 3-look images in this dataset as well.</p>
<div><pre><span></span><span>SELECT</span><span>   </span><span>ROUND</span><span>(</span><span>azimuth_looks</span><span>)::</span><span>INT</span><span> </span><span>as</span><span> </span><span>looks</span><span>,</span>
<span>         </span><span>COUNT</span><span>(</span><span>*</span><span>)</span><span> </span><span>num_images</span>
<span>FROM</span><span>     </span><span>umbra</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>1</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>1</span><span>;</span>
</pre></div>
<div><pre><span></span>┌───────┬────────────┐
│ looks │ num_images │
│ int32 │   int64    │
├───────┼────────────┤
│     1 │        119 │
│     2 │         21 │
│     3 │          2 │
└───────┴────────────┘
</pre></div>
<p>The number of looks correlates with a higher resolution.</p>
<div><pre><span></span><span>WITH</span><span> </span><span>a</span><span> </span><span>AS</span><span> </span><span>(</span>
<span>    </span><span>SELECT</span><span> </span><span>(</span><span>ROUND</span><span>(</span><span>ROUND</span><span>(</span><span>azimuth_meters</span><span> </span><span>*</span><span> </span><span>100</span><span>)</span><span> </span><span>/</span><span> </span><span>10</span><span>)</span><span> </span><span>*</span><span> </span><span>10</span><span>)::</span><span>INT</span><span> </span><span>AS</span><span> </span><span>cm_res</span><span>,</span>
<span>           </span><span>ROUND</span><span>(</span><span>azimuth_looks</span><span>)::</span><span>INT</span><span> </span><span>AS</span><span> </span><span>looks</span>
<span>    </span><span>FROM</span><span>   </span><span>umbra</span>
<span>)</span>
<span>PIVOT</span><span>    </span><span>a</span>
<span>ON</span><span>       </span><span>looks</span>
<span>USING</span><span>    </span><span>COUNT</span><span>(</span><span>*</span><span>)</span>
<span>GROUP</span><span> </span><span>BY</span><span> </span><span>cm_res</span>
<span>ORDER</span><span> </span><span>BY</span><span> </span><span>cm_res</span><span>;</span>
</pre></div>
<div><pre><span></span>┌────────┬───────┬───────┬───────┐
│ cm_res │   1   │   2   │   3   │
│ int32  │ int64 │ int64 │ int64 │
├────────┼───────┼───────┼───────┤
│     20 │     0 │     8 │     2 │
│     30 │     2 │    13 │     0 │
│     40 │    20 │     0 │     0 │
│     50 │    71 │     0 │     0 │
│     60 │    23 │     0 │     0 │
│     70 │     3 │     0 │     0 │
└────────┴───────┴───────┴───────┘
</pre></div>
</div>
<div id="downloading-sardet-100k-weights">
<h2>Downloading SARDet_100K Weights</h2>
<p>SARDet_100K hosts their model weights on Baidu Disk and <a href="https://liveuclac-my.sharepoint.com/:f:/g/personal/zcablii_ucl_ac_uk/EsCsvTBzlGlJkQogQ2_svKcB9K-0LXfgZPoUJBySPP2L5g?e=RsL0VA">OneDrive</a>. The ZIP file with the weights is 20 GB. Below are its contents.</p>
<div><pre><span></span>$<span> </span>du<span> </span>-hs<span> </span>/home/mark/ckpts/*<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>grep<span> </span><span>'[0-9][MG]'</span><span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>cut<span> </span>-d/<span> </span>-f1,5<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>sed<span> </span><span>'s/\///g'</span>
</pre></div>
<div><pre><span></span>1.1G    convnext_b_sar
1.1G    convnext_b_sar_wavelet
598M    convnext_s_sar
603M    convnext_s_sar_wavelet
339M    convnext_t_sar
344M    convnext_t_sar_wavelet
457M    fg_frcnn_dota_pretrain_sar_convnext_b_wavelet
228M    fg_frcnn_dota_pretrain_sar_convnext_t_wavelet
287M    fg_frcnn_dota_pretrain_sar_r101_wavelet
346M    fg_frcnn_dota_pretrain_sar_r152_wavelet
308M    fg_frcnn_dota_pretrain_sar_swin_s_wavelet
227M    fg_frcnn_dota_pretrain_sar_swin_t_wavelet
222M    fg_frcnn_dota_pretrain_sar_van_b_wavelet
173M    fg_frcnn_dota_pretrain_sar_van_s_wavelet
135M    fg_frcnn_dota_pretrain_sar_van_t_wavelet
214M    fg_frcnn_dota_pretrain_sar_wavelet_r50
2.2G    gfl_r50_denodet_sardet
210M    hrsid_frcnn_van_sar_wavelet_bs32_3
481M    pretrain_frcnn_dota_convnext_b_sar_wavelet
335M    pretrain_frcnn_dota_convnext_s_sar_wavelet
2.3G    pretrain_frcnn_dota_r101_sar
312M    pretrain_frcnn_dota_r101_sar_wavelet
372M    pretrain_frcnn_dota_r152_sar_wavelet
239M    pretrain_frcnn_dota_r50_sar_wavelet
333M    pretrain_frcnn_dota_swin_s_sar_wavelet
251M    pretrain_frcnn_dota_swin_t_sar_wavelet
246M    pretrain_frcnn_dota_van_b_sar_wavelet
197M    pretrain_frcnn_dota_van_s_sar_wavelet
160M    pretrain_frcnn_dota_van_t_sar_wavelet
377M    r101_sar_epoch_100.pth
382M    r101_sar_wavelet_epoch_100.pth
502M    r152_sar_wavelet
231M    r50_sar_epoch_100.pth
237M    r50_sar_wavelet_epoch_100.pth
210M    ssdd_frcnn_van_sar_wavelet_bs32_3
1.1G    swin_b_sar
596M    swin_s_sar
601M    swin_s_sar_wavelet
351M    swin_t_sar
356M    swin_t_sar_wavelet
338M    van_b_sar_wavelet_epoch_100.pth
173M    van_s_sar_epoch_100.pth
179M    van_s_sar_wavelet_epoch_100.pth
61M     van_t_sar_epoch_100.pth
66M     van_t_sar_wavelet_epoch_100.pth
</pre></div>
</div>
<div id="sar-aircraft-training-data">
<h2>SAR Aircraft Training Data</h2>
<p>Three of the ten datasets in SARDet_100K contain aircraft imagery.</p>
<div><pre><span></span>Datasets       | Objects     | Resolution | Band    | Polarization    | Satellites
---------------|-------------|------------|---------|-----------------|------------------------------
MSAR           | A, T, B, S  | ≤ 1m       | C       | HH, HV, VH, VV  | HISEA-1
SADD           | A           | 0.5-3m     | X       | HH              | TerraSAR-X
SAR-AIRcraft   | A           | 1m         | C       | Uni-polar       | GF-3
AIR_SARShip    | S           | 1,3m       | C       | VV              | GF-3
HRSID          | S           | 0.5-3m     | C/X     | HH, HV, VH, VV  | S-1B, TerraSAR-X, TanDEMX
ShipDataset    | S           | 3-25m      | C       | HH, VV, VH, HV  | S-1, GF-3
SSDD           | S           | 1-15m      | C/X     | HH, VV, VH, HV  | S-1, RadarSat-2, TerraSAR-X
OGSOD          | B, H, T     | 3m         | C       | VV/VH           | GF-3
SIVED          | C           | 0.1,0.3m   | Ka,Ku,X | VV/HH           | Airborne SAR synthetic slice
</pre></div>
<p>The object acronyms are (A)ircraft, (B)ridges, (C)ars, (H)arbours, (S)hips and (T)anks.</p>
<p>The Umbra imagery in this post uses VV polarisation which some of the MSAR data used as well.</p>
<p>SAR radar beams can switch between horizontal and vertical polarisation both when sending and receiving. These are denoted by two letters. For example: HV would mean horizontal transmission and vertical receiving.</p>
<p>Horizontal polarisation is ideal for flat and low surfaces, like rivers, bridges and power lines. Vertical polarisation is ideal for buildings, rough seas and transmission towers.</p>
<p>The aircraft training imagery was collected using the X band in some cases but the C band in others. The X band is good for urban monitoring and seeing through snow but it can't penetrate vegetation very well. This might make aircraft under a natural canopy harder to detect. The Umbra imagery in this post was all collected with the X band.</p>
</div>
<div id="bangkok-suvarnabhumi-international-airport">
<h2>Bangkok Suvarnabhumi International Airport</h2>
<p>I'll walk through the layout of Bangkok Suvarnabhumi International Airport (BKK). I used GeoFabrik's OpenStreetMap (OSM) <a href="https://download.geofabrik.de/asia.html">distributable</a> for Thailand and <a href="https://tech.marksblogg.com/extracting-osm-features.html">osm_split</a> to extract the geometry and metadata in the following images.</p>
<p>There are aircraft parking gates south of the main passenger terminal and all around the satellite terminal.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_hGG17JrgKN.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_hGG17JrgKN.jpg"></a></p><p>Below is a closer view of the parking spots around the main passenger terminal.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_QvcK1CZsAA.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_QvcK1CZsAA.jpg"></a></p><p>Below is a closer view of the parking spots around the satellite terminal.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_jwGpNvaRCU.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_jwGpNvaRCU.jpg"></a></p><p>I've overlaid four hours of ADS-B data from the evening of May 25th on the map. This gives a bit of ground truth as to where aircraft were located around the time Umbra took their image for that day.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_wgGXmp0h3I.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_wgGXmp0h3I.jpg"></a></p><p>The ADS-B data was sourced from the <a href="https://tech.marksblogg.com/global-flight-tracking-adsb.html">adsb.lol</a> project. I built a tool called <a href="https://github.com/marklit/adsb_json">adsb_json</a> that converts the JSON adsb.lol produces into GeoPackage (GPKG) files.</p>
</div>
<div id="deondet">
<h2>DeonDet</h2>
<p>I'll run DeonDet on Umbra's image of BKK for May 25th. Below is a configuration file that ships with MSFA.</p>
<div><pre><span></span>$<span> </span>cat<span> </span>~/SARDet_100K/MSFA/local_configs/DeonDet/gfl_r50_denodet_sardet.py
</pre></div>
<div><pre><span></span><span>_base_</span> <span>=</span> <span>[</span>
    <span>'../../configs/_base_/datasets/SARDet_100k.py'</span><span>,</span>
    <span>'../../configs/_base_/schedules/schedule_1x.py'</span><span>,</span> <span>'../../configs/_base_/default_runtime.py'</span>
<span>]</span>
<span>num_classes</span> <span>=</span> <span>6</span>
<span>model</span> <span>=</span> <span>dict</span><span>(</span>
    <span>type</span><span>=</span><span>'GFL'</span><span>,</span>
    <span>data_preprocessor</span><span>=</span><span>dict</span><span>(</span>
        <span>type</span><span>=</span><span>'DetDataPreprocessor'</span><span>,</span>
        <span>mean</span><span>=</span><span>[</span><span>123.675</span><span>,</span> <span>116.28</span><span>,</span> <span>103.53</span><span>],</span>
        <span>std</span><span>=</span><span>[</span><span>58.395</span><span>,</span> <span>57.12</span><span>,</span> <span>57.375</span><span>],</span>
        <span>bgr_to_rgb</span><span>=</span><span>True</span><span>,</span>
        <span>pad_size_divisor</span><span>=</span><span>32</span><span>),</span>
    <span>backbone</span><span>=</span><span>dict</span><span>(</span>
        <span>type</span><span>=</span><span>'ResNet'</span><span>,</span>
        <span>depth</span><span>=</span><span>50</span><span>,</span>
        <span>num_stages</span><span>=</span><span>4</span><span>,</span>
        <span>out_indices</span><span>=</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>),</span>
        <span>frozen_stages</span><span>=</span><span>1</span><span>,</span>
        <span>norm_cfg</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'BN'</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>),</span>
        <span>norm_eval</span><span>=</span><span>True</span><span>,</span>
        <span>style</span><span>=</span><span>'pytorch'</span><span>,</span>
        <span>init_cfg</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'Pretrained'</span><span>,</span> <span>checkpoint</span><span>=</span><span>'torchvision://resnet50'</span><span>)),</span>
    <span>neck</span><span>=</span><span>dict</span><span>(</span>
        <span>type</span><span>=</span><span>'FrequencySpatialFPN'</span><span>,</span>
        <span>in_channels</span><span>=</span><span>[</span><span>256</span><span>,</span> <span>512</span><span>,</span> <span>1024</span><span>,</span> <span>2048</span><span>],</span>
        <span>out_channels</span><span>=</span><span>256</span><span>,</span>
        <span>start_level</span><span>=</span><span>1</span><span>,</span>
        <span>add_extra_convs</span><span>=</span><span>'on_input'</span><span>,</span>
        <span>num_outs</span><span>=</span><span>5</span><span>,</span>
        <span>norm_cfg</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'GN'</span><span>,</span> <span>num_groups</span><span>=</span><span>32</span><span>,</span> <span>requires_grad</span><span>=</span><span>True</span><span>)),</span>
    <span>bbox_head</span><span>=</span><span>dict</span><span>(</span>
        <span>type</span><span>=</span><span>'GFLHead'</span><span>,</span>
        <span>num_classes</span><span>=</span><span>num_classes</span><span>,</span>
        <span>in_channels</span><span>=</span><span>256</span><span>,</span>
        <span>stacked_convs</span><span>=</span><span>4</span><span>,</span>
        <span>feat_channels</span><span>=</span><span>256</span><span>,</span>
        <span>anchor_generator</span><span>=</span><span>dict</span><span>(</span>
            <span>type</span><span>=</span><span>'AnchorGenerator'</span><span>,</span>
            <span>ratios</span><span>=</span><span>[</span><span>1.0</span><span>],</span>
            <span>octave_base_scale</span><span>=</span><span>8</span><span>,</span>
            <span>scales_per_octave</span><span>=</span><span>1</span><span>,</span>
            <span>strides</span><span>=</span><span>[</span><span>8</span><span>,</span> <span>16</span><span>,</span> <span>32</span><span>,</span> <span>64</span><span>,</span> <span>128</span><span>]),</span>
        <span>loss_cls</span><span>=</span><span>dict</span><span>(</span>
            <span>type</span><span>=</span><span>'QualityFocalLoss'</span><span>,</span>
            <span>use_sigmoid</span><span>=</span><span>True</span><span>,</span>
            <span>beta</span><span>=</span><span>2.0</span><span>,</span>
            <span>loss_weight</span><span>=</span><span>1.0</span><span>),</span>
        <span>loss_dfl</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'DistributionFocalLoss'</span><span>,</span> <span>loss_weight</span><span>=</span><span>0.25</span><span>),</span>
        <span>reg_max</span><span>=</span><span>16</span><span>,</span>
        <span>loss_bbox</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'GIoULoss'</span><span>,</span> <span>loss_weight</span><span>=</span><span>2.0</span><span>)),</span>
    <span># training and testing settings</span>
    <span>train_cfg</span><span>=</span><span>dict</span><span>(</span>
        <span>assigner</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'ATSSAssigner'</span><span>,</span> <span>topk</span><span>=</span><span>9</span><span>),</span>
        <span>allowed_border</span><span>=-</span><span>1</span><span>,</span>
        <span>pos_weight</span><span>=-</span><span>1</span><span>,</span>
        <span>debug</span><span>=</span><span>False</span><span>),</span>
    <span>test_cfg</span><span>=</span><span>dict</span><span>(</span>
        <span>nms_pre</span><span>=</span><span>1000</span><span>,</span>
        <span>min_bbox_size</span><span>=</span><span>0</span><span>,</span>
        <span>score_thr</span><span>=</span><span>0.05</span><span>,</span>
        <span>nms</span><span>=</span><span>dict</span><span>(</span><span>type</span><span>=</span><span>'nms'</span><span>,</span> <span>iou_threshold</span><span>=</span><span>0.6</span><span>),</span>
        <span>max_per_img</span><span>=</span><span>100</span><span>))</span>


<span>backend_args</span> <span>=</span> <span>None</span>
<span>train_pipeline</span> <span>=</span> <span>[</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'LoadImageFromFile'</span><span>,</span> <span>backend_args</span><span>=</span><span>backend_args</span><span>),</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'LoadAnnotations'</span><span>,</span> <span>with_bbox</span><span>=</span><span>True</span><span>),</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'Resize'</span><span>,</span> <span>scale</span><span>=</span><span>(</span><span>1024</span><span>,</span> <span>1024</span><span>),</span> <span>keep_ratio</span><span>=</span><span>False</span><span>),</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'RandomFlip'</span><span>,</span> <span>prob</span><span>=</span><span>0.5</span><span>),</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'PackDetInputs'</span><span>)</span>
<span>]</span>

<span>test_pipeline</span> <span>=</span> <span>[</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'LoadImageFromFile'</span><span>,</span> <span>backend_args</span><span>=</span><span>backend_args</span><span>),</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'Resize'</span><span>,</span> <span>scale</span><span>=</span><span>(</span><span>1024</span><span>,</span> <span>1024</span><span>),</span> <span>keep_ratio</span><span>=</span><span>False</span><span>),</span>
    <span># If you don't have a gt annotation, delete the pipeline</span>
    <span>dict</span><span>(</span><span>type</span><span>=</span><span>'LoadAnnotations'</span><span>,</span> <span>with_bbox</span><span>=</span><span>True</span><span>),</span>
    <span>dict</span><span>(</span>
        <span>type</span><span>=</span><span>'PackDetInputs'</span><span>,</span>
        <span>meta_keys</span><span>=</span><span>(</span><span>'img_id'</span><span>,</span> <span>'img_path'</span><span>,</span> <span>'ori_shape'</span><span>,</span> <span>'img_shape'</span><span>,</span>
                   <span>'scale_factor'</span><span>))</span>
<span>]</span>

<span>train_dataloader</span> <span>=</span> <span>dict</span><span>(</span>
    <span>dataset</span><span>=</span><span>dict</span><span>(</span>
        <span>pipeline</span><span>=</span><span>train_pipeline</span><span>))</span>
<span>val_dataloader</span> <span>=</span> <span>dict</span><span>(</span>
    <span>dataset</span><span>=</span><span>dict</span><span>(</span>
        <span>pipeline</span><span>=</span><span>test_pipeline</span><span>))</span>
<span>test_dataloader</span> <span>=</span> <span>dict</span><span>(</span>
    <span>dataset</span><span>=</span><span>dict</span><span>(</span>
        <span>pipeline</span><span>=</span><span>test_pipeline</span><span>))</span>


<span># find_unused_parameters = True</span>
<span>optim_wrapper</span> <span>=</span> <span>dict</span><span>(</span>
    <span>optimizer</span><span>=</span><span>dict</span><span>(</span>
        <span>_delete_</span><span>=</span><span>True</span><span>,</span>
        <span>betas</span><span>=</span><span>(</span>
            <span>0.9</span><span>,</span>
            <span>0.999</span><span>,</span>
        <span>),</span> <span>lr</span><span>=</span><span>0.0001</span><span>,</span> <span>type</span><span>=</span><span>'AdamW'</span><span>,</span> <span>weight_decay</span><span>=</span><span>0.05</span><span>),</span>
    <span>type</span><span>=</span><span>'OptimWrapper'</span><span>)</span>
</pre></div>
<p>I'll copy Umbra's image into a working folder.</p>
<div><pre><span></span>$<span> </span>mkdir<span> </span>~/working
$<span> </span><span>cd</span><span> </span>~/working
$<span> </span>cp<span> </span>~/umbra_bkk/80f35b05-2902-46cc-9642-5d0e8916bf49/2024-05-25-15-37-54_UMBRA-06/2024-05-25-15-37-54_UMBRA-06_GEC.tif<span>" ./</span>
</pre></div>
</div>
<div id="rational-polynomial-coefficients">
<h2>Rational Polynomial Coefficients</h2>
<p>This year, Umbra began including <a href="https://en.wikipedia.org/wiki/Rational_polynomial_coefficient">Rational polynomial coefficient</a> (RPC) metadata in their SAR imagery.</p>
<div><pre><span></span>$<span> </span>gdalinfo<span> </span>-json<span> </span><span>2024</span>-05-25-15-37-54_UMBRA-06_GEC.tif<span> </span><span>\</span>
<span>    </span><span>|</span><span> </span>jq<span> </span>-S<span> </span>.metadata.RPC
</pre></div>
<div><pre><span></span><span>{</span>
<span>  </span><span>"ERR_BIAS"</span><span>:</span><span> </span><span>"-1"</span><span>,</span>
<span>  </span><span>"ERR_RAND"</span><span>:</span><span> </span><span>"-1"</span><span>,</span>
<span>  </span><span>"HEIGHT_OFF"</span><span>:</span><span> </span><span>"-30.6818097811395"</span><span>,</span>
<span>  </span><span>"HEIGHT_SCALE"</span><span>:</span><span> </span><span>"1414.21349937642"</span><span>,</span>
<span>  </span><span>"LAT_OFF"</span><span>:</span><span> </span><span>"13.6906792091218"</span><span>,</span>
<span>  </span><span>"LAT_SCALE"</span><span>:</span><span> </span><span>"0.0138548308915265"</span><span>,</span>
<span>  </span><span>"LINE_DEN_COEFF"</span><span>:</span><span> </span><span>"1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"</span><span>,</span>
<span>  </span><span>"LINE_NUM_COEFF"</span><span>:</span><span> </span><span>"1.02231794327927e-05 0.215896351104129 -0.481367152531649 -0.848789645419274 -1.03956610619756e-05 0.00802968239297444 -0.00163693193778641 9.08138917922306e-05 0.000110851333886418 -0.0101589838895881 2.87171326664789e-05 -9.52685642544797e-07 -1.04184901360726e-06 0.000306411120496931 1.41330069687536e-07 2.04421034231927e-07 -5.98024000515252e-05 -7.13359480585107e-05 -1.9284524777315e-07 -0.00026831462493207"</span><span>,</span>
<span>  </span><span>"LINE_OFF"</span><span>:</span><span> </span><span>"8000"</span><span>,</span>
<span>  </span><span>"LINE_SCALE"</span><span>:</span><span> </span><span>"9297.59735239297"</span><span>,</span>
<span>  </span><span>"LONG_OFF"</span><span>:</span><span> </span><span>"100.750015998877"</span><span>,</span>
<span>  </span><span>"LONG_SCALE"</span><span>:</span><span> </span><span>"0.0141698426933747"</span><span>,</span>
<span>  </span><span>"SAMP_DEN_COEFF"</span><span>:</span><span> </span><span>"1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"</span><span>,</span>
<span>  </span><span>"SAMP_NUM_COEFF"</span><span>:</span><span> </span><span>"1.07814928459238e-05 0.36389181356614 0.163208589638044 -0.916269610228506 -1.89359320515592e-05 0.00849839808635423 -0.00157666021207809 0.000118158419879778 0.000120048388193579 -0.010713791039376 3.02836674119795e-05 -1.00610258315293e-06 -1.10270841284789e-06 0.000323144987016618 1.47738197655541e-07 2.09165970874344e-07 -6.30683565149244e-05 -7.52268175348179e-05 -2.02759366303555e-07 -0.000282967947931177"</span><span>,</span>
<span>  </span><span>"SAMP_OFF"</span><span>:</span><span> </span><span>"8000"</span><span>,</span>
<span>  </span><span>"SAMP_SCALE"</span><span>:</span><span> </span><span>"12299.1294903148"</span>
<span>}</span>
</pre></div>
<p>RPC allows for sophisticated and very accurate placement of imagery. Not all tools in GDAL work with RPC <a href="https://github.com/OSGeo/gdal/issues/10333">out of the box</a>. Issues of misplacement can come up if RPC isn't taken into account when processing imagery through different tooling. Below is an example where Tokyo's Haneda is east of where it should be.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_muOV8cZhCN.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_muOV8cZhCN.jpg"></a></p><p>Before working with Umbra's imagery, I'll run it through <tt>gdalwarp</tt>. This will create a GeoTIFF that will be placed properly without the need for tools to fully understand RPC.</p>
<div><pre><span></span>$<span> </span>gdalwarp<span> </span><span>\</span>
<span>    </span>-t_srs<span> </span><span>"EPSG:4326"</span><span> </span><span>\</span>
<span>    </span><span>2024</span>-05-25-15-37-54_UMBRA-06_GEC.tif<span> </span><span>\</span>
<span>    </span>warped.tif
</pre></div>
</div>
<div id="tiling-imagery">
<h2>Tiling Imagery</h2>
<p>Umbra's images can be 10s of thousands of pixels wide and tall. When I ran this model on the original images, I was seeing fewer items detected than when I broke the image up into smaller tiles. Below I'll break up Umbra's BKK image into 4096x4096-pixel GeoTIFFs.</p>
<div><pre><span></span>$<span> </span>gdal_retile.py<span> </span><span>\</span>
<span>    </span>-s_srs<span> </span><span>"EPSG:4326"</span><span> </span><span>\</span>
<span>    </span>-ps<span> </span><span>4096</span><span> </span><span>4096</span><span> </span><span>\</span>
<span>    </span>-targetDir<span> </span>./<span> </span><span>\</span>
<span>    </span>warped.tif
</pre></div>
<p>Below is a screenshot of the tiles.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/tiles.png"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/tiles.png"></a>
</p></div>
<div id="inference-on-tiles">
<h2>Inference on Tiles</h2>
<p>I'll run DeonDet on each tile and output the results into individual folders. Each folder will include an annotated image as well as a JSON file of the detections.</p>
<div><pre><span></span>$<span> </span><span>for</span><span> </span>FILENAME<span> </span><span>in</span><span> </span>warped<span>\_</span>*<span>\_</span>*.tif<span>;</span><span> </span><span>do</span>
<span>    </span><span>STEM</span><span>=</span><span>`</span><span>echo</span><span> </span><span>"</span><span>$FILENAME</span><span>"</span><span> </span><span>|</span><span> </span>grep<span> </span>-o<span> </span><span>'[0-9]\_[0-9]'</span><span>`</span>
<span>    </span><span>echo</span><span> </span><span>$STEM</span>
<span>    </span>mkdir<span> </span>-p<span> </span><span>"out.</span><span>$STEM</span><span>"</span>

<span>    </span>python3<span> </span><span>\</span>
<span>        </span>~/SARDet_100K/MSFA/image_demo.py<span> </span><span>\</span>
<span>        </span><span>"./</span><span>$FILENAME</span><span>"</span><span> </span><span>\</span>
<span>        </span>~/SARDet_100K/MSFA/local_configs/DeonDet/gfl_r50_denodet_sardet.py<span> </span><span>\</span>
<span>        </span>--weights<span> </span>~/ckpts/gfl_r50_denodet_sardet/epoch_best.pth<span> </span><span>\</span>
<span>        </span>--out-dir<span> </span><span>"out.</span><span>$STEM</span><span>"</span>
<span>  </span><span>done</span>
</pre></div>
<p>I wrote a Python script that reads the resulting detections and produces a single GPKG file of them.</p>

<div><pre><span></span><span>import</span> <span>itertools</span>
<span>from</span>   <span>glob</span>             <span>import</span> <span>glob</span>
<span>import</span> <span>json</span>

<span>import</span> <span>geopandas</span>        <span>as</span> <span>gpd</span>
<span>from</span>   <span>osgeo</span>            <span>import</span> <span>gdal</span>
<span>import</span> <span>pandas</span>           <span>as</span> <span>pd</span>
<span>from</span>   <span>shapely.geometry</span> <span>import</span> <span>box</span>
<span>import</span> <span>typer</span>


<span>app</span> <span>=</span> <span>typer</span><span>.</span><span>Typer</span><span>(</span><span>rich_markup_mode</span><span>=</span><span>'rich'</span><span>)</span>


<span>def</span> <span>get_coords</span><span>(</span><span>mx</span><span>,</span> <span>my</span><span>,</span> <span>x_min</span><span>,</span> <span>x_size</span><span>,</span> <span>y_min</span><span>,</span> <span>y_size</span><span>):</span>
    <span>px</span> <span>=</span> <span>mx</span> <span>*</span> <span>x_size</span> <span>+</span> <span>x_min</span>
    <span>py</span> <span>=</span> <span>my</span> <span>*</span> <span>y_size</span> <span>+</span> <span>y_min</span>

    <span>return</span> <span>px</span><span>,</span> <span>py</span>


<span>def</span> <span>get_scores</span><span>(</span><span>tile</span><span>=</span><span>'4_3'</span><span>):</span>
    <span>(</span><span>x_min</span><span>,</span>
     <span>x_size</span><span>,</span>
     <span>_</span><span>,</span>
     <span>y_min</span><span>,</span>
     <span>_</span><span>,</span>
     <span>y_size</span><span>)</span> <span>=</span> <span>gdal</span><span>.</span><span>Open</span><span>(</span><span>'warped_</span><span>%s</span><span>.tif'</span> <span>%</span> <span>tile</span><span>)</span><span>.</span><span>GetGeoTransform</span><span>()</span>

    <span>try</span><span>:</span>
        <span>recs</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>open</span><span>(</span><span>'out.</span><span>%s</span><span>/preds/warped_</span><span>%s</span><span>.json'</span> <span>%</span> <span>(</span><span>tile</span><span>,</span> <span>tile</span><span>))</span><span>.</span><span>read</span><span>())</span>
    <span>except</span> <span>FileNotFoundError</span><span>:</span> <span># Inference failed for some reason</span>
        <span>return</span> <span>[]</span>

    <span>out</span> <span>=</span> <span>[]</span>

    <span>for</span> <span>num</span><span>,</span> <span>score</span> <span>in</span> <span>enumerate</span><span>(</span><span>recs</span><span>[</span><span>'scores'</span><span>]):</span>
        <span>x1</span><span>,</span> <span>y1</span><span>,</span> <span>x2</span><span>,</span> <span>y2</span> <span>=</span> <span>recs</span><span>[</span><span>'bboxes'</span><span>][</span><span>num</span><span>]</span>

        <span>r1</span> <span>=</span> <span>get_coords</span><span>(</span><span>x1</span><span>,</span> <span>y1</span><span>,</span> <span>x_min</span><span>,</span> <span>x_size</span><span>,</span> <span>y_min</span><span>,</span> <span>y_size</span><span>)</span>
        <span>r2</span> <span>=</span> <span>get_coords</span><span>(</span><span>x2</span><span>,</span> <span>y2</span><span>,</span> <span>x_min</span><span>,</span> <span>x_size</span><span>,</span> <span>y_min</span><span>,</span> <span>y_size</span><span>)</span>

        <span>out</span><span>.</span><span>append</span><span>((</span><span>score</span><span>,</span> <span>r1</span><span>[</span><span>0</span><span>],</span> <span>r1</span><span>[</span><span>1</span><span>],</span> <span>r2</span><span>[</span><span>0</span><span>],</span> <span>r2</span><span>[</span><span>1</span><span>]))</span>

    <span>return</span> <span>out</span>


<span>@app</span><span>.</span><span>command</span><span>()</span>
<span>def</span> <span>main</span><span>(</span><span>out</span><span>:</span><span>str</span><span>):</span>
    <span>tiles</span> <span>=</span> <span>[</span><span>x</span><span>.</span><span>split</span><span>(</span><span>'.'</span><span>)[</span><span>0</span><span>]</span><span>.</span><span>split</span><span>(</span><span>'warped_'</span><span>)[</span><span>-</span><span>1</span><span>]</span>
             <span>for</span> <span>x</span> <span>in</span> <span>glob</span><span>(</span><span>'warped_*_*.tif'</span><span>)]</span>

    <span>res</span> <span>=</span> <span>list</span><span>(</span><span>itertools</span><span>.</span><span>chain</span><span>.</span><span>from_iterable</span><span>(</span>
                    <span>list</span><span>(</span><span>filter</span><span>(</span><span>None</span><span>,</span>
                                <span>[</span><span>get_scores</span><span>(</span><span>tile</span><span>)</span>
                                 <span>for</span> <span>tile</span> <span>in</span> <span>tiles</span><span>]))))</span>

    <span>gdf</span> <span>=</span> <span>gpd</span><span>.</span><span>GeoDataFrame</span><span>(</span>
                <span>pd</span><span>.</span><span>DataFrame</span><span>({</span><span>'score'</span><span>:</span> <span>[</span><span>x</span><span>[</span><span>0</span><span>]</span> <span>for</span> <span>x</span> <span>in</span> <span>res</span><span>],</span>
                              <span>'geom'</span><span>:</span>  <span>[</span><span>box</span><span>(</span><span>x</span><span>[</span><span>1</span><span>],</span> <span>x</span><span>[</span><span>2</span><span>],</span> <span>x</span><span>[</span><span>3</span><span>],</span> <span>x</span><span>[</span><span>4</span><span>])</span>
                                       <span>for</span> <span>x</span> <span>in</span> <span>res</span><span>]}),</span>
                           <span>crs</span><span>=</span><span>'4326'</span><span>,</span>
                           <span>geometry</span><span>=</span><span>'geom'</span><span>)</span>
    <span>gdf</span><span>.</span><span>to_file</span><span>(</span><span>out</span><span>,</span> <span>driver</span><span>=</span><span>"GPKG"</span><span>)</span>


<span>if</span> <span>__name__</span> <span>==</span> <span>"__main__"</span><span>:</span>
    <span>app</span><span>()</span>
</pre></div>
<div><pre><span></span>$<span> </span>python<span> </span>json_to_gpkg.py<span> </span><span>\</span>
<span>    </span>--out<span>=</span>gfl_r50_denodet_sardet.gpkg
</pre></div>
<p>I've disregarded the classes of objects being detected for this exercise. I just wanted to see how many plane parking spots were detected with a decent confidence score.</p>
<p>Most of the 0.4+ scores are in and around the runways, taxi and parking areas.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_SFV69QcbFS.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_SFV69QcbFS.jpg"></a></p><p>There aren't many detections around where I suspect there are aircraft. I'll first show an RGB image of the satellite terminal parking area to help give some context.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_F323bHOojH.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_F323bHOojH.jpg"></a></p><p>To the right of the image seems to be a lot of aircraft that haven't been detected. It's the same story when I look around the other parking areas in this results set as well.</p>
<p><a href="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_h1BJIVfqgt.jpg"><img alt="Umbra + MSFA" src="https://tech.marksblogg.com/theme/images/umbra-msfa/qgis-bin_h1BJIVfqgt.jpg"></a></p><p>I would accept a lot of permanent infrastructure being falsely detected as an aircraft because I could geofence the results to just the areas aircraft can park or taxi. But in this case, the model is missing the aircraft altogether.</p>
</div>
<div id="further-research">
<h2>Further Research</h2>
<p>As I uncover more insights, I'll update this post with my findings.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
    </channel>
</rss>