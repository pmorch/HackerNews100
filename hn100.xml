<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Mon, 29 Dec 2025 21:30:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google is dead. Where do we go now? (144 pts)]]></title>
            <link>https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/</link>
            <guid>46425198</guid>
            <pubDate>Mon, 29 Dec 2025 20:29:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/">https://www.circusscientist.com/2025/12/29/google-is-dead-where-do-we-go-now/</a>, See on <a href="https://news.ycombinator.com/item?id=46425198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page">
		<p><a href="#content">
			Skip to content		</a></p><!-- .site-header -->

		<div id="content">
	<main id="main">
		
<article id="post-3482">
	<!-- .entry-header -->

	
	
	<div>
		
<div><p>It’s anecdotal, I know, but my main <a href="https://bigtop.co.za/" target="_blank" rel="noreferrer noopener">entertainment business</a> revenue is down 50% over the past 3 months. Our main paid source of leads was Google Ads, which have served us well over the past 10 years or so – I think I know what I am doing in adwords by now. </p><p>Once per month I check the analytics, updating keywords and tweaking ad campaigns. Over the past year we increased our budget, and then I started looking at it once per week, running simultaneous campaigns with different settings, just trying to get SOMETHING. </p><p>Last month Google gave us a bonus – free money! This was 5x our monthly ad spend, to spend just when we needed it most – over the December holidays. I added another new campaign, updated the budgets for the existing ones. Still no change. The last week there was money to burn, left over from unused ad spend. I increased our budget to 10x. ZERO RETURN. </p><p>The money ran out. I am not putting more in. Where do we go from here? </p></div>



<h2>What is next</h2>



<p>Research shows that many young people are getting their information from short video platforms like TikTok and Instagram. We are trying ads on there. </p>



<p><br>Our customer base is comprised of 50% returning customers (I am proud of that statistic!). We have an email newsletter, we started sending them regularly over the past 2 months. Remember us? </p>



<div><p>We also plan to do some actual physical advertising – I am going to a market next weekend, doing a free show or two, handing out cards. </p><p>Also, we are branching out – I have some projects I want to make, related to the <a href="http://www.patreon.com/c/CircusScientist" target="_blank" rel="noreferrer noopener">Magic Poi project</a>, and hopefully sell. We ordered supplies last week. </p><p>Right now, though – I’m broke. Anyone need a <a href="https://devsoft.co.za/" target="_blank" rel="noreferrer noopener">website or IOT project built</a>? I am AI assisted, very fast! </p></div>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article><!-- #post-3482 -->

<!-- .comments-area -->

	<nav aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
	</main><!-- .site-main -->

	
</div><!-- .site-content -->

		<!-- .site-footer -->
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLMs Are Not Fun (180 pts)]]></title>
            <link>https://orib.dev/nofun.html</link>
            <guid>46424136</guid>
            <pubDate>Mon, 29 Dec 2025 19:06:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://orib.dev/nofun.html">https://orib.dev/nofun.html</a>, See on <a href="https://news.ycombinator.com/item?id=46424136">Hacker News</a></p>
<div id="readability-page-1" class="page">
<h2><a href="https://orib.dev/">orib.dev</a>: LLMS Are Not Fun</h2>

<p>I got into programming because it let me turn thought into
reality.  If I could reason precisely, then a computer could
carry out my thoughts unerringly. I could step away from tiring,
ambiguous human interaction for a while.</p>

<p>Over time, I warmed up to people.  The most fun I've had in
my career has been watching my colleagues improve. Handing them
projects just outside the range of their abilities, and giving
them an environment where they were unafraid to fail.</p>

<p>Today, I consult with several companies on all aspects of
building software.  Sometimes, that means thinking through
product direction and building a team to execute on it. 
Sometimes, it means designing system architecture and writing code.</p>

<p>I would not be doing my duty to my clients if I wasn't
able to guide them on the effective use of LLMs.  So I,
through gritted teeth, regularly use LLMs for projects.</p>

<p>Some people describe LLMs as the ultimate programming tool.
Others describe it as an extra machine teammate. It's a dark
parody of both.</p>

<p>LLMs are not fun.</p>

<p>For me, the joy of programming is understanding a problem
in full depth, so that when considering a change, I can follow
the ripples through the connected components of the system.</p>

<p>The joy of management is seeing my colleagues learn and
excel, carving their own paths as they grow. Watching them
rise to new challenges. As they grow, I learn from their
growth; mentoring benefits the mentor alongside the mentee.</p>

<p>Using LLMs undercuts both.</p>

<p>On the engineering side, using LLMs to write code is as fun as
hiring a taskrabbit to solve my jigsaw puzzles. And if you think
of LLMs as an extra teammate, there's no fun in managing them either.
Nurturing the personal growth of an LLM is an obvious waste of time.
Micromanaging them, watching to preempt slop and derailment, is
frustrating and rage-inducing.</p>

<p>I can make effective use of LLMs. It merely costs me my care
for my craft, and my joy in its practice.</p>



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[List of domains censored by German ISPs (179 pts)]]></title>
            <link>https://cuiiliste.de/domains</link>
            <guid>46423566</guid>
            <pubDate>Mon, 29 Dec 2025 18:21:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cuiiliste.de/domains">https://cuiiliste.de/domains</a>, See on <a href="https://news.ycombinator.com/item?id=46423566">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><p><a href="https://cuiiliste.de/"><span>CUII</span><span>Liste.de</span></a></p><div><ul><!--[--><li><a href="https://cuiiliste.de/"><div><p><span><!--[-->Home<!--]--></span></p><p><span><!--[-->Home<!--]--></span></p></div></a></li><li><a aria-current="page" href="https://cuiiliste.de/domains"><div><p><span><!--[-->Gesperrte Domains<!--]--></span></p><!----></div></a></li><li><a href="https://cuiiliste.de/probe"><div><p><span><!--[-->Domain hinzufügen<!--]--></span></p><p><span><!--[-->Domain hinzufügen<!--]--></span></p></div></a></li><li><a href="https://cuiiliste.de/isp"><div><p><span><!--[-->Bin ich betroffen?<!--]--></span></p><p><span><!--[-->Bin ich betroffen?<!--]--></span></p></div></a></li><li><a href="https://cuiiliste.de/umgehen"><div><p><span><!--[-->Zensur umgehen<!--]--></span></p><p><span><!--[-->Zensur umgehen<!--]--></span></p></div></a></li><li><a href="https://cuiiliste.de/about"><div><p><span><!--[-->Über uns<!--]--></span></p><p><span><!--[-->Über uns<!--]--></span></p></div></a></li><!--]--></ul></div></div><section><!--[--><!--[--><h2>Von der <span>CUII</span> gesperrte Domains</h2><!--]--><!--]--></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tesla's 4680 battery supply chain collapses as partner writes down deal by 99% (247 pts)]]></title>
            <link>https://electrek.co/2025/12/29/tesla-4680-battery-supply-chain-collapses-partner-writes-down-dea/</link>
            <guid>46423290</guid>
            <pubDate>Mon, 29 Dec 2025 17:57:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://electrek.co/2025/12/29/tesla-4680-battery-supply-chain-collapses-partner-writes-down-dea/">https://electrek.co/2025/12/29/tesla-4680-battery-supply-chain-collapses-partner-writes-down-dea/</a>, See on <a href="https://news.ycombinator.com/item?id=46423290">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>

	<img width="1600" height="800" src="https://electrek.co/wp-content/uploads/sites/3/2021/10/Tesla-4680-Battery-cell.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Tesla 4680 Battery cell" srcset="https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/10/Tesla-4680-Battery-cell.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/10/Tesla-4680-Battery-cell.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/10/Tesla-4680-Battery-cell.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/electrek.co/wp-content/uploads/sites/3/2021/10/Tesla-4680-Battery-cell.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high">
	</figure>

<p>A major link in Tesla’s 4680 battery supply chain has just snapped. South Korean battery material supplier L&amp;F Co. announced today that the value of its massive supply deal with Tesla has been slashed by over 99%, signaling a catastrophic drop in demand for the automaker’s in-house battery cells.</p>



<p>This is arguably the strongest evidence yet that Tesla’s 4680 program, and the vehicle that relies on it, the Cybertruck, is in serious trouble.</p>



<p>In early 2023, L&amp;F <a href="https://www.reuters.com/world/asia-pacific/south-koreas-lf-says-value-battery-material-supply-deal-with-tesla-cut-7386-2025-12-29/" target="_blank" rel="noreferrer noopener">announced a $2.9 billion contract</a> to supply high-nickel cathode materials directly to Tesla.</p>



<p>At the time, the industry saw this as a major move by Tesla to secure materials for its ramp-up of the 4680 battery cell, which Elon Musk had touted as the key to halving battery costs and enabling cheaper electric vehicles, a plan he later scrapped.</p>	
	



<p>Right now, Tesla’s Cybertruck is the only vehicle using the automaker’s own 4680 cells.</p>



<p>In a regulatory filing today, L&amp;F revealed that the contract’s value has been written down to just $7,386.</p>



<p>No, that is not a typo. $2.9 billion to roughly $7,400.</p>



<p>L&amp;F did not explicitly state the reason for the cut, citing only a “change in supply quantity,” but the dots are easy to connect. The high-nickel cathode was destined for Tesla’s 4680 cells, and the primary consumer of those cells is the Cybertruck.</p>



<p>We have been reporting on the Cybertruck’s demand issues for the better part of this year. In March, we noted that the truck was <a target="_blank" rel="noreferrer noopener" href="https://electrek.co/2025/03/06/tesla-now-offers-discounted-financing-on-cybertruck-as-the-truck-turns-out-to-be-a-flop/">turning out to be a flop</a> as Tesla began offering discounted financing to move inventory. By June, Tesla <a target="_blank" rel="noreferrer noopener" href="https://electrek.co/2025/06/05/tesla-goes-full-desperate-cybertruck-with-biggest-discount-yet/">became desperate</a>, launching 0% APR incentives as inventory piled up in lots across the US.</p>



<p>Despite a production capacity of 250,000 units per year at Giga Texas, the Cybertruck is currently selling at a run rate of roughly 20,000 to 25,000 units annually. We even saw Tesla <a target="_blank" rel="noreferrer noopener" href="https://electrek.co/2025/09/13/tesla-discontinues-cheapest-cybertruck/">discontinue the cheapest Cybertruck</a> in September because, frankly, no one wanted a gutted version of a truck that was already struggling to find buyers.</p>



<p>If Tesla isn’t building Cybertrucks, it doesn’t need 4680 cells. And if it doesn’t need 4680 cells, L&amp;F has no one to sell its cathode material to.</p>



<h3 id="h-electrek-s-take">Electrek’s Take</h3>



<p>This is not a good look Tesla’s 4680 program.</p>



<p>For years, we’ve been told that the 4680 cell was the “holy grail” that would allow Tesla to produce a $25,000 electric car. But five years after Battery Day, the cells are still reportedly difficult to manufacture at scale due to the dry electrode process, and their only application is a low-volume pickup truck that has become a commercial failure.</p>



<p>The math here is brutal. A 99% reduction in a supply contract basically means the contract was cancelled. It means Tesla is not ramping 4680 production; if anything, they might be winding it down.</p>




	<p>The ‘Cybercab’ is also supposed to be using the 4680 cells, but we will have to wait and see how that goes.</p>



<p>It’s also a vehicle program that could go the way of the Cybertruck. CEO Elon Musk is insisting that it will launch in early 2026 without a steering wheel, but Tesla has yet to solve level 4 autonomous driving.</p>



<p>If it does launch without a steering wheel, it will be a program even more limited in volume than the Cybertruck.</p>



<p><em>The battery supply situation and the critical minerals behind it are evolving fast, and China controls most of it. In <a href="https://zalkon.substack.com/p/china-is-running-out-of-some-crticial">a new Substack, I shared a full list</a> of the years of reserve remaining for each mineral.</em></p>
	<p><a target="_blank" rel="nofollow" href="https://google.com/preferences/source?q=https://electrek.co" aria-label="Add Electrek as a preferred source on Google">
			<img decoding="async" src="https://electrek.co/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-dark.png" alt="Add Electrek as a preferred source on Google">
			<img decoding="async" src="https://electrek.co/wp-content/themes/ninetofive/dist/images/google-preferred-source-badge-light.png" alt="Add Electrek as a preferred source on Google">
		</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://electrek.co/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nvidia takes $5B stake in Intel under September agreement (144 pts)]]></title>
            <link>https://www.reuters.com/legal/transactional/nvidia-takes-5-billion-stake-intel-under-september-agreement-2025-12-29/</link>
            <guid>46423010</guid>
            <pubDate>Mon, 29 Dec 2025 17:32:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/legal/transactional/nvidia-takes-5-billion-stake-intel-under-september-agreement-2025-12-29/">https://www.reuters.com/legal/transactional/nvidia-takes-5-billion-stake-intel-under-september-agreement-2025-12-29/</a>, See on <a href="https://news.ycombinator.com/item?id=46423010">Hacker News</a></p>
Couldn't get https://www.reuters.com/legal/transactional/nvidia-takes-5-billion-stake-intel-under-september-agreement-2025-12-29/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[GOG is getting acquired by its original co-founder: What it means for you (426 pts)]]></title>
            <link>https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/</link>
            <guid>46422412</guid>
            <pubDate>Mon, 29 Dec 2025 16:43:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/">https://www.gog.com/blog/gog-is-getting-acquired-by-its-original-co-founder-what-it-means-for-you/</a>, See on <a href="https://news.ycombinator.com/item?id=46422412">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-3173">
  
  <div data-hide-featured-media="0">
<p>Hey everyone, GOG Team here.<br></p>



<p>Today, Michał Kiciński, one of the co-founders of CD PROJEKT, and the co-founder of GOG, has acquired GOG from CD PROJEKT.</p>



<h2>Why GOG and Michal Kicinski are getting together</h2>



<p>We believe the games that shaped us deserve to stay alive: easy to find, buy, download, and play forever. But time is annoyingly good at erasing them. Rights get tangled, compatibility breaks, builds disappear, and a nostalgic evening often turns into a troubleshooting session. That’s the difference between “I’m playing today” (the game lives on) and “I’ll play someday” (the game dies).</p>



<p>As Michał put it: “GOG stands for freedom, independence, and genuine control.”</p>



<p>GOG has always been built on strong values and clear principles. When Marcin Iwiński and Michał Kiciński first came up with the idea for GOG in 2007, the vision was simple: bring classic games back to players, and make sure that once you buy a game, it truly belongs to you, forever. In a market increasingly defined by mandatory clients and closed ecosystems, that philosophy feels more relevant than ever.</p>



<p>This new chapter is about doubling down on that vision. We want to do more to preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow, including new games with real retro spirit.</p>



<h2>What it means for you</h2>



<p>First of all, DRM-free is more central to GOG than ever. Your library stays yours to enjoy: same access, same offline installers, same sense of ownership. Your data stays with GOG, and GOG GALAXY remains optional.</p>



<p>We’ll keep our relationship with CD PROJEKT. CD PROJEKT RED games will continue to be available on <a href="http://gog.com/" rel="nofollow noopener" target="_blank">GOG</a>, and upcoming titles from the studio will also be released on the platform.&nbsp;</p>



<p>If you’re a GOG Patron, or you donate to support the Preservation Program, those funds stay within GOG. Your support has been huge this year, and we think that with your help, we can undertake even more ambitious rescue missions in 2026 and 2027. We’ll have more to say about that sometime in 2026.</p>



<p>GOG will remain&nbsp; independent in its operations. We will continue building a platform that’s ethical, non-predatory, and made to last, while helping indie developers reach the world. We’re also committed to giving the community a stronger voice, with new initiatives planned for 2026.</p>



<p>Thanks for being the reason this all matters.</p>



<p>A lot of companies sell games. Fewer do the unglamorous work of making sure the games that shaped people’s lives don’t quietly rot into incompatibility.</p>



<p>Thanks for caring about this mission with us. We’ll keep you posted as we ship, and in the meantime, you can dig into the full FAQ for the detailed answers.</p>











<h2>FAQ</h2>


<div id="rank-math-faq">
<div id="faq-question-1767024478694">
<h3><strong>What is happening?</strong></h3>
<p>Michał Kiciński, the original co-founder of GOG and co-founder of CD PROJEKT, has acquired GOG from CD PROJEKT. GOG will continue operating as GOG, a distinct company, with the same mission to Make Games Live Forever.</p>
</div>
<div id="faq-question-1767024494514">
<h3><strong>What is GOG’s position in this?</strong></h3>
<p>To us at GOG, this feels like the best way to accelerate what is unique about GOG. Michał Kiciński is one of the people who created GOG around a simple idea: bring classic games back, and make sure that once you purchase a game, you have control over it forever. With him acquiring GOG, we keep long-term backing that is aligned with our values: freedom, independence, control, and making games stay playable over time.</p>
</div>
<div id="faq-question-1767024505630">
<h3><strong>Why is Michał Kiciński doing this?</strong></h3>
<p>Because he wants to preserve and grow the original philosophy behind GOG. In a PC market that keeps moving toward mandatory clients and closed ecosystems, he believes GOG’s approach is more relevant than ever: no lock-in, no forced platforms, sense of ownership. His goal is to keep supporting both gamers and developers, and strengthen GOG’s mission: preserve the classics of the past, celebrate standout games of today, and help shape the classics of tomorrow.</p>
</div>
<div id="faq-question-1767024516030">
<h3><strong>Why is CD PROJEKT doing this?</strong></h3>
<p>Selling GOG fits CD PROJEKT’s long-term strategy. CD PROJEKT wants to focus its full attention on creating top-quality RPGs and providing our fans with other forms of entertainment based on our brands. This deal lets CD PROJEKT keep that focus, while GOG gets stronger backing to pursue its own mission.</p>
</div>
<div id="faq-question-1767024528013">
<h3><strong>Does the mission of GOG change?</strong></h3>
<p>No. Our mission remains to Make Games Live Forever.</p>
</div>
<div id="faq-question-1767024541869">
<h3><strong>Is DRM-free still central to GOG?</strong></h3>
<p>Yes. DRM-free is more central to GOG than ever.</p>
</div>
<div id="faq-question-1767024551235">
<h3><strong>What happens to my GOG account?</strong></h3>
<p>Nothing changes. Your account stays a GOG account.</p>
</div>
<div id="faq-question-1767024556896">
<h3><strong>Is GOG financially unstable?</strong></h3>
<p>No. GOG is stable and has had a really encouraging year. In fact, we’ve seen more enthusiasm from gamers towards our mission than ever before.</p>
</div>
<div id="faq-question-1767024569496">
<h3><strong>Will my tips or GOG Patrons donations be shared with Michał Kiciński, or any other party?</strong></h3>
<p>No. These funds stay within GOG to support preservation work, and they are not shared with publishers or other companies.</p>
</div>
<div id="faq-question-1767024582077">
<h3><strong>What happens to my library?</strong></h3>
<p>Nothing. Your library remains yours to enjoy, even if a game gets delisted, as it always has.</p>
</div>
<div id="faq-question-1767024620707">
<h3><strong>Can I still download offline installers?</strong></h3>
<p>Yes.</p>
</div>
<div id="faq-question-1767024630144">
<h3><strong>Will you share my data with Michał Kiciński, or other parties?</strong></h3>
<p>No. GOG remains the controller of your data, and nothing changes here.</p>
</div>
<div id="faq-question-1767024638626">
<h3><strong>Will CD PROJEKT RED games continue to release on GOG?</strong></h3>
<p>CD PROJEKT RED games will continue to be available on GOG, and upcoming titles from the studio will also be released on the platfo<strong>rm.</strong></p>
</div>
</div></div><!--/inner-wrap-->
    
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Static Allocation with Zig (130 pts)]]></title>
            <link>https://nickmonad.blog/2025/static-allocation-with-zig-kv/</link>
            <guid>46422009</guid>
            <pubDate>Mon, 29 Dec 2025 16:07:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nickmonad.blog/2025/static-allocation-with-zig-kv/">https://nickmonad.blog/2025/static-allocation-with-zig-kv/</a>, See on <a href="https://news.ycombinator.com/item?id=46422009">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
            <p>Over the past few months I've been chipping away at a small Redis-compatible key/value server called
<a rel="noopener" target="_blank" href="https://github.com/nickmonad/kv"><code>kv</code></a>. The goal is to have something (mostly) production-ready, while implementing
only a small subset of commands. The world doesn't necessarily need another key/value store, I'm just interested in
implementing it in Zig and learning about some new (to me) techniques for systems programming.</p>
<p>One of those techniques is static memory allocation during initialization. The idea here is that all memory is requested
and allocated from the OS <em>at startup</em>, and held until termination. I first heard about this while learning about
<a rel="noopener" target="_blank" href="https://tigerbeetle.com/">TigerBeetle</a>, and they reference it explicitly in their development style guide dubbed "TigerStyle".</p>
<blockquote>
<p>All memory must be statically allocated at startup. <strong>No memory may be dynamically allocated (or freed and reallocated)
after initialization.</strong> This avoids unpredictable behavior that can significantly affect performance, and avoids use-after-free.
As a second-order effect, it is our experience that this also makes for more efficient, simpler designs that are more
performant and easier to maintain and reason about, compared to designs that do not consider all possible memory usage
patterns upfront as part of the design.</p>
<p><a rel="noopener" target="_blank" href="https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md">TigerStyle</a></p>
</blockquote>
<p>Although, this isn't as straightforward as it might sound at first. The first question that comes to mind might be:
"How much memory do I allocate?" Of course, the answer depends on the system. If we're writing a server, how many
concurrent connections do we allow? How much space is each connection allowed to work with? How much data do we expect
to process at any given time? Are there limits in response size? Do we need all the data at once, or can it streamed in
some fashion?</p>
<p>These are all questions that depend on the nature of the system and the context in which it will operate. I believe
that going through the exercise of answering these questions is ultimately a good thing, as it seems to have a strong
possibility of resulting in more stable systems, and forces us to understand the nature of our program at a deeper level.<sup id="fr-1-1"><a href="#fn-1">1</a></sup></p>
<p>On the language front, I feel like Zig is currently the best option out there for doing this with <em>relative</em> ease,
considering its design choices around explicit memory allocation and the <code>std.mem.Allocator</code> interface, which allows
the standard library to ship with a variety of different allocators.</p>
<p>Let's take a look at how we can manage static allocation in <code>kv</code>, considering three areas of request handling in
sequence: connection handling, command parsing, and key/value storage.</p>
<blockquote>
<p>A lot of this is pretty new to me, and I'm still wrestling with all these concepts. (And learning Zig!) I'm sure
there are better ways of handling this stuff. I'm presenting this as one possible implementation completed as a
learning exercise. I'll speak more about the trade-offs and where I think it can go further at the end of this post.</p>
</blockquote>
<h2 id="connection-handling">Connection Handling</h2>
<p>The first thing we have to consider is how data comes into the system, which we'll maintain through the concept
of a <code>Connection</code>.</p>
<p>A connection represents the communication to a particular client that wants to access the key/value store.
Since we're using <code>io_uring</code> for asynchronous I/O, we have to keep some information around through the life-cycle of
a request, so the kernel can use it. The space for that information is what we'll statically allocate and re-use across
different connections as they come and go.</p>
<pre data-lang="zig"><code data-lang="zig"><span>const </span><span>Connection = </span><span>struct </span><span>{
</span><span>	</span><span>completion</span><span>: </span><span>Completion </span><span>= </span><span>undefined</span><span>,
</span><span>	</span><span>client</span><span>: </span><span>posix.socket_t </span><span>= </span><span>undefined</span><span>,
</span><span>	
</span><span>	</span><span>recv_buffer</span><span>: *</span><span>ByteArray</span><span>,
</span><span>	</span><span>send_buffer</span><span>: *</span><span>ByteArray</span><span>,
</span><span>};
</span></code></pre>
<blockquote>
<p>Connections also must maintain something called a "completion". This detail is related to integration with <code>io_uring</code>,
the full details of which are outside the scope of this post. There are some good resources
<a rel="noopener" target="_blank" href="https://unixism.net/loti/index.html">here</a> and <a rel="noopener" target="_blank" href="https://man7.org/linux/man-pages/man7/io_uring.7.html">here</a>.
I also took some inspiration from TigerBeetle's <a rel="noopener" target="_blank" href="https://github.com/tigerbeetle/tigerbeetle/blob/522c0b9d15b112b20c57903987f140f1c16f627b/src/io/linux.zig">IO module</a>.</p>
</blockquote>
<p>During initialization, we create three pools: one for the Connection structs themselves, one for receive buffers
(requests), and one for send buffers (responses). When a request comes in to the server, a Connection is pulled
from a <code>std.heap.MemoryPool</code>, and then two buffers are associated with that <code>Connection</code>. The buffers are
implemented as <code>ByteArray</code> structs, which are in turn allocated as part of a <code>ByteArrayPool</code>. The <code>ByteArrayPool</code>
is custom and uses a <a rel="noopener" target="_blank" href="https://en.wikipedia.org/wiki/Free_list">free list</a> to keep track of which buffers are available
to reserve for a new connection.</p>
<pre data-lang="zig"><code data-lang="zig"><span>const </span><span>ConnectionPool = </span><span>struct </span><span>{
</span><span>    </span><span>const</span><span> Pool = std.heap.</span><span>MemoryPoolExtra</span><span>(Connection, .{ .growable = </span><span>false </span><span>});
</span><span>
</span><span>    </span><span>recv_buffers</span><span>: </span><span>ByteArrayPool</span><span>,
</span><span>    </span><span>send_buffers</span><span>: </span><span>ByteArrayPool</span><span>,
</span><span>
</span><span>    </span><span>connections</span><span>: </span><span>Pool</span><span>,
</span><span>
</span><span>    </span><span>fn </span><span>init</span><span>(
</span><span>        </span><span>config</span><span>: </span><span>Config</span><span>,
</span><span>        </span><span>gpa</span><span>: </span><span>std.mem.Allocator</span><span>,
</span><span>    ) !</span><span>ConnectionPool </span><span>{
</span><span>        </span><span>const</span><span> allocation = config.</span><span>allocation</span><span>();
</span><span>        </span><span>const</span><span> recv_size = allocation.connection_recv_size;
</span><span>        </span><span>const</span><span> send_size = allocation.connection_send_size;
</span><span>
</span><span>        </span><span>const</span><span> pool = </span><span>try</span><span> Pool.</span><span>initPreheated</span><span>(gpa, config.connections_max);
</span><span>        </span><span>const</span><span> recv_buffers = </span><span>try</span><span> ByteArrayPool.</span><span>init</span><span>(gpa, config.connections_max, recv_size);
</span><span>        </span><span>const</span><span> send_buffers = </span><span>try</span><span> ByteArrayPool.</span><span>init</span><span>(gpa, config.connections_max, send_size);
</span><span>
</span><span>        </span><span>return</span><span> .{
</span><span>            .recv_buffers = recv_buffers,
</span><span>            .send_buffers = send_buffers,
</span><span>            .connections = pool,
</span><span>        };
</span><span>    }
</span><span>	
</span><span>...
</span><span>};
</span></code></pre>
<p><img src="https://nickmonad.blog/images/kv-connection-pool.png" alt="A diagram showing how the kv connection pool works. A connection has two buffers, one for receive and one for send. Each come from a separate pool."></p>
<p>At runtime, connections are created and destroyed (marked as available) using these pools and no actual allocation
needs to happen. If no <code>Connection</code> is available in the pool, the request is rejected and the client will have to try again.</p>
<p>This does mean that the server must be configured with an upper limit on the number of connections. Each connection
must also have a limit on how much data it can receive and send.</p>
<p>At first this might seem limiting, but in practice, it creates a more robust system. Databases in particular will
enforce a limit on the number of active connections for that exact reason! For a backend, networked system like <code>kv</code>,
I would say something like 1000 active connections is a pretty reasonable limit. For a public facing system you'd
likely want more. Of course, this should all be configurable by the user.</p>
<p>The <code>Config</code> struct given to the <code>ConnectionPool</code> represents these user-configured options. Note that the <code>Config</code>
struct has a method <code>.allocation()</code> which is computed after the options have been set. In this case,
the <code>connection_recv_size</code> and <code>connection_send_size</code> depend on other options, such as <code>config.key_count</code> and
<code>config.key_size_max</code>. We'll revisit those later.</p>
<p>Now that data can get into the system, the next step is to parse out Redis commands.</p>
<h2 id="command-parsing">Command Parsing</h2>
<p>In an attempt to be compatible with Redis (at least a very small subset of it), <code>kv</code> has to parse incoming commands
following the <a rel="noopener" target="_blank" href="https://redis.io/docs/latest/develop/reference/protocol-spec/">Redis serialization protocol</a> ("RESP") format.</p>
<p>Here's an example of an incoming <code>GET key</code> command.</p>
<pre><code><span>*2\r\n$3\r\nGET\r\n$3\r\nkey\r\n
</span></code></pre>
<p>I won't go into detail on how these commands are structured, the RESP document will do a much better job there.
Basically, what we're looking at is "Here's an array with 2 elements. The first element has 3 characters,
with the content <code>GET</code> and the second element has 3 characters, with the contenhttps://github.com/nickmonad/kvt <code>key</code>."</p>
<p>In order to parse this command, we need to look at the buffer that contains the request data, create some kind of
iterator over that buffer, and split each entry on the CRLF <code>\r\n</code> byte sequence. Here's the signature for <code>parse</code>,
the function that does just that.</p>
<pre data-lang="zig"><code data-lang="zig"><span>pub fn </span><span>parse</span><span>(</span><span>config</span><span>: </span><span>Config</span><span>, </span><span>alloc</span><span>: </span><span>std.mem.Allocator</span><span>, </span><span>buf</span><span>: []</span><span>const u8</span><span>) !</span><span>Command
</span></code></pre>
<p>The allocator is used to create some book-keeping structure as we parse through the command. We need to create a list
of <code>[]const u8</code> slices that points into the whole buffer and then is given to a command's <code>parse()</code> function, once
we know the command. This has the benefit of being a "zero copy" approach to parsing. No request data needs to be copied,
only pointed to.</p>
<p>Zig's <code>std.heap.FixedBufferAllocator</code> is perfect for this kind of operation. During initialization, we ask for buffer
space from a general purpose allocator, and pass it to the <code>FixedBufferAllocator</code>. This allocator works as "bump" allocator,
where each internal allocation happens in a linear fashion, up to the amount of available space. The trade-off here is
that memory allocated within the fixed buffer can't be free'd directly. Instead, the entire buffer is reset after use,
which simply resets an index back to <code>0</code>. (Just about as cheap as an operation can get!)</p>
<p>Since our server is single-threaded<sup id="fr-2-1"><a href="#fn-2">2</a></sup> and processes one request at a time, we can re-use this <code>FixedBufferAllocator</code>
across every request. After the request is processed, the response is copied to a <code>Writer</code> object backed by the
connection's <code>send</code> buffer and the <code>FixedBufferAllocator</code> is reset for the next request.</p>
<p><img src="https://nickmonad.blog/images/kv-parsing-state.png" alt="A diagram showing how parsing state works in kv."></p>
<p>Knowing how much space to give the <code>FixedBufferAllocator</code> depends again on our system configuration. We need space for
the <code>ArrayList</code> of parsed command items, and space for any copied list items that are written back as a response during
command execution. Parsing must be able to support the largest possible command (a list <code>PUSH</code> of maximum size/length) and
copying has to support the largest possible response (again, a maximally sized list).</p>
<p>Copying has the extra consideration that we have to actually store the copied list items, which are duplicated when
read from the key/value store. During parsing, we just need to keep <em>slices</em> into the request buffer. For the copied
items, we need to keep a list of slices that point to the items, and the items themselves. As long as we give the
<code>FixedBufferAllocator</code> space, we can use it for all these (sub-)allocations.</p>
<pre data-lang="zig"><code data-lang="zig"><span>pub const </span><span>Runner = </span><span>struct </span><span>{
</span><span>    </span><span>config</span><span>: </span><span>Config</span><span>,
</span><span>    </span><span>fba</span><span>: </span><span>std.heap.FixedBufferAllocator</span><span>,
</span><span>    </span><span>kv</span><span>: *</span><span>Store</span><span>,
</span><span>
</span><span>    </span><span>pub fn </span><span>init</span><span>(</span><span>config</span><span>: </span><span>Config</span><span>, </span><span>gpa</span><span>: </span><span>std.mem.Allocator</span><span>, </span><span>kv</span><span>: *</span><span>Store</span><span>) !</span><span>Runner </span><span>{
</span><span>        </span><span>const</span><span> L = config.list_length_max;
</span><span>        </span><span>const</span><span> V = config.val_size_max;
</span><span>
</span><span>        </span><span>// ArrayList([]const u8) of largest possible command.
</span><span>        </span><span>// "[L/R]PUSH list item1 item2 ... itemL"
</span><span>        </span><span>const</span><span> parse_cap = (</span><span>1 </span><span>+ </span><span>1 </span><span>+ L);
</span><span>        </span><span>const </span><span>parse_size</span><span>: </span><span>u64 </span><span>= (parse_cap * </span><span>@sizeOf</span><span>([]</span><span>const</span><span> u8));
</span><span>
</span><span>        </span><span>// ArrayList([]const u8) pointing to duplicated values.
</span><span>        </span><span>const</span><span> copy_size = (L * </span><span>@sizeOf</span><span>([]</span><span>const</span><span> u8));
</span><span>        </span><span>const</span><span> copy_data = (L * V);
</span><span>
</span><span>        </span><span>const </span><span>fba_size</span><span>: </span><span>u64 </span><span>= parse_size + copy_size + copy_data;
</span><span>        </span><span>const</span><span> buffer = </span><span>try</span><span> gpa.</span><span>alloc</span><span>(u8, fba_size);
</span><span>        </span><span>const</span><span> fba = std.heap.FixedBufferAllocator.</span><span>init</span><span>(buffer);
</span><span>
</span><span>        </span><span>return</span><span> .{
</span><span>            .config = config,
</span><span>            .fba = fba,
</span><span>            .kv = kv,
</span><span>        };
</span><span>    }
</span><span>...
</span><span>};
</span></code></pre>
<p><img src="https://nickmonad.blog/images/kv-fixed-buffer-space.png" alt="A diagram showing how the Fixed Buffer Allocator is utilized in kv. The first section is the parse array, the second is the copy array, and the third is copy data."></p>
<p>The underlying <code>Store</code> will use the <code>FixedBufferAllocator</code> to allocate an <code>ArrayList</code> of fixed capacity
(determined by <code>config.list_length_max</code>) and then use the remaining space in the allocator for the copied data.</p>
<p>Hopefully all is clear so far! Now we can move on to the core of the system: key/value storage.</p>
<h2 id="key-value-storage">Key/Value Storage</h2>
<p>Perhaps obviously, the fundamental data structure in <code>kv</code> is a hash map, used to associate user provided keys with
user provided values.</p>
<p>Without looking too closely at the standard library, if you grab one of the provided hash map implementations,
it will accept a <code>std.mem.Allocator</code> and hold onto the allocator for the lifetime of the map. When a key/value pair
is added to the map, it will use that same allocator and request the appropriate amount of memory to store that data.
This won't work for our case though, since we need to control allocation prior to adding any data to the map.</p>
<p>Fortunately, Zig also provides an "unmanaged" version of a hash map. Generally, these unmanaged versions of data
structures in the standard library mean that an allocator is not held by the structure itself. It's up to us to provide
that allocator when needed. A cool trick we can play with an unmanaged map is ask it to ensure it has enough capacity
up front, and then "assume" that capacity during runtime when adding data to it. The hashing and internal details are
still handled by the map.</p>
<pre data-lang="zig"><code data-lang="zig"><span>var </span><span>map</span><span>: std.</span><span>StringHashMapUnmanaged</span><span>(Value) =</span><span> .empty</span><span>;
</span><span>try</span><span> map.</span><span>ensureTotalCapacity</span><span>(gpa, capacity);
</span></code></pre>
<p>The <code>ensure</code> operation can fail with <code>error.OutOfMemory</code>, which is OK during initialization. But, assuming this succeeds,
we no longer need to pass an allocator when we store data in the map.</p>
<pre data-lang="zig"><code data-lang="zig"><span>store.map.</span><span>putAssumeCapacity</span><span>(key, value);
</span></code></pre>
<p>This <em>could</em> in theory fail, in which case there would an assertion failure. It's ultimately up to us to check against
the map's available capacity before calling this function. But, again, no allocation is required.</p>
<p>Since the map itself doesn't do any allocation at runtime, we have to provide space for incoming keys and values. We'll
reuse the same <code>ByteArrayPool</code> implementation that we used for connection buffers. Basically, we have a big space
allocated for keys and values and the hash map just maintains an association of pointers from keys to values.
The key/value data isn't literally <em>stored</em> "in the map." The allocation that happens in <code>ensureTotalCapacity</code> is for
the internal book-keeping structure of the map, not for the user data.</p>
<h3 id="navigating-the-map">Navigating the map</h3>
<p>At the highest level, the primary challenge with storing keys and values in a statically allocated map is that we
could get poor utilization of the allocated space, especially when we need to support keys pointed at lists as values.</p>
<p>To illustrate this, let's say our map is configured to allocate space for 5 keys and 5 values. If each key maps to one
value, we get perfect utilization. At the other extreme, if one key maps to a value containing a list of 5 elements,
we have to use all the allocated value space for this one key, preventing other keys from using any value space,
causing the map to be "biased". The store wouldn't be able to hold any more key/value pairs, even though there is
allocated memory "on the table."</p>
<p><img src="https://nickmonad.blog/images/kv-map-utilization.png" alt="A diagram showing three different possibilities of how allocated memory can be used in a static hash map."></p>
<p>Basically, the only way to mitigate this is to make sure there's enough allocated space for <em>every</em> key to hold a
list of maximum size. This definitely inflates the amount of space we have to allocate, but the alternative is a
system that doesn't support its configured properties. Every key must be <em>able</em> to store a list of maximum size,
even if they don't during actual use.</p>
<p>Another issue with static allocation in the context of a map is dealing with map deletions. Our
<code>std.StringHashMapUnmanaged</code> structure uses open-addressing and linear probing to place keys in the map when hash
collisions occur. Deletions are tricky because they can break the map's ability to know if a key is actually present
in the map. To handle this, a "tombstone" technique is used to mark a space as logically (but not physically) deleted
in order to preserve accurate lookups.</p>
<p>There's a lot more to figure out here, but it's my understanding that a map will have to periodically rehash the
keys in order to reclaim space if too many tombstones pile up. When this occurs is still a bit of mystery to me.
If it occurs when the map needs to grow to accommodate more key/value pairs, we'll never actually trigger that
condition in a static context. If it occurs at some other point, based on number of keys compared to capacity,
perhaps that could work. Or maybe, it's up to us to call <code>rehash()</code> whenever it appears there is no space left,
and try the operation again.</p>
<p>All of this considered, I think a custom map implementation is more appropriate for the context of static allocation.
This current implementation proves the concept, but definitely leaves room for improvement!</p>
<h2 id="revisiting-allocation-size">Revisiting allocation size</h2>
<p>Now that we have a method for statically allocating space for these three components (connections, parsing, and storage),
we can finally answer the first question: How much space do we allocate?</p>
<p>In this current iteration of <code>kv</code>, the answer can really only be determined after the fact, once configuration has
been set and all the allocations have been made. There are five options that can be configured by the user,
and two derived properties based on those options.</p>
<pre data-lang="zig"><code data-lang="zig"><span>pub const </span><span>Config = </span><span>struct </span><span>{
</span><span>    </span><span>/// Allocation is a calculated set of values (in bytes), based on the given configuration.
</span><span>    </span><span>/// This informs static allocation requested at initialization.
</span><span>    </span><span>pub const </span><span>Allocation = </span><span>struct </span><span>{
</span><span>        </span><span>connection_recv_size</span><span>: </span><span>u64</span><span>,
</span><span>        </span><span>connection_send_size</span><span>: </span><span>u64</span><span>,
</span><span>    };
</span><span>
</span><span>    </span><span>/// Maximum number of concurrent connections.
</span><span>    </span><span>connections_max</span><span>: </span><span>u32</span><span>,
</span><span>
</span><span>    </span><span>/// Key count is the number of possible keys we can store.
</span><span>    </span><span>/// Internally, the store will allocate (key_count * list_length_max) number
</span><span>    </span><span>/// of values, such that each key could support the maximum number of list elements.
</span><span>    </span><span>key_count</span><span>: </span><span>u32</span><span>,
</span><span>
</span><span>    </span><span>/// The maximum allowable key size in bytes.
</span><span>    </span><span>key_size_max</span><span>: </span><span>u32</span><span>,
</span><span>    </span><span>/// The maximum allowable value size in bytes.
</span><span>    </span><span>val_size_max</span><span>: </span><span>u32</span><span>,
</span><span>
</span><span>    </span><span>/// The maximum allowable length for a list as a value.
</span><span>    </span><span>list_length_max</span><span>: </span><span>u32</span><span>,
</span><span>
</span><span>    </span><span>pub fn </span><span>allocation</span><span>(</span><span>config</span><span>: </span><span>Config</span><span>) </span><span>Allocation </span><span>{
</span><span>		</span><span>// ... calculate recv and send size ....
</span><span>		
</span><span>        </span><span>return</span><span> .{
</span><span>            .connection_recv_size = connection_recv_size,
</span><span>            .connection_send_size = connection_send_size,
</span><span>        };
</span><span>    }
</span><span>};
</span></code></pre>
<p>The <code>connection_recv_size</code> and <code>connection_send_size</code> properties of <code>Allocation</code> depend on some details of the
RESP protocol, but is mostly informed by our user configuration. In the interest of wrapping this post up,
I'll gloss over those details, and encourage you to check out <a rel="noopener" target="_blank" href="https://github.com/nickmonad/kv/blob/master/src/config.zig"><code>src/config.zig</code></a>.</p>
<p>This <code>Config</code> struct doesn't directly specify <em>every</em> aspect of allocation, but it does provide the basis for it.
Something not listed in the <code>Config</code> struct directly is the "list item pool", part of the key/value <code>Store</code> struct.
When keys point to lists as values, there is a linked list backing that value in the hash map, and we need a pool of
structs to assist in the construction and iteration of that linked list.</p>
<p>With some reasonable configuration options set, let's see just how much memory we allocate!</p>
<pre data-lang="sh"><code data-lang="sh"><span>$</span><span> zig build run
</span><span>config
</span><span>  </span><span>connections_max</span><span> = 1000
</span><span>  </span><span>key_count</span><span>       = 1000
</span><span>  </span><span>key_size_max</span><span>    = 1024
</span><span>  </span><span>val_size_max</span><span>    = 4096
</span><span>  </span><span>list_length_max</span><span> = 50
</span><span>allocation
</span><span>  </span><span>connection_recv_size</span><span> = 206299
</span><span>  </span><span>connection_send_size</span><span> = 205255
</span><span>map</span><span> capacity = 2048, map size = 0, available = 1638
</span><span>total_requested_bytes</span><span> = 748213015
</span><span>ready!
</span></code></pre>
<p>Everything here is measured in bytes, so we're looking at approximately <code>750 MB</code> of memory for the given configuration.
<code>total_requested_bytes</code> is a feature of Zig's <code>std.heap.DebugAllocator</code>. The exact number of bytes will be different
on each run, although it will hover around that value. I think the reason for this is how Zig requests <em>pages</em> of
memory from the OS. It won't always be the same and the OS is very likely doing some fancy book-keeping of its own.</p>
<p>If you play around with the configuration options and see how <code>total_requested_bytes</code> changes, it might be
surprising just how much memory is allocated up-front, before any of it is actually used! For example, if we
double <code>val_size_max</code> to <code>8192</code> and <code>list_length_max</code> to <code>100</code>, we're looking at about <code>2.8 GB</code> of allocated memory.</p>
<p>In the context of modern servers, this isn't a lot, but it can quickly grow as we adjust these parameters.
Should we be asking ourselves: Is this inefficient? What if we don't use all that memory?</p>
<p>Like all good engineering decisions, we have to consider them in the context of the problem we're trying to
solve, and the guarantees we expect from our systems. With this design, ensuring that each request and each
key/value pair <em>can</em> utilize the maximum configured space, seems like a worthy trade-off to make.</p>
<h2 id="final-thoughts">Final thoughts</h2>
<p>Like most projects, this one took a lot longer than I expected! Trying to incorporate both <code>io_uring</code> and
static allocation was something I had never done before, but I'm pretty happy with the result.</p>
<p>I'm looking forward to improving the internal hash map to better fit a static context, consider alternative
allocator implementations to improve memory utilization, and incorporate fuzz testing to find the limits of the system.</p>
<p>Checkout the code on <a rel="noopener" target="_blank" href="https://github.com/nickmonad/kv">GitHub</a>!</p>
<h3 id="notes">Notes</h3>
<section>
<ol>
<li id="fn-1">
<p>At the risk of stating the obvious, these limits can (and likely should) be configured at runtime by the user.
These aren't values that have to be set at compile time and enforced upon all users in every context,
although some might be. Again, it depends on <em>which</em> part of the system is using the memory.
The point is that once the program starts it will allocate memory, but after that, it does not. <a href="#fr-1-1">↩</a></p>
</li>
<li id="fn-2">
<p>Going with a single-threaded design simplifies a lot! Even though processing in <code>kv</code> is single-threaded,
it still enjoys the benefits of I/O concurrency via <code>io_uring</code>. The kernel handles writing responses back out to
clients and waiting for that operation to complete, so we don't have to worry (as much) about slow clients. <a href="#fr-2-1">↩</a></p>
</li>
</ol>
</section>

        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swapping SIM cards used to be easy, and then came eSIM (167 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/</link>
            <guid>46421653</guid>
            <pubDate>Mon, 29 Dec 2025 15:30:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/">https://arstechnica.com/gadgets/2025/12/i-switched-to-esim-in-2025-and-i-am-full-of-regret/</a>, See on <a href="https://news.ycombinator.com/item?id=46421653">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          
          
<h2>Maybe this isn’t a good idea</h2>
<p>Many people have had the same phone number for years—even decades at this point. These numbers aren’t just a way for people to get in touch because, stupidly, we have also settled on phone numbers as a means of authentication. Banks, messaging apps, crypto exchanges, this very website’s publishing platform, and even the carriers managing your number rely on SMS multifactor codes. And those codes <a href="https://arstechnica.com/gadgets/2025/02/google-plans-to-stop-using-insecure-sms-verification-in-gmail/">aren’t even very secure</a>.</p>
<p>So losing access to your phone number doesn’t just lock you out of your phone. Key parts of your digital life can also become inaccessible, and that could happen more often now due to the fungible nature of eSIMs.</p>
<p>Most people won’t need to move their phone number very often, but the risk that your eSIM goes up in smoke when you do is very real. Compare that to a physical SIM card, which will virtually never fail unless you damage the card. Swapping that tiny bit of plastic takes a few seconds, and it never requires you to sit on hold with your carrier’s support agents or drive to a store. In short, a physical SIM is essentially foolproof, and eSIM is not.</p>
<p>Obviously, the solution is not to remove multifactor authentication—your phone number is, unfortunately, too important to be unguarded. However, carriers’ use of SMS to control account access is self-defeating and virtually guarantees people are going to have bad experiences in the era of eSIM. Enshittification has truly come for SIM cards.</p>
<p>If this future is inevitable, there ought to be a better way to confirm account ownership when your eSIM glitches. It doesn’t matter what that is as long as SMS isn’t the default. Google actually gets this right with Fi. You can download an eSIM at any time via the Fi app, and it’s secured with the same settings as your Google account. That’s really as good as it gets for consumer security. Between Google Authenticator, passkeys, and push notifications, it’s pretty hard to get locked out of Google, even if you take advantage of advanced security features.</p>
<p>We gave up the headphone jack. We gave up the microSD card. Is all this worthwhile to boost battery capacity by 8 percent? That’s a tough sell.</p>


          
                  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Libgodc: Write Go Programs for Sega Dreamcast (168 pts)]]></title>
            <link>https://github.com/drpaneas/libgodc</link>
            <guid>46420672</guid>
            <pubDate>Mon, 29 Dec 2025 13:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/drpaneas/libgodc">https://github.com/drpaneas/libgodc</a>, See on <a href="https://news.ycombinator.com/item?id=46420672">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">libgodc - Go runtime for Sega Dreamcast</h2><a id="user-content-libgodc---go-runtime-for-sega-dreamcast" aria-label="Permalink: libgodc - Go runtime for Sega Dreamcast" href="#libgodc---go-runtime-for-sega-dreamcast"></a></p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/drpaneas/libgodc/blob/main/logo.png"><img src="https://github.com/drpaneas/libgodc/raw/main/logo.png" alt="libgodc" width="400"></a>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/drpaneas/libgodc/blob/main/examples/pong/pong.gif"><img src="https://github.com/drpaneas/libgodc/raw/main/examples/pong/pong.gif" alt="Pong" width="240" data-animated-image=""></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/drpaneas/libgodc/blob/main/examples/brkout/brkout.gif"><img src="https://github.com/drpaneas/libgodc/raw/main/examples/brkout/brkout.gif" alt="Breakout" width="240" data-animated-image=""></a>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/drpaneas/libgodc/blob/main/examples/platformer/platformer.gif"><img src="https://github.com/drpaneas/libgodc/raw/main/examples/platformer/platformer.gif" alt="Platformer" width="240" data-animated-image=""></a>
</p>
<p dir="auto">Replaces the standard Go runtime with one designed for the Dreamcast's
constraints: memory 16MB RAM, CPU single-core SH-4, no operating system. Provides garbage
collection, goroutines, channels, and the core runtime functions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Quick Start</h2><a id="user-content-quick-start" aria-label="Permalink: Quick Start" href="#quick-start"></a></p>
<p dir="auto"><strong>Prerequisites:</strong> Go 1.25.3+, <code>make</code>, and <code>git</code> must be installed.</p>
<div dir="auto" data-snippet-clipboard-copy-content="go install github.com/drpaneas/godc@latest
godc setup
godc doctor # to check (optional)"><pre>go install github.com/drpaneas/godc@latest
godc setup
godc doctor <span><span>#</span> to check (optional)</span></pre></div>
<blockquote>
<p dir="auto"><strong>Note:</strong> The <a href="https://github.com/drpaneas/godc"><code>godc</code></a> CLI tool is a separate project that handles toolchain setup and builds.</p>
</blockquote>
<p dir="auto">Create and run a project:</p>
<div dir="auto" data-snippet-clipboard-copy-content="mkdir myproject &amp;&amp; cd myproject
godc init
# write you main.go and other *.go files
godc build
godc run"><pre>mkdir myproject <span>&amp;&amp;</span> <span>cd</span> myproject
godc init
<span><span>#</span> write you main.go and other *.go files</span>
godc build
godc run</pre></div>
<p dir="auto">See the <a href="https://drpaneas.github.io/libgodc/getting-started/quick-start.html" rel="nofollow">Quick Start Guide</a> for your first program.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">📚 <strong><a href="https://drpaneas.github.io/libgodc/" rel="nofollow">Full Documentation</a></strong></p>
<ul dir="auto">
<li><a href="https://drpaneas.github.io/libgodc/getting-started/installation.html" rel="nofollow">Installation</a> — Setup and configuration</li>
<li><a href="https://drpaneas.github.io/libgodc/getting-started/quick-start.html" rel="nofollow">Quick Start</a> — First program walkthrough</li>
<li><a href="https://drpaneas.github.io/libgodc/reference/design.html" rel="nofollow">Design</a> — Runtime architecture</li>
<li><a href="https://drpaneas.github.io/libgodc/reference/effective-dreamcast-go.html" rel="nofollow">Effective Dreamcast Go</a> — Best practices</li>
<li><a href="https://drpaneas.github.io/libgodc/reference/kos-wrappers.html" rel="nofollow">KOS Wrappers</a> — Calling C from Go</li>
<li><a href="https://drpaneas.github.io/libgodc/reference/limitations.html" rel="nofollow">Limitations</a> — What doesn't work</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Performance</h2><a id="user-content-performance" aria-label="Permalink: Performance" href="#performance"></a></p>
<p dir="auto">Measured on real hardware (SH-4 @ 200MHz):</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Operation</th>
<th>Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gosched yield</td>
<td>~120 ns</td>
</tr>
<tr>
<td>Allocation</td>
<td>~186 ns</td>
</tr>
<tr>
<td>Buffered channel</td>
<td>~1.8 μs</td>
</tr>
<tr>
<td>Context switch</td>
<td>~6.4 μs</td>
</tr>
<tr>
<td>Unbuffered channel</td>
<td>~13 μs</td>
</tr>
<tr>
<td>Goroutine spawn</td>
<td>~31 μs</td>
</tr>
<tr>
<td>GC pause</td>
<td>72 μs - 6 ms</td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">The <code>examples/</code> directory contains working programs:</p>
<ul dir="auto">
<li><code>hello</code> — Minimal program (debug output)</li>
<li><code>hello_screen</code> — Hello World on screen using BIOS font</li>
<li><code>blue_screen</code> — Minimal graphics</li>
<li><code>input</code> — Controller input</li>
<li><code>goroutines</code> — Concurrent bouncing balls</li>
<li><code>channels</code> — Producer/consumer pattern</li>
<li><code>timer</code> — Frame-rate independent animation</li>
<li><code>bfont</code> — BIOS font rendering</li>
<li><code>filesystem</code> — Directory browser</li>
<li><code>vmu</code> — VMU LCD and buzzer</li>
<li><code>brkout</code> — Breakout clone (GPL v2, port of Jim Ursetto's original)</li>
<li><code>pong</code> — Pong clone with 1P/2P mode, particle effects, and AI</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">BSD 3-Clause License. See <a href="https://github.com/drpaneas/libgodc/blob/main/LICENSE">LICENSE</a> for details.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Vibe coding a bookshelf with Claude Code (239 pts)]]></title>
            <link>https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/</link>
            <guid>46420453</guid>
            <pubDate>Mon, 29 Dec 2025 13:22:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/">https://balajmarius.com/writings/vibe-coding-a-bookshelf-with-claude-code/</a>, See on <a href="https://news.ycombinator.com/item?id=46420453">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>I own more books than I can read. Not in a charming, aspirational way, but in
the practical sense that at some point I stopped knowing what I owned.
Somewhere around 500 books, memory stopped being a reliable catalog.</p>
<p>For years, I told myself I would fix this. Nothing elaborate, nothing worthy
of a startup idea. A spreadsheet would have been enough. I never did it, not
because it was hard, but because it was tedious.</p>
<figure><img src="https://balajmarius.com/images/vibe-coding-a-bookshelf-with-claude-code/part-of-my-personal-library.jpg" alt="Part of my personal library"><figcaption>Part of my personal library</figcaption></figure>
<p>The gap between intention and execution was small, but it was enough to keep the
project permanently parked in the someday pile.</p>
<p>By the end of 2025, I had been working with AI agents long enough that this
kind of project finally felt possible. Not because they made things more
impressive, but because they removed the part I always stalled on. Execution.</p>
<p>The bookshelf project is where I clearly understood what my role becomes once
execution stops being the bottleneck.</p>
<h2>The problem</h2>
<p>I tried the obvious tools first. <a href="https://apps.apple.com/us/app/isbn-scan-book-info-ratings/id6463488866">ISBN scanner apps</a> failed on Romanian
editions, and <a href="https://goodreads.com/">Goodreads</a> could not identify obscure publishers or antiquarian
finds. Anything even slightly nonstandard came back incomplete or wrong.
Partial data felt worse than no data at all, so every attempt ended the same
way: a few entries filled in, followed by abandonment.</p>
<p>What I needed was not a better app, but a way to tolerate imperfection without
the whole system falling apart.</p>
<h2>The data</h2>
<p>Every project starts with bad data, and this one started with worse data. One
afternoon, I photographed every book I own: spines, covers, duplicates, and
the occasional blurry thumb. <strong>Four hundred and seventy photos in total</strong>. Once
the images were on my laptop, I opened Claude.</p>
<figure><img src="https://balajmarius.com/images/vibe-coding-a-bookshelf-with-claude-code/470-shots-one-afternoon.jpg" alt="470 shots, one afternoon"><figcaption>470 shots, one afternoon</figcaption></figure>
<p>The first steps were mechanical. Renaming files. Converting <code>HEIC</code> to <code>JPG</code>. Then
I asked for something real: a script that sends each image to OpenAI's vision
API, extracts author, title, and publisher, normalizes names, resizes images
to avoid wasting tokens, and writes everything to a <code>JSON</code> file.</p>
<p>Claude wrote the script and ran it. It worked. Not perfectly, but well enough
to matter.</p>
<pre><code>{
  "id": "ZfEPBCMZDaCKm6k0NVJ8F",
  "title": "Simulacre și simulare",
  "author": "Jean Baudrillard",
  "publisher": "Colectia Panopticon",
  "source": "dataset/83.jpg",
},
</code></pre>
<p>Roughly 90 percent of the books came back correct. The failures were
predictable: poor lighting, damaged covers, unreadable spines. One novel was
confidently identified as a 1987 Soviet agricultural manual.</p>
<p>I fixed the rest by hand. That decision was not technical, it was judgment.
Ninety percent accuracy was enough. Chasing the remaining ten percent would
have meant days of edge cases for very little additional value. That was the
first moment where my role became clear.</p>
<p>Later, when I received a few books for Christmas, we added a second script that
runs the same pipeline for new additions. <em>Photo in, metadata and images out</em>.</p>
<h2>The covers</h2>
<p>With metadata sorted, covers were still missing. My photos showed spines, not
artwork, and I wanted a clean visual representation. Claude suggested using
<a href="https://openlibrary.org/">Open Library</a>'s API to fetch covers, which mostly worked. Half the covers were
low quality or incorrect, and Romanian editions barely existed in the
database.</p>
<p>We iterated. Claude wrote a second pass, another model call that scored cover
quality and flagged bad matches. For flagged books, it fell back to Google
Images via <a href="https://serpapi.com/">SerpAPI</a>. That handled most cases. A few remained: antiquarian finds
and obscure Soviet boxing manuals that no database was ever going to have
clean assets for.</p>
<p>I opened Photoshop and fixed ten covers by hand. For a collection of 460
books, ten manual edits felt like a win.</p>
<h2>The shelf</h2>
<p>Once the data and covers were in place, the UI came next. The obvious solution
was a grid of covers. It was correct, and it was lifeless. I kept looking at
my physical bookshelf instead. What makes it interesting is not the covers,
but the spines. Different widths, uneven pressure, colors blending into a
single texture.</p>
<figure><img src="https://balajmarius.com/images/vibe-coding-a-bookshelf-with-claude-code/the-shelf-version-zero.jpg" alt="The shelf, version zero"><figcaption>The shelf, version zero</figcaption></figure>
<p>That was the thing I wanted to recreate.</p>
<p>Claude did not invent that idea. It executed it. It wrote a script to extract
dominant colors from each cover using color quantization, then computed
contrasting text colors for readability. The result was better, but still
wrong. Every book had the same width, and real books are not like that.</p>
<p><a href="https://openlibrary.org/">Open Library</a> had page counts. We mapped page count to spine width and added
slight variation to break the uniformity. At that point, it finally looked
like a bookshelf.</p>
<pre><code>{
  "id": "ZfEPBCMZDaCKm6k0NVJ8F",
  "title": "Simulacre si simulare",
  "author": "Jean Baudrillard",
  "backgroundColor": "#f0f0ff",
  "color": "#1f1f2e",
  "paddingLeft": 13,
  "paddingRight": 13,
  "height": 384,
  "cover": "/images/bookshelf/simulacre-si-simulare@2x.webp",
  "source": "dataset/83.jpg"
},
</code></pre>
<h2>The animation</h2>
<p>Visually, the shelf worked, but it felt static. A real shelf responds to
touch. When you run your finger along the spines, they tilt slightly. I asked
Claude for an animation, and it came back with a scroll based tilt using
<a href="https://motion.dev/">Framer Motion</a>.</p>
<p>It was close, but wrong. The movement snapped instead of flowing. I did not
know why, I just knew it felt off. That was enough.</p>
<figure><video src="https://balajmarius.com/images/vibe-coding-a-bookshelf-with-claude-code/scroll-animation.webm" autoplay="" loop="" muted="" playsinline=""></video><figcaption>Scroll-based tilt animation</figcaption></figure>
<p>Claude explained the issue immediately. We were updating React state on every
scroll event, causing unnecessary re renders. The fix was to use motion values
and springs that animate outside React's render cycle. Two minutes later, it
was fixed. I spent the next few minutes scrolling back and forth, just
watching it move. This was the moment my caution dropped, not because the tool
was always right, but because the cost of trying ideas had collapsed.</p>
<h2>Killing good code</h2>
<p>That confidence had a downside. I started asking for things I did not need.
Infinite scroll seemed sensible. Why render 460 books at once? Claude
implemented it, and technically it worked. Memory stayed flat, and the DOM
updated correctly.</p>
<p>But scrolling broke. The container height desynced, the last books were
unreachable, and every attempted fix introduced new jank. The feature worked,
but the experience did not. So we removed it. Not because it was broken, but
because it was unnecessary. Four hundred and sixty books is not a scale
problem. Knowing when to delete working code is not something an AI can decide
for you.</p>
<h2>The stack view</h2>
<p>The shelf looked great on desktop, but on mobile, horizontal scrolling felt
cramped. I wanted an alternative layout: books lying flat, stacked vertically,
readable without tilting your head. I pointed Claude at the shelf
implementation and asked for a stack view.</p>
<figure><img src="https://balajmarius.com/images/vibe-coding-a-bookshelf-with-claude-code/stack-ui-on-mobile.jpg" alt="Stack UI on mobile"><figcaption>Stack UI on mobile</figcaption></figure>
<p>It read the code, inferred the patterns, and reused them: animation timing,
color extraction, scroll based opacity, the same data shape. It built the new
component and wired up a toggle between layouts. It worked without
explanation. That surprised me more than anything else.</p>
<h2>What I actually did</h2>
<p>Claude wrote all the code. So what did I do?</p>
<ul>
<li>I decided that 90 percent accuracy was enough.</li>
<li>I fixed the ten covers no API could find.</li>
<li>I rejected a grid because I wanted spines.</li>
<li>I deleted infinite scroll because I did not need it.</li>
<li>I kept scrolling the animation until it felt right.</li>
</ul>
<p>Claude handled implementation. I handled taste.</p>
<p>After years of false starts, my bookshelf finally exists. Four hundred and
sixty books, cataloged and displayed at <a href="https://balajmarius.com/bookshelf">bookshelf</a>. I almost
dismissed Claude Code as hype. Now, the times when I wrote everything by hand
feel distant, almost strange.</p>
<p>Execution keeps getting cheaper. Taste still does not.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UK accounting body to halt remote exams amid AI cheating (160 pts)]]></title>
            <link>https://www.theguardian.com/business/2025/dec/29/uk-accounting-remote-exams-ai-cheating-acca</link>
            <guid>46420289</guid>
            <pubDate>Mon, 29 Dec 2025 13:06:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/business/2025/dec/29/uk-accounting-remote-exams-ai-cheating-acca">https://www.theguardian.com/business/2025/dec/29/uk-accounting-remote-exams-ai-cheating-acca</a>, See on <a href="https://news.ycombinator.com/item?id=46420289">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>The world’s largest accounting body is to stop students being allowed to take exams remotely to crack down on a rise in cheating on tests that underpin professional qualifications.</p><p>The Association of Chartered Certified Accountants (ACCA), which has almost 260,000 members, has said that from March it will stop allowing students to take online exams in all but exceptional circumstances.</p><p>“We’re seeing the sophistication of [cheating] systems outpacing what can be put in, [in] terms of safeguards,” Helen Brand, the chief executive of the ACCA, said in an interview with the Financial Times.</p><figure id="159db197-4b56-44e8-ab42-f3a5716959a9" data-spacefinder-role="richLink" data-spacefinder-type="model.dotcomrendering.pageElements.RichLinkBlockElement"><gu-island name="RichLinkComponent" priority="feature" deferuntil="idle" props="{&quot;richLinkIndex&quot;:3,&quot;element&quot;:{&quot;_type&quot;:&quot;model.dotcomrendering.pageElements.RichLinkBlockElement&quot;,&quot;prefix&quot;:&quot;Related: &quot;,&quot;text&quot;:&quot;Nvidia insists it isn’t Enron, but its AI deals are testing investor faith&quot;,&quot;elementId&quot;:&quot;159db197-4b56-44e8-ab42-f3a5716959a9&quot;,&quot;role&quot;:&quot;richLink&quot;,&quot;url&quot;:&quot;https://www.theguardian.com/technology/2025/dec/28/nvidia-insists-it-isnt-enron-but-its-ai-deals-are-testing-investor-faith&quot;},&quot;ajaxUrl&quot;:&quot;https://api.nextgen.guardianapps.co.uk&quot;,&quot;format&quot;:{&quot;design&quot;:0,&quot;display&quot;:0,&quot;theme&quot;:0}}"></gu-island></figure><p>Remote testing was introduced during the Covid pandemic to allow students to continue to be able to qualify at a time when lockdowns prevented in-person exam assessment.</p><p>In 2022, the Financial Reporting Council (FRC), the UK’s accounting and auditing industry regulator, said that cheating in professional exams <a href="https://www.theguardian.com/business/2022/dec/21/exam-cheating-at-uk-audit-firms-uncovered-by-watchdog" data-link-name="in body link">was a “live” issue at Britain’s biggest companies.</a></p><p>A number of multimillion-dollar fines have been issued to large auditing and accounting companies around the world over cheating scandals in tests.</p><p>The FRC’s investigation found that instances of cheating also included some tier-one auditors, a category comprising the “big four” accountants – KPMG, PwC, Deloitte and EY – along with Mazars, Grant Thornton and BDO.</p><p>In 2022, EY agreed to <a href="https://www.theguardian.com/business/2022/jun/28/ernst-and-young-fined-cheating-audit-settlement" data-link-name="in body link">pay a record $100m (£74m) to US regulators</a> over claims that dozens of its employees cheated on an ethics exam and that the company then misled investigators.</p><p>The ACCA said it had concluded that online tests have become too difficult to police, given the rise in artificial intelligence (AI) tools available to students.</p><p>Brand said the ACCA, which has more than half a million students, had worked “intensively” to combat cheating but “people who want to do bad things are probably working at a quicker pace”.</p><figure data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.NewsletterSignupBlockElement"><gu-island name="EmailSignUpWrapper" priority="feature" deferuntil="visible" props="{&quot;index&quot;:11,&quot;listId&quot;:4139,&quot;identityName&quot;:&quot;business-today&quot;,&quot;description&quot;:&quot;Get set for the working day – we'll point you to all the business news and analysis you need every morning&quot;,&quot;name&quot;:&quot;Business Today&quot;,&quot;frequency&quot;:&quot;Every weekday&quot;,&quot;successDescription&quot;:&quot;We'll send you Business Today every weekday&quot;,&quot;theme&quot;:&quot;news&quot;,&quot;idApiUrl&quot;:&quot;https://idapi.theguardian.com&quot;}"></gu-island></figure><p>She added that the rapid rise of technology, led by AI tools, had pushed the issue of cheating to a “tipping point”.</p><p>Last year, the Institute of Chartered Accountants in England and Wales (ICAEW), which also trains accountants around the world, said reports of cheating were still increasing.</p><p>However, the ICAEW still permits some exams to be sat online.</p><p>“There are very few high-stakes examinations now that are allowing [remote invigilation],” Brand said.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kidnapped by Deutsche Bahn (825 pts)]]></title>
            <link>https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/</link>
            <guid>46419970</guid>
            <pubDate>Mon, 29 Dec 2025 12:24:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/">https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/</a>, See on <a href="https://news.ycombinator.com/item?id=46419970">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>If you live in Germany, you have been treated like livestock by Deutsche Bahn (DB). Almost all of my friends have a story: they traveled with DB, got thrown out in the middle of the night in some cow village, and had to wait hours for the next train.</p><p>I have something better. I was kidnapped.</p><hr><p>December 24th, 2024. 15:30. Cologne Main Station, Platform 9 D-G.</p><p>I am taking the RE5 (ID 28521) to my grandmother’s house in Meckenheim. Scheduled departure: 15:32. Scheduled arrival in Bonn: 15:54. From there, the S23 to Meckenheim. A journey of 35 kilometers, or, in DB units, somewhere between forty-five minutes and the heat death of the universe.</p><p>I wanted to arrive early to spend more time with her. My father, who lives near Troisdorf, was supposed to join us later.</p><p>I board the train. It is twenty minutes late. I consider this early. At least the train showed up. In DB’s official statistics, a train counts as “on time” if it’s less than six minutes late.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> Cancelled trains are not counted at all.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> If a train doesn’t exist, it cannot be late.</p><p>The train starts moving. The driver announces there are “issues around Bonn.” He does not specify what kind. No one asks. We have learned not to ask. He suggests we exit at Cologne South and take the subway, or continue to Troisdorf and catch a bus from there.</p><p>I decide to continue to Troisdorf. My father can just pick me up there and we drive together. The plan adapts.</p><p>The driver announces the full detour: from Cologne South to Troisdorf to Neuwied to Koblenz. The entire left bank of the Rhine is unavailable. Only then I notice: the driver has been speaking German only. If you were a tourist who got on in Cologne to visit Brühl, thirteen minutes away, you were about to have a very confusing Christmas in Troisdorf.</p><p>A woman near me is holding chocolates and flowers. She is on the phone with her mother. “Sorry Mama, I’ll be late.” Pause. “Deutsche Bahn.” Pause. Her mother understood.</p><p>Twenty minutes later. We are approaching Troisdorf. I stand up. I gather my things. My father texts me: he is at the station, waiting.</p><p>The driver comes back on: “Hello everyone. Apparently we were not registered at Troisdorf station, so we are on the wrong tracks. We cannot stop.”</p><p>He says this the way someone might say “the coffee machine is broken.”</p><p>Silence. Laughter. Silence.</p><p>I watch Troisdorf slide past the window. Somewhere in the parking lot outside the station, my father is sitting in his car, watching his son pass by as livestock.</p><p>My father calls.</p><p>“The train couldn’t stop.”</p><p>“What?”</p><p>“Next stop is Neuwied.”</p><p>“Neuwied?” Pause. “That’s in Rheinland-Pfalz.” Pause. “That’s a different <em>federal state</em>.”</p><p>“Yup.”</p><p>I was trying to travel 35 kilometers. I was now 63 kilometers from my grandmother’s house. Further away than when I started.</p><p>There are fifteen stations between Troisdorf and Neuwied. We pass all of them.</p><p>At some point you stop being a passenger and start being cargo. A cow transporter. Mooohhhhh. A cow transporter going to a cow village. (Germany has a word for this: Kuhdorf. The cows are metaphorical. Usually.) I reached this point around Oberkassel.</p><p>DB once operated a bus to Llucalcari, a Mallorcan village of seventeen people.<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> I wanted to take it home.</p><p>An English speaker near the doors is getting agitated. “What is happening? Why didn’t we stop?”</p><p>“We are not registered for this track.”</p><p>“But where will we stop?”</p><p>“Neuwied. Fifty-five minutes.”</p><p>“Fifty-five minutes.” He said it again, quieter. “I am being kidnapped.”</p><p>My seatmate, who had not looked up from his book in forty minutes, turned a page. “Deutsche Bahn.”</p><hr><p>I looked up my compensation.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup> 1.50 EUR. Minimum payout threshold: 4.00 EUR.</p><p>I had been kidnapped at a loss.</p><figure><img sizes="(min-width: 35em) 1200px, 100vw" srcset="https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/neuwied-station_hu11707903120371504183.jpg 480w,
https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/neuwied-station_hu924015653143594208.jpg 800w,
https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/neuwied-station_hu13922672958805438656.jpg 1200w,
https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/neuwied-station_hu10343310073074022003.jpg 1500w," src="https://www.theocharis.dev/blog/kidnapped-by-deutsche-bahn/neuwied-station_hu924015653143594208.jpg" alt="Neuwied station. My final destination. Photo: Frila, CC BY-SA 3.0"><figcaption><p>Neuwied station. My final destination. Photo: <a href="https://commons.wikimedia.org/wiki/File:Bahnhof_Neuwied.jpg">Frila</a>, CC BY-SA 3.0</p></figcaption></figure></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux DAW: Help Linux musicians to quickly and easily find the tools they need (140 pts)]]></title>
            <link>https://linuxdaw.org/</link>
            <guid>46419968</guid>
            <pubDate>Mon, 29 Dec 2025 12:23:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://linuxdaw.org/">https://linuxdaw.org/</a>, See on <a href="https://news.ycombinator.com/item?id=46419968">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: See what readers who loved your favorite book/author also loved to read (102 pts)]]></title>
            <link>https://shepherd.com/bboy/2025</link>
            <guid>46419822</guid>
            <pubDate>Mon, 29 Dec 2025 11:58:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shepherd.com/bboy/2025">https://shepherd.com/bboy/2025</a>, See on <a href="https://news.ycombinator.com/item?id=46419822">Hacker News</a></p>
Couldn't get https://shepherd.com/bboy/2025: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Feynman's Hughes Lectures: 950 pages of notes (150 pts)]]></title>
            <link>https://thehugheslectures.info/the-lectures/</link>
            <guid>46419273</guid>
            <pubDate>Mon, 29 Dec 2025 10:43:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehugheslectures.info/the-lectures/">https://thehugheslectures.info/the-lectures/</a>, See on <a href="https://news.ycombinator.com/item?id=46419273">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" role="main">

				
					
<article id="post-17" class="page">
	<!-- .entry-header -->

	<div>
		<p>These lectures notes run from the fall of 1966 to 1971. Feynman lectured prior to this period and continued on after 1971. With a few exceptions, the actual 2 hours lectures were not dated. However, the volumes in chronological order.</p>
<p><em><b>I want to stress, again, that these are my personal notes and are only a representation of the lectures I attended. They are to the best of my ability my recreation from memory and my original real time notes. No AV recording system was used in the transcription of my raw notes.</b></em></p>
<div id="attachment_20"><p><a href="https://thehugheslectures.info/wp-content/uploads/lectures/FeynmanHughesLectures_Vol1.pdf"><img fetchpriority="high" decoding="async" aria-describedby="caption-attachment-20" title="Feynman Hughes Lectures - Volume 1" src="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-1-231x300.jpg" alt="Feynman Hughes Lectures - Volume 1" width="231" height="300" srcset="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-1-231x300.jpg 231w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-1-791x1024.jpg 791w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-1.jpg 1700w" sizes="(max-width: 231px) 100vw, 231px"></a></p><p id="caption-attachment-20">25 MB Download</p></div>
<p><strong>Volume 1<br>
</strong><strong>Astronomy, Astrophysics, and Cosmology<br>
</strong><strong>(224 pages)</strong></p>
<p>Feynman solicited topic input from the scientists and engineers at the Labs for the coming year. New discoveries were being made in astronomy, astrophysics, and cosmology at the time. This 1966-1967 lecture series focused on these subjects. This volume is unique since, as far as I can tell, Feynman did not lecture on this subject matter at CalTech. While much of the material is now dated, what remains is a look into the mind of Feynman as he worked to explain such topics as stellar evolution, nuclear synthesis, cosmology, “black stars” (aka black holes), and general relativity.</p>
<p>I inserted more current content from the web which relates to the 1966-67 lectures with recent experimental observations and discoveries. While this lecture series has been “eclipsed” by the tremendous theoretical and experimental advancements over the past 45 years, I am sure the reader(s) will find in these lectures the power of Feynman’s insight and ability to have fun with a new subject not touched on by him at CalTech in his “normal” class and research work. I trust others, more specialized in the topics of volume 1, can and will contribute to the additional information to further enrich the notes in the future. This editing will best be done when the notes are moved and dropped in a dynamic and editable platform, yet to be identified.</p>
<p>The Volume I subject matter was not part of his prior lecture activity, Feynman would talk with some of his CalTech colleagues who worked in the field of astronomy, astrophysics, and cosmology about their work and theories. He would then come to the lecture literally with a (maybe 2 or 3) 3×5 cards and proceed to pour out 2 hours of theory and complex mathematical representations of the topic of the day. This was his genius and almost mystical in his ability to focus his thinking and presentation ability on the most important aspects of a given topic.</p>

<div id="attachment_21"><p><a href="https://thehugheslectures.info/wp-content/uploads/lectures/FeynmanHughesLectures_Vol2.pdf"><img decoding="async" aria-describedby="caption-attachment-21" title="Feynman Hughes Lectures - Volume 2" src="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-2-231x300.jpg" alt="Feynman Hughes Lectures - Volume 2" width="231" height="300" srcset="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-2-231x300.jpg 231w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-2-791x1024.jpg 791w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-2.jpg 1700w" sizes="(max-width: 231px) 100vw, 231px"></a></p><p id="caption-attachment-21">36 MB Download</p></div>
<p><b></b><strong>Volumes 2<br>
</strong><strong>Relativity, Electrostatics, Electrodynamics, Matter-Wave Interaction<br>
(209 pages)</strong></p>
<p>Feynman reflected on how he could teach his original FLP’s volume 2 &amp; 3 differently and better than in his first pass through the subjects five years earlier. The attendees wanted him to lecture a couple years on the subject matter in the original FLP and essentially let him give his revised, enhanced, and expanded lectures. This then led more naturally into QED with a good foundation layer established. Feynman also tailored his lectures more to the level of his audience understanding they were not freshman and sophomore undergraduates but post graduate, doctorate level scientists, employed doing advanced research.&nbsp;</p>



<div id="attachment_22"><p><a href="https://thehugheslectures.info/wp-content/uploads/lectures/FeynmanHughesLectures_Vol3.pdf"><img decoding="async" aria-describedby="caption-attachment-22" title="Feynman Hughes Lectures - Volume 3" src="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-3-231x300.jpg" alt="Feynman Hughes Lectures - Volume 3" width="231" height="300" srcset="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-3-231x300.jpg 231w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-3-791x1024.jpg 791w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-3.jpg 1700w" sizes="(max-width: 231px) 100vw, 231px"></a></p><p id="caption-attachment-22">49 MB Download</p></div>
<p><b></b><strong>Volume 3<br>
</strong><strong>More on Matter-Wave Interaction, Intro to Quantum Mechanics, Scattering Theory, Quantum Theory of Angular Momentum, Intro to Lie Group, SU 2 &amp; 3 “stuff”, Quantum Electrodynamics (QED), Pair Production<br>
(314 pages)<br>
</strong></p>
<p>Feynman went on in greater detail to complete his lectures on wave-matter interaction. From there he started into quantum mechanics and his path history formulation. He extended his lectures to include Lie Group theory and the SU 2&amp;3 “Stuff”.</p>
<p>Feynman diagrams are discussed in Volume 3 at some length as he went deep into QED theory including such topics as quantum scattering. As better understood today, his diagrams represent a visual language of the complex physical processes at the particle interaction level. I have noted recently that with the power of new computers and new concepts the Feynman diagrams have, arguably, run their course. While this is possibly the case, I would assert that bypassing a fundamental understanding of the Feynman diagram concept makes it hard to understand what replaces them. This is like hand held calculators replacing the need to know the fundamental multiplication tables and being able to check what the calculator is telling you. I personally observed in a number of lectures where Feynman would self-check himself as he was working out the math because he could sense that if he kept going he would not get the right physics. This was his true genius at work. That was truly amazing to both watch and try to absorb in real time<b>. &nbsp;</b></p>
<div id="attachment_19"><p><a href="https://thehugheslectures.info/wp-content/uploads/lectures/FeynmanHughesLectures_Vol4.pdf"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-19" title="Feynman Hughes Lectures - Volume 4" src="https://thehugheslectures.info/wp-content/uploads/2014/04/FHLcover-dark-Green-Vol-4-231x300.jpg" alt="Feynman Hughes Lectures - Volume 4" width="231" height="300" srcset="https://thehugheslectures.info/wp-content/uploads/2014/04/FHLcover-dark-Green-Vol-4-231x300.jpg 231w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHLcover-dark-Green-Vol-4-791x1024.jpg 791w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHLcover-dark-Green-Vol-4.jpg 1700w" sizes="auto, (max-width: 231px) 100vw, 231px"></a></p><p id="caption-attachment-19">13 MB Download</p></div>
<p><b></b><strong>Volume 4<br>
</strong><strong>Molecular Biology<br>
(65 pages)</strong></p>
<p>The Molecular Biology lectures started out and then eventually died out as the year progressed. Feynman found the material challenging to get his head around before the lecture and, therefore, very time consuming. He apparently found a CalTech colleague, Seymour Benzer, who changed from physics to biophysics as a person who stimulated Feynman’s interest in this topic.</p>
<p>By consensus the lecture series ended early. Feynman was deep into his own parton theory which was his version of quark theory. He and Gell-Mann were collegial competitors in those days.</p>
<p>In preparing these notes for release I decided to include what notes I had of those lectures only to give evidence of Feynman’s interest to explore all the dimensions of science and nature. For those involved in the field these notes will not provide much informational value particularly with all the advancements on research and understanding of molecular biology. The value, I believe, for the reader is how Feynman thought through the subject matter and mentally organized it so he could lecture on it. That might aid teachers in this field to sharpen up their own presentation material. At the end of the volume are my un-transcribed real-time notes that I never got to but I decided to include for those who are into this field.</p>
<div id="attachment_23"><p><a href="https://thehugheslectures.info/wp-content/uploads/lectures/FeynmanHughesLectures_Vol5.pdf"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-23" title="Feynman Hughes Lectures - Volume 5" src="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-5-231x300.jpg" alt="Feynman Hughes Lectures - Volume 5" width="231" height="300" srcset="https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-5-231x300.jpg 231w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-5-791x1024.jpg 791w, https://thehugheslectures.info/wp-content/uploads/2014/04/FHL-cover-dark-Green-Vol-5.jpg 1700w" sizes="auto, (max-width: 231px) 100vw, 231px"></a></p><p id="caption-attachment-23">6 MB Download</p></div>
<p><b></b><strong>Volume 5<br>
</strong><strong>Mathematical Methods/Techniques in Physics and Engineering<br>
(163&nbsp;pages)</strong></p>
<p>By some who have seen samples of my notes Volume 5 has been referred to as the “missing lectures” to the FLP “Red Books”. Feynman himself felt that he should have taught the mathematical methods first and then the physics since math <b>is&nbsp;</b><b>the</b> “language” of physics. Feynman was apparently talked out of starting with a course in math-physics. The attendees at the lab talked him into a year-long lecture on his approach to mathematics as the language of physics.</p>
<p>I note here also that the math lectures have been referred on the Reddit by someone as “sophomoric” since all physic students must take similar course work and presumably “master” math while learning the physics. In my own case I wanted to learn the physics and minimize the math, or better said, not confused by the physics because the math was too difficult to grasp.</p>
<p>This is how Feynman approached physics and how he taught himself, at an early age, by developing many shortcuts through the math; “Feynman diagrams” were one clear by product of his self learning process. He did not want to get bogged down and distracted from understanding the physics. This is why and how he got involved in the Manhattan Project; he was their math wizard.</p>
<p>One story he told of those days: Someone came running into him needing a quick answer to a nuclear decay process that was described by some expansion series like the Sum from 1 to infinity of 1/(1+n^2)[probably not the real one]. Feynman asked how accurately he wanted the answer and the person said 10% would do for now. Feynman said he took a few seconds and said the answer was 1.3 (or something like that); the person was amazed how fast he could give him that and asked how he did it. He said since you told me you only wanted the answer to 10%, it was only necessary to go to the second term in the series expansion and that was good enough for better than 10% accuracy. This story is emblematic of Feynman’s mathematical thinking which is not sophomoric. This is why he made such a contribution to the Manhattan project and ultimately QED. He did indeed “think different”.</p>
<p>In my own experience I found in my graduate studies that the some of the professors tended to focus more on the math rigor than in teaching the real physics. In Feynman’s world he “felt” the physics and used the math to express that “feeling” and understanding. Language does not necessarily express the essence of the content contained in the idea being described. One must understand both the power and limitations of the language used when discussing a subject. Words don’t always express what one wants to say; so it is for math and physics.</p>
<p><strong>Lecture Sidebars</strong><strong>:</strong> Another “feature”, or aspect, of the notes is my attempt to capture “side bar” topics. These special topics or thoughts (including some philosophical ones) added color and currency to the lectures as only Feynman could deliver. He was unconstrained in the lecture environment to take off on a sidebar and the attendees both enjoyed and encouraged him to do so.</p>
		
			</div><!-- .entry-content -->
	</article><!-- #post-17 -->
					

	<!-- #comments .comments-area -->

				
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EU to build no-fee payments service like Visa/Mastercard and Apple/Google Pay (134 pts)]]></title>
            <link>https://www.independent.ie/business/digital-euro-what-it-is-and-how-we-will-use-the-new-form-of-cash/a165973061.html</link>
            <guid>46419121</guid>
            <pubDate>Mon, 29 Dec 2025 10:12:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.independent.ie/business/digital-euro-what-it-is-and-how-we-will-use-the-new-form-of-cash/a165973061.html">https://www.independent.ie/business/digital-euro-what-it-is-and-how-we-will-use-the-new-form-of-cash/a165973061.html</a>, See on <a href="https://news.ycombinator.com/item?id=46419121">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="CIKAc2QkuuB" data-fragment-name="articleDetail"><header data-testid="article-header"></header><p data-testid="title-summary">
The European Central Bank is determined to break the US grip on card payments
</p><div><figure data-testid="article-image-wrapper"><figcaption data-testid="image-caption"><p>Digital euro. Image: Getty</p></figcaption></figure></div><div><div data-testid="article-author"><p><span data-testid="author-name">John Burns</span></p></div><p><time datetime="2025-12-29T05:30:00Z" data-testid="article-date">Today at 06:30</time></p></div><p>It’s January 1, 2029, the first day of the digital euro. You are in a shop buying milk and bread, and decide to pay with this new money. How exactly will it work?</p><p>If you have a bank account, the digital euro will sit inside its app on your phone. The cost of the bread and milk can be taken out of your digital-euro wallet, which is separate from your regular bank account. You don’t have a bank?</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Asking Gemini 3 to generate Brainfuck code results in an infinite loop (101 pts)]]></title>
            <link>https://teodordyakov.github.io/brainfuck-agi/</link>
            <guid>46418966</guid>
            <pubDate>Mon, 29 Dec 2025 09:40:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://teodordyakov.github.io/brainfuck-agi/">https://teodordyakov.github.io/brainfuck-agi/</a>, See on <a href="https://news.ycombinator.com/item?id=46418966">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
        <header>
            
        </header>

        <section>
            <p>Asking Gemini 3 to generate Brainf*ck code results in an infinite loop, akin amost to a DDoS attack:</p>
            <img src="https://teodordyakov.github.io/brainfuck-agi/bf.gif">
            <p>That is fascinating. So it made me wonder.
            Is Brainf*ck the ultimate test for AGI?
            I think so, and for 3 good reasons.
            </p>
            <h2>1. The Data Scarcity Problem</h2>
            <p>Large Language Models (LLMs) thrive on sheer volume. To master JavaScript, an LLM has been trained on
                virtually every available line of open-source code—hundreds of millions of lines of code (LOC). By
                comparison, the amount of functional <strong>Brainf*ck</strong> code on the web is a statistical
                rounding error.</p>
            <p>We are talking about a <span>million times less training data</span>. Without the
                luxury of infinite patterns to copy, the model can't rely on mimicry; it has to understand the
                underlying logic.</p>
        </section>

        <section>
            <h2>2. Anti-Literate Programming</h2>
            <p>Brainf*ck is the antithesis of modern software engineering. There are no comments, no meaningful variable
                names, and no structure. In many ways, looking at existing Brainf*ck code is actually
                <em>detrimental</em> to a novice. Consider this typical snippet:</p>

            <code>&gt;++++++++[&lt;+++++++++&gt;-]&lt;.&gt;++++[&lt;+++++++&gt;-]&lt;+.+++++++..+++.&gt;&gt;++++++[&lt;+++++++&gt;-]&lt;+
+.------------.&gt;++++++[&lt;+++++++++&gt;-]&lt;+.&lt;.+++.------.--------.&gt;&gt;&gt;++++[&lt;++++++++&gt;-
]&lt;+.</code>

            <p>Writing in this environment is akin to <span>zero-shot learning</span>. Success
                requires reasoning at a high level of abstraction based on the fundamental rules of the language and a
                precise mental model of semantics, rather than memorized syntax.</p>
        </section>

        <section>
            <h2>3. The Repetition Problem</h2>
              <div>
            <p>As we saw earlier, asking a modern model for complex Brainf*ck code often results in the model falling into an infinite loop—spewing the same characters over and over. The minimalistic nature of the language results in highly repetitive structures in the code. This poses a unique challenge to the way LLMs work.</p>
            <p>An LLM is more likely to output what it has already seen based on previous tokens, and that pertains to its own output too. When some structure is repeated more than a couple of times, there is a likelihood that the model may learn that token <strong>X</strong> is the most likely output following <strong>itself</strong>. With every subsequent iteration, this increases the likelihood of outputting <strong>X</strong> in a self-fulfilling prophecy, resulting in the infinite loop.</p>
        </div>
        </section>

        <section>
            <p> <strong>So, is Brainf*ck the ultimate test for LLMs? You be the judge.</strong></p>
        </section>

        
    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can't design software you don't work on (196 pts)]]></title>
            <link>https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/</link>
            <guid>46418415</guid>
            <pubDate>Mon, 29 Dec 2025 07:54:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/">https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/</a>, See on <a href="https://news.ycombinator.com/item?id=46418415">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header></header><section><p>Only the engineers who work on a large software system can meaningfully participate in the design process. That’s because you cannot do good software design without an intimate understanding of the concrete details of the system. In other words, <strong>generic software design advice is typically useless</strong> for most practical software design problems.</p>
<h3>Generic software design</h3>
<p>What is generic software design? It’s “designing to the problem”: the kind of advice you give when you have a reasonable understanding of the <em>domain</em>, but very little knowledge of the existing <em>codebase</em>. Unfortunately, this is the only kind of advice you’ll read in software books and blog posts<sup id="fnref-1"><a href="#fn-1">1</a></sup>. Engineers love giving generic software design advice for the same reason that all technical professionals love “talking shop”. However, you should be very careful about applying generic advice to your concrete day-to-day work problems<sup id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p><strong>When you’re doing real work, concrete factors dominate generic factors</strong>. Having a clear understanding of what the code looks like right now is far, far more important than having a good grasp on general design patterns or principles. For instance:</p>
<ul>
<li>In large codebases, consistency is more important than “good design”. I won’t argue that point here, but I wrote about it at length in <a href="https://www.seangoedecke.com/large-established-codebases"><em>Mistakes engineers make in large established codebases</em></a>.</li>
<li>Real codebases are typically full of complex, hard-to-predict consequences. If you want to make your change safely, that typically constrains your implementation choices down to a bare handful of possibilities.</li>
<li>Large shared codebases never reflect a single design, but are always in some intermediate state between different software designs. How the codebase will hang together after an individual change is thus way more important than what ideal “north star” you’re driving towards.</li>
</ul>
<p>In a world where you could rewrite the entire system at will, generic software design advice would be much more practical. Some projects are like this! But <strong>the majority of software engineering work is done on systems that cannot be safely rewritten</strong>. These systems cannot rely on “software design”, but must instead rely on internal consistency and the carefulness of their engineers.</p>
<h3>Concrete software design</h3>
<p>What does good software design look like, then?</p>
<p>In my experience, the most useful software design happens in conversations between a small group of engineers who all have deep understanding of the system, because they’re the ones working on it every day. These design discussions are often <strong>really boring</strong> to outsiders, because they revolve around arcane concrete details of the system, not around general principles that any technical person can understand and have an opinion on.</p>
<p>The kinds of topic being discussed are not “is DRY better than WET”, but instead “could we put this new behavior in subsystem A? No, because it needs information B, which isn’t available to that subsystem in context C, and we can’t expose that without rewriting subsystem D, but if we split up subsystem E here and here…“.</p>
<p>Deep philosophical points about design are rarely important to the discussion. Instead, the most critical contributions point out small misunderstandings of concrete points, like: “oh, you thought B wasn’t available in context C, but we recently refactored C so now we could thread in B if we needed to”.</p>
<h3>When generic software design is useful</h3>
<p>Generic software design advice is not useful for practical software design problems, but that doesn’t mean it’s totally useless.</p>
<p><strong>Generic software design advice is useful for building brand-new projects.</strong> As I argued above, when you’re designing a new feature in an existing system, concrete factors of the system dominate. But when you’re designing a <em>new system</em>, there are no concrete factors, so you can be entirely guided by generic advice.</p>
<p><strong>Generic software design advice is useful for tie-breaking concrete design decisions.</strong> I don’t think you should start with a generic design, but if you have a few candidate concrete pathways that all seem acceptable, generic principles can help you decide between them.</p>
<p>This is particularly true at the level of the entire company. In other words, <strong>generic software design advice can help ensure consistency across different codebases</strong>. This is one of the most useful functions of an official “software architect” role: to provide a set of general principles so that individual engineers can all tie-break their concrete decisions in the same direction<sup id="fnref-3"><a href="#fn-3">3</a></sup>.</p>
<p><strong>Generic software design principles can also guide company-wide architectural decisions.</strong> Should we run our services in our own datacenter, or in the cloud? Should we use k8s? AWS or Azure? Once you get broad enough, the concrete details of individual services almost don’t matter, because it’s going to be a huge amount of work either way. Still, even for these decisions, concrete details matter a lot. There are certain things you just can’t do in the cloud (like rely on bespoke hardware setups), or that you can’t do in your own datacenter (like deploy your service to the edge in twelve different regions). If the concrete details of your codebase rely on one of those things, you’ll be in for a bad time if you ignore them when making company-wide architectural decisions.</p>
<h3>Architects and local minima</h3>
<p>Those are all good reasons to do generic software design. One bad reason companies do generic software design is that it just sounds like a really good idea to people who aren’t working software engineers. Once you’re doing it, the incentives make it hard to stop. Many tech companies fall into this local minimum.</p>
<p>Why not have your highest-paid software engineers spend their time exclusively making the most abstract, highest-impact decisions? You want your structural engineers to be drawing, not laying bricks, after all. I don’t know if structural engineering works like this, but I do know that software engineering doesn’t. In practice, <strong>software architecture advice often has to be ignored by the people on the ground</strong>. There’s simply no way to actually translate it into something they can implement, in the context of the current system as it exists.</p>
<p>However, for a practice that doesn’t work, “have your top engineers just do generic design” is surprisingly robust. <strong>Architects don’t have any skin in the game</strong><sup id="fnref-4"><a href="#fn-4">4</a></sup>, because their designs are handed off to actual engineering teams to implement. Because those designs can never be implemented perfectly, architects can both claim credit for successes (after all, it was their design) and disclaim failures (if only those fools had followed my design!)</p>
<h3>Summary</h3>
<p>When working on large existing codebases, useful software design discussions are way, way more concrete than many people believe. They typically involve talking about individual files or even lines of code. You thus can’t do useful software design without being intimately familiar with the codebase (in practice, that almost always means being an active contributor).</p>
<p>Purely generic architecture is not <em>useless</em>, but its role should be restricted to (a) setting out paved paths for brand new systems, (b) tie-breaking decisions on existing systems, and (c) helping companies make broad technology choices.</p>
<p>In my opinion, formal “big-picture software architect” roles that spend all their time laying out the initial designs for projects are doomed to failure. They sound like a good idea (and they’re a good deal for the architect, who can claim credit without risking blame), but they provide very little value to the engineering teams that are tasked with actually writing the code.</p>
<p>Personally, I believe that <strong>if you come up with the design for a software project, you ought to be responsible for the project’s success or failure</strong>. That would rapidly ensure that the people designing software systems are the people who know how to ship software systems. It would also ensure that the <em>real</em> software designers - the ones that have to take into account all the rough edges and warts of the codebase - get credit for the difficult design work they do.</p>
</section><p>If you liked this post, consider<!-- --> <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> <!-- -->to email updates about my new posts, or<!-- --> <a href="https://news.ycombinator.com/submitlink?u=https://www.seangoedecke.com/you-cant-design-software-you-dont-work-on/&amp;t=You%20can%27t%20design%20software%20you%20don%27t%20work%20on" target="_blank">sharing it on Hacker News</a>.<!-- --> Here's a preview of a related post that shares tags with this one.</p><blockquote><p>Pure and impure software engineering</p><div><p>Why do solo game developers tend to get into fights with big tech engineers? Why do high-profile external hires to large companies often fizzle out? Why is AI-assisted development amazing for some engineers and completely useless for others?</p><p>I think it’s because some engineers are doing very different kinds of work to other engineers. Those two types of engineers often assume their counterparts are simply incompetent, but they’re really just working in different fields.<br><a href="https://www.seangoedecke.com/pure-and-impure-engineering/">Continue reading...</a></p></div></blockquote><hr></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Staying ahead of censors in 2025 (226 pts)]]></title>
            <link>https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898</link>
            <guid>46417844</guid>
            <pubDate>Mon, 29 Dec 2025 05:47:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898">https://forum.torproject.org/t/staying-ahead-of-censors-in-2025-what-weve-learned-from-fighting-censorship-in-iran-and-russia/20898</a>, See on <a href="https://news.ycombinator.com/item?id=46417844">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_1">
            <div>
              

                

              <p><span>
                  <time datetime="2025-12-03T20:13:00Z">
                    December 3, 2025,  8:13pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-03T20:13:00Z">
              <span itemprop="position">1</span>
              </span>
            </p></div>
            <div itemprop="text">
              <div><p>
      by <a href="https://blog.torproject.org/author/meskio">meskio</a> and <a href="https://blog.torproject.org/author/shelikhoo">shelikhoo</a>
 | December 3, 2025
    </p><div><a href="https://forum.torproject.org/uploads/default/original/2X/7/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf.jpeg" data-download-href="https://forum.torproject.org/uploads/default/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf" title="lead.jpg"><img src="https://forum.torproject.org/uploads/default/optimized/2X/7/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf_2_690x388.jpeg" data-dominant-color="8F9097" width="690" height="388" srcset="https://forum.torproject.org/uploads/default/optimized/2X/7/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf_2_690x388.jpeg, https://forum.torproject.org/uploads/default/original/2X/7/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf.jpeg 1.5x, https://forum.torproject.org/uploads/default/original/2X/7/7ba3eeb4bd12624922b10dd57b1b986cb32f15bf.jpeg 2x"></a></div><p>From internet blackouts in Iran to Russia's evolving censorship tactics, 2025 has tested Tor's anti-censorship tools like never before. These are the moments where the work of Tor's anti-censorship team is more important than ever, to fulfill our mission of preserving connectivity between users in affected regions and the rest of the world.</p><p>In this blog post, we want to talk about what we've learned, how we've adapted, and what other internet users can do to keep Tor users connected.</p><h2>Iran</h2><p>In June, during the war between Iran and Israel, the censorship in <a href="https://filter.watch/english/2025/10/02/irans-stealth-blackout-a-multi-stakeholder-analysis-of-the-june-2025-internet-shutdown/">Iran intensified up to a point where internet was disconnected for few days</a>. Presumably to impede espionage-related communication while simultaneously consolidating political power.</p><h3>Monitoring the censorship landscape</h3><p>During this period, we were constantly monitoring the situation using our in-region vantage-point system. This vantage-point system is a network of monitoring locations inside Iran that provides more recent and accurate information about censorship than is available from public data.</p><p>One clear example is domain-fronting data. Domain-fronting is a technique that makes Tor traffic look like other popular, harder-to-block websites (like major cloud services). To determine which domain-fronting configurations perform best across the most locations, we deployed an automated testing tool that detects and reports the accessibility of the Snowflake broker and the Moat service for each domain-fronting configuration at each of our vantage points. This information is then aggregated by the log collector and subsequently used to monitor the domain-fronting configurations currently in use and to select the configurations to use in the future.</p><h3>Strengthening Snowflake</h3><p><a href="https://snowflake.torproject.org/">Snowflake</a> is the most used network traffic obfuscation tool in Iran. Over the past year we have been working on improving it to ensure that it remains strong and accessible to users.</p><p>We have upgraded the web extension to Manifest Version 3 (the latest browser extension standard), to be compatible with modern browsers. We improved the NAT checking logic which helps us figure out what kind of network setup each user has. This way, the proxies are more accurately assigned to the clients depending on their network capabilities. And  we enhanced the metrics reported by the standalone proxy, providing better tooling for proxy operators to assist what is happening with their proxies.</p><p>Under the hood we have created a staging server for Snowflake, so we have a robust infrastructure to stress test new features making sure they're fit for real deployment. This will help us bring big changes in the coming year to improve the efficiency of the protocol where networks are severely disrupted and to create better mechanisms to prevent censors from blocking Snowflake.</p><h3>Deploying Conjure</h3><p>Censorship agencies like those in Iran often attempt to block bridges by obtaining bridge information in bulk and then inputing the network address of these bridges to their censorship gateways to block them. That's why we developed Conjure.</p><p>Conjure is a pluggable transport designed to stay ahead of proxy-listing-based blocking by leveraging unused address space within cooperating ISP networks, thereby limiting the damage caused by blocking individual network addresses. Think of it like the act of generating temporary email addresses to avoid spam emails, by making sure the address is temporary and easy to regenerate, anything blocked at that address won't affect your ability to get new ones.</p><p>We are working on distributing Conjure in places with strong censorship. To make it hard for censors, we have improved Tor's implementation of Conjure by extending the protocols used both for bootstrapping the connection and transport the data. We added multiple registration methods (DNS and AMP-cache), making the bootstrap of the conjure connection more censorship-resistant and the connection will look as if the user is connecting to widely used service. We also integrated additional transports from upstream (DTLS and prefix) that makes the Tor traffic look like common protocols–meaning regular internet traffic.</p><h2>Russia</h2><p>Another region that has experienced many changes this year is Russia. With continued conflict and attrition, internet censorship has intensified, including increased allowlist-based censorship and address-block-based censorship.</p><p>Last year, we introduced WebTunnel as a new pluggable transport. We have seen this year how WebTunnel has become a key tool for users in Russia, thanks to its ability to blend into regular web traffic. As the severity of censorship in Russia has increased, WebTunnel has also received several fixes, such as SNI imitation and safe non-WebPKI certificate support with certificate-chain pinning to ensure it can withstand more kinds of censorship, including SNI allowlisting and the rapid blocking of distributed bridges.</p><p>Many of these improvements come from volunteers or are shaped by user feedback. Our community of users and supporters makes all this work possible and helps us stay ahead at Tor. Thanks to our Tor community team, we have first-hand insights into what works and what doesn't. This gives us access to the best information in the region. Additionally, through the community team's work with people on the ground, we receive support in testing and identifying the best technology for each censorship scenario.</p><h3>Experimenting with bridge distribution</h3><p>When we started distributing WebTunnel bridges in December they were a very useful tool to connect to Tor. They worked well for months, and Tor Browser users got them configured automatically over Connect Assist if they were located in Russia. However, in June, the Russian censors began listing most of our WebTunnel bridges, prompting us to shift strategies.</p><p>In recent history, our Telegram distributor has proven to be a useful tool in Russia, as the censor has a harder time extracting all the bridges from it. This is why we have now added support for WebTunnel in our Telegram distributor. We are always trying to meet our users where they are, and while Telegram might not be the safest place for your online communications, many users in Russia already uses it. And is not only useful for Russian users, but also for Iranian ones that are currently using webtunnel bridges distributed over Telegram.</p><p>All these fast changes of bridges distribution are possible thanks to <a href="https://blog.torproject.org/making-connections-from-bridgedb-to-rdsys/">rdsys</a>, Tor's new bridge distribution system that we introduced last year. This year we kept improving rdsys adding a staging server, so we can stress-test it in a similar environments to the ones used in production. For our censored users that means that by the time new and updated anticensorship features arrive, we have already been able to fix many stability issues.</p><h2>Where do we go from here?</h2><p>Supporting our users to continue fighting censorship is what our work is all about. Making it possible to connect to the Tor network on censored networks–whatever they are. Whether it is your university, your internet service provider, or your government trying to keep you from getting the information you are entitled to. Next year we'll start rolling out Conjure, keep improving WebTunnel,and prepare Snowflake for the next big censorship events.</p><p>You too can help us fight censorship today, by sharing your bandwidth and <a href="https://snowflake.torproject.org/">running your own Snowflake</a>. The easiest way is to install a snowflake plugin in your browser to help others access the Tor network. And if you have a website <a href="https://community.torproject.org/relay/setup/webtunnel/">consider running a webtunnel bridge</a>.</p></div>
<hr>
<p><small>This is a companion discussion topic for the original entry at <a href="https://blog.torproject.org/staying-ahead-of-censors-2025">https://blog.torproject.org/staying-ahead-of-censors-2025</a></small>
            </p></div>

            


            
          </div><div id="post_2" itemprop="comment" itemscope="" itemtype="http://schema.org/Comment">
            <p><span itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                <a itemprop="url" rel="nofollow" href="https://forum.torproject.org/u/BobbyB"><span itemprop="name">BobbyB</span></a>
                
              </span>



              <span>
                  <time itemprop="datePublished" datetime="2025-12-04T17:17:23Z">
                    December 4, 2025,  5:17pm
                  </time>
                  <meta itemprop="dateModified" content="2025-12-04T17:17:23Z">
              <span itemprop="position">2</span>
              </span>
            </p>
            <div itemprop="text">
              
<p>OK like what? I run a standalone so what better tools are now available and how do I use them? I have all my logs since startup last year. I could tell you what I would like to see but this is not the point of your thread.</p>
            </div>

            


            
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Z80-μLM, a 'Conversational AI' That Fits in 40KB (450 pts)]]></title>
            <link>https://github.com/HarryR/z80ai</link>
            <guid>46417815</guid>
            <pubDate>Mon, 29 Dec 2025 05:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/HarryR/z80ai">https://github.com/HarryR/z80ai</a>, See on <a href="https://news.ycombinator.com/item?id=46417815">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Z80-μLM: A Retrocomputing Micro Language Model</h2><a id="user-content-z80-μlm-a-retrocomputing-micro-language-model" aria-label="Permalink: Z80-μLM: A Retrocomputing Micro Language Model" href="#z80-μlm-a-retrocomputing-micro-language-model"></a></p>
<p dir="auto">Z80-μLM is a 'conversational AI' that generates short character-by-character sequences, with quantization-aware training (QAT) to run on a Z80 processor with 64kb of ram.</p>
<p dir="auto">The root behind this project was the question: how small can we go while still having personality, and can it be trained or fine-tuned easily? With easy self-hosted distribution?</p>
<p dir="auto">The answer is Yes! And a 40kb .com binary (including inference, weights &amp; a chat-style UI) running on a 4MHz processor from 1976.</p>
<p dir="auto">It won't pass the Turing test, but it might make you smile at the green screen.</p>
<p dir="auto">For insight on how to best train your own model, see <a href="https://github.com/HarryR/z80ai/blob/main/TRAINING.md">TRAINING.md</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Examples</h2><a id="user-content-examples" aria-label="Permalink: Examples" href="#examples"></a></p>
<p dir="auto">Two pre-built examples are included:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/HarryR/z80ai/blob/main/examples/tinychat">tinychat</a></h3><a id="user-content-tinychat" aria-label="Permalink: tinychat" href="#tinychat"></a></p>
<p dir="auto">A conversational chatbot trained on casual Q&amp;A pairs. Responds to greetings, questions about itself, and general banter with terse personality-driven answers.</p>
<div data-snippet-clipboard-copy-content="> hello
HI
> are you a robot
YES
> do you dream
MAYBE"><pre><code>&gt; hello
HI
&gt; are you a robot
YES
&gt; do you dream
MAYBE
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/HarryR/z80ai/blob/main/examples/guess">guess</a></h3><a id="user-content-guess" aria-label="Permalink: guess" href="#guess"></a></p>
<p dir="auto">A 20 Questions game where the model knows a secret topic and answers YES/NO/MAYBE to your questions. Guess correctly to WIN.</p>
<div data-snippet-clipboard-copy-content="> is it alive
YES
> is it big
YES
> does it have a trunk
YES
> is it grey
MAYBE
> elephant
WIN"><pre><code>&gt; is it alive
YES
&gt; is it big
YES
&gt; does it have a trunk
YES
&gt; is it grey
MAYBE
&gt; elephant
WIN
</code></pre></div>
<p dir="auto">Includes tools for generating training data with LLMs (Ollama or Claude API) and balancing class distributions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li><strong>Trigram hash encoding</strong>: Input text is hashed into 128 buckets - typo-tolerant, word-order invariant</li>
<li><strong>2-bit weight quantization</strong>: Each weight is {-2, -1, 0, +1}, packed 4 per byte</li>
<li><strong>16-bit integer inference</strong>: All math uses Z80-native 16-bit signed arithmetic</li>
<li><strong>~40KB .COM file</strong>: Fits in CP/M's Transient Program Area (TPA)</li>
<li><strong>Autoregressive generation</strong>: Outputs text character-by-character</li>
<li><strong>No floating point</strong>: Everything is integer math with fixed-point scaling</li>
<li><strong>Interactive chat mode</strong>: Just run <code>CHAT</code> with no arguments</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Interaction Style</h2><a id="user-content-interaction-style" aria-label="Permalink: Interaction Style" href="#interaction-style"></a></p>
<p dir="auto">The model doesn't understand you. But somehow, it <em>gets</em> you.</p>
<p dir="auto">Your input is hashed into 128 buckets via trigram encoding - an abstract "tag cloud" representation. The model responds to the <em>shape</em> of your input, not the exact words:</p>
<div data-snippet-clipboard-copy-content="&quot;hello there&quot;  →  [bucket 23: 64, bucket 87: 32, ...]
&quot;there hello&quot;  →  [bucket 23: 64, bucket 87: 32, ...]  (same!)
&quot;helo ther&quot;    →  [bucket 23: 32, bucket 87: 32, ...]  (similar - typo tolerant)"><pre><code>"hello there"  →  [bucket 23: 64, bucket 87: 32, ...]
"there hello"  →  [bucket 23: 64, bucket 87: 32, ...]  (same!)
"helo ther"    →  [bucket 23: 32, bucket 87: 32, ...]  (similar - typo tolerant)
</code></pre></div>
<p dir="auto">This is semantically powerful for short inputs, but there's a limit: longer or order-dependent sentences blur together as concepts compete for the same buckets. "Open the door and turn on the lights" will likely be too close to distringuish from "turn on the door and open the lights."</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Small Responses, Big Meaning</h3><a id="user-content-small-responses-big-meaning" aria-label="Permalink: Small Responses, Big Meaning" href="#small-responses-big-meaning"></a></p>
<p dir="auto">A 1-2 word response can convey surprising nuance:</p>
<ul dir="auto">
<li><code>OK</code> - acknowledged, neutral</li>
<li><code>WHY?</code> - questioning your premise</li>
<li><code>R U?</code> - casting existential doubt</li>
<li><code>MAYBE</code> - genuine uncertainty</li>
<li><code>AM I?</code> - reflecting the question back</li>
</ul>
<p dir="auto">This isn't necessarily a limitation - it's a different mode of interaction. The terse responses force you to infer meaning from context or ask probing direct yes/no questions to see if it understands or not (e.g. 'are you a bot', 'are you human', 'am i human' displays logically consistent memorized answers)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What It's Good At</h3><a id="user-content-what-its-good-at" aria-label="Permalink: What It's Good At" href="#what-its-good-at"></a></p>
<ul dir="auto">
<li>Short, varied inputs with consistent categorized outputs</li>
<li>Fuzzy matching (typos, rephrasing, word order)</li>
<li>Personality through vocabulary choice</li>
<li>Running on constrianed 8-bit hardware</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">What It's Not</h3><a id="user-content-what-its-not" aria-label="Permalink: What It's Not" href="#what-its-not"></a></p>
<ul dir="auto">
<li>A chatbot that generates novel sentences</li>
<li>Something that tracks multi-turn context deeply</li>
<li>A parser that understands grammar</li>
<li>Anything approaching general intelligence</li>
</ul>
<p dir="auto">It's small, but functional. And sometimes that's exactly what you need</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<ul dir="auto">
<li><strong>Input</strong>: 128 query trigram buckets + 128 context buckets</li>
<li><strong>Hidden layers</strong>: Configurable depth/width, e.g., 256 → 192 → 128</li>
<li><strong>Output</strong>: One neuron per character in charset</li>
<li><strong>Activation</strong>: ReLU between hidden layers</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Quantization Constraints</h3><a id="user-content-quantization-constraints" aria-label="Permalink: Quantization Constraints" href="#quantization-constraints"></a></p>
<p dir="auto">The Z80 is an 8-bit CPU, but we use its 16-bit register pairs (HL, DE, BC) for activations and accumulators. Weights are packed 4-per-byte (2-bit each) and unpacked into 8-bit signed values for the multiply-accumulate.</p>
<p dir="auto">The 16-bit accumulator gives us numerical stability (summing 256 inputs without overflow), but the model's expressiveness is still bottlenecked by the 2-bit weights, and naive training may overflow or act 'weirdly' without QAT.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Z80 Inner Loops</h3><a id="user-content-z80-inner-loops" aria-label="Permalink: Z80 Inner Loops" href="#z80-inner-loops"></a></p>
<p dir="auto">The core of inference is a tight multiply-accumulate loop. Weights are packed 4-per-byte:</p>
<div data-snippet-clipboard-copy-content="; Unpack 2-bit weight from packed byte
ld a, (PACKED)      ; Get packed weights
and 03h             ; Mask bottom 2 bits
sub 2               ; Map 0,1,2,3 → -2,-1,0,+1
ld (WEIGHT), a

; Rotate for next weight
ld a, (PACKED)
rrca
rrca
ld (PACKED), a"><pre lang="z80"><code>; Unpack 2-bit weight from packed byte
ld a, (PACKED)      ; Get packed weights
and 03h             ; Mask bottom 2 bits
sub 2               ; Map 0,1,2,3 → -2,-1,0,+1
ld (WEIGHT), a

; Rotate for next weight
ld a, (PACKED)
rrca
rrca
ld (PACKED), a
</code></pre></div>
<p dir="auto">The multiply-accumulate handles the 4 possible weight values:</p>
<div data-snippet-clipboard-copy-content="MULADD:
    or a
    jr z, DONE       ; weight=0: skip entirely
    jp m, NEG        ; weight<0: subtract
    ; weight=+1: add activation
    ld hl, (ACC)
    add hl, de
    ld (ACC), hl
    ret
NEG:
    cp 0FFh
    jr z, NEG1       ; weight=-1
    ; weight=-2: subtract twice
    ld hl, (ACC)
    sbc hl, de
    sbc hl, de
    ld (ACC), hl
    ret
NEG1:
    ; weight=-1: subtract once
    ld hl, (ACC)
    sbc hl, de
    ld (ACC), hl
    ret"><pre lang="z80"><code>MULADD:
    or a
    jr z, DONE       ; weight=0: skip entirely
    jp m, NEG        ; weight&lt;0: subtract
    ; weight=+1: add activation
    ld hl, (ACC)
    add hl, de
    ld (ACC), hl
    ret
NEG:
    cp 0FFh
    jr z, NEG1       ; weight=-1
    ; weight=-2: subtract twice
    ld hl, (ACC)
    sbc hl, de
    sbc hl, de
    ld (ACC), hl
    ret
NEG1:
    ; weight=-1: subtract once
    ld hl, (ACC)
    sbc hl, de
    ld (ACC), hl
    ret
</code></pre></div>
<p dir="auto">After each layer, arithmetic right-shift by 2 to prevent overflow:</p>
<div data-snippet-clipboard-copy-content="sra h        ; Shift right arithmetic (preserves sign)
rr l
sra h
rr l         ; ACC = ACC / 4"><pre lang="z80"><code>sra h        ; Shift right arithmetic (preserves sign)
rr l
sra h
rr l         ; ACC = ACC / 4
</code></pre></div>
<p dir="auto">That's the entire neural network: unpack weight, multiply-accumulate, shift. Repeat ~100K times per character generated.</p>
<hr>
<p dir="auto">License: MIT or Apache-2.0 as you see fit.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Huge Binaries (175 pts)]]></title>
            <link>https://fzakaria.com/2025/12/28/huge-binaries</link>
            <guid>46417791</guid>
            <pubDate>Mon, 29 Dec 2025 05:35:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fzakaria.com/2025/12/28/huge-binaries">https://fzakaria.com/2025/12/28/huge-binaries</a>, See on <a href="https://news.ycombinator.com/item?id=46417791">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>A problem I experienced when pursuing my PhD and submitting academic articles was that I had built solutions to problems that required dramatic scale to be effective and worthwhile. Responses to my publication submissions often claimed such problems did not exist; however, I had observed them during my time within industry, such as at Google, but I couldn’t cite it!</p>

<p>One problem that is only present at these mega-codebases is <em>massive binaries</em>. What’s the largest binary (ELF file) you’ve ever seen? I had observed binaries beyond 25GiB, including debug symbols. How is this possible? These companies prefer to statically build their services to speed up startup and simplify deployment. Statically including all code in some of the world’s largest codebases is a recipe for massive binaries.</p>

<p>Similar to the sound barrier, there is a point at which code size becomes problematic and we must re-think how we link and build code. For x86_64, that is the 2GiB “Relocation Barrier.”</p>

<p>Why 2GiB? 🤔</p>

<p>Well let’s take a look at how position independent code is put-together.</p>

<p>Let’s look at a simple example.</p>

<div><pre><code><span>extern</span> <span>void</span> <span>far_function</span><span>();</span>

<span>int</span> <span>main</span><span>()</span> <span>{</span>
    <span>far_function</span><span>();</span>
    <span>return</span> <span>0</span><span>;</span>
<span>}</span>
</code></pre></div>

<p>If we compile this <code>gcc -c simple-relocation.c -o simple-relocation.o</code> we can inspect it with <code>objdump</code>.</p>

<div><pre><code><span>&gt;</span> objdump <span>-dr</span> simple-relocation.o

0000000000000000 &lt;main&gt;:
   0:	55                   	push   %rbp
   1:	48 89 e5             	mov    %rsp,%rbp
   4:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
   9:	e8 00 00 00 00       	call   e &lt;main+0xe&gt;
			a: R_X86_64_PLT32	far_function-0x4
   e:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
  13:	5d                   	pop    %rbp
  14:	c3                   	ret
</code></pre></div>

<p>There’s a lot going on here, but one important part is <code>e8 00 00 00 00</code>. <code>e8</code> is the <code>CALL</code> opcode [<a href="https://c9x.me/x86/html/file_module_x86_id_26.html">ref</a>] and it takes a <strong>32bit signed relative offset</strong>, which happens to be 0 (four bytes of 0) right now. <code>objdump</code> also lets us know there is a “relocation” necessary to fixup this code when we finalize it. We can view this relocation with <code>readelf</code> as well.</p>

<blockquote>
  <p><strong>Note</strong>
If you are wondering why we need <code>-0x4</code>, it’s because the offset is relative to the instruction-pointer which has already moved to the next instruction. The 4 bytes is the operand it has skipped over.</p>
</blockquote>

<div><pre><code><span>&gt;</span> readelf <span>-r</span> simple-relocation.o <span>-d</span>

Relocation section <span>'.rela.text'</span> at offset 0x170 contains 1 entry:
  Offset          Info           Type           Sym. Value    Sym. Name + Addend
00000000000a  000400000004 R_X86_64_PLT32    0000000000000000 far_function - 4
</code></pre></div>

<p>This is additional information embedded in the binary which tells the linker in susbsequent stages that it has code that needs to be fixed. Here we see the address <code>00000000000a</code>, and <code>a</code> is 9 + 1, which is the offset of the start of the operand for our <code>CALL</code> instruction.</p>

<p>Let’s now create the C file for our missing function.</p>



<p>We will now compile it and link the two object files together using our linker.</p>

<div><pre><code><span>&gt;</span> gcc simple-relocation.o far-function.o <span>-o</span> simple-relocation
</code></pre></div>

<p>Let’s now inspect that same callsite and see what it has.</p>

<div><pre><code><span>&gt;</span> objdump <span>-dr</span> simple-relocation

0000000000401106 &lt;main&gt;:
  401106:	55                   	push   %rbp
  401107:	48 89 e5             	mov    %rsp,%rbp
  40110a:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
  40110f:	e8 07 00 00 00       	call   40111b &lt;far_function&gt;
  401114:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
  401119:	5d                   	pop    %rbp
  40111a:	c3                   	ret

000000000040111b &lt;far_function&gt;:
  40111b:	55                   	push   %rbp
  40111c:	48 89 e5             	mov    %rsp,%rbp
  40111f:	90                   	nop
  401120:	5d                   	pop    %rbp
  401121:	c3                   	ret
</code></pre></div>

<p>We can see that the linker did the right thing with the relocation and calculated the relative offset of our symbol <code>far_function</code> and fixed the <code>CALL</code> instruction.</p>

<p>Okay cool…🤷 What does this have to do with huge binaries?</p>

<p>Notice that this call instruction, <code>e8</code>, only takes 32bits <strong>signed</strong> which means it’s limited to 2^31 bits. This means a callsite can only jump roughly 2GiB forward or 2GiB backward. The “2GiB Barrier” represents the total reach of a single relative jump.</p>

<p>What happens if our callsite is over 2GiB away?</p>

<p>Let’s build a synthetic example by asking our linker to place <code>far_function</code> <em>really really far away</em>. We can do this using a “linker script”, which is a mechanism we can instruct the linker how we would like our code sections laid out when the program starts.</p>

<div><pre><code>SECTIONS
{
    /* 1. Start with standard low-address sections */
    . = 0x400000;
    
    /* Catch everything except our specific 'far' object */
    .text : { 
        simple-relocation.o(.text.*) 
    }
    .rodata : { *(.rodata .rodata.*) }
    .data   : { *(.data .data.*) }
    .bss    : { *(.bss .bss.*) }

    /* 2. Move the cursor for the 'far' island */
    . = 0x120000000; 
    
    .text.far : { 
        far-function.o(.text*) 
    }
}
</code></pre></div>

<p>If we now try to link our code we will see a “relocation overflow”.</p>

<blockquote>
  <p><strong>TIP</strong>
I used <code>lld</code> from <a href="https://lld.llvm.org/">LLVM</a> because the error messages are a bit prettier.</p>
</blockquote>

<div><pre><code><span>&gt;</span> gcc simple-relocation.o far-function.o <span>-T</span> overflow.lds <span>-o</span> simple-relocation-overflow <span>-fuse-ld</span><span>=</span>lld

ld.lld: error: &lt;internal&gt;:<span>(</span>.eh_frame+0x6c<span>)</span>:
relocation R_X86_64_PC32 out of range:
5364513724 is not <span>in</span> <span>[</span><span>-2147483648</span>, 2147483647]<span>;</span> references section <span>'.text'</span>
ld.lld: error: simple-relocation.o:<span>(</span><span>function </span>main: .text+0xa<span>)</span>:
relocation R_X86_64_PLT32 out of range:
5364514572 is not <span>in</span> <span>[</span><span>-2147483648</span>, 2147483647]<span>;</span> references <span>'far_function'</span>
<span>&gt;&gt;&gt;</span> referenced by simple-relocation.c
<span>&gt;&gt;&gt;</span> defined <span>in </span>far-function.o
</code></pre></div>

<p>When we hit this problem what solutions do we have?
Well this is a complete other subject on “code models”, and it’s a little more nuanced depending on whether we are accessing data (i.e. static variables) or code that is far away. A great blog post that goes into this is <a href="https://maskray.me/blog/2023-05-14-relocation-overflow-and-code-models">the following</a> by <a href="https://github.com/maskray">@maskray</a> who wrote <code>lld</code>.</p>

<p>The simplest solution however is to use <code>-mcmodel=large</code> which changes all the relative <code>CALL</code> instructions to absolute 64bit ones; kind of like a <code>JMP</code>.</p>

<div><pre><code><span>&gt;</span> gcc simple-relocation.o far-function.o <span>-T</span> overflow.lds <span>-o</span> simple-relocation-overflow

<span>&gt;</span> gcc <span>-c</span> simple-relocation.c <span>-o</span> simple-relocation.o <span>-mcmodel</span><span>=</span>large <span>-fno-asynchronous-unwind-tables</span>

<span>&gt;</span> gcc simple-relocation.o far-function.o <span>-T</span> overflow.lds <span>-o</span> simple-relocation-overflow

./simple-relocation-overflow
</code></pre></div>

<blockquote>
  <p><strong>Note</strong>
I needed to add <code>-fno-asynchronous-unwind-tables</code> to disable some additional data that might cause overflow for the purpose of this demonstration.</p>
</blockquote>

<p>What does the disassembly look like now?</p>

<div><pre><code><span>&gt;</span> objdump <span>-dr</span> simple-relocation-overflow 

0000000120000000 &lt;far_function&gt;:
   120000000:	55                   	push   %rbp
   120000001:	48 89 e5             	mov    %rsp,%rbp
   120000004:	90                   	nop
   120000005:	5d                   	pop    %rbp
   120000006:	c3                   	ret

00000000004000e6 &lt;main&gt;:
  4000e6:	55                   	push   %rbp
  4000e7:	48 89 e5             	mov    %rsp,%rbp
  4000ea:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
  4000ef:	48 ba 00 00 00 20 01 	movabs <span>$0x120000000</span>,%rdx
  4000f6:	00 00 00 
  4000f9:	ff d2                	call   <span>*</span>%rdx
  4000fb:	b8 00 00 00 00       	mov    <span>$0x0</span>,%eax
  400100:	5d                   	pop    %rbp
  400101:	c3                   	ret
</code></pre></div>

<p>There is no longer a sole <code>CALL</code> instruction, it has become <code>MOVABS</code> &amp; <code>CALL</code> 😲. This changed the instructions from 5 (opcode + 4 bytes for 32bit relative offset) to a whopping 12 bytes (2 bytes for <code>ABS</code> opcode + 8 bytes for absolute 64 bit address + 2 bytes for <code>CALL</code>).</p>

<p>This has notable downsides among others:</p>
<ul>
  <li><em>Instruction Bloat</em>: We’ve gone from 5 bytes per call to 12. In a binary with millions of callsites, this can add up.</li>
  <li><em>Register Pressure</em>: We’ve burned a general-purpose register, <code>%rdx</code>, to perform the jump.</li>
</ul>

<blockquote>
  <p><strong>Caution</strong>
I had a lot of trouble building a benchmark that demonstrated a worse lower IPC (instructions per-cycle) for the large <code>mcmodel</code>, so let’s just take my word for it. 🤷</p>
</blockquote>

<p>Changing to a larger code-model is possible but it comes with these downsides. Ideally, we would like to keep our small code-model when we need it. What other strategies can we pursue?</p>

<p>More to come in subsequent writings.</p>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Panoramas of Star Trek Sets (149 pts)]]></title>
            <link>https://mijofr.github.io/st-panorama/</link>
            <guid>46417752</guid>
            <pubDate>Mon, 29 Dec 2025 05:26:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mijofr.github.io/st-panorama/">https://mijofr.github.io/st-panorama/</a>, See on <a href="https://news.ycombinator.com/item?id=46417752">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="bodyText"><p>Aliquam nulla felis, luctus ut ullamcorper eget, ultricies sed mi. Duis gravida massa vitae purus sodales, sit amet aliquet sem varius.</p><p>Nullam varius mi vitae felis imperdiet, ac pharetra nulla porttitor. Duis porttitor enim leo, id pulvinar orci ornare non. Praesent dapibus ante rutrum lacus eleifend egestas. Aliquam fringilla mauris non commodo placerat.</p><p>Nulla facilisi. Curabitur ut odio consectetur, egestas tortor quis, aliquet velit. Fusce fringilla neque at ultrices consectetur. Nam aliquam arcu in sem convallis, quis pretium tellus consequat.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My not-for-profit search engine with no ads, no AI, & all DDG bangs (175 pts)]]></title>
            <link>https://nilch.org</link>
            <guid>46417748</guid>
            <pubDate>Mon, 29 Dec 2025 05:25:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nilch.org">https://nilch.org</a>, See on <a href="https://news.ycombinator.com/item?id=46417748">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <p>No AI, no ads, just search.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[My First Meshtastic Network (142 pts)]]></title>
            <link>https://rickcarlino.com/notes/electronics/my-first-meshtastic-network.html</link>
            <guid>46417676</guid>
            <pubDate>Mon, 29 Dec 2025 05:12:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rickcarlino.com/notes/electronics/my-first-meshtastic-network.html">https://rickcarlino.com/notes/electronics/my-first-meshtastic-network.html</a>, See on <a href="https://news.ycombinator.com/item?id=46417676">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
          
          <div><p>I first heard about <a href="https://meshtastic.org/" target="_blank" rel="noopener">Meshtastic</a> from <a href="https://blog.noforeignland.com/off-grid-boat-communications-with-meshtastic/" target="_blank" rel="noopener">a blog post</a> that made the rounds on Hacker News.</p><p>
The author lived on a boat and used Meshtastic radios to stay in touch without cellular networks. Meshtastic allows you to send short text messages (around 200 characters) over long ranges without cell towers or satellites. It works by creating a mesh network of low-power LoRa devices that relay messages on behalf of peers. Because it uses license-free radio frequencies (in the ~915 MHz ISM band), no ham license is required.</p></div>

<h2>My First Radio</h2>

<p><img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224634.png" alt="Pasted image 20251228224634.png"><br>
I ordered a pair of Heltec V3 LoRa radios (<a href="https://www.amazon.com/dp/B0FG6XXSTW" target="_blank" rel="noopener">the ones I bought</a>), which are small devices based on the ESP32 microcontroller with a LoRa modem. These radios didn't come with GPS, which in hindsight I regret because Meshtastic can share your location if a GPS is present. I also picked up a <a href="https://www.amazon.com/dp/B0F1CXG94J" target="_blank" rel="noopener">third-party antenna upgrade</a>, since the community warned that the cheap antennas bundled with these devices are nearly useless (yet another thing I learned in hindsight)</p>

<p>Out of the box, the devices had outdated firmware and wouldn't communicate with current Meshtastic apps. Fortunately, flashing the latest firmware was straightforward using the official <strong>Meshtastic Web Flasher</strong> (a browser-based tool at <a href="https://flasher.meshtastic.org/" target="_blank" rel="noopener">flasher.meshtastic.org</a>). By connecting the device via USB and using Chrome (which supports the WebSerial API), I flashed the newest Meshtastic firmware without installing any software.</p>

<h2>Initial Setup</h2>

<p><img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224604.png" alt="Pasted image 20251228224604.png"><br>
With fresh firmware, I could configure and manage the radios using the Meshtastic mobile app (available for Android/iOS) over Bluetooth. There's also a web client (<a href="https://client.meshtastic.org/" target="_blank" rel="noopener">client.meshtastic.org</a>) that works over USB or Wi-Fi. One quirk I learned: many Meshtastic devices can work over both Wi-Fi and Bluetooth, but you typically use one interface at a time for management. On my device, it was not possible to use both at the same time, which led to some confusion.</p>

<p>After setup, I had my two devices chatting with each other. Sending a message from one device would pop up on the other in a few seconds. Meshtastic uses a mesh protocol where every node repeats messages, so two devices in direct range will communicate one-to-one, and if more nodes are around they can hop messages further. I noticed that if I tried sending when only one device was on, the app would show <q>Waiting to be acknowledged...</q> and eventually <q>Maximum retransmission reached.</q> In other words, my message went nowhere because no other node heard it.</p>

<h2>First Contact</h2>

<p><img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224128.png" alt="Pasted image 20251228224128.png"></p>

<p>Up to this point, I hadn't heard any traffic besides my own test messages. I suspected I was the only Meshtastic user in my immediate area (the far west suburbs of Chicago). I left one radio powered on and placed it by a second-story window facing toward the city. The next morning, I was surprised to see that it had logged messages from a handful of unknown nodes overnight.</p>

<p>Looking up the node identifiers online, I discovered a website called <a href="https://meshmap.net/" target="_blank" rel="noopener">MeshMap</a> that shows public Meshtastic nodes on a map. Sure enough, some of the node names I saw appeared on a community mesh map of the Chicagoland area. A few even had labels referencing <q>ChiMesh</q>. There's an active group of Meshtastic enthusiasts in Chicago, <a href="https://chicagolandmesh.org/" target="_blank" rel="noopener">Chicagoland Mesh</a>, and somehow my little device had picked up their transmissions from roughly 40-50 miles away. This was an early sign that mesh networking can extend beyond line-of-sight with the help of intermediate nodes.</p>

<h2>Joining a Local Community</h2>

<p>Excited by this discovery, I joined the Chicagoland Mesh (ChiMesh) Discord server and introduced myself. To my surprise, there was another member only a mile or two from my house. We coordinated a simple experiment: he sent a test message from his device at home, and I was able to receive it on mine. However, when I tried to reply, he never saw my message. It became clear that while I could <q>hear</q> the network, my transmission range was too short for others to hear me.</p>

<p>Community members quickly pointed to the antenna as the culprit. The stock rubber-duck antenna that came with my Heltec radio was likely low-quality. I switched to the high-gain antenna I bought and tried again. This time my messages started getting through. Antenna quality (and placement) makes a huge difference in radio range.</p>

<h2>Expanding the Network</h2>

<p><img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224246.png" alt="Pasted image 20251228224246.png"><br>
My success rekindled interest among a couple of local makers. Some fellow members of my local makerspace had dabbled with Meshtastic earlier but stopped due to the lack of active users. With my second device to spare, we set it up as a relay node at the makerspace, which is a few miles from my house. Positioned near a roofline, this node acted like a little tower, rebroadcasting messages between my home and the other member's location.<br>
<img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224518.png" alt="Pasted image 20251228224518.png"><br>
It took a bit of fiddling with placement and settings, but eventually we managed to pass messages between our homes via the makerspace node. It wasn't instantaneous or foolproof, but messages eventually hopped from my device to the relay and then to my friend's device, reaching farther than any single link could.</p>

<h2>Next Steps</h2>

<p><img src="https://rickcarlino.com/notes/images/Pasted%20image%2020251228224347.png" alt="Pasted image 20251228224347.png"><br>
To better understand and improve our coverage, we started playing with the <a href="https://site.meshtastic.org/" target="_blank" rel="noopener">Meshtastic Site Planner</a>. This is a web tool that lets you simulate radio coverage on a map given a node's location, antenna, and power. Being in a river valley, our area has some challenging terrain that limits range. The planner helped confirm that putting a node on higher ground (a tall building) could dramatically extend reach.</p>

<p>In the coming months, we plan to upgrade to better antennas (perhaps an outdoor mounted one on a mast) and add more nodes at strategic spots. I'm also interested in experimenting with Meshtastic's other capabilities. For example, it can interface with sensors and send telemetry. A fun project idea is an off-grid weather station broadcasting its data over the mesh network.</p>

<h2>Continuing the Exploration</h2>

<p>Working with Meshtastic has been fun. It's impressive how a few inexpensive devices can form a communications network covering many miles. The system is limited, but within those constraints it feels magical to send a message into the ether and have it hop across a county line to a stranger.</p>

<p>Meshtastic isn't very useful alone, but as more people join, the mesh becomes stronger and more useful for everyone. If you're in the Illinois Fox Valley area and interested, feel free to reach out or drop by our makerspace meetup - we'd love to grow the network. And if you're elsewhere, consider looking up <a href="https://meshtastic.org/docs/community/local-groups/" target="_blank" rel="noopener">Meshtastic groups in your region</a>. I hope to see you on the air.</p>

        </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You can make up HTML tags (493 pts)]]></title>
            <link>https://maurycyz.com/misc/make-up-tags/</link>
            <guid>46416945</guid>
            <pubDate>Mon, 29 Dec 2025 02:47:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maurycyz.com/misc/make-up-tags/">https://maurycyz.com/misc/make-up-tags/</a>, See on <a href="https://news.ycombinator.com/item?id=46416945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<time datetime="2025-08-02">Aug 2, 2025</time>



(<a href="http://maurycyz.com/tags/programming">Programming</a>) 



<p>Instead of writing HTML like this:</p>
<div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>div</span> <span>class</span><span>=</span><span>cool-thing</span>&gt;
</span></span><span><span>Hello, World!
</span></span><span><span>&lt;/<span>div</span>&gt;
</span></span></code></pre></div><p>… you can write HTML like this:</p>
<div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>cool-thing</span>&gt;
</span></span><span><span>Hello, World!
</span></span><span><span>&lt;/<span>cool-thing</span>&gt;
</span></span></code></pre></div><p>… and CSS like this:</p>
<div><pre tabindex="0"><code data-lang="css"><span><span><span>cool-thing</span> {
</span></span><span><span>	<span>display</span>: <span>block</span>;
</span></span><span><span>	<span>font-weight</span>: <span>bold</span>;
</span></span><span><span>	<span>text-align</span>: <span>center</span>;
</span></span><span><span>	<span>filter</span>: drop-shadow(<span>0</span> <span>0</span> <span>0.5</span><span>em</span> <span>#ff0</span>);
</span></span><span><span>	<span>color</span>: <span>#ff0</span>;
</span></span><span><span>}
</span></span></code></pre></div>
<cool-thing>
Hello, World!
</cool-thing>

<p>Browsers handle unrecognized tags by treating them as a generic element, with no effect beyond what’s specified in the CSS.
This isn’t just a weird quirk, but is <a href="https://html.spec.whatwg.org/multipage/dom.html#htmlunknownelement">standardized behavior</a>.
If you include hyphens in the name, you can guarantee that your tag won’t appear in any future versions of HTML.</p>
<p>While you should use descriptive built-in tags if they exist, if it’s a choice between &lt;div&gt; and &lt;span&gt;,
making up your own tag provides better readability then using a bunch of class names.</p>
<p>As an example, if you have a bunch of nested tags:</p>
<div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>div</span> <span>class</span><span>=</span><span>article</span>&gt;
</span></span><span><span>&lt;<span>div</span> <span>class</span><span>=</span><span>article-header</span>&gt;
</span></span><span><span>&lt;<span>div</span> <span>class</span><span>=</span><span>article-quote</span>&gt;
</span></span><span><span>&lt;<span>div</span> <span>class</span><span>=</span><span>quote-body</span>&gt;
</span></span><span><span>... a bunch more HTML ...
</span></span><span><span>&lt;/<span>div</span>&gt;
</span></span><span><span>&lt;/<span>div</span>&gt;
</span></span><span><span>&lt;/<span>div</span>&gt;
</span></span><span><span>&lt;/<span>div</span>&gt;
</span></span></code></pre></div><p>Good luck trying to insert something inside of “article-heading” but after “article-quote” on the first try.
This problem vanishes if you use descriptive tag names — no &lt;/div&gt; counting required:</p>
<div><pre tabindex="0"><code data-lang="html"><span><span>&lt;<span>main-article</span>&gt;
</span></span><span><span>&lt;<span>article-header</span>&gt;
</span></span><span><span>&lt;<span>article-quote</span>&gt;
</span></span><span><span>&lt;<span>quote-body</span>&gt;
</span></span><span><span>... a bunch more HTML ...
</span></span><span><span>&lt;/<span>quote-body</span>&gt;
</span></span><span><span>&lt;/<span>article-quote</span>&gt;
</span></span><span><span><span>&lt;!-- here! --&gt;</span>
</span></span><span><span>&lt;/<span>article-header</span>&gt;
</span></span><span><span>&lt;/<span>main-article</span>&gt;
</span></span></code></pre></div>

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rich Hickey: Thanks AI (177 pts)]]></title>
            <link>https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f</link>
            <guid>46415945</guid>
            <pubDate>Mon, 29 Dec 2025 00:20:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f">https://gist.github.com/richhickey/ea94e3741ff0a4e3af55b9fe6287887f</a>, See on <a href="https://news.ycombinator.com/item?id=46415945">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="file-thanksai-md" tabindex="0" role="region" aria-label="ThanksAI.md content, created by richhickey on 09:25PM yesterday.">
    <article itemprop="text">
<p dir="auto">I got this email:</p>
<blockquote>
<p dir="auto">Rich,</p>
<p dir="auto">Your creation of Clojure ...</p>
<p dir="auto"><code>sycophantic blather worthy of a third grader's homework assignment to write a letter to a public figure you don't know, using sources you don't understand, to  express an emotion unfelt, with no intention whatsoever</code></p>
<p dir="auto">Claude Haiku 4.5</p>
</blockquote>
<p dir="auto">Ah, Christmas time. That time of year when our hearts are warmed by the best wishes of an idiot robot. All tingly from the experience, and in the holiday spirit, I thought I'd write my own letter of thanks to those bringing us this next generation of 'AI'.</p>
<p dir="auto"><h3 dir="auto"><em>Dear 'AI' purveyors,</em></h3><a id="user-content-dear-ai-purveyors" aria-label="Permalink: Dear 'AI' purveyors," href="#dear-ai-purveyors"></a></p>
<p dir="auto">How shall I thank thee, let me count the ways:</p>
<p dir="auto">Should I thank you for pirating the entire historical output of creative humanity and then asserting ownership of your loot?</p>
<p dir="auto">For destroying education?</p>
<p dir="auto">For raising utility rates and killing the environment?</p>
<p dir="auto">For wasting vast quantities of developer time trying to coax some useful output from your BS generators, time which could instead be used communicating to interns and entry-level devs who, being actually intelligent, could learn from what they are told, and maintain what they make?</p>
<p dir="auto">For eliminating entry-level jobs, and thus the path to experience, ensuring future generations of unskilled, unemployable people?</p>
<p dir="auto">For giving me a fake person to talk to when I need support instead of an actual person who understands what I'm saying, can help me faster, and has a chance of caring?</p>
<p dir="auto">For replacing search results with summary BS?</p>
<p dir="auto">For providing the tools to fill the internet with slop, making actual human content almost impossible to find?</p>
<p dir="auto">For enticing CEOs with the promise to save some fraction of a percent on personnel costs, not actually be any faster, cutting off their future labor pool while only experiencing a modest to severe reduction in product quality, integrity and customer satisfaction (tradeoffs they are apparently eager to make)?</p>
<p dir="auto">For replacing musical expression with the sounds of robot parrots chirping?</p>
<p dir="auto">For adding an 'AI' feature to every flipping thing, most such features requiring a deep invasion of privacy?</p>
<p dir="auto">For running the second biggest and most damaging con of this century (running hard at first)?</p>
<p dir="auto"><strong><em>I think not.</em></strong></p>
<p dir="auto">This email was a reminder that agentic 'AI' is sure to flood the remainder of human communication channels with BS, swamping many services, and making every interaction with people not in the same room suspect, and filtering it a time-consuming waste, forever.</p>
<p dir="auto"><strong><em>When did we stop considering things failures that create more problems than they solve?</em></strong></p>
<p dir="auto"><h3 dir="auto"><em>Rich</em></h3><a id="user-content-rich" aria-label="Permalink: Rich" href="#rich"></a></p>
</article>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: My app just won best iOS Japanese learning tool of 2025 award (blog) (116 pts)]]></title>
            <link>https://skerritt.blog/best-japanese-learning-tools-2025-award-show/</link>
            <guid>46415819</guid>
            <pubDate>Mon, 29 Dec 2025 00:01:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://skerritt.blog/best-japanese-learning-tools-2025-award-show/">https://skerritt.blog/best-japanese-learning-tools-2025-award-show/</a>, See on <a href="https://news.ycombinator.com/item?id=46415819">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p>Welcome to the award show everyone! Hosted by your favourite bee... Bee! 🥳</p><p>I wanted to summarise the best tools etc out there in 2025, and what better way then to put on a fake award show!</p><p>And like all true award shows and Christmas themed events, let's get into the spirit of giving.</p><h2 id="best-overall">Best Overall</h2><p>This category features 3 tools.</p><p>If I could only pick 3 to learn Japanese with, it would be these 3.</p><p>The best overall winner of the 2025 Japanese Learning Awards is....</p><h2 id="%F0%9F%8F%86yomitan">🏆Yomitan</h2><p>Yomitan is the go-to dictionary application.</p><p>It works in all browsers (Chrome, Firefox, Edge) and even on mobile browsers.</p><figure><a href="https://yomitan.wiki/?ref=skerritt.blog"><div><p>Yomitan</p><p>Powerful and versatile pop-up dictionary for language learning used by 90,000+ language learners.</p><p><img src="https://skerritt.blog/content/images/icon/yomitan-icon64-2.png" alt=""><span>logo</span><span>Yomitan Authors</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/index-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>You install it easily and just select your language and some dictionaries</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4C63DE91-C0EC-481E-9796-BC94F35D6FB4-.png" alt="" loading="lazy" width="775" height="534" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-4C63DE91-C0EC-481E-9796-BC94F35D6FB4-.png 600w, https://skerritt.blog/content/images/2025/12/-4C63DE91-C0EC-481E-9796-BC94F35D6FB4-.png 775w" sizes="(min-width: 720px) 720px"></figure><p>It supports:</p><ul><li>Many dictionaries across many languages</li><li>Anki</li><li>Native audio</li></ul><figure><a href="https://yomitan.wiki/?ref=skerritt.blog"><div><p>Yomitan</p><p>Powerful and versatile pop-up dictionary for language learning used by 90,000+ language learners.</p><p><img src="https://skerritt.blog/content/images/icon/yomitan-icon64-1.png" alt=""><span>logo</span><span>Yomitan Authors</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/index.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Even if you don't use any of the fancy features, having a dictionary you can use at the click of a button is useful.</p><h2 id="anki">Anki</h2><p>If you use Yomitan, you must also use Anki too.</p><p>Anki is the premier flashcard software.</p><p>You see a word you don't know, and create a flashcard for it in Anki.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image.png" alt="" loading="lazy" width="1047" height="642" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image.png 1000w, https://skerritt.blog/content/images/2025/12/image.png 1047w" sizes="(min-width: 720px) 720px"></figure><p>Anki solves the issue of forgetting, mostly. You will still forget things, but significantly less.</p><figure><a href="https://apps.ankiweb.net/?ref=skerritt.blog"><div><p>Anki - powerful, intelligent flashcards</p><p>Anki - a program which makes remembering things easy.</p><p><img src="https://skerritt.blog/content/images/icon/logo-1.svg" alt=""><span>powerful, intelligent flashcards</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/logo.svg" alt="" onerror="this.style.display = 'none'"></p></a></figure><h2 id="gsm">GSM</h2><p><a href="https://github.com/bpwhelan/GameSentenceMiner?ref=skerritt.blog">https://github.com/bpwhelan/GameSentenceMiner</a></p><p>Game Sentence Miner (GSM) is an all-in-one toolkit to turn any visual media into Anki flashcards.</p><figure data-kg-thumbnail="https://skerritt.blog/content/media/2025/12/2025-12-08_21-51-02-cut-merged-1765249662879_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://skerritt.blog/content/media/2025/12/2025-12-08_21-51-02-cut-merged-1765249662879.mp4" poster="https://img.spacergif.org/v1/1920x1080/0a/spacer.png" width="1920" height="1080" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:21</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>1. Look up words natively in game. 2. Click Add to Anki. 3. Anki card automatically made in the background with game audio + a gif of the game</span></p></figcaption>
        <img src="https://skerritt.blog/content/media/2025/12/2025-12-08_21-51-02-cut-merged-1765249662879_thumb.jpg"></figure><p>Use the overlay to directly look words up (using Yomitan) in your game, anime, or manga without needing to go to another website to look it up.</p><figure data-kg-thumbnail="https://skerritt.blog/content/media/2025/12/pythonw_s7SkczNvqJ_thumb.jpg" data-kg-custom-thumbnail="">
            <div>
                <video src="https://skerritt.blog/content/media/2025/12/pythonw_s7SkczNvqJ.mp4" poster="https://img.spacergif.org/v1/1096x672/0a/spacer.png" width="1096" height="672" playsinline="" preload="metadata"></video>
                
                <div>
                        <p>
                        
                        <span>0:00</span></p><p>
                            /<span>0:03</span>
                        </p>
                        </div>
            </div>
            <figcaption><p><span>Anki card created with GSM</span></p></figcaption>
        <img src="https://skerritt.blog/content/media/2025/12/pythonw_s7SkczNvqJ_thumb.jpg"></figure><p>Create flashcards in one click with the real audio used, and a gif of what happened on screen.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4C71478D-9E7A-458A-94A1-F911B7E59597-.png" alt="" loading="lazy" width="1199" height="794" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-4C71478D-9E7A-458A-94A1-F911B7E59597-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-4C71478D-9E7A-458A-94A1-F911B7E59597-.png 1000w, https://skerritt.blog/content/images/2025/12/-4C71478D-9E7A-458A-94A1-F911B7E59597-.png 1199w" sizes="(min-width: 720px) 720px"></figure><p>Analyse your statistics to help you learn to read better, over 30+ graphs and extensive goal planning.</p><p>Best of all? It's 100% free, works offline, and works for many other languages – not just Japanese!</p><div><p>💡</p><div><p>Because of the way GSM was built, it kind of acts like glue between many different tools to enhance them.</p><p>GSM can take in text from anywhere, create statistics based on it and enhance your Anki cards with gifs + audio along with an overlay dictionary.</p><p>For this reason you'll see it come up a lot... It's a well loved tool, and for good reason!</p></div></div><p>GSM's main problem is the barrier to entry can be high, it's got a lot of features and many settings. Thankfully the author has created many, many blog posts and YouTube videos on how to use it.</p><h2 id="best-phone-apps">Best Phone Apps</h2><h2 id="%F0%9F%8F%86renshuuoverall-winner">🏆Renshuu - Overall Winner</h2><p>Renshuu wins the best app of 2025!</p><p>It's like Duolingo but better in every way.</p><p>It can work out your level and adjust the difficulty of words or sentences</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4501E388-57F6-4556-A137-6A68E2417BA2-.png" alt="" loading="lazy" width="1279" height="485" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-4501E388-57F6-4556-A137-6A68E2417BA2-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-4501E388-57F6-4556-A137-6A68E2417BA2-.png 1000w, https://skerritt.blog/content/images/2025/12/-4501E388-57F6-4556-A137-6A68E2417BA2-.png 1279w" sizes="(min-width: 720px) 720px"></figure><p>It gives you <em>varied</em> practice. Writing kanji, flipping flashcards, and fun games.</p><p>If you're looking for an easy app to replace the Green Owl™️ but actually be somewhat effective, this is it!</p><figure><a href="https://www.renshuu.org/?ref=skerritt.blog"><div><p>renshuu.org - cute Japanese studying that’s built around you</p><p><img src="https://skerritt.blog/content/images/icon/apple-icon-144x144.png" alt=""><span>renshuu</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/feature.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><h3 id="best-android-apps">Best Android Apps</h3><p>Let's split this up into two, IOS and Android.</p><h2 id="%F0%9F%8F%86jidoujishooverall-android-winner">🏆Jidoujisho - Overall Android Winner</h2><figure><a href="https://github.com/arianneorpilla/jidoujisho?ref=skerritt.blog"><div><p>GitHub - arianneorpilla/jidoujisho: A full-featured immersion language learning suite for mobile.</p><p>A full-featured immersion language learning suite for mobile. - arianneorpilla/jidoujisho</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-17.svg" alt=""><span>GitHub</span><span>arianneorpilla</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/jidoujisho" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>This is an everything-in-one kinda app.</p><ul><li>Supports Ankidroid</li><li>Dictionary lookups similar to Yomitan</li><li>Watch videos or listen to audio, and make flashcards from them</li><li>Read books and make flashcards from them</li><li>Read Manga! </li><li>Play video games, visual novels etc.</li><li>Instantly look up the lyrics of the song you're listening to, and make flashcards</li></ul><figure><img src="https://skerritt.blog/content/images/2025/12/image-19.png" alt="" loading="lazy" width="1280" height="591" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-19.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-19.png 1000w, https://skerritt.blog/content/images/2025/12/image-19.png 1280w" sizes="(min-width: 720px) 720px"><figcaption><span>Mining from videos</span></figcaption></figure><p>If you do not have access to a computer, this is perhaps the best app to do everything on Android.</p><p>But! It does require some time to setup and learn how it all works.</p><h4 id="poe">Poe</h4><p>Poe is Yomitan for Android</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-1.png" alt="" loading="lazy" width="1080" height="1237" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-1.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-1.png 1000w, https://skerritt.blog/content/images/2025/12/image-1.png 1080w" sizes="(min-width: 720px) 720px"></figure><p>It supports Anki, native audio and pitch accent.</p><p>I wrote more about this here:</p><figure><a href="https://skerritt.blog/poe-yomitan-for-android/"><div><p>Poe - Yomitan for Android</p><p>I’ve been playing with Poe recently: Poe: Language Lens - Apps on Google PlayPop-up dictionary and language learning tool for Japanese, Chinese, and more.Apps on Google PlaySlime Creative This is Yomitan for Androids but anywhere on the screen. It’s really easy to install. You just have to: 1. Install</p><p><img src="https://skerritt.blog/content/images/icon/favicon-19.ico" alt=""><span>Skerritt.blog</span><span>Autumn Skerritt</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/photo-1495615080073-6b89c9839ce0" alt="" onerror="this.style.display = 'none'"></p></a></figure><h3 id="ios">iOS</h3><p>Now let's look at the options on IOS, albeit limited options.</p><h2 id="%F0%9F%8F%86manabi-readeroverall-ios-winner">🏆Manabi Reader - Overall IOS Winner</h2><figure><a href="https://reader.manabi.io/?ref=skerritt.blog"><div><p>Manabi Reader – Learn Japanese by Reading on iOS, iPadOS &amp; macOS</p><p><img src="https://skerritt.blog/content/images/icon/iTunesArtwork@1x.png" alt=""><span>Manabi Reader</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/iTunesArtwork@1x.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Manabi Reader is a way to read on IOS, similar to Jidoujisho but with less features. Not their fault, mostly IOS has a lot of walls.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-20.png" alt="" loading="lazy" width="690" height="329" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-20.png 600w, https://skerritt.blog/content/images/2025/12/image-20.png 690w"></figure><p>You can look words up in dictionaries and send things to Anki.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-25.png" alt="" loading="lazy" width="934" height="1016" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-25.png 600w, https://skerritt.blog/content/images/2025/12/image-25.png 934w" sizes="(min-width: 720px) 720px"></figure><p>See breakdown of sentences, how many words in a sentence do you know? </p><figure><img src="https://skerritt.blog/content/images/2025/12/image-26.png" alt="" loading="lazy" width="946" height="904" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-26.png 600w, https://skerritt.blog/content/images/2025/12/image-26.png 946w" sizes="(min-width: 720px) 720px"></figure><p>You can read books and webpages and get full comprehension statistics about that page. </p><p>You can also look up words using OCR or by pasting the text.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-28.png" alt="" loading="lazy" width="1170" height="2532" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-28.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-28.png 1000w, https://skerritt.blog/content/images/2025/12/image-28.png 1170w" sizes="(min-width: 720px) 720px"></figure><p>The author is working on a bunch of new features as they told me:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-57447E28-7891-4E9E-BC3E-0A0ED41A721B-.png" alt="" loading="lazy" width="1165" height="531" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-57447E28-7891-4E9E-BC3E-0A0ED41A721B-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-57447E28-7891-4E9E-BC3E-0A0ED41A721B-.png 1000w, https://skerritt.blog/content/images/2025/12/-57447E28-7891-4E9E-BC3E-0A0ED41A721B-.png 1165w"></figure><p>Here are some exclusive behind the scenes screenshots of the new Manabi Reader, coming soon!</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-29.png" alt="" loading="lazy" width="1590" height="1828" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-29.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-29.png 1000w, https://skerritt.blog/content/images/2025/12/image-29.png 1590w" sizes="(min-width: 720px) 720px"><figcaption><span>HIghlighting for words you know / don't know</span></figcaption></figure><figure><img src="https://skerritt.blog/content/images/2025/12/image-30.png" alt="" loading="lazy" width="1224" height="960" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-30.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-30.png 1000w, https://skerritt.blog/content/images/2025/12/image-30.png 1224w" sizes="(min-width: 720px) 720px"></figure><figure><img src="https://skerritt.blog/content/images/2025/12/image-31.png" alt="" loading="lazy" width="750" height="1334" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-31.png 600w, https://skerritt.blog/content/images/2025/12/image-31.png 750w" sizes="(min-width: 720px) 720px"><figcaption><span>Comprehension stats</span></figcaption></figure><figure><img src="https://skerritt.blog/content/images/2025/12/image-32.png" alt="" loading="lazy" width="1280" height="200" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-32.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-32.png 1000w, https://skerritt.blog/content/images/2025/12/image-32.png 1280w" sizes="(min-width: 720px) 720px"></figure><div><p>👻</p><p>I am not an IOS user (as you may be able to tell), but I have heard Migaku works great on IOS too.</p></div><h4 id="shiori-reader">Shiori Reader</h4><figure><a href="https://apps.apple.com/kg/app/shiori-reader/id6744979827?ref=skerritt.blog"><div><p>Shiori Reader App - App Store</p><p>Download Shiori Reader by Clint Russell Graviet Jr on the App&nbsp;Store. See screenshots, ratings and reviews, user tips and more games like Shiori Reader.</p><p><img src="https://skerritt.blog/content/images/icon/favicon-180.png" alt=""><span>App Store</span><span>Clint Russell Graviet Jr</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/1200x630wa.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>This is another "look things up and make anki cards" app, but this time it focusses on reading books.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-02A41F86-6E1E-4C11-9273-39927F9BA0AA-.png" alt="" loading="lazy" width="691" height="482" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-02A41F86-6E1E-4C11-9273-39927F9BA0AA-.png 600w, https://skerritt.blog/content/images/2025/12/-02A41F86-6E1E-4C11-9273-39927F9BA0AA-.png 691w"></figure><h2 id="best-anki-decks">Best Anki Decks</h2><p>Since we've talked so much about Anki, one of the big questions people have who begun using it is "what decks do I use?"</p><h2 id="%F0%9F%8F%86kaishioverall-best-anki-deck">🏆Kaishi - Overall Best Anki Deck</h2><figure><a href="https://github.com/donkuri/kaishi?ref=skerritt.blog"><div><p>GitHub - donkuri/kaishi: Kaishi 1.5k is a modern, modular Japanese Anki deck made for beginners who want to learn basic vocabulary.</p><p>Kaishi 1.5k is a modern, modular Japanese Anki deck made for beginners who want to learn basic vocabulary. - donkuri/kaishi</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-18.svg" alt=""><span>GitHub</span><span>donkuri</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/kaishi" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>This is the definitive Anki deck for people just getting into learning Japanese with Anki.</p><p>The idea is that this teaches the most common words found in media, not necessarily the words you'll come across ordering food in Japan.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-22.png" alt="" loading="lazy" width="690" height="357" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-22.png 600w, https://skerritt.blog/content/images/2025/12/image-22.png 690w"></figure><p>Once you finish this deck you then know enough Japanese to read books / immerse. You will still struggle, but it won't be as bad as starting from 0.</p><h2 id="japanese-proper-nouns">Japanese Proper Nouns</h2><p>Do you have problems reading city names? What about names of people?</p><p>The proper nouns Anki deck is designed to teach you all the important <em>proper nouns</em> you'll encounter, and then pretty much every proper noun ever.</p><figure><a href="https://github.com/friedrich-de/Japanese-Proper-Nouns-Deck?ref=skerritt.blog"><div><p>GitHub - friedrich-de/Japanese-Proper-Nouns-Deck</p><p>Contribute to friedrich-de/Japanese-Proper-Nouns-Deck development by creating an account on GitHub.</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-20.svg" alt=""><span>GitHub</span><span>friedrich-de</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/Japanese-Proper-Nouns-Deck" alt="" onerror="this.style.display = 'none'"></p></a></figure><h2 id="best-anki-notetypes">Best Anki Notetypes</h2><blockquote>"okay bee, I finished Kaishi. I want to use Yomitan to make my own Anki deck but it wants a note type... what do I use?"</blockquote><p>I hear you say! <em>probably</em>....</p><h2 id="%F0%9F%8F%86kiku">🏆Kiku</h2><figure><a href="https://kiku-docs.vercel.app/?ref=skerritt.blog"><div><p>Kiku</p><p>Modern Anki notes, built like web apps.</p><p><img src="https://skerritt.blog/content/images/icon/faviconV2-1" alt=""><span>Getting Started</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/logo.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Kiku came out swinging towards the end of 2025 as the go to Anki note type.</p><p>Other note types were static, but Kiku harnessed the power of Javascript in Anki.</p><p>View similar Kanji, and view other flashcards that you made that use that kanji!</p><figure><img src="https://skerritt.blog/content/images/2025/12/-2A827281-C1DD-48E1-B968-6EEF36202BCD-.png" alt="" loading="lazy" width="609" height="562" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-2A827281-C1DD-48E1-B968-6EEF36202BCD-.png 600w, https://skerritt.blog/content/images/2025/12/-2A827281-C1DD-48E1-B968-6EEF36202BCD-.png 609w"></figure><p>Sometimes you come across a word used in a really nice context, but you already have a flashcard for it!</p><p>You want to make another flashcard because you love this context, but it's just not possible without duplicating them or deleting your old card 🫠</p><p>Kiku solves this by allowing you to have multiple contexts in one card.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-E7BDABDF-318D-48EC-B79F-3CC6EC44BFB2-.png" alt="" loading="lazy" width="641" height="531" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-E7BDABDF-318D-48EC-B79F-3CC6EC44BFB2-.png 600w, https://skerritt.blog/content/images/2025/12/-E7BDABDF-318D-48EC-B79F-3CC6EC44BFB2-.png 641w"></figure><p>Most Anki card themes come in either light mode or dark mode. </p><p>Kiku has over 35 themes.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-DA9D423A-4CB0-4347-B79E-479AAA93FE5A-.png" alt="" loading="lazy" width="976" height="653" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-DA9D423A-4CB0-4347-B79E-479AAA93FE5A-.png 600w, https://skerritt.blog/content/images/2025/12/-DA9D423A-4CB0-4347-B79E-479AAA93FE5A-.png 976w" sizes="(min-width: 720px) 720px"></figure><p>Kiku also has a settings page and a plugins system to really customise it for yourself.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-ADBE1EAB-0073-4974-B9E7-2DCC6BD56199-.png" alt="" loading="lazy" width="938" height="748" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-ADBE1EAB-0073-4974-B9E7-2DCC6BD56199-.png 600w, https://skerritt.blog/content/images/2025/12/-ADBE1EAB-0073-4974-B9E7-2DCC6BD56199-.png 938w" sizes="(min-width: 720px) 720px"></figure><p>Here's a bullet pointed list of my favourite features:</p><ul><li>Fade out the front of the card after 3 seconds, encouraging you to answer faster.</li><li>Blur images which are tagged NSFW</li><li>Only blur them between 9 - 5pm workdays... In case you want to see said images when you're at home :) </li><li>Display extra fields, such as SentenceTranslation.</li><li>Randomise the font, so you learn the word in any font not just the main one you use.</li><li>Add external links to your cards to easily see the card in Jisho, Nadeshiko etc.</li><li>Hover over Kanji in your cards and see it broken down.</li></ul><h2 id="lapis">Lapis</h2><figure><a href="https://github.com/donkuri/lapis?ref=skerritt.blog"><div><p>GitHub - donkuri/lapis: Lapis is a modern Anki note type designed with compatibility in mind.</p><p>Lapis is a modern Anki note type designed with compatibility in mind. - donkuri/lapis</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-23.svg" alt=""><span>GitHub</span><span>donkuri</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/lapis" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Lapis is made by the same person who made Kaishi.</p><p>It's very similar to Kiku but without all the fancy features (Kiku is based on Lapis).</p><p>If you want a less Javascript heavy card, this is great!</p><figure><img src="https://skerritt.blog/content/images/2025/12/Lapis.gif" alt="" loading="lazy" width="600" height="311" srcset="https://skerritt.blog/content/images/2025/12/Lapis.gif 600w"></figure><h2 id="best-anki-addons">Best Anki Addons</h2><p>Now you use Anki, another common question people have is:</p><blockquote>What Anki addons can I use to maximise it?</blockquote><figure><img src="https://media.tenor.com/2EDMMbpXXrsAAAAC/chuu-kim-jiwoo.gif" alt="" loading="lazy" width="498" height="498"><figcaption><span>i got Chuu!</span></figcaption></figure><h2 id="%F0%9F%8F%86priority-reorder">🏆Priority reorder</h2><p>When you make Anki cards, they kinda go into a semi random order.</p><p>Not every word in Japanese is equally important.</p><p>Migaku, who ran an analysis on Netflix found these statistics.</p><p>If you select a word at random, there is a 10% chance that word is one of these three:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-B06D0AD7-A2CE-4EFA-A9C4-8FA443249368-.png" alt="" loading="lazy" width="722" height="314" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-B06D0AD7-A2CE-4EFA-A9C4-8FA443249368-.png 600w, https://skerritt.blog/content/images/2025/12/-B06D0AD7-A2CE-4EFA-A9C4-8FA443249368-.png 722w" sizes="(min-width: 720px) 720px"></figure><p>50% chance it will be one of 45 words:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-A1478711-1FA5-477B-81B5-530A9A9FFB19-.png" alt="" loading="lazy" width="707" height="283" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-A1478711-1FA5-477B-81B5-530A9A9FFB19-.png 600w, https://skerritt.blog/content/images/2025/12/-A1478711-1FA5-477B-81B5-530A9A9FFB19-.png 707w"></figure><p>Words are repeated, often. Just learning the top 1500 words or so means you can understand 80% of all words in a show.</p><p>Therefore it makes sense to learn your Anki cards in the order of most frequent first.</p><p>Priority Reorder does this.</p><figure><a href="https://skerritt.blog/priority-reorder-anki-addon/"><div><p>Priority Reorder Anki Addon</p><p>I use this Anki Addon to reorder my Japanese cards GitHub - tomahtoes/priority-reorderContribute to tomahtoes/priority-reorder development by creating an account on GitHub.GitHubtomahtoes Specifically I want to reorder them based on 2 things: 1. Cards I recently mined, as they are still fresh in my memory. 2. Cards</p><p><img src="https://skerritt.blog/content/images/icon/favicon-20.ico" alt=""><span>Skerritt.blog</span><span>Autumn Skerritt</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/photo-1761839257961-4dce65b72d99" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>But not all media is equal. One Piece has a lot of pirate talk, but you won't find that in other media.</p><p>Wouldn't it be cool to learn the most frequent words in One Piece if your goal is to watch it?</p><p>Priority reorder does that.</p><p>Finally, you have a short term memory. Flashcards you made today will stick better than flashcards made 50 days ago.</p><p>Wouldn't it be cool to also prioritise recently made flashcards that appear frequently in One Piece?</p><figure><img src="https://skerritt.blog/content/images/2025/12/-9C6C5643-DF1E-468C-9226-E574222AE564-.png" alt="" loading="lazy" width="1024" height="542" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-9C6C5643-DF1E-468C-9226-E574222AE564-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-9C6C5643-DF1E-468C-9226-E574222AE564-.png 1000w, https://skerritt.blog/content/images/2025/12/-9C6C5643-DF1E-468C-9226-E574222AE564-.png 1024w" sizes="(min-width: 720px) 720px"></figure><p>Priority Reorder does this!</p><p>Wouldn't it be cool to mine words that have a high frequency?</p><h2 id="kanji-grid">Kanji Grid</h2><p>Looking to take the JLPT or similar and wondering "god, do I really know all the kanji in that exam?"</p><p>Or wanting to just see how you progress in terms of Kanji?</p><p>The Kanji Grid addon is for you!</p><p><a href="https://ankiweb.net/shared/info/1610304449?ref=skerritt.blog">https://ankiweb.net/shared/info/1610304449</a></p><figure><img src="https://skerritt.blog/content/images/2025/12/---------------------_2025_12_03_03_42_03.png" alt="" loading="lazy" width="1000" height="940" srcset="https://skerritt.blog/content/images/size/w600/2025/12/---------------------_2025_12_03_03_42_03.png 600w, https://skerritt.blog/content/images/2025/12/---------------------_2025_12_03_03_42_03.png 1000w" sizes="(min-width: 720px) 720px"><figcaption><span>My grid</span></figcaption></figure><h2 id="local-audio-server">Local Audio Server</h2><p>This is an addon that works with Yomitan or similar tools.</p><p>It lets you listen to native audio in Yomitan, and even add that to your Anki cards.</p><figure><a href="https://github.com/yomidevs/local-audio-yomichan?ref=skerritt.blog"><div><p>GitHub - yomidevs/local-audio-yomichan: Anki add-on to run a local audio server for Yomichan.</p><p>Anki add-on to run a local audio server for Yomichan. - yomidevs/local-audio-yomichan</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-24.svg" alt=""><span>GitHub</span><span>yomidevs</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/local-audio-yomichan" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It takes a bit to set up, but once you do you don't have to mess with it. You can now have native audio on all of your Anki cards!</p><h2 id="best-paid-solution">Best paid solution</h2><blockquote>This is all too much setup! I wish there was some sort of company I could pay to do this all for me</blockquote><p>Not to worry, there is!</p><h2 id="%F0%9F%8F%86migaku">🏆Migaku</h2><p>Migaku is an all-in-one solution.</p><figure><a href="https://migaku.com/?ref=skerritt.blog"><div><p>Migaku - The fastest way to really learn a language</p><p>The fastest way to learn a foreign language is by reading and watching your favorite content on YouTube, Netflix, Disney+, Viki, X, Reddit and more.</p><p><img src="https://skerritt.blog/content/images/icon/apple-touch-icon-2.png" alt=""><span>The fastest way to really learn a language</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/og-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>They aim to do everything mentioned here already, albeit imperfectly and for a price.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-3.png" alt="" loading="lazy" width="675" height="499" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-3.png 600w, https://skerritt.blog/content/images/2025/12/image-3.png 675w"></figure><p>They have courses which teach you the top 1500 words, Kanji and grammar designed to help you immerse as soon as possible similar to Kaishi.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-BB17B007-579D-438B-A7CB-6337CEDCBA22-.png" alt="" loading="lazy" width="726" height="939" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-BB17B007-579D-438B-A7CB-6337CEDCBA22-.png 600w, https://skerritt.blog/content/images/2025/12/-BB17B007-579D-438B-A7CB-6337CEDCBA22-.png 726w" sizes="(min-width: 720px) 720px"></figure><p>They have their own SRS alternative to Anki, so you don't need addons etc to make anything work.</p><p>You can watch Netflix and look up all the words you want. They'll even highlight good words you should make flashcards out of.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-5.png" alt="" loading="lazy" width="2000" height="1094" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-5.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-5.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/image-5.png 1600w, https://skerritt.blog/content/images/size/w2400/2025/12/image-5.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>They can tell you how much of a specific video you know in terms of words, what is your expected comprehension of it:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-18E4E2DF-0202-437B-828B-C0CF6189CE32-.png" alt="" loading="lazy" width="459" height="602"></figure><p>You can:</p><ul><li>Read books</li><li>Watch videos locally</li><li>Study Netflix / YouTube videos</li><li>Generate subtitles if none exist </li></ul><p>If you are looking for an alright solution to learning Japanese and you don't mind spending money, in my opinion this is it.</p><div><p>🤖</p><p>PS: The Migaku Android app is a really great app. You can immerse from your phone as if you were on your PC.</p></div><p>For me personally, messing with tools is one of my little joys so I don't mind it. </p><h2 id="best-for-games">Best for Games:</h2><p>Japanese games are the greatest, let's look at options to learn Japanese from them.</p><p>The only real option is to use OCR, which is a fancy word to mean "the computer will read the text on the screen and give you the sentence so you can copy it / look it up".</p><h2 id="%F0%9F%8F%86game-sentence-minerwinner-of-best-for-games">🏆Game Sentence Miner - Winner of Best For Games</h2><p>After winning overall earlier, it does make sense that <strong>game</strong> sentence miner is the best for games.</p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/sVL9omRbGc4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" title="GameSentenceMiner - Extended Install Walkthrough"></iframe></figure><p>Once you setup OCR, you can then setup the overlay to be able to look words up directly in the game.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-7.png" alt="" loading="lazy" width="1222" height="794" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-7.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-7.png 1000w, https://skerritt.blog/content/images/2025/12/image-7.png 1222w" sizes="(min-width: 720px) 720px"></figure><p>It takes around 1 second to go from "text appearing on screen" to "being able to look up the text".</p><p>If you have a GPU it could be even less time, around 0.5 seconds or so.</p><div><p>🔥</p><div><p>GSM OCR is powered by a fork of OwOcr, which is a really really good OCR program. Auora, the dev who made it, is cracked and writes great software. </p><p><a href="https://github.com/AuroraWright/owocr?ref=skerritt.blog">https://github.com/AuroraWright/owocr</a></p></div></div><p>You can then click the plus icon to make a flashcard, and GSM will make it all in the background. You don't have to constantly switch between enjoying a game and making flashcards.</p><h2 id="meikipop">Meikipop</h2><figure><a href="https://github.com/rtr46/meikipop?ref=skerritt.blog"><div><p>GitHub - rtr46/meikipop: meikipop - universal japanese ocr popup dictionary for windows, linux and macos</p><p>meikipop - universal japanese ocr popup dictionary for windows, linux and macos - rtr46/meikipop</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-25.svg" alt=""><span>GitHub</span><span>rtr46</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/meikipop" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>This is a really fast OCR that works anywhere on Windows, Linux or Mac.</p><p>It's super simple to setup and use, and it works similar to GSM's "hover over the word to see the meaning"</p><figure><img src="https://skerritt.blog/content/images/2025/12/-77D1FA66-13AA-4E49-BA67-0C266237B155-.png" alt="" loading="lazy" width="1348" height="839" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-77D1FA66-13AA-4E49-BA67-0C266237B155-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-77D1FA66-13AA-4E49-BA67-0C266237B155-.png 1000w, https://skerritt.blog/content/images/2025/12/-77D1FA66-13AA-4E49-BA67-0C266237B155-.png 1348w" sizes="(min-width: 720px) 720px"></figure><p>The only downside is that you can't mine to Anki with it, however it is extremely simple to use, fast, and works on anything on your screen (even Windows settings) so for that reason it's winning second place.</p><h3 id="yomininja">Yomininja</h3><p>Yomininja is another tool similar to GSM.</p><p>It uses OCR to scan the screen and lets you look things up:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-7C99E26D-730B-4F99-A451-ACFCD60ADC34-.png" alt="" loading="lazy" width="1868" height="961" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-7C99E26D-730B-4F99-A451-ACFCD60ADC34-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-7C99E26D-730B-4F99-A451-ACFCD60ADC34-.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/-7C99E26D-730B-4F99-A451-ACFCD60ADC34-.png 1600w, https://skerritt.blog/content/images/2025/12/-7C99E26D-730B-4F99-A451-ACFCD60ADC34-.png 1868w" sizes="(min-width: 720px) 720px"></figure><p>It's a lot simpler than GSM, but in my opinion it's not as pretty. </p><p>GSM doesn't highlight boxes red by default, and you can hover over the words and see the definition above them as you read it.</p><p>With Yomininja there's this extra box on the side you have to read.</p><p>Not to mention the fact that GSM lets you easily make flashcards with the audio and a gif from the game itself.</p><p>Still, Yomininja is extremely easy to use and a fan favourite.</p><h2 id="best-for-visual-novels">Best for visual novels</h2><h2 id="%F0%9F%8F%86gsmbest-for-visual-novels">🏆GSM - Best for Visual Novels</h2><p>GSM is really, really good for visual media on a computer.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-9.png" alt="" loading="lazy" width="1860" height="954" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-9.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-9.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/image-9.png 1600w, https://skerritt.blog/content/images/2025/12/image-9.png 1860w" sizes="(min-width: 720px) 720px"></figure><p>But when it comes to visual novels, we can use texthookers.</p><div><p>🤔</p><div><p>A texthooker is a program that "hooks" into your game when it is running. Everytime you see text on the screen, this text appears somewhere in your computer. </p><p>A texthooker grabs this text and gives it to you, letting you look things up without OCR.</p><p>I'll talk more about this next!</p></div></div><p>Texthookers work with the overlay just like OCR does with games. </p><p>Let's explore an under-rated feature in GSM, as our next tool will have this too – stats.</p><p>GSM has over 35 charts related to statistics about everything you read, designed to help you answer questions such as:</p><ul><li>Do I read better in the morning or evening?</li><li>Do I read faster reading horror or slice of life?</li><li>Do I play more games or visual novels? Which one is better for me in terms of learning?</li><li>Am I improving?</li></ul><figure><div><div><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_0mnh7ScReH.png" width="1492" height="843" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_0mnh7ScReH.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_0mnh7ScReH.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_0mnh7ScReH.png 1492w" sizes="(min-width: 720px) 720px"></p><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_kMLqMEKPvy.png" width="1495" height="831" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_kMLqMEKPvy.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_kMLqMEKPvy.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_kMLqMEKPvy.png 1495w" sizes="(min-width: 720px) 720px"></p><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_PewOVmVCeH.png" width="1208" height="826" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_PewOVmVCeH.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_PewOVmVCeH.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_PewOVmVCeH.png 1208w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_QtCzx4grXP.png" width="1489" height="792" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_QtCzx4grXP.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_QtCzx4grXP.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_QtCzx4grXP.png 1489w" sizes="(min-width: 720px) 720px"></p><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_v18833bbIh.png" width="1486" height="822" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_v18833bbIh.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_v18833bbIh.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_v18833bbIh.png 1486w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_WgyEMlYSEN.png" width="1499" height="806" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_WgyEMlYSEN.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_WgyEMlYSEN.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_WgyEMlYSEN.png 1499w" sizes="(min-width: 720px) 720px"></p><p><img src="https://skerritt.blog/content/images/2025/12/GameSentenceMiner_XEhc3bdkRd.png" width="1501" height="760" loading="lazy" alt="" srcset="https://skerritt.blog/content/images/size/w600/2025/12/GameSentenceMiner_XEhc3bdkRd.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/GameSentenceMiner_XEhc3bdkRd.png 1000w, https://skerritt.blog/content/images/2025/12/GameSentenceMiner_XEhc3bdkRd.png 1501w" sizes="(min-width: 720px) 720px"></p></div></div></figure><h2 id="jl">JL</h2><p>JL is an alternative Japanese dictionary program.</p><p>It works really, really well for visual novels.</p><p>I wrote extensively about it here:</p><figure><a href="https://skerritt.blog/jl-vs-yomitan-for-japanese-learning-gsm/"><div><p>JL vs Yomitan for Japanese Learning</p><p>JL is a popular alternative desktop dictionary app, Yomitan is a famous browser based dictionary app. Which one should you use?</p><p><img src="https://skerritt.blog/content/images/icon/favicon-21.ico" alt=""><span>Skerritt.blog</span><span>Autumn Skerritt</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/photo-1762838896748-12aba8c2a7b8" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>But in short:</p><ul><li>It's really fast</li><li>It has amazing features specifically for Japanese</li><li>It requires a texthooker and is Windows only :( </li></ul><figure><img src="https://skerritt.blog/content/images/2025/12/5555-1-1.gif" alt="" loading="lazy" width="1920" height="1080" srcset="https://skerritt.blog/content/images/size/w600/2025/12/5555-1-1.gif 600w, https://skerritt.blog/content/images/size/w1000/2025/12/5555-1-1.gif 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/5555-1-1.gif 1600w, https://skerritt.blog/content/images/2025/12/5555-1-1.gif 1920w" sizes="(min-width: 720px) 720px"></figure><p>I like how you can put the textbox over the visual novel, which lets you do something similar to GSM's overlay.</p><p>JL also has some stats:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-F422B2DC-D901-44F7-929E-B3A8DF7E3F5B-.png" alt="" loading="lazy" width="236" height="475"></figure><p>And even more excitedly they have stats on how many times you looked up a word, something that no other dictionary app has.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-B5558A67-6F91-4548-9F9D-8AECA90268D9-.png" alt="" loading="lazy" width="380" height="444"><figcaption><span>I hover my mouse over all words as I read along, so technically I look up every word 😓</span></figcaption></figure><div><p>💡</p><p>Note: JL is the only dictionary app to show you how many times you have looked up a word.</p></div><h2 id="best-texthookers">Best texthookers</h2><p>Texthookers are lil programs that "hook" into visual novels or some games, take the text on your screen and give it to you.</p><p>They are faster and more accurate than OCR, but sometimes awkward to use.</p><h2 id="%F0%9F%8F%86lunatranslator">🏆LunaTranslator</h2><figure><a href="https://docs.lunatranslator.org/en/?ref=skerritt.blog"><div><p>LunaTranslator</p><p>A VitePress site</p><p><img src="https://skerritt.blog/content/images/icon/luna.ico" alt=""><span>Software Download &amp; FAQ</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/luna.ico" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>By far the best texthooker out there.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-87F320FA-67F8-46F4-8617-1A20019D0D39-.png" alt="" loading="lazy" width="578" height="65"></figure><p>It works on everything I try in terms of visual novels.</p><p>It can hook emulated devices such as PSP, PS2, and the Nintendo Switch.</p><p>If you find a "bad" hook (one that has a lot of junk) there's a million things you can do to make it more normal.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-1B7D53BB-0C16-4E31-B77E-CCB6758FA673-.png" alt="" loading="lazy" width="975" height="703" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-1B7D53BB-0C16-4E31-B77E-CCB6758FA673-.png 600w, https://skerritt.blog/content/images/2025/12/-1B7D53BB-0C16-4E31-B77E-CCB6758FA673-.png 975w" sizes="(min-width: 720px) 720px"></figure><p>Like filtering out curly braces, filtering out non Japanese text etc.</p><p>If this doesn't help you, you can even write a Python file to preprocess the text!</p><figure><img src="https://skerritt.blog/content/images/2025/12/-A04CFB2F-F549-4812-8C01-1601B0B1DCAB-.png" alt="" loading="lazy" width="734" height="118" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-A04CFB2F-F549-4812-8C01-1601B0B1DCAB-.png 600w, https://skerritt.blog/content/images/2025/12/-A04CFB2F-F549-4812-8C01-1601B0B1DCAB-.png 734w" sizes="(min-width: 720px) 720px"></figure><p>On top of this, Luna supports much more than just hooking.</p><ul><li>OCR</li><li>Speech Recognition (it listens to the sound the game is making, and tries to turn those sounds into sentences)</li><li>You can install Yomitan and use it similarly to JL (it wont overlay like GSM does though)</li></ul><figure><img src="https://skerritt.blog/content/images/2025/12/image-10.png" alt="" loading="lazy" width="1180" height="568" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-10.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-10.png 1000w, https://skerritt.blog/content/images/2025/12/image-10.png 1180w" sizes="(min-width: 720px) 720px"></figure><p>But there are some rumours that the author has stolen the code of other people and rebranded it as their own (I have not found evidence of this, please let me know in the comments if you have proof).</p><div><p>🐉</p><div><p><b><strong>Update</strong></b>: Some developers reached out to me. They definitely took code and didn't credit them, but there are some nuances.</p><p>1. The license is MIT. Legally you're allowed to take the code and not credit. But, morally it's right to credit.<br>2. The work is transformative, they took code in one language and rewrote it to work with C++ for Luna.<br>3. Agent (talked about next) also has its own drama. It's not open source, and it encrypts the Nintendo Switch hooks to prevent competition from getting them.</p><p>All in all, it's really confusing and messy. Decide for yourself.</p></div></div><p>Also, when writing code people often tell you what has changed since the last release.</p><p>The Luna author does not do this, it's kind of confusing to figure out what's been added or removed.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-686AB389-3852-4691-84C1-58C9821DD3EE-.png" alt="" loading="lazy" width="436" height="691"></figure><p>This led to people not trusting them.</p><p>On top of this, it supports <strong>a lot</strong>. Like translation, yomitan, Japanese parsing etc.</p><p>This led to someone forking the code and creating their own version, removing all of this and keeping just the hooking part.</p><figure><a href="https://github.com/AuroraWright/LunaTranslator?ref=skerritt.blog"><div><p>GitHub - AuroraWright/LunaTranslator: Galgame翻译器，支持HOOK、OCR、剪贴板等。Visual Novel Translator , support HOOK / OCR / clipboard</p><p>Galgame翻译器，支持HOOK、OCR、剪贴板等。Visual Novel Translator , support HOOK / OCR / clipboard - AuroraWright/LunaTranslator</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-27.svg" alt=""><span>GitHub</span><span>AuroraWright</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/LunaTranslator" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Auora is a cracked dev, if you spend any time in Japanese Tool GitHub™️you will see them</span></p></figcaption></figure><h2 id="agent">Agent</h2><p>Agent is a much simpler texthooking program.</p><figure><a href="https://github.com/0xDC00/agent?ref=skerritt.blog"><div><p>GitHub - 0xDC00/agent: Universal script based text hooker (powered by FRIDA).</p><p>Universal script based text hooker (powered by FRIDA). - 0xDC00/agent</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-28.svg" alt=""><span>GitHub</span><span>0xDC00</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/agent" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It has a database of hooks, and you click on the game or visual novel you want to play.</p><p>You then have a perfect hook, it's not dirty and works first time without much issue.</p><p>The problem is that it doesn't have hooks for all games and visual novels, yet.</p><p>Chen's textractor works similarly to Luna and Agent, closer to Luna in the sense that you find the hook yourself.</p><figure><a href="https://git.chenx221.cyou/chenx221/textractor?ref=skerritt.blog"><div><p>Chen x221 / Textractor · GitLab</p><p>Extracts text from video games and visual novels. Highly extensible.</p><p><img src="https://skerritt.blog/content/images/icon/favicon-72a2cad5025aa931d6ea56c3201d1f18e68a8cd39788c7c80d5b2b82aa5143ef.png" alt=""><span>GitLab</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/twitter_card-570ddb06edf56a2312253c5872489847a0f385112ddbcd71ccfa1570febab5d2.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>It's most similar to Textractor (it's a fork) which is an older texthooking program, so many people love this as its similar to what they already use and love.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-23.png" alt="" loading="lazy" width="664" height="256" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-23.png 600w, https://skerritt.blog/content/images/2025/12/image-23.png 664w"></figure><h2 id="texthooking-pages">Texthooking Pages</h2><p>Let's say you want to use Yomitan to mine from games / visual novels without GSM / JL or Yomininja etc.</p><p>Yomitan is browser only.</p><p>You need to take the text from OCR / Texthooking and place it onto a webpage to look up words.</p><p>There's some opinions about these, so let's list the most popular ones.</p><h2 id="%F0%9F%8F%86kizuna">🏆Kizuna</h2><p>Kizuna is a texthooking paged based on another one by Renji.</p><figure><a href="https://kizuna-texthooker-ui.fly.dev/?ref=skerritt.blog"><div><p>Home • Kizuna</p><p>Track Visual Novel immersion with your friends</p><p><img src="https://skerritt.blog/content/images/icon/favicon.svg" alt=""></p></div></a></figure><figure><img src="https://skerritt.blog/content/images/2025/12/-1D17EF41-F8BF-44BC-8A80-014E08401935--1.png" alt="" loading="lazy" width="739" height="318" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-1D17EF41-F8BF-44BC-8A80-014E08401935--1.png 600w, https://skerritt.blog/content/images/2025/12/-1D17EF41-F8BF-44BC-8A80-014E08401935--1.png 739w" sizes="(min-width: 720px) 720px"></figure><p>Everytime your texthooker receives a line of text, it sends it to this page.</p><p>Here you can use Yomitan to look up words.</p><p>Kizuna is special because it's a <em>social</em> texthooking page.</p><p>It records characters read and time spent per visual novel:</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-11.png" alt="" loading="lazy" width="1297" height="917" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-11.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-11.png 1000w, https://skerritt.blog/content/images/2025/12/image-11.png 1297w" sizes="(min-width: 720px) 720px"></figure><p>And you can create "rooms" with your friends to compare your stats together and motivate each other:</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-12.png" alt="" loading="lazy" width="1297" height="917" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-12.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-12.png 1000w, https://skerritt.blog/content/images/2025/12/image-12.png 1297w" sizes="(min-width: 720px) 720px"></figure><p>It's three main downsides are that it is focused primarily on Japanese visual novels, it's not open source and it doesn't let you export your data.</p><div><p>🤓</p><p>Actually you can export your data! You need to log a "reading session" and then you can export your data in a CSV 💪</p></div><h2 id="renjis-texthooking-page">Renji's Texthooking Page</h2><figure><a href="https://github.com/Renji-XD/texthooker-ui?ref=skerritt.blog"><div><p>GitHub - Renji-XD/texthooker-ui: A web interface for Textractor with websocket plugins</p><p>A web interface for Textractor with websocket plugins - Renji-XD/texthooker-ui</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-30.svg" alt=""><span>GitHub</span><span>Renji-XD</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/texthooker-ui" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Renji's page inspired Kizuna's.</p><p>It's a simple texthooking page that can be downloaded and ran entirely locally, with its source code published online.</p><p>It looks pretty much exactly the same:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-FCBDA706-CA75-4F64-8E2E-25F0DCF027D3-.png" alt="" loading="lazy" width="896" height="202" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-FCBDA706-CA75-4F64-8E2E-25F0DCF027D3-.png 600w, https://skerritt.blog/content/images/2025/12/-FCBDA706-CA75-4F64-8E2E-25F0DCF027D3-.png 896w" sizes="(min-width: 720px) 720px"></figure><p>Many people use Renji's because it's lightweight and has many settings to allow you to configure things.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-58C81A06-553F-4BC2-BD71-603109F3F0DC-.png" alt="" loading="lazy" width="766" height="846" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-58C81A06-553F-4BC2-BD71-603109F3F0DC-.png 600w, https://skerritt.blog/content/images/2025/12/-58C81A06-553F-4BC2-BD71-603109F3F0DC-.png 766w" sizes="(min-width: 720px) 720px"></figure><p>Maybe too many settings.</p><p>Because Renji's texthooker is open source it is often bundled into other software like Game Sentence Miner which has added a few specific features.</p><p>While Kizuna has stats, Renji's does not other than characters read and time spent.</p><p>The author, Renji, actually suggests people use GameSentenceMiner for stats with their texthooker:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-F59C3E04-27D6-4D48-A66E-ACFDD2DDFCFB-.png" alt="" loading="lazy" width="960" height="790" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-F59C3E04-27D6-4D48-A66E-ACFDD2DDFCFB-.png 600w, https://skerritt.blog/content/images/2025/12/-F59C3E04-27D6-4D48-A66E-ACFDD2DDFCFB-.png 960w" sizes="(min-width: 720px) 720px"></figure><h2 id="exstatic">ExStatic</h2><figure><a href="https://github.com/KamWithK/exSTATic?ref=skerritt.blog"><div><p>GitHub - KamWithK/exSTATic: Zero effort language learning reading tracker with graphs and stats</p><p>Zero effort language learning reading tracker with graphs and stats - KamWithK/exSTATic</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-29.svg" alt=""><span>GitHub</span><span>KamWithK</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/f0e4819d-f548-4654-a44d-7fdb391d1fa6" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>You may have noticed in the previous paragraph someone mentioned <em>ExStatic</em>.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-13.png" alt="" loading="lazy" width="1366" height="667" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-13.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-13.png 1000w, https://skerritt.blog/content/images/2025/12/image-13.png 1366w" sizes="(min-width: 720px) 720px"></figure><p>This is another texthooking page but with a lot more stats.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-32DB9010-0DFA-4683-A4B2-B0C3F152DF29-.png" alt="" loading="lazy" width="767" height="798" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-32DB9010-0DFA-4683-A4B2-B0C3F152DF29-.png 600w, https://skerritt.blog/content/images/2025/12/-32DB9010-0DFA-4683-A4B2-B0C3F152DF29-.png 767w" sizes="(min-width: 720px) 720px"></figure><p>But in terms of functionality of the page itself, it's lacking somethings that Renji's has.</p><p>Still, many people use ExStatic as an easy way to get beautiful stats. It is open source and can be run entirely locally, without an internet connection.</p><div><p>😋</p><div><p>ExStatic stores the raw lines of what you read. This is useful if you want to calculate your own stats or see the first time you encountered a kanji etc.</p><p>Other stat apps form opinions, like what is Japanese? Do English letters count? Like does the T in "Ｔシャツ" count?</p><p>By giving you raw data, you can decide for yourself what counts and change the stats at any time.</p><p>Only ExStatic, Renji's and GSM support storing raw lines of data.</p></div></div><h2 id="best-for-manga">best for manga</h2><h2 id="%F0%9F%8F%86mokuro-mokuro-readerjoint-1st">🏆Mokuro + Mokuro Reader - Joint 1st</h2><p>Mokuro is a file format for manga. It's basically a HTML overlay over a bunch of Manga images that let you look up the words in that manga panel using Yomitan.</p><p>It does this by OCRing the manga to generate this.</p><p>Mokuro Reader is the app that lets you read these files:</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-14.png" alt="" loading="lazy" width="1011" height="789" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-14.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-14.png 1000w, https://skerritt.blog/content/images/2025/12/image-14.png 1011w" sizes="(min-width: 720px) 720px"></figure><p>The main problem with this is finding manga.</p><p>You have to find a way to buy the manga, get the raw images, and process it. There are less than legal ways to do this, but I won't talk about that here.</p><p>You can store all of your manga in the cloud along with progress etc and easily read from any device (you can read from any device, and use Yomitan on Android if you want to mine):</p><figure><img src="https://skerritt.blog/content/images/2025/12/-8ED39592-9D0E-4527-AD5C-1B37ABC0CAA0-.png" alt="" loading="lazy" width="679" height="520" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-8ED39592-9D0E-4527-AD5C-1B37ABC0CAA0-.png 600w, https://skerritt.blog/content/images/2025/12/-8ED39592-9D0E-4527-AD5C-1B37ABC0CAA0-.png 679w"></figure><p>When you read manga, since it is in the browser, you can use Yomitan to look things up:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-C65BBC7A-946C-4CCB-8245-B73E02ACADFC-.png" alt="" loading="lazy" width="764" height="437" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-C65BBC7A-946C-4CCB-8245-B73E02ACADFC-.png 600w, https://skerritt.blog/content/images/2025/12/-C65BBC7A-946C-4CCB-8245-B73E02ACADFC-.png 764w" sizes="(min-width: 720px) 720px"></figure><p>For reading it has some cool features, like a night mode to block out blue light to make night reading easier:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4F8AE90A-1635-4800-9A6D-A2152A278026-.png" alt="" loading="lazy" width="449" height="463"></figure><p>And a million other minor settings to alter how you read.</p><p>Of course it also has Anki support. Make a word card using Yomitan and then</p><figure><img src="https://skerritt.blog/content/images/2025/12/-1C0FD3B0-9B18-4462-B7DB-2CF6600C64D2-.png" alt="" loading="lazy" width="457" height="740"></figure><p>Double click the screen to grab an image.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-B3CD2CB7-E531-4360-8DBC-8E577CD2546F-.png" alt="" loading="lazy" width="819" height="825" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-B3CD2CB7-E531-4360-8DBC-8E577CD2546F-.png 600w, https://skerritt.blog/content/images/2025/12/-B3CD2CB7-E531-4360-8DBC-8E577CD2546F-.png 819w" sizes="(min-width: 720px) 720px"></figure><p>There's even stats!</p><p>In the manga itself you can see how long it'd take to read your current volume:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-38850B58-7888-45B1-8D90-8B064D4D0617-.png" alt="" loading="lazy" width="235" height="64"></figure><p>The stats are visually appealing</p><figure><img src="https://skerritt.blog/content/images/2025/12/-F1A240D0-AF6F-489B-B243-FC22D5D185C7-.png" alt="" loading="lazy" width="1882" height="398" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-F1A240D0-AF6F-489B-B243-FC22D5D185C7-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-F1A240D0-AF6F-489B-B243-FC22D5D185C7-.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/-F1A240D0-AF6F-489B-B243-FC22D5D185C7-.png 1600w, https://skerritt.blog/content/images/2025/12/-F1A240D0-AF6F-489B-B243-FC22D5D185C7-.png 1882w" sizes="(min-width: 720px) 720px"></figure><p>With cool per volume / series data:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-8D0EB30D-53F1-4236-8996-F8CEEE295172-.png" alt="" loading="lazy" width="1842" height="516" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-8D0EB30D-53F1-4236-8996-F8CEEE295172-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-8D0EB30D-53F1-4236-8996-F8CEEE295172-.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/-8D0EB30D-53F1-4236-8996-F8CEEE295172-.png 1600w, https://skerritt.blog/content/images/2025/12/-8D0EB30D-53F1-4236-8996-F8CEEE295172-.png 1842w" sizes="(min-width: 720px) 720px"></figure><p>My main complaints are:</p><ul><li>The stats are precomputed, it does not store your raw data. This means if you change your mind about something (like I don't want to count English letters, I don't want to count repeated things like aaaaaa, or I want to set my AFK timer to be lower) it's impossible to change.</li><li>You have to use Mokuro'd manga, which is very hard to find and painful to convert if you don't have a powerful computer.</li></ul><h2 id="%F0%9F%8F%86mangatanjoint-1st">🏆MangaTan - Joint 1st</h2><figure><a href="https://github.com/kaihouguide/Mangatan?tab=readme-ov-file&amp;ref=skerritt.blog"><div><p>GitHub - kaihouguide/Mangatan: Yomitan On manga Sites no pre-processing no self-processing either! (using Suwayomi)</p><p>Yomitan On manga Sites no pre-processing no self-processing either! (using Suwayomi) - kaihouguide/Mangatan</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-31.svg" alt=""><span>GitHub</span><span>kaihouguide</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/Mangatan" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>MangaTan was born out of anger.</p><p>Anger at Mokuro files being terrible to create.</p><p>Anger at current manga websites shoving 10 ads / second down your throat.</p><p>Hear it from the creator:</p><blockquote>We can just drag and drop light novels into ttsu, hook into visual novels with ease, or load up anime with subtitles in ASB or Memento. <p>But manga? Manga has always been the exception. </p><p>The process often meant needing a high-end GPU just to run tools like Mokuro, and then waiting for it to finish </p><p>And don't get me started with those terrible websites that use the worst hosts in existence. </p><p>Who doesn't miss the old days of simply reading manga in bed? Before we were hardcore weebs, we didn't have to deal with our tenth ad on a limited 100kbit-speed-limited hoster while using Tachiyomi. But you can cast aside that </p></blockquote><div><p>👾</p><div><p>The reader for Mokuro is super cool and I love it. Mokuro files themselves..... They bring me despair.</p><p>If it was Mokuro (the converter) vs Mangatan, Mangatan would win. I once spent 27 hours converting a single volume of manga into Mokuro.... and the OCR sucked. A lot.</p></div></div><p>Mangatan aims to get rid of all the painpoints of Mokuro:</p><ul><li>You don't have to spend 20+ hours converting manga to mokuro files</li><li>Or searching for Mokuro files across the web</li><li>It works with over 1200+ sites, the second a new volume of manga is released you can read it 🥳</li></ul><p>The setup requires some computer skills but once its done, it works alright.</p><div><p>📲</p><div><p>They are working on adding a much easier to install <code spellcheck="false">.exe</code>, which would make Mangatan extremely easy to use.</p><p><b><strong>Update</strong></b>: After I joined their Discord thread and talked about this, there is now a one-click <code spellcheck="false">.exe</code> you can use!</p><p><b><strong>Mangatan is my favourite</strong></b>. I don't want to update this, as it happened after my imaginary award show. But future readers: <b><strong>use Mangatan</strong></b>.</p><p>Use this fork <a href="https://github.com/KolbyML/Mangatan?ref=skerritt.blog">https://github.com/KolbyML/Mangatan</a></p></div></div><figure><img src="https://skerritt.blog/content/images/2025/12/-8A8FA34E-0D7D-4AB0-BF20-F4F94B876662-.png" alt="" loading="lazy" width="1165" height="518" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-8A8FA34E-0D7D-4AB0-BF20-F4F94B876662-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-8A8FA34E-0D7D-4AB0-BF20-F4F94B876662-.png 1000w, https://skerritt.blog/content/images/2025/12/-8A8FA34E-0D7D-4AB0-BF20-F4F94B876662-.png 1165w" sizes="(min-width: 720px) 720px"></figure><p>Every time you load a page it will OCR it and let you look things up.</p><p>You can even crop images to send to Anki:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4A1949BE-90F0-49AE-96B4-4C5027F886DF-.png" alt="" loading="lazy" width="774" height="423" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-4A1949BE-90F0-49AE-96B4-4C5027F886DF-.png 600w, https://skerritt.blog/content/images/2025/12/-4A1949BE-90F0-49AE-96B4-4C5027F886DF-.png 774w" sizes="(min-width: 720px) 720px"></figure><p>It's very lightweight once its running. All it does is OCR the manga and crop images for anki cards.</p><p>From the author themselves:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-BBCFF5B1-1BC8-4DB4-8120-12672E485ADA-.png" alt="" loading="lazy" width="560" height="92"></figure><p>There's also a bunch of settings, for example you can use your own OCR server if you want:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-DBFE35CC-65DB-45CB-9688-613825036F60-.png" alt="" loading="lazy" width="738" height="439" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-DBFE35CC-65DB-45CB-9688-613825036F60-.png 600w, https://skerritt.blog/content/images/2025/12/-DBFE35CC-65DB-45CB-9688-613825036F60-.png 738w" sizes="(min-width: 720px) 720px"></figure><p>I like setting the colour to purple and font colour to black, as it makes it look nicer to me:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-9DDEC5FF-258C-4224-8788-CFD944761CCE-.png" alt="" loading="lazy" width="1217" height="697" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-9DDEC5FF-258C-4224-8788-CFD944761CCE-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-9DDEC5FF-258C-4224-8788-CFD944761CCE-.png 1000w, https://skerritt.blog/content/images/2025/12/-9DDEC5FF-258C-4224-8788-CFD944761CCE-.png 1217w" sizes="(min-width: 720px) 720px"></figure><p>There's no stats, but in this case it's a good thing. Mangatan is extremely lightweight.</p><p>It also works on Android devices.</p><h2 id="gsm-1">GSM</h2><p>You can also use GSM's OCR to read manga if you so wish.</p><p>Set up the OCR, and then open the overlay and you can create flashcards etc similar to how Mangatan does it.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-6.png" alt="" loading="lazy" width="526" height="743"></figure><p>GSM even has stats for manga:</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-15.png" alt="" loading="lazy" width="945" height="535" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-15.png 600w, https://skerritt.blog/content/images/2025/12/image-15.png 945w" sizes="(min-width: 720px) 720px"></figure><div><p>🐬</p><div><p>Note: GSM stores your raw data, the actual sentences you read and calculates stats from that. You can take this data and do anything you want to it, you're not limited.</p><p>Because GSM stores the raw data, you can also use it to calculate how many times you've seen a kanji, or search to see the first time you encountered a word or kanji. You can do anything you want.</p></div></div><p>But the downsides are that GSM isn't made for manga.</p><p>When it takes a screenshot, it can't crop the manga like Mokuro does (due to it being built for games / full screen things).</p><p>Look at this Anki card:</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-16.png" alt="" loading="lazy" width="907" height="847" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-16.png 600w, https://skerritt.blog/content/images/2025/12/image-16.png 907w" sizes="(min-width: 720px) 720px"></figure><p>The image has black bars around it, whereas if it was cropped it would not.</p><p>If you already have a good GSM setup, it makes sense to use this. But if you want a really good manga setup, maybe this isn't so good.</p><p>It's also not as automatic as Mangatan, if you want nice stats you need to tell GSM you're reading something new.</p><p>Mangatan can only use Suwayomi whereas GSM can OCR any type of manga. But, Suwayomi has all the manga already so it's not that much of an improvement.</p><p>Also, GSM does not work on Android unlike Mokuro Reader and Mangatan.</p><h2 id="best-video-players">Best Video Players</h2><p>Most people get into learning Japanese because of anime, so now let's talk about the best video players out there!</p><h2 id="%F0%9F%8F%86migaku-1">🏆Migaku</h2><p>Despite being a paid for product, I believe Migaku offers the best service for watching videos.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-17.png" alt="" loading="lazy" width="1915" height="904" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-17.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-17.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/image-17.png 1600w, https://skerritt.blog/content/images/2025/12/image-17.png 1915w" sizes="(min-width: 720px) 720px"></figure><p>Firstly let's look at this.</p><p>Migaku shows you an estimated comprehension score for the video you want to watch. It uses the frequency of the words and your known words to calculate this.</p><p>It's not as simple as "you know these words, you don't know these" – it uses an algorithm to work out the average frequency of words you do and don't know and uses that to calculate how hard a media is.</p><p>For example if the words you don't know are very high frequency, it will be a lower difficulty than a video with a bunch of words you don't know.</p><p>If the show doesn't have subtitles, you can also generate them using the top bar.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-EE32F9B9-F2F3-45E6-A8BC-12EE5E286C2B-.png" alt="" loading="lazy" width="228" height="116"></figure><p>Migaku has  about a million different presets for you to watch videos, or you can make your own:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-BCAF527E-D51B-4530-B342-D14C456B1408-.png" alt="" loading="lazy" width="1013" height="829" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-BCAF527E-D51B-4530-B342-D14C456B1408-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-BCAF527E-D51B-4530-B342-D14C456B1408-.png 1000w, https://skerritt.blog/content/images/2025/12/-BCAF527E-D51B-4530-B342-D14C456B1408-.png 1013w" sizes="(min-width: 720px) 720px"></figure><p>As well as keyboard shortcuts so you don't even have to use a mouse:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-B9A2A705-B022-48E3-B563-61A607F612BB-.png" alt="" loading="lazy" width="1469" height="829" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-B9A2A705-B022-48E3-B563-61A607F612BB-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-B9A2A705-B022-48E3-B563-61A607F612BB-.png 1000w, https://skerritt.blog/content/images/2025/12/-B9A2A705-B022-48E3-B563-61A607F612BB-.png 1469w" sizes="(min-width: 720px) 720px"></figure><p>If the UI is too cluttered you can just hide everything apart from this tab:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-6F604C3A-C1A8-4F28-819D-5EF3FE7762A0-.png" alt="" loading="lazy" width="1913" height="754" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-6F604C3A-C1A8-4F28-819D-5EF3FE7762A0-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-6F604C3A-C1A8-4F28-819D-5EF3FE7762A0-.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/-6F604C3A-C1A8-4F28-819D-5EF3FE7762A0-.png 1600w, https://skerritt.blog/content/images/2025/12/-6F604C3A-C1A8-4F28-819D-5EF3FE7762A0-.png 1913w" sizes="(min-width: 720px) 720px"></figure><p>Migaku will highlight words in good sentences to mine with high frequency, and you can even tell it to include a lil single definition under the word to help you read the sentence:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-7D220246-26E5-49A8-B526-ECC99F7876D4-.png" alt="" loading="lazy" width="724" height="209" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-7D220246-26E5-49A8-B526-ECC99F7876D4-.png 600w, https://skerritt.blog/content/images/2025/12/-7D220246-26E5-49A8-B526-ECC99F7876D4-.png 724w" sizes="(min-width: 720px) 720px"></figure><p>Migaku works on Netflix and Disney+, but all of these screenshots use their local player. This is just a DVD I'm playing 😃</p><p>Its main downside is that it costs money, but I believe Migaku is easily the best video player out there in terms of features and ease of use.</p><p>But... It does cost money and do you <strong>really</strong> need those extra features? It's up to you.</p><h2 id="asb-player">ASB Player</h2><p>ASB Player is the GOAT of free video players for Japanese.</p><p>It's offline.</p><p>It works with Netflix, YouTube etc.</p><p>It extracts subtitles from them.</p><p>You can mine from it just like Migaku.</p><figure><img src="https://skerritt.blog/content/images/2025/12/image-18.png" alt="" loading="lazy" width="1920" height="911" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-18.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-18.png 1000w, https://skerritt.blog/content/images/size/w1600/2025/12/image-18.png 1600w, https://skerritt.blog/content/images/2025/12/image-18.png 1920w" sizes="(min-width: 720px) 720px"></figure><p>There's really not much to it. It plays videos well, has great subtitle support and lets you mine from it.</p><p>Which is why it's so good and well loved, it does one thing and does it well.</p><p>However, compared to Migaku it is missing a few features some people may want:</p><ul><li>Ability to generate subtitles</li><li>Highlighting words you should mine</li></ul><div><p>🐬</p><p>ASB Player just added highlighting for words you don't have in Anki (not released yet, but it's in the codebase). It's not as good as Migaku's algorithmic-method but still it's amazing to see.</p></div><p>But if you don't care for those features, ASB Player is great.</p><div><p>🥸</p><div><p>Update: Someone told me they use the ASB Player Websocket to send subtitles to GameSentenceMiner, to get both stats and a gif of the scene from the anime.</p><p>While I haven't tried this, it sounds cool. I would love gifs of anime scenes on my Anki cards 😄</p><p><b><strong>Update 2</strong></b>: I tried this. The ASB Player websocket is for controlling the player, not for subtitles ☹️<br>but you can use MPV with <a href="https://github.com/kuroahna/mpv_websocket?ref=skerritt.blog">MPV Web Socket</a> (which does do subtitles) to get pretty gifs<br>However you need to download files locally and sync the subtitles which is a bit of a pain....</p></div></div><h2 id="yomine">Yomine</h2><p>Yomine is a relatively new player in the field, and not actually a video player but something that supports video players.</p><figure><a href="https://github.com/mcgrizzz/Yomine/?ref=skerritt.blog"><div><p>GitHub - mcgrizzz/Yomine: A Japanese vocabulary mining tool designed to help language learners mine new words and expressions.</p><p>A Japanese vocabulary mining tool designed to help language learners mine new words and expressions. - mcgrizzz/Yomine</p><p><img src="https://skerritt.blog/content/images/icon/pinned-octocat-093da3e6fa40-32.svg" alt=""><span>GitHub</span><span>mcgrizzz</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/9220b9c7-418e-4ad5-a456-2d744cdcc008" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>To use their words:</p><blockquote>A Japanese vocabulary mining tool designed to help language learners extract and study words from subtitle files. It integrates with ASBPlayer and MPV for timestamp navigation, ranks terms by frequency, and supports Anki integration to filter out known words.</blockquote><p>So it's kinda of like the Migaku "you should mine this word" feature, but for ASB Player and free.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-7DB77542-B6D8-4035-9BF9-D004A52F5B55-.png" alt="" loading="lazy" width="1595" height="862" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-7DB77542-B6D8-4035-9BF9-D004A52F5B55-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-7DB77542-B6D8-4035-9BF9-D004A52F5B55-.png 1000w, https://skerritt.blog/content/images/2025/12/-7DB77542-B6D8-4035-9BF9-D004A52F5B55-.png 1595w" sizes="(min-width: 720px) 720px"></figure><p>It basically extracts all the words from a video, checks to see if you have them in Anki and sorts them by a frequency.</p><p>From this you can then click a button to mine that word.</p><p>Here are some interesting ways people use it, which may not be obvious:</p><ul><li>If you saw a word you want to mine, but you were too busy to mine it you can use Yomine to search your favourite anime etc to find the word and mine it later on.</li><li>If you want to play a video game or visual novel, you could download a Let's Play of it and generate a comprehension score or pre-mine words you need to know.<ul><li>This is different than downloading Anki decks from Jiten and friends, because it's specifically words <strong>you</strong> need to know.</li><li>Also you get to watch a Let's Play which is like double immersion points 😯</li></ul></li><li>Using GSM to generate a Long Play (basically a large <code>mp4</code> recording of a game you're playing with a subtitles file), loading it into Yomine and mining all the words you didn't mine during that playthrough. Useful if you want to play now, mine later.</li></ul><figure><img src="https://skerritt.blog/content/images/2025/12/image-24.png" alt="" loading="lazy" width="1402" height="837" srcset="https://skerritt.blog/content/images/size/w600/2025/12/image-24.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/image-24.png 1000w, https://skerritt.blog/content/images/2025/12/image-24.png 1402w" sizes="(min-width: 720px) 720px"></figure><h2 id="best-websites">Best Websites</h2><p>Now let's look at some of the best websites out there.</p><h2 id="%F0%9F%8F%86jiten">🏆Jiten</h2><figure><a href="https://jiten.moe/?ref=skerritt.blog"><div><p>Jiten - Jiten</p><p>Vocabulary lists and anki decks for all your Japanese media.</p><p><img src="https://skerritt.blog/content/images/icon/favicon-22.ico" alt=""><span>Vocabulary lists and anki decks for all your Japanese media</span></p></div></a></figure><p>Jiten is a website that stores thousands of Japanese media and tells you a rough difficulty for them, among other things.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-C083E1C2-C305-42A2-8140-1B0CCFE98C5C-.png" alt="" loading="lazy" width="1372" height="576" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-C083E1C2-C305-42A2-8140-1B0CCFE98C5C-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-C083E1C2-C305-42A2-8140-1B0CCFE98C5C-.png 1000w, https://skerritt.blog/content/images/2025/12/-C083E1C2-C305-42A2-8140-1B0CCFE98C5C-.png 1372w" sizes="(min-width: 720px) 720px"></figure><p>They support all sorts of media, not just visual novels or anime.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-EBF7D73C-55B4-4DF3-8C5F-24009F56E663-.png" alt="" loading="lazy" width="1426" height="89" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-EBF7D73C-55B4-4DF3-8C5F-24009F56E663-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-EBF7D73C-55B4-4DF3-8C5F-24009F56E663-.png 1000w, https://skerritt.blog/content/images/2025/12/-EBF7D73C-55B4-4DF3-8C5F-24009F56E663-.png 1426w" sizes="(min-width: 720px) 720px"></figure><p>In Jiten you can upload a list of vocaburary you know (syncs with Anki and other tools):</p><figure><img src="https://skerritt.blog/content/images/2025/12/-4D944937-435B-4174-AD8E-0CAAC8CB6EF2-.png" alt="" loading="lazy" width="1226" height="288" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-4D944937-435B-4174-AD8E-0CAAC8CB6EF2-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-4D944937-435B-4174-AD8E-0CAAC8CB6EF2-.png 1000w, https://skerritt.blog/content/images/2025/12/-4D944937-435B-4174-AD8E-0CAAC8CB6EF2-.png 1226w" sizes="(min-width: 720px) 720px"></figure><p>Jiten can then rank how many words in the media you know, and tell you a personalised coverage score.</p><p>Something I like to do is find visual novels with &gt;80% external rating (external rating == reviews on vndb, anilist etc) which I have &gt;80% coverage for (meaning I will understand most of it).</p><figure><img src="https://skerritt.blog/content/images/2025/12/-9E4F48A2-7405-41D7-AE68-A8A0370ADC14-.png" alt="" loading="lazy" width="1425" height="606" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-9E4F48A2-7405-41D7-AE68-A8A0370ADC14-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-9E4F48A2-7405-41D7-AE68-A8A0370ADC14-.png 1000w, https://skerritt.blog/content/images/2025/12/-9E4F48A2-7405-41D7-AE68-A8A0370ADC14-.png 1425w" sizes="(min-width: 720px) 720px"></figure><p>Once you've found a piece of media you like hit "statistics" and see a cool graph.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-67D8D78A-3A82-438F-829A-DC14AA62B4F9-.png" alt="" loading="lazy" width="1046" height="875" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-67D8D78A-3A82-438F-829A-DC14AA62B4F9-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-67D8D78A-3A82-438F-829A-DC14AA62B4F9-.png 1000w, https://skerritt.blog/content/images/2025/12/-67D8D78A-3A82-438F-829A-DC14AA62B4F9-.png 1046w" sizes="(min-width: 720px) 720px"></figure><p>For example, in this anime to understand 95% of it you just need to know the most frequency 1944 words that appear in it.</p><p>To understand 99%, you need to know an extra 3200ish.</p><p>Learning 1944 of the words in this anime seems great, but how do we actually learn them?</p><p>No worries, Jiten lets you download Anki decks with the exact freq order of that show. Learn the words you need to know for the media you want to watch.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-E6570983-254B-47F0-A687-E93959CEB741-.png" alt="" loading="lazy" width="512" height="521"></figure><p>If doing premade Anki decks isn't your thing, download the "occurrences" dictionary to get a frequency list you can use in Yomitan to tell you how often a word appears in the show.</p><p>That way I can mine high frequent words in shows I want to watch, without watching them yet. Almost like I'm prepping for it.</p><div><p>💡</p><p>Priority Reorder Anki addon from earlier works really well with this by the way</p></div><figure><img src="https://skerritt.blog/content/images/2025/12/-F793A297-3F26-4328-A85B-413EB7D9537A-.png" alt="" loading="lazy" width="521" height="267"></figure><p>Jiten also has a dictionary you can use. Search a word to see its frequency, and all the media that word is in:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-ECF0AC9B-393D-499B-A0E1-10AFE8E88E5A-.png" alt="" loading="lazy" width="692" height="649" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-ECF0AC9B-393D-499B-A0E1-10AFE8E88E5A-.png 600w, https://skerritt.blog/content/images/2025/12/-ECF0AC9B-393D-499B-A0E1-10AFE8E88E5A-.png 692w"></figure><p>Speaking of dictionaries, you can download global frequency lists on Jiten too:</p><figure><img src="https://skerritt.blog/content/images/2025/12/-2435444D-B7EA-47A9-8534-F0E238BDDAEC-.png" alt="" loading="lazy" width="1028" height="566" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-2435444D-B7EA-47A9-8534-F0E238BDDAEC-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-2435444D-B7EA-47A9-8534-F0E238BDDAEC-.png 1000w, https://skerritt.blog/content/images/2025/12/-2435444D-B7EA-47A9-8534-F0E238BDDAEC-.png 1028w" sizes="(min-width: 720px) 720px"></figure><p>And finally, Jiten has <strong>a lot</strong> of data and every single week it is improving. </p><figure><img src="https://skerritt.blog/content/images/2025/12/-A8E2F40B-4EAB-48A5-A847-056CD2F2CD1C-.png" alt="" loading="lazy" width="1069" height="346" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-A8E2F40B-4EAB-48A5-A847-056CD2F2CD1C-.png 600w, https://skerritt.blog/content/images/size/w1000/2025/12/-A8E2F40B-4EAB-48A5-A847-056CD2F2CD1C-.png 1000w, https://skerritt.blog/content/images/2025/12/-A8E2F40B-4EAB-48A5-A847-056CD2F2CD1C-.png 1069w" sizes="(min-width: 720px) 720px"></figure><h2 id="yokubimorg-grammar-guide">Yokubi - Morg Grammar Guide</h2><figure><a href="https://yoku.bi/?ref=skerritt.blog"><div><p>Introduction - Yokubi</p><p><img src="https://skerritt.blog/content/images/icon/favicon-1.svg" alt=""><span>Yokubi</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/logo-1.svg" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>If you have spent any amount of time in Japanese spaces online you may have heard of "Morg". Especially on Discord or Reddit.</p><p>He's a really nice guy who knows a lot about grammar and wants to help you learn Japanese.</p><p>This year he took it upon himself to improve the Sakubi grammar guide, and ended up writing Yokubi.</p><p>It's a really succinct grammar guide designed to get you immersing ASAP.</p><figure><img src="https://skerritt.blog/content/images/2025/12/-1D7255EA-2332-4E47-9984-6903D7837A6F-.png" alt="" loading="lazy" width="757" height="490" srcset="https://skerritt.blog/content/images/size/w600/2025/12/-1D7255EA-2332-4E47-9984-6903D7837A6F-.png 600w, https://skerritt.blog/content/images/2025/12/-1D7255EA-2332-4E47-9984-6903D7837A6F-.png 757w" sizes="(min-width: 720px) 720px"></figure><h2 id="best-for-books"><strong>Best for books</strong></h2><h2 id="%F0%9F%8F%86lumiereader">🏆LumieReader</h2><figure><a href="https://lumireader.app/?ref=skerritt.blog"><div><p>Lumi Reader: fast and modern ebook reader.</p><p>Fast and modern ebook reader. Track your reading sessions, sync your library with the cloud, and enjoy customizable themes. Try Lumi Reader for multi-platform, efficient and distraction-free reading!</p><p><img src="https://skerritt.blog/content/images/icon/apple-touch-icon-3.png" alt=""><span>Lumi Reader Team</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/og-image.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><p>Lumie Reader came out hitting this year with a single premise:</p><blockquote>What if we made ttsu but good?</blockquote><figure><a href="https://reader.ttsu.app/manage?ref=skerritt.blog"><div><p>ッツ Ebook Reader</p><p>Online e-book reader that supports dictionary extensions like Yomitan</p><p><img src="https://skerritt.blog/content/images/icon/apple-touch-icon-4.png" alt=""><span>ッツ Ebook Reader</span></p></div><p><img src="https://skerritt.blog/content/images/thumbnail/regular-icon@512x512-1.png" alt="" onerror="this.style.display = 'none'"></p></a><figcaption><p><span>Another reading app for Japanese</span></p></figcaption></figure><p>It's entirely offline, very fast and supports a lot of features.</p><p>It has some features over other readers like:</p><ul><li>Bookmarks</li><li>Extensive statistics for reading</li><li>Cloud sync (this is the biggest one)</li><li>Social features, like sharing what you are reading</li></ul><p>You can tell I don't read books can't ya...</p><h2 id="conclusion">Conclusion</h2><p>This year has been amazing for Japanese learning tools.</p><p>If you want to give back to the community but don't want to code, <strong>many</strong> of these devs have donation links listed.</p><p>What's your favourite tool? Tell me in the comments :) &lt;3</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CEOs are hugely expensive. Why not automate them? (234 pts)]]></title>
            <link>https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots</link>
            <guid>46415488</guid>
            <pubDate>Sun, 28 Dec 2025 23:17:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots">https://www.newstatesman.com/business/companies/2023/05/ceos-salaries-expensive-automate-robots</a>, See on <a href="https://news.ycombinator.com/item?id=46415488">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="the-post">

        
<header>
    <!-- Normal closing div -->
          
     
</header><!-- .entry-header-outer /-->
    
        <div id="figarocontent">
                <div>
        <figure>

        <img width="1038" height="692" src="https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-scaled.jpg" alt="" decoding="async" fetchpriority="high" srcset="https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-scaled.jpg 2560w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-300x200.jpg 300w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-1024x683.jpg 1024w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-768x512.jpg 768w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-1536x1024.jpg 1536w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-2048x1365.jpg 2048w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-101x67.jpg 101w, https://dl6pgk4f88hky.cloudfront.net/2021/06/gettyimages-838665582-397x265.jpg 397w" sizes="(max-width: 1407px) 1407px, (max-width: 335px) 335px, (max-width: 705px) 705px, (max-width: 335px) 335px, (max-width: 689px) 689px, (max-width: 336px) 336px, (max-width: 210px) 210px, (max-width: 101px) 101px, (max-width: 1024px) 1024px, (max-width: 101px) 101px, (max-width: 397px) 397px, (max-width: 464px) 464px, (max-width: 797px) 797px, (max-width: 960px) 960px, (max-width: 314px) 314px, (max-width: 464px) 464px, (max-width: 735px) 735px, (max-width: 1038px) 1038px">
                <figcaption> (Photo By Chung Sung-Jun/Getty images)</figcaption>
            </figure>
</div>



                <p><em>On Wednesday 31 May, it was reported that Alex Mahon, CEO of Channel 4, could receive<a href="https://www.theguardian.com/uk-news/2023/may/31/channel-4-boss-alex-mahon-could-get-stations-biggest-ever-payday-of-14m" target="_blank" rel="noopener"> record annual pay of £1.4m</a>. This article was originally published on 26 April 2021 and asks, as Executive pay continues to rise, does a company need a CEO at all?</em></p>



<p>Over the next two weeks, the boards of <a href="https://news.sky.com/story/bae-systems-on-defensive-over-golden-handcuffs-deal-for-sought-after-ceo-12280257" target="_blank" rel="noopener nofollow">BAE Systems</a>, <a href="https://news.sky.com/story/looming-astrazeneca-pay-row-provides-latest-headache-for-ceo-soriot-12284677" target="_blank" rel="noopener nofollow">AstraZeneca</a>, <a href="https://www.reuters.com/article/glencore-ceo-pay-idUSL1N2M90QZ" target="_blank" rel="noopener nofollow">Glencore</a>,  <a href="https://www.thetimes.co.uk/article/flutter-entertainment-shareholder-revolt-chief-executive-pay-rise-hostelworld-v6j3khkx2" target="_blank" rel="noopener nofollow">Flutter Entertainment</a> and the <a href="https://news.sky.com/story/london-stock-exchange-faces-investor-backlash-over-chiefs-25-pay-rise-12262725" target="_blank" rel="noopener nofollow">London Stock Exchange</a> all face the possibility of shareholder revolts over executive pay at their forthcoming annual general meetings (AGMs). As the AGM season begins, there is a particular focus on pay.</p>



<p>Executive pay is often the most contentious item at an AGM, but this year is clearly exceptional. The people running companies that have been severely impacted by <a href="https://www.newstatesman.com/tag/covid-19" target="_blank" rel="noopener">Covid-19</a> can’t be blamed for the devastation of their revenues by the pandemic, but they also can’t take credit for the government stimulus that has kept them afloat. Last week, for example, nearly 40 per cent of shareholders in the estate agents Foxtons <a href="https://propertyindustryeye.com/last-weeks-foxtons-agm-must-serve-as-a-wake-up-call-for-the-board/" target="_blank" rel="noopener nofollow">voted against</a> its chief executive officer, Nicholas Budden, receiving a bonus of just under £1m; Foxtons has received about £7m in direct government assistance and is benefiting from the government’s <a href="https://www.newstatesman.com/business/sectors/2021/04/who-benefits-when-government-pumps-housing-market" target="_blank" rel="noopener">continued inflation of the housing market</a>. The person who has done most to ensure Foxtons’ ongoing good fortune is not Nicholas Budden but <a href="https://www.newstatesman.com/tag/rishi-sunak" target="_blank" rel="noopener">Rishi Sunak</a>. </p>



<p>Under the Enterprise and Regulatory Reform Act, executive pay is voted on at least every three years, and this process forces shareholders and the public to confront how much the people at the top take home. Tim Steiner, the highest-paid CEO in the FTSE 100, was paid £58.7m in 2019 for running Ocado, which is <a href="https://www.cipd.co.uk/Images/ftse-100-executive-pay-report_tcm18-82375.pdf" target="_blank" rel="noopener">2,605 times the median income of his employees</a> for that year, while the average FTSE100 CEO makes more than £15,000 a day.  </p><section>
                        <p><a href="https://www.newstatesman.com/business/companies/2023/05/javascript(void);"><img decoding="async" src="https://dl6pgk4f88hky.cloudfront.net/2021/09/TNS_master_logo.svg"></a>
                        </p>
                            <p>Treat yourself or a friend this Christmas to a New Statesman subscription for just £2 </p>
                            
                        </section>



<p>As the High Pay Centre’s annual assessment of CEO pay points out, a top-heavy wage bill extends beyond the CEO, and could be unsustainable for any company this year. “When one considers high earners beyond the CEO”, says the report, ”there is actually quite significant potential for companies to safeguard jobs and incomes by asking higher-paid staff to make sacrifices”.</p>



<p>In the longer term, as companies commit to greater automation of many roles, it’s pertinent to ask whether a company needs a CEO at all.  </p>



<p>A few weeks ago Christine Carrillo, an American tech CEO, raised this question herself when she tweeted a spectacularly tone-deaf appreciation of her executive assistant, whose work allows Carrillo to “write [and] surf every day” as well as “cook dinner and read every night”. In Carrillo’s unusually frank description of the work her EA does – most of her emails, most of the work on fundraising, playbooks, operations, recruitment, research, updating investors, invoicing “and so much more” – she guessed that this unnamed worker “saves me 60% of time”.</p>



<p>Predictably, a horde arrived to point out that if someone else is doing 60 per cent of Carrillo’s job, they should be paid 50 per cent more than her. But as Carrillo – with a frankly breathtaking lack of self-awareness – informed another commenter, her EA is based in the Philippines. The main (and often the only) reason to outsource a role is to pay less for it.</p>



<p><strong><em>[See also: <a href="https://www.newstatesman.com/new-statesman-view/2023/05/the-scourge-of-greedflation" target="_blank" rel="noopener">The scourge of greedflation</a>]</em></strong></p>



<p>If most of a CEO’s job can be outsourced, this suggests it could also be automated. But while <a href="https://www.newstatesman.com/business/companies" target="_blank" rel="noopener">companies</a> are racing to automate entry- and mid-level roles, senior executives and decision makers show much less interest in automating themselves.</p>



<p>There’s a good argument for automating from the top rather than from the bottom. As we know from the annotated copy of <em>Thinking, Fast and Slow</em> that sits (I assume) on every CEO’s Isamu Noguchi nightstand, human decision-making is the product of irrational biases and assumptions. This is one of the reasons strategy is so difficult, and roles that involve strategic decision-making are so well paid. But the difficulty of making genuinely rational strategic decisions, and the cost of the people who do so, are also good reasons to hand this work over to software.</p>



<p>Automating jobs can be risky, especially in public-facing roles. After Microsoft sacked a large team of journalists in 2020 in order to <a href="https://www.theverge.com/2020/5/30/21275524/microsoft-news-msn-layoffs-artificial-intelligence-ai-replacements" target="_blank" rel="noopener nofollow">replace them with AI</a>, it almost immediately had to contend with the <a href="https://www.standard.co.uk/tech/microsoft-robot-ai-editors-jade-thirwall-little-mix-a4463706.html" target="_blank" rel="noopener nofollow">PR disaster</a> of the software’s failure to distinguish between two women of colour. Amazon had to abandon its AI recruitment tool after it <a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G" target="_blank" rel="noopener nofollow">learned to discriminate against women</a>. And when GPT-3, one of the most advanced AI language models, was used as a medical chatbot in 2020, it responded to a (simulated) patient presenting with suicidal ideation by <a href="https://www.theregister.com/2020/10/28/gpt3_medical_chatbot_experiment/" target="_blank" rel="noopener nofollow">telling them to kill themselves</a>.</p>



<p>What links these examples is that they were all attempts to automate the kind of work that happens without being scrutinised by lots of other people in a company. Top-level strategic decisions are different. They are usually debated before they’re put into practice – unless, and this is just another reason to automate them, employees feel they can’t speak up for fear of incurring the CEO’s displeasure.</p>



<p>Where automated management – or “<a href="https://www.ibm.com/blogs/journey-to-ai/2020/04/the-rise-of-decision-intelligence-ai-that-optimizes-decision-making/" target="_blank" rel="noopener nofollow">decision intelligence</a>”, as Google and IBM call it – has been deployed, it’s produced impressive results. Hong Kong’s mass transit system put software in charge of scheduling its maintenance <a href="https://www.cs.cityu.edu.hk/~hwchun/AIProjects/stories/allocation/mtrcscheduling/" target="_blank" rel="noopener nofollow">in 2004</a>, and enjoys a reputation as one of the world’s most punctual and best-run metros.   </p>



<p>Clearly, chief execs didn’t get where they are today by volunteering to clear out their corner offices and hand over their caviar spittoons to robots. But management is a very large variable cost that only seems to increase – Persimmon’s <a href="https://www.verdict.co.uk/persimmons-chief-executive-bonus-comparison/" target="_blank" rel="noopener nofollow">bonus scheme</a> paid out half a billion pounds to 150 execs in a single year – while technology moves in the other direction, becoming cheaper and more reliable over time.</p>



<p>It is often asked whether CEO pay is fair or ethical. But company owners and investors should be asking if their top management could be done well by a machine – and if so, why is it so expensive?</p>



<p><strong><em>[See also: <a href="https://www.newstatesman.com/business/the-business-interview/2023/02/modern-milkman-ceo-simon-mellin-boris-johnson" target="_blank" rel="noopener">The milkman on a mission</a>]</em></strong></p>
<div>
    <h6>Content from our partners</h6>

        
            
            
        </div>

                        
                                        
        

    
                
                                    <!-- <div class="dianomiMainDiv" style="min-height: 50px;position: relative!important;">
         
                        <div id='div-gpt-ad-3393934-1'>
                        <script>
                            googletag.cmd.push(function() { googletag.display('div-gpt-ad-3393934-1'); });
                        </script>
                        </div>
                    </div> -->
                    
                    
				            </div><!-- .entry-content /-->

        



    </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spherical Cow (122 pts)]]></title>
            <link>https://lib.rs/crates/spherical-cow</link>
            <guid>46415458</guid>
            <pubDate>Sun, 28 Dec 2025 23:11:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lib.rs/crates/spherical-cow">https://lib.rs/crates/spherical-cow</a>, See on <a href="https://news.ycombinator.com/item?id=46415458">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="readme-deps">
<div>
<section>
<section id="versions">
<h3>
<a href="https://lib.rs/crates/spherical-cow/versions">5 releases</a>
</h3>
<table>
<tbody><tr><th content="0.1.4" property="softwareVersion">
0.1.4
</th><td property="datePublished">
Feb 12, 2021
</td></tr>
<tr><th content="0.1.3">
0.1.3
</th><td>
Feb 11, 2021
</td></tr>
<tr><th content="0.1.2">
0.1.2
</th><td>
Oct 29, 2018
</td></tr>
<tr><th content="0.1.1">
0.1.1
</th><td>
Jan 22, 2018
</td></tr>
<tr><th content="0.1.0">
0.1.0
</th><td>
Jan 21, 2018
</td></tr>
</tbody></table>
</section>
<section id="downloads">
<p>#<b>2556</b> in <a href="https://lib.rs/algorithms" title="Rust implementations of core algorithms such as hashing, sorting, searching, and more.">Algorithms</a></p>
<p>
<b>29</b> downloads per month
<br>
Used in <a href="https://lib.rs/crates/sphere_pack_from_json">sphere_pack_from_json</a>
</p>
</section>
<section id="sloc">
<section id="license">
<b property="license">MIT/Apache</b>
</section>
<p>
<span title="34KB as tarball">
49KB
</span>
<br>
764 <span title="Lines of code, excluding comments">lines</span>
</p>
</section>
</section>
<section id="readme" vocab="">
<h2>Spherical Cow</h2>
<p>A high volume fraction sphere packing library</p>

<p><a href="https://crates.io/crates/spherical-cow" rel="ugc noopener">
<img src="https://img.shields.io/crates/v/spherical-cow.svg" alt="Crates.io" decoding="async" width="102" height="20">
</a>
│
<a href="https://docs.rs/spherical-cow/" rel="ugc noopener">
<img src="https://img.shields.io/badge/api-documentation-blue.svg" alt="Docs.rs" decoding="async" width="120" height="20">
</a>
│
<a href="https://travis-ci.org/Libbum/spherical-cow" rel="ugc noopener">
<img alt="Travis-ci" decoding="async" crossorigin="anonymous" src="https://img.gs/czjpqfbdkz/full/https://travis-ci.org/Libbum/spherical-cow.svg?branch=master">
</a>
│
<a href="https://codecov.io/gh/Libbum/spherical-cow" rel="ugc noopener">
<img alt="Codecov" decoding="async" crossorigin="anonymous" src="https://img.gs/czjpqfbdkz/full/https://codecov.io/gh/Libbum/spherical-cow/branch/master/graph/badge.svg" width="112" height="20">
</a>
│
<a href="https://app.fossa.io/projects/git%2Bgithub.com%2FLibbum%2Fspherical-cow?ref=badge_shield" rel="ugc noopener">
<img alt="FOSSA Status" decoding="async" crossorigin="anonymous" src="https://img.gs/czjpqfbdkz/full/https://app.fossa.io/api/projects/git%2Bgithub.com%2FLibbum%2Fspherical-cow.svg?type=shield" width="155" height="20">
</a>
</p>

<p>Based on the advancing fronts algorithm outlined in Valera <em>et al.</em>, <a href="https://doi.org/10.1007/s40571-015-0045-8" rel="ugc noopener">Computational Particle Mechanics 2, 161 (2015)</a>.</p>
<blockquote>
<p>Milk production at a dairy farm was low, so the farmer wrote to the local university, asking for help from academia.
A multidisciplinary team of professors was assembled, headed by a theoretical physicist, and two weeks of intensive on-site investigation took place.
The scholars then returned to the university, notebooks crammed with data, where the task of writing the report was left to the team leader.
Shortly thereafter the physicist returned to the farm, saying to the farmer, "I have the solution, but it works only in the case of spherical cows in a vacuum".</p>
</blockquote>
<h2 id="readme-usage">Usage</h2>
<p>Complete documentation can be found at <a href="https://docs.rs/spherical-cow/" rel="ugc noopener">docs.rs</a>.</p>
<p>A simple example to get you packing spheres of radii (0.1..0.2) into a container sphere of radius 2.</p>
<pre lang="rust"><code><tt><tt>use</tt> <tt>spherical_cow<tt>::</tt></tt><tt>shapes<tt>::</tt></tt>Sphere<tt>;</tt>
<tt>use</tt> <tt>rand<tt>::</tt></tt><tt>distributions<tt>::</tt></tt>Uniform<tt>;</tt>
<tt>use</tt> <tt>nalgebra<tt>::</tt></tt>Point3<tt>;</tt>

<tt><tt>fn</tt> <tt>main</tt></tt><tt><tt><tt>(</tt></tt><tt><tt>)</tt></tt></tt><tt> </tt><tt><tt><tt>{</tt>
    <tt>let</tt> boundary <tt>=</tt> <tt>Sphere<tt>::</tt></tt>new<tt>(</tt><tt>Point3<tt>::</tt></tt>origin<tt>(</tt><tt>)</tt><tt>,</tt> <tt>2.</tt><tt>0</tt><tt>)</tt><tt>.</tt><tt>unwrap</tt><tt>(</tt><tt>)</tt><tt>;</tt>
    <tt>let</tt> <tt>mut</tt> sizes <tt>=</tt> <tt>Uniform<tt>::</tt></tt>new<tt>(</tt><tt>0.</tt><tt>1</tt><tt>,</tt> <tt>0.</tt><tt>2</tt><tt>)</tt><tt>;</tt>

    <tt>let</tt> spheres <tt>=</tt> <tt>spherical_cow<tt>::</tt></tt>pack_spheres<tt>(</tt>boundary<tt>,</tt> <tt>&amp;</tt><tt>mut</tt> sizes<tt>)</tt><tt>.</tt><tt>unwrap</tt><tt>(</tt><tt>)</tt><tt>;</tt>

    <tt>println!</tt><tt>(</tt><tt><tt>"</tt>Number of spheres: <tt>{}</tt><tt>"</tt></tt><tt>,</tt> spheres<tt>.</tt><tt>len</tt><tt>(</tt><tt>)</tt><tt>)</tt><tt>;</tt>
</tt><tt><tt>}</tt></tt></tt>
</tt></code></pre>
<p>More elaborate examples can be found in the <a href="https://github.com/libbum/spherical-cow/blob/HEAD/examples/" rel="ugc noopener">examples</a> directory.</p>
<h2 id="readme-output">Output</h2>
<p>True to its name, it is indeed possible to build a spherical cow:</p>
<p><img alt="spherical cow in vacuum" decoding="async" crossorigin="anonymous" src="https://img.gs/czjpqfbdkz/full/https://raw.githubusercontent.com/Libbum/spherical-cow/master/examples/objects/cow_output.jpg?raw=true" width="800"></p>
<p>You can run this example yourself from <a href="https://github.com/libbum/spherical-cow/blob/HEAD/examples/show_in_cow.rs" rel="ugc noopener">show_in_cow</a>.</p>
<h2 id="readme-example-use-cases">Example use cases</h2>
<p>The paper which this algorithm comes from gives two examples of real world use cases:</p>
<ol>
<li>Sphere packing a skull model to study fractures due to shocks and penetrating objects.</li>
<li>Sphere packing a cutting tool to identify the failure / breaking points when the tool is placed under load.</li>
</ol>
<p>The reason this library was initially written was to optimise the layout of inflatable <a href="https://github.com/Libbum/space-habitats" rel="ugc noopener">space habitats</a> which may one day be constructed on the Moon and Mars.</p>
<h2 id="readme-license">License</h2>
<p>Licensed under the Apache License, <a href="http://www.apache.org/licenses/LICENSE-2.0" rel="ugc noopener">Version 2.0</a> or the <a href="http://opensource.org/licenses/MIT" rel="ugc noopener">MIT license</a>, at your option.
These files may not be copied, modified, or distributed except according to those terms.</p>
<p><a href="https://app.fossa.io/projects/git%2Bgithub.com%2FLibbum%2Fspherical-cow?ref=badge_large" rel="ugc noopener"><img alt="FOSSA Status" decoding="async" crossorigin="anonymous" src="https://img.gs/czjpqfbdkz/full/https://app.fossa.io/api/projects/git%2Bgithub.com%2FLibbum%2Fspherical-cow.svg?type=large" width="240" height="272"></a></p>
<h2 id="readme-deeply-linked-dependencies">Deeply linked dependencies</h2>
<p>The <a href="https://github.com/Smithay/wayland-rs/tree/master/wayland-protocols" rel="ugc noopener">wayland-protocols</a> library (released under an MIT license) is used by <a href="https://github.com/sebcrozet/kiss3d" rel="ugc noopener">kiss3d</a> and <a href="https://github.com/Smithay/wayland-rs/tree/master/wayland-protocols" rel="ugc noopener">obj</a>, both of which are only extant in the examples directory of this project (and thus are NOT a part of the library).
Content therein: the file <code><tt>misc<tt>/</tt>server<tt>-</tt>decoration<tt>.</tt>xml</tt></code>, is Copyright (C) 2015 Martin Gräßlin and licensed under the GNU Lesser General Public Library, version 2.1. You can find a copy of this license at <a href="https://www.gnu.org/licenses/lgpl-2.1.en.html" rel="ugc noopener">https://www.gnu.org/licenses/lgpl-2.1.en.html</a></p>
</section>
</div>
<section id="deps">
<h4>Dependencies</h4>
<p>
<span title="amortized size; approx. 1.5MB compressed">
~6MB
</span>
<br>
<span title="estimated">~114K</span> <abbr title="approx. additional lines of code">SLoC</abbr>
</p>
<nav aria-label="Dependencies">
<ul>
<li property="requirements">
<a title="obsolete" href="https://lib.rs/crates/float-cmp">float-cmp</a>&nbsp;<span>0.8</span>
</li>
<li property="requirements">
<a title="outdated" href="https://lib.rs/crates/itertools">itertools</a>&nbsp;<span>0.10</span>
</li>
<li property="requirements">
<a title="obsolete" href="https://lib.rs/crates/nalgebra">nalgebra</a>&nbsp;<span>0.24</span>
</li>
<li property="requirements">
<a title="outdated" href="https://lib.rs/crates/rand">rand</a>&nbsp;<span>0.8</span>
</li>
<li property="requirements">
<a title="outdated" href="https://lib.rs/crates/rand_distr">rand_<wbr>distr</a>&nbsp;<span>0.4</span>
</li>
<li property="requirements">
<a href="https://lib.rs/crates/spherical-cow/features#feature-serde-1" title="optional feature">serde-1?</a>
<a title="1.0" href="https://lib.rs/crates/serde">serde</a>
</li>
</ul>
<ul>
<li property="requirements">
<span>
dev
</span>
<a title="obsolete" href="https://lib.rs/crates/criterion">criterion</a>&nbsp;<span>0.3</span>
</li>
<li property="requirements">
<span>
dev
</span>
<a title="obsolete" href="https://lib.rs/crates/kiss3d">kiss3d</a>&nbsp;<span>0.29</span>
</li>
<li property="requirements">
<span>
dev
</span>
<a title="0.10" href="https://lib.rs/crates/obj">obj</a>
</li>
<li property="requirements">
<span>
dev
</span>
<a title="1.0" href="https://lib.rs/crates/serde_json">serde_<wbr>json</a>
</li>
</ul>
</nav>
</section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[62 years in the making: NYC's newest water tunnel nears the finish line (136 pts)]]></title>
            <link>https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-</link>
            <guid>46415426</guid>
            <pubDate>Sun, 28 Dec 2025 23:05:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-">https://ny1.com/nyc/all-boroughs/news/2025/11/09/water--dep--tunnels-</a>, See on <a href="https://news.ycombinator.com/item?id=46415426">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Turn on the tap, and water flows without a second thought. But deep beneath New York City, hundreds of feet below street level, workers are finishing a project that’s been under construction for more than half a century — a massive water tunnel that will help keep that simple act possible for generations to come.</p>
<p>Tunnel No. 3, as it’s known, is one of the most ambitious infrastructure projects in the city’s history.</p>
<p>When complete, it will ensure New Yorkers continue to receive clean water from upstate reservoirs — some more than 125 miles away — while allowing long-overdue maintenance on the city’s two older tunnels, built in 1917 and 1936.</p>
<div><hr>

<h4><b>What You Need To Know</b></h4>
<h5><ul>
<li>About 95% of New York City’s water supply flows into the city by gravity through three water tunnels</li>
<br>
<li>Tunnels 1 and 2 were completed in 1917 and 1936, respectively</li>
<br>
<li>Construction on Tunnel 3 began in 1970 and currently serves the Bronx and Manhattan</li>
<br>
<li>The final two shafts in Queens are expected to be completed by 2032, allowing for long-term repairs to the city’s aging water infrastructure</li>
</ul>
</h5>
<hr>

</div>
<p>City Department of Environmental Protection Commissioner Rohit Aggarwala and DEP Portfolio Manager Lauren D’Attile recently took an elevator nearly 800 feet down to see the progress for themselves.</p>
<p>“It’s not quite as far down as the Empire State Building is tall, but it’s getting there,” Aggarwala said during the 10-minute descent.</p>
<p>Down below, flashlights cut through the darkness as water dripped from the rock walls. Workers stood in waterproof boots along the cool, damp concrete — the result of decades of digging, drilling and sealing off bare rock to create a watertight tunnel system.</p>
<p>“When this tunnel was originally constructed, it was built by a tunnel boring machine, which is a very large piece of equipment with cutter heads on the front,” said D’Attile. “We drill the tunnel and after that we line that bare rock with a couple of feet of concrete — so that’s what you’re seeing now, because this tunnel is complete.”</p>
<p>Construction on Tunnel No. 3 began in 1970.</p>
<p>The Bronx and Manhattan already receive water from it, and the final phase — extending service to Brooklyn and Queens — is expected to be completed by 2032.</p>
<p>“The project started in 1970, it will be finished in 2032 — that’s 62 years to build this thing,” Aggarwala said. “But a project like this is going to serve New York for two, three hundred years, who knows how much longer than that. Seems worth it. Totally worth it. It’s what makes the city work because we are constantly investing in our future.”</p>
<p>When it’s complete, the DEP will finally be able to take the older tunnels offline for repairs — a step city engineers have waited decades to take.</p>
<p>Above ground, New Yorkers will keep turning on their faucets, washing dishes, and filling glasses — rarely thinking about the billion gallons of water flowing through the underground arteries that make city life possible.</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[As AI gobbles up chips, prices for devices may rise (262 pts)]]></title>
            <link>https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram</link>
            <guid>46415338</guid>
            <pubDate>Sun, 28 Dec 2025 22:52:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram">https://www.npr.org/2025/12/28/nx-s1-5656190/ai-chips-memory-prices-ram</a>, See on <a href="https://news.ycombinator.com/item?id=46415338">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storytext">
      <div id="resg-s1-103738">
            <div data-crop-type="">
        <picture>
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/400/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/900/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1200/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1600/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1800/quality/85/format/webp/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/webp">
            <source srcset="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/400/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 400w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 800w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/900/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 900w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1200/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1200w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1600/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1600w,
https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1800/quality/85/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg 1800w" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg" sizes="(min-width: 1025px) 650px, calc(100vw - 30px)" type="image/jpeg">
            <img src="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/1100/quality/50/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg" data-template="https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2550x1509+0+0/resize/{width}/quality/{quality}/format/{format}/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fbd%2Fcb%2Fe6930aae4194aa1d81d15e00b18b%2Fap101105125088.jpg" alt="Idaho-based Micron Technology is one of the world’s top makers of RAM chips and it’s benefited from increase demand. " fetchpriority="high">
        </picture>
</div>
<div>
    <div>
        <p>
                Idaho-based Micron Technology is one of the world's top makers of RAM chips and it's benefited from increase demand.
                <b aria-label="Image credit">
                    
                    Charlie Litchfield/ASSOCIATED PRESS/FR164915AP
                    
                </b>
                <b><b>hide caption</b></b>
            </p>


            <p><b><b>toggle caption</b></b>
    </p></div>

    <p><span aria-label="Image credit">
        
        Charlie Litchfield/ASSOCIATED PRESS/FR164915AP
        
    </span>
</p></div>
   </div>
   <p>The world has a memory problem, thanks to artificial intelligence.</p>   <p>The explosion in AI-related cloud computing and data centers has led to so much demand for certain types of memory chips that now there's a shortage. The imbalance is expected to start affecting prices of all sorts of products powered by technology.</p>   <p>"I keep telling everybody that if you want a device, you buy it now," said Avril Wu, a senior research vice president at TrendForce, a Taiwan-based consultancy that tracks markets for computer components. "I myself bought an iPhone 17 already,"</p>   <p>The chips are known as RAM, or random access memory, and are crucial to making sure that things like smartphones, computers and game consoles run smoothly. Chips allow you to keep multiple tabs open in browsers, for instance, or watch videos without them being choppy.</p>   
   
   
<!-- END ID="RESNX-S1-5656190-100" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Wu said TrendForce's data indicates that demand for RAM chips exceeds supply by 10% – and it's growing so fast that manufacturers are having to shell out a lot more to buy them each month.</p>   <p>Wu said this quarter alone, they're paying 50% more than the previous quarter for the most common type of RAM, known as DRAM – dynamic random access memory. And if producers want the chips sooner, they're paying two to three times more.</p>   <p>Wu expects DRAM prices to rise another 40% in the coming quarter, and she doesn't expect the prices to go down in 2026.</p>   <h3>How AI is gobbling up memory</h3>   <p>AI data centers require huge amounts of memory to accompany their cutting-edge graphics processing unit (GPU) microprocessors that train and operate AI models.</p>   <p>"AI workloads are built around memory," said Sanchit Vir Gogia, CEO of the tech advisory firm Greyhound Research.</p>   <p>What's more, AI companies are spending billions of dollars constructing data centers at warp speed around the world. It's the reason why Gogia says the demand for these chips isn't just a cyclical blip.</p>   <p>"AI has changed the nature of demand itself," he said. "Training and inference systems require large, persistent memory footprints, extreme bandwidth, and tight proximity to compute. You cannot dial this down without breaking performance."</p>   
   <h3>More chips for AI means fewer&nbsp; chips for other products</h3>   <p>Idaho-based Micron Technology is one of the world's top makers of RAM and it's benefited from this increase in demand. It reported better-than-expected quarterly earnings last week on the back of higher memory chip prices.</p>   <p>CEO Sanjay Mehrotra said the company expected the market to remain strong, as the AI boom continues apace. "We believe that the aggregate industry supply will remain substantially short of the demand for the foreseeable future," he said on a webcast after the earnings report.</p>   
   
<!-- END ID="RESNX-S1-5656190-101" CLASS="BUCKETWRAP INTERNALLINK INSETTWOCOLUMN INSET2COL " -->
   <p>Chipmakers like Micron have <a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business" target="_blank"><u>shifted production</u></a> to meet as much of the lucrative AI-related demand for high-end memory as they can, according to analysts. That translates into fewer chips for other segments of the market – personal computers, mobile phones, games and consumer products like TVs.</p>   <p>And that means higher costs. Dell Technologies Chief Operating Officer Jeff Clarke noted the higher costs on an <a href="https://investors.delltechnologies.com/static-files/9d07092c-7a61-42be-a1c2-54aea7465d17" target="_blank"><u>earnings call</u></a> on Nov. 25. For PC's, he said "I don't see how this will certainly not make its way into the customer base."</p>   <p>Analysts say there is no short-term fix.</p>   <p>Tech consultant Wu said the memory chip industry faces a significant bottleneck. By the end of 2026, she said, chip makers will have maxed out how much they can expand production in their current facilities.</p>   <p>She said the next new factory expected to come online is being built by Micron in Idaho. The company says it will be operational in 2027.</p>   <p>Expect suppliers to keep raising prices for the foreseeable future, Wu said.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What an unprocessed photo looks like (2101 pts)]]></title>
            <link>https://maurycyz.com/misc/raw_photo/</link>
            <guid>46415225</guid>
            <pubDate>Sun, 28 Dec 2025 22:35:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://maurycyz.com/misc/raw_photo/">https://maurycyz.com/misc/raw_photo/</a>, See on <a href="https://news.ycombinator.com/item?id=46415225">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<time datetime="2025-12-27">Dec 27, 2025</time>



(<a href="http://maurycyz.com/tags/photography">Photography</a>) 



<p>Here’s a photo of a Christmas tree, as my camera’s sensor sees it:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas1.jpg" alt=""></p>
<center>Sensor data with the 14 bit ADC values mapped to 0-255 RGB.</center>
<!-- Black point compensation --> 
<p>It’s not even black-and-white, it’s gray-and-gray.
This is becuase while the ADC’s output can theoretically go from 0 to 16382, the actual data doesn’t cover that whole range:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/hist1.png" alt=""></p>
<center>Histogram of raw image</center>
<p>The real range of ADC values is ~2110 to ~136000.
Let’s set those values as the white and black in the image:</p>
<p><strong>V<sub>new</sub> = (V<sub>old</sub> - Black)/(White - Black)</strong></p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas2.jpg" alt=""></p>
<center>Progress</center>
<p>Much better, but it’s still more monochromatic then I remember the tree being.
Camera sensors aren’t actually able to see color: They only measure how much light hit each pixel.</p>
<p>In a color camera, the sensor is covered by a grid of alternating <a href="https://en.wikipedia.org/wiki/Bayer_filter">color filters</a>:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas3.png" alt=""></p>
<p>Let’s color each pixel the same as the filter it’s looking through:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas4.png" alt=""></p>
<center>Bayer matrix overlay</center>
<p>This version is  more colorful, but each pixel only has one third of it’s RGB color.
To fix this, I just averaged the values each pixel with it’s neighbors:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas5.png" alt=""></p>
<center>Demosaicing results</center>
<p>Applying this process to the whole photo gives the lights some color:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas6.jpg" alt=""></p>
<center>Demosaiced tree</center>
<p>However, the image is still very dark.
This is because monitors don’t have as much dynamic range as the human eye, or a camera sensor:
Even if you are using an OLED, the screen still has some ambient light reflecting off of it and limiting how black it can get.</p>
<p>There’s also another, sneaker factor causing this:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/lin.png" alt=""></p>
<center>True linear gradient</center>
<p>Our perception of brightness is non-linear.</p>
<p>If brightness values are quantized, most of the ADC bins will be wasted on nearly identical shades of white while every other tone is crammed into the bottom.
Because this is an inefficient use of memory, most color spaces assign extra bins to darker colors:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/nonlin.png" alt=""></p>
<center>sRGB gradient</center>
<p>As a result of this, if the linear data is displayed directly, it will appear much darker then it should be.</p>
<p>Both problems can be solved by applying a non-linear curve to each color channel to brighten up the dark areas… but this doesn’t quite work out:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas7.jpg" alt=""></p>
<center>ohno</center>
<p>Some of this green cast is caused by the camera sensor being intrinsically more sensitive to green light, but some of it is my fault:
There are twice as many green pixels in the filter matrix.
When combined with my rather naive demosaicing, this resulted in the green channel being boosted even higher.</p>
<p>In either case, it can fixed with proper white-balance:
Equalize the channels by multiply each one with a constant.</p>
<p>However, because the image is now non-linear, I have to go back a step to do this.
Here’s the dark image from before with all the values temporarily scaled up so I can see the problem:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas8.jpg" alt=""></p>
<p>… here’s that image with the green taken down to mach the other channels:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas9.jpg" alt=""></p>
<center>Banishing the green</center>
<p>… and after re-applying the curve:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas10.jpg" alt=""></p>
<center>Finally: A decent photo.</center>
<p>This is really just the bare minimum:
I haven’t done any color calibration, the white balance isn’t perfect, there’s lots of noise that needs to be cleaned up…</p>
<p>Additionally, applying the curve to each color channel accidentally desaturated the highlights.
This effect looks rather good — and is what we’ve come to expect from film — but it’s has de-yellowed the star.
It’s possible to separate the luminance and curve it while preserving color.
On it’s own, this would make the LED Christmas lights into an overstaturated mess, but combining both methods can produce nice results.</p>
<p>For comparison, here’s the image my camera produced from the same data:</p>
<p><img src="https://maurycyz.com/misc/raw_photo/xmas0.jpg" alt=""></p>
<center>"in camera" JPEG image.</center>
<p>Far from being an “unedited” photo: there’s a huge amount of math that’s gone into making an image that nicely represents what the subject looks like in person.</p>
<p>There’s nothing that happens when you adjust the contrast or white balance in editing software that the camera hasn’t done under the hood.
The edited image isn’t “faker” then the original: they are different renditions of the same data.</p>
<p>In the end, replicating human perception is hard, and it’s made harder when constrained to the limitations of display technology or printed images.
There’s nothing wrong with tweaking the image when the automated algorithms make the wrong call.</p>


        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Researchers discover molecular difference in autistic brains (174 pts)]]></title>
            <link>https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/</link>
            <guid>46415129</guid>
            <pubDate>Sun, 28 Dec 2025 22:23:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/">https://medicine.yale.edu/news-article/molecular-difference-in-autistic-brains/</a>, See on <a href="https://news.ycombinator.com/item?id=46415129">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main tabindex="-1" id="page-container"><article><header><figure></figure></header><div><div><div><p>Yale School of Medicine (YSM) scientists have discovered a molecular difference in the brains of autistic people compared to their neurotypical counterparts. </p> <p>Autism is a neurodevelopmental condition associated with behavioral differences including difficulties with social interaction, restrictive or intense interests, and repetitive movements or speech. But it’s not clear what makes autistic brains different. </p> <p>Now, a new study in <a href="https://psychiatryonline.org/doi/10.1176/appi.ajp.20241084?url_ver=Z39.88-2003&amp;rfr_id=ori:rid:crossref.org&amp;rfr_dat=cr_pub%20%200pubmed" data-link-type="external" target="_blank"><em>The American Journal of Psychiatry</em></a> has found that brains of autistic people have fewer of a specific kind of receptor for glutamate, the most common excitatory neurotransmitter in the brain. The reduced availability of these receptors may be associated with various characteristics linked to autism.</p> <p>“We have found this really important, never-before-understood difference in autism that is meaningful, has implications for intervention, and can help us understand autism in a more concrete way than we ever have before,” says <a href="https://medicine.yale.edu/profile/james-mcpartland/" data-link-type="external" target="_blank">James McPartland, PhD</a>, Harris Professor of Child Psychiatry and Psychology in the Child Study Center at YSM and the study’s co-principal investigator. </p></div><section><h2>Signaling imbalance in autism</h2><div><p>Neurons in the brain communicate with one another using electrical signals and chemical messengers called neurotransmitters. When an electrical current propagates through a neuron, it prompts the release of neurotransmitters that relay a signal to other neurons. This signaling in the brain can be either excitatory or inhibitory. Excitatory signaling primarily triggers the release of the neurotransmitter glutamate, and it acts as a green light telling other neurons to fire. Inhibitory signaling, on the other hand, acts as a brake that suppresses activity. </p> <p>The brain needs a precise balance of these two types of signaling in order to function properly. One of the leading hypotheses on the underlying causes of autism is an imbalance of excitatory and inhibitory signaling in the brain. Researchers propose the involvement of this central mechanism might explain the wide range of differences observed among autistic individuals.<br></p></div></section><blockquote><p>Now, we’ve found something that is meaningful,  measurable, and different in the autistic brain.</p><a href="https://medicine.yale.edu/profile/james-mcpartland/" aria-label="James McPartland, PhD"><div><p><span>James McPartland, PhD</span></p><p>Harris Professor in the Child Study Center</p></div></a></blockquote><div><p>Based on this hypothesis, the researchers used magnetic resonance imaging (MRI) and positron emission tomography (PET) to look for differences in the brains of 16 autistic adults and 16 people considered neurotypical. MRI scans enabled the researchers to examine the anatomy of each of the participants’ brains, while PET scans revealed how the brains were functioning at the molecular level.</p> <p>“PET scans can help us pinpoint a molecular map of what’s going on in this glutamate system,” says <a href="https://medicine.yale.edu/profile/david-matuskey/" data-link-type="external" target="_blank">David Matuskey, MD</a>, associate professor of radiology and biomedical imaging at YSM, and co-principal investigator of the study.</p></div><section><h2>Autistic brains have reduced availability of a crucial receptor</h2><div><p>These analyses revealed less brain-wide availability of a specific kind of glutamate receptor, known as metabotropic glutamate receptor 5 (mGlu5) in autistic participants. The findings support the idea that an imbalance of excitatory and inhibitory signals in the brain could be contributing to traits associated with autism, the researchers say.<br></p> <p>Fifteen of the autistic participants also underwent an electroencephalogram (EEG), a measure of electrical activity of the brain. Based on the EEG, the researchers identified that these electrical measurements were associated with lower mGlu5 receptors.</p> <p>This finding could have significant clinical implications, the researchers say. While PET scans are a powerful tool for studying the brain, they are also costly and involve exposure to radiation. EEG could be a cheaper and more accessible way to further investigate excitatory function in the brain.</p> <p>“EEG isn’t going to completely replace PET scans, but it might help us understand how these glutamate receptors might be contributing to the ongoing brain activity in a person,” says <a href="https://medicine.yale.edu/profile/adam-naples/" data-link-type="external" target="_blank">Adam Naples, PhD</a>, assistant professor in the Child Study Center at YSM and the study’s first author.</p></div></section><blockquote><p>While many neurodivergent people aren’t hindered by autism  and may not need or want medication, novel treatments could help those on the  spectrum that experience symptoms that affect their quality of life.</p></blockquote><div><p>The study gives the researchers novel mechanistic insight into how the brains of autistic individuals are different from those of neurotypical people. Because the molecular underpinnings of autism are still so poorly understood, clinicians today rely on behavioral observation to diagnose it. Elucidating the “molecular backbone” of autism, researchers say, could potentially lead to better diagnostic tools and ways to support autistic people. </p> <p>“Today, I go into a room and play with a child to diagnose autism,” says McPartland, “Now, we’ve found something that is meaningful, measurable, and different in the autistic brain.”</p> <p>There are currently no medications that treat the difficulties experienced by many with autism. The findings could also help researchers come up with therapeutics for autism that target the mGlu5 receptor. While many neurodivergent people aren’t hindered by autism and may not need or want medication, novel treatments could help those on the spectrum that experience symptoms that affect their quality of life. </p></div><section><h2>Future research directions</h2><div><p>The current study only included autistic adults. It is still unclear whether the lower receptor availability is a driver of autism or a result of living with it for decades. Previously, research involving PET scans has been limited to adults due to the risks associated with radiation exposure. But Matuskey, co-investigator <a href="https://medicine.yale.edu/profile/richard-carson/" data-link-type="external" target="_blank">Richard Carson, PhD</a>, and their colleagues have developed <a href="https://medicine.yale.edu/news-article/powerful-new-brain-pet-scanner-is-opening-new-research-pathways/" data-link-type="external" target="_blank">more sophisticated techniques</a> that open a pathway for much lower exposure to radiation. </p> <p>In future studies, the team plans to conduct research with these new technologies in children and adolescents. “We want to start creating a developmental story and start understanding whether the things that we’re seeing are the root of autism or a neurological consequence of having had autism your whole life,” says McPartland. </p> <p>All autistic participants in the study had average or above average cognitive abilities. McPartland and collaborators are also working together on developing other approaches to PET scans that will enable them to include individuals with intellectual disabilities in future studies.</p></div></section></div><section><h2>Article outro</h2><div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><path d="M12.712 16.386 9 16.916l.53-3.716 9.546-9.546a2.25 2.25 0 1 1 3.182 3.186Z" style="fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px"></path><path d="M5.25 15h-3a1.5 1.5 0 0 0-1.5 1.5v3a1.5 1.5 0 0 0 1.5 1.5h19.5a1.5 1.5 0 0 0 1.5-1.5v-3a1.5 1.5 0 0 0-1.5-1.5h-3" style="fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px"></path></svg><h3>Author</h3></p></div><section><h3>Tags</h3><ul><li><a>Autism</a></li></ul></section><section><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" focusable="false"><rect width="22.5" height="15" x="0.747" y="2.25" rx="1.5" ry="1.5" style="fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px"></rect><path d="M19.5 20.856a9.6 9.6 0 0 0-15 0" style="fill:none;stroke:currentColor;stroke-linecap:round;stroke-linejoin:round;stroke-width:1.5px"></path></svg><h3>Media Contact</h3></p><p>For media inquiries, please contact us.</p></section></section><div><section id="explore-more-section-undefined" aria-label="Explore More"><div><h2 id="explore-more">Explore More</h2></div><div><p><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" focusable="false"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" clip-path="url(#people-avatar_svg__a)"><path d="M.416 10a9.583 9.583 0 1 0 19.166 0A9.583 9.583 0 0 0 .416 10"></path><path d="M3.299 16.85a21.3 21.3 0 0 1 4.51-1.96c.698-.258.774-1.857.274-2.408-.722-.794-1.333-1.724-1.333-3.971A3.177 3.177 0 0 1 10 5.038a3.177 3.177 0 0 1 3.25 3.473c0 2.25-.612 3.177-1.334 3.972-.5.55-.424 2.15.274 2.407a21.3 21.3 0 0 1 4.51 1.96"></path></g><defs><clipPath id="people-avatar_svg__a"><path fill="#fff" d="M0 0h20v20H0z"></path></clipPath></defs></svg><h3>Featured in this article</h3></p></div></section><div><h2 id="related-news">Related News</h2></div></div></div></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Dolphin Progress Report: Release 2512 (123 pts)]]></title>
            <link>https://dolphin-emu.org/blog/2025/12/22/dolphin-progress-report-release-2512/</link>
            <guid>46414916</guid>
            <pubDate>Sun, 28 Dec 2025 21:57:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dolphin-emu.org/blog/2025/12/22/dolphin-progress-report-release-2512/">https://dolphin-emu.org/blog/2025/12/22/dolphin-progress-report-release-2512/</a>, See on <a href="https://news.ycombinator.com/item?id=46414916">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
<img src="https://dolphin-emu.org/m/user/blog/progress-report/2512/progressreportheader2512.avif" alt="progressreportheader2512.avif"> 
<img src="https://dolphin-emu.org/m/user/blog/progress-report/2512/progressreportheader2512-mini.avif" alt="progressreportheader2512-mini.avif"> 
</header>

<p>With the holiday season reaching its apex, we have a few surprises for those of you that have been patiently waiting.  The latest release of Dolphin is stuffed with treats.  Our first present is presentation - <em>frame presentation</em>, that is.  Two new options have arrived and will help users both reduce latency and smooth out games that struggle with frame pacing.</p>
<p>Some games do outright naughty things that make emulation difficult.  A slew of them are being coerced onto the nice list this year thanks to a sack full of patches that bypass their troublesome behaviors.  Fans of the Broadband Adapter (BBA) have a great present tailored just to them: a new local mode BBA!  Designed for allowing multiple instances of Dolphin on the same computer to connect together, it's perfect for use with Parsec or other similar services.  And perhaps another gift will have you singing your favorite Wii hits?</p>
<p>But alas, what fun would the holiday season be if we spoiled all the gifts?  Read on to unwrap the latest edition of the Dolphin Progress Report.</p>
<p>...</p>
<p>...</p>
<p>Huh?  We've received word that apparently the <em>Android users</em> have made the nice list?  Really?  That can't be right... but this gift is addressed to them.</p>


<p>After <em>more than a couple</em> bumps in the road, RetroAchievements support has finally arrived on the Android version of Dolphin!  In Release 2512, the core achievement experience is now available in your pocket.  This initial version hasn't quite reached parity yet with the desktop experience, but we didn't want to hold things up any longer.  The important thing for Android RetroAchievements users is that you can log in and unlock achievements in <a href="https://retroachievements.org/system/16-gamecube/games">supported GameCube games</a>. Because some menus are incomplete, it may be best to have the <a href="https://retroachievements.org/">RetroAchievements website</a> open in the background for achievement lists and other things while we finish up the in-app UI.</p>

<h3 id="notable-changes"><strong>Notable Changes</strong><a href="#notable-changes" title="Permanent link">¶</a></h3>
<h4 id="2509-493-add-rush-frame-presentation-and-smooth-frame-presentation-options-by-billiard"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-493/">2509-493 - Add Rush Frame Presentation and Smooth Frame Presentation Options</a></strong>  by <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong><a href="#2509-493-add-rush-frame-presentation-and-smooth-frame-presentation-options-by-billiard" title="Permanent link">¶</a></h4>
<p>Latency was once a huge challenge for emulators, and it is still a major concern.  At one point not too long ago, it was pretty much infeasible for most emulators to match the latency of their console counterparts.  Compared to a dedicated game console racing the beam on a CRT television, emulators had to deal with sluggish OS window managers with on-by-default triple-buffer V-Sync holding three frames back, first-gen wireless controllers that added latency right at the source of input, slower displays that added several <a href="https://www.gamesindustry.biz/digitalfoundry-tech-focus-battle-against-latency">additional frames of latency</a> (much more if that display was a TV without game mode), and on top of everything the emulator still needed to do its job and take time to actually emulate everything.</p>
<p>Dolphin <em>just</em> missed out on the worst of this. By the time Dolphin's performance and compatibility were good enough for users to worry about things like latency, the overall situation had improved dramatically.  Low latency and high refresh rate monitors paired with features like <a href="https://dolphin-emu.org/blog/2014/07/31/dolphin-progress-report-july-2014/#40-2286-d3d-exclusive-fullscreen-by-armada651">Exclusive Fullscreen</a> removed most of the major bottlenecks that emulators had to fight against.</p>
<p>The designs of the GameCube and Wii <em>also</em> afford Dolphin some opportunities that other emulators don't have.  The GameCube/Wii are double buffer V-Sync'd by default, resulting in a final input latency of roughly 60ms on a CRT in an optimized 60fps title.  However, because of <a href="https://dolphin-emu.org/blog/2017/11/19/hybridxfb/">how the XFB Output Pipeline works</a>, Dolphin has the opportunity to bypass the buffers and grab those XFB copies early, and immediately present them directly to the screen. We call this feature <em>Immediately Present XFB</em>, and it cuts out quite a bit of the latency present on the console.</p>
<p>Tricks like these let Dolphin match console latency as long as the host device is capable enough.  On extremely optimal setups with low latency VRR monitors combined with <em>Immediately Present XFB</em>, Dolphin could even dip a frame <em>below</em> real console latency!</p>
<p>There are some caveats, though.  While <em>Immediately Present XFB</em> is a powerful tool for reducing latency, it is also a hack that relies on games behaving in a specific manner.  If a game messes with the XFB pipeline, such as if it applies post processing using the CPU, or stitches together multiple XFB Copies, the hack will cause Dolphin to output nonsensical garbage.</p>




<p>Even when <em>Immediate</em> isn't outright breaking the game, the XFB pipeline is a big part of how some games handle frame pacing, so bypassing it can make the game feel less smooth.  For the best experience in many games, a user probably would prefer superior latency <em>and</em> good frame pacing.</p>


<p>And that was <strong><a href="https://github.com/jordan-woyak">Billiard's</a></strong> goal.  He saw an opportunity to improve the situation and started work on two new options.  One feature was a way to reduce latency without disrupting how the game rendered, and the other would allow smoother frame pacing even in games that struggled on real console.  How would he accomplish these feats?  Well, by making the emulator throttle <em>smarter</em>.</p>
<hr>
<p>Modern computers are powerful enough to emulate <a href="https://wiki.dolphin-emu.org/index.php?title=Star_Wars_Rogue_Squadron_III:_Rebel_Strike"><em>most</em></a> GameCube and Wii games faster than the original hardware could run them. Disable the framelimiter and try it for yourself! To keep emulation on pace, Dolphin’s <em>Throttle</em> function stalls emulation to produce a <em>mostly</em> properly paced simulation of the original hardware. Throttling is necessary for playable emulation on modern systems, but making the host CPU wait <em>adds time</em>. If input from the user and stalls are aligned poorly, throttling can add a small amount of latency.</p>
<p>To address this, Billiard has added a new throttling mode called <em>Rush Frame Presentation</em>, where throttling becomes centered around presenting the frame as soon as it can after the input is read. In theory, this reduces the time between click and photon and can have a very noticeable effect, especially in lower frame rate titles.  To the end user, all of this is completely invisible.  All of this is happening <em>sub-frame</em>, so Dolphin still will throttle the appropriate amount of time to maintain the correct frame rate.</p>
<p>The faster your computer, the more of an effect this will have because it will be able to emulate the part of the frame faster.  We can easily catch the difference in <a href="https://wiki.dolphin-emu.org/index.php?title=The_Legend_of_Zelda:_The_Wind_Waker">The Legend of Zelda: Wind Waker</a> and <a href="https://wiki.dolphin-emu.org/index.php?title=Super_Mario_Sunshine">Super Mario Sunshine</a> by using a high speed camera, for example.</p>
<p>
<figure>
<video controls="" playsinline="">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/cropped-sms-av1.webm" alt="cropped-sms-av1.webm (AV1)" type="video/webm" codecs="av01.0.00M.08, opus">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/cropped-sms-vp9.webm" alt="cropped-sms-vp9.webm (VP9)" type="video/webm" codecs="vp9">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/cropped-sms-h264.mp4" alt="cropped-sms-h264.mp4 (h.264)" type="video/mp4">
</video>

<figcaption>Here we can visually see the reduced latency that Rush provides. This is somewhere between 8 - 14ms.<br><sub>Click/tap to Play.</sub></figcaption>
</figure>
</p>

<p>Some games will also see a <em>further</em> benefit by combining <em>Immediately Present XFB</em> with <em>Rush Frame Presentation</em>.  Unfortunately, this combination can lead certain games to looking gnarly as they will just be spitting out the image at whatever point they're finished rendering during a frame.  That's why the second new option is important.</p>
<h5 id="smoothing-things-out"><strong>Smoothing Things Out</strong><a href="#smoothing-things-out" title="Permanent link">¶</a></h5>
<p><em>Immediately Present XFB</em> could already cause poor frame pacing, and now <em>Rush Frame Presentation</em> could make it even worse.  In some games, it can get so poor that VRR monitors will fall out of their operating range!</p>
<p><em>Smooth Frame Presentation</em> allows Dolphin to delay presentation by roughly 1-2ms so that it can more consistently output frames, using previous frame times as a heuristic.  This option can be used in any game that has poor frame pacing in order to try to improve the situation.</p>


<p>The results of <em>Smooth Frame Presentation</em> are good enough that a lot of games that needed the XFB Output Pipeline enabled by default because of frame pacing issues can now take advantage of the lower input latency provided by <em>Immediately Present XFB</em> and <em>Rush Frame Presentation</em> without any noticeable side-effects.</p>
<p>Even if you're using neither of the latency features, some games just have bad frame pacing even on real console.  <em>Smooth Frame Presentation</em> can help them, too.  There are rare cases, especially when using <em>Rush Frame Presentation</em> alongside <em>Immediately Present XFB</em>, where a game's output will be so inconsistent that smoothing won't help.  We're looking at you, <a href="https://wiki.dolphin-emu.org/index.php?title=Dragon_Ball_Z:_Budokai">Dragon Ball Z: Budokai</a>.</p>


<h5 id="outside-verification"><strong>Outside Verification</strong><a href="#outside-verification" title="Permanent link">¶</a></h5>
<p>All of these results sound great, but outside of a few camera tests on lower frame rate games, we were mostly trusting latency offset numbers <em>provided by Dolphin</em>.  Testers did also report better latency, but given that the <a href="https://en.wikipedia.org/wiki/Placebo">placebo effect</a> exists and these are such small differences, we wanted more concrete data.  But other than just <em>game feel</em>, how do you test latency?</p>
<p>In the past, we've used the light sensor present on <a href="https://wiki.dolphin-emu.org/index.php?title=Rock_Band_3">Rock Band 3</a> guitars alongside an in-game synchronization test to get some very rough offset values.  The problem with that is that it will only ever test one game, and the guitar controller isn't particularly viable in most games outside of unusual controller runs.</p>
<p>Before making any claims about our latency, we wanted to do our due dilligence in respect to both Dolphin and real hardware.  To accomplish this, we contacted some professionals that have been fighting against latency for quite some time.  <strong><a href="https://github.com/JLaferri">Fizzi</a></strong> from <a href="https://slippi.gg/">Slippi.gg</a>  and adapter expert <strong><a href="https://github.com/JulienBernard3383279">Arte</a></strong> graciously donated their time and helped us measure latency in the latest version of Dolphin versus console.   <strong><a href="https://github.com/JulienBernard3383279">Arte</a></strong> <a href="https://www.input-integrity.com/">specifically developed a GameCube controller adapter with a photon sensor</a> designed to determine controller latency, which is rather convenient because <em>that's exactly what we want to measure</em>.</p>
<p>Their GameCube controller adapter polls the controller at 1000Hz, and a light sensor on it can be programmed to look for certain changes in output from the game.  By having access to both the source of the input and the change on the screen, the adapter can provide a real world measurement of how exactly how long it takes for a user input to result in a change on the display - what is commonly referred to as "click to photon".  As an added bonus, the adapter can also be hooked up to real console with no conversion or added latency, letting us compare directly with games running on real hardware and a CRT.</p>

<p>The exciting thing about these numbers is that they confirm our experience.  Dolphin's latency compares favorably to console.  To be fair to the GameCube, <strong><a href="https://github.com/JulienBernard3383279">Arte</a></strong>'s emulation setup included a modern low-latency 144Hz monitor and the lowest latency controller adapter.   Dolphin couldn't quite compete with Slippi, but most of that can be attributed to deep modifications to how <a href="https://wiki.dolphin-emu.org/index.php?title=Super_Smash_Bros._Melee">Super Smash Bros. Melee</a> outputs.</p>
<p>For all the samples above, an input bug present in the original game has been patched out.  This is to make getting consistent results a little bit easier.  That fix did mean that combining <em>Rush Frame Presentation</em> and <em>Immediately Present XFB</em> no longer benefited that title when testing.  However, <strong><a href="https://github.com/JulienBernard3383279">Arte</a></strong> modified the test to work with other games, and some games respond incredibly well when combining Rush and Immediate.</p>

<!--
Immediate: 105.9 / 105.8 / 110.2 / 110.2
Immediate + RFP: 96.5 / 99.6 / 99.2 / 96.8
-->

<!-- The below is the windwakerlatency.js backup. If we can get that working, comment this out instead
<div class="media-block wider"> 
<figure>
<img src="https://dolphin-emu.org/m/user/blog/progress-report/2512/windwakerlatency-lowchart.png" alt="windwakerlatency-backupchart.png">
<figcaption></figcaption>
</figure>
</div>
--->

<p>Due to time constraints with holiday vacations, adjusting the photon sensor for different games, we weren't able to gather all of the numbers we wanted from other games.  However, with <a href="https://wiki.dolphin-emu.org/index.php?title=The_Legend_of_Zelda:_The_Wind_Waker">Wind Waker</a> the numbers were interesting enough that we managed to get enough samples <em>right under the wire</em>, at least in Dolphin.  The graph above shows latency with default settings, <em>Immediately Present XFB</em> and the combined efforts of <em>Immediately Present XFB</em> and <em>Rush Frame Presentation</em>.  Note that the default settings were measured last, and as such the error range is estimated.</p>
<p>The reason why we wanted to squeeze in this particular case is because it demonstrates that combining <em>Rush Frame Presentation</em> and <em>Immediately Present XFB</em> can result in lower latency than what was possible before.  This is not always true, and this 10ms reduction is from an ideal example.  Unmodified <a href="https://wiki.dolphin-emu.org/index.php?title=Super_Smash_Bros._Melee">Melee</a>, for instance, showed the combination reducing latency by less than 4ms.  Some games saw no benefit from combining both features together.</p>
<p>In the end, how much these two new options will help varies greatly depending on the game and setup.  Currently, we've left them both disabled by default in the Configuration -&gt; Advanced Tab, but that may change as the settings get more testing and we gauge what users want the most.</p>
<h4 id="2509-74-gamecube-add-sdl-stock-profile-by-samb"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-74/">2509-74 - GameCube - Add SDL Stock Profile</a></strong>  by <strong><a href="https://github.com/Sam-Belliveau">Samb</a></strong><a href="#2509-74-gamecube-add-sdl-stock-profile-by-samb" title="Permanent link">¶</a></h4>
<p>Unlike the wacky Wii Remote and its mess of attachments, the GameCube Controller mostly mirrors modern controllers.  Sure, the sticks can't be pushed in, it's missing a shoulder button, the two stage analog+digital triggers can be tricky, and the face buttons are weird. But at the end of the day, it's a four face button, twin stick controller with a D-pad. Close enough?</p>
<p>So we have added an SDL Profile that players can use to speed up their GameCube controller mapping. </p>


<p>Using the profile is simple:</p>
<ol>
<li>Go the GameCube "Standard Controller" mapping window.</li>
<li>Select your controller from the Device dropdown. Pick the variant that starts with "SDL".</li>
<li>Select the <code>SDL Gamepad (Stock)</code> Profile and click Load.</li>
<li>Optional: Calibrate your joysticks (please don't skip this Hall effect users!) and set up deadzone.</li>
<li>Enjoy.  </li>
</ol>
<p>Since the GameCube controller doesn't map 1:1 to modern controllers, this is a <em>best guess</em> stock profile that will <em>reasonably</em> work for most people, most controllers, and most games. If a game demands a button combination that doesn't work with your hands on your controller, or if the face buttons or any other button simply aren't to your liking, you can build from the stock profile and adjust everything until it's just right.</p>
<h4 id="2509-237-and-2509-339-add-option-to-reset-settings-back-to-default-by-joshuavandaele-and-simonx22"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-237/">2509-237</a></strong> and <strong><a href="https://dolphin-emu.org/download/dev/master/2509-339/">2509-339</a></strong> - <strong>Add Option to Reset Settings Back to Default</strong> by <strong><a href="https://github.com/JoshuaVandaele">JoshuaVandaele</a></strong> and <strong><a href="https://github.com/Simonx22">Simonx22</a></strong><a href="#2509-237-and-2509-339-add-option-to-reset-settings-back-to-default-by-joshuavandaele-and-simonx22" title="Permanent link">¶</a></h4>
<p>Dolphin is a complicated emulator with a lot of options.  Some of it is definitely our fault, and some of it is just the reality of trying to emulate something as complicated as the GameCube and Wii.  To the average user, a lot of these settings aren't immediately obvious, even with descriptions.</p>
<p>"<em>Emulated Memory this?  EFB, XFB that, VBI what?</em>"</p>
<p>The most experienced users (and even developers) can sometimes get frustrated enough with an issue that <a href="https://en.wiktionary.org/wiki/percussive_maintenance">percussive maintenance</a> is necessary, leading to one changing lots of settings around until <em>something</em> happens.  Sometimes this works, leading to a temporary moment of joy, before it all comes crashing down when none of your other games will boot.  Some people might be able to backtrack and figure out what went wrong, but a lot of users are left lost.</p>
<p>Until recently, users had to delete the Dolphin settings files from their computer to restore everything to default.   No one liked that, but a button to reset settings was non-trivial thanks to <a href="https://dolphin-emu.org/blog/2017/07/01/dolphin-progress-report-june-2017/#50-4171-videoconfig-port-to-layered-configuration-system-by-merrymage">Dolphin's multiple layers of settings</a> that are a handful to manage.</p>
<p>Thankfully, <strong><a href="https://github.com/JoshuaVandaele">JoshuaVandaele</a></strong> was finally able to sort out all of the implementation details and give us the long awaited "Reset All Settings" button.</p>


<p>Thanks to <strong><a href="https://github.com/Simonx22">Simonx22</a></strong>, Android users also get access to this feature.  It can be found in the advanced settings menu in the Android GUI.</p>
<h4 id="2509-217-gamepatch-modify-certain-games-to-behave-better-in-dolphin-by-supersamus-with-additional-contributors"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-217/">2509-217 - GamePatch:  Modify Certain Games to Behave Better in Dolphin</a></strong>  by <strong><a href="https://github.com/SuperSamus">SuperSamus</a></strong> with additional contributors<a href="#2509-217-gamepatch-modify-certain-games-to-behave-better-in-dolphin-by-supersamus-with-additional-contributors" title="Permanent link">¶</a></h4>
<p>Some games are more strenuous to emulate than others.  Sometimes it's because the <a href="https://wiki.dolphin-emu.org/index.php?title=The_Last_Story">game pushes the console</a>, and other times it's because the game does something <a href="https://dolphin-emu.org/blog/2025/09/16/dolphin-progress-report-release-2509/#2506-29-game-patch-defang-the-disney-trio-of-destruction-by-josjuice-and-billiard">annoying to emulate - maliciously or not.</a></p>
<p>The latter situation can be especially frustrating, as sometimes 99% of what the game does is fine, with just <em>one</em> little behavior, design decision, or sometimes even an underlying bug causing problems for Dolphin.</p>
<p><strong><a href="https://github.com/SuperSamus">SuperSamus</a></strong> identified a few common game behaviors that causes Dolphin a lot of headaches.  Properly fixing these issues would difficult, with some requiring large rewrites to the emulator that probably will never come.  Fortunately, there is a simpler solution: patch out the difficult to emulate game behaviors.  These small patches increase emulation performance, and sometimes even work around other unwanted behaviors.</p>
<h5 id="complex-idle-loops"><strong>Complex Idle Loops</strong><a href="#complex-idle-loops" title="Permanent link">¶</a></h5>
<p>Idle loops are a sequence of instructions that makes the CPU run laps doing nothing while it waits for something to happen.  Obviously, emulating a CPU burning cycles doing nothing isn't efficient for an emulator, so one of Dolphin's earliest optimizations was the ability to detect and skip these idle loops. This feature is fairly standard among emulators and is called <em>Idle Skipping</em>.</p>
<p>Idle loops are more common than you might think. It's very rare for a game to absolutely max out the console's CPU, so <em>for the majority of frames</em> the CPU will idle a bit. And as long as Dolphin can detect the idle loops, they can be skipped to increase overall performance.  <em>Idle Skipping</em> is one of the key things that make some areas of a game more demanding than others, especially on the CPU side of things.  Essentially, <em>Idle Skipping</em> is less effective the more that the CPU has to do.</p>
<p>Dolphin can't detect all idle loops, and the scope of what it can find is rather limited.  We have to find as many idle loops as accurately as possible while also ensuring that we don't invest too much time and resources into finding them.  The more complex the heuristic is, the more of an impact it will have on the JIT.  As such, some games have annoying idle loops that we know about but are not worth detecting.</p>
<p><a href="https://wiki.dolphin-emu.org/index.php?title=Need_for_Speed:_Nitro">Need for Speed: Nitro</a> and <a href="https://wiki.dolphin-emu.org/index.php?title=Rayman_Raving_Rabbids">Rayman Raving Rabbids</a> are two such games with these complex idle loops. After examining their code in detail, <strong><a href="https://github.com/SuperSamus">SuperSamus</a></strong> realized that their idling behaviors could be modified with a patch that would allow Dolphin to more easily detect and skip them.</p>
<p>This results in <em>massive</em> performance boosts, especially in lighter areas, with some menus running at four times as fast.  Of course, heavier areas see less of a benefit. We'll have a chart showing some of the raw numbers at the end.</p>
<h5 id="running-uncapped"><strong>Running Uncapped</strong><a href="#running-uncapped" title="Permanent link">¶</a></h5>
<p>Dolphin is not a cycle accurate emulator.  In fact, Dolphin is fundamentally not designed to be cycle accurate, especially with GPU operations. Dolphin's emulated GPU was designed to be <strong>infinitely fast</strong>, only being limited by other factors like CPU emulation or the maximum performance of the host device. Nowadays, it has been tamed a bit with synchronization points that provide a rough approximation of the timings of the original <a href="https://dolphin-emu.org/blog/2025/09/16/dolphin-progress-report-release-2509/#processor-processing">command processor</a>, but it is still by no means accurate, let alone <em>cycle</em> accurate. Also, rendering is still infinitely fast, as the only thing that limits it is the speed of your host GPU.</p>
<p>And yet, most games run at the correct frame rate, because they <em>limit themselves</em>.</p>
<p>The GameCube and Wii were designed for analog TVs, so they used the analog television's sign to start a new frame as a synchronization point called the <a href="https://dolphin-emu.org/blog/2023/02/12/dolphin-progress-report-december-2022-january-2023/#50-18271-video-hack-vbi-skip-by-sam-belliveau">Vertical Blanking Interupt (VBI)</a>. All a game had to do was start a new frame every time it saw a VBI and finish the frame before the next VBI, and it would be perfectly synchronized to the frame rate of the display. That's it. It was simple and efficient, so the vast majority of GameCube and Wii library tie their frame rates to the VBI. Thanks to that, Dolphin's early developers didn't even need to care about how fast the emulated GPU runs; as long Dolphin emits VBIs at the correct frequency, most games will just run at the correct frame rate regardless of what's going on under the hood. And this gives Dolphin bonuses like not emulating GPU slowdown, allowing games that struggled on console to perform much better in Dolphin.</p>
<p>But the GameCube and Wii don't have operating systems. Games run on the bare metal and have full control of the machine, so developers could do <em>whatever they wanted</em>.  And some games eschew the VBI and run <strong>uncapped</strong>.</p>
<p>Uncapped games break Dolphin's assumptions. They can render way, <em>way</em> too fast.  For example, when running <a href="https://wiki.dolphin-emu.org/index.php?title=Hulk">Hulk (2003)</a> in Dolphin, it only displays at 60 FPS, but the physics engine could be doing hundreds of steps per second behind the scenes.  This is cool because technically a game like this can easily be hacked to run at higher frame rates, but <em>terrible</em> because it hammers people's devices with all kinds of unnecessary work!</p>
<p>It gets even worse from here.  The physics engine's independence from the output frame rate isn't <em>perfect</em>.  If the number of steps per second gets too high, small rounding and math errors start to accumulate, and parts of the game not properly tuned to run at these higher frame rates start to break.  This mostly results in dialogue timing issues, but in one stage halfway through the game, it causes a physics calculation issue where a required ledge can't be climbed, essentially softlocking the player.</p>


<p>Some other games choose to synchronize to the VBI, but don't bother in incredibly simple scenes where the frame rate doesn't matter.  This is most commonly seen with splash screens and loading screens.  One such game with this behavior is <a href="https://wiki.dolphin-emu.org/index.php?title=Bully:_Scholarship_Edition">Bully: Scholarship Edition</a>. Most of the time it is perfectly stable, but because the loading screens are uncapped, it can actually cause the game to randomly hang on transitions.  <a href="https://wiki.dolphin-emu.org/index.php?title=The_Simpsons_Hit_%26_Run">The Simpsons Hit &amp; Run</a> also has this issue, but only during the initial load.  While most desktop computers are able to handle the initial load relatively well, Android users have reported tremendous slowdowns where the loading time would take more than 45 seconds.</p>
<p><strong><a href="https://github.com/SuperSamus">SuperSamus</a></strong> identified these problems and either created patches for these behaviors themselves, or helped others with those titles create patches.  Most of these patches are only one or two lines and simply limit the game's frame rate to the VBI frequency.  The patches prevent the game from slamming Dolphin's overpowered emulated GPU, greatly increasing performance while fixing issues caused by the games running internally at too high of a frame rate.</p>
<p>This comes with a little bonus - since they are now bound to the VBI frequency, you can now use <em>VBI Frequency Override</em> to adjust their frame rate up or down as desired, allowing Dolphin to take advantage of the fact that these games "support" running at higher frame rates.</p>
<p>In addition to the games above that had emulation bugs caused by framelimiting, <strong><a href="https://github.com/SuperSamus">SuperSamus</a></strong> and <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong> also created limiter patches for the following games purely for performance reasons:</p>
<ul>
<li><a href="https://wiki.dolphin-emu.org/index.php?title=Conduit_2">Conduit 2</a></li>
<li><a href="https://wiki.dolphin-emu.org/index.php?title=Driver:_San_Francisco">Driver: San Francisco</a></li>
<li><a href="https://wiki.dolphin-emu.org/index.php?title=Monsters_Inc._Scream_Arena">Monsters Inc. Scream Arena</a></li>
<li><a href="https://wiki.dolphin-emu.org/index.php?title=Tetris_Worlds">Tetris Worlds</a></li>
</ul>
<!--
BEFORE/AFTER PATCHES PERF

AMD 9950x3D RTX 4090 4x IR

Need For Speed Nitro: Main Menu:  Before 236, After 806
Driver: San Francisco: City: Before 86, After 210
Conduit 2: - Before: 142 fps, After 264 fps
Hulk(2003): Before: 179, After: 511 fps
-->

<center><p>Somewhat ironically, by slowing down uncapped games we improve their performance in Dolphin, so when turning off Dolphin's framelimiter they go much faster. ...ignore that and use this as a measure of their performance improvement. If a game's frame rate doubled in this chart, then the performance required to run that game at fullspeed has been roughly halved.</p></center>

<p>These numbers don't tell the whole story on some of these games.  <a href="https://wiki.dolphin-emu.org/index.php?title=Hulk">Hulk (2003)</a> and other games which ran uncapped would get progressively worse performance in Dolphin depending on how <em>light</em> the scene was to render.</p>
<p>As a final reminder, these patches should not be considered proper solutions to fixing uncapped games.  They are purely to increase playability of these titles.</p>
<h5 id="eggmania-eggstream-madness"><strong>Eggmania: Eggstream Madness</strong><a href="#eggmania-eggstream-madness" title="Permanent link">¶</a></h5>
<p>The <em>Force Progressive Output</em> patch we included for the Japanese version of this game was causing it to crash on boot.  Since we were adding new patches and <strong><a href="https://github.com/extrems">extrems</a></strong> had already pointed us to a <em>correct</em> version of the patch, we decided to update it alongside all of the other patches.</p>
<p>Despite being the Japanese version of a rather obscure game, this crash was somehow reported multiple times by different users.  For the two of you out there waiting, the fix is finally here.</p>
<h4 id="2509-242-bba-ipc-for-bba-between-multiple-instances-of-dolphin-on-the-same-machine-by-cristian64"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-242/">2509-242 - BBA:  IPC for BBA Between Multiple Instances of Dolphin on the Same Machine</a></strong>  by <strong><a href="https://github.com/cristian64">cristian64</a></strong><a href="#2509-242-bba-ipc-for-bba-between-multiple-instances-of-dolphin-on-the-same-machine-by-cristian64" title="Permanent link">¶</a></h4>
<p>The Broadband Adapter (BBA) for the GameCube was all about connecting games to a network. Whether across the hall with a Local Area Network (LAN) or across the continent with the information superhighway, the BBA brought Nintendo consoles together like nothing before it! Well, except the <a href="https://dolphin-emu.org/blog/2024/04/30/dolphin-progress-report-february-march-and-april-2024/#50-21253-implement-modem-adapter-by-fuzziqersoftware">Modem Adapter</a>, but we're not talking about that today.</p>
<p>Dolphin has supported the BBA emulation for many years.  An early LLE implementation was fairly accurate, but required the user to setup TAP servers and suffered from performance bottlenecks with said TAP servers depending on the operating system.  It wasn't until <a href="https://dolphin-emu.org/blog/2022/09/13/dolphin-progress-report-july-and-august-2022/#50-16838-add-hle-broadband-device-by-schthack-and-many-additional-fixes-by-sepalani">BBA-HLE</a> in 2022 that BBA emulation became readily accessible to the average user.</p>


<p>BBA-HLE is great if you want to connect Dolphin to another computer or real hardware.  But if you want to run multiple instances <em>on the same computer</em>, things get more complicated.  While it was technically possible through networking trickery, it was far beyond what we'd expect of the average user.</p>
<p><strong><a href="https://github.com/cristian64">cristian64</a></strong> realized that this problem could be pretty cleanly solved by using a library called <code>cpp-ipc</code>.  This library would allow <em>separate instances</em> of Dolphin on the same machine to share data easily and efficiently through Inter-Process Communication (IPC).  Instead of using a network stack, we can just simulate the instances of Dolphin being in their very own network and let them communicate without the need for any outside support. </p>
<p>With BBA-IPC added, here's a quick rundown of the many options you have for BBA.</p>
<ul>
<li><strong>Broadband Adapter (TAP)</strong>: Dolphin's LLE solution for the Broadband Adapter that requires a TAP interface.</li>
<li><strong>Broadband Adapter (XLink-Kai)</strong>:  LLE solution that can connect to the Xlink-Kai service to allow players on separate networks to connect together.  Success highly depends on each game's latency tolerance and the latency between the players.</li>
<li><strong>Broadband Adapter (HLE)</strong>:  HLE solution that hooks the Broadband Adapter up to the host's network interface to connect with other devices on the network or to a server for certain online games.</li>
<li><strong>Broadband Adapter (IPC)</strong>:  HLE solution that allows Dolphin instances on the same machine to share memory and communicate directly without the need for a host network.</li>
</ul>
<p>BBA-IPC allows for easy testing of BBA features on a single PC, and can also be used in conjunction with game streaming services like Parsec to play BBA titles over the internet without needing to meet the strict latency requirements of emulating the BBA over the internet. This is admittedly a rather niche usecase for BBA, but the feature is relatively compact and easy to maintain.</p>
<p>Currently, only Windows and Linux are supported by BBA-IPC, but if there is enough interest, <strong><a href="https://github.com/cristian64">cristian64</a></strong> has already found another library that could let us support this in other operating systems.</p>
<h4 id="2509-250-iptop-make-inetaton-async-by-sepalani"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-250/">2509-250 - IP/Top: Make InetAToN async</a></strong>  by <strong><a href="https://github.com/sepalani">sepalani</a></strong><a href="#2509-250-iptop-make-inetaton-async-by-sepalani" title="Permanent link">¶</a></h4>
<p>Playing <a href="https://wiki.dolphin-emu.org/index.php?title=Mario_Kart_Wii">Mario Kart Wii</a> online in Dolphin back when WFC support was first added was a rather rough experience.  It was functional, but slow and stuttery, just enough to be playable and help players with preserving traffic to the official WFC in the final months before the servers were finally shut down.</p>
<p>But that wasn't the end.  Thanks to revival efforts, many Wii games still have online communities, with <a href="https://wiki.dolphin-emu.org/index.php?title=Mario_Kart_Wii">Mario Kart Wii</a> being quite possibly the biggest.  In the years since, Dolphin's Wi-Fi emulation has gotten to the point that users can play alongside real Wiis in most games without any issues.</p>
<p>Well, <em>most</em> users.  Unfortunately, even in modern builds of Dolphin there were a couple of users having severe stuttering and freezing problems, and all of them were using Android devices.  These problems were handwaved away as typical Android performance issues at first, but after closer examination, it was obvious that there was something else going wrong.  One user recorded us tons of examples and helped narrow down what was happening.</p>
<p>
<figure>
<video controls="" playsinline="">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/mariokartjoinhang-av1.webm" alt="mariokartjoinhang-av1.webm (AV1)" type="video/webm" codecs="av01.0.00M.08, opus">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/mariokartjoinhang-vp9.webm" alt="mariokartjoinhang-vp9.webm (VP9)" type="video/webm" codecs="vp9">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/mariokartjoinhang-h264.mp4" alt="mariokartjoinhang-h264.mp4 (h.264)" type="video/mp4">
</video>

<figcaption>For one user, the pauses were long enough to lock up the emulator altogether.<br><sub>Click/tap to Play. File has audio.</sub></figcaption>
</figure>
</p>

<p>They (accurately!) surmized that it had something to do with a player joining or leaving the online lobby.  In <a href="https://wiki.dolphin-emu.org/index.php?title=Mario_Kart_Wii">Mario Kart Wii</a>, players can join or leave a race mid-match, with new players spectating until the next race began.  The only mystery left was figuring out why this was only happening to a couple of users while everyone else was fine.</p>
<p>Using their wealth of knowledge from working on the <a href="https://wiki.dolphin-emu.org/index.php?title=Monster_Hunter_Tri">Monster Hunter Tri</a> <a href="https://forum.wii-homebrew.com/index.php/Thread/60432-Monster-Hunter-Tri-private-server-progress/">replacement Wi-Fi servers</a>, <strong><a href="https://github.com/sepalani">sepalani</a></strong> jumped in and investigated the issue.  Same as us, they couldn't find any issues at first. They were able to play without any kind of freezing, even on their phone.  However, after unlocking the frame rate on a desktop computer, <strong><a href="https://github.com/sepalani">sepalani</a></strong> noticed a small hitch when the function <code>InetAToN</code> was called.  This hitch was impossible to see when running at normal speed, but when running at 1000%+ speed, the dip was just barely noticeable.</p>
<p><code>InetAToN</code> is a standard networking function that takes an IP address string and converts it into its equivalent binary form. For example, the string <code>192.51.100.50</code> would be transformed into the hexadecimal number <code>0xC0336432</code>. On the Wii, <code>InetAToN</code> has some additional functionality that is unusual: in addition to text-based IPs, it also accepts <em>hostnames</em> as inputs. This means that a developer can choose to pass a hostname like <code>google.com</code> into <code>InetAToN</code>, and the function will perform a <a href="https://www.cloudflare.com/en-us/learning/dns/what-is-dns/">Domain Name System</a> (DNS) lookup to resolve the hostname into an IP address. </p>
<p><strong><a href="https://github.com/sepalani">sepalani</a></strong> figured out that if a hostname was provided to <code>InetAToN</code> and the resulting DNS lookup was slow enough, the function would cause a stutter because Dolphin waits for the result of the lookup before continuing. The time it takes for a DNS lookup to finish varies depending on various factors, such as the user's internet connection. On a desktop PC connected to home internet, the lookup would complete before the user even saw a frame drop. But many of our Android users are connected to the internet via cellular data, which can have particularly poor latency depending on signal strength and network congestion.</p>
<p>Therefore, <strong><a href="https://github.com/sepalani">sepalani</a></strong> changed <code>InetAToN</code> to be non-blocking so that DNS resolution can take as long as it wants without locking up the emulator.</p>
<h4 id="2509-481-sdio-fix-csdcid-emulation-by-naim2000"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-481/">2509-481 - SDIO: Fix CSD/CID emulation</a></strong>  by <strong><a href="https://github.com/Naim2000">Naim2000</a></strong><a href="#2509-481-sdio-fix-csdcid-emulation-by-naim2000" title="Permanent link">¶</a></h4>
<p>There's no way to sugarcoat this one.  This was a pretty bad %$#&amp; up.  Dolphin's SD card emulation was broken, and has been broken for a very long time.  Yet somehow, <em>it worked</em>. This is one of the dangers of emulation - sometimes you can do something very wrong yet the software just trudges on.  </p>
<p>The modern Wii homebrew scene lives off of SD Cards.  It's a convenient tool available on <em>most</em> Wiis, and can be used in conjunction with the <a href="https://wiki.dolphin-emu.org/index.php?title=Homebrew_Channel">Homebrew Channel</a>, BootMii, and other essential homebrew.  It's not uncommon to see a 32GB SD card in a Wii loaded with homebrew emulators, software, and game mods.</p>
<p>One oddity about the Wii is that it was released <em>without</em> support for the then new <a href="https://en.wikipedia.org/wiki/SD_card#SDHC">SDHC standard</a> (2GB to 32GB SD cards).  This would be rectified in <a href="https://wiibrew.org/wiki/System_Menu_4.0">an update three years later</a>, but the damage was done.  <a href="https://hackmii.com/2009/02/why-the-wii-will-never-get-any-better/">Thanks to the way Nintendo designed the Wii's software stack</a>, games released prior to SDHC support being added would be forced to use older IOSes that lacked SDHC support.  Even if your System Menu, homebrew, and newer games worked with your big SD card, a game like <a href="https://wiki.dolphin-emu.org/index.php?title=Super_Smash_Bros._Brawl">Super Smash Bros. Brawl</a> wouldn't.  At least without some help.</p>
<p>The <a href="https://wiki.dolphin-emu.org/index.php?title=Super_Smash_Bros._Brawl">Brawl</a> community refused to be restricted by Nintendo's paltry limitations. They used homebrew to load the game with newer IOS versions and even patched the game to remove the nonsensical 3 minute time limit on replays.  Small patches like these were just the beginning, as the community started making bigger changes to the game, like balance patches Brawl+ and Brawl-, and eventually full game overhauls like Project M and its offspring.</p>
<p>The Smash Bros. community is still pushing the limits of homebrew on both the Wii and Dolphin.  One such effort is <a href="https://www.rexbuild.site/">Super Smash REX</a>.  REX has an astounding amount of stages, music, and characters to the point that just the base mod is over <em>8GB</em> in size.  And that's where we come into all of this.</p>
<p>A developer from REX reached out to us about how they were reaching some kind of limit with the virtual SD card in Dolphin. Once the amount of data on the SD card exceeded <strong>10.7GiB</strong>, Dolphin's SD card support would completely fall apart.</p>


<p>This actually wasn't <em>too surprising</em>.  There have been scattered reports from <a href="https://wiki.dolphin-emu.org/index.php?title=Rock_Band_3">Rock Band 3</a> and <a href="https://wiki.dolphin-emu.org/index.php?title=Category:Just_Dance_(Series)">Just Dance</a> failing when too much DLC was installed to the SD card.  However, reproducing those issues required owning a mountain of DLC, which none of us did.  With REX, everything was readily available and the actual launcher was just a homebrew application.  This made reproducing the issue much easier, and they did most of the work for us.</p>
<p>The key detail was that the files on the SD card didn't even have to relate to the mod.  We could just fill it with cat pictures, load up the <a href="https://wiki.dolphin-emu.org/index.php?title=Homebrew_Channel">Homebrew Channel</a> or any other homebrew that relied on the SD card, and watch the fireworks.  What could be going wrong?</p>
<hr>
<p>In a community as old as the GameCube/Wii scene, there are some voices that <em>must</em> be heeded.  Before the REX developers reached out to us, one of the Supreme GameCube/Wii Sages, <strong><a href="https://github.com/extrems">extrems</a></strong>, hadst been prognosticating doom since the new year if a bug with the card ID (CID) and card-specific data (CSD) registers in our SD card emulation was not mended.  Verily, it came to pass.</p>


<p>The problem was extremely straightforward.  When the emulated Wii queried the virtual SD card for its capabilities, such as size and speed, Dolphin would return <em>complete garbage</em> due to the reports being in the wrong byte order.  The strange part of this was that SD card emulation even worked <em>at all</em>.</p>
<p>When REX reported there were SD card issues, we quickly thought of the flaws that <strong><a href="https://github.com/extrems">extrems</a></strong> pointed out to us.  However, knowing the problem and fixing it were two very different things.  Our attempts to make things right only made things worse.  Two different developers took a stab at it, and both left SD card emulation completely non-functional.  We were left defeated and efforts on the problem slowed.</p>
<p>In mid-November, <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong> was perusing <s>Pull Requests</s> ancient tomes of knowledge that fell through the cracks. One such tome was named "SDIO: report write lock status".  Emulating a SD card's write lock switch was a rather unimportant implementation detail that would not affect emulation.  As such, there was no rush to review it.  But when <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong> did finally get to reviewing it, he saw that it <em>also fixed Dolphin's CID and CSD byte ordering</em>.</p>
<p>A few quick reviews and a rebase later, and the bug was finally quelled.  Now Dolphin properly works with virtual SD cards up to 32GB in size.</p>
<p> 
<figure>
<a href="https://dolphin-emu.org/m/user/blog/progress-report/2512/projectrexfixed.mp4">
<video muted="" autoplay="" loop="" playsinline="">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/projectrexfixed-av1.webm" alt="projectrexfixed-av1.webm (AV1)" type="video/webm" codecs="av01.0.00M.08, opus">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/projectrexfixed-vp9.webm" alt="projectrexfixed-vp9.webm (VP9)" type="video/webm" codecs="vp9">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/projectrexfixed-h264.mp4" alt="projectrexfixed-h264 (h.264)" type="video/mp4">
</video>
</a>
<figcaption>This is how Super Smash REX's intro is supposed to look.</figcaption>
</figure>
</p>

<h4 id="2509-542-usb-emulated-support-for-logitech-microphone-for-wii-by-biendeo"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-542/">2509-542 - USB:  Emulated Support for Logitech Microphone for Wii</a></strong>  by <strong><a href="https://github.com/Biendeo">Biendeo</a></strong><a href="#2509-542-usb-emulated-support-for-logitech-microphone-for-wii-by-biendeo" title="Permanent link">¶</a></h4>
<p>The Logitech Microphone is an iconic accessory for the Nintendo Wii.  <a href="https://wiki.dolphin-emu.org/index.php?title=Category:USB_Microphone_(Input_supported)">Roughly 100 games</a>, including popular titles in the <a href="https://wiki.dolphin-emu.org/index.php?title=Category:Guitar_Hero_(Series)">Guitar Hero</a> and <a href="https://wiki.dolphin-emu.org/index.php?title=Category:Rock_Band_(Series)">Rock Band</a> series, support the peripheral.</p>
<p>Dolphin has had support for the <em>physical</em> Logitech Microphone via USB Passthrough for years, but even as other USB peripherals received their emulated counterparts, those wanting to sing into an emulated Wii using generic microphones had to wait.  <a href="https://dolphin-emu.org/blog/2025/06/04/dolphin-progress-report-release-2506/#2503-580-wii-speak-emulation-by-shuffle2-degasus-noahpistilli-and-sepalani">Even the maligned Wii Speak received support</a> before the Logitech Microphone, despite the fact that it supported far fewer games and was much more complicated to implement.  This is just how emulation works sometimes. The more complicated, less useful accessory was just far more interesting than the popular, yet generic accessory.</p>
<p>But the foundation created for emulating the Wii Speak did lend itself well toward implementing a second microphone accessory in Dolphin.  After all, the Wii Speak <em>was</em> a microphone, so a few people took shots at adapting the Wii Speak code to work with the Logitech Microphone.</p>
<p>First, <a href="https://github.com/supermilkdude67">supermilkdude67</a> posted a WIP fork with very basic support that fizzled out.  A few months later, <a href="https://retroachievements.org/forums/topic/32317">a certain announcement</a> brought renewed interest toward making the many singing games more accessible to users.  <strong><a href="https://github.com/Biendeo">Biendeo</a></strong> took over the mantle using the initial fork as a base.  With some help from veteran Dolphin developers, a few fixes, and some upgrades to the GUI, it was ready to go.</p>


<p>You can now emulate the Logitech Microphone with any standard PC microphone!  The exact volume levels might need to be adjusted depending on the input source, so make sure you properly calibrate before your first jam session. Given that this is a new feature, compatiblity isn't perfect, and some games may take more fiddling than others.</p>
<p>For our Android users, things aren't ready yet.  While the core feature should be mostly compatible between the two environments, it will need a completely different GUI.  As such, it might be a while before everything gets ported over to our Android builds.</p>
<h4 id="2509-406-on-screen-display-add-new-default-font-by-trytwo"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-406/">2509-406 - On-Screen Display:  Add New Default Font</a></strong>  by <strong><a href="https://github.com/TryTwo">TryTwo</a></strong><a href="#2509-406-on-screen-display-add-new-default-font-by-trytwo" title="Permanent link">¶</a></h4>
<p>Dolphin's On-Screen Display is an important part of communicating with users, whether it's statistics, performance metrics, or notifications from features like RetroAchievements.  Unfortunately, the pixel font we were using could be hard to read, especially for some users with HiDPI screens.  <strong><a href="https://github.com/TryTwo">TryTwo</a></strong> improved the situation by <a href="https://dolphin-emu.org/download/dev/master/2509-59/">adding the ability to change the font size of on-screen display messages</a>, but this didn't solve the problem - making a pixel font larger can look quite bad, especially at non-integer scales.  Dolphin needed a new font that could scale to arbitrary resolutions while still looking clear.</p>
<p>With this change, Dolphin's OSD now uses a proper vector font!</p>


<p>Vera Sans Mono is the new default font.  We think that in pretty much every scenario, it's easier to read than our old font.  However, <strong><a href="https://github.com/TryTwo">TryTwo</a></strong> figured that since we were adding a font anyway, that Dolphin could just provide the ability for users to override the font.  So if you're unsatisfied with our typography tastes, add the font of your choice to the "Load" folder of your User directory and name it "OSD_Font.ttf". Any TrueType font will work!</p>


<p>Now that the On-Screen Display has even more options, we've decided to consolidate them within their own config section.  So if you're looking to adjust what is displayed, font size, or if you want them disabled altogether, the settings can now be found in one place within the On-Screen Display section of the Configuration window.</p>
<h4 id="2509-554-ax-hle-fix-low-pass-filter-edge-case-by-flacs"><strong><a href="https://dolphin-emu.org/download/dev/master/2509-554/">2509-554 - AX-HLE: Fix Low Pass Filter Edge Case</a></strong>  by <strong><a href="https://github.com/tilka">flacs</a></strong><a href="#2509-554-ax-hle-fix-low-pass-filter-edge-case-by-flacs" title="Permanent link">¶</a></h4>
<p>If you were an arcade junkie in the late 90s and early 2000s, then you're probably familiar with the Midway classic <em><a href="https://wiki.dolphin-emu.org/index.php?title=Category:NFL_Blitz_(Series)">NFL Blitz</a></em>.  Authentic teams thrust into a deliberately inaccurate simulation with colorful gameplay and flashy graphics made the series a bombastic hit in arcades.  And of course, the blisteringly difficult AI that would <em>absolutely cheat</em> drained quarters from avid players, making it enticing for arcade owners as well.</p>


<p>In this, we're not talking about beloved or hated arcade games, but instead the home console exclusive <a href="https://wiki.dolphin-emu.org/index.php?title=NFL_Blitz_Pro">NFL Blitz Pro</a>.  This attempt to adapt to the console landscape was a commercial failure that lacked the charm and personality of the earlier titles.  And we can confirm the popularity thing - this title had broken audio by default in Dolphin for <em>over four years</em> with no one making a formal bug report and only two forgotten comments throughout our community.</p>
<p>It wasn't until <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong> was testing games to see if they worked with <em>Immediately Present XFB</em> that we became aware that the game's sound was broken.</p>


<p>After doing some quick testing, <strong><a href="https://github.com/jordan-woyak">Billiard</a></strong> realized that DSP-LLE resolved the issue, and with no further leads he made a setting adjustment and <a href="https://dolphin-emu.org/download/dev/master/2509-551/">disabled HLE audio for this game by default in 2509-551</a>.</p>


<p>The very next day, <strong><a href="https://github.com/tilka">flacs</a></strong> saw the audio regression, found what build broke it, figured out why it was broken, <em>and</em> fixed the issue.  <a href="https://wiki.dolphin-emu.org/index.php?title=NFL_Blitz_Pro">NFL Blitz Pro</a> is another game that uses the low-pass filter.  In Dolphin, the low-pass filter was notoriously broken and left <a href="https://dolphin-emu.org/blog/2021/08/01/dolphin-progress-report-june-and-july-2021/#50-14712-dsp-hle-re-enable-low-pass-filter-by-flacs-original-fix-by-merrymage">completely disabled for many, many years</a>.  During the process in which the feature was finally fixed and re-enabled, no one thought to test this game. If we had, we would have noticed that it hit an edge-case in the HLE implementation of the low-pass filter.</p>
<p>When the game reserves a region of memory, its memory allocator sets all bytes within the region to <code>0xAB</code>.  This behavior isn't necessarily abnormal, as the initial state of the memory shouldn't matter. When using freshly allocated memory, it is best practice to first initialize it with sane default values before attempting to use it. Unfortunately, the developers of <a href="https://wiki.dolphin-emu.org/index.php?title=NFL_Blitz_Pro">NFL Blitz Pro</a> forgot to perfom this initialization when reserving memory for the game's audio engine. The <a href="https://www.ign.com/articles/1999/07/30/the-musyx-experience">MusyX library</a> determines if the low-pass filter is enabled by checking if the appropriate bytes within its assigned memory region are not zero.  Because <code>0xAB</code> is not zero, the library assumes that the low-pass filter should be enabled.</p>
<p>Because most audio related memory is still in the default state of <code>0xAB</code>, the two low pass filter coefficients are <em>also</em> set to <code>0xABAB</code>.  When DSP-HLE goes to calculate how much the low-pass filter should quiet or amplify things, it adds these values together.  For our purposes, the values are interpreted by the game as 16-bit fixed point values, so the coefficients would be roughly <code>1.341156</code>.  These two values <em>added together</em> are supposed to add up to about <code>1.0</code>, but in this case the result is roughly <code>2.682312</code>.  DSP-HLE sees this number and boosts the volume accordingly, making things sound rather unpleasant.</p>
<p> 
<figure>
<audio controls="">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/nfl-blitz-pro_broken-filter.mp3" type="audio/mpeg"> Your browser does not support the audio tag. </audio> 
<figcaption>It's a little loud.</figcaption>
</figure>
</p>
<p>The programmers were using uninitialized memory by accident, which is a game bug.  Regardless of their intent, these were the values that the game used.  So why was Dolphin broken?  <strong><a href="https://github.com/tilka">flacs</a></strong> figured out that the second filter value is actually a <em>signed</em> value.  For a typical 16-bit fixed point value, this means positive values are <code>0x0000</code> to <code>0x7FFF</code>, and negative values are <code>0x8000</code> to <code>0xFFFF</code>.  <code>0xABAB</code> is supposed to be interpreted as a <em>negative</em> value.</p>
<p>With the bug fixed, the equation changes to <code>1.34 + (-0.66)</code>, giving us a coefficient sum of roughly <code>0.68</code>.  The filter is still active and now <em>lowering</em> the volume of the game, but this is accurate to real hardware. It seems that the developers worked around the filter's unintentional activation by just making the game louder to compensate.  By handling these uninitialized values correctly, DSP-HLE now produces proper audio in this title.</p>
<p> 
<figure>
<audio controls="">
<source src="https://dolphin-emu.org/m/user/blog/progress-report/2512/nfl-blitz-pro_correct-filter.mp3" type="audio/mpeg">Your browser does not support the audio tag. </audio> 
<figcaption>With the filtering corrected, we can now enjoy NFL Blitz Pro's sporting and uplifting football music in DSP-HLE once again.</figcaption>
</figure>
</p>
<hr>
<h3 id="this-releases-contributors"><strong>This Release's Contributors...</strong><a href="#this-releases-contributors" title="Permanent link">¶</a></h3>
<p>Special thanks to <a href="https://github.com/dolphin-emu/dolphin/graphs/contributors?from=2025-09-15&amp;to=2025-12-21&amp;type=c">all of the contributors</a> that incremented Dolphin by 585 commits after Release 2509!</p>

<hr>


<!-- tooltip code below -->







    
    
    

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Unity's Mono problem: Why your C# code runs slower than it should (239 pts)]]></title>
            <link>https://marekfiser.com/blog/mono-vs-dot-net-in-unity/</link>
            <guid>46414819</guid>
            <pubDate>Sun, 28 Dec 2025 21:41:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/">https://marekfiser.com/blog/mono-vs-dot-net-in-unity/</a>, See on <a href="https://news.ycombinator.com/item?id=46414819">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>Execution of C# code in Unity’s Mono runtime is slow by today’s standards, much slower than you might expect! Our game runs 2-3x faster on modern .NET compared to Unity’s Mono, and in a few small benchmarks I measured speedups of up to 15x. I’ve spent some time investigating what’s going on and in this article I will present my findings and why everyone should want Unity’s .NET modernization to become production-ready as soon as possible.</p>
		
<h2 id="How-did-we-get-here">How did we get here</h2>

<p>
	Unity uses the Mono framework to run C# programs and back in 2006 it was one of the only viable multi-platform implementations of .NET.
	Mono is also open-source, allowing Unity to do some tweaks to better suit game development.
</p>
<p>
	An interesting twist happened nearly 10 years later.
	In 2014, Microsoft began open-sourcing .NET (notably .NET Core later that year) and in June 2016, .NET Core 1.0 shipped with official cross-platform support.
	Since then, the .NET ecosystem gained momentum and lots of improvements have been made, including the Roslyn compiler platform, a new JIT (just-in-time compiler), performance improvements, more features, etc.
</p>
<p>
	In 2018, <a href="https://xoofx.github.io/blog/2018/04/06/porting-unity-to-coreclr/" target="_blank">Unity engineers discussed</a> that they are working on porting the engine to .NET CoreCLR, the multi-platform version of Common Language Runtime (CLR), a component that runs .NET programs.
	Their main motivations behind this project were performance and convergence.
	In their post they said:
</p>
<blockquote>...CoreCLR could be great for Unity game developers, as it will provide a significant boost in performance, by an order of 2x to 5x compare to the Mono runtime sometimes up to x10 on some workload!</blockquote>
<p>
	Unfortunately, now it’s the end of 2025 and we still can’t run games on CoreCLR.
</p>


<h2 id="The-performance-gap">The performance gap</h2>

<p>
	We don’t hear about the performance gap between Mono and .NET much, likely because it is not possible to run games written for Unity under modern .NET.
	But we can still do a direct comparison with code that does not depend on Unity directly.
</p>
<p>
	Our game has a unique architecture – we strictly separate the game simulation code (business logic) from rendering.
	So much so that the simulation code does not depend on Unity’s libraries and can be compiled and run under any .NET version.
</p>
<p>
	One day I was debugging an issue in map generation and it was time-consuming because it was taking over 2 minutes to start a game.
	To make debugging faster, I’ve written a unit test, hoping to cut down on the turn-around time since Unity takes 15+ seconds just to crunch new DLLs and reload the domain before the game can be launched and it also initializes rendering stuff that I did not care about.
	When I ran the test, it finished in 40 seconds.
	I was quite surprised that it was more than 3x faster, so I started digging deeper.
</p>
<p>
	Long story short, <a href="#figure-1" title="Go to figure 1: A trace of game startup in Unity and .NET, Debug mode.">Figure 1</a> shows traces from a profiler showing the difference between the game launching in Unity running under Mono vs. a unit test running under .NET.
</p>
<p>
	<i>Note that all shown benchmarks are using either Unity 6.0 or .NET 10.</i>
</p>

<div id="figure-1"><p><span>Figure 1: A trace of game startup in Unity and .NET, Debug mode.</span></p></div>

<p>
	So our benchmark shows that loading a save file, generating a map, and initializing the simulation takes 100 seconds in Unity/Mono but only 38 seconds in .NET.
	This result alone is already something that may raise eyebrows and has real consequences of how you may want to approach debugging and testing.
</p>
<p>
	I also know from experience with Unity that Release mode running as a standalone executable (without the Unity editor) is much faster, so I decided to test that next.
</p>

<h2 id="NET-vs-Mono-in-standalone-Release-mode">.NET vs. Mono in standalone Release mode</h2>
<p>
	Debug mode slowness is not great, but even non-optimized C++ code can be slow.
	To compare the real performance gap between Mono and .NET, let’s run the same benchmark as above but in release mode, standalone executable.
</p>
<p>
	First up: Unity. I’ve run our deploy script to get an optimized executable and run it directly.
	Unsurprisingly, optimized standalone executable is beating Unity editor by a big margin, more than 3x faster.
	Next, the same code running under .NET in Release mode.
	<a href="#figure-2" title="Go to figure 2: A trace of game startup in Unity and .NET, Release mode">Figure 2</a> shows the results.
</p>
<div id="figure-2"><p><span>Figure 2: A trace of game startup in Unity and .NET, Release mode</span></p></div>

<p>
	Yep. 12 seconds.
	It’s actually mind-boggling how much work is being done in these 12 seconds and when I saw this for the first time, I was not only shocked, but also impressed.
	Just so you know, a 4k × 4k map is being generated using all available threads out of hundreds of combined noise functions in like 3 seconds.
	<a href="#figure-3" title="Go to figure 3: The expanded trace of the 12-second run from above. The blue boxes after the highlighted area is the actual unit test, stepping the game.">Figure 3</a> shows the trace expanded.
</p>
<p><a href="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/img/DotNet-release-detail.982.png" title="The expanded trace of the 12-second run from above. The blue boxes after the highlighted area is the actual unit test, stepping the game." data-size="2272x1619"><img src="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/img/DotNet-release-detail-w719-h512.982.png" width="719" height="512" alt="The expanded trace of the 12-second run from above. The blue boxes after the highlighted area is the actual unit test, stepping the game." srcset="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/img/DotNet-release-detail-w1437-h1024.982.png 2x"></a><span>Figure 3: The expanded trace of the 12-second run from above. The blue boxes after the highlighted area is the actual unit test, stepping the game.</span></p>
<p>
	If you are interested in seeing the actual x86 assembly generated by Mono and .NET JITs, see the Extras section at the end of this article.
</p>


<h2 id="Conclusion">Conclusion</h2>

<p>
	As you can see from the presented benchmarks, Mono is massively behind .NET in terms of performance.
	This is primarily due to differences in runtime optimizations and JIT that generates unoptimized assembly.
	The actual speedup surely depends on the code itself, but from my research, 1.5-3x speedup of C# execution is very likely for most projects.
</p>
<p>
	If you are a game developer using Unity, or even a player, you can now understand that <b>CoreCLR would be a massive boost to performance of games and even the Unity editor</b>.
	Unfortunately, for the past 8 years, Unity leadership was more interested in “other things” and did not give .NET modernization the attention it deserves.
</p>
<p>
	Some view .NET modernization as support for new language features in C#, but that is just a cherry on top.
	New C# adds some handy features, but the new JIT can deliver multi-x speedups.
</p>
<p>
	At this year's Unite conference, <a href="https://www.youtube.com/watch?v=rEKmARCIkSI&amp;t=1502s" target="_blank">Unity announced</a>
	that CoreCLR is still ongoing but it won’t be production ready in 2026.
	The good news is that it now seems to be on the Unity 6.x roadmap, and not left for later versions as suggested by 2024’s Unite presentation.
</p>
<div id="figure-4"><p><span>Figure 4: Slides from Unite 2025 showing that .NET Modernization is planned for 6.x release with unannounced date.</span></p></div>
<p>
	Moreover, CoreCLR is not just new JIT and C#, it unlocks broader and better-optimized support for things like Span&lt;T&gt;-style APIs, hardware intrinsics, and newer SIMD paths that devs cannot use these days.
	These features could add another multiplier to the performance gains for some classes of code.
	For example, our map generator heavily uses 2D and 3D simplex noise.
	I bet that having access to new runtime features in CoreCLR could speed up the map generation by another 2x.
</p>
<p>
	Unity has a technology called Burst that automatically converts marked C# methods to optimized native assembly via the LLVM compiler.
	This sounds neat as it can avoid the poor JIT performance, but the downside is that Burst has strict limitations on what can be converted and supports only subset of C#.
	I believe that CoreCLR with modern JIT will have very similar performance characteristics to Burst.
	I am curious what would happen in a universe where Unity invested all the time and effort in CoreCLR support and high-performance C#, instead of developing and maintaining Burst.
</p>
<p>
	Another interesting consequence of CoreCLR support is the ability to pre-compile the .NET intermediate assembly to machine code using ahead-of-time compilation (AOT).
	AOT can further improve startup time and is essential on platforms where JIT is restricted (notably iOS).
	Nowadays, Unity solves this with IL2CPP that takes the intermediate code and compiles it to C++ which is then optimized and compiled to native assembly.
	However, <a href="https://discussions.unity.com/t/the-unity-engine-roadmap-unite-2025/1696495/65" target="_blank">according to RichardFine (Unity staff)</a>,
	using CoreCLR AOT is not planned and IL2CPP is here to stay:
</p>
<blockquote>AOT for IL2CPP is completely independent of AOT for CoreCLR (which we have no plans to adopt anyway).
GC behaviour on IL2CPP improves when we upgrade the GC there, it’s not really affected by CoreCLR at all.</blockquote>
<p>
	In conclusion, CoreCLR won’t magically fix every bottleneck in a Unity game, but it does fix many of the code generation inefficiencies and allows writing higher-performance code.
	The benchmark presented in this article is meant to illustrate that modern .NET has spent years squeezing more work into fewer CPU cycles, and Unity users are largely locked out of those gains today.
</p>
<p>
	If Unity can deliver production-ready CoreCLR support, it won’t just mean “newer C#”.
	It will mean faster runtime performance, faster iteration times, more performance headroom, no domain reload, 
	better GC behavior, and maybe even more managed code and less native code. 
	Until then, the gap will remain an invisible tax on every Unity project that leans on managed code.
</p>
<p>
	I’m cheering for you, Unity devs, CoreCLR for the win!
</p>
<p><img src="https://marekfiser.com/blog/mono-vs-dot-net-in-unity/img/CoreClrForTheWin.606.png" width="613" height="433" alt="CoreCLR in Unity engine for the win!"></p>

<p>
	I have actually dug much deeper into the performance aspects of Mono vs .NET but for the sake of this article not being too long, here is a brief summary.
</p>
<p>
	<a href="#code-1" title="Go to code listing 1: Simple benchmark code">Code listing 1</a> shows the testing code.
	It does some basic summing of custom structs that are wrappers around ints.
	This is an interesting example because Mono is very bad at inlining and simplifying expressions, even obvious ones, and we have plenty of structs like these in our code base (e.g. Quantity, MechPower, Tile2i, etc).

</p>

<div id="code-1"><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td><pre><span>static</span> <span>class</span> Program {

	<span>static</span> <span>void</span> Main() {
		Console.WriteLine(RunTest(<span>int</span>.MaxValue));
	}

	<span>public</span> <span>static</span> TestStruct RunTest(<span>int</span> iterations) {
		TestStruct value1 = <span>new</span> TestStruct(iterations % 2);
		TestStruct value2 = <span>new</span> TestStruct(iterations % 7);
		TestStruct value3 = <span>new</span> TestStruct(iterations % 13);

		TestStruct result = <span>default</span>;

		<span>for</span> (<span>int</span> i = 0; i &lt; iterations; ++i) {
			result += value1 + value2;
			result += value1 + value3;
		}

		<span>return</span> result;
	}

}

<span>readonly</span> <span>struct</span> TestStruct {

    <span>public</span> <span>readonly</span> <span>int</span> Value;

    <span>public</span> TestStruct(<span>int</span> value) {
        Value = value;
    }

    <span>public</span> <span>static</span> TestStruct <span>operator</span> +(TestStruct lhs, TestStruct rhs) {
        <span>return</span> <span>new</span> TestStruct(lhs.Value + rhs.Value);
    }

    <span>public</span> <span>override</span> <span>string</span> ToString() =&gt; Value.ToString();

}</pre></td></tr></tbody></table></code></p></div>

<p>
	To obtain assembly code, I’ve compiled the code in Release mode and ran it as a standalone executable.
	Then, I attached a debugger to the running process. An easy way to find this loop was to make it long/infinite and just break the program at any time, it would end up in that loop.
</p>
<p>
	First, let’s take a look at .NET. Here is the x64 assembly of the for-loop section of the code.
</p>
<div id="code-2"><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
</pre></td><td><pre><span>add</span> r8d,edx
<span>add</span> edx,r10d
00007FFDEC338E88:
    <span>mov</span>  r10d,r8d
    <span>add</span>  r9d,r10d
    <span>mov</span>  r10d,edx
    <span>add</span>  r9d,r10d
    <span>inc</span>  ecx
    <span>cmp</span>  ecx,eax
    <span>jl</span>   00007FFDEC338E88</pre></td></tr></tbody></table></code></p></div>
<p>
	In both cases, the full loop of <code>int.MaxValue</code> iterations took around 750 ms on my machine.
</p>
<p>
	This looks neat. Even if you don’t read assembly, you can see that there are two add instructions, one decrement, and one jump.
	It seems that the JIT hoisted the invariant sums <code>a = value1 + value2</code> and <code>b = value1 + value3</code> out of the loop and then just accumulates them.
</p>
<p>
	I also tested x86 assembly, and it looks very similar:
</p>
<div id="code-3"><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
</pre></td><td><pre>082E18D0:
    <span>lea</span>  ebx,[esi+edi]
    <span>add</span>  eax,ebx
    <span>lea</span>  ebx,[esi+edx]
    <span>add</span>  eax,ebx
    <span>dec</span>  ecx
    <span>jne</span>  082E18D0</pre></td></tr></tbody></table></code></p></div>
<p>
	Interestingly, the loop direction was reversed, counting down.
	This saves one instruction as comparison to zero and conditional jump can be done as one instruction.
</p>
<p>
	Now let’s look at Mono’s x64 assembly.
</p>
<div id="code-4"><p><code><table><tbody><tr><td><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre></td><td><pre>1E87D2F3E20:
    <span>movsxd</span>  rax,dword ptr [rsp+0C0h]
    <span>mov</span>     dword ptr [rsp+40h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+0B8h]
    <span>mov</span>     dword ptr [rsp+38h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+40h]
    <span>mov</span>     dword ptr [rsp+0A0h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+38h]
    <span>mov</span>     dword ptr [rsp+98h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+0A0h]
    <span>movsxd</span>  rcx,dword ptr [rsp+98h]
    <span>add</span>     eax,ecx
    <span>mov</span>     dword ptr [rsp+90h],0
    <span>mov</span>     dword ptr [rsp+90h],eax
    <span>mov</span>     dword ptr [rsp+30h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+0A8h]
    <span>mov</span>     dword ptr [rsp+88h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+30h]
    <span>mov</span>     dword ptr [rsp+80h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+88h]
    <span>movsxd</span>  rcx,dword ptr [rsp+80h]
    <span>add</span>     eax,ecx
    <span>mov</span>     dword ptr [rsp+78h],0
    <span>mov</span>     dword ptr [rsp+78h],eax
    <span>mov</span>     dword ptr [rsp+0A8h],eax
    <span>mov</span>     dword ptr [rsp+28h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+0C0h]
    <span>mov</span>     dword ptr [rsp+20h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+0B0h]
    <span>mov</span>     dword ptr [rsp+18h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+20h]
    <span>mov</span>     dword ptr [rsp+70h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+18h]
    <span>mov</span>     dword ptr [rsp+68h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+70h]
    <span>movsxd</span>  rcx,dword ptr [rsp+68h]
    <span>add</span>     eax,ecx
    <span>mov</span>     dword ptr [rsp+60h],0
    <span>mov</span>     dword ptr [rsp+60h],eax
    <span>mov</span>     dword ptr [rsp+10h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+28h]
    <span>mov</span>     dword ptr [rsp+58h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+10h]
    <span>mov</span>     dword ptr [rsp+50h],eax
    <span>movsxd</span>  rax,dword ptr [rsp+58h]
    <span>movsxd</span>  rcx,dword ptr [rsp+50h]
    <span>add</span>     eax,ecx
    <span>mov</span>     dword ptr [rsp+48h],0
    <span>mov</span>     dword ptr [rsp+48h],eax
    <span>mov</span>     dword ptr [rsp+0A8h],eax
    <span>inc</span>     esi
    <span>cmp</span>     esi,7FFFFFFFh
    <span>jl</span>      1E87D2F3E20</pre></td></tr></tbody></table></code></p></div>
<p>
	As you can see just from the number of instructions, this code will run way slower.
	The full loop of <code>int.MaxValue</code> iterations took around 11500 ms, that’s ~15x slower.
</p>
<p>
	In the assembly you can see the four add instructions in the loop, the “inefficient” increment + comparison + jump (instead of decrement + conditional jump), and most importantly a sea of mov instructions, which are just memory copies from inefficient inlining of the struct fields.
	Basically Mono is just tossing values around memory.
</p>
<p>
	I have also tested assembly compiled in Debug mode running in the Unity editor and it’s even worse.
	The full loop takes 67 seconds (67000 ms)! In Unity Editor, the JIT likely switches to far less optimized codegen and includes additional checks/sequence-point overhead, which balloons runtime.
</p>
<p>
	Takeaway: modern .NET’s JIT can scalarize tiny value types and hoist invariant work so the hot loop becomes a handful of register ops, while Mono often fails to do so and ends up shuffling values through memory, exactly the kind of gap that shows up as slowdowns in real simulation-heavy code.
</p>

	</div></div>]]></description>
        </item>
    </channel>
</rss>