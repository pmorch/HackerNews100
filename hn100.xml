<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 28 Oct 2023 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google agrees to invest up to $2B in OpenAI rival Anthropic (106 pts)]]></title>
            <link>https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</link>
            <guid>38048155</guid>
            <pubDate>Sat, 28 Oct 2023 08:24:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/">https://www.reuters.com/technology/google-agrees-invest-up-2-bln-openai-rival-anthropic-wsj-2023-10-27/</a>, See on <a href="https://news.ycombinator.com/item?id=38048155">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="primary-image" role="figure" aria-describedby="primary-image-caption"><figure><div data-testid="Image"><p><img src="https://cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg" srcset="https://www.reuters.com/resizer/R4Z2uz8RcluFRA-5t_A4vBPD5ZQ=/480x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 480w,https://www.reuters.com/resizer/0JGEgq7x2OviOX_SI0PEgvGT6O4=/960x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 960w,https://www.reuters.com/resizer/lzEztbC-1tYEiIpLH_S6m41FrYQ=/1080x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1080w,https://www.reuters.com/resizer/Ht-xjY7xrUmaianFeFUDU9ssJ_U=/1200x0/filters:quality(80)/cloudfront-us-east-2.images.arcpublishing.com/reuters/ATYPQN5I4VP2NNBWZUDSFQ5TD4.jpg 1200w" sizes="(min-width: 1024px) 560px, (min-width: 1440px) 700px, 100vw" width="5342" height="3027" alt="An illuminated Google logo is seen inside an office building in Zurich"></p></div><p data-testid="Body"><span>An illuminated Google logo is seen inside an office building in Zurich, Switzerland December 5, 2018. REUTERS/Arnd Wiegmann/File Photo <a data-testid="Link" href="https://www.reutersagency.com/en/licensereuterscontent/?utm_medium=rcom-article-media&amp;utm_campaign=rcom-rcp-lead" target="_blank" referrerpolicy="no-referrer-when-downgrade"> Acquire Licensing Rights</a></span></p></figure></div><div><p data-testid="paragraph-0">Oct 27 (Reuters) - Alphabet's <a data-testid="Link" href="https://www.reuters.com/markets/companies/GOOGL.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(GOOGL.O)</a> Google has agreed to invest up to $2 billion in the artificial intelligence company Anthropic, a spokesperson for the startup said on Friday.</p><p data-testid="paragraph-1">The company has invested $500 million upfront into the OpenAI rival and agreed to add $1.5 billion more over time, the spokesperson said.</p><p data-testid="paragraph-2">Google is already an investor in Anthropic, and the fresh investment would underscore a ramp-up in its efforts to better compete with Microsoft <a data-testid="Link" href="https://www.reuters.com/markets/companies/MSFT.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(MSFT.O)</a>, a major backer of ChatGPT creator OpenAI, as Big Tech companies race to infuse AI into their applications.</p><p data-testid="paragraph-3">Amazon.com <a data-testid="Link" href="https://www.reuters.com/markets/companies/AMZN.O" target="_blank" referrerpolicy="no-referrer-when-downgrade">(AMZN.O)</a> also said <a data-testid="Link" href="https://www.reuters.com/markets/deals/amazon-steps-up-ai-race-with-up-4-billion-deal-invest-anthropic-2023-09-25/" referrerpolicy="no-referrer-when-downgrade">last month</a> it would invest up to $4 billion in Anthropic to compete with growing cloud rivals on AI.</p><p data-testid="paragraph-4">In Amazon's quarterly report to the U.S. Securities and Exchange Commission this week, the online retailer detailed it had invested in a $1.25 billion note from Anthropic that can convert to equity, while its ability to invest up to $2.75 billion in a second note expires in the first quarter of 2024.</p><p data-testid="paragraph-5">Google declined to comment, and Amazon did not immediately respond to a Reuters request for comment.</p><p data-testid="paragraph-6">The Wall Street Journal earlier reported the news of Google's latest agreement with Anthropic.</p><p data-testid="paragraph-7">The rising number of investments shows ongoing maneuvering by cloud companies to secure ties with the AI startups that are reshaping their industry.</p><p data-testid="paragraph-8">Anthropic, which was co-founded by former OpenAI executives and siblings Dario and Daniela Amodei, has shown efforts to secure the resources and deep-pocketed backers needed to compete with OpenAI and be leaders in the technology sector.</p><p data-testid="Body">Reporting by Krystal Hu in New York and Chavi Mehta in Bengaluru; Additional reporting by Jeffrey Dastin; Editing by Anil D'Silva, Devika Syamnath and Chris Reese</p><p data-testid="Body">Our Standards: <a data-testid="Link" href="https://www.thomsonreuters.com/en/about-us/trust-principles.html" target="_blank" referrerpolicy="no-referrer-when-downgrade">The Thomson Reuters Trust Principles.</a></p><div><address><p data-testid="Body">Krystal reports on venture capital and startups for Reuters. She covers Silicon Valley and beyond through the lens of money and characters, with a focus on growth-stage startups, tech investments and AI. She has previously covered M&amp;A for Reuters, breaking stories on Trump's SPAC and Elon Musk's Twitter financing. Previously, she reported on Amazon for Yahoo Finance, and her investigation of the company's retail practice was cited by lawmakers in Congress. Krystal started a career in journalism by writing about tech and politics in China. She has a master's degree from New York University, and enjoys a scoop of Matcha ice cream as much as getting a scoop at work. </p></address></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The fun factor of the video game Uplink (168 pts)]]></title>
            <link>https://vertette.github.io/post/funfactoruplink</link>
            <guid>38047861</guid>
            <pubDate>Sat, 28 Oct 2023 07:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vertette.github.io/post/funfactoruplink">https://vertette.github.io/post/funfactoruplink</a>, See on <a href="https://news.ycombinator.com/item?id=38047861">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <p>Remember family computers? Before we had tablets, middle class families would buy overpriced computers from dodgy computer stores that the whole family got to share. As they were mostly used by children and adults who didn't have the slightest understanding of technology, it didn't take long for the family computer to down to a crawl, plagued by dodgy Kazaa downloads and suspicious Internet Explorer toolbars. Between a lack of money to buy a better computer, no easy way to buy digital games in Europe for the longest time and no nearby stores that sold computer games, it took until my brother moved out of the home before I really started getting into PC gaming.</p>
<p>My mom bought a gaming PC for my 14th birthday, and while it was incredibly overpriced for what it was, that didn't matter to me; it was mine and I could do whatever I wanted with it. As the only computer games I was familiar with were <a href="https://www.newgrounds.com/games">free Flash games on Newgrounds</a>, I asked around on various communities what kind of games I could play on a slightly outdated hunk of junk like mine. I forgot who exactly was responsible, but someone recommended I try this old hacking game called <a href="https://store.steampowered.com/app/1510/Uplink/"><em>Uplink</em></a>, and so I found a torrent on ThePirateBay and tried it. I make the following statement with zero hyperbole - that single throwaway recommendation changed my life. Various hours later, I fell in love with it so hard that I made a Steam account just so I could buy the game and support the developers properly.</p>
<p><img src="https://vertette.github.io/img/uplink_hardware.png" alt="A screenshot of the Uplink hardware upgrade screen">
<em>Uplink's interface might be outdated and slightly janky, but dammit, it still looks very cool.</em></p>
<p>If you're unfamiliar with it, here's how it works: <em>Uplink</em> is a hacking simulator reminiscent of old hacking movies like <a href="https://www.imdb.com/title/tt0105435/"><em>Sneakers</em></a> and <a href="https://www.imdb.com/title/tt0113243/"><em>Hackers</em></a>, where the portrayal is less about realism and more about flashiness. You play as a hacker who does various odd jobs like changing people's identities or destroying valuable data. At the beginning of the game, hacking is as easy as using the password breaker on a password screen and finishing up in less than five minutes to avoid getting caught, but the game quickly starts bombarding you with new concepts - deleting logs to avoid being tracked down, shutting down security systems that get in your way and travelling through local area networks. The game is never outright unfair, as most information you need to get through the game can be found in the in-game help section, but certain concepts require a bit of trial and error before you truly get how they work and that can result in you getting caught by the authorities, which results in an instant game over. Thankfully, starting over and getting back to where you were before isn't as daunting as it seems due to <em>Uplink</em>'s fairly open structure. While the game is a bit on the short side, there's enough depth to its mechanics to feel satisfying to master, and the realization that a game that gave you so much trouble at first has turned into a total cakewalk can't be matched.</p>
<p>Before <em>Uplink</em>, I only really played games like <em>Mario</em>, <em>Grand Theft Auto</em> and <em>Alien Hominid</em>; games that might or might not feature mature content, but were decidedly arcadey in nature. They didn't care that much about immersion or emotional engagement. <em>Uplink</em> was a different beast: it pulled me right in with its <a href="https://www.youtube.com/watch?v=QliQ0livbeQ">beautiful ambient soundtrack</a>, retrofuturistic visuals and gameplay that was unlike anything I had ever experienced. Sure, it might not have any fancy 3D models and complex shaders, but I still felt absorbed in a way no other game had done before. Its gameplay was highly addictive and its presentation deceptively brilliant, with a story that would've been deemed too ambitious for an AAA game even at the time. I became an obsessed man, looking up everything that I could find about the game and its developers, buying their newer games <a href="https://store.steampowered.com/app/1500/Darwinia/"><em>Darwinia</em></a> and <a href="https://store.steampowered.com/app/1520/DEFCON/"><em>DEFCON</em></a> and reading the <em>Uplink</em> design documents on the Bonus Disk religiously. Before <em>Uplink</em>, I never gave game design much consideration. I never thought about all the possibilities games have to tell unique stories or how certain game mechanics can make you feel certain emotions. So what is it about the gameplay that makes it so engaging, so immersive and so much fun? Well, the answer might not be what you'd expect: even though there's plenty to praise about <em>Uplink</em>'s design, it manages to be so engaging and immersive because it isn't actually that much fun.</p>
<p><img src="https://vertette.github.io/img/uplink_hack.png" alt="A screenshot of an Uplink hack in progress">
<em>Fun fact: the Trace Tracker's beeps were a last minute addition. There is a world map upgrade that shows you exactly how far the administrator's trace is, but nobody buys it because it doesn't beep.</em></p>
<p>That might make it sound like yet another pretentious indie game that sacrificies good gameplay in service of a Very Important Message™, but that's not actually the case. The anticipation of planning your next attack, the tension as the trace tracker's beeps become quicker and quicker as the system administrator starts closing in on you, the euphoria of a successful job that gets quickly swallowed up by the creeping paranoia of whether you properly correctly cleaned up after yourself or not - <em>Uplink</em> is a hurricane of emotions, but a lot of the emotions it invokes aren't exactly what you'd call positive ones. In that sense, while the game <em>can</em> be fun, it can also feel very tense, obtuse and frustrating, and that's important. Without that, the experience would not nearly be as effective at making you feel like a real hacker as it is, even if the moment-to-moment gameplay pretty much boils down to a script kiddie simulator. The way it goes about it elevates it to something much grander, something truly innovative and memorable, and in that sense "fun" is simply too limiting a term to describe <em>Uplink</em>'s design.</p>
<p>That might sound silly to a lot of players, because "if the game's not fun, why bother", right? But there is an actual precedence for this claim, for example horror games. Most people play horror games not to feel amused but to feel spooked, and those two emotions are almost directly on the opposite end of the emotion wheel. If a horror game is fun to you, then it's doing a very bad job. Another good example is <em>Pathologic</em>, a game that deliberately goes out of its way to be an unpleasant experience to sell the setting of a plague-ridden town in a very effective and memorable way. Even though I find it hard to recommend it, I also find it hard to dismiss it as not being worth your time. And I don't want to ruin people's laughs from back when <a href="https://www.gamesradar.com/we-dont-use-the-word-fun-says-the-last-of-us-2-director-neil-druckmann/&amp;utm_campaign=buffer_grtw/">Neill Druckmann infamously claimed they didn't use the word "fun" during development of <em>The Last of Us 2</em></a>, but that <em>is</em> a valid way to design your game. The way he phrased it made it come across as more pretentious than he meant it to, but the gameplay of <em>TLOU2</em> invokes a lot of the same emotions that <em>Uplink</em> does: tension, paranoia, and euphoria. If games really are an art form, then limiting the design to what's fun is ignoring so many other emotional reactions your game can inspire in others.</p>
<p>That doesn't mean that designing a fun game isn't valuable, but it <em>does</em> mean that it's worth exploring emotions through your game that aren't directly adjacent to fun. With all the opportunities the medium of video games has over others, it would be a waste not to.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New Study Says Maybe Helicopter Parenting Is Making Kids Depressed (121 pts)]]></title>
            <link>https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</link>
            <guid>38046910</guid>
            <pubDate>Sat, 28 Oct 2023 03:40:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/">https://www.techdirt.com/2023/10/26/new-study-in-the-journal-of-pediatrics-says-maybe-its-not-social-media-but-helicopter-parenting-thats-making-kids-depressed/</a>, See on <a href="https://news.ycombinator.com/item?id=38046910">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-423910">


<h3>from the <i>correlation-and-causation</i> dept</h3>

<p>We’ve been covering, at great length, the moral panic around the claims that social media is what’s making kids depressed. The problem with this narrative is that there’s basically no real evidence to support it. As the American Psychological Association found when it reviewed all the literature, despite many, many dozens of studies done on the impact of social media on kids, <a target="_blank" rel="noreferrer noopener" href="https://www.techdirt.com/2023/05/12/apa-report-says-that-media-politicians-are-simply-wrong-about-kids-social-media-media-then-lies-about-report/">no one was able to establish a causal relationship</a>.</p>
<p>As that report noted, the research seemed to show no inherent benefit or harm for most kids. For some, it showed a real benefit (often around kids being able to find like-minded people online to communicate with). For a very small percentage, it appeared to potentially exacerbate existing issues. And those are really the cases that we should be focused on.</p>
<p>But, instead, the narrative that continues to make the rounds is that social media is inherently bad for kids. That leads to various bills around age verification and age gating to keep kids off of social media.</p>
<p>Supporters of these bills will point to charts like this one, regarding teen suicide rates, noting the uptick correlates with the rise of social media.</p>

<p>Of course, they seem to cherry pick the start date of that chart, because if you go back further, you realize that while the uptick is a concern, it’s still way below what it had been in the 1990s (pre-social media).</p>

<p>In case that embed isn’t working, here’s an image of it:</p>
<figure><img decoding="async" src="https://i0.wp.com/lex-img-p.s3.us-west-2.amazonaws.com/img/bd036c6a-0422-4e33-9c37-427f9f9ddc81-RackMultipart20231025-142-jzngu7.png?ssl=1" alt="Image" data-recalc-dims="1"></figure>
<p>Obviously, the increase in suicides is a concern. But, considering that every single study that tries to link it to social media ends up failing to do so, that suggests that there might be some other factor at play here.</p>
<p>A recent study in the Journal of Pediatrics suggests a compelling alternative. It’s not social media, but the rise of helicopter parenting, in which kids no longer have spaces to just hang out with each other and be kids. It’s titled: <a target="_blank" rel="noreferrer noopener" href="https://doi.org/10.1016/j.jpeds.2023.02.004">Decline in Independent Activity as a Cause of Decline in Children’s Mental Well-being: Summary of the Evidence</a>. If you can’t see the full version, there’s <a target="_blank" rel="noreferrer noopener" href="https://cdn2.psychologytoday.com/assets/2023-02/Children's%20Independence%20IN%20PRESS%20.pdf">a preprint version</a> here.</p>
<p>The research summarizes the decline in “independent mobility” for kids over the last few decades:</p>
<blockquote>
<p><em>Considerable research, mostly in Europe, has focused on children’s independent mobility (CIM), defined as children’s freedom to travel in their neighborhood or city without adult accompaniment. That research has revealed significant declines in CIM, especially between 1970 and 1990, but also some large national differences. For example, surveys regarding the “licenses” (permissions) parents grant to their elementary school children revealed that in England, license to walk home alone from school dropped from 86% in 1971 to 35% in 1990 and 25% in 2010; and license to use public buses alone dropped from 48% in 1971 to 15% in 1990 to 12% in 2010.11 In another study, comparing CIM in 16 different countries (US not included), conducted from 2010 to 2012, Finland stood out as allowing children the greatest freedom of movement. The authors wrote: “At age 7, a majority of Finnish children can already travel to places within walking distance or cycle to places alone; by age 8 a majority can cross main roads, travel home from school and go out after dark alone, by age 9 a majority can cycle on main roads alone, and by age 10 a majority can travel on local buses alone.” Although we have found no similar studies of parental permissions for US children, other data indicate that the US is more like the UK concerning children’s independent mobility than like Finland. For example, National Personal Transportation Surveys revealed that only 12.7% walked or biked to school in 2009 compared with 47.7% in 1969.</em></p>
</blockquote>
<p>And then it notes the general decline in mental health as well, which they highlight started long before social media existed:</p>
<blockquote>
<p><em>Perhaps the most compelling and disturbing evidence comes from studies of suicide and suicidal thoughts. Data compiled by the CDC indicate that the rate of suicide among children under age 15 rose 3.5-fold between 1950 and 2005 and by another 2.4-fold between 2005 and 2020. No other age group showed increases nearly this large. By 2019, suicide was the second leading cause of death for children from age 10 through 15, behind only unintentional injury. Moreover, the 2019 YRBS survey revealed that during the previous year 18.8% of US high school students seriously considered attempting suicide, 15.7% made a suicide plan, 8.9% attempted suicide one or more times, and 2.5% made a suicide attempt requiring medical treatment. We are clearly experiencing an epidemic of psychopathology among young people.</em></p>
</blockquote>
<p>But, unlike those who assume correlation is causation with regards to social media, the researchers here admit there needs to be more. And they bring the goods, pointing to multiple studies that suggest a pretty clear causal relationship, rather than just correlation.</p>
<blockquote>
<p><em>Several studies have examined relationships between the amount of time young children have for self-directed activities at home and psychological characteristics predictive of future wellbeing. These have revealed significant positive correlations between amount of self-structured time (largely involving free play) and (a) scores on two different measures of executive functioning; (b) indices of emotional control and social ability; and (c) scores, two years later, on a measure of self-regulation. There is also evidence that risky play, where children deliberately put themselves in moderately frightening situations (such as climbing high into a tree) helps protect against the development of phobias and reduces future anxiety by increasing the person’s confidence that they can deal effectively with emergencies.</em></p>
<p><em>Studies with adults involving retrospections about their childhood experiences provide another avenue of support for the idea that early independent activity promotes later wellbeing. In one such study, those who reported much free and adventurous play in their elementary school years were assessed as having more social success, higher self-esteem, and better overall psychological and physical health in adulthood than those who reported less such play. In another very similar study, amount of reported free play in childhood correlated positively with measures of social success and goal flexibility (ability to adapt successfully to changes in life conditions) in adulthood. Also relevant here are studies in which adults (usually college students) rated the degree to which their parents were overprotective and overcontrolling (a style that would reduce opportunity for independent activity) and were also assessed for their current levels of anxiety and depression. A systematic review of such studies revealed, overall, positive correlations between the controlling, overprotective parenting style and the measures of anxiety and depression.</em></p>
</blockquote>
<p>They also note that they are not claiming (of course) that this is the sole reason for the declines in mental health. Just that there is strong evidence that it is a key component. They explore a few other options that may contribute, including increased pressure at schools and societal changes. They also consider the impact of social media and digital technologies and note (as we have many times) that there just is no real evidence to support the claims:</p>
<blockquote>
<p><em>Much recent discussion of young people’s mental health has focused on the role of increased use of digital technologies, especially involvement with social media. However, systematic reviews of research into this have provided little support for the contention that either total screen time or time involved with social media is a major cause of, or even correlate of, declining mental health. One systematic review concluded that research on links between digital technology use and teens’ mental health “has generated a mix of often conflicting small positive, negative and null associations” (Odgers &amp; Jensen, 2020). Another, a “review of reviews” concluded that “the association between digital technology use, or social media use in particular, and psychological well-being is, on average, negative but very small” and noted some evidence, from longitudinal research, that negative correlations may result from declining mental health leading to more social media use rather than the reverse (Orben, 2020)</em></p>
</blockquote>
<p>Indeed, if this theory is true, that the lack of spaces for kids to explore and play and experiment without adult supervision <em>is</em> a leading cause of mental health decline, you could easily see how those who are depressed are more likely to <em>seek out</em> those private spaces, and turn to social media, given the lack of any such spaces they can go to physically.</p>
<p>And, if that’s the case, then all of these efforts to ban social media for kids, or to make social media <a rel="noreferrer noopener" href="https://www.techdirt.com/2022/09/20/the-internet-is-not-disneyland-people-should-stop-demanding-it-become-disneyland/" target="_blank">more like Disneyland</a>, could likely end up doing <strong>a lot more harm than good</strong> by cutting off one of the last remaining places where kids can communicate with their peers without adults watching over their every move. Indeed, the various proposals to give parents more access to what their kids are doing online could worsen the problem as well, taking away yet another independent space for kids.</p>
<p>Over the last few years, there’s been a push to <a target="_blank" rel="noreferrer noopener" href="https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html">bring back more “dangerous” play for kids</a>, as people have begun to realize that things may have gone too far in the other direction. Perhaps it’s time we realize that social media fits into that category as well.</p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/age-appropriate-design/" rel="tag">age appropriate design</a>, <a href="https://www.techdirt.com/tag/age-verification/" rel="tag">age verification</a>, <a href="https://www.techdirt.com/tag/depression/" rel="tag">depression</a>, <a href="https://www.techdirt.com/tag/independent-spaces/" rel="tag">independent spaces</a>, <a href="https://www.techdirt.com/tag/mental-health/" rel="tag">mental health</a>, <a href="https://www.techdirt.com/tag/social-media/" rel="tag">social media</a>, <a href="https://www.techdirt.com/tag/studies/" rel="tag">studies</a>, <a href="https://www.techdirt.com/tag/suicide/" rel="tag">suicide</a>, <a href="https://www.techdirt.com/tag/teens/" rel="tag">teens</a>
<br>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A small warning about UDP based protocols (114 pts)]]></title>
            <link>https://boston.conman.org/2023/10/25.1</link>
            <guid>38046448</guid>
            <pubDate>Sat, 28 Oct 2023 02:13:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://boston.conman.org/2023/10/25.1">https://boston.conman.org/2023/10/25.1</a>, See on <a href="https://news.ycombinator.com/item?id=38046448">Hacker News</a></p>
Couldn't get https://boston.conman.org/2023/10/25.1: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[When gradient descent is a kernel method (159 pts)]]></title>
            <link>https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</link>
            <guid>38045665</guid>
            <pubDate>Sat, 28 Oct 2023 00:23:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html">https://cgad.ski/blog/when-gradient-descent-is-a-kernel-method.html</a>, See on <a href="https://news.ycombinator.com/item?id=38045665">Hacker News</a></p>
<div id="readability-page-1" class="page">
    
    
    <p>Suppose that we sample a large number <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> of independent random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi mathvariant="double-struck">R</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f_i \colon \R \to \R</annotation></semantics></math></span></span> from a certain distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and propose to solve a regression problem by choosing a linear combination
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>λ</mi><mi>i</mi></msub><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f} = \sum_i \lambda_i f_i.</annotation></semantics></math></span></span></span>
For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">N,</annotation></semantics></math></span></span> adjusting the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> to fit some fixed constraints of the form <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\bar{f}(t_i) = y_i</annotation></semantics></math></span></span> amounts to solving a highly underdetermined linear system, meaning that a high-dimensional space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> of vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>λ</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>λ</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda_1, \dots, \lambda_N)</annotation></semantics></math></span></span> fit our constraints perfectly. So, choosing one element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> requires some additional decision-making. To use the picturesque idea of a
"loss landscape" over parameter space, our problem will have a <em>ridge</em> of equally performing parameters rather than just a single optimal <em>peak</em>.</p>
<p>Now, we make a very strange proposal. What if we simply initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mi mathvariant="normal">/</mi><msqrt><mi>n</mi></msqrt></mrow><annotation encoding="application/x-tex">\lambda_i = 1/\sqrt{n}</annotation></semantics></math></span></span> for all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span> and proceed by minimizing some loss function using gradient descent? If all goes well, we should end up with an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Lambda.</annotation></semantics></math></span></span> Of course, many elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> give very bad models. To see this, it's enough to remember that we can expect a linear combination of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> random functions to fit <em>any</em> <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> data points, so if we have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> data points, there exist models in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span> that perfectly interpolate any adversarial selection of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">N - m</annotation></semantics></math></span></span> additional data points! Does gradient descent tend to make a "good" choice?</p>
<p>Let's test this empirically. In the widget below, I've chosen functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> by sampling <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trajectories of a Wiener process, also known as Brownian noise. Click anywhere to introduce data points and click the play button in the top right to run gradient descent for the squared loss <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mi>i</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\sum_i (\bar{f}(t_i) - y_i)^2.</annotation></semantics></math></span></span></p>

<p>Interestingly, the functions we obtain are not that bad. They seem to concentrate around piecewise linear interpolations of our data. In fact, in the limit <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>→</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">N \to \infty</annotation></semantics></math></span></span> of many random functions, it turns out that running gradient descent to convergence has a meaningful statistical interpretation. Specifically, if we view the Wiener process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> as a prior, then <strong>running gradient descent to convergence samples from the posterior for our data points.</strong> Since many optimal solutions to our minimization problem are meaningless, it is not possible to explain this fact if we see gradient descent as "just some optimization method." What explains its relative success?</p>
<p>As we will show in this post, our intriguing Bayesian interpretation can be explained by the relationship between the behavior of gradient descent steps, the statistical properties of our random functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span>, and our initialization. In particular, it does <em>not</em> depend on the loss function—so long as it leads gradient descent to converge to an exact interpolation—but <em>does</em> depend significantly on our choice of parameters at initialization. Our analysis will rely on a "tangent kernel" of the sort introduced in the <em>Neural Tangent Kernel</em> paper by Jacot et al.. Specifically, viewing gradient descent as a process occurring in the function space of our regression problem, we will find that its dynamics can be described in terms of a certain kernel function, which in this case is just the kernel function of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span></p>
<p>Of course, there are much easier ways to sample posteriors for low-dimensional Gaussian processes. Nevertheless, it's interesting to notice a relationship between Bayesian inference and gradient descent methods at large, since the latter tend to apply to situations where direct estimation of a posterior distribution is not practical. Furthermore, the results of Jacot suggest that kernel-based interpretations may also hold for large, non-trivial neural networks. To the extent that this is true, we can use the kernel interpretation to reason about many sorts of neural network phenomena, including the benefits of early stopping, the existence of "implicit regularization", and the fact that overparameterization often <em>increases</em> performance despite the apparent risk of overfitting.</p>
<p>In this post, we will focus exclusively on the toy problem introduced above. Our discussion (which admittedly takes a bit of a scenic route) is divided into three sections.</p>
<ul>
<li>First, we will discover how the covariance kernel of the process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> is related to the dynamics of gradient descent.</li>
<li>Next, we recall the theory of reproducing kernel Hilbert spaces and show how the kernel-based behavior of gradient descent is related to regularization.</li>
<li>Finally, we recall some special properties of Gaussian processes and explain why regularization is related to the Bayesian interpretation of our trained model.</li>
</ul>
<h2>Kernel Functions</h2>
<p>Let's begin by considering the effect that a single step of gradient descent has on our function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In general, the differential of a loss can be written as a sum of differentials <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> is the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> at an input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t,</annotation></semantics></math></span></span> so by linearity it is enough for us to understand how <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> "responds" to differentials of this form.</p>
<p>In response to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">d \pi_t,</annotation></semantics></math></span></span> the parameters <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> are assigned differentials
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial \pi_t}{\partial \lambda_i} = f_i(t).</annotation></semantics></math></span></span></span>
So, gradient descent will increase <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span> proportional to the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i(t).</annotation></semantics></math></span></span> In terms of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\bar{f},</annotation></semantics></math></span></span> we find that the value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\bar{f}(s)</annotation></semantics></math></span></span> at another input <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> will increase proportional to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msub><mi>f</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\Delta_t(s) = \sum_{i = 1}^N f_i(s) f_i(t). \tag{1}</annotation></semantics></math></span></span></span>
Note that this expression is independent of the coefficients <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\lambda_i.</annotation></semantics></math></span></span> This means that the gradient descent step we apply to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> depends only on our learning rate and differential of the loss at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\bar{f}.</annotation></semantics></math></span></span> In other words, we can view gradient descent as a process happening in the function space of our regression problem.</p>
<p>For large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> we get the approximation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>≈</mo><msub><mi>E</mi><mrow><mi>f</mi><mo>∼</mo><mi mathvariant="script">F</mi></mrow></msub><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\frac{1}{N} \Delta_t(s) \approx E_{f \sim \Fc}[f(s) f(t)]. \tag{2}</annotation></semantics></math></span></span></span>
This last expression is familiar in the study of Gaussian processes; it is called the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and denoted <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> For the Wiener process, the covariance kernel takes the form
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t).</annotation></semantics></math></span></span></span>
In the following, we'll assume <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> is large enough for us to make the approximation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2)</annotation></semantics></math></span></span> confidently. Then, we conclude that a request of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> will cause gradient descent to push <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> in the direction of the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>You can get a visual sense of this behavior in the widget below. As above, I've generated <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span></span> trials of the Wiener process to use as my functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f_i.</annotation></semantics></math></span></span> You can choose the request <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">d \pi_t</annotation></semantics></math></span></span> by clicking on the graph, and your browser will compute the corresponding response <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Δ</mi><mi>t</mi></msub><mi mathvariant="normal">/</mi><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Delta_t/N.</annotation></semantics></math></span></span> For comparison I've also drawn the prediction <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>

<p>This is already a significant conclusion. In particular, it means that every step of gradient descent modifies <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> by a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K(-, t_i),</annotation></semantics></math></span></span> where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">t_i</annotation></semantics></math></span></span> ranges over the inputs in our training set. Since the linear span of these functions is a certain space of piecewise affine functions, if we initialize <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda_i = 0</annotation></semantics></math></span></span> and run gradient descent to convergence with <em>any</em> reasonable loss function, we should approximately converge to a piecewise affine interpolation of our data points. We've run this experiment in the widget below.</p>

<p>This new model has less variance than before. In fact, in the large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span> limit, its behavior will be exactly deterministic! In comparison, our previous model will <em>always</em> exhibit variance due to initialization of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f_i</annotation></semantics></math></span></span> even for large <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">N.</annotation></semantics></math></span></span> In other words, when given a finite training set, gradient descent cannot entirely "forget" its initialization even when run to convergence.</p>
<p>Another important conclusion is that, when we optimize least squares with gradient descent, the evolution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> is linear in the sense of approximately obeying a linear ODE. Indeed, for data points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(t_i, y_i)</annotation></semantics></math></span></span> our loss differential will be
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>d</mi><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>d</mi><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{1}{2} d \sum_i (\bar{f}(t_i) - y_i)^2 = \sum_i d \pi_{t_i} (\bar{f}(t_i) - y_i).</annotation></semantics></math></span></span></span>
So, if we view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> as evolving under the flow of gradient descent with respect to a continuous parameter <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>τ</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\tau,</annotation></semantics></math></span></span> we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\frac{d \bar{f}}{d \tau} = \sum_i K(-, t_i) (y_i - \bar{f}(t_i)),</annotation></semantics></math></span></span></span>
where the right-hand side is a linear function of the empirical error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">h = (y_i - \bar{f}(t_i)).</annotation></semantics></math></span></span> Restricting this equation to the evaluation of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>f</mi><mo>ˉ</mo></mover></mrow><annotation encoding="application/x-tex">\bar{f}</annotation></semantics></math></span></span> over the input points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t_i,</annotation></semantics></math></span></span> we find that the error vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> solves the ODE
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>h</mi></mrow><mrow><mi>d</mi><mi>τ</mi></mrow></mfrac><mo>=</mo><mo>−</mo><mi>K</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\frac{d h}{d \tau} = -K h</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(K(t_i, t_j)).</annotation></semantics></math></span></span> Since this matrix is positive-definite—it is a covariance matrix—we conclude that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> will converge to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> over the training process if our learning rate is sufficiently small. Furthermore, knowing the eigenvalues of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> lets us understand the nature of our convergence; gradient descent will "correct" the error <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span></span> along the components of the eigenbasis for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> with largest eigenvalues first, and take longer to correct components with smaller eigenvalues.</p>
<p>Now, we could have chosen a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> of random functions with a different covariance kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K.</annotation></semantics></math></span></span> Here the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> were easy to interpret, but in general, what does it mean to fit a data set using a linear combination of functions like these? One interesting perspective comes from the idea of <em>regularization</em>, which we discuss next.</p>
<h2>Regularization</h2>
<p>Consider a Hilbert space <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> equipped with bounded projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\pi_t \colon H \to \R</annotation></semantics></math></span></span> for each <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">t \in \R,</annotation></semantics></math></span></span> and suppose that we want to find an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">v \in H</annotation></semantics></math></span></span> that minimizes some loss function depending on the values <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi_{t}(v)</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span></span> belonging to some collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\{t_i\}.</annotation></semantics></math></span></span> (Note that elements of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> can be viewed as functions from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\R</annotation></semantics></math></span></span> by viewing <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> as the evaluation at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span>) If this problem is underdetermined—which necessarily will happen if <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> is infinite-dimensional and our collection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{t_i\}</annotation></semantics></math></span></span> is finite—then we may ask for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> that both minimizes our loss and has minimal norm in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span> In machine learning, this is called regularization.</p>
<p>Let's write <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> for the product of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>t</mi><mi>i</mi></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{t_i}</annotation></semantics></math></span></span> Because <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is chosen with minimal norm, it cannot be made smaller by adjusting it by an element of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\ker \Pi,</annotation></semantics></math></span></span> so <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span> is orthogonal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\ker \Pi.</annotation></semantics></math></span></span> But since the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{t}</annotation></semantics></math></span></span> are continuous, they can be represented by vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span> in the sense that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mo>−</mo><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_{t}(-) = \langle K_{t}, - \rangle.</annotation></semantics></math></span></span></span>
(This is the Riesz representation theorem.) Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> can be described as the orthogonal complement to the set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mo stretchy="false">}</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\{K_{t_i}\},</annotation></semantics></math></span></span> the orthogonal complement to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ker</mi><mo>⁡</mo><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\ker \Pi</annotation></semantics></math></span></span> is exactly the closure of the span of the vectors <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><msub><mi>t</mi><mi>i</mi></msub></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K_{t_i}.</annotation></semantics></math></span></span> We conclude that any regularized solution to our loss function is a (limit of) linear combinations of these vectors.</p>
<p>What are the projections of the "representative elements" <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">K_{t}</annotation></semantics></math></span></span>? By our own definition, we have
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>s</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>s</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>t</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\pi_s(K_t) = \langle K_s, K_t \rangle</annotation></semantics></math></span></span></span>
for any other <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">s \in \R.</annotation></semantics></math></span></span> This last expression is a positive semidefinite kernel, which we will denote <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(s, t).</annotation></semantics></math></span></span> In other words, the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> and the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\pi_t</annotation></semantics></math></span></span> work together to produce a kernel function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> whose partial evaluations <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(-, t)</annotation></semantics></math></span></span> help us solve optimization problems regularized by the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H.</annotation></semantics></math></span></span></p>
<p>In the literature, a Hilbert space equipped with bounded projections indexed over a set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is called a reproducing kernel Hilbert space (or RKHS). In fact, we can also go in the other direction: every positive definite kernel on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span></span> is "reproduced" by some RKHS, which also turns out to be unique in a certain sense. This is known as the Moore-Aronszajn theorem.</p>
<p>What RKHS corresponds to our kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(s, t) = \min(s, t)</annotation></semantics></math></span></span>? In general, determining the RKHS of a kernel is not entirely straightforward. In fact, notice that for positive definite kernels over a finite set <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">I,</annotation></semantics></math></span></span> the inner product for the RKHS expressed in the dual basis for our projections turns out to be the <em>inverse</em> of the matrix encoded by our kernel. Indeed, where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">K_i</annotation></semantics></math></span></span> are representatives of the projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\pi_i</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">e_i</annotation></semantics></math></span></span> is a dual basis verifying <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>i</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\pi_i(e_j) = \delta_{i, j},</annotation></semantics></math></span></span> we find that
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><mi>K</mi><mo stretchy="false">(</mo><mi>j</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>e</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>K</mi><mi>k</mi></msub><mo stretchy="false">⟩</mo><mo>=</mo><msub><mi>π</mi><mi>k</mi></msub><mo stretchy="false">(</mo><msub><mi>e</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>k</mi></mrow></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle e_i, e_j \rangle K(j, k) = \langle e_i, e_j \rangle \pi_j(K_k) = \langle e_i, K_k \rangle = \pi_k(e_j) = \delta_{i, k}.</annotation></semantics></math></span></span></span>
So, interpreting a RKHS norm in terms of projections of elements requires solving some sort of inverse problem.</p>
<p>The RKHS for a centered Gaussian process <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(X_t)</annotation></semantics></math></span></span> can be viewed as an isometric embedding of the observables <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> with respect to the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L^2</annotation></semantics></math></span></span> norm for the process measure <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\Fc.</annotation></semantics></math></span></span> Specifically, if we define <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f(X_t) = K_t,</annotation></semantics></math></span></span> then clearly
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>X</mi><mi>k</mi></msub><msub><mo stretchy="false">⟩</mo><mrow><msup><mi>L</mi><mn>2</mn></msup><mi mathvariant="script">F</mi></mrow></msub><mo>=</mo><mo stretchy="false">⟨</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>k</mi></msub><mo stretchy="false">)</mo><msub><mo stretchy="false">⟩</mo><mtext>RKHS</mtext></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, X_k \rangle_{L^2 \Fc} = \langle f(X_t), f(X_k) \rangle_\text{RKHS}.</annotation></semantics></math></span></span></span>
Indeed, the space of observables of a Gaussian process is already a RKHS for its covariance kernel, if we take the projections to be the maps <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>X</mi><mi>t</mi></msub><mo separator="true">,</mo><mo>−</mo><mo stretchy="false">⟩</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\langle X_t, - \rangle.</annotation></semantics></math></span></span> However, we would like to view the RKHS more directly as a space of functions.</p>
<p>We may begin by observing, then, that observables of the Wiener process can be isometrically mapped into <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mn>1</mn></msup><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L^1 [0, \infty)</annotation></semantics></math></span></span> by sending <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span></span> to
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>K</mi><mi>s</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>:</mo><mi>t</mi><mo>≤</mo><mi>s</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>0</mn><mo>:</mo><mtext>otherwise.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">K_s(t) = \begin{cases}
1 : t \le s \\
0 : \text{otherwise.}
\end{cases}</annotation></semantics></math></span></span></span>
Under this perspective, our projections become
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>π</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">⟨</mo><msub><mi>K</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">⟩</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>t</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>s</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\pi_t(f) = \langle K_t, f \rangle = \int_0^t f(s) \, ds.</annotation></semantics></math></span></span></span>
Ultimately, we are led to view the RKHS of the Wiener process as the Sobolev space of absolutely continuous functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f \colon [0, \infty) \to \R</annotation></semantics></math></span></span> such that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(0) = 0</annotation></semantics></math></span></span> and such that the norm
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">∥</mo><mi>f</mi><mo stretchy="false">∥</mo><mo>=</mo><mrow><mo fence="true">(</mo><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert f \rVert = \left( \int_0^\infty (f'(t))^2 \, dt \right)</annotation></semantics></math></span></span></span>
is finite. In fact, solving the regularized interpolation problem
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><msubsup><mo>∫</mo><mn>0</mn><mi mathvariant="normal">∞</mi></msubsup><mo stretchy="false">(</mo><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mtext> </mtext><mi>d</mi><mi>t</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mtext>  </mtext><mtext>for&nbsp;all&nbsp;</mtext><mi>i</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \int_0^\infty (f'(s))^2 \, dt \\
\textbf{subject to} &amp; \quad f(0) = 0, f(t_i) = y_i \; \text{for all }i
\end{align*}</annotation></semantics></math></span></span></span>
results in the piecewise affine interpolations we observed in the widget above.</p>
<p>So far, we have shown that its relationship with kernel functions gives gradient descent a distinct flavor of <em>implicit regularization</em>. We did not have a penalty function in mind when we set up our problem, but our distribution of random functions ended up making our gradient updates interpretable in terms of a RKHS for an associated kernel function. In the last section of this post, we address how this fact is related to the statistical idea of a conditional distribution for a Gaussian process.</p>
<h2>Bayesian Interpretation</h2>
<p>When <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> are jointly Gaussian distributed, we know that the remainder
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span></span>
of the conditional expectation is independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> (Remember that for other distributions, this remainder will have zero covariance with all <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable events but will not necessarily be independent!) So, we can decompose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> into two components
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>=</mo><mo stretchy="false">(</mo><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y = (Y - E(Y|X)) + E(Y|X),</annotation></semantics></math></span></span></span>
the first being a Gaussian variable independent from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> and the second being <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span>-measurable. This clarifies the nature of the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span>: it will have constant variance equal to the variance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> and mean equal to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> a linear function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">X.</annotation></semantics></math></span></span> In particular, if we want to sample the conditional distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span></span> given <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X = x,</annotation></semantics></math></span></span> we could take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Y</mi><mo>+</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>−</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Y + E(Y|X = x) - E(Y|X) = Y + E(Y|X = x - X),</annotation></semantics></math></span></span></span>
where the apparently nonsensical conditional expectation on the RHS should be interpreted as the evaluation of the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">E(Y|X),</annotation></semantics></math></span></span> viewed as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>−</mo><mi>X</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">x - X.</annotation></semantics></math></span></span> Keep in mind that this is a very special property of Gaussian distributions; in general, the distribution of the remainder <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Y - E(Y|X)</annotation></semantics></math></span></span> conditional on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> will depend on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">X,</annotation></semantics></math></span></span> and so we won't be able to sample the conditional distribution under another "counterfactual" value <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">X = x</annotation></semantics></math></span></span> simply by translating a sample of the remainder.</p>
<p>Now, consider a random function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> drawn from a Gaussian distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">F</mi></mrow><annotation encoding="application/x-tex">\Fc</annotation></semantics></math></span></span> and let <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> give the values of our trajectory on a finite set of inputs. If we want to produce a sample from the distribution of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> conditional on some data <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi = \pi,</annotation></semantics></math></span></span> we can take
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo>−</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><mi>π</mi><mo>−</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">f - E(f | \Pi = \pi - \Pi).</annotation></semantics></math></span></span></span>
But, as it turns out, the conditional expectation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f|\Pi = \pi^*)</annotation></semantics></math></span></span> will be exactly the function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span></span> in the RKHS of our process that solves the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> regularized by the RKHS norm! This explains the Bayesian interpretation of our model.</p>
<p>One way to understand this is just to write out the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> at a given value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t.</annotation></semantics></math></span></span> We know that this will be the linear function <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\lambda \Pi</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> uniquely determined by the equation
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>−</mo><mi>λ</mi><mi mathvariant="normal">Π</mi><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.</mn></mrow><annotation encoding="application/x-tex">\Cov(f(t) - \lambda \Pi, \Pi) = 0.</annotation></semantics></math></span></span></span>
Where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mo stretchy="false">[</mo><mi>K</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>t</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">K = [K(t_i, t_j)]</annotation></semantics></math></span></span> is the covariance matrix of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mo stretchy="false">[</mo><mi mathvariant="normal">Cov</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">v = [\Cov(f(t), f(t_i))]</annotation></semantics></math></span></span> gives the covariance of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> with the components <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t_i)</annotation></semantics></math></span></span> of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\Pi,</annotation></semantics></math></span></span> this equation can be written as
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>v</mi><mo>−</mo><mi>λ</mi><mi>K</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v - \lambda K = 0,</annotation></semantics></math></span></span></span>
and <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mo>=</mo><mi>v</mi><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi mathvariant="normal">Π</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">E(f(t) | \Pi) = v K^{-1} \Pi.</annotation></semantics></math></span></span> We conclude that the function
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mo>↦</mo><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t \mapsto E(f(t)|\Pi)</annotation></semantics></math></span></span></span>
is a linear combination of the functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K(t, t_i)</annotation></semantics></math></span></span>—the coordinates of the vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span></span>—with constant coefficients, determined by the constraints that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi)</annotation></semantics></math></span></span> should agree with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi></mrow><annotation encoding="application/x-tex">\Pi</annotation></semantics></math></span></span> at the points <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><msub><mi>t</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">t = t_i.</annotation></semantics></math></span></span> But, as we saw above, this is the same as the solution to the problem of interpolating some constraints <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(t_i) = y_i</annotation></semantics></math></span></span> regularized by the RKHS norm of our process.</p>
<p>To see this connection more directly, remember that the mean of a Gaussian distribution coincides with the mode—the point of highest probability density under linear coordinates. So, for example, <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E(f(t)|\Pi = \pi^*)</annotation></semantics></math></span></span> is exactly the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span></span> that minimizes
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mo>+</mo><mo>−</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext>  </mtext><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>  </mtext><mo>…</mo><mtext>  </mtext><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msup><mi>π</mi><mo>∗</mo></msup><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi><mi mathvariant="normal">⋮</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\ln(p(f(t), \pi^*)) = C + -\frac{1}{2} 
[f(t)  \; \pi^*(1)  \; \dots \; \pi(m)] K^{-1}
\begin{bmatrix}
f(t)   \\
\pi^*(1)   \\
\vdots  \\
\pi(m)]
\end{bmatrix}</annotation></semantics></math></span></span></span>
where <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span></span> is the inverse of the covariance matrix for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">(f(t), \Pi).</annotation></semantics></math></span></span> But from the previous section we know that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">K^{-1}</annotation></semantics></math></span></span> expresses the inner product of the RKHS derived from the covariance kernel of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> in the dual basis for the projections. So in fact we are asking for the value of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Π</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t), \Pi)</annotation></semantics></math></span></span> that satisfies the constraint <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Π</mi><mo>=</mo><msup><mi>π</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\Pi = \pi^*</annotation></semantics></math></span></span> and is regularized by the RKHS norm corresponding to a restriction of the covariance kernel of our process, which by the representer theorem will be a linear combination of restrictions of functions <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mo>−</mo><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K(-, t).</annotation></semantics></math></span></span></p>
<p>Abstractly, we are relying on the fact that whenever we have a positive definite kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>I</mi><mo>×</mo><mi>I</mi><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">K \colon I \times I \to \R</annotation></semantics></math></span></span> and a finite subset <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>⊆</mo><mi>I</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">J \subseteq I,</annotation></semantics></math></span></span> we get a natural projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mi>H</mi><mo>→</mo><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">P \colon H \to H_J</annotation></semantics></math></span></span> onto the RKHS for the kernel restricted to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>×</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">J \times J</annotation></semantics></math></span></span> given by
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>π</mi><mi>j</mi></msub><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mi>K</mi><mi>j</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(v) = \sum_j \pi_j(v) K_j.</annotation></semantics></math></span></span></span>
Given a vector <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>H</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">v \in H,</annotation></semantics></math></span></span> what is the norm of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(v)</annotation></semantics></math></span></span> in <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span>? Since <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span></span> is an isometry over the span of the elements <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>j</mi></msub><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K_j,</annotation></semantics></math></span></span> we can view <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">∥</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><msub><mo stretchy="false">∥</mo><msub><mi>H</mi><mi>J</mi></msub></msub></mrow><annotation encoding="application/x-tex">\lVert P(v) \rVert_{H_J}</annotation></semantics></math></span></span> as the minimum possible norm for an element <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>∈</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">w \in H</annotation></semantics></math></span></span> solving the equation <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>v</mi><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">P(w) = P(v).</annotation></semantics></math></span></span> In particular, solving a regularized problem over <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span></span> that depends on projections <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\pi_j</annotation></semantics></math></span></span> for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>∈</mo><mi>J</mi></mrow><annotation encoding="application/x-tex">j \in J</annotation></semantics></math></span></span> and then restricting the solution to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> is the same as restricting to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub></mrow><annotation encoding="application/x-tex">H_J</annotation></semantics></math></span></span> and solving the regularized problem with respect to the norm on <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mi>J</mi></msub><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">H_J.</annotation></semantics></math></span></span></p>
<p>As a final remark, note that we can informally imagine the RKHS of a Gaussian process as specifying the "energy" of the process in a statistical mechanics sense; although the norm of the RKHS is not defined over the same function space that the process takes values, we get the energy for the joint distribution of any finite projection <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(f(t_1), \dots, f(t_m))</annotation></semantics></math></span></span> as a function of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>y</mi><mi>m</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(y_1, \dots, y_m)</annotation></semantics></math></span></span> by solving
<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext mathvariant="bold">minimize</mtext></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mo stretchy="false">∥</mo><mi>f</mi><msub><mo stretchy="false">∥</mo><mi>H</mi></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext mathvariant="bold">subject</mtext><mtext>&nbsp;</mtext><mtext mathvariant="bold">to</mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mspace width="1em"></mspace><mi>f</mi><mo stretchy="false">(</mo><msub><mi>t</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">.</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align*}
\textbf{minimize} &amp; \quad \lVert f \rVert_H \\
\textbf{subject to} &amp; \quad f(t_i) = y_i.
\end{align*}</annotation></semantics></math></span></span></span>
This is the most satisfactory way that I've found to connect the interpretation of kernel functions in terms of regularization with their interpretation in terms of conditional expectations of a Gaussian process.</p>

    


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The destruction of Gaza's internet is complete (113 pts)]]></title>
            <link>https://www.wired.com/story/gaza-internet-blackout-israel/</link>
            <guid>38045514</guid>
            <pubDate>Sat, 28 Oct 2023 00:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/gaza-internet-blackout-israel/">https://www.wired.com/story/gaza-internet-blackout-israel/</a>, See on <a href="https://news.ycombinator.com/item?id=38045514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>For more than three weeks, Gaza has faced an almost total internet blackout. The cables, cell towers, and infrastructure needed to keep people online have been damaged or destroyed as Israel launched thousands of missiles in response to <a href="https://www.wired.com/story/israel-hamas-war-surveillance/">Hamas attacking Israel</a> and taking <a href="https://www.wired.com/story/livestream-hostages-israel-hamas-war/">hundreds of hostages</a> on October 7. Then, this evening, amid reports of heavy bombing in Gaza, some of the last remaining connectivity disappeared.</p><p>In the days after October 7, people living in Gaza have been unable to communicate with family or friends, leaving them unsure whether loved ones are alive. Finding reliable news about events has become harder. Rescue workers have not been able to connect to mobile networks, hampering recovery efforts. And information flowing out of Gaza, showing the conditions on the ground, has been stymied.</p><p>As the Israel Defense Forces said it was expanding its ground operations in Gaza this evening, internet connectivity fell further. Paltel, the main Palestinian communications company, has been able to keep some of its services online during Israel’s military response to Hamas’ attack. However, at around 7:30 pm local time today, internet monitoring firm NetBlocks <a data-offer-url="https://twitter.com/netblocks/status/1717942556703551590" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/netblocks/status/1717942556703551590&quot;}" href="https://twitter.com/netblocks/status/1717942556703551590" rel="nofollow noopener" target="_blank">confirmed</a> a “collapse” in connectivity in the Gaza Strip, mostly impacting remaining Paltel services.</p><p>“We regret to announce a complete interruption of all communications and internet services within the Gaza Strip,” Paltel <a data-offer-url="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l&quot;}" href="https://www.facebook.com/paltel.970/posts/pfbid02RhT28obiwuAnasm2P64TbwCkTWssucQUWJqpBNyHDhtWS5fdWDGsSwXwa56vQq3l" rel="nofollow noopener" target="_blank">posted</a> in a post on its Facebook page. The company claimed that bombing had “caused the destruction of all remaining international routes.” An identical post was made on the Facebook page of Jawwal, the region’s biggest mobile provider, which is owned by Paltel. Separately, Palestinian Red Crescent, a humanitarian organization, <a data-offer-url="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet&quot;}" href="https://twitter.com/PalestineRCS/status/1717953723605901373?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet" rel="nofollow noopener" target="_blank">said on X</a> (formerly Twitter) that it had lost contact with its operation room in Gaza and is “deeply concerned” about its ability to keep caring for people, with landline, cell, and internet connections being inaccessible.</p><p>“This is a terrifying development,” Marwa Fatafta, a policy manager focusing on the Middle East and North Africa at the <a data-offer-url="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.accessnow.org/press-release/communications-blackout-gaza-strip/&quot;}" href="https://www.accessnow.org/press-release/communications-blackout-gaza-strip/" rel="nofollow noopener" target="_blank">digital rights group Access Now</a>, tells WIRED. “Taking Gaza completely off the grid while launching an unprecedented bombardment campaign only means something atrocious is about to happen.”</p><p>A WIRED review of internet analysis data, social media posts, and Palestinian internet and telecom company statements shows how connectivity in the Gaza Strip drastically plummeted after October 7 and how some buildings linked to internet firms have been damaged in attacks. Photos and videos show sites that house various internet and telecom firms have been damaged, while reports from official organizations, including the United Nations, describe the impact of people being offline.</p><p>Damaged Lines</p><p>Around the world, the internet and telecoms networks that typically give web users access to international video calls, online banking, and endless social media are a complicated, sprawling mix of hardware and software. Networks of networks, combining data centers, servers, switches, and reams of cables, communicate with each other and send data globally. Local internet access is provided by a mix of companies with no clear public documentation of their infrastructure, making it difficult to monitor the overall status of the system as a whole. In Gaza, experts say, internet connectivity is heavily reliant on Israeli infrastructure to connect to the outside world.</p><p>Amid Israel’s intense bombing of Gaza, physical systems powering the internet have been destroyed. On October 10, the United Nations’ Office for the Coordination of Humanitarian Affairs (OCHA), which oversees emergency responses, <a data-offer-url="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4&quot;}" href="https://www.ochaopt.org/content/hostilities-gaza-strip-and-israel-flash-update-4" rel="nofollow noopener" target="_blank">said</a> air strikes “targeted several telecommunication installations” and had destroyed two of the three main lines of communications going into Gaza.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google can turn ANC earbuds into a heart rate monitor with no extra hardware (152 pts)]]></title>
            <link>https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</link>
            <guid>38045342</guid>
            <pubDate>Fri, 27 Oct 2023 23:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/">https://9to5google.com/2023/10/27/google-anc-earbuds-heart-rate/</a>, See on <a href="https://news.ycombinator.com/item?id=38045342">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
			<img src="https://9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?quality=82&amp;strip=all&amp;w=1600" srcset="https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5google.com/wp-content/uploads/sites/4/2023/10/google-anc-headphones-heart-rate-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" width="1600" height="800" alt="" fetchpriority="high">
	
	</figure>

<p>Google today <a href="https://research.google/pubs/pub52460/">detailed its research</a> into audioplethysmography (APG) that adds heart rate sensing capabilities to active noise canceling (ANC) headphones and earbuds “with a simple software upgrade.”</p>



<p><a href="https://blog.research.google/2023/10/audioplethysmography-for-cardiac.html?m=1">Google says</a> the “ear canal [is] an ideal location for health sensing” given that the deep ear artery “forms an intricate network of smaller vessels that extensively permeate the auditory canal.”</p>



<p>This audioplethysmography approach works by “sending a low intensity ultrasound probing signal through an ANC headphone’s speakers.”</p>



<blockquote>
<p>This signal triggers echoes, which are received via on-board feedback microphones. We observe that the tiny ear canal skin displacement and heartbeat vibrations modulate these ultrasound echoes.</p>
</blockquote>



<p>A model that Google created works to process that feedback into a heart rate reading, as well as heart rate variability (HRV) measurement. This technique works even with music playing and “bad earbuds seals.” However, it was impacted by body motion, and Google countered with a multi-tone approach that serves as a calibration tool to “find the best frequency that measures heart rate, and use only the best frequency to get high-quality pulse waveform.”</p>



<p>Google performed two sets of studies with 153 people that found APG “achieves consistently accurate heart rate (3.21% median error across participants in all activity scenarios) and heart rate variability (2.70% median error in inter-beat interval) measurements.”</p>



<p>Compared to existing HR sensors, it’s not impacted by skin tones. Ear canal size and “sub-optimal seal conditions” also do not impact accuracy. Google believes this is a better approach than putting traditional&nbsp;photoplethysmograms (PPG) and electrocardiograms (ECG) sensors, as well as a microcontroller, in headphones/earbuds:</p>



<blockquote>
<p>…this sensor mounting paradigm inevitably adds cost, weight, power consumption, acoustic design complexity, and form factor challenges to hearables, constituting a strong barrier to its wide adoption.</p>
</blockquote>



<p>Google closes on:&nbsp;&nbsp;</p>



<blockquote>
<p>APG transforms any TWS ANC headphones into smart sensing headphones with a simple software upgrade, and works robustly across various user activities. The sensing carrier signal is completely inaudible and not impacted by music playing. More importantly, APG represents new knowledge in biomedical and mobile research and unlocks new possibilities for low-cost health sensing.</p>
</blockquote>



<p>“APG is the result of collaboration across Google Health, product, UX and legal teams,” so this coming to <a href="https://9to5google.com/guides/google-pixel-buds-pro/">Pixel Buds</a> is far from guaranteed at this point.</p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBwgKMMqA-Qow-c_gAg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Google to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Large Balloon Reflector: a potentially game-changing antenna design (197 pts)]]></title>
            <link>https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/</link>
            <guid>38043955</guid>
            <pubDate>Fri, 27 Oct 2023 21:08:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/">https://www.nasa.gov/directorates/stmd/nasa-tech-breathes-life-into-potentially-game-changing-antenna-design/</a>, See on <a href="https://news.ycombinator.com/item?id=38043955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Some 30 years ago, a young engineer named Christopher Walker was home in the evening making chocolate pudding when he got what turned out to be a very serendipitous call from his mother.</p>
<p>Taking the call, he shut off the stove and stretched plastic wrap over the pot to keep the pudding fresh. By the time he returned, the cooling air in the pot had drawn the wrap into a concave shape, and in that warped plastic, he saw something – the magnified reflection of an overhead lightbulb – that gave him an idea that could revolutionize space-based sensing and communications.</p>
<p>That idea became the Large Balloon Reflector (LBR), an inflatable device that creates wide collection apertures that weigh a fraction of today’s deployable antennas. Now, with an assist from <a href="https://www.nasa.gov/stmd-the-nasa-innovative-advanced-concepts-niac/">NASA’s Innovative Advanced Concepts</a> (NIAC) program, funded by the agency’s Space Technology Mission Directorate, which supports visionary innovations from diverse sources, Walker’s decades-old vision is coming to fruition.</p>
<p>The concept turns part of the inside surface of an inflated sphere into a parabolic antenna. A section comprising about a third of the balloon’s interior surface is aluminized, giving it reflective properties.</p>
<p>With NIAC funding, and a grant from the U.S. Naval Research Laboratory, Walker was able to develop and demonstrate technologies for a 33-foot-diameter (10 meters) LBR that was carried to the stratosphere by a giant balloon. For comparison, the aperture of NASA’s massive <a href="https://webb.nasa.gov/" rel="noopener">James Webb Space Telescope</a> is over 21 feet (6.5 meters) in diameter.</p>
<p>“There was no place other than NIAC within NASA to get this off the ground,” says Walker, now a astronomy and optical engineering professor at the University of Arizona in Tucson. “At first, I was afraid to share the idea with colleagues because it sounded so crazy. You need a program within NASA that will actually look at the radical ideas, and NIAC is it.”</p>
<p>Parabolic dish antennas use their concave shape to capture and concentrate electromagnetic radiation. The larger the antenna’s diameter, or aperture, the more effective it is for capturing light or radio waves and transmitting radio signals over great distances.</p>
<p>In astronomy, there is a tremendous advantage to placing telescopes above the Earth’s atmosphere, which tends to distort or degrade signals coming from space. The challenge is that traditional large reflector antennas are heavy, unwieldy, and difficult to stow, leading to launch constraints and risky in-space deployment schemes.</p>
<p>The LBR design solves both problems. Made of a thin film structure, it inflates like a beachball, providing a stable parabolic-dish shape without the need for bulky and complex deployable hardware, and can fold into a tiny volume. &nbsp;</p>
<p>In 2018, Freefall Aerospace, a company co-founded by Walker to develop and market the technology, demonstrated the LBR’s potential aboard NASA’s stadium-sized stratospheric balloon, which carried a 3.28-foot scale model to an altitude of 159,000 feet.</p>
<figure>
<p>
<iframe title="CatSat | Student-led CubeSat Project" width="640" height="360" src="https://www.youtube.com/embed/Z4OQG4ABJ_k?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p>
</figure>
<p>Next up for the technology is a high-speed communications demonstration in low Earth orbit aboard a 6-unit CubeSat, about the size of a shoebox, called CatSat. It was selected for flight in 2019 as part of NASA’s CubeSat Launch Initiative. It is a joint effort involving NASA, Freefall Aerospace, the University of Arizona, and Rincon Research Corporation in Tucson, Arizona.</p>
<p>After reaching low-Earth orbit, CatSat’s inflatable antenna deployment system will deploy from its container, inflate to a diameter of about one-and-a-half feet, and begin transmitting back high-definition Earth photos. The mission is slated for launch with several other CubeSats on Firefly Aerospace’s Alpha rocket as part of the Educational Launch of Nanosatellites (ELaNa) 43 mission.</p>
<p>A more ambitious lunar mission concept is also being explored. NASA’s Goddard Space Flight Center in Greenbelt, Maryland, would use the inflatable antenna in tandem with a new instrument called Terahertz Spectrometer for In-Situ Resource Utilization, a miniature, high-power laser precisely calibrated to detect water, a critical exploration resource.</p>
<p>“The technology demonstrated by CatSat opens the door to the possibility of future lunar, planetary and deep-space missions using CubeSats,” said Walker.</p>
<p>It might be difficult to believe this all started because a young engineer’s idea of dinner one evening was what most would consider dessert. Then again, one could say the proof was in the pudding.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visualizing quaternions (2018) (153 pts)]]></title>
            <link>https://eater.net/quaternions</link>
            <guid>38043644</guid>
            <pubDate>Fri, 27 Oct 2023 20:41:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eater.net/quaternions">https://eater.net/quaternions</a>, See on <a href="https://news.ycombinator.com/item?id=38043644">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Android 14's user-profile data bug (166 pts)]]></title>
            <link>https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/</link>
            <guid>38043574</guid>
            <pubDate>Fri, 27 Oct 2023 20:35:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/">https://arstechnica.com/gadgets/2023/10/android-14s-ransomware-data-storage-bug-locks-out-users-remains-unfixed/</a>, See on <a href="https://news.ycombinator.com/item?id=38043574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <header>
            <h4>
      Back up your data!    —
</h4>
            
            <h2 itemprop="description">Users with multiple profiles are getting locked out of local storage and losing data.</h2>
                    </header>
        <div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2013/10/android-lockup.jpg" alt="Android 14’s user-profile data bug seems indistinguishable from ransomware">
      <figcaption><p>Aurich Lawson</p></figcaption>  </figure>

  




<!-- cache hit 188:single/related:e6e51722ea3a7db761540f225f7d54d6 --><!-- empty -->
<p>Android 14 has a nasty storage bug that seems to be affecting users of the "multiple profiles" feature. The bug is about as bad as you can get, with users having "unusable" devices due to getting locked out of device storage. A few users are <a href="https://issuetracker.google.com/issues/305766503#comment284">likening</a> the experience to getting hit with "ransomware."</p>
<p><a href="https://www.theverge.com/2023/10/16/23919957/pixel-6-android-14-upgrade-bugs-multiple-user-profiles">Earlier reports</a> had this bug limited to the Pixel 6, but Google seemed to ignore those reports, and now with a wider rollout, this does not seem device-specific. Everything upgrading to Android 14 this early seems to be affected: <a href="https://issuetracker.google.com/issues/305766503?pli=1">Pixel 6</a>, <a href="https://issuetracker.google.com/issues/305766503#comment122">6a</a>, <a href="https://issuetracker.google.com/issues/305766503#comment7">7</a>, <a href="https://issuetracker.google.com/issues/305766503#comment104">7a</a>, <a href="https://issuetracker.google.com/issues/305766503#comment121">Pixel Fold</a>, and <a href="https://issuetracker.google.com/issues/305766503#comment353">Pixel Tablet.</a></p>
<p>The Google <a href="https://issuetracker.google.com/issues/305766503?pli=1">issue tracker</a> for this is now up to over 350 replies and has had no response from Google. The bug is languishing at only the medium "P2" priority (P0 is the highest) and remains "unassigned," meaning, assuming the tracker is up to date, no one is looking into it.</p>
<p>Some users have <a href="https://issuetracker.google.com/issues/305766503#comment45">helpfully posted</a> log files full of worrying messages, like, "Failed to open directory /data/media/0: Structure needs cleaning." Being locked out of your own device's data partition causes all sorts of bizarre issues. Some users are boot looping, others are stuck on a "Pixel is starting..." message, while others can get into the phone. If your phone tries to continue trucking with no local storage, you'll be inundated with all sorts of error messages. The camera app claims to be "<a href="https://issuetracker.google.com/issues/305766503#comment38">out of storage</a>," and you can't take screenshots because there's nowhere to store the screenshots. The file manager lists <a href="https://issuetracker.google.com/issues/305766503#comment38">0 bytes</a> for every type of file and empty folder, and the files also aren't viewable from a PC over USB. The System UI and Settings also keep crashing. Basically, computers need storage to function!</p>
<p>Android's user-profile system allows for both multiple users on a single device (which is good for tablets) and splitting up <a href="https://arstechnica.com/information-technology/2015/03/a-review-of-android-for-work-dual-persona-support-comes-to-android/">"home" and "work" profiles</a> to keep your work data separate from your personal data, via duplicate apps. It sounds like the bug is only hitting users who take advantage of this rarely used feature, with lots of reports that the primary profile—that's usually the important one—gets locked out.</p>                                            
                                                        
<p>Several users are complaining about the data lost from all of this, so it's a good time to remind people to always have a backup of everything on their phones. Even straight out of the box, Android has options for Google Photos automatic backups, Play Games storage of your game data, and a million other cloud-based data features (it would be nice if Android phones had a comprehensive whole-phone backup feature, though). While it is totally reasonable to expect your OS to keep running after an update, phones are uniquely vulnerable to getting lost/stolen/damaged, so having everything get stored somewhere else is a great idea. Shockingly, <a href="https://www.reddit.com/r/GooglePixel/comments/178jj3i/need_help_pixel_7_stuck_on_phone_is_starting/">several users</a> report the phone is<a href="https://issuetracker.google.com/issues/305766503#comment43"><em> automatically</em></a> doing a factory reset, which deletes all your data, shutting down any possibility of data recovery. This feature should probably not exist, but it's another sign that phones are not a reliable storage medium for critical data.</p>
<p>What's so strange about how Google is handling this bug is that the company <em>has tools</em> to deal with this. Google delivers software on a slow and often frustrating "roll out" strategy, where a small percentage of users will get an update at first, and as the days pass, more and more users opt in to the update. Google does this to see if any problems pop up via its extensive Android analytics system, and if a problem is detected, the update rollout can be halted, limiting the problem to as few people as possible.</p>
<p>Why didn't that happen here? Surely, a bug where people are locked out of their phones and possibly lose data is worth halting a rollout, but it never happened. Google's entire response to this problem has been lacking. To our knowledge, no one from Google has officially addressed the issue in the 10-ish days it has been around. It hasn't issued statements to the several sites that have already reported on this. No one is replying to the bug tracker, and the issue is unassigned. What's up, Google?</p>

                                                </div>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New devices could change the way we measure blood pressure (107 pts)]]></title>
            <link>https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure</link>
            <guid>38043555</guid>
            <pubDate>Fri, 27 Oct 2023 20:34:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure">https://knowablemagazine.org/article/technology/2023/devices-could-change-how-we-measure-blood-pressure</a>, See on <a href="https://news.ycombinator.com/item?id=38043555">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>Knowable Magazine is from <a href="http://www.annualreviews.org/">Annual Reviews</a>, a nonprofit publisher dedicated to synthesizing and integrating knowledge for the progress of science and the benefit of society.</p>
            <p>© 2023 Annual Reviews</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Images from the 2023 Nikon Small World Photomicrography Competition (155 pts)]]></title>
            <link>https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition</link>
            <guid>38043127</guid>
            <pubDate>Fri, 27 Oct 2023 19:54:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition">https://www.nikonsmallworld.com/galleries/2023-photomicrography-competition</a>, See on <a href="https://news.ycombinator.com/item?id=38043127">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  
  <section id="galleryapp" v-touch:swipe.left="pageNext" v-touch:swipe.right="pagePrev">

            <section id="winner_gallery">
    <h2>Top 20</h2>
        
        <hr>
    
        
              </section>
              <section id="honorable-mention_gallery">
    <h2>Honorable Mentions</h2>
        
        <hr>
        
    
        
              </section>
              <section id="image-of-distinction_gallery">
    <h2>Images of Distinction</h2>
        
        <hr>
        
    
            
          </section>
      <section id="judges">
    <h2>Judges</h2>
  <div>
                      <div>
          <h3>Dr. Clare Waterman</h3>
          <p><em>Cell Biologist and Member of the National Academy of Sciences</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Clare-Waterman.jpg" width="120"></p><p>Clare Waterman graduated from Mount Holyoke College with a B.A. in biochemistry in 1989. From there, she received an M.S. in exercise science in 1991 from the <a href="https://www.nikonsmallworld.com/organizations/university-of-massachusetts-boston">University of Massachusetts</a>, and her Ph.D. in cell biology from the <a href="https://www.nikonsmallworld.com/organizations/university-of-pennsylvania">University of Pennsylvania</a> in 1995. She then spent nine years as a professor in the Department of Cell Biology at <a href="https://www.nikonsmallworld.com/organizations/the-scripps-research-institute">The Scripps Research Institute</a> in La Jolla, California. Dr. Waterman has received numerous awards and honors for her work, including election to the National Academy of Sciences and the Arthur S. Flemming Award for Public Service in Basic Science. Dr. Waterman has made fundamental advances in the understanding of cell migration and has authored or co-authored more than 150 papers. She currently serves on the editorial boards of <em>Current Biology</em> and <em>Journal of Microscopy</em>. Dr. Waterman is a member of the American Society for Cell Biology, Royal Microscopical Society, Biophysical Society, and is a Council Member of Gordon Research Conferences Organization.</p>
        </div>
                      <div>
          <h3>James Cutmore</h3>
          <p><em>Picture Editor at BBC Science Focus Magazine</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/James-Cutmore.jpg" width="120"></p><p>James Cutmore has worked for BBC’s science and technology publication <em><a href="https://www.sciencefocus.com/" target="_blank" rel="noreferrer noopener">BBC Science Focus</a></em> since 2004, telling compelling science stories through stunning science imagery. He holds a bachelor’s degree in fine art from the University of West England and was highly commended in 2020, having been nominated for the British Society of Magazine Editors Talent Awards. He is passionate about sourcing images that not only illustrate a range of difficult and complex concepts but highlight positive technology and the natural world. For many years he was a judge for the Wellcome Trust’s image competition, as well as the Royal Photographic Society.</p>
        </div>
                      <div>
          <h3>Ed Cara</h3>
          <p><em>Science and Health Reporter at Gizmodo</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Ed-Cara.jpg" width="120"></p><p>Born and raised in New York City, Ed Cara currently covers the public health and science beat at <em><a href="https://gizmodo.com/author/edcara" target="_blank" rel="noreferrer noopener">Gizmodo</a></em>. His past feature and investigative reporting can be seen in <em>The Atlantic</em>, <em>Pacific Standar</em>d and <em>Undark Magazine</em>.</p>
        </div>
                      <div>
          <h3><a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">Dr. Igor Siwanowicz</a></h3>
          <p><em>Research Scientist at Howard Hughes Medical Institute</em><br></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Igor-Siwanowicz.jpg" width="120"></p><p><a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">Igor Siwanowicz</a> began his career in biochemistry, but his love for animals and nature, and a desire to see the bigger picture, drove him to refocus his scientific discipline on neurobiology. This transition was facilitated by his expertise in invertebrate morphology, which he developed through macro photography. A confocal microscope became his key tool of trade, allowing for even more intimate insight into natural forms than a macro lens. Dr. Siwanowicz’s images have placed a total of <a href="https://www.nikonsmallworld.com/people/dr.-igor-siwanowicz">25 times in the Nikon Small World</a> and other scientific imaging competitions. In 2020, he received an award from the Royal Photographic Society for scientific imaging. For the past 10 years, Dr. Siwanowicz has been studying the anatomy and biomechanics of movement in invertebrates at the <a href="https://www.nikonsmallworld.com/organizations/howard-hughes-medical-institute-hhmi">Janelia Research Campus of Howard Hughes Medical Institute</a> in Ashburn, Virginia.</p>
        </div>
                      <div>
          <h3>Dr. Gary Laevsky</h3>
          <p><em>Director of the Confocal Imaging Facility at Princeton University</em><br><a href="https://www.nikonsmallworld.com/organizations/princeton-university"></a></p>
                    <p><img src="https://www.nikonsmallworld.com/images/bio-photos/_squareThumb2x/Gary-Laevsky.jpg" width="120"></p><p>Gary Laevsky received his Ph.D. in Cell Biology from the University of Connecticut in 2003 and completed his post-doctoral research at <a href="https://www.nikonsmallworld.com/organizations/the-scripps-research-institute">The Scripps Research Institute</a> in La Jolla, California. Prior to joining the Department of Molecular Biology at <a href="https://www.nikonsmallworld.com/organizations/princeton-university">Princeton University</a>, he was the Product Manager for Olympus, a Senior Biosystems Applications Manager for Nikon, and an Imaging Applications Specialist for Andor Technology. Since joining Princeton University in 2013 as Director of the Confocal Imaging Facility, he has achieved the title of Senior Professional Specialist, the highest non-faculty level position that can be achieved. He has also been selected to sit on the Committee on Appointments and Advancement for Professional Researchers and Professional Specialists. Dr. Laevsky is a co-founder of the North Atlantic Microscopy Society (NAMS), Course Laboratory Director for the Analytical and Quantitative Light Microscopy (AQLM) course at the <a href="https://www.nikonsmallworld.com/organizations/marine-biological-laboratory">Marine Biological Laboratory (MBL)</a>, and co-founder of the Light-Sheet Fluorescence Microscopy (LSFM) Conference and Workshop, also at MBL.</p>
        </div>
          </div>
    
      </section>
  </section>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shadow: New browser engine made almost entirely in JavaScript (417 pts)]]></title>
            <link>https://goose.icu/introducing-shadow/</link>
            <guid>38043033</guid>
            <pubDate>Fri, 27 Oct 2023 19:46:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://goose.icu/introducing-shadow/">https://goose.icu/introducing-shadow/</a>, See on <a href="https://news.ycombinator.com/item?id=38043033">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><h3>&lt;shadow&gt; is a new novel browser engine made almost entirely in JS</h3></p></div><section><p>So I started making a browser engine (for fun) a few days ago, it felt kind of inevitable so here we are. Here’s a short rundown. <a href="https://github.com/canadahonk/shadow" target="_blank" rel="noopener">Source code too!</a></p><h2 id="try-it-in-your-browserhttpsshadowgooseicu"><a href="https://shadow.goose.icu/" target="_blank" rel="noopener">Try it in your browser!</a></h2><h4 id="screenshot-of-ltshadowgts-welcome-page-running-inside-ltshadowgt-as-of-writing">Screenshot of <em><strong>&lt;shadow&gt;</strong></em>’s welcome page running inside <em><strong>&lt;shadow&gt;</strong></em> (as of writing)</h4><p><img src="https://github.com/CanadaHonk/shadow/assets/19228318/9431b624-94aa-4209-87c9-60d2478badf6" alt="Screenshot of shadow&amp;rsquo;s welcome page in shadow"></p><h3 id="what">What?</h3><p>A browser(/web) engine essentially takes in a URL(/etc) and gives you it rendered into a window for you to view and interact with. <em><strong>&lt;shadow&gt;</strong></em> does this too, almost entirely from scratch, made in JS. It runs in your browser! Node backend soon™ too? The host browser(/etc) is only used for networking (<code>fetch</code>) and renderer backend (<code>&lt;canvas&gt;</code>).</p><h4 id="components-of-ltshadowgt">Components of <em><strong>&lt;shadow&gt;</strong></em></h4><p><img src="https://mermaid.ink/svg/pako:eNpVkT1vhDAMhv8KygxVaTuB1Kljp96axSLukQtJUByK0N399zogFJAyOI_f1x_yXXReoWjENcDYF98_rXRF0YH7A6peqs_eU0zEYZx9MNpdj5R6UH6uGEQ7jBAIwwlnV8LKW2YdUVZmH2c4vzbfBYwGWPwUszn_t4hRQKcwbOX2-KQ8wG2vBGcgmzahUXPSemdwOXKjkXDIjR9-jJV2j9vqvlEed42TTbp1Lu3MJS4DFq9lXdb83gqKwRtsAqr2LKnfy_pjT_MFllaUwvI8oBXf5J7UUsQeLUrRcKggGCmke7IOpugvi-tEE8OEpZhGBRG_NHAhK5pfGAif_zYlpas?bgColor=101418" alt="Component flowchart">
(red = external/not me)</p><h3 id="why-make-it">Why make it?</h3><p>It’s just for fun, no really. Learning too. It will probably never work with 90% of websites, and that’s okay. <del>Also it’s funny to see people react in many forms of “wtf?"</del></p><h3 id="screenshot-of-serenityosorg-running-inside-ltshadowgt-left-vs-firefox-right">Screenshot of serenityos.org running inside <em><strong>&lt;shadow&gt;</strong></em> (left) vs Firefox (right)</h3><p>Pretty spot on! No list markers yet. Colors are different as UA/browser defined.</p><h3 id="name">Name</h3><p>As with all my recent projects, the name is because I thought it was kind of funny at the time. <em><strong>&lt;shadow&gt;</strong></em> is named after the defunct &lt;shadow&gt; element. I mostly stole this idea from Blink (was it intentional to name it after a dead HTML element at the time?). Also it sounds spooky and mysterious so that’s a bonus I guess? (It’s also stylized as <em><strong>&lt;shadow&gt;</strong></em> because of this.)</p><h3 id="it-supports-javascript">It supports JavaScript??!</h3><p>Yes, kind of. This is quite complicated (as you can probably imagine), so I’ll do it in a separate future dedicated post if you’re interested (DM/reply me on twitter).</p><h3 id="why-publish-it">Why publish it?</h3><p>Why not? If someone can learn something or just find what I make fun/interesting, that makes me happy :)</p><h3 id="but-making-a-new-browser-engine-is-impossible">But making a new browser engine is impossible!</h3><p><a href="https://ladybird.dev/" target="_blank" rel="noopener">No</a>. <a href="https://servo.org/" target="_blank" rel="noopener">It</a>. <a href="https://www.ekioh.com/flow-browser/" target="_blank" rel="noopener">Isn’t</a>! <em>(Also I don’t really care how possible/feasible something is.)</em></p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The convolution empire strikes back (123 pts)]]></title>
            <link>https://gonzoml.substack.com/p/the-convolution-empire-strikes-back</link>
            <guid>38042954</guid>
            <pubDate>Fri, 27 Oct 2023 19:39:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gonzoml.substack.com/p/the-convolution-empire-strikes-back">https://gonzoml.substack.com/p/the-convolution-empire-strikes-back</a>, See on <a href="https://news.ycombinator.com/item?id=38042954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><p><strong>Authors</strong><span>: Samuel L. Smith, Andrew Brock, Leonard Berrada, Soham De</span><br><strong>Paper</strong><span>: </span><a href="https://arxiv.org/abs/2310.16764" rel="">https://arxiv.org/abs/2310.16764</a></p><p><span>The empire strikes back for the second time (I’d say the first time was </span><a href="https://arxiv.org/abs/2201.03545" rel="">ConvNeXt</a><span>).</span></p><p><span>There's a common perception that convolutional networks (ConvNets) perform well on small to medium-sized datasets, but when it comes to extremely large datasets, they fall short of transformers, particularly Vision Transformers (ViT) - more on this </span><a href="https://arxiv.org/abs/2010.11929" rel="">here</a><span>. The latest research from DeepMind challenges this notion.</span></p><p><span>It has been believed that the scalability of transformers surpasses that of ConvNets, but there's scant evidence to back this up. Furthermore, many studies that delve into ViT compare them with relatively weak convolutional baselines, sometimes training with enormous computational budgets exceeding 500k TPU-v3 core hours. This equates to approximately $250k based on </span><a href="https://cloud.google.com/tpu/pricing" rel="">current on-demand prices</a><span>. Such a budget is significantly beyond what's typically allocated for training convolutional networks.</span></p><p><span>In this study, the authors utilize the </span><a href="https://arxiv.org/abs/2102.06171" rel="">NFNet (Normalizer-Free ResNets) family</a><span>. They progressively increase the width and depth of these networks. This is a purely convolutional architecture and the latest of its kind to achieve </span><a href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="">state-of-the-art (SoTA) results on ImageNet</a><span>. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png" width="484" height="260.3092105263158" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:327,&quot;width&quot;:608,&quot;resizeWidth&quot;:484,&quot;bytes&quot;:232842,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77ee4a70-25de-4ae3-8672-8130a7ddeebc_608x327.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Without major modifications (except for simple hyperparameter tuning), these architectures are pre-trained on the expansive JFT-4B dataset (with 4 billion labeled images across 30k classes) with computational budgets ranging from 0.4k to 110k TPU-v4 core compute hours. It's noteworthy that the TPU-v4 has about twice the computational power as the v3, but with the same memory capacity. </p><p><span>Subsequently, the pre-trained networks are fine-tuned on ImageNet using </span><a href="https://arxiv.org/abs/2010.01412" rel="">Sharpness-Aware Minimization (SAM)</a><span>. The results show performance on par with ViT models that have comparable budgets. All models consistently improve as computational power is added. The largest model, NFNet-F7+, is pre-trained over 8 epochs (110k TPU-v4 hrs), fine-tuned (1.6k TPU-v4 hrs), and achieves 90.3% top-1 accuracy (90.4% with 4x augmentation).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png" width="1211" height="750" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:1211,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:580050,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b0744f9-7fb3-40e2-9a23-196f66c0ad79_1211x750.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>An interesting observation during the training process is the clear linear trend of the validation loss curve. This is consistent with the log-log scaling law between validation loss and the amount of computation during pre-training. This mirrors the same scaling laws observed for transformers in language modeling tasks. The authors identified an optimal scaling regime wherein the model size and training epochs increase at the same rate. They also pinpointed optimal learning rates.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png" width="1204" height="645" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:645,&quot;width&quot;:1204,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:693438,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1a1dc9e-8f80-4029-8de6-5fbded805068_1204x645.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Another intriguing finding is that models with the lowest validation loss don't always yield the best performance post fine-tuning. A similar phenomenon has been observed with transformers. For fine-tuning, slightly larger models and slightly smaller epoch budgets consistently outperform others. Occasionally, a slightly higher learning rate can also be beneficial.</p><p><span>The takeaway? </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html" rel="">The bitter lesson</a><span>! Computational power and data remain the key driving factors.</span></p><p>However, it's important to note that models have their own inductive biases. The authors acknowledge that in certain situations, ViT might be a more suitable choice, possibly due to its ability to employ uniform components across different modalities.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft is failing to deliver emails that mention newclimate.org (173 pts)]]></title>
            <link>https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the</link>
            <guid>38042114</guid>
            <pubDate>Fri, 27 Oct 2023 18:30:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the">https://newclimate.org/news/microsoft-error-or-external-attack-causing-disruption-to-email-communication-across-the</a>, See on <a href="https://news.ycombinator.com/item?id=38042114">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                            <center>
<p><em>(in case you share a link to this page by email, please use this bit.ly link <a href="https://bit.ly/NewClimate_MicrosoftStatement">https://bit.ly/NewClimate_MicrosoftStatement</a> to ensure delivery)</em></p>
</center>

<p>Since 17 October, to our knowledge any email with the NewClimate URL (newclimate dot org) in the email body, link, signature, reply header or contained anywhere in an attachment is unjustifiably quarantined by Microsoft email servers without any notice, regardless of who sends or receives the email.</p>

<p>That our own email flow is disrupted is the least of our problems: all Microsoft tenants are afflicted by the same issue when this URL appears in their emails, or any attachments they share, even when we are not a party to the communications.</p>

<p><strong>Hundreds of governmental and non-governmental organisations working on climate change appear to be experiencing disruption to email communication when their communications contain any reference to the NewClimate URL. </strong></p>

<p>For example, as per our understanding:</p>

<ul><li>No organisation using Microsoft email services can currently send the <strong>IPCC Sixth Assessment Report of Working Group 3 </strong>as an attachment to anyone else (newclimate dot org URL appears 11 times in the report). The same applies to <strong>hundreds of other relevant scientific papers and reports </strong>from any organisations, where NewClimate URLs appear on the reference lists.</li>
	<li>Any <strong>multi-organisation email chain</strong> where <em>any </em>of the participants uses Microsoft email services is breaking down in the case that NewClimate URLs are included. This could arise either because a NewClimate colleague is on the mailing list in the chat history, or if a NewClimate publication is linked to, in the email or the chat history.</li>
	<li>Even <strong>a link to this article on the NewClimate website cannot be spread by email </strong>if the sender or recipient uses Microsoft as email service.</li>
</ul><p>We understand that the majority of our partner organisations within the climate community use Microsoft email services, including the United Nations Framework Convention for Climate Change (UNFCCC) and the Intergovernmental Panel on Climate Change (IPCC).</p>

<p>It came as a surprise to us that this is even possible. It remains unclear whether this is the result of a targeted attack on Microsoft’s infrastructure against NewClimate, or simply a highly unfortunate error on the part of Microsoft. In our consultations with Microsoft and a number of independent IT experts, we have confirmed that we are not on any blacklist and our website is also free of malware.</p>

<p>We are fully dependent on Microsoft to prioritise and solve the issue, but Microsoft support agents have been difficult to engage and – to our understanding – disinclined to prioritise the issue. It is not clear whether Microsoft is aware of the inconvenience and disruption beyond our own organisation.</p>

<p>Beyond being an existential threat to our own organisation, this issue could significantly disrupt communication within the climate community at a time when it is most critical in the run up to COP28 in at the end of 2023 in Dubai. <strong>We greatly appreciate any support to bring this issue to the attention of Microsoft’s senior management so that it can be prioritised and resolved.</strong></p>

<p>This is extremely unfortunate and we apologise for any inconvenience that this may cause or may have caused.</p>

<p>We understand that partner organisations who use Microsoft email exchange services and that are experiencing email deliverability problems because of this issue may be able to apply a band-aid fix for email flow, by reporting to Microsoft that the NewClimate URL should not be blocked (see ‘Submission form for reporting false positives for Microsoft’s URL detonation policy’, available in the Microsoft admin center). This may help individual organisations to improve issues with their own incoming and outgoing email flow, but it does not help to resolve communication issues with other organisations that use Microsoft.</p>

<p>Emails are only quarantined when the text “newclimate(dot)org” appears in the email and not if a diverted link to the NewClimate website through e.g. tinyurl is included instead.</p>

                                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google paid $26.3B in 2021 to be the default search engine everywhere (174 pts)]]></title>
            <link>https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion</link>
            <guid>38041335</guid>
            <pubDate>Fri, 27 Oct 2023 17:26:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion">https://www.theverge.com/2023/10/27/23934961/google-antitrust-trial-defaults-search-deal-26-3-billion</a>, See on <a href="https://news.ycombinator.com/item?id=38041335">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The <a href="https://www.theverge.com/23869483/us-v-google-search-antitrust-case-updates"><em>US v. Google</em> antitrust trial</a> is about many things, but more than anything, it’s about the power of defaults. Even if it’s easy to switch browsers or platforms or search engines, the one that appears when you turn it on matters a lot. Google obviously agrees and has paid a staggering amount to make sure it is the default: testimony in the trial revealed that Google spent a total of $26.3 billion in 2021 to be the default search engine in multiple browsers, phones, and platforms.</p><p>That number, the sum total of all of Google’s search distribution deals, came out during the Justice Department’s cross-examination of Google’s search head, Prabhakar Raghavan. It was made public after a debate earlier in the week between the two sides and Judge Amit Mehta over whether the figure should be redacted. Mehta has begun to push for more openness in the trial in general, and this was one of the most significant new pieces of information to be shared openly.</p><p>Just to put that $26.3 billion in context: Alphabet, Google’s parent company, announced in <a href="https://www.theverge.com/2023/10/24/23929496/google-alphabet-q3-2023-earnings-ads-ai-sge">its recent earnings report</a> that Google Search ad business brought in about $44 billion over the last three months and about $165 billion in the last year. Its entire ad business — which also includes YouTube ads — made a bit under $90 billion in profit. This is all back-of-the-napkin math, but essentially, Google is giving up about 16 percent of its search revenue and about 29 percent of its profit to those distribution deals. </p><div><p>Google is giving up about 16 percent of its search revenue and about 29 percent of its profit to those distribution deals</p></div><p>Most of that money, of course, goes to Apple. <em>The New York Times</em> recently reported that Google’s deal to be the default search engine in Safari across Google products cost the company <a href="https://www.theverge.com/2023/10/26/23933206/google-apple-search-deal-safari-18-billion">about $18 billion</a> in 2021. (Apple’s outsize percentage of the total is why that particular deal <a href="https://www.theverge.com/2023/10/11/23913287/us-v-google-apple-search-deal">has been such a focus</a> of the first weeks of the trial.) In addition, Google pays Mozilla for default placement in Firefox; it pays Samsung for the same on its devices; and it has deals with many device makers, wireless carriers, and other platforms to be the default as well.</p><p>Until now, these numbers have been closely held secrets, leaving competitors and analysts to speculate about exactly what it’s worth to Google to be the near-universal default choice. The information also comes as Google is beginning its defense portion of the trial, which started with Raghavan testifying that Google is at perpetual risk of losing its cool — and its users — to platforms like TikTok and ChatGPT. Raghavan said that some users call his search engine “Grandpa Google.” (Raghavan has been <a href="https://www.theverge.com/23365101/tiktok-search-google-replacement">saying stuff like this</a> for a while now.) He also said that he sees Yelp and Amazon as competitors and that, in such a hot market, Google has to do everything it can to stay relevant and compete. The Justice Department, on the other hand, is making the case that spending $26.3 billion on securing default status everywhere is actually a way to make sure the market <em>isn’t</em> competitive. After a few more weeks of testimony, Mehta will have to decide who’s right.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generate images in one second on your Mac using a latent consistency model (210 pts)]]></title>
            <link>https://replicate.com/blog/run-latent-consistency-model-on-mac</link>
            <guid>38040702</guid>
            <pubDate>Fri, 27 Oct 2023 16:37:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://replicate.com/blog/run-latent-consistency-model-on-mac">https://replicate.com/blog/run-latent-consistency-model-on-mac</a>, See on <a href="https://news.ycombinator.com/item?id=38040702">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>Latent consistency models (LCMs) are based on Stable Diffusion, but they can generate images much faster, needing only 4 to 8 steps for a good image (compared to 25 to 50 steps). By running an LCM on your M1 or M2 Mac you can generate 512x512 images at a rate of one per second.</p>
<p><a href="https://arxiv.org/abs/2310.04378">Simian Luo et al</a> released the first Stable Diffusion distilled model. It’s distilled from the Dreamshaper fine-tune by incorporating classifier-free guidance into the model’s input. Only one model has been distilled so far, but more will be released. Stable Diffusion 2.1 and SDXL are being worked on by the paper authors.</p>
<p>You can run the first <a href="https://replicate.com/luosiallen/latent-consistency-model">latent consistency model in the cloud on Replicate</a>, but it’s also possible to run it locally. As well as generating predictions, you can hack on it, modify it, and build new things.</p>
<p>We’ve written this guide to help you get started.</p>
<video autoplay="" controls="" loop="" src="https://replicate.com/static/blog/run-latent-consistency-model-on-mac/output.mp4"></video>

<h2 id="prerequisites">Prerequisites</h2>
<p>You’ll need:</p>
<ul>
<li>a Mac with an M1 or M2 chip</li>
<li>16GB RAM or more</li>
<li>macOS 12.3 or higher</li>
<li>Python 3.10 or above</li>
</ul>
<p>We’ve found that an M1 Max or M2 with 32GB RAM can generate images in 1 second. An M1 Pro with 16GB RAM can generate images in 2 to 4 seconds. Please <a href="https://github.com/replicate/latent-consistency-model">share your benchmarks with us on our Github repository</a>.</p>
<h2 id="set-up-python">Set up Python</h2>
<p>You need Python 3.10 or above. Run <code>python -V</code> to see what Python version you have installed:</p>
<pre><code>$ python3 -V
Python 3.10.6
</code></pre>
<p>If it’s 3.10 or above, like here, you’re good to go! Skip on over to the next step.</p>
<p>Otherwise, you’ll need to install Python 3.10. The easiest way to do that is with Homebrew. First, <a href="https://brew.sh/">install Homebrew</a> if you haven’t already.</p>
<p>Then, install the latest version of Python:</p>
<pre><code>brew update
brew install python
</code></pre>
<p>Now if you run <code>python3 -V</code> you should have 3.10 or above. You might need to reopen your console to make it work.</p>
<h2 id="clone-the-repository-and-install-the-dependencies">Clone the repository and install the dependencies</h2>
<p>Run this to <a href="https://github.com/replicate/latent-consistency-model">clone the LCM script from Github</a>:</p>
<pre><code>git clone https://github.com/replicate/latent-consistency-model.git
cd latent-consistency-model
</code></pre>
<p>Then, set up a virtualenv to install the dependencies:</p>
<pre><code>python3 -m pip install virtualenv
python3 -m virtualenv venv
</code></pre>
<p>Activate the virtualenv:</p>
<pre><code>source venv/bin/activate
</code></pre>
<p>(You’ll need to run this command again any time you want to run the script.)</p>
<p>Then, install the dependencies:</p>
<pre><code>pip install -r requirements.txt
</code></pre>
<h2 id="run-it">Run it!</h2>
<p>Now, you can run your latent consistency model. The script will automatically download the <a href="https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7"><code>SimianLuo/LCM_Dreamshaper_v7</code></a> (3.44 GB) and <a href="https://huggingface.co/CompVis/stable-diffusion-safety-checker">safety checker</a> (1.22 GB) models from HuggingFace.</p>
<pre><code>python main.py \
  "a beautiful apple floating in outer space, like a planet" \
  --steps 4 --width 512 --height 512
</code></pre>
<p>You’ll see an output like this:</p>
<pre><code>Output image saved to: output/out-20231026-144506.png
Using seed: 48404
100%|███████████████████████████| 4/4 [00:00&lt;00:00,  5.54it/s]
</code></pre>
<p>We’ve also added a <code>--continous</code> flag, so you can keep on generating image after image until your harddrive is full. Generations after the first one will run a bit faster too.</p>
<pre><code>python main.py \
  "a beautiful apple floating in outer space, like a planet" \
  --steps 4 --width 512 --height 512 --continuous
</code></pre>
<p>That’s it!</p>
<p><img alt="Latent consistency model generation of a beautiful apple floating in outer space, like a planet" src="https://replicate.com/static/blog/run-latent-consistency-model-on-mac/output.webp"></p>
<h2 id="next-steps">Next steps</h2>
<ul>
<li>If you’re struggling to get this set up, or you aren’t getting fast speeds, <a href="https://discord.gg/replicate">ask in our Discord for some help</a>. Or take a look at <a href="https://github.com/replicate/latent-consistency-model">our Github repository</a>.</li>
<li>You can <a href="https://replicate.com/docs/guides/push-a-model">push custom models to Replicate</a> if you want to host your creations.</li>
<li><a href="https://replicate.com/fofr/latent-consistency-model">Try out a latent consistency model with img2img support</a> on Replicate</li>
<li><a href="https://x.com/replicate">Follow @replicate on X</a>.</li>
</ul>
<p>Happy hacking!</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Biblos – Semantic Bible Embedded Vector Search and Claude LLM (125 pts)]]></title>
            <link>https://github.com/dssjon/biblos</link>
            <guid>38040591</guid>
            <pubDate>Fri, 27 Oct 2023 16:28:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/dssjon/biblos">https://github.com/dssjon/biblos</a>, See on <a href="https://news.ycombinator.com/item?id=38040591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-biblos---bible-exploration-with-vector-search-and-summarization" dir="auto"><a href="#biblos---bible-exploration-with-vector-search-and-summarization">Biblos - Bible Exploration with Vector Search and Summarization</a></h2>
<p dir="auto">Biblos allows semantic search and summarization of Bible passages using state-of-the-art NLP techniques:</p>
<ul dir="auto">
<li>Vector search over the entire Bible text using <a href="https://github.com/chroma-core/chroma">Chroma</a> and BAAI BGE embeddings</li>
<li>Summarization of search results using <a href="https://www.anthropic.com/" rel="nofollow">Anthropic's Claude</a> large language model</li>
</ul>
<p dir="auto">This enables powerful semantic search over biblical texts to find related passages, along with high quality summaries of the relationships between verses on a given topic.</p>
<h2 tabindex="-1" id="user-content-features" dir="auto"><a href="#features">Features</a></h2>
<ul dir="auto">
<li>Semantic search over the entire Bible text</li>
<li>Summarization of search results using Claude LLM</li>
<li>Web UI built with Streamlit for easy exploration</li>
<li>Leverages Chroma for vector search over BAAI BGE embeddings</li>
<li>Modular design allowing swapping of components like DB, embeddings, LLM etc.</li>
</ul>
<h2 tabindex="-1" id="user-content-architecture" dir="auto"><a href="#architecture">Architecture</a></h2>
<p dir="auto">Biblos follows a RAG (Retrieval Augmented Generation) architecture:</p>
<ol dir="auto">
<li>Bible text is indexed in a Chroma vector database using BGE sentence embeddings</li>
<li>User searches for a topic, and relevant passages are retrieved by semantic similarity</li>
<li>Top results are collated and passed to Claude to generate a summarization</li>
</ol>
<p dir="auto">This enables combining the strengths of dense vector search for retrieval with a powerful LLM for summarization.</p>
<p dir="auto">The UI is built using Streamlit for easy exploration, with Python code modularized for maintainability.</p>
<h2 tabindex="-1" id="user-content-running-biblos" dir="auto"><a href="#running-biblos">Running Biblos</a></h2>
<p dir="auto">To run Biblos locally:</p>
<ol dir="auto">
<li>Install requirements</li>
<li>Download and preprocess Bible text into a Chroma database</li>
<li>Launch the Streamlit app:</li>
</ol>
<div data-snippet-clipboard-copy-content="pip install -r requirements.txt

python create_db.py

streamlit run app.py"><pre><code>pip install -r requirements.txt

python create_db.py

streamlit run app.py
</code></pre></div>
<h2 tabindex="-1" id="user-content-credits" dir="auto"><a href="#credits">Credits</a></h2>
<p dir="auto">Biblos leverages the following open source projects:</p>
<ul dir="auto">
<li><a href="https://github.com/langchain-ai/langchain">Langchain</a> - Building LLMs through composability</li>
<li><a href="https://github.com/chroma-core/chroma">Chroma</a> - Vector similarity search</li>
<li><a href="https://www.anthropic.com/" rel="nofollow">Anthropic</a> - Claude summarization model</li>
<li><a href="https://huggingface.co/BAAI/bge-large-en-v1.5" rel="nofollow">BAAI BGE Embeddings</a> - Text embeddings</li>
<li><a href="https://streamlit.io/" rel="nofollow">Streamlit</a> - Web UI</li>
</ul>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA['Not of faculty quality': How Penn mistreated Nobel Prize-winning researcher (478 pts)]]></title>
            <link>https://www.thedp.com/article/2023/10/penn-katalin-kariko-university-relationship-mistreatment</link>
            <guid>38040468</guid>
            <pubDate>Fri, 27 Oct 2023 16:16:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thedp.com/article/2023/10/penn-katalin-kariko-university-relationship-mistreatment">https://www.thedp.com/article/2023/10/penn-katalin-kariko-university-relationship-mistreatment</a>, See on <a href="https://news.ycombinator.com/item?id=38040468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                
        <p>Three weeks ago, Penn hosted a <a href="https://www.thedp.com/article/2023/10/penn-nobel-prize-winners-flash-mob" target="_blank">flash mob</a> for Katalin Karikó after she won the 2023 Nobel Prize in Medicine. But the celebration masked a tumultuous, decades-long relationship between Karikó and the University.</p>
<p>Karikó, an adjunct professor of neurosurgery at the Perelman School of Medicine, won the <a href="https://www.thedp.com/article/2023/10/penn-medicine-kariko-weissman-scientists-nobel-prize-mrna-covid-technology" target="_blank">Nobel Prize in Medicine</a> for her past research into mRNA technology alongside co-laureate and Roberts Family Professor in Vaccine Research at the Medical School Drew Weissman. Karikó and Weissman's research was critical for the development of the COVID-19 vaccines — from <a href="https://www.thedp.com/article/2023/04/penn-licensing-revenue-first-national-universities" target="_blank">which Penn has earned around $1.2 billion</a>.</p>
<p>"At a University built around [a] Franklin spirit, there are no better exemplars of these character traits than our Nobel laureates, Dr. Kati Karikó and Dr. Drew Weissman," Penn President Liz Magill <a href="https://www.thedp.com/article/2023/10/penn-medicine-kariko-weissman-scientists-nobel-prize-mrna-covid-technology" target="_blank">said</a> at a press conference the day the prize was named.</p>
<p>However, eight current and former colleagues of Karikó told The Daily Pennsylvanian that — over the course of three decades — the University repeatedly shunned Karikó and her research, despite its groundbreaking potential.&nbsp;</p>
<p>The colleagues told a story of a researcher whose work ethic helped her succeed against all odds — including doubtful administrators, language barriers, and a system that cuts costs by demoting researchers who fail to earn grant funding.</p>
<p>"We acknowledge and are grateful for the valuable contributions Dr. Karikó has made to science and to Penn throughout her time with the University," a University spokesperson wrote in a statement to the DP.</p>
<p>In 1989, four years after she arrived in the United States, Karikó was appointed an adjunct professor at the Medical School. She worked on mRNA research under cardiologist Elliot Barnathan until his departure in 1997.</p>
<p>From the very beginning of her time at Penn, Karikó's research was overlooked by medical school executives, she wrote in her recently published memoir, "Breaking Through: My Life in Science." These executives included Jim Wilson, the director of Penn’s <a href="https://www.thedp.com/article/2022/04/upenn-gene-therapy-program-jim-wilson-corrupt-investigation-toxic-workplace" target="_blank">embattled Gene Therapy Program</a>, and Judith Swain, chief of cardiovascular medicine. Wilson remains a member of Penn's faculty.</p>
<p>Wilson did not respond to a request for comment.&nbsp;</p>
<p>“[Jim] Wilson never seemed interested in mRNA or my research," Karikó wrote. "He barely glanced my way; on the rare occasions he did, it always felt as if he were looking right through me."</p>
<p>After Karikó unsuccessfully appealed to Wilson for her research to be included in an upcoming grant, Swain requested that Karikó not attend similar meetings in the future — and asked Karikó to stop speaking to her Hungarian colleague in their native language.</p>
        
        <hr>
<!-- gryphon/ads/rectangle.tpl -->

<hr>

        <p>"She told me 'people' were complaining about me, saying that I was too difficult," Karikó wrote of a time she was called to Swain's office.</p>
<p>During these early years, Karikó wrote that Penn prevented her from having access to basic lab supplies, such as deionized water. All of her grant applications for future research, directed at private and government agencies and the University Research Foundation, were also denied.</p>
<p>Five years into her tenure at Penn, Karikó was informed that she would not be promoted to the position of research associate professor, the typical stepping stone for researchers with her level of experience. In 1997, Barnathan, her supervisor, left the University, leaving Karikó without a clear path forward.&nbsp;</p>
<p>"[I had] no grants, no funding, [and] no respect from anyone with any formal power,” she wrote.</p>
<figure>
    <img src="https://snworksceo.imgix.net/dpn/a9ac37d2-2c61-4d59-b602-9171cced0e1c.sized-1000x1000.JPG?w=1000">
    <figcaption>
      <small>
                (Photo courtesy of Hamna Shahnawaz).
      </small>
    </figcaption>
</figure>
<p>After Barnathan left, Karikó was helped by a colleague, David Langer — now the chair of neurosurgery at New York’s Lenox Hill Hospital. Langer convinced the chair of Penn's neurosurgery department to hire Karikó as the department's senior head of research.</p>
<p>“If she wasn’t hired, there may not have been a COVID vaccine," Langer said.</p>
<p>Even at that early point in her career, Langer said he was confident that Karikó would make substantial strides in the field of mRNA.</p>
<p>“She is incredibly hardworking, just insane,” Langer said. "And she’s a genius, so eventually she was going to solve this problem, whether with me or with someone else.”</p>
<p>Not long after, Karikó met Drew Weissman, her future research partner and co-Nobel Prize laureate, during a chance encounter at a copy machine. Karikó and Weissman began to cooperate on research into mRNA technology.</p>
<p>2001 College graduate David Scales — now an assistant professor of medicine at Weill Cornell Medicine — worked in Karikó and Weissman’s lab as an undergraduate student at Penn. Scales said that he was surprised by the funding challenges faced by Karikó and other talented scientists.</p>

        <hr>
<!-- gryphon/ads/rectangle_2.tpl -->

<hr>

        <p>“It was weird to me to see this idea that they’re only going to fund you for a short amount of time, and if you’re not able to get external grants, then they’re going to wash their hands with you and say good luck," he said.</p>
<p>In 1999, Sean Grady — now the chair of the neurosurgery department — arrived at Penn and immediately sought to revisit the department's existing allocation of resources.</p>
<p>“Not long after arriving, Sean sat me down,” Karikó wrote. “He observed that I’d had some publications in reputable, if small, journals. But, he said, he was under tremendous budgetary pressure and was concerned about my lack of funding.”&nbsp;</p>
<p>Over the following years, Grady repeatedly critiqued Karikó — paying minimal attention to her research in favor of the metrics used by Penn to evaluate her success, such as publication records, citations, and funding.</p>
<p>“Unless something changes, this isn’t going to go well," Grady told Karikó, according to her memoir.</p>
<p>Grady and others within Penn Medicine aimed to maximize the returns on their investments in individual researchers, Langer said.&nbsp;</p>
<p>To Grady, $35,000 spent on Karikó was "$35,000 that could be spent supporting a new scientist who may make a discovery," Langer added.</p>
<p>In 2005, Karikó and Weissman jointly made the discovery that would later win them the Nobel Prize, receiving minimal recognition from the academic community at the time.</p>
<figure>
    <img src="https://snworksceo.imgix.net/dpn/9faead5e-a33d-45ae-b58b-d2135c3e2d34.sized-1000x1000.jpg?w=1000">
    <figcaption>
      <small>
                (Photo courtesy of Robert Sobol).
      </small>
    </figcaption>
</figure>
<p>Robert Sobol, a professor who worked with Karikó at Temple University before she moved to Penn, said that the development was "groundbreaking" in retrospect.</p>
<p>“Karikó and I started investigating RNA in the late 1990s, and we figured out what made it so inflammatory and how to get rid of the inflammation. We published that research in 2005,” Weissman <a href="https://www.thedp.com/article/2021/01/covid-19-vaccine-rollout-penn-health-system-staff-researchers-pennsylvania" target="_blank">previously told the DP</a>. “That is when people, at least academics, started to get interested in its potential, and companies started to get interested in it around 2010.”</p>

        <hr>
<!-- gryphon/ads/rectangle_3.tpl -->

<hr>

        <p>Karikó told the DP in 2020, as the <a href="https://www.thedp.com/article/2021/01/covid-19-vaccine-rollout-penn-health-system-staff-researchers-pennsylvania" target="_blank">first COVID-19 vaccines became available to Penn Med's frontline health workers and researchers</a>, that she knew the the discovery in 2005 would lead to major developments in the future.</p>
<p>“When I discovered the potential of the RNA, I had a feeling that it could be anything,” Karikó told the DP. “But nobody believed, so I couldn’t release that feeling.”</p>
<p>Her former colleague Norbert Pardi, who is now an assistant professor in Penn’s Department of Microbiology, said that Karikó and Weissman's work ethic motivated the entire lab to work harder.</p>
<p>Penn was eventually awarded a patent for the modified RNA developed by Karikó and Weissman, enabling the University to have the final say in how the patent would be licensed. To control the direction of future research, Karikó and Weissman sought to purchase the patent themselves — but it was sold in its entirety to a different company.&nbsp;</p>
<p>Karikó requested to be reinstated to a faculty position at Penn in 2010 but was initially rejected. Karikó wrote that administrators told her that she was "not of faculty quality" — citing how individuals who have previously been demoted can not be promoted back to the faculty track.</p>
<p>After Karikó appealed the decision and rejoined the faculty, colleagues said that Grady continued to undermine Karikó. But it was the removal of her lab space that pushed Karikó to leave Penn, according to an employee close to Karikó who requested anonymity due to fear of retaliation from the University.</p>
<p>“She was kicked out of her lab so many times, but she made something good out of all of these failures," Éva Remenyik, a Hungarian dermatologist close to Karikó, said.&nbsp;</p>
<p>In 2013, Karikó said she returned to her lab after spending time away to find all of her belongings having been packed, moved, and misplaced at Grady's direction. Karikó then left Penn's campus to work at BioNTech, a German company that focuses on mRNA-based technologies later that year.</p>
<p>Langer told the DP that many of Karikó’s superiors may not have recognized the impacts of her research and potential successes.</p>
<p>“People didn't necessarily see her as who she was going to become,” Langer said. “Michael Jordan was overlooked by two teams and is the greatest basketball player of all time, [and] Tom Brady was drafted 199th. The value and the ultimate success of somebody is not always readily apparent, even when it’s right in front of your eyes.”</p>
<p>Scales also said that Penn's approach of giving minimal funds to Karikó followed a similar model to most peer institutions. He said many research institutions provide some degree of startup funds, and the expectation is for researchers to acquire external grants otherwise.</p>
<p>All of those interviewed commended Karikó for winning the Nobel Prize alongside Weissman.</p>
<p>“I think it’s a testament to her fortitude,” Sobol said. “Now that you look back on the calendar, you see that she was 20 years ahead of where everyone is now.”</p>
<p>Scales said he hopes that Karikó's win will prompt changes to funding allocations in research.</p>
<p>“I do hope that it causes Penn and a bunch of other institutions that fund science this way to reflect a little bit on what the chances are that some scientists who do not get funding, and wind up leaving, end up being like Katalin Karikó,” Scales said.&nbsp;</p>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[18-year-old built a computer monitor that doesn't strain your eyes (174 pts)]]></title>
            <link>https://www.fastcompany.com/90971739/this-18-year-old-built-a-better-computer-monitor-that-doesnt-strain-your-eyes</link>
            <guid>38040164</guid>
            <pubDate>Fri, 27 Oct 2023 15:54:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fastcompany.com/90971739/this-18-year-old-built-a-better-computer-monitor-that-doesnt-strain-your-eyes">https://www.fastcompany.com/90971739/this-18-year-old-built-a-better-computer-monitor-that-doesnt-strain-your-eyes</a>, See on <a href="https://news.ycombinator.com/item?id=38040164">Hacker News</a></p>
Couldn't get https://www.fastcompany.com/90971739/this-18-year-old-built-a-better-computer-monitor-that-doesnt-strain-your-eyes: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[OpenBSD: Removing syscall(2) from libc and kernel (136 pts)]]></title>
            <link>https://marc.info/?l=openbsd-tech&amp;m=169841790407370&amp;w=2</link>
            <guid>38039689</guid>
            <pubDate>Fri, 27 Oct 2023 15:23:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://marc.info/?l=openbsd-tech&#x26;m=169841790407370&#x26;w=2">https://marc.info/?l=openbsd-tech&#x26;m=169841790407370&#x26;w=2</a>, See on <a href="https://news.ycombinator.com/item?id=38039689">Hacker News</a></p>
<div id="readability-page-1" class="page">
<pre><b>[<a href="https://marc.info/?l=openbsd-tech&amp;m=169840739832111&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-tech&amp;m=169842095809570&amp;w=2">next in list</a>] [<span color="#c0c0c0">prev in thread</span>] [<a href="https://marc.info/?l=openbsd-tech&amp;m=169842095809570&amp;w=2">next in thread</a>] </b>
<b><span size="+1">
List:       <a href="https://marc.info/?l=openbsd-tech&amp;r=1&amp;w=2">openbsd-tech</a>
Subject:    <a href="https://marc.info/?t=169841803800006&amp;r=1&amp;w=2">Removing syscall(2) from libc and kernel</a>
From:       <a href="https://marc.info/?a=146809448200002&amp;r=1&amp;w=2">"Theo de Raadt" &lt;deraadt () openbsd ! org&gt;</a>
Date:       <a href="https://marc.info/?l=openbsd-tech&amp;r=1&amp;w=2&amp;b=202310">2023-10-27 14:45:41</a>
Message-ID: <a href="https://marc.info/?i=69276.1698417941%20()%20cvs%20!%20openbsd%20!%20org">69276.1698417941 () cvs ! openbsd ! org</a></span>
[Download RAW <a href="https://marc.info/?l=openbsd-tech&amp;m=169841790407370&amp;q=mbox">message</a> or <a href="https://marc.info/?l=openbsd-tech&amp;m=169841790407370&amp;q=raw">body</a>]</b>

Piece by piece, I've been trying to remove the easiest of the
terminal-actions that exploit code uses (ie. getting to execve, or performing
other system calls, etc).

I recognize we can never completely remove all mechanisms they
use. However, I hope I am forcing attack coders into using increasingly
more complicated methods. Same time, it means fewer methods are
available.  Other methods make exploitation more fragile.  This is
pushing success rates into "low-percent statistical" success. If we
teach more software stacks to "fail hard, don't try to recover", that is
an improvement in security.

I already made it difficult to call execve() directly in a few ways.
The kernel must be entered via the exact syscall instruction, inside the
libc syscall stub.  Immediately before that syscall instruction, the
SYS_execve instruction is loaded into a register.  On some
architectures, the PLT-reachable stub performs a retguard check, which
can be triggered by a few methods.  Stack pivots are also mostly
prevented because of other checks.  It is not possible to enter via
the SYS_syscall (syscall register = 0) case either.

Attack code can try to do perform other system calls, to create
filesystem damage or network communication.  They could still load other
syscall numbers and jump to a found syscall instruction, if they are
able to cheat the retguard epilogue (It is a bit unfortunate that libc
syscall stubs tend to use the same save register, but at least the
compare offset is chosen random at compile time).  Or, they could know
where all the system calls are from a pre-read libc, which requires them
to be on the machine before performing an online or offline attack (libc
is random relinked, but still readable in the filesystem).  It's
difficult to discover code-locations online only, because most
architectures also have xonly code now.  Some methods can use PLT
entries (which also vary based upon random relink), but I've not seem
much methodology using PLT entry + offset.

Anyways, everyone of these things I mention, and the ones I don't mention,
tend to be more difficult than the previous methods.  I'm trying to remove
simple methods, and force attackers into more and more complex methods.
I promise that I will circle back and damage the more complex methods in
the future.


So in this next step, I'm going to take away the ability to perform syscall #0
(SYS_syscall), with the first argument being the real system call.

This library interface, and all the pieces below it, will be going away:

    <a href="https://man.openbsd.org/syscall.2" rel="nofollow">https://man.openbsd.org/syscall.2</a>

There's going to be some fallout which takes time to fix, especially in the
"go" ecosystem.

Snapshots for some architectures now contain kernel diffs which reject
syscall(2).  The symbol still remains libc.

I'm including a piece of this diff.




Index: sys/arch/alpha/alpha/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/alpha/alpha/trap.c,v
diff -u -p -u -r1.108 trap.c
--- sys/arch/alpha/alpha/trap.c	8 Mar 2023 04:43:07 -0000	1.108
+++ sys/arch/alpha/alpha/trap.c	27 Oct 2023 03:26:49 -0000
@@ -497,17 +497,15 @@ dopanic:
  * a3, and v0 from the frame before returning to the user process.
  */
 void
-syscall(code, framep)
-	u_int64_t code;
-	struct trapframe *framep;
+syscall(u_int64_t code, struct trapframe *framep)
 {
-	const struct sysent *callp;
+	const struct sysent *callp = sysent;
 	struct proc *p;
-	int error, indirect = -1;
+	int error;
 	u_int64_t opc;
 	u_long rval<a name="-2"></a><a href="#2">[2]</a>;
 	u_long args<a name="-10"></a><a href="#10">[10]</a>;					/* XXX */
-	u_int hidden, nargs;
+	u_int nargs;
 
 	atomic_add_int(&amp;uvmexp.syscalls, 1);
 	p = curproc;
@@ -515,24 +513,11 @@ syscall(code, framep)
 	framep-&gt;tf_regs[FRAME_SP] = alpha_pal_rdusp();
 	opc = framep-&gt;tf_regs[FRAME_PC] - 4;
 
-	switch(code) {
-	case SYS_syscall:
-		indirect = code;
-		code = framep-&gt;tf_regs[FRAME_A0];
-		hidden = 1;
-		break;
-	default:
-		hidden = 0;
-	}
-
-	error = 0;
-	callp = sysent;
-	if (code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
-	nargs = callp-&gt;sy_narg + hidden;
+	nargs = callp-&gt;sy_narg;
 	switch (nargs) {
 	default:
 		if (nargs &gt; 10)		/* XXX */
@@ -559,7 +544,7 @@ syscall(code, framep)
 	rval<a name="-0"></a><a href="#0">[0]</a> = 0;
 	rval<a name="-1"></a><a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, args + hidden, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
Index: sys/arch/amd64/amd64/locore.S
===================================================================
RCS file: /cvs/src/sys/arch/amd64/amd64/locore.S,v
diff -u -p -u -r1.141 locore.S
--- sys/arch/amd64/amd64/locore.S	24 Oct 2023 13:20:09 -0000	1.141
+++ sys/arch/amd64/amd64/locore.S	27 Oct 2023 03:26:49 -0000
@@ -508,6 +508,7 @@ ENTRY(savectx)
 	lfence
 END(savectx)
 
+// XXX this should not behave like a nop
 IDTVEC(syscall32)
 	sysret		/* go away please */
 END(Xsyscall32)
Index: sys/arch/amd64/amd64/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/amd64/amd64/trap.c,v
diff -u -p -u -r1.101 trap.c
--- sys/arch/amd64/amd64/trap.c	5 Jul 2023 12:58:55 -0000	1.101
+++ sys/arch/amd64/amd64/trap.c	27 Oct 2023 03:26:49 -0000
@@ -553,7 +553,7 @@ syscall(struct trapframe *frame)
 	caddr_t params;
 	const struct sysent *callp;
 	struct proc *p;
-	int error, indirect = -1;
+	int error = ENOSYS;
 	size_t argsize, argoff;
 	register_t code, args<a name="-9"></a><a href="#9">[9]</a>, rval<a href="#2">[2]</a>, *argp;
 
@@ -570,26 +570,9 @@ syscall(struct trapframe *frame)
 	argp = &amp;args<a href="#0">[0]</a>;
 	argoff = 0;
 
-	switch (code) {
-	case SYS_syscall:
-		/*
-		 * Code is first argument, followed by actual args.
-		 */
-		indirect = code;
-		code = frame-&gt;tf_rdi;
-		argp = &amp;args<a href="#1">[1]</a>;
-		argoff = 1;
-		break;
-	default:
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
-
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp = sysent + code;
 	argsize = (callp-&gt;sy_argsize &gt;&gt; 3) + argoff;
 	if (argsize) {
 		switch (MIN(argsize, 6)) {
@@ -620,7 +603,7 @@ syscall(struct trapframe *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, argp, rval);
+	error = mi_syscall(p, code, callp, argp, rval);
 
 	switch (error) {
 	case 0:
Index: sys/arch/arm/arm/syscall.c
===================================================================
RCS file: /cvs/src/sys/arch/arm/arm/syscall.c,v
diff -u -p -u -r1.26 syscall.c
--- sys/arch/arm/arm/syscall.c	11 Feb 2023 23:07:26 -0000	1.26
+++ sys/arch/arm/arm/syscall.c	27 Oct 2023 03:26:49 -0000
@@ -93,8 +93,8 @@ void
 swi_handler(trapframe_t *frame)
 {
 	struct proc *p = curproc;
-	const struct sysent *callp;
-	int code, error, indirect = -1;
+	const struct sysent *callp = sysent;
+	int code, error;
 	u_int nap = 4, nargs;
 	register_t *ap, *args, copyargs[MAXARGS], rval<a href="#2">[2]</a>;
 
@@ -103,32 +103,19 @@ swi_handler(trapframe_t *frame)
 	/* Before enabling interrupts, save FPU state */
 	vfp_save();
 
-	/* Re-enable interrupts if they were enabled previously */
-	if (__predict_true((frame-&gt;tf_spsr &amp; PSR_I) == 0))
-		enable_interrupts(PSR_I);
+	enable_interrupts(PSR_I);
 
 	p-&gt;p_addr-&gt;u_pcb.pcb_tf = frame;
 
 	/* Skip over speculation-blocking barrier. */
 	frame-&gt;tf_pc += 8;
 
-	code = frame-&gt;tf_r12;
-
 	ap = &amp;frame-&gt;tf_r0;
 
-	switch (code) {	
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	code = frame-&gt;tf_r12;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
 	nargs = callp-&gt;sy_argsize / sizeof(register_t);
 	if (nargs &lt;= nap) {
@@ -145,27 +132,23 @@ swi_handler(trapframe_t *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = frame-&gt;tf_r1;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
 		frame-&gt;tf_r0 = rval<a href="#0">[0]</a>;
 		frame-&gt;tf_r1 = rval<a href="#1">[1]</a>;
-
 		frame-&gt;tf_spsr &amp;= ~PSR_C;	/* carry bit */
 		break;
-
 	case ERESTART:
 		/*
 		 * Reconstruct the pc to point at the swi.
 		 */
 		frame-&gt;tf_pc -= 12;
 		break;
-
 	case EJUSTRETURN:
 		/* nothing to do */
 		break;
-
 	default:
 	bad:
 		frame-&gt;tf_r0 = error;
Index: sys/arch/arm64/arm64/syscall.c
===================================================================
RCS file: /cvs/src/sys/arch/arm64/arm64/syscall.c,v
diff -u -p -u -r1.14 syscall.c
--- sys/arch/arm64/arm64/syscall.c	13 Apr 2023 02:19:04 -0000	1.14
+++ sys/arch/arm64/arm64/syscall.c	27 Oct 2023 03:26:49 -0000
@@ -33,7 +33,7 @@ svc_handler(trapframe_t *frame)
 {
 	struct proc *p = curproc;
 	const struct sysent *callp;
-	int code, error, indirect = -1;
+	int code, error = ENOSYS;
 	u_int nap = 8, nargs;
 	register_t *ap, *args, copyargs[MAXARGS], rval<a href="#2">[2]</a>;
 
@@ -50,19 +50,9 @@ svc_handler(trapframe_t *frame)
 
 	ap = &amp;frame-&gt;tf_x<a href="#0">[0]</a>;
 
-	switch (code) {	
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp = sysent + code;
 
 	nargs = callp-&gt;sy_argsize / sizeof(register_t);
 	if (nargs &lt;= nap) {
@@ -79,25 +69,22 @@ svc_handler(trapframe_t *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
 		frame-&gt;tf_x<a href="#0">[0]</a> = rval<a href="#0">[0]</a>;
 		frame-&gt;tf_spsr &amp;= ~PSR_C;	/* carry bit */
 		break;
-
 	case ERESTART:
 		/*
 		 * Reconstruct the pc to point at the svc.
 		 */
 		frame-&gt;tf_elr -= 12;
 		break;
-
 	case EJUSTRETURN:
 		/* nothing to do */
 		break;
-
 	default:
 	bad:
 		frame-&gt;tf_x<a href="#0">[0]</a> = error;
Index: sys/arch/hppa/hppa/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/hppa/hppa/trap.c,v
diff -u -p -u -r1.161 trap.c
--- sys/arch/hppa/hppa/trap.c	11 Feb 2023 23:07:26 -0000	1.161
+++ sys/arch/hppa/hppa/trap.c	27 Oct 2023 03:26:49 -0000
@@ -764,8 +764,8 @@ void
 syscall(struct trapframe *frame)
 {
 	struct proc *p = curproc;
-	const struct sysent *callp;
-	int retq, code, argsize, argoff, error, indirect = -1;
+	const struct sysent *callp = sysent;
+	int code, argsize, argoff, error;
 	register_t args<a name="-8"></a><a href="#8">[8]</a>, rval<a href="#2">[2]</a>;
 #ifdef DIAGNOSTIC
 	int oldcpl = curcpu()-&gt;ci_cpl;
@@ -778,29 +778,16 @@ syscall(struct trapframe *frame)
 
 	p-&gt;p_md.md_regs = frame;
 
-	argoff = 4; retq = 0;
-	switch (code = frame-&gt;tf_t1) {
-	case SYS_syscall:
-		indirect = code;
-		code = frame-&gt;tf_arg0;
-		args<a href="#0">[0]</a> = frame-&gt;tf_arg1;
-		args<a href="#1">[1]</a> = frame-&gt;tf_arg2;
-		args<a href="#2">[2]</a> = frame-&gt;tf_arg3;
-		argoff = 3;
-		break;
-	default:
-		args<a href="#0">[0]</a> = frame-&gt;tf_arg0;
-		args<a href="#1">[1]</a> = frame-&gt;tf_arg1;
-		args<a href="#2">[2]</a> = frame-&gt;tf_arg2;
-		args<a name="-3"></a><a href="#3">[3]</a> = frame-&gt;tf_arg3;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	argoff = 4;
+	code = frame-&gt;tf_t1;
+	args<a href="#0">[0]</a> = frame-&gt;tf_arg0;
+	args<a href="#1">[1]</a> = frame-&gt;tf_arg1;
+	args<a href="#2">[2]</a> = frame-&gt;tf_arg2;
+	args<a href="#3">[3]</a> = frame-&gt;tf_arg3;
+
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
 	if ((argsize = callp-&gt;sy_argsize)) {
 		register_t *s, *e, t;
@@ -830,7 +817,7 @@ syscall(struct trapframe *frame)
 		 */
 		i = 0;
 		switch (code) {
-		case SYS_lseek:		retq = 0;
+		case SYS_lseek:
 		case SYS_truncate:
 		case SYS_ftruncate:	i = 2;	break;
 		case SYS_preadv:
@@ -851,12 +838,12 @@ syscall(struct trapframe *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = frame-&gt;tf_ret1;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
 		frame-&gt;tf_ret0 = rval<a href="#0">[0]</a>;
-		frame-&gt;tf_ret1 = rval[!retq];
+		frame-&gt;tf_ret1 = rval<a href="#1">[1]</a>;
 		frame-&gt;tf_t1 = 0;
 		break;
 	case ERESTART:
@@ -872,7 +859,7 @@ syscall(struct trapframe *frame)
 		break;
 	}
 
-	ast(p);
+	ast(p);		// XXX why?
 
 	mi_syscall_return(p, code, error, rval);
 
Index: sys/arch/i386/i386/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/i386/i386/trap.c,v
diff -u -p -u -r1.162 trap.c
--- sys/arch/i386/i386/trap.c	16 Apr 2023 06:43:49 -0000	1.162
+++ sys/arch/i386/i386/trap.c	27 Oct 2023 03:26:49 -0000
@@ -516,9 +516,9 @@ void
 syscall(struct trapframe *frame)
 {
 	caddr_t params;
-	const struct sysent *callp;
-	struct proc *p;
-	int error, indirect = -1;
+	const struct sysent *callp = sysent;
+	struct proc *p = curproc;
+	int error;
 	register_t code, args<a href="#8">[8]</a>, rval<a href="#2">[2]</a>;
 #ifdef DIAGNOSTIC
 	int ocpl = lapic_tpr;
@@ -540,38 +540,22 @@ syscall(struct trapframe *frame)
 	}
 #endif
 
-	p = curproc;
 	p-&gt;p_md.md_regs = frame;
-	code = frame-&gt;tf_eax;
-
-	params = (caddr_t)frame-&gt;tf_esp + sizeof(int);
 
-	switch (code) {
-	case SYS_syscall:
-		/*
-		 * Code is first argument, followed by actual args.
-		 */
-		indirect = code;
-		copyin(params, &amp;code, sizeof(int));
-		params += sizeof(int);
-		break;
-	default:
-		break;
-	}
+	code = frame-&gt;tf_eax;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
 	argsize = callp-&gt;sy_argsize;
+	params = (caddr_t)frame-&gt;tf_esp + sizeof(int);
 	if (argsize &amp;&amp; (error = copyin(params, args, argsize)))
 		goto bad;
 
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = frame-&gt;tf_edx;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
Index: sys/arch/m88k/m88k/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/m88k/m88k/trap.c,v
diff -u -p -u -r1.128 trap.c
--- sys/arch/m88k/m88k/trap.c	2 Aug 2023 06:14:46 -0000	1.128
+++ sys/arch/m88k/m88k/trap.c	27 Oct 2023 03:26:49 -0000
@@ -1153,9 +1153,9 @@ void
 m88100_syscall(register_t code, struct trapframe *tf)
 {
 	int i, nap;
-	const struct sysent *callp;
+	const struct sysent *callp = sysent;
 	struct proc *p = curproc;
-	int error, indirect = -1;
+	int error;
 	register_t args<a href="#8">[8]</a> __aligned(8);
 	register_t rval<a href="#2">[2]</a> __aligned(8);
 	register_t *ap;
@@ -1172,19 +1172,9 @@ m88100_syscall(register_t code, struct t
 	ap = &amp;tf-&gt;tf_r<a href="#2">[2]</a>;
 	nap = 8; /* r2-r9 */
 
-	switch (code) {
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
 	i = callp-&gt;sy_argsize / sizeof(register_t);
 	if (i &gt; sizeof(args) / sizeof(register_t))
@@ -1200,7 +1190,7 @@ m88100_syscall(register_t code, struct t
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = tf-&gt;tf_r<a href="#3">[3]</a>;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	/*
 	 * system call will look like:
@@ -1266,7 +1256,7 @@ void
 m88110_syscall(register_t code, struct trapframe *tf)
 {
 	int i, nap;
-	const struct sysent *callp;
+	const struct sysent *callp = sysent;
 	struct proc *p = curproc;
 	int error;
 	register_t args<a href="#8">[8]</a> __aligned(8);
@@ -1285,17 +1275,8 @@ m88110_syscall(register_t code, struct t
 	ap = &amp;tf-&gt;tf_r<a href="#2">[2]</a>;
 	nap = 8;	/* r2-r9 */
 
-	switch (code) {
-	case SYS_syscall:
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
+	// XXX out of range stays on syscall0, which we assume is enosys
+	if (code &gt;= 0 || code &lt;= SYS_MAXSYSCALL)
 		callp += code;
 
 	i = callp-&gt;sy_argsize / sizeof(register_t);
Index: sys/arch/mips64/mips64/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/mips64/mips64/trap.c,v
diff -u -p -u -r1.167 trap.c
--- sys/arch/mips64/mips64/trap.c	26 Apr 2023 16:53:59 -0000	1.167
+++ sys/arch/mips64/mips64/trap.c	27 Oct 2023 03:26:49 -0000
@@ -396,14 +396,12 @@ fault_common_no_miss:
 	case T_SYSCALL+T_USER:
 	    {
 		struct trapframe *locr0 = p-&gt;p_md.md_regs;
-		const struct sysent *callp;
-		unsigned int code, indirect = -1;
+		const struct sysent *callp = sysent;
+		unsigned int code;
 		register_t tpc;
 		uint32_t branch = 0;
 		int error, numarg;
-		struct args {
-			register_t i<a href="#8">[8]</a>;
-		} args;
+		register_t args<a href="#8">[8]</a>;
 		register_t rval<a href="#2">[2]</a>;
 
 		atomic_inc_int(&amp;uvmexp.syscalls);
@@ -422,51 +420,22 @@ fault_common_no_miss:
 			    trapframe-&gt;pc, 0, branch);
 		} else
 			locr0-&gt;pc += 4;
-		callp = sysent;
 		code = locr0-&gt;v0;
-		switch (code) {
-		case SYS_syscall:
-			/*
-			 * Code is first argument, followed by actual args.
-			 */
-			indirect = code;
-			code = locr0-&gt;a0;
-			if (code &gt;= SYS_MAXSYSCALL)
-				callp += SYS_syscall;
-			else
-				callp += code;
-			numarg = callp-&gt;sy_argsize / sizeof(register_t);
-			args.i<a href="#0">[0]</a> = locr0-&gt;a1;
-			args.i<a href="#1">[1]</a> = locr0-&gt;a2;
-			args.i<a href="#2">[2]</a> = locr0-&gt;a3;
-			if (numarg &gt; 3) {
-				args.i<a href="#3">[3]</a> = locr0-&gt;a4;
-				args.i<a name="-4"></a><a href="#4">[4]</a> = locr0-&gt;a5;
-				args.i<a name="-5"></a><a href="#5">[5]</a> = locr0-&gt;a6;
-				args.i<a name="-6"></a><a href="#6">[6]</a> = locr0-&gt;a7;
-				if (numarg &gt; 7)
-					if ((error = copyin((void *)locr0-&gt;sp,
-					    &amp;args.i<a name="-7"></a><a href="#7">[7]</a>, sizeof(register_t))))
-						goto bad;
-			}
-			break;
-		default:
-			if (code &gt;= SYS_MAXSYSCALL)
-				callp += SYS_syscall;
-			else
-				callp += code;
-
-			numarg = callp-&gt;sy_narg;
-			args.i<a href="#0">[0]</a> = locr0-&gt;a0;
-			args.i<a href="#1">[1]</a> = locr0-&gt;a1;
-			args.i<a href="#2">[2]</a> = locr0-&gt;a2;
-			args.i<a href="#3">[3]</a> = locr0-&gt;a3;
-			if (numarg &gt; 4) {
-				args.i<a href="#4">[4]</a> = locr0-&gt;a4;
-				args.i<a href="#5">[5]</a> = locr0-&gt;a5;
-				args.i<a href="#6">[6]</a> = locr0-&gt;a6;
-				args.i<a href="#7">[7]</a> = locr0-&gt;a7;
-			}
+
+		if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+			goto bad;
+		callp += code;
+
+		numarg = callp-&gt;sy_narg;
+		args<a href="#0">[0]</a> = locr0-&gt;a0;
+		args<a href="#1">[1]</a> = locr0-&gt;a1;
+		args<a href="#2">[2]</a> = locr0-&gt;a2;
+		args<a href="#3">[3]</a> = locr0-&gt;a3;
+		if (numarg &gt; 4) {
+			args<a href="#4">[4]</a> = locr0-&gt;a4;
+			args<a href="#5">[5]</a> = locr0-&gt;a5;
+			args<a href="#6">[6]</a> = locr0-&gt;a6;
+			args<a href="#7">[7]</a> = locr0-&gt;a7;
 		}
 
 		rval<a href="#0">[0]</a> = 0;
@@ -477,29 +446,24 @@ fault_common_no_miss:
 		    TRAPSIZE : trppos[ci-&gt;ci_cpuid]) - 1].code = code;
 #endif
 
-		error = mi_syscall(p, code, indirect, callp, args.i, rval);
+		error = mi_syscall(p, code, callp, args, rval);
 
 		switch (error) {
 		case 0:
 			locr0-&gt;v0 = rval<a href="#0">[0]</a>;
 			locr0-&gt;a3 = 0;
 			break;
-
 		case ERESTART:
 			locr0-&gt;pc = tpc;
 			break;
-
 		case EJUSTRETURN:
 			break;	/* nothing to do */
-
 		default:
-		bad:
 			locr0-&gt;v0 = error;
 			locr0-&gt;a3 = 1;
 		}
 
 		mi_syscall_return(p, code, error, rval);
-
 		return;
 	    }
 
Index: sys/arch/powerpc/powerpc/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/powerpc/powerpc/trap.c,v
diff -u -p -u -r1.131 trap.c
--- sys/arch/powerpc/powerpc/trap.c	11 Feb 2023 23:07:27 -0000	1.131
+++ sys/arch/powerpc/powerpc/trap.c	27 Oct 2023 03:26:49 -0000
@@ -239,11 +239,11 @@ trap(struct trapframe *frame)
 	struct vm_map *map;
 	vaddr_t va;
 	int access_type;
-	const struct sysent *callp;
+	const struct sysent *callp = sysent;
 	size_t argsize;
 	register_t code, error;
 	register_t *params, rval<a href="#2">[2]</a>, args<a href="#10">[10]</a>;
-	int n, indirect = -1;
+	int n;
 
 	if (frame-&gt;srr1 &amp; PSL_PR) {
 		type |= EXC_USER;
@@ -360,27 +360,13 @@ trap(struct trapframe *frame)
 	case EXC_SC|EXC_USER:
 		uvmexp.syscalls++;
 
-		code = frame-&gt;fixreg<a href="#0">[0]</a>;
 		params = frame-&gt;fixreg + FIRSTARG;
 
-		switch (code) {
-		case SYS_syscall:
-			/*
-			 * code is first argument,
-			 * followed by actual args.
-			 */
-			indirect = code;
-			code = *params++;
-			break;
-		default:
-			break;
-		}
+		code = frame-&gt;fixreg<a href="#0">[0]</a>;
+	        if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+			goto bad;
+                callp += code;
 
-		callp = sysent;
-		if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-			callp += SYS_syscall;
-		else
-			callp += code;
 		argsize = callp-&gt;sy_argsize;
 		n = NARGREG - (params - (frame-&gt;fixreg + FIRSTARG));
 		if (argsize &gt; n * sizeof(register_t)) {
@@ -395,7 +381,7 @@ trap(struct trapframe *frame)
 		rval<a href="#0">[0]</a> = 0;
 		rval<a href="#1">[1]</a> = frame-&gt;fixreg[FIRSTARG + 1];
 
-		error = mi_syscall(p, code, indirect, callp, params, rval);
+		error = mi_syscall(p, code, callp, params, rval);
 
 		switch (error) {
 		case 0:
Index: sys/arch/powerpc64/powerpc64/syscall.c
===================================================================
RCS file: /cvs/src/sys/arch/powerpc64/powerpc64/syscall.c,v
diff -u -p -u -r1.11 syscall.c
--- sys/arch/powerpc64/powerpc64/syscall.c	11 Feb 2023 23:07:27 -0000	1.11
+++ sys/arch/powerpc64/powerpc64/syscall.c	27 Oct 2023 03:26:49 -0000
@@ -30,27 +30,17 @@ void
 syscall(struct trapframe *frame)
 {
 	struct proc *p = curproc;
-	const struct sysent *callp;
-	int code, error, indirect = -1;
+	const struct sysent *callp = sysent;
+	int code, error;
 	int nap = 8, nargs;
 	register_t *ap, *args, copyargs[MAXARGS], rval<a href="#2">[2]</a>;
 
-	code = frame-&gt;fixreg<a href="#0">[0]</a>;
 	ap = &amp;frame-&gt;fixreg<a href="#3">[3]</a>;
+	code = frame-&gt;fixreg<a href="#0">[0]</a>;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
-	switch (code) {
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
 	nargs = callp-&gt;sy_argsize / sizeof(register_t);
 	if (nargs &lt;= nap) {
 		args = ap;
@@ -66,7 +56,7 @@ syscall(struct trapframe *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
@@ -74,15 +64,12 @@ syscall(struct trapframe *frame)
 		frame-&gt;fixreg<a href="#3">[3]</a> = rval<a href="#0">[0]</a>;
 		frame-&gt;cr &amp;= ~0x10000000;
 		break;
-
 	case ERESTART:
 		frame-&gt;srr0 -= 4;
 		break;
-
 	case EJUSTRETURN:
 		/* nothing to do */
 		break;
-
 	default:
 	bad:
 		frame-&gt;fixreg<a href="#0">[0]</a> = error;
Index: sys/arch/riscv64/riscv64/syscall.c
===================================================================
RCS file: /cvs/src/sys/arch/riscv64/riscv64/syscall.c,v
diff -u -p -u -r1.16 syscall.c
--- sys/arch/riscv64/riscv64/syscall.c	13 Apr 2023 02:19:05 -0000	1.16
+++ sys/arch/riscv64/riscv64/syscall.c	27 Oct 2023 03:26:49 -0000
@@ -39,33 +39,20 @@ void
 svc_handler(trapframe_t *frame)
 {
 	struct proc *p = curproc;
-	const struct sysent *callp;
-	int code, error, indirect = -1;
+	const struct sysent *callp = sysent;
+	int code, error;
 	u_int nap = 8, nargs;
 	register_t *ap, *args, copyargs[MAXARGS], rval<a href="#2">[2]</a>;
 
 	uvmexp.syscalls++;
 
-	/* Re-enable interrupts if they were enabled previously */
-	if (__predict_true(frame-&gt;tf_scause &amp; EXCP_INTR))
-		intr_enable();
-
 	ap = &amp;frame-&gt;tf_a<a href="#0">[0]</a>;
 	code = frame-&gt;tf_t<a href="#0">[0]</a>;
 
-	switch (code) {
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp += code;
 
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
 	nargs = callp-&gt;sy_argsize / sizeof(register_t);
 	if (nargs &lt;= nap) {
 		args = ap;
@@ -81,21 +68,18 @@ svc_handler(trapframe_t *frame)
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
 		frame-&gt;tf_a<a href="#0">[0]</a> = rval<a href="#0">[0]</a>;
 		frame-&gt;tf_t<a href="#0">[0]</a> = 0;		/* syscall succeeded */
 		break;
-
 	case ERESTART:
 		frame-&gt;tf_sepc -= 4;		/* prev instruction */
 		break;
-
 	case EJUSTRETURN:
 		break;
-
 	default:
 	bad:
 		frame-&gt;tf_a<a href="#0">[0]</a> = error;
Index: sys/arch/sh/sh/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/sh/sh/trap.c,v
diff -u -p -u -r1.54 trap.c
--- sys/arch/sh/sh/trap.c	11 Feb 2023 23:07:27 -0000	1.54
+++ sys/arch/sh/sh/trap.c	27 Oct 2023 03:26:49 -0000
@@ -516,44 +516,20 @@ syscall(struct proc *p, struct trapframe
 {
 	caddr_t params;
 	const struct sysent *callp;
-	int error, opc, indirect = -1;
-	int argoff, argsize;
+	int error, opc;
+	int argsize;
 	register_t code, args<a href="#8">[8]</a>, rval<a href="#2">[2]</a>;
 
 	uvmexp.syscalls++;
 
 	opc = tf-&gt;tf_spc;
 	code = tf-&gt;tf_r0;
-
 	params = (caddr_t)tf-&gt;tf_r15;
 
-	switch (code) {
-	case SYS_syscall:
-		/*
-		 * Code is first argument, followed by actual args.
-		 */
-		indirect = code;
-	        code = tf-&gt;tf_r4;
-		argoff = 1;
-		break;
-	default:
-		argoff = 0;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else
-		callp += code;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp = sysent + code;
 	argsize = callp-&gt;sy_argsize;
-#ifdef DIAGNOSTIC
-	if (argsize &gt; sizeof args) {
-		callp += SYS_syscall - code;
-		argsize = callp-&gt;sy_argsize;
-	}
-#endif
-
 	if (argsize) {
 		register_t *ap;
 		int off_t_arg;
@@ -570,19 +546,16 @@ syscall(struct proc *p, struct trapframe
 		}
 
 		ap = args;
-		switch (argoff) {
-		case 0:	*ap++ = tf-&gt;tf_r4; argsize -= sizeof(int);
-		case 1:	*ap++ = tf-&gt;tf_r5; argsize -= sizeof(int);
-		case 2: *ap++ = tf-&gt;tf_r6; argsize -= sizeof(int);
-			/*
-			 * off_t args aren't split between register
-			 * and stack, but rather r7 is skipped and
-			 * the entire off_t is on the stack.
-			 */
-			if (argoff + off_t_arg == 3)
-				break;
+		*ap++ = tf-&gt;tf_r4; argsize -= sizeof(int);
+		*ap++ = tf-&gt;tf_r5; argsize -= sizeof(int);
+		*ap++ = tf-&gt;tf_r6; argsize -= sizeof(int);
+		/*
+		 * off_t args aren't split between register
+		 * and stack, but rather r7 is skipped and
+		 * the entire off_t is on the stack.
+		 */
+		if (off_t_arg != 3) {
 			*ap++ = tf-&gt;tf_r7; argsize -= sizeof(int);
-			break;
 		}
 
 		if (argsize &gt; 0) {
@@ -594,7 +567,7 @@ syscall(struct proc *p, struct trapframe
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = tf-&gt;tf_r1;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 	case 0:
Index: sys/arch/sparc64/sparc64/trap.c
===================================================================
RCS file: /cvs/src/sys/arch/sparc64/sparc64/trap.c,v
diff -u -p -u -r1.115 trap.c
--- sys/arch/sparc64/sparc64/trap.c	11 Feb 2023 23:07:28 -0000	1.115
+++ sys/arch/sparc64/sparc64/trap.c	27 Oct 2023 03:26:49 -0000
@@ -1109,9 +1109,10 @@ syscall(struct trapframe *tf, register_t
 	int64_t *ap;
 	const struct sysent *callp;
 	struct proc *p = curproc;
-	int error, new, indirect = -1;
+	int error = ENOSYS, new;
 	register_t args<a href="#8">[8]</a>;
 	register_t rval<a href="#2">[2]</a>;
+	register_t *argp;
 
 	if ((tf-&gt;tf_out<a href="#6">[6]</a> &amp; 1) == 0)
 		sigexit(p, SIGILL);
@@ -1137,44 +1138,31 @@ syscall(struct trapframe *tf, register_t
 	ap = &amp;tf-&gt;tf_out<a href="#0">[0]</a>;
 	nap = 6;
 
-	switch (code) {
-	case SYS_syscall:
-		indirect = code;
-		code = *ap++;
-		nap--;
-		break;
-	}
-
-	callp = sysent;
-	if (code &lt; 0 || code &gt;= SYS_MAXSYSCALL)
-		callp += SYS_syscall;
-	else {
-		register_t *argp;
-
-		callp += code;
-		i = callp-&gt;sy_narg; /* Why divide? */
-		if (i &gt; nap) {	/* usually false */
-			if (i &gt; 8)
-				panic("syscall nargs");
-			/* Read the whole block in */
-			if ((error = copyin((caddr_t)tf-&gt;tf_out<a href="#6">[6]</a>
-			    + BIAS + offsetof(struct frame, fr_argx),
-			    &amp;args[nap], (i - nap) * sizeof(register_t))))
-				goto bad;
-			i = nap;
-		}
-		/*
-		 * It should be faster to do &lt;= 6 longword copies than
-		 * to call bcopy
-		 */
-		for (argp = args; i--;)
-			*argp++ = *ap++;
+	if (code &lt;= 0 || code &gt;= SYS_MAXSYSCALL)
+		goto bad;
+	callp = sysent + code;
+	i = callp-&gt;sy_narg; /* Why divide? */
+	if (i &gt; nap) {	/* usually false */
+		if (i &gt; 8)
+			panic("syscall nargs");
+		/* Read the whole block in */
+		if ((error = copyin((caddr_t)tf-&gt;tf_out<a href="#6">[6]</a>
+		    + BIAS + offsetof(struct frame, fr_argx),
+		    &amp;args[nap], (i - nap) * sizeof(register_t))))
+			goto bad;
+		i = nap;
 	}
+	/*
+	 * It should be faster to do &lt;= 6 longword copies than
+	 * to call bcopy
+	 */
+	for (argp = args; i--;)
+		*argp++ = *ap++;
 
 	rval<a href="#0">[0]</a> = 0;
 	rval<a href="#1">[1]</a> = 0;
 
-	error = mi_syscall(p, code, indirect, callp, args, rval);
+	error = mi_syscall(p, code, callp, args, rval);
 
 	switch (error) {
 		vaddr_t dest;
Index: sys/kern/kern_ktrace.c
===================================================================
RCS file: /cvs/src/sys/kern/kern_ktrace.c,v
diff -u -p -u -r1.112 kern_ktrace.c
--- sys/kern/kern_ktrace.c	11 May 2023 09:51:33 -0000	1.112
+++ sys/kern/kern_ktrace.c	27 Oct 2023 03:26:49 -0000
@@ -160,7 +160,7 @@ ktrsyscall(struct proc *p, register_t co
 	u_int nargs = 0;
 	int i;
 
-	if ((code &amp; KTRC_CODE_MASK) == SYS_sysctl) {
+	if (code == SYS_sysctl) {
 		/*
 		 * The sysctl encoding stores the mib[]
 		 * array because it is interesting.
Index: sys/sys/ktrace.h
===================================================================
RCS file: /cvs/src/sys/sys/ktrace.h,v
diff -u -p -u -r1.46 ktrace.h
--- sys/sys/ktrace.h	23 Feb 2023 01:33:20 -0000	1.46
+++ sys/sys/ktrace.h	27 Oct 2023 03:26:49 -0000
@@ -76,8 +76,6 @@ struct ktr_header {
 #define KTR_SYSCALL	1
 struct ktr_syscall {
 	int	ktr_code;		/* syscall number */
-#define KTRC_CODE_MASK			0x0000ffff
-#define KTRC_CODE_SYSCALL		0x20000000
 	int	ktr_argsize;		/* size of arguments */
 	/*
 	 * followed by ktr_argsize/sizeof(register_t) "register_t"s
Index: sys/sys/syscall_mi.h
===================================================================
RCS file: /cvs/src/sys/sys/syscall_mi.h,v
diff -u -p -u -r1.28 syscall_mi.h
--- sys/sys/syscall_mi.h	11 Feb 2023 23:07:23 -0000	1.28
+++ sys/sys/syscall_mi.h	27 Oct 2023 03:26:49 -0000
@@ -51,8 +51,8 @@
  * The MD setup for a system call has been done; here's the MI part.
  */
 static inline int
-mi_syscall(struct proc *p, register_t code, int indirect,
-    const struct sysent *callp, register_t *argp, register_t retval<a href="#2">[2]</a>)
+mi_syscall(struct proc *p, register_t code, const struct sysent *callp,
+    register_t *argp, register_t retval<a href="#2">[2]</a>)
 {
 	uint64_t tval;
 	int lock = !(callp-&gt;sy_flags &amp; SY_NOLOCK);
@@ -73,15 +73,8 @@ mi_syscall(struct proc *p, register_t co
 #ifdef KTRACE
 	if (KTRPOINT(p, KTR_SYSCALL)) {
 		/* convert to mask, then include with code */
-		switch (indirect) {
-		case SYS_syscall:
-			indirect = KTRC_CODE_SYSCALL;
-			break;
-		default:
-			indirect = 0;
-		}
 		KERNEL_LOCK();
-		ktrsyscall(p, code | indirect, callp-&gt;sy_argsize, argp);
+		ktrsyscall(p, code, callp-&gt;sy_argsize, argp);
 		KERNEL_UNLOCK();
 	}
 #endif

<b>[<a href="https://marc.info/?l=openbsd-tech&amp;m=169840739832111&amp;w=2">prev in list</a>] [<a href="https://marc.info/?l=openbsd-tech&amp;m=169842095809570&amp;w=2">next in list</a>] [<span color="#c0c0c0">prev in thread</span>] [<a href="https://marc.info/?l=openbsd-tech&amp;m=169842095809570&amp;w=2">next in thread</a>] </b>
</pre>
  <br><center>
    <a href="https://marc.info/?q=configure">Configure</a> | 

    <a href="https://marc.info/?q=about">About</a> |
    <a href="https://marc.info/?q=news">News</a> |
    <a href="mailto:webguy@marc.info?subject=Add%20a%20list%20to%20MARC">Add&nbsp;a&nbsp;list</a> |
    Sponsored&nbsp;by&nbsp;<a href="http://www.korelogic.com/">KoreLogic</a>
</center>


</div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Xiaomi 14 with the new Snapdragon 8 Gen 3 has a 32 to 64-bit translator (114 pts)]]></title>
            <link>https://twitter.com/MishaalRahman/status/1717916987387469842</link>
            <guid>38039489</guid>
            <pubDate>Fri, 27 Oct 2023 15:09:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/MishaalRahman/status/1717916987387469842">https://twitter.com/MishaalRahman/status/1717916987387469842</a>, See on <a href="https://news.ycombinator.com/item?id=38039489">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="ScriptLoadFailure"><form action="" method="GET"><div><p><span>Something went wrong, but don’t fret — let’s give it another shot.</span></p><br></div></form></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[EPA moves towards banning leaded aviation gas (435 pts)]]></title>
            <link>https://www.federalregister.gov/documents/2023/10/20/2023-23247/finding-that-lead-emissions-from-aircraft-engines-that-operate-on-leaded-fuel-cause-or-contribute-to</link>
            <guid>38039133</guid>
            <pubDate>Fri, 27 Oct 2023 14:44:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.federalregister.gov/documents/2023/10/20/2023-23247/finding-that-lead-emissions-from-aircraft-engines-that-operate-on-leaded-fuel-cause-or-contribute-to">https://www.federalregister.gov/documents/2023/10/20/2023-23247/finding-that-lead-emissions-from-aircraft-engines-that-operate-on-leaded-fuel-cause-or-contribute-to</a>, See on <a href="https://news.ycombinator.com/item?id=38039133">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main">
        <h3>Request Access</h3>
        

        <p>
  Due to aggressive automated scraping of FederalRegister.gov and eCFR.gov, programmatic access to these sites is limited to access to our extensive developer APIs.
</p>

<p>
  If you are human user receiving this message, we can add your IP address to a set of IPs that can access FederalRegister.gov &amp; eCFR.gov; complete the CAPTCHA (bot test) below and click "Request Access". This process will be necessary for each IP address you wish to access the site from, requests are valid for approximately one quarter (three months) after which the process may need to be repeated.
</p>

<form action="/request" method="post">
  

          

  
</form>

<p>
  <em>An official website of the United States government.</em>
</p>

<p>
  If you want to request a wider IP range, first request access for your current IP, and then use the "Site Feedback" button found in the lower left-hand side to make the request.
</p>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Automatic fraud detection is making my life hell (340 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=38038713</link>
            <guid>38038713</guid>
            <pubDate>Fri, 27 Oct 2023 14:15:09 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=38038713">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="38038713">
      <td><span></span></td>      <td><center><a id="up_38038713" href="https://news.ycombinator.com/vote?id=38038713&amp;how=up&amp;goto=item%3Fid%3D38038713"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=38038713">Tell HN: Automatic fraud detection is making my life hell</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_38038713">108 points</span> by <a href="https://news.ycombinator.com/user?id=aiProgMach">aiProgMach</a> <span title="2023-10-27T14:15:09"><a href="https://news.ycombinator.com/item?id=38038713">6 hours ago</a></span> <span id="unv_38038713"></span> | <a href="https://news.ycombinator.com/hide?id=38038713&amp;goto=item%3Fid%3D38038713">hide</a> | <a href="https://hn.algolia.com/?query=Tell%20HN%3A%20Automatic%20fraud%20detection%20is%20making%20my%20life%20hell&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=38038713&amp;auth=5cd7d72fac8fa3d031b70c421368defe4328d1c4">favorite</a> | <a href="https://news.ycombinator.com/item?id=38038713">86&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>I've been in India for a while now, to support family member as she's here for medical reasons. I rely on online services to save on cash especially that it's hard to carry cash from my country (for "security" reasons, as most airports limit how much cash you can carry).</p><p>Yet, many online services are giving me hell with their "smart" anti fraud detection and things like that, at this point I can really understand the position of the people who are dooming about cashless society, because at some point here I felt trapped not being able to get services I needed so much (until I asked shop owner to pay for me and I paid him in cash + small profit...).</p><p>The thing is, the attitude of these companies is so frustrating;  like if my card was already accepted once and I successfully approved the payment via 3D secure with my bank, who are you (as a random online service) to assume you can act as my big brother? Even more, if I'm using a balance paid by gift card, who give Amazon or other services the right to put my account on hold while it still contains my hard earned money (I had to try literally multiple services just to buy expensive gift card as Amazon payment won't allow me to choose the correct currency of my Card). Mind you, I'm just a random guy and not world class criminal, or an Activist who's being actively targeted, this make me wonder what these services can do once we go completely cashless.</p><p>Simple tasks like downloading region-specific Indian apps become unnecessarily complex, as Google play have this "smart" rule that says I can only change my region once per year, what?? It's just an app just give me the apk, and you can just ask for my location! (I had to install the apks from some random websites at risk of getting some malware...).</p><p>I would said what this experience taught me as a developer, but it won't matter, as most products are designed to help the stake holders and upper managers and even Governments, and a dev's empathy won't matter much...</p><p>Apologies for this vent, but I really felt I need to post something about this frustrating situation I'm in.</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Everything I've learned building the fastest Arm desktop (143 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2023/everything-ive-learned-building-fastest-arm-desktop</link>
            <guid>38038682</guid>
            <pubDate>Fri, 27 Oct 2023 14:13:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2023/everything-ive-learned-building-fastest-arm-desktop">https://www.jeffgeerling.com/blog/2023/everything-ive-learned-building-fastest-arm-desktop</a>, See on <a href="https://news.ycombinator.com/item?id=38038682">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><img src="https://www.jeffgeerling.com/sites/default/files/images/ampere-altra-developer-platform-hero-shot_0.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-55f6ffb6-9f1f-48d9-8186-ab81519ca28a" data-insert-attach="{&quot;id&quot;:&quot;55f6ffb6-9f1f-48d9-8186-ab81519ca28a&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Ampere Altra Developer Platform Hero Shot"></p>

<p>This is the fastest Arm desktop in the world, yes, even faster than the <a href="https://www.apple.com/mac-pro/">M2 Ultra Mac Pro</a>. And today, I made it even faster.</p>

<p>I upgraded everything: Faster RAM, 128 core CPU, 40 series GPU, I did it all, and we'll see how much we can obliterate the M2 Mac Pro.</p>

<p>128 cores—that's five times more cores, I'm also going to upgrade this thing from 96 all the way to <em>384 gigabytes</em> of RAM. The Mac Pro? Sorry, it only goes up to 192.</p>

<p>And we're just in time for the <a href="https://www.maxon.net/en/article/maxon-introduces-cinebench-2024">new Cinebench 2024 benchmark</a>, which—yes—this machine dominates.</p>

<p>But it's not all perfection. The M2's individual cores are faster, and I didn't even mention Intel or AMD—they also do really well single-core. And 128 cores can be overkill if your application can't use all of them.</p>

<div>
<p><iframe src="https://www.youtube.com/embed/argfZlPZKdY" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<p><em>The following is a transcript of the above video. Please watch the <a href="https://www.youtube.com/watch?v=argfZlPZKdY">original video</a> for more context.</em></p>

<p>I tested two systems for this video:</p>

<ul>
<li><a href="https://www.ipi.wiki/products/ampere-altra-developer-platform">Ampere Altra Developer Platform</a> (a full system build, in my case with the 96-core and 128-core CPU)</li>
<li><a href="https://www.ipi.wiki/products/com-hpc-ampere-altra">Ampere Altra Dev Kit</a>, in my case with the 64-core CPU)</li>
</ul>

<p>Ampere and ADLINK designed this system to be the ultimate Arm development workstation. And that, it is. But I like doing crazy things. Can this also be a great gaming rig? Can the CPU break a teraflop? Can we install a 4070 Ti in it?</p>

<h2>Current Status</h2>

<p>As of today, I have this thing running 128 CPU cores at 2.8 GHz. I upgraded the RAM to 384 GB of DDR4 3200 ECC RAM, specifically six Samsung 64 GB sticks. I installed an Nvidia 4070 Ti.</p>

<p>It's running both Ubuntu 22.04 Server and Windows 11 for Arm now. I even got Steam installed on Ubuntu, after so many commenters kindly pointed out <a href="https://box86.org/">Box86</a> and <a href="https://github.com/ptitSeb/box64">Box64</a> exist!</p>

<p>But before we get into full benchmarks and gaming, I want walk you through the RAM and CPU upgrades, because I learned a lot about this platform. Like, did you know at a certain point, more CPU cores doesn't necessarily give you more performance, even on pure multicore tasks?</p>

<p>Yeah. And the main reason for that? Memory is just not getting faster at the same rate as our CPUs.</p>

<h2>Upgrading the RAM (The importance of benchmarking)</h2>

<p>Have you ever seen one of those fancy server motherboards on Serve The Home? <a href="https://www.servethehome.com/supermicro-as-2015hs-tnr-review-a-server-with-amd-epyc-bergamo/2/">This is a new server</a> for AMD's Bergamo CPU.</p>

<p>Do you see how many memory slots are in this thing? There are <em>twenty four</em>! There're two memory slots for each of the <em>twelve</em> memory channels on the processor.</p>

<p>That's a lot, compared to a normal desktop, where you might get two or four slots for RAM. What gives? Well, modern multicore CPUs are getting faster and faster, but feeding them with data hasn't kept up. So big servers need more and more sticks of RAM feeding data to individual CPU cores just so they don't sit around waiting.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/ram-samsung-transcend-detail.jpeg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-0f7c92de-9952-4382-9922-0df52168d44b" data-insert-attach="{&quot;id&quot;:&quot;0f7c92de-9952-4382-9922-0df52168d44b&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="RAM - Samsung and Transcend detail DDR 3200 ECC CL22"></p>

<p>And RAM goes a lot deeper too. Look at these two sticks of RAM. See how the one on the right has twice the number of memory modules? That allows the individual stick of RAM to pump through data more quickly than the one on the left, even though <em>both</em> of them are rated at DDR 3200 and CL22.</p>

<p>If you look <em>really</em> closely on the labels, you can see one is 1Rx4, and the other one is 1Rx8. <a href="https://www.youtube.com/watch?v=w2bFzQTQ9aI">Actually Hardcore Overclocking</a> has a great video on this, but bottom line, this x4 stick is about 30% faster than the x8!</p>

<p>I tested three setups: 96 GB of Transcend RAM, 96 GB of Samsung RAM, and <em>384 GB</em> of Samsung RAM, and I learned a lot about memory latency and bandwidth.</p>

<p>But the most important thing I learned is this system design only exposes 6 out of the 8 available memory channels on this beefy CPU.</p>

<p>So there's actually an upper limit to how much performance I can get just upgrading the CPU.</p>

<p>This system's memory bandwidth <a href="https://github.com/AmpereComputing/HPL-on-Ampere-Altra/issues/11#issuecomment-1732099325">tops out around 174 Gigabytes per second</a>, but you might get more with more memory channels.</p>

<h2>Upgrading the CPU (96 to 128 cores!)</h2>

<p>A lot of Ampere server builds do give all eight, and maybe I'll get one someday. But for now, I still wanted to test all 128 cores.</p>

<p>For the upgrade, it's a little different than a normal desktop CPU. I popped off the water cooling block, which is normal, but underneath, I made sure to follow the 'OPEN' pattern for loosening the CPU bracket.</p>

<p>I pulled out the 96-core CPU, and that is a <em>lot</em> of pins:</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/lga-socket-ampere-altra-max-cpu.jpeg" width="700" height="467" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-96583318-1f22-4894-a512-5524737ec8ab" data-insert-attach="{&quot;id&quot;:&quot;96583318-1f22-4894-a512-5524737ec8ab&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="LGA Socket for Ampere Altra Max CPU"></p>

<p>I mean, on any of these modern Epyc, or Xeon, or Altra systems, there's just an enormous amount of pins, in this case for up to <em>8</em> memory channels and <em>128</em> lanes of PCI express. That's just a ton of bandwidth.</p>

<p>So in goes the 128-core CPU, and in my case, it's the 2.8 Gigahertz version. They actually make a 3 GHz version, but since Ampere sent this thing to me for testing, and the overall efficiency's probably a tiny bit better at 2.8 Gigahertz anyway, I'm not complaining.</p>

<p>I plugged it back in, booted back up, and it posted at 2.8 Gigahertz, so the next step was to boot into Linux and make sure I could see all 128 cores, which it did.</p>

<p>I ran my linpack benchmark again. On the 96-core CPU it got about 1.2 Teraflops—but I could still only get around 1.2 Teraflops.</p>

<p>After a ton of research, and after upgrading the system to 384 gigabytes of RAM, I could eke out about 1.3 teraflops, but it seems like that's the upper limit on this particular motherboard.</p>

<p>Which, I mean... that's not bad at all. But if you put the same CPU in a server with all 8 channels of RAM, this thing should go even faster, probably past 1.5 teraflops.</p>

<p>But if you <em>really</em> care about teraflops, you need a graphics card.</p>

<p>Nowadays more and more workloads can go <em>way</em> faster using a GPU, especially AI and machine learning. Not to mention games and design apps.</p>

<p>And sorry, Apple, but only allowing your own integrated GPU just doesn't cut it.</p>

<h2>Installing the GPU (4070 Ti)</h2>

<p>I decided to go with an Nvidia 4070 Ti, and before you start yelling at me about not using AMD for a Linux-first build... AMD's drivers on Arm aren't quite as stable yet.</p>

<p>Nvidia making their own massive Arm processors probably has something to do with that, but in any case, I went with <a href="https://amzn.to/49chAc0">this understated ProArt GPU from ASUS</a>. It's not gaudy like most modern graphics cards, and it fits nicely in the case. I test-fitted my 4090 but that thing's a monster, and would also require this bigger power supply.</p>

<p>Maybe I'll upgrade to it someday, but for now, 4070 it is.</p>

<p>Now, before I could get anything out through the graphics card, I had to get drivers going.</p>

<p>Ampere has <a href="https://github.com/AmpereComputing/NVIDIA-GPU-Accelerated-Linux-Desktop-on-Ampere">a guide for the process</a>, but basically I installed a desktop environment since I was running Ubuntu Server, then I installed the Nvidia drivers.</p>

<p>I shut down the system, plugged my monitor into the card with HDMI instead of the integrated VGA port, and away it went!</p>

<p>One thing to note is you won't get any of the early boot stuff like the BIOS screen through a graphics card. Those things still go through the integrated ASPEED controller. But everything else in the OS goes through the GPU now.</p>

<h3>GPU support in Linux</h3>

<p>And Ubuntu had no problems!</p>

<p>I ran Glmark2 and got a score of 10,260, and installed OBS and was excited to see the NVENC hardware encoding worked without any extra setup.</p>

<p>I used OBS to record all the rest of my testing, and it worked without a hitch, allowing the GPU to do all the heavy compression for screen recordings.</p>

<p>Next I booted up SuperTuxKart and got an easy 100 fps with all the settings completely maxed out. I mean, this beat the pants off any other Arm system I've tested so far.</p>

<p>I also installed Blender and messed around with a demo scene. The UI was responsive, and rendering wasn't too painful, but I did notice Blender's CUDA support wasn't working. So the 128-core CPU could keep things moving, but I'm guessing a little more work is required for GPU acceleration.</p>

<p>GPUs are also huge for things like ChatGPT or Llama. And it's easy enough to install Llama locally so I grabbed a massive 13 billion parameter model and installed a web UI. The Ampere chewed through it as I asked a series of questions.</p>

<p>It worked okay, but could be a lot faster if I could get GPU support going. I had a little trouble but again, it's probably not too difficult to get it working, it's just that not many devs working on this software have access to these fast Arm workstations yet.</p>

<p>I mean, even without the GPU, large language models are certainly one way to utilize all 128 cores!</p>

<p>To round things out, I also played back a YouTube video at 4K60 and there was zero issue there. Firefox seems to be using the GPU just fine.</p>

<h3>GPU support in Windows</h3>

<p>I rebooted into Windows 11 and things were a lot more bleak there.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/microsoft-basic-display-adapter.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-9eb1b12f-bc5a-42bd-a9c8-c66d5549c20d" data-insert-attach="{&quot;id&quot;:&quot;9eb1b12f-bc5a-42bd-a9c8-c66d5549c20d&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Microsoft Basic Display Adapter"></p>

<p>The GPU can be seen by Windows, but Nvidia only publishes Arm drivers for Linux, not Windows. So in device manager you just see a Basic Display Adapter, and it can't really do anything.</p>

<p>OBS runs in Windows, but only with software encoding. And Blender won't start at all since it requires OpenGL and a graphics card, neither of which Windows can get going yet on Arm.</p>

<h2>Windows and Cinebench 2024</h2>

<p>But Windows <em>didn't</em> have any problems with the CPU or RAM. it picked up on all 128 cores, and all 384 gigs of RAM.</p>

<p>I was excited, because Cinebench just released their latest 2024 version, and one of the headline features is Windows on Arm support!</p>

<p>They mentioned Snapdragon CPUs, like the one that's in the Windows Dev Kit 2023 I tested last year.</p>

<p>I booted up that system and after waiting an hour or so for Windows Update to finish, I ran Cinebench and got 69 single and 435 multicore.</p>

<p>Just for fun, I also ran it on my M1 Max Mac Studio, and got 111 single, and 799 multi.</p>

<p>On the Ampere? 47 single and 2,409 multi!</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/cinebench-2024-scores-arm-cpus.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-c441ed28-3b56-458c-ba78-9ba90061c956" data-insert-attach="{&quot;id&quot;:&quot;c441ed28-3b56-458c-ba78-9ba90061c956&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Cinebench 2024 Arm CPU Scores"></p>

<p>That even beats the <a href="https://www.cpu-monkey.com/en/cpu_benchmark-cinebench_2024_multi_core">M2 Ultra</a>, which gets a maximum of 1,918 on the multicore test.</p>

<p>Now, the M2 Ultra is gimped a little bit: it only has 24 CPU cores.</p>

<p>But I noticed the MP ratio is only 51x on the Ampere. That ratio should be a lot higher, like at least 100 times. What gives?</p>

<p>Well, I opened up Task Manager, and whether I ran the 96-core or 128-core CPU, and even trying the <em>Enterprise</em> edition of Windows on Arm, there was no way to get Cinebench to use more than 64 CPU cores. <em>Windows</em> used all the cores, it was just Cinebench that seemed to have an issue.</p>

<p>If that gets fixed we should be able to go way past 2,400. Maybe around 4,000–but we'll see. I've been in contact with Maxon, and they now have access to some beefier hardware for testing.</p>

<p>So Cinebench is one thing, but something a lot of people mentioned is I could try Minecraft on Windows, since there may be an Arm native version in the Microsoft Store.</p>

<h3>Games: Minecraft on Windows</h3>

<p>I installed the Java and Bedrock edition from the Store, and... well... it ran. It wasn't quite playable, and it looks like it's trying to run the game off the tiny ASPEED graphics, which can barely do 10 frames a second.</p>

<h2>Steam on Ampere with Box86 and Box64</h2>

<p>But it's an entirely different experience on Linux. I installed Minecraft with Pi-Apps, and it ran beautifully. Zero issues getting 60 fps. I can't get ray tracing on this version, though. It's the one from the Google Play store, and I don't think that edition has RTX support.</p>

<p>Next I also tried installing Steam, and finally have that running! I followed Ampere's guide for installing Steam using Box86 and Box64, though I did have to tweak one install step to get the latest version.</p>

<p>The <a href="https://github.com/AmpereComputing/Steam-on-Ampere/issues/11#issuecomment-1732650185">main developer of Box86</a> actually has some Ampere hardware to test now, too, and he's already fixed some emulation bugs while I was making this video.</p>

<p>But anyway, with Steam installed, I started downloading all the games, to see what works. I made sure Proton was enabled, then booted up each game.</p>

<p>CS:GO installed and seemed to start launching, but it kept getting stuck in a boot loop where it would just die silently.</p>

<p>Halo Master Chief Collection would launch and eventually get to a black screen, but it died every time with this little Fatal Error message.</p>

<p>Portal 2 did the same thing as Counter Strike, where it would just silently die every time I launched it.</p>

<p>Despite the fact I got Crysis to run at like 1 frame per second on Windows, I couldn't get it to launch at all on Ubuntu.</p>

<p>I tried Quake but got an OpenGL error, and Need for Speed also died. Obduction gave me the same little fatal error Halo did, and Portal 1 also died.</p>

<p>Finally I tried Superhot, and... that actually worked! It was nice and smooth.</p>

<p>Seeing some progress, I downloaded another older game, Horizon Chase Turbo, and got it running decently well, but only like 10 or 20 fps.</p>

<p>My lucky streak was over though as I couldn't get Batman Arkham Knight to launch either, and Doom gave me this weird fatal error about some OpenGL function not being available.</p>

<p><img src="https://www.jeffgeerling.com/sites/default/files/images/ksp-steam.jpg" width="700" height="394" data-insert-type="image" data-entity-type="file" data-entity-uuid="insert-image-cf1c63fa-cbaa-4002-bbae-4f9d468735e8" data-insert-attach="{&quot;id&quot;:&quot;cf1c63fa-cbaa-4002-bbae-4f9d468735e8&quot;,&quot;attributes&quot;:{&quot;alt&quot;:[&quot;alt&quot;,&quot;description&quot;],&quot;title&quot;:[&quot;title&quot;]}}" alt="Kerbal Space Program running on Ampere Altra Max with 4070 Ti"></p>

<p>My luck was back though, once I ran Kerbal Space Program, it seemed to run great and was plenty fast to be enjoyable. Blasting Jeb off in a rocket never gets old.</p>

<p>This stuff is in very active development right now, so things'll change. I highly recommend you follow Box64's development to find out if your favorite games could run on a Dev Workstation yet.</p>

<p>I mean, gaming isn't at all Ampere's main goal here, but it's cool to see how quickly the community's made things work, and I'm glad Ampere and ADLINK have been supportive of getting more stuff running.</p>

<h2>Devkit and what's next</h2>

<p>After all that testing on the Workstation, I also set up a bare Dev Kit on my test bench. Ampere sent me a board with a smaller 64-core CPU, and I installed the 96 GB of RAM that I originally bought for the Workstation along with a Kioxia SSD and a Corsair power supply.</p>

<p>And it ran actually a tiny bit more efficient than the full Workstation, putting it squarely at the top of my <a href="https://github.com/geerlingguy/top500-benchmark#results">top500 efficiency ranking</a>, outperforming even the Orange Pi 5!</p>

<p>There are a few quirks to it, though. Like the main fan header isn't actually wired up, so you have to use an adapter to get the CPU fan working. And (like I mentioned earlier) it only exposes 6 of the 8 memory channels, so the highest-end CPUs can't perform to their full potential. Finally, power consumption is about 3W powered off since it runs a built-in BMC for remote access, and booted up it idles around 50W.</p>

<p>But if you use it for anything that needs lots of CPU power and expansion, it's one of the most energy efficient computers on the market.</p>

<p>These things are infinitely more upgradeable than a Mac Pro, while costing less than half as much, and I'm excited to see where ADLINK and Ampere take this platform in the future. This is a good start, and I think they could actually make a dent in areas even outside the workstation space, but we'll see. Right now a lot of focus is on the even <em>more</em> massive <a href="https://amperecomputing.com/briefs/ampereone-family-product-brief">AmpereOne CPUs</a>, with up to <em>192</em> cores, DDR5, and PCIe Gen 5!</p>

<p>This thing can't do media production like my Mac, but for all my dev work, it could definitely be my main computer.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US immigration enforcement used AI-powered tool to scan posts derogatory to US (116 pts)]]></title>
            <link>https://www.techspot.com/news/100642-ice-used-ai-powered-tool-scan-social-media.html</link>
            <guid>38038512</guid>
            <pubDate>Fri, 27 Oct 2023 14:00:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techspot.com/news/100642-ice-used-ai-powered-tool-scan-social-media.html">https://www.techspot.com/news/100642-ice-used-ai-powered-tool-scan-social-media.html</a>, See on <a href="https://news.ycombinator.com/item?id=38038512">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>TechSpot is celebrating its 25th anniversary. TechSpot means tech analysis and advice <a href="https://www.techspot.com/ethics.html" target="_blank">you&nbsp;can&nbsp;trust</a>.</p><div>
<p id="why-it-matters"><strong>A hot potato:</strong> The rule about being careful what you put on social media appears to be especially relevant for anyone entering the US. Immigration and Customs Enforcement (ICE) has used an AI-powered tool to scan through social platforms to identify any visa applicants' posts that are "derogatory" to the US. </p>
<p>The system, called Giant Oak Search Technology (GOST), ranks a person's social media scores from one to 100 based on what it thinks is relevant to the user's specific mission. The database is searchable using identifiers such as a person's name, address, email address, and country of citizenship.</p>
<p>After clicking on a specific individual, analysts can review images collected from the subject's social media accounts and elsewhere, and give them a thumbs up or thumbs down rating. It's also possible for the analysts to look at the person's social media profiles, and their "social graph," themselves to see any potential connections with others.</p>
<p><picture><source type="image/webp" data-srcset="https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j.webp 2560w" data-sizes="(max-width: 960px) 100vw, 680px"><img alt="" height="1730" src="https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26.jpg" width="2560" data-src="https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26.jpg" data-srcset="https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j.webp 2560w" sizes="(max-width: 960px) 100vw, 680px" srcset="https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_500.webp 500w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j_1100.webp 1100w, https://www.techspot.com/images2/news/bigimage/2022/03/2022-03-04-image-26-j.webp 2560w"></picture></p>
<p>404 Media <a href="https://www.404media.co/inside-ices-database-derogatory-information-giant-oak-gost/">reports</a> that a Freedom of Information Act (FOIA) lawsuit brought by both the ACLU and the ACLU of Northern California showed GOST has been used by immigration services and multiple government agencies since 2014. ICE has paid Giant Oak Inc. more than $10 million since 2017, according to public procurement records.</p>
<p>GOST was part of a 2016 pilot called the HSI [Homeland Security Investigations] PATRIOT Social Media Pilot Program that targeted potential overstay violators from countries of concern.</p>
<p>Customs and Border Protection (CBP), the Drug Enforcement Administration (DEA), the State Department, the Air Force, and the Bureau of the Fiscal Service, which is part of the US Treasury, have all paid for Giant Oak services over the last nearly ten years.</p>
<p>The GOST <a href="https://www.giantoak.com/government">website</a> states that it leverages information on the open and deep web and applies search parameters focused on behavioral patterns rather than identity labels.</p>
<p>"The government should not be using algorithms to scrutinize our social media posts and decide which of us is 'risky.' And agencies certainly shouldn't be buying this kind of black box technology in secret without any accountability," said Patrick Toomey, Deputy Director of the ACLU's National Security Project. "DHS needs to explain to the public how its systems determine whether someone is a 'risk' or not, and what happens to the people whose online posts are flagged by its algorithms."</p>
<p>The records state that the contract between the DHS and Giant Oak ended in August 2022.</p>
<p>Back in 2019, the Trump administration brought in new rules first proposed in March 2018 in which visa applicants must hand over <a href="https://www.techspot.com/news/80332-us-visas-now-demand-social-media-details.html">details</a> of any social media channels they have used in the past 5 years. The State Department and the Department of Homeland Security can hold onto this information indefinitely, share it with other federal agencies, and disclose it - in some circumstances, to foreign governments.</p>
<p>2019 was the same year that a Harvard student was <a href="https://www.techspot.com/news/81644-harvard-student-denied-entry-us-over-friends-social.html">denied entry</a> to the US because of his friends' social media activity.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A third of chocolate products are high in heavy metals (272 pts)]]></title>
            <link>https://www.consumerreports.org/health/food-safety/a-third-of-chocolate-products-are-high-in-heavy-metals-a4844566398/</link>
            <guid>38038465</guid>
            <pubDate>Fri, 27 Oct 2023 13:57:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.consumerreports.org/health/food-safety/a-third-of-chocolate-products-are-high-in-heavy-metals-a4844566398/">https://www.consumerreports.org/health/food-safety/a-third-of-chocolate-products-are-high-in-heavy-metals-a4844566398/</a>, See on <a href="https://news.ycombinator.com/item?id=38038465">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main">
                    

<div>
                      <figure>
            <picture>
                    <source srcset="https://article.images.consumerreports.org/image/upload/w_510,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(max-width: 767px) and (-webkit-min-device-pixel-ratio: 2),
            (max-width: 767px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_510,f_auto,q_auto,ar_16:9,c_lfill/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(max-width: 767px)">
                            <source srcset="https://article.images.consumerreports.org/image/upload/w_770,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(max-width: 992px) and (-webkit-min-device-pixel-ratio: 2),
            (max-width: 992px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_770,f_auto,q_auto,ar_16:9,c_lfill/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(max-width: 992px)">
                            <source srcset="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill,dpr_2.0/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(min-width: 1200px) and (-webkit-min-device-pixel-ratio: 2),
            (min-width: 1200px) and (min-resolution: 192dpi)">
        <source srcset="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" media="(min-width: 1200px)">
            
    <img src="https://article.images.consumerreports.org/image/upload/w_945,f_auto,q_auto,ar_16:9,c_lfill/v1696263716/prod/content/dam/CRO-Images-2023/10October/Special-Projects/CR-SP-InlineHero-Heavy-Metals-in-Chocolate-Products-1023" alt="Lead and Cadmium element symbols on pieces of milk and dark chocolate surrounded by chocolate cake, brownies, chocolate chips, chocolate syrup, and Cocoa powder." fetchpriority="high" width="945" height="531">
</picture>
    
                <span>
            Photo Illustration: Chris Griggs/Consumer Reports, Getty Images
        </span>
    </figure>

                </div>

<div>
            <p><time datetime="2023-10-25T06:00">
                    October 25, 2023
            </time>
</p>
                        <div>
                    <p><span itemprop="author">Data Visualizations by Andy Bergmann</span>
                    </p>
                </div>
                        
        </div>

    



                    <div id="intro">
                <p>With the holiday season approaching, many of us will be indulging in a favorite treat: chocolate. Yet despite dark chocolate’s reputation as a healthier sweet, it can also be contaminated with lead and cadmium, two heavy metals linked to serious health problems, as many people learned from <a href="https://www.consumerreports.org/health/food-safety/lead-and-cadmium-in-dark-chocolate-a8480295550/">Consumer Reports’ testing last year</a>.</p>
<p>Now CR has the results of our new tests on heavy metal levels in other kinds of chocolates and foods made with it.</p>
            </div>
    <div>
                <p>In chocolate products, the lead and cadmium are concentrated in the cocoa (or cacao), the ingredient that gives chocolate its distinctive flavor. Dark chocolate tends to have higher levels of cacao. But other chocolate products contain cacao, too, in varying quantities—from cocoa powder, which is essentially pure cocoa, to <a href="https://www.consumerreports.org/health/food-safety/are-there-heavy-metals-in-milk-chocolate-a1095452037/">milk chocolate</a>, which can have very little.</p>
<p>CR’s experts wanted to see whether other cacao-containing foods posed a risk, so we tested 48 different products in seven categories—cocoa powder, chocolate chips, milk chocolate bars, and mixes for brownies, chocolate cake, and hot chocolate. We also added a few more dark chocolate bars to our test. Products came from big name brands such as Hershey’s, Ghirardelli, and Nestlé; national retailers like Costco, Target, Trader Joe’s, Walmart, and Whole Foods; and specialty makers such as Droste and Navitas.</p>
<p>As expected, dark chocolates tended to have higher levels of heavy metals and milk chocolate lower. “But every product we tested had detectable amounts of lead and cadmium,” says James E. Rogers, PhD, director and acting head of product safety testing at CR. “Sixteen of the 48 products had amounts above CR’s levels of concern for at least one of the heavy metals—in some cases more than twice our limit—but we did find safer options in each category of chocolate products.”</p>
            </div>
    
    <div id="heavy-metals-in-chocolate">
                    <p>Heavy metals can be found in many foods—such as arsenic in rice, mercury in some types of fish, cadmium in spinach, and lead in carrots and sweet potatoes. And you can also be exposed through drinking water or your environment (such as lead paint in your house). All these sources can add up, so it is important to be aware of different pathways that contribute to your overall heavy metal intake. Chocolate may just be one of a number of contributing factors to overall heavy metal levels, but it’s a popular treat eaten by children and adults and not an essential part of a someone’s diet. So it makes sense to try to limit the amount of heavy metals people get from chocolate.&nbsp;</p>
<p>Exposure to heavy metals is of greatest concern in children and during pregnancy, because they can damage the brain and nervous system, causing developmental delays, learning and behavior problems, and more. But adults can also experience negative effects. For example, frequent lead exposure has been linked to <a href="https://www.consumerreports.org/health/nutrition-healthy-eating/immune-boosting-foods-that-help-keep-you-well-a4816367434/">immune system suppression</a>, reproductive issues, kidney damage, and hypertension.</p>
<p>Lead and cadmium are the two heavy metals that CR’s tests have found to be the most problematic in chocolate. Research indicates that <a href="https://www.consumerreports.org/health/food-safety/how-lead-and-cadmium-get-into-dark-chocolate-a3299517114/">lead and cadmium get into cocoa in different ways</a>. For cadmium, it appears that the cocoa plant takes it up from the soil. Lead, however, can be deposited on the cocoa beans after harvest, potentially from dust and soil as beans dry outdoors. These metals are both found in the cocoa solids—which, along with cocoa butter, make up cacao. That’s why products rich in cocoa solids, such as dark chocolate and cocoa powder, tend to be higher in heavy metals.</p>
                </div>
    <div id="how-c-r-tested-chocolate-products">
                    <p>We measured the amount of lead, cadmium, mercury, and arsenic in three samples of each food and averaged the results. None of the products posed a risk of arsenic or mercury exposure.</p>
<p>To assess the risk from lead and cadmium, we looked at whether a serving of each product would expose someone to California’s standard maximum allowable dose levels (MADL) for lead (0.5 micrograms per day) and cadmium (4.1 mcg per day) in food. Note that as part of a settlement to a lawsuit currently in place brought by As You Sow, an organization that pushes for corporate accountability, the vast majority of chocolate products sold in the state are subject to less stringent standards, while companies work to reduce the levels of metals in their chocolate products. </p>
<p>CR’s scientists measured heavy metal content against California’s standard levels because there are no federal limits for the amount of lead and cadmium most foods can contain, and they believe that California’s standard levels are the most protective available. However, our tests are not assessments of whether a product exceeds California’s or any other legal standard—they are meant to indicate which products had comparatively higher levels of heavy metals.&nbsp;</p>
<p>Our results are shown by category in the charts in each section below. We list the percentages of the standard MADL levels for lead and cadmium supplied in one serving of the foods. While both heavy metals increase the risk of serious health problems, products within each category are listed in order of lead level, because that heavy metal poses particular concerns and no amount of it is considered safe.</p>
                </div>
    <div id="dark-chocolate">
                    <p>When we tested dark chocolate bars last year, we found lead or cadmium levels above CR’s thresholds in 23 of 28 bars, or 82 percent of them. Our results this time were similar. Of the seven bars we tested, five, or 71 percent, were above our levels for lead, cadmium, or both.</p>
<p>Two bars—Divine 70% Deliciously Smooth Dark Chocolate and Sam’s Choice (Walmart) Dark Chocolate 85% Cocoa—fell below CR’s levels for both lead and cadmium, based on a serving of about 1 ounce. (The following bars also came in below our thresholds when we tested them last year: Ghirardelli Intense Dark Chocolate 86% Cacao, Ghirardelli Intense Dark Chocolate Twilight Delight 72% Cacao, Mast Organic Dark Chocolate 80% Cocoa, Taza Chocolate Organic Deliciously Dark Chocolate 70% Cacao, and Valrhona Abinao Dark Chocolate 85% Cacao.)&nbsp;</p>
<p>Eating an ounce of four others would put you over our limit for lead. The Perugina Premium Dark Chocolate bars had the highest amounts. One of the four, Evolved Signature Dark 72% Cacao Chocolate Bar was high in both lead and cadmium. Another bar, Sam’s Choice Dark Chocolate 72% Cocoa, was high in cadmium only.&nbsp;</p>
<p>Nestlé, which owns Perugina, told CR, “We apply strict standards to ensure our products are high quality and comply with all applicable regulatory requirements, including limits for cadmium and lead<em>.</em>” And Rick Gusmano, co-founder of Evolved Chocolate, said the company’s chocolate products fall well below levels set in the As You Sow settlement and that the company “regularly tests raw materials and finished goods to ensure compliance and, ultimately, consumer safety.” Other makers of dark chocolate with high levels of lead or cadmium did not respond to a request for comment.&nbsp;</p>
                </div>
    
    <div id="milk-chocolate">
                <p>Milk chocolate tends to be lower in heavy metals than dark chocolate because it has less cocoa solids. And in fact none of the five milk chocolate bars in our tests were over CR’s limit for either heavy metal. Hershey’s Milk Chocolate bar had the most lead, reaching 67 percent of CR’s limit. Feastables Mr. Beast Bar Milk Chocolate, with 80 percent of CR’s limit, had the most cadmium per serving. Lindt Classic Recipe Milk Chocolate Bar was the lowest overall, with one serving (about 1 ounce) containing 11 percent of the daily maximum amount of lead and 13 percent of the daily cadmium limit.</p>
            </div>
    
    <div id="chocolate-chips">
                    <p>None of these 12 products had high levels of cadmium, and only two—Hu Dark Chocolate Gems and Good &amp; Gather (Target) Semi-Sweet Mini Chocolate Chips—were over CR’s limit for lead.&nbsp;</p>
<p>But there’s a caveat: The serving size for chocolate chips is just around ½ ounce (about 1 tablespoon)—the amount you might expect to get in a cookie or two, depending on the size of the cookie. If you’re the type that likes to eat more than a few cookies, or a handful of chips straight out of the bag, with many of these you could exceed the daily limits for both cadmium and lead by eating just two servings. Some good options for snacking that are relatively low in both heavy metals are 365 Whole Foods Market Semi-Sweet Chocolate Baking Chips, Kirkland Signature Semi-Sweet Chocolate Chips, and Nestlé Toll House Semi-Sweet Morsels.&nbsp;</p>
<p>A spokesperson for Hu told CR that our test results were in line with the company’s own testing, but added that those levels fall far below those set in the As You Sow lawsuit settlement. Target did not respond to a request for comment.</p>
                </div>
    
    <div id="cocoa-powder">
                    <p>Cocoa powder is almost all cocoa solids, so you might expect that most would be too high in lead and cadmium, even in small amounts. But none of those we tested were high in cadmium, and only two had high levels of lead.&nbsp;</p>
<p>Most of the cocoa powders in our tests were natural-style—the kind most commonly available in the U.S.—and of those, a serving (1 tablespoon) of Hershey’s Cocoa Naturally Unsweetened 100% Cacao exceeded our lead limit.&nbsp;</p>
<p>Droste Cacao Powder was the only Dutch processed cocoa in our tests. This type of cocoa is alkalized to give it a less bitter taste. It was also the highest in lead of any product in our tests, supplying 324 percent of CR’s limit.&nbsp;</p>
<p>The best cocoa powder overall was Navitas Organics Organic Cacao Powder, which reached 77 percent of CR’s lead limit and 17 percent of the cadmium limit. Navitas has a third party test all finished products for heavy metals to ensure low levels, according to the <a href="https://navitasorganics.zendesk.com/hc/en-us/articles/4408161123604-Are-there-heavy-metals-in-your-cacao-">company website</a>.</p>
<p>Neither Droste nor Hershey responded to a request for comment.</p>
                </div>
    
    <div id="hot-chocolate-mixes">
                    <p>These mixes contain cocoa powder plus sugar and other ingredients, so we expected that they would be relatively low in lead and cadmium. That’s not what we found. Four of the six mixes we tested exceeded our lead limit: Great Value (Walmart) Milk Chocolate Flavor Hot Cocoa Mix, had the highest levels, with mixes from Trader Joe’s and Nestlé (which also makes hot chocolate mix for Starbucks), above CR’s cutoff.&nbsp;</p>
<p>The Nestlé spokesperson said that the company stands by the safety of its products and that it works with its “suppliers on an ongoing basis to closely monitor and minimize the presence of these substances in our foods as much as possible.” Other makers of hot chocolate with high levels of lead did not respond to requests for comment.</p>
                </div>
    
    <div id="brownie-and-cake-mixes">
                    <p>These products fared well overall in our tests. None were high in cadmium, and just one brownie mix and two cake mixes exceeded CR’s lead limits—one by quite a bit. One serving of Bob’s Red Mill Gluten Free Chocolate Cake Mix had 216 percent. The heavy metal levels refer to the amounts of the mix that are in one serving of the finished cake or brownie. (We list the number of servings each mix makes in the charts below.)&nbsp;</p>
<p>However, the serving sizes are small. For instance, Duncan Hines Devil’s Food Cake mix makes a cake that the manufacturer says will serve 10. The company’s Double Fudge Brownie mix makes 20 servings. If your cake or brownie portions are more generous, keep in mind that you’ll be getting more lead and cadmium than we list here.&nbsp;</p>
<p>Bob’s Red Mill, Simple Mills, and Ghirardelli did not respond to requests for comment.</p>
                </div>
    
    
    <div id="making-chocolate-safer">
                    <p>Since any intake of heavy metals can be harmful over time, it’s important that products contain the lowest amount possible. There are ways for manufacturers to reduce the heavy metals in their products—such as sourcing chocolate from areas that have low levels of cadmium in the soil, and making improvements in cocoa harvesting, processing, and cleaning procedures.&nbsp;</p>
<p>CR reached out to an industry trade group as well as the Food and Drug Administration for comment. </p>
<p>“Chocolate and cocoa are safe to eat and can be enjoyed as treats, as they have been for centuries,” says Christopher Gindlesperger, senior vice president of public affairs and communications for the National Confectioners Association, a candy industry group. “Food safety and product quality remain our highest priorities, and we remain dedicated to being transparent and socially responsible.”</p>
<p>The Food and Drug Administration told CR that “While the presence of cadmium and lead in chocolate has been the subject of considerable media attention, experts from around the world have found that chocolate is a minor source of exposure to these contaminants internationally.” And the agency added&nbsp;that “ all food manufacturers and processors are responsible for ensuring the safety of their food."</p>
<p>Our results, however, show that some companies may be doing a better job of keeping metals out of their products than others. That’s true even for dark chocolate and cocoa powders. “In general, products with higher cocoa content tend to have higher levels of metals, but not always,” says Eric Boring, PhD, a CR chemist who oversaw our chocolate tests. “There’s enough variation in the lead levels within each category of foods that it’s clear factors other than cocoa content affect lead levels, and that means manufacturers have the ability to reduce the heavy metals in their products to the lowest levels possible.” </p>
<p>For example, if Navitas Organic Cacao can be lower in lead, Boring says, why can’t Hershey’s make a cocoa powder that is lower in the heavy metal, too?</p>
<p>Brian Ronholm, director of food policy at CR, adds that "Earlier this year, a Hershey executive stated that the company continues to look for ways to remove more of the metals through additional cleaning and alternate sourcing. We would like for them to honor that commitment." </p>
<p>“Since the metals occur naturally in soil, it may seem that it would be difficult to reduce contamination, but <a href="https://www.consumerreports.org/health/food-safety/chocolate-makers-urged-to-get-lead-cadmium-out-of-products-a6449371819/">there are some steps that chocolate makers can take</a> to make their products safer,” Ronholm says. These include sourcing from areas with lower levels and mixing beans from different areas to ensure that the final product has lower levels. Producers could also test lots of cocoa to identify problem areas and reject particularly contaminated lots, he says.</p>
                </div>
    <div id="making-healthier-choices">
                    <p>• As much as possible, it makes sense to try to avoid heavy metals in your diet—but that doesn’t mean you should never eat chocolate.</p>
<p>• Kids and pregnant people should consume dark chocolate sparingly, if at all, because heavy metals pose the highest risk to young children and developing babies. And if you do eat it, pick products that our tests showed to have lower levels of heavy metals. When consuming other cocoa-containing items—like hot chocolate or brownies—it may be best to limit these to not every day, and also choose products lower in heavy metals. And, of course, you should limit how much you eat other foods that tend to be high in heavy metals, such as rice and rice products, carrots, and sweet potatoes.</p>
<p>• Milk chocolate can be a fine alternative for those who want to limit heavy metal exposure, but don’t treat it as a health food—it’s packed with more sugar than dark chocolate, and should still be consumed in moderation.</p>
<p>• For other adults who want to eat dark chocolate, occasional consumption won’t necessarily expose you to extremely high levels of heavy metals. But as much as possible, try to be aware of potential metal exposure from multiple sources. For more tips, see our <a href="https://www.consumerreports.org/health/food-safety/lead-and-cadmium-in-dark-chocolate-a8480295550/">previous article on metals in chocolate</a>.&nbsp;</p>
<p>• When consuming hot chocolate, brownies, chocolate cake, and other cocoa-containing products, know that they can contribute to your overall heavy metal burden. As with other types of chocolate, these are best consumed in moderation.</p>
                </div>
    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: ScratchDB – Open-Source Snowflake on ClickHouse (211 pts)]]></title>
            <link>https://github.com/scratchdata/ScratchDB</link>
            <guid>38038239</guid>
            <pubDate>Fri, 27 Oct 2023 13:34:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/scratchdata/ScratchDB">https://github.com/scratchdata/ScratchDB</a>, See on <a href="https://news.ycombinator.com/item?id=38038239">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" id="user-content-scratchdb" dir="auto"><a href="#scratchdb">ScratchDB</a></h2>
<p dir="auto">ScratchDB is a wrapper around Clickhouse that lets you input arbitrary JSON and
perform analytical queries against it. It automatically creates tables
and columns when new data is added.</p>
<h2 tabindex="-1" id="user-content-quickstart" dir="auto"><a href="#quickstart">Quickstart</a></h2>
<h4 tabindex="-1" id="user-content-1-run-the-server" dir="auto"><a href="#1-run-the-server">1. Run the server</a></h4>
<p dir="auto">Clone the repo:</p>
<div dir="auto" data-snippet-clipboard-copy-content="$ git clone git@github.com:scratchdata/ScratchDB.git
$ cd ScratchDB"><pre>$ git clone git@github.com:scratchdata/ScratchDB.git
$ <span>cd</span> ScratchDB</pre></div>
<p dir="auto">Start clickhouse and localstack:</p>

<p dir="auto">In a separate terminal, start the insert service:</p>

<p dir="auto">Finally, in an additional terminal window, start the ingest service:</p>

<h4 tabindex="-1" id="user-content-2-insert-json-data" dir="auto"><a href="#2-insert-json-data">2. Insert JSON data</a></h4>
<div dir="auto" data-snippet-clipboard-copy-content="$ curl -X POST http://localhost:3000/data \
    -H 'Content-Type: application/json' \
    -H 'X-Api-Key: local' \
    -d '{&quot;table&quot;:&quot;my_table&quot;,&quot;data&quot;:{&quot;fruit&quot;: &quot;apple&quot;}}'"><pre>$ curl -X POST http://localhost:3000/data \
    -H <span><span>'</span>Content-Type: application/json<span>'</span></span> \
    -H <span><span>'</span>X-Api-Key: local<span>'</span></span> \
    -d <span><span>'</span>{"table":"my_table","data":{"fruit": "apple"}}<span>'</span></span></pre></div>
<h4 tabindex="-1" id="user-content-3-query" dir="auto"><a href="#3-query">3. Query</a></h4>
<p dir="auto">To view data in JSON format: <a href="http://localhost:3000/query?q=select%20*%20from%20my_table" rel="nofollow">http://localhost:3000/query?q=select * from my_table</a></p>
<div dir="auto" data-snippet-clipboard-copy-content="curl -H 'X-Api-Key: local' &quot;http://localhost:3000/query?q=select%20*%20from%20my_table&quot;"><pre>curl -H <span><span>'</span>X-Api-Key: local<span>'</span></span> <span><span>"</span>http://localhost:3000/query?q=select%20*%20from%20my_table<span>"</span></span></pre></div>
<p dir="auto">To view data in an HTML table: <a href="http://localhost:3000/query?format=html&amp;q=select%20*%20from%20my_table" rel="nofollow">http://localhost:3000/query?format=html&amp;q=select * from my_table</a></p>
<div data-snippet-clipboard-copy-content="curl -H 'X-Api-Key: local' &quot;http://localhost:3000/query?format=html&amp;q=select%20*%20from%20my_table&quot;"><pre><code>curl -H 'X-Api-Key: local' "http://localhost:3000/query?format=html&amp;q=select%20*%20from%20my_table"
</code></pre></div>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Soap Works: The Science Behind Handwashing (151 pts)]]></title>
            <link>https://www.pfizer.com/news/articles/how_soap_works_the_science_behind_handwashing</link>
            <guid>38038174</guid>
            <pubDate>Fri, 27 Oct 2023 13:28:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pfizer.com/news/articles/how_soap_works_the_science_behind_handwashing">https://www.pfizer.com/news/articles/how_soap_works_the_science_behind_handwashing</a>, See on <a href="https://news.ycombinator.com/item?id=38038174">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>While there’s still much we don’t understand about COVID-19, there's one piece of advice that experts in the health care community agree on: washing your hands with soap and water is one of the most effective ways people can keep from getting sick, and from passing the virus to others<sup>1</sup>.</p><p>Soap, which has been in around for thousands of years,<sup>2</sup> is uniquely structured to help destroy the virus. “Soap works because it’s the right structure to do the job,” says Rebecca Gallego, senior principal scientist, medicinal sciences with Pfizer. “No mater how hard you try, if you were to scrub with just water, or if you were to scrub with sand, it wouldn't work.”<b>&nbsp;</b></p><h2><b>Here’s why it’s soap is so effective: </b></h2><p>Coronavirus molecules look a bit like spiky sea urchins. The outer layer is a plasma membrane made of two layers of phospholipids. The phospholipids consist of a greasy lipid part that is hydrophobic, meaning it hates water, and a phosphate head group that is hydrophilic, meaning it loves water. The spikes on a coronavirus are made of protein. They help transmit the illness by penetrating a host cell in a human being and transferring the genetic information of the virus, which then replicates itself. That’s when a person gets sick.</p><p>And that’s what soap is able to stop.</p><p>A soap molecule, which looks like a tadpole, has a hydrophilic (water-loving) head and a hydrophobic (water-hating) tail. The water-hating part of the soap wants to get away from the water. If the virus is on a person’s hands, that water-hating tail is drawn to that fatty layer. It pries its way in.</p><p>“When soap comes into contact with the&nbsp;plasma membrane of the virus, it’ll try to wedge itself in there,” says Gallego. “If you get enough of these soap molecules into the plasma membrane, it breaks it apart, destroying it.” The virus pops like a balloon, spilling its insides.</p><p>When a person scrubs his or her hands for 20 seconds, as the&nbsp;CDC guidelines recommend<sup>3</sup>, the motion builds up more bubbles, which finds their way into the cracks and crevices of the hands. This allows the soap to do its job more thoroughly by destroying more and more of the virus, preventing someone from getting sick, themselves, and from passing the virus on to others.</p><p>Gallego says that this approach is simple, but effective, when done correctly.</p><p>“Doing all these&nbsp;little things can add up to a big impact factor when it comes to preventing the disease,” she says.</p><p><img alt="" height="1" src="https://pixel.welcomesoftware.com/px.gif?key=YXJ0aWNsZT1hZTkwYjlhZjhiMTBhMWIxYThiNTU1ZjYzN2U0YTdhMw==" width="1"></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Slow Death of Authenticity in an Attention Economy (439 pts)]]></title>
            <link>https://www.coryzue.com/writing/authenticity-and-engagement/</link>
            <guid>38037851</guid>
            <pubDate>Fri, 27 Oct 2023 13:00:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.coryzue.com/writing/authenticity-and-engagement/">https://www.coryzue.com/writing/authenticity-and-engagement/</a>, See on <a href="https://news.ycombinator.com/item?id=38037851">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
                <p><img src="https://www.coryzue.com/images/authenticity/midwit-twitter.jpg" alt="Midwit meme"></p>

<p>How I wish people used Twitter versus how they seem to be doing it lately.</p>

<p>I’m not quite sure when I started feeling uncomfortable with Twitter.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup></p>

<p>It <em>wasn’t</em> when Elon Musk bought it, or during the mass firings, or the bluecheck fiasco, or the rebrand to X, or any of the other politically-obvious times that many people decided to give up on it.</p>

<p>No, I was a Twitter optimist for a long time.</p>

<p>But somehow, somewhere over the last few months, my positive outlook on the platform has slowly eroded away. As I—ironically—spend as much time as ever scrolling through my feed—I find myself more and more annoyed, jealous, outraged, yes, but mostly just… <em>bored</em>.</p>

<p>And what’s funny is that the content is more “engaging” than ever. My feed is full of posts that have obviously had more effort put into it than most of what I used to see. Megathreads about AI, thoughtful, longform narratives that could have been blog posts, carefully curated images, and super-positive business updates. It’s mostly <em>engaging stuff</em>.</p>

<p>And therein, I think, lies the problem. <strong>The content I now see on Twitter is content that has been designed to be seen on Twitter.</strong> Tweeting has become a job. Quite literally, for many people, ever since they’ve started paying creators a share of ad revenue.</p>

<p>And yes, on the surface this incentivizes people to create better content. The better your content is, the more it gets seen, and the more money you make. And yet, my felt experience of this change is the exact opposite—as people seek more engagement, their content gets <em>worse</em>. What’s going on here?</p>

<p>One possibility is that I am unusual. I go on Twitter for <em>authenticity</em>. I have carefully curated a list of human beings who I know by name, and whose ideas and actions interest me. But authenticity is often at odds with growth.</p>

<p>Why? Well to <em>grow</em> you need to be noticed. To be noticed, you need to stand out. And to stand out is—usually—inauthentic.
Yes, we all say and do noteworthy things, but not every day. To do or say noteworthy things every day involves some degree of forcedness, repetition, or <em>trying</em>. The opposite of authenticity.</p>

<p>If I wanted to get 10x more engagement than usual on a Tweet tomorrow, I could. I could post some celebratory brag about how much money I’m earning from my businesses (“omg $10k MRR!”). I could pick a fight on a topic people feel strongly about (“React sucks!”).
I could mention it’s my birthday and post a picture of myself (“Can’t believe I’m 41!”).<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>
I could ask a question that lets people promote themselves (“What’s your favorite personal website?”),
angrily quote-tweet a terrible take, and so on.</p>

<p>I know these things work. And I occasionally do them, knowing they will work.
I try to do this only a handful of times a year, because even though they <em>work</em>, and even though they <em>are useful for me and <a href="https://scriv.ai/">my</a> <a href="https://www.saaspegasus.com/">businesses</a></em>,
and even though they <em>make my lizard brain feel good</em>, a part of me still hates them. Even a few times a year.</p>

<blockquote><div lang="en" dir="ltr"><p>Take the red pill and get thousands of likes and new followers. You just have to sacrifice a tiny bit of your authenticity.</p><p>Take the blue pill and be your true, honest, and proud self. Except no one really notices or cares.</p><p>What do you do? <a href="https://t.co/OsIirl93Te">pic.twitter.com/OsIirl93Te</a></p></div>— Cory Zue (@czue) <a href="https://twitter.com/czue/status/1709291338406015447?ref_src=twsrc%5Etfw">October 3, 2023</a></blockquote>


<p>Clearly I’ve been wrestling with this issue for a little while now.</p>

<p>But oh boy, not the people in my feed. The people in my feed—most of whom I’m not following, by the way—<em>love</em> posting for engagement. Some of them love it so much that they offer courses teaching other people how to do it—which amplifies this godforsaken death spiral even further.</p>

<p>And so now we find ourselves in a situation where all these asshats with 20k followers and a Stripe account are now <em>running their Twitter account as a business</em>. And this has led to a slow and inevitable decline from authenticity to some version of marketing (look at my content!) and sales (follow me!).</p>

<blockquote><p lang="en" dir="ltr">The natural end state of marketplaces and social media is the eventual shift from user generated supply to professionalized supply/content. They can fight it for some time, but eventually there is no other way to keep scaling (or prevent power users from peeling off)</p>— Kevin Kwok (@kevinakwok) <a href="https://twitter.com/kevinakwok/status/1688978920001970177?ref_src=twsrc%5Etfw">August 8, 2023</a></blockquote>


<p>What we are witnessing now is the <em>professionalization</em> of Twitter.</p>

<p>And look, I don’t think it’s just the algorithm and the incentives. Elon’s political antics chased away a lot of good people.
Many of my favorite follows have moved to Mastodon, Threads, and Bluesky.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>
Also, more and more people are waking up and realizing that social media is actually quite bad for you, and leaving it behind.
Good for them. Bad for me.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" rel="footnote">4</a></sup></p>

<p>Still—and quite ironically—if anything actually gets me to stop using this platform,
it’s going to be the changes that are supposed to make it grow.</p>

<hr>

<p><em>If you liked this, you can <a href="https://x.com/czue/status/1717889142997086412">share it on Twitter</a>, discuss it <a href="https://news.ycombinator.com/item?id=38037851">on Hacker News</a>,
or sign up below to get emailed when I post new stuff.</em></p>

<p><strong>Notes</strong></p>



            </div></div>]]></description>
        </item>
    </channel>
</rss>