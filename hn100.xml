<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 18 Jul 2023 22:00:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[AutoChain, lightweight and testable alternative to LangChain (104 pts)]]></title>
            <link>https://github.com/Forethought-Technologies/AutoChain</link>
            <guid>36775475</guid>
            <pubDate>Tue, 18 Jul 2023 16:47:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Forethought-Technologies/AutoChain">https://github.com/Forethought-Technologies/AutoChain</a>, See on <a href="https://news.ycombinator.com/item?id=36775475">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">AutoChain</h2>
<p dir="auto">Large language models (LLMs) have shown huge success in different text generation tasks and
enable developers to build generative agents based on objectives expressed in natural language.</p>
<p dir="auto">However, most generative agents require heavy customization for specific purposes, and
supporting different use cases can sometimes be overwhelming using existing tools
and frameworks. As a result, it is still very challenging to build a custom generative agent.</p>
<p dir="auto">In addition, evaluating such generative agents, which is usually done by manually trying different scenarios, is a very manual, repetitive, and expensive task.</p>
<p dir="auto">AutoChain takes inspiration from LangChain and AutoGPT and aims to solve
both problems by providing a lightweight and extensible framework
for developers to build their own agents using LLMs with custom tools and
<a href="#workflow-evaluation">automatically evaluating</a> different user scenarios with simulated
conversations. Experienced user of LangChain would find AutoChain is easy to navigate since
they share similar but simpler concepts.</p>
<p dir="auto">The goal is to enable rapid iteration on generative agents, both by simplifying agent customization and evaluation.</p>
<p dir="auto">If you have any questions, please feel free to reach out to Yi Lu <a href="mailto:yi.lu@forethought.ai">yi.lu@forethought.ai</a></p>
<h2 tabindex="-1" dir="auto">Features</h2>
<ul dir="auto">
<li><g-emoji alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">🚀</g-emoji> lightweight and extensible generative agent pipeline.</li>
<li><g-emoji alias="link" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f517.png">🔗</g-emoji> agent that can use different custom tools and
support OpenAI <a href="https://platform.openai.com/docs/guides/gpt/function-calling" rel="nofollow">function calling</a></li>
<li><g-emoji alias="floppy_disk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png">💾</g-emoji> simple memory tracking for conversation history and tools' outputs</li>
<li><g-emoji alias="robot" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png">🤖</g-emoji> automated agent multi-turn conversation evaluation with simulated conversations</li>
</ul>
<h2 tabindex="-1" dir="auto">Setup</h2>
<p dir="auto">Quick install</p>

<p dir="auto">Or install from source after cloning this repository</p>
<div dir="auto" data-snippet-clipboard-copy-content="cd autochain
pyenv virtualenv 3.10.11 venv
pyenv local venv

pip install ."><pre><span>cd</span> autochain
pyenv virtualenv 3.10.11 venv
pyenv <span>local</span> venv

pip install <span>.</span></pre></div>
<p dir="auto">Set <code>PYTHONPATH</code> and <code>OPENAI_API_KEY</code></p>
<div dir="auto" data-snippet-clipboard-copy-content="export OPENAI_API_KEY=
export PYTHONPATH=`pwd`"><pre><span>export</span> OPENAI_API_KEY=
<span>export</span> PYTHONPATH=<span><span>`</span>pwd<span>`</span></span></pre></div>
<p dir="auto">Run your first conversation with agent interactively</p>
<div dir="auto" data-snippet-clipboard-copy-content="python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py -i"><pre>python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py -i</pre></div>
<h2 tabindex="-1" dir="auto">How does AutoChain simplify building agents?</h2>
<p dir="auto">AutoChain aims to provide a lightweight framework and simplifies the agent building process in a few
ways, as compared to existing frameworks</p>
<ol dir="auto">
<li>Easy prompt update<br>
Engineering and iterating over prompts is a crucial part of building generative
agent. AutoChain makes it very easy to update prompts and visualize prompt
outputs. Run with <code>-v</code> flag to output verbose prompt and outputs in console.</li>
<li>Up to 2 layers of abstraction<br>
As part of enabling rapid iteration, AutoChain chooses to remove most of the
abstraction layers from alternative frameworks</li>
<li>Automated multi-turn evaluation<br>
Evaluation is the most painful and undefined part of building generative agents. Updating the agent to better perform in one scenario often causes regression in other use cases. AutoChain
provides a testing framework to automatically evaluate agent's ability under different
user scenarios.</li>
</ol>
<h2 tabindex="-1" dir="auto">Example usage</h2>
<p dir="auto">If you have experience with LangChain, you already know 80% of the AutoChain interfaces.</p>
<p dir="auto">AutoChain aims to make building custom generative agents as straightforward as possible, with as little abstractions as possible.</p>
<p dir="auto">The most basic example uses the default chain and <code>ConversationalAgent</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="from autochain.chain.chain import Chain
from autochain.memory.buffer_memory import BufferMemory
from autochain.models.chat_openai import ChatOpenAI
from autochain.agent.conversational_agent.conversational_agent import ConversationalAgent

llm = ChatOpenAI(temperature=0)
memory = BufferMemory()
agent = ConversationalAgent.from_llm_and_tools(llm=llm)
chain = Chain(agent=agent, memory=memory)

print(chain.run(&quot;Write me a poem about AI&quot;)['message'])"><pre><span>from</span> <span>autochain</span>.<span>chain</span>.<span>chain</span> <span>import</span> <span>Chain</span>
<span>from</span> <span>autochain</span>.<span>memory</span>.<span>buffer_memory</span> <span>import</span> <span>BufferMemory</span>
<span>from</span> <span>autochain</span>.<span>models</span>.<span>chat_openai</span> <span>import</span> <span>ChatOpenAI</span>
<span>from</span> <span>autochain</span>.<span>agent</span>.<span>conversational_agent</span>.<span>conversational_agent</span> <span>import</span> <span>ConversationalAgent</span>

<span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>temperature</span><span>=</span><span>0</span>)
<span>memory</span> <span>=</span> <span>BufferMemory</span>()
<span>agent</span> <span>=</span> <span>ConversationalAgent</span>.<span>from_llm_and_tools</span>(<span>llm</span><span>=</span><span>llm</span>)
<span>chain</span> <span>=</span> <span>Chain</span>(<span>agent</span><span>=</span><span>agent</span>, <span>memory</span><span>=</span><span>memory</span>)

<span>print</span>(<span>chain</span>.<span>run</span>(<span>"Write me a poem about AI"</span>)[<span>'message'</span>])</pre></div>
<p dir="auto">Just like in LangChain, you can add a list of tools to the agent</p>
<div dir="auto" data-snippet-clipboard-copy-content="tools = [
   Tool(
    name=&quot;Get weather&quot;,
    func=lambda *args, **kwargs: &quot;Today is a sunny day&quot;,
    description=&quot;&quot;&quot;This function returns the weather information&quot;&quot;&quot;
   )
]

memory = BufferMemory()
agent = ConversationalAgent.from_llm_and_tools(llm=llm, tools=tools)
chain = Chain(agent=agent, memory=memory)
print(chain.run(&quot;What is the weather today&quot;)['message'])"><pre><span>tools</span> <span>=</span> [
   <span>Tool</span>(
    <span>name</span><span>=</span><span>"Get weather"</span>,
    <span>func</span><span>=</span><span>lambda</span> <span>*</span><span>args</span>, <span>**</span><span>kwargs</span>: <span>"Today is a sunny day"</span>,
    <span>description</span><span>=</span><span>"""This function returns the weather information"""</span>
   )
]

<span>memory</span> <span>=</span> <span>BufferMemory</span>()
<span>agent</span> <span>=</span> <span>ConversationalAgent</span>.<span>from_llm_and_tools</span>(<span>llm</span><span>=</span><span>llm</span>, <span>tools</span><span>=</span><span>tools</span>)
<span>chain</span> <span>=</span> <span>Chain</span>(<span>agent</span><span>=</span><span>agent</span>, <span>memory</span><span>=</span><span>memory</span>)
<span>print</span>(<span>chain</span>.<span>run</span>(<span>"What is the weather today"</span>)[<span>'message'</span>])</pre></div>
<p dir="auto">AutoChain also added support for <a href="https://platform.openai.com/docs/guides/gpt/function-calling" rel="nofollow">function calling</a>
in OpenAI models. Behind the scenes, it turns the function spec into OpenAI format without explicit
instruction, so you can keep following the same <code>Tool</code> interface you are familiar with.</p>
<div dir="auto" data-snippet-clipboard-copy-content="llm = ChatOpenAI(temperature=0)
agent = OpenAIFunctionsAgent.from_llm_and_tools(llm=llm, tools=tools)"><pre><span>llm</span> <span>=</span> <span>ChatOpenAI</span>(<span>temperature</span><span>=</span><span>0</span>)
<span>agent</span> <span>=</span> <span>OpenAIFunctionsAgent</span>.<span>from_llm_and_tools</span>(<span>llm</span><span>=</span><span>llm</span>, <span>tools</span><span>=</span><span>tools</span>)</pre></div>
<p dir="auto">See <a href="https://github.com/Forethought-Technologies/AutoChain/blob/main/docs/examples.md">more examples</a> under <code>autochain/examples</code> and <a href="https://github.com/Forethought-Technologies/AutoChain/blob/main/docs/workflow-evaluation.md">workflow
evaluation</a> test cases which can also be run interactively.</p>
<p dir="auto">Read more about detailed <a href="https://github.com/Forethought-Technologies/AutoChain/blob/main/docs/components_overview.md">components overview</a></p>
<h2 tabindex="-1" dir="auto">Workflow Evaluation</h2>
<p dir="auto">It is notoriously hard to evaluate generative agents in LangChain or AutoGPT. An agent's behavior
is nondeterministic and susceptible to small changes to the prompt or model. As such, it is
hard to know what effects an update to the agent will have on all relevant use cases.</p>
<p dir="auto">The current path for
evaluation is running the agent through a large number of preset queries and evaluate the
generated responses. However, that is limited to single turn conversation, general and not
specific to tasks and expensive to verify.</p>
<p dir="auto">To effectively evaluate agents, AutoChain introduces the workflow evaluation framework
which simulates the conversation between a generative agent and simulated users with LLM under
different user contexts and desired outcomes of the conversation. This way, we could add test
cases for different user scenarios and use LLMs to evaluate if multi-turn conversations reached
the desired outcome.</p>
<p dir="auto">To facilitate agent evaluation, AutoChain introduces the workflow evaluation framework. This framework runs conversations between a generative agent and LLM-simulated test users. The test users incorporate various user contexts and desired conversation outcomes, which enables easy addition of test cases for new user scenarios and fast evaluation. The framework leverages LLMs to evaluate whether a given multi-turn conversation has achieved the intended outcome.</p>
<p dir="auto">Read more about our <a href="https://github.com/Forethought-Technologies/AutoChain/blob/main/docs/workflow-evaluation.md">evaluation strategy</a>.</p>
<h3 tabindex="-1" dir="auto">How to run workflow evaluations</h3>
<p dir="auto">You can either run your tests in interactive mode, or run the full suite of test cases at once.
<code>autochain/workflows_evaluation/conversational_agent_eval /change_shipping_address_test.py</code> contains a few example test cases.</p>
<p dir="auto">To run all the cases defined in a test file:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py"><pre>python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py</pre></div>
<p dir="auto">To run your tests interactively <code>-i</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py -i"><pre>python autochain/workflows_evaluation/conversational_agent_eval/change_shipping_address_test.py -i</pre></div>
<p dir="auto">Looking for more details on how AutoChain works? See our <a href="https://github.com/Forethought-Technologies/AutoChain/blob/main/docs/components_overview.md">components overview</a></p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sweden Sans (186 pts)]]></title>
            <link>https://identity.sweden.se/en/design-elements/typography</link>
            <guid>36775398</guid>
            <pubDate>Tue, 18 Jul 2023 16:43:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://identity.sweden.se/en/design-elements/typography">https://identity.sweden.se/en/design-elements/typography</a>, See on <a href="https://news.ycombinator.com/item?id=36775398">Hacker News</a></p>
<div id="readability-page-1" class="page">
		
		
    
                        



<!-- #GA4 Integration#} -->
<!-- Google tag (gtag.js) -->


            


    

    


    <div id="wrapper">

        <div>
            <ul>
                <li>
                        <a href="https://identity.sweden.se/en/strategy" target="">Strategy</a>
                        						<ul>
				<li>
					<a href="https://identity.sweden.se/en/strategy/introduction" target="">Introduction</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/strategy/the-starting-point" target="">The starting point</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/strategy/design-principles" target="">Design principles</a>
				</li>
				</ul>
			                    </li>
                <li>
                        <a href="https://identity.sweden.se/en/design-elements" target="">Design elements</a>
                        						<ul>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/brand-marks" target="">Brand marks</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/colour" target="">Colour</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/accessibility" target="">Accessibility</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/typography" target="">Typography</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/grid" target="">Grid</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/tone-of-motion" target="">Tone of motion</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/illustration" target="">Illustration</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/design-elements/graphic-element" target="">Graphic element</a>
				</li>
				</ul>
			                    </li>
                <li>
                        <a href="https://identity.sweden.se/en/download" target="">Download</a>
                        						<ul>
				<li>
					<a href="https://identity.sweden.se/en/download/brand-assets" target="">Brand  assets</a>
				</li>
				</ul>
			                    </li>
                <li>
                        <a href="https://identity.sweden.se/en/inspiration" target="">Inspiration</a>
                        						<ul>
				<li>
					<a href="https://identity.sweden.se/en/inspiration/best-practice" target="">Best practice</a>
				</li>
				</ul>
			                    </li>
                <li>
                        <a href="https://identity.sweden.se/en/about-this-site" target="">About this site</a>
                        						<ul>
				<li>
					<a href="https://identity.sweden.se/en/about-this-site#abouttermsanduse" target="">Terms of use</a>
				</li>
				<li>
					<a href="https://identity.sweden.se/en/about-this-site#contact" target="">Contact us</a>
				</li>
				</ul>
			                    </li>
                            </ul>
        </div>

                    <div id="header" role="banner">
                <div>                    
                    <p><a href="https://identity.sweden.se/">
                        <img src="https://identity.sweden.se/theme/img/sweden_logo.svg" width="auto" height="25px">
                        <span>Brand guidelines</span>
                    </a>
                </p></div>
                
                <p>
                    <form action="/search" method="get" role="search" accept-charset="UTF-8" autocomplete="off">
                        
                        
                        <span>
                            <span>Search</span>
                            <i></i>
                        </span>
                        
                    </form>
                </p>

                <div id="workspace-nav">
                        <ul>
                                                    </ul>
                    </div>
                <div>
                    <p><span>Menu</span></p>

                </div>

            </div>
        
        <div id="main" role="main">
                            

                
                    </div>

                        <div id="skeleton">
                
                <div>
                    <div>
                        
                        
                    </div>
                    <div>
                        
                        
                    </div>
                    <div>
                        
                        
                    </div>
                </div>
            </div>
        
                    <!-- Policy view modal -->
        
        
    </div>             		
		
					
		
		

		
		
					
        
		
		
		
        
    



</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Payment systems while working at a pizza place (163 pts)]]></title>
            <link>https://nickjanetakis.com/blog/what-i-learned-about-payment-systems-while-working-at-a-pizza-place</link>
            <guid>36775098</guid>
            <pubDate>Tue, 18 Jul 2023 16:27:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nickjanetakis.com/blog/what-i-learned-about-payment-systems-while-working-at-a-pizza-place">https://nickjanetakis.com/blog/what-i-learned-about-payment-systems-while-working-at-a-pizza-place</a>, See on <a href="https://news.ycombinator.com/item?id=36775098">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><p>Updated on July 18th, 2023 in <a href="https://nickjanetakis.com/blog/tag/dev-business-tips-tricks-and-tutorials"> #dev-business </a></p><img src="https://nickjanetakis.com/assets/blog/cards/what-i-learned-about-payment-systems-while-working-at-a-pizza-place-f8c5c0e95f575a9c0231f4fbdacfbba0df899ee63290761d81cb9e8cc0dd1290.jpg" width="750" height="422" alt="blog/cards/what-i-learned-about-payment-systems-while-working-at-a-pizza-place.jpg"><h2>Placing an order and capturing a payment are really different actions. I also learned a lot in a few days.</h2><p> <strong>Quick Jump: </strong> <a href="#placing-an-order-for-pickup">Placing an Order for Pickup</a> <small>|</small> <a href="#placing-orders-in-multiple-ways">Placing Orders in Multiple Ways</a> <small>|</small> <a href="#multiple-payments-per-order">Multiple Payments Per Order</a> <small>|</small> <a href="#extra-charges-tips-and-discounts">Extra Charges, Tips and Discounts</a> <small>|</small> <a href="#voids-and-refunds">Voids and Refunds</a> <small>|</small> <a href="#customers">Customers</a> <small>|</small> <a href="#stripe-shopify-and-square">Stripe, Shopify and Square</a> <small>|</small> <a href="#video">Video</a></p><p><strong>Prefer video? I’ve put up a <a href="#video">YouTube video</a> going over this post with a bit more color and detail.</strong></p><p>Up until recently I’ve never worked at a register or collected payments at a physical location. I’ve only ever sold online video courses where you typically start the checkout process and digitally purchase an item within a few seconds or minutes.</p><p>I was helping a friend out recently and worked the counter at a pizza place for a couple of weeks after my normal job. It’s a much different world than doing remote programming and making YouTube videos. I’ll likely make more posts related to this in a programming context but let’s focus on payments in this one.</p><h3 id="placing-an-order-for-pickup">Placing an Order for Pickup</h3><p>Check out this run of the mill workflow that may happen many times a day:</p><ol><li>Customer calls in on the phone to place an order for pickup</li><li>They want to buy a large lightly cooked pie and 6 garlic knots</li><li>You input their order into the POS (point of sale) system</li><li>Each item is listed as a line item with a name, quantity, note and price</li><li>You ask for their name and number and put it into the system</li><li>A total is calculated with sales tax being a separate line item</li><li>The order gets “Created” and the following things happen:<ul><li>The order is placed into “In Progress” with a little icon showing it’s not paid for<ul><li>Being paid or not is independent of the order being in progress</li></ul></li><li>A receipt is printed at the register which is marked “INCOMPLETE”<ul><li>You hang this receipt up so you and others know an order is pending</li></ul></li><li>An order to make 1 pie is printed where the pizza maker can see it</li></ul></li></ol><p>Step 7 is going to be different depending on which POS system you use but the general ideas are the same. There’s the concept of an order being in the system with a status, a human workflow is executed to track that order physically and receipts get printed.</p><p>At no point did we even think about capturing a payment here. For this pickup order there’s a few ways a payment can be collected:</p><ul><li>While on the phone with the customer you enter their credit card details into the system on their behalf, with our above workflow that could have been done after step 6 and then step 7 would be different but let’s put that on hold for now</li><li>They pay for it with either cash or credit when they pick it up</li></ul><p><em>For now we’re also ignoring other types of ways for orders to be input such as them ordering it online and paying for it on the spot or a delivery which could either be paid for and done online or have cash collected upon delivery. More on that later!</em></p><p>Ok so now let’s say the customer comes in 20 minutes from now to pick up their order. At this point you may have accepted 10 other orders from different customers. Some might be in progress and others might be completed. Maybe someone popped in to buy 2 slices to go and was in and out in 3 minutes.</p><p>You check the receipts that are hanging and notice this order is #137, so you go back to the POS system’s “In Progress” tab and find that order. Yep there it is.</p><p>You can see the full total of the order and see if the customer has cash out or a card (or their phone if they’re using Apple / Google Pay). If not, you ask them cash or card and the POS system will let you choose which one.</p><p>If they pick cash, you input the amount they give you and it auto-calculates their change. If they pick card or another digital payment method, they can use the terminal to tap or insert their card / device. It will get authorized and then hopefully be successful unless their card has issues.</p><p>In any case, if the payment is successful then the following happens:</p><ul><li>The order is marked as “Completed” in the system</li><li>Another receipt is printed except with payment information filled in</li><li>You save the receipt if the business owner wants physical receipts saved</li><li>You remove and throw away the hung “INCOMPLETE” receipt so you know it’s done</li></ul><p><em>The pizza maker threw away their print out for that pie independently of the order being completed. From their POV, they are finished when the pie is first placed IN the oven or OUT depending on if the counter workers know how to gauge when a pie is ready to be taken out.</em></p><p>At this point you have a clean slate. The order is complete and it’s been moved through the workflow. Depending on how busy the place is, this might be executed 50 or even 300 times a day with surges coming in around popular lunch and dinner times.</p><p>If you need to go back to an order that’s already been completed you can. You can also print receipts on demand and if there’s been a payment captured you can issue a full or partial refund.</p><p>There’s so much more to go into around payment details but let’s go over a few technical takeaways from the above.</p><h4 id="technical-takeaways">Technical Takeaways</h4><p>Here’s a couple of things I got out of the above and remember thinking about while taking orders and observing the system. If you were building this system out think about the database terms and relationships you might have.</p><p>I’ve quoted the words that could be tables or models.</p><h5 id="an-order-product-and-payment-are-different-things">An “Order”, “Product” and “Payment” are different things</h5><p>An order really pulls together multiple products into something that can be associated as a thing but in my opinion it’s not a join-table between products and payments.</p><p>Order #137 has N products. It also has X payments where X can even be 0 if the order happens to be free, such as if they have a coupon to get 1 free pie after buying 10.</p><p>You would still want that free pie to be put through the workflow and tracked. The only difference is you wouldn’t collect a payment.</p><p>An order has a status to clearly show the stage an order is at. You can almost think of it as a Kanban board that moves from created to completed.</p><h5 id="a-product-has-variants-and-notes">A “Product” has “Variants” and “Notes”</h5><p>At this place a pie can be personal (12”), medium (16”) or large (18”). A customer may want full pepperonis or meatballs. Maybe they want 1/2 and 1/2, or maybe they want 2 full toppings.</p><p>The pie can also have no notes or something specific that the customer wants such as lightly cooked, crispy or well done.</p><p>A variant and note are different things. A variant is something the customer can choose when making the purchase and it may or may not affect the price. Variants are standard things (T-shirt size, etc.) and notes are ADHOC customer requests.</p><p>If enough customers have the same request, maybe that’s a sign you can turn it into a variant but not always. For example, if 90% of people are ok with a normally cooked pizza you may not want to explicitly ask them to choose between regular, light, crispy or well done since it complicates the order process especially if it’s online where they are clicking form fields.</p><p>On the other hand, requiring them to pick large or personal is a good idea since those are different product variants with different prices. However if you were selling shirts you’d likely want colors as a variant even if they don’t adjust the price.</p><h5 id="a-receipt-is-immutable">A “Receipt” is immutable</h5><p>A receipt is created at a specific point in time and it reflects the details at that point in time. It may or may not be printed.</p><p>That’s a very important distinction because even if you didn’t print out orders onto paper you wouldn’t want to modify old orders with different details based on future events.</p><p>For example, if a pie costs $16.95 right now and you have 1,000 orders that happened in the past at that price and then bump your pie to $17.95 you can’t go back and adjust all of those previous orders to have $17.95.</p><p>No way right? The payment was already collected for $16.95 and it’s a done deal.</p><p>That means you shouldn’t have something like a reference to a product’s foreign key to dynamically grab the price when you render the receipt. Just about everything about that receipt should be denormalized, AKA. the details about the receipt are all self contained in that 1 row. It should be an individually rendered thing with its own separate data.</p><h3 id="placing-orders-in-multiple-ways">Placing Orders in Multiple Ways</h3><p>In the above example we handled the use case of a single counter worker entering in an order through the POS system but that’s not the only way orders can be collected.</p><p>If your POS system is integrated with a website or app, customers can place orders online. They still need to come up into your system as “In Progress”. In this case the order isn’t “Completed” yet but the payment was captured.</p><p>Online orders still get receipts hung because the hanging receipt is an indicator to let workers know if a customer received their order. It’s fully independent of a payment being captured or not.</p><p>Additionally, orders can be placed through DoorDash which may or may not even integrate with your POS system. If not, you might get the DoorDash order through a different system but the idea of hanging a receipt while incomplete can still hold true.</p><p>Lastly if we go back to someone taking an order at the pizza shop there could technically be multiple POS systems and multiple terminals to accept credit card payments.</p><p>Each POS system should sync its orders and statuses to each other otherwise you may have conflicting information which will get very confusing. Other than that, it’s the same as our original plan of taking orders at the register. The hanging receipt workflow holds true.</p><h4 id="technical-takeaways-1">Technical Takeaways</h4><p>In the DoorDash case, it’s an interesting idea in that 2 different systems or inputs can still funnel into the same workflow for ensuring an order is fulfilled. In the DoorDash case all orders are already paid for, you’re only keeping track when someone will pick it up. Them picking it up and marking the order as picked up on their phone is the action to complete the order on our end.</p><p>That opens the door for a POS system to integrate with third party order collection systems such as DoorDash. There is a benefit of your POS system being aware of DoorDash so orders get shown in the same place.</p><p>At the place I worked at they were separate. It meant keeping track of 2 different iPad screens and the DoorDash system didn’t even have a printer so we had to write orders out by hand to hang them. Hooking up a receipt printer for DD is a short term TODO item for them.</p><p>The real takeaway here is thinking about separation of concerns, abstractions and the flexibility that’s available when you have decoupled systems. Having DD integration would be a nice quality of live improvement but it’s not a necessity.</p><p>The pizza maker on the back-end doesn’t even need to think about this detail. They just know they have to make a specific type of pizza before X time.</p><h3 id="multiple-payments-per-order">Multiple Payments Per Order</h3><p>Most orders will be either cash or credit on their own but technically someone can pay with both cash and credit. This happened on the first day I was there where someone had a $14 order but they paid with both a gift card and the rest was cash.</p><p>Separate to that you may need to collect additional payments after a transaction has been completed. For example, someone places an order online for a regular pie with a custom note that says “Add buffalo chicken topping”.</p><p>Some folks do this on purpose in hopes to get a $5.50 topping for free. They should have chosen that as a topping through the menu system and it would have been calculated into the total before they paid.</p><p>Others make honest mistakes. Perhaps they’re not used to using online menus and didn’t notice the toppings section or the POS system’s software for online ordering isn’t ideal and didn’t make this noticeable enough.</p><p>In cases like this we’ll go back into the order and then charge an extra $5.50 onto their card. The POS allows you to add $X to an order as long as it’s under a certain amount (I think it’s based on a percent). In almost all cases this can be done without contacting the customer.</p><p>95%+ of the time this is fine and it’s what the customer wants. We always let them know when they pick it up or deliver it that we made the adjustment and it’s on the receipt. They are fully aware of the charge when they receive their food.</p><p>In the very off chance they don’t want the extra charge for the topping they ordered we refund them for just the topping’s charge and either give them a regular pie or let them keep the topping for free. It really depends on the situation.</p><p>Most business owners are ok with making you happy but if you try to take advantage of the situation and do this all the time they will notice and if you continue to abuse the system they may choose not to do business with you anymore.</p><h4 id="technical-takeaways-2">Technical Takeaways</h4><p>A way to describe all of this is an order has many payments. When displaying orders or showing receipts make sure that each payment method has its own line item.</p><h3 id="extra-charges-tips-and-discounts">Extra Charges, Tips and Discounts</h3><p>Besides collecting payments after the initial transaction you may want to add additional charges before a transaction. This is done by clicking an “Extra charge” link near the total and then adding in the amount.</p><p>For example, there’s a $2 delivery charge that gets added to all orders marked as delivery.</p><p>There’s also scenarios where someone may request a custom order that doesn’t have a comparable menu item. Sometimes it makes sense to ring someone up for the closest menu item and then add an extra charge.</p><p>For example, an order of meatballs comes with (2) meatballs and there’s no way to order (1) meatball, so if someone wants (3) meatballs we may ring them up as (2) but then add an extra charge.</p><p>If they have a truly custom order the POS system also has an option to add an ADHOC item with a custom name, quantity and price. This isn’t an extra charge, it’s basically creating a 1 off menu item just for that 1 order.</p><p>Tips are basically extra charges with their own name. It’s important to be able to see “Tip” on the receipt in the expected spot. The credit card terminal allows someone to enter in no tip, a few common percents or a custom amount. There’s also a jar to collect cash tips. I was surprised at how many folks add tips even if they pick the order up themselves.</p><p>There’s also a way to reduce an order’s total with a discount. That’s done by clicking an “Add discount” link. You can choose to apply a fixed amount or percent off discount.</p><p>This may be through the customer having a coupon such as $3 off any pie or maybe the business owner gives a 5% discount on a large bulk order and also waives the delivery fee.</p><h4 id="technical-takeaways-3">Technical Takeaways</h4><p>It’s worth it to support both fixed and percent based discounts. I saw a number of both being applied in a short while and I do the same with courses. In the end a percent based discount applies a fixed amount discount so it’s not too bad to implement.</p><p>It should be easy to create custom items with a name and price for when the situation comes up. If a big order comes in, you don’t want to be struggling to figure out how to do it on the spot. If you’re sure it will happen, IMO it’s worth coding that feature up.</p><p>Likewise with courses, this comes up when organizations want to buy a team license. I ended up creating a custom package for the course for let’s say 25 licenses which goes through the normal checkout process. I have a couple of preset license amounts but I can add a custom amount too.</p><h3 id="voids-and-refunds">Voids and Refunds</h3><p>Once in a while you may create an order that hasn’t received a payment yet and you want to cancel it. This could happen if let’s say you’ve made a menu adjustment on the back-end of the POS system and you want to ensure the price is up to date on the front-end.</p><p>Their specific POS system will auto-create an order as soon as you select any item. Personally I don’t like this but that’s how it is. It makes training more difficult because there’s no training mode and you have to create an order.</p><p>In those cases you can cancel or void the order. It still shows up as an order, except it’s been canceled with no payment. This audit trail is useful to help business owners protect themselves from employees ringing up orders, canceling them and then pocketing the cash.</p><p>Both partial and full refunds can happen on any order that has a payment. Sometimes you may need to refund someone for 1 specific item or the whole order. In our case we can also add a note to the refund so we and the customer know why.</p><p>All of this gets reflected back onto a new receipt that gets printed out.</p><h4 id="technical-takeaways-4">Technical Takeaways</h4><p>Generally speaking when dealing with money having more information is better than less. You want an audit trail covering every event. You should be able to trace through your events to recreate an order backwards and forwards.</p><p>Stripe does a very good job with this if you’ve ever used them before. You usually have multiple events being tied into a specific payment. The time lines all match up and the event data (JSON) is locked in at the time it occurred.</p><h3 id="customers">Customers</h3><p>It’s useful to collect information such as at least the first name of someone who is picking up an order so it can be added to the receipt.</p><p>Having their phone number is handy too in case you need to call them to remind them to pick up their order or if something unexpected happens.</p><p>This is especially important for deliveries. For example, you might be delivering an order to an airport but the airport has 5 different terminals. Maybe the person who took the order over the phone didn’t get the details on where to go.</p><p>In this case the delivery driver can call the customer and arrange where to meet.</p><p>At this place they have a separate caller ID box that gets the name, number and address of the caller. It’s pretty handy. It’s not always correct or available but when it works it saves a good amount of time.</p><p>One thing I learned is it’s hard to hear things when folks have really bad phone habits and the store is busy. Also, trying to get a full address from someone who might be drunk is no easy task! Automating as much of this as possible helps.</p><p>Customer information gets saved too, this way if the same number is input again, the address is auto-filled out. Of course with mobile phones it’s possible that an address can change so we always ask them to confirm their address but again, 95%+ of the time this is a nice time saver.</p><p>Besides that, it helps business owners learn more about who likes what. If you know someone always orders similar pizzas every week then you can ask them “Hi Terry, do you want it lightly cooked like last time?”. This builds a nice rapport.</p><h4 id="technical-takeaways-5">Technical Takeaways</h4><p>Automating data input when possible is good but if it’s important you should always confirm it with another human on the other side. In this case a delivery to the wrong address is a really poor outcome.</p><p>For online orders outside of this pizza place, things like pre-filling a user’s state and zip code from a geo-location service is nice but allow the user to edit this information too. Especially if it’s for shipping an order.</p><h3 id="stripe-shopify-and-square">Stripe, Shopify and Square</h3><p>If you squint hard enough at this post related to “Order”, “Product” and “Customer” objects, this isn’t too different than <a href="https://stripe.com/docs/api" rel="noopener" target="_blank">Stripe</a>, <a href="https://shopify.dev/docs/api" rel="noopener" target="_blank">Shopify</a> and <a href="https://developer.squareup.com/reference/square" rel="noopener" target="_blank">Square’s</a> APIs to accept payments for digital and physical goods.</p><p>Of course, this post only scratches the surface of what to think about. I wrote this post because I was excited about what I learned briefly working at a pizza place. I love thinking about optimizing workflows and self improvement.</p><p>I’m literally the definition of the “will work for pizza” meme but in a good way.</p><p>Seriously though, it really opened my eyes at how important a good user experience is, especially under pressure at a decently busy place. Every action should have a purpose and if there’s spots where you can forget something or you have to think about it, that’s a good opportunity to see how you can refine the workflow to make it seamless.</p><p>Personally if I were building a POS system I’d use their APIs as inspiration or even use one of them directly. Modern day POS systems can end up being basically an iPad mounted onto a register.</p><p>I can’t believe how much I learned about decomposing the problem of “taking orders” just by being in a physical location for a bit and getting into the thick of it. I highly encourage anyone to experience this for a bit at least once in their life.</p><p>So that’s about it in terms of payments. The video below goes into similar detail as this post with a bit more detail and personal asides.</p><h3 id="video">Video</h3><p> <iframe src="https://www.youtube.com/embed/YSUxLRlTzZ4??rel=0&amp;iv_load_policy=3&amp;modestbranding=1&amp;controls=0" allowfullscreen="allowfullscreen" frameborder="0"> </iframe></p><h4 id="timestamps">Timestamps</h4><ul><li>2:06 – Placing an order for pickup</li><li>9:48 – Technical takeaways for picking up an order</li><li>16:02 – Placing orders in multiple ways</li><li>18:31 – Technical takeaways for placing orders in different ways</li><li>20:37 – Multiple payments per order</li><li>25:09 – Voids and refunds</li><li>27:38 – Technical takeaways for refunds</li><li>28:44 – Customers</li><li>31:27 – Technical takeaways about customers</li><li>32:02 – Using Stripe, Shopify and Square’s API as inspiration</li><li>33:38 – Will work for pizza</li></ul><p><strong>What are your payment related takeaways when working at a register? Let us know!</strong></p></article><p>Like you, I'm super protective of my inbox, so don't worry about getting spammed. You can expect a few emails per month (at most), and you can 1-click unsubscribe at any time. <a href="https://nickjanetakis.com/newsletter">See what else you'll get</a> too.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Refusing to teach kids math will not improve equity (555 pts)]]></title>
            <link>https://www.noahpinion.blog/p/refusing-to-teach-kids-math-will</link>
            <guid>36774940</guid>
            <pubDate>Tue, 18 Jul 2023 16:18:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.noahpinion.blog/p/refusing-to-teach-kids-math-will">https://www.noahpinion.blog/p/refusing-to-teach-kids-math-will</a>, See on <a href="https://news.ycombinator.com/item?id=36774940">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg" width="1210" height="753" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:753,&quot;width&quot;:1210,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:233222,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27dc0037-15a4-4b31-849e-46b63d5a109e_1210x753.jpeg 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><em>“The result of the educative process is capacity for further education.” — John Dewey</em></p><p><span>A couple of weeks ago, Armand Domalewski wrote </span><a href="https://www.noahpinion.blog/p/california-needs-real-math-education" rel="">a guest post for Noahpinion</a><span> about how the new California Math Framework threatened to dumb down math education in the state — for example, by forbidding kids from taking algebra before high school:</span></p><div data-component-name="DigestPostEmbed"><a href="https://www.noahpinion.blog/p/california-needs-real-math-education" target="_blank" rel="noopener"><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg"><img src="https://substackcdn.com/image/fetch/w_140,h_140,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4431a65f-8a19-46b7-af11-ff6fd81b09b1_2000x810.jpeg" sizes="100vw" alt="California needs real math education, not gimmicks" width="140" height="140"></picture></div></a></div><p>Well, I’m going to write another post about this subject, because the direction in which math education is trending in America under “progressive” guidance just frustrates me so deeply. </p><p><span>A few days after Armand’s post, the new California Math Framework was adopted. Some of the worst provisions had been </span><a href="https://www.edweek.org/teaching-learning/california-adopts-controversial-new-math-framework-heres-whats-in-it/2023/07'" rel="">thankfully watered down</a><span>, but the basic strategy of trying to delay the teaching of subjects like algebra remained. It’s a sign that the so-called “progressive” approach to math education championed by people like Stanford’s Jo Boaler has not yet engendered a critical mass of pushback. </span></p><p><span>And meanwhile, the idea that teaching kids less math will create “equity” has spread far beyond the Golden State. The city of Cambridge, Massachusetts </span><a href="https://www.bostonglobe.com/2023/07/14/metro/cambridge-schools-divided-over-middle-school-math/" rel="">recently removed algebra</a><span> and all advanced math from its junior high schools, on similar “equity” grounds.</span></p><p><span>It is difficult to find words to describe how bad this idea is without descending into abject rudeness. The idea that offering children fewer educational resources through the public school system will help the poor kids catch up with rich ones, or help the Black kids catch up with the White and Asian ones, is unsupported by any available evidence of which I am aware. More fundamentally, though, it runs counter to </span><em>the whole reason that public schools exist in the first place</em><span>.</span></p><p><span>The idea behind universal public education is that all children — or almost all, making allowance for those with severe learning disabilities — are fundamentally </span><em>educable</em><span>. It is the idea that there is some set of subjects — reading, writing, basic mathematics, etc. — that essentially all children can learn, if sufficient resources are invested in teaching them. </span></p><p><span>Before public education — one of the key crusades of the original Progressive movement — only private actors invested resources in educating children. Families taught their kids skills, rich families hired tutors, companies trained their workers, churches provided some classes, etc. But it was a threadbare, uneven patchwork. Worse, it was highly </span><em>unequal</em><span> — if you weren’t born to a family with lots of time and/or money to spare, you didn’t get nearly as much education. This led to inequality throughout society, as well as the preservation of intergenerational wealth. Furthermore, it wasted much of society’s productive potential, because the unlucky kids weren’t learning as many useful skills as they could have been.</span></p><p><span>Universal public education was a way to attack all of these problems at once. By investing state resources in childhood education, it not only boosted human capital and economic growth, it eroded inequality of birth and circumstance. Teachers, hired by the state, provided some of what poor kids’ families couldn’t provide. Everyone except for a very few fringe ideologues now agrees that this model was a success; public education is pretty much universally believed to be </span><a href="https://gpseducation.oecd.org/revieweducationpolicies/#!node=41761&amp;filter=all" rel="">a key input</a><span> into economic development, and plenty of research supports that notion that it fosters </span><a href="https://cepr.org/voxeu/columns/invest-public-education-increase-intergenerational-mobility" rel="">intergenerational mobility</a><span> as well. It is not a perfect equalizer and never will be, but it is one of the more important equalizers that exist in our society. </span></p><p><span>When you ban or discourage the teaching of a subject like algebra in junior high schools, what you are doing is </span><em>withdrawing state resources</em><span> from public education. There is a thing you could be teaching kids how to do, but instead you are refusing to teach it. In what way is refusing to use state resources to teach children an important skill “progressive”? How would this further the goal of equity?</span></p><p>I can easily see one (very twisted) perspective that might lead someone to think it would do so. If you take a strongly “hereditarian” view toward education, then you believe that inborn mental ability — what we typically call “IQ” — determines most of what people can and can’t learn. If you are a strong hereditarian, then perhaps you believe that students who don’t learn algebra well are simply born without the cognitive ability to learn it. </p><p><span>And if you believe </span><em>that</em><span>, then you might conclude that the only way to create equity in society would be to handicap the kids who were born with the ability to learn algebra. Like the Handicapper General in Kurt Vonnegut’s satirical sci-fi short story “</span><a href="https://en.wikipedia.org/wiki/Harrison_Bergeron" rel="">Harrison Bergeron</a><span>”, you might try to kneecap the opportunities of the kids you believed to be the genetic elite. In doing so, you would then become a living, breathing embodiment of this meme that </span><em>opponents</em><span> of equity created in order to ridicule the whole idea:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg" width="960" height="576" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:576,&quot;width&quot;:960,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:181548,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4473ccca-46a0-4e82-88fa-6ec558b7ca8f_960x576.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><span>Now, if you went up and asked “progressive” educators like Jo Boaler if they believed in a version of hereditarianism so strong as to make </span><a href="https://twitter.com/charlesmurray/status/1680475688355217411" rel="">Charles Murray himself blush</a><span>, they would undoubtedly deny that they do. And I’m sure they don’t </span><em>consciously</em><span> think that math ability is all in the genes. But when you think about the idea of creating equity by restricting access to advanced math classes, it’s pretty much impossible to avoid the conclusion that the idea is to make all kids equal by making them equally unable to learn. </span></p><p><span>But whatever you think about the morality of this idea, it simply </span><em>will not work</em><span>. The reason is that </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6088505/" rel="">the strong hereditarian hypothesis is wrong</a><span>; practically all kids </span><em>are </em><span>educable, with the proper investment of resources. </span></p><p><span>I know this from personal experience. I was a math tutor for many years, in high school and college, and part of that was volunteer work tutoring poor Black and Hispanic kids. Guess what? They learned a lot of math! But this isn’t just my own anecdote; evidence </span><em>consistently shows</em><span> that tutoring is highly effective. </span><a href="https://www.nber.org/papers/w27476" rel="">Nickow, Oreopoulos, and Quan (2020)</a><span> surveyed 96 studies using a variety of different tutoring approaches in a variety of different contexts. Here’s </span><a href="https://www.brookings.edu/articles/tutoring-a-time-tested-solution-to-an-unprecedented-pandemic/" rel="">a summary of what they found</a><span>:</span></p><blockquote><p><span>We found that tutoring is remarkably effective at helping students learn, with over 80% of the 96 included studies reporting statistically significant effects. Averaging results across the studies included in this analysis, we found a pooled effect size of 0.37 standard deviations. In other words, with the help of tutoring, a student at the 50</span><sup>th</sup><span> percentile would improve to the 66</span><sup>th</sup><span> percentile. In the field of K-12 education research where there is little agreement on what works, these findings are remarkable not only for their magnitude but also for their consistency. The evidence is clear that tutoring can reliably help students catch up.</span></p></blockquote><p><span>The effectiveness of tutoring can help us understand achievement gaps in math classes. Some students do well because of greater </span><em>preparation</em><span> — they show up already knowing much of the material, or having the general concepts in place to pick it up quickly. This is because they had a tutor at home growing up: </span><em>their parents</em><span>, who had the leisure time and educational background required to teach their kids some math. A few rich parents also hire private tutors, but the main tutors are almost always Mom and Dad.</span></p><p>Now imagine what will happen if we ban kids from learning algebra in public junior high schools. The kids who have the most family resources — the rich kids, the kids with educated parents, etc. — will be able to use those resources to compensate for the retreat of the state. Either their parents will teach them algebra at home, or hire tutors, or even withdraw them to private schools. Meanwhile, the kids without family resources will be out of luck; since the state was the only actor who could have taught them algebra in junior high, there’s now simply no one to teach them. The rich kids will learn algebra and the poor kids will not. </p><p>That will not be an equitable outcome.</p><p>Taken to its logical extreme, the idea of restricting what can be taught in public schools — for whatever reason — leads us back to the pre-public-school era. It leads us to a world of private schools and home schooling. That’s not an era we should seek to return to. Nor am I being hyperbolic; Cambridge’s restriction of junior high algebra is effectively a small step toward that bygone era, since kids with resources will just learn algebra from their parents or be pulled out into private schools.</p><p><span>So what should we do instead? </span><a href="https://www.the74million.org/article/dallas-isds-opt-out-policy-dramatically-boosts-diversity-in-its-honors-classes/" rel="">Dallas came up with an answer</a><span>: Teach kids </span><em>more</em><span> math instead of teaching them less. In 2019, Dallas Independent School District implemented a new equity policy that encouraged many more people to take honors math classes:</span></p><blockquote><p>Many capable Hispanic, Black and English learner students did not elect to join these classes on their own or were passed over by their instructors. And their parents were often unaware they could make the request.&nbsp;</p><p>Dallas ISD, which serves some 142,000 children, took note of the disparity and in 2017 formed a racial equity advisory council — some of whose members had children in the district — with the goal of improving opportunity for all…It decided to move from an opt-in model to an opt-out policy in the 2019-20 school year. Since then…students cannot opt out [of advances classes] without written parent permission. The move has dramatically increased participation among traditionally marginalized children.</p></blockquote><p>The results were pretty incredible. First of all, many more students enrolled in honors math:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg" width="1200" height="720" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg&quot;,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:720,&quot;width&quot;:1200,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123922,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F755f0a25-2265-4f8b-891f-6ca4e6693148_1200x720.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Source: Dallas Independent School District via </span><a href="https://www.the74million.org/article/dallas-isds-opt-out-policy-dramatically-boosts-diversity-in-its-honors-classes/" rel="">The74</a></figcaption></figure></div><p>Before the change, three times as many White students as Black students enrolled in honors math; after the change, it was less than twice as many. Not perfect equity, but progress in that direction.</p><p><span>But did all these new students actually </span><em>learn</em><span> the honors math? Yes, indeed they did:</span></p><blockquote><p><span>And the policy has not led to a decrease in student scores as some speculated: Last year’s 8</span><sup>th</sup><span>-grade Algebra I students had similar pass rates as those in years prior, the district said, with 95% of Hispanic students passing the test and 76% meeting grade-level proficiency; 91% of Black students passing and 65% meeting grade level and 95% of English learner students passing the state exam and 74% meeting grade level.&nbsp;</span></p></blockquote><p><span>Guess what? </span><em>Children are educable</em><span>. If you invest the resources of the state in poor kids and underrepresented minorities, </span><em>they will learn</em><span>. The true path toward equity is to have the state teach </span><em>more</em><span>, not less. Instead of trying to prevent the well-prepared kids from learning algebra, invest more in teaching algebra to the disadvantaged kids!</span></p><p>How did we end up in a world where “progressive” places like California and Cambridge, Massachusetts believe in teaching children less math via the public school system, while a city in Texas believes in and invests in its disadvantaged kids? What combination of performativity, laziness, and tacit disbelief in human potential made the degradation of public education a “progressive” cause célèbre? I cannot answer this question; all I know is that the “teach less math” approach will work against the cause of equity, while also weakening the human capital of the American workforce in the process. </p><p>We created public schools for a reason, and that reason still makes sense. Teach the kids math. They can learn.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.noahpinion.blog/p/refusing-to-teach-kids-math-will?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://www.noahpinion.blog/p/refusing-to-teach-kids-math-will?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Llama 2 – Meta AI (1513 pts)]]></title>
            <link>https://ai.meta.com/llama/</link>
            <guid>36774627</guid>
            <pubDate>Tue, 18 Jul 2023 16:01:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/llama/">https://ai.meta.com/llama/</a>, See on <a href="https://news.ycombinator.com/item?id=36774627">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><div><p>Introducing Llama 2</p><p>The next generation of our <br>open source large language model</p></div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt=""></p></div><hr><div><p>This release includes model weights and starting code for pretrained and fine-tuned Llama language models — ranging from 7B to 70B parameters.</p></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/361590483_235309902165028_8558564526447568738_n.jpg?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=H4QmTUInwY8AX-aw7zF&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfBUXb9SphNJVjfXZqk4TLuDX56Dv5hY7SlYL3717dDiVA&amp;oe=64BB360F" alt="" id="u_0_a_In"></p><div><p>Llama 2 pretrained models are trained on 2 trillion tokens, and have double the context length than Llama 1. Its fine-tuned models have been trained on over 1 million human annotations.</p></div></div><div><p>Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.</p></div><div><p><img src="https://scontent.fzrh3-1.fna.fbcdn.net/v/t39.8562-6/361265668_276217774995411_4529778090866658620_n.jpg?_nc_cat=104&amp;ccb=1-7&amp;_nc_sid=6825c5&amp;_nc_ohc=gSMV6flCjbAAX8EYsIv&amp;_nc_ht=scontent.fzrh3-1.fna&amp;oh=00_AfAOByNIrPkyufRv5V0AFHlgDxiWQ-xWsAqgMkaI-aHZtQ&amp;oe=64BC6826" alt="" id="u_0_b_35"></p></div><hr><div><p>Llama 2 was pretrained on publicly available online data sources. The fine-tuned model, Llama-2-chat, leverages publicly available instruction datasets and over 1 million human annotations.</p><a href="https://ai.meta.com/resources/models-and-libraries/llama/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal-link&quot;,&quot;creative_detail&quot;:&quot;llama_landing_cta_tech_details&quot;}" target="_self"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#1C2B33"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#1C2B33"></path><path d="M21.9657 12L28.9287 18.963L21.9657 25.926L20.7348 24.7193L25.6203 19.8334L10.0001 19.8334V18.0926L25.5966 18.0926L20.7348 13.2309L21.9657 12Z" fill="#1C2B33"></path></svg>Technical details</a></div><hr><div><h5>Partnerships</h5><h2>Our global partners and supporters</h2></div><div><p>We have a broad range of supporters around the world who believe in our open approach to today’s AI — companies that have given early feedback and are excited to build with Llama 2, cloud providers that will include the model as part of their offering to customers, researchers committed to doing research with the model, and people across tech, academia, and policy who see the benefits of Llama and an open platform as we do.</p></div><div><p>Statement of support for Meta’s open approach to today’s AI</p><p><i>“We support an open innovation approach to AI. Responsible and open innovation gives us all a stake in the AI development process, bringing visibility, scrutiny and trust to these technologies. Opening today’s Llama models will let everyone benefit from this technology.”</i></p><a href="https://about.fb.com/news/2023/07/llama-2-statement-of-support" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_d_Ze"><svg viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg"><path opacity="0.4" fill-rule="evenodd" clip-rule="evenodd" d="M19 37C9.05887 37 1 28.9411 1 19C1 9.05887 9.05887 1 19 1C28.9411 1 37 9.05887 37 19C37 28.9411 28.9411 37 19 37Z" stroke="#1C2B33"></path><path d="M16.3102 12.2707L26.1574 12.2707L26.1574 22.1179L24.4338 22.135L24.4335 15.2256L13.3883 26.2707L12.1574 25.0398L23.1858 14.0114L16.3102 14.0115L16.3102 12.2707Z" fill="#1C2B33"></path><path d="M16.3102 12.2707L26.1574 12.2707L26.1574 22.1179L24.4338 22.135L24.4335 15.2256L13.3883 26.2707L12.1574 25.0398L23.1858 14.0114L16.3102 14.0115L16.3102 12.2707Z" fill="#1C2B33"></path></svg>See the complete list of signatories</a></div><div><div><p>Download the Model</p><p>Get Llama 2 now: complete the download form via the link below. By submitting the form, you agree to Meta's <a href="https://www.facebook.com/privacy/policy/?entry_point=data_policy_redirect&amp;entry=0" target="_blank">privacy policy</a>.</p><p><span id="resources"></span><a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_internal&quot;}" target="_self">Get Started</a></p></div><p><img src="https://static.xx.fbcdn.net/rsrc.php/v3/y4/r/-PAXP-deijE.gif" alt=""></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Highlight.io (YC W23) – Open-source, full stack web app monitoring (164 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36774611</link>
            <guid>36774611</guid>
            <pubDate>Tue, 18 Jul 2023 16:01:08 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36774611">Hacker News</a></p>
Couldn't get https://news.ycombinator.com/item?id=36774611: Error: Request failed with status code 502]]></description>
        </item>
        <item>
            <title><![CDATA[The office is a theatre for work (2019) (280 pts)]]></title>
            <link>https://tomcritchlow.com/2019/11/18/yes-and/</link>
            <guid>36773554</guid>
            <pubDate>Tue, 18 Jul 2023 15:01:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tomcritchlow.com/2019/11/18/yes-and/">https://tomcritchlow.com/2019/11/18/yes-and/</a>, See on <a href="https://news.ycombinator.com/item?id=36773554">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  


  <p>I recently read the book <a href="https://www.amazon.com/Impro-Improvisation-Theatre-Keith-Johnstone/dp/0878301178">Impro - Improvisation and the Theatre</a> by Keith Johnstone<sup id="fnref:vgr" role="doc-noteref"><a href="#fn:vgr" rel="footnote">1</a></sup>. It’s a delightful book all about improvisational theatre and importantly <em>how to teach</em> improvisational theatre.</p>

<p>The book inspired me to draw many analogies between the improv actor and the consultant and I have written a five part series. They loosely flow in order but each can be read in any order:</p>

<p><strong>Chapter 1 - The Office is a Theatre for Work</strong>. This post looks at the central problem of “performing” work and how important it is for modern knowledge work - especially for the consultant. We end with some ideas around how to think on your feet without bullshitting.</p>

<p><strong><a href="https://tomcritchlow.com/2019/11/19/optimism-operating-system/">Chapter 2 - Optimism as an Operating System</a></strong>. This post highlights the tendency for consultants to be critical and to see everything as a problem. We’ll reframe this by showing the power of being positive - especially for long-term retainers.</p>

<p><strong><a href="https://tomcritchlow.com/2019/12/06/blocking-unblocking-clients/">Chapter 3 - Blocking &amp; Unblocking Clients</a></strong>. Here we think of clients as blocked actors and we take inspiration from the “Yes! and…” exercise to see how we can unblock clients to generate new strategies and creative thinking with clients.</p>

<p><strong><a href="https://tomcritchlow.com/2020/06/24/navigating-power-status/">Chapter 4 - Navigating Power &amp; Status</a></strong>. How to get things done inside organizations by understanding power potholes and status switching. We explore the concept of high status and low status and show how the consultant has to become adept at a new skill of “status switching” in order to be successful inside client organizations.</p>

<p><strong><a href="https://tomcritchlow.com/2020/08/18/the-fool/">Chapter 5 - The Contrary Consultant</a></strong>. Embracing the fool &amp; the power of not fitting in. We explore the identity of outsider and how to reconcile this with working inside client organizations.</p>

<h2 id="chapter-1---the-office-is-a-theatre-for-work">Chapter 1 - The Office is a Theatre for Work</h2>

<p>Which of these do you recognize?</p>

<p><em>Working on a detailed presentation only for the meeting to get derailed 5 slides in.</em></p>

<p><em>Ritually showing our face at our desks at the appropriate time to signal that we’re working.</em></p>

<p><em>Attending weekly status meetings to create the performance of keeping things moving.</em></p>

<p><em>Brainstorm sessions to create the illusion of inclusive creativity.</em></p>

<p><em>Workshops with scripted games and exercises and sticky notes to ensure everyone has a good time.</em></p>

<p><em>Jealousy of colleagues who don’t do good work but excel at presenting their work.</em></p>

<p><em>People who talk about their work getting promoted more than those that don’t.</em></p>

<p>Much of modern knowledge work is performance… In fact:</p>

<p><strong>The office is a theatre,  and work is an unfolding narrative on the stage.</strong></p>

<p>Many people aspire to “silent success” at work - to do work that “speaks for itself”. Unfortunately this is the wrong move in the theatre of work. Instead we should aspire to the opposite - <strong>for knowledge work, the performance of the work <em>is</em> the work.</strong></p>

<p>Because in truth - how else could it function?</p>

<p>Much as we might like to think of organizations as rational machines - the reality is that companies are social organizations and people interacting with people is the way decisions are made and how work gets done.</p>

<p>And in this theatre of human work it’s crucial to speak up. Spending time on the performance is not wasted - in fact quite the opposite.</p>

<p>Without performance work gets sidelined, ignored or worse:</p>

<blockquote>
  <p>Among more than 120 evaluation and program executives surveyed at private foundations the US and Canada, more than three-quarters had difficulty commissioning evaluations that result in meaningful insights for the field, their grantees, or the foundation itself, and 70% have found it challenging to incorporate evaluation results into the foundation’s future work. <strong>A survey of over 1600 civil servants in Pakistan and India found that “simply presenting evidence to policymakers doesn’t necessarily improve their decision-making,” with respondents indicating “that they had to make decisions too quickly to consult evidence and that they weren’t rewarded when they did.”</strong> - <a href="https://medium.com/@iandavidmoss/this-is-why-your-hard-work-sits-on-the-shelf-and-heres-what-to-do-about-it-a7ca5e063038">why your hard work sits on the shelf and what to do about it</a></p>
</blockquote>

<p>And in fact <a href="https://twitter.com/round/status/892486242721554433">this excellent Twitter thread from Maxim</a> shows the value in caring deeply about the presentation and context of the work:</p>

<blockquote><p lang="en" dir="ltr">Nobody ever picks the third, wacky, design direction.</p>— Maxim Leyzerovich (@round) <a href="https://twitter.com/round/status/892486242721554433?ref_src=twsrc%5Etfw">August 1, 2017</a></blockquote>


<p>What does the “performance” of work look like?</p>

<p>Things like:</p>
<ul>
  <li>Presentations of your work</li>
  <li>Hallway chats</li>
  <li>1:1 conversations to build alignment</li>
  <li>Crafting the wrapper and positioning of your work</li>
  <li>Changing the language of your work to match others</li>
  <li>Including wider contexts of the organization, the industry and the market in your work</li>
  <li>Reformatting your presentations to fit other team’s strategies</li>
  <li>and much more…</li>
</ul>

<p><strong>In the theatre of work the performance of work is intimately tied to the work itself.</strong></p>

<p>But many employees attempt to hide or ignore the performance of work and the politics of the organization. They imagine that this public sphere of voice and politics is wasted energy or somehow “unfair”<sup id="fnref:unfair" role="doc-noteref"><a href="#fn:unfair" rel="footnote">2</a></sup>.</p>

<h2 id="being-present-public-and-acting-with-voice">Being present, public and acting with voice</h2>

<p>If we think of the office as a theatre for work the <em>only</em> meaningful way forward is through this sphere of politics and voice. As Venkatesh Rao from Ribbonfarm outlines in his post <a href="https://www.ribbonfarm.com/2017/09/14/how-to-make-history/">how to make history</a>:</p>

<blockquote>
  <p><strong>You do not appear in public through labor, let alone make history. Laboring humans are fungible as individuals, and only consequential actors with a political voice en masse</strong> (whether organized in egalitarian ways as a working class or non-egalitarian ways as a patriarchy, or ethno-nationalist clientelistic identity group). […]</p>

  <p><strong>If labor is about blending into the processes of nature, and making about interrupting and slowing it to create a durable world, action is about free behaviors that make history.</strong></p>
</blockquote>

<p>This ideas of “being public” and “exercising voice” both have relatively specific meanings in the Hannah Arendt sense and I’d encourage you to spend time reading here if you’re not familiar<sup id="fnref:arendt" role="doc-noteref"><a href="#fn:arendt" rel="footnote">3</a></sup>.</p>

<p>Here are two examples that might help explain opportunities for being public and having voice:</p>

<p>Example #1: choosing to take a position on something that situates <strong>you</strong> in the company’s world. A great example of this is <a href="https://gist.github.com/chitchcock/1281611">Steve Yegge’s platform rant</a>. Note how closely Steve is situated in the narrative here - it’s not simply an observation of the market but of taking a position on the world that situates Google, Amazon and Steve. This can feel daunting but ask yourself the last time you showed up in Slack to post something that has voice? To say something about the organization…</p>

<p>Example #2: asking questions in public forums (all-hands, quarterly business updates etc). These are opportunities where executives are inviting public voice - they’re asking for people to take positions and have opinions.</p>

<p>Of course there are an infinite number of other situations where voice and being public are possible<sup id="fnref:speech" role="doc-noteref"><a href="#fn:speech" rel="footnote">4</a></sup>… but most employees would rather shy away from these instead of embracing them as part of the work.</p>

<p>Orchestrating performances for your work is the key to more influence and more impact.</p>

<h2 id="enter-stage-left-the-consultant">Enter Stage Left: The Consultant</h2>

<p>Ah, but so far what we’re describing is the reality of the working world. Now let’s situate the consultant in the theatre of work.</p>

<p>The luxury that full time employees gain is the ability to script and rehearse their performances - to plan ahead. The consultant is like an improv actor thrust into a play mid-performance, forced to find ways to fit in, go with the flow and steer the performance all while <em>on stage</em>.</p>

<p>For example - full time employees get the luxury of being involved in 2020 planning. <strong>Consultants are an outcome of 2020 planning</strong> and so get brought in mid-performance to course correct.</p>

<p>Consultants that attempt to halt the performance cause pain and typically don’t stick around. Instead consultants that accept and embrace their nature as improv actors thrive and become integrated into the great performance of the organization.</p>

<h2 id="the-theatre-of-work-is-an-improv-theatre">The Theatre of Work is an Improv Theatre</h2>

<p>As a consultant, attempting to tightly script and design the performance of the work will inevitably lead to <strong>missed timings, missed context or missed feedback</strong>.</p>

<p><strong>Missed timings</strong> - As a consultant you’re often working less than 5-days a week with a client. So you’re slower than full-time employees. This means if you try and slow down to polish the performance of your work you can miss the window of opportunity. So the improv consultant must learn to un-polish, prototype and improvise in real-time just to keep up with the client’s organization.</p>

<p><strong>Missed context</strong> - Because you’re not a full-time employee (even if you’re working 5 days a week) you may not be included on all-hands emails, announcements and so on and so you always have to work hard to gain the full context of a client. <strong>Tightly scripting a performance doesn’t leave room for new contexts to emerge during the performance</strong>. Instead there should always be room for new context to emerge and get integrated into the performance in real-time.</p>

<p><strong>Missed feedback</strong> - <strong>It’s not uncommon as a consultant to be the most proficient powerpoint user in the org</strong> (or at least your portion of the org). This has benefits but it also has the unintended consequence of making everything you touch look “finished”. And finished work gets very different feedback from people than raw materials and thinking. So sometimes it’s important to un-design and un-polish your work, to invite people onto the stage to co-create the performance - this way you ensure that you get the appropriate feedback.</p>

<p><strong>Good consultants think of performances more like an unfolding series of improv sessions.</strong></p>

<p>Many organizations at the executive level already function like tight improv groups - reacting to implicit and explicit cues and playing off each other to make in-the-moment decisions.</p>

<p>As a consultant - this executive level improv troupe is where the real work gets done and where you’re aiming for. It’s the essence of why Venkatesh’s idea of client “sparring”<sup id="fnref:sparring" role="doc-noteref"><a href="#fn:sparring" rel="footnote">5</a></sup> is so appealing - because it embraces this verbal improv decision making that everyone recognizes.</p>

<p>So how does this idea of improv begin to shape our thinking? Some obvious ideas:</p>

<ul>
  <li>We need to be present in the room. Where the performance of the work <em>is</em> the work we can’t leave the presentation and narrative of the work to others.</li>
  <li>Study the people as much as you study the organization. When you’re reacting to work mid-performance half of what you’re reacting to is the strategy and half of it is the set of relationships and biases of the people in the room.</li>
</ul>

<p>And one non-obvious idea:</p>

<h2 id="the-serendipity-deficit-of-consultants--manufacturing-improv-sessions">The Serendipity-Deficit of Consultants &amp; Manufacturing Improv Sessions</h2>

<p>A failure mode for employees is striving for work to happen in the official channels - to wait for the meeting to talk about the strategy, to wait for the email chain to pitch in. This desire to make work “official” means many employees are uncomfortable talking about their work on the way to the coffee shop or in the hallway between meetings.</p>

<p>But it’s exactly these free-form sessions when executives drop their guard, open up and the possibility to operate in a liminal space between contexts is possible.</p>

<p><strong>And the most important thing to understand about these “spontaneous” improv sessions is that they are most useful for laying the first seed of an idea. Good ideas don’t instantly make an impact and if you want executives to pay attention you have to make sure they hear the idea several times. So drop it first in the hallway before formalizing it in a meeting.</strong></p>

<p>Of course I say “spontaneous” because actually humans are remarkably predictable and it’s often trivial to engineer these serendipitous hallway conversations. And you <em>should</em> be manufacturing them.</p>

<p>Remember - if you’re a consultant in a client’s office two days a week you’re operating at a serendipity-deficit. You simply have less surface area to bump into people. So you have to cheat.</p>

<p>A fellow consultant friend brought up the idea of getting into the office early to deliberately catch the CEO on the way in the door before they settled into focus mode. To intercept them intentionally to create a small improv session. To lay the first seeds for ideas.</p>

<h2 id="how-to-think-on-your-feet-without-bullshitting">How to think on your feet without bullshitting</h2>

<p>Above all else however, you need to embrace a level of “thinking on your feet” - it’s essential to the consultant’s work and a consequence of being brought in mid-performance.</p>

<p>This “thinking on your feet” is about the balance between deflecting decisions for further analysis and providing the answer there and then.</p>

<p>Example: one of the most visible ways this manifests is the first day or week with a new client. Executives love to probe you with “difficult” questions - learning to provide an answer that you believe in but leaves room for revision later is key. The real game that’s being played here is not one of being right or wrong - it’s the executive asking two questions at once - firstly “how much do you know?” and secondly “can you improv?” to understand how useful you’re going to be in the theatre of work.</p>

<p><strong>Unfortunately -  in the theatre of work there’s a fine line between thinking on your feet and bullshitting…</strong></p>

<p>Who’s been in a meeting and been disgusted with people spouting things that are half-true, made-up or masks over the real truth?</p>

<p>There’s a fine line between reacting to a situation in the room and bullshitting.</p>

<p>As a consultant this is especially hard to avoid. Your default mode of operating is the liminal space between industries, businesses and markets. A few times a year I’m forced to learn something new from scratch. This forces us to work in spaces where we’re often the <em>least</em> knowledgeable about a specific business (even if we are <em>experts</em> in the industry… And sometimes we’re <em>experts</em> at a discipline but neither knowledgeable about the <em>business</em> or the <em>industry</em>).</p>

<p>So here’s a little guide to avoiding bullshit:</p>

<ul>
  <li>Immerse yourself in the core business mechanics, you should be able to draw a diagram explaining the core business revenue &amp; profit function reasonably well. The more abstraction here the more you risk a fundamental mis-understanding and straying into BS.</li>
  <li>Become a language chameleon - study and adopt the language,  acronyms and buzzwords of the client’s business. If they call it “earned marketing” you should too. If their CMS is called PinkCloud you call it that too. Specificity allows you to avoid confusion and helps you distinguish between the client’s CMS and the market’s CMS (for example).</li>
  <li>Speak clearly and within your limits. Ask for clarification on points that don’t seem to make sense. No pretending. Don’t adopt the language too quickly! (ha, see how hard this is?)</li>
  <li>Ask stupid questions. Ask questions about company history, about alternative approaches, about failed previous ideas. Just because people at the client choose not to talk about obvious things doesn’t mean you shouldn’t.</li>
  <li>Defend your ideas but not your points. Be willing to defend your ideas even if some data points get challenged. It’s very common that the number or stat you’re using to support your point will be wrong but that the broader idea still holds. Don’t concede the idea but also don’t try and put muscle behind the numbers - accept they’re wrong but keep pressing on the idea to see where you get.</li>
  <li>When you get challenged pull out some counter-factuals - when someone is claiming that your data is wrong challenge them with proving the opposite.</li>
  <li>Keep your eye on the prize for business outcomes, not intellectual debates. Executives are fond of theorizing and debating ideas that stray into the get into the abstract - you can play this game a little but try to be the one to ground conversations in reality.</li>
  <li>Read widely - analogy is the core of cognition. Don’t be afraid to source things from different industries. This is the kind of cross-industry vantage point that clients find hard to get internally.</li>
</ul>

<p>These are all ways to avoid bullshitting - but unfortunately there’s one simple way to avoid bullshit - by being critical.</p>

<p><strong>It’s far easier to retreat to the critical, negative position and say why things won’t or can’t work. Except… this is a mistake.</strong></p>

<p>Being positive and optimistic is far harder but more effective. And we’re going to talk about that in the next post…. <a href="https://tomcritchlow.com/2019/11/19/optimism-operating-system/">Optimism as an Operating System</a>.</p>



  
    
    






<hr>














</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How to write a great README (135 pts)]]></title>
            <link>https://www.appsmith.com/blog/write-a-great-readme</link>
            <guid>36773022</guid>
            <pubDate>Tue, 18 Jul 2023 14:34:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.appsmith.com/blog/write-a-great-readme">https://www.appsmith.com/blog/write-a-great-readme</a>, See on <a href="https://news.ycombinator.com/item?id=36773022">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>As a company, we have aspired to build an amazing open-source community for <a target="_blank" rel="noopener noreferrer" href="https://docs.appsmith.com/">Appsmith</a>. To date, Appsmith's <a target="_blank" rel="noopener noreferrer" href="https://github.com/appsmithorg/appsmith">Github page</a> has amassed over 28,000 stars and just under 3,000 forks. So what is the key to getting these numbers? It all starts with the all-important README. Your README is the front page for your project — it's often the first impression you make on your users and contributors. An effective README file needs to tell your audience what your project does, how to use it, and how they can help out.</p>
<p>We’ve optimized this by putting the information that’s most useful to the widest audience up front and slowly working our way down to the technical details, by linking to other sources whenever more detail is required to avoid walls of text, and by using visuals wherever possible. In this article, we’ll discuss in more detail how these three principles make a great README file for your project.</p>
<h2 id="principles-of-a-good-readme">Principles of a good README</h2>
<p>There’s no shortage of projects to contribute to on GitHub, which means that it’s easy for potential contributors to overlook projects that aren’t yet very popular, even if they are great projects. This is a tough pill to swallow for many project maintainers who want to believe that their project will succeed just because it is high quality.</p>
<p>In order to make sure your project doesn’t fade into obscurity, it is critical to communicate the value of your project as soon as possible without requiring a ton of effort from potential contributors. The best way to do this is with a clear and concise README that not only has all the important information but looks good and is easy to read. In this article, we recommend a few key principles for accomplishing this and explain how we’ve applied them with the Appsmith README.</p>
<p>When you read something, you start at the top, so start your README file with the most useful information and then gradually work your way down to the technical details. This prevents readers from becoming overwhelmed before they have enough context to understand the inner workings of your project.</p>
<p>It’s also important to recognize what <em>not</em> to include. README files work best when they are succinct and link out to formal documentation whenever in-depth detail is required. The alternative is to have walls of text all over, which can be very off-putting to readers.</p>
<p>We look at the README as the central hub for all the information that users need to understand a project. It should provide a high-level overview of the project and also show users where to go to find more details, including how it works, links to up-to-date installation options, and finally links to detailed information for contributors.</p>
<p>We also think it is beneficial to <em>show</em> instead of <em>tell</em> wherever possible. We recommend using icons, images, and GIFs to keep it visually engaging and provide visual signposts — helping readers quickly navigate to the details that are important to them.</p>
<p>And if a picture is worth a thousand words, a good GIF is worth a million. We’ve found that it’s a good idea to use GIFs whenever an aspect of your project is particularly easy to use, but hard to describe in words. An example of this with our project is showing off how easy it is to build good-looking, functional applications quickly on Appsmith even though there is a massive amount of complexity behind the scenes that is hidden from users.</p>
<p>GIFs and other visuals can never replace the job of documentation for the nitty-gritty details, but they can absolutely provide that extra “wow” factor when showing off the user interface for your project. And they can make it easy for readers to consume a lot of information about your project quickly with little effort, which is the key to increasing adoption and ultimately contribution for your project.</p>
<h2 id="deconstructing-the-appsmith-readme">Deconstructing the Appsmith README</h2>
<p>Let’s go over the <a target="_blank" rel="noopener noreferrer" href="https://github.com/appsmithorg/appsmith">Appsmith README</a> in detail and examine each section, why we made it the way it is, and how it uses the principles outlined above to give readers the information they need as painlessly as possible.</p>
<h3 id="the-heading">The heading</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/5mUmwa6DLMq1zy7hYsK5uk/0daa9413bb7815ceb9c286ad6d82d107/readme1.png" alt="readme1" loading="lazy"></p>
<p>The <a target="_blank" rel="noopener noreferrer" href="https://github.com/appsmithorg/appsmith/blob/release/README.md">Appsmith README</a> begins with our logo, which helps users visually identify our project. Investing in a good logo for your project is more than worth it — brand recognition is critical, even if your project isn’t for-profit. It’s important for your project to look professional and be memorable, so that it's easy to find and doesn’t get supplanted by a better-branded fork.</p>
<p>Next, we introduce Appsmith with a short, two-sentence overview. Here, we focus only on what Appsmith does and who it is for, not how it works; we’ve found that making this section too long can cause readers to quickly lose interest. By this point, readers should have been able to decide whether the project is relevant to their needs or interests at a high level, before we dive into the technical details.</p>
<p>Next, we include some automated stats to give readers an idea of what’s going on in our community. The number of online users from the Discord community, commit stats, and Docker pulls can give users a sense of how many others are also interested in Appsmith. Seeing that our community is active encourages both users and contributors to join the project.</p>
<p>The final ingredients in our README heading are links to Appsmith’s cloud version signup page, our YouTube channel, and templates (a new feature that developers in particular might find very useful). These are the most common “next steps'' for new arrivals, so we make them highly visible.</p>
<p>That’s a total of three examples showing the activity of the project (Discord users, commit activity, Docker pulls) and three calls to action (Get Started, YouTube, Templates) within this first section. This may seem like a lot, but each one calls out to users with a different purpose, and other readers are not negatively impacted by their presence.</p>
<p>We believe that learning how to use the project is the most important information for a user who is interested in the project. However, different users prefer to learn in different ways: some like to read, some like to watch videos, and some like to reference example templates. This is why we’ve added so many different options here to show users how to get up and running with Appsmith.</p>
<p>With the information provided, they can decide whether they’ve found the project that fulfills their requirements, and find the next step they need to take. By using good-looking visuals and links, we’ve packed a lot of information into a very small space without overwhelming the reader.</p>
<h3 id="how-appsmith-works-for-end-users">How Appsmith works (for end users)</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/508Eyhk80F99VvYq3GD2fY/8ee9c1f6129d0668e765f6289f90bb28/readme2.png" alt="readme2" loading="lazy"></p>
<p>Next, we get into how the software works at a high level. We build on the information from the heading, but give more depth so that users can start to form a mental model of the software to understand its most basic concepts. However at this stage, we still hold back on the in-depth details, which we can elaborate on later.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/7p8YMnJGTTjSxv6QsHkmXr/d2134d6178b3e2dcde73dd3c0ce5947d/readme3.gif" alt="readme3" loading="lazy"></p>
<p>Now that the reader has context, we expand on the basic process diagram by showing Appsmith in action using GIFs. Here we’re trying to communicate that building an app with Appsmith is as easy as four simple steps. We’re not trying to teach them the inner workings of the platform yet – that is for later. However, if users are interested in more details at this point, they can follow the included links to the documentation on the Appsmith site.</p>
<h3 id="features-for-end-users">Features (for end users)</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/6OhOah9ZpAgu9vjOL9nb3a/82afc43cb85d73ea1bade570791d4640/readme4.png" alt="readme4" loading="lazy"></p>
<p>Only now do we get to the information that a lot of READMEs start with: a detailed list of features. At this point, if a reader has read all the way through, they already understand the intentions of the project. Or if a reader is skimming the README, which is more common, it is clear that this is the section geared towards users more familiar with Appsmith who are looking for more details.</p>
<p>Either way, this is the part where we start telling readers more about Appsmith. Text now takes precedence over illustrations so that we can increase the density of information as we explain concepts like our UI builder, pre-built widgets, native database integrations, etc.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/5eEdaQnwdeefFfi8Yvk0Dr/1f9c45230243ef8fdba9d97e71ad7c53/readme5.png" alt="readme5" loading="lazy"></p>
<p>Towards the end of the README we start to get into the real technical details. To keep our README skimmable, we do everything we can to avoid walls of text: users should be able to quickly navigate to the major sections, guided by structured, eye-catching formatting.</p>
<p>To assist with this, every feature is bolded with an intuitive icon, with a <span>maximum of one paragraph</span> for the details. We also try to include code snippets wherever it makes sense (for example, when explaining our JavaScript support). And we link out to other resources where it is useful, particularly in sections that include complex topics such as encryption.</p>
<h3 id="how-to-get-set-up-for-end-users">How to get set up (for end users)</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/3tikF45pNyhoOoaDeDfNKB/97927fa4f39adf1da754fb023368342d/readme6.png" alt="readme6" loading="lazy"></p>
<p>Users reaching this section either have engaged with the previous content and are interested in the project, or have arrived looking for installation instructions and clicked straight through on the appropriate CTA in the heading.</p>
<p>Our installation instructions are succinct and actionable, showing our deployment options and linking to the relevant documentation. We also have many additional deployment options, but we intentionally limit the ones we show here because we believe in recommending only the best to our users and keeping the README skimmable.</p>
<p>This emphasizes that the Appsmith project is <span>easy to set up</span> and has <span>great community support</span>. And of course, we’re still following all of the principles from before: we rely very lightly on text, link out for more details wherever possible, and make sure to include intuitive visuals that are easy on the eyes.</p>
<h3 id="the-contributor-section">The contributor section</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/3xTXrgS39fRjH5vSvhDppy/1136638efc31808b19a48dbb6987c2d7/readme7.png" alt="readme7" loading="lazy"></p>
<p>Here’s where we show off our favorite part of Appsmith: the community! We explain our values and how we make sure we maintain a community that people want to be a part of. We then explain more technical information that would benefit contributors, from how to make contributions to how to set up their development environment.</p>
<p>When creating a community around a project, a Code of Conduct can help shape the communication inside the community. We use our Code of Conduct to clearly explain up front what type of community we’re trying to build, as well as which behaviors aren’t acceptable and how enforcement works. This section helps readers tell whether they share Appsmith’s values and whether they should trust that those values will be maintained.</p>
<p>Assuming that they want to be a part of the Appsmith community after reading the Code of Conduct, the next two components show them how. The contribution guide helps new contributors understand how to make an impact while the setup docs show how easy it is to make a contribution. Both of these are designed to cut down the friction to starting, so that potential contributors can jump right in.</p>
<p>Another addition that has been great for us is the <em><a target="_blank" rel="noopener noreferrer" href="https://github.com/appsmithorg/appsmith/issues?q=is%3Aissue+is%3Aopen+label%3A%22Good+First+Issue%22">good first issues</a></em> section in GitHub. We feel that it’s important to make it as easy as possible to start contributing to Appsmith. We do this by tagging open issues in GitHub with “Good First Issue,” which is a trend that <a target="_blank" rel="noopener noreferrer" href="https://github.blog/2020-01-22-how-we-built-good-first-issues/">GitHub has encouraged</a> over the last few years. This helps new contributors get early wins, improving the project without having to understand the deep, complicated architecture of a project.</p>
<p>All of these components play an integral part in the contributor section. Each document by itself is short and to the point, so we can communicate what kind of community we’re building and how they can be a part of it without massive walls of text.</p>
<p>We then conclude our README on a high note — with a showcase of our top contributors. We want to show all of the developers who are working to make the Appsmith project great and encourage others to join.</p>
<h3 id="an-often-overlooked-part-of-the-readme--the-license">An often overlooked part of the README — the license</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/2JCA9GrRYrrErCDUf2oAxt/d3733a6c785f6398d7a1c01bee99ea58/readme8.png" alt="readme8" loading="lazy"></p>
<p>As a footnote, we close out our README with a link to the project license. The license should be a separate file stored within your repository and linked at the bottom of the README.</p>
<p>Many people overlook the license as the boring legal component, but the license you choose often has a dramatic impact on your project in the open-source world. Your license can affect the type of users and contributors that you attract, and in some cases the types of derivative projects that exist years after you’ve created yours.</p>
<p>For that reason, it is important to pay special attention to your license up front to make sure that you choose the best one for your project in the short- and long-term. There are plenty of guides out there about how to choose the right license for your project, including <a target="_blank" rel="noopener noreferrer" href="https://github.com/readme/guides/open-source-licensing">this one from GitHub</a>, so we won’t go too far into the details.</p>
<p>One thing we will say, however, is that we chose the Apache License 2.0 because we wanted to give others the opportunity to build on the work we’ve done (commercially or privately) to drive competition and innovation in the internal tools space.</p>
<h2 id="other-great-readmes">Other great READMEs</h2>
<p>The way we’ve built our README is of course not the only or best way. Each project has different goals and expectations. In order to give some more examples, we also wanted to discuss a couple of different READMEs that we liked across various types of projects.</p>
<h3 id="a-great-package-manager-readme">A great package manager README</h3>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/1uRYBLcpQmNJfiYWntHVBB/32476c78232daff7d9753edb714a4b23/readme9.png" alt="readme9" loading="lazy"></p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/npm/cli">npm</a> is the most popular package manager for JavaScript. Given that this is a package manager, it is harder to explain the project through visuals. This project does a great job of embracing simplicity in the README itself, keeping things incredibly to-the-point and linking out to the more complicated and detailed information.
<img src="https://images.ctfassets.net/lpvian6u6i39/XAt3NtLFHb0Op28IJfLyM/19f2f2780208bd86192f8520ad63c463/readme10.png" alt="readme10" loading="lazy"></p>
<h3 id="a-great-framework-readme">A great framework README</h3>
<p>We also really like the <a target="_blank" rel="noopener noreferrer" href="https://github.com/laravel/laravel">README for Laravel</a>, a popular PHP framework. You can see that they pay attention to a lot of the principles that we focused on in our README, including linking out when more detail is required and avoiding major walls of text.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/WY5cnnjF43tHXmhtrvJee/69bb610152f993b33141ab99b0f18ac4/readme11.png" alt="readme11" loading="lazy"></p>
<p>The Laravel README heavily links to the documentation, but, more importantly, it provides links to learning resources for the community. This includes the Laravel Bootcamp to quickly get up and running and Laracasts, a comprehensive video tutorial library. These resources are one of the most appreciated aspects of Laravel according to developers, so it’s important to communicate this to potential users as early as possible (i.e. in the README).</p>
<p>One thing that we did not include in our README, which can be beneficial for emerging open-source projects, is listing how to sponsor the project. Tools like Patreon are a great avenue for developers looking for additional resources to support their project.</p>
<h3 id="a-great-desktop-app-readme">A great desktop app README</h3>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/vscode">VSCode</a> is ubiquitous, and it also has a great README.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/59hp2m2qmNdoz1CeQyNUVo/73aed78dd744fe00063b7b80c2856126/readme12.png" alt="readme12" loading="lazy"></p>
<p>This README does a lot of things well, including showing what the IDE looks like during use so that users can immediately get an idea of what the product is. This product category is more established and understood by developers than internal tools builders like Appsmith, so more visuals aren’t necessarily required. It has short, to-the-point descriptions of the product with links to more detailed information.</p>
<h3 id="a-great-non-product-readme">A great non-product README</h3>
<p>We also wanted to take a look at an example of a project that’s focused more on education than a product. <a target="_blank" rel="noopener noreferrer" href="https://github.com/trekhleb/homemade-machine-learning">Homemade Machine Learning</a> from Trekhleb is one such example. Machine learning can get very complex, so a project like this can benefit <span>heavily</span> from good visuals and links. We find that this README does that very well, as it links out to great learning resources about different machine learning techniques.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/29bpsCiVqL2AGIFxq9HGSh/24f200ed074e52a151539c5ab68ac76b/readme13.png" alt="readme13" loading="lazy"></p>
<p>This project makes great use of visuals to help students develop a mental model for the machine learning field, which is quite vast. There are so many different algorithms to learn, but after remembering them, there is a logical way to classify them, which this diagram shows well.</p>
<p><img src="https://images.ctfassets.net/lpvian6u6i39/6T3t4hCPYrdzrJ7V6jAMq5/10195efbbb9b6956ad0609c42dcb4f09/readme14.png" alt="readme14" loading="lazy"></p>
<h2 id="readmes-can-go-beyond-providing-the-bare-minimum-information-and-give-your-project-an-identity">READMEs can go beyond providing the bare-minimum information and give your project an identity</h2>
<p>We hope this article has spurred some ideas for your own project. In addition to telling users what your project does, your README sets expectations for quality and ease of use, and is a great way to establish your brand.</p>
<p>We’ve put a lot of thought into the content and structure of our README, and have found that with each iteration we’ve been able to engage more visitors as users and community members by prioritizing the content. We’re all on this development journey, and there’s no one right way…</p>
<p><a target="_blank" rel="noopener noreferrer" href="https://discord.gg/rBTTVJp">Let us know</a> if there’s something you think our README could do better, or you have your own to show off!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Minecraft Grub Theme (332 pts)]]></title>
            <link>https://github.com/Lxtharia/minegrub-theme</link>
            <guid>36771980</guid>
            <pubDate>Tue, 18 Jul 2023 13:38:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Lxtharia/minegrub-theme">https://github.com/Lxtharia/minegrub-theme</a>, See on <a href="https://news.ycombinator.com/item?id=36771980">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Minegrub</h2>
<p dir="auto">A Grub Theme in the style of Minecraft!</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Lxtharia/minegrub-theme/blob/main/assets/preview_minegrub.png"><img src="https://github.com/Lxtharia/minegrub-theme/raw/main/assets/preview_minegrub.png" alt="Minegrub Preview &quot;Screenshot&quot;"></a></p>
<h2 tabindex="-1" dir="auto">Installation</h2>
<blockquote>
<h3 tabindex="-1" dir="auto">Note: grub vs grub2</h3>
<ul dir="auto">
<li>Check if you have a <code>/boot/grub2</code> folder instead of a <code>/boot/grub</code> folder in which case you would just have to adjust the file paths mentioned here and in the <code>assets/minegrub-update.service</code> file</li>
<li>Also if you're not sure, run <code>grub-mkconfig -V</code> to check if you have grub version 2 (you should have)</li>
</ul>
</blockquote>
<ul dir="auto">
<li>Clone this repository</li>
</ul>
<div data-snippet-clipboard-copy-content="git clone https://github.com/Lxtharia/minegrub-theme.git"><pre><code>git clone https://github.com/Lxtharia/minegrub-theme.git
</code></pre></div>
<ul dir="auto">
<li>Copy the folder to your boot partition: (for info: <code>-ruv</code> = recursive, update, verbose)</li>
</ul>
<div data-snippet-clipboard-copy-content="sudo cp -ruv ./minegrub-theme/* /boot/grub/themes/minegrub-theme/"><pre><code>sudo cp -ruv ./minegrub-theme/* /boot/grub/themes/minegrub-theme/
</code></pre></div>
<ul dir="auto">
<li>Change/add this line in your <code>/etc/default/grub</code>:</li>
</ul>
<div data-snippet-clipboard-copy-content="GRUB_THEME=/boot/grub/themes/minegrub-theme/theme.txt"><pre><code>GRUB_THEME=/boot/grub/themes/minegrub-theme/theme.txt
</code></pre></div>
<ul dir="auto">
<li>Update your live grub config by running</li>
</ul>
<div data-snippet-clipboard-copy-content="sudo grub-mkconfig -o /boot/grub/grub.cfg"><pre><code>sudo grub-mkconfig -o /boot/grub/grub.cfg
</code></pre></div>
<ul dir="auto">
<li>You're good to go!</li>
</ul>
<h2 tabindex="-1" dir="auto">Random splash texts and accurate "x Packages Installed" text!</h2>
<p dir="auto">The <code>update_theme.py</code> script chooses a random line from <code>resources/splashes.txt</code> and generates and replaces the <code>logo.png</code> which holds the splash text, as well as updates the amount of packages currently installed</p>
<ul dir="auto">
<li>Make sure <code>neofetch</code> is installed</li>
<li>Make sure Python 3 (or an equivalent) and the Pillow python package are installed
<ul dir="auto">
<li>Install Pillow either with the python-pillow package from the AUR or with
<code>sudo -H pip3 install pillow</code></li>
<li>It's important to use <code>sudo -H</code>, because it needs to be available for the root user</li>
</ul>
</li>
<li>To add new splash texts simply edit <code>./resources/splashes.txt</code> and add them to the end of the file (if you add it at the beginning or in the middle, some splashes may never get used because the image cashing uses the line of the file the splash is on)</li>
<li>If you want to remove splashes you should reset the cache by deleting <code>/boot/grub/themes/minegrub-theme/cache</code></li>
</ul>
<h3 tabindex="-1" dir="auto">Update splash and "Packages Installed"...</h3>
<h4 tabindex="-1" dir="auto">...without systemd</h4>
<ul dir="auto">
<li>Just run <code>python /boot/grub/themes/minegrub-theme/update_theme.py</code> (from anywhere) after boot using whatever method works for you</li>
</ul>
<h4 tabindex="-1" dir="auto">...with systemd</h4>
<ul dir="auto">
<li>Edit <code>./assets/minegrub-update.service</code> to use <code>/boot/grub2/</code> on line 5 if applicable</li>
<li>Copy <code>./assets/minegrub-update.service</code> to <code>/etc/systemd/system</code></li>
<li>Enable the service: <code>systemctl enable minegrub-update.service</code></li>
<li>If it's not updating after rebooting (it won't update on the first reboot because it updates after you boot into your system), check systemctl status minegrub-update.service for any errors (for example if pillow isn't installed in the correct scope)</li>
</ul>
<h2 tabindex="-1" dir="auto">Adjusting for a different amount of boot options:</h2>
<ul dir="auto">
<li>When you have more/less than 4 boot options, you might want to adjust the height of the bottom bar (that says "Options" and "Console")</li>
<li>The formula and some precalculated values (for 2,3,4,5... boot options) are in the <code>theme.txt</code>, so you should be able to easily change it to the correct value.</li>
</ul>
<h2 tabindex="-1" dir="auto">Notes:</h2>
<ul dir="auto">
<li>the <code>GRUB_TIMEOUT_STYLE</code> in the defaults/grub file should be set to <code>menu</code>, so it immediately shows the menu (else you would need to press ESC and you dont want that)</li>
<li>I'm no Linux expert, that's why I explain it so thoroughly, for other newbies :&gt;</li>
<li>i use arch btw</li>
<li>i hope u like it, cause i sure do lmao</li>
</ul>
<h3 tabindex="-1" dir="auto">Thanks to</h3>
<ul dir="auto">
<li><a href="https://github.com/toboot">https://github.com/toboot</a> for giving me this wonderful idea!</li>
<li>the internet for giving me wisdom lmao (Mainly <a href="http://wiki.rosalab.ru/en/index.php/Grub2_theme_tutorial" rel="nofollow">http://wiki.rosalab.ru/en/index.php/Grub2_theme_tutorial</a>)</li>
<li>The contributors for contributing and giving me some motivation to improve some little things here and there</li>
</ul>
<p dir="auto">Font downloaded from <a href="https://www.fontspace.com/minecraft-font-f28180" rel="nofollow">https://www.fontspace.com/minecraft-font-f28180</a> and used for non commercial use.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Silicon Valley’s business model is a scam (239 pts)]]></title>
            <link>https://www.businessinsider.com/venture-capital-big-tech-antitrust-predatory-pricing-uber-wework-bird-2023-7</link>
            <guid>36771737</guid>
            <pubDate>Tue, 18 Jul 2023 13:22:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/venture-capital-big-tech-antitrust-predatory-pricing-uber-wework-bird-2023-7">https://www.businessinsider.com/venture-capital-big-tech-antitrust-predatory-pricing-uber-wework-bird-2023-7</a>, See on <a href="https://news.ycombinator.com/item?id=36771737">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-type="content-lock" data-load-strategy="exclude">
                                  <p>In 2016, Matt Wansley was trying to get work as a lawyer for a tech company — specifically, working on self-driving cars. He was making the rounds, interviewing at all the companies whose names you know, and eventually found himself talking to an executive at Lyft. So Wansley asked her, straight-out: How committed was Lyft, really, to<a href="https://www.businessinsider.com/fourth-color-traffic-lights-streets-self-driving-robot-cars-trucks-2023-4" data-analytics-product-module="body_link" rel=""> autonomous driving</a>?</p><p>"Of course we're committed to automated driving," the exec told him. "The numbers don't pencil out any other way."</p><p>Wait a minute, Wansley thought. Unless someone invents a robot that can drive as well as humans, one of America's biggest <a href="https://www.businessinsider.com/you-can-request-lyft-as-soon-land-airport-uber-rideshare-2023-5" data-analytics-product-module="body_link" rel="">ride-hailing companies</a> doesn't expect to turn a profit? Like, ever? Something was clearly very, very screwy about the business model of Big Tech.</p>
                          
                          <p>"So what was the investment thesis behind Uber and Lyft?" says Wansley, now a professor at the Cardozo School of Law. "Putting billions of dollars of capital into a money-losing business where the path to profitability wasn't clear?"</p><p>Wansley and a Cardozo colleague, Sam Weinstein, set out to understand the money behind the madness. Progressive economists had long understood that tech companies, backed by gobs of <a href="https://www.businessinsider.com/early-stage-startups-turn-to-cvcs-as-vc-funding-slows-2023-6" data-analytics-product-module="body_link" rel="">venture capital</a>, were effectively subsidizing the price of their products until users couldn't live without them. <a href="https://www.businessinsider.com/amazon-meta-layoffs-usher-in-era-of-revenue-per-employee-2023-3" data-analytics-product-module="body_link" rel="">Think Amazon</a>: Offer stuff cheaper than anyone else, even though you lose money for years, until you scale to unimaginable proportions. Then, once you've crushed the competition and become the only game in town, you can raise prices and make your money back. It's called predatory pricing, and it's supposed to be illegal. It's one of the arguments that progressives in the Justice Department used to <a href="https://www.businessinsider.com/ftc-amazon-antitrust-suit-marketplace-third-party-sellers-lawsuit-bloomberg-2023-6" data-analytics-product-module="body_link" rel="">bust up monopolies</a> like Standard Oil in the early 20th century. Under the rules of capitalism, you aren't allowed to use your size to bully competitors out of the market.</p><p>The problem is, conservative economists at the University of Chicago have spent the past 50 years insisting that under capitalism, predatory pricing is not a thing. Their <a href="https://heinonline.org/HOL/LandingPage?handle=hein.journals/antlervi4&amp;div=38&amp;id=&amp;page=" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">head-spinning argument</a> goes like this: Predators have a larger market share to begin with, so if they cut prices, they stand to lose much more money than their competitors. Meanwhile their prey can simply flee the market and return later, like protomammals sneaking back to the jungle after the velociraptors leave. Predatory companies could never recoup their losses, which meant predatory behaviors are <em>irrational</em>. And since Chicago School economists are the kind of economists who believe that markets are always rational, that means predatory pricing cannot, by definition, exist.</p><p>The Supreme Court bought the argument. In the 1986 case <a href="https://supreme.justia.com/cases/federal/us/475/574/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Matsushita Electric Industry Co. v. Zenith Radio Corp<em>.</em></a>, the court famously ruled that "predatory pricing schemes are rarely tried, and even more rarely successful." And in 1993, in <a href="https://supreme.justia.com/cases/federal/us/509/209/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Brooke Group v. Brown &amp; Williamson Tobacco Corp.</a>, the court said that to convict a company of predatory pricing, prosecutors had to show not only that the accused predators had cut prices below market rates but also that they had a "dangerous probability" of recouping their losses. That effectively shut down the government's ability to prosecute companies for predatory pricing.</p><p>"The last time I checked, no one — including the United States government — has won a predatory pricing case since Brooke Group," says Spencer Waller, an antitrust expert at Loyola's School of Law. "Either they can't prove below-cost pricing, or they can't prove recoupment, because a nonexpert generalist judge who buys the basic theory when they read Matsushita and Brooke Group is super-skeptical this stuff is ever rational, absent really compelling evidence."</p><p>Lots of economists have come up with <a href="https://doi.org/10.1146/annurev-economics-082222-070822" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">solid counter-counterarguments</a> to the Chicago School's skepticism about predatory pricing. But none of them have translated to winnable antitrust cases. Wansley and Weinstein — who, not coincidentally, used to work in antitrust enforcement at the Justice Department — set out to change that. In a new paper titled "<a href="https://ssrn.com/abstract=4437360" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">Venture Predation</a>," the two lawyers make a compelling case that the classic model of venture capital — disrupt incumbents, build a scalable platform, move fast, break things — isn't the peak of modern capitalism that Silicon Valley says it is. According to this new thinking, it's <em>anti</em>capitalist. It's illegal. And it should be aggressively prosecuted, to promote free and fair competition in the marketplace.</p><p>"We think real world examples are not hard to find — if you look in the right place," Wansley and Weinstein write. "A new breed of predator is emerging in Silicon Valley." And the mechanism those predators are using to illegally dominate the market is venture capital itself.</p><hr><p>Venture investing is the answer to the question of what would happen if you staffed a bank's loan department with adrenaline junkies. The limited partners in venture funds demand high returns, and those funds are transient things, lasting maybe a decade, which means the clock is ticking. Venture capitalists and the investors who put money into their funds aren't necessarily looking for a successful product (though they wouldn't turn one down). For VCs and their limited partners, the most profitable endgame is a quick exit — either selling off the company or taking it public in an IPO.</p>
                          
                          <p>Those pressures, Wansley and Weinstein argue, encourage risky strategies — including predatory pricing. "If you buy what the Chicago School of economists think about self-funded predators, you might think it's irrational for a company to engage in predatory pricing for a bunch of reasons," Weinstein says. "But it might not be irrational for a VC." The idea that it's so irrational as to be nonexistent is "a bullshit line that has somehow become common wisdom."</p><p>Take Uber, one of their key examples. It'd be one thing if the company had simply outcompeted taxicabs on the merits. Cabs, after all, were themselves a fat and complacent monopoly. "Matt and I don't have any problem with that," Weinstein says. "You have a new product, scale quickly, and use some subsidies to get people on board." Disrupt an old business and make a new one.</p><p>But that's not what happened. As in a soap opera or a <a href="https://www.wired.com/story/wandavision-marvel-dc-star-wars-multiverse-collapse/" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">comic-book multiverse</a>, the ending never arrived. Uber kept subsidizing riders and drivers, losing billions trying to spend its competitors into oblivion. The same goes for a lot of other VC-backed companies. "WeWork was setting up offices right next to other coworking spaces and saying, 'We'll give you 12 months free.' Bird was scattering its scooters all over cities," Wansley says. "The pattern to us just seems very familiar."&nbsp;</p><blockquote><q>Uber is one of the best investments in history, and it was a predatory pricing.</q></blockquote><p>On its face, it also seems to prove the point of the Chicago School: that companies can never recoup the losses they incur through predatory pricing. Matsushita and Brooke Group require that prosecutors show harm. But if the only outcome of the scaling strategy used by Uber and other VC startups is to create an endless "<a href="https://www.nytimes.com/2021/06/08/technology/farewell-millennial-lifestyle-subsidy.html" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">millennial lifestyle subsidy</a>," that just means wealth is being transferred from investors to consumers. The only victims of predatory pricing are the predators themselves.</p><p>Where Wansley and Weinstein break important new ground is on the other legal standard set by the Supreme Court: recoupment of losses. If Uber and WeWork and the rest of the unicorns are perpetual money losers, it sounds like the standard isn't met. But Wansley and Weinstein point out that it can be — even if the companies never earn a dime and even if everyone who invests in the companies, post-IPO, loses their bets. That's because the venture capitalists who seeded the company <em>do</em> profit from the predatory pricing. They get in, get a hefty return on their investment, and get out before the whole scheme collapses.</p><p>"Will Uber ever recoup the losses from its sustained predation?" Wansley and Weinstein write. "We do not know. Our point is that, <em>from the perspective of the VCs who funded the predation, it does not matter</em>. All that matters is that investors were willing to buy the VCs' shares at a high price."</p><p>Let's be clear here: This isn't the traditional capitalist story of "you win some, you lose some." The point isn't that venture capitalists sometimes invest in companies that don't make their money back. The point is that the entire model deployed by VCs is to profit by disrupting the marketplace with predatory pricing, and leave the losses to the suckers who buy into the IPO. A company that engages in predatory pricing and its late-stage investors might not recoup, but the venture investors do.&nbsp;</p><p>"The single most important fact in this paper is that Benchmark put $12 million into Uber and got $5.8 billion back," Wansley says. "That's one of the best investments in history, and it was a predatory pricing."</p><hr><p>This new insight — that venture capital is predatory pricing in a new wrapper — could prove transformative. By translating the Silicon Valley jargon of exits and scaling into the legalese of antitrust law, Wansley and Weinstein have opened a door for the prosecution of tech investors and their anticompetitive behavior. "Courts will have to adjust the way they're thinking about recoupment," Weinstein says. "What did the investors who bought from the VCs think was going to happen? Did they think they were going to recoup?" That, he says, would be a "pretty good pathway" for courts to follow in determining whether a company's practices are anticompetitive.</p>
                          
                          <blockquote><q>Capitalism is supposed to allow competition to foster innovation and choice; monopolies quash all that so a few people can get rich.</q></blockquote><p>What makes this argument particularly powerful, from a legal perspective, is that it doesn't reject the basics of the Chicago School's thinking on antitrust. It accepts that consumer welfare and the efficiency of markets are paramount. It just points out that something <a href="https://spectrum.ieee.org/the-uncanny-valley" target="_blank" rel="noopener nofollow" data-analytics-product-module="body_link">uncanny</a> — and illegal — is taking place in Silicon Valley. "I'm pro-enforcement and anti-Chicago School, so I'm always looking for areas where I think they're wrong," Weinstein says. "And here's one."</p><p>That kind of "serious legal scholarship" can be particularly successful with the courts, according to Waller, the antitrust expert at Loyola. "It's a good, modest strategy to say, 'We think your model's wrong, but even if your model's right in general, it's not right here.' That's both how you win cases and how you chip away at an edifice you want to challenge."</p><p>With so many industries imploding into oligopolies — tech, healthcare, pharma, entertainment, journalism, retail — it's a hopeful sign to see the trustbusting mindset stirring to life once more. Capitalism is supposed to allow competition to foster innovation and choice; monopolies quash all that so a few people can get rich. But the new scholarship on predatory pricing could ripple well beyond the courts. Wansley and Weinstein's paper put me in mind of "The Big Con," David Maurer's linguistic study of con artists first published in 1940. Maurer said the most delicate part of a con was the end — the <em>blow-off</em>. After the sucker has been bled dry, the grifter has to ditch the victim, ideally in such a way that they won't go to the cops. In the perfect crime, the mark doesn't even know they've been had. The transfer of lousy tech equity to late-stage investors who have been led to believe it's valuable sure looks like a good blow-off to me.</p><p>So now that we know precisely how Silicon Valley's big con works, maybe the marks won't be so quick to fall for it. Once you know what a phishing email looks like, you tend to stop replying to them. The same goes for recognizing the outlines of this particular grift. "It's not a Ponzi scheme, but it favors certain investors," Weinstein says. "If people in Silicon Valley start thinking about this as a predatory pricing scam, then I think the late-stage investors will start asking questions."</p><p>And not just about ride hailing or office sharing. Maybe grocery delivery? Or streaming-service subscriptions? The same kind of <em>aha! </em>light that went off for Wansley during his interview with the Lyft executive could start to go off for other people as well. Some of them will be investors who decide not to park their money in predatory tech companies. And some of them, perhaps, will be government regulators who are looking for ways to bust our modern-day trusts.</p><hr><p><a href="https://www.businessinsider.com/author/adam-rogers" data-analytics-product-module="body_link" rel=""><em>Adam Rogers</em></a> <em>is a senior correspondent at Insider.</em></p>
                      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Styling Flowcharts Using CSS (133 pts)]]></title>
            <link>https://flowchart.fun/blog/post/styling-flowcharts-using-css</link>
            <guid>36771637</guid>
            <pubDate>Tue, 18 Jul 2023 13:15:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://flowchart.fun/blog/post/styling-flowcharts-using-css">https://flowchart.fun/blog/post/styling-flowcharts-using-css</a>, See on <a href="https://news.ycombinator.com/item?id=36771637">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[A Theory on Adam Instability in Large-Scale Machine Learning (109 pts)]]></title>
            <link>https://arxiv.org/abs/2304.09871</link>
            <guid>36771484</guid>
            <pubDate>Tue, 18 Jul 2023 13:02:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2304.09871">https://arxiv.org/abs/2304.09871</a>, See on <a href="https://news.ycombinator.com/item?id=36771484">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
    
    <div><p><span>Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Molybog%2C+I">Igor Molybog</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Albert%2C+P">Peter Albert</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M">Moya Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=DeVito%2C+Z">Zachary DeVito</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Esiobu%2C+D">David Esiobu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goyal%2C+N">Naman Goyal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koura%2C+P+S">Punit Singh Koura</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Narang%2C+S">Sharan Narang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Poulton%2C+A">Andrew Poulton</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Silva%2C+R">Ruan Silva</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang%2C+B">Binh Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liskovich%2C+D">Diana Liskovich</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+P">Puxin Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yuchen Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kambadur%2C+M">Melanie Kambadur</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Roller%2C+S">Stephen Roller</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+S">Susan Zhang</a></p></div>
      
    
  
  
  
    <p><a aria-describedby="download-button-info" href="https://arxiv.org/pdf/2304.09871">Download PDF</a></p><blockquote>
            <span>Abstract:</span>  We present a theory for the previously unexplained divergent behavior noticed
in the training of large language models. We argue that the phenomenon is an
artifact of the dominant optimization algorithm used for training, called Adam.
We observe that Adam can enter a state in which the parameter update vector has
a relatively large norm and is essentially uncorrelated with the direction of
descent on the training loss landscape, leading to divergence. This artifact is
more likely to be observed in the training of a deep model with a large batch
size, which is the typical setting of large-scale language model training. To
argue the theory, we present observations from the training runs of the
language models of different scales: 7 billion, 30 billion, 65 billion, and 546
billion parameters.

    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Igor Molybog [<a href="https://arxiv.org/show-email/fcecba99/2304.09871">view email</a>]
      <br>
    
        <strong><a href="https://arxiv.org/abs/2304.09871v1">[v1]</a></strong>
    
        Wed, 19 Apr 2023 06:15:11 UTC (560 KB)<br>
    
    <strong>[v2]</strong>
    
        Tue, 25 Apr 2023 04:48:47 UTC (560 KB)<br>
    </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ten years of “Go: The good, the bad, and the meh” (171 pts)]]></title>
            <link>https://blog.carlmjohnson.net/post/2023/ten-years-of-go-good-bad-meh/</link>
            <guid>36771227</guid>
            <pubDate>Tue, 18 Jul 2023 12:37:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.carlmjohnson.net/post/2023/ten-years-of-go-good-bad-meh/">https://blog.carlmjohnson.net/post/2023/ten-years-of-go-good-bad-meh/</a>, See on <a href="https://news.ycombinator.com/item?id=36771227">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main><article><header><time datetime="2023-07-18T00:00:00Z">Tuesday, July 18, 2023</time></header><section><p>Ten years ago, I wrote <a href="https://blog.carlmjohnson.net/post/google-go-the-good-the-bad-and-the-meh/">Go: The Good, the Bad, and the Meh</a>. Way back in 2013, it made it to the front page of <a href="https://news.ycombinator.com/item?id=5200916">Hacker News</a> and got over 400 comments on <a href="https://www.reddit.com/r/programming/comments/18b0qb/google_go_the_good_the_bad_and_the_meh/">/r/programming</a>. I don’t have analytics from back then, but I suspect it’s one of my more discussed pieces of writing, and it was definitely one of my first experiences of getting a lot of feedback for my writing. (Then again, I don’t have any evidence of whether <a href="https://twitter.com/ID_AA_Carmack/status/1293311943995002881">John Carmack read it</a>, so maybe it’s not the one for my obituary.)</p><p>Anyway, it’s been a decade, and in that time I’ve gone from playing around with Go as an amateur to being a professional programmer and using Go as one of my core languages. So, I thought it would be fun to look back at what I got right, what’s changed since I wrote it, what I missed, and what I got wrong. Feel free to read or re-read <a href="https://blog.carlmjohnson.net/post/google-go-the-good-the-bad-and-the-meh/">the original post</a>, or just stick to my reflections here without digging back into it. Just know that as its title suggests, I wrote it with three sections for what I thought was “good”, “bad”, and “meh” about Go at that time.</p><p>&nbsp;<br><em>Note for anyone submitting this post to social media, the quotation marks in the title are load bearing. <strong>Do not remove them.</strong></em></p><hr><h2 id="what-i-got-right">What I got right</h2><p>I still agree with almost everything I listed in the “good” section from before. All of these things are still good:</p><ul><li>That Go was designed for working on large projects in a team with a modern version control system</li><li>Using capitalization for the public/private distinction in functions, methods, variables, and fields</li><li>Using the directory as the fundamental unit of packaging</li><li>Having a single binary for deployment</li><li>That the go tool is fast and has built in <code>go fmt</code>, <code>go doc</code>, and <code>go test</code></li><li>Using type last style (<code>var x int</code>) rather than type first style (<code>int x</code>)</li><li>Having explicit variable declaration (vs. Python’s implicit declaration by using <code>=</code>)</li><li><a href="https://go.dev/play/">The Go Playground</a></li><li>Logical type names (int64, float32, etc. vs. long and double)</li><li>Having the three basic data types of string, variable length array, and hash map</li><li>Interfaces as compile time duck typing</li><li>Not providing inheritance</li></ul><p>The other sections, well, we’ll come back to them.</p><h2 id="whats-changed">What’s changed</h2><p>The biggest change to Go the language in the last ten years has obviously been the addition of generics.</p><p>In the original post, I listed lack of generics under the “bad” section:</p><blockquote><p>Idiomatic Go code has a couple of different tricks for using interfaces so that you don’t need generics… but sometimes you just have to bite the bullet and copy-and-paste a function three times so you have three differently typed versions of it. If you’re trying to make your own data structure, you can just use <code>interface {}</code> as a universal object type, but then you lose compile time type safety. If you want to make a universal <code>sum</code> function, on the other hand, there’s no really good way to do it.</p></blockquote><p>Generics were added to Go in <a href="https://blog.carlmjohnson.net/post/2021/golang-118-minor-features/">version 1.18</a> in February 2022. It has basically solved this complaint. A generic <code>sum</code> looks <a href="https://go.dev/play/p/R-QDDqcp3w9">like this</a>:</p><div><pre tabindex="0"><code data-lang="go"><span><span><span>type</span> <span>Numeric</span> <span>interface</span> <span>{</span>
</span></span><span><span>    <span>~</span><span>int</span> <span>|</span> <span>~</span><span>int8</span> <span>|</span> <span>~</span><span>int16</span> <span>|</span> <span>~</span><span>int32</span> <span>|</span> <span>~</span><span>int64</span> <span>|</span>
</span></span><span><span>        <span>~</span><span>uint</span> <span>|</span> <span>~</span><span>uint8</span> <span>|</span> <span>~</span><span>uint16</span> <span>|</span> <span>~</span><span>uint32</span> <span>|</span> <span>~</span><span>uint64</span> <span>|</span> <span>~</span><span>uintptr</span> <span>|</span>
</span></span><span><span>        <span>~</span><span>float64</span> <span>|</span> <span>~</span><span>float32</span>
</span></span><span><span><span>}</span>
</span></span><span><span>
</span></span><span><span><span>func</span> <span>sum</span><span>[</span><span>N</span> <span>Numeric</span><span>](</span><span>vals</span> <span>...</span><span>N</span><span>)</span> <span>N</span> <span>{</span>
</span></span><span><span>    <span>var</span> <span>total</span> <span>N</span> <span>=</span> <span>0</span>
</span></span><span><span>    <span>for</span> <span>_</span><span>,</span> <span>val</span> <span>:=</span> <span>range</span> <span>vals</span> <span>{</span>
</span></span><span><span>        <span>total</span> <span>+=</span> <span>val</span>
</span></span><span><span>    <span>}</span>
</span></span><span><span>    <span>return</span> <span>total</span>
</span></span><span><span><span>}</span>
</span></span></code></pre></div><p>So far, generics have been a change for the better.</p><p>The other major change from the Go of 2013 is the addition of Go modules. I listed the <code>GOPATH</code> system of that time under the “good” section:</p><blockquote><p>[If] you run <code>go get github.com/userA/project/</code> it will use git to download the project from github.com and put it into the right place. Even better, if that project contains the line <code>import "bitbucket.com/userB/library"</code>, the go tool can also download and install <em>that</em> library in the right place. So, in one stroke, Go has its own elegant solution to packaging and a modern, decentralized version of CPAN or PyPI: the VCS you were already using!</p></blockquote><p>Go modules took this existing system and added the ability to specify version requirements for the imported packages and even Go itself. There were some bumps and hurt feelings in the transition from <code>GOPATH</code> to Go modules, but overall, it was very smooth and day-to-day, I can’t say that I’ve run into many problems with them. It just works, and you mostly don’t think about it.</p><p>I said at the time that I was “meh” about the fact that the Go compiler has no warnings, only errors, but the way that this has evolved over the last decade is that warnings with a low false positive rate have become <code>go vet</code> errors, and other warnings just end up in miscellaneous linters. So, it’s still technically true about the compiler that there are no warnings, but the Go ecosystem certainly has lots of avenues for warnings in practice, so it’s only an interesting distinction when you want to complain about Go.</p><h2 id="what-i-missed">What I missed</h2><p>Rereading the post now, what jumps out to me is that there was no mention of unions, sum types, or optional values. I think this just goes to show how much the state of the industry has changed in the last decade. Back then, sum types and optional values were still considered academic features. You saw them in ML derived languages like Haskell, but not in mainstream C-influence languages. Swift wouldn’t come out until 2014, and Rust until 2015, but languages since then have all been judged by whether they have a solution to <a href="https://en.wikipedia.org/wiki/Null_pointer#History">the billion dollar mistake</a>.</p><p>Unfortunately, I don’t think there’s any chance that <code>nil</code> could be removed from Go now. Too much code has been written with it to ever really move on to optional types, at least in Go as we know it. However, there is a proposal to <a href="https://github.com/golang/go/issues/57644">add sum types by restricting interface values</a>, and I think that has a good chance of happening in one form or another.</p><p>Again, it shows how things have changed that I praised Go’s type inference as an advance in the state of the art, but now the Hacker News crowd considers Go’s type inference to be very limited compared to other languages. <a href="https://www.stroustrup.com/quotes.html">You either die a language nobody uses or live long enough to be one people complain about</a>.</p><p>The other thing that stands out as an omission in the original post in retrospect is that there is no mention of <a href="https://go.dev/doc/go1compat">the Go 1 guarantee</a>. When Go 1.0 was released, the Go team promised not to break source compatibility. Of course, there have always been caveats, bugs, and exceptions, but by most reckonings, they have kept their word to a remarkable degree. To be fair to myself, at the time, <a href="https://go.dev/doc/devel/release#go1">Go 1.0.3 was the current version</a>, so it was impossible for me to know that the Go 1 guarantee would last for a decade plus, but I think that it really was critical in the development of Go into the language it is today. Before Go 1, it was common for the Go team to issue changes to the language or the standard library that needed <code>go fix</code> to be run over a codebase to get it caught up to the new standard. Since then however, if you write a Go program, use the standard library, and don’t rely on unsafe features or security holes, your code should just keep on working indefinitely.</p><p>It’s a real breath of fresh air compared to other ecosystems, and it’s one of my favorite things as a developer. With other languages, upgrading to a new version (<a href="https://github.com/alpinejs/alpine/issues/580">even a minor version</a>) is fraught with worry that something, somewhere will break unexpectedly. Even when the breakage is declared in advance with deprecation warnings, it’s still churn that you have to make time to fix as part of the upgrade process. With Go, I just don’t worry about that. Yes, there can be bugs, but nothing is going to break on purpose just because it made life easier for someone else. It feels like the language team is working for me instead of against me.</p><h2 id="what-i-got-wrong">What I got wrong</h2><p>I would say that on the whole, the old post holds up pretty well and nothing in it was egregiously wrong. However, there are a few things where at the time I felt one way, but now I have a slightly more nuanced view without totally disagreeing with myself.</p><p>In the “good” section specifically, there was nothing that was actually bad exactly, but in hindsight, I do see more of the tradeoffs to concurrency.</p><p>I wrote at the time,</p><blockquote><p>It works like you thought concurrency should work before you learned about concurrency. It’s nice. You don’t have to think about it too much. If you want to run a function concurrently, just do <code>go function(arguments)</code>. If you want to communicate with the function, you can use channels, which are synchronous by default, meaning they pause execution until both sides are ready.</p></blockquote><p>This is still true, and I still think working with concurrency in Go is very easy compared to something like Python. (Incidentally, I was writing before <code>Promise</code> and <code>await</code> were added to JavaScript to give context.) On the other hand, nothing in the type system prevents you from creating a data race, so you have to use the race detector religiously in testing, and using channels directly without following <a href="https://blog.carlmjohnson.net/post/share-memory-by-communicating/">a well known pattern</a> for creating <a href="https://github.com/carlmjohnson/flowmatic">structured concurrency</a> is <a href="https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/">a surefire recipe for spaghetti code</a>. It’s a tradeoff in that Go gives you just enough rope to hang yourself, but most of the time when you’re just writing a web server or something, you can get most of the benefits of concurrency without the drawbacks.</p><p>On the other hand, most of the things I listed in the “bad” section have mostly been non-problems for me in practice. I’m not sure why I was worried about the string type not having methods. The lack of a <code>#!</code> starting line for scripts turned out <a href="https://github.com/carlmjohnson/go-run">not to matter in practice</a>. That Go isn’t <a href="https://en.wikipedia.org/wiki/Don%27t_repeat_yourself">DRY</a> is more of a <em>tradeoff</em> than “bad” per se.</p><p>The lack of generics was an occasional problem, but there were usually pretty obvious workarounds for it, like falling back on dynamic typing, using reflection, or writing a code generator. I’m happy to have generics now, but it’s true that when we didn’t have it, it only occasionally presented a real problem. I am excited to watch how generics impact the future of Go, especially as the Go team work on iterators, but I do worry a bit about “useless uses of generics” where people try to shoehorn in generics where regular interfaces would work just fine. We’ll see how it goes.</p><p>The things I listed in the “meh” section also mostly continue to be tradeoffs, but in retrospect, I think Go took the better side of the tradeoff for the design space it occupies. Not having exceptions is a tradeoff, and while I might wish that Go had something like Zig’s <code>try</code> and <code>errdefer</code>, in practice, it’s fine. Using <code>if err != nil</code> is the worst system except all the other systems that have been tried from time to time, and it turns out to allow <a href="https://blog.carlmjohnson.net/post/2020/working-with-errors-as/">useful evolutions in user code</a>.</p><p><a href="https://matklad.github.io/">Matklad</a> put it well <a href="https://lobste.rs/s/aocv9o/trouble_with_checked_exceptions_2003#c_bsxqyu">in a recent comment</a>,</p><blockquote><p>It seems we are roughly converging on the error handling. Midori, Go, Rust, Swift, Zig follow similarlish design, which isn’t quite checked exceptions, but is surprisingly close.</p><ul><li>there’s a way to mark functions which can fail. Often, this is a property of the return type, rather than a property of the function (Result in Rust, error pair in Go, <code>!</code> types in Zig, and bare throws decl in Midori and Swift)</li><li>we’ve cranked-up annotation burden and we mark not only throwing function declarations, but call-sites as well (<code>try</code> in Midori, Swift, Zig, <code>?</code> in Rust, <code>if err != nil</code> in Go).</li><li>Default is existentially-typed <code>AnyError</code> (<code>error</code> in Go, <code>Error</code> in Swift, <code>anyhow</code> in Rust, <code>anyerror</code> in Zig). In general, the value is shifted to distinguishing between zero and one error, rather than exhaustively specifying the set of errors.</li></ul></blockquote><p>That seems right to me. Go’s error handling is more verbose than those other languages, but structurally, there’s a lot of commonality under the surface.</p><p>I listed that Go doesn’t have operator overloading, function/method overloading, or keyword arguments as being “meh” features, but I’m pretty happy about them now. The only case where I wish Go had operator overloading is for <a href="https://pkg.go.dev/math/big#Int">big.Int</a>, and I’m hopeful that maybe someday those will be <a href="https://github.com/golang/go/issues/19624">added to the language itself</a>. Keyword arguments might be nice to have, but in practice, structs work just fine, and there’s always <a href="https://blog.carlmjohnson.net/post/2021/requests-golang-http-client/">builders with method chaining</a> for really hairy cases.</p><h2 id="takeaways">Takeaways</h2><p>Before I wrote <a href="https://blog.carlmjohnson.net/post/google-go-the-good-the-bad-and-the-meh/">the post</a>, I naively thought people would talk about how I had a section that dismissed object oriented programming out of hand. Instead people latched onto my misuse of the word “idiot” to describe people who debate the color of the bikeshed around public/private fields.</p><p>Go’s use of interfaces and lack of inheritance were probably the biggest questions to me when I wrote the article. I thought it was a good idea, but I was prepared to be surprised by experience and run into strong arguments against it. Hynek Schlawack wrote <a href="https://hynek.me/articles/python-subclassing-redux/">a great post in 2021</a> explaining why having interfaces and not having inheritance was a good design choice for Go. Basically, inheritance only makes sense when a subclass is a true specialization of the superclass, in which case Go’s type embedding works fine. So, I got the discussion I wanted, but it took about eight more years.</p><p>It would take until 2019 for Dan Abramov to write <a href="https://overreacted.io/name-it-and-they-will-come/">the definitive explanation of the experience of writing for Hacker News</a>:</p><blockquote><p>Congratulations!</p><p>Your project hit the front page of a popular news aggregator. Somebody visible in the community tweeted about it too. What are they saying?</p><p>Your heart sinks.</p><p>It’s not that people <em>didn’t like</em> the project. You know it has tradeoffs and expected people to talk about them. But that’s not what happened.</p><p><strong>Instead, the comments are largely <em>irrelevant</em> to your idea.</strong></p><p>The top comment thread picks on the coding style in a README example. It turns into an argument about indentation with over a hundred replies and a brief history of how different programming languages approached formatting. There are obligatory mentions of gofmt and Python. Have you tried Prettier?</p></blockquote><p>…</p><blockquote><p><strong>Confused, you close the tab.</strong></p><p>What happened?</p><p>It might be that your idea is simply not as interesting as you thought. That happens. It might also be that you poorly explained it for a casual visitor.</p><p>However, there might be another reason why you didn’t get relevant feedback.</p><p><strong>We tend to discuss things that are easy to talk about.</strong></p></blockquote><p>I’ve come to accept that people on discussion boards (myself included) talk about whatever they were already thinking about when we see the title of a post, and not whatever the article itself proposes. It is what it is, and it’s not going to change any time soon.</p><p>I think Rachel’s <a href="https://rachelbythebay.com/w/2021/05/26/irc/">Run XOR Use rule</a> about running an IRC chat or message board also applies to writing XOR discussing a blog post. If you write the post, you have to be prepared for the discussion to go to something you yourself weren’t even thinking about.</p><p>In terms of Go, I’m about as happy with the language as I’ve ever been. It’s not as fast as Rust, but it already well outpaces my scaling needs, and without too much boilerplate or too much temptation to abstraction. I think the language team has good instincts, and they tend to move at a measured pace in the right direction. I’m excited to see what happens next.</p><p>It’s hard to make predictions, especially about the future. Will I still be using Go in a decade? I don’t know. A decade feels like a longer time in prospect than in retrospect. Maybe we will all just be spot checking the output of code generated by AI by then, <a href="https://blog.carlmjohnson.net/post/2016-04-09-alphago-and-our-dystopian-ai-future/">as grim as that sounds</a>. But for now at least, I’m happy to use it as my core language.</p><p>Here’s the original conclusion to <a href="https://blog.carlmjohnson.net/post/google-go-the-good-the-bad-and-the-meh/">the post</a> with my 2023 updates:</p><blockquote><p>Go is rad. <del>It’s not</del> [edit: <ins>It is</ins>] my everyday language (<del>that’s still</del> [edit: <ins>it used to be</ins>] Python), <del>but</del> [edit: <ins>and</ins>] it’s definitely a fun language to use, and one that would be great for doing big projects in. If you’re interested in learning about Go, I recommend <a href="http://tour.golang.org/">doing the tour</a> then <a href="http://golang.org/ref/spec">reading the spec</a> while you put test programs into the <a href="http://play.golang.org/">Playground</a>. The spec is very short and quite readable, so it’s a great way to learn Go.</p></blockquote><p>See you in 2033 for “Ten Years of ‘Ten Years of “Go” ’ ”.</p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Voder Speech Synthesizer (210 pts)]]></title>
            <link>https://griffin.moe/voder/</link>
            <guid>36771149</guid>
            <pubDate>Tue, 18 Jul 2023 12:29:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://griffin.moe/voder/">https://griffin.moe/voder/</a>, See on <a href="https://news.ycombinator.com/item?id=36771149">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <header>
      <h2>The Voder</h2>
      <img id="logo" src="https://griffin.moe/voder/img/logo_final.svg" alt="Voder Logo, as seen on the original 1939 World's Fair poster.">
    </header>
    <section>
      <p>
        The Voder was an early attempt at speech synthesis developed by Bell
        Telephone Laboratory for the 1939-40 New York World's Fair. Controlled
        by hand, the operator manually forms each syllable using complex button
        sequences and it would take about a year of practice to able to
        produce fluid speech.
      </p>
      <p>
        Helen Harper was one of the first people to figure out how to operate
        the Voder effectively, and was the live demonstrator at the World's
        Fair. She later went on to form a year-long course instructing women to
        use the Voder. Of 100 applied students, only 20 were able to graduate
        and match Harper's skills.
      </p>
      <p>
        This application puts you in the shoes of the few women capable of
        operating the Voder.
      </p>
      <p>
        <iframe title="YouTube embedded video demonstrating the original Voder at the 1939 World's Fair." width="560" height="315" src="https://www.youtube.com/embed/0rAyrmm7vv0" frameborder="0" allowfullscreen="">
        </iframe>
      </p>
      <p>
        Try create vowel formants pressing the button combinations below on
        your keyboard, or if you are using a mobile device scroll down and tap
        console display:
      </p>
      <ul>
        <li><kbd>D + V + J</kbd>: [ɑ] as in "father"</li>
        <li><kbd>A + K</kbd>: [ē] as in "heed"</li>
        <li><kbd>A + D</kbd>: [ōō] as in "pool"</li>
        <li><kbd>Space</kbd>: unvoiced noise</li>
      </ul>
      <p>
        Replicating the sequence "She saw me" like in the video above
        requires the following inputs with exact timing; experiment until it
        sounds right to you. The results will not sound exactly like the video
        because of subtle articulations the operator is performing for
        inflection and dynamics.
      </p>
      <dl>
        <dt>“She (shē)</dt>
        <dd>
          <ol>
            <li><kbd>Space</kbd>: [sh]</li>
            <li><kbd>A + K</kbd>: [ē]</li>
          </ol>
        </dd>
        <dt>saw (sô)</dt>
        <dd>
          <ol>
            <li><kbd>Space</kbd>: [s]</li>
            <li><kbd>D + V + J</kbd>: [ô]</li>
          </ol>
        </dd>
        <dt>me.” (mē)</dt>
        <dd>
          <ol>
            <li><kbd>A</kbd>: [m]</li>
            <li><kbd>A + K</kbd>: [ē]</li>
          </ol>
        </dd>
      </dl>
      <p>
        After you get the hang of the former example, try saying
        "She seesaws" by resequencing some of the formants.
      </p>
    </section>
    <section id="canvas-container">
      <canvas id="console-display" width="420" height="320">
        Your browser does not have the necessary support to run this
        application. You might also have Javascript disabled.
      </canvas>
      <p>
        <span id="mobile-warning">
          Rotate your device to landscape for an optimal experience.
        </span>
      </p>
    </section>
    
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Firefox has surpassed Chrome on Speedometer (1085 pts)]]></title>
            <link>https://treeherder.mozilla.org/perfherder/graphs?timerange=31536000&amp;series=mozilla-central,3735773,1,13&amp;series=mozilla-central,3412459,1,13</link>
            <guid>36770883</guid>
            <pubDate>Tue, 18 Jul 2023 12:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://treeherder.mozilla.org/perfherder/graphs?timerange=31536000&#x26;series=mozilla-central,3735773,1,13&#x26;series=mozilla-central,3412459,1,13">https://treeherder.mozilla.org/perfherder/graphs?timerange=31536000&#x26;series=mozilla-central,3735773,1,13&#x26;series=mozilla-central,3412459,1,13</a>, See on <a href="https://news.ycombinator.com/item?id=36770883">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Medicine is plagued by untrustworthy clinical trials (270 pts)]]></title>
            <link>https://www.nature.com/articles/d41586-023-02299-w</link>
            <guid>36770624</guid>
            <pubDate>Tue, 18 Jul 2023 11:27:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nature.com/articles/d41586-023-02299-w">https://www.nature.com/articles/d41586-023-02299-w</a>, See on <a href="https://news.ycombinator.com/item?id=36770624">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25769868.gif?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25769868.gif?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Conceptual illustration showing a detective with a torch investigating a suspicious person at a desk." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25769868.gif">
  <figcaption>
   <p><span>Illustration by Piotr Kowalczyk</span></p>
  </figcaption>
 </picture>
</figure><p>How many clinical-trial studies in medical journals are fake or fatally flawed? In October 2020, John Carlisle reported a startling estimate<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>Carlisle, an anaesthetist who works for England’s National Health Service, is <a href="https://www.nature.com/articles/d41586-019-02241-z" data-track="click" data-label="https://www.nature.com/articles/d41586-019-02241-z" data-track-category="body text link">renowned for his ability to spot dodgy data in medical trials</a>. He is also an editor at the journal <i>Anaesthesia</i>, and in 2017, he decided to scour all the manuscripts he handled that reported a randomized controlled trial (RCT) — the gold standard of medical research. Over three years, he scrutinized more than 500 studies<sup><a href="#ref-CR1" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">1</a></sup>.</p><p>For more than 150 trials, Carlisle got access to anonymized individual participant data (IPD). By studying the IPD spreadsheets, he judged that 44% of these trials contained at least some flawed data: impossible statistics, incorrect calculations or duplicated numbers or figures, for instance. And 26% of the papers had problems that were so widespread that the trial was impossible to trust, he judged — either because the authors were incompetent, or because they had faked the data.</p><p>Carlisle called these ‘zombie’ trials because they had the semblance of real research, but closer scrutiny showed they were actually hollow shells, masquerading as reliable information. Even he was surprised by their prevalence. “I anticipated maybe one in ten,” he says.</p><p>When Carlisle couldn’t access a trial’s raw data, however, he could study only the aggregated information in the summary tables. Just 1% of these cases were zombies, and 2% had flawed data, he judged (see ‘The prevalence of ‘zombie’ trials’). This finding alarmed him, too: it suggested that, without access to the IPD — which journal editors usually don’t request and reviewers don’t see — even an experienced sleuth cannot spot hidden flaws.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25770400.gif?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25770400.gif?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="The prevalence of 'zombie' trials. Bar chart showing the proportion of manuscripts with flawed data." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25770400.gif">
  <figcaption>
   <p><span>Source: Ref. 1</span></p>
  </figcaption>
 </picture>
</figure><p>“I think journals should assume that all submitted papers are potentially flawed and editors should review individual patient data before publishing randomised controlled trials,” Carlisle wrote in his report.</p><p>Carlisle rejected every zombie trial, but by now, almost three years later, most have been published in other journals — sometimes with different data to those submitted with the manuscript he had seen. He is writing to journal editors to alert them, but expects that little will be done.</p><p>Do Carlisle’s findings in anaesthesiology extend to other fields? For years, a number of scientists, physicians and data sleuths have argued that fake or unreliable trials are frighteningly widespread. They’ve scoured RCTs in various medical fields, such as women’s health, pain research, anaesthesiology, bone health and COVID-19, and have found dozens or hundreds of trials with seemingly statistically impossible data. Some, on the basis of their personal experiences, say that one-quarter of trials being untrustworthy might be an underestimate. “If you search for all randomized trials on a topic, about a third of the trials will be fabricated,” asserts Ian Roberts, an epidemiologist at the London School of Hygiene &amp; Tropical Medicine.</p><p>The issue is, in part, a subset of the <a href="https://www.nature.com/articles/d41586-021-00733-5" data-track="click" data-label="https://www.nature.com/articles/d41586-021-00733-5" data-track-category="body text link">notorious paper-mill problem</a>: over the past decade, journals in many fields have published tens of thousands of suspected fake papers, some of which are thought to have been produced by third-party firms, termed paper mills.</p><p>But faked or unreliable RCTs are a particularly dangerous threat. They not only are about medical interventions, but also can be laundered into respectability by being included in meta-analyses and systematic reviews, which thoroughly comb the literature to assess evidence for clinical treatments. Medical guidelines often cite such assessments, and physicians look to them when deciding how to treat patients.</p><p>Ben Mol, who specializes in obstetrics and gynaecology at Monash University in Melbourne, Australia, argues that as many as 20–30% of the RCTs included in systematic reviews in women’s health are suspect.</p><p>Many research-integrity specialists say that the problem exists, but its extent and impact are unclear. Some doubt whether the issue is as bad as the most alarming examples suggest. “We have to recognize that, in the field of high-quality evidence, we increasingly have a lot of noise. There are some good people championing that and producing really scary statistics. But there are also a lot in the academic community who think this is scaremongering,” says Žarko Alfirević, a specialist in fetal and maternal medicine at the University of Liverpool, UK.</p><p>This year, he and others are conducting more studies to assess how bad the problem is. Initial results from a study led by Alfirević are not encouraging.</p><h2><b>Laundering fake trials</b></h2><p>Medical research has always had fraudsters. Roberts, for instance, first came across the issue when he co-authored a 2005 systematic review for the Cochrane Collaboration, a prestigious group whose reviews of medical research evidence are often used to shape clinical practice. The review suggested that high doses of a sugary solution could reduce death after head injury. But Roberts retracted it<sup><a href="#ref-CR2" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">2</a></sup> after doubts arose about three of the key trials cited in the paper, all authored by the same Brazilian neurosurgeon, Julio Cruz. (Roberts never discovered whether the trials were fake, because Cruz died by suicide before investigations began. Cruz’s articles have not been retracted.)</p><p>A more recent example is that of Yoshihiro Sato, a Japanese bone-health researcher. Sato, who died in 2016, <a href="https://www.nature.com/articles/d41586-019-01884-2" data-track="click" data-label="https://www.nature.com/articles/d41586-019-01884-2" data-track-category="body text link">fabricated data in dozens of trials of drugs or supplements that might prevent bone fracture</a>. He has 113 retracted papers, according to a list compiled by the website <a href="https://retractionwatch.com/" data-track="click" data-label="https://retractionwatch.com/" data-track-category="body text link">Retraction Watch</a>. His work has had a wide impact: researchers found that 27 of Sato’s retracted RCTs had been cited by 88 systematic reviews and clinical guidelines, some of which had informed Japan’s recommended treatments for osteoporosis<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>Some of the findings in about half of these reviews would have changed had Sato’s trials been excluded, says Alison Avenell, a medical researcher at the University of Aberdeen, UK. She, along with medical researchers Andrew Grey, Mark Bolland and Greg Gamble, all at the University of Auckland in New Zealand, have pushed universities to investigate Sato’s work and monitored its influence. “It probably diverted people from being given more effective treatment for fracture prevention,” Avenell says.</p><figure>
 <picture>
  <source type="image/webp" srcset="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25601540.jpg?as=webp 767w, https://media.nature.com/lw319/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25601540.jpg?as=webp 319w" sizes="(max-width: 319px) 319px, (min-width: 1023px) 100vw,  767px">
  <img alt="Anaesthetist John Carlisle portrayed at work." loading="lazy" src="https://media.nature.com/lw767/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25601540.jpg">
  <figcaption>
   <p><span>Anaesthetist John Carlisle at work.</span><span>Credit: Emli Bendixen</span></p>
  </figcaption>
 </picture>
</figure><p>The concerns over zombie trials, however, are beyond individual fakers flying under the radar. In some fields, swathes of RCTs from different research groups might be unreliable, researchers worry.</p><p>During the pandemic, for instance, a flurry of RCTs was conducted into whether ivermectin, an anti-parasite drug, could treat COVID-19. But researchers who were not involved have since <a href="https://www.nature.com/articles/s41591-021-01535-y" data-track="click" data-label="https://www.nature.com/articles/s41591-021-01535-y" data-track-category="body text link">pointed out data flaws</a> in many of the studies, some of which have been retracted. A 2022 update of a Cochrane review argued that more than 40% of these RCTs were untrustworthy<sup><a href="#ref-CR4" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">4</a></sup>.</p><p>“Untrustworthy work must be removed from systematic reviews,” says Stephanie Weibel, a biologist at the University of Wuerzberg in Germany, who co-authored the review.</p><p>In maternal health — another field seemingly rife with problems — Roberts and Mol have flagged studies into whether a drug called tranexamic acid can stem dangerously heavy bleeding after childbirth. Every year, around 14 million people experience this condition, and some 70,000 die: it is the world’s leading cause of maternal death.</p><p>In 2016, Roberts reviewed evidence for using tranexamic acid to treat serious blood loss after childbirth. He reported that many of the 26 RCTs investigating the drug had serious flaws. Some had identical text, others had data inconsistencies or no records of ethical approval. Some seemed not to have adequately randomized the assignment of their participants to control and treatment groups<sup><a href="#ref-CR5" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">5</a></sup>.</p><p>When he followed up with individual authors to ask for more details and raw data, he generally got no response or was told that records were missing or had been lost because of computer theft. Fortunately, in 2017, a large, high-quality multi-centre trial, which Roberts helped to run, established that the drug was effective<sup><a href="#ref-CR6" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">6</a></sup>. It’s likely, says Roberts, that in these and other such cases, some of the dubious trials were copycat fraud — researchers saw that a large trial was going on and produced small, substandard copies that no one would question. This kind of fraud isn’t a victimless crime, however. “It results in narrowed confidence intervals such that the results look much more certain than they are. It also has the potential to amplify a wrong result, suggesting that treatments work when they don’t,” he says.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-022-00025-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_25770474.gif"><p>Stamp out fake clinical data by working together</p></a>
 </article><p>That might have happened for another question: what if doctors were to inject the drug into everyone undergoing a caesarean, just after they give birth, as a preventative measure? A 2021 review<sup><a href="#ref-CR7" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">7</a></sup> of 36 RCTs investigating this idea, involving a total of more than 10,000 participants, concluded that this would reduce the risk of heavy blood loss by 60%.</p><p>Yet this April, an enormous US-led RCT with 11,000 people reported only a slight and not statistically significant benefit<sup><a href="#ref-CR8" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">8</a></sup>.</p><p>Mol thinks problems with some of the 36 previous RCTs explains the discrepancy. The 2021 meta-analysis had included one multi-centre study in France of more than 4,000 participants, which found a modest 16% reduction in severe blood loss, and another 35 smaller, single-centre studies, mostly conducted in India, Iran, Egypt and China, which collectively estimated a 93% drop. Many of the smaller RCTs were untrustworthy, says Mol, who has dug into some of them in detail.</p><p>It’s unclear whether the untrustworthy studies affected clinical practice. The World Health Organization (WHO) recommends using tranexamic acid to treat blood loss after childbirth, but it doesn’t have a guideline on preventive administration.</p><h2><b>From four trials to one</b></h2><p>Mol points to a different example in which untrustworthy trials might have influenced clinical practice. In 2018, researchers published a Cochrane review<sup><a href="#ref-CR9" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">9</a></sup> on whether giving steroids to people due to undergo caesarean-section births helped to reduce breathing problems in their babies. Steroids are good for a baby’s lungs but can harm the developing brain, says Mol; benefits generally outweigh harms when babies are born prematurely, but the balance is less clear when steroids are used later in pregnancy.</p><p>The authors of the 2018 review, led by Alexandros Sotiriadis, a specialist in maternal–fetal medicine at the Aristotle University of Thessaloniki in Greece, analysed the evidence for administering steroids to people delivering by caesarean later in pregnancy. They ended up with four RCTs: a British study from 2005 with more than 940 participants, and three Egyptian trials conducted between 2015 and 2018 that added another 3,000 people into the evidence base. The review concluded that the steroids “may” reduce rates of breathing problems; it was cited in more than 200 documents and some clinical guidelines.</p><p>In January 2021, however, Mol and others, who had looked in more depth into the papers, raised concerns about the Egyptian trials. The largest study, with nearly 1,300 participants, was based on the second author’s thesis, he noted — but the trial end dates in the thesis differed from the paper. And the reported ratio of male to female babies was an impossible 40% to 60%. Mol queried the other papers, too, and wrote to the authors, but says he did not get satisfactory replies. (One author told him he’d lost the data when moving house.) Mol’s team also reported statistical issues with some other works by the same authors.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-019-02241-z" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_17403998.jpg"><p>How a data detective exposed suspicious medical trials</p></a>
 </article><p>In December 2021, Sotiriadis’s team updated its review<sup><a href="#ref-CR10" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">10</a></sup>. But this time, it adopted a new screening protocol. Until that year, Cochrane reviews had aimed to include all relevant RCTs; if researchers spotted potential issues with a trial, using a ‘risk of bias’ checklist, they would downgrade their confidence in its findings, but not remove it from their analysis. But in 2021, Cochrane’s research-integrity team introduced new guidance: authors should try to identify ‘problematic’ or ‘untrustworthy’ trials and exclude them from reviews. Sotiriadis’s group now excluded all but the British research. With only one trial left, there was “insufficient data” to draw firm conclusions about the steroids, the researchers said.</p><p>By last May, <a href="https://retractionwatch.com/2022/05/08/journal-retracts-c-section-paper-with-impossible-data" data-track="click" data-label="https://retractionwatch.com/2022/05/08/journal-retracts-c-section-paper-with-impossible-data" data-track-category="body text link">as Retraction Watch reported</a>, the large Egyptian trial was retracted (to the disagreement of its authors). The journal’s editors wrote in the retraction notice that they had not received its data or a satisfactory response from the authors, adding that “if the data is unreliable, women and babies are being harmed”. The other two trials are still under investigation by publisher Taylor &amp; Francis as part of a larger case of papers, says Sabina Alam, director of publishing ethics at the firm. Before the 2018 review, some clinical guidelines had suggested that administering steroids later in pregnancy could be beneficial, and the practice had been growing in some countries, such as Australia, Mol has reported. The latest updated WHO and regional guidelines, however, recommend against this practice.</p><p>Overall, Mol and his colleagues have alleged problems in more than 800 published medical research papers, at least 500 of which are on RCTs. So far, the work has led to more than 80 retractions and 50 expressions of concern. Mol has focused much of his work on papers from countries in the Middle East, and particularly in Egypt. One researcher responded to some of his e-mails by accusing him of racism. Mol, however, says that it’s simply a fact that he has encountered many suspect statistics and refusals to share data from RCT authors in countries such as Iran, Egypt, Turkey and China — and that he should be able to point that out.</p><h2><b>Screening for trustworthiness</b></h2><p>“Ben Mol has undoubtedly been a pioneer in the field of detecting and fighting data falsification,” says Sotiriadis — but he adds that it is difficult to prove that a paper is falsified. Sotiriadis says he didn’t depend on Mol’s work when his team excluded those trials in its update, and he can’t say whether the trials were corrupt.</p><p>Instead, his group followed a screening protocol designed to check for ‘trustworthiness’. It had been developed by one of Cochrane’s independent specialist groups, the Cochrane Pregnancy and Childbirth (CPC) group, coordinated by Alfirević. (This April, Cochrane formally dissolved this group and some others, as part of a reorganization strategy.) It provides a detailed list of criteria that authors should follow to check the trustworthiness of an RCT — such as whether a trial is prospectively registered and whether the study is free of unusual statistics, such as implausibly narrow or wide distributions of mean values in participant height, weight or other characteristics, and other red flags. If RCTs fail the checks, then reviewers are instructed to contact the original study authors — and, if the replies are not adequate, to exclude the study.</p><p>“We’re championing the idea that, if a study doesn’t pass these bars, then no hard feelings, but we don’t call it trustworthy enough,” Alfirević explains.</p><p>For Sotiriadis, the merit of this protocol was that it avoided his having to declare the trials faulty or fraudulent; they had merely failed a test of trustworthiness. His team ultimately reported that it excluded the Egyptian trials because they hadn’t been prospectively registered and the authors didn’t explain why.</p><p>Other Cochrane authors are starting to adopt the same protocol. For instance, a review<sup><a href="#ref-CR11" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">11</a></sup> of drugs aiming to prevent pre-term labour, published last August, used it to exclude 44 studies — one-quarter of the 122 trials in the literature.</p><h2><b>What counts as trustworthy?</b></h2><p>Whether trustworthiness checks are sometimes unfair to the authors of RCTs, and exactly what should be checked to classify untrustworthy research, is still up for debate. In a 2021 editorial<sup><a href="#ref-CR12" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">12</a></sup> introducing the idea of trustworthiness screening, Lisa Bero, a senior research integrity editor at Cochrane, and a bioethicist at the University of Colorado Anschutz Medical Campus in Aurora, pointed out that there was no validated, universally agreed method.</p><p>“Misclassification of a genuine study as problematic could result in erroneous review conclusions. Misclassification could also lead to reputational damage to authors, legal consequences, and ethical issues associated with participants having taken part in research, only for it to be discounted,” she and two other researchers wrote.</p><p>For now, there are multiple trustworthiness protocols in play. In 2020, for instance, Avenell and others <a href="https://www.nature.com/articles/d41586-019-03959-6" data-track="click" data-label="https://www.nature.com/articles/d41586-019-03959-6" data-track-category="body text link">published REAPPRAISED</a>, a checklist aimed more at journal editors. And when Weibel and others reviewed trials investigating ivermectin as a COVID-19 treatment last year, they created their own checklist, which they call a ‘research integrity assessment’.</p><p>Bero says some of these checks are more labour-intensive than editors and systematic reviewers are generally accustomed to. “We need to convince systematic reviewers that this is worth their time,” she says. She and others have consulted biomedical researchers, publishers and research-integrity experts to come up with a set of red flags that might serve as the basis for creating a widely agreed method of assessment.</p><p>Despite the concerns of researchers such as Mol, many scientists remain unsure how many reviews have been compromised by unreliable RCTs. This year, a team led by Jack Wilkinson, a health researcher at the University of Manchester, UK, is using the results of Bero’s consultation to apply a list of 76 trustworthiness checks to all trials cited in 50 published Cochrane reviews. (The 76 items include detailed examination of the data and statistics in trials, as well as inspecting details on funding, grants, trial registration, the plausibility of study methods and authors’ publication records — but, in this exercise, data from individual participants are not being requested.)</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-019-03959-6" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_17650444.jpg"><p>Check for publication integrity before misconduct</p></a>
 </article><p>The aim is to see how many RCTs fail the checks, and what impact removing those trials would have on the reviews’ conclusions. Wilkinson says a team of 50 is working on the project. He aims to produce a general trustworthiness-screening tool, as well as a separate tool to aid in inspecting participant data, if authors provide them. He will discuss the work in September at Cochrane’s annual colloquium.</p><p>Alfirević’s team, meanwhile, has found in <a href="https://events.cochrane.org/colloquium-2023/session/1473810/assessment-of-trustworthiness-has-a-significant-impact-on-conclusions-of-cochrane-reviews" data-track="click" data-label="https://events.cochrane.org/colloquium-2023/session/1473810/assessment-of-trustworthiness-has-a-significant-impact-on-conclusions-of-cochrane-reviews" data-track-category="body text link">a study yet to be published</a> that 25% of around 350 RCTs in 18 Cochrane reviews on nutrition and pregnancy would have failed trustworthiness checks, using the CPC’s method. With these RCTs excluded, the team found that one-third of the reviews would require updating because their findings would have changed. The researchers will report more details in September.</p><p>In Alfirević’s view, it doesn’t particularly matter which trustworthiness checks reviewers use, as long as they do something to scrutinize RCTs more closely. He warns that the numbers of systematic reviews and meta-analyses that journals publish have themselves been soaring in the past decade — and many of these reviews can’t be trusted because of shoddy screening methods. “An untrustworthy systematic review is far more dangerous than an untrustworthy primary study,” he says. “It is an industry that is completely out of hand, with little quality assurance.”</p><p>Roberts, who first published in 2015 his concerns over problematic medical research in systematic reviews<sup><a href="#ref-CR13" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">13</a></sup>, says that the Cochrane organization took six years to respond and still isn’t taking the issue seriously enough. “If up to 25% of trials included in systematic reviews are fraudulent, then the whole Cochrane endeavour is suspect. Much of what we think we know based on systematic reviews is wrong,” he says.</p><p>Bero says that Cochrane consulted widely to develop its 2021 guide on addressing problematic trials, including incorporating suggestions from Roberts, other Cochrane reviewers and research-integrity experts.</p><h2><b>Asking for data</b></h2><p>Many researchers worried by medical fakery agree with Carlisle that it would help if journals routinely asked authors to share their IPD. “Asking for raw data would be a good policy. The default position has just been to trust the study, but we’ve been operating from quite a naive position,” says Wilkinson. That advice, however, runs counter to current practice at most medical journals.</p><p>In 2016, the International Committee of Medical Journal Editors (ICMJE), an influential body that sets policy for many major medical titles, had proposed requiring mandatory data-sharing from RCTs. But it got pushback — including over perceived risks to the privacy of trial participants who might not have consented to their data being shared, and the availability of resources for archiving the data. As a result, <a href="https://www.icmje.org/news-and-editorials/data_sharing_june_2017.pdf" data-track="click" data-label="https://www.icmje.org/news-and-editorials/data_sharing_june_2017.pdf" data-track-category="body text link">in the latest update to its guidance</a>, in 2017, it settled for merely encouraging data sharing and requiring statements about whether and where data would be shared.</p><article data-label="Related">
  <a href="https://www.nature.com/articles/d41586-021-00733-5" data-track="click" data-track-label="recommended article"><img alt="" src="https://media.nature.com/w400/magazine-assets/d41586-023-02299-w/d41586-023-02299-w_18989362.jpg"><p>The fight against fake-paper factories that churn out sham science</p></a>
 </article><p>The ICMJE secretary, Christina Wee, says that “there are major feasibility challenges” to be resolved to mandate IPD sharing, although the committee might revisit its practices in future. Many publishers of medical journals told <i>Nature</i>’s news team that, following ICMJE advice, they didn’t require IPD from authors of trials. (These publishers included Springer Nature; <i>Nature</i>’s news team is editorially independent.)</p><p>Some journals, however — including Carlisle’s <i>Anaesthesia</i> — have gone further and do already require IPD. “Most authors provide the data when told it is a requirement,” Carlisle says.</p><p>Even when IPD are shared, says Wilkinson, scouring it in the way that Carlisle does is a time-consuming exercise — creating a further burden for reviewers — although computational checks of statistics might help.</p><p>Besides asking for data, journal editors could also speed up their decision-making, research-integrity specialists say. When sleuths raise concerns, editors should be prepared to put expressions of concern on medical studies more quickly if they don’t hear back from authors, Avenell says. This April, a <a href="https://committees.parliament.uk/publications/39343/documents/194466/default" data-track="click" data-label="https://committees.parliament.uk/publications/39343/documents/194466/default" data-track-category="body text link">UK parliamentary report into reproducibility and research integrity</a> said that it should not take longer than two months for publishers to publish corrections or retractions of research when academics raise issues.</p><p>And if journals do retract studies, authors of systematic reviews should be required to correct their work, Avenell and others say. This rarely happens. Last year, for instance, Avenell’s team reported that it had carefully and repeatedly e-mailed authors and journal editors of the 88 reviews that cited Sato’s retracted trials to inform them that their reviews included retracted work. They got few responses — only 11 of the 88 reviews have been updated so far — suggesting that authors and editors didn’t generally care about correcting the reviews<sup><a href="#ref-CR3" data-track="click" data-action="anchor-link" data-track-label="go to reference" data-track-category="references">3</a></sup>.</p><p>That was dispiriting but not surprising to the team, which <a href="https://www.nature.com/articles/d41586-019-01884-2" data-track="click" data-label="https://www.nature.com/articles/d41586-019-01884-2" data-track-category="body text link">has previously recounted how institutional investigations into Sato’s work were opaque and inadequate</a>. The Cochrane collaboration, for its part, stated in updated guidance in 2021 that systematic reviews must be updated when retractions occur.</p><p>Ultimately, a lingering question is — as with paper mills — why so many suspect RCTs are being produced in the first place. Mol, from his experiences investigating the Egyptian studies, blames lack of oversight and superficial assessments that promote academics on the basis of their number of publications, as well as the lack of stringent checks from institutions and journals on bad practices. Egyptian authorities have taken some steps to improve governance of trials, however; Egypt’s parliament, for instance, published its first clinical research law in December 2020.</p><p>“The solution’s got to be fixes at the source,” says Carlisle. “When this stuff is churned out, it’s like fighting a wildfire and failing.”</p>
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linux has nearly half of the desktop OS Linux market (101 pts)]]></title>
            <link>https://www.theregister.com/2023/07/18/linux_desktop_debate/</link>
            <guid>36770320</guid>
            <pubDate>Tue, 18 Jul 2023 10:46:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2023/07/18/linux_desktop_debate/">https://www.theregister.com/2023/07/18/linux_desktop_debate/</a>, See on <a href="https://news.ycombinator.com/item?id=36770320">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Opinion</span> Linux is now a little more than three 3 per cent of the desktop OS market, excluding the just over <em>four</em> per cent that is ChromeOS. Which is <em>also</em> Linux, but the <em>wrong kind</em> of Linux.</p>
<p>Web server statistics aggregator Statcounter announced last week that as of June 2023, Linux <a href="https://gs.statcounter.com/os-market-share/desktop/worldwide" rel="nofollow">accounts</a> for 3 per cent of desktop operating system use. However, this is still surpassed by ChromeOS, which means that desktop Linux has less than half of the desktop Linux market. If you feel that this is a bit weird, we agree with you.</p>
<p>Apparently, desktop Linux use measures 3.08 per cent, lagging about a quarter behind the usage of ChromeOS at 4.15 per cent. The problem with this is that ChromeOS is <em>also</em> a Linux distribution. It's a strange distro, non-standard in several ways, but current versions are <a href="https://www.theregister.com/2023/02/14/chromeos_opinion_column/">built on the basis of Gentoo Linux</a>, switching from an Ubuntu basis some years earlier.</p>

    

<p>We feel that a more accurate reckoning would be that Linux has now reached 7.23 per cent of Statcounter's usage figures, with ChromeOS at just over half: 57.4 per cent of the total. That seems like a more positive interpretation, one that Linux fans would be keen to make, but apparently not. By example, Linux advocacy site Linuxiac doesn't even <a href="https://linuxiac.com/linux-hits-3-percent-market-share/" rel="nofollow">mention</a> ChromeOS in its write-up.</p>

        


        

<p>(As an aside, we suspect that quite a lot of the 3.23 per cent of OSes grouped under "Unknown" are probably <em>also</em> Linux users, just extra-paranoid ones who are obscuring their user-agent or something.)</p>
<p>Why <em>shouldn't</em> ChromeOS count? It's a Linux kernel and a Linux userland on top of the standard <code>glibc</code> C library. You can open a shell. If you want, you can <a href="https://www.theregister.com/2018/04/27/linux_vms_on_chrome_os/">run a Debian container</a> and once you're in there install and run any Debian app; <em>The Reg</em> FOSS desk's experimental ChromeOS Flex machine runs Firefox and DOSemu.</p>

        

<p>If someone said "Android is not a Linux," that is defensible. You can't easily download it for free, and you can't run it on your own generic off-the-shelf computer. There have been a few experimental Android-based desktop OSes, but so far they've all flopped. On its native platform of smartphones and tablets, you can't run ordinary Linux apps on Android. It's a different sort of beast, even though <em>technically</em> it is a Linux inasmuch as it has a Linux kernel. But that's about it. It even has a weird, non-standard, non-GPL <code>libc</code>, "Bionic". By default and unless cracked, aside from the kernel, it has nothing else Linux-like about it. No shell, no desktop, no X11 or Wayland, nothing.</p>
<p>But that's not the case for ChromeOS. Underneath its unique GUI layer – which, unlike the one in macOS, is <a href="https://chromium.googlesource.com/chromium/src.git/+/lkgr/ash/" rel="nofollow">open source</a> – it's a relatively standard Linux which can run standard Linux apps, out of the box, on both x86 and Arm. As such, it's the most successful desktop Linux there is.</p>
<p>It's not a <em>typical</em> Linux, because typical Linuxes are tools for nerdy hacker types, and that kind of OS will never, ever go mainstream unless someone forces people to use it. (As the <a href="https://www.theregister.com/2022/07/03/china_openkylin/">government of the People's Republic of China is currently doing</a>, but that is irrelevant to this. No Google inside the Great Firewall means no ChromeOS.)</p>

        

<p>ChromeOS is a desktop Linux with the Linuxiness stripped out. No choice about partitioning. No weird dual-boot mechanisms. No choice of desktops or package managers. No package manager!</p>
<p>But in every way that matters, it is mainstream, it is commercially successful, it's a polished end-user desktop OS, and it's a Linux.</p>
<p>So naturally the Forces of FOSS hate it. Of <em>course</em> they do. And how do they express that contempt? By saying it's not a True Linux.</p>
<p>Unix is like a religion: somehow, it encourages schisms and splinter sects, all of whom deny that the others are legitimate. It's almost a defining characteristic.</p>
<p>Ignoring all the commercial Unixes as <a href="https://www.theregister.com/2023/01/17/unix_is_dead/">they are effectively all dead now</a>, and just looking at the FOSS ones, there are about a dozen rival sects: NetBSD, FreeBSD, OpenBSD, DragonflyBSD, Minix, HURD and L4 and its <a href="https://www.theregister.com/2022/02/24/neptune_os_sel4_windows/">various splinter groups</a>, Plan 9 (<a href="https://www.theregister.com/2022/11/02/plan_9_fork_9front/">9front</a>, HarveyOS, Jeanne etc.), Inferno, <a href="https://github.com/mit-pdos/xv6-public" rel="nofollow">xv6</a>, <a href="https://www.nordier.com/" rel="nofollow">v7/86</a>, and of course, Linux and its thousand distributions.</p>
<ul>

<li><a href="https://www.theregister.com/2023/07/17/almalinux_project_switches_focus/">AlmaLinux project climbs down from being a one-to-one RHEL clone</a></li>

<li><a href="https://www.theregister.com/2023/07/13/wayland_is_coming/">Three signs that Wayland is becoming the favored way to get a GUI on Linux</a></li>

<li><a href="https://www.theregister.com/2023/07/12/suse_announces_rhel_fork/">SUSE announces its own RHEL-compatible distro... again</a></li>

<li><a href="https://www.theregister.com/2023/07/10/oracle_ibm_rhel_code/">Oracle pours fuel all over Red Hat source code drama</a></li>
</ul>
<p>Precisely two (2) Linux-based OSes have enjoyed large-scale commercial success as user-facing GUI systems for non-technical users. One has <a href="https://www.theregister.com/2012/06/11/android_activation_nears_one_million_daily/">billions of users</a>, the other a <a href="https://www.theregister.com/2021/05/05/chromebook_shipments_canalys_figures/">significant fraction of a billion</a>. Both are from Google, and both share a defining attribute: the FOSS world rejects them.</p>
<p>ChromeOS comes in two flavors: there's ordinary ChromeOS, which you can only get by buying hardware built to run it (just like Apple's macOS), and there's <a href="https://www.theregister.com/2022/02/16/google_chrome_os/">ChromeOS Flex</a>. Flex <a href="https://www.theregister.com/2022/02/16/google_chrome_os/">used to be called Neverware Cloudready</a>. Neverware grew from <a href="https://www.theregister.com/2010/11/08/google_chrome_os_is_not_android/">Hexxeh's remixed and rebuilt ChromiumOS</a> for ordinary PCs. Hexxeh made ChromeOS Flow, and that is the direct grandparent of ChromeOS Flex: both are ChromeOS for generic PC hardware. That is the significant angle. It shows that ChromeOS is just another Linux distro.</p>
<p>ChromeOS Flex is not like Android. It is a Linux in every way that matters. It's both small-f free and it's <a href="https://www.chromium.org/chromium-os/" rel="nofollow">open source</a>; there have been multiple remixes and rebuilds. It runs on generic kit with ordinary BIOS or UEFI, including with Secure Boot. It has a desktop, albeit its own unique one. You can pop a shell and install and run any arbitrary Linux app.</p>
<p>It looks like a Linux, it acts like a Linux, and it runs like a Linux. It is based on a generic Linux kernel, and it does Linuxy stuff with Linuxy binaries like any other desktop Linux.</p>
<p>It doesn't have systemd, but thank the great god Torvalds and his apostle St Cox that is not yet a requirement for a Linux distro. It has <code>upstart</code>, which was one of the most widespread init systems.</p>
<p>It is 100 percent on-brand for the Linux world that when one specific form of Linux-based OS goes mainstream, and is used by <a href="https://www.bankmycell.com/blog/how-many-android-users-are-there" rel="nofollow">approximately half</a> the human race, the True Believers of the Linux world disown it. Android is not a Linux. OK, they kind of have a point.</p>
<p>But ChromeOS, and especially ChromeOS Flex? Of course, all the advocates decry it as not being a True Linux, but then again, this just quickly descends into a <a href="https://www.scribbr.com/fallacies/no-true-scotsman-fallacy/" rel="nofollow">"no true Scotsman"</a> argument. As it is, the Fedora lot think Ubuntu is junk, and the Debianisti think everything else is junk, and the Arch folk think they are the true cutting edge, <a href="http://www.slackware.com/changelog/current.php?cpu=x86_64" rel="nofollow">Slackware</a> enthusiasts consider everyone else newbies, while the NixOS folk think all the rest are still in the stone age somewhere… ®</p>                                
                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft lost its keys, and the government got hacked (252 pts)]]></title>
            <link>https://techcrunch.com/2023/07/17/microsoft-lost-keys-government-hacked/</link>
            <guid>36770235</guid>
            <pubDate>Tue, 18 Jul 2023 10:30:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2023/07/17/microsoft-lost-keys-government-hacked/">https://techcrunch.com/2023/07/17/microsoft-lost-keys-government-hacked/</a>, See on <a href="https://news.ycombinator.com/item?id=36770235">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Microsoft still doesn’t know — or want to share — how China-backed hackers stole a key that allowed them to stealthily break into dozens of email inboxes, including those belonging to<a href="https://techcrunch.com/2023/07/12/chinese-hackers-us-government-microsoft-email/"> several federal government agencies</a>.</p>
<p>In<a href="https://www.microsoft.com/en-us/security/blog/2023/07/14/analysis-of-storm-0558-techniques-for-unauthorized-email-access/" target="_blank" rel="noopener"> a blog post</a> Friday, Microsoft said it was a matter of “ongoing investigation” how the hackers obtained a Microsoft signing key that was abused to forge authentication tokens that allowed the hackers’ access to inboxes as if they were the rightful owners. Reports say targets include U.S. Commerce Secretary Gina Raimondo, U.S. State Department officials and other organizations not yet publicly revealed.</p>
<p>Microsoft disclosed the incident last Tuesday, attributing the month-long activity to a newly discovered espionage group it calls Storm-0558, which it believes has a strong nexus to China. U.S. cybersecurity agency CISA said the hacks, which began in mid-May, included <a href="https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-193a" target="_blank" rel="noopener">a small number</a> of government accounts said to be in the single digits and that the hackers exfiltrated some unclassified email data. While the U.S. government has not publicly attributed the hacks, China’s top foreign ministry spokesperson denied the allegations on Wednesday.</p>
<p>Where China has used previously unknown vulnerabilities to <a href="https://techcrunch.com/2021/03/02/microsoft-says-china-backed-hackers-are-exploiting-exchange-zero-days/">individually hack into Microsoft-powered email servers</a> to steal corporate data, this hacking group instead went directly to the source by targeting new and undisclosed vulnerabilities in Microsoft’s cloud.</p>
<p>In its blog post, Microsoft said the hackers acquired one of its consumer signing keys, or MSA key, which the company uses to secure consumer email accounts, like for accessing Outlook.com. Microsoft said it initially thought the hackers were forging authentication tokens using an acquired enterprise signing key, which are used to secure corporate and enterprise email accounts. But, Microsoft found that the hackers were using that consumer MSA key to forge tokens that allowed them to break into enterprise inboxes. Microsoft said this was because of a “validation error in Microsoft code.”</p>
<p>Microsoft said it has blocked “all actor activity” related to this incident, suggesting that the incident is over and that the hackers lost access. Though it’s unclear how Microsoft lost control of its own keys, the company said it’s hardened its key issuance systems, presumably to prevent hackers from churning out another digital skeleton key.</p>
<p>The hackers made one key mistake. By using the same key to raid several inboxes, Microsoft said this allowed investigators “to see all actor access requests which followed this pattern across both our enterprise and consumer systems.” To wit, Microsoft knows who was compromised and said it notified those affected.</p>
<p>With the immediate threat thought to be over, Microsoft now faces scrutiny for its handling of the incident, thought to be the biggest breach of unclassified government data since<a href="https://techcrunch.com/2020/12/17/fireeye-breach-solarwinds-federal-agencies/"> the Russian espionage campaign that hacked SolarWinds</a> in 2020.</p>
<p>As noted by <a href="https://arstechnica.com/security/2023/07/microsoft-takes-pains-to-obscure-role-in-0-days-that-caused-email-breach/" target="_blank" rel="noopener">Ars Technica’s Dan Goodin</a>, Microsoft went to great lengths to do damage control in its blog post, avoiding terms like “zero-day,” referring to when a software maker has zero days notice to fix a vulnerability that has already been exploited. Whether or not the bug or its exploitation fits everyone’s definition of a zero-day, Microsoft went out of its way to avoid describing it as such, or even to call it a vulnerability.</p>
<p>Compounding the key leak and its misuse was a lack of visibility into the intrusions by the government departments themselves. Microsoft is also taking heat for reserving security logs for the government accounts with the company’s top-tier package that may have helped other incident responders identify malicious activity.</p>
<p>CNN <a href="https://www.cnn.com/2023/07/12/politics/china-based-hackers-us-government-email-intl-hnk/index.html" target="_blank" rel="noopener">first reported</a> that the State Department initially detected the breach and reported it to Microsoft. But not every government department had the same level of security logging, which according to The <a href="https://www.wsj.com/articles/china-hacking-was-undetectable-for-some-who-had-less-expensive-microsoft-services-58730629" target="_blank" rel="noopener">Wall Street Journal</a> was available to departments with higher-paid tier Microsoft accounts but not others. Mary Jo Foley, editor in chief of Directions on Microsoft, a consultancy firm for Microsoft customers, said in <a href="https://www.directionsonmicrosoft.com/blog/2023-07-17/microsofts-latest-email-hack-m365-e3-subscribers-beware" target="_blank" rel="noopener">a blog post Monday</a>&nbsp;that the lower government tier offers some logging, but “does not keep track of specific mailbox data which would have revealed the attack.” A CISA official criticized the lack of available logging in a call with reporters last week. Microsoft told the Journal that it was “evaluating feedback.”</p>
<p>Microsoft’s expanded disclosure on Friday offered a glimmer of more technical details and indicators of compromise that incident responders can check if their networks were targeted, the technology giant still has questions to answer. Whether or not Microsoft has the answers ready, it’s not likely to be an investigation the technology giant can shake any time soon.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Swig – Connect C/C++ programs with high-level programming languages (102 pts)]]></title>
            <link>https://www.swig.org/</link>
            <guid>36769912</guid>
            <pubDate>Tue, 18 Jul 2023 09:37:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.swig.org/">https://www.swig.org/</a>, See on <a href="https://news.ycombinator.com/item?id=36769912">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<h2>Welcome to SWIG</h2>

<p>
SWIG is a software development tool that connects programs written in
C and C++ with a variety of high-level programming
languages.  SWIG is used with different types of target languages including common scripting languages such as
Javascript, Perl, PHP, Python, Tcl and Ruby. The list of 
<a href="https://www.swig.org/compat.html#SupportedLanguages">supported languages</a> also includes 
non-scripting languages such as C#, D, Go language,
Java including Android, Lua, OCaml, Octave, Scilab and R.
Also several interpreted and compiled Scheme implementations (Guile, MzScheme/Racket)
are supported. SWIG is most
commonly used to create high-level interpreted or compiled programming
environments, user interfaces, and as a tool for testing and prototyping C/C++ software. 
SWIG is typically used to parse C/C++ interfaces and generate the 'glue code' required for the above target languages to call into the C/C++ code.
SWIG can also export its parse tree in the form of XML.
SWIG is free software and the code that SWIG generates is compatible with both commercial and non-commercial projects.

</p><ul>
<li> <a href="https://www.swig.org/survey.html">Download</a> the latest version.
</li><li> <a href="https://www.swig.org/doc.html">Documentation, papers, and presentations</a>
</li><li> <a href="https://www.swig.org/compare.html">Features</a>.
</li><li> <a href="https://www.swig.org/mail.html">Mailing Lists</a>
</li><li> <a href="https://www.swig.org/bugs.html">Bug tracking</a>
</li><li> <a href="https://github.com/swig/swig/wiki">SwigWiki!</a>
</li></ul>

<h3>
Recent News
<a href="https://sourceforge.net/p/swig/news/feed"><img src="https://www.swig.org/images/rss16px.png" alt="SWIG RSS Feed"></a>
</h3>


    
    <dl><dt><b>2022/11/30</b> - <a href="https://sourceforge.net/p/swig/news/2022/11/-swig-411-released/"> SWIG-4.1.1 released</a></dt><dd><div><p>SWIG-4.1.1 summary:</p>
<ul>
<li>Couple of stability fixes.</li>
<li>Stability fix in ccache-swig when calculating hashes of inputs.</li>
<li>Some template handling improvements.</li>
<li>R - minor fixes plus deprecation for rtypecheck typemaps being optional.</li>
</ul></div></dd><dt><b>2022/10/24</b> - <a href="https://sourceforge.net/p/swig/news/2022/10/swig-410-released/">SWIG-4.1.0 released</a></dt><dd><div><p><strong>SWIG-4.1.0 summary:</strong></p>
<ul>
<li>Add Javascript Node v12-v18 support, remove support prior to v6.</li>
<li>Octave 6.0 to 6.4 support added.</li>
<li>Add PHP 8 support.</li>
<li>PHP wrapping is now done entirely via PHP's C API - no more .php wrapper.</li>
<li>Perl 5.8.0 is now the oldest version SWIG supports.</li>
<li>Python 3.3 is now the oldest Python 3 version SWIG supports.</li>
<li>Python 3.9-3.11 support added.</li>
<li>Various memory leak fixes in Python generated code.</li>
<li>Scilab 5.5-6.1 support improved.</li>
<li>Many improvements for each and every target language.</li>
<li>Various preprocessor expression handling improvements.</li>
<li>Improved C99, C++11, C++14, C++17 support. Start adding C++20 standard.</li>
<li>Make SWIG much more move semantics friendly.</li>
<li>Add C++ std::unique_ptr support.</li>
<li>Few minor C++ template handling improvements.</li>
<li>Various C++ using declaration fixes.</li>
<li>Few fixes for handling Doxygen comments.</li>
<li>GitHub Actions is now used instead of Travis CI for continuous integration.</li>
<li>Add building SWIG using CMake as a secondary build system.</li>
<li>Update optional SWIG build dependency for regex support from PCRE to PCRE2.</li>
</ul></div></dd><dt><b>2020/06/08</b> - <a href="https://sourceforge.net/p/swig/news/2020/06/swig-402-released/">SWIG-4.0.2 released</a></dt><dd><div><p><strong>SWIG-4.0.2 summary:</strong></p>
<ul>
<li>A few fixes around doxygen comment handling.</li>
<li>Ruby 2.7 support added.</li>
<li>Various minor improvements to C#, D, Java, OCaml, Octave, Python, R, Ruby.</li>
<li>Considerable performance improvement running SWIG on large interface files.</li>
</ul></div></dd><dt><b>2019/08/21</b> - <a href="https://sourceforge.net/p/swig/news/2019/08/swig-401-released/">SWIG-4.0.1 released</a></dt><dd><div><p>SWIG-4.0.1 summary:</p>
<ul>
<li>SWIG now cleans up on error by removing all generated files.</li>
<li>Add Python 3.8 support.</li>
<li>Python Sphinx compatibility added for Doxygen comments.</li>
<li>Some minor regressions introduced in 4.0.0 were fixed.</li>
<li>Fix some C++17 compatibility problems in Python and Ruby generated code.</li>
<li>Minor improvements/fixes for C#, Java, Javascript, Lua, MzScheme, Ocaml, Octave and Python.</li>
</ul></div></dd><dt><b>2019/04/28</b> - <a href="https://sourceforge.net/p/swig/news/2019/04/swig-400-released/">SWIG-4.0.0 released</a></dt><dd><div><p>SWIG-4.0.0 summary</p>
<ul>
<li>Support for Doxygen documentation comments which are parsed and converted into JavaDoc or PyDoc comments.</li>
<li>STL wrappers improved for C#, Java and Ruby.</li>
<li>C++11 STL containers added for Java, Python and Ruby.</li>
<li>Improved support for parsing C++11 and C++14 code.</li>
<li>Various fixes for shared_ptr.</li>
<li>Various C preprocessor corner case fixes.</li>
<li>Corner case fixes for member function pointers.</li>
<li>Python module overhaul by simplifying the generated code and turning most optimizations on by default.</li>
<li>%template improvements wrt scoping to align with C++ explicit template instantiations.</li>
<li>Added support for a command-line options file (sometimes called a response file).</li>
<li>Numerous enhancements and fixes for all supported target languages.</li>
<li>SWIG now classifies the status of target languages into either 'Experimental' or 'Supported' to indicate the expected maturity level.</li>
<li>Support for CFFI, Allegrocl, Chicken, CLISP, S-EXP, UFFI, Pike, Modula3 has been removed.</li>
<li>Octave 4.4-5.1 support added.</li>
<li>PHP5 support removed, PHP7 is now the supported PHP version.</li>
<li>Minimum Python version required is now 2.7, 3.2-3.7 are the only other versions supported.</li>
<li>Added support for Javascript NodeJS versions 2-10.</li>
<li>OCaml support is much improved and updated, minimum OCaml version required is now 3.12.0.</li>
</ul></div></dd><dt><b>2017/01/28</b> - <a href="https://sourceforge.net/p/swig/news/2017/01/swig-3012-released/">SWIG-3.0.12 released</a></dt><dd><div><p>SWIG-3.0.12 summary:</p>
<ul>
<li>Add support for Octave-4.2.</li>
<li>Enhance %extend to support template functions.</li>
<li>Language specific enhancements and fixes for C#, D, Guile, Java, PHP7.</li>
</ul></div></dd><dt><b>2016/12/29</b> - <a href="https://sourceforge.net/p/swig/news/2016/12/swig-3011-released/">SWIG-3.0.11 released</a></dt><dd><p>SWIG-3.0.11 summary:<br>
- PHP 7 support added.<br>
- C++11 alias templates and type aliasing support added.<br>
- Minor fixes and enhancements for C# Go Guile Java Javascript Octave PHP Python R Ruby Scilab XML.</p></dd><dt><b>2016/06/12</b> - <a href="https://sourceforge.net/p/swig/news/2016/06/swig-3010-released/">SWIG-3.0.10 released</a></dt><dd><p>This release fixes a couple of important regressions in SWIG-3.0.9 for smart pointers and importing Python modules.</p></dd><dt><b>2016/05/29</b> - <a href="https://sourceforge.net/p/swig/news/2016/05/swig-309-released/">SWIG-3.0.9 released</a></dt><dd><div><p>Summary of changes in SWIG-3.0.9</p>
<ul>
<li>Add support for Python's implicit namespace packages.</li>
<li>Fixes to support Go 1.6.</li>
<li>C++11 std::array support added for Java.</li>
<li>Improved C++ multiple inheritance support for Java/C# wrappers.</li>
<li>Various other minor fixes and improvements for C#, D, Go, Java, Javascript, Lua, Python, R, Ruby, Scilab.</li>
</ul></div></dd><dt><b>2015/12/31</b> - <a href="https://sourceforge.net/p/swig/news/2015/12/swig-308-released/">SWIG-3.0.8 released</a></dt><dd><p>SWIG-3.0.8 summary:<br>
- pdf documentation enhancements.<br>
- Various Python 3.5 issues fixed.<br>
- std::array support added for Ruby and Python.<br>
- shared_ptr support added for Ruby.<br>
- Minor improvements for CFFI, Go, Java, Perl, Python, Ruby.</p></dd></dl> 
 

<p>
<a href="https://www.swig.org/news.php">More news</a>



</p><hr>
Feedback and questions concerning this site should be posted to the <a href="https://www.swig.org/mail.html">swig-devel</a> mailing list.

<p>
Last modified : Thu Apr 18 20:11:49 2019


             </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stalwart All-in-One Mail Server (IMAP, JMAP, SMTP) (192 pts)]]></title>
            <link>https://github.com/stalwartlabs/mail-server</link>
            <guid>36769173</guid>
            <pubDate>Tue, 18 Jul 2023 07:50:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/stalwartlabs/mail-server">https://github.com/stalwartlabs/mail-server</a>, See on <a href="https://news.ycombinator.com/item?id=36769173">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-target="readme-toc.content">
            <article itemprop="text"><h2 tabindex="-1" dir="auto">Stalwart Mail Server</h2>
<p dir="auto"><a href="https://github.com/stalwartlabs/mail-server/actions/workflows/test.yml"><img src="https://github.com/stalwartlabs/mail-server/actions/workflows/test.yml/badge.svg" alt="Test"></a>
<a href="https://github.com/stalwartlabs/mail-server/actions/workflows/build.yml"><img src="https://github.com/stalwartlabs/mail-server/actions/workflows/build.yml/badge.svg" alt="Build"></a>
<a href="https://www.gnu.org/licenses/agpl-3.0" rel="nofollow"><img src="https://camo.githubusercontent.com/ff112811173c529c5c1b7ed5ab8dc2fd2d388c2a86b0fd4efb5dd119850fcdb3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4147504c5f76332d626c75652e737667" alt="License: AGPL v3" data-canonical-src="https://img.shields.io/badge/License-AGPL_v3-blue.svg"></a>
<a href="https://discord.gg/jtgtCNj66U" rel="nofollow"><img src="https://camo.githubusercontent.com/47f0dbe3b4ccd2cc3f76aff81b090501444b15642b8c74721cae1ba552238e4e/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f3932333631353836333033373339303838393f6c6162656c3d43686174" alt="" data-canonical-src="https://img.shields.io/discord/923615863037390889?label=Chat"></a>
<a href="https://twitter.com/stalwartlabs" rel="nofollow"><img src="https://camo.githubusercontent.com/2adff3c3fbf787d8832a131b73e58ae967305fa4a7a0572c7eaaef782526a387/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f7374616c776172746c6162733f7374796c653d666c6174" alt="" data-canonical-src="https://img.shields.io/twitter/follow/stalwartlabs?style=flat"></a></p>
<p dir="auto"><strong>Stalwart Mail Server</strong> is an open-source mail server solution with JMAP, IMAP4, and SMTP support and a wide range of modern features. It is written in Rust and designed to be secure, fast, robust and scalable.</p>
<p dir="auto">Key features:</p>
<ul dir="auto">
<li><strong>JMAP</strong> server:
<ul dir="auto">
<li>JMAP Core (<a href="https://datatracker.ietf.org/doc/html/rfc8620" rel="nofollow">RFC 8620</a>)</li>
<li>JMAP Mail (<a href="https://datatracker.ietf.org/doc/html/rfc8621" rel="nofollow">RFC 8621</a>)</li>
<li>JMAP over WebSocket (<a href="https://datatracker.ietf.org/doc/html/rfc8887" rel="nofollow">RFC 8887</a>)</li>
<li>JMAP for Sieve Scripts (<a href="https://www.ietf.org/archive/id/draft-ietf-jmap-sieve-13.html" rel="nofollow">DRAFT-SIEVE-13</a>)</li>
</ul>
</li>
<li><strong>IMAP4</strong> server:
<ul dir="auto">
<li>IMAP4rev2 (<a href="https://datatracker.ietf.org/doc/html/rfc9051" rel="nofollow">RFC 9051</a>) full compliance.</li>
<li>IMAP4rev1 (<a href="https://datatracker.ietf.org/doc/html/rfc3501" rel="nofollow">RFC 3501</a>) backwards compatible.</li>
<li>ManageSieve (<a href="https://datatracker.ietf.org/doc/html/rfc5804" rel="nofollow">RFC 5804</a>) server.</li>
<li>Numerous <a href="https://stalw.art/docs/development/rfcs#imap4-and-extensions" rel="nofollow">extensions</a> supported.</li>
</ul>
</li>
<li><strong>SMTP</strong> server:
<ul dir="auto">
<li>Built-in <a href="https://datatracker.ietf.org/doc/html/rfc7489" rel="nofollow">DMARC</a>, <a href="https://datatracker.ietf.org/doc/html/rfc6376" rel="nofollow">DKIM</a>, <a href="https://datatracker.ietf.org/doc/html/rfc7208" rel="nofollow">SPF</a> and <a href="https://datatracker.ietf.org/doc/html/rfc8617" rel="nofollow">ARC</a> support for message authentication.</li>
<li>Strong transport security through <a href="https://datatracker.ietf.org/doc/html/rfc6698" rel="nofollow">DANE</a>, <a href="https://datatracker.ietf.org/doc/html/rfc8461" rel="nofollow">MTA-STS</a> and <a href="https://datatracker.ietf.org/doc/html/rfc8460" rel="nofollow">SMTP TLS</a> reporting.</li>
<li>Inbound throttling and filtering with granular configuration rules and Sieve scripting.</li>
<li>Virtual queues with delayed delivery, priority delivery, quotas, routing rules and throttling support.</li>
</ul>
</li>
<li><strong>Flexible</strong>:
<ul dir="auto">
<li><strong>LDAP</strong> directory and <strong>SQL</strong> database authentication.</li>
<li>Full-text search available in 17 languages.</li>
<li>Disk quotas.</li>
<li>Sieve scripting language with support for all <a href="https://www.iana.org/assignments/sieve-extensions/sieve-extensions.xhtml" rel="nofollow">registered extensions</a>.</li>
<li>Email aliases, mailing lists, subaddressing and catch-all addresses support.</li>
<li>Integration with <strong>OpenTelemetry</strong> to enable monitoring, tracing, and performance analysis.</li>
</ul>
</li>
<li><strong>Secure</strong>:
<ul dir="auto">
<li>OAuth 2.0 <a href="https://www.rfc-editor.org/rfc/rfc8628" rel="nofollow">authorization code</a> and <a href="https://www.rfc-editor.org/rfc/rfc8628" rel="nofollow">device authorization</a> flows.</li>
<li>Access Control Lists (ACLs).</li>
<li>Rate limiting.</li>
</ul>
</li>
<li><strong>Robust and scalable</strong>:
<ul dir="auto">
<li><strong>FoundationDB</strong> or <strong>SQLite</strong> database backends.</li>
<li><strong>S3-compatible</strong> blob storage support.</li>
<li>Memory safe (thanks to Rust).</li>
</ul>
</li>
</ul>
<h2 tabindex="-1" dir="auto">Get Started</h2>
<p dir="auto">Install Stalwart Mail Server on your server by following the instructions for your platform:</p>
<ul dir="auto">
<li><a href="https://stalw.art/docs/install/linux" rel="nofollow">Linux / MacOS</a></li>
<li><a href="https://stalw.art/docs/install/windows" rel="nofollow">Windows</a></li>
<li><a href="https://stalw.art/docs/install/docker" rel="nofollow">Docker</a></li>
</ul>
<p dir="auto">All documentation is available at <a href="https://stalw.art/docs/get-started" rel="nofollow">stalw.art/docs/get-started</a>.</p>
<h2 tabindex="-1" dir="auto">Support</h2>
<p dir="auto">If you are having problems running Stalwart Mail Server, you found a bug or just have a question,
do not hesitate to reach us on <a href="https://github.com/stalwartlabs/mail-server/discussions">Github Discussions</a>,
<a href="https://www.reddit.com/r/stalwartlabs" rel="nofollow">Reddit</a> or <a href="https://discord.gg/aVQr3jF8jd" rel="nofollow">Discord</a>.
Additionally you may become a sponsor to obtain priority support from Stalwart Labs Ltd.</p>
<h2 tabindex="-1" dir="auto">Funding</h2>
<p dir="auto">Part of the development of this project was funded through the <a href="https://nlnet.nl/entrust" rel="nofollow">NGI0 Entrust Fund</a>, a fund established by <a href="https://nlnet.nl/" rel="nofollow">NLnet</a> with financial support from the European Commission's <a href="https://ngi.eu/" rel="nofollow">Next Generation Internet</a> programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594.</p>
<p dir="auto">If you find the project useful you can help by <a href="https://github.com/sponsors/stalwartlabs">becoming a sponsor</a>. Thank you!</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto">Licensed under the terms of the <a href="https://www.gnu.org/licenses/agpl-3.0.en.html" rel="nofollow">GNU Affero General Public License</a> as published by
the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
See <a href="https://github.com/stalwartlabs/mail-server/blob/main/LICENSE">LICENSE</a> for more details.</p>
<p dir="auto">You can be released from the requirements of the AGPLv3 license by purchasing
a commercial license. Please contact <a href="mailto:licensing@stalw.art">licensing@stalw.art</a> for more details.</p>
<h2 tabindex="-1" dir="auto">Copyright</h2>
<p dir="auto">Copyright (C) 2023, Stalwart Labs Ltd.</p>
</article>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Windows 11 collects an awful lot of telemetry about your PC (131 pts)]]></title>
            <link>https://www.extremetech.com/computing/342941-windows-11-collects-an-awful-lot-of-telemetry-about-your-pc</link>
            <guid>36769145</guid>
            <pubDate>Tue, 18 Jul 2023 07:46:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.extremetech.com/computing/342941-windows-11-collects-an-awful-lot-of-telemetry-about-your-pc">https://www.extremetech.com/computing/342941-windows-11-collects-an-awful-lot-of-telemetry-about-your-pc</a>, See on <a href="https://news.ycombinator.com/item?id=36769145">Hacker News</a></p>
Couldn't get https://www.extremetech.com/computing/342941-windows-11-collects-an-awful-lot-of-telemetry-about-your-pc: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Generative AI space and the mental imagery of alien minds (217 pts)]]></title>
            <link>https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/</link>
            <guid>36767837</guid>
            <pubDate>Tue, 18 Jul 2023 04:34:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/">https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/</a>, See on <a href="https://news.ycombinator.com/item?id=36767837">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p><img title="Generative AI Space and the Mental Imagery of Alien Minds" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec1img1.jpg" alt="Generative AI Space and the Mental Imagery of Alien Minds" width="620" height=" "></p>
<h2 id="ais-and-alien-minds">AIs and Alien Minds</h2>
<p>How do <a href="https://writings.stephenwolfram.com/category/language-and-communication/">alien minds perceive the world</a>? It’s an old and oft-debated question in philosophy. And it now turns out to also be a question that rises to prominence in connection with the <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">concept of the ruliad</a> that’s emerged from our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Wolfram Physics Project</a>.</p>
<p>I’ve wondered about alien minds for a long time—and tried all sorts of ways to imagine what it might be like to see things from their point of view. But in the past I’ve never really had a way to build my intuition about it. That is, until now. So, what’s changed? It’s AI. Because in AI we finally have an accessible form of alien mind.<span id="more-52580"></span></p>
<p>We typically go to <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">a lot of trouble to train</a> our AIs to produce results that are like we humans would do. But what if we take a human-aligned AI, and modify it? Well, then we get something that’s in effect an alien AI—an AI aligned not with us humans, but with an alien mind.</p>
<p>So how can we see what such an alien AI—or alien mind—is “thinking”? A convenient way is to try to capture its “mental imagery”: the image it forms in its “mind’s eye”. Let’s say we use a typical <a href="https://reference.wolfram.com/language/ref/ImageSynthesize.html">generative AI</a> to go from a description in human language—like “a cat in a party hat”—to a generated image:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323img2.png" alt="" title="" width="149" height="149"> </p>
</div>
<p>It’s exactly the kind of image we’d expect—which isn’t surprising, because it <a href="https://resources.wolframcloud.com/FunctionRepository/resources/StableDiffusionSynthesize/">comes from a generative AI</a> that’s trained to “do as we would”. But now let’s imagine taking the neural net that implements this generative AI, and modifying its insides—say by resetting <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#neural-nets">weights that appear in its neural net</a>.</p>
<p>By doing this we’re in effect going from a human-aligned neural net to some kind of “alien” one. But this “alien” neural net will still produce some kind of image—because that’s what a neural net like this does. But what will the image be? Well, in effect, it’s showing us the mental imagery of the “alien mind” associated with the modified neural net. </p>
<p>But what does it actually look like? Well, here’s a sequence obtained by progressively modifying the neural net—in effect making it “progressively more alien”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec1img3.png" alt="" title="" width="612" height="201"> </p>
</div>
<p>At the beginning it’s still a very recognizable picture of “a cat in a party hat”. But it soon becomes more and more alien: the mental image in effect diverges further from the human one—until it no longer “looks like a cat”, and in the end looks, at least to us, rather random. </p>
<p>There are many details of how this works that we’ll be discussing below. But what’s important is that—by studying the effects of changing the neural net—we now have a systematic “experimental” platform for probing at least one kind of “alien mind”. We can think of what we’re doing as a kind of “artificial neuroscience”, probing not actual human brains, but neural net analogs of them.</p>
<p>And we’ll see many parallels to neuroscience experiments. For example, we’ll often be “knocking out” particular parts of our “neural net brain”, a little like how injuries such as strokes can knock out parts of a human brain. But we know that when a human brain suffers a stroke, this can lead to phenomena like “<a href="https://en.wikipedia.org/wiki/Hemispatial_neglect" target="_blank" rel="noopener">hemispatial neglect</a>”, in which a stroke victim asked to draw a clock will end up drawing just one side of the clock—a little like the pictures of cats “degrade” when parts of the “neural net brain” are knocked out. </p>
<p>Of course, there are many differences between real brains and artificial neural nets. But most of the core phenomena we’ll observe here seem robust and fundamental enough that we can expect them to span very different kinds of “brains”—human, artificial and alien. And the result is that we can begin to build up intuition about what the worlds of different—and alien—minds can be like. </p>
<h2 id="generating-images-with-ais">Generating Images with AIs</h2>
<p>How does an AI manage to create a picture, say of a cat in a party hat? Well, the AI has to be trained on “what makes a reasonable picture”—and how to determine what a picture is of. Then in some sense what the AI does is to start generating “reasonable” pictures at random, in effect continually checking what the picture it’s generating seems to be “of”, and tweaking it to guide it towards being a picture of what one wants.</p>
<p>So what counts as a “reasonable picture”? If one looks at billions of pictures—say on the web—there are lots of regularities. For example, the pixels aren’t random; nearby ones are usually highly correlated. If there’s a face, it’s usually more or less symmetrical. It’s more common to have blue at the top of a picture, and green at the bottom. And so on. And the important technological point is that it turns out to be possible to use a neural network to capture regularities in images, and to generate random images that exhibit them.</p>
<p>Here are some examples of “random images” generated in this way:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec2img1.jpg" alt="" title="" width="624" height="834"> </p>
</div>
<p>And the idea is that these images—while each is “random” in its specifics—will in general follow the “statistics” of the billions of images from the web on which the neural network has been “trained”. We’ll be talking more about images like these later. But for now suffice it to say that while some may just look like abstract patterns, others seem to contain things like landscapes, human forms, etc. And what’s notable is that none just look like “random arrays of pixels”; they all show some kind of “structure”. And, yes, given that they’ve been trained from pictures on the web, it’s not too surprising that the “structure” sometimes includes things like human forms. </p>
<p>But, OK, let’s say we specifically want a picture of a cat in a party hat. From all of the almost infinitely large number of possible “well-structured” random images we might generate, how do we get one that’s of a cat in a party hat? Well, a first question is: how would we know if we’ve succeeded? As humans, we could just look and see what our image is of. But it turns out we can also <a href="https://reference.wolfram.com/language/ref/ImageIdentify.html">train a neural net to do this</a> (and, no, it <a href="https://writings.stephenwolfram.com/2015/05/wolfram-language-artificial-intelligence-the-image-identification-project/">doesn’t always get it exactly right</a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec2img3.png" alt="" title="" width="610" height="108"> </p>
</div>
<p>How is the <a href="https://reference.wolfram.com/language/ref/NetTrain.html">neural net trained</a>? The basic idea is to take billions of images—say from the web—for which corresponding captions have been provided. Then one progressively tweaks the parameters of the neural net to make it reproduce these captions when it’s fed the corresponding images. But the critical point is the <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#neural-nets">neural net turns out to do more</a>: it also successfully produces “reasonable” captions for images it’s never seen before. What does “reasonable” mean? Operationally, it means captions that are similar to what we humans might assign. And, yes, it’s far from obvious that a computationally constructed neural net will behave at all like us humans, and the fact that it does <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">is presumably telling us</a> fundamental things about how human brains work.</p>
<p>But for now what’s important is that we can use this captioning capability to progressively guide images we produce towards what we want. Start from “pure randomness”. Then try to “structure the randomness” to make a “reasonable” picture, but at every step see in effect “what the caption would be”. And try to “go in a direction” that “leads towards” a picture with the caption we want. Or, in other words, progressively try to get to a picture that’s of what we want. </p>
<p>The way this is set up in practice, one starts from an array of random pixels, then iteratively forms the picture one wants:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec2img5.jpg" alt="" title="" width="612" height="196"> </p>
</div>
<p>Different initial arrays lead to different final pictures—though if everything works correctly, the final pictures will all be of “what one asked for”, in this case a cat in a party hat (and, yes, there are a few “glitches”):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec2img6.png" alt="" title="" width="618" height="198"> </p>
</div>
<p>We don’t know how mental images are formed in human brains. But it seems conceivable that the process is not too different. And that in effect as we’re trying to “conjure up a reasonable image”, we’re continually checking if it’s aligned with what we want—so that, for example, if our checking process is impaired we can end up with a different image, as in hemispatial neglect. </p>
<h2 id="the-notion-of-interconcept-space">The Notion of Interconcept Space</h2>
<p>That everything can ultimately be represented in terms of digital data is foundational to the whole computational paradigm. But the effectiveness of neural nets relies on the slightly different idea that it’s useful to treat at least many kinds of things as being characterized by arrays of real numbers. In the end one might extract from a neural net that’s giving captions to images the word “cat”. But inside the neural net it’ll operate with arrays of numbers that correspond in some fairly abstract way to the image you’ve given, and the textual caption it’ll finally produce.</p>
<p>And in general neural nets can typically be thought of as associating “feature vectors” with things—whether those things are images, text, or anything else. But whereas words like “cat” and “dog” are discrete, the <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#the-concept-of-embeddings">feature vectors associated with them</a> just contain collections of real numbers. And this means that we can think of a whole space of possibilities, with “cat” and “dog” just corresponding to two specific points. </p>
<p>So what’s out there in that space of possibilities? For the feature vectors we typically deal with in practice <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/CLIP-Multi-domain-Feature-Extractor/">the space is many-thousand-dimensional</a>. But we can for example look at the (nominally straight) line from the “dog point” to the “cat point” in this space, and even generate sample images of what comes between:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec3Aimg1.png" alt="" title="" width="609" height="196"> </p>
</div>
<p>And, yes, if we want to, we can keep going “beyond cat”—and pretty soon things start becoming quite weird:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec3Aimg2.png" alt="" title="" width="610" height="196"> </p>
</div>
<p>We can also do things like look at the line from a plane to a cat—and, yes, there’s strange stuff in there (wings <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123rightarrow.png" alt="" width="16" height=" "> hat <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123rightarrow.png" alt="" width="16" height=" "> ears?):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec3Aimg3.png" alt="" title="" width="619" height="199"> </p>
</div>
<p>What about elsewhere? For example, what happens “around” our standard “cat in a party hat”? With the <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/CLIP-Multi-domain-Feature-Extractor/">particular setup we’re using</a>, there’s a 2304-dimensional space of possibilities. But as an example, we look at what we get on a particular 2D plane through the “standard cat” point:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071623sec3-45img1A.jpeg" alt="" title="" width="621" height=""> </p>
</div>
<p>Our “standard cat” is in the middle. But as we move away from the “standard cat” point, progressively weirder things happen. For a while there are recognizable (if perhaps demonic) cats to be seen. But soon there isn’t much “catness” in evidence—though sometimes hats do remain (in what we might characterize as an “all hat, no cat” situation, reminiscent of the Texan “all hat, no cattle”). </p>
<p>How about if we pick other planes through the standard cat point? All sorts of images appear:</p>

<p>But the fundamental story is always the same: there’s a kind of “cat island”, beyond which there are weird and only vaguely cat-related images—encircled by an “ocean” of what seem like purely abstract patterns with no obvious cat connection. And in general the picture that emerges is that in the immense space of possible “statistically reasonable” images, there are islands dotted around that correspond to “<a href="https://writings.stephenwolfram.com/2018/11/logic-explainability-and-the-future-of-understanding/#the-concept-of-concepts">linguistically describable concepts</a>”—like cats in party hats. </p>
<p>The islands normally seem to be roughly “spherical”, in the sense that they extend about the same nominal distance in every direction. But relative to the whole space, each island is absolutely tiny—something like perhaps a fraction 2<sup>–2000</sup> ≈ 10<sup>–600</sup> of the volume of the whole space. And between these islands there lie huge expanses of what we might call “interconcept space”. </p>
<p>What’s out there in interconcept space? It’s full of images that are “statistically reasonable” based on the images we humans have put on the web, etc.—but aren’t of things we humans have come up with words for. It’s as if in developing our civilization—and our human language—we’ve “colonized” only certain small islands in the space of all possible concepts, leaving vast amounts of interconcept space unexplored. </p>
<p>What’s out there is pretty weird—and sometimes a bit disturbing. Here’s what we see zooming in on the same (randomly chosen) plane around “cat island” as above:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071623sec3Yimg1B.jpg" alt="" title="" width="617" height="620"> </p>
</div>

<p>What are all these things? In a sense, words fail us. They’re things on the shores of interconcept space, where human experience has not (yet) taken us, and for which human language has not been developed.</p>
<p>What if we venture further out into interconcept space—and for example just sample points in the space at random? It’s just like we already saw above: we’ll get images that are somehow “statistically typical” of what we humans have put on the web, etc., and on which our AI was trained. Here are a few more examples:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071623sec3Zimg1.jpg" alt="" title="" width="620" height="828"> </p>
</div>
<p>And, yes, we can pick out at least two basic classes of images: ones that seem like “pure abstract textures”, and ones that seem “representational”, and remind us of real-world scenes from human experience. There are intermediate cases—like “textures” with structures that seem like they might “represent something”, and “representational-seeming” images where we just can’t place what they might be representing.</p>
<p>But when we do see recognizable “real-world-inspired” images they’re a curious reflection of the concepts—and general imagery—that we humans find “interesting enough to put on the web”. We’re not dealing here with some kind of “arbitrary interconcept space”; we’re dealing with “human-aligned” interconcept space that’s in a sense anchored to human concepts, but extends between and around them. And, yes, viewed in these terms it becomes quite unsurprising that in the interconcept space we’re sampling, there are so many images that remind us of human forms and common human situations. </p>
<p>But just what were the images that the AI saw, from which it formed this model of interconcept space? There were a <a href="https://laion.ai/blog/laion-5b/" target="_blank" rel="noopener">few billion of them, “foraged” from the web</a>. Like things on the web in general, it’s a motley collection; here’s a random sample:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071523sec3photogridimg1.jpg" alt="" title="" width="620" height="394"> </p>
</div>
<p>Some can be thought of as capturing aspects of “life as it is”, but many are more aspirational, coming from staged and often promotionally oriented photography. And, yes, there are lots of Net-a-Porter-style “clothing-without-heads” images. There are also lots of images of “things”—like food, etc. But somehow when we sample randomly in interconcept space it’s the human forms that most distinctively stand out, conceivably because “things” are not particularly consistent in their structure, but human forms always have a certain consistency of “head-body-arms, etc.” structure.</p>
<p>It’s notable, though, that even the most real-world-like images we find by randomly sampling interconcept space seem to typically be “painterly” and “artistic” rather than “photorealistic” and “photographic”. It’s a different story close to “concept points”—like on cat island. There more photographic forms are common, though as we go away from the “actual concept point”, there’s a tendency towards either a rather toy-like appearance, or something more like an illustration.</p>
<p>By the way, even the most “photographic” images the AI generates won’t be anything that comes directly from the training set. Because—as we’ll discuss later—the AI is not set up to directly store images; instead its training process in effect “grinds up” images to extract their “statistical properties”. And while “statistical features” of the original images will show up in what the AI generates, any detailed arrangement of pixels in them is overwhelmingly unlikely to do so.</p>
<p>But, OK, what happens if we start not at a “describable concept” (like “a cat in a party hat”), but just at a random point in interconcept space? Here are the kinds of things we see:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071623sec3Zimg3.jpg" alt="" title="" width="616" height="616"> </p>
</div>

<p>The images often seem to be a bit more diverse than those around “known concept points” (like our “cat point” above). And occasionally there’ll be a “flash” of something “representationally familiar” (perhaps like a human form) that’ll show up. But most of the time we won’t be able to say “what these images are of”. They’re of things that are somehow “statistically” like what we’ve seen, but they’re not things that are familiar enough that we’ve—at least so far—developed a way to describe them, say with words.</p>
<h2 id="the-images-of-interconcept-space">The Images of Interconcept Space</h2>
<p>There’s something strangely familiar—yet unfamiliar—to many of the images in interconcept space. It’s fairly common to see pictures that seem like they’re of people:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img1.jpg" alt="" title="" width="607" height="295"> </p>
</div>
<p>But they’re “not quite right”. And for us as humans, being particularly attuned to faces, it’s the faces that tend to seem the most wrong—even though other parts are “wrong” as well.</p>
<p>And perhaps in commentary on our nature as a social species (or maybe it’s as a social media species), there’s a great tendency to see pairs or larger groups of people:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img2.jpg" alt="" title="" width="615" height="300"> </p>
</div>
<p>There’s also a strange preponderance of torso-only pictures—presumably the result of “fashion shots” in the training data (and, yes, with some rather wild “fashion statements”):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img3.png" alt="" title="" width="618" height="146"> </p>
</div>
<p>People are by far the most common identifiable elements. But one does sometimes see other things too:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img4.png" alt="" title="" width="620" height="145"> </p>
</div>
<p>Then there are some landscape-type scenes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img5.png" alt="" title="" width="618" height="146"> </p>
</div>
<p>Some look fairly photographically literal, but others build up the impression of landscapes from more abstract elements: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img6.jpg" alt="" title="" width="619" height="302"> </p>
</div>
<p>Occasionally there are cityscape-like pictures:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img7.png" alt="" title="" width="621" height="146"> </p>
</div>
<p>And—still more rarely—indoor-like scenes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img8Bpng-1A.png" alt="" title="" width="620" height=""> </p>
</div>
<p>Then there are pictures that look like they’re “exteriors” of some kind:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img9.png" alt="" title="" width="617" height="145"> </p>
</div>
<p>It’s common to see images built up from lines or dots or otherwise “impressionistically formed”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img10A.jpg" alt="" title="" width="619" height="460"> </p>
</div>
<p>And then there are lots of images of that seem like they’re trying to be “of something”, but it’s not at all clear what that “thing” is, and whether indeed it’s something we humans would recognize, or whether instead it’s something somehow “fundamentally alien”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img12.jpg" alt="" title="" width="619" height="939"> </p>
</div>
<p>It’s also quite common to see what look more like “pure patterns”—that don’t really seem like they’re “trying to be things”, but more come across like “decorative textures”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img13.jpg" alt="" title="" width="620" height="620"> </p>
</div>
<p>But probably the single most common type of images are somewhat uniform textures, formed by repeating various simple elements, though usually with “dislocations” of various kinds: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img14.jpg" alt="" title="" width="620" height="302"> </p>
</div>
<p>Across interconcept space there’s tremendous variety to the images we see. Many have a certain artistic quality to them—and a feeling that they are some kind of “mindful interpretation” of a perhaps mundane thing in the world, or a simple, essentially mathematical pattern. And to some extent the “mind” involved is a collective version of our human one, reflected in a neural net that has “experienced” some of the many images humans have put on the web, etc. But in some ways the mind is also a more alien one, formed from the computational structure of the neural net, with its particular features, and no doubt in some ways computationally irreducible behavior.</p>
<p>And indeed there are some motifs that show up repeatedly that are presumably reflections of features of the underlying structure of the neural net. The “granulated” appearance, with alternation between light and dark, for example, is presumably a consequence of the dynamics of the convolutional parts of the neural net—and analogous to the results of what amounts to iterated blurring and sharpening with a certain effective pixel scale (reminiscent, for example, of video feedback):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec4img15.jpg" alt="" title="" width="609" height="236"> </p>
</div>
<h2 id="making-minds-alien">Making Minds Alien</h2>
<p>We can think of what we’ve done so far as exploring what a mind trained from human-like experiences can “imagine” by generalizing from those experiences. But what might a different kind of mind imagine?</p>
<p>As a very rough approximation, we can think of just taking the trained “mind” we’ve created, and explicitly modifying it, then seeing what it now “imagines”. Or, more specifically, we can take the neural net we have been using, and start making changes to it, and seeing what effect that has on the images it produces.</p>
<p>We’ll discuss later the details of how the network is set up, but suffice it to say here that it <a href="https://resources.wolframcloud.com/FunctionRepository/resources/StableDiffusionSynthesize/">involves 391 distinct internal modules</a>, involving altogether nearly a billion numerical weights. When the network is trained, those numerical weights are carefully tuned to achieve the results we want. But what if we just change them? We’ll still (normally) get a network that can generate images. But in some sense it’ll be “thinking differently”—so potentially the images will be different. </p>
<p>So as a very coarse first experiment—reminiscent of many that are done in biology—let’s just “knock out” each successive module in turn, setting all its weights to zero. If we ask the resulting network to generate a picture of “a cat in a party hat”, here’s what we now get:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071223sec5Aimg1B.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>Let’s look at these results in a bit more detail. In quite a few cases, zeroing out a single module doesn’t make much of a difference; for example, it might basically only change the facial expression of the cat:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg1C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>But it can also more fundamentally change the cat (and its hat):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg2C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>It can change the configuration or position of the cat (and, yes, some of those paws are not anatomically correct):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg3C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>Zeroing out other modules can in effect change the “rendering” of the cat:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg4C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>But in other cases things can get much more mixed up, and difficult for us to parse:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg5C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>Sometimes there’s clearly a cat there, but its presentation is at best odd:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg6C.jpg" alt="" title="" width="609" height=""> </p>
</div>
<p>And sometimes we get images that have definite structure, but don’t seem to have anything to do with cats:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg7C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>Then there are cases where we basically just get “noise”, albeit with things superimposed:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Bimg8C.jpg" alt="" title="" width="610" height=""> </p>
</div>
<p>But—much like in neurophysiology—there are some modules (like the very first and last ones in our original list) where zeroing them out basically makes the system not work at all, and just generate “pure random noise”.</p>
<p>As we’ll discuss below, the whole <a href="https://resources.wolframcloud.com/FunctionRepository/resources/StableDiffusionSynthesize/">neural net that we’re using</a> has a fairly complex internal structure—for example, with a few fundamentally different kinds of modules. But here’s a sample of what happens if one zeros out modules at different places in the network—and what we see is that for the most part there’s no obvious correlation between where the module is, and what effect zeroing it out will have:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Cimg1A.png" alt="" title="" width="620" height=""> </p>
</div>
<p>So far, we’ve just looked at what happens if we zero out a single module at a time. Here are some randomly chosen examples of what happens if one zeros out successively more modules (one might call this a “HAL experiment” in remembrance of the fate of the <a href="https://writings.stephenwolfram.com/2018/04/learning-about-the-future-from-2001-a-space-odyssey-fifty-years-later/">fictional HAL AI in the movie <em>2001</em></a>):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5KBimg1.jpg" alt="" title="" width="617" height=""> </p>
</div>
<p>And basically once the “catness” of the images is lost, things become more and more alien from there on out, ending either in apparent randomness, or sometimes barren “zeroness”.</p>
<p>Rather than zeroing out modules, we can instead <a href="https://reference.wolfram.com/language/ref/NetInitialize.html">randomize the weights</a> in them (perhaps a bit like the effect of a tumor rather than a stroke in a brain)—but the results are usually at least qualitatively similar:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5KBimg2A.jpg" alt="" title="" width="618" height=""> </p>
</div>
<p>Something else we can do is just to progressively mix randomness uniformly into every weight in the network (perhaps a bit like globally “drugging” a brain). Here are three examples where in each case 0%, 1%, 2%, … of randomness was added—all “fading away” in a very similar way:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5KBimg3A.jpg" alt="" title="" width="617" height=""> </p>
</div>
<p>And similarly, we can progressively scale down towards zero (in 1% increments: 100%, 99%, 98%, …) all the weights in the network:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Cimg8B.jpg" alt="" title="" width="619" height=""> </p>
</div>
<p>Or we can progressively increase the numerical values of the weights—eventually in some sense “blowing the mind” of the network (and going a bit “psychedelic” in the process):</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec5Cimg9B.jpg" alt="" title="" width="617" height=""> </p>
</div>
<h2 id="minds-in-rulial-space">Minds in Rulial Space</h2>
<p>We can think of what we’ve done so far as exploring some of the “natural history” of what’s out there in generative AI space—or as providing a small taste of at least one approximation to the kind of mental imagery one might encounter in alien minds. But how does this fit into a more general picture of alien minds and what they might be like?</p>
<p>With the <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">concept of the ruliad</a> we finally have a principled way to <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/#alien-views-of-the-ruliad">talk about alien minds</a>—at least at a theoretical level. And the key point is that any alien mind—or, for that matter, any mind—can be thought of as “observing” or sampling the ruliad from its own particular point of view, or in effect, its own position in rulial space.</p>
<p>The ruliad is defined to be the entangled limit of all possible computations: a unique object with an inevitable structure. And the idea is that anything—whether one interprets it as a phenomenon or an observer—must be part of the ruliad. The key to our <a href="https://www.wolframphysics.org/" target="_blank" rel="noopener">Physics Project</a> is then that “observers like us” have <a href="https://writings.stephenwolfram.com/2021/03/what-is-consciousness-some-new-perspectives-from-our-physics-project/">certain general characteristics</a>. We are computationally bounded, with “finite minds” and limited sensory input. And we have a certain coherence that comes from our belief in our persistence in time, and our consistent thread of experience. And what we then discover in our Physics Project is the rather remarkable result that from these characteristics and the general properties of the ruliad alone it’s essentially inevitable that we must perceive the universe to exhibit the fundamental physical laws it does, in particular the three big theories of twentieth-century physics: general relativity, quantum mechanics and statistical mechanics. </p>
<p>But what about more detailed aspects of what we perceive? Well, that will depend on more detailed aspects of us as observers, and of how our minds are set up. And in a sense, each different possible mind can be thought of as existing in a certain place in rulial space. Different human minds are mostly close in rulial space, animal minds further away, and more alien minds still further. But how can we characterize what these minds are “thinking about”, or how these minds “perceive things”?</p>
<p>From inside our own minds we can form a sense of what we perceive. But we don’t really have good ways to reliably probe what another mind perceives. But what about what another mind imagines? Well, that’s where what we’ve been doing here comes in. Because with generative AI we’ve got a mechanism for exposing the “mental imagery” of an “AI mind”. </p>
<p>We could consider doing this with words and text, say with an LLM. But for us humans images have a certain fluidity that text does not. Our eyes and brains can perfectly well “see” and absorb images even if we don’t “understand” them. But it’s very difficult for us to absorb text that we don’t “understand”; it usually tends to seem just like a kind of “word soup”. </p>
<p>But, OK, so we generate “mental imagery” from “minds” that have been “made alien” by various modifications. How come we humans can understand anything such minds make? Well, it’s bit like one person being able to understand the thoughts of another. Their brains—and minds—are built differently. And their “internal view” of things will inevitably be different. But the crucial idea—that’s for example central to language—is that it’s possible to “package up” thoughts into something that be “transported” to another mind. Whatever some particular internal thought might be, by the time we can express it with words in a language, it’s possible to communicate it to another mind that will “unpack” it into different internal thoughts.</p>
<p>It’s a nontrivial fact of physics that <a href="https://writings.stephenwolfram.com/2022/03/on-the-concept-of-motion/">“pure motion” in physical space is possible</a>; in other words, that an “object” can be moved “without change” from one place in physical space to another. And now, in a sense, we’re asking about <a href="https://writings.stephenwolfram.com/2022/03/on-the-concept-of-motion/#motion-in-rulial-space">pure motion in rulial space</a>: can we move something “without change” from one mind at one place in rulial space to another mind at another place? In physical space, things like particles—as well as things like black holes—are the fundamental elements that are imagined to move without change. So what’s now the analog in rulial space? It seems to be concepts—as often, for example, represented by words. </p>
<p>So what does that mean for our exploration of generative AI “alien minds”? We can ask whether when we move from one potentially alien mind to another concepts are preserved. We don’t have a perfect proxy for this (though we could make a better one by appropriately training neural net classifiers). But as a first approximation this is like asking whether as we “change the mind”—or move in rulial space—we can still recognize the “concept” the mind produces. Or, in other words, if we start with a “mind” that’s generating a cat in a party hat, will we still recognize the concepts of cat or hat in what a “modified mind” produces? </p>
<p>And what we’ve seen is that sometimes we do, and sometimes we don’t. And for example when we looked at “cat island” we saw a certain boundary beyond which we could no longer recognize “catness” in the image that was produced. And by studying things like cat island (and particularly its analogs when not just the “prompt” but also the underlying neural net is changed) it should be possible to map out how far concepts “extend” across alien minds.</p>
<p>It’s also possible to think about a kind of inverse question: just what is the <a href="https://www.wolframphysics.org/bulletins/2020/06/exploring-rulial-space-the-case-of-turing-machines/#the-emerging-picture-of-rulial-space" target="_blank" rel="noopener">extent of a mind in rulial space</a>? Or, in other words, what range of points of view, ultimately about the ruliad, can it hold? Will it be “narrow-minded”, able to think only in particular ways, with particular concepts? Or will it be more “broad-minded”, encompassing more ways of thinking, with more concepts?</p>
<p>In a sense the whole arc of the intellectual development of our civilization can be thought of as corresponding to an expansion in rulial space: with us progressively being able to think in new ways, and about new things. And as we expand in rulial space, we are in effect encompassing more of what we previously would have had to consider the domain of an alien mind.</p>
<p>When we look at images produced by generative AI away from the specifics of human experience—say in interconcept space, or with modified rules of generation—we may at first be able to make little from them. Like inkblots or arrangements of stars we’ll often find ourselves wanting to say that what we see looks like this or that thing we know.</p>
<p>But the real question is whether we can devise some way of describing what we see that allows us to build thoughts on what we see, or “reason” about it. And what’s very typical is that we manage to do this when we come up with a general “symbolic description” of what we see, say captured with words in natural language (or, now, <a href="https://writings.stephenwolfram.com/2019/05/what-weve-built-is-a-computational-language-and-thats-very-important/">computational language</a>). Before we have those words, or that symbolic description, we’ll tend just not to absorb what we see.</p>
<p>And so, for example, even though <a href="https://www.wolframscience.com/nks/p187--substitution-systems-and-fractals/">nested patterns</a> have always existed in nature, and were even explicitly <a href="https://writings.stephenwolfram.com/2018/01/showing-off-to-the-universe-beacons-for-the-afterlife-of-our-civilization/#lessons-from-the-past">created by mosaic artisans in the early 1200s</a>, they seem to have never been systematically noticed or discussed at all until the latter part of the 20th century, when finally the framework of “fractals” was developed for talking about them. </p>
<p>And so it may be with many of the forms we’ve seen here. As of today, we have no name for them, no systematic framework for thinking about them, and no reason to view them as important. But particularly if the things we do repeatedly show us such forms, we’ll eventually come up with names for them, and start incorporating them into the domain that our minds cover. </p>
<p>And in a sense what we’ve done here can be thought of as showing us a preview of what’s out there in rulial space, in what’s currently the domain of alien minds. In the general exploration of <a href="https://writings.stephenwolfram.com/category/ruliology/">ruliology</a>, and the investigation of what arbitrary simple programs in the computational universe do, we’re able to jump far across the ruliad. But it’s typical that what we see is not something we can connect to things we’re familiar with. In what we’re doing here, we’re moving only much smaller distances in rulial space. We’re starting from generative AI that’s closely aligned with current human development—having been trained from images that we humans have put on the web, etc. But then we’re making small changes to our “AI mind”, and looking at what it now generates. </p>
<p>What we see is often surprising. But it’s still close enough to where we “currently are” in rulial space that we can—at least to some extent—absorb and reason about what we’re seeing. Still, the images often don’t “make sense” to us. And, yes, quite possibly the AI has invented something that has a rich and “meaningful” inner structure. But it’s just that we don’t (yet) have a way to talk about it—and if we did, it would immediately “make perfect sense” to us. </p>
<p>So if we see something we don’t understand, can we just “train a translator”? At some level the answer must be yes. Because the <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">Principle of Computational Equivalence</a> implies that ultimately there’s a fundamental uniformity to the ruliad. But the problem is that the translator is likely to have to do an <a href="https://www.wolframscience.com/nks/p737--computational-irreducibility/">irreducible amount of computational work</a>. And so it won’t be implementable by a “mind like ours”. Still, even though we can’t create a “general translator” we can expect that certain features of what we see will still be translatable—in effect by exploiting certain pockets of computational reducibility that must necessarily exist even when the system as a whole is full of computational irreducibility. And operationally what this means in our case is that the AI may in effect have found certain regularities or patterns that we don’t happen to have noticed but that are useful in exploring further from the “current human point” in rulial space.</p>
<p>It’s very challenging to get an intuitive understanding of what rulial space is like. But the approach we’ve taken here is for me a promising first effort in “humanizing” rulial space, and seeing just how we might be able to relate to what is so far the domain of alien minds. </p>
<hr>
<h2 id="appendix-how-does-the-generative-ai-work?">Appendix: How Does the Generative AI Work?</h2>
<p>In the main part of this piece, we’ve mostly just talked about what generative AI does, not how it works inside. Here I’ll go a little deeper into what’s inside the particular type of generative AI system that I’ve used in my explorations. It’s a <a href="https://resources.wolframcloud.com/FunctionRepository/resources/StableDiffusionSynthesize/">method called stable diffusion</a>, and its operation is in many ways both clever and surprising. As it’s implemented today it’s steeped in fairly complicated engineering details. To what extent these will ultimately be necessary isn’t clear. But in any case here I’ll mostly concentrate on general principles, and on giving a broad outline of how generative AI can be used to produce images.</p>
<h3 id="the-distribution-of-typical-images">The Distribution of Typical Images</h3>
<p>At the core of generative AI is the ability to produce things of some particular type that “follow the patterns of” known things of that type. So, for example, <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">large language models (LLMs)</a> are intended to produce text that “follows the patterns” of text written by humans, say on the web. And generative AI systems for images are similarly intended to produce images that “follow the patterns” of images put on the web, etc. </p>
<p>But what kinds of patterns exist in typical images, say on the web? Here are some examples of “typical images”—scaled down to 32×32 pixels and taken from a <a href="https://datarepository.wolframcloud.com/resources/CIFAR-100/" target="_blank" rel="noopener">standard set of 60,000 images</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img1.png" alt="" title="" width="642" height="155"> </p>
</div>
<p>And as a very first thing, we can ask what colors show up in these images. They’re not uniform in RGB space: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img2.png" alt="" title="" width="192" height="72"> </p>
</div>
<p>But what about the positions of different colors? Adjusting to accentuate color differences, the “average image” turns out to have a curious “HAL’s eye” look (presumably with blue for sky at the top, and brown for earth at the bottom): </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img3.png" alt="" title="" width="67" height="67"> </p>
</div>
<p>But just picking pixels separately—even with the color distribution inferred from actual images—won’t produce images that in any way look “natural” or “realistic”: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img4.png" alt="" title="" width="630" height="48"> </p>
</div>
<p>And the immediate issue is that the pixels aren’t really independent; most pixels in most images are correlated in color with nearby pixels. And in a first approximation one can capture this for example by <a href="https://reference.wolfram.com/language/ref/LearnDistribution.html">fitting the list of colors of all the pixels</a> to a multivariate Gaussian distribution with a covariance matrix that represents their correlation. <a href="https://reference.wolfram.com/language/ref/RandomVariate.html">Sampling from this distribution</a> gives images like these—that indeed look somehow “statistically natural”, even if there isn’t appropriate detailed structure in them:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img5A.png" alt="" title="" width="630" height="158"> </p>
</div>
<p>So, OK, how can one do better? The basic idea is to use neural nets, which can in effect encode detailed long-range connections between pixels. In some way it’s similar to <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#inside-chatgpt">what’s done in LLMs like ChatGPT</a>—where one has to deal with long-range connections between words in text. But for images it’s structurally a bit more difficult, because in some sense one has to “consistently fit together 2D patches” rather than just progressively extend a 1D sequence.</p>
<p>And the typical way this is done at first seems a bit bizarre. The basic idea is to start with a random array of pixels—corresponding in effect to “pure noise”—and then progressively to “reduce the noise” to end up with a “reasonable image” that follows the patterns of typical images, all the while guided by some prompt that says what one wants the “reasonable image” to be of.</p>
<h3 id="attractors-and-inverse-diffusion">Attractors and Inverse Diffusion</h3>
<p>How does one go from randomness to definite “reasonable” things? The key is to use the <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-7--the-notion-of-attractors">notion of attractors</a>. In a very simple case, one might have a system—like this “mechanical” example—where from any “randomly chosen” initial condition one also evolves to one of (here) two definite (fixed-point) attractors:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img6.png" alt="" title="" width="573" height="62"> </p>
</div>
<p>One has something similar in a neural net that’s for example <a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/050b1a0a-f43a-4c28-b7e0-72607a918467/">trained to recognize digits</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img7.png" alt="" title="" width="649" height="30"> </p>
</div>
<p>Regardless of exactly how each digit is written, or noise that gets added to it, the network will take this input and evolve to an attractor corresponding to a digit. </p>
<p>Sometimes there can be lots of attractors. Like in <a href="https://www.wolframscience.com/nks/chap-6--starting-from-randomness#sect-6-2--four-classes-of-behavior">this (“class 2”) cellular automaton</a> evolving down the page, many different initial conditions can lead to the same attractor, but there are many possible attractors, corresponding to different final patterns of stripes:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img8.png" alt="" title="" width="665" height="75"> </p>
</div>
<p>The same can be true for example in 2D cellular automata, where now the attractors can be thought of as being different “images” with structure determined by the cellular automaton rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img9.png" alt="" title="" width="679" height="172"> </p>
</div>
<p>But what if one wants to arrange to have particular images as attractors? Here’s where the somewhat surprising idea of “stable diffusion” can be used. Imagine we start with two possible images, <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" "> and <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/B2helvetica.png" alt="" width="11" height=" ">, and then in a series of steps progressively add noise to them:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img10.png" alt="" title="" width="679" height="192"> </p>
</div>
<p>Here’s the bizarre thing we now want to do: train a neural net to take the image we get at a particular step, and “go backwards”, removing noise from it. The neural net we’ll use for this is somewhat complicated, with “<a href="https://reference.wolfram.com/language/ref/ConvolutionLayer.html">convolutional</a>” pieces that basically operate on blocks of nearby pixels, and “<a href="https://reference.wolfram.com/language/ref/AttentionLayer.html">transformers</a>” that get applied with certain weights to more distant pixels. Schematically in <a href="https://www.wolfram.com/language/">Wolfram Language</a> the <a href="https://resources.wolframcloud.com/FunctionRepository/resources/StableDiffusionSynthesize/">network looks at a high level like this</a>:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img11.png" alt="" title="" width="594" height="278"> </p>
</div>
<p>And roughly what it’s doing is to make an informationally compressed version of each image, and then to expand it again (through what is usually called a “U-net” neural net). We start with an untrained version of this network (say just randomly initialized). Then we feed it a couple of million examples of noisy pictures of <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" "> and <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/B2helvetica.png" alt="" width="11" height=" ">, and the denoised outputs we want in each case. </p>
<p>Then if we take the trained neural net and successively apply it, for example, to a “noised <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">”, the net will “correctly” determine that the “denoised” version is a “pure <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img12.png" alt="" title="" width="370" height="28"> </p>
</div>
<p>But what if we apply this network to pure noise? The network has been set up to always eventually evolve either to the “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">” attractor or the “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/B2helvetica.png" alt="" width="11" height=" ">” attractor. But which it “chooses” in a particular case will depend on the details of the initial noise—so in effect the network will seem to be picking at random to “fish” either “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">” or “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/B2helvetica.png" alt="" width="11" height=" ">” out of the noise:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img13.png" alt="" title="" width="572" height="164"> </p>
</div>
<p>How does this apply to our original goal of generating images “like” those found for example on the web? Well, instead of just training our “denoising” (or “inverse diffusion”) network on a couple of “target” images, let’s imagine we train it on billions of images from the web. And let’s also assume that our network isn’t big enough to store all those images in any kind of explicit way.</p>
<p>In the abstract it’s not clear what the network will do. But the remarkable empirical fact is that it seems to manage to successfully generate (“from noise”) images that “follow the general patterns” of the images it was trained from. There isn’t any clear way to “formally validate” this success. It’s really just a matter of human perception: to us the images (generally) “look right”. </p>
<p>It could be that with a different (alien?) <a href="https://www.wolframscience.com/nks/chap-10--processes-of-perception-and-analysis/">system of perception</a> we’d immediately see “something wrong” with the images. But for purposes of human perception, the neural net seems to give “reasonable-looking” images—perhaps not least because the neural net operates at least approximately like our brains and our processes of perception seem to operate. </p>
<h3 id="injecting-a-prompt">Injecting a Prompt</h3>
<p>We’ve described how a denoising neural net seems to be able to start from some configuration of random noise and generate a “reasonable-looking” image. And from any particular configuration of noise, a given neural net will always generate the same image. But there’s no way to tell what that image will be of; it’s just something to empirically explore, as we did above.</p>
<p>But what if we want to “guide” the neural net to generate an image that we’d describe as being of a definite thing, like “a cat in a party hat”? We could imagine “continually checking” whether the image we’re generating would be recognized by a neural net as being of what we wanted. And conceptually that’s what we can do. But we also need a way to “redirect” the image generation if it’s “not going in the right direction”. And a convenient way to do this is to mix a “description of what we want” right into the denoising training process. In particular, if we’re training to “recover an <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">”, mix a description of the “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">” right alongside the image of the “<img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/A2helvetica.png" alt="" width="11" height=" ">”.</p>
<p>And here we can make use of a key feature of neural nets: that ultimately they operate on arrays of (real) numbers. So whether they’re dealing with images composed of pixels, or text composed of words, all these things eventually have to be “ground up” into arrays of real numbers. And when a neural net is trained, what it’s ultimately “learning” is just how to appropriately transform these “disembodied” arrays of numbers. </p>
<p>There’s a fairly natural way to generate an array of numbers from an image: just take the triples of red, green and blue intensity values for each pixel. (Yes, we could pick a different detailed representation, but it’s not likely to matter—because the neural net can always effectively “learn a conversion”.) But what about a textual description, like “a cat in a party hat”?</p>
<p>We need to find a way to encode text as an array of numbers. And actually LLMs face the same issue, and we can solve it in basically <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#the-concept-of-embeddings">the same way here as LLMs do</a>. In the end what we want is to derive from any piece of text a “feature vector” consisting of an array of numbers that provide some kind of representation of the “effective meaning” of the text, or at least the “effective meaning” relevant to describing images. </p>
<p>Let’s say we train a neural net to reproduce associations between images and captions, as found for example on the web. If we feed this neural net an image, it’ll try to generate a caption for the image. If we feed the neural net a caption, it’s not realistic for it to generate a whole image. But we can look at the innards of the neural net and see the array of numbers it derived from the caption—and then use this as our feature vector. And the idea is that because captions that “mean the same thing” should be associated in the training set with “the same kind of images”, they should have similar feature vectors. </p>
<p>So now let’s say we want to generate a picture of a cat in a party hat. First we find the feature vector associated with the text “a cat in a party hat”. Then this is what we keep mixing in at each stage of denoising to guide the denoising process, and end up with an image that the image captioning network will identify as “a cat in a party hat”.</p>
<h3 id="the-latent-space-“trick”">The Latent Space “Trick”</h3>
<p>The most direct way to do “denoising” is to operate directly on the pixels in an image. But it turns out there’s a considerably more efficient approach, which operates not on pixels but on “features” of the image—or, more specifically, on a feature vector which describes an image. </p>
<p>In a “raw image” presented in terms of pixels, there’s a lot of redundancy—which is why, for example, image formats like JPEG and PNG manage to compress raw images so much without even noticeably modifying them for purposes of typical human perception. But with neural nets it’s possible to do much greater compression, particularly if all we want to do is to preserve the “meaning” of an image, without worrying about its precise details.</p>
<p>And in fact as part of training a neural net to associate images with captions, we can derive a “latent representation” of images, or in effect a feature vector that captures the “important features” of the image. And then we can do everything we’ve discussed so far directly on this latent representation—decoding it only at the end into the actual pixel representation of the image.</p>
<p>So what does it look like to build up the latent representation of an image? With the particular setup we’re using here, it turns out that the feature vector in the latent representation still preserves the basic spatial arrangement of the image. The “latent pixels” are much coarser than the “visible” ones, and happen to be characterized by 4 numbers rather than the 3 for RGB. But we can decode things to see the “denoising” process happening in terms of “latent pixels”:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071323sec7img14.png" alt="" title="" width="658" height="67"> </p>
</div>
<p>And then we can take the latent representation we get, and once again use a trained neural net to fill in a “decoding” of this in terms of actual pixels, getting out our final generated image.</p>
<h2 id="an-analogy-in-simple-programs">An Analogy in Simple Programs</h2>
<p>Generative AI systems work by having attractors that are carefully constructed through training so that they correspond to “reasonable outputs”. A large part of what we’ve done above is to study what happens to these attractors when we change the internal parameters of the system (neural net weights, etc.). What we’ve seen has been complicated, and, indeed, often quite “alien looking”. But is there perhaps a simpler setup in which we can see similar core phenomena?</p>
<p>By the time we’re thinking about creating attractors for realistic images, etc. it’s inevitable that things are going to be complicated. But what if we look at systems with much simpler setups? For example, consider a dynamical system whose state is characterized just by a single number—such as an iterated map on the interval, like <em>x</em> <img title="" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123rightarrow.png" alt="" width="16" height=" "> <em>a</em> <em>x</em> (1 – <em>x</em>).</p>
<p>Starting from a uniform array of possible <em>x</em> values, we can show down the page which values of <em>x</em> are achieved at successive iterations:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img1.png" alt="" title="" width="613" height="180"> </p>
</div>
<p>For <em>a</em> = 2.9, the system evolves from any initial value to a single attractor, which consists of a single fixed final value. But if we change the “internal parameter” <em>a</em> to 3.1, we now get two distinct final values. And at the “bifurcation point” <em>a</em> = 3 there’s a sudden change from one to two distinct final values. And indeed in our generative AI system it’s fairly common to see similar discontinuous changes in behavior even when an internal parameter is continuously changed.</p>
<p>As another example—slightly closer to image generation—consider (as above) a 1D cellular automaton that exhibits class 2 behavior, and evolves from any initial state to some fixed final state that one can think of as an attractor for the system:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img2.png" alt="" title="" width="575" height="148"> </p>
</div>
<p>Which attractor one reaches depends on the initial condition one starts from. But—in analogy to our generative AI system—we can think of all the attractors as being “reasonable outputs” for the system. But now what happens if we change the parameters of the system, or in this case, the cellular automaton rule? In particular, what will happen to the attractors? It’s like what we did above in changing weights in a neural net—but a lot simpler.</p>
<p>The particular rule we’re using here has 4 possible colors for each cell, and is defined by just 64 discrete values from 0 to 3. So let’s say we randomly change just one of those values at a time. Here are some examples of what we get, always starting from the same initial condition as in the first picture above: </p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img3.png" alt="" title="" width="596" height="285"> </p>
</div>
<p>With a couple of exceptions these seem to produce results that are at least “roughly similar” to what we got without changing the rule. In analogy to what we did above, the cat might have changed, but it’s still more or less a cat. But let’s now try “progressive randomization”, where we modify successively more values in the definition of the rule. For a while we again get “roughly similar” results, but then—much like in our cat examples above—things eventually “fall apart” and we get “much more random” results:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img4.png" alt="" title="" width="596" height="285"> </p>
</div>
<p>One important difference between “stable diffusion” and cellular automata is that while in cellular automata, the evolution can lead to continued change forever, in stable diffusion there’s an annealing process used that always makes successive steps “progressively smaller”—and essentially forces a fixed point to be reached.</p>
<p>But notwithstanding this, we can try to get a closer analogy to image generation by looking (again as above) at 2D cellular automata. Here’s an example of the (not-too-exciting-as-images) “final states” reached from three different initial states in a particular rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img5.png" alt="" title="" width="573" height="181"> </p>
</div>
<p>And here’s what happens if one progressively changes the rule:</p>
<div>
<p><img loading="lazy" src="https://content.wolfram.com/uploads/sites/43/2023/07/sw071123sec8img6.png" alt="" title="" width="594" height="229"> </p>
</div>
<p>At first one still gets “reasonable-according-to-the-original-rule” final states. But if one changes the rule further, things get “more alien”, until they look to us quite random. </p>
<p>In changing the rule, one is in effect “<a href="https://writings.stephenwolfram.com/2022/03/on-the-concept-of-motion/#motion-in-rulial-space">moving in rulial space</a>”. And by looking at how this works in cellular automata, one can get a certain amount of intuition. (<a href="https://www.wolframscience.com/nks/p391--fundamental-issues-in-biology/">Changes to the rule</a> in a cellular automaton seem a bit like “changes to the genotype” in biology—with the behavior of the cellular automaton representing the corresponding “phenotype”.) But seeing how “rulial motion” works in a generative AI that’s been trained on “human-style input” gives a more accessible and humanized picture of what’s going on, even if it seems still further out of reach in terms of any kind of traditional explicit formalization.</p>
<h2 id="thanks">Thanks</h2>
<p>This project is the first I’ve been able to do with our new <a href="https://www.wolframinstitute.org/">Wolfram Institute</a>. I thank our Fourmilab Fellow Nik Murzin and Ruliad Fellow Richard Assar for help. I also thank Jeff Arle, Nicolò Monti, Philip Rosedale and the Wolfram Research Machine Learning Group.</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Businessmen Broke Hollywood (160 pts)]]></title>
            <link>https://www.theatlantic.com/ideas/archive/2023/07/hollywoods-cruel-strategy/674730/</link>
            <guid>36767606</guid>
            <pubDate>Tue, 18 Jul 2023 03:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/ideas/archive/2023/07/hollywoods-cruel-strategy/674730/">https://www.theatlantic.com/ideas/archive/2023/07/hollywoods-cruel-strategy/674730/</a>, See on <a href="https://news.ycombinator.com/item?id=36767606">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body"><p>The Hollywood machine—from script writing, to shooting and production, to late-night talk-show PR—has officially ground to a halt.</p><p>On Thursday,<a data-event-element="inline link" href="https://abcnews.go.com/US/sag-aftra-national-board-votes-approve-strike/story?id=101210303#:~:text=More%20than%20160%2C000%20members%20of,to%20hit%20the%20picket%20lines."> the actors went on strike. The 160,000 members</a> of SAG-AFTRA, led by Fran Drescher, the fearless sitcom nanny, stopped working after talks with the studios collapsed. They join the ranks of the Writers Guild of America, whose members (myself included) have been on strike since May.</p><p>Our two unions have not been on strike together since 1960. The writers’ pickets at shooting locations had already shut down an estimated 80 percent of productions. Now SAG’s strike rules dictate that actors not only can’t shoot or do voice-over work for productions; they also cannot attend red carpets or promote any Motion Picture Association projects—something that was already a challenge, given that the writers’ strike had shut down the nighttime talk shows that were such a staple of the press circuit.</p><p>Much like the writers, actors are looking for increases in their residual pay—compensation that’s akin to royalty checks—once-reliable income that has all but vanished in the pivot to streaming. Actors are also seeking protections against artificial intelligence using their voice and image.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/ideas/archive/2023/04/writers-guild-of-america-strike-residuals-pay-streaming/673876/">Xochitl Gonzalez: ‘The whole thing starts with us’</a></p><p>Bob Iger, Disney’s CEO, called these expectations “just not realistic.” He accused the strikers of “adding to a set of challenges that this business is already facing that is quite frankly very disruptive and dangerous.”</p><p>This was a bit rich, coming days after a studio executive told <a data-event-element="inline link" href="https://deadline.com/2023/07/writers-strike-hollywood-studios-deal-fight-wga-actors-1235434335/"><i>Deadline</i></a> that their strategy was “to allow things to drag on until union members start losing their apartments and losing their houses.”</p><p>Eviction is a pretty cruel labor-negotiation strategy.</p><p>Hollywood’s CEOs <i>are </i>suffering. Not primarily from labor disputes or industry disruption or public-relations issues, but from vincible ignorance, which seems to be endemic in C-suites of all industries. Under pressure to deliver to Wall Street, too many CEOs have lost the plot of their own movie. They are not running companies to profitably deliver a good product, such as a book or a cup of coffee or, in this case, a movie or TV show. They are running companies to deliver good profit. The quality of their product has ceased to matter.</p><p>If you doubt this, consider that when Emmy nominations were announced last week, the lions’ share went to HBO Max, a prestige platform that has <a data-event-element="inline link" href="https://www.insiderintelligence.com/content/charting-new-course-adapting-advertising-landscape-after-hbo-max-discovery-merger">ceased to exist by that name</a>, because Warner Bros. Discovery took the streaming arm of the legacy brand and folded it into a messy app crowded with low-budget reality programs. We are in the upside-down.</p><p>Writers and actors have been caught up in the pivot to streaming, the mad logic of which has <a data-event-element="inline link" href="https://variety.com/2023/tv/news/writers-guild-contract-negotiation-mini-room-1235568173/">upended long-standing working practices</a>, slowly begun to replace human instinct with<a data-event-element="inline link" href="https://www.theverge.com/2019/5/28/18637135/hollywood-ai-film-decision-script-analysis-data-machine-learning"> artificial intelligence</a>, and obliterated workers’ income streams.</p><p>The actor Mark Proksch, for example, made more money off residuals from one season of guest appearances on <i>The Office</i>,<i> </i>under the old system, than he has in five seasons of starring in <i>What We Do in the Shadows</i>, under the new system<i>.</i></p><p>Now, just as Hollywood workers are arguing that we need to adjust our compensation models to fit the streaming era, the studios are telling us that we cannot be fairly paid, because the streaming model is broken. And we’re being told this by the very studio executives—many of them <a data-event-element="inline link" href="https://deadline.com/2023/05/ceo-pay-wga-writers-strike-1235351572/">multimillionaires</a>—who broke it.</p><p>This is another aspect of C-suite ignorance: Bonkers executive compensation has utterly detached leaders from the lives of the people they employ. The fact that David Zaslav, the CEO of Warner Bros. and Discovery, earned $247 million in 2021 makes it very hard to swallow his refusal to budge on issues that are costing middle-class actors thousands of dollars a year in lost income.</p><p>You can argue all you like about whether anyone should ever earn this much, but these are leaders who have made some disastrous business decisions.</p><p>The pivot to streaming was extremely profitable for the brief moment when everyone was trapped at home during a pandemic. People couldn’t spend money on concerts or eating out or traveling, so they felt comfortable spending an abnormal amount on streaming services.</p><p>Hollywood CEOs saw the success of Netflix and raced to copy a model without knowing whether it was sustainable, a model that relied on the constant production of new (and costly) entertainment content created by unionized talent. They were wrong about the business, but they were even more wrong to presume that labor would comply. The actors and writers didn’t make this pivot; why should they pay the price?</p><p id="injected-recirculation-link-1" data-view-action="view link - injected link - item 2" data-event-element="injected link" data-event-position="2"><a href="https://www.theatlantic.com/culture/archive/2023/05/hollywood-writers-strike-wga/673926/">Read: Why you should pay attention to the Hollywood writers’ strike</a></p><p>If the pivot to streaming was such a mistake that these businesses truly are going under—a case that’s hard to make, given the size of these executives’ compensation packages—we will have to suffer too. But if we suffer during lean times, we should also share in the profits during fat ones. That is what the negotiations are about. The only way that executives will be able to right this ship is to return to making unmissable programming, and they won’t be able to do that without us.</p><p>Absent good script writers, Hollywood executives have taken their lines from Marie Antoinette. But the revolutionaries are already outside, dismantling the palace. In London, the cast of <i>Oppenheimer</i> <a data-event-element="inline link" href="https://www.cbsnews.com/news/oppenheimer-stars-walk-out-london-premiere-actors-strike-emily-blunt-matt-damon-florence-pugh-cillian-murphy/">walked out</a> of the film’s premiere. Press tours for <i>Barbie</i> have been halted; even the stars’ pink-laden social-media accounts have gone dark. The Emmys will likely be <a data-event-element="inline link" href="https://www.nytimes.com/2023/06/21/business/media/emmy-awards-writers-strike.html">postponed</a>. Comic-Con will be sans actors or writers. I am desperately hopeful that the studios will realize sooner rather than later that even if it hurts shareholders for a time, good entertainment, long-term, is always good business.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A few things to know before stealing my 914 (2022) (341 pts)]]></title>
            <link>https://www.hagerty.com/media/advice/a-few-things-to-know-before-you-steal-my-914/</link>
            <guid>36767092</guid>
            <pubDate>Tue, 18 Jul 2023 02:32:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hagerty.com/media/advice/a-few-things-to-know-before-you-steal-my-914/">https://www.hagerty.com/media/advice/a-few-things-to-know-before-you-steal-my-914/</a>, See on <a href="https://news.ycombinator.com/item?id=36767092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										<p>Dear Thief,</p>
<p>Welcome to my Porsche 914. I imagine that at this point (having found the door unlocked) your intention is to steal my car. Don’t be encouraged by this; the tumblers sheared off in 1978. I would have locked it up if I could, so don’t think you’re too clever or that I’m too lazy. However, now that you’re in the car, there are a few things you’re going to need to know. First, the battery is disconnected, so slide-hammering my ignition switch is not your first step. I leave the battery disconnected, not to foil hoodlums such as yourself, but because there is a mysterious current drain from the 40-year-old German wiring harness that I can’t locate and/or fix. So, connect the battery first. Good luck finding the engine cover release. Or the engine, for that matter.</p>
<p>Now, you can skip your slide hammer. The ignition switch’s tumblers are so worn that any flat-bladed screwdriver or pair of scissors will do. Don’t tell anyone.</p>
<p>Once you’ve figured that out and try to start the car, you’ll run into some trouble. The car is most likely in reverse gear, given that the parking brake cable froze up sometime during the Carter administration. Since there is not a clutch safety switch on the starting circuit, make sure to press the clutch down before you try to crank the engine. (I don’t want you running into my other car in the driveway.) This is doubly necessary because my starter is too weak to crank the clutch-transmission input shaft assembly with any success.</p>
<p>With the clutch pedal depressed, the engine should turn over fast enough to get things going. But first, you’ll need to press the gas pedal to the floor exactly four times. Not three. Not five. Four. The dual Webers don’t have chokes and you’ll be squirting fuel down the barrels with the accelerator pumps for the necessary priming regime. If you don’t do it right, the car won’t start before the battery gives up the ghost. Consider yourself forewarned.</p>
<figure>
			<img decoding="async" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640" alt="Porsche 914 front three-quarter" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=768&amp;ixlib=php-3.3.0&amp;w=1024 1024w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=225&amp;ixlib=php-3.3.0&amp;w=300 300w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=576&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1152&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=2048 2048w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-15-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=120 120w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1">		<figcaption>
			<span>Norman Garrett</span>		</figcaption>
	</figure>

<p>If you’ve followed along so far, the engine should fire right up. Don’t be fooled—it will die in eight seconds when the priming fuel runs out. Repeat the gas pedal priming procedure, but only pump two times. Deviate from this routine at your own peril.</p>
<p>Now you have the engine running. Make sure the green oil light in the dash goes out. If it does not, you only have about 100 yards to drive before the engine locks up, so be attentive. If all goes well with the oil pressure, you may now attend to the gear shift lever. Some explanation follows.</p>
<p>This is a Porsche 914. It has a mid-engine layout. The transmission is in the far back of the car, and the shift linkage’s main component is a football-field-long steel rod formed loosely in the shape of your lower intestine. Manipulating the gear shift lever will deliver vague suggestions to this rod, which, in turn, will tickle small parts deep within the dark bowels of the transaxle case. It is akin to hitting a bag of gears with a stick, hopefully finding one that works.</p>
<figure id="attachment_212556" aria-describedby="caption-attachment-212556"><img decoding="async" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640" alt="Porsche 914 drivetrain" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=768&amp;ixlib=php-3.3.0&amp;w=1024 1024w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=225&amp;ixlib=php-3.3.0&amp;w=300 300w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=576&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1152&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=2048 2048w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-3-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=120 120w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1"><figcaption id="caption-attachment-212556">I’ll make sure the drivetrain is in the car, by the way. <span>Norman Garrett</span></figcaption></figure>
<p>If you are successful in finding first gear (there is a shift pattern printed on the knob; they say German engineers don’t have a sense of humor), congratulations. You may launch the vehicle into motion.</p>
<p>Do not become emboldened by your progress, as you will quickly need to shift to another gear. Ouija boards are more communicative than the shift knob you will be trusting to aid your efforts. Depress the clutch as you would in any car, and pull the knob from its secure location out of first gear. Now you will become adrift in the zone known to early Porsche owners as “Neverland” and your quest will be to find second gear. Prepare yourself for a ten-second-or-so adventure. Do not go straight forward with the shift knob, as you will only find Reverse waiting there to mock you with a shriek of high-speed gear teeth machining themselves into round cylinders. Should you hear this noise, retreat immediately to the only easy spot to find in this transmission: neutral. This is a safe place, no real damage can occur here, but alas, no forward motion will happen either. From this harbor of peace, you can re-attempt to find second, but you may just want to go for any “port in a storm”, given that the traffic behind you is now cheering you on in your quest with vigorous horn-honks of support and encouragement. Most 914 owners at this point pull over to the side of the road and feign answering a cell phone call to a) avoid further humiliation; b) allow traffic to pass; and c) gather the courage for another first gear start. You may choose to do likewise.</p>
<figure>
			<img decoding="async" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640" alt="Porsche 914 front three-quarter" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=768&amp;ixlib=php-3.3.0&amp;w=1024 1024w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=225&amp;ixlib=php-3.3.0&amp;w=300 300w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=576&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1152&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=2048 2048w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-8-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=120 120w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1">		<figcaption>
			<span>Norman Garrett</span>		</figcaption>
	</figure>

<p>If you press onward without taking a break, you may re-enter first. This is how the car mocks you for your lack of skill, but sometimes it is the only path forward. Once you are ready to again try for second, I can offer some advice. One trick that works is to declutch the transmission, pull the lever from the first-gear position, enter into the aforementioned neutral zone, and then rapidly wig-wag the shift knob side-to-side along a lateral axis. If you move the knob quickly enough, the transmission will be out-smarted and cannot anticipate your next move. It is at this time that you should re-attempt to enter second, and most likely you will do so. Surprise is your best weapon against this transmission.</p>
<p>The move to third should be straightforward, as it’s the only easily-accessible gear in the set. You should now be out of my neighborhood and on the main four-lane road. Third gear will be good for 45 mph, so I would advise you just staying there. Trying to get to fourth gear will only frustrate you and your nearby drivers (see: first-to-second shift).</p>
<p>You don’t need to check for gasoline in the car. It will be full, even though the fuel gauge reads zero. The odometer reads “0”, not because it was reset when I filled the tank, but because it is just broken. Ignore it. If it is night, and it most likely will be, you will need to turn on the lights. I’ll leave it to you to find the switch since I’ve helped a lot so far. Suffice to say that once you get them active, you will find that the seven inch sealed beams from 1971 will only illuminate sufficient roadway for travel below 45 mph. Since you are still in third , this shouldn’t be a problem. Oh, and the lights only work on high beam, so ignore the flashing lights and vulgar gestures from opposing traffic.</p>
<figure>
			<img decoding="async" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640" alt="Porsche 914 front three-quarter" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=768&amp;ixlib=php-3.3.0&amp;w=1024 1024w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=225&amp;ixlib=php-3.3.0&amp;w=300 300w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=576&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1152&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=2048 2048w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=480&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-9-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=120 120w" sizes="(max-width: 640px) 100vw, 640px" data-recalc-dims="1">		<figcaption>
			<span>Norman Garrett</span>		</figcaption>
	</figure>

<p>By now you’ve certainly noticed the smell. That is the aroma of Mobil 1 oil being boiled off of long sections of horizontal exhaust pipes, which were cleverly encased by the factory with a second shroud of oil-holding chambers. They filled with oil during my last drive and you are now operating a small thermal refinery that is making light short-chained vaporous hydrocarbons from what was once $8-a-quart oil. They are being conveniently routed to the cabin through carefully formed channels in the heating system, plus the rust holes in the floor provided by Mother Nature herself over the past few decades.</p>
<p>You’ll feel less dizzy if you open a window. But mind that driver’s window does not work, so you’ll have to lean over and roll down the passenger window half-way. I say half-way in a manner that will become apparent once you try to get the window to go <em>all</em> the way down, which it will refuse to do. Instead, simply open the driver’s door slightly and drive along, as I do. Once the oil vapors are exhumed from the cabin, you should start to feel a little better. There is a rag behind the driver’s seat that you can use to wipe the oil film off of the inside of the windshield.</p>
<p>Knowing which road you’re probably on by now, you will be hitting stop lights. Try as hard as you can to not bring the 914 to a stop. The brake system is ideal for this situation, being known more as “scrubbers” than “brakes”. Since you can’t effectively stop the car, use this to your advantage and don’t try. Remember: You certainly don’t want to have to go back into first.</p>
<p>If you have made it within sight of to the highway entrance, don’t get any ideas. The front right wheel is severely bent and the vibration at velocities above 50 mph will crack the windshield and cause the doors to open by themselves. So stay on the surface streets, stoplights notwithstanding.</p>
<p>It may be at this point that you consider abandoning the car to avoid further calamity. There is an Exxon station right before the freeway entrance. The last guy who stole my 914 used this very spot and it was rather convenient for all concerned parties. I suggest you ditch the car there and scope out a nice, reliable Camry to heist.</p>

		
		<div id="gallery-1"><figure>
			<p><img width="1920" height="2560" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;ixlib=php-3.3.0" alt="Porsche 914 welding" decoding="async" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;ixlib=php-3.3.0&amp;w=1920 1920w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=300&amp;ixlib=php-3.3.0&amp;w=225 225w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1024&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=1152 1152w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=2048&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=853&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-2-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=68 68w" sizes="(max-width: 1920px) 100vw, 1920px">
			</p>
				<figcaption>
				 <span>Norman Garrett</span>
				</figcaption></figure><figure>
			<p><img width="1920" height="2560" src="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;ixlib=php-3.3.0" alt="Porsche 914 rear" decoding="async" srcset="https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;ixlib=php-3.3.0&amp;w=1920 1920w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=300&amp;ixlib=php-3.3.0&amp;w=225 225w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1024&amp;ixlib=php-3.3.0&amp;w=768 768w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=1536&amp;ixlib=php-3.3.0&amp;w=1152 1152w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=2048&amp;ixlib=php-3.3.0&amp;w=1536 1536w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=853&amp;ixlib=php-3.3.0&amp;w=640 640w, https://hagerty-media-prod.imgix.net/2022/03/Garrett-Porsche-914-4-scaled.jpg?auto=format%2Ccompress&amp;fit=crop&amp;h=90&amp;ixlib=php-3.3.0&amp;w=68 68w" sizes="(max-width: 1920px) 100vw, 1920px">
			</p>
				<figcaption>
				 <span>Norman Garrett</span>
				</figcaption></figure>
		</div>

<p><em>Norman Garrett was the Concept Engineer for the original Miata back in his days at Mazda’s Southern California Design Studio. He currently teaches automotive engineering classes at UNC-C’s Motorsports Engineering Department in Charlotte, North Carolina and curates his small collection of dysfunctional automobiles and motorcycles.</em></p>
				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Drivers Cooperative: New York’s driver-owned ride-hailing app (271 pts)]]></title>
            <link>https://nextcity.org/urbanist-news/new-yorks-driver-owned-ride-hailing-app-is-putting-its-foot-on-the-accelera</link>
            <guid>36766844</guid>
            <pubDate>Tue, 18 Jul 2023 02:01:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nextcity.org/urbanist-news/new-yorks-driver-owned-ride-hailing-app-is-putting-its-foot-on-the-accelera">https://nextcity.org/urbanist-news/new-yorks-driver-owned-ride-hailing-app-is-putting-its-foot-on-the-accelera</a>, See on <a href="https://news.ycombinator.com/item?id=36766844">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Sometimes the money’s good, sometimes not so good. Either way, Shaun Beckles loves the human aspect of working as a for-hire vehicle driver — getting to know fares even just a little in passing, gaining a glimpse into so many different lives over the course of a single shift.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“I’ve always liked driving for the mere fact that you get to meet some interesting people who can connect with your passion, you’d be surprised,” says Beckles, now operations manager at The Drivers Cooperative. The driver-owned ride-hailing platform, which </span><a href="https://nextcity.org/podcast/this-ride-hailing-app-alternative-is-owned-by-the-drivers">launched</a> in 2021, is hitting some major milestones this year: It has a new app, it’s the official transportation partner for Juneteenth NY, and – per new revenue data provided exclusively to Next City – it’s nearly breaking even financially. The co-op expects to see its first profits this year.</p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Beckles moved to New York from Guyana as a teen in the ‘80s, and quickly took to driving as a way to help make ends meet. He’d sit in his Dodge Shadow outside the popular Ess-a-Bagel shop in Manhattan’s posh Gramercy Park neighborhood, waiting for a call on the radio from dispatch. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Though Beckles eventually moved into the telecommunications industry, he later took up driving part-time for a ride-hailing platform as an opportunity to do something he loved while earning some income to help cover the ever-rising cost of living.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“You couldn’t pass a billboard without the enticing $5,000 a month or make this crazy amount of money and in such little time,” Beckles says. “So I got caught by that –and then quickly realized it was also an exploiting situation.”</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">It wasn’t just the failed promise of $5,000 a month. There was also the impersonal nature of working for a company that seemed more concerned with its investors than its drivers.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Now, as one of the top executives at The Drivers Cooperative, Beckles relishes the opportunity to run a ride-hailing company where he interfaces with drivers regularly. Some sit on the co-op’s board of directors. Many others participate regularly in meetings as part of running the fast-growing startup: meetings about the company’s recently overhauled app, </span><a href="https://drivers.coop/download">“Co-Op Ride,”</a> meetings about setting the co-op’s minimum hourly wage, or meetings to discuss setting up a $1 surcharge on each trip to help other co-op members purchase electric vehicles. And sometimes it’s just conversations between peers, colleagues and friends.</p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“There is no other company where drivers can connect with an operations leader or finance leader and have a one-on-one conversation about either struggles on the road or their expectations for the week,” Beckles says. “That for me is a big differentiator for why members are here, regardless of the growing pains, because they’re not going to be able to have someone listen and adapt to what they’re trying to accomplish for themselves.”</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">In 2022, its first full year of operation, The Driver’s Cooperative earned $5.9 million in revenue from 162,294 successful trips — and $5.2 million of that went directly to driver wages. That 2022 revenue is 12 times what the co-op took in for 2021, though it didn’t launch until the end of May 2021. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Even with its minimum hourly wage of $30 an hour, the co-op is nearly breaking even, recording a net loss of just $318,000 in 2022. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">With the growth it’s already seeing in 2023, The Drivers Cooperative expects to earn its first annual profit — some of which will be distributed back out as dividends to drivers. Currently there are 9,000 drivers who are or will soon be members of the co-op — an estimated 15% percent of the total ride-hailing platform driver workforce in New York City. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The co-op’s leadership team now consists entirely of former drivers, all of whom are people of color, including Beckles, driver engagement manager David Alexis and finance manager Cynette Wilson.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Born and raised in Chicago, Wilson started out in ride-hailing in New York as a driver with one of the co-op’s behemoth rivals back in 2018. She recently had the opportunity to walk her fellow driver-owners through all the good financial news in their annual membership meeting, which she followed up with an internal fireside chat about the co-op’s finances. She plans to make the fireside chats a quarterly event for member-owners of the Drivers Cooperative.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“I went through the [profit and loss statement] with the membership, took any questions they had about any particular line items and painted the picture of what profitability looks like, what it’s going to take for us to get there and how close we are,” Wilson says. </span></p>
<h3 dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Taking on the competition</span></h3>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The Drivers Cooperative got to where it is by mapping out a phased growth path for its business model. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The co-op realized early on that it couldn’t initially compete on the same terms with Uber or Lyft because it didn’t have access to the same pool of deep-pocketed venture capital investors. It didn’t have what those kinds of investors eventually want — the chance to go public and get listed on a stock exchange, allowing early investors to cash out their shares for five times, ten times or some other crazy multiple of their startup investment. Those are the investors that can fund a massive marketing blitz to riders (and drivers) and an army of software engineers working out bugs in the app almost in real time. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The Drivers Cooperative does have outside investors: It raised $1.6 million from more than 1,100 individuals through the crowdfund investment platform WeFunder </span><a href="https://wefunder.com/driverscoop">last year</a>. But those investors are limited to earning back 2.5 times what they invested, as opposed to typical venture capital investors seeking five, 10 or 100 times what they invest in a company. Limiting returns to early stage investors gives the co-op the financial flexibility to do things like set its $30 an hour minimum wage or create an internal fund to help drivers purchase electric vehicles. Not only that, venture capital investors typically take a strong role on the board of a company, so the probability that an investor-oriented company proactively does those things for drivers is much lower.</p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The market entry bar is also higher now that the ride-hailing giants are so established. Riders expect a certain level of functionality – and the shortest possible wait times for a ride. That means any business in this space somehow needs to have a critical mass of drivers signed up behind the scenes before gaining any real traction from ride-hailing. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">So The Drivers Cooperative instead focused first on prescheduled trips: primarily government-funded paratransit, non-emergency medical transportation, and staff transportation for larger businesses. The Drivers Cooperative estimates the overall market for prescheduled trips to be about 30,000 total trips per day across New York City, representing $300 million in revenue.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Focusing on prescheduled trips allowed the co-op to establish its $30-an-hour minimum wage. It works by asking drivers to sign up for a four-, six- or eight-hour shift for the next day. As long as The Drivers Cooperative can calculate at least one day in advance how many drivers it will have for the next day, it can calculate how many prescheduled trip appointments it can schedule based on pick up and drop off locations and estimated drive times. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Scheduling trips is a tricky calculation, and it all comes down to routing — you want drivers to spend as much of their shift as they can actually driving clients around, but you have to leave enough time in-between trips so the driver isn’t late for their next client. So far, the only way to do this successfully has been by using a team of human dispatchers, though the co-op is working on an automated solution using artificial intelligence. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Thanks to the predictability of the overall market for prescheduled trips, The Drivers Cooperative was able to set its $30-an-hour minimum wage. Most of the time, the revenues from each driver’s prescheduled trips on a shift net the driver at least that amount, sometimes more. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">There are some rare occasions when a driver’s prescheduled trips don’t net enough to cover the minimum. When that happens, the co-op has an automated mechanism to pay however much is necessary to ensure the promised minimum hourly wage for the driver. In 2022, the co-op made about $70,000 in extra payouts to make up for prescheduled shifts that didn’t bring in enough revenue to cover the $30-an-hour minimum wage.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“We have a driver-centric focus because we were drivers,” Wilson says. “So even now with the design of how we improve our pay flows, or with the new app, all these little tweaks and minute differences, that makes such a big difference in the experience of the driver.”</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">The Drivers Cooperative is now banking on that $30-an-hour minimum wage as one of the pillars of its driver recruiting efforts, key to getting its ride-hailing wait times down to the point where it can compete with the incumbent ride-hailing apps. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">There are an estimated 600,000 ride-hailing trips per day in New York City, representing $4.4 billion in revenue. As part of its early marketing shift to the broader ride-hailing market, the co-op was just announced as the </span><a href="https://www.newswire.com/news/confirmed-schedule-of-events-sponsors-for-the-14th-annual-juneteenth-22054808">official transportation partner for Juneteeth NY’s</a> upcoming weekend of festivities — offering 5% discounts on rides to events and also free trips for Juneteenth NY festival staff.</p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Other recruiting pillars include the Drivers Cooperative’s various programs for drivers, like the fund to help purchase electric vehicles, which includes low-interest or zero-interest vehicle loans through partnerships with the Lower East Side People’s Federal Credit Union and the Hebrew Free Loan Society. Members of the Drivers Cooperative can also refinance any existing high-interest predatory vehicle loans with a lower-interest loan from one of the lending partners. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">And of course, there’s also the opportunity to join a cooperative and participate in it as a member-owner, meeting face-to-face with top executives on a regular basis to help shape the company. As part of the board of directors and other committees, drivers got to be part of interviewing and choosing Wilson to manage the co-op’s finances. </span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">She still likes to take a driving shift every now and then – just this month, she took on a Wednesday shift.</span></p>
<p dir="ltr"><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">“Being cooperative, everyone does have a voice and their voice is important,” Wilson says. “Especially in finance, to actually open the door and invite people in and create a space where they’re comfortable to come visit at least once a quarter is important. These are those things where we keep a finger on the pulse…keep the humanity in the face of all the technology.”</span></p>
<p dir="ltr"><em><span id="docs-internal-guid-cbbe18b9-7fff-3c2a-99d8-f10a83c1bf3f">Reporting for this story was made possible with funding from the Mastercard Impact Fund in partnership with the </span><a href="https://bit.ly/33IMQgt">Mastercard Center for Inclusive Growth</a>.</em></p>



<div>
<p><img src="https://nextcity.org/images/made/OscarPerryAbello_NextCityHeadshot_April_2019_200_200_80_c1.jpg" alt=""></p><p>Oscar is Next City's senior economic justice&nbsp;correspondent. He previously served as&nbsp;Next City’s editor from&nbsp;2018-2019, and was a Next City Equitable Cities Fellow from 2015-2016. Since 2011, Oscar has covered community development finance,&nbsp;community banking, impact investing, economic development, housing&nbsp;and more for media outlets such&nbsp;as Shelterforce, B Magazine, Impact Alpha&nbsp;and Fast Company.</p>
<p><i></i><a href="https://twitter.com/oscarthinks">Follow Oscar</a> <i></i><span data-eeencemail_wazgzulwfw="1">.(JavaScript must be enabled to view this email address)</span></p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The HTML review is an annual journal of literature made to exist on the web (105 pts)]]></title>
            <link>https://thehtml.review/</link>
            <guid>36766816</guid>
            <pubDate>Tue, 18 Jul 2023 01:57:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thehtml.review/">https://thehtml.review/</a>, See on <a href="https://news.ycombinator.com/item?id=36766816">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
            <a data-contributor="Maxwell Neely-Cohen" href="https://thehtml.review/02/a-lettter-from-the-editor.html">
                <div>
                    <div>
                        <p>letter</p>
                        <p>
                            <span>M</span>
                            <span>a</span>
                            <span>x</span>
                            <span>w</span>
                            <span>e</span>
                            <span>l</span>
                            <span>l</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>N</span>
                            <span>e</span>
                            <span>e</span>
                            <span>l</span>
                            <span>y</span>
                            <span>-</span>
                            <span>C</span>
                            <span>o</span>
                            <span>h</span>
                            <span>e</span>
                            <span>n</span>
                        </p>
                    </div>
                    <h3>
                        A Letter from<br>the Editor
                    </h3>
                    <p>
                            welcome to the second issue of <em>the html review</em>
                        </p>
                    
                    
                </div>
            </a>

            <a data-contributor="Katy Ilonka Gero" href="https://thehtml.review/02/blank-page-with-machine.html">
                <div>
                    <div>
                        <p>
                            <svg viewBox="0 0 350 310">
                            <g transform="scale(1.5)">
                            <path id="spiral" stroke="#000" stroke-width=".5" fill="none" d="m 116.34334,98.285749 c 0.6985,-0.16626 1.3893,-0.20301 1.97427,-1.0033 0.63693,-0.87399 0.14047,-1.9244 -1.01215,-1.8592 -2.63814,0.14932 -4.59701,4.675521 -1.15454,5.135411 4.26797,0.56997 6.91187,-6.131791 2.30909,-7.390631 -3.10573,-0.84936 -5.97285,2.04894 -6.63287,4.92529 -0.34251,1.48918 0.10584,3.388791 1.44126,4.153671 3.89082,2.23078 9.42302,-1.37333 9.24021,-5.693061 -0.0654,-1.5417 0.22129,-2.88829 -1.16224,-3.86696 -2.48035,-1.75163 -6.27688,-2.45918 -8.62638,-0.14047 -2.3572,2.32584 -3.72726,5.13523 -2.99989,8.433191 0.24245,1.10586 0.46951,2.06837 1.43163,2.76148 3.45979,2.49613 6.74255,0.24207 9.80979,-1.45377 0.86014,-0.47528 1.62021,-0.82473 2.29177,-1.50841 1.651,-1.680831 1.5875,-3.134781 1.37776,-5.379601 -0.12892,-1.39162 -0.13277,-2.96257 -1.20457,-3.97761 -2.24367,-2.12686 -8.61676,-3.04838 -11.31647,-1.40931 -0.53302,0.32308 -0.91594,0.70485 -1.32773,1.15435 -1.2777,1.3995 -1.52977,3.44574 -2.2225,5.19469 -1.05449,2.65988 -1.90308,7.687151 0.52917,9.904271 2.05509,1.87151 5.13003,0.89073 7.44681,0.24514 3.06532,-0.85417 6.85993,-2.62697 9.00353,-4.97128 2.62659,-2.874241 3.1827,-7.846671 0.35022,-10.909871 -1.68949,-1.8265 -4.24681,-1.88538 -6.46738,-2.44918 -3.86965,-0.98252 -7.95674,-2.96718 -11.33379,0.50127 -1.81456,1.86555 -1.21805,4.27297 -1.96273,6.5251 -1.03485,3.12747 -3.47903,8.513231 -0.92363,11.353031 0.71966,0.79875 1.46434,1.77742 2.48227,2.16631 2.16862,0.82853 5.44753,0.36479 7.69697,0.0877 2.38798,-0.29441 4.445,-1.39854 6.54242,-2.48805 4.29876,-2.23212 9.66355,-5.53854 9.21905,-11.311461 -0.11353,-1.45358 -1.12376,-2.74109 -1.93002,-3.84849 -0.66001,-0.90747 -1.09104,-2.01006 -1.81071,-2.86847 -1.11606,-1.32907 -2.89406,-1.66023 -4.5162,-2.02391 -3.07301,-0.68907 -6.26341,-2.14682 -9.42878,-2.22442 -1.19111,-0.02925 -3.22696,-0.32982 -4.23334,0.30557 -1.27962,0.80799 -2.15322,1.89402 -3.25197,2.99546 -1.89711,1.90442 -1.23363,4.74383 -2.20287,7.08698 -1.4326,3.46364 -2.62698,6.377711 -2.62698,10.198491 0,1.47974 -0.10333,2.95583 0.76374,4.22506 0.62191,0.91017 1.6635,1.20573 2.6999,1.47628 2.22827,0.5815 4.02744,1.03751 6.35,1.03351 3.65414,-0.006 6.88879,-1.41567 10.19849,-2.80478 2.99604,-1.25672 5.92666,-2.48131 8.25692,-4.73267 0.74468,-0.71909 1.43164,-1.54748 2.1032,-2.46861 1.47012,-2.016421 3.62527,-6.770261 2.35527,-9.236371 -0.81011,-1.57499 -2.06664,-2.12167 -3.25005,-3.30681 -0.98521,-0.98425 -1.45087,-2.48227 -2.54577,-3.3474 -0.97559,-0.76989 -2.47457,-0.97329 -3.64836,-1.36352 -3.49635,-1.16109 -7.09276,-1.988132 -10.77576,-1.988322 -2.02815,-1.93e-4 -4.53736,-0.96347 -6.35,0.186462 -1.16609,0.7391 -2.26579,1.76799 -3.18173,2.75494 -2.4638,2.6539 -2.14592,6.94227 -3.27372,10.14344 -1.425673,4.04688 -2.329683,6.785461 -2.028343,11.160611 0.09506,1.38199 -0.06966,3.22965 0.76354,4.40574 2.296003,3.2412 6.304013,2.16458 9.644493,2.60312 3.47711,0.45681 5.92667,-0.99368 9.04394,-2.15419 3.59256,-1.33773 7.53534,-2.29755 10.77384,-4.46328 0.84089,-0.56284 1.40662,-1.40123 2.11859,-2.12321 1.10259,-1.11568 2.2148,-2.0472 3.12497,-3.27121 1.77222,-2.382211 2.23981,-5.386531 2.47265,-8.274251 0.0731,-0.92556 0.4291,-2.01583 -0.0231,-2.87885 -0.34059,-0.65271 -0.96597,-1.02428 -1.53362,-1.46512 -1.24306,-0.96559 -2.83056,-1.60444 -4.03321,-2.64661 -1.30272,-1.12894 -1.93579,-2.80111 -3.47134,-3.6828 -5.35901,-3.077248 -13.99886,-3.690888 -19.81969,-1.972925 -1.65485,0.488566 -3.62393,0.405246 -5.00015,1.551133 -1.22478,1.020042 -1.59673,2.707602 -2.12975,4.167902 -1.317333,3.60853 -1.872673,7.33175 -2.960833,10.96818 -1.40566,4.697271 -3.54234,11.446941 -0.51185,15.971221 3.140563,4.68899 9.446113,2.9108 14.066213,2.13225 7.6835,-1.29444 16.15402,-3.73111 21.93637,-9.25965 1.48936,-1.42336 2.39375,-3.04396 3.52906,-4.80291 2.07818,-3.217911 4.58739,-8.962351 2.54769,-12.700001 -1.33927,-2.45629 -4.52197,-2.77109 -6.65403,-4.35455 -1.7068,-1.26808 -2.94216,-3.328172 -4.8106,-4.242188 -5.65343,-2.764174 -12.4537,-3.383204 -18.66515,-2.424354 -2.70934,0.41833 -6.35212,0.98983 -8.45917,2.810164 -1.273073,1.099896 -1.690253,2.603118 -2.190363,4.170028 -1.19053,3.72994 -1.7322,7.65886 -2.94929,11.35302 -0.55726,1.6918 -1.49667,3.25255 -1.88864,5.003031 -0.59498,2.65642 -0.29133,5.56876 -0.29133,8.27425 0,1.81879 -0.11661,4.42229 0.90497,5.96515 0.91998,1.3893 2.86193,3.08456 4.48291,3.66202 3.064163,1.09143 8.162633,0.24727 11.160603,-0.70946 8.78609,-2.80401 19.09618,-4.43076 26.162,-10.78981 2.17055,-1.95426 4.0005,-5.44599 5.09732,-8.133961 1.14877,-2.81171 1.12761,-5.10675 1.48359,-8.08182 0.15202,-1.28116 0.508,-3.0607 -0.16548,-4.23334 -0.33675,-0.58554 -0.76585,-1.06025 -1.22959,-1.5059 -2.25522,-2.16554 -5.05114,-3.001822 -7.87208,-4.412869 -2.71126,-1.35659 -4.83947,-3.885623 -7.69697,-4.941839 -3.96394,-1.465504 -9.48651,-0.877261 -13.66212,-0.877261 -3.11727,0 -6.18836,0.170487 -9.23636,0.895541 -1.17014,0.278439 -2.57349,0.935183 -3.463643,1.66928 -0.81876,0.675216 -1.29829,1.233633 -2.11667,2.05336 -0.6552,0.656359 -1.18937,1.13511 -1.74278,1.924243 -2.65834,3.789985 -1.34197,9.034505 -2.81671,13.084845 -0.79433,2.18113 -1.9354,4.25411 -2.43166,6.542421 -0.76027,3.50578 -0.35233,7.28942 0.24072,10.77576 0.34271,2.01488 0.7189,5.07365 1.87286,6.73466 1.23152,1.7728 3.70148,2.58541 5.83969,2.96064 2.035083,0.35714 3.958753,-0.15664 5.965163,-0.4039 2.20133,-0.27113 4.59124,0.0277 6.73484,-0.59151 1.19496,-0.34502 2.17825,-1.16148 3.27122,-1.72278 2.50728,-1.28809 5.28204,-1.17744 7.88939,-2.10839 5.05306,-1.80456 9.41917,-4.23795 13.85455,-7.17704 1.58172,-1.04794 3.73495,-2.14418 4.99533,-3.46518 1.52592,-1.59943 1.79724,-3.81943 2.77283,-5.77195 1.68949,-3.377631 1.95507,-6.973841 2.04547,-10.775761 0.0289,-1.24249 0.54456,-2.74763 -0.14239,-3.84771 -0.4522,-0.72313 -0.95443,-0.98079 -1.50476,-1.57288 -0.8178,-0.87745 -1.50668,-2.227502 -2.39761,-3.005088 -1.28539,-1.122604 -2.8498,-1.218237 -4.42191,-1.731434 -1.14684,-0.37465 -1.93963,-1.22959 -3.07878,-1.63849 -1.20073,-0.43084 -2.6824,-0.33328 -3.82155,-0.85398 -0.74468,-0.3404 -1.04486,-1.396807 -1.58365,-1.971773 -0.55611,-0.592857 -1.15262,-1.088543 -1.90692,-1.395074 -2.31102,-0.9398 -5.60532,-0.521276 -8.08182,-0.757189 -4.30646,-0.409864 -9.35952,-1.585384 -13.66212,-0.86668 -3.00124,0.501073 -6.703493,2.651413 -9.043943,4.49176 -1.8796,1.47801 -3.81674,3.331826 -4.72132,5.643996 -1.30214,3.327782 -0.20686,6.854142 -0.93403,10.198482 -0.50357,2.31544 -2.25829,4.34571 -2.83075,6.73485 -1.1559,4.823301 -0.42507,10.950671 0.38869,15.778781 0.33059,1.96119 0.42834,4.16272 1.65735,5.77273 0.73314,0.96039 1.56018,1.65601 2.59157,2.24405 1.8721,1.06719 4.37477,2.02931 6.54243,2.14149 2.567713,0.13278 5.092893,-1.11433 7.696973,-1.11433 2.31486,0 4.90874,0.21725 7.11969,-0.59516 1.95503,-0.71832 3.58872,-2.16824 5.58031,-2.87809 2.26483,-0.80742 4.69515,-1.09759 6.92727,-2.00179 4.25642,-1.72432 8.03564,-4.10614 11.73788,-6.78373 1.41624,-1.02369 3.03645,-2.05855 4.20062,-3.33048 1.5721,-1.71854 2.59965,-4.22717 3.556,-6.34711 0.8255,-1.83284 1.68948,-3.572361 1.99929,-5.580301 0.30788,-1.98621 0.0269,-3.99454 0.30788,-5.96515 0.36945,-2.58041 1.08142,-5.532 -0.43681,-7.8892 -0.96982,-1.506105 -2.75744,-2.700292 -4.2391,-3.645865 -3.48673,-2.22577 -7.16781,-3.342407 -10.96819,-4.675523 -2.19363,-0.769313 -3.21348,-2.941204 -5.38787,-3.600834 -3.60796,-1.094703 -8.18766,-0.393123 -11.93031,-0.393123 -4.36033,0 -9.35566,-1.25634 -13.66212,-0.283057 -3.148443,0.711393 -5.985353,3.18924 -8.466663,5.068647 -0.60171,0.455853 -1.18438,0.860906 -1.7147,1.389303 -2.4078,2.399916 -3.63061,7.162032 -3.67241,10.566012 -0.0231,1.87344 0.63385,4.40979 -0.0685,6.15757 -0.68175,1.69699 -1.74144,3.19694 -2.159,5.00303 -1.13284,4.900471 -0.30888,10.246791 -0.0899,15.201521 0.13759,3.11458 0.49434,7.68523 2.41897,10.19829 1.31002,1.71046 4.05245,3.70032 6.24763,4.22583 4.60702,1.10279 9.355663,-1.26326 13.854543,-1.69141 2.1744,-0.20685 4.14097,0.8765 6.35,0.48126 2.39953,-0.4293 4.32377,-2.67797 6.73485,-3.46383 3.95239,-1.28809 8.23191,-2.39915 11.9303,-4.3207 2.03585,-1.05775 3.68493,-2.65776 5.58031,-3.87273 1.76645,-1.13318 3.68684,-1.78993 5.18775,-3.29603 1.43164,-1.43683 2.01469,-3.42188 2.95564,-5.18795 2.43417,-4.56354 4.17176,-7.985031 4.17176,-13.277281 0,-3.54984 1.36044,-9.42685 -0.74084,-12.506798 -0.85244,-1.250564 -2.55539,-1.75741 -3.62142,-2.769754 -1.22767,-1.166283 -1.51053,-2.786687 -3.14229,-3.544646 -2.02045,-0.938261 -4.54506,-0.405437 -6.54242,-1.38257 -2.67277,-1.307714 -5.03767,-3.453244 -7.8894,-4.406321 -2.35719,-0.787979 -5.23971,-0.23572 -7.69697,-0.645006 -4.80675,-0.80087 -9.3749,-0.90151 -14.23939,-1.114523 -2.43282,-0.106563 -5.30494,-0.65343 -7.696973,-0.0065 -2.94467,0.796636 -6.49258,3.360496 -8.6562,5.419243 -1.04429,0.99368 -1.75126,2.372397 -2.47977,3.638743 -1.91482,3.32894 -3.66492,6.713682 -4.03283,10.583332 -0.14682,1.54228 0.64116,2.93832 0.52089,4.42576 -0.15567,1.92905 -1.96138,3.52636 -2.22981,5.5803 -0.59882,4.583551 -0.03367,9.256181 -0.47317,13.854551 -0.30345,3.17615 -1.53074,6.43004 -1.29694,9.62121 0.15086,2.05701 1.21131,4.54814 2.40915,6.15757 1.26019,1.69353 3.29122,2.64276 5.2705,3.46595 4.32108,1.79705 8.52132,1.72008 13.084853,0.6656 1.60231,-0.37042 2.92004,-1.30137 4.61818,-1.42048 3.42323,-0.2398 7.2467,0.47736 10.58333,-0.0799 1.69911,-0.28363 3.1904,-1.37949 4.81061,-1.92559 3.84271,-1.29501 8.14532,-2.47496 11.73788,-4.35879 1.74913,-0.91709 3.31547,-2.32853 5.00303,-3.38494 1.48936,-0.93287 3.16345,-1.77338 4.61818,-2.7176 0.90824,-0.58882 1.86651,-1.33331 2.6747,-2.07145 1.17956,-1.07873 1.88191,-2.79342 2.8498,-4.14405 1.01407,-1.41432 1.8415,-2.79785 2.81517,-4.23333 0.60228,-0.88438 1.45472,-1.90654 1.83572,-2.88637 1.47205,-3.775171 1.37006,-9.403581 1.37006,-13.469691 0,-2.88059 -0.30788,-5.11233 -0.98136,-7.889395 -0.30211,-1.24133 -0.4445,-2.578677 -1.15647,-3.65587 -0.83705,-1.267113 -1.905,-1.33927 -2.98258,-2.25194 -0.59651,-0.505497 -0.88515,-1.229206 -1.42971,-1.781656 -0.87938,-0.890347 -2.01276,-1.765491 -3.26351,-2.210954 -1.96465,-0.70004 -4.2545,-0.34598 -6.15758,-1.170323 -4.08709,-1.77088 -8.42241,-3.972793 -12.89242,-4.810416 -2.43417,-0.456044 -5.02612,-0.115838 -7.50455,-0.529934 -4.47194,-0.7468 -8.28963,-1.676787 -12.89242,-1.676787 -5.155243,0 -11.136363,0.66983 -15.184393,4.296064 -1.06218,0.951539 -1.57076,1.863436 -2.51941,2.823633 -0.63596,0.64366 -1.12126,1.070073 -1.66774,1.73182 -2.13572,2.585796 -4.42191,7.315006 -4.66437,10.775758 -0.15778,2.25579 1.43241,4.14097 1.30907,6.35 -0.11315,2.02392 -1.50476,3.94893 -1.84073,5.96515 -0.4624,2.77399 -0.06543,5.840651 -0.27228,8.659091 -0.40025,5.45561 -1.92367,10.85658 -2.30909,16.35606 -0.19147,2.72915 -0.56573,5.74521 1.17975,8.08182 6.75852,9.04625 19.35749,3.7898 28.470703,2.91908 4.23526,-0.40467 8.94388,0.52416 13.08485,-0.35387 3.1904,-0.67657 6.99462,-1.59212 9.81364,-3.26313 2.68239,-1.58981 5.18968,-3.74939 7.69697,-5.51334 2.54,-1.78608 5.10501,-3.52675 7.69697,-5.32938 0.71774,-0.49877 1.30656,-0.93249 1.92232,-1.5471 0.52531,-0.52416 0.85821,-0.97693 1.3816,-1.48089 2.15515,-2.07241 4.22949,-4.47598 5.67844,-7.17531 1.08527,-2.02257 0.9525,-4.39516 1.60867,-6.54242 1.72027,-5.633611 2.63428,-12.523361 1.86074,-18.472732 -0.3175,-2.44071 -1.52592,-4.766927 -2.91523,-6.734847 -1.26038,-1.786083 -2.81901,-2.387794 -4.72594,-3.318357 -4.19677,-2.048163 -8.84381,-3.748617 -13.27727,-5.153893 -4.09286,-1.296937 -8.04141,-3.21329 -12.31515,-3.946043 -1.70296,-0.291907 -3.41745,0.57304 -5.19545,0.195889 -7.65849,-1.623869 -14.41643,-3.902749 -22.321223,-1.794742 -4.3559,1.161473 -8.89057,4.112299 -12.12272,7.107189 -0.64905,0.60152 -1.11703,1.09682 -1.73163,1.7145 -0.46297,0.465283 -0.83762,0.830697 -1.25711,1.34697 -0.67598,0.83185 -1.27365,1.905193 -1.77473,2.886364 -1.52053,2.977189 -2.69413,6.012103 -2.32275,9.428791 0.27305,2.51402 1.68121,4.68457 1.4886,7.31212 -0.1628,2.22057 -1.55864,4.21582 -2.12167,6.35 -0.8382,3.177111 -0.92441,6.371161 -1.14935,9.621211 -0.16818,2.42743 -1.15339,4.74499 -1.6283,7.1197 -0.82531,4.1275 -1.35928,9.69087 -0.57361,13.85454 0.37272,1.97562 2.16515,4.38651 3.3551,5.9538 0.91036,1.19919 4.81234,1.59308 6.36848,1.88537 3.42207,0.64306 6.58245,0.2076 10.00606,0.0577 1.71411,-0.0752 3.49442,0.17725 5.19545,-0.0652 1.813023,-0.25843 3.551383,-0.95308 5.387883,-1.07969 3.42515,-0.23575 6.95421,-0.35252 10.39091,-0.20205 3.43477,0.15048 7.04465,0.45027 10.39091,-0.58978 4.3738,-1.3589 8.10491,-5.09116 11.9303,-7.48992 5.1358,-3.22157 10.92585,-5.93321 15.20152,-10.2083 1.51437,-1.51496 2.25136,-3.52117 3.38859,-5.38769 0.6985,-1.14396 1.76453,-1.93117 2.40145,-3.07898 1.90308,-3.42515 3.12497,-8.445501 3.66376,-12.314951 0.74468,-5.33574 1.72027,-12.4512 -1.28732,-17.318185 -2.40338,-3.890433 -6.01711,-8.85825 -10.09073,-11.100379 -4.08132,-2.24636 -8.83612,-2.250977 -13.27727,-3.001624 -1.81456,-0.306533 -3.19039,-1.60405 -5.00303,-1.975043 -2.90368,-0.59459 -6.11524,-1.492637 -9.04394,-1.785504 -1.3258,-0.132777 -2.75744,0.309029 -4.04091,0.0837 -1.38545,-0.243033 -2.67469,-1.06911 -4.04091,-1.433754 -4.80098,-1.28193 -11.20948,-2.6797 -16.163633,-1.688716 -1.78089,0.35618 -3.61065,1.518036 -5.19546,2.36913 -3.09264,1.66062 -6.10908,3.371849 -9.04394,5.31899 -1.55652,1.032547 -3.66856,1.875943 -4.94742,3.22503 -1.05487,1.112983 -1.89942,2.444556 -2.60407,3.83059 -1.5927,3.133436 -2.79169,7.415066 -2.19941,10.968185 0.36541,2.19344 1.88806,3.88139 2.47938,5.96515 0.50589,1.78223 0.0052,4.02629 -0.45546,5.77273 -0.84147,3.19001 -3.24562,5.54971 -4.25239,8.659091 -0.8482,2.62024 -0.76777,5.20835 -1.18995,7.88939 -0.77528,4.92337 -1.92443,10.59219 -1.20477,15.58637 0.34367,2.3851 1.3868,6.33422 2.7611,8.27347 1.70315,2.40338 5.91493,3.10226 8.72663,3.51675 4.69862,0.69234 9.29563,1.05987 14.04697,0.49029 2.83075,-0.34059 5.582803,-1.28982 8.466673,-1.48763 1.75491,-0.12026 3.46556,-0.69503 5.19545,-0.77913 2.96911,-0.14431 5.86702,0.69273 8.85152,0.56208 5.13387,-0.22495 9.78669,-1.7655 14.43181,-3.90044 1.1584,-0.53186 1.99544,-1.17052 2.88637,-2.0574 1.4451,-1.44049 2.54,-2.89426 4.42575,-3.89044 3.67531,-1.94079 8.3897,-3.24273 11.54546,-5.92378 0.77354,-0.6577 1.42586,-1.23478 2.11667,-1.92366 0.58112,-0.57978 1.09296,-1.07007 1.60096,-1.73163 1.22767,-1.59597 1.42779,-3.89755 2.83249,-5.38788 0.61191,-0.6477 1.11221,-1.08527 1.7068,-1.73182 0.7697,-0.83897 1.24499,-1.84496 1.80879,-2.88636 2.77668,-5.13753 3.60795,-10.640481 3.99858,-16.356061 0.15586,-2.26926 0.7466,-4.43826 0.52724,-6.73485 -0.28094,-2.958909 -0.8332,-6.139489 -2.34373,-8.659092 -0.84859,-1.415087 -2.01276,-2.860964 -3.05762,-4.23314 -1.08527,-1.426634 -2.54577,-2.453796 -3.8023,-3.64875 -0.45797,-0.436033 -0.86976,-0.749684 -1.31426,-1.194763 -1.45088,-1.450877 -2.31486,-3.337597 -4.26605,-4.092477 -1.79724,-0.69523 -3.86965,-0.764503 -5.77272,-0.894966 -3.25005,-0.22302 -6.62132,-0.444307 -9.81364,-1.140307 -12.5653,-2.739737 -24.52447,-6.2103 -37.522733,-4.6507 -2.45687,0.294793 -4.88796,-0.03002 -7.31212,0.697346 -1.56902,0.470667 -2.54885,1.669471 -3.84848,2.566937 -1.91982,1.325997 -3.96856,2.487277 -5.5728,4.090747 -0.52089,0.520893 -0.84974,0.994833 -1.3716,1.498986 -0.94461,0.912667 -2.47919,1.78512 -3.39148,2.744163 -2.22404,2.337761 -2.52653,6.4389 -3.65182,9.603894 -0.55303,1.555557 -1.45896,3.70205 -1.13588,5.387882 0.57304,2.9895 4.18753,6.96807 2.7913,10.19848 -0.6273,1.45127 -1.96927,1.95119 -2.94313,3.0863 -1.40527,1.63791 -2.24232,3.61141 -3.13786,5.572791 -1.76972,3.87562 -2.0039,7.96598 -2.40126,12.12273 -0.46605,4.87853 -1.23671,9.71646 -0.7697,14.62425 0.24438,2.56751 0.08121,5.59608 1.47186,7.8892 2.27599,3.75246 7.33136,3.6043 11.06843,3.86599 2.68393,0.18858 5.17929,1.42394 7.88939,1.68179 4.31993,0.41179 8.92098,-0.15971 13.27728,-0.15971 3.039723,0 6.211453,0.25978 9.236363,-0.0327 2.38606,-0.23091 4.72401,-1.01023 7.1197,-1.11414 4.75287,-0.20782 10.27353,0.1828 14.81666,-1.35467 1.53747,-0.52127 2.86712,-1.43972 4.04091,-2.44648 0.7139,-0.61306 1.42009,-1.12799 2.11667,-1.79435 1.58173,-1.51284 2.44379,-2.84346 4.23333,-4.14405 3.56177,-2.58792 8.80918,-3.17346 12.10541,-6.08504 1.04486,-0.92363 2.22827,-1.82322 3.24812,-2.78399 1.27577,-1.20323 1.24499,-3.54946 2.08973,-5.0673 1.35466,-2.43917 3.81,-4.65493 5.34554,-7.1197 3.20772,-5.15158 4.71825,-10.533111 6.24417,-16.356061 1.42394,-5.4227 2.4053,-11.441932 -0.59267,-16.548485 -1.651,-2.813627 -4.43922,-5.38634 -6.69444,-7.69697 -0.67156,-0.689071 -1.10066,-1.254797 -1.74144,-1.924243 -1.26615,-1.324457 -2.72857,-2.402031 -4.03321,-3.663564 -1.92809,-1.86671 -3.45209,-3.75747 -5.96515,-4.933373 -4.191,-1.96119 -8.57057,-0.876493 -12.89242,-1.691603 -4.37381,-0.82473 -8.6437,-2.352577 -13.08485,-3.097453 -5.95361,-0.998681 -12.46332,-0.897467 -18.47273,-0.485678 -6.01402,0.411787 -12.867983,-1.100666 -18.665153,1.259031 -2.58368,1.051599 -5.37845,2.790536 -7.31135,4.724016 -1.31676,1.317144 -2.25406,2.754167 -3.65683,4.006657 -2.01064,1.79532 -4.00435,3.412836 -5.67036,5.613786 -3.14209,4.15059 -2.42858,11.265863 -0.64039,15.778789 0.66156,1.66967 3.98704,4.77482 3.18982,6.54243 -0.31288,0.69349 -0.95731,1.19091 -1.49725,1.73181 -0.62307,0.62404 -1.07912,1.09336 -1.73182,1.72894 -0.90786,0.88419 -2.14591,1.63714 -3.04588,2.5373 -0.80511,0.80472 -1.63869,1.83188 -2.14996,2.853461 -1.56094,3.11958 -1.30136,6.31806 -1.81591,9.62121 -1.07507,6.89976 -2.14418,13.58227 -1.31753,20.5894 0.37542,3.18135 0.78529,5.96765 2.55001,8.65909 2.20884,3.36935 7.29442,4.30645 10.97472,4.83562 6.68232,0.96212 13.83184,1.80494 20.5894,1.50668 2.10993,-0.0924 4.236223,-0.71774 6.350003,-0.92171 7.36792,-0.71389 14.57421,-0.46567 21.93636,-1.34889 2.51883,-0.30403 5.49179,-0.35406 7.88939,-1.20458 1.14108,-0.40601 2.18594,-1.30464 3.0711,-2.05894 0.90439,-0.77354 1.47204,-1.89191 2.31678,-2.72954 1.98197,-1.96484 4.62396,-3.60276 6.92728,-5.19334 1.75683,-1.21323 3.7465,-2.49882 5.77272,-3.20982 2.4765,-0.86899 5.51488,-1.28617 7.44682,-3.24485 1.54709,-1.56961 0.79086,-4.24007 2.05702,-5.86471 0.73891,-0.94596 1.61059,-1.49514 2.42647,-2.31063 1.76453,-1.76511 3.22695,-4.1579 4.53159,-6.34923 4.23333,-7.11816 7.12931,-14.876321 8.00869,-23.090911 0.44258,-4.14424 0.6427,-8.549219 -1.43933,-12.315155 -0.57535,-1.040437 -1.08912,-1.71123 -1.87229,-2.494011 -0.6196,-0.620376 -1.3893,-1.174366 -2.10897,-1.682749 -0.96982,-0.684454 -2.03392,-1.29848 -2.86904,-2.176127 -2.59581,-2.728383 -5.11271,-5.686906 -7.90671,-8.28848 -0.48107,-0.447773 -0.86784,-0.86437 -1.34697,-1.297133 -0.46567,-0.42064 -0.88323,-0.755457 -1.34697,-1.194184 -1.21805,-1.149543 -2.65931,-2.304859 -4.23334,-3.04877 -2.5958,-1.22632 -5.49563,-1.138383 -8.27424,-1.402773 -2.63814,-0.250923 -5.12041,-1.196493 -7.69697,-1.4732 -2.69009,-0.289013 -5.3898,0.136813 -8.08182,-0.278823 -9.73859,-1.503603 -19.33478,-3.62893 -29.248483,-1.71681 -4.61279,0.88977 -8.95735,1.825913 -13.4697,3.170383 -3.17654,0.946534 -6.15315,1.776844 -8.65909,4.019934 -1.4503,1.298093 -3.51597,3.070899 -4.80772,4.393046 -0.96731,0.990023 -1.62829,1.793393 -2.48073,2.886363 -2.86808,3.677613 -4.61934,7.96598 -3.78287,12.7 0.46894,2.654686 1.46339,5.218349 2.49382,7.696969 0.64424,1.54979 1.519,3.71206 0.88149,5.38788 -0.43083,1.13319 -1.32234,1.8975 -2.11762,2.69394 -1.37295,1.37507 -2.73666,2.68297 -3.49712,4.61818 -1.38296,3.519831 -1.87518,8.005621 -2.54962,11.737881 -1.24922,6.91111 -2.37798,13.32731 -1.85978,20.39697 0.2692,3.67165 0.6527,8.9689 2.60869,12.12273 0.81011,1.30656 1.85882,1.98197 3.18116,2.64583 1.51496,0.76008 3.32105,1.49322 5.00303,2.16478 4.38111,1.74751 8.47302,1.53747 13.08485,1.73951 2.66565,0.11742 5.40731,0.40602 8.08182,0.15202 2.36778,-0.22514 4.74229,-0.91791 7.11969,-0.92941 6.905343,-0.0327 13.542823,-1.23152 20.396973,-1.5317 5.79197,-0.254 11.98418,0.29826 17.51061,-1.87613 3.62527,-1.42779 6.49431,-4.99341 9.23636,-7.54323 1.09682,-1.02081 1.81264,-1.80455 3.07879,-2.8169 2.99989,-2.3976 7.14279,-2.77745 10.19656,-5.01977 1.47204,-1.08181 3.15768,-3.44805 3.88504,-5.07288 0.4522,-1.01061 0.42526,-2.11185 1.03525,-3.07879 0.93903,-1.48974 2.35142,-2.32467 3.49057,-3.65606 1.61637,-1.88845 3.04223,-4.20043 4.32955,-6.35 1.91269,-3.19328 3.20578,-6.41157 4.66244,-9.81363 1.02369,-2.392031 2.44188,-4.628771 3.23658,-7.119701 0.86593,-2.70933 0.91403,-5.83931 0.91403,-8.65909 0,-4.142129 -0.35216,-7.874582 -2.47845,-11.545459 -0.65232,-1.126643 -1.65292,-2.32564 -2.51113,-3.271213 -0.43296,-0.47875 -0.72737,-0.873606 -1.18534,-1.329843 -0.3733,-0.372147 -0.72544,-0.653857 -1.13722,-0.97636 -2.54193,-1.992747 -6.26341,-3.39956 -8.65909,-5.568757 -0.58305,-0.5284 -1.15455,-0.992717 -1.73182,-1.556713 -1.87999,-1.833227 -4.0005,-3.258704 -5.96515,-5.032857 -1.66447,-1.503603 -2.94217,-3.025486 -5.00303,-3.94816 -1.90308,-0.851477 -4.11596,-0.849746 -6.15758,-1.054677 -3.56562,-0.357909 -7.23515,-0.549756 -10.77576,-0.984636 -12.99825,-1.596544 -27.0991,-5.508913 -40.024243,-1.657734 -1.7728,0.528204 -3.7617,-0.02617 -5.5803,0.257848 -3.78499,0.591126 -7.6708,1.996786 -11.16061,3.489422 -1.59885,0.683684 -3.15114,1.122604 -4.42287,2.254441 -0.67098,0.597093 -1.09855,1.18341 -1.66043,1.868633 -1.24691,1.520536 -2.49497,2.942936 -3.7846,4.425757 -3.69935,4.253923 -5.20812,8.738756 -5.14042,14.431819 0.03175,2.67104 0.77797,5.60474 1.74298,8.081819 0.62269,1.59827 1.82187,3.43651 1.072,5.19546 -0.6987,1.63907 -2.13861,2.93774 -3.09534,4.42575 -1.16744,1.81591 -2.11628,4.14675 -2.90233,6.15758 -1.87441,4.794441 -2.38183,9.983741 -3.22311,15.009091 -0.45797,2.73704 -1.4809,5.30302 -1.81418,8.08182 -0.63846,5.32014 0.18261,9.9135 1.64966,15.00909 0.92402,3.20964 1.38006,6.84068 3.34087,9.61929 3.01567,4.27182 8.55345,4.39882 13.23474,4.84524 7.10642,0.67541 14.18148,1.79917 21.35909,1.29694 3.43901,-0.24053 6.745243,-1.69718 10.198493,-2.06664 12.16698,-1.30848 25.83488,-0.15971 37.3303,-5.01265 1.37006,-0.57727 2.82479,-1.47204 3.78498,-2.54192 0.85629,-0.9525 1.37006,-2.22058 2.1109,-3.2539 0.48298,-0.6754 1.0006,-1.12375 1.60866,-1.73181 0.79472,-0.79491 1.42202,-1.48148 2.3091,-2.1084 2.10512,-1.48667 4.79328,-2.84537 7.11969,-4.00146 1.30849,-0.65039 2.7459,-1.05083 3.84464,-2.01295 1.93771,-1.69526 2.78823,-5.13965 4.41036,-7.27113 1.31618,-1.73086 2.64968,-2.70857 4.02744,-4.23334 0.90632,-1.00387 1.79917,-2.16073 2.68817,-3.27121 2.56501,-3.2054 4.6836,-6.68924 6.52896,-10.39091 4.80867,-9.643341 5.2224,-22.583681 1.36237,-32.5197 -0.88323,-2.271953 -1.73374,-4.71882 -3.1115,-6.734079 -1.50862,-2.205758 -3.5868,-1.907117 -5.89589,-2.53288 -2.96911,-0.804527 -6.9215,-2.037387 -9.42879,-3.801147 -4.69708,-3.30431 -8.5725,-8.302146 -14.04697,-10.512907 -2.84595,-1.14935 -6.24417,-0.621723 -9.23636,-1.411817 -6.11717,-1.616172 -12.04768,-1.760876 -18.28031,-2.364126 -7.25439,-0.702154 -15.00139,-1.872287 -22.321213,-1.08835 -3.39147,0.363104 -6.91091,1.549593 -10.19848,2.438977 -6.31113,1.707189 -14.36755,2.204606 -19.43485,6.783916 -2.41646,2.183824 -4.43268,3.768437 -6.50682,6.137564 -0.78933,0.9017 -1.62676,2.518063 -2.19999,3.656062 -1.44934,2.877897 -3.2283,5.243944 -2.80305,8.659091 0.14817,1.189566 0.79703,2.296197 1.07354,3.463636 0.39274,1.658697 0.35598,3.424574 1.04101,5.00303 1.326,3.056279 3.4088,5.560869 1.91405,9.043939 -0.89689,2.08992 -2.47073,3.68858 -3.66915,5.58031 -2.06836,3.26544 -3.87888,6.756011 -5.19526,10.390911 -2.03181,5.61032 -1.74221,11.65379 -1.74221,17.5106 0,10.04397 -0.97617,19.63497 5.11271,28.28637 2.63487,3.74457 7.9148,3.52906 12.01305,4.34878 3.59371,0.71967 7.15279,0.72737 10.77576,1.07181 3.68396,0.35021 7.65367,1.05833 11.35303,0.70427 5.07557,-0.48491 10.166353,-2.25714 15.201513,-3.14998 3.7388,-0.66194 7.58729,-0.0385 11.35303,-0.29826 6.58091,-0.4522 13.66597,-1.87229 20.01212,-3.60988 1.76838,-0.48299 3.68493,-0.36561 5.38788,-1.0795 1.52785,-0.6427 2.90176,-1.66832 4.04091,-2.79785 0.81973,-0.81395 1.7068,-1.51053 2.44571,-2.31679 0.63693,-0.69465 1.18918,-1.43933 1.80494,-2.10897 1.47397,-1.60482 2.39568,-3.00951 4.21602,-4.33147 1.45857,-1.05949 3.44247,-2.2148 5.19545,-2.69394 2.20711,-0.60305 4.81446,-0.65405 6.45968,-2.49343 0.86206,-0.96405 0.45797,-2.77822 1.18149,-3.95085 1.0718,-1.73394 2.48997,-2.98527 3.68107,-4.61819 1.56826,-2.15072 2.51306,-4.64858 3.85234,-6.92727 3.08264,-5.24471 6.93691,-10.06071 9.1748,-15.77879 2.11474,-5.406931 2.59001,-10.818671 2.59001,-16.548491 0,-7.133739 -0.62537,-13.295549 -4.63357,-19.434845 -4.66436,-7.14702 -13.94306,-6.159114 -20.76643,-10.070907 -2.48997,-1.42798 -4.90104,-3.835593 -7.11969,-5.679786 -2.34373,-1.947717 -5.55914,-3.443047 -8.46667,-4.43865 -5.29359,-1.813407 -11.26644,-1.00407 -16.74091,-1.379297 -2.35142,-0.161253 -4.58932,-1.036206 -6.92727,-1.29771 -5.11464,-0.57227 -10.63144,-0.635193 -15.77879,-0.40948 -1.69141,0.07428 -3.17057,0.735254 -4.8106,0.837433 -1.542093,0.09621 -2.781503,-0.02906 -4.233343,0.287481 -2.61004,0.568806 -5.54008,0.958656 -8.27424,1.314836 -6.96941,0.90805 -13.58919,2.567516 -20.20454,4.924136 -2.5173,0.896697 -5.73444,1.43914 -7.66349,3.422264 -1.67505,1.722004 -2.77149,3.608723 -4.29741,5.369983 -3.68262,4.250267 -5.20412,9.45188 -3.54118,15.009093 1.53419,5.12618 7.76354,11.220639 5.59839,16.933339 -1.30214,3.43592 -4.24623,5.24721 -6.62748,7.70447 -1.63407,1.68621 -2.56617,3.932001 -3.39917,6.150071 -0.72814,1.93906 -1.072,4.21236 -1.72028,6.15758 -0.35175,1.05487 -0.68404,1.90538 -0.66694,2.88636 0.01905,1.0872 0.72506,2.16381 0.76316,3.27121 0.05099,1.47724 0.46643,2.68605 0.89881,4.04091 0.39331,1.23286 0.55649,2.56213 0.66001,3.84849 0.04926,0.61345 0.14278,0.95712 0.16838,1.53939 0.0255,0.57323 0.008,1.01696 0.008,1.5394 0,0.97347 -0.16474,2.44552 -0.18954,3.46363 -0.12681,5.17467 0.58228,10.7315 1.72682,15.77879 0.41717,1.83958 0.75699,4.61626 1.88845,6.14026 2.97199,4.00435 9.26715,3.95816 13.89245,4.47579 3.82347,0.4291 8.09818,1.36621 11.9303,1.83572 1.94156,0.23861 3.82501,0.0924 5.77273,0.35984 2.66739,0.3637 5.82815,0.79856 8.4659,0.4291 0.72467,-0.10198 1.14704,-0.55995 1.74971,-0.92748 0.69658,-0.42526 1.49495,-0.59652 2.29197,-0.87553 2.0572,-0.72352 4.316073,-0.55996 6.350003,-1.28155 0.54436,-0.19242 0.97809,-0.37904 1.53939,-0.47144 0.56592,-0.0962 0.97367,-0.0635 1.53939,-0.0635 0.48684,0 0.8409,0 1.34697,0 0.75816,0 1.40278,0.0116 2.11667,-0.0808 2.921,-0.38483 5.76118,-0.0154 8.65909,-0.35983 1.06796,-0.12512 2.14168,-0.13662 3.27121,-0.13662 2.01276,0 3.91391,-0.41564 5.96515,-0.55995 0.54841,-0.0385 1.00831,-0.01 1.5394,-0.0731 3.17885,-0.3733 6.34615,-0.96212 9.62121,-1.49898 0.92364,-0.15202 1.79147,-0.21936 2.69394,-0.51377 1.80686,-0.58882 3.31547,-1.46435 5.00303,-2.33603 0.72544,-0.37523 1.44703,-0.62538 2.06086,-1.21613 2.36682,-2.27445 4.22372,-5.0088 6.32499,-7.46413 0.77739,-0.90824 1.57018,-2.02046 2.58426,-2.69971 1.57018,-1.05064 3.66183,-1.83766 5.38595,-2.35701 2.21095,-0.66579 4.3257,-0.91228 5.95361,-2.64776 1.1353,-1.21169 0.60228,-3.21329 1.45857,-4.8031 0.84667,-1.57268 1.77223,-3.04838 2.96911,-4.42576 1.82995,-2.10743 2.81709,-4.93741 4.28721,-7.31212 1.12377,-1.81129 2.5246,-3.05877 3.7061,-4.8106 1.97617,-2.93159 3.45017,-6.2003 5.05304,-9.42879 0.82166,-1.65581 1.52209,-3.408991 2.0955,-5.195461 0.60426,-1.8819 0.49263,-4.04571 0.67926,-5.96515 0.29634,-3.06666 0.81011,-6.3042 0.51187,-9.42879 -0.1732,-1.816869 -0.78703,-3.574472 -0.9121,-5.387876 -0.36177,-5.258956 -0.0366,-10.312016 -2.99606,-15.001586 -1.12951,-1.7882 -2.794,-2.58599 -4.71824,-3.315663 -5.53989,-2.099927 -12.60955,-2.42243 -17.70302,-5.46889 -0.99483,-0.59459 -1.65293,-1.32311 -2.50152,-1.99794 -1.28731,-1.024467 -2.77091,-1.849776 -4.04091,-2.84942 -0.60228,-0.47471 -1.08142,-0.884766 -1.73181,-1.22632 -1.61059,-0.844933 -3.21156,-1.522074 -4.81061,-2.321406 -2.31486,-1.157431 -4.95108,-1.987934 -7.50454,-2.696634 -2.6824,-0.744296 -5.89973,-1.826873 -8.6591,-2.0497 -3.74457,-0.3023 -7.97598,0.414866 -11.73787,0.1574 -2.19172,-0.150283 -5.08193,-0.480293 -7.31213,-0.535323 -2.85942,-0.07062 -5.79389,0.479523 -8.65909,0.549563 -5.37287,0.13162 -10.627973,-0.0585 -15.971213,1.003493 -4.20581,0.835891 -8.59559,1.043324 -12.7,2.214804 -2.46053,0.702156 -4.8385,2.15265 -7.11969,3.161146 -2.74532,1.21362 -5.87626,1.750867 -8.46667,3.113807 -2.40165,1.263843 -5.13446,3.053003 -7.08429,4.99649 -0.83416,0.831657 -1.46204,1.863243 -2.12052,2.83133 -1.29694,1.906153 -2.82017,4.528896 -3.07686,6.927273 -0.14182,1.32407 0.38254,2.564244 0.51089,3.848483 0.15721,1.571144 -0.05735,2.72723 0.50222,4.233334 0.84763,2.28119 2.16228,4.717859 3.20983,6.927273 1.23383,2.60196 3.99973,5.010342 3.60738,8.081822 -0.12604,0.98521 -0.73949,1.6308 -1.17533,2.50152 -0.38177,0.76296 -0.59209,1.43452 -1.06968,2.11666 -0.8205,1.17187 -1.94522,1.99737 -2.83557,3.07879 -2.42647,2.94717 -4.62049,5.875481 -6.01537,9.428791 -0.77586,1.97639 -1.69083,5.22624 -2.06625,7.31212 -0.1399,0.77624 -0.0933,1.27577 -0.0893,2.11667 0.0027,0.51396 -0.0227,0.88034 0.13508,1.34696 0.35445,1.04795 1.58481,1.86671 1.7526,3.07879 0.437,3.15615 0.28229,6.67751 0.58093,9.81364 0.20975,2.20153 -0.14872,4.5085 -0.15952,6.73485 -0.01751,3.62912 0.38177,7.00232 0.90959,10.58333 0.39159,2.65546 0.70177,5.70731 2.23828,7.8817 0.73064,1.03524 1.60078,1.5875 2.62467,2.07433 0.57034,0.27132 0.90997,0.38678 1.53939,0.42718 1.42663,0.0885 2.62063,0.77355 4.04091,1.13723 2.42397,0.62153 4.86756,0.65424 7.31212,1.22574 2.93197,0.68311 6.03866,1.06988 8.85152,1.98775 1.68621,0.55225 3.91102,0.82357 5.77272,1.03524 0.74218,0.0847 1.1736,0.19627 1.92425,0.19627 0.83127,0 1.2521,-0.16353 2.11666,-0.18473 0.87438,-0.0212 1.36468,-0.002 2.30909,-0.002 1.29656,0 1.76242,0 2.88637,0 0.73063,0 1.07469,0.027 1.73182,-0.0173 0.55398,-0.0385 0.98232,-0.0962 1.53939,-0.21744 1.29386,-0.28479 3.28468,-0.53494 4.42576,-1.09104 0.83743,-0.40794 1.54997,-0.56577 2.50151,-0.60037 1.680253,-0.0616 3.122283,-1.01985 4.810613,-1.1353 3.34241,-0.23091 7.38139,0.46567 10.77575,0.14239 3.96394,-0.37715 7.92596,-1.70872 11.93031,-1.88383 0.67926,-0.031 1.10836,0.0173 1.73182,-0.025 0.60806,-0.0423 1.07565,-0.15589 1.73181,-0.17319 3.45594,-0.0847 8.65717,-0.51184 11.73788,-1.97619 2.6824,-1.27578 5.46677,-3.05185 7.69697,-5.15312 2.60158,-2.45341 4.81638,-5.7381 7.69697,-7.82397 0.92941,-0.67156 1.87229,-1.05064 2.88636,-1.4047 1.2777,-0.44643 2.413,-0.86783 3.65607,-1.28347 1.66062,-0.55303 3.79845,-0.96886 5.08384,-2.22135 1.52016,-1.47897 1.72412,-2.92735 2.59581,-4.76942 1.53747,-3.24774 3.90621,-6.68944 5.92474,-9.81364 1.16223,-1.79782 2.80363,-3.81154 4.15636,-5.38788 1.00447,-1.16936 2.0397,-1.67736 2.7786,-3.07879 0.5792,-1.09893 0.94097,-2.19652 1.42394,-3.27121 1.70103,-3.77498 3.69456,-7.85533 4.6586,-11.930301 1.51246,-6.39542 1.18533,-12.98249 0.90246,-19.434857 -0.1367,-3.106689 0.34249,-6.343263 -0.0443,-9.428786 -0.279,-2.24309 -1.8588,-5.770804 -3.0557,-7.69697 -1.7876,-2.882517 -4.7471,-3.822123 -7.6835,-5.151197 -4.40458,-1.99313 -9.05547,-3.077056 -13.66211,-4.47752 -2.42839,-0.738333 -4.87988,-1.922703 -7.11969,-2.972956 -2.27061,-1.064874 -3.32702,-2.626783 -5.38788,-3.825586 -4.33724,-2.523451 -9.73859,-5.345161 -14.62424,-6.633634 -6.79258,-1.791276 -14.13934,-2.429547 -21.16667,-1.937133 -2.01853,0.141623 -3.95047,0.979053 -5.96515,1.120293 -3.84079,0.26901 -7.53072,-0.142777 -11.35303,0.02463 -4.682263,0.205126 -9.581773,0.82069 -14.239403,1.394499 -4.16348,0.513004 -8.03679,2.173047 -12.12272,3.088217 -7.56189,1.693717 -14.1124,4.13789 -20.39697,8.76473 -1.58808,1.16917 -3.40187,2.531727 -4.69207,4.066117 -1.61829,1.924243 -3.22157,4.348403 -3.68512,6.924386 -0.26593,1.476857 0.24649,2.806317 0.46201,4.233334 0.70504,4.670136 2.58368,9.127836 4.90335,13.084849 1.70142,2.902334 5.08424,6.707904 3.31644,10.583334 -1.24768,2.73492 -4.27567,3.84502 -6.23436,5.97265 -2.36412,2.56829 -4.36168,6.340001 -5.60339,9.613711 -1.21477,3.20252 -1.98486,6.66057 -0.828,10.00606 0.44835,1.29656 1.71854,1.79494 2.07914,3.08168 0.56053,1.99909 -0.19471,4.98205 -0.10141,7.11681 0.21802,4.97609 1.16686,9.79709 0.94712,14.81666 -0.15587,3.55985 -0.35079,7.04658 0.27401,10.58334 0.21955,1.24306 0.20185,2.55539 0.91363,3.65413 0.51589,0.79472 1.22882,1.28925 1.89865,1.92617 0.80818,0.76777 1.46877,1.38545 2.50151,1.66447 3.01741,0.81007 6.61112,0.17318 9.62122,0.90247 1.94617,0.47144 4.18041,0.70427 5.96515,1.42201 1.92232,0.77163 3.7363,0.69658 5.77272,1.12953 2.61082,0.55804 5.49179,1.59713 8.27425,1.79147 2.49247,0.17511 5.01958,0.21552 7.50454,0.38485 1.98659,0.13662 3.87408,-0.52724 5.77273,-1.016 2.7813,-0.71582 5.61147,-1.41239 8.466673,-1.79724 0.73121,-0.0981 1.8186,-0.229 2.50151,-0.2463 1.05833,-0.0251 1.46627,0.0115 2.69394,-0.002 0.91017,-0.0115 1.4047,-0.1674 2.30909,-0.1905 1.04679,-0.025 2.05894,0.0558 3.07879,-0.0847 4.8183,-0.65424 9.57888,-1.92232 14.43182,-2.38606 6.39233,-0.60998 13.38118,-0.71004 19.2251,-3.65029 1.82418,-0.91786 3.21733,-3.4136 4.40267,-5.04151 1.63945,-2.25137 4.53544,-5.12234 6.78295,-6.83106 0.69273,-0.52725 1.46435,-0.72159 2.3014,-0.93711 1.98774,-0.51185 3.92738,-1.23344 5.96515,-1.57595 1.21227,-0.20397 2.16669,-0.23476 3.07494,-1.06796 1.01022,-0.9244 1.50091,-2.19498 2.413,-3.21464 4.34494,-4.85467 8.61674,-10.20291 11.68014,-16.00662 0.59267,-1.12164 1.03333,-2.66873 1.67024,-3.84848 2.10706,-3.89909 4.8414,-7.65291 6.17106,-11.9303 1.58173,-5.079811 1.7857,-10.027231 2.59194,-15.201521 0.64463,-4.1377 0.49262,-8.138966 0.49262,-12.315149 0,-2.233854 0.3079,-4.726324 -0.0885,-6.927274 -0.4849,-2.689127 -2.2321,-6.728303 -3.60986,-9.043937 -1.46627,-2.462262 -5.09733,-4.656859 -7.65464,-6.007293 -7.05812,-3.72745 -15.29773,-4.412866 -22.51364,-7.825893 -1.75876,-0.83185 -3.14806,-2.118016 -4.81061,-3.040113 -6.61362,-3.66568 -13.59862,-7.117387 -20.97424,-8.86614 -9.22097,-2.18613 -17.43941,0.357333 -26.55454,1.268654 -5.16332,0.516273 -10.416703,-0.06696 -15.586373,0.450273 -8.69353,0.869757 -17.70957,3.436506 -25.97727,6.189133 -4.45731,1.48417 -8.9,2.336416 -12.7,5.413857 -1.35697,1.098743 -2.97507,2.371629 -4.23333,3.559079 -0.75276,0.710623 -1.34447,1.103744 -2.05875,1.93194 -2.921,3.38628 -4.80541,9.246947 -4.0361,13.853774 0.33463,2.004676 1.67236,4.439036 2.57406,6.157576 0.69677,1.328113 1.19265,2.703947 1.95696,4.04091 1.43818,2.515753 3.17519,4.505613 4.40805,7.119693 0.62483,1.32465 1.27481,2.97276 0.55823,4.42499 -0.62288,1.26249 -2.64988,1.96965 -3.53984,2.90445 -3.52771,3.70494 -8.00658,9.023541 -9.82172,14.222081 -0.87303,2.50036 -1.01157,7.60345 0.13008,10.00606 0.64385,1.35524 2.41284,1.71334 2.50844,3.2714 0.3075,5.01092 -0.12373,10.58661 -0.60152,15.58617 -0.31615,3.30778 -0.90227,6.26341 -0.57727,9.62122 0.32424,3.3501 1.32754,8.09721 3.17923,10.74881 0.76066,1.0872 2.40896,1.0795 3.39591,1.9685 0.57766,0.51955 0.99887,1.00831 1.73182,1.26423 0.82088,0.28479 1.54247,0.39447 2.30909,0.79471 1.04909,0.54456 1.89768,1.48359 3.07878,1.83958 1.67236,0.50411 3.65337,0.24245 5.38788,0.59651 6.98423,1.42779 13.82318,3.42131 20.97424,4.11404 1.68853,0.16356 3.35954,-0.21167 5.00304,-0.14047 1.43394,0.0635 2.77822,0.63881 4.23333,0.54071 3.11689,-0.20974 5.61263,-1.88961 8.65909,-2.37067 10.408233,-1.6433 20.885733,-3.4925 31.365153,-4.50658 3.04415,-0.2944 6.21145,-0.67348 9.23636,-0.96212 1.75684,-0.16741 4.32955,0.004 5.96515,-0.70812 1.98197,-0.86398 3.98703,-3.03453 5.22432,-4.71247 2.24367,-3.03645 5.08385,-7.06582 8.24538,-9.27485 1.01408,-0.70812 2.18786,-1.03331 3.46364,-1.35466 0.55225,-0.14046 0.98521,-0.12316 1.53939,-0.16356 0.61576,-0.0443 1.06411,-0.15199 1.73182,-0.17319 1.17571,-0.0346 2.56309,-0.0673 3.42323,-0.84666 0.54841,-0.49838 0.67733,-1.03909 1.12568,-1.65677 1.0718,-1.47397 2.83056,-2.50883 4.0082,-4.04091 1.16609,-1.51515 2.35527,-3.15249 3.38281,-4.81061 0.58881,-0.95135 0.76584,-1.78589 1.58751,-2.66123 1.2315,-1.31079 2.21864,-2.75724 3.2308,-4.26604 1.397,-2.08415 2.6362,-4.45385 4.24103,-6.35 0.9525,-1.12357 2.07047,-2.2989 2.69587,-3.65606 4.54313,-9.87676 6.2403,-19.732531 6.7156,-30.595461 0.13084,-2.998543 0.18857,-6.048853 0.39447,-9.043936 0.18663,-2.722033 0.60806,-5.974194 -0.79471,-8.466667 -1.64716,-2.921383 -3.93509,-5.715 -6.01712,-8.274243 -0.95441,-1.17398 -1.95504,-2.467647 -3.21154,-3.277176 -2.1744,-1.399501 -5.43793,-2.605424 -7.8894,-3.53926 -5.77851,-2.201717 -12.02267,-3.232341 -17.70304,-5.68171 -2.15515,-0.929024 -3.93507,-2.451484 -5.96515,-3.581014 -1.47974,-0.823 -3.20194,-1.196687 -4.61818,-2.126096 -1.0872,-0.712933 -1.69911,-1.603087 -2.88636,-2.281767 -3.429,-1.959456 -8.44935,-3.2004 -12.31515,-4.09806 -3.04223,-0.70639 -6.14411,-0.74064 -9.23637,-1.040053 -2.89791,-0.280563 -5.54951,-0.669443 -8.46666,-0.297103 -3.50213,0.447003 -6.89456,1.870173 -10.39091,2.221153 -7.920763,0.795097 -15.811893,1.033896 -23.668183,2.460146 -4.53371,0.822997 -8.85094,2.5527 -13.27728,3.779404 -4.08035,1.130683 -8.02928,2.02103 -11.9303,3.773439 -3.84406,1.726814 -8.41606,4.543137 -11.32012,7.547647 -1.48475,1.536124 -2.79978,3.13921 -4.1479,4.79348 -3.08322,3.78364 -3.15557,11.149256 -1.37333,15.586366 0.39735,0.989253 1.13491,1.74625 1.79955,2.501514 1.27134,1.444336 1.82476,3.148063 2.81382,4.810606 1.14242,1.920203 2.35585,3.96952 3.66318,5.77273 0.56726,0.78258 1.52284,1.63637 1.0616,2.69393 -0.78797,1.80629 -2.56386,3.94201 -3.8404,5.38788 -1.30656,1.48013 -2.60658,2.84461 -4.05034,4.22583 -0.48048,0.4597 -0.86514,0.74488 -1.31426,1.19476 -1.73047,1.733361 -3.29084,4.368421 -4.17599,6.702141 -1.00176,2.64083 -2.098,6.80085 -0.78855,9.42879 0.51416,1.03159 1.04198,2.35123 1.78319,3.23022 0.97752,1.15898 2.22347,1.36679 2.33796,3.11978 0.22475,3.44151 -0.71409,7.16357 -1.08162,10.58333 -0.58073,5.40443 -0.78163,11.1279 -0.25708,16.54849 0.2846,2.94024 0.45374,6.2711 2.04701,8.85151 1.00696,1.62984 2.77726,2.57272 4.33571,3.65222 1.37698,0.9525 2.81093,1.74528 4.23333,2.45725 1.17552,0.59075 2.20845,1.25076 3.46363,1.58365 2.56444,0.68311 5.6236,0.79664 8.27425,0.98329 2.80323,0.19628 5.53373,1.09105 8.27424,1.31234 2.18305,0.17514 4.35668,-0.43099 6.54242,-0.0904 4.65532,0.72736 9.48286,2.23981 14.2394,2.0243 1.47243,-0.0674 2.48593,-0.87553 3.84848,-1.23152 2.22924,-0.58497 4.81927,-0.78316 7.119703,-1.21227 3.88851,-0.72544 7.8586,-1.06795 11.73788,-1.79916 4.53351,-0.85629 8.85921,-1.93579 13.46969,-2.27446 4.56431,-0.33482 8.93618,-0.33097 13.4697,-1.09297 1.47782,-0.2463 3.22118,-0.26362 4.61818,-0.8255 1.92424,-0.77354 3.49635,-2.71511 5.00303,-4.0082 1.7857,-1.53554 3.47711,-2.93831 4.91067,-4.86641 1.39123,-1.86844 2.64968,-4.11018 4.51812,-5.53412 2.18017,-1.65869 5.3263,-1.60866 7.88939,-2.36297 2.62082,-0.77354 5.02228,-2.89021 6.89456,-4.82022 1.41817,-1.46262 2.31294,-3.34222 3.69648,-4.79252 1.56247,-1.63773 2.88827,-3.24562 4.1679,-5.19623 0.79857,-1.21804 1.77993,-2.22692 2.54577,-3.46363 0.35213,-0.56573 0.63116,-1.14146 0.9275,-1.73182 0.68116,-1.3564 1.51053,-2.32795 2.53423,-3.46364 1.34696,-1.49109 2.34563,-3.16288 3.23657,-5.00303 2.692,-5.56395 5.2628,-11.44539 6.55203,-17.510611 1.01023,-4.75037 1.13727,-10.15846 1.19497,-15.00908 0.0462,-3.798457 -0.19244,-7.558813 -0.19244,-11.353033 0,-2.2708 0.075,-5.121373 -1.08143,-7.119697 -1.6433,-2.842876 -4.27566,-6.382136 -6.80796,-8.577503 -0.95827,-0.83108 -1.59134,-1.471469 -2.69394,-2.24232 -0.88707,-0.619413 -1.7222,-0.969049 -2.69393,-1.448376 -4.21987,-2.080107 -8.57828,-4.027827 -13.08485,-5.42983 -4.14867,-1.29078 -8.4532,-2.11628 -12.50758,-3.67357 -2.22057,-0.853017 -4.33339,-2.23193 -6.54242,-3.177693 -1.15455,-0.494724 -2.02623,-1.06757 -3.07879,-1.812637 -2.04739,-1.45088 -4.07747,-2.023726 -6.35,-3.05993 -2.90368,-1.323687 -5.5322,-2.61851 -8.65909,-3.356843 -2.61697,-0.617877 -5.1512,-0.26131 -7.69697,-0.565727 -1.07373,-0.128346 -2.15515,-0.381576 -3.27121,-0.489526 -0.52532,-0.05094 -0.85437,-0.01597 -1.34697,-0.04984 -0.55226,-0.0381 -0.98521,-0.13277 -1.5394,-0.17049 -1.71834,-0.117377 -3.62527,-0.544944 -5.38787,-0.356177 -5.27628,0.56515 -10.47693,2.468034 -15.778793,3.18212 -6.59303,0.88823 -13.2259,0.77624 -19.8197,1.56518 -10.52887,1.2598 -21.76337,4.795403 -30.2106,11.343023 -2.33853,1.812637 -4.18562,3.597564 -6.1747,5.613207 -0.6071,0.615183 -1.22401,1.034283 -1.89962,1.630797 -0.78124,0.689649 -1.18533,1.21131 -1.89691,2.032193 -1.90866,2.20191 -2.84173,5.06095 -2.70876,8.081817 0.20859,4.734983 1.64465,8.971782 4.29491,12.892426 1.12145,1.659273 2.10223,3.602759 3.31547,5.195453 1.69491,2.225006 2.89887,4.328006 1.70911,7.119696 -0.86918,2.0397 -2.99528,3.51078 -4.49695,5.19546 -2.96853,3.33028 -5.9819,8.014271 -7.39237,12.315151 -1.39834,4.2647 -1.79955,8.56846 0.22514,12.69711 0.80395,1.63926 3.09765,1.77569 3.2689,3.85137 0.31616,3.83444 -0.74025,7.91672 -1.10393,11.73788 -0.47818,5.0242 -0.35022,10.19079 0.127,15.20152 0.36502,3.83309 0.6908,7.80088 2.8167,11.15868 0.8561,1.35082 2.34681,1.91462 3.75843,2.56309 2.74974,1.2623 5.44407,2.28793 8.27424,3.36743 5.39365,2.05701 10.5056,2.00313 16.16364,2.57078 5.71923,0.57343 11.41384,1.18726 17.12576,1.73182 2.06221,0.19627 4.28759,0.77546 6.35,0.72736 1.49552,-0.0366 2.76244,-1.00637 4.23333,-1.1276 1.52554,-0.125 3.10188,0.15974 4.618183,-0.0577 8.00485,-1.1507 15.80573,-4.34301 23.86061,-5.12233 5.8901,-0.56958 11.64744,-0.6427 17.5106,-1.62021 1.73374,-0.28864 3.95624,-0.20205 5.57261,-0.89093 1.35082,-0.57535 2.36104,-2.60157 3.3097,-3.67915 0.78316,-0.89092 1.68178,-1.85882 2.4534,-2.69394 0.81781,-0.88515 1.57211,-1.94733 2.32641,-2.88636 0.78702,-0.97944 1.61059,-1.97428 2.50152,-2.88637 0.91786,-0.93903 1.27577,-1.73759 2.31679,-2.62082 1.94348,-1.65292 4.28721,-2.40722 6.72715,-3.1115 2.63044,-0.75815 6.41927,-3.38474 8.1915,-5.42828 2.59001,-2.98566 5.46678,-5.73944 7.87014,-8.85152 1.37007,-1.77434 1.95311,-3.72129 3.36167,-5.38788 1.30463,-1.54555 2.37066,-3.53041 3.7003,-5.00303 0.61383,-0.67945 1.08913,-1.11009 1.6972,-1.73182 0.71003,-0.72851 1.778,-1.98504 2.34757,-2.88636 2.3418,-3.70994 2.5804,-7.53668 3.60026,-11.73788 0.4291,-1.7678 1.22767,-3.43573 1.62213,-5.195451 1.57211,-6.975 2.36297,-14.64945 1.87804,-21.743943 -0.33866,-4.945496 1.3874,-10.020107 -1.23727,-14.624243 -0.76586,-1.343699 -1.88383,-2.282729 -3.01336,-3.303923 -0.49837,-0.451427 -0.85051,-0.838393 -1.34697,-1.280777 -1.88577,-1.682173 -4.3661,-3.239846 -6.54243,-4.615103 -1.14684,-0.72544 -2.17631,-1.397193 -3.2712,-2.094537 -1.06411,-0.678489 -2.0859,-1.569029 -3.27123,-2.179013 -1.90884,-0.981556 -4.1275,-1.282316 -6.15756,-1.926166 -2.97681,-0.94365 -5.842,-2.246554 -8.85152,-3.071091 -2.25329,-0.617296 -4.72209,-0.865716 -6.92727,-1.611169 -0.99291,-0.33578 -1.81456,-0.984057 -2.69394,-1.386031 -2.8575,-1.306753 -5.78427,-2.813436 -8.46667,-4.635693 -0.71774,-0.48818 -1.34312,-0.878803 -2.11666,-1.267113 -1.85113,-0.92864 -3.6426,-1.9914 -5.58031,-2.729153 -1.67986,-0.639427 -3.20194,-1.345047 -5.00303,-1.7145 -0.86013,-0.176454 -1.651,-0.215517 -2.50151,-0.310957 -1.33543,-0.1499 -3.09803,-0.300953 -4.42576,-0.427183 -1.16802,-0.11103 -2.93255,-0.17934 -4.23333,-0.392737 -5.07615,-0.832236 -10.62759,-1.26519 -15.77879,-0.904396 -3.39052,0.237646 -6.8605,1.6612 -10.198493,2.328913 -5.73251,1.146847 -11.69092,1.86863 -17.5106,2.431856 -8.12896,0.78663 -15.67007,1.85651 -23.47576,4.45847 -4.07631,1.358707 -8.34736,3.012017 -11.54545,5.82776 -0.42103,0.37061 -0.75565,0.702734 -1.15455,1.099704 -0.69003,0.68657 -1.1861,1.160126 -1.91674,1.924053 -0.584,0.610946 -0.97943,1.127603 -1.5544,1.731817 -0.69869,0.734483 -1.3716,1.462616 -2.05894,2.30909 -5.29551,6.521259 -6.93458,14.056399 -3.74457,21.936365 1.10586,2.731847 3.13767,5.443297 4.78559,7.889394 1.36082,2.019496 2.66199,3.161146 2.3368,5.772726 -0.37465,3.00798 -2.72742,5.68729 -4.35283,8.08182 -2.89348,4.26316 -5.96015,8.683911 -7.5363,13.662121 -0.89131,2.81478 -2.05374,7.36292 -0.78105,10.19849 0.59633,1.32868 1.90981,1.71603 2.81709,2.72953 1.16667,1.30329 1.61002,3.45614 1.77416,5.15986 0.2692,2.79323 -1.0162,5.69018 -1.28155,8.46667 -0.35541,3.71879 -0.22514,7.61422 -0.22514,11.35303 0,3.00566 -0.17472,6.12294 0.63366,9.04394 0.67752,2.44763 1.34985,4.96839 2.743,7.11969 0.63577,0.98137 1.22401,1.54709 2.01122,2.3014 4.0005,3.82924 8.99891,5.06268 14.2394,6.12871 10.7288,2.18017 21.73566,3.57909 32.71212,3.30585 1.50764,-0.0385 2.91772,-0.69268 4.42575,-0.76008 3.27911,-0.14435 6.75294,0.15587 10.006063,-0.23475 9.81172,-1.17957 19.50028,-3.07687 29.24849,-4.72017 2.67854,-0.4522 5.56876,-1.00253 8.27424,-1.19303 2.32833,-0.16356 5.37249,-0.19627 7.50262,-1.23537 2.26099,-1.10259 4.33147,-3.63874 6.1595,-5.34169 0.49453,-0.4599 0.86976,-0.84475 1.34505,-1.32965 0.84474,-0.86591 1.51438,-1.67217 2.30331,-2.50152 0.42526,-0.44835 0.762,-0.86783 1.13916,-1.34697 0.65616,-0.83127 1.41431,-1.55864 2.13206,-2.30909 0.50992,-0.53109 0.81972,-0.94288 1.37198,-1.45858 1.37968,-1.28347 2.69971,-2.33795 4.40844,-3.20578 0.95635,-0.48876 1.778,-0.96982 2.69394,-1.44319 2.52653,-1.30656 5.02612,-2.95948 7.31212,-4.65281 1.5798,-1.17187 2.79592,-2.62082 4.23332,-4.05631 0.52726,-0.52628 0.9987,-1.00811 1.5394,-1.48436 0.47723,-0.42006 0.8794,-0.76104 1.34697,-1.21169 1.87036,-1.80206 3.67146,-3.77248 5.15313,-5.96304 0.6042,-0.89265 0.94287,-2.032 1.6741,-2.85288 0.5561,-0.62499 1.2238,-0.98348 1.7953,-1.59019 1.14877,-1.21459 1.96657,-2.79747 3.12306,-4.0236 1.59904,-1.69795 2.86518,-4.23121 3.99087,-6.35 5.94397,-11.18773 4.6836,-24.519461 5.84777,-36.753027 0.34444,-3.623733 0.54457,-7.323667 0.54457,-10.968183 0,-1.82264 0.0693,-3.783637 -0.88707,-5.387877 -0.6427,-1.075846 -1.24693,-1.70738 -2.007,-2.501516 -0.52917,-0.554567 -0.96983,-0.955194 -1.51437,-1.539394 -2.41686,-2.594263 -4.93569,-5.01015 -8.09913,-6.692516 -4.86064,-2.583873 -9.25177,-4.963583 -14.62425,-6.54454 -2.4207,-0.712354 -5.03767,-0.78509 -7.50455,-1.28116 -3.36935,-0.67791 -6.72907,-1.678133 -10.00606,-2.705293 -0.98329,-0.308071 -2.0243,-0.696 -2.88636,-1.105863 -1.24499,-0.592088 -2.35527,-1.075074 -3.65606,-1.730471 -0.69658,-0.350983 -1.28924,-0.666943 -1.92424,-1.115099 -0.84475,-0.596321 -1.32196,-1.175711 -2.11667,-1.826684 -1.71065,-1.400847 -3.96971,-1.783003 -5.77273,-2.885786 -1.9608,-1.199571 -4.13519,-2.270604 -6.35,-2.829983 -0.64654,-0.163184 -1.12183,-0.120074 -1.73182,-0.178954 -1.33927,-0.129117 -3.17692,0.370993 -4.61818,0.16741 -2.59388,-0.366377 -5.09924,-1.34697 -7.69697,-1.78281 -5.25895,-0.881687 -11.02783,-1.20323 -16.35606,-0.836277 -4.49888,0.309997 -9.252913,2.133023 -13.662123,3.128817 -6.60246,1.49148 -13.38445,2.43898 -20.01212,3.87427 -2.57233,0.55707 -5.28897,0.544176 -7.88939,0.953656 -5.79043,0.911897 -11.32994,2.37894 -16.35606,5.41905 -1.79166,1.083734 -3.0938,2.515757 -4.61818,3.89255 -1.10741,1.000223 -2.38895,2.023727 -3.45614,3.114387 -2.18112,2.22885 -5.08307,5.886066 -6.14487,8.84863 -0.83512,2.329873 -1.66255,4.80868 -1.5521,7.31212 0.07062,1.597123 0.29133,3.387243 0.8486,4.810606 0.34963,0.893234 0.57015,1.579997 0.85359,2.501517 0.9194,2.988923 2.35469,6.23262 3.82924,9.043939 1.16224,2.215957 3.03992,3.38359 2.6797,6.15757 -0.67598,5.20393 -4.17599,9.02701 -6.99558,13.27728 -2.29524,3.459791 -4.23391,7.591901 -5.47793,11.545451 -0.9119,2.89849 -1.00927,6.47258 0.43045,9.23637 0.40524,0.77816 0.95057,1.28635 1.51245,1.92424 0.8663,0.98367 1.41586,2.49921 1.52439,3.84848 0.28421,3.53599 -1.14762,7.76682 -1.48879,11.35303 -0.53898,5.66305 0.61768,11.3896 1.61752,16.93334 0.49549,2.74782 0.85282,5.87856 2.38337,8.27424 1.55805,2.43802 3.29988,5.09732 5.78004,6.64633 3.49846,2.18402 8.70565,3.00375 12.69999,4.2141 4.98418,1.5086 9.76553,3.60218 15.0091,4.28528 5.19699,0.67734 10.58833,0.23669 15.77878,-0.1193 4.90663,-0.33674 9.73628,-0.32135 14.62424,-0.92364 11.418463,-1.40662 22.681053,-3.05762 34.059093,-4.81445 4.84332,-0.74661 10.63144,-0.36753 15.20152,-2.28985 1.91847,-0.80626 3.08648,-2.79015 4.61818,-4.11211 0.70043,-0.60421 1.26615,-1.11221 1.92424,-1.6991 0.82166,-0.73506 1.79147,-1.78955 2.47266,-2.72665 1.50475,-2.07241 3.51174,-4.71247 5.41674,-6.2865 2.40338,-1.98582 5.95553,-3.64837 8.46666,-5.52258 0.9525,-0.71197 1.76646,-1.33735 2.69394,-1.98004 1.14878,-0.79664 2.21673,-1.78185 3.27122,-2.76322 1.83956,-1.7145 3.97932,-2.83441 5.77272,-4.61433 0.96983,-0.96308 1.71257,-2.0878 2.71127,-3.04531 1.06023,-1.01561 2.58616,-1.51187 3.581,-2.59272 1.29886,-1.41066 1.99543,-3.40033 3.12306,-4.9453 0.73697,-1.01062 1.61057,-1.76934 2.51304,-2.69394 1.58559,-1.62734 2.5708,-3.50924 3.86773,-5.38788 1.27386,-1.84362 3.06917,-3.71591 3.93507,-5.77273 0.79087,-1.88056 0.7293,-4.13019 1.22383,-6.15758 0.76393,-3.13324 1.70296,-5.98939 1.92617,-9.236361 0.21166,-3.08629 -0.2136,-6.16777 0.32713,-9.23636 0.76583,-4.32474 2.12627,-8.501883 2.54383,-12.892426 0.29057,-3.051273 -0.34636,-5.872597 -0.84473,-8.851517 -0.73123,-4.358602 -1.4355,-8.335819 -3.8735,-12.122726 -1.18343,-1.83592 -2.50344,-3.176347 -4.10057,-4.63473 -5.52643,-5.04806 -13.8276,-8.410096 -21.16666,-10.053976 -2.86712,-0.641927 -5.80544,-0.566881 -8.65909,-1.108364 -2.70741,-0.513773 -5.45523,-1.51207 -8.08182,-2.33295 -4.87218,-1.523039 -9.19211,-4.437303 -13.85454,-6.417926 -8.27232,-3.513667 -16.84482,-5.743287 -25.59243,-7.757584 -6.47315,-1.490519 -13.48124,-3.333749 -20.20454,-2.682972 -10.653963,1.031009 -20.435463,7.206673 -30.595463,10.022416 -6.18297,1.713347 -12.59763,2.918883 -18.66515,5.13619 -1.76087,0.643466 -3.75881,1.058333 -5.38788,2.00429 -1.12722,0.654627 -2.06086,1.72874 -3.07859,2.54712 -0.75142,0.604403 -1.29348,1.1557 -1.94156,1.803976 -0.71043,0.710431 -1.33658,1.158587 -2.09666,1.929824 -0.61402,0.622876 -1.06564,1.279043 -1.60482,1.923473 -3.27198,3.911216 -7.47664,8.368916 -8.05834,13.854546 -0.12661,1.193607 0.65617,2.271183 0.77586,3.463637 0.37369,3.722446 0.78913,7.00963 2.09088,10.583333 0.5969,1.6383 1.42471,3.142864 2.55058,4.425757 1.70911,1.947526 3.63028,3.908709 3.22311,6.734849 -0.53667,3.72399 -2.93947,6.76929 -4.65879,10.00606 -0.90978,1.71277 -1.59866,3.71321 -2.85134,5.192571 -0.72274,0.85378 -1.46742,1.36044 -2.23808,2.06529 -1.57827,1.44356 -2.64103,4.30684 -3.17981,6.40426 -0.39601,1.54132 -0.67638,3.24601 -0.67638,4.81061 0,0.55726 0,0.99041 0,1.53939 0,1.11433 -0.15644,2.18767 0.49492,3.44651 1.03409,1.99833 3.52098,2.82402 3.72033,5.40501 0.14451,1.8719 -0.41814,4.14077 -0.5919,5.96515 -0.12603,1.32349 -0.41294,2.69009 -0.62345,4.04091 -0.10853,0.6958 -0.11334,1.29578 -0.11353,1.92424 -1.9e-4,0.48491 0,0.84378 0,1.34697 0,2.59195 -0.1678,5.68806 0.13739,8.27424 0.56303,4.7702 1.5519,9.9695 3.55869,14.43182 0.85167,1.89346 1.17398,3.90044 2.35374,5.5803 1.08873,1.55094 2.55,3.1269 4.34109,3.83502 1.05198,0.41564 1.93155,0.73506 2.88636,1.21612 3.43708,1.7299 6.88532,2.87674 10.58333,4.05823 2.94602,0.94288 5.87799,0.8967 8.85152,1.59904 3.57505,0.84667 6.92188,2.54 10.58333,3.0634 6.36559,0.91209 13.21589,0.50992 19.62727,0.27709 1.50688,-0.0539 3.45209,-0.28094 4.81061,-0.44258 2.95583,-0.34828 5.836803,-0.99868 8.851513,-1.6356 4.87026,-1.02755 10.10228,-0.85629 15.0091,-1.44318 4.47386,-0.53494 8.95734,-2.21481 13.46969,-2.77476 3.87927,-0.48299 7.53726,-1.24884 11.35303,-2.09165 1.61637,-0.35599 3.71764,-0.51378 5.19546,-1.23537 4.07939,-1.98966 5.98247,-6.1441 9.04394,-9.23636 1.29693,-1.30849 2.24366,-2.72665 3.49634,-4.02359 0.74469,-0.77162 1.48359,-1.35082 2.27638,-2.13399 1.37006,-1.35082 2.76899,-2.79015 4.23334,-4.01204 1.20265,-1.00446 2.62081,-1.85305 3.84078,-2.84981 2.08203,-1.70295 4.09672,-3.61565 6.35771,-5.05883 1.0795,-0.68888 2.5977,-1.72797 3.65604,-2.44764 0.72736,-0.49645 1.26232,-0.97559 1.92426,-1.59519 1.80107,-1.69218 3.97163,-2.35143 5.74,-4.08421 0.46374,-0.45508 0.79664,-0.98752 1.245,-1.44876 0.43487,-0.44816 0.82933,-0.73679 1.2796,-1.21247 0.53303,-0.56303 0.73314,-1.06449 1.0776,-1.72893 0.55033,-1.05795 1.43547,-1.70103 2.1667,-2.69394 0.59266,-0.80087 0.84664,-1.6356 1.40083,-2.50151 1.91463,-2.99432 4.68167,-5.26165 6.26147,-8.46667 0.5773,-1.1734 1.05834,-2.57425 1.57213,-3.84848 2.80361,-6.94998 2.6439,-14.979651 3.3424,-22.321221 0.27131,-2.84441 0.99867,-5.6336 1.28154,-8.46666 0.21556,-2.16593 0.052,-4.375923 0.25786,-6.542426 0.28674,-3.010093 0.87937,-6.242436 0.26554,-9.236366 -1.14493,-5.586267 -4.94723,-11.140787 -8.0953,-15.778787 -0.55033,-0.810106 -1.13337,-1.44453 -1.82417,-2.109163 -3.0153,-2.895407 -6.1826,-3.65375 -9.81363,-5.42059 -1.76647,-0.859943 -3.3405,-2.030076 -5.19547,-2.742237 -1.87804,-0.720823 -4.0101,-0.75796 -5.96513,-1.206309 -1.76647,-0.404861 -3.42131,-1.222854 -5.19546,-1.556327 -2.34565,-0.441037 -4.78174,-0.06408 -7.1197,-0.563803 -4.64897,-0.993294 -9.57503,-3.102841 -14.04697,-4.766347 -2.33218,-0.867257 -4.47963,-2.357007 -6.73485,-3.421304 -2.54769,-1.202266 -5.29551,-2.009873 -7.88939,-3.118813 -2.89406,-1.237673 -5.68036,-2.785726 -8.65909,-3.825009 -4.88758,-1.704494 -9.77323,-1.608667 -14.81667,-2.09069 -5.90357,-0.563994 -12.20046,-1.305021 -18.087873,-0.197427 -4.89682,0.921327 -9.76611,2.717993 -14.2394,4.89816 -3.90583,1.903846 -7.45297,4.196773 -11.73788,5.212196 -1.92982,0.4572 -3.89274,0.77547 -5.77272,1.408737 -0.59921,0.202046 -1.14223,0.44931 -1.73182,0.65078 -1.07508,0.367723 -2.11109,0.285173 -3.27121,0.54629 -7.18859,1.618673 -15.57675,3.359343 -21.16667,8.506116 -1.26269,1.162627 -2.02045,2.408957 -3.11208,3.638164 -0.48702,0.548603 -0.92267,0.95481 -1.37525,1.539393 -1.68314,2.174396 -3.37282,4.817343 -4.48965,7.312123 -2.22153,4.96243 -2.5246,10.662033 -1.5315,15.971213 0.27459,1.46781 0.14778,3.088986 0.86822,4.425757 1.26884,2.35431 2.8702,4.985326 4.65512,6.927273 1.62137,1.763956 3.65895,3.991456 2.98027,6.734846 -0.39793,1.60867 -1.85555,3.22311 -2.72396,4.61818 -1.21823,1.95696 -1.91962,4.20736 -3.38108,5.96515 -1.27019,1.52785 -2.60216,2.749751 -3.83559,4.233341 -1.34986,1.62329 -2.9135,3.88908 -3.55697,5.96515 -1.24537,4.0184 -0.61268,9.60351 0.90113,13.4697 0.645,1.64753 2.28003,2.99393 2.45841,4.8106 0.19165,1.94965 -0.27244,3.84041 -0.35714,5.77273 -0.26093,5.95976 -1.12318,12.11311 -0.55207,18.08788 0.66714,6.98115 4.45097,13.91805 8.2423,19.62727 0.59421,0.89478 1.18187,1.56441 1.92367,2.30909 0.98271,0.98522 2.042,2.00122 3.27102,2.71704 5.22047,3.04222 11.71921,4.01012 17.70303,4.87025 12.79486,1.83765 25.79331,5.13388 38.86969,3.73303 3.14248,-0.33674 6.165473,-1.51822 9.236373,-2.19556 7.239,-1.59519 14.50878,-3.76382 21.93636,-4.47386 4.6355,-0.44258 9.21712,-0.94673 13.85455,-1.51823 3.04607,-0.37523 6.4135,-0.75623 9.23636,-2.03777 2.49382,-1.13338 4.64704,-3.25582 6.53473,-5.1435 0.65809,-0.65809 1.06025,-1.21612 1.74144,-1.91655 1.34504,-1.37775 2.38221,-2.81709 3.62334,-4.23333 1.4047,-1.60097 2.69394,-3.50405 4.08902,-5.00303 1.04679,-1.12568 2.10704,-2.30332 3.25389,-3.46364 0.57535,-0.58497 0.98714,-1.10259 1.57211,-1.64715 3.99473,-3.72533 8.78223,-6.41735 13.24456,-9.44803 1.56056,-1.06026 3.44246,-1.90692 4.8106,-3.16153 0.49069,-0.45027 0.84666,-0.83416 1.34697,-1.27462 0.9525,-0.83974 1.76069,-1.5594 2.67663,-2.55655 2.06663,-2.25175 3.24813,-5.41135 5.24933,-7.69697 0.54263,-0.61999 0.95827,-0.9779 1.49513,-1.53939 0.3887,-0.4064 0.66194,-0.74411 1.02563,-1.15455 0.36751,-0.41775 0.70041,-0.73775 1.08141,-1.15454 1.5702,-1.71681 2.41686,-4.04226 3.4752,-6.15758 1.11606,-2.23193 2.78053,-4.38669 3.61373,-6.73485 1.74527,-4.91798 1.89537,-9.72877 2.63234,-14.816661 0.83706,-5.77639 2.03393,-11.04034 2.03393,-16.933336 0,-7.257857 0.17319,-16.530976 -3.76381,-22.898486 -0.65426,-1.059487 -1.36622,-1.854007 -2.20133,-2.69394 -0.991,-0.995987 -1.95696,-1.85035 -2.90753,-3.078787 -1.9127,-2.46861 -3.43093,-5.65304 -6.13643,-7.34772 -7.08504,-4.436533 -15.68447,-6.929583 -23.86059,-8.622916 -4.50657,-0.933257 -9.25753,-1.203227 -13.66212,-2.646217 -2.50344,-0.820303 -4.71247,-2.303703 -7.1197,-3.301616 -4.94145,-2.047777 -10.19848,-3.252354 -15.20151,-5.15697 -1.91462,-0.729094 -3.65799,-1.911734 -5.5803,-2.661034 -3.44825,-1.34466 -7.34099,-2.352003 -10.96819,-3.081673 -1.80493,-0.363106 -3.57524,-0.13893 -5.38787,-0.311536 -6.53858,-0.6223 -13.34174,-0.437957 -19.819703,0.699269 -5.44368,0.95558 -11.32667,2.477847 -16.35606,4.78078 -4.36168,1.99698 -8.71162,4.83216 -13.27727,6.284961 -6.77237,2.155152 -13.96692,2.990273 -20.5894,5.673052 -3.44208,1.3945 -6.22242,3.59968 -9.23636,5.680557 -1.40008,0.96674 -2.67932,1.72412 -3.84772,2.892136 -2.91234,2.911187 -6.10081,6.955944 -7.92691,10.582564 -0.64809,1.287126 -0.91014,2.392796 -0.92614,3.65606 -0.0098,0.770853 -0.08813,1.199957 -0.32077,1.924243 -0.37215,1.159547 -0.82242,2.414153 -0.96905,3.65606 -0.37426,3.169804 0.89439,8.120689 2.42204,10.775757 0.92787,1.612709 3.09977,2.812473 3.96221,4.425759 0.76143,1.424517 1.68776,2.706254 2.42936,4.040913 1.02543,1.84554 1.15127,4.79136 0.92286,6.92727 -0.15798,1.47666 -1.52881,3.01221 -2.45841,4.23333 -0.5459,0.71697 -1.13973,1.48378 -1.76164,2.10916 -0.376,0.37792 -0.71717,0.64944 -1.09201,1.02466 -2.94736,2.94871 -5.55606,6.877441 -6.70772,10.913151 -0.699652,2.45206 -1.006182,5.38557 -0.834732,7.8894 0.03464,0.50492 0.12392,0.86013 0.15817,1.34697 0.06966,0.99598 -0.0094,2.06875 0.468172,3.27121 0.44508,1.12048 1.68737,1.84073 2.31736,2.88713 0.77201,1.28212 1.08643,3.27795 1.2396,4.80984 0.22167,2.21596 -0.53357,4.34032 -0.54417,6.54242 -0.03676,7.59691 -0.0044,15.01294 1.29867,22.51364 0.81568,4.69515 1.89615,9.68664 4.57392,13.66212 1.08066,1.60482 2.36778,3.41938 3.76574,4.77789 4.00666,3.89082 9.91255,6.39041 15.37662,7.68543 3.72284,0.8813 7.7572,1.51245 11.54546,1.77223 0.74006,0.05 1.22728,0.15777 1.92424,0.18857 0.65136,0.0289 1.07758,-0.01 1.73182,0.0924 1.69545,0.26362 3.6526,0.6388 5.38788,0.6831 8.57481,0.21012 17.34877,0.42949 25.97727,0.01 6.063673,-0.29441 11.785983,-1.86074 17.703033,-3.23272 2.14938,-0.49838 4.37958,-0.53879 6.54242,-0.97175 3.85426,-0.76969 7.65079,-1.82803 11.54546,-2.45341 5.0723,-0.81587 10.14268,-1.53554 15.20151,-2.54769 2.72858,-0.54649 5.79774,-1.0872 8.27424,-2.21481 1.23344,-0.56187 1.99929,-1.41431 2.82287,-2.44186 1.06218,-1.32773 2.29177,-2.40723 3.52713,-3.65606 0.78894,-0.79856 1.39316,-1.58942 2.06472,-2.50152 0.99483,-1.34889 2.1513,-2.56886 3.32316,-3.67337 0.41756,-0.39447 0.72929,-0.71005 1.15455,-1.13723 1.23344,-1.23921 2.21673,-2.75937 3.46556,-4.04091 0.72736,-0.74468 1.22382,-1.22382 1.92232,-1.92424 0.71774,-0.72159 1.41239,-1.55672 2.12436,-2.30909 0.65039,-0.69081 1.23536,-1.42779 1.92425,-2.09935 3.0326,-2.95372 6.69827,-5.20315 10.19077,-7.52187 2.0243,-1.34504 4.67013,-3.1673 6.3173,-5.00322 2.08396,-2.32256 4.1044,-5.85739 6.22107,-8.27405 0.6273,-0.71832 1.22573,-1.42125 1.89539,-2.11667 1.22957,-1.27692 2.20324,-2.76802 3.073,-4.42575 1.63754,-3.11978 3.93124,-6.26707 5.51297,-9.42879 4.1294,-8.2575 6.39617,-19.272641 6.6117,-28.671211 0.0365,-1.63292 -0.44643,-3.194055 -0.55804,-4.810609 -0.16753,-2.431473 -0.025,-4.87911 -0.19243,-7.312123 -0.4926,-7.193396 -1.10066,-14.40815 -5.15503,-20.589393 -1.40856,-2.148994 -3.35783,-3.262553 -5.06077,-5.198343 -1.56056,-1.77338 -2.33026,-3.90294 -3.8658,-5.577417 -0.47143,-0.512233 -0.84283,-0.877069 -1.32966,-1.33927 -0.72927,-0.69369 -1.77607,-1.441836 -2.69394,-1.95561 -1.9454,-1.08912 -4.13326,-0.6121 -6.15756,-1.340619 -5.05884,-1.821104 -10.262,-3.564658 -15.58637,-4.693611 -0.95058,-0.201666 -1.93194,-0.0033 -2.88636,-0.162406 -1.68564,-0.280747 -3.3251,-0.986174 -5.00303,-1.34928 -2.5631,-0.55476 -5.15697,-0.791057 -7.69697,-1.50418 -7.38525,-2.07414 -14.02581,-5.156584 -21.16667,-7.815503 -5.3417,-1.98909 -10.82579,-3.35992 -16.35606,-4.640504 -1.98967,-0.460663 -3.71956,-1.622906 -5.77273,-1.890376 -9.15747,-1.19303 -17.61798,0.5969 -26.362123,2.99393 -3.2868,0.90093 -6.82009,1.537663 -10.00606,2.735696 -5.72308,2.15188 -10.86312,5.062104 -16.74091,6.978264 -5.81794,1.896726 -11.98168,2.993159 -17.70303,5.144463 -7.09776,2.668733 -14.13125,6.682509 -19.42715,12.113106 -1.68486,1.727584 -3.07494,4.210434 -4.21909,6.347113 -2.977769,5.56164 -5.962462,12.893773 -2.541928,19.05 0.660018,1.187836 1.640808,2.058747 2.277148,3.271213 1.18033,2.248477 1.80494,4.726324 3.41091,6.73485 1.36622,1.708919 3.83654,3.328749 3.75074,5.772729 -0.06774,1.93194 -1.21689,3.89274 -2.06067,5.5803 -2.6545,5.30918 -7.316936,9.898881 -8.726059,15.778791 -0.670406,2.79708 -0.89612,5.80659 -1.167823,8.65909 -0.14432,1.5163 -0.444119,3.32528 0.03117,4.8106 1.009073,3.15268 3.955472,4.88931 4.217552,8.46667 0.16452,2.24636 -0.58209,4.4985 -0.73526,6.73485 -0.38784,5.66305 -0.77393,11.63974 -0.38484,17.31818 0.39504,5.76118 3.17403,11.47811 5.70807,16.54849 0.85629,1.7145 1.36255,3.59063 2.42512,5.19545 0.92191,1.39315 1.85074,2.89021 3.043,3.94278 0.55091,0.4849 1.07642,0.86783 1.6991,1.29693 1.397,0.95828 3.12882,1.4836 4.61819,2.45726 1.42124,0.93134 2.51383,1.81071 4.0409,2.36682 4.35072,1.58943 8.71105,2.0397 13.27728,2.4765 5.0367,0.48106 10.13286,1.2777 15.20151,1.49899 3.03684,0.13277 6.22839,-0.18473 9.23636,-0.18473 0.83628,0 1.30291,0 2.11667,0 0.84532,0 1.32754,0 2.11667,0 1.97889,0 4.506,0.16932 6.54242,0.19242 9.635263,0.11546 19.461793,-1.34119 28.863643,-3.63682 5.18391,-1.26615 10.17732,-3.29237 15.39394,-4.46616 6.05366,-1.36429 12.84431,-0.17318 18.66515,-2.84596 1.4605,-0.67156 3.14613,-1.87806 4.23333,-2.9056 0.86976,-0.82358 1.88961,-1.89153 2.70164,-2.67662 0.65616,-0.635 1.20457,-0.95443 1.91654,-1.58943 1.54902,-1.38353 3.41168,-2.69971 4.79329,-4.20062 1.41624,-1.53939 2.43224,-3.49442 3.8658,-5.00303 0.47529,-0.5003 0.77932,-0.85821 1.21035,-1.34697 0.44065,-0.50223 0.83512,-0.89092 1.29117,-1.34697 0.96404,-0.96405 2.83248,-2.68239 3.84849,-3.64836 2.5573,-2.43225 5.89203,-3.88697 8.64177,-6.09023 0.95443,-0.76393 1.86843,-1.72412 2.70933,-2.57656 1.80493,-1.8338 2.91714,-4.02744 4.81253,-5.74002 0.8178,-0.73891 2.66507,-2.13975 3.46364,-2.92196 0.5407,-0.53013 0.92747,-0.92902 1.42009,-1.53651 0.41754,-0.51415 0.72544,-0.97366 1.09104,-1.53939 1.83573,-2.84365 4.30453,-5.45138 5.88626,-8.46667 1.18724,-2.26406 1.91847,-4.76288 3.01721,-7.1197 1.52593,-3.26659 3.9139,-6.08811 5.12426,-9.42878 0.51377,-1.42317 0.52917,-2.98142 0.97751,-4.42576 0.55419,-1.79397 1.55093,-3.37589 2.01853,-5.19546 0.56573,-2.196711 0.59843,-4.506191 0.94673,-6.734841 0.83127,-5.32881 1.45473,-10.36551 1.45473,-15.778792 0,-8.412787 -0.30019,-16.70935 -4.4931,-24.245453 -0.7216,-1.296747 -2.02239,-2.173047 -2.91139,-3.2741 -1.74914,-2.164964 -3.36934,-4.623183 -5.28204,-6.731963 -0.4522,-0.499724 -0.74277,-0.87091 -1.18533,-1.34697 -0.59653,-0.639617 -1.11991,-1.112787 -1.6972,-1.731817 -0.42717,-0.457007 -0.71387,-0.87245 -1.18917,-1.271923 -1.21613,-1.023697 -2.6112,-1.327343 -3.83117,-2.401263 -0.76393,-0.67233 -1.84536,-1.80109 -2.88636,-2.24867 -1.81457,-0.780087 -4.05823,-0.768543 -5.96514,-1.33658 -3.09226,-0.921327 -6.11717,-2.242317 -9.23637,-3.040494 -2.44572,-0.625763 -5.03382,-0.591513 -7.50455,-1.116639 -1.02562,-0.217824 -1.88576,-0.884957 -2.88636,-1.161664 -1.37006,-0.378113 -2.55732,0.174913 -4.04091,-0.315383 -3.16923,-1.047173 -6.64633,-3.372427 -9.81364,-3.977987 -1.27385,-0.24361 -2.18209,-0.443537 -3.27121,-0.9779 -3.14036,-1.54228 -6.37117,-2.280227 -9.81364,-3.384357 -1.76837,-0.567076 -3.23465,-1.485516 -5.00303,-2.010256 -3.50981,-1.041786 -7.24477,-0.902086 -10.77575,-1.7526 -1.72028,-0.414674 -3.26929,-1.24537 -5.00303,-1.679094 -5.00496,-1.251529 -11.22026,-2.138412 -16.35606,-1.786852 -5.79467,0.39697 -11.836023,1.789736 -17.318183,3.639706 -8.94157,3.01721 -17.30433,7.506083 -26.36212,10.439206 -3.8966,1.261727 -7.99177,1.795704 -11.93031,2.919461 -6.00344,1.713153 -11.76385,3.991649 -17.12575,7.191279 -1.98755,1.186294 -3.96433,2.099927 -5.58031,3.695507 -0.70889,0.699847 -1.36063,1.290013 -2.03488,1.978506 -2.20018,2.246167 -4.5595,5.054217 -5.997285,7.834361 -3.5204,6.807969 -5.736743,14.840719 -2.295813,22.128789 0.960196,2.03373 3.084558,3.126123 4.537938,4.625686 2.27099,2.343534 2.91715,4.619914 3.90313,7.689466 0.53013,1.64965 1.24345,3.02414 1.09932,4.8106 -0.26824,3.32259 -3.69378,6.60593 -5.35978,9.23637 -0.9908,1.564411 -1.603085,3.013751 -2.511712,4.618181 -2.117437,3.73861 -4.187346,7.74508 -5.441566,11.9303 -0.6198,2.06895 -0.322117,4.87546 0.848976,6.73485 0.58189,0.92402 1.499947,1.76684 2.064134,2.69394 0.31115,0.51127 0.415253,0.96847 0.58728,1.5394 0.502226,1.66639 1.507643,3.39744 1.852466,5.19545 1.283857,6.69136 -1.369673,14.26633 -1.818407,20.97424 -0.237646,3.556 0.986944,7.61038 2.033924,10.96819 0.417176,1.33734 0.606327,2.81516 1.309445,4.0409 2.41184,4.20255 5.61243,8.88038 9.35662,11.99188 2.8373,2.3572 6.51838,3.81962 9.60409,5.74772 1.19053,0.74468 1.99794,1.59327 3.27121,2.09357 2.25695,0.88708 4.95512,0.91402 7.31212,1.50668 5.94514,1.49129 11.78349,2.67662 17.89546,3.25775 2.69721,0.25592 5.39538,-0.19052 8.08182,-0.15972 3.96336,0.0481 7.9502,0.38105 11.9303,0.40025 2.23885,0.01 4.48021,0.31365 6.73485,0.15971 2.38336,-0.16356 4.72998,-0.8332 7.119693,-0.93711 4.09864,-0.17895 8.04911,0.0154 12.12273,-0.66963 6.75024,-1.13338 13.16567,-3.66569 19.8197,-5.10887 6.0325,-1.30656 12.03806,-2.23982 17.89545,-4.23526 1.81649,-0.61768 3.87735,-1.00253 5.5803,-1.8896 1.11799,-0.58305 2.18787,-1.52015 3.04608,-2.37644 0.5715,-0.5715 0.96212,-1.09297 1.57211,-1.69718 1.21419,-1.19688 2.45725,-2.28793 3.65606,-3.46364 0.96789,-0.94865 2.10127,-1.85689 3.06147,-2.88829 1.22959,-1.3258 1.91847,-3.28276 3.15383,-4.58547 0.42911,-0.45412 0.80433,-0.74468 1.28924,-1.18725 0.67349,-0.61384 1.40277,-1.16032 2.11667,-1.74722 1.03524,-0.85436 1.96079,-1.66639 2.88635,-2.4765 0.82357,-0.71966 1.83957,-1.2623 2.69394,-1.9531 1.4047,-1.13146 2.86713,-2.62275 4.23333,-3.81962 2.7055,-2.36682 5.79197,-4.23911 8.19343,-6.93497 1.41433,-1.5875 3.58294,-2.85943 5.27627,-4.19485 1.47974,-1.16802 2.75167,-2.20134 3.94277,-3.69647 0.56573,-0.71178 1.0622,-1.40432 1.61446,-2.11667 3.5175,-4.5441 4.94721,-10.85465 7.52184,-15.97121 2.3649,-4.69881 5.42446,-9.05703 7.1832,-14.04697 1.39509,-3.95798 1.16033,-8.749531 1.7049,-12.892431 0.45603,-3.46325 1.25073,-6.88955 1.49127,-10.3909 0.19436,-2.846345 -0.30211,-5.639382 -0.35791,-8.466672 -0.13853,-7.231303 -0.66193,-14.498973 -4.52196,-20.974243 -1.43357,-2.40511 -3.47517,-4.330893 -5.26857,-6.35 -1.2065,-1.35813 -2.2764,-3.135746 -3.50406,-4.425757 -0.82934,-0.872453 -1.59327,-1.553056 -2.4765,-2.501516 -0.46567,-0.499147 -0.73697,-0.874376 -1.18917,-1.339464 -0.43487,-0.446426 -0.8717,-0.773162 -1.32774,-1.194763 -1.3893,-1.28982 -2.14746,-2.799387 -3.8504,-3.748616 -3.54446,-1.976583 -7.8432,-2.585604 -11.73787,-3.508471 -3.75999,-0.891309 -7.59116,-1.438756 -11.35304,-2.309669 -1.48166,-0.343284 -2.74781,-1.19303 -4.23333,-1.506104 -2.87097,-0.604983 -5.80159,-0.759113 -8.65909,-1.493983 -2.62274,-0.67464 -4.71632,-1.960417 -7.1197,-2.849996 -1.10451,-0.408707 -2.33603,-0.34059 -3.46363,-0.672714 -1.89538,-0.558223 -3.69647,-1.52958 -5.58031,-2.158037 -1.37006,-0.4572 -2.73627,-0.940186 -4.04091,-1.533816 -0.66771,-0.303643 -1.22959,-0.529743 -1.92424,-0.829347 -1.40277,-0.60479 -2.9133,-1.249219 -4.42576,-1.450493 -1.18725,-0.158173 -2.51306,-0.12681 -3.65606,-0.400243 -4.28528,-1.025237 -8.92078,-2.516524 -13.27727,-3.44247 -6.78295,-1.442027 -13.91227,-1.668127 -20.781823,-0.970203 -5.40096,0.548793 -10.61027,2.626399 -15.58636,4.572576 -6.34461,2.481697 -12.79852,4.923753 -19.24242,7.15953 -6.60343,2.291003 -13.21358,4.65128 -19.8197,6.89995 -3.14556,1.070647 -6.49182,1.84092 -9.42879,3.40764 -0.63019,0.336163 -1.14569,0.636153 -1.73181,1.046403 -3.76228,2.632556 -9.05645,4.785397 -11.90029,8.63138 -1.804935,2.440903 -3.483451,5.02439 -4.609132,7.889396 -2.879629,7.328284 -4.660516,15.707397 -1.409316,23.283333 2.21923,5.171594 8.430098,7.480493 8.272698,13.854549 -0.27517,11.14251 -10.980298,18.665921 -13.565135,29.056061 -1.6136671,6.48604 5.08693,9.78073 5.810443,15.58636 0.371573,2.98123 -0.368683,6.62132 -0.654433,9.62121 -0.182227,1.9127 0.08139,3.85619 -0.04984,5.77273 -0.20262,2.95949 -0.683104,5.87086 -0.55264,8.85152 0.22802,5.20507 1.125487,10.68147 3.672416,15.20151 0.462971,0.82165 0.717164,1.69141 1.19861,2.50152 2.717599,4.57007 8.205929,7.18512 12.824489,9.37876 0.90767,0.43295 1.70065,0.68503 2.69394,1.14107 1.74895,0.80049 3.33491,2.18402 5.19546,2.68624 1.6864,0.45412 3.12401,1.10452 4.61818,1.85112 1.12068,0.55803 2.19768,0.98714 3.46364,1.32196 8.54113,2.25329 17.71322,3.96394 26.55454,4.61241 2.85942,0.20974 5.80775,1.18341 8.65909,0.91209 4.43211,-0.42141 8.98641,-0.77355 13.4697,-1.58173 3.664143,-0.65996 7.302113,-0.33482 10.968183,-0.72736 3.91198,-0.41949 7.67965,-1.75684 11.54545,-2.22058 5.1512,-0.6196 10.44094,-1.0718 15.58637,-1.86844 4.77404,-0.74083 9.97527,-2.29947 14.43181,-4.15444 5.39558,-2.24559 12.02267,-5.1435 16.16364,-9.23444 0.56765,-0.55995 0.98714,-0.96212 1.52208,-1.53939 0.69272,-0.74661 1.34504,-1.63753 1.93578,-2.50152 1.61059,-2.35719 2.70356,-4.55853 5.00881,-6.30381 2.3418,-1.77416 4.81254,-3.34241 7.1197,-5.17814 0.55994,-0.4445 1.01791,-0.90824 1.53938,-1.40277 1.61059,-1.53362 3.46173,-2.99028 4.98573,-4.62588 0.44449,-0.47721 0.7389,-0.87553 1.18916,-1.33927 0.9679,-1.00061 1.9512,-1.92425 2.85174,-2.89407 0.86976,-0.93518 1.39123,-1.6356 2.51883,-2.53807 1.52207,-1.21997 3.41553,-2.11859 4.8106,-3.43477 1.0795,-1.01985 2.02623,-2.24559 3.0788,-3.25582 0.58497,-0.55996 1.28347,-1.08335 1.92423,-1.5875 0.86591,-0.68311 1.69717,-1.38449 2.35527,-2.26868 1.10644,-1.48533 1.6356,-3.20695 2.515,-4.81061 2.29177,-4.17407 4.36803,-8.20516 6.38463,-12.50758 2.4053,-5.13349 5.30127,-10.58448 6.51164,-16.16363 1.11797,-5.151591 0.98136,-10.900841 1.5086,-16.163641 0.7312,-7.306539 0.92556,-14.438168 0.19243,-21.743939 -0.35023,-3.490383 -0.6119,-7.243233 -2.32257,-10.390909 -1.02946,-1.893647 -2.4688,-4.20697 -3.83693,-5.772727 -0.8967,-1.02562 -1.8338,-2.331797 -2.73243,-3.271213 -0.6119,-0.64058 -1.08143,-1.06122 -1.7068,-1.731817 -1.04871,-1.12472 -1.87614,-2.07876 -3.0961,-3.268326 -0.56574,-0.551487 -0.9833,-0.87861 -1.5394,-1.382567 -1.09874,-0.99445 -2.09934,-2.19383 -3.1115,-3.20579 -1.372,-1.370637 -2.65737,-2.075296 -4.20063,-3.366463 -1.17571,-0.983287 -2.16091,-1.871517 -3.27121,-2.857114 -1.72026,-1.527079 -3.69646,-2.877703 -5.96513,-3.68069 -1.9589,-0.693306 -4.1333,-0.734293 -6.1576,-1.178213 -8.2877,-1.817833 -16.81209,-2.338149 -25.01514,-4.618183 -4.38342,-1.21843 -8.39932,-3.349913 -12.7,-4.723437 -4.8337,-1.544013 -9.85597,-3.456326 -14.81667,-4.495803 -5.03381,-1.05429 -10.30431,-1.39719 -15.39393,-2.13456 -4.318,-0.6255734 -8.47629,-2.0256504 -12.89243,-2.2192304 -3.8327,-0.167793 -7.497813,0.922097 -11.160603,1.88672 -3.85003,1.0140774 -7.74258,1.8057104 -11.54546,3.0453074 -1.79801,0.58593 -3.40225,1.604047 -5.19545,2.174586 -10.67647,3.397634 -21.60771,6.0579 -31.94243,10.44094 -5.09289,2.160154 -10.00298,5.146963 -14.62424,8.163597 -2.74493,1.791856 -5.5753,3.607763 -7.88939,5.836806 -0.69869,0.672907 -1.23383,1.058717 -1.91674,1.74144 -0.876679,0.876684 -1.688518,1.915774 -2.508245,2.878857 -1.130877,1.32869 -2.372013,2.713953 -3.357226,4.04091 -3.334134,4.490606 -2.911187,10.333183 -3.411104,15.586363 -0.26516,2.78688 -0.494916,6.251866 0.767966,8.851516 1.079691,2.222307 2.535958,5.123487 3.876384,7.11681 0.792213,1.17783 2.34315,1.71123 3.244273,2.699709 2.327172,2.55309 2.020832,7.6456 1.10644,10.77287 -2.28927,7.827821 -9.684903,13.041171 -11.9589731,20.974251 -0.64462,2.24905 -1.596353,5.15697 -1.173403,7.50454 0.283249,1.57115 1.672743,2.46746 2.3654701,3.84849 1.222663,2.43801 1.971193,6.11832 2.326217,8.85151 0.38177,2.93948 -0.303261,6.10755 -0.504537,9.04394 -0.397547,5.80544 -0.734483,11.68016 -0.59151,17.51061 0.0991,4.04668 1.371983,8.79379 3.355493,12.12273 1.287703,2.16092 2.88925,4.29298 4.915863,5.86701 1.569022,1.21612 3.161142,2.19171 4.810602,3.15384 0.5003,0.29056 0.88861,0.60036 1.34697,0.93903 1.86824,1.38738 4.04399,2.17054 6.15757,3.175 5.81718,2.76706 11.96263,4.96069 18.28031,6.69828 2.51594,0.69273 5.17659,1.0237 7.69696,1.69719 6.78027,1.81071 13.6121,3.78883 20.5894,4.75095 4.01185,0.55226 8.26462,1.66836 12.31515,1.61636 11.618383,-0.14624 22.854233,-3.69262 34.251513,-5.49178 4.28722,-0.67734 8.64755,-0.67156 12.89243,-1.67025 3.83116,-0.90054 7.27363,-2.69394 10.96818,-3.89466 2.31486,-0.75238 4.52197,-0.7774 6.73485,-1.99737 2.921,-1.61059 5.75156,-4.00627 8.46666,-6.02288 0.6427,-0.47721 1.17572,-0.94287 1.72413,-1.5086 0.63115,-0.65232 1.08527,-1.2142 1.68756,-1.92424 1.85497,-2.19364 3.556,-4.80484 5.22047,-7.1197 1.26807,-1.76453 3.17307,-3.71379 4.83755,-5.13773 1.18147,-1.01215 2.52267,-1.62021 3.84847,-2.47265 1.4297,-0.91979 2.5708,-2.032 3.8485,-3.03645 1.2392,-0.97367 2.6612,-1.84728 3.84847,-2.87867 0.47913,-0.41564 0.85823,-0.80049 1.32966,-1.23344 0.80433,-0.73891 1.50474,-1.35659 2.3264,-2.17439 0.72737,-0.72544 1.21997,-1.25461 1.92424,-1.89153 0.60806,-0.55034 1.11993,-0.94673 1.72413,-1.57211 0.6523,-0.67349 1.17377,-1.40277 1.7953,-2.11667 0.50224,-0.57727 0.93134,-0.96982 1.4759,-1.53939 0.51956,-0.54264 0.82357,-0.97367 1.372,-1.50668 1.15453,-1.12184 2.1744,-1.89346 3.2539,-2.92678 1.16993,-1.11798 2.2148,-2.18209 3.26927,-3.26351 1.8819,-1.93425 3.04223,-4.63897 4.5316,-6.92727 1.3739,-2.10782 2.9768,-3.95047 4.2391,-6.15758 3.96393,-6.92362 6.96576,-15.28368 8.5494,-23.09091 0.78126,-3.859071 0.8159,-7.838211 1.18726,-11.737881 0.32137,-3.37204 1.14484,-6.8072 1.12184,-10.198482 -0.025,-3.54561 -0.97751,-7.23323 -1.31427,-10.77576 -0.6369,-6.68751 0.22514,-12.72713 -3.2789,-18.857576 -1.4528,-2.53846 -3.05953,-5.618594 -4.83753,-7.889393 -0.39447,-0.503381 -0.71197,-0.887267 -1.16033,-1.344084 -1.19111,-1.215929 -2.6266,-2.284846 -3.84078,-3.469409 -0.72546,-0.708121 -1.22766,-1.248448 -1.92426,-1.913851 -0.55417,-0.53013 -0.9775,-0.853593 -1.5394,-1.371023 -0.58303,-0.536863 -1.1122,-1.020233 -1.7318,-1.53824 -4.52967,-3.787677 -9.9387,-6.79046 -15.58637,-8.818803 -9.86366,-3.542337 -20.46816,-4.134043 -30.59545,-6.53742 -18.82486,-4.467707 -36.75687,-12.2478034 -56.3803,-13.0996664 -1.89922,-0.082357 -3.72822,0.538789 -5.580303,0.873029 -6.5584,1.183217 -12.85856,3.9537414 -19.24242,5.8473874 -1.82649,0.541674 -3.76613,0.57381 -5.58031,1.117407 -13.93344,4.174069 -27.65752,7.76682 -40.40909,14.97157 -3.79441,2.143799 -7.55111,4.55141 -10.96818,7.241886 -0.55784,0.439304 -1.02735,0.80972 -1.53189,1.313873 -1.736235,1.735667 -3.123615,3.683193 -4.818102,5.380374 -0.800677,0.802023 -1.384109,1.333693 -2.083186,2.116666 -1.444144,1.617327 -2.74243,3.94874 -3.736494,5.965153 -1.5636391,3.172114 -2.0306531,6.181437 -2.5680931,9.621211 -0.68503,4.383809 -1.308293,8.026786 0.31827,12.315152 0.862444,2.273877 2.2273101,4.992254 3.6897341,6.927273 2.428586,3.213296 6.099272,6.866846 5.693833,11.353026 -0.894194,9.899081 -10.3816711,15.056821 -12.6420801,24.437881 -0.601717,2.49805 -0.891507,6.01711 0.02193,8.46667 1.077,2.88829 2.97815,4.35456 3.206943,7.69697 0.70331,10.26776 -1.951374,20.52397 -0.722553,30.78788 0.429683,3.58871 1.325226,7.01194 2.5386531,10.39091 0.4064,1.13145 0.942493,1.98389 1.522844,3.07879 1.237096,2.3341 3.735149,3.31739 5.522192,5.19545 1.091237,1.14685 1.799358,2.52653 3.086677,3.51174 1.124332,0.86014 2.604272,1.87614 3.848292,2.50152 0.59959,0.30018 1.11972,0.56765 1.73182,0.86398 0.5842,0.28287 1.09932,0.51378 1.73182,0.71967 5.18737,1.69333 10.58929,2.70549 15.97121,3.68108 2.2554,0.40986 4.31992,1.43163 6.54242,1.97427 6.95883,1.69911 13.88072,3.31355 20.78182,5.20508 7.32924,2.0089 14.64752,3.98895 22.32121,4.42383 1.76626,0.10006 3.63297,-0.34059 5.38788,-0.52724 3.934113,-0.42141 7.820123,-0.79087 11.737883,-1.35467 5.13195,-0.73699 10.11189,-2.05509 15.20151,-3.01529 8.42241,-1.5875 17.42402,-2.54385 25.4,-5.86317 5.09539,-2.12051 10.24467,-5.78619 14.43182,-9.3191 2.23212,-1.88191 3.51944,-4.46424 5.42251,-6.65018 0.66387,-0.762 1.24306,-1.37584 1.89923,-2.11667 0.43103,-0.48683 0.75815,-0.86783 1.16994,-1.34697 1.01408,-1.17764 1.92424,-2.55924 3.06148,-3.62335 0.91017,-0.84859 2.33794,-1.79917 3.2635,-2.59195 0.5792,-0.49646 1.00447,-0.94673 1.5394,-1.48167 0.635,-0.63308 1.12953,-1.12953 1.73183,-1.73182 1.0256,-1.02562 2.05317,-2.10897 3.07877,-3.06147 1.61444,-1.50091 3.2135,-2.5958 4.8106,-4.04476 2.39954,-2.17439 5.22433,-3.96393 7.69697,-6.16142 0.52726,-0.46759 0.97946,-0.83897 1.45859,-1.36429 0.67154,-0.73891 1.29887,-1.43356 2.00504,-2.09165 0.6985,-0.65039 1.19883,-1.03332 1.92427,-1.74914 1.6202,-1.59904 3.19616,-3.12881 4.8029,-4.8106 0.82357,-0.86206 1.35466,-1.50668 2.11666,-2.30909 0.54264,-0.5715 0.95634,-1.13357 1.44317,-1.73182 0.97367,-1.19784 2.2225,-2.4613 3.13653,-3.65606 3.6272,-4.74095 5.66304,-10.04262 7.68543,-15.58637 3.50788,-9.6239 5.56297,-19.521051 5.44561,-29.825751 -0.0558,-4.75558 -1.00447,-9.499412 -1.32967,-14.239398 -0.3211,-4.699961 0.4984,-9.562714 -0.3271,-14.239394 -1.1334,-6.412923 -3.68493,-12.03575 -7.11393,-17.510606 -1.25077,-1.998517 -2.50153,-4.06727 -4.12943,-5.772727 -0.61961,-0.649623 -1.0949,-1.12241 -1.72411,-1.724313 -0.4753,-0.454314 -0.85437,-0.767773 -1.34699,-1.177253 -2.61118,-2.171314 -5.74577,-3.943927 -8.65907,-5.767147 -2.70164,-1.691023 -5.29937,-3.701087 -8.27424,-4.90624 -5.6669,-2.295623 -11.73403,-3.131127 -17.70303,-4.209666 -6.3173,-1.14146 -12.49989,-2.52345 -18.85759,-3.464214 -2.99027,-0.442576 -5.88626,-1.357746 -8.85151,-1.91693 -4.3257,-0.815879 -8.61099,-1.328883 -12.89243,-2.456296 -6.47122,-1.7043004 -12.79813,-4.0362904 -19.43485,-5.0813474 -2.59195,-0.40832 -5.26665,-0.060036 -7.88939,-0.30961 -3.04607,-0.28979 -6.17278,-0.809147 -9.23636,-0.734097 -3.318943,0.081201 -6.394843,1.03197 -9.621213,1.640417 -2.61139,0.492604 -5.27647,0.482984 -7.8894,0.975783 -2.11647,0.39928 -4.08228,1.281351 -6.15757,1.828991 -15.79803,4.1681034 -30.55755,9.4070464 -44.64243,17.7859664 -4.75461,2.828443 -9.21943,5.821987 -13.662115,9.155546 -1.76761,1.326187 -3.68223,2.711834 -5.38788,4.057073 -0.855903,0.674831 -1.51534,1.23594 -2.276377,2.076064 -2.010643,2.21942 -3.9048661,4.892773 -5.1681301,7.69697 -2.366243,5.252413 -3.35665,12.0015 -2.786496,17.70303 0.16202,1.620213 0.116416,3.465369 0.699846,5.00303 1.320417,3.480763 3.116887,7.239579 5.095971,10.390908 1.1955331,1.90346 2.9109931,3.48096 3.8502161,5.5803 0.980403,2.19133 1.17552,4.79387 0.858983,7.1197 -0.09948,0.73044 -0.143356,1.22093 -0.274013,1.92424" stroke-dasharray="" stroke-dashoffset="0.00"></path>
                            </g>
                        </svg>
                            <span>language<br>model<br>self-portrait</span>
                        </p>
                        <p>
                            <span>K</span>
                            <span>a</span>
                            <span>t</span>
                            <span>y</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>I</span>
                            <span>l</span>
                            <span>o</span>
                            <span>n</span>
                            <span>k</span>
                            <span>a</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>G</span>
                            <span>e</span>
                            <span>r</span>
                            <span>o</span>
                        </p>
                    </div>
                    <h3>
                        Blank Page<br>with Machine 
                    </h3>
                    <p>
                            The text in this piece was written in collaboration with a language model trained on my own private writing. In this way, the words are all my own but I had to confront the recombination of my thoughts in ways I could not control.
                        </p>
                    
                    
                </div>
            </a>

            <a data-contributor="Chia Amisola" href="https://whenwe.love/forms">
                <div>
                    
                    <h3>
                        Concrete Form
                    </h3>
                    
                    
                    
                </div>
            </a>
            <a href="https://www.aworldof.space/" data-contributor="Ivan Zhao">
                <div>
                    <div>
                        <p>
                            <span>
                                interactive fiction
                            </span>
                        </p>
                        <p>
                            <span>I</span>
                            <span>v</span>
                            <span>a</span>
                            <span>n</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>Z</span>
                            <span>h</span>
                            <span>a</span>
                            <span>o</span>
                        </p>
                    </div>
                    <h3>
                        A World<br>of Space
                    </h3>
                    <p>
                            Maybe your consciousness touched mine
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Alicia Guo" href="https://thehtml.review/02/found-poem/">
                <div>
                    <div>
                        <p>
                            <span>
                                poetry
                            </span>
                            <img src="https://thehtml.review/02/found-poem/assets/shovel.png" alt="a small pixelated shovel illustration">
                        </p>
                        <p>
                            <span>A</span>
                            <span>l</span>
                            <span>i</span>
                            <span>c</span>
                            <span>i</span>
                            <span>a</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>G</span>
                            <span>u</span>
                            <span>o</span>
                        </p>
                    </div>
                    <h3>
                        found poem
                    </h3>
                    <p>
                            what does it mean to find a poem?
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="AC Gillette" href="https://thehtml.review/02/call-and-response.html">
                <div>
                    <div>
                        <p><span>poetry</span></p>
                        <p>
                            <span>A</span>
                            <span></span>
                            <span>C</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>G</span>
                            <span>i</span>
                            <span>l</span>
                            <span>l</span>
                            <span>e</span>
                            <span>t</span>
                            <span>t</span>
                            <span>e</span>
                            <span></span>
                        </p>
                    </div>
                    <h3>
                        Call &amp; Response
                    </h3>
                    <p>
                            a seed, emerging
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Alexander Miller" href="https://alex.miller.garden/grid-world">
                <div>
                    <div>
                        <p><span>essay</span></p>
                        <p>
                            <span>A</span>
                            <span>l</span>
                            <span>e</span>
                            <span>x</span>
                            <span>a</span>
                            <span>n</span>
                            <span>d</span>
                            <span>e</span>
                            <span>r</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>M</span>
                            <span>i</span>
                            <span>l</span>
                            <span>l</span>
                            <span>e</span>
                            <span>r</span>
                        </p>
                    </div>
                    <h3>
                        Grid World
                    </h3>
                    <p>
                            If you want to know how something is made, you should look for the grids.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Betsy Kenyon" href="https://www.betsykenyon.com/attention">
                <div>
                    <div>
                        <p><span>video<br>collage</span></p>
                        <p>
                            <span>B</span>
                            <span>e</span>
                            <span>t</span>
                            <span>s</span>
                            <span>y</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>K</span>
                            <span>e</span>
                            <span>n</span>
                            <span>y</span>
                            <span>o</span>
                            <span>n</span>
                        </p>
                    </div>
                    <h3>
                        Seek Shelter
                    </h3>
                    <p>
                            A poetic performance, exploring “virus” both in its digital form and environmental implication.<br>The thermal footage, recording elemental forces, was created while traveling to the extreme edges of the contiguous United
                            States.
                        </p>
                    <p>
                            This piece was made in collaboration with creative web designer Belle Krupcheck.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Logan Williams" href="https://observatory.subject.space/">
                <div>
                    <div>
                        <p>exquisite<br>corpse<br>poetry</p>
                        <p>
                            <span>L</span>
                            <span>o</span>
                            <span>g</span>
                            <span>a</span>
                            <span>n</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>W</span>
                            <span>i</span>
                            <span>l</span>
                            <span>l</span>
                            <span>i</span>
                            <span>a</span>
                            <span>m</span>
                            <span>s</span>
                        </p>
                    </div>
                    <h3>
                        Observatory
                    </h3>
                    <p>
                            Observatory pulls recent feelings from the internet into an unpredictable exquisite corpse. The words are (mostly) written by people, not generated by machines.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Nathan Mifsud" href="https://mifsud.org/sun">
                <div>
                    <div>
                        <p>
                            <span lang="mt">
                                esej 
                            </span>
                            <span>
                                essay
                            </span>
                        </p>
                        <p>
                            <span>N</span>
                            <span>a</span>
                            <span>t</span>
                            <span>h</span>
                            <span>a</span>
                            <span>n</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>M</span>
                            <span>i</span>
                            <span>f</span>
                            <span>s</span>
                            <span>u</span>
                            <span>d</span>
                        </p>
                    </div>
                    <h3>
                        Sun Letters
                    </h3>
                    <p>
                            Each sunrise suffuses agony and ecstasy alike. Bodies glow, oceans sparkle. The sun speaks the same fiery tongue as ever. By contrast, language is many-tongued, ever-constructed, in constant advance and retreat. Each sentence is a swell of voices, a tide
                            of generations.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Katherine Yang" href="https://www.proseplay.net/">
                <div>
                    <div>
                        <p>
                            <span>
                            poem
                        </span>
                            <span>
                            tool
                        </span>
                            <span>
                            experience
                        </span>
                            <span>
                            toy
                        </span>
                        </p>
                        <p>
                            <span>K</span>
                            <span>a</span>
                            <span>t</span>
                            <span>h</span>
                            <span>e</span>
                            <span>r</span>
                            <span>i</span>
                            <span>n</span>
                            <span>e</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>Y</span>
                            <span>a</span>
                            <span>n</span>
                            <span>g</span>
                        </p>
                    </div>
                    <h3>
                        Prose Play
                    </h3>
                    <p>
                            A tool for exploring alternate word choices, worlds, and possibilities.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Esther Bouquet" href="https://thehtml.review/02/whispers-from-a-soft-garden.html">
                <div>
                    <div>
                        <p>
                            <span>     
                                <span>`·, </span>
                            <span>poetry, illustration, garden</span>
                            
                            </span>
                        </p>
                        <p>
                            <span>E</span>
                            <span>s</span>
                            <span>t</span>
                            <span>h</span>
                            <span>e</span>
                            <span>r</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>B</span>
                            <span>o</span>
                            <span>u</span>
                            <span>q</span>
                            <span>u</span>
                            <span>e</span>
                            <span>t</span>
                        </p>
                    </div>
                    <h3>
                        whispers from<br>a soft garden
                    </h3>
                    <p>
                            It is not about how fast you can read the entire poem or even its readability, it is more about the experience of discovering it bits by bits, drawing it little by little, with patience and care, just like you would trim off bushes in a garden to shape
                            them.
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Matthew Baker" href="https://thehtml.review/02/discrepancies.html">
                <div>
                    <div>
                        <p><span>f</span><span>i</span><span>c</span><span>t</span><span>i</span><span>o</span><span>n</span></p>
                        <p>
                            <span>M</span>
                            <span>a</span>
                            <span>t</span>
                            <span>t</span>
                            <span>h</span>
                            <span>e</span>
                            <span>w</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>B</span>
                            <span>a</span>
                            <span>k</span>
                            <span>e</span>
                            <span>r</span>
                        </p>
                    </div>
                    <h3>
                        Discrepencies
                    </h3>
                    <p>
                            We were all there that day at the farm, gathered around the table...
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Lucy Zhang" href="https://thehtml.review/02/cinderella-has-anterograde-amnesia.html">
                <div>
                    
                    <h3>
                        Cinderella<br>Has Anterograde<br>Amnesia 
                    </h3>
                    <p>
                            I wore the glass slipper because my brain shattered every fifteen minutes
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Reese Oxner" href="https://www.reese.land/siggyshop.html">
                <div>
                    <div>
                        <p>
                            <marquee scrollamount="3" direction="right">essay</marquee>
                        </p>
                        <p>
                            <span>R</span>
                            <span>e</span>
                            <span>e</span>
                            <span>s</span>
                            <span>e</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>O</span>
                            <span>x</span>
                            <span>n</span>
                            <span>e</span>
                            <span>r</span>
                        </p>
                    </div>
                    <h3>
                        Siggy Shop
                    </h3>
                    <p>
                            A love letter to online message boards
                        </p>
                    
                    
                </div>
            </a>
            <a data-contributor="Andy Wallace" href="https://thehtml.review/02/intimate-codex/index.html">
                <div>
                    <div>
                        <p>
                            <img src="https://thehtml.review/02/images/intimate-codex.gif" alt="pixelated rain">
                            <span>interactive<br>non-linear<br>text explorer</span>
                        </p>
                        <p>
                            <span>A</span>
                            <span>n</span>
                            <span>d</span>
                            <span>y</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>W</span>
                            <span>a</span>
                            <span>l</span>
                            <span>l</span>
                            <span>a</span>
                            <span>c</span>
                            <span>e</span>
                        </p>
                    </div>
                    <h3>
                        Intimate Codex
                    </h3>
                    <p>
                            Move between passages by typing a word you see on screen and pressing enter.
                        </p>
                    
                    
                </div>
            </a>

            <a data-contributor="Daniel Lichtman" href="https://www.daniellichtman.com/cicada-mountain/">
                <div>
                    <div>
                        <p><span>speculative<br>radio<br>play</span></p>
                        <p>
                            <span>D</span>
                            <span>a</span>
                            <span>n</span>
                            <span>i</span>
                            <span>e</span>
                            <span>l</span>
                            <span></span>
                            <span>&nbsp;</span>
                            <span>L</span>
                            <span>i</span>
                            <span>c</span>
                            <span>h</span>
                            <span>t</span>
                            <span>m</span>
                            <span>a</span>
                            <span>n</span>
                        </p>
                    </div>
                    <h3>
                        Cicada Mountain
                    </h3>
                    <p>
                            She has this to say about herself:
                            <br>I am not an insect,
                            <br>I am a goddess.
                            <br>I am a sacred creature of the night.
                            <br>I am not a pest,
                            <br>I am a blessing.
                            <br>I am not a noise-maker,
                            <br>I am a singer.
                            <br>I am not a nuisance,
                            <br>I am a miracle.
                        </p>
                    
                    
                </div>
            </a>

            <a data-contributor="Spencer Chang" href="https://htmlgarden.spencerchang.me/">
                <div>
                    
                    <h3>
                        html garden
                    </h3>
                    <p>
                            Visit again on different days to see the garden grow
                        </p>
                    
                    
                </div>
            </a>
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[fMRI-to-image with contrastive learning and diffusion priors (141 pts)]]></title>
            <link>https://stability.ai/research/minds-eye</link>
            <guid>36766700</guid>
            <pubDate>Tue, 18 Jul 2023 01:43:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://stability.ai/research/minds-eye">https://stability.ai/research/minds-eye</a>, See on <a href="https://news.ycombinator.com/item?id=36766700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="64517de42ae5bd1de7109aea">
  
  
    
    


  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="bright-inverse" data-section-id="64517de42ae5bd1de7109aec" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionTheme&quot;: &quot;bright-inverse&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-basic-grid&quot;
}" data-animation="none">
  <article id="article-">
  
    <div data-layout-label="Post Body" data-type="item" id="item-649ef0e9ffa5d34c6e503803"><div data-block-type="2" id="block-ffe1340be8f54ef91d86">

<p><span><strong>Authors: Dr. Paul Scotti (Princeton Neuroscience Institute, MedARC), Dr. Tanishq Mathew Abraham (Stability.AI, MedARC)</strong></span></p>



</div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="21.95589645254075" data-block-type="5" id="block-yui_3_17_2_1_1688137963339_2681">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p><em>Example images reconstructed from human brain activity.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688137963339_3489">
  <p><strong>Introduction</strong></p><p>MedARC, in collaboration with researchers at Princeton Neuroscience Institute, Ecole Normale Supérieure, PSL University, University of Toronto, and the Hebrew University of Jerusalem, along with <a href="https://eleuther.ai/" target="_blank"><span>EleutherAI</span></a><a href="https://eleuther.ai/"> and </a><a href="https://stability.ai/" target="_blank"><span>Stability AI</span></a>, is proud to release its collaborative paper on MindEye.&nbsp;</p><p><a href="https://www.medarc.ai/mindeye" target="_blank"><span>MindEye</span></a> is a state-of-the-art approach that reconstructs and retrieves images from fMRI brain activity. Functional magnetic resonance imaging (fMRI) measures brain activity by detecting changes in oxygenated blood flow. It is used to analyze which parts of the brain handle different functions and to assist in evaluating treatments for the brain. MindEye was trained and evaluated on the <a href="https://naturalscenesdataset.org/" target="_blank"><span>Natural Scenes Dataset</span></a> [1], an offline fMRI dataset containing data from human participants who each agreed to spend up to 40 hours viewing a series of static images, for a few seconds each, inside the MRI machine.&nbsp;</p><p>This is the first preprint put out by <a href="https://medarc.ai/" target="_blank"><span>MedARC</span></a> since its public launch and is currently undergoing peer review. MedARC is a Discord-based research community supported by Stability AI that is building foundation generative AI models for medicine using a decentralized, collaborative, and open research approach.&nbsp;</p><p><strong>Method &amp; Results</strong></p><p>MindEye achieves state-of-the-art performance across both image retrieval and reconstruction. That is, given a sample of fMRI activity from a participant viewing an image, MindEye can either identify which image out of a pool of possible image candidates was the original seen image (retrieval), or it can recreate the image that was seen (reconstruction).&nbsp;</p><p>To achieve the goals of retrieval and reconstruction with a single model trained end-to-end, we adopt a novel approach of using two parallel submodules that are specialized for retrieval (using contrastive learning) and reconstruction (using a diffusion prior).&nbsp;</p><p>Each unique image in the dataset was viewed three times, for three seconds at a time. Corresponding fMRI activity (flattened spatial patterns across 1.8mm cubes of cortical tissue called “voxels”) was collected for each image presentation. fMRI activity across the three same-image viewings was averaged together and input to MindEye to retrieve and reconstruct the original image.&nbsp;</p>
</div><div data-test="image-block-inline-outer-wrapper" data-aspect-ratio="64.9425287356322" data-block-type="5" id="block-yui_3_17_2_1_1688137963339_5352">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p><em>MindEye overall schematic depicts the retrieval and reconstruction submodules alongside an independent low-level perceptual pipeline meant to enhance reconstruction fidelity.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688137963339_4822">
  <p><strong>Retrieval</strong></p><p>For retrieval, MindEye finds the exact (top-1) match in a pool of test samples with &gt;90% accuracy for both image and brain retrieval, outperforming previous work which showed &lt;50% retrieval accuracy. This suggests that MindEye brain embeddings retain fine-grained image-specific signals.</p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688137963339_6344">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p><em>MindEye image retrieval. Given a pool of candidate images, the nearest neighbor search in CLIP space enables searching for the original image based on brain activity. </em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688137963339_7192">
  <p>We accomplish this feat through contrastive learning. First fMRI brain activity from regions of the brain receptive to visual information are flattened and fed through a dense 940M parameter multilayer perceptron (MLP). This outputs brain embeddings that are the same dimensionality as the outputs from the last hidden layer of CLIP ViT-L/14 [2] (although we could use any multimodal latent space). These brain embeddings are fed through a lightweight MLP projector and then we use a novel bidirectional implementation of mixup contrastive data augmentation using CLIP loss to train the model to map brain embeddings into the same space as pre-trained, frozen CLIP image embeddings.&nbsp;&nbsp;</p><p>For inference, you can simply compute the CLIP image embedding for every possible image in the pool of image candidates and see which one has the highest cosine similarity with the brain embedding output from MindEye. We found that this approach worked even when scaling up to the billions of image candidates contained in LAION-5B.</p><p><strong>Reconstruction</strong></p>
</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688137963339_8345">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p><em>Side-by-side comparison of reconstructions from fMRI-to-Image NSD papers.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688137963339_8694">

<p>For reconstructions, we take the outputs from the dense MLP backbone mentioned above and feed them through a diffusion prior trained from scratch to better align brain embeddings to CLIP image space. This is the same approach used by DALL-E 2 to align their CLIP text embeddings to CLIP image space before they feed the aligned embeddings through another diffusion model to output images. As visualized by UMAP dimensionality reduction, the inputs from the MLP backbone are clearly disjointed in reference to the CLIP embeddings following the initial MLP backbone (left subplot below), but they are well-aligned following the diffusion prior (right subplot).</p>



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1688137963339_9590">

      

      
        <figure>
          
        
        

        
          
            
          
        

        
          
          <figcaption>
            <p><em>UMAP plots depict CLIP image latents (blue), MindEye MLP backbone latents (orange), MindEye MLP projector latents (green), and MindEye diffusion prior latents (red).</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" id="block-yui_3_17_2_1_1688137963339_10395">
  <p>This alignment allows us to substitute CLIP image latents for MindEye brain latents. We can simply take any pre-trained generative model that accepts CLIP image latents as inputs and feed the model a brain latent instead (no fine-tuning required!). This flexibility suggests that MindEye reconstructions will continue to improve as newer, more powerful image generation models are released.</p><p><strong>Conclusion</strong></p><p><strong>Privacy Concerns &amp; Societal Benefits</strong></p><p>The ability to reconstruct perception from brain activity offers many societal benefits, as well as certain risks, including privacy concerns, as noted below:&nbsp;</p><p><strong><em>Benefits</em></strong></p><ul data-rte-list="default"><li><p>Clinical applications could offer new diagnostic and assessment methods, as reconstructions are expected to be systematically distorted due to mental state or neurological conditions.</p></li><li><p>Current models trained on perception can potentially generalize to mental imagery, as similar patterns of brain activity are observed across perception and mental imagery of the same stimuli [3, 4].</p></li><li><p>Fine-grained visual communication facilitated by MindEye could potentially enhance communication with patients in a pseudocoma state beyond simple classification.</p></li><li><p>If adapted to real-time fMRI analysis [5] or non-fMRI neuroimaging modalities, MindEye could improve the performance of brain-computer interfaces.</p></li></ul><p><span><strong><br></strong></span><strong><em>Risks and Limitations</em></strong></p><ul data-rte-list="default"><li><p>Each participant in the dataset spent up to 40 hours in the MRI machine to gather sufficient training data.</p></li><li><p>Models were trained separately for every participant and are not generalizable across people.</p></li><li><p>Image limitations: MindEye is limited to the kinds of natural scenes used for training the model. For other image distributions, additional data collection and specialized generative models would be needed.</p></li><li><p>Data protection: Ensuring the protection of sensitive brain data and transparency from data-collecting companies is essential. MindEye used the <a href="https://naturalscenesdataset.org/" target="_blank"><span>Natural Scenes Dataset</span></a><a href="https://naturalscenesdataset.org/"><span> </span></a>where data collection was approved by the University of Minnesota institutional review board and where participants provided informed written consent to share their data.</p></li><li><p>Data contamination: Non-invasive neuroimaging methods like fMRI not only require participant compliance but also full concentration on following instructions during the lengthy scan process. Data become noisy or unusable if participants move their heads or fail to pay attention to the task.&nbsp;&nbsp;<br></p></li></ul><p>This adaptation to real-time fMRI, in combination with training foundation neuroimaging models, is actively being developed in MedARC and we invite interested readers to explore our <a href="https://medarc-ai.github.io/mind-reading" target="_blank"><span>ongoing projects</span></a> and join us on <a href="https://discord.com/invite/CqsMthnauZ" target="_blank"><span>Discord</span></a> as volunteer contributors.</p><p><strong>Open Research</strong></p><p>MindEye was developed using a 100% transparent volunteer-driven open research approach. The source code was accessible via a <a href="https://github.com/MedARC-AI/fMRI-reconstruction-NSD" target="_blank"><span>public GitHub repository</span></a> throughout the lifespan of the project. Research discussions were held via public Discord channels, and weekly video conference calls were recorded and shared publicly.</p><p>We want to establish an internationally diversified, volunteer-driven research team composed of members from varied backgrounds possessing a wide array of expertise. Fully transparent open-research initiatives such as this and others like EleutherAI, LAION, OpenBioML, and ML Collective could redefine the traditional framework of scientific research, democratizing entry into machine learning and medical research through the harnessing of crowd-sourced collective intelligence and community collaboration.</p><p><strong>Authors</strong></p><p>MindEye was developed by co-first authors Dr. Paul Scotti (Princeton Neuroscience Institute &amp; MedARC) and Atmadeep Banerjee (MedARC), with the support of joint senior authors Dr. Tanishq Abraham (MedARC CEO, Stability.AI) and Dr. Kenneth Norman (Princeton Neuroscience Institute). MindEye contributors also include Jimmie Goode (core contributor), Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, and David Weisberg.&nbsp;</p><p>For more information, visit our <a href="https://www.medarc.ai/mindeye" target="_blank"><span>project website</span></a> or email Dr. Paul Scotti at <a href="mailto:scottibrain@gmail.com"><span>scottibrain@gmail.com</span></a>.</p><p><strong>References</strong></p><ol data-rte-list="default"><li><p>Emily J. Allen, Ghislain St-Yves, Yihan Wu, Jesse L. Breedlove, Jacob S. Prince, Logan T. Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, J. Benjamin Hutchinson, Thomas Naselaris, and Kendrick Kay. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience, 25(1):116–126, January 2022. ISSN 1097-6256, 1546-1726. doi: 10.1038/s41593-021-00962-x. URL <a href="https://www.nature.com/articles/s41593-021-00962-x" target="_blank"><span>https://www.nature.com/articles/s41593-021-00962-x</span></a>.</p></li><li><p>Learning Transferable Visual Models From Natural Language Supervision. (2021). CLIP model card [Repository file]. GitHub. Retrieved June 20, 2023, from <a href="https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md" target="_blank"><span>https://github.com/openai/CLIP/blob/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1/model-card.md</span></a></p></li><li><p>Mark Stokes, Russell Thompson, Rhodri Cusack, and John Duncan. Top-Down Activation of Shape-Specific Population Codes in Visual Cortex during Mental Imagery. Journal of Neuroscience, 29(5): 1565–1572, February 2009. ISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.4657-08.2009. URL <a href="https://www.jneurosci.org/content/29/5/1565" target="_blank"><span>https://www.jneurosci.org/content/29/5/1565</span></a>. Publisher: Society for Neuroscience Section: Articles.</p></li><li><p>Leila Reddy, Naotsugu Tsuchiya, and Thomas Serre. Reading the mind’s eye: Decoding category information during mental imagery. NeuroImage, 50(2):818–825, April 2010. ISSN 1053-8119. doi: 10. 1016/j.neuroimage.2009.11.084. URL <a href="https://www.sciencedirect.com/science/article/pii/S1053811909012701" target="_blank"><span>https://www.sciencedirect.com/science/article/pii/S1053811909012701</span></a>.</p></li><li><p>Grant Wallace, Stephen Polcyn, Paula P. Brooks, Anne C. Mennen, Ke Zhao, Paul S. Scotti, Sebastian Michelmann, Kai Li, Nicholas B. Turk-Browne, Jonathan D. Cohen, and Kenneth A. Norman. RTCloud: A cloud-based software framework to simplify and standardize real-time fMRI. NeuroImage, 257:119295, August 2022. ISSN 10538119. doi: 10.1016/j.neuroimage.2022.119295. URL <a href="https://linkinghub.elsevier.com/retrieve/pii/S1053811922004141" target="_blank"><span>https://linkinghub.elsevier.com/retrieve/pii/S1053811922004141</span></a>.</p></li></ol>
</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Statistical Arbitrage – An Easy Walkthrough (134 pts)]]></title>
            <link>https://dm13450.github.io/2023/07/15/Stat-Arb-Walkthrough.html</link>
            <guid>36766556</guid>
            <pubDate>Tue, 18 Jul 2023 01:20:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dm13450.github.io/2023/07/15/Stat-Arb-Walkthrough.html">https://dm13450.github.io/2023/07/15/Stat-Arb-Walkthrough.html</a>, See on <a href="https://news.ycombinator.com/item?id=36766556">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        

<article itemscope="" itemtype="https://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Statistical arbitrage (stat arb) is a pillar of quantitate trading
that relies on mean reversion to predict the future returns of an
asset. Mean reversion believes that if a stock has risen higher it’s
more likely to revert in the short term which is the opposite of a
momentum strategy that believes if a stock has been rising it will
continue to rise. This blog post will walk you the ‘the’ statistical
arbitrage paper
<a href="https://math.nyu.edu/~avellane/AvellanedaLeeStatArb071108.pdf">Statistical Arbitrage in the US Equities Market</a>
apply it to a stock/ETF pair and then look at an intraday crypto stat
arb strategy.</p>


<hr>
<p>Enjoy these types of posts? Then you should sign up for my newsletter. It’s a short monthly recap of anything and everything I’ve found interesting recently plus
any posts I’ve written. So sign up and stay informed!</p>




<hr>


<p>I’m using Julia 1.9 and my <a href="https://github.com/dm13450/AlpacaMarkets.jl">AlpacaMarkets.jl</a> package gets all the data we
need.</p>

<div><pre><code><span>using</span> <span>AlpacaMarkets</span>
<span>using</span> <span>DataFrames</span><span>,</span> <span>DataFramesMeta</span>
<span>using</span> <span>Dates</span>
<span>using</span> <span>Plots</span>
<span>using</span> <span>RollingFunctions</span><span>,</span> <span>Statistics</span>
<span>using</span> <span>GLM</span>
</code></pre></div>

<p>To start with we simply want the daily prices of JPM, XLF, and SPY. JPM
is the stock we think will go through mean reversion, XLF is the
financial sector ETF and SPY is the general SPY ETF.</p>

<p>We this that if JPM rises higher than XLF then it will soon revert and
trade lower shortly. Likewise, if JPM falls lower than XLF
then we think it will soon trade higher. Our mean reversion is all
about JPM around XLF. We’ve chosen XLF as it represents the general
financial sector landscape, so will represent the general sector outlook
more consistently than JPM on its own.</p>

<div><pre><code><span>jpm</span> <span>=</span> <span>AlpacaMarkets</span><span>.</span><span>stock_bars</span><span>(</span><span>"JPM"</span><span>,</span> <span>"1Day"</span><span>;</span> <span>startTime</span> <span>=</span> <span>Date</span><span>(</span><span>"2017-01-01"</span><span>),</span> <span>limit</span> <span>=</span> <span>10000</span><span>,</span> <span>adjustment</span><span>=</span><span>"all"</span><span>)[</span><span>1</span><span>]</span>
<span>xlf</span> <span>=</span> <span>AlpacaMarkets</span><span>.</span><span>stock_bars</span><span>(</span><span>"XLF"</span><span>,</span> <span>"1Day"</span><span>;</span> <span>startTime</span> <span>=</span> <span>Date</span><span>(</span><span>"2017-01-01"</span><span>),</span> <span>limit</span> <span>=</span> <span>10000</span><span>,</span> <span>adjustment</span><span>=</span><span>"all"</span><span>)[</span><span>1</span><span>];</span>
<span>spy</span> <span>=</span> <span>AlpacaMarkets</span><span>.</span><span>stock_bars</span><span>(</span><span>"SPY"</span><span>,</span> <span>"1Day"</span><span>;</span> <span>startTime</span> <span>=</span> <span>Date</span><span>(</span><span>"2017-01-01"</span><span>),</span> <span>limit</span> <span>=</span> <span>10000</span><span>,</span> <span>adjustment</span><span>=</span><span>"all"</span><span>)[</span><span>1</span><span>];</span>
</code></pre></div>

<p>We want to clean the data to format the date correctly and select the
close and open columns.</p>

<div><pre><code><span>function</span><span> parse_date</span><span>(</span><span>t</span><span>)</span>
   <span>Date</span><span>(</span><span>string</span><span>(((</span><span>split</span><span>(</span><span>t</span><span>,</span> <span>"T"</span><span>)))[</span><span>1</span><span>]))</span>
<span>end</span>

<span>function</span><span> clean</span><span>(</span><span>df</span><span>,</span> <span>x</span><span>)</span> 
    <span>df</span> <span>=</span> <span>@transform</span><span>(</span><span>df</span><span>,</span> <span>:</span><span>Date</span> <span>=</span> <span>parse_date</span><span>.</span><span>(</span><span>:</span><span>t</span><span>),</span> <span>:</span><span>Ticker</span> <span>=</span> <span>x</span><span>,</span> <span>:</span><span>NextOpen</span> <span>=</span> <span>[</span><span>:</span><span>o</span><span>[</span><span>2</span><span>:</span><span>end</span><span>];</span> <span>NaN</span><span>])</span>
   <span>@select</span><span>(</span><span>df</span><span>,</span> <span>:</span><span>Date</span><span>,</span> <span>:</span><span>c</span><span>,</span> <span>:</span><span>o</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>NextOpen</span><span>)</span>
<span>end</span>
</code></pre></div>

<p>Now we calculate the close-to-close log returns and format the data
into a column for each asset.</p>

<div><pre><code><span>jpm</span> <span>=</span> <span>clean</span><span>(</span><span>jpm</span><span>,</span> <span>"JPM"</span><span>)</span>
<span>xlf</span> <span>=</span> <span>clean</span><span>(</span><span>xlf</span><span>,</span> <span>"XLF"</span><span>)</span>
<span>spy</span> <span>=</span> <span>clean</span><span>(</span><span>spy</span><span>,</span> <span>"SPY"</span><span>)</span>
<span>allPrices</span> <span>=</span> <span>vcat</span><span>(</span><span>jpm</span><span>,</span> <span>xlf</span><span>,</span> <span>spy</span><span>)</span>
<span>allPrices</span> <span>=</span> <span>sort</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>Date</span><span>)</span>

<span>allPrices</span> <span>=</span> <span>@transform</span><span>(</span><span>groupby</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>Ticker</span><span>),</span> 
                      <span>:</span><span>Return</span> <span>=</span> <span>[</span><span>NaN</span><span>;</span> <span>diff</span><span>(</span><span>log</span><span>.</span><span>(</span><span>:</span><span>c</span><span>))],</span> 
                      <span>:</span><span>ReturnO</span> <span>=</span> <span>[</span><span>NaN</span><span>;</span> <span>diff</span><span>(</span><span>log</span><span>.</span><span>(</span><span>:</span><span>o</span><span>))],</span>
                      <span>:</span><span>ReturnTC</span> <span>=</span> <span>[</span><span>NaN</span><span>;</span> <span>diff</span><span>(</span><span>log</span><span>.</span><span>(</span><span>:</span><span>NextOpen</span><span>))]);</span>

<span>modelData</span> <span>=</span> <span>unstack</span><span>(</span><span>@select</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>Date</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>Return</span><span>),</span> <span>:</span><span>Date</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>Return</span><span>)</span>
<span>modelData</span> <span>=</span> <span>modelData</span><span>[</span><span>2</span><span>:</span><span>end</span><span>,</span> <span>:</span><span>];</span>

<span>last</span><span>(</span><span>modelData</span><span>,</span> <span>4</span><span>)</span>
</code></pre></div>

<div><p>4 rows × 4 columns</p><table><thead><tr><th></th><th>Date</th><th>JPM</th><th>XLF</th><th>SPY</th></tr><tr><th></th><th title="Date">Date</th><th title="Union{Missing, Float64}">Float64?</th><th title="Union{Missing, Float64}">Float64?</th><th title="Union{Missing, Float64}">Float64?</th></tr></thead><tbody><tr><th>1</th><td>2023-06-30</td><td>0.0138731</td><td>0.00864001</td><td>0.0117316</td></tr><tr><th>2</th><td>2023-07-03</td><td>0.00799894</td><td>0.00562049</td><td>0.00114985</td></tr><tr><th>3</th><td>2023-07-05</td><td>-0.00661524</td><td>-0.00206703</td><td>-0.0014883</td></tr><tr><th>4</th><td>2023-07-06</td><td>-0.00993581</td><td>-0.00860923</td><td>-0.00786148</td></tr></tbody></table></div>

<p>Looking at the actual returns we can see that all three move in sync</p>

<div><pre><code><span>plot</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>modelData</span><span>.</span><span>JPM</span><span>),</span> <span>label</span> <span>=</span> <span>"JPM"</span><span>)</span>
<span>plot!</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>modelData</span><span>.</span><span>XLF</span><span>),</span> <span>label</span> <span>=</span> <span>"XLF"</span><span>)</span>
<span>plot!</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>modelData</span><span>.</span><span>SPY</span><span>),</span> <span>label</span> <span>=</span> <span>"SPY"</span><span>,</span> <span>legend</span> <span>=</span> <span>:</span><span>left</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_returns.png" alt="Stock returns"></p>

<p>The key point is that they are moving in sync with each other. Given XLF has JPM included in it, this is expected but it also presents the opportunity to trade around any dispersion between the ETF and the individual name.</p>

<h2 id="the-stat-arb-modelling-process">The Stat Arb Modelling Process</h2>

<ul>
  <li>https://math.stackexchange.com/questions/345773/how-the-ornstein-uhlenbeck-process-can-be-considered-as-the-continuous-time-anal</li>
</ul>

<p>Let’s think simply about pairs trading. We have two securities that we want to trade if their prices change too much, so our variable of interest is</p><p>

\[e = P_1 - P_2\]

</p><p>and we will enter a trade if \(e\) becomes large enough in both the positive and negative directions.</p>

<p>To translate that into a statistical problem we have two steps.</p>
<ol>
  <li>Work out the difference between the two securities</li>
  <li>Model how the difference changes over time.</li>
</ol>

<p>Step 1 is a simple regression of the stock vs the ETF we are trading against. Step 2 needs a bit more thought, but is still only a simple regression.</p>

<h3 id="the-macro-regression---stock-vs-etf">The Macro Regression - Stock vs ETF</h3>

<p>In our data, we have the daily returns of JPM, the XLF ETF, and the SPY ETF. To work out the interdependence, it’s just a case of simple linear regression.</p>

<div><pre><code><span>regModel</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>JPM</span> <span>~</span> <span>XLF</span> <span>+</span> <span>SPY</span><span>),</span> <span>modelData</span><span>)</span>
</code></pre></div>

<div><pre><code>JPM ~ 1 + XLF + SPY

Coefficients:
──────────────────────────────────────────────────────────────────────────────────
                    Coef.   Std. Error       t  Pr(&gt;|t|)   Lower 95%     Upper 95%
──────────────────────────────────────────────────────────────────────────────────
(Intercept)   0.000188758  0.000162973    1.16    0.2469  -0.0001309   0.000508417
XLF           1.35986      0.0203485     66.83    &lt;1e-99   1.31995     1.39977
SPY          -0.363187     0.0260825    -13.92    &lt;1e-41  -0.414345   -0.312028
──────────────────────────────────────────────────────────────────────────────────
</code></pre></div>

<p>From the slope of the model, we can see that JPM = 1.36XLF - 0.36SPY,
so JPM has a \(\beta\) of 1.36 to the XLF index and a \(\beta\) of
-0.36 to the SPY ETF, or general market. So each day, we can
approximate JPMs return by multiplying the XLF returns and SPY
returns.</p>

<p>This is our economic factor model, which describes from a
‘big picture’ kind of way how the stock trades vs the general market (SPY)
and its sector-specific market (XLF).</p>

<p>What we need to do next is look at what this model <em>doesn’t</em> explain
and try and describe that.</p>

<h3 id="the-reversion-regression">The Reversion Regression</h3>

<p>Any difference around this model can be explained by the summation of
the residuals over time. In the paper the sum of the residuals
over time is called the ‘auxiliary process’ and this is the data behind
the second regression.</p>

<div><pre><code><span>plot</span><span>(</span><span>scatter</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span> <span>residuals</span><span>(</span><span>regModel</span><span>),</span> <span>label</span> <span>=</span> <span>"Residuals"</span><span>),</span>
       <span>plot</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span><span>cumsum</span><span>(</span><span>residuals</span><span>(</span><span>regModel</span><span>)),</span>
       <span>label</span> <span>=</span> <span>"Aux Process"</span><span>),</span>
	  <span>layout</span> <span>=</span> <span>(</span><span>2</span><span>,</span><span>1</span><span>))</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/aux_process.png" alt="Auxiliary process" title="Auxiliary
 process"></p>

<p>We believe the auxiliary process (cumulative sum of the residuals)
can be modeled using a
<a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">Ornstein-Uhlenbeck</a>
(OU) process.</p>

<p>An OU process is a type of differential equation that displays mean
reversion behaviour. If the process falls away from its average level
then it will be forced back.</p><p>

\[dX = \kappa (m - X(t))dt + \sigma \mathrm{d} W\]

</p><p>\(\kappa\) represents how quickly the mean reversion occurs.</p>

<p>To fit this type of process we need to recognise that the above differential form of an OU process can be discretised to become a simple AR(1) model where the model parameters can be transformed to get the OU parameters.</p>

<p>We now fit the OU process onto the cumulative sum of the residuals from the first model. If the residuals have some sort of structure/pattern then this means our original model was missing some variable that explains the difference.</p>

<div><pre><code><span>X</span> <span>=</span> <span>cumsum</span><span>(</span><span>residuals</span><span>(</span><span>regModel</span><span>))</span>
<span>xDF</span> <span>=</span> <span>DataFrame</span><span>(</span><span>y</span><span>=</span><span>X</span><span>[</span><span>2</span><span>:</span><span>end</span><span>],</span> <span>x</span> <span>=</span> <span>X</span><span>[</span><span>1</span><span>:</span><span>end</span><span>-</span><span>1</span><span>])</span>
<span>arModel</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>y</span><span>~</span><span>x</span><span>),</span> <span>xDF</span><span>)</span>
</code></pre></div>

<div><pre><code>y ~ 1 + x

Coefficients:
─────────────────────────────────────────────────────────────────────────────────
                  Coef.   Std. Error       t  Pr(&gt;|t|)     Lower 95%    Upper 95%
─────────────────────────────────────────────────────────────────────────────────
(Intercept)  4.41618e-6  0.000162655    0.03    0.9783  -0.000314618  0.000323451
x            0.997147    0.00186733   534.00    &lt;1e-99   0.993484     1.00081
─────────────────────────────────────────────────────────────────────────────────
</code></pre></div>

<p>We take these coefficients and transform them into the parameters from the paper.</p>

<div><pre><code><span>varEta</span> <span>=</span> <span>var</span><span>(</span><span>residuals</span><span>(</span><span>arModel</span><span>))</span>
<span>a</span><span>,</span> <span>b</span> <span>=</span> <span>coef</span><span>(</span><span>arModel</span><span>)</span>
<span>k</span> <span>=</span> <span>-</span><span>log</span><span>(</span><span>b</span><span>)</span><span>*</span><span>252</span>
<span>m</span> <span>=</span> <span>a</span><span>/</span><span>(</span><span>1</span><span>-</span><span>b</span><span>)</span>
<span>sigma</span> <span>=</span> <span>sqrt</span><span>((</span><span>varEta</span> <span>*</span> <span>2</span> <span>*</span> <span>k</span><span>)</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
<span>sigma_eq</span> <span>=</span> <span>sqrt</span><span>(</span><span>varEta</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
<span>[</span><span>m</span><span>,</span> <span>sigma_eq</span><span>]</span>
</code></pre></div>

<div><pre><code>2-element Vector{Float64}:
 0.0015477568390823153
 0.08709971423424319
</code></pre></div>

<p>So \(m\) gives us the average level and \(\sigma_{\text{eq}}\) the
appropriate scale.</p>

<p>Now to build the mean reversion signal. We still have \(X\) as our
auxiliary process which we believe is mean reverting. We now have the
estimated parameters on the scale of this mean reversion so we can
transform the auxiliary process by these parameters and use this to see when the process is higher or lower than the model suggests it should be.</p>

<div><pre><code><span>modelData</span><span>.</span><span>Score</span> <span>=</span> <span>(</span><span>X</span> <span>.-</span> <span>m</span><span>)</span><span>./</span><span>sigma_eq</span><span>;</span>

<span>plot</span><span>(</span><span>modelData</span><span>.</span><span>Date</span><span>,</span> <span>modelData</span><span>.</span><span>Score</span><span>,</span> <span>label</span> <span>=</span> <span>"s"</span><span>)</span>
<span>hline!</span><span>([</span><span>-</span><span>1.25</span><span>],</span> <span>label</span> <span>=</span> <span>"Long JPM, Short XLF"</span><span>,</span> <span>color</span> <span>=</span> <span>"red"</span><span>)</span>
<span>hline!</span><span>([</span><span>-</span><span>0.5</span><span>],</span> <span>label</span> <span>=</span> <span>"Close Long Position"</span><span>,</span> <span>color</span> <span>=</span> <span>"red"</span><span>,</span> <span>ls</span><span>=:</span><span>dash</span><span>)</span>

<span>hline!</span><span>([</span><span>1.25</span><span>],</span> <span>label</span> <span>=</span> <span>"Short JPM, Long XLF"</span><span>,</span> <span>color</span> <span>=</span> <span>"purple"</span><span>)</span>
<span>hline!</span><span>([</span><span>0.75</span><span>],</span> <span>label</span> <span>=</span> <span>"Close Short Position"</span><span>,</span> <span>color</span> <span>=</span> <span>"purple"</span><span>,</span> <span>ls</span> <span>=</span> <span>:</span><span>dash</span><span>,</span> <span>legend</span><span>=:</span><span>topleft</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_signal.png" alt="Stock signal" title="Stock signal"></p>

<p>The red lines indicate when JPM has diverged from XLF on the negative side, i.e. we expect JPM to move higher and XLF to move lower. We enter the position if s &lt; -1.25 (solid red line) and exit the position when s &gt; -0.5 (dashed red line).</p>

<ul>
  <li>Buy to open if \(s &lt; -s_{bo}\) (&lt; -1.25) Buy 1 JPM, sell Beta XLF</li>
  <li>Close long if \(s &gt; -s_{c}\) (-0.5)</li>
</ul>

<p>The purple line is the same but in the opposite direction.</p>

<ul>
  <li>Sell to open if \(s &gt; s_{so}\) (&gt;1.25) Sell 1 JPM, buy Beta XLF</li>
  <li>Close short if \(s &lt; s_{bc}\) (&lt;0.75)</li>
</ul>

<p>That’s the modeling part done. We model how the stock moves based on
the overall market and then any differences to this we use the OU
process to come up with the mean reversion parameters.</p>

<p>So, does it make money?</p>

<h2 id="backtesting-the-stat-arb-strategy">Backtesting the Stat Arb Strategy</h2>

<p>To backtest this type of model we have to roll through time and
calculate both regressions to construct the signal.</p>

<p>A couple of new additions too</p>

<ul>
  <li>We shift and scale the returns when doing the macro regression.</li>
  <li>The auxiliary process on the last day is always 0, which makes
calculating the signal simple.</li>
</ul>

<div><pre><code><span>paramsRes</span> <span>=</span> <span>Array</span><span>{</span><span>DataFrame</span><span>}(</span><span>undef</span><span>,</span> <span>length</span><span>(</span><span>90</span><span>:</span><span>(</span><span>nrow</span><span>(</span><span>modelData</span><span>)</span> <span>-</span> <span>90</span><span>)))</span>

<span>for</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>90</span><span>:</span><span>(</span><span>nrow</span><span>(</span><span>modelData</span><span>)</span> <span>-</span> <span>90</span><span>))</span>
    <span>modelDataSub</span> <span>=</span> <span>modelData</span><span>[</span><span>i</span><span>:</span><span>(</span><span>i</span><span>+</span><span>90</span><span>),</span> <span>:</span><span>]</span>
    <span>modelDataSub</span><span>.</span><span>JPM</span> <span>=</span> <span>(</span><span>modelDataSub</span><span>.</span><span>JPM</span> <span>.-</span> <span>mean</span><span>(</span><span>modelDataSub</span><span>.</span><span>JPM</span><span>))</span> <span>./</span> <span>std</span><span>(</span><span>modelDataSub</span><span>.</span><span>JPM</span><span>)</span>
    <span>modelDataSub</span><span>.</span><span>XLF</span> <span>=</span> <span>(</span><span>modelDataSub</span><span>.</span><span>XLF</span> <span>.-</span> <span>mean</span><span>(</span><span>modelDataSub</span><span>.</span><span>XLF</span><span>))</span> <span>./</span> <span>std</span><span>(</span><span>modelDataSub</span><span>.</span><span>XLF</span><span>)</span>
    <span>modelDataSub</span><span>.</span><span>SPY</span> <span>=</span> <span>(</span><span>modelDataSub</span><span>.</span><span>SPY</span> <span>.-</span> <span>mean</span><span>(</span><span>modelDataSub</span><span>.</span><span>SPY</span><span>))</span> <span>./</span> <span>std</span><span>(</span><span>modelDataSub</span><span>.</span><span>SPY</span><span>)</span>
    
    <span>macroRegr</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>JPM</span> <span>~</span> <span>XLF</span> <span>+</span> <span>SPY</span><span>),</span> <span>modelDataSub</span><span>)</span>
    <span>auxData</span> <span>=</span> <span>cumsum</span><span>(</span><span>residuals</span><span>(</span><span>macroRegr</span><span>))</span>
    <span>ouRegr</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>y</span><span>~</span><span>x</span><span>),</span> <span>DataFrame</span><span>(</span><span>x</span><span>=</span><span>auxData</span><span>[</span><span>1</span><span>:</span><span>end</span><span>-</span><span>1</span><span>],</span> <span>y</span><span>=</span><span>auxData</span><span>[</span><span>2</span><span>:</span><span>end</span><span>]))</span>
    
    <span>varEta</span> <span>=</span> <span>var</span><span>(</span><span>residuals</span><span>(</span><span>ouRegr</span><span>))</span>
    <span>a</span><span>,</span> <span>b</span> <span>=</span> <span>coef</span><span>(</span><span>ouRegr</span><span>)</span>
    <span>k</span> <span>=</span> <span>-</span><span>log</span><span>(</span><span>b</span><span>)</span><span>*</span><span>252</span>
    <span>m</span> <span>=</span> <span>a</span><span>/</span><span>(</span><span>1</span><span>-</span><span>b</span><span>)</span>
    <span>sigma</span> <span>=</span> <span>sqrt</span><span>((</span><span>varEta</span> <span>*</span> <span>2</span> <span>*</span> <span>k</span><span>)</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
    <span>sigma_eq</span> <span>=</span> <span>sqrt</span><span>(</span><span>varEta</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
    
    
    <span>paramsRes</span><span>[</span><span>j</span><span>]</span> <span>=</span> <span>DataFrame</span><span>(</span><span>Date</span><span>=</span> <span>modelDataSub</span><span>.</span><span>Date</span><span>[</span><span>end</span><span>],</span> 
                             <span>MacroBeta_XLF</span> <span>=</span> <span>coef</span><span>(</span><span>macroRegr</span><span>)[</span><span>2</span><span>],</span> <span>MacroBeta_SPY</span> <span>=</span> <span>coef</span><span>(</span><span>macroRegr</span><span>)[</span><span>3</span><span>],</span> <span>MacroAlpha</span> <span>=</span> <span>coef</span><span>(</span><span>macroRegr</span><span>)[</span><span>1</span><span>],</span>
                             <span>VarEta</span> <span>=</span> <span>varEta</span><span>,</span> <span>OUA</span> <span>=</span> <span>a</span><span>,</span> <span>OUB</span> <span>=</span> <span>b</span><span>,</span> <span>OUK</span> <span>=</span> <span>k</span><span>,</span> <span>Sigma</span> <span>=</span> <span>sigma</span><span>,</span> <span>SigmaEQ</span><span>=</span><span>sigma_eq</span><span>,</span>
                             <span>Score</span> <span>=</span> <span>-</span><span>m</span><span>/</span><span>sigma_eq</span><span>)</span>
    
<span>end</span>

<span>paramsRes</span> <span>=</span> <span>vcat</span><span>(</span><span>paramsRes</span><span>...</span><span>)</span>
<span>last</span><span>(</span><span>paramsRes</span><span>,</span> <span>4</span><span>)</span>
</code></pre></div>

<div><p>4 rows × 11 columns (omitted printing of 4 columns)</p><table><thead><tr><th></th><th>Date</th><th>MacroBeta_XLF</th><th>MacroBeta_SPY</th><th>MacroAlpha</th><th>VarEta</th><th>OUA</th><th>OUB</th></tr><tr><th></th><th title="Date">Date</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>2023-06-30</td><td>0.974615</td><td>-0.230273</td><td>1.10933e-17</td><td>0.331745</td><td>0.175358</td><td>0.830417</td></tr><tr><th>2</th><td>2023-07-03</td><td>0.96943</td><td>-0.228741</td><td>-5.73883e-17</td><td>0.331222</td><td>0.198176</td><td>0.826816</td></tr><tr><th>3</th><td>2023-07-05</td><td>0.971319</td><td>-0.230438</td><td>2.38846e-17</td><td>0.335844</td><td>0.242754</td><td>0.841018</td></tr><tr><th>4</th><td>2023-07-06</td><td>0.974721</td><td>-0.232765</td><td>5.09875e-17</td><td>0.331695</td><td>0.256579</td><td>0.823822</td></tr></tbody></table></div>

<p>The benefit of doing it this way also means we can see how each
\(\beta\) in the macro regression evolves.</p>

<div><pre><code><span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>paramsRes</span><span>.</span><span>MacroBeta_XLF</span><span>,</span> <span>label</span> <span>=</span> <span>"XLF Beta"</span><span>)</span>
<span>plot!</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>paramsRes</span><span>.</span><span>MacroBeta_SPY</span><span>,</span> <span>label</span> <span>=</span> <span>"SPY Beta"</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_betas.png" alt="Stock betas" title="Stock betas"></p>

<p>Good to see they are consistent in their signs and generally don’t
vary a great deal.</p>

<p>In the OU process, we are also interested in the speed of the mean
reversion as we don’t want to take a position that is very slow to
revert to the mean level.</p>

<div><pre><code><span>kplot</span> <span>=</span> <span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>paramsRes</span><span>.</span><span>OUK</span><span>,</span> <span>label</span> <span>=</span> <span>:</span><span>none</span><span>)</span>
<span>kplot</span> <span>=</span> <span>hline!</span><span>([</span><span>252</span><span>/</span><span>45</span><span>],</span> <span>label</span> <span>=</span> <span>"K Threshold"</span><span>)</span>
</code></pre></div>

<p>In the paper, they suggest making sure the reversion happens with half
of the estimation period. As we are using 90 days, that means the
horizontal line shows when \(k\) is above this value.</p>

<p><img src="https://dm13450.github.io/assets/statarb/stock_k.png" alt="svg"></p>

<p>Plotting the score function also shows how the model wants to go
long/short the different components over time.</p>

<div><pre><code><span>splot</span> <span>=</span> <span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>paramsRes</span><span>.</span><span>Score</span><span>,</span> <span>label</span> <span>=</span> <span>"Score"</span><span>)</span>
<span>hline!</span><span>([</span><span>-</span><span>1.25</span><span>],</span> <span>label</span> <span>=</span> <span>"Long JPM, Short XLF"</span><span>,</span> <span>color</span> <span>=</span> <span>"red"</span><span>)</span>
<span>hline!</span><span>([</span><span>-</span><span>0.5</span><span>],</span> <span>label</span> <span>=</span> <span>"Close Long Position"</span><span>,</span> <span>color</span> <span>=</span> <span>"red"</span><span>,</span> <span>ls</span><span>=:</span><span>dash</span><span>)</span>

<span>hline!</span><span>([</span><span>1.25</span><span>],</span> <span>label</span> <span>=</span> <span>"Short JPM, Long XLF"</span><span>,</span> <span>color</span> <span>=</span> <span>"purple"</span><span>)</span>
<span>hline!</span><span>([</span><span>0.75</span><span>],</span> <span>label</span> <span>=</span> <span>"Close Short Position"</span><span>,</span> <span>color</span> <span>=</span> <span>"purple"</span><span>,</span> <span>ls</span> <span>=</span> <span>:</span><span>dash</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_position.png" alt="Stock position" title="Stock position"></p>

<p>We run through the allocation procedure and label whether we are long
(+1) or short (-\(\beta\)) an amount of either the stock or ETFs.</p>

<div><pre><code><span>paramsRes</span><span>.</span><span>JPM_Pos</span> <span>.=</span> <span>0.0</span>
<span>paramsRes</span><span>.</span><span>XLF_Pos</span> <span>.=</span> <span>0.0</span>
<span>paramsRes</span><span>.</span><span>SPY_Pos</span> <span>.=</span> <span>0.0</span>

<span>for</span> <span>i</span> <span>in</span> <span>2</span><span>:</span><span>nrow</span><span>(</span><span>paramsRes</span><span>)</span>
    
    <span>if</span> <span>paramsRes</span><span>.</span><span>OUK</span><span>[</span><span>i</span><span>]</span> <span>&gt;</span> <span>252</span><span>/</span><span>45</span>
    
        <span>if</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&gt;=</span> <span>1.25</span>
            <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>XLF_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta_XLF</span><span>[</span><span>i</span><span>]</span>
            <span>paramsRes</span><span>.</span><span>SPY_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta_SPY</span><span>[</span><span>i</span><span>]</span>
        <span>elseif</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&gt;=</span> <span>0.75</span> <span>&amp;&amp;</span> <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>XLF_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta_XLF</span><span>[</span><span>i</span><span>]</span>    
            <span>paramsRes</span><span>.</span><span>SPY_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta_SPY</span><span>[</span><span>i</span><span>]</span>
        <span>end</span>

        <span>if</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&lt;=</span> <span>-</span><span>1.25</span>
            <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>XLF_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta_XLF</span><span>[</span><span>i</span><span>]</span>   
            <span>paramsRes</span><span>.</span><span>SPY_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta_SPY</span><span>[</span><span>i</span><span>]</span>
        <span>elseif</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&lt;=</span> <span>-</span><span>0.5</span> <span>&amp;&amp;</span> <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>XLF_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta_XLF</span><span>[</span><span>i</span><span>]</span> 
            <span>paramsRes</span><span>.</span><span>SPY_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta_SPY</span><span>[</span><span>i</span><span>]</span>
        <span>end</span>
    <span>end</span>
        
<span>end</span>
</code></pre></div>

<p>To make sure we use the right price return we lead the return columns
by one so that we enter the position and get the next return.</p>

<div><pre><code><span>modelData</span> <span>=</span> <span>@transform</span><span>(</span><span>modelData</span><span>,</span> <span>:</span><span>NextJPM</span><span>=</span> <span>lead</span><span>(</span><span>:</span><span>JPM</span><span>,</span> <span>1</span><span>),</span> 
                                   <span>:</span><span>NextXLF</span> <span>=</span> <span>lead</span><span>(</span><span>:</span><span>XLF</span><span>,</span> <span>1</span><span>),</span>
                                   <span>:</span><span>NextSPY</span> <span>=</span> <span>lead</span><span>(</span><span>:</span><span>SPY</span><span>,</span> <span>1</span><span>))</span>

<span>paramsRes</span> <span>=</span> <span>leftjoin</span><span>(</span><span>paramsRes</span><span>,</span> <span>modelData</span><span>[</span><span>:</span><span>,</span> <span>[</span><span>:</span><span>Date</span><span>,</span> <span>:</span><span>NextJPM</span><span>,</span> <span>:</span><span>NextXLF</span><span>,</span> <span>:</span><span>NextSPY</span><span>]],</span> <span>on</span><span>=:</span><span>Date</span><span>)</span>

<span>portRes</span> <span>=</span> <span>@combine</span><span>(</span><span>groupby</span><span>(</span><span>paramsRes</span><span>,</span> <span>:</span><span>Date</span><span>),</span> <span>:</span><span>Return</span> <span>=</span> <span>:</span><span>NextJPM</span> <span>.*</span> <span>:</span><span>JPM_Pos</span> <span>.+</span> <span>:</span><span>NextXLF</span> <span>.*</span> <span>:</span><span>XLF_Pos</span> <span>.+</span> <span>:</span><span>NextSPY</span> <span>.*</span> <span>:</span><span>SPY_Pos</span><span>);</span>

<span>plot</span><span>(</span><span>portRes</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>portRes</span><span>.</span><span>Return</span><span>),</span> <span>label</span> <span>=</span> <span>"Stat Arb Return"</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_portfolio_return.png" alt="Stock portfolio return" title="Stock portfolio return"></p>

<p>Sad trombone noise. This is not a great result as we’ve ended up
negative over the period. However, given the paper is 15 years old it
would be very rare to still be able to make money this way
after <em>everyone</em> knows how to do it. Plus, I’ve only used one stock vs
the ETF portfolio, you typically want to diversify out and use all the
stocks in the ETF to be long and short multiple single names and use
the ETF as a minimal hedge,</p>

<p>The good thing about it being a negative result means that we don’t have
to start considering transaction costs or other annoying things like
that.</p>

<p>When we break out the components of the strategy we can see that
it appears to pick out the right times to short/long JPM and
SPY, its the hedging with the XLF ETF that is bringing the portfolio
down.</p>

<div><pre><code><span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>paramsRes</span><span>.</span><span>NextJPM</span> <span>.*</span> <span>paramsRes</span><span>.</span><span>JPM_Pos</span><span>),</span> <span>label</span> <span>=</span> <span>"JPM Component"</span><span>)</span>
<span>plot!</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>paramsRes</span><span>.</span><span>NextXLF</span> <span>.*</span> <span>paramsRes</span><span>.</span><span>XLF_Pos</span><span>),</span> <span>label</span> <span>=</span> <span>"XLF Component"</span><span>)</span>
<span>plot!</span><span>(</span><span>paramsRes</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>paramsRes</span><span>.</span><span>NextSPY</span> <span>.*</span> <span>paramsRes</span><span>.</span><span>SPY_Pos</span><span>),</span> <span>label</span> <span>=</span> <span>"SPY Component"</span><span>)</span>
<span>plot!</span><span>(</span><span>portRes</span><span>.</span><span>Date</span><span>,</span> <span>cumsum</span><span>(</span><span>portRes</span><span>.</span><span>Return</span><span>),</span> <span>label</span> <span>=</span> <span>"Stat Arb Portfolio"</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/stock_components.png" alt="Stock portfolio components" title="Stock portfolio components"></p>

<p>So whilst naively trying to trade the stat arb portfolio is probably
a loss maker, there might be some value in using the model as a signal
input or overlay to another strategy.</p>

<p>What about if we up the frequency and look at intraday stat arb?</p>

<h2 id="intraday-stat-arb-in-crypto---eth-and-btc">Intraday Stat Arb in Crypto - ETH and BTC</h2>

<p>Crypto markets are open 24 hours a day 7 days a week and so gives that
much more opportunity to build out a continuous trading model. We look
back since the last year and repeat the backtesting process to see if
this bares any fruit.</p>

<p>Once again AlpacaMarkets gives us an easy way to pull the hourly bar
data for both ETH and BTC.</p>

<div><pre><code><span>btcRaw</span> <span>=</span> <span>AlpacaMarkets</span><span>.</span><span>crypto_bars</span><span>(</span><span>"BTC/USD"</span><span>,</span> <span>"1Hour"</span><span>;</span> <span>startTime</span> <span>=</span> <span>now</span><span>()</span> <span>-</span> <span>Year</span><span>(</span><span>1</span><span>),</span> <span>limit</span> <span>=</span> <span>10000</span><span>)[</span><span>1</span><span>]</span>
<span>ethRaw</span> <span>=</span> <span>AlpacaMarkets</span><span>.</span><span>crypto_bars</span><span>(</span><span>"ETH/USD"</span><span>,</span> <span>"1Hour"</span><span>;</span> <span>startTime</span> <span>=</span> <span>now</span><span>()</span> <span>-</span> <span>Year</span><span>(</span><span>1</span><span>),</span> <span>limit</span> <span>=</span> <span>10000</span><span>)[</span><span>1</span><span>];</span>

<span>btc</span> <span>=</span> <span>@transform</span><span>(</span><span>btcRaw</span><span>,</span> <span>:</span><span>ts</span> <span>=</span> <span>DateTime</span><span>.</span><span>(</span><span>chop</span><span>.</span><span>(</span><span>:</span><span>t</span><span>)),</span> <span>:</span><span>Ticker</span> <span>=</span> <span>"BTC"</span><span>)</span>
<span>eth</span> <span>=</span> <span>@transform</span><span>(</span><span>ethRaw</span><span>,</span> <span>:</span><span>ts</span> <span>=</span> <span>DateTime</span><span>.</span><span>(</span><span>chop</span><span>.</span><span>(</span><span>:</span><span>t</span><span>)),</span> <span>:</span><span>Ticker</span> <span>=</span> <span>"ETH"</span><span>)</span>

<span>btc</span> <span>=</span> <span>btc</span><span>[</span><span>:</span><span>,</span> <span>[</span><span>:</span><span>ts</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>c</span><span>]]</span>
<span>eth</span> <span>=</span> <span>eth</span><span>[</span><span>:</span><span>,</span> <span>[</span><span>:</span><span>ts</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>c</span><span>]]</span>

<span>allPrices</span> <span>=</span> <span>vcat</span><span>(</span><span>btc</span><span>,</span> <span>eth</span><span>)</span>
<span>allPrices</span> <span>=</span> <span>sort</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>ts</span><span>)</span>

<span>allPrices</span> <span>=</span> <span>@transform</span><span>(</span><span>groupby</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>Ticker</span><span>),</span> 
                      <span>:</span><span>Return</span> <span>=</span> <span>[</span><span>NaN</span><span>;</span> <span>diff</span><span>(</span><span>log</span><span>.</span><span>(</span><span>:</span><span>c</span><span>))]);</span>

<span>modelData</span> <span>=</span> <span>unstack</span><span>(</span><span>@select</span><span>(</span><span>allPrices</span><span>,</span> <span>:</span><span>ts</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>Return</span><span>),</span> <span>:</span><span>ts</span><span>,</span> <span>:</span><span>Ticker</span><span>,</span> <span>:</span><span>Return</span><span>);</span>
<span>modelData</span> <span>=</span> <span>@subset</span><span>(</span><span>modelData</span><span>,</span> <span>.!</span> <span>isnan</span><span>.</span><span>(</span><span>:</span><span>ETH</span> <span>.+</span> <span>:</span><span>BTC</span><span>))</span>
</code></pre></div>

<p>Plotting out the returns we can see they are loosely related just like
the stock example.</p>

<div><pre><code><span>plot</span><span>(</span><span>modelData</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>modelData</span><span>.</span><span>BTC</span><span>),</span> <span>label</span> <span>=</span> <span>"BTC"</span><span>)</span>
<span>plot!</span><span>(</span><span>modelData</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>modelData</span><span>.</span><span>ETH</span><span>),</span> <span>label</span> <span>=</span> <span>"ETH"</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/crypto_returns.png" alt="Crypto returns" title="Crypto returns"></p>

<p>We will be using BTC as the ‘index’ and see how ETH is related.</p>

<div><pre><code><span>regModel</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>ETH</span> <span>~</span> <span>BTC</span><span>),</span> <span>modelData</span><span>)</span>
</code></pre></div>

<div><pre><code>ETH ~ 1 + BTC

Coefficients:
─────────────────────────────────────────────────────────────────────────────
                  Coef.  Std. Error       t  Pr(&gt;|t|)    Lower 95%  Upper 95%
─────────────────────────────────────────────────────────────────────────────
(Intercept)  7.72396e-6  3.64797e-5    0.21    0.8323  -6.37847e-5  7.92327e-5
BTC          1.115       0.00673766  165.49    &lt;1e-99   1.10179     1.12821
─────────────────────────────────────────────────────────────────────────────
</code></pre></div>

<p>Fairly high beta for ETH and against BTC.  We use a 90-hour rolling window now instead of a 90 day.</p>

<div><pre><code><span>window</span> <span>=</span> <span>90</span>

<span>paramsRes</span> <span>=</span> <span>Array</span><span>{</span><span>DataFrame</span><span>}(</span><span>undef</span><span>,</span> <span>length</span><span>(</span><span>window</span><span>:</span><span>(</span><span>nrow</span><span>(</span><span>modelData</span><span>)</span> <span>-</span> <span>window</span><span>)))</span>

<span>for</span> <span>(</span><span>j</span><span>,</span> <span>i</span><span>)</span> <span>in</span> <span>enumerate</span><span>(</span><span>window</span><span>:</span><span>(</span><span>nrow</span><span>(</span><span>modelData</span><span>)</span> <span>-</span> <span>window</span><span>))</span>
    <span>modelDataSub</span> <span>=</span> <span>modelData</span><span>[</span><span>i</span><span>:</span><span>(</span><span>i</span><span>+</span><span>window</span><span>),</span> <span>:</span><span>]</span>
    <span>modelDataSub</span><span>.</span><span>ETH</span> <span>=</span> <span>(</span><span>modelDataSub</span><span>.</span><span>ETH</span> <span>.-</span> <span>mean</span><span>(</span><span>modelDataSub</span><span>.</span><span>ETH</span><span>))</span> <span>./</span> <span>std</span><span>(</span><span>modelDataSub</span><span>.</span><span>ETH</span><span>)</span>
    <span>modelDataSub</span><span>.</span><span>BTC</span> <span>=</span> <span>(</span><span>modelDataSub</span><span>.</span><span>BTC</span> <span>.-</span> <span>mean</span><span>(</span><span>modelDataSub</span><span>.</span><span>BTC</span><span>))</span> <span>./</span> <span>std</span><span>(</span><span>modelDataSub</span><span>.</span><span>BTC</span><span>)</span>
    
    <span>macroRegr</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>ETH</span> <span>~</span> <span>BTC</span><span>),</span> <span>modelDataSub</span><span>)</span>
    <span>auxData</span> <span>=</span> <span>cumsum</span><span>(</span><span>residuals</span><span>(</span><span>macroRegr</span><span>))</span>
    <span>ouRegr</span> <span>=</span> <span>lm</span><span>(</span><span>@formula</span><span>(</span><span>y</span><span>~</span><span>x</span><span>),</span> <span>DataFrame</span><span>(</span><span>x</span><span>=</span><span>auxData</span><span>[</span><span>1</span><span>:</span><span>end</span><span>-</span><span>1</span><span>],</span> <span>y</span><span>=</span><span>auxData</span><span>[</span><span>2</span><span>:</span><span>end</span><span>]))</span>
    <span>varEta</span> <span>=</span> <span>var</span><span>(</span><span>residuals</span><span>(</span><span>ouRegr</span><span>))</span>
    <span>a</span><span>,</span> <span>b</span> <span>=</span> <span>coef</span><span>(</span><span>ouRegr</span><span>)</span>
    <span>k</span> <span>=</span> <span>-</span><span>log</span><span>(</span><span>b</span><span>)</span><span>/</span><span>((</span><span>1</span><span>/</span><span>24</span><span>)</span><span>/</span><span>252</span><span>)</span>
    <span>m</span> <span>=</span> <span>a</span><span>/</span><span>(</span><span>1</span><span>-</span><span>b</span><span>)</span>
    <span>sigma</span> <span>=</span> <span>sqrt</span><span>((</span><span>varEta</span> <span>*</span> <span>2</span> <span>*</span> <span>k</span><span>)</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
    <span>sigma_eq</span> <span>=</span> <span>sqrt</span><span>(</span><span>varEta</span> <span>/</span> <span>(</span><span>1</span><span>-</span><span>b</span><span>^</span><span>2</span><span>))</span>
    
    
    <span>paramsRes</span><span>[</span><span>j</span><span>]</span> <span>=</span> <span>DataFrame</span><span>(</span><span>ts</span><span>=</span> <span>modelDataSub</span><span>.</span><span>ts</span><span>[</span><span>end</span><span>],</span> <span>MacroBeta</span> <span>=</span> <span>coef</span><span>(</span><span>macroRegr</span><span>)[</span><span>2</span><span>],</span> <span>MacroAlpha</span> <span>=</span> <span>coef</span><span>(</span><span>macroRegr</span><span>)[</span><span>1</span><span>],</span>
                             <span>VarEta</span> <span>=</span> <span>varEta</span><span>,</span> <span>OUA</span> <span>=</span> <span>a</span><span>,</span> <span>OUB</span> <span>=</span> <span>b</span><span>,</span> <span>OUK</span> <span>=</span> <span>k</span><span>,</span> <span>Sigma</span> <span>=</span> <span>sigma</span><span>,</span> <span>SigmaEQ</span><span>=</span><span>sigma_eq</span><span>,</span>
                             <span>Score</span> <span>=</span> <span>-</span><span>m</span><span>/</span><span>sigma_eq</span><span>)</span>
    
<span>end</span>

<span>paramsRes</span> <span>=</span> <span>vcat</span><span>(</span><span>paramsRes</span><span>...</span><span>)</span>
</code></pre></div>

<p>Again, looking at \(\beta\) overtime we see there has been a sudden
shift</p>

<div><pre><code><span>plot</span><span>(</span><span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>ts</span><span>,</span> <span>paramsRes</span><span>.</span><span>MacroBeta</span><span>,</span> <span>label</span> <span>=</span> <span>"Macro Beta"</span><span>,</span> <span>legend</span> <span>=</span> <span>:</span><span>left</span><span>),</span> 
     <span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>ts</span><span>,</span> <span>paramsRes</span><span>.</span><span>OUK</span><span>,</span> <span>label</span> <span>=</span> <span>"K"</span><span>),</span> <span>layout</span> <span>=</span> <span>(</span><span>2</span><span>,</span><span>1</span><span>))</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/crypto_params.png" alt="Crypto params" title="Crypto params"></p>

<p>Interesting that there has been a big change in \(\beta\) between ETH and BTC
recently that has suddenly reverted. Ok, onto the backtesting again.</p>

<div><pre><code><span>paramsRes</span><span>.</span><span>ETH_Pos</span> <span>.=</span> <span>0.0</span>
<span>paramsRes</span><span>.</span><span>BTC_Pos</span> <span>.=</span> <span>0.0</span>

<span>for</span> <span>i</span> <span>in</span> <span>2</span><span>:</span><span>nrow</span><span>(</span><span>paramsRes</span><span>)</span>
    
    <span>if</span> <span>paramsRes</span><span>.</span><span>OUK</span><span>[</span><span>i</span><span>]</span> <span>&gt;</span> <span>(</span><span>252</span><span>/</span><span>(</span><span>1</span><span>/</span><span>24</span><span>)</span><span>/</span><span>45</span><span>)</span>
    
        <span>if</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&gt;=</span> <span>1.25</span>
            <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>BTC_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta</span><span>[</span><span>i</span><span>]</span>   
        <span>elseif</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&gt;=</span> <span>0.75</span> <span>&amp;&amp;</span> <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>1</span>
            <span>paramsRes</span><span>.</span><span>BTC_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>paramsRes</span><span>.</span><span>MacroBeta</span><span>[</span><span>i</span><span>]</span>     
        <span>end</span>

        <span>if</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&lt;=</span> <span>-</span><span>1.25</span>
            <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>BTC_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta</span><span>[</span><span>i</span><span>]</span>   
        <span>elseif</span> <span>paramsRes</span><span>.</span><span>Score</span><span>[</span><span>i</span><span>]</span> <span>&lt;=</span> <span>-</span><span>0.5</span> <span>&amp;&amp;</span> <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>-</span><span>1</span><span>]</span> <span>==</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>1</span>
            <span>paramsRes</span><span>.</span><span>BTC_Pos</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>-</span><span>paramsRes</span><span>.</span><span>MacroBeta</span><span>[</span><span>i</span><span>]</span>     
        <span>end</span>
    <span>end</span>
        
<span>end</span>


<span>modelData</span> <span>=</span> <span>@transform</span><span>(</span><span>modelData</span><span>,</span> <span>:</span><span>NextETH</span><span>=</span> <span>lead</span><span>(</span><span>:</span><span>ETH</span><span>,</span> <span>1</span><span>),</span> <span>:</span><span>NextBTC</span> <span>=</span> <span>lead</span><span>(</span><span>:</span><span>BTC</span><span>,</span> <span>1</span><span>))</span>

<span>paramsRes</span> <span>=</span> <span>leftjoin</span><span>(</span><span>paramsRes</span><span>,</span> <span>modelData</span><span>[</span><span>:</span><span>,</span> <span>[</span><span>:</span><span>ts</span><span>,</span> <span>:</span><span>NextETH</span><span>,</span> <span>:</span><span>NextBTC</span><span>]],</span> <span>on</span><span>=:</span><span>ts</span><span>)</span>

<span>portRes</span> <span>=</span> <span>@combine</span><span>(</span><span>groupby</span><span>(</span><span>paramsRes</span><span>,</span> <span>:</span><span>ts</span><span>),</span> <span>:</span><span>Return</span> <span>=</span> <span>:</span><span>NextETH</span> <span>.*</span> <span>:</span><span>ETH_Pos</span> <span>.+</span> <span>:</span><span>NextBTC</span> <span>.*</span> <span>:</span><span>BTC_Pos</span><span>);</span>

<span>plot</span><span>(</span><span>portRes</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>portRes</span><span>.</span><span>Return</span><span>))</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/crypto_portfolioreturn.png" alt="Crypto stat arb returns" title="Crypto stat arb
 returns"></p>

<p>This looks slightly better. At least it is positive at the end of the
testing period.</p>

<div><pre><code><span>plot</span><span>(</span><span>paramsRes</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>paramsRes</span><span>.</span><span>NextETH</span> <span>.*</span> <span>paramsRes</span><span>.</span><span>ETH_Pos</span><span>),</span> <span>label</span> <span>=</span> <span>"ETH Component"</span><span>)</span>
<span>plot!</span><span>(</span><span>paramsRes</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>paramsRes</span><span>.</span><span>NextBTC</span> <span>.*</span> <span>paramsRes</span><span>.</span><span>BTC_Pos</span><span>),</span> <span>label</span> <span>=</span> <span>"BTC Component"</span><span>)</span>
<span>plot!</span><span>(</span><span>portRes</span><span>.</span><span>ts</span><span>,</span> <span>cumsum</span><span>(</span><span>portRes</span><span>.</span><span>Return</span><span>),</span> <span>label</span> <span>=</span> <span>"Stat Arb Portfolio"</span><span>,</span> <span>legend</span><span>=:</span><span>topleft</span><span>)</span>
</code></pre></div>

<p><img src="https://dm13450.github.io/assets/statarb/crypto_components.png" alt="Crypto components" title="Crypto components"></p>

<p>Again, the components of the portfolio seem to be ok in the ETH case
but generally, this is from the overall long bias. Unlike the JPM/XLF
example, there isn’t much more diversification we can add anything that might
help. We could add in more crypto assets, or an equity/gold angle, but
it becomes more of an asset class arb than something truly
statistical.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The original paper is one of those that all quants get recommended to
read and statistical arbitrage is a concept that you probably
understand in theory but practically doing is another
question. Hopefully, this blog post gets you up to speed with the
basic concepts and how to implement them.
It can be boiled down to two steps.</p>

<ol>
  <li>Model as much as you can with a simple regression</li>
  <li>Model what’s left over as an OU process.</li>
</ol>

<p>It can work with both high-frequency and low-frequency data, so have a
look at different combinations or assets and see if you have more luck
then I did backtesting.</p>

<p>If you do end up seeing something positive, make sure you are
backtesting properly!</p>


  </div>

  
    

  
  
  


  

  
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[StableDiffusion can now run directly in the browser on WebGPU (433 pts)]]></title>
            <link>https://islamov.ai/stable-diffusion-webgpu/</link>
            <guid>36766523</guid>
            <pubDate>Tue, 18 Jul 2023 01:14:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://islamov.ai/stable-diffusion-webgpu/">https://islamov.ai/stable-diffusion-webgpu/</a>, See on <a href="https://news.ycombinator.com/item?id=36766523">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Strlcpy and strlcat added to glibc 2.38 (164 pts)]]></title>
            <link>https://sourceware.org/git/?p=glibc.git;a=commit;h=454a20c8756c9c1d55419153255fc7692b3d2199</link>
            <guid>36765747</guid>
            <pubDate>Mon, 17 Jul 2023 23:46:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sourceware.org/git/?p=glibc.git;a=commit;h=454a20c8756c9c1d55419153255fc7692b3d2199">https://sourceware.org/git/?p=glibc.git;a=commit;h=454a20c8756c9c1d55419153255fc7692b3d2199</a>, See on <a href="https://news.ycombinator.com/item?id=36765747">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<table>
<tbody><tr><td>author</td><td><a href="https://sourceware.org/git?p=glibc.git;a=search;h=454a20c8756c9c1d55419153255fc7692b3d2199;s=Florian+Weimer;st=author" title="Search for commits authored by Florian Weimer">Florian Weimer</a> <a href="https://sourceware.org/git?p=glibc.git;a=search;h=454a20c8756c9c1d55419153255fc7692b3d2199;s=fweimer@redhat.com;st=author" title="Search for commits authored by fweimer@redhat.com">&lt;fweimer@redhat.com&gt;</a></td><td rowspan="2"></td></tr>
<tr><td></td><td><span>Wed, 14 Jun 2023 16:10:08 +0000</span> (18:10 +0200)</td></tr>
<tr><td>committer</td><td><a href="https://sourceware.org/git?p=glibc.git;a=search;h=454a20c8756c9c1d55419153255fc7692b3d2199;s=Florian+Weimer;st=committer" title="Search for commits committed by Florian Weimer">Florian Weimer</a> <a href="https://sourceware.org/git?p=glibc.git;a=search;h=454a20c8756c9c1d55419153255fc7692b3d2199;s=fweimer@redhat.com;st=committer" title="Search for commits committed by fweimer@redhat.com">&lt;fweimer@redhat.com&gt;</a></td><td rowspan="2"></td></tr>
<tr><td></td><td><span>Wed, 14 Jun 2023 16:10:08 +0000</span> (18:10 +0200)</td></tr>
<tr><td>commit</td><td>454a20c8756c9c1d55419153255fc7692b3d2199</td></tr>
<tr><td>tree</td><td><a href="https://sourceware.org/git?p=glibc.git;a=tree;h=a65ad84288a247995183089f4400e4fd080ecc9d;hb=454a20c8756c9c1d55419153255fc7692b3d2199">a65ad84288a247995183089f4400e4fd080ecc9d</a></td><td><a href="https://sourceware.org/git?p=glibc.git;a=tree;h=a65ad84288a247995183089f4400e4fd080ecc9d;hb=454a20c8756c9c1d55419153255fc7692b3d2199">tree</a></td></tr>
<tr><td>parent</td><td><a href="https://sourceware.org/git?p=glibc.git;a=commit;h=7ba426a1115318fc11f4355f3161f35817a06ba4">7ba426a1115318fc11f4355f3161f35817a06ba4</a></td><td><a href="https://sourceware.org/git?p=glibc.git;a=commit;h=7ba426a1115318fc11f4355f3161f35817a06ba4">commit</a> | <a href="https://sourceware.org/git?p=glibc.git;a=commitdiff;h=454a20c8756c9c1d55419153255fc7692b3d2199;hp=7ba426a1115318fc11f4355f3161f35817a06ba4">diff</a></td></tr>
</tbody></table></div><div><p>
Implement&nbsp;strlcpy&nbsp;and&nbsp;strlcat&nbsp;[BZ&nbsp;#178]</p><p>

These&nbsp;functions&nbsp;are&nbsp;about&nbsp;to&nbsp;be&nbsp;added&nbsp;to&nbsp;POSIX,&nbsp;under&nbsp;Austin&nbsp;Group<br>
issue&nbsp;986.</p><p>

The&nbsp;fortified&nbsp;strlcat&nbsp;implementation&nbsp;does&nbsp;not&nbsp;raise&nbsp;SIGABRT&nbsp;if&nbsp;the<br>
destination&nbsp;buffer&nbsp;does&nbsp;not&nbsp;contain&nbsp;a&nbsp;null&nbsp;terminator,&nbsp;it&nbsp;just<br>
inherits&nbsp;the&nbsp;non-failing&nbsp;regular&nbsp;strlcat&nbsp;behavior.</p><p>

<span>Reviewed-by: Siddhesh Poyarekar &lt;siddhesh@sourceware.org&gt;</span></p></div><p>
This page took <span id="generating_time">0.052476 seconds </span> and <span id="generating_cmd">5</span> git commands  to generate.
</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Loss of smell may be an early sign of brain diseases (182 pts)]]></title>
            <link>https://nautil.us/loss-of-smell-may-be-an-early-sign-of-brain-diseases-354483/</link>
            <guid>36765558</guid>
            <pubDate>Mon, 17 Jul 2023 23:29:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://nautil.us/loss-of-smell-may-be-an-early-sign-of-brain-diseases-354483/">https://nautil.us/loss-of-smell-may-be-an-early-sign-of-brain-diseases-354483/</a>, See on <a href="https://news.ycombinator.com/item?id=36765558">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
              <!-- collection-box -->
              <p><span>I</span>t’s summer in San Diego, California, where I live. When I walk down the street, I can smell the delirious musk of jasmine clinging to the air<strong>.</strong>  My nose picks up the languid smokiness of briquets burning on barbecues, taking me back to decades of yard parties. But given the heat wave descending on the city, the garbage is also throwing off a sickly stench, hinting at decay. These odors are all good signs.&nbsp;</p><p>We have long known that smell has potent connections to memory and emotion, as the endurance of Proust’s proverbial madeleine moment attests. In fact, the limbic system, including the amygdala and the hippocampus, is responsible for processing not just memory and emotion, but also smell. While Western culture has generally <a aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8946147/#:~:text=The%20American%20Medical%20Association%20Guides,sense%20of%20smell%20much%20value." target="_blank" rel="noreferrer noopener">relegated</a> the sense of smell to the lowest rank since the time of Plato, some hunter-gatherer groups such as the Semaq Beri of the Malay peninsula have more words for smells than for color hues. Now, researchers are learning that the sense of smell is critical as a marker of brain disease and health.</p><p>Over the past decade, loss of smell—known as hyposmia—has emerged as an early indicator of several neurodegenerative disorders, according to a recent <em>International Journal of Molecular Science</em> <a aria-label=" (opens in a new tab)" href="https://www.mdpi.com/1422-0067/24/3/2117" target="_blank" rel="noreferrer noopener">review</a> of 20 papers. The high prevalence and early onset of smell loss, along with the development of sensitive olfactory tests, has raised interest in it as a diagnostic tool for Parkinson’s disease and Alzheimer’s disease, in particular. But loss of smell also afflicts people with <a aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8673514/" target="_blank" rel="noreferrer noopener">Huntington’s disease</a>, <a aria-label=" (opens in a new tab)" href="https://www.msard-journal.com/article/S2211-0348(18)30084-1/fulltext" target="_blank" rel="noreferrer noopener">multiple sclerosis</a>, and <a aria-label=" (opens in a new tab)" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9393534/#:~:text=A%20meta%2Danalysis%20study%20found,et%20al.%2C%202017)." target="_blank" rel="noreferrer noopener">mild cognitive impairment</a>. Catching these diseases early by testing for powers of smell could give patients a head start on neuroprotective and disease-modifying therapies.</p><blockquote>
<p>The power of the nose peaks in middle age, around age 40.</p>
</blockquote><p>The connection between odor detection and brain health makes sense. Our sense of smell begins in the nose and then proceeds directly to the olfactory bulb at the base of the brain. It’s the first sensory structure we have that provides direct contact with sensory stimuli and environmental pathogens in the air. Experimental evidence suggests olfactory bulb neurons play a key role not just in the loss of smell but also in the early stages of brain disease.&nbsp;</p><p>Some researchers at the University of Padova in Italy are looking at how the sudden smell loss characteristic of COVID-19 <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9444897/" target="_blank" aria-label=" (opens in a new tab)" rel="noreferrer noopener">connects</a> with Parkinson’s disease. Viral infections, like COVID-19, and regional inflammatory processes may trigger defective proteins to aggregate, a feature of Parkinson’s, and lead neurons to subsequently degenerate, the researchers hypothesize. Certain psychiatric diseases are also associated with smell dysfunction: Schizophrenia patients have shallower nasal cavities and smaller olfactory bulbs than healthy individuals, as well as other nasal abnormalities that Alzheimer’s patients also experience.</p><p>Not everyone with smell loss will develop brain disease. The acuity of our sense of smell declines as we age but follows a different trajectory from that of some other cognitive and perceptual abilities. While short-term memory peaks at age 25, for example, the power of the nose peaks in middle age, around age 40, and then gradually begins to slip. Age-related loss of smell is greater in men than women, and women have a higher total number of olfactory bulb cells than men—at least 50 percent more, on average, <a aria-label=" (opens in a new tab)" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0111733" target="_blank" rel="noreferrer noopener">research</a> suggests.</p><p>Aside from its connection to neurodegeneration, smell is also a powerful marker of overall health and mortality risk. In one longitudinal <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3916729/" target="_blank" aria-label=" (opens in a new tab)" rel="noreferrer noopener">study</a> of 1,162 healthy older people (without dementia) the mortality rate over a four-year period was 45 percent for those with the lowest baseline olfactory test scores, compared to 18 percent for those with the highest scores, even after controlling for age and other factors.</p>
          <p>It is surprisingly difficult to know when we have lost our sense of smell: About 70 percent of people living with a loss of smell don’t realize it until they’re tested. (If you’re curious, you can take a simple scratch and sniff <a aria-label=" (opens in a new tab)" href="https://mysmelltest.org/sfmc?et_cid=2447792&amp;et_rid=237629145&amp;et_lid=https%3A%2F%2Fmysmelltest.org%2Fsfmc" target="_blank" rel="noreferrer noopener">smell test</a> to find out how well your nose performs, and depending on the results, you could receive an invite to join a research study sponsored by the Michael J. Fox Foundation.)&nbsp; There may be more truth than we realized to the saying, “Follow your nose.”&nbsp; <img decoding="async" src="https://assets.nautil.us/sites/3/nautilus/nautilus-favicon-14.png?fm=png" alt=""></p><p><em>Lead image: Lightspring / Shutterstock</em></p>

            
                          <ul>
            <li>
              <div>
              <h6>Kristen French</h6>
          <p>Posted on July 17, 2023</p>
              </div>
                                    <p>is a contributing editor at <i>Nautilus</i>.</p>
                        </li>     
          </ul>
          <!-- article-author -->

        <!--- Sponsored Text ----------------->
                <!--- End Sponsored Text -------------->
        
                
              
      <!-- comp-grid end -->
        <div>
  <p><img decoding="async" src="https://nautil.us/wp-content/themes/nautilus-block-theme/images/icons/logo-icon.svg" alt="new_letter"></p><div>
    <h4>Get the Nautilus newsletter</h4>
    <p>Cutting-edge science, unraveled by the very brightest living thinkers.</p>
  </div>

  
</div>        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Never waste a midlife crisis (393 pts)]]></title>
            <link>https://austinkleon.com/2023/07/10/never-waste-a-midlife-crisis/</link>
            <guid>36765479</guid>
            <pubDate>Mon, 17 Jul 2023 23:22:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://austinkleon.com/2023/07/10/never-waste-a-midlife-crisis/">https://austinkleon.com/2023/07/10/never-waste-a-midlife-crisis/</a>, See on <a href="https://news.ycombinator.com/item?id=36765479">Hacker News</a></p>
<div id="readability-page-1" class="page"><article aria-label="Never waste your midlife crisis" itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text"><p><img decoding="async" src="https://austinkleon.com/wp-content/uploads/2023/07/791px-thumbnail-600x776.jpg" alt="" width="600" height="776" srcset="https://austinkleon.com/wp-content/uploads/2023/07/791px-thumbnail-600x776.jpg 600w, https://austinkleon.com/wp-content/uploads/2023/07/791px-thumbnail-300x388.jpg 300w, https://austinkleon.com/wp-content/uploads/2023/07/791px-thumbnail-768x993.jpg 768w, https://austinkleon.com/wp-content/uploads/2023/07/791px-thumbnail.jpg 791w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p>I turned 40 last month and spent three weeks reading <em>Don Quixote,</em>&nbsp;so the mid-life crisis has been on my brain.</p>
<p>“Never waste your midlife crisis.”</p>
<p>That’s advice I heard while&nbsp;listening to <a href="https://www.adam-buxton.co.uk/podcasts/wmcgmfe7zj7chps-89fa4-2e48e-rwzy8-824b8-4mdjd-2hcs8-chrb2-7y8jj-5pcse-2wtsh-cz8zd-e3cdy-g68s7-3dezn-7k5mw-y739l-p75rr-jr8a7-7akgj-l4zms-mbmhm-486s5-jg45z-n3dba-3g2sh-3bme9-m284e">a podcast interview</a>&nbsp;with <a href="https://johnhiggs.com/">John Higgs</a>, author of <a href="https://geni.us/5eE3"><em>William Blake vs. The World.</em></a> (One of my <a href="https://austinkleon.com/2022/12/28/my-reading-year-2022/">favorite reads of 2022</a>.)</p>
<p>Higgs was saying that the artists he admires are people like David Lynch, “People who you wouldn’t think there’s an obvious place for them in the world, but they just do their stuff regardless, and a place sort of builds around them.”</p>
<p>He continues:</p>
<blockquote><p>There’s a concept in ecology of ‘niche creation.’ And the idea is: it’s not the case that a species will sort of come along and go, ‘oh, I could do well here, there’s lots of food,’ and things like that. A species comes along and just does his thing, and by acting in the world, he sort of creates the very environment he needs to survive.</p>
<p>It was when I made the decision to attempt to become a full-time writer — knowing full well the absurdity of it given all the business models of writing — there was a sort of act of faith that if I just <em>did it</em>, people who read my books would start to appear. And slowly over time, I’d build people who would go, ‘Oh, that guy’s interesting, I’ll read his next book.’ Just enough to support me.</p>
<p>You sort of create the niche that need. It’s not like the world was going, ‘Oh, there’s a real need for books by John Higgs, where are they?’ But if you do them, the world sort of reacts around them.</p></blockquote>
<p>“It’s always on an edge of never working out properly,” he admits, but it’s working so far, and it all started when he turned 40 and made the decision to go for it:</p>
<blockquote><p><strong>You should never waste your midlife crisis. You can do great things with a midlife crisis</strong>. If you just waste it on like a car, it’s just a lack of imagination. Mine was the decision to write books and attempt to make a living there.</p>
<p>The options seemed to be: If I went for it, I’d be penniless, and if I didn’t go for it, I’d be bitter. I’d be bitter going forward. Penniless certainly beats bitter. So I made the decision. And that was ten years ago! And I’m still going.</p></blockquote>
<p>Loved this. <em>Never waste your midlife crisis.</em></p>
<p>(I’m currently listening to his book <a href="https://geni.us/nPTk"><em>Love and Let Die: James Bond, The Beatles, and the British Psyche</em></a> while playing Zelda. He points out that James Bond was basically Ian Fleming’s mid-life crisis.)</p>
</div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Whatever happened to the “coming wave” of delivery drones? (126 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=36764829</link>
            <guid>36764829</guid>
            <pubDate>Mon, 17 Jul 2023 22:17:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=36764829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="36765038"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765038" href="https://news.ycombinator.com/vote?id=36765038&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I don't remember where, but I read a lengthy comment from someone in the industry which said there were two main things preventing delivery drones from being a viable market:<p>1. FAA regulations - delivery drones can't operate within X miles of an airport (technically they can, but it requires a much stricter degree of certification and compliance nobody wants to bother with)</p><p>2. Drones need a landing space, so people without yards (like apartment and townhouse dwellers, who make up a lot of the population in exactly the densely populated areas where you'd want to use drones to begin with) can't be served</p><p>And it turns out that once you exclude "houses within X miles of an airport" and "houses without an LZ", there aren't enough customers left to make delivery drones worth it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36765083"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765083" href="https://news.ycombinator.com/vote?id=36765083&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>2 is surprising as that’s the opposite of where I’d expect delivery drones to be useful. You don’t need them in dense urban environments where a stocked truck offloads inventory with low mileage. You want them in rural areas where as the crow flies makes a difference and it’s expensive to have a truck driving about sparsely populated areas.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36765492"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765492" href="https://news.ycombinator.com/vote?id=36765492&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>The thing that makes rural areas inefficient for trucks (distance between customers) is far worse for drones. Drones have to be based out of somewhere close to the customer due to their short battery life, which means you have to have a lot of bases, each of which only serves a handful of customers. A truck will burn a lot of fuel per customer,  but a single one can still serve a large area on one tank and can be based out of a nearby city.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767647"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767647" href="https://news.ycombinator.com/vote?id=36767647&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>A truck will burn a lot less fuel per kg of cargo. That's because it is carrying many packages at once and fuel consumption is split across all packages. A drone only carries one package. So in reality a truck is a way more efficient way of delivering good.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767761"><td></td></tr>
                <tr id="36768169"><td></td></tr>
                      <tr id="36768282"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_36768282" href="https://news.ycombinator.com/vote?id=36768282&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I suspect this still only works if you can utilize the capacity and have simultaneous deliveries, which would required a cluster of deliveries, thus likely not good for rural areas.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36769402"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_36769402" href="https://news.ycombinator.com/vote?id=36769402&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt;I suspect this still only works if you can utilize the capacity and have simultaneous deliveries<p>You can still eliminate inefficient stop-and-go driving for some fraction of stops (ie not U-turns), and (most important) the time-wasting "last 100 feet" walk. Sure it's not <i>as superior</i> in rural areas vs suburban, but with low enough overhead you could still beat a traditional truck.</p><p>Naturally if the drone gear cost millions of dollars and took up half the truck, then I'd agree it wouldn't be worth it. In reality I don't think that's the tradeoff.</p><p>UPS showed off an interesting "roof mounted" hardware approach (ignoring how they depict of the rest of the ops model): <a href="https://www.youtube.com/watch?v=TYBu2glKHTA">https://www.youtube.com/watch?v=TYBu2glKHTA</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36767970"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_36767970" href="https://news.ycombinator.com/vote?id=36767970&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Anytime you think, "ah, the solution is this thing from Ready Player One", step back and remember the following:<p>- that book was set in a shitty gig economy dystopia, even if the main character managed to personally get rich in the end</p><p>- that book was fucking terrible and it's frankly embarrassing that grown adults make reference to it in a positive manner
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36769217"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36769217" href="https://news.ycombinator.com/vote?id=36769217&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>You're probably right, it's the equivalent of public transportation for cargo. And you still need a truck to deliver the packages to all those drone bases anyway.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36767856"><td></td></tr>
                <tr id="36768065"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36768065" href="https://news.ycombinator.com/vote?id=36768065&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>that sounds like a fun travelling salesman problem with more moving parts. What is the best route for this drone platform truck, taking into account how efficient it is for each drone to make its delivery, and how the truck’s position will have changed in the time it takes for the drone to return.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767888"><td></td></tr>
                        <tr id="36765424"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765424" href="https://news.ycombinator.com/vote?id=36765424&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I'd like to see someone try integrating them into the delivery trucks.  Roof mount it, make it easy to load from the back storage area, and have it auto charge.  That way drivers can choose to hand off smaller packages to it and launch when they are delivering close by.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767042"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767042" href="https://news.ycombinator.com/vote?id=36767042&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>That's how I've always imagined it. The top of the FedEx truck is a mini-helipad, and there are two people on the truck. As the truck makes its normal route stopping to deliver heavy packages on foot, there are two drones darting back and forth delivering light ones, with someone constantly feeding them packages and batteries.<p>Somewhere years ago I have a chat log where I describe the whole thing as imagined, including the phrase "print out helipad_qr.pdf and tape it to your driveway" as part of the process. ;)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36767539"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36767539" href="https://news.ycombinator.com/vote?id=36767539&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I would bet money that the cost of<p>- a second person (pay + benefits + training)</p><p>- drones</p><p>- helipad</p><p>- vehicle idle time for loading the drones</p><p>is more expensive than one person just driving the boxes around.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36768023"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_36768023" href="https://news.ycombinator.com/vote?id=36768023&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I don't think so.<p>You're describing the total cost and ignoring the larger amount of packages being delivered.</p><p>Cost per package drops.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36768867"><td><table>  <tbody><tr>    <td indent="6"><img src="https://news.ycombinator.com/s.gif" height="1" width="240"></td><td>
      <center><a id="up_36768867" href="https://news.ycombinator.com/vote?id=36768867&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>What delivery men need isn't messing around with slow and complicated drone launches. They need the equivalent of a mailbox, i.e. package lockers in front of every apartment building and in front of every house. Then all they have to do is get off the delivery vehicle, retrieve the package, open the locker, insert the package, close the locker and then go back to the vehicle.<p>At that point you have reached human peak efficiency and then the only thing you can do is build a self driving delivery van with an integrated crane/robot arm to operate the locker.</p><p>Self driving is hard. Building a robot arm that inserts packages sideways into a locker doesn't appear as difficult. The post office can dictate the package formats and add visual markers to make it easier for the robot to grab the right side.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                              <tr id="36767038"><td></td></tr>
                <tr id="36767795"><td></td></tr>
                  <tr id="36765501"><td></td></tr>
                <tr id="36765715"><td></td></tr>
                        <tr id="36765205"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765205" href="https://news.ycombinator.com/vote?id=36765205&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I saw a demo recently where they had a drone flying high and it zip-lined a package down to the destination directly below it, so the drone never had to get close to the ground. I’m not sure if that’s current strategy but it seems like a cool idea to cut down on noise and proximity issues.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36768622"><td></td></tr>
            <tr id="36765448"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765448" href="https://news.ycombinator.com/vote?id=36765448&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I think this is the popular route they’re going, to avoid drone theft/vandalism, problem is still the liability of dropping things on people’s heads regardless of CV or other sensors.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767809"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767809" href="https://news.ycombinator.com/vote?id=36767809&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>That’s actually an idea: create a lightweight Ariel tram way just for package delivery. Sort of like the old vacuum tube networks some cities used to have.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765239"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765239" href="https://news.ycombinator.com/vote?id=36765239&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>The issue there is that the battery tech to support long distance delivery drones does not exist yet.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767718"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767718" href="https://news.ycombinator.com/vote?id=36767718&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Now I’m imagining a larger, fixed-wing loitering drone carrying the package(s) and a small last-mile drone that detaches, drops its payload at the destination and then returns to the mothership.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765635"><td></td></tr>
            <tr id="36766711"><td></td></tr>
                <tr id="36767530"><td></td></tr>
                        <tr id="36766710"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36766710" href="https://news.ycombinator.com/vote?id=36766710&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>2 is wrong and you are exactly right. Amazon explains drones are exactly for meant for rural areas here: <a href="https://youtu.be/yMqbj4Kj-z0?t=688" rel="nofollow noreferrer">https://youtu.be/yMqbj4Kj-z0?t=688</a><p>A lone driver might get paid X $/hour but in a rural area, only reach a single house with his truck, whereas the same driver might make 10x deliveries in the same time period in an urban area. And optimally, you'd have one drone pilot controlling multiple drones at the same time with each one taking less time than a car.</p><p>I'm guessing the biggest cost here isn't fuel or energy, it's the staffing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766771"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36766771" href="https://news.ycombinator.com/vote?id=36766771&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>In his point #2, he is saying you <i>can</i> deliver to rural areas, but not to densely populated areas like cities, where there are not enough landing sites. I have no idea whether this is correct or not.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36768303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36768303" href="https://news.ycombinator.com/vote?id=36768303&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>3. Drones, autonomous or not, aren't anywhere near reliable enough to do autonomous remote delivery. Final delivery is fraught with a myriad of things that can go wrong. GPS is not 100% reliable or accurate in urban areas. Failure in flight means both drone and package are lost, with the added bonus of potentially hurting or killing someone, liability for which the insurance company is not going to underwrite because drones aren't reliable. Backup flight modes add to weight and cost, and reduce deliverable payload. And even after all of that, aviation authorities at least in my jurisdiction are extremely conservative and won't approve them for autonomous delivery and flight near anywhere people live.<p>4. Payloads on flying things have severe weight and volume limitations. This limits what you can deliver to things that are small and light.</p><p>5. An autonomous drone represents an opportunity for theft or interference. See the drone coming, then: steal the goods when it delivers, interfere with its GPS and force it into an error mode that lands the drone, etc. You don't even need to be at the destination.</p><p>6. Multicopters, the devices most people think of when they think drones, have terrible range and payload capacity. The flight mode is akin to helicopters, "beat the air into submission". On the upside, they can hover on the spot. In contrast, plane-style drones which fly aerodynamically or "on the wing", have more range (being more efficient), but have difficulty landing on specific spots and then returning back to base. There are solutions around this, but not 100% viable in all cases. VTOL planes exist for instance, but they're finicky. Parachutes as a delivery mechanism also exist, but not everything can be delivered by dropping a payload.</p><p>And much more...</p><p>Air delivery exists in places where these aren't concerns and where the cost benefit analysis skews the other way. Zipline [0] has been doing a great job delivering blood to remote areas in Rwanda for a while now. Their product, payload, delivery method, geographical location, regulatory environment, all align to make it worth it. Watch some youtube videos of their operation, it's pretty neat.</p><p>[0] <a href="https://www.flyzipline.com/" rel="nofollow noreferrer">https://www.flyzipline.com/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36768883"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36768883" href="https://news.ycombinator.com/vote?id=36768883&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Forget Drones, even the land robots [the ones that look like small moving pillars] haven't been put to actual use - the companies that  made them or built software for them ended up raising good cash and exiting by getting sold to other big companies in the logistics space, but no one knows for sure what their future is, lol.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36769353"><td></td></tr>
                  <tr id="36765704"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765704" href="https://news.ycombinator.com/vote?id=36765704&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt; And it turns out that once you exclude "houses within X miles of an airport" and "houses without an LZ", there aren't enough customers left to make delivery drones worth it.<p>That would make sense if this was some incredibly capital-intense thing to develop and enjoyed immense economies of scale. But I don't see why that would be the case. Surely there are many thousands of towns each with tens of thousands of single-family homes that are sufficiently far from an airport and currently served by daily package deliveries via automobile. Unless we're considering every tiny community airport.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36767784"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767784" href="https://news.ycombinator.com/vote?id=36767784&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>1 is a bit surprising. It’s not like housing around airports are particularly popular for noise reasons. Here in Seattle, if you don’t count float planes, we don’t even have general aviation airports to worry about until you hit Boeing field to the south and Paine field in the north (again Boeing related).<p>Most new town homes have rooftop decks (because Seattle doesn’t get much snow), why can’t they land on those?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36767864"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36767864" href="https://news.ycombinator.com/vote?id=36767864&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Renton Airport, Boeing Field, Seatac, Paine Field Harvey Field, Arlington Field, plus scores of smaller poorly marked airstrips if you check OpenStreetMap.  It turns out very few places in the US are sufficiently far away from an "airport" to allow drones to operate.  Even out in the boonies.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767918"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767918" href="https://news.ycombinator.com/vote?id=36767918&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>There is literally nothing in Seattle between Paine field and Boeing field in the main I5 corridor if you don’t count sea planes (all the airport symbols along the lake are float planes, Boeing field, SeaTac, and Renton are all basically right next to each other). It is a pretty air strip dead zone, you had to go all the way out to Arlington to mention another one (they have a nice playground next to it that the kid likes to watch planes from)! And ya, the boonies have landing strips everywhere (like the one out near Snohomish used for sky diving).</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36768305"><td></td></tr>
                        <tr id="36767236"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767236" href="https://news.ycombinator.com/vote?id=36767236&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Also in the LZ column: trees and power lines. From above, looking down, power lines are almost impossible to spot for human pilots. A CV model steering down from 400 feet would need to avoid such.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765333"><td></td></tr>
                <tr id="36768165"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36768165" href="https://news.ycombinator.com/vote?id=36768165&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Yeah my complaint is that they cant see my driveway and wont deliver to me.<p>Its logan, so you know, kids on dirtbikes, dogs, road noise, fireworks, loud music etc I barely notice the drones.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765611"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765611" href="https://news.ycombinator.com/vote?id=36765611&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I live in a Durham, NC and there is a small radius where a company called Flytrex does drone food delivery. We use it every couple weeks since they have almost no fees unlike Uber Eats. From what I understand they place the order, drive a car to pick up your food, drive back to the launch pad, then load and send the drone. It then drops the food using a long cable that leaves behind a reusable bag in your front lawn. It’s surprisingly accurate since it has about a 15ft diameter circle to hit in my front yard to avoid trees and the house. 
The one downside is the weight limit and weather. If you go over something like 8 pounds then the drone will fly what it can and someone will follow begging in a car with the remainder. Bad weather means car delivery.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36766779"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766779" href="https://news.ycombinator.com/vote?id=36766779&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Fascinating.  But curious how it can operate without higher fees?  There's still the person driving to the place, waiting for the order, then driving back to launch.  In fact, it almost feels like this would be more expensive in many cases than just bringing it straight over.  Burning VC money?</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36766822"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36766822" href="https://news.ycombinator.com/vote?id=36766822&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Im in a different Flytrex delivery area and I don’t have exact answers but probably some insight. There is a strip mall or two with like 15 restaurants and these are the ones on their service so the driving is really under a mile. Also they seemingly have a new 30% off code for one particular restaurant every few days, likely to funnel orders and make the “last mile” delivery person able to batch pick up. The drone really only took under 4 minutes to fly to my house (I was on the outer edge of the delivery ring so it’s gotta be the furthest atm).<p>They don’t have to deal with traffic or 3rd party delivery drivers. From hitting order to my first bite was under 30 minutes (on my second order, my first order was super late but they comped me - the first delivery they have a member watch the drone land in your yard to ensure it’s working).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36768150"><td></td></tr>
                  <tr id="36766683"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36766683" href="https://news.ycombinator.com/vote?id=36766683&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I interviewed a few months ago with one of the leaders in the space, and when I asked the HM my standard “what do you see as the biggest challenge facing the business right now” I got a surprising answer: “the weather.”<p>My sense is that drone delivery has the same weather  dependency as self-driving cars, only greatly magnified.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36767019"><td></td></tr>
                <tr id="36767122"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36767122" href="https://news.ycombinator.com/vote?id=36767122&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Except it wouldn't be 10% slower for 60% cheaper of their item, it's 60% cheaper for the <i>delivery</i> cost which is a small fraction of the overall cost.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767407"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767407" href="https://news.ycombinator.com/vote?id=36767407&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Have you looked up small, light items on Amazon and seen that they pack them up packs of large N? They do that to hide the fact that delivery is actually a very large fraction of cost on such items. Food delivery, as mentioned in the OP, is another example.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767439"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36767439" href="https://news.ycombinator.com/vote?id=36767439&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Exactly - rule of thumb is that about $4 of any small item on Amazon goes to Amazon. That seven dollar cable is really at best a $3 cable, including costs to get the item to Amazon in the first place.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36767106"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36767106" href="https://news.ycombinator.com/vote?id=36767106&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>This is unintuivite at first, but makes sense. As we approach last mile, economies of scale begin to not only diminish, but invert.<p>Let me put it another way... it's very inefficient to move cargo by a fleet of cars vs a fleet of trucks, and it's even more efficient to move cargo by train.</p><p>But it would be terribly inefficient to move that train to your home. A UPS truck is more efficient than a train, and a guy on a bike might be more efficient than a UPS truck, other than labor.</p><p>But if we can get an autonomous vehicle, then the labor goes away, and if the delivery drone is electric, then it can largely be very efficient- going point to point in a straight line.</p><p>I hadn't thought about it before, but it makes so much sense.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36767098"><td></td></tr>
                <tr id="36767115"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767115" href="https://news.ycombinator.com/vote?id=36767115&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Not 0%, but a drone center can have a few people operating many drones, so the labor cost is distributed there too.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36767390"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767390" href="https://news.ycombinator.com/vote?id=36767390&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Does wildfire haze get in the way?  Can IR or LIDAR or something see through it reasonably well?<p>If you built your drone on mostly visual data... or visual-centric sensor fusion... imagine trying to deliver in NYC with The Fog.  The tradeoff drones make for their airspace is being at an altitude where there's an awful lot of "terrain".</p><p>Fortunately it's not like that here but I'm not in an area that's probably dense enough for drone delivery for a bit (except subdivisions/etc).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766787"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766787" href="https://news.ycombinator.com/vote?id=36766787&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I'm not surprised at all - that was the every first constraint that came to mind when I first heard about this idea.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765355"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765355" href="https://news.ycombinator.com/vote?id=36765355&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>The “killer app” (pun intended) for drone delivery was precision delivery of high explosive munitions to their target. Increasingly, they also deliver supplies.<p>At these tasks, they excel and are in so much demand that both sides of the Russian invasion of Ukraine can’t produce enough quantity to meet demand.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765376"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765376" href="https://news.ycombinator.com/vote?id=36765376&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I live in an area with Wing drone delivery. On a typical day I hear approximately zero drones flying, and we have only personally used it 5-6 times in 3 years (mostly as a novelty to impress kids that are visiting).<p>There isn't that great a selection of stuff that you can get, and some stores have come and gone from the app.</p><p>It's just not that useful. We live in the suburbs, and a weekly trip to the grocery store or big box store (5-10 min drives) can get so much more and at a better price than buying things individually. It's kind of like milk delivery - it made sense before refrigeration but after that, you would buy weekly and just store it.</p><p>And right now delivery is free, but it would be even worse if/when it isn't.</p><p>One area where it could work is coffee/tea delivery (Starbucks, etc). These are things you buy individually and typically everyday. But AFAIK we don't have that and its not on the horizon.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766697"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36766697" href="https://news.ycombinator.com/vote?id=36766697&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Long story short, It's the FAA's fault. Initially they said, "no, you can't do that" because there are no regulations for that. With pressure from companies, the FAA took years to create the rules to make drone deliveries legal. Finally there is some regulatory framework.<p>Drones have to be specifically designed and manufactured to pass the FAA's certification program. We've all seen capable drones from Amazon's marketing, but does it please the FAA? A drone can't do the job until the FAA signs off on that make/model. It's not easy to do and requires developing a craft with all sorts of safety features.</p><p>Companies are making progress, and I think the most insightful videos to watch on the topic come from Zipline. They're already operating in Rwanda with support from their government. They have a streamlined service and make 100s of deliveries per day.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766977"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766977" href="https://news.ycombinator.com/vote?id=36766977&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>To be fair, this sounds like the FAA is doing its job. Tiny consumer drones are one thing, but if there are going to be fleets of drones carrying packages all over my city, I'll take a slow and thorough FAA over move fast and break things.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36768268"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36768268" href="https://news.ycombinator.com/vote?id=36768268&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Drone deliveries substitute for cars and trucks, which kill 20 pedestrians every day in the US.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36767043"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767043" href="https://news.ycombinator.com/vote?id=36767043&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Good.  Without their careful and considered work, the US skies would experience it's own tragedy of the commons.  There's a reason air travel is so damn safe, and it's not because private companies were left to their own devices.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767285"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36767285" href="https://news.ycombinator.com/vote?id=36767285&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Wikipedia somewhat disagrees with you:<p>"The Air Commerce Act of May 20, 1926, is the cornerstone of the U.S. federal government's regulation of civil aviation. This landmark legislation was passed at the urging of the aviation industry, whose leaders believed the airplane could not reach its full commercial potential without federal action to improve and maintain safety standards. "</p><p>Maybe people were smarter in the '20s, maybe they were hoping for regulatory capture? Not sure, but obviously it's a good thing...</p><p><a href="https://en.m.wikipedia.org/wiki/Federal_Aviation_Administration" rel="nofollow noreferrer">https://en.m.wikipedia.org/wiki/Federal_Aviation_Administrat...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36767298"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767298" href="https://news.ycombinator.com/vote?id=36767298&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>That quote does not disagree with OP at all. It says that the aviation industry <i>itself</i> determined that they should not be "left to their own devices" (OP's words) and asked for federal regulation.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36767037"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767037" href="https://news.ycombinator.com/vote?id=36767037&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>"It's the FAA's fault" makes it sound like that's a bad thing. I think it's good that the FAA took time to create meaningful regulations and that somebody hasn't been able to just slap something together and be able to pass them.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767150"><td></td></tr>
            <tr id="36766784"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766784" href="https://news.ycombinator.com/vote?id=36766784&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Zipline's work in Rwanda is really inspiring. I love that they are doing real good in the world first and can leverage that to grow their commercial business.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767222"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36767222" href="https://news.ycombinator.com/vote?id=36767222&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Zip line also works because it’s for rural areas that don’t have a good road network, in the US driving would almost certainly be faster.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36767406"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36767406" href="https://news.ycombinator.com/vote?id=36767406&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I imagine it depends on the size of delivery. Even with extremely good roads, zipline would probably be faster (I can't imagine that flying in a straight line is _ever_ slower than driving), just lower bandwidth. Single small package? Zipline is probably much faster. Many and/or large packages? Driving is almost certainly faster. Good roads don't change this basic fact, they just change the break even point.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="36767233"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36767233" href="https://news.ycombinator.com/vote?id=36767233&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>in addition to car accidents, in the future , you will have one more way to die prematurely : drone accidents</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765175"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765175" href="https://news.ycombinator.com/vote?id=36765175&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Jeff Bezos, 2013: "There's no reason they can't be effectively used as delivery vehicles"<p>"It can't be before 2015, because that's the earliest that we can get the rules from the FAA. But it could be 4 or 5 years."</p><p>- 10 mile radius from Amazon FC</p><p>- half hour delivery</p><p>- Objects up to 5 lbs, which is 86% of items Amazon delivered at that time</p><p>Said the biggest problems were redundancy, reliability, and safety</p><p><a href="https://www.cbsnews.com/video/amazon-ceo-unveils-drone-delivery-concept/" rel="nofollow noreferrer">https://www.cbsnews.com/video/amazon-ceo-unveils-drone-deliv...</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765251"><td></td></tr>
                <tr id="36765421"><td></td></tr>
            <tr id="36765341"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765341" href="https://news.ycombinator.com/vote?id=36765341&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>This seems like the kind of high importance deliveries that drones could be utilized for, not brining me some munchies at 3am.</span></p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="36764970"><td></td></tr>
            <tr id="36765475"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765475" href="https://news.ycombinator.com/vote?id=36765475&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Great video on the current state of drone air delivery: <a href="https://www.youtube.com/watch?v=yMqbj4Kj-z0">https://www.youtube.com/watch?v=yMqbj4Kj-z0</a><p>Basically Amazon is massively hampered by FAA regulations that say they can't fly drones beyond operator line of sight. In other words, the remote pilot must be able to see the drone at all times. It's also that drone delivery was never meant to replace all deliveries. It's only specific use cases where it is economical, such as delivery to a lone, rural house, where sending a car would be time consuming and expensive. You can get waivers for the line of sight rule but it requires sophisticated auto-avoidance tech that they're still working on (that must handle automated avoidance in rain or shine, night and day, clouds or not, winds and so on). So it's massively hard problem that was never meant to be a whole "wave" of next gen delivery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765306"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765306" href="https://news.ycombinator.com/vote?id=36765306&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>From a European perspective: regulation is just catching up. Risk assessment and safety frameworks have been established and (non-autonomous) beyond line of sight operations are slowly becoming feasible to do on a regular basis. Operating many drones (+ manned vehicles) in the same airspace needs a traffic management solution (U-Space) which is also seeing some initial trial runs now.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765060"><td></td></tr>
            <tr id="36764988"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36764988" href="https://news.ycombinator.com/vote?id=36764988&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I was working with drones for a while and one issue with carrying weight was distance. If you want a drone capable of carrying weight it has to have a specific size to accommodate the heavy object as well as batteries that get heavier the longer you want to fly and the more power you need to transport said object. Outside of that rules about line of sight were a thing, so you couldn’t just let the drone fly a pre-programmed path but you also needed to see it. There is also a risk of being shot down. I think at the end of the day search and rescue or coast guards might use drones to deliver something very quickly if there is no other way but outside of that I see challenges that are rather hard to overcome.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36769500"><td></td></tr>
            <tr id="36767190"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767190" href="https://news.ycombinator.com/vote?id=36767190&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Every time I've heard about drone delivery; it was either implied or directly stated that it would take a long time to be generally available. It's easy to forget that, historically, "tech" would often take decades to develop. (The rapid changes we saw in the 1990s and early 2000s is unusual.)<p>One thing that I think would be interesting is "hybrid" drone delivery: A delivery truck could drive to a central location, and then a "swarm" of drones could drop off packages at homes within a small radius.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765677"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765677" href="https://news.ycombinator.com/vote?id=36765677&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Zeyi Yang from China Report at MIT Tech Review had a story on how well it's going in China: <a href="https://www.technologyreview.com/2023/05/23/1073500/drone-food-delivery-shenzhen-meituan/" rel="nofollow noreferrer">https://www.technologyreview.com/2023/05/23/1073500/drone-fo...</a>. The BLUF is it still requires a fair amount of human labor and it works way better when it can go to a designated pickup point.<p>His teaser for the article also had a more detailed story about how he kept on getting human drivers delivering him milk teas when he wanted a drone, but that might just be an aside about the cost of human labor in China and how the system schedules the delivery method.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766665"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766665" href="https://news.ycombinator.com/vote?id=36766665&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt;The BLUF is it still requires a fair amount of human labor and it works way better when it can go to a designated pickup point.<p>If you need a designated pickup point, it would seem like the already-existing Amazon pickup box system would be a better choice.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765449"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765449" href="https://news.ycombinator.com/vote?id=36765449&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>We have autonomous robots that deliver groceries where we live, through a company called Starship.<p>They definitely felt like the future when they first arrived, zipping along the pavement or waiting patiently to cross a road. My daughter loves counting them as we drive past.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765311"><td></td></tr>
            <tr id="36765349"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765349" href="https://news.ycombinator.com/vote?id=36765349&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>It's kinda here, Zipline is delivering high value products to places that don't have good infrastructure, and has gotten some big inventions in propeller designs to get rid of noise.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767335"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767335" href="https://news.ycombinator.com/vote?id=36767335&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Ground delivery drones are making great strides. They're not uncommon in larger restaurants where I live, and they're also being used in the hotel industry for making deliveries from the front desk to someone's room. I expect we'll be seeing street delivery drones like this soon as well in very high density urban areas.<p>Aerial drone delivery however ends up requiring additional infrastructure and is far too expensive for what it's trying to accomplish, particularly because of the small payload requirements. Maybe that will change in the near future, but I still think we're a long way away.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765673"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765673" href="https://news.ycombinator.com/vote?id=36765673&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>The technology hasn't demonstrated the required level of reliability yet. Several companies are getting close, and and early attempts by the companies listed in this thread have paved the way for the FAA to start rolling out the process for scalable compliance. Up until very recently it's all been "approval by waiver/exception", which is very slow, while the FAA figures out along with everyone else what success looks like.<p>Currently there's approvals in limited areas in the US for testing, and several companies are approved for significant steps towards our shared dream of 5 minute burrito deliveries to our back patios. Nobody has gotten approved for blanket deliveries yet; the safety levels aren't quite there.</p><p>Plug: End State Solutions consults and supports companies in developing the conops, safety case, and approval packages. Reach out once your drone company has a design you're ready to freeze for the approval process and we'll help you out. Our team got Insitu and Matternet the first ever commercial UAS type certificates issued by the FAA.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36767925"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767925" href="https://news.ycombinator.com/vote?id=36767925&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>It's funny cause those talks happened when labor was relatively cheaper, similar with the "we'll just replace fast food workers with robots."<p>Now that labor is even less readily available we're talking about it... less. And that's even off that back of low interest rates and easier money for such initiatives.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765170"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765170" href="https://news.ycombinator.com/vote?id=36765170&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Great question. I'm not aware of any concrete reasons, but I suppose issues relating to these sorts of points:<p>- despite the sky being generally less constrained than ground delivery there are challenging obstructions, which are potentially far more risky than for ground travel: nudge past many ground obstructions you'd be okay, do the same in the sky, your drone is toast</p><p>- liability is far greater in the sky if your drone carrying something comes down in an uncontrolled manner (on someone or something)</p><p>- obstructions are likely to be high at the very points people are willing to come meet the drone (antennas, overhead power lines, washing lines, nets, etc) and they're often hard to spot</p><p>- unattended drop off is harder for drones in the places where customers are most dense (ie cities) upping the complexity further</p><p>- potential regulatory issues (but I understand below certain heights it's generally not regulated in many countries)</p><p>- bad PR from noisy drones!</p><p>- risks from non customers interfering</p><p>- challenges with carrying what you're dropping off: if it's heavy you need bigger drones; if it's light you'd be tying up a drone with something small, unless you can figure how to drop off multiple items, or you have a mixture of drone sizes</p><p>They all seem like they could plausibly be solved but I'm no drone engineer!</p><p>Maybe it could get going with targeting particular items that are strongly appealing to customers and might narrow the complexities due to being more uniform than a random Amazon basket. I believe an early use in Bhutan was for medical deliveries. Maybe something premium like ice-cream or cocktails might appeal with the right marketing?!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766673"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766673" href="https://news.ycombinator.com/vote?id=36766673&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>For small items, I sometimes get it delivered by an Amazon truck but a lot of the time it just comes from my USPS driver who is putting (mostly junk mail) in my inbox anyway.<p>Anything I get frozen gets sent in a relatively big box with dry ice or ice packs.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="36768953"><td></td></tr>
            <tr id="36767750"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767750" href="https://news.ycombinator.com/vote?id=36767750&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>delivery of drugs across flood ravaged infrastructure, or for temporary wifi/cell antenna (ruinously costly in battery life) -check.<p>checking wildlife with lower impact, finding wildlife and feral animals and lost hikers -check</p><p>taking aerial photos for weddings and documentaries -check</p><p>routine, bladerunner-esque floods of drones sending me parcels.. un-checked.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765286"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765286" href="https://news.ycombinator.com/vote?id=36765286&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Same thing that happened to self driving cars, bloodless blood tests, the 'Metaverse', and 'Social Media but with Nice People'.<p>The point of the thing is to have an impossibile to achieve project that is nonetheless popular enough to generate endless rounds of new investment.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765238"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765238" href="https://news.ycombinator.com/vote?id=36765238&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I seen some drones in Africa delivering small packages into remote villages, mostly medication. It is small RC airplane (not quad copter),  dropping parashutes on an airfield. It replaced much bigger Cesnas.<p>And drone delivery is quite successful at Ukraine...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765080"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765080" href="https://news.ycombinator.com/vote?id=36765080&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>There's a lot of research done with drones but the world isn't quite ready yet. Realistically, drones delivering packages will get shot down in the US, simply for being unmanned and flying over people's neighbourhoods. People easily feel threatened.<p>Perhaps more importantly, there's too much that could go wrong. What about legislation where they may fly and how high? What if a drone crashes into someone or something? What if someone's package gets stolen?</p><p>As much as the technology enthusiasts in us enjoy the concept of delivery drones, most of us humans still prefer a fellow human in the process of delivering packages to handle edge cases where things might go wrong.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766692"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766692" href="https://news.ycombinator.com/vote?id=36766692&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Honestly, I don't expect a lot of skeet practice in general. But you need to beat the cost of USPS and Amazon delivery trucks which seem pretty efficient and--if not zero liability--at least liability that's been well established over a good hundred years. It's theoretically cool but once you get out of medical deliveries to remote areas it's not clear how practical it is.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765373"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765373" href="https://news.ycombinator.com/vote?id=36765373&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>There's a trial near me of ground-based autonomous delivery robots (though it doesn't cover my area) which seems to have been pretty successful. People are using it and it seems to work pretty well from what my friends in the area have said. There have of course been a few hiccups but nothing major (it helps that the delivery robots are cute and will ask for help if they are stuck or need something like a button pushing to use a pedestrian crossing). The main limitation is that their range is only about 2 miles so they basically are only really saving you a quick trip to a nearby shop.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767453"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767453" href="https://news.ycombinator.com/vote?id=36767453&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>There are food delivery drones near me. Two recent data points:<p>1) They’re expanding the delivery territory.</p><p>2) They’re still using pilots with line of sight to make the delivery.</p><p>I think that’s odd. Maybe someone is moving the goalposts, or maybe there’s not a clear roadmap to broad approval.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36768175"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36768175" href="https://news.ycombinator.com/vote?id=36768175&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>When you say "line of sight" do you mean via camera, or that someone literally has to drive out and watch the drone land? The latter seems to not make much sense but that's what I think of when I read the phrase.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36768568"><td></td></tr>
            <tr id="36768731"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36768731" href="https://news.ycombinator.com/vote?id=36768731&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I was thinking about this as everyone is talking about A.I. Taking over and being the next big thing. Ironic how I googled about drones and this came up</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765565"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765565" href="https://news.ycombinator.com/vote?id=36765565&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt; Food delivery companies as well seemed to be testing out robots that would bring your food to you albeit in a more sidewalk bound way<p>Former delivery robot startup cofounder (Robby Technologies) here. To be honest hardware was by far the most time-consuming thing. I wanted to spend 80%+ of our engineering time and funds on software but it turned out that we ended up spending 80%+ of the time dealing with electromechanical issues, supply chain issues, bad USB cables, motor controller issues, shitty crimping jobs, thermal management, plastic breakage, bad PCBs, bad BMS, bad lithium cells, snapping drive belts, malfunctioning locks, malfunctioning cameras, manufacturer-mislabeled motor wires, sensors that didn't meet specs, antenna placement and RF interference, and lots more. Then there was the operational overhead of figuring out how to charge, move, and store all of them, and how to get things in/out of them when businesses weren't willing to walk from the store to the sidewalk to drop something in a robot, and the customers weren't always willing to come outside to grab their stuff. Then there were robots that got stuck in potholes and the like, and had to be rescued by driving out to them. All that while trying to scale up manufacturing, which never really happened beyond a certain scale. Guess how much time we had left for writing autonomous software.</p><p>The thing is, autonomous driving, especially on the sidewalk is actually <i>much</i> easier than the hardware problem of figuring out how to design, build, and scale a new type of vehicle from scratch. The main issue is we, and likely all the other companies, were stuck in a hellhole of hardware problems that <i>something</i> was always "on fire" hardware-wise.</p><p>In retrospect I see the companies that went to the roads instead of the sidewalks had it slightly easier on the hardware side: they could just buy a reliable car and mod it, and get to work on software. Safety-wise, of course, they have it much harder, it's a trade-off.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766094"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766094" href="https://news.ycombinator.com/vote?id=36766094&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Thanks for sharing your story honestly. These are things I wish investors understood -- building good HW is just not as sexy as computer vision on the pitch deck</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36768155"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36768155" href="https://news.ycombinator.com/vote?id=36768155&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Same thing that kills everything worthwhile. REGULATION.<p>That and the tech isnt there yet. Drones like to act vertically. I have a lot of big trees. Theres a KFC 200 meters away doing a drone delivery trial, it wouldnt deliver to me because it couldnt ID my driveway from aerial photography. When the drone is more like, a driverless van, this wont be an issue.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36768257"><td></td></tr>
                  <tr id="36766810"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36766810" href="https://news.ycombinator.com/vote?id=36766810&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>We have some where I live (Flytrex in Durham). It's a pretty fun and occasionally useful novelty but - FAA regulations mean a driver has to come watch it deliver the food so - not quite ready for prime time.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36767945"><td></td></tr>
            <tr id="36767070"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767070" href="https://news.ycombinator.com/vote?id=36767070&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>The first time a drone is ingested into a jet engine, the news headlines will explode and all drones will be grounded for a long time, if not forever.<p>Only takes one not following rules.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765249"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765249" href="https://news.ycombinator.com/vote?id=36765249&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>My friend had coffee delivered via drone (Canberra, Australia) and it was pretty fun. Certainly feels less wasteful (energetically) than a car &amp; driver. The massive drone was pretty cool, too. That being said, Canberra is probably the perfect city for a drone delivery service, with so much open space.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765824"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765824" href="https://news.ycombinator.com/vote?id=36765824&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I think that Wendover has the best summary: <a href="https://youtu.be/J-M98KLgaUU" rel="nofollow noreferrer">https://youtu.be/J-M98KLgaUU</a><p>Here's the tl;dr:</p><p>The "last foot" problem is the biggest killer.  Getting a drone into the air and to its target is not the hard problem - getting a package safely to the ground at someone's home is.</p><p>It would require either a very specific neighborhood or a big advance in computer vision and AI tech.</p><p>Where we have seen success in this space are places with dedicated delivery zones in controlled environments where existing transport infrastructure is not a good alternative.</p><p>Also this quote: "most people are simply willing to wait two or three days to receive their package while ALL customers wants their packages delivered as cheaply and simply as possible."</p><p>Uber eats and door dash largely solve the problem that drone delivery was supposed to solve and if you're going to have autonomous delivery doing it via ground robots is much easier than trying to do it from a drone.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766723"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36766723" href="https://news.ycombinator.com/vote?id=36766723&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>And I'm pretty sure none of those prepared food or even meal kit companies actually made money so once the free money evaporated people weren't generally willing to pay what it cost to get a cold soggy hamburger delivered.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36767153"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767153" href="https://news.ycombinator.com/vote?id=36767153&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>When a drone gets ingested by a jet engine, all drones will be grounded for a long time, if not forever.<p>It only takes one not following the rules.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766760"><td></td></tr>
            <tr id="36766581"><td></td></tr>
            <tr id="36767542"><td></td></tr>
            <tr id="36768322"><td></td></tr>
            <tr id="36767083"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767083" href="https://news.ycombinator.com/vote?id=36767083&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>A brief survey of the replies in this comment thread (beyond the dismissive comments) reveals that there are in fact several small scale and ongoing uses of delivery drones in a variety of urban and rural settings. It's obviously not a mass market yet but there are enough of those that you can't simply dismiss the technology as unworkable. Because clearly there are some minor successes here and there.<p>There are a few things where you could expect some progress to happen over time:</p><p>- FAA regulations are evolving and called out as an obstacle. But that's just the US of course. Keep an eye on countries like China which is in any case where a lot of the components are being developed. They are not waiting for FAA approval over there. And that's also the reason you can expect the FAA to be adapting over time.</p><p>- Cost is a big factor. The war time use of drones is a case where the use case justifies a higher cost as it means exposing less humans to enemy fire. Losses are high and yet it seems a highly successful niche use for drones even with the current state of the art in technology. Drones are being used successfully in places like the Ukraine, Yemen, and in other conflict zones. As cost comes down, that also opens up more civilian use cases.</p><p>- Battery tech is improving. CATL recently launched a 500wh/kg battery product intended for drone companies. Mass production of these is probably going to be a few years down the line. But the technology is shipping this year and not just some proof of concept kind of thing. For most current drones, that would be a doubling of battery capacity; which is a big deal of course. Bigger batteries are under development by a range of companies. Higher capacity, faster charging, lower cost, etc. batteries are coming to market.</p><p>- Several cities now have autonomous taxis. Both inside and outside the US. China especially seems very bullish and aggressive on this front. Mostly that's aimed at human transport but the extension to goods delivery seems like it's a logical next step. This stuff seems to be ramping up in more and more cities over the next decade or so.</p><p>- A lot of factories already deploy autonomous vehicles on factory floors. Particularly automotive companies have been investing in this. Mostly that's about delivering parts in the workplace.</p><p>- Companies like Tesla, Boston Dynamics, etc. have been developing autonomous robots that can move around and operate in more chaotic places. It seems like these could ultimately also get involved in delivery use cases.</p><p>So, the future is coming. It might not be in the form some people are expecting. Or happening at the (unrealistic) pace they seem to be expecting it. Hardware just isn't like software. It takes time to develop and it takes more time to ramp up manufacturing. And you don't do that before you have a market. The road from a handful to hundreds to thousands to hundreds of thousands units is just very long and not a straight line.</p><p>There is a lot of stuff happening right now. And we're past the point where you can dismiss a lot of this stuff as impossible because there are countless working proofs of concept and real world products challenging that notion already.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766473"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36766473" href="https://news.ycombinator.com/vote?id=36766473&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I posit two major reasons.<p>1. Command and control - Human in the loop is still a necessity because common delivery environments are too complex for an algorithm to navigate successfully so far.</p><p>2. Energy - it takes too long to recharge a drone battery pack. Flying is very energy intensive and there just isn't a way to refuel fast enough from the hub for this to make sense yet. The tested, working alternative is to use a two stroke engine but the pollution per mile from those is astronomical.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36766371"><td></td></tr>
            <tr id="36767685"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36767685" href="https://news.ycombinator.com/vote?id=36767685&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>it was determined that the drones created too much wind, worsening the global airspeed crisis.  hard to justify them after that.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765273"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765273" href="https://news.ycombinator.com/vote?id=36765273&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>It's probably because exploiting humans with offering them "gig work" is still cheaper... that and bribing^W lobbying lawmakers to allow continuation said exploitation.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765199"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36765199" href="https://news.ycombinator.com/vote?id=36765199&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>There are still pretty serious technical and engineering hurdles to overcome to make this kind of idea feasible. It was always hype without substance. But even assuming these challenges were overcome, which is inevitable eventually even if it's a century from now, I get the feeling this would largely end up getting banned in many places. I think regular people hanging out in their backyards and going for walks in the neighborhood would feel a vague persistent menace if there were constantly drones zipping around overhead. Think about the fact that you can load up a delivery truck with hundreds of packages at a time. Drones don't have that kind of cargo capacity. The fleet would need to be massive to replace truck deliveries. The sky would be full of them.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36764998"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_36764998" href="https://news.ycombinator.com/vote?id=36764998&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Are you kidding me? It was all basically vaporware that disappeared when the free money ended. The Kiwi food delivery robots were basically fake -- they only went about 1/8 of a mile automatically. So they'd drive a car to the end of a street with your food and a robot, and then get the robot out of the car, put the food inside, and then the robot would hobble over a couple hundred feet to your door. Absolutely useless.<p>From <a href="https://www.sfchronicle.com/business/article/Kiwibots-win-fans-at-UC-Berkeley-as-they-deliver-13895867.php" rel="nofollow noreferrer">https://www.sfchronicle.com/business/article/Kiwibots-win-fa...</a></p><p>&gt; The Kiwibots do not figure out their own routes. Instead, people in Colombia, the home country of Chavez and his two co-founders, plot “waypoints” for the bots to follow, sending them instructions every five to 10 seconds on where to go</p><p>&gt; On the ground in Berkeley, people also do a lot of robot support. Traveling at 1 to 1½ mph, the bots would take too long to chug to local restaurants, so Kiwi workers pick up the food at restaurants and take it via bikes or scooters to meeting spots around campus to insert into an insulated bag in the bots’ storage compartment.</p><p>&gt; The average distance a robot covers for a delivery is about 200 meters (656 feet, or one-eighth of a mile) which makes them fall short of a “last-mile” solution.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36765392"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765392" href="https://news.ycombinator.com/vote?id=36765392&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Much like Theranos, basic immutable laws of nature make the whole idea absurd. Namely, that it requires a ton of energy to keep an object aloft.<p>It's just wildly inefficient, and will never see serious usage outside of tiny niches. Not until we get science fiction antigravity or tiny fusion reactors or something.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36765557"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765557" href="https://news.ycombinator.com/vote?id=36765557&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Doesn't it also require a ton of energy to move a five ton truck to my driveway in order to deliver a 100g charging cable?<p>I'm not sure the laws of physics are on the side of the status quo here.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36765814"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765814" href="https://news.ycombinator.com/vote?id=36765814&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Hovering like a drone has to overcome gravity, which is hard/expensive. A truck only has to overcome rolling friction, which is easy/cheap.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765623"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765623" href="https://news.ycombinator.com/vote?id=36765623&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>As usual, the difference is energy density between hydrocarbon fuels and Li-ion batteries. Power the drone with gas and it becomes a different story. But then you just re-created the helicopter and all the problems which explain why no one is doing helicopter package deliveries.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765581"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765581" href="https://news.ycombinator.com/vote?id=36765581&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>I would bet that even the worst Amazon truck gets more pounds of product per Joule expended than the best drone for any reasonably real world distance/load.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36765869"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_36765869" href="https://news.ycombinator.com/vote?id=36765869&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>The difference is both direct ground contact and fuel capacity.<p>Even if a drone could carry the energy equivalent of a 20 gallon fuel tank with it, it would waste the majority of that energy both keeping itself and its energy aloft.</p><p>Tires and other ground plane vehicles, mitigate the majority of that waste in exchange for increased navigational difficulty (regarding obstacles like buildings and trees but also moving objects like vehicles in its path).</p><p>The self-driving robots made some sense but only for very local (couple of block radius) delivery.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36767168"><td></td></tr>
                        <tr id="36765840"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765840" href="https://news.ycombinator.com/vote?id=36765840&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>you probably paid at least $5 for that cable, but the cost of materials in the cable was likely less than a dollar<p>the rest of the money went into the gas tank of the truck</p><p>you and everyone else who ordered a cable that day on that route chipped in to fill the truck up
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36768713"><td></td></tr>
                  <tr id="36765800"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_36765800" href="https://news.ycombinator.com/vote?id=36765800&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Amazon's delivery always felt odd to me because of this exact situation. Long term how sustainable can it be to work on such an extreme scale like that? The margins they must be hiding on via products must be good.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765865"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765865" href="https://news.ycombinator.com/vote?id=36765865&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Ironically one of the areas it _is_ being used successfully is in delivery of blood to some areas in Rwanda, but they're using fixed-wing drones and are even doing slingshot launches. [1]<p>The economics are totally different, though. The "product" is very high value and has a short shelf life, so being able to deliver it quickly is a big benefit.</p><p>On top of that, using a fixed-wing delivery vehicle and launching it with a human-powered slingshot makes it actually reasonably energy-efficient.</p><p>[1]<a href="https://youtu.be/bnoUBfLxZz0" rel="nofollow noreferrer">https://youtu.be/bnoUBfLxZz0</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36766238"><td></td></tr>
                  <tr id="36765610"><td></td></tr>
            <tr id="36768187"><td></td></tr>
                  <tr id="36765426"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765426" href="https://news.ycombinator.com/vote?id=36765426&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Surprisingly, I still see them around in Berkeley. As a pedestrian they're a bit annoying underfoot but mostly harmless. As a potential customer, they're so slow and ineffective that I've never felt tempted to try them out. They're thoroughly dull even as a novelty.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="36768591"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36768591" href="https://news.ycombinator.com/vote?id=36768591&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>Hmm, I don't live there anymore but have visited the university a few times lately. Haven't seen any Kiwi bots. Last time was this month, which is summer and not peak business there, but other times were during school.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="36765559"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_36765559" href="https://news.ycombinator.com/vote?id=36765559&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt; &gt; &gt; The Kiwibots do not figure out their own routes. Instead, people in Colombia, the home country of Chavez and his two co-founders, plot “waypoints” for the bots to follow, sending them instructions every five to 10 seconds on where to go<p>ooh ooh! When people say reporting can be racist, this is a perfect example! Including extra negative details that are not pertinent to the story at all.</p><p>Imagine if every story about a US company mentioned started with, "The new startup, which is based in the United States (home of Ted Bundy and John Wayne Gacy)..."</p><p>See also: <a href="https://en.wikipedia.org/wiki/Association_fallacy#Guilt_by_association" rel="nofollow noreferrer">https://en.wikipedia.org/wiki/Association_fallacy#Guilt_by_a...</a></p><p>SFChronicle could have instead opened with: "people in Colombia (a country with free childcare)" or "people in Colombia (a country that has worked to eliminate childhood hunger)", but, instead, blatant racism!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="36765695"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765695" href="https://news.ycombinator.com/vote?id=36765695&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>Felipe Chavez is the CEO and one of the founders of the company they're talking about. That's why they mention his two co-founders. All from Columbia.<p>From your examples I assume you thought they were talking about the Chavez from Venezuela? I don't know of any other Chavez that could get that kind of reaction.
Anyway, unless I've missed something, that hair trigger might need some adjustment.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765894"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765894" href="https://news.ycombinator.com/vote?id=36765894&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt; Imagine if every story about a US company mentioned started with, "The new startup, which is based in the United States (home of Ted Bundy and John Wayne Gacy)..."<p>If Ted Bundy and John Wayne Gacy founded a company together then your headline would make sense, but I don't understand why this would compare to anything the author of this article wrote.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765891"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765891" href="https://news.ycombinator.com/vote?id=36765891&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>I think you ran across the Chavez name and your brain stopped processing. Its evident in your replacement examples not even making sense in the original quote.<p>It seems you may be the one having issues with race.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="36765647"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765647" href="https://news.ycombinator.com/vote?id=36765647&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><br><div>
                  <p><span>This is a silly comment and a perfect example of seeing only what you want to see. The founder’s name is “Chavez” - he’s not referencing Hugo Chavez here.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="36765668"><td></td></tr>
            <tr id="36768001"><td></td></tr>
            <tr id="36765634"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_36765634" href="https://news.ycombinator.com/vote?id=36765634&amp;how=up&amp;goto=item%3Fid%3D36764829"></a></center>    </td><td><p><span>&gt; ooh ooh! When people say reporting can be racist, this is a perfect example! Including extra negative details that are not pertinent to the story at all.<p>Hold your excitement. Chavez is the CEO.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="36766618"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
    </channel>
</rss>