<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 05 Jun 2025 00:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[After court order, OpenAI is now preserving all ChatGPT user logs (310 pts)]]></title>
            <link>https://mastodon.laurenweinstein.org/@lauren/114627064774788581</link>
            <guid>44185913</guid>
            <pubDate>Wed, 04 Jun 2025 21:47:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastodon.laurenweinstein.org/@lauren/114627064774788581">https://mastodon.laurenweinstein.org/@lauren/114627064774788581</a>, See on <a href="https://news.ycombinator.com/item?id=44185913">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor 1.0 (119 pts)]]></title>
            <link>https://www.cursor.com/en/changelog/1-0</link>
            <guid>44185256</guid>
            <pubDate>Wed, 04 Jun 2025 20:39:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cursor.com/en/changelog/1-0">https://www.cursor.com/en/changelog/1-0</a>, See on <a href="https://news.ycombinator.com/item?id=44185256">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><h2>BugBot, Background Agent access to everyone and one-click MCP install</h2><p>Cursor 1.0 is here!</p>
<p>This release brings BugBot for code review, a first look at memories, one-click MCP setup, Jupyter support and general availability of Background Agent.</p>
<h3 id="automatic-code-review-with-bugbot"><span>Automatic code review with BugBot</span></h3>
<p>BugBot automatically reviews your PRs and catches potential bugs and issues.</p>
<p>When an issue is found, BugBot leaves a comment on your PRs in GitHub. You can click "<em><strong>Fix in Cursor</strong></em>" to move back to the editor with a pre-filled prompt to fix the issue.</p>
<p>To set it up, follow instructions in our <a target="_blank" href="https://docs.cursor.com/bugbot">BugBot docs</a></p>

<h3 id="background-agent-for-everyone"><span>Background Agent for everyone</span></h3>
<p>Since we released Background Agent, our remote coding agent, in early access a few weeks ago, the early signals we've been seeing have been positive.</p>
<p>We're now excited to expand Background Agent to all users! You can start using it right away by clicking the cloud icon in chat or hitting <code>Cmd/Ctrl+E</code> if you have privacy mode disabled. For users with privacy mode enabled - we'll soon have a way to enable it for you too!</p>

<!-- -->
<h3 id="agent-in-jupyter-notebooks"><span>Agent in Jupyter Notebooks</span></h3>
<p>Cursor can now implement changes in Jupyter Notebooks!</p>
<p>Agent will now create and edit multiple cells directly inside of Jupyter, a significant improvement for research and data science tasks. Only supported with Sonnet models to start.</p>

<h3 id="memories"><span>Memories</span></h3>
<p>With Memories, Cursor can remember facts from conversations and reference them in the future. Memories are stored per project on an individual level, and can be managed from Settings.</p>
<p>We're rolling out Memories as a beta feature. To get started, enable from Settings → Rules.</p>

<h3 id="mcp-one-click-install-and-oauth-support"><span>MCP one-click install and OAuth support</span></h3>
<p>You can now set up MCP servers in Cursor with one click and together with OAuth support, you can easily authenticate servers that support it.</p>
<p>We've curated a short list of official MCP servers you can add to Cursor at <a target="_blank" href="https://docs.cursor.com/tools">docs.cursor.com/tools</a>.</p>
<p>If you're an MCP developer, you can easily make your server available to developers by adding a <em>Add to Cursor</em> button in your documentation and READMEs. Generate one at <a target="_blank" href="https://docs.cursor.com/deeplinks">docs.cursor.com/deeplinks</a></p>

<h3 id="richer-chat-responses"><span>Richer Chat responses</span></h3>
<p>Cursor can now render visualizations inside of a conversation. In particular, Mermaid diagrams and Markdown tables can now be generated and viewed in the same place!</p>

<h3 id="new-settings-and-dashboard"><span>New Settings and Dashboard</span></h3>
<p>The setting and dashboard page have gotten some polish with this release.</p>
<p>With the new Dashboard, you can view your individual or team's usage analytics, update your display name, and view detailed statistics broken down by tool or model.</p>

<details><summary><span>Keyboard<!-- --> <span>(<!-- -->1<!-- -->)</span></span></summary><div><ul>
<li><span>Open Background Agent control panel with <kbd><code>Cmd/Ctrl+E</code></kbd></span></li>
</ul></div></details>
<details><summary><span>Improvements<!-- --> <span>(<!-- -->4<!-- -->)</span></span></summary><div><ul>
<li><span><code>@Link</code> and web search can now parse PDFs and include in context</span></li>
<li><span>Network diagnostics in settings to verify connectivity</span></li>
<li><span>Faster responses with parallel tool calls</span></li>
<li><span>Collapsable tool calls in Chat</span></li>
</ul></div></details>
<details><summary><span>Account<!-- --> <span>(<!-- -->3<!-- -->)</span></span></summary><div><ul>
<li><span>Enterprise users can only access stable release (no pre-release)</span></li>
<li><span>Team admins can now disable Privacy Mode</span></li>
<li><span><a target="_blank" href="https://docs.cursor.com/account/teams/admin-api">Admin API for teams</a> to access usage metrics and spend data</span></li>
</ul></div></details></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Autonomous drone defeats human champions in racing first (106 pts)]]></title>
            <link>https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</link>
            <guid>44184900</guid>
            <pubDate>Wed, 04 Jun 2025 20:03:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first">https://www.tudelft.nl/en/2025/lr/autonomous-drone-from-tu-delft-defeats-human-champions-in-historic-racing-first</a>, See on <a href="https://news.ycombinator.com/item?id=44184900">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

                    
                            
                                    
    
    
    <p>
        News
        -
        15 April 2025
        
    </p>

                                
                        

                    <!--TYPO3SEARCH_begin-->
                    

    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580269">
        
            <p><strong>A team of scientists and students from TU Delft has taken first place at the A2RL Drone Championship in Abu Dhabi - an international race that pushes the limits of physical artificial intelligence, challenging teams to fly fully autonomous drones using only a single camera. The TU Delft drone competed against 13 autonomous drones and even human drone racing champions, using innovative methods to train deep neural networks for high-performance control. The gained knowledge on highly-efficient robust AI &nbsp;will contribute to many robotics applications, from self-driving cars to humanoid robots.</strong></p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580270">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" data-imagezoom="" data-width="1248" data-height="1173">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/a/d/csm_WK%20Drone%20race%20groepsfoto_bde529bc18.jpg" width="1248" height="1173" alt="Group photo winners WK Drone race">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The TU Delft team: Anton Lang, Quentin Missine, Aderik Verraest, Erin Lucassen, Till Blaha, Robin Ferede, Stavrow Bahnam, Christophe De Wagter and Guido de Croon.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580271">
        
            <h3>
                Beating human pilots
            </h3>
        



            
        



            



        
    

    




        
        

    <p>For the first time, a drone has beaten human pilots in an international drone racing competition, marking a new milestone in the development of artificial intelligence. On Saturday April 14, 2025, two drone racing events took place simultaneously: The Falcon Cup Finals for human pilots and the A2RL Drone Championship for AI-powered, autonomous drones. As a climax, the best AI drones also competed against the best human pilots. The AI drone developed by TU Delft first won the A2RL Grand Challenge. It then went on to win the knockout tournament against human pilots, beating three former DCL world champions and reaching flight speeds up to 95.8 km/h on the very winding track.</p>
<p>The team of scientists and students from TU Delft achieved this by developing an efficient and robust AI system, capable of split-second, high-performance control. Whereas earlier breakthroughs, like AI defeating world champions at chess or Go, have taken place in virtual settings, this achievement happened in the real world. Two years ago, the Robotics and Perception Group at the University of Zürich was the first to beat human drone racing champions with an autonomous drone. However, that impressive achievement occurred in a flight lab environment, where conditions, hardware, and the track were still controlled by the researchers – a very different situation from this world championship, where the hardware and track were fully designed and managed by the competition organisers.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580272">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" data-imagezoom="" data-width="4032" data-height="2395">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/c/csm_TU_Delft_drone_on_track_3b9e85d62a.jpg" width="4032" height="2395" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>The drone designed by the organizers, A2RL and DCL for use by the AI teams and human pilots.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580274">
        
            <h3>
                Pushing the frontiers of physical AI
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The goal of the 2025 A2RL Drone Championship in Abu Dhabi was to push the frontier of physical AI, by stimulating research on robotic AI under extreme time pressure and with very limited computational and sensory resources. The drone had access to just one forward-looking camera, a major difference from previous autonomous drone races. This is more similar to how human FPV pilots fly, and leads to additional perception challenges for the AI.</p>
<p>The AI that won against the three former DCL world champions was developed by a team of scientists and students from the MAVLab at TU Delft’s Faculty of Aerospace Engineering. Team lead Christophe De Wagter is both exhausted and exhilarated.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580275">
        <blockquote>
            <p>I always wondered when AI would be able to compete with human drone racing pilots in real competitions. I’m extremely proud of the team that we were able to make it happen already this year. I hope that this achievement and this type of competition in general forms a springboard for real-world robot applications.</p>
            
            <cite>
                
                
                    Christophe De Wagter 
                
                
            </cite>
        </blockquote>
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580283">
                
                        
    


    
        
                
                        <a href="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" data-imagezoom="" data-width="1920" data-height="974">
                            
    
            
    
            <picture>
                
                <img src="https://filelist.tudelft.nl/_processed_/2/b/csm_timelapse_penultimate_gate_44b32560c9.jpg" width="1919" height="974" alt="">
            </picture>
        

        

                        </a>
                    
            
    

    
            <figcaption>Timelapse of the TU Delft drone flying through a gate of the racing track.</figcaption>
        

                    
            </div>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <div id="c1580284">
        
            <h3>
                AI that directly commands the motors
            </h3>
        



            
        



            



        
    

    




        
        

    <p>One of the core new elements of the drone’s AI is the use of a deep neural network that doesn’t send control commands to a traditional human controller, but directly to the motors. These networks were originally developed by the Advanced Concepts Team at the European Space Agency (ESA) under the name of “Guidance and Control Nets”. Traditional, human-engineered algorithms for optimal control were computationally so expensive that they would never be able to run onboard resource-constrained systems such as drones or satellites. ESA found that deep neural networks were able to mimick the outcomes of traditional algorithms, while requiring orders of magnitude less processing time. As it was hard to test whether the networks would perform well on real hardware in space, a collaboration was formed with the MAVLab at TU Delft.</p>
<p>“We now train the deep neural networks with reinforcement learning, a form of learning by trial and error. ”, says Christophe De Wagter. “This allows the drone to more closely approach the physical limits of the system. To get there, though, we had to redesign not only the training procedure for the control, but also how we can learn about the drone’s dynamics from its own onboard sensory data.”</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    <div id="c1580286">
        
            <h3>
                Optimising robotic applications
            </h3>
        



            
        



            



        
    

    




        
        

    <p>The highly efficient AI developed for robust perception and optimal control are not only vital to autonomous racing drones but will extend to other robots. Christophe De Wagter: “Robot AI is limited by the required computational and energy resources. Autonomous drone racing is an ideal test case for developing and demonstrating highly-efficient, robust AI. Flying drones faster will be important for many economic and societal applications, ranging from delivering blood samples and defibrillators in time to finding people in natural disaster scenarios. Moreover, we can use the developed methods to strive not for optimal time but for other criteria such as optimal energy or safety. This will have an impact on many other applications, from vacuum robots to self-driving cars”.</p>


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    
        
    

    
        
        
    

    <div id="c1580458">
        
            <h3>
                Watch the video of the drone race
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </div>
    
        



    

    

    



    
            
        

    
            
        

    
    

    
        
        
    

    
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    <p id="c1580288">
        
        
            



        
        
            


        
    
        
            

    
            
                

    
            <h3>
                Contact
            </h3>
        



            
        



            



        
    

    




        
        

    


        
            



        
    </p>
    
        



    

    

    



    
            
        

    
            
        

    
        
    
    

    
        
        
    

    
    
        



    

    

    


                    <!--TYPO3SEARCH_end-->

                    



                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redesigned Swift.org is now live (106 pts)]]></title>
            <link>https://swift.org/</link>
            <guid>44184542</guid>
            <pubDate>Wed, 04 Jun 2025 19:26:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://swift.org/">https://swift.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44184542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <header>
    
  
    <!-- mobile-navigation -->
    
  </header>
   
<section id="what-is-swift">
    <div>
        <h2>Swift is the powerful, flexible,<br> multiplatform programming language.</h2>
        <p><h2>Fast. Expressive. Safe.</h2></p>
        <p><a href="https://swift.org/install/" data-text="Install">Install</a></p><p>Tools for Linux, macOS, and Windows</p>
        <h2>Create using Swift</h2>
    </div>
    <nav aria-label="Get started with Swift">
        <ul>
            
            <li>
                <a href="https://swift.org/get-started/cloud-services" data-text="Cloud Services">
                    <i></i>
                    <div>
                        <h3>Cloud Services</h3>
                        <p>Run performant services on Linux and deploy to the cloud.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/command-line-tools" data-text="Command Line">
                    <i></i>
                    <div>
                        <h3>Command Line</h3>
                        <p>Create powerful CLI tools that are fast and memory safe.</p>
                    </div>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/get-started/embedded" data-text="Embedded">
                    <i></i>
                    <div>
                        <h3>Embedded</h3>
                        <p>Develop efficient, reliable firmware for devices like microcontrollers.</p>
                    </div>
                </a>
            </li>
            
        </ul>
        <ul>
            
            <li>
                <a href="https://swift.org/getting-started/swiftui/" data-text="iOS apps">
                    <h4>iOS apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/swift-everywhere-windows-interop/" data-text="Windows apps">
                    <h4>Windows apps</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/blog/mlx-swift/" data-text="Machine learning and AI">
                    <h4>Machine Learning &amp; AI</h4>
                </a>
            </li>
            
            <li>
                <a href="https://swift.org/getting-started/library-swiftpm/" data-text="Packages">
                    <h4>Packages</h4>
                </a>
            </li>
            
        </ul>
    </nav>
    
</section>

<section id="pillar-1">
    <div>
        <p>
            Swift is the only language that scales from embedded devices and kernels to apps and cloud infrastructure. It’s simple, and expressive, with incredible performance and safety. And it has unmatched interoperability with C and C++.
        </p>
        
        <p>
            It's the combination of approachability, speed, safety, and all of<br> Swift’s strengths that make it so unique.
        </p>
    </div>
    
 
<div>
  <div>
    
    <h3>Fast</h3>
     
    <p>Build with speed and performance.</p>
     
    <p>Swift meets the most performance-critical needs, while allowing your code to remain expressive and approachable. Swift compiles directly to native code and provides predictable memory management.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Vectorized check that a utf8 buffer is all ASCII</span>
<span>func</span> <span>isASCII</span><span>(</span><span>utf8</span><span>:</span> <span>Span</span><span>&lt;</span><span>SIMD16</span><span>&lt;</span><span>UInt8</span><span>&gt;&gt;</span><span>)</span> <span>-&gt;</span> <span>Bool</span> <span>{</span>
  <span>// combine all the code units into a single entry</span>
  <span>utf8</span><span>.</span><span>indices</span><span>.</span><span>reduce</span><span>(</span><span>into</span><span>:</span> <span>SIMD16</span><span>())</span> <span>{</span>
    <span>// fold each set of code units into the result</span>
    <span>$0</span> <span>|=</span> <span>utf8</span><span>[</span><span>$1</span><span>]</span>
  <span>}</span>
  <span>// check that every entry is in the ASCII range</span>
  <span>.</span><span>max</span><span>()</span> <span>&lt;</span> <span>0x80</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Expressive</h3>
     
    <p>Concise code. Powerful results.</p>
     
    <p>Swift empowers you to write advanced code in a concise, readable syntax that even a beginner can understand. Swift supports object-oriented, functional, and generic programming patterns that experienced developers are familiar with. Its progressive disclosure allows you to pick up the language quickly, taking advantage of power-user features as you need them.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>ArgumentParser</span>

<span>// Complete implementation of a command line tool</span>
<span>@main</span> <span>struct</span> <span>Describe</span><span>:</span> <span>ParsableCommand</span> <span>{</span>
  <span>@Argument</span><span>(</span><span>help</span><span>:</span> <span>"The values to describe."</span><span>)</span>
  <span>var</span> <span>values</span><span>:</span> <span>[</span><span>Double</span><span>]</span> <span>=</span> <span>[]</span>

  <span>mutating</span> <span>func</span> <span>run</span><span>()</span> <span>{</span>
    <span>values</span><span>.</span><span>sort</span><span>()</span>
    <span>let</span> <span>total</span> <span>=</span> <span>values</span><span>.</span><span>reduce</span><span>(</span><span>0</span><span>,</span> <span>+</span><span>)</span>

    <span>print</span><span>(</span>
      <span>"""
      Smallest: </span><span>\(</span><span>values</span><span>.</span><span>first</span><span>,</span> <span>default</span><span>:</span> <span>"No value"</span><span>)</span><span>
      Total:    </span><span>\(</span><span>total</span><span>)</span><span>
      Mean:     </span><span>\(</span><span>total</span> <span>/</span> <span>Double</span><span>(</span><span>values</span><span>.</span><span>count</span><span>)</span><span>)</span><span>
      """</span><span>)</span>
  <span>}</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Safe</h3>
     
    <p>Protect memory safety.</p>
     
    <p>Swift prioritizes safety and eliminates entire classes of bugs and vulnerabilities by its design. Memory safety and data race safety are core features of the language, making them straightforward to integrate into your codebase. Safety is required at compile time, before your applications are ever run.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>let</span> <span>transform</span> <span>=</span> <span>Affine2DTransformBuilder</span><span>()</span>
    <span>.</span><span>translate</span><span>([</span><span>10.0</span><span>,</span> <span>20.0</span><span>]</span><span>.</span><span>span</span><span>)</span>
    <span>.</span><span>rotate</span><span>(</span><span>30.0</span><span>)</span>
    <span>.</span><span>build</span><span>()</span>

<span>let</span> <span>v</span> <span>=</span> <span>[</span><span>11.0</span><span>,</span> <span>22.0</span><span>,</span> <span>1.0</span><span>]</span>

<span>// Call C functions safely with Swift types</span>
<span>let</span> <span>u</span> <span>=</span> <span>mat_vec_mul</span><span>(</span>
  <span>transform</span><span>,</span> <span>rowCount</span><span>,</span> <span>colCount</span><span>,</span> <span>v</span><span>.</span><span>span</span><span>,</span> <span>allocator</span><span>)</span>
<span>let</span> <span>uMagnitude</span> <span>=</span> <span>vec_mag</span><span>(</span><span>u</span><span>.</span><span>span</span><span>)</span>
</code></pre></div>
    
</div>

    

    

</section>

<section id="pillar-2">
    
 
<div>
  <div>
    
    <h3>Interoperable</h3>
     
    <p>Adopt in existing code incrementally.</p>
     
    <p>Swift provides unmatched interoperability with its combination of natively understanding C and C++ types without the need for foreign function interfaces, and by providing bridging for bi-directional access. Swift’s interoperability features allow you to incrementally adopt the language into existing codebases without requiring a full code rewrite.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>import</span> <span>CxxStdlib</span>

<span>// Use types from C++, like std::string, directly</span>
<span>let</span> <span>beverages</span><span>:</span> <span>[</span><span>std</span><span>.</span><span>string</span><span>]</span> <span>=</span> <span>[</span>
  <span>"apple juice"</span><span>,</span> <span>"grape juice"</span><span>,</span> <span>"green tea"</span>
<span>]</span>

<span>let</span> <span>juices</span> <span>=</span> <span>beverages</span><span>.</span><span>filter</span> <span>{</span> <span>cppstring</span> <span>in</span>
  <span>// and call methods directly on C++ types</span>
  <span>cppstring</span><span>.</span><span>find</span><span>(</span><span>.</span><span>init</span><span>(</span><span>"juice"</span><span>))</span> <span>!=</span> <span>std</span><span>.</span><span>string</span><span>.</span><span>npos</span>
<span>}</span>
</code></pre></div>
    
</div>

    
 
<div>
  <div>
    
    <h3>Adaptable</h3>
     
    <p>From microcontrollers to servers.</p>
     
    <p>The only language that can span from embedded and kernel, to server and apps. Swift excels no matter where it’s used: from constrained environments like firmware where every byte counts, to cloud services handling billions of requests a day.</p>
    
  </div>
  
  <!-- prettier-ignore -->
  <div><pre><code><span>// Configure UART by direct register manipulation </span>
<span>// using Swift MMIO. Enables RX and TX, and sets</span>
<span>// baud rate to 115,200. Compiles down to an</span>
<span>// optimal assembly sequence with no overhead.</span>

<span>usart1</span><span>.</span><span>brr</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>raw</span><span>.</span><span>brr_field</span> <span>=</span> <span>16_000_000</span> <span>/</span> <span>115_200</span>
<span>}</span>

<span>usart1</span><span>.</span><span>cr1</span><span>.</span><span>modify</span> <span>{</span> <span>rw</span> <span>in</span>
  <span>rw</span><span>.</span><span>ue</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>re</span> <span>=</span> <span>.</span><span>Enabled</span>
  <span>rw</span><span>.</span><span>te</span> <span>=</span> <span>.</span><span>Enabled</span>
<span>}</span>
</code></pre></div>
    
</div>

    
    
</section>

<div id="pillar-3">
    
    <h3>Open Source</h3>
     
    <p>Contribute and get involved.</p>
     
  </div>
 

    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Curtis Yarvin's Plot Against America (234 pts)]]></title>
            <link>https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</link>
            <guid>44184305</guid>
            <pubDate>Wed, 04 Jun 2025 19:04:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile">https://www.newyorker.com/magazine/2025/06/09/curtis-yarvin-profile</a>, See on <a href="https://news.ycombinator.com/item?id=44184305">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><figure></figure><p>In the spring and summer of 2008, when <a href="https://www.newyorker.com/tag/donald-trump">Donald Trump</a> was still a registered Democrat, an anonymous blogger known as Mencius Moldbug posted a serial manifesto under the heading “An Open Letter to Open-Minded Progressives.” Written with the sneering disaffection of an ex-believer, the hundred-and-twenty-thousand-word letter argued that egalitarianism, far from improving the world, was actually responsible for most of its ills. That his bien-pensant readers thought otherwise, Moldbug contended, was due to the influence of the media and the academy, which worked together, however unwittingly, to perpetuate a left-liberal consensus. To this nefarious alliance he gave the name the Cathedral. Moldbug called for nothing less than its destruction and a total “reboot” of the social order. He proposed “the liquidation of democracy, the Constitution, and the rule of law,” and the eventual transfer of power to a C.E.O.-in-chief (someone like Steve Jobs or Marc Andreessen, he suggested), who would transform the government into “a heavily-armed, ultra-profitable corporation.” This new regime would sell off public schools, destroy universities, abolish the press, and imprison “decivilized populations.” It would also fire civil servants en masse (a policy Moldbug later called <em>RAGE</em>—Retire All Government Employees) and discontinue international relations, including “security guarantees, foreign aid, and mass immigration.”</p><p>Moldbug acknowledged that his vision depended on the sanity of his chief executive: “Clearly, if he or she turns out to be Hitler or Stalin, we have just recreated Nazism or Stalinism.” Yet he dismissed the failures of twentieth-century dictators, whom he saw as too reliant on popular support. For Moldbug, any system that sought legitimacy in the passions of the mob was doomed to instability. Though critics labelled him a techno-fascist, he preferred to call himself a royalist or a Jacobite—a nod to partisans of James II and his descendants, who, in the seventeenth and eighteenth centuries, opposed Britain’s parliamentary system and upheld the divine right of kings. Never mind the French Revolution, the bête noire of reactionary thinkers: Moldbug believed that the English and American Revolutions had gone too far.</p><p>If Moldbug’s “Open Letter” showed little affection for the masses, it intimated that they might still have a use. “Communism was not overthrown by <a href="https://www.newyorker.com/news/our-columnists/fifty-years-later-andrei-sakharovs-most-famous-essay-is-a-powerful-model-of-writing-for-social-change">Andrei Sakharov</a>, <a href="https://www.newyorker.com/magazine/1996/02/12/perfect-pitch-2">Joseph Brodsky</a>, and <a href="https://www.newyorker.com/books/page-turner/vaclav-havels-lessons-on-how-to-create-a-parallel-polis">Václav Havel</a>,” he wrote. “What was needed was the combination of philosopher and crowd.” The best place to recruit this crowd, he said, was on the internet—a shrewd intuition. Before long, links to Moldbug’s blog, “Unqualified Reservations,” were being passed around by libertarian techies, disgruntled bureaucrats, and self-styled rationalists—many of whom formed the shock troops of an online intellectual movement that came to be known as neo-reaction, or the Dark Enlightenment. While few turned into outright monarchists, their contempt for Obama-era uplift seemed to find voice in Moldbug’s heresies. In his most influential coinage, which quickly gained currency among the nascent alt-right, Moldbug urged his readers to rouse themselves from their ideological slumber by taking the “red pill,” like Keanu Reeves’s character in “The Matrix,” who chooses daunting truth over contented ignorance.</p><p>In 2013, an article on the news site <em>TechCrunch</em>, titled “Geeks for Monarchy,” revealed that Mencius Moldbug was the cyber alias of a forty-year-old programmer in San Francisco named Curtis Yarvin. At the same time that he was trying to redesign the U.S. government, Yarvin was also dreaming up a new computer operating system that he hoped would serve as a “digital republic.” He founded a company that he named Tlon, for the <a href="https://www.newyorker.com/magazine/1970/09/19/jorge-luis-borges-profile-autobiographical-notes">Borges</a> story “Tlön, Uqbar, Orbis Tertius,” in which a secret society describes an elaborate parallel world that begins to overtake reality. As he raised money for his startup, Yarvin became a kind of Machiavelli to his big-tech benefactors, who shared his view that the world would be better off if they were in charge. Tlon’s investors included the venture-capital firms Andreessen Horowitz and Founders Fund, the latter of which was started by the billionaire <a href="https://www.newyorker.com/news/letter-from-silicon-valley/what-is-it-about-peter-thiel">Peter Thiel</a>. Both Thiel and Balaji Srinivasan, then a general partner at Andreessen Horowitz, had become friends with Yarvin after reading his blog, though e-mails shared with me revealed that neither was thrilled to be publicly associated with him at the time. “How dangerous is it that we are being linked?” Thiel wrote to Yarvin in 2014. “One reassuring thought: one of our hidden advantages is that these people”—social-justice warriors—“wouldn’t believe in a conspiracy if it hit them over the head (this is perhaps the best measure of the decline of the Left). Linkages make them sound really crazy, and they kinda know it.”</p><p>A decade on, with the Trumpian right embracing strongman rule, Yarvin’s links to élites in Silicon Valley and Washington are no longer a secret. In a 2021 appearance on a far-right podcast, Vice-President J.&nbsp;D. Vance, a former employee of one of Thiel’s venture-capital firms, cited Yarvin when suggesting that a future Trump Administration “fire every single mid-level bureaucrat, every civil servant in the administrative state, replace them with our people,” and ignore the courts if they objected. Marc Andreessen, one of the heads of Andreessen Horowitz and an informal adviser to the so-called Department of Government Efficiency (<em>DOGE</em>), has started quoting his “good friend” Yarvin about the need for a founder-like figure to take charge of our “out of control” bureaucracy. Andrew Kloster, the new general counsel at the government’s Office of Personnel Management, has said that replacing civil servants with loyalists could help Trump defeat “the Cathedral.”</p><p>“There are figures who channel a Zeitgeist—Nietzsche calls them timely men—and Curtis is definitely a timely man,” a State Department official who has been reading Yarvin since the Moldbug era told me. Back in 2011, Yarvin said that Trump was one of two figures who seemed “biologically suited” to be an American monarch. (The other was Chris Christie.) In 2022, he recommended that Trump, if reëlected, appoint Elon Musk to run the executive branch. On a podcast with his friend Michael Anton, now the director of policy planning at the State Department, Yarvin argued that the institutions of civil society, such as Harvard, would need to be shut down. “The idea that you’re going to be a Caesar&nbsp;.&nbsp;.&nbsp;. with someone else’s Department of Reality in operation is just manifestly&nbsp;absurd,” he said.</p><p>In another timeline, Yarvin might have remained an obscure and ineffectual internet crank, a digital de Maistre. Instead, he has become one of America’s most influential illiberal thinkers, an engineer of the intellectual source code for the second Trump Administration. “Yarvin has pushed the Overton window,” Nikhil Pal Singh, a history professor at N.Y.U., told me. His work has revived ideas that once seemed outside the bounds of polite society, Singh said, and created a road map for the dismantling of “the administrative state and the global postwar order.”</p><p>As his ideas have been surrealized in <em>DOGE</em> and Trump has taken to self-identifying as a king, one might expect to find Yarvin in an exultant mood. In fact, he has spent the past few months fretting that the moment will go to waste. “If you have a Trump boner right now, enjoy it,” he wrote two days after the election. “It’s as hard as you’ll ever get.” What many see as the most dangerous assault on American democracy in the nation’s history Yarvin dismisses as woefully insufficient—a “vibes coup.” Without a full-blown autocratic takeover, he believes, a backlash is sure to follow. When I spoke to him recently, he quoted the words of Louis de Saint-Just, the French philosopher who championed the Reign of Terror: “He who makes half a revolution digs his own grave.”</p><p>Earlier this year, Yarvin and I had lunch in Washington, D.C., where he had come to celebrate the regime change. He was in his usual getup: bluejeans, Chelsea boots, a rumpled dress shirt under a motorcycle jacket. After taking a few bites of a cheeseburger topped with crispy onions, he pushed his plate away. Last year, he explained, he’d decided to start taking an Ozempic-like drug after a debate with the right-wing commentator Richard Hanania about the relative merits of monarchy and democracy. “I destroyed him in almost every way,” Yarvin said, nudging a tomato with his fork. “But he had one huge advantage, which was that I was fat and he was not.”</p><p>The injections seemed to be working. As I ate, Yarvin’s phone filled with messages, some of them complimenting his glow-up. That morning, the <em>Times Magazine</em> had published an interview with him, accompanied by a moody black-and-white portrait. Until recently, Yarvin, with his frazzled curtain of shoulder-length hair and ill-fitting wardrobe, had seemed indifferent to his appearance. Now, wearing his leather jacket, he glared out at the reader through stylishly tousled hair. His friend Steve Sailer, a writer for white-nationalist websites, said he looked like “the fifth Ramone.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61101&quot;}" href="https://www.newyorker.com/cartoon/a61101" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“How can a hunter-gatherer and a rock designer afford such a nice cave?”</span></p><p><span>Cartoon by Enrico Pinto</span></p></div></span></p></figure><p>In person, as in print, Yarvin expresses himself with imperious self-assurance. He is nearly impossible to interrupt. “When the rabbi is speaking, you let the rabbi speak,” Razib Khan, a right-wing science blogger and a close friend of Yarvin’s, told me. Even his friends and family, however, acknowledge that he has room to grow as a communicator. He talks in a halting monotone, rarely answers questions directly, and is prone to disorienting asides. In the middle of saying one thing, he is always getting distracted by something else he could be saying, like a G.P.S. that keeps suggesting faster routes.</p><p>Yarvin, for his part, was relieved at how the interview with the <em>Times</em> had gone. “My main goal was, how do I not damage any of my relationships?” he said. For years, Yarvin was best known, to the extent that he was known at all, as the court philosopher of the Thiel-verse, the network of heterodox entrepreneurs, intellectuals, and hangers-on surrounding the tech mogul. He mentioned that a businessman he knew had once complained to a journalist that Thiel had not invested enough money in his company. “That’s one strike and you’re out, and he was out,” Yarvin said, sighing theatrically. His second goal, he said, was to reach the <em>Times</em> audience. This seemed surprising: he has called for the government to shut down the paper. “I tend to be more interested in outreach to people who share my own cultural background,” Yarvin explained.</p><p>He likes to tell the story of his paternal grandparents, Jewish Communists from Brooklyn who met at a leftist gathering in the thirties. (He has less to say about his maternal grandparents, Tarrytown Wasps with a cottage on Nantucket.) “The vibe of American communism was ‘We’ve got thirty I.Q. points on these people, and we’re going to win,’&nbsp;” he said. “It’s like, what if all the gifted kids formed a political party and tried to take over the world?” Yarvin’s parents met at Brown, where his father, Herbert, was pursuing a Ph.D. in philosophy. After finishing school and failing to get tenure (“too arrogant,” Yarvin said), Herbert tried his hand at writing the Great American Novel, then joined the Foreign Service as a diplomat. In the following years, the family lived in the Dominican Republic and Cyprus. Herbert was cynical about working for the government, and Yarvin seems to have inherited his disdain: he has repeatedly proposed closing America’s embassies, a prospect the State Department is now considering in parts of Europe and Africa.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin is reticent on the subject of his childhood, but friends and family suggested to me that his father could be harsh, domineering, and impossible to please. “He controlled their life with an iron fist,” someone with close knowledge of the family told me. “It was absolutely his domain.” (Yarvin vehemently rejected this view, saying that people who are controlling tend to be insecure, “and that is very much <em>not</em> the way of my father.” Better words to describe him, he said, would be “stubborn,” “intense,” and “formidable”—like “a good manager.”)</p><p>Growing up, Yarvin was sometimes homeschooled by his mother, and skipped three grades. (His older brother, Norman, skipped four.) The family eventually moved to Columbia, Maryland, where Yarvin entered high school as a twelve-year-old sophomore. “When you’re much younger than your classmates, you’re either an adorable mascot or a weird, threatening, disturbing alien,” Yarvin said, adding that he was the latter. Yarvin was selected to participate in a Johns Hopkins study of math prodigies. He attended the university’s Center for Talented Youth, a summer camp for gifted children, and was a Baltimore-area champion on “It’s Academic,” a television trivia show. Andrew Cone, a software engineer who currently lives in a spare room in Yarvin’s home, told me that Yarvin’s childhood seems to have left him with a lifelong feeling of inadequacy. “I think he has this sense of being not good enough, that he’s seen as ridiculous or small, and that the only way out is to perform,” Cone said.</p><p>Yarvin went to Brown, graduated at eighteen, and then entered a Ph.D. program in computer science at the University of California, Berkeley. Former peers told me that he wore a bicycle helmet in class and seemed eager to show off his knowledge to the professor. “Oh, you mean helmet-head?” one said when I asked about Yarvin. The joke among some of his classmates was that the helmet prevented new ideas from penetrating his mind. He found more of a community on Usenet, a precursor to today’s online forums. But even in groups like talk.bizarre, where intellectual peacocking was the norm, he stood out for his desire to dominate. Along with posting jokes, advice, light verse, and “flames” (blistering takedowns of other users), he maintained a “kill file,” a list of members he had blocked because he found their posts uninteresting. “He wanted to be viewed as the smart guy—that was really, really important to him,” his first girlfriend, Meredith Tanner, told me. She was drawn to Yarvin after reading one of his virtuosic flames, and the pair dated for a few years. “Don’t get involved with someone just because you’re impressed by how creatively they insult people,” she warned. “They will turn that skill on you.”</p><p>Friends from Yarvin’s twenties described him as a reflexive contrarian who revelled in provocation. “He wasn’t a sweet kid, and he could sometimes be nasty,&nbsp;but he wasn’t Moldbug,” one said. Politically and culturally, Yarvin was a liberal—“a big old hippie,” as Tanner put it. He had a ponytail, wore a silver hoop earring, dropped acid at raves, and wrote poetry. Tanner recalled that when she once questioned the value of affirmative action in college admissions, it was Yarvin who convinced her of its necessity.</p><p>After a year and a half of doctoral work, Yarvin left academia to seek his fortune in the tech industry. He helped design an early version of a mobile web browser for a company that came to be known as Phone.com. In 2001, he began dating Jennifer Kollmer, a playwright he met on Craigslist, whom he later married and had two children with. Phone.com had gone public, leaving him with a windfall of a million dollars. He used some of the money to buy a condo near the Haight-Ashbury neighborhood of San Francisco and the rest to fund a self-directed study of computer science and political theory. “I was used to getting pats on the head for being smart,” he said of his decision to leave the <em>cursus honorum</em> of the gifted child. “Diverging from the pat-on-the-head economy was a strange and scary choice.”</p><p>Out in the wilderness, Yarvin delved into recondite history and economics texts, many of them newly accessible through Google Books. He read Thomas Carlyle, James Burnham, and Albert Jay Nock, alongside an early-aughts profusion of political blogs. Yarvin traces his own red-pill moment to the Presidential election of 2004. As many of his peers were being driven to the left by lies about weapons of mass destruction in Iraq, Yarvin was pulled in the opposite direction by fabrications of a different sort: the Swift Boat conspiracy theory pushed by veterans allied with the George&nbsp;W. Bush campaign, who claimed that the Democratic candidate, John Kerry, had lied about his service in Vietnam. It seemed obvious to Yarvin, who believed the accusations, that once the truth emerged Kerry would be forced to drop out of the race. When that didn’t happen, he began to question what else he’d naïvely taken on trust. Facts no longer felt stable. How could he be confident in what he’d been told about Joseph McCarthy, the Civil War, or global warming? What about democracy itself? After years of energetic debates in the comments sections of other people’s blogs, he decided to start his own. It did not lack for ambition. The first post began, “The other day I was tinkering around in my garage and I decided to build a new ideology.”</p><p>The German academic Hans-Hermann Hoppe is sometimes described as an intellectual gateway to the far right. A retired economics professor at the University of Nevada, Las Vegas, Hoppe argues that universal suffrage has supplanted rule by a “natural élite”; advocates for breaking nations into smaller, homogenous communities; and calls for communists, homosexuals, and others who oppose this rigid social order to be “physically removed.” (Some white nationalists have made memes pairing Hoppe’s face with a helicopter—an allusion to the Chilean dictator Augusto Pinochet’s practice of executing opponents by throwing them from aircraft.) Though Hoppe favors a minimal state, he believes that freedom is better preserved by monarchy than by democracy.</p><p>Yarvin nearly ended up a libertarian. As a Bay Area coder and a devotee of Austrian-school economists in his late twenties, he exhibited all the risk factors. Then he discovered Hoppe’s book “<a data-offer-url="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684&quot;}" href="https://www.amazon.com/Democracy-Economics-Politics-Perspectives-Democratic/dp/0765808684" rel="nofollow noopener" target="_blank">Democracy: The God That Failed</a>” (2001) and changed his mind. Yarvin soon adopted Hoppe’s imago of a benevolent strongman—someone who would govern efficiently, avoid senseless wars, and prioritize the well-being of his subjects. “It’s not copy-and-pasted, but it is such a direct influence that it’s kind of obscene,” Julian Waller, a scholar of authoritarianism at George Washington University, said. (Over e-mail, Hoppe recalled that he met Yarvin once at an exclusive gathering at Peter Thiel’s home, where Hoppe had been invited to speak. He acknowledged his influence on Yarvin, but added, “For my taste his writing has always been a bit too flowery and rambling.”) Hoppe argues that, unlike democratically elected officials, a monarch has a long-term incentive to safeguard his subjects and the state, because both belong to him. Anyone familiar with the history of dictatorships might find this idea disingenuous. Not Yarvin.</p><p>“You don’t ransack your own house,” he told me one afternoon, at an open-air café in Venice Beach. I’d asked him what would stop his C.E.O.-monarch from plundering the country—or enslaving his people—for personal gain. “For Louis XIV, when he says, ‘<em>L’état, c’est moi</em>,’ ransacking the state holds no meaning because it’s all his anyway.” Following Hoppe, Yarvin proposes that nations should eventually be broken up into a “patchwork” of statelets, like Singapore or Dubai, each with its own sovereign ruler. The eternal political problems of legitimacy, accountability, and succession would be solved by a secret board with the power to select and recall the otherwise all-powerful C.E.O. of each sovereign corporation, or SovCorp. (How the board itself would be selected is unclear, but Yarvin has suggested that airline pilots—“a fraternity of intelligent, practical, and careful people who are already trusted on a regular basis with the lives of others. What’s not to like?”—could manage the transition between regimes.) To prevent a C.E.O. from staging a military coup, the board members would have access to cryptographic keys that would allow them to disarm all government weapons, from nuclear missiles down to small arms, with the push of a button.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Mass political participation would cease, and the only way that people could vote would be with their feet, by moving from one SovCorp to another if they became dissatisfied with the terms of service, like switching from X to Bluesky. The irony that dissenters like Yarvin would probably be repressed in such a state appears not to concern him. In his imagined polity, he insists, there would still be freedom of speech. “You can think, say, or write whatever you want,” he has promised. “Because the state has no reason to care.”</p><p>Yarvin’s congenital cynicism about governance disappears as soon as he starts talking about dictatorial regimes. He has kind words for El Salvador’s strongman, <a href="https://www.newyorker.com/magazine/2022/09/12/the-rise-of-nayib-bukele-el-salvadors-authoritarian-president">Nayib Bukele</a>, and has encouraged Trump to let Putin end the liberal order “not just in Russian-speaking territories—but all the way to the English Channel.” Picking at a plate of fried calamari, Yarvin praised China and Rwanda (neither of which he has visited) for having strong governments that insured both public safety and personal liberty. In China, he told me, “you can think and pretty much say whatever you want.” He may have sensed my skepticism, given the country’s record of imprisoning critics and detaining ethnic minorities in concentration camps. “If you want to organize against the government, you’re gonna have problems,” he admitted. Then he returned to his airbrush: “Not Stalin problems. You’ll just, like, be cancelled.”</p><p>For certain people, like meth addicts or four-year-olds, Yarvin said, too much freedom could be deadly. Then, gesturing to the homeless population camped in the neighborhood, he suddenly began to cry. “The idea that this represents success, or this represents the ‘worst of all systems, except for all the others’&nbsp;”—he was referencing Churchill’s famous comment about democracy, which I’d paraphrased earlier—“is highly delusional,” he said, wiping away the tears. (A few weeks later, on a trip to London, I watched him break down while giving a similar speech to a member of the House of Lords. It was less affecting the second time around.)</p><p>Presumably, Yarvin’s monarch would act decisively to safeguard his wards. At the Venice café, Yarvin lauded the Delancey Street Foundation, a nonprofit rehab organization, whose strict program he has characterized as exerting “fascist-parent-level control.” Some of his own proposals go further. On his blog, he once joked about converting San Francisco’s underclasses into biodiesel to power the city’s buses. Then he suggested another idea: putting them in solitary confinement, hooked up to a virtual-reality interface. Whatever the exact solution, he has written, it is crucial to find “a humane alternative to genocide,” an outcome that “achieves the same result as mass murder (the removal of undesirable elements from society) but without any of the moral stigma.”</p><p>Yarvin’s call for an American strongman is often treated as an eccentric provocation. In fact, he considers it the only answer to a world in which most people are unfit for democracy. An “African country today,” he told me, has “enough smart people in the country to run it—you just don’t have enough smart people to have a democratic election in which everyone is smart.” Because of such remarks, Yarvin is sometimes identified as a white nationalist, a label he delicately resists. In a 2007 blog post titled “Why I Am Not a White Nationalist,” he explained that, though he is “not exactly allergic to the stuff,” he finds both whiteness and nationalism to be unhelpful political concepts. During lunch, he told me that he feels a rueful sympathy for the bigots of the past, who had some of the right intuitions but lacked the proper science. Neo-reactionaries tend to subscribe to what they call “human biodiversity,” a set of fringe beliefs which holds, among other things, that not all racial or population groups are equally intelligent. As Yarvin came to see it from his online research, these genetic differences contributed to (and, conveniently, helped explain away) demographic differences in poverty, crime, and educational attainment. “In this house, we believe in science—<em>race</em> science,” he wrote last year.</p><p>For several hours, Yarvin shuffled through his pitches for strongman rule, like an auctioneer desperate to clinch a sale. I listened patiently, though I was often puzzled by his factual distortions and peculiar asides. “What is the right policy in a completely new-from-scratch regime for African Americans?” he wondered aloud at one point. At first, this seemed like a non sequitur: I’d been pressing him on how he would define success in the second Trump Administration. Answering himself, he said that the “obvious solution” to problems of inner-city drug abuse and poverty would be to “put the church Blacks in charge of the ghetto Blacks.” Yarvin, who is an atheist, is not particularly interested in theocratic rule, but he advocates creating different legal codes to govern different populations. (He has cited the Ottoman <em>millet</em> system, which granted religious communities a measure of autonomy.) To keep the “ghetto Blacks” in line, he went on, they should be forced to live in a “traditional way,” like Orthodox Jews or the Amish. “The approach that the twentieth century took is, if we could just make the schools good enough, they would all turn into Unitarians,” he said. “If you’ve seen ‘The Wire’ and lived in Baltimore, both of which I have, that does not seem to work at all.” It wasn’t until he reached the end of his speech, ten minutes later, that I realized he was, in his own way, addressing my initial question. “Unless we can totally reëngineer DNA to change what a human being is, there are many people who should not live in a modern way but in a traditional way,” he concluded. “And <em>that</em> is a level of revolution that is so far beyond anything the Trump-Vance regime is doing.”</p><p>Yarvin is not known for his discretion. He has a habit of sharing private correspondence, as I discovered when he started sending me unsolicited screenshots of text messages and e-mails he’d exchanged with his wife, his friends, a fact checker at the <em>Times Magazine</em>, and someone nominated to the new Administration. He seemed troubled by the thought that the wit and wisdom they contained might be lost to posterity. He was more guarded about his friendship with Thiel, but he did mention a conversation they’d privately filmed together last year and boasted about a fortieth-birthday gift he’d received from the billionaire: Francis Neilson’s “The Tragedy of Europe,” a contemporaneous commentary on the Second World War, though not the first edition that Yarvin had been hoping for.</p><p>Thiel has always had a prophetic touch. He co-founded PayPal, became the first outside investor in Facebook, and created Palantir, a data-mining firm that has just received a new contract to help Immigration and Customs Enforcement officers carry out deportations. Thiel supported Trump back when doing so still made one a pariah in Silicon Valley. In 2022, he donated fifteen million dollars to J.&nbsp;D. Vance’s Senate campaign, the largest amount given to a single candidate in congressional history. A longtime libertarian, Thiel appears to have taken a Yarvinian turn around 2009, when, in a widely quoted essay published online by the Cato Institute, he wrote, “I no longer believe that freedom and democracy are compatible.” Yarvin linked to it approvingly in a blog post titled “Democraphobia Goes (Slightly) Viral.” They soon met for the first time, at Thiel’s house in San Francisco, and, according to private messages I reviewed, struck up a confiding correspondence. Yarvin’s e-mails were long and homiletic, full of precepts gleaned from pickup-artist blogs; Thiel’s were straightforward and concise. Both men seemed to take for granted that America was a communist country, that journalists acted like the Stasi, and that tech C.E.O.s were their prey.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>In the fall of 2014, Thiel published “Zero to One,” a best-selling treatise on startups, with Blake Masters, his employee and a longtime Moldbug fan. Before the book tour, Thiel asked Yarvin for advice on fielding questions he might get on how to steer more women into tech. The premise appeared to strike them both as misguided, since women, in their view, were less likely to have men’s aptitude for computer science. As Yarvin put it in one e-mail, “There’s simply no way short of becoming a farce for Google, YC”—Y Combinator, the startup accelerator—“etc, etc, to ‘look like America.’&nbsp;” Yarvin suggested that Thiel deploy a pickup-artist tactic called “agree and amplify”—that is, ask a journalist, who probably had no solution in mind, what she would do to tackle the problem. “The purpose here is not to get the interlocutor to sleep with you, but to get her to fear this issue and run away from it—and ditto for future interviewers,” he wrote. Once, at a dinner, Thiel quizzed Yarvin on how one might go about taking down <em>Gawker</em>. (As it turned out, Thiel had already decided to secretly bankroll Hulk Hogan’s defamation lawsuit against the online publication, which eventually bankrupted it, in 2016.) In e-mails obtained by <em>BuzzFeed</em>, Yarvin bragged to Milo Yiannopoulos, the <em>Breitbart</em> editor, that he’d watched Trump’s first election at Thiel’s house and had been “coaching” him. “Peter needs guidance on politics for sure,” Yiannopoulos replied. Yarvin wrote back, “Less than you might think!&nbsp;.&nbsp;.&nbsp;. He’s fully enlightened, just plays it very carefully.”</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a60750&quot;}" href="https://www.newyorker.com/cartoon/a60750" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>“Slowest to hit Skip has to watch the whole ad.”</span></p><p><span>Cartoon by Juan Astasio and Colin Mills</span></p></div></span></p></figure><p>When I recently visited Yarvin’s Craftsman home, in Berkeley, I noticed a painting that Thiel had given him: a portrait of Yarvin in the style of a role-playing-game character card, bearing the legend “Philosopher.” As I sipped tea from a novelty mug featuring an image of Yarvin with a cartoon crown, he told me that it would be “cringe” for him to broadcast his relationship with Thiel—or with Vance, for that matter, whom he met through Thiel around 2015. “Does a normal Ohio voter read&nbsp;.&nbsp;.&nbsp;. Mencius Moldbug? No,” Vance reportedly said one night at a bar during the 2021 National Conservatism Conference. “But do they agree with the broad thrust of where we think American public policy should go? Absolutely.” “He’s a really cool guy,” Yarvin said of the Vice-President, who followed him on X earlier this year. (The White House did not respond to requests for comment.)</p><p>Although Yarvin tried to be discreet, he mentioned that Thiel has a bit of a “weirdo edge” and described Andreessen, the venture capitalist, as someone who, “apart from the bizarre and possibly even nonhuman shape of his head, would seem much more normal than Peter.” After Andreessen invested in Yarvin’s startup, Tlon, the two got to know each other; they texted and went to brunch long before Andreessen came out as a Trump supporter, last year. Andreessen has been known to urge his associates to read Yarvin’s blog. “Tech people are not interested in appeals to virtue or beauty or tradition, like most conservatives,” the State Department official said. “They are more like right-wing progressives, and for a long time Moldbug was the only person speaking to them this way.” (Andreessen and Thiel declined to comment.) Apropos of his relationships with powerful men, Yarvin paraphrased to me “a wonderful piece of advice for courtiers” that he’d picked up from Lord Chesterfield’s “Letters to His Son,” an eighteenth-century etiquette manual addressed to the author’s illegitimate child: “Never bug them. And never let them forget you exist.”</p><p>Yarvin has had more success as a courtier to startup founders than as a founder himself. He launched Tlon in 2013, with a twentysomething former Thiel fellow. Yarvin approached computer science the same way he approached the U.S. government—with, as he put it, “utopian megalomania.” Yarvin’s visionary goal was to build a peer-to-peer computer network, named Urbit, that would allow users to control their own data, free from scolds, spies, and monopolies. Each user on the Urbit network is identified with an N.F.T. that acts like a digital passport. Even though Urbit promotes decentralization, the system is designed around a hierarchical model of virtual real estate, with users owning “planets,” “stars,” or “galaxies.”</p><p>In an early sketch of the system, Yarvin named himself its “prince,” but he struggled to attract subjects to his imaginary kingdom. Like Yarvin’s political theory, his programming language, which he wrote himself, was daring, abstruse, and sometimes mistaken for a hoax. Ever the contrarian, he reversed the meaning of zeros and ones. After decades of work and an estimated thirty million dollars of investment, Urbit seems to function less like a feudal society and more like the Usenet forums of Yarvin’s youth. (The trade publication <em>CoinDesk</em> has called it “a slower version of AOL Instant Messenger.”) “It doesn’t work the way it’s supposed to,” a former Urbit employee told me, describing Yarvin as “the world’s first computer-science crank.” Yarvin left the company in 2019.</p><p>No longer needing to worry about spooking investors, Yarvin threw himself into the life style of a self-described “rogue intellectual.” Under his own name, he launched a Substack newsletter, “Gray Mirror of the Nihilist Prince.” (Today, it is the platform’s third most popular “history” publication.) He became a fixture on the right-wing podcast circuit and seemed never to turn down an invitation to party. On his travels, he often hosted “office hours”—informal, freewheeling discussions with readers, many of them thoughtful young men, alienated by liberal guilt and groupthink. What wins Yarvin converts is less the soundness of his arguments than the transgressive energy they exude: he makes his listeners feel that he is granting them access to forbidden knowledge—about racial hierarchy, historical conspiracies, and the perfidy of democratic rule—that progressive culture is at pains to suppress. His approach seizes on the reality that most Americans have never learned how to defend democracy; they were simply brought up to believe in it.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin advises his followers to avoid culture-war battles over issues like D.E.I. and abortion. It is wiser, he argues, to let the democratic system collapse on its own. In the meantime, dissidents should focus on becoming “fashionable” by building a reactionary subculture—a counter-Cathedral. Sam Kriss, a left-wing writer who has debated Yarvin, said of his work, “It flatters people who believe they can change the world simply by having weird ideas on the Internet and decadent parties in Manhattan.”</p><p>Such people have come to be known as the “dissident right,” a loose constellation of artists and strivers clustered around the Bay Area, Miami, and the Lower East Side micro-neighborhood Dimes Square. The milieu was drawn together by a frustration with electoral politics, <em>Covid</em> lockdowns, and the strictures of “wokeness.” Vice signalling has been central to the scene’s countercultural allure: instead of sharing pronouns and employing the approved nomenclature (“unhoused,” “Latinx,” “justice-involved person”), its members have revived insults like “gay” and “retarded.” Dasha Nekrasova and Anna Khachiyan, the hosts of the “Red Scare” podcast, are among the most prominent avatars of the scene. In 2021, Thiel helped to fund an anti-woke film festival in New York, and Yarvin read his poetry at one of its packed events. Urbit now hosts a literary magazine designed to look like <em>The New York Review of Books</em>. “If you are an intelligent Jewish-American urbanite who wants to play around with certain Nietzschean and eugenic themes, you aren’t going to join tiki-torch-bearing marchers chanting that ‘the Jews will not replace us,’&nbsp;” the conservative commentator Sohrab Ahmari observed in an essay last year. “No, you turn to the dissident right.”</p><p>Yarvin has emerged as a veteran edgelord of this crowd, which he compared to San Francisco’s gay subculture in the seventies and to the Lost Generation of literary modernists—tight-knit communities whose members bonded over their sense of being outsiders. James Joyce, he said, sold few copies of “Ulysses,” but his friends, like Ezra Pound and T.&nbsp;S. Eliot, “knew that what he was doing was good.” So it was with the creatives of the dissident right, whose endeavors, he felt, had been overlooked by the intolerant Cathedral. This past April, Yarvin pitched Darren Beattie, the acting Under-Secretary of State for Public Diplomacy, on a plan for “dissident-right art hos” to take over the American pavilion at the Venice Biennale.</p><p>Lately, Yarvin has been trying to flip some of his newly acquired cultural capital into the real thing. Last year, he returned to Urbit as a “wartime C.E.O.,” after which several top employees resigned, and in February he raised more money from Andreessen Horowitz. According to a draft of an unpublished Substack post, his newest plan is to promote Urbit as an élite private club whose members, he believes, are destined to become “the stars of the new public sphere—a new Usenet, a new digital Athens built to last forever.”</p><p>The night before Trump’s Inauguration, I drove Yarvin to a black-tie “Coronation Ball” at the Watergate Hotel, in Washington, D.C. The event was organized by a neo-reactionary publishing house, Passage Press, which recently released Yarvin’s book “<a data-offer-url="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P&quot;}" href="https://www.amazon.com/Gray-Mirror-Fascicle-I-Disturbance-ebook/dp/B0DV36SK5P" rel="nofollow noopener" target="_blank">Gray Mirror, Fascicle I: Disturbance</a>,” the first of a planned four-part cycle outlining his vision for a new political regime. Its endnotes predominantly consist of QR-code links to Wikipedia pages: “Denazification,” “L’État, c’est moi,” “Presentism (historical analysis).” As I negotiated the icy streets, Yarvin explained that during the Elizabethan era the finest minds in the arts and sciences were to be found at court. When I asked if he saw a parallel with Trump’s inner circle, he burst out laughing. “Oh, no,” he said. “My God.”</p><p>Like most journalists, I had been denied entry to the ball, so I ordered a drink at a bar in the lobby. Standing next to me was a man wearing a cowboy hat and a burgundy velour suit—a Yarvin enthusiast, it turned out, named Alex Maxa. He ran a party-bus company in San Francisco, and in his free time he made memes featuring Yarvin’s likeness. He said that he was drawn to Yarvin’s work because “it makes me feel like I’ve got something that people in Washington who think they’re really smart can’t actually make a compelling argument against.” He’d wanted to go to the ball but tickets, whose price had surged to twenty thousand dollars, were now sold out. Not long afterward, I met two of Yarvin’s friends, who encouraged me, and another journalist I was with, to confidently walk into the party with them. Maxa was already inside, having taken a similar approach. “Lol I just waltzed right in by asking where the coat check was,” he texted.</p><p>Passage Press had billed the event as “<em>MAGA</em> meets the Tech Right.” It was not false advertising. In a banquet hall awash in pink and purple light, Anton, from the State Department, Laura Loomer, a Trump whisperer known for her anti-Muslim bigotry, and Jack Posobiec, who popularized the Pizzagate conspiracy theory, mingled with venture capitalists, crypto accelerationists, and Substack all-stars. Earlier that evening, as guests dined on seared scallops and filet mignon, Steve Bannon, the ball’s keynote speaker, called for mass deportations, the “Götterdämmerung” of the administrative state, and Mark Zuckerberg’s imprisonment.</p><p>Eight years ago, Mike Cernovich, a first-gen alt-right influencer, had co-hosted an inaugural party known as the DeploraBall, a winking reference to Hillary Clinton’s unfortunate crack about half of Trump’s supporters belonging in a “basket of deplorables.” It was, by all accounts, a shambolic affair, plagued by journalists and protesters. One of Cernovich’s co-organizers, Tim Gionet, who goes by the online pseudonym Baked Alaska, was removed from his role after posting antisemitic content on Twitter. Now, at the Coronation Ball, Baked Alaska was served for dessert—a nod, it seemed, to Gionet, who was then on probation for participating in the January 6th insurrection. (He was pardoned by Trump the next day.) Cernovich pushed a baby around in a stroller and marvelled, like a proud father, at how&nbsp;far the movement had come. “I was one of the oldest guys in the place!” he tweeted the following afternoon. “Real right wing. High energy and high IQ.” In 2008, Yarvin, in his “Open Letter,” had called for a reactionary vanguard to form an underground political party. The Coronation Ball made it clear that this was no longer necessary. His web-addled counter-élite was now the establishment.</p><p>Yarvin was dressed in the same tuxedo, including a bright-red cummerbund, that he’d worn to a party at Thiel’s house in D.C. the night before, where, as <em>Politico</em> reported, Vance had amiably greeted him with “You reactionary fascist!” He’d also worn the tux to his wedding last year. Yarvin’s first wife died in 2021, from a hereditary heart disease, at the age of fifty. At the ball, he was accompanied by his second wife, Kristine Militello. A former Bernie Sanders supporter and an aspiring novelist, Kristine described herself as having been “red-pilled” during the pandemic, after losing her customer-service job at an online wine retailer. She first encountered Yarvin on YouTube, where she watched a video of him arguing against the legitimacy of the American Revolution, and proceeded to read everything he’d written. She sent him an admiring e-mail in 2022, seeking advice on how to break into New York’s dissident-right literary scene, and they met for drinks a few weeks later.</p><p>Recently, Yarvin has taken to describing himself as a “dark elf” whose role is to seduce “high elves”—blue-state élites—by planting “acorns of dark doubt in their high golden minds.” (In this Tolkien-inspired metaphor, red-state conservatives are “hobbits” who should submit to the “absolute power” of a new ruling class made up, unsurprisingly, of dark elves.) He didn’t always express himself so quaintly. In 2011, the day after the far-right terrorist Anders Behring Breivik killed sixty-nine people, many of them teen-agers, at a summer camp in Norway, Yarvin wrote, “If you’re going to change Norway into something new, you need the present ruling class of Norway to <em>join</em> and <em>follow</em> you. Or at least, you’ll need their children.” He praised Breivik for targeting the right group (“communists, not Muslims”), but condemned his methods: “Rape is beta. Seduction is alpha. Don’t slaughter the youth camp—<em>recruit</em> the youth camp.”</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Yarvin’s own recruitment efforts seemed to be working. Near the open bar, I spoke to Stevie Miller, a sprightly sophomore at Carnegie Mellon who has been reading Yarvin since the seventh grade. (Yarvin told me that he’d encountered several gifted Zoomers who’d read him as preteens because his “high-I.Q. style” served as a “high-I.Q. magnet.”) Two years ago, Miller hung out with Yarvin at Vibecamp, a gathering for nerds and techies in rural Maryland. Yarvin, who left early, asked Miller to help him throw his own party in D.C., which came to be known as Vibekampf. Afterward, Miller became Yarvin’s first personal intern. “My parents, New York Jewish liberals who I love, were totally mystified,” he said.</p><p>After half an hour, I was escorted out of the party, as were other reporters throughout the evening. Security mistook Maxa, my friend from the lobby, for one of our kind, and he was ejected, too, though not before pressing through the crowd to get his photo taken with the dark elf.</p><p>Even Trump’s most pessimistic critics have been startled by the speed with which the President, in his second term, has moved to impose autocracy on America, concentrating power in the executive branch—and often enough in the hands of the richest men on earth. Elon Musk, an unelected citizen, has led a squadron of twentysomethings on a spree through the federal government, laying off tens of thousands of civil servants, shuttering the U.S. Agency for International Development, and seizing control of the Treasury Department’s payment system. Meanwhile, the Administration has launched an assault on civil society, revoking funding at Harvard and other universities that it claims are bastions of ideological indoctrination and punishing law firms that have represented Trump’s opponents. It has expanded the machinery of immigration enforcement, deporting three U.S.-born children to Honduras, a group of Asian and Latin American immigrants to Africa, and more than two hundred Venezuelan migrants to a maximum-security prison in El Salvador, where they may remain until the end of their lives. U.S. citizens now find themselves with a government that claims the right to disappear them without due process: as Trump told Bukele, the President of El Salvador, during an Oval Office meeting, “Homegrowns are next.” Without a vigorous system of checks and balances, one man’s crank ideas—like starting an incoherent trade war that upends the global economy—don’t get filtered out. They become policies that enrich his family and his allies.</p><p>Since January, a cottage industry has arisen online to trace links between the government’s chaotic blitz of actions and Yarvin’s writings. Yarvin is hardly the Rasputin-like figure with Oval Office access that certain Bluesky users imagine him to be, but it isn’t difficult to see why some people may have come to this view. Last month, an anonymous <em>DOGE</em> adviser told the Washington <em>Post</em> that it was “an open secret that everyone in policymaking roles has read Yarvin.” Stephen Miller, the President’s deputy chief of staff, recently quote-tweeted him. Vance has called for the U.S. to retrench from Europe, a longtime Yarvin desideratum. Last spring, Yarvin proposed expelling all Palestinians from the Gaza Strip and turning it into a luxury resort. “Did I hear someone say ‘beachfront?’&nbsp;” he wrote on Substack. “The new Gaza—developed, of course, by Jared Kushner—is the LA of the Mediterranean, an entirely new charter city on humanity’s oldest ocean, sublime real estate with an absolutely perfect, Apple-quality government.” This February, during a joint press conference with Benjamin Netanyahu, the Israeli Prime Minister, Trump surprised his advisers when he made a nearly identical proposal, describing his redeveloped Gaza as “the Riviera of the Middle East.”</p><p>Whenever I asked Yarvin about resonances between his writing and real-world events, his response was nonchalant. He seemed to see himself as a conduit for pure reason—the only mystery, to him, was why it had taken others so long to catch up. “You can invent a lie, but you can only discover the truth,” he told me. We were in London, where he was attending the Alliance for Responsible Citizenship, a conservative conference co-founded by the psychologist Jordan Peterson. (Yarvin described Peterson to me as “a dandy” with “a weird narcissistic energy coming off of him.”) Accompanying Yarvin on his travels were Eduardo Giralt Brun and Alonso Esquinca Díaz, two millennial filmmakers who were shooting a documentary about his life. Their goal was to make a naturalistic character study in the style of “Grey Gardens,” in which, as Brun put it, “the camera just happens to be around.” It wasn’t going to plan. Yarvin kept repeating the same monologues, which meant that much of the footage was the same. The filmmakers worried that his racist remarks would turn viewers off. One afternoon in London, Díaz had filmed Yarvin getting his portrait painted with Lord Maurice Glasman, a post-liberal political theorist who has been called “Labour’s <em>MAGA</em> Lord,” for his support of Brexit and his ongoing dialogue with figures like Steve Bannon. At one point in their discussion, Yarvin had pulled out his iPhone to show Glasman that he’d hacked the chatbot Claude to get it to call him by the N-word.</p><p>Some thinkers would envy the attention Yarvin is receiving. But he dismissed his influence as a “fraudulent currency” since it has yet to cash out in the revolution he desires. He poured scorn on <em>DOGE</em> (“so much libertarian DNA”) and Trump’s tariff plan (not mercantilist enough). In a recent essay on Substack, he criticized the decision to dispatch plainclothes <em>ICE</em> officers to jail college students and professors for political speech—not on moral grounds, but because the thuggish optics were likely to provoke resistance. Yarvin’s oracular pronouncements and bottomless disdain for actually existing politics have inspired a viral post: his face under the words “Your anti-regime actions work well in practice. But do they work in theory?” The conservative activist Christopher Rufo has compared Yarvin to “a sullen teenager who insists that everything is pointless.” I came to think of him as a reactionary Goldilocks who would be satisfied with nothing less than the inch-perfect autocracy that he’d constructed in his mind.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a61050&quot;}" href="https://www.newyorker.com/cartoon/a61050" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by Vi-An Nguyen</span></p></div></span></p></figure><p>This apparent desire for control also shows up in some of his relationships. Not long ago, I visited Lydia Laurenson, Yarvin’s ex-fiancée, in Berkeley. The two began dating in September, 2021, after Yarvin posted a personal ad on Substack, explaining that he’d recently lost his “widower virginity” and was looking to meet someone of “childbearing age.” Laurenson, a freelance writer and editor, replied the same day: “I have historically been a liberal but my IQ is really high, I want kids, and I’m incredibly curious to talk to you.” Yarvin went on Zoom dates with other women who answered the post—among them, Caroline Ellison, the ex-girlfriend of the now imprisoned crypto entrepreneur Sam Bankman-Fried—but he and Laurenson soon found themselves in an all-consuming romance. She told me that the ethos of her relationship with Yarvin was “&nbsp;‘We’re going to be geniuses together and have genius babies.’ I’m making fun of it a little bit, but that really was it.”</p><p>Like Yarvin, Laurenson had been a precocious child who went to college early. She’d also maintained a blog with a cult following, where, under the pseudonym Clarisse Thorn, she wrote about sex-positive feminism, B.D.S.M., and pickup artistry. She and Yarvin fought often, sometimes about politics. Laurenson had moved away from the left, but she hadn’t fully embraced neo-reaction. When I asked her if she’d ever changed Yarvin’s mind about anything, she said she’d gotten him to stop using the N-word, at least around her. (He later told this magazine that he was not using the word in the spirit of “a Southern plantation owner.”)</p><p>The bigger source of tension, according to Laurenson, was Yarvin’s autocratic attachment style. When they fought, Laurenson said, he insisted that she provide a rational justification for ending hostilities. She felt that Yarvin’s slippery personal attacks resembled his manner in public debates. “He makes up explanations that seem reasonable, but are actually false; he attacks the character of the person who is trying to point out what he’s doing; it’s like a DDOS attack of the soul,” she told me in an e-mail, referencing the cyberattack strategy of overwhelming a server with traffic from multiple sources. James Dama, a friend of Laurenson’s who had his own falling out with Yarvin, recalled, “He would make a coarse joke about Lydia’s weight or looks, not get a laugh, and then get angry at Lydia for being too stuck up.” (Tanner, Yarvin’s first girlfriend, described a similar pattern of insults and demands.)</p><p>Laurenson and Yarvin broke up in the summer of 2022, while Laurenson was pregnant. He told me that his desire for closeness might have struck Laurenson as “overbearing and stifling,” and that he had a bad habit of making “a joke that’s sort of a barb,” but he denied that he was ever purposefully cruel during the relationship. (He added that, after the relationship ended, “my natural instinct was, I’m going to cut her down to size every time I can”—something, he noted, he was “very good at.”) A few weeks after their son was born, that December, Yarvin sued for partial custody, which he received. An ongoing family-court case remains acrimonious. “The parents are in disagreement about nearly every issue,” their mediator observed last year.</p><p>Now that they share a toddler, Laurenson spends a lot of time thinking about Yarvin’s own childhood. “He has this class-clown thing going on, where he very much craves attention,” she said. To her, it seemed that his embrace of a provocative ideology was a kind of “repetition compulsion,” a psychological defense that allowed him to reframe the ostracization he experienced growing up. As America’s most famous living monarchist, he could tell himself that people were rejecting him for his outré ideas, not for his personality. She wondered if he’d first adopted “the monarchist thing” as a kind of intellectual sport, a bit from Usenet, and then, like the parallel world in the Borges story, it had slowly taken on a reality of its own. “Is it just like you found this place where people admire you and allow you to troll as much as you want, and then you just live in that world?” she asked.</p><p>In the past decade, liberalism has taken a beating from both sides of the political spectrum. Its critics to the left view its measured gradualism as incommensurate to the present’s multiple emergencies: climate change, inequality, the rise of an ethno-nationalist right. Conservatives, by contrast, paint liberalism as a cultural leviathan that has trampled traditional values underfoot. In “<a data-offer-url="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447&quot;}" href="https://www.amazon.com/Why-Liberalism-Failed-Politics-Culture/dp/0300223447" rel="nofollow noopener" target="_blank">Why Liberalism Failed</a>” (2018), the Notre Dame political scientist Patrick Deneen argues that the contemporary American emphasis on individual freedom has come at the expense of family, faith, and community, turning us into “increasingly separate, autonomous, non-relational selves replete with rights and defined by our liberty, but insecure, powerless, afraid, and alone.” Other post-liberal theorists, including Adrian Vermeule, have proposed that the state curtail certain rights in the service of an explicitly Catholic “common good.”</p><p>Yarvin is calling for something simpler and more libidinally satisfying: to burn it all down and start again from scratch. Since the advent of neoliberalism in the late seventies, political leaders have increasingly treated governance like corporate management, turning citizens into customers and privatizing services. The result has been greater inequality, a weakened social safety net, and the widespread perception that democracy itself is to blame for these ills, creating an appetite for exactly the kind of autocratic efficiency Yarvin now extolls. “A Yarvin program might seem seductive during a period of neoliberal rule, where efforts to change things, whether it is global warming or the war machine, feel futile,” the historian Suzanne Schneider told me. “You can sit back, not give a fuck, and let someone else run the show.” Yarvin has little to say on the question of human flourishing, or about humans in general, who appear in his work as sheep to be herded, idiots to be corrected, or marionettes controlled by leftist puppeteers.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>Whatever gift Yarvin has for attracting attention, his work does not survive scrutiny. It is full of spurious syllogisms and arguments retconned to match his jaundiced intuitions. He has read widely, but he uses his knowledge merely as grist for the same reactionary fairy tale: once upon a time, people knew their place and lived in harmony; then along came the Enlightenment, with its “noble lie” of egalitarianism, plunging the world into disorder. Yarvin often criticizes academics for treating history like a Marvel movie, with oversimplified heroes and villains, but it’s unclear what he adds to the picture by calling Napoleon a “startup guy.” (He has favored the revisionist theories that Shakespeare’s plays were really written by the seventeenth Earl of Oxford and that the American Civil War, which he calls the War of Secession, worsened living conditions for Black Americans.) “The neat thing about primary sources is that often, it takes only one to prove your point,” he has proclaimed, which would come as news to historians.</p><p>Some of his most thoroughgoing critics are on the right. Rufo, the conservative activist, has written that Yarvin is a “sophist” whose debating style consists of “childish insults, bouts of paranoia, heavy italics, pointless digressions, competitive bibliography, and allusions to cartoons.” He added, “When one tries to locate what it is that you actually think, he cannot help but discover that there really isn’t much substance there.” The most generous engagement with Yarvin’s ideas has come from bloggers associated with the rationalist movement, which prides itself on weighing evidence for even seemingly far-fetched claims. Their formidable patience, however, has also worn thin. “He never addressed me as an equal, only as a brainwashed person,” Scott Aaronson, an eminent computer scientist, said of their conversations. “He seemed to think that if he just gave me one more reading assignment about happy slaves singing or one more monologue about F.D.R., I’d finally see the light.”</p><p>Intellectual seriousness may not be the point. Yarvin’s polemics have proved useful for those on the right in search of a rationale for nerd ressentiment and plutocratic will to power. “The guy does not have a coherent theory of the case,” the Democratic senator Chris Murphy, from Connecticut, told me. “He just happens to be saying something out loud that a lot of Republicans are eager to hear.”</p><p>It is not difficult to anticipate the totalitarian endgame of a world view that marries power worship with a contempt for human dignity—fascism, as some might call it. Like his ideological nemeses the Bolsheviks, Yarvin seems to believe that the only thing standing in the way of Utopia is an unwillingness to use every means possible to achieve it. He claims that the transition to his regime will be peaceful, even joyous, but fantasies of violence flicker throughout his work. “Unless the monarch is ready to actually <em>genocide</em> the nobility or the masses, he has to capture their loyalty,” he wrote in a Substack post in March. “You’re not going to <em>foam</em> these people, like turkeys with bird flu. Right?”</p><p>Yarvin’s strong opinions on how the world ought to work extended to this profile. Some of his suggestions were intriguing: he floated the idea of staging a debate with one of his ex-girlfriends, and invited me to follow him to Doha for a meeting with Omar bin Laden, one of Osama’s sons. Others were officious. At one point, he sent me nine texts objecting to my use of the word “extreme”—“a hostile pejorative,” he explained, which my article would be better off without. (He’d previously boasted several times in our taped conversations that he was more “extreme” than anyone in the current Administration.) A few days after the Coronation Ball at the Watergate Hotel, he wrote to <em>The New Yorker</em> to complain that I’d walked in without his publisher’s permission; he said that he hoped the incident would not turn into “Watergate 2,” and referred to himself as “certainly the most media-friendly person in the scene!” (Jonathan Keeperman, his publisher at Passage Press and the host of the ball, once suggested that the Republican Party should “lamppost”—that is, lynch—“the journos,” so this was not a particularly high bar to clear.)</p><p>One morning this winter, I woke up to twenty-eight texts from Yarvin expressing concerns about my reporting technique. “The problem is that your process is slack and I can feel it generating low-quality content—because it’s not adversarial enough,” he wrote. “When the process is not adversarial, I don’t know what I am contending against.” He briefly considered whether I was “too dumb to understand the ideas,” or whether I’d succumbed to the mental self-censorship that Orwell called “crimestop.” He urged me to watch “The Lives of Others,” an Oscar-winning film that depicts the relationship between an East German playwright and a Stasi agent who is tasked with surveilling him. The Stasi agent, he wrote, “can actually write up the ideas of the playwright, *without even thinking them* It is not even that he is ‘opposed’ to the dissident ideas. It is that he does not even let them touch his brain.” In the film, the Stasi agent eventually “cracks,” after he comes to sympathize with the playwright’s views. Yarvin, presumably, was the playwright.</p><p>He said that he was coming to see me, on the other hand, as an “NPC,” or non-player character. He proposed giving me a Voight-Kampff test, the fictional exam in “Blade Runner” used to distinguish androids from humans. His version would involve the two of us debating “the ‘blank slate theory’ versus ‘racism’&nbsp;” and recording the conversation. (“By ‘racism’ I mean of course human biodiversity,” he elaborated.) When I explained that my reporting process did not include submitting to on-demand tests, Yarvin sent me a screenshot of “August 1968,” W.&nbsp;H. Auden’s poem about the Soviet-led invasion of Czechoslovakia to suppress the Prague Spring:</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><blockquote data-testid="blockquote-wrapper"><p>The Ogre does what ogres can<br>Deeds quite impossible for Man,<br>But one prize is beyond his reach,<br>The Ogre cannot master Speech</p></blockquote><p>He went on to say that although he’d agreed to participate in this story because “no publicity is bad publicity,” he would now try to kill it if he could.</p><p>I was struck by the contrast between his messages and the coolheaded tone he’d recommended that Thiel and other friends deploy when handling the media. After the 2013 <em>TechCrunch</em> article identifying Yarvin came out, Balaji Srinivasan, the entrepreneur, proposed in an e-mail “to sic the Dark Enlightenment audience on a single vulnerable hostile reporter to dox them.” Yarvin dissuaded him. “What would Heartiste say?” Yarvin asked, referring to the white-nationalist pickup-artist blog “Chateau Heartiste.” “Almost always, the right alpha answer is ‘nothing.’ Say nothing. Do nothing.”</p><p>On a balmy afternoon in late February, Yarvin and his wife, Kristine, were driving down a country road in the South of France. They were accompanied by the documentarians, Brun and Díaz. “Where are we going, Kristine?” Brun asked from the passenger seat, turning the camera around to film her in the back beside me.</p><p>She said that she had only the vaguest notion. “Honestly, he just tells me everything last minute,” she explained. “It’s kind of like being a dog. You just know that you’re going in the car, and you don’t know if you’re gonna go to the dog park, or you’re gonna go to the vet, and you’ll find out when you get there.”</p><p>“Spontaneity,” Yarvin chimed in.</p><p>“That’s a word for it,” Kristine teased.</p><figure><p><span><div data-attr-viewport-monitor=""><a data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.newyorker.com/cartoon/a22211&quot;}" href="https://www.newyorker.com/cartoon/a22211" rel="nofollow noopener" target="_blank"><picture></picture></a><p><span>Cartoon by John O’Brien</span></p></div></span></p></figure><p>We were on our way to meet Renaud Camus, a seventy-eight-year-old novelist and pamphleteer, who, in 2011, published “The Great Replacement,” an incendiary manifesto that argued that liberal élites were behind a conspiracy to replace white Europeans with migrants from Africa and the Middle East. The title phrase has since become a rallying cry for white nationalists around the world, from Charlottesville, Virginia, where, in 2017, marchers chanted, “You will not replace us,” to Christchurch, New Zealand, where, two years later, a man who’d published a manifesto with the same title as Camus’s killed fifty-one Muslims.</p><p>As we crested a hill, the walls of Camus’s castle, Château de Plieux, loomed into view. “Does anyone know if he’s related to Albert Camus?” Yarvin asked. “I think he’s not related to Albert, but he’s a lovely, old, gay, literary Frenchman.”</p><p>Brun, who is Venezuelan, wondered what he would do if Camus “has a sign that says ‘No Foreigners Allowed.’&nbsp;”</p><p>“Well, are you here to replace us?” Kristine joked. Nobody replied.</p><p>Yarvin rang an impressive metal bell beside the door, and we were soon ushered inside by Pierre Jolibert, Camus’s partner. Upstairs, Camus was waiting for us with a bottle of champagne. With his manicured white beard and brown corduroy jacket, complete with a bow tie and gold pocket-watch chain, he looked like a nineteenth-century man of letters. Speaking perfect English, with an English accent, he made it sound as though he’d had no choice but to buy the castle, which dated from the early thirteen-hundreds, after his library grew too large for his small Parisian flat. That was thirty-five years ago. Now, acknowledging the stacks of books that were overtaking his cavernous study, he said that he was running into the same problem here.</p><p>Over several glasses of champagne, Yarvin fired a series of questions at Camus, though he rarely waited long enough for his host to give a full answer. What did Camus think of Philippe Pétain? Charles de Gaulle? Napoleon III? Napoleon I? Ernst Jünger? Ernst von Salomon? Ezra Pound? Basil Bunting? More than an interaction, Yarvin, the former trivia champion, seemed to want a pat on the head for his display of learning.</p><p>After we headed downstairs for lunch—strips of sizzling duck, a quiche Lorraine, red wine—Yarvin resumed his cross-examination. Did Camus rate Thomas Carlyle? Michel Houellebecq? Louis XIV? What would he say to Charles Maurras if he were alive today? What would Dostoyevsky have thought about the <em>Covid</em> lab-leak theory?</p><p>Camus let out a high-pitched giggle whenever Yarvin asked a particularly odd question, but he was baffled by his guest’s repeated inquiries about Brigitte Macron, the French First Lady, who Yarvin suspected was actually a man. “We are dealing with the most important thing in the history of the Continent,” Camus exclaimed, referring to the rise of nonwhite immigration to Europe. “What does it matter if Mrs. Macron is a man or woman?”</p><p>Brun asked the men to move to a window so that he could shoot them from outside. As Yarvin gazed at the patchwork of neatly tended fields below, he spoke about the Great Replacement as “one of the greatest crimes” in history. “Is it greater than the Holocaust? I don’t know.&nbsp;.&nbsp;.&nbsp;. We haven’t seen it play out yet.” He’d been drinking since his arrival and seemed to be in an emotional state. “I have three children,” he told Camus. “Will they be basically lined up and marched into mass graves?” They had been discussing Jean Raspail’s apocalyptic novel, “The Camp of the Saints” (1973), which depicts an invasion of Indian migrants destroying European nations. Sobbing now, he continued, “I want my children to die in the twenty-second century. I don’t want them to experience some kind of insane post-colonial Holocaust.”</p><p>After dessert, coffee, and a rum from Guadeloupe, it was time for an evening stroll. Carrying a wooden cane, Camus led Yarvin through the small town of Plieux. Spring had arrived early: a cherry tree was blossoming with little flowers. As they passed the local church, Yarvin took out his phone to show Camus a photo of the toddler he shares with Laurenson. “The mother of that child was not my wife,” he said confidingly. A moment later, he was reading a poem by C.&nbsp;P. Cavafy, in tears once again.</p><p>When Yarvin and Camus went on ahead, the filmmakers paused to assess the day’s shoot. Brun said that Yarvin reminded him of the long-winded character in “Airplane!” who talks so incessantly that it drives his seatmates to kill themselves. We wondered what Camus was making of the afternoon. It wasn’t long before we found out. “If intellectual exchanges were commercial exchanges—which they are, to a certain extent—the amount of my exports would not reach one per cent of that of my imports,” Camus wrote in his diary, which he posted online the following day. “The visitor spoke without interruption from his arrival to his departure, for five hours, very quickly and very loudly, interrupting himself only for curious fits of tears, when he spoke of his deceased wife, but also, more strangely, certain political situations.”</p><p>It was dark by the time we all returned to the château. “Thank you so much for your hospitality and your duck and your castle,” Yarvin said, looking around. “How much money did you spend on it?”</p><p>Lovingly squeezing Yarvin’s arm, Kristine said, “You can’t just ask people that!”</p><p>Camus gave Yarvin some of his books as souvenirs, but Yarvin’s mind already seemed elsewhere. Tomorrow, he would fly to Paris to meet with a group of red-pilled Zoomers and Éric Zemmour, a far-right polemicist who once ran to be the President of France.</p><p>As we headed to the car, Yarvin was buzzing with boyish excitement about his performance. He turned to me and the filmmakers. “Was that good?” he asked. “Was that good?”&nbsp;♦</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple Notes Expected to Gain Markdown Support in iOS 26 (111 pts)]]></title>
            <link>https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</link>
            <guid>44183923</guid>
            <pubDate>Wed, 04 Jun 2025 18:29:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/">https://www.macrumors.com/2025/06/04/apple-notes-rumored-markdown-support-ios-26/</a>, See on <a href="https://news.ycombinator.com/item?id=44183923">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="maincontent"><article expanded="true"><div data-io-article-url="/2025/06/04/apple-notes-rumored-markdown-support-ios-26/"><p>Apple's Notes app is rumored to be getting limited Markdown support in iOS 26 and macOS 26, according to <em><a href="https://9to5mac.com/2025/06/03/exclusive-ios-26-messages-carplay-more/">9to5Mac</a></em>. The feature would allow users to export text in the markdown format.</p>
<p><img src="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy" srcset="https://images.macrumors.com/t/KX5s3QPN8F9_4EB6jHu3huH7j-0=/400x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 400w,https://images.macrumors.com/t/rNrX5sZI2CoLmeluvBSY9PV3oIQ=/800x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg?lossy 800w,https://images.macrumors.com/t/MFdPb4DjJL7dZ3MuK5-yqgdRmro=/1600x0/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 1600w,https://images.macrumors.com/t/_Eb67huucTUo6d63cc3KL-IEMjQ=/2500x0/filters:no_upscale()/article-new/2025/05/iOS-26-Mock-Rainbow-Feature.jpg 2500w" sizes="(max-width: 900px) 100vw, 697px" alt="iOS 26 Mock Rainbow Feature" width="2500" height="1406"><br>Markdown is a lightweight markup language that some writers prefer to use over rich text. Rather than using HTML for bold, italics, links, and headers, Markdown uses quick character shortcuts like **bold** or #header. It sounds like the feature will only add support for exporting text with markdown formatting and not writing in markdown directly. </p>
<p>If the rumor holds up, it's likely to be unveiled at next week's Worldwide Developers Conference alongside other iOS 26 improvements, including <a href="https://www.macrumors.com/2025/06/03/ios-26-messages-app-rumors/">automatic translation and polls in Messages</a>, not to mention a <a href="https://www.macrumors.com/2025/03/17/apple-believes-users-love-major-ios-19-redesign/">major visual redesign</a>.</p>
</div></article><p><h2>Popular Stories</h2></p><div><h3><a href="https://www.macrumors.com/2025/06/01/new-icloud-plus-perk-this-year/">iPhone Users Who Pay for iCloud Storage Received a New Perk This Year</a></h3><p>If you pay for iCloud storage on your iPhone, Apple introduced an additional perk for you this year, at no additional cost.
The perk is the ability to create invitations in the Apple Invites app for the iPhone, which was released in the App Store in February.
In the Apple Invites app, iCloud+ subscribers can create invitations for any occasion, such as birthday parties, graduations, baby...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/apple-sleek-peek/">Apple Shares New 'Sleek Peek' Teaser Ahead of WWDC 2025 Next Week</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time. Ahead of the annual developer conference, Apple updated its WWDC page today with a new "Sleek peek" tagline, which replaces the original "On the horizon" tagline that it used over the past few weeks.
The graphic for WWDC 2025 has also been updated. It is now a...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/what-to-expect-from-ios-18-6/">What to Expect From iOS 18.6 as One of the Final Updates Before iOS 26</a></h3><p>It has been three weeks as of today since Apple released iOS 18.5, and we are still waiting for the first iOS 18.6 beta to follow.
Below, we outline everything we know about iOS 18.6 so far.
Timing
Apple's software engineers have been internally testing iOS 18.6 since late March, according to the MacRumors visitors logs.
The first betas of iOS 13.6 through iOS 16.6 were all released...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ios-26-to-upgrade-carplay-in-two-ways/">iOS 26 to Upgrade CarPlay in Two Ways</a></h3><p>While the spotlight has been on CarPlay Ultra lately, the regular version of CarPlay is set to receive some enhancements alongside iOS 26.
Apple will announce iOS 26 at WWDC 2025 next week, and the software update is expected to upgrade the CarPlay experience in at least two ways.
The first iOS 26 beta should be seeded to developers shortly after Apple's keynote, and the update will...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/macos-26-tahoe-what-to-expect/">WWDC 2025: What to Expect From macOS 26 Tahoe</a></h3><p>Monday June 2, 2025 4:17 pm PDT by <a href="https://www.macrumors.com/author/juli-clover/" rel="author">Juli Clover</a></p><p>WWDC is less than a week away, and as we ramp up to the big announcement, we're going to share details on what we know about each operating system. We're starting with the next-generation version of macOS, which Apple is apparently going to call macOS Tahoe.
Name
Since the current version of macOS is macOS 15, it would normally be followed by macOS 16, but Apple is changing its naming...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/02/ios-26-rumored-features/">iOS 26 Will Add These New Features to Your iPhone</a></h3><p>WWDC 2025 is just one week away, with Apple's opening keynote scheduled to begin on Monday, June 9 at 10 a.m. Pacific Time.
Apple will announce its latest software updates, including iOS 26, iPadOS 26, macOS 26, tvOS 26, watchOS 26, and visionOS 26, which are all rumored to feature a sleek new glass-like design. There might not be any hardware announcements, however, as Bloomberg's Mark...</p></div><div><h3><a href="https://www.macrumors.com/2025/06/04/ex-apple-designer-living-glass-ios-concepts/">Ex-Apple Designer Reveals 'Living Glass' iOS 26 Concepts</a></h3><p>Wednesday June 4, 2025 4:17 am PDT by <a href="https://www.macrumors.com/author/tim-hardwick/" rel="author">Tim Hardwick</a></p><p>Designer Sebastiaan de With has published an impressive preview of what Apple's rumored iOS redesign might look like, complete with detailed mockups and a design philosophy that he believes could reshape how users interact with their devices.
With WWDC just days away, de With – co-founder of photography app maker Lux and former Apple designer – has created what he calls "Living Glass"...</p></div><div><h3><a href="https://www.macrumors.com/2025/05/27/iphone-17-pro-rumors-list/">iPhone 17 Pro Launching Later This Year With These 12 New Features</a></h3><p>While the iPhone 17 Pro and iPhone 17 Pro Max are not expected to launch until September, there are already plenty of rumors about the devices.
Below, we recap key changes rumored for the iPhone 17 Pro models as of May 2025:
Aluminum frame: iPhone 17 Pro models are rumored to have an aluminum frame, whereas the iPhone 15 Pro and iPhone 16 Pro models have a titanium frame, and the iPhone X ...</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A proposal to restrict sites from accessing a users’ local network (218 pts)]]></title>
            <link>https://github.com/explainers-by-googlers/local-network-access</link>
            <guid>44183799</guid>
            <pubDate>Wed, 04 Jun 2025 18:15:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/explainers-by-googlers/local-network-access">https://github.com/explainers-by-googlers/local-network-access</a>, See on <a href="https://news.ycombinator.com/item?id=44183799">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Explainer for Local Network Access</h2><a id="user-content-explainer-for-local-network-access" aria-label="Permalink: Explainer for Local Network Access" href="#explainer-for-local-network-access"></a></p>
<p dir="auto">This proposal is an early design sketch by the Chrome Secure Web and Network team to describe the problem below and solicit feedback on the proposed solution. It has not been approved to ship in Chrome.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proponents</h2><a id="user-content-proponents" aria-label="Permalink: Proponents" href="#proponents"></a></p>
<ul dir="auto">
<li>Chrome Secure Web and Network team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Participate</h2><a id="user-content-participate" aria-label="Permalink: Participate" href="#participate"></a></p>
<ul dir="auto">
<li><a href="https://github.com/explainers-by-googlers/local-network-access/issues">https://github.com/explainers-by-googlers/local-network-access/issues</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Currently public websites can probe a user's local network, perform CSRF attacks against vulnerable local devices, and generally abuse the user's browser as a "confused deputy" that has access inside the user's local network or software on their local machine. For example, if you visit <code>evil.com</code> it can use your browser as a springboard to attack your printer (given an HTTP accessible printer exploit).</p>
<p dir="auto">Local Network Access aims to prevent these undesired requests to insecure devices on the local network. This is achieved by deprecating direct access to private IP addresses from public websites, and instead requiring that the user grants permission to the initiating website to make connections to their local network.</p>
<p dir="auto"><em>Note:</em> This proposal builds on top of Chrome's previously paused <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access (PNA) work</a>, but differs by gating access on a permission rather than via preflight requests. This increases the level of user control (at the expense of new permissions that have to be explained to the user) but removes the explicit "device opt-in" that the preflight design achieved. We believe this simpler design will be easier to ship, in order to mitigate the real risks of local network access today. Unlike the previous Private Network Access proposal, which required changes to devices on local networks, this approach should only require changes to sites that need to access the local network. Sites are much easier to update than devices, and so this approach should be much more straightforward to roll out.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Goals</h2><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<ul dir="auto">
<li><strong>Stop exploitation of vulnerable devices and servers from the drive-by web.</strong></li>
<li>Allow public websites to communicate to private network devices when the user expects it and explicitly allows it.</li>
</ul>
<p dir="auto">An adjacent goal is that we want a path for browsers to be good stewards of OS-level local network access permissions. These OS-level permissions are increasingly common (<a href="https://developer.apple.com/documentation/technotes/tn3179-understanding-local-network-privacy" rel="nofollow">on iOS, and more recently on macOS</a>) -- simply because the <em>browser</em> has been granted the permission (for legitimate browser functionality the user may want to use, like mirroring the contents of a tab on a local device) should not expose users' local devices to the risks of the open web.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Non-goals</h2><a id="user-content-non-goals" aria-label="Permalink: Non-goals" href="#non-goals"></a></p>
<ul dir="auto">
<li>Break existing workflows and services that rely on a public web frontend that can control local network devices.
<ul dir="auto">
<li>As long as there is <em>some</em> path forward we should be okay with breaking some use cases (e.g., iframe and HTML subresources that aren't explicitly sourced from local hostnames), but overall we want to minimize breakage.</li>
</ul>
</li>
<li>Solve the local network HTTPS problem.
<ul dir="auto">
<li>As stated in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#non-goals">original Private Network Access explainer</a>: <em>Provide a secure mechanism for initiating HTTPS connections to services running on the local network or the user's machine. This piece is missing to allow secure public websites to embed non-public resources without running into mixed content violations, with the exception of <a href="http://localhost/" rel="nofollow">http://localhost</a> which is embeddable. While a useful goal, and maybe even a necessary one in order to deploy Private Network Access more widely, it is out of scope of this specification.</em></li>
</ul>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Use cases</h2><a id="user-content-use-cases" aria-label="Permalink: Use cases" href="#use-cases"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 1</h3><a id="user-content-use-case-1" aria-label="Permalink: Use case 1" href="#use-case-1"></a></p>
<p dir="auto">The most common case is for users who don't have any services or devices on their local network that expect connections from websites. Today, browsers freely allow JavaScript and subresource requests to these devices without any indication to the end user. Unless this behavior is expected by the user, users should not be exposed to this risk by default.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Use case 2</h3><a id="user-content-use-case-2" aria-label="Permalink: Use case 2" href="#use-case-2"></a></p>
<p dir="auto">Public web frontend for controlling or setting up local devices (such as an IoT device, home router, etc.).</p>
<p dir="auto">Device manufacturers want to be able to give users an easy process for setting up a new device, and one method that is used is to have a page hosted on the manufacturer's public website which then communicates with the device via the user's browser, explicitly relying on the browser's vantage point inside the user's network.</p>
<p dir="auto">This also reduces the complexity needed on the device itself -- for example, a smart toothbrush does not need to support a full webserver. Additionally, by being a public webpage under the control of the manufacturer, the setup page is always up to date.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Proposed Solution</h2><a id="user-content-proposed-solution" aria-label="Permalink: Proposed Solution" href="#proposed-solution"></a></p>
<p dir="auto">We propose gating the ability for a site to make requests to the users' local network behind a new "local network access" permission. Any origin that has not been granted this permission would be blocked from making such requests.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Address spaces</h3><a id="user-content-address-spaces" aria-label="Permalink: Address spaces" href="#address-spaces"></a></p>
<p dir="auto">As defined in the <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md#address-spaces">original Private Network Access proposal</a>, we organize an IP network into three layers from the point of view of a node, from most to least private:</p>
<ul dir="auto">
<li>Localhost: accessible only to the node itself, by default</li>
<li>Private IP addresses: accessible only to the members of the local network (e.g. RFC1918)</li>
<li>Public IP addresses: accessible to anyone</li>
</ul>
<p dir="auto">We call these layers <strong>address spaces</strong>: <code>loopback</code>, <code>local</code>, and <code>public</code>.</p>
<p dir="auto"><em>(Note: The original PNA proposal called these <code>local</code>, <code>private</code>, and <code>public</code>. Changing this was considered in <a data-error-text="Failed to load title" data-id="1459537487" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/91" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/91/hovercard" href="https://github.com/WICG/private-network-access/issues/91">WICG/private-network-access#91</a> but reverted due to already using the "private network access" name and values in headers implemented by sites and device manufacturers.)</em></p>
<p dir="auto">We note that <code>local</code> includes <a href="https://datatracker.ietf.org/doc/html/rfc1918" rel="nofollow">RFC 1918</a>/<a href="https://datatracker.ietf.org/doc/html/rfc4193" rel="nofollow">RFC 4193</a> private/local IP addresses and <a href="https://datatracker.ietf.org/doc/html/rfc6762" rel="nofollow">RFC 6762</a> link-local names (<code>.local</code> hostnames). (See <a href="https://github.com/WICG/private-network-access/issues/4" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/4/hovercard">discussion on the original PNA proposal repository</a>.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Local network requests</h3><a id="user-content-local-network-requests" aria-label="Permalink: Local network requests" href="#local-network-requests"></a></p>
<p dir="auto">We define a <strong>local network request</strong> as a request crossing an address space boundary to a more-private address space. That is, any of the following are considered to be local network requests:</p>
<ol dir="auto">
<li><code>public</code> -&gt; <code>local</code></li>
<li><code>public</code> -&gt; <code>loopback</code></li>
<li><code>local</code> -&gt; <code>loopback</code></li>
</ol>
<p dir="auto">Note that <code>local</code> -&gt; <code>local</code> is not a local network request, as well as <code>loopback</code> -&gt; anything. (See "cross-origin requests" below for a discussion on potentially expanding this definition in the future.)</p>
<p dir="auto">A request is considered to be going to a <code>local</code> space if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC 1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain (per RFC 6762), or</li>
<li>The <code>fetch()</code> call is annotated with <code>targetAddressSpace="local"</code> (see "Integration with Fetch" below).</li>
</ul>
<p dir="auto">In these cases we know <em>a priori</em> that the request is <code>local</code>.</p>
<p dir="auto">Similarly, if a request is to a loopback IP literal (e.g., 127.0.0.1), <code>localhost</code>, or the <code>fetch()</code> call is annotated with <code>targetAddressSpace="loopback"</code>, then the request is <code>loopback</code>.</p>
<p dir="auto">Separately, a request may eventually end up being considered a local network request if the request's hostname resolves to a private or loopback IP address. (We do not know this a priori however, and so cannot exempt these requests from mixed content blocking -- see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Permission prompts</h3><a id="user-content-permission-prompts" aria-label="Permalink: Permission prompts" href="#permission-prompts"></a></p>
<p dir="auto">When a site makes a local network request, the UA should check if the origin has already been granted the "local network access" permission. If not, the request should be blocked while the UA displays a prompt to the user asking whether they want to allow the origin to make requests to their local network. If the user denies the permission prompt, the request fails. If the user accepts the permission prompt, the request continues.</p>
<p dir="auto">To reduce breakage (due to the lack of local network HTTPS), the permission also exempts requests that are known to be <code>local</code> or <code>loopback</code> from mixed content blocking (see "Mixed Content" below.)</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">How this solution would solve the use cases</h3><a id="user-content-how-this-solution-would-solve-the-use-cases" aria-label="Permalink: How this solution would solve the use cases" href="#how-this-solution-would-solve-the-use-cases"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 1: unexpected usage</h4><a id="user-content-use-case-1-unexpected-usage" aria-label="Permalink: Use case 1: unexpected usage" href="#use-case-1-unexpected-usage"></a></p>
<p dir="auto">For a user who is not expecting a site to connect to their local network, when <code>example.com</code> tries to call <code>fetch("http://192.168.0.1/routerstatus")</code>, the user's browser will ask whether or not to allow <code>example.com</code> to make connections to the local network. Since the user is not expecting this behavior, they can deny the permission request, and <code>example.com</code> is blocked from making these connections.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Use case 2: controlling local devices</h4><a id="user-content-use-case-2-controlling-local-devices" aria-label="Permalink: Use case 2: controlling local devices" href="#use-case-2-controlling-local-devices"></a></p>
<p dir="auto">An existing site run by a device manufacturer that talks to a local device by making <code>fetch()</code> requests would potentially make minor modifications (for example, ensuring that they either use a private IP address or <code>.local</code> name when referring to the device, or adding the <code>targetAddressSpace="local"</code> property to their <code>fetch()</code> calls). When the site first tries to make a request to the device, the user sees a permission prompt. If the user expects the site to be communicating with devices on their local network, they can choose to grant the permission and the site will continue to function. If the user does not expect this or does not want to grant the permission to the site, they choose to not grant the permission to the site, and no local network requests from the site will be allowed.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Detailed design discussion</h2><a id="user-content-detailed-design-discussion" aria-label="Permalink: Detailed design discussion" href="#detailed-design-discussion"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Fetch</h3><a id="user-content-integration-with-fetch" aria-label="Permalink: Integration with Fetch" href="#integration-with-fetch"></a></p>
<p dir="auto">The Fetch spec does not integrate the details of DNS resolution, only defining an <strong>obtain a connection</strong> algorithm, thus Local Network Access checks are applied to the newly-obtained connection. Given complexities such as Happy Eyeballs (<a href="https://datatracker.ietf.org/doc/html/rfc6555" rel="nofollow">RFC6555</a>, <a href="https://datatracker.ietf.org/doc/html/rfc8305" rel="nofollow">RFC8305</a>), these checks might pass or fail non-deterministically for hosts with multiple IP addresses that straddle IP address space boundaries.</p>
<p dir="auto">After we have obtained a connection, if we detect a local network request:</p>
<ul dir="auto">
<li>If the client is not in a secure context, block the request.</li>
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the request will proceed.</li>
</ul>
<p dir="auto">However, the requirement that local network requests be made from secure contexts means that any insecure request will be blocked as mixed content unless we can know ahead of time that the request should be considered a local network request.</p>
<p dir="auto">To make this easier for developers, we propose adding a new parameter to the <code>fetch()</code> options bag to explicitly tag the address space of the request. For example:</p>
<div dir="auto" data-snippet-clipboard-copy-content="fetch(&quot;http://router.com/ping&quot;, {
  targetAddressSpace: &quot;local&quot;,
});"><pre><span>fetch</span><span>(</span><span>"http://router.com/ping"</span><span>,</span> <span>{</span>
  <span>targetAddressSpace</span>: <span>"local"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></pre></div>
<p dir="auto">This would instruct the browser to allow the fetch even though the scheme is non-secure and obtain a connection to the target server. The <code>targetAddressSpace</code> should be either <code>local</code> or <code>loopback</code>.</p>
<p dir="auto">If the remote IP address does not belong to the IP address space specified as the targetAddressSpace option value, then the request is failed. This ensures the feature cannot be abused to bypass mixed content in general. See "Mixed content" below for more details.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Mixed content</h3><a id="user-content-mixed-content" aria-label="Permalink: Mixed content" href="#mixed-content"></a></p>
<p dir="auto">There is a challenge in the combination of (1) requiring secure contexts in order to make PNA requests, (2) trying to load PNA subresources over HTTP (due to the lack of local network HTTPS), and (3) applying mixed content blocking. The Fetch specification applies mixed content upgrading and blocking steps well before we have obtained a connection.</p>
<p dir="auto">We can know ahead of the mixed content checks whether a request has a local target address if:</p>
<ul dir="auto">
<li>The hostname is a private IP address literal (per RFC1918 etc.), or</li>
<li>The hostname is a <code>.local</code> domain, or</li>
<li>The <code>fetch()</code> call is annotated with the <code>{ targetAddressSpace: "local" }</code> option.</li>
</ul>
<p dir="auto">If the request meets any of those requirements, we skip steps 6 and 7 of <strong>main fetch</strong> (upgrading mixed content and blocking mixed content) and mark the request as a local network request. Then, after obtaining the connection the local network access checks can fully run, and if the origin is not granted the local network access permission the request will be blocked. Additionally, if the request ends up not resolving to a local network endpoint, we need to block the request (as it should have been blocked as mixed content).</p>
<p dir="auto">A new parameter <code>targetAddressSpace</code> will be added as a <code>fetch()</code> API option, allowing a developer to specify that the request should be treated as going to a <code>local</code> or <code>loopback</code> address space. This allows for HTTP local network requests that are not private IP literals or .local domains as long as they are explicitly tagged with the target address space. This also allows the developer to deterministically request the permission.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with HTML</h3><a id="user-content-integration-with-html" aria-label="Permalink: Integration with HTML" href="#integration-with-html"></a></p>
<p dir="auto"><code>Document</code>s and <code>WorkerGlobalScope</code>s store an additional <strong>address space</strong> value. This is initialized from the IP address the document or worker was sourced from.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with WebRTC</h3><a id="user-content-integration-with-webrtc" aria-label="Permalink: Integration with WebRTC" href="#integration-with-webrtc"></a></p>
<p dir="auto">Local connection attempts that use WebRTC should also be gated behind the Local Network Access permission prompt.</p>
<p dir="auto">In the WebRTC spec, algorithms that add candidates to the ICE Agent already have steps that ensure <a href="https://www.w3.org/TR/webrtc/#dfn-administratively-prohibited" rel="nofollow">“administratively prohibited”</a> addresses are not used. We can modify these algorithms to perform the following steps if the candidate has a loopback or local address:</p>
<ul dir="auto">
<li>Check if the origin has previously been granted the local network access permission; if not, prompt the user.</li>
<li>If the user grants permission, the algorithm will continue.</li>
<li>If the user denies the permission, we won’t add the candidate to the ICE Agent and it won’t be used when establishing a connection.</li>
</ul>
<p dir="auto">Note that these checks are done asynchronously and don’t block resolving the methods where they are used i.e. setRemoteDescription() and addIceCandidate().</p>
<p dir="auto">The same checks should also be performed when connecting to STUN/TURN servers with loopback or local addresses.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions Policy</h3><a id="user-content-integration-with-permissions-policy" aria-label="Permalink: Integration with Permissions Policy" href="#integration-with-permissions-policy"></a></p>
<p dir="auto">By default the ability to make local network requests will be limited to top-level documents that are secure contexts. There are use cases where a site needs to be able to delegate this permission into a subframe. To support these use cases, a new policy-controlled feature ("local-network-access") will be added that will allow top-level documents to delegate access to this feature to subframes.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Integration with Permissions API</h3><a id="user-content-integration-with-permissions-api" aria-label="Permalink: Integration with Permissions API" href="#integration-with-permissions-api"></a></p>
<p dir="auto">The permission will be integrated with the Permissions API, which will allow sites to query the status of the permission grant.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">HTML subresources</h3><a id="user-content-html-subresources" aria-label="Permalink: HTML subresources" href="#html-subresources"></a></p>
<p dir="auto">HTML subresource fetches go through the standard Fetch algorithm, but will not have the ability to specify an explicit <code>targetAddressSpace</code>. This includes subframe navigations.</p>
<p dir="auto">For HTTP subresources, only "local names" (i.e., private IP literals or .local hostnames) are allowed for local network requests. This is required for resolving the mixed content problem (see "Mixed content" above, and see "Considered alternatives" for more involved methods that have been discussed).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Websockets</h3><a id="user-content-websockets" aria-label="Permalink: Websockets" href="#websockets"></a></p>
<p dir="auto">The <strong>establish a WebSocket connection</strong> algorithm depends on the Fetch algorithm (<a href="https://websockets.spec.whatwg.org/#websocket-protocol" rel="nofollow">in the updated WHATWG spec</a>), so Websockets should behave like other Fetch requests and trigger local network access prompts without additional work.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Service Workers</h3><a id="user-content-service-workers" aria-label="Permalink: Service Workers" href="#service-workers"></a></p>
<p dir="auto">Requests from a service worker go through the Fetch algorithm, and will be included in local network access restrictions.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Considered alternatives</h2><a id="user-content-considered-alternatives" aria-label="Permalink: Considered alternatives" href="#considered-alternatives"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Private Network Access (PNA)</h3><a id="user-content-private-network-access-pna" aria-label="Permalink: Private Network Access (PNA)" href="#private-network-access-pna"></a></p>
<p dir="auto">The previously proposed <a href="https://github.com/WICG/private-network-access/blob/main/explainer.md">Private Network Access work</a> (PNA for short, also previously referred to as CORS-RFC1918) required a secure CORS preflight response from the private subresource server. If the preflight failed, the request would be blocked. If the request was for an insecure resource (e.g., due to the lack of trusted local HTTPS), they <a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md">proposed a separate permission prompt</a> to allow the connection to a specific endpoint device <em>and</em> relax the mixed content restrictions on the connection.</p>
<p dir="auto">A lot of effort and developer outreach went into this effort, but it was never able to ship. Chrome currently has an opt-in mode behind an enterprise policy, and for a while Chrome shipped a restriction where private network access was restricted to secure contexts only (with a deprecation trial for developers who could not yet meet this requirement due to having HTTP-only private network endpoints). The previous plan was to build and ship the "mixed content permission prompt" to get these remaining developers out of the reverse origin trial.</p>
<p dir="auto">PNA met a lot of different developer and user needs, and in the "good case" (secure website talking to a local network device that had a publicly trusted TLS certificate and a "PNA-aware" server) could be quite seamless, since it required no user intervention. In the non-ideal cases, PNA accumulated a lot of workarounds to address use cases that would result in a permission prompt in many cases (a device chooser style prompt for insecure devices). For example:</p>
<ul dir="auto">
<li>Some routers filter out DNS responses mapping public domain names to private IP addresses -- this means that <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to direct (insecure) private IP addresses is required</a> (and thus a permission prompt to bypass mixed content blocking), even for developers who provision publicly trusted certs to local network servers.</li>
<li>Some devices could not update to supply preflight, so the permission prompt was relaxed to allow this (<a href="https://github.com/WICG/private-network-access/blob/main/permission_prompt/explainer.md#ephemeral-permission">but only grant an ephemeral permission</a>).</li>
</ul>
<p dir="auto">Even if the local device has opted in to connections from a top level site, we believe there is value in user awareness and control over this exchange.</p>
<p dir="auto">The use of preflights (without any user consent speed bump) also exposed its own risks. For example, timing attacks could be used to determine valid IP addresses on the network (<a href="https://crbug.com/40051437" rel="nofollow">crbug.com/40051437</a>, <a data-error-text="Failed to load title" data-id="822980731" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/41" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/41/hovercard" href="https://github.com/WICG/private-network-access/issues/41">WICG/private-network-access#41</a>).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Block all local network requests</h3><a id="user-content-block-all-local-network-requests" aria-label="Permalink: Block all local network requests" href="#block-all-local-network-requests"></a></p>
<p dir="auto">Given the risks of allowing sites to use the browser as an access point into the user's local network, we could simply block all local network requests (or just any requests that cross from a public address space to a more private one). This would be simplest, but would break many existing use cases, or require expensive workarounds.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Do nothing</h3><a id="user-content-do-nothing" aria-label="Permalink: Do nothing" href="#do-nothing"></a></p>
<p dir="auto">As more operating systems are implementing local network access permissions at the application level, we believe it is the duty of user agents to broker access to that privilege in order to be good stewards of it. A user may have legitimate reasons to grant access for their <em>browser</em> (e.g., Chrome's "cast this tab" functionality) but never want a <em>site</em> to be able to access their local network.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Alternatives for the mixed content problem</h3><a id="user-content-alternatives-for-the-mixed-content-problem" aria-label="Permalink: Alternatives for the mixed content problem" href="#alternatives-for-the-mixed-content-problem"></a></p>
<p dir="auto">In our proposal above we recommend restricting local network HTML subresources to "assumed local" hostnames (such as <code>.local</code> domains) as a middle ground that meets developer needs, is relatively easy to deploy, and doesn't require complex technical or specification work to accomplish.</p>
<p dir="auto">Below are some alternatives that have been considered for addressing the mixed content problem.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Require secure local network subresources</h4><a id="user-content-require-secure-local-network-subresources" aria-label="Permalink: Require secure local network subresources" href="#require-secure-local-network-subresources"></a></p>
<p dir="auto">In order to restrict local network access to secure contexts (which is necessary in order for a permission prompt to make sense), we need some resolution of the mixed content problem. The Fetch specification orders the "upgrade or block mixed content" steps <em>before</em> we have obtained a connection, and thus before we can know the IP address.</p>
<p dir="auto">Currently, some developers can work around this by getting publicly trusted certificates for their servers running on local networks (e.g., Plex getting Let's Encrypt certs under a different subdomain for every install) but it is a substantial engineering and maintenance burden. Even for developers that go to the trouble of using publicly trusted certificates, <a href="https://github.com/WICG/private-network-access/issues/23" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/23/hovercard">fallback to HTTP is required in some network circumstances</a>.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">List subresource address space details in a header or meta tag</h4><a id="user-content-list-subresource-address-space-details-in-a-header-or-meta-tag" aria-label="Permalink: List subresource address space details in a header or meta tag" href="#list-subresource-address-space-details-in-a-header-or-meta-tag"></a></p>
<p dir="auto">This could be a "treat hostname as public" / "treat hostname as local" property that could be specified in a response header or in a meta tag (or a response header meta equiv).</p>
<p dir="auto">An initial idea was to add this on to <a href="https://www.w3.org/TR/CSP3/" rel="nofollow">CSP</a>, however that was rejected due to CSP already being a bit overloaded and this not being a great conceptual fit for it. A separate "Sec-Treat-Origin-As-Private" header (or something along those lines) could be used to list origins that should be assumed to resolve to private IP addresses.</p>
<p dir="auto">Additionally, it might be useful if such a header could be specified via an http-equiv meta tag (like one can do with CSP).</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Potential future changes</h3><a id="user-content-potential-future-changes" aria-label="Permalink: Potential future changes" href="#potential-future-changes"></a></p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Top-level navigations to local network</h4><a id="user-content-top-level-navigations-to-local-network" aria-label="Permalink: Top-level navigations to local network" href="#top-level-navigations-to-local-network"></a></p>
<p dir="auto">Top-level navigations remain a risk after restrictions on subresource local network requests are in place. For an attacker, main frame navigations are noisier (compared to subresource requests and iframe navigations), although popunder techniques could potentially be used to hide navigations from the user.</p>
<p dir="auto">To prevent these, we could block or show an interstitial warning when a public page navigates to a local one. To avoid too much breakage or over-warning, we could maybe scope protections to just these cases. We might consider that "complex" requests such as POST navigations and GET requests with URL parameters are particularly risky. We might also be able to "defang" navigations by stripping the URL parameters to reduce the risk of exploitation.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Consider all cross-origin requests to private addresses as "local network requests"</h4><a id="user-content-consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests" aria-label="Permalink: Consider all cross-origin requests to private addresses as &quot;local network requests&quot;" href="#consider-all-cross-origin-requests-to-private-addresses-as-local-network-requests"></a></p>
<p dir="auto">We could instead define a <strong>local network request</strong> as "any request targeting an IP address in the local or loopback address space, regardless of the requestor's address space", while maintaining the exception for not blocking same-origin requests. This is a stricter / broader definition, and would likely cause more widespread breakage than our proposal to only consider requests that cross from one address space class to a more private one.</p>
<p dir="auto">This would also allow the mixed content relaxation that is granted when an origin is given the local network access permission to apply to <code>local</code> -&gt; <code>local</code> or <code>loopback</code> -&gt; <code>local</code> requests. This has been raised as a concern by developers in <a data-error-text="Failed to load title" data-id="1795130611" data-permission-text="Title is private" data-url="https://github.com/WICG/private-network-access/issues/109" data-hovercard-type="issue" data-hovercard-url="/WICG/private-network-access/issues/109/hovercard" href="https://github.com/WICG/private-network-access/issues/109">WICG/private-network-access#109</a>. Note that the status quo could continue here for now -- i.e., the top level page can get around mixed content checks by remaining on HTTP.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">More granular permission grants</h4><a id="user-content-more-granular-permission-grants" aria-label="Permalink: More granular permission grants" href="#more-granular-permission-grants"></a></p>
<p dir="auto">A <a href="https://martina.lindorfer.in/files/papers/nwscanning_oakland25.pdf" rel="nofollow">recent study</a> (Schmidt et al., S&amp;P 2025) found that the <em>transitivity</em> of the local networks permission on iOS was the hardest for users to understand. It might be beneficial to scope the permission grant to an origin to the specific local network the user is currently connected to. Should the user grant permission to example.com on their home network, it may be reasonable for a UA to re-prompt the user if they bring their device to a different location and visit example.com again.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Add address space properties to HTML</h4><a id="user-content-add-address-space-properties-to-html" aria-label="Permalink: Add address space properties to HTML" href="#add-address-space-properties-to-html"></a></p>
<p dir="auto">PNA 1.0 worked to add a new <code>targetAddressSpace</code> parameter to <code>fetch()</code> to label the target address space of the request, so that mixed content checks could be relaxed (and then enforced if that connection ended up being public, to avoid it being a mixed content blocking bypass). The challenge is how to handle HTML subresources (e.g., img, iframe, etc.). We could add a new property to these HTML elements allowing developers to "label" them as public/local/loopback. This would function similarly to the parameter on <code>fetch()</code> and allow the user agent to initially bypass mixed content checks (and to know it should trigger the permission prompt). Currently we don't think this is necessary, as most use cases that require explicitly marking the <code>targetAddressSpace</code> should be able to switch to using <code>fetch()</code>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security &amp; Privacy Considerations</h2><a id="user-content-security--privacy-considerations" aria-label="Permalink: Security &amp; Privacy Considerations" href="#security--privacy-considerations"></a></p>
<p dir="auto"><strong>Security</strong></p>
<ul dir="auto">
<li>While the local network access permission exempts requests to a priori known local endpoints from mixed content blocking, the page should still be considered to have loaded mixed content if such a request is made. This means that, for example, browsers can choose to show a different security UI for pages that make insecure connections to the local network.</li>
<li>Compared to the original PNA proposal, a site granted the local network access permission has more power to probe and connect to devices on the local network, regardless of whether those devices expect it.</li>
<li>There is some risk of users accepting the permission without understanding it (which couldn't happen with preflights).</li>
</ul>
<p dir="auto"><strong>Privacy</strong></p>
<ul dir="auto">
<li>Compared to the original PNA proposal, no local network connections are allowed until the user has explicitly granted permission to a site.</li>
<li>Compared to the original PNA proposal, there are no preflights (and thus no risk of timing/probing attacks from them).</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Stakeholder Feedback / Opposition</h2><a id="user-content-stakeholder-feedback--opposition" aria-label="Permalink: Stakeholder Feedback / Opposition" href="#stakeholder-feedback--opposition"></a></p>
<p dir="auto">The previous PNA proposal (using preflights) was positively received by Mozilla (<a data-error-text="Failed to load title" data-id="419961868" data-permission-text="Title is private" data-url="https://github.com/mozilla/standards-positions/issues/143" data-hovercard-type="issue" data-hovercard-url="/mozilla/standards-positions/issues/143/hovercard" href="https://github.com/mozilla/standards-positions/issues/143">mozilla/standards-positions#143</a>) and WebKit (<a data-error-text="Failed to load title" data-id="1656934582" data-permission-text="Title is private" data-url="https://github.com/WebKit/standards-positions/issues/163" data-hovercard-type="issue" data-hovercard-url="/WebKit/standards-positions/issues/163/hovercard" href="https://github.com/WebKit/standards-positions/issues/163">WebKit/standards-positions#163</a>).</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">References &amp; acknowledgements</h2><a id="user-content-references--acknowledgements" aria-label="Permalink: References &amp; acknowledgements" href="#references--acknowledgements"></a></p>
<p dir="auto">Many thanks for valuable feedback and advice from:</p>
<ul dir="auto">
<li>Titouan Rigoudy, Jonathan Hao, and Yifan Luo who worked on the original PNA proposals and specification, and generously discussed their work with me and helped brainstorm paths forward.</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The iPhone 15 Pro's Depth Maps (199 pts)]]></title>
            <link>https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</link>
            <guid>44183591</guid>
            <pubDate>Wed, 04 Jun 2025 17:57:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html">https://tech.marksblogg.com/apple-iphone-15-pro-depth-map-heic.html</a>, See on <a href="https://news.ycombinator.com/item?id=44183591">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article_text">
            <p>Since 2017, Apple have supported depth maps in the images its iPhones capture either via LiDAR scanners, 3D <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">time-of-flight</a> scanner-less LIDAR or <a href="https://en.wikipedia.org/wiki/Structured-light_3D_scanner">structured-light</a> 3D scanning.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_j61zCFLlcU.png">
</a></p><p>These depth maps, along with other imagery, are stored in High Efficiency Image File Format (HEIF) container files. These files can contain multiple images and vast amounts of metadata. This format was originally designed between 2013 and 2015 and Apple adopted its HEIC variant in 2017.</p>
<p>Since then, HEIC files are the default storage container format for images captured on the iPhone. But with that said, it is possible to use the JPEG format instead if things like depth maps and HDR are not of interest.</p>
<p>Finn Jaeger, who is the head of VFX at Replayboys, a film production firm in Hamburg, Germany, <a href="https://www.linkedin.com/posts/finn-j%C3%A4ger-b44058176_did-you-know-your-iphone-heic-files-are-activity-7319460880750940162-Rn23/">posted</a> a screenshot a few weeks ago showing how multiple depth maps were being produced by his iPhone.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_qqlvRZSjK9.png">
</a></p><p>He announced he was working on a project called <a href="https://github.com/finnschi/heic-shenanigans">HEIC Shenanigans</a>. This project contains scripts to separate out images and their metadata from HEIC containers as well as convert them into EXR Files. As of this writing, the project contains 374 lines of Python.</p>
<p>In this post, I'll walk through Finn's codebase with an example image from an iPhone 15 Pro.</p>
<div id="my-workstation">
<h2>My Workstation</h2>
<p>I'm using a 5.7 GHz AMD Ryzen 9 9950X CPU. It has 16 cores and 32 threads and 1.2 MB of L1, 16 MB of L2 and 64 MB of L3 cache. It has a liquid cooler attached and is housed in a spacious, full-sized Cooler Master HAF 700 computer case.</p>
<p>The system has 96 GB of DDR5 RAM clocked at 4,800 MT/s and a 5th-generation, Crucial T700 4 TB NVMe M.2 SSD which can read at speeds up to 12,400 MB/s. There is a heatsink on the SSD to help keep its temperature down. This is my system's C drive.</p>
<p>The system is powered by a 1,200-watt, fully modular Corsair Power Supply and is sat on an ASRock X870E Nova 90 Motherboard.</p>
<p>I'm running Ubuntu 24 LTS via Microsoft's Ubuntu for Windows on Windows 11 Pro. In case you're wondering why I don't run a Linux-based desktop as my primary work environment, I'm still using an Nvidia GTX 1080 GPU which has better driver support on Windows and ArcGIS Pro only supports Windows natively.</p>
</div>
<div id="installing-prerequisites">
<h2>Installing Prerequisites</h2>
<p>I'll use Python 3.12.3 and a few other tools in this post.</p>
<div><pre><span></span>$<span> </span>sudo<span> </span>add-apt-repository<span> </span>ppa:deadsnakes/ppa
$<span> </span>sudo<span> </span>apt<span> </span>update
$<span> </span>sudo<span> </span>apt<span> </span>install<span> </span><span>\</span>
<span>    </span>jq<span> </span><span>\</span>
<span>    </span>openexr<span> </span><span>\</span>
<span>    </span>libimage-exiftool-perl<span> </span><span>\</span>
<span>    </span>libopenexr-dev<span> </span><span>\</span>
<span>    </span>python3-pip<span> </span><span>\</span>
<span>    </span>python3.12-venv
</pre></div>
<p>Note, the <tt><span>libimage-exiftool-perl</span></tt> package installs <tt>exiftool</tt> version 12.76+dfsg-1 which was released at the end of January 2024. Since then, there have been at least ten releases that have addressed issues or enhanced exiftool's HEIC support.</p>
<p>The above version should work fine for the steps in this post but be mindful that any issues you encounter going forward might be resolved with a more up to date version of exiftool.</p>
<p>I'll be using <a href="https://github.com/kellyjonbrazil/jc?tab=readme-ov-file">JSON Convert</a> (jc) to convert the output of various CLI tools into JSON.</p>
<div><pre><span></span>$<span> </span>wget<span> </span>https://github.com/kellyjonbrazil/jc/releases/download/v1.25.2/jc_1.25.2-1_amd64.deb
$<span> </span>sudo<span> </span>dpkg<span> </span>-i<span> </span>jc_1.25.2-1_amd64.deb
</pre></div>
<p>I'll clone Finn Jaeger's HEIC Shenanigans repo.</p>
<div><pre><span></span>$<span> </span>git<span> </span>clone<span> </span>https://github.com/finnschi/heic-shenanigans<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans
</pre></div>
<p>I'll set up a Python Virtual Environment and install a few dependencies.</p>
<div><pre><span></span>$<span> </span>python3<span> </span>-m<span> </span>venv<span> </span>~/.iphone_depth
$<span> </span><span>source</span><span> </span>~/.iphone_depth/bin/activate

$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span>-r<span> </span><span>\</span>
<span>    </span>~/heic-shenanigans/requirements.txt
$<span> </span>python3<span> </span>-m<span> </span>pip<span> </span>install<span> </span><span>\</span>
<span>    </span>OpenImageIO
</pre></div>
<p>I'll use <a href="https://github.com/darbyjohnston/DJV/releases">DJV</a> v2.0.8 to view EXR imagery produced in this post.</p>
</div>
<div id="an-iphone-15-pro-image">
<h2>An iPhone 15 Pro Image</h2>
<p><a href="https://www.linkedin.com/in/joel-joseph-b865981ab/">Joel Joseph</a>, a subject matter expert on the ArcGIS Desktop Products Suite in Mumbai, India, was kind enough to send me an HEIC-contained image he took on his iPhone 15 Pro. I'll use this image in this post. Below is a screenshot.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/Photos_5WbqRgJbWO.jpg">
</a>
</p></div>

<div id="converting-an-heic-into-exr">
<h2>Converting an HEIC into EXR</h2>
<p>The <a href="https://github.com/AcademySoftwareFoundation">Academy Software Foundation</a> fosters various <a href="https://www.aswf.io/projects/">open source projects</a> and standards used in film, television and other creative industries. Its <a href="https://www.aswf.io/members/">members</a> include the Academy of Motion Picture Arts and Sciences, Disney, Nvidia and Netflix to name a few.</p>
<p>Below is a screenshot of their <a href="https://landscape.aswf.io/">landscape</a> of projects.</p>
<p><a href="https://landscape.aswf.io/">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/brave_g5ZWZ7GWLE.png">
</a></p><p>One of their projects is <a href="https://github.com/AcademySoftwareFoundation/openexr">OpenEXR</a>, a high-dynamic range (HDR) image file format. It was originally developed by Industrial Light and Magic (ILM) in 1999 and was later made open source in 2003. It's used in producing visual effects and 3D renderings.</p>
<p>Below I'll convert iPhone 15 Pro image into an OpenEXR file.</p>
<div><pre><span></span>$<span> </span>python<span> </span>~/heic-shenanigans/heic_to_exr.py<span> </span><span>\</span>
<span>    </span>IMG_E2153.HEIC
</pre></div>
<p>The resulting file is 468 MB. Below is a screenshot of it in DJV.</p>
<p><a href="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
<img alt="iPhone 15 Pro Depth Maps" src="https://tech.marksblogg.com/theme/images/iphone_depth/djv_EIMLqnjJmM.jpg">
</a></p><p>The above file was produced by making several calls to <tt>oiiotool</tt>, an image processing tool from the <a href="https://github.com/AcademySoftwareFoundation/OpenImageIO">OpenImageIO</a> project, which is also apart of the Academy Software Foundation.</p>
<p>Below I've annotated the calls to oiiotool made by the above script.</p>
<p>The script will first get the dimensions of the source image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>--info<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>It will then produce a base image, converting the source image from an sRGB curve through Linear P3 and onto ACEScg.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_base.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'sRGB - Texture'</span><span> </span><span>'Linear Rec.709 (sRGB)'</span><span> </span><span>\</span>
<span>    </span>--colorconvert<span> </span><span>'Linear P3-D65'</span><span>  </span><span>'ACES - ACEScg'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/base.exr
</pre></div>
<p>The above OCIO file is an <a href="https://opencolorio.org/">OpenColorIO</a> colour profile file. OpenColorIO, also under the Academy Software Foundation, is a colour management suite. The project provides several <a href="https://github.com/AcademySoftwareFoundation/OpenColorIO-Config-ACES/releases">reference configurations</a> including one from the <a href="https://en.wikipedia.org/wiki/Academy_Color_Encoding_System">Academy Color Encoding System (ACES)</a>. The OCIO file is text-based and contains 1,242 lines of content.</p>
<div><pre><span></span>$<span> </span>head<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio
</pre></div>
<div><pre><span></span>ocio_profile_version: 2.1

environment:
  {}
search_path: ""
strictparsing: true
luma: [0.2126, 0.7152, 0.0722]
name: studio-config-v1.0.0_aces-v1.3_ocio-v2.1
description: |
  Academy Color Encoding System - Studio Config [COLORSPACES v1.0.0] [ACES v1.3] [OCIO v2.1]
</pre></div>
<p>Then, an EXR-based gain map will be produced by converting the source HDR gain map from Rec709 curve to Linear.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_hdrgainmap_50.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>--colorconfig<span> </span>studio-config-v1.0.0_aces-v1.3_ocio-v2.1.ocio<span> </span><span>\</span>
<span>    </span>--ocionamedtransform<span> </span><span>'Rec.709 - Curve'</span><span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap.exr
</pre></div>
<p>That gain map will then be converted into RGB by duplicating the Y channel three times.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>gainmap.Y,gainmap.Y,gainmap.Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr
</pre></div>
<p>Then, the HDR gain map's headroom value will be extracted.</p>
<div><pre><span></span>$<span> </span>exiftool<span> </span>-HDRGainMapHeadroom<span> </span>-b<span> </span>/tmp/tmpc3kmiaka/input_base.tiff
</pre></div>
<p>The gain map will then be scaled using the inverse of the HDR headroom value captured above.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>    </span>--mulc<span> </span>-0.12135654640000004<span> </span><span>\</span>
<span>    </span>--addc<span> </span><span>1</span>.0<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr
</pre></div>
<p>Then, the base image will be multiplied by the scaled gain map to create an HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_scaled.exr<span> </span><span>\</span>
<span>           </span>--mul<span> </span><span>\</span>
<span>           </span>--chnames<span> </span>R,G,B<span> </span><span>\</span>
</pre></div>
<p>Then, the Y channel from the depth map will be used to create an EXR-formatted depth map.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/input_depth_0.tiff<span> </span><span>\</span>
<span>    </span>--ch<span> </span>Y<span> </span><span>\</span>
<span>    </span>--chnames<span> </span>depth.Y<span> </span><span>\</span>
<span>    </span>--resize<span> </span>4032x3024<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/depth.exr
</pre></div>
<p>If the source image contained mattes they would be processed at this stage.</p>
<p>The first step in constructing the final EXR file starts by copying in the RGB channels from the EXR-formatted HDR base image.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/hdr_base.exr<span> </span><span>\</span>
<span>    </span>--ch<span> </span>R,G,B<span> </span><span>\</span>
<span>    </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the SDR channels are added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/base.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>sdr.R,sdr.G,sdr.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the gain map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/gainmap_rgb.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>gainmap.R,gainmap.G,gainmap.B<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>Then the depth map is added.</p>
<div><pre><span></span>$<span> </span>oiiotool<span> </span>/tmp/tmpc3kmiaka/final.exr<span> </span><span>\</span>
<span>           </span>/tmp/tmpc3kmiaka/depth.exr<span> </span><span>\</span>
<span>      </span>--ch<span> </span>depth.Y<span> </span><span>\</span>
<span>      </span>--siappend<span> </span><span>\</span>
<span>      </span>-o<span> </span>/tmp/tmpc3kmiaka/final.exr
</pre></div>
<p>If there were any matte layers, they would be added here.</p>
<p>The <tt>final.exr</tt> file is then moved to a <tt>&lt;prefix&gt;_acesCG.exr</tt> file alongside the source imagery.</p>
</div>

        </div><p>
            Thank you for taking the time to read this post. I offer both consulting and hands-on development services to clients in North America and Europe. If you'd like to discuss how my offerings can help your business please contact me via <a href="https://uk.linkedin.com/in/marklitwintschik/">LinkedIn</a>.
        </p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Code (171 pts)]]></title>
            <link>https://mistral.ai/products/mistral-code</link>
            <guid>44183515</guid>
            <pubDate>Wed, 04 Jun 2025 17:50:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/products/mistral-code">https://mistral.ai/products/mistral-code</a>, See on <a href="https://news.ycombinator.com/item?id=44183515">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><nav aria-label="Main" data-orientation="horizontal" dir="ltr"><div><a rel="home" aria-label="Home" href="https://mistral.ai/"></a><div><ul data-orientation="horizontal" dir="ltr"><li></li><li></li><li></li><li></li><li><a target="_self" href="https://mistral.ai/pricing">Pricing</a></li><li></li></ul></div></div></nav></div><div><div><p>Lightning-fast completions, deep code understanding, and agentic software engineering—right where you work.</p></div><div><p><img alt="744a3c97-aba4-4cbd-9697-a4a4ead168c1" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F744a3c97-aba4-4cbd-9697-a4a4ead168c1&amp;w=3840&amp;q=75"></p></div></div><div id=""><div><p>Code with intelligence, control with confidence.</p><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--><div><div><h3><p>Code with intelligence, control with confidence.</p></h3><p>Transform your development workflows with an AI coding assistant that deeply understands your codebase. Mistral Code delivers intelligent code completion, generation, and autonomous task execution right where you work.</p></div><div data-state="active" data-orientation="horizontal" role="tabpanel" aria-labelledby="radix-«Rm6fdbnb»-trigger-Intelligent coding assistance" id="radix-«Rm6fdbnb»-content-Intelligent coding assistance" tabindex="0" dir="ltr"><div><p>Elevate your coding with AI that understands, completes, and optimizes your code with frontier, purpose-built software engineering models.</p></div><div><p><img alt="ac37ae27-1d08-49aa-b191-26758baca735" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fac37ae27-1d08-49aa-b191-26758baca735&amp;w=3840&amp;q=75"></p></div></div></div></div><div id=""><div><h3><p>Why Mistral Code?</p></h3></div><div><div><div><p><img alt="icon-key-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F78e613ea-3ca5-442d-9c39-9df525983ec4&amp;w=3840&amp;q=75"></p></div><div><p><span><p>State-of-the art AI-powered software development, in your control.</p></span></p><p>10X developer productivity and efficiency without sacrificing the privacy and safety of your codebase. Mistral Code can be deployed where your most important code resides.</p></div></div><div><div><p><img alt="icon-pencil-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fba1cf7ea-52a0-4e1d-8efd-44104be7fd8a&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Frontier coding models, fully customizable and tunable to your codebase.</p></span></p><p>Advanced models such as Codestral and Devstral under the hood, further customizable with fine-tuning, post-training, and distillation.</p></div></div><div><p><span><p>One platform across the full AI-powered software stack.</p></span></p><p>End‑to‑end tooling from models to IDE in one first-party platform. Unified SLA, faster support, and simpler compliance posture.</p></div><div><div><p><img alt="icon-lightning-beige" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Ffb0d1f98-7144-473a-a89a-123a1251df17&amp;w=3840&amp;q=75"></p></div><div><p><span><p>Deep code understanding, generation, and completion.</p></span></p><p>Advanced understanding of files and integrations with agent scaffolds for benchmark-setting performance on agentic tasks, along with lightning-fast code generation and completion.</p></div></div></div></div><div id=""><h4>
<center>Frontier intelligence, in your favorite IDE.</center>
</h4><div>
<center>Our unique mix of purpose-built models delivers an unmatched combination of lightning-fast completions and deep code understanding.</center>
</div></div><div id=""><p><img alt="c7ab7262-be13-45c1-8498-e37551db9527" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fc7ab7262-be13-45c1-8498-e37551db9527&amp;w=3840&amp;q=75"></p></div><div id=""><p>Speed development time.</p><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div><div><div><div><p>Tab to complete</p><p>Get intelligent code suggestions in real-time as you type, with multi-line completions tailored to your codebase.</p></div><p><img alt="350b6506-9400-45c2-8c1f-e79f7f56ba66" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F350b6506-9400-45c2-8c1f-e79f7f56ba66&amp;w=3840&amp;q=75"></p></div><div><div><p>Context-aware chat</p><p>Ask questions about your code with the AI assistant that understands your entire project.</p></div><p><img alt="4778258c-77fe-4535-b018-d48546f1f0ba" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4778258c-77fe-4535-b018-d48546f1f0ba&amp;w=3840&amp;q=75"></p></div><div><div><p>Code edit</p><p>Select any code block and transform it using natural language instructions.</p></div><p><img alt="69a1aa81-e622-4844-a100-bd86b5705e98" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F69a1aa81-e622-4844-a100-bd86b5705e98&amp;w=3840&amp;q=75"></p></div><div><div><p>Intelligent search and retrieval</p><p>Find and fetch relevant code through natural language queries.</p></div><p><img alt="80c19f46-01f7-45fa-a34f-42e2dc161ddc" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F80c19f46-01f7-45fa-a34f-42e2dc161ddc&amp;w=3840&amp;q=75"></p></div><div><div><p>Autonomous coding</p><p>Handle complex engineering tasks without leaving your IDE.</p></div><p><img alt="5e1da16b-081d-44d3-b6d4-b5859ebcb75e" loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 768px) 90vw, (max-width: 1200px) 50vw, 75vw" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=384&amp;q=75 384w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=640&amp;q=75 640w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=750&amp;q=75 750w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=828&amp;q=75 828w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1080&amp;q=75 1080w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1200&amp;q=75 1200w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=1920&amp;q=75 1920w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=2048&amp;q=75 2048w, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75 3840w" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5e1da16b-081d-44d3-b6d4-b5859ebcb75e&amp;w=3840&amp;q=75"></p></div></div></div><section><div><h3><p>How teams use Mistral Code today.</p></h3></div><div><div><div><p>Code completion</p><div><p><img alt="check-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F374393f2-f613-4c4f-add6-74eb3c006ae3&amp;w=64&amp;q=75"></p><p>Suggests completions for code as you type, helping to speed up the coding process and reduce errors.</p></div></div><div><p>Code debugging and refactoring</p><div><p><img alt="glasses-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F4d8c0af8-86d9-4daa-8a2b-b60b9e788a2e&amp;w=64&amp;q=75"></p><p>Identifies syntax and logical errors, typos in real-time and provides suggestions for improving code structure.</p></div></div><div><p>Code documentation</p><div><p><img alt="icon-copy-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F66f140a3-3862-43a7-b7ab-a010db5947fc&amp;w=64&amp;q=75"></p><p>Automatically generates documentation for code, including comments and API documentation, to improve code maintainability.</p></div></div></div><div><div><p>Code testing</p><div><p><img alt="icon-computer-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F98875468-ed52-40e5-8007-124bd2782ac5&amp;w=64&amp;q=75"></p><p>Generates unit and system tests to ensure that the code generated is fully functional.</p></div></div><div><p>Code migration</p><div><p><img alt="icon-earth-black" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2Fecdac1ed-522e-4a58-a3cc-23d03c40ba6e&amp;w=64&amp;q=75"></p><p>Generate code snippets in the target language, helping to translate and adapt existing codebases to new languages or frameworks.</p></div></div><div><p>Code performance</p><div><p><img alt="icon-lightbulb" loading="lazy" width="27" height="27" decoding="async" data-nimg="1" srcset="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=32&amp;q=75 1x, https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75 2x" src="https://mistral.ai/_next/image?url=https%3A%2F%2Fcms.mistral.ai%2Fassets%2F5d48662e-536d-48d5-8a7b-1f388b32e873&amp;w=64&amp;q=75"></p><p>Analyzes code performance and identifies bottlenecks, helping developers optimize their code for better speed and efficiency.</p></div></div></div></div></section><div id=""><p>Powering the world's pioneers.</p></div><div id=""><h2><p>Your AI, from models to outputs.</p></h2><p>Experience the power of Mistral Code directly in your favorite IDE.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Are No Longer a Serious Country – Paul Krugman (136 pts)]]></title>
            <link>https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</link>
            <guid>44182634</guid>
            <pubDate>Wed, 04 Jun 2025 16:39:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country">https://paulkrugman.substack.com/p/we-are-no-longer-a-serious-country</a>, See on <a href="https://news.ycombinator.com/item?id=44182634">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>“If you’re explaining, you’re losing.” This line is usually attributed to Ronald Reagan. Whoever said it definitely had a point, and not just about politics. If you’re trying to explain to people, be they voters or bond investors, that you aren’t really as bad or untrustworthy as you seem, you’re already in deep trouble.</p><p><span>So when I saw Scott Bessent, the treasury secretary, </span><a href="https://www.ft.com/content/9c652f09-cfca-4078-9e2b-5fdacd772a75" rel="">declaring</a><span> Sunday that “The United States of America is never going to default, that is never going to happen,” my reaction was, “Uh-oh.”</span></p><p>And it’s not just me. For generations investors have treated U.S. government debt as the ultimate safe asset. Whenever disaster strikes — even if it’s disaster largely made in America, like the 2008 subprime crisis — bond buyers pile into U.S. Treasuries, because America is a serious country, and the idea that we would fail to honor our debts was unthinkable.</p><p>But are we still that country? Markets seem to have doubts.</p><p><span>Yesterday the Financial Times had a neat chart showing that there used to be a clear relationship between U.S. interest rates and the international value of the dollar. Actually, the chart was a bit </span><em>too</em><span> neat: When I set out to reproduce it, I found that the FT chose a time period during which the relationship looked especially clear. Still, it used to be true that when U.S. interest rates rose, so did the dollar, because higher yields pulled in foreign capital. But since Donald Trump returned to power, that relationship has broken down. Instead, we’ve seen a combination of rising interest rates and a </span><em>falling</em><span> dollar:</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75158,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F288d0287-3f49-483e-bdb7-9bc89b35655a_800x450.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>As many have noted, what we’ve been seeing in recent months, with interest rates and the dollar moving in opposite directions, doesn’t look like what we normally see in the United States, or for that matter advanced nations in general. Instead, it’s the kind of thing one sees in emerging markets, where big market moves often reflect crises of confidence: International investors lose faith, pulling their money out, and capital flight causes both a falling currency and rising interest rates.</p><p>Here, for example, is what Mexico looked like during the “tequila crisis” of 1994-5, which involved both soaring interest rates and a plunging peso:</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png" width="800" height="450" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:450,&quot;width&quot;:800,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:61317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://paulkrugman.substack.com/i/165025603?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d9f259a-0b9d-4799-8b3a-5480d474ef30_800x450.png 1456w" sizes="100vw"></picture></div></a></figure></div><p>Why are markets beginning to treat America as unreliable? It’s not just the debt numbers. Yes, we have large debts, but we’re an immensely wealthy country that, among other things, has lower average taxes than most of our peers. So we certainly have the resources to honor our debts.</p><p><span>But do we have the political will? Maybe even more important, do we have the political </span><em>seriousness</em><span>?</span></p><p>Like many economists, I’ve spent a lot of time analyzing the substance of Trump’s tariffs — how much they are likely to raise prices and reduce trade volumes. I’ve also written about the impacts of policy uncertainty, about how hard it is for businesses to make plans when they have little idea what tariff rates will be even a few months from now, let alone over the next few years.</p><p><span>But I wonder whether we’ve spent enough time looking at the policy </span><em>process</em><span> — how decisions get made in Trump’s America. Consider: On April 2, “Liberation Day,” Trump announced extremely high tariffs on many countries, the biggest tariff hike in U.S. history. The tariff rates — which differed hugely from country to country — were determined by a formula universally panned as stupid and ridiculous. And this tariff announcement was made with so little planning and forethought that it included taxes on imports from remote islands inhabited only by </span><a href="https://www.bbc.com/news/articles/cly8xlj0485o" rel="">penguins</a><span>.</span></p><p><span>Then, a week later, these tariffs were replaced by a completely different set of tariffs. How did that happen? Two of Trump’s cabinet members were able to beard him in the Oval Office while Peter Navarro, responsible for the original tariffs, was </span><a href="https://www.wsj.com/politics/policy/trump-tariff-pause-navarro-bessent-lutnick-b9e864fb?gaa_at=eafs&amp;gaa_n=ASWzDAiBOSVk0wHQwGquJzeffoF7WWhmHKUg490AKNKAI7beYwEiwJ8au1jW-gBGH4A%3D&amp;gaa_ts=683dcdff&amp;gaa_sig=xIdEPFjIsG6tDwLXo4RJ5eBSP5faPCp3McPx6CkxYZ4dSRr3EJUr_ysQIYStqcwE5VL_0ZhadzjRqX58YVM2eA%3D%3D" rel="">in another meeting</a><span>.</span></p><p>Does this sound like policymaking in a serious country?</p><p>Then there’s the budget bill making its way through Congress. It’s a terrible thing, imposing savage cuts on social programs (and decimating U.S. science) while giving such big tax cuts to the wealthy that it will explode the deficit. But content aside, notice that this hugely important piece of legislation is being rushed through with essentially no hearings or analysis.</p><p>And when outsiders, including the Congressional Budget Office and a variety of think tanks — conservative and centrist as well as progressive — have put out the analyses the bill’s sponsors won’t, pointing out the likely effects on debt, health coverage, and so on, the G.O.P. response has basically been to accuse all of the independent analysts of being part of a globalist conspiracy.</p><p><span>Wait, it gets worse. The name of the legislation — not its nickname, its official title — is the </span><a href="https://www.congress.gov/bill/119th-congress/house-bill/1/text" rel="">One Big Beautiful Bill Act</a><span>, because that’s what Trump has been calling it. Are we a mature republic with a normal head of state, or are we being ruled by Kim Jong Un in orange makeup?</span></p><p><span>Why, next thing you’ll be telling me that key policy decisions, leading to layoffs of hundreds of thousands of federal workers and many deaths around the world, have been made by a presidential crony whose erratic behavior may have reflected massive consumption of ketamine, Ecstasy and psychedelic mushrooms. </span><a href="https://www.nytimes.com/2025/05/30/us/elon-musk-drugs-children-trump.html" rel="">Oh, wait</a><span>.</span></p><p><span>Imagine yourself as a foreigner considering investing in the United States. You may well know that the One Big Beautiful Bill Act contains a “</span><a href="https://www.bloomberg.com/news/articles/2025-05-30/trump-revenge-tax-would-lower-foreign-investment-in-us-scorekeeper-predicts?sref=qzusa8bC" rel="">revenge</a><span>” provision that would allow the U.S. government to impose extra taxes on foreign investors whose home countries have policies America doesn’t like. You probably know that one of Trump’s advisers has suggested the forced conversion of short-term debt into </span><a href="https://thedailyeconomy.org/article/turning-treasury-securities-into-century-bonds-is-a-dead-end/" rel="">century bonds</a><span>. Once upon a time everyone would have dismissed these things as stuff that couldn’t happen in America. Now? Who knows?</span></p><p>In a way, the amazing thing is that we haven’t seen even more capital flight. Presumably investors still can’t believe that America has changed so much from the responsible, reliable nation it seemed to be just a few months ago.</p><p>But I think they’re headed for a rude awakening.</p><p>MUSICAL CODA</p><div id="youtube2-pvlbrKKEPdA" data-attrs="{&quot;videoId&quot;:&quot;pvlbrKKEPdA&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/pvlbrKKEPdA?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[VC money is fueling a global boom in worker surveillance tech (188 pts)]]></title>
            <link>https://restofworld.org/2025/employee-surveillance-software-vc-funding/</link>
            <guid>44182582</guid>
            <pubDate>Wed, 04 Jun 2025 16:35:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://restofworld.org/2025/employee-surveillance-software-vc-funding/">https://restofworld.org/2025/employee-surveillance-software-vc-funding/</a>, See on <a href="https://news.ycombinator.com/item?id=44182582">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<!-- Article Start -->
				
<p>Technologies that promise to track, manage, and supervise workers, increasingly using artificial intelligence, are getting entrenched in the developing world, according to a new <a href="https://home.coworker.org/little-tech-goes-global/">report</a> by Coworker.org, a labor rights nonprofit based in New York.&nbsp;</p>



<p>Audits of more than 150 startups and regional companies based in Kenya, Nigeria, Colombia, Brazil, Mexico, and India showed workplace surveillance is expanding in scale and sophistication, the researchers said. While large <a href="https://www.computerweekly.com/news/252521757/Microsoft-Office-365-has-ability-to-spy-on-workers">corporations</a> are <a href="https://www.cisco.com/site/us/en/learn/topics/general/what-is-employee-monitoring.html">known</a> to develop surveillance technologies, a so-called Little Tech ecosystem of mostly unregulated, venture capital-funded startups and small vendors making these products has grown since Covid-19, the report found. The term “Little Tech” was popularized by the VC firm <a href="https://a16z.com/the-little-tech-agenda/">Andreessen Horowitz</a>, which argued that excessive regulation was stifling innovation.</p>



<figure></figure>



<p><a href="https://restofworld.org/2022/workers-managed-algorithms/">Algorithmic management</a> and surveillance tools are getting even more intrusive in gig work, and are entering offices and the informal labor sector as well, Wilneida Negrón, director of research and policy at Coworker.org and a co-author of the report, told <em>Rest of World</em>.&nbsp;</p>



<p>“The pressure of the hyper-surveillance creates a lot of stress and creates a lot of uncertainty for workers. It brings a culture of suspiciousness,” she said.&nbsp;</p>



<p>Investments by Silicon Valley-based VC firms led to a boom in tech startups globally after Covid-19, Negrón said. This has carried over to companies building bossware products in the developing world, she said.&nbsp;&nbsp;</p>



<figure></figure>



<p>The technologies include biometric tracking, AI-powered productivity monitoring, and predictive analytics, the report found. Worker data is continuously collected and analyzed by algorithms with the stated aim to improve hiring, evaluate performance, and optimize processes.&nbsp;</p>



<p>Most managers in wealthier nations say algorithmic management tools improve their decision-making, according to a 2024 <a href="https://www.oecd.org/content/dam/oecd/en/publications/reports/2025/02/algorithmic-management-in-the-workplace_3c84ed6d/287c13c4-en.pdf?utm_source=chatgpt.com">survey</a> of over 6,000 employers by the Organisation for Economic Co-operation and Development. More than 90% of American managers used such tools, especially to reward or sanction employees.</p>



<p>Many tools are first deployed in Latin America, where labor laws are less strictly enforced, according to Ayden Férdeline, a tech policy researcher in Berlin and a co-author of the report.</p>



<p>“There is a Latin America testing ground for products,” he told <em>Rest of World</em>. “If they are successful, they tend to be deployed in other jurisdictions, oftentimes with additional safeguards, sometimes not.”</p>



<p>Many workers are unaware of how their information is collected and used, Férdeline said.&nbsp;</p>



<p>Some gig workers in Kenya, Guatemala, and Brazil said bossware tools make them feel surveilled, and that they have less control over their work. In Porto Alegre, Brazil, Uber driver Carina Trindade told <em>Rest of World</em> she feels the app monitors her continuously, tracking her speed and braking patterns. The app has permissions <a href="https://dig.watch/updates/uber-starts-testing-tool-records-video-inside-car-during-rides">to access</a> her mic and camera, she said.&nbsp;</p>



<p>Uber spokesperson Gabriel Gabira said drivers have the option to record trips, and <a href="https://help.uber.com/en/driving-and-delivering/article/record-my-ride---frequently-asked-questions?nodeId=7f86cc51-2c3e-4888-8dc1-0ac2b6337b92">privacy terms</a> are followed to access the footage.</p>



<p>In Nairobi, Godfrey Sanya Wanga, a driver for ride-hailing firm SafeBoda, told <em>Rest of World </em>he felt the app undercharged a customer. “I really wanted to ask [the customer] to pay me more, but I remembered that I was being monitored and this would bring me trouble if the client reported me,” he said. SafeBoda did not respond to a request for comment.</p>



<p>Several nations have data protection and privacy laws, including Brazil, Nigeria, and Kenya. But enforcement is inconsistent, the report said.</p>



<p>Here are five current uses of algorithmic management tools. The companies mentioned below did not comment, unless otherwise stated.&nbsp;</p>



<p>1. <strong>Timekeeping and attendance systems&nbsp;</strong></p>



<p>What: Platforms that track the attendance of workers, often using geolocation and biometrics to verify presence.&nbsp;</p>



<p>Example: <a href="https://www.rankmi.com/es/productos/control-asistencia-y-turnos">Rankmi</a>, based in Chile, uses biometrics and geolocation to track workers. The platform also <a href="https://www.rankmi.com/es/ia-para-hr">gives</a> workers continuous performance feedback and evaluates job applicants using AI.&nbsp;</p>



<p>2. <strong>Biometric and identity verification tools</strong></p>



<p>What: Tools that use fingerprint and facial-recognition checks, special digital signatures stored on a secure network, and official records to confirm a worker’s identity before granting access.&nbsp;</p>



<p>Example: Cincel, based in Mexico, provides <a href="https://www.cincel.digital/productos/verificacion-identidad/">identity verification</a> tools that do various checks including biometrics, and also cross-check against government databases and <a href="https://www.cincel.digital/productos/background-check/">blacklists</a>.</p>



<p>3. <strong>Performance and productivity monitoring platforms</strong></p>



<p>What: Dashboards that score workers using tracked metrics such as keystrokes, transaction counts, customer interactions, and task completion times.</p>



<p>Example: <a href="https://www.ahgora.com/">Ahgora</a>, based in Brazil, offers HR software that allows managers to continually “oversee team attendance in real-time” and that tracks productivity. It uses the data to offer predictions about work, such as potential issues with attendance, which can inform decision-making.&nbsp;</p>



<p>4. <strong>Algorithmic management and predictive analytics</strong></p>



<p>What: Platforms that<strong> </strong>automate HR functions, such as hiring shortlists, performance reviews, attrition forecasting, and also unionization-risk scoring.</p>



<p>Example: Visier’s AI-powered analytics platform <a href="https://www.visier.com/solutions/internal-mobility/">analyzes</a> HR data and provides insights, including resignation risk. The platform is used by global firms including Deloitte, Accenture, and Tata Consultancy Services.&nbsp;</p>



<p>Andrea Derler, principal of research and customer value at Visier, told <em>Rest of World</em> the platform only “processes data that organizations load into the platform, and we are not responsible for the way the data and insights we help provide is being used.”&nbsp;</p>



<p>5.&nbsp; <strong>Gig economy and field workforce tracking</strong></p>



<p>What: Apps that use the workers’ smartphones to dispatch and route deliveries. They use location, trip history, and ratings to allocate jobs and <a href="https://restofworld.org/2021/only-gojek-knows-this-mystery/">evaluate performance</a>. Workers are managed mostly by platforms rather than humans.</p>



<p>Example: <span><mark><a href="https://restofworld.org/tag/rappi" aria-label="Click to learn more about Rappi.">Rappi<span>i</span></a></mark><span><span><a href="https://restofworld.org/tag/rappi">Rappi</a><svg aria-label="Close" role="button"><use xlink:href="#X"></use></svg></span>Rappi, a Colombian company, has been providing delivery services across most Latin American countries since 2015.<span><a href="https://restofworld.org/tag/rappi"><span>READ MORE</span><svg><use xlink:href="#chevron"></use></svg></a></span></span></span>, a Colombian delivery app, tracks workers in real time. It has <a href="https://americasmi.com/insights/rappi-evolving-business-model-impact-latam-logistics/">auto accept</a>, where a rider can’t decline orders — and it’s mandatory to qualify for bonuses. Delivery worker Carolina Ramírez told <em>Rest of World </em>she works 14-hour days to earn a bonus of 100,000 pesos ($25) every week, leaving her little time for anything else. “My boss is the app. It’s unfair because to earn a good salary, I have to dedicate myself almost exclusively to this,” she said.</p>
				<!-- Article End -->
							</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[IRS Direct File on GitHub (405 pts)]]></title>
            <link>https://chrisgiven.com/2025/05/direct-file-on-github/</link>
            <guid>44182356</guid>
            <pubDate>Wed, 04 Jun 2025 16:16:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://chrisgiven.com/2025/05/direct-file-on-github/">https://chrisgiven.com/2025/05/direct-file-on-github/</a>, See on <a href="https://news.ycombinator.com/item?id=44182356">Hacker News</a></p>
Couldn't get https://chrisgiven.com/2025/05/direct-file-on-github/: Error: getaddrinfo ENOTFOUND chrisgiven.com]]></description>
        </item>
        <item>
            <title><![CDATA[Meta found 'covertly tracking' Android users through Instagram and Facebook (195 pts)]]></title>
            <link>https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</link>
            <guid>44182204</guid>
            <pubDate>Wed, 04 Jun 2025 16:00:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083">https://news.sky.com/story/meta-found-covertly-tracking-android-users-through-instagram-and-facebook-13379083</a>, See on <a href="https://news.ycombinator.com/item?id=44182204">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-component-name="ui-article-body" data-highlight-intro="true">
      
      <p>Meta and search engine company Yandex have been "covertly tracking" Android users in the background of their devices, according to experts.</p><p>Academics at the Radboud University in the Netherlands and IMDEA Networks said they discovered Meta and Yandex have been tracking Android users' browser activity without their consent and then using the data in their apps.</p>
<p>Meta said it was looking into the issue, while Yandex denied collecting any sensitive data.</p><p>Gunes Acar, assistant professor at Radboud University, said the "covert" data collection was spotted in January.</p><p>He said he discovered Meta's apps, including Facebook and Instagram, and Yandex's apps, such as Yandex Maps, were sitting in the background of Android devices and loading a script that sent data locally back to apps on users' phones.</p>
<p>The scripts bypassed Android's security measures and meant that Meta and Yandex could track what users were doing on web browsers, without the user consenting or even knowing, according to the expert.</p><p>"They are bridging these two worlds that we think are separate; web browsing and mobile app activities," Dr Acar told Sky News.</p><p>"That's very shocking."</p><p>The apps were able to track users' browser data on all major Android browsers, even if the user was in incognito mode, the academics said.</p>        
<p>"It's really concerning because it negates every privacy control that you have in modern browsers and also in modern mobile platforms like Android," said Narseo Vallina-Rodriguez, associate professor at IMDEA Networks, to Sky News.</p><p><strong><a href="https://news.sky.com/topic/google-5876/1" target="_blank">Google</a></strong>, which owns the Android operating system, confirmed the covert activity to Sky News.</p><p>It said Meta and Yandex used <strong><a href="https://news.sky.com/topic/android-8090/1" target="_blank">Android's</a></strong> capabilities "in unintended ways that blatantly violate our security and privacy principles".</p><p><strong>What have Meta and Yandex said?</strong></p><p>Meta told Sky News it was quickly looking into the issue.</p><p>"We are in discussions with Google to address a potential miscommunication regarding the application of their policies," said a Meta spokesperson.</p><p>"Upon becoming aware of the concerns, we decided to pause the feature while we work with Google to resolve the issue."</p><p>Yandex said it "strictly complies with data protection standards", adding: "The feature in question does not collect any sensitive information and is solely intended to improve personalisation within our apps."</p><p><strong>Read more science and tech news:<br><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">AI foot scanner recognises heart warning signs</a></strong><br><strong><a href="https://news.sky.com/story/ai-foot-scanner-recognises-warning-signs-of-heart-failure-to-keep-people-out-of-hospital-researchers-say-13378605" target="_blank">Coffee 'helps women age more healthily'</a></strong></p>    
<p>Meta appeared to have been doing the data tracking for around eight months, while Yandex had since 2017, the academics said.</p><p>"We found that Facebook was doing it on roughly 16,000 websites when visited from the EU, [...] Yandex was doing this on 1,300 websites," said Tim Vlummens, a PHD student at KU Leuven who worked on the research.</p><p>Google told Sky News it had already "implemented changes to mitigate these invasive techniques and have opened our own investigation and are directly in touch with the parties".</p>     <a href="https://news.sky.com/download-app" target="blank" data-tracking-label="ui-app-promo-download-link" data-type="" data-component-name="ui-app-promo">
        
    </a>


<p>The tech giant did not respond when asked what repercussions Meta and Yandex were facing for their conduct.</p><p>Firefox, Microsoft Edge and DuckDuckGo browsers were also affected, with Firefox owner Mozilla and DuckDuckGo engineers taking action to stop any future covert tracking.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Prompt Engineering Playbook for Programmers (157 pts)]]></title>
            <link>https://addyo.substack.com/p/the-prompt-engineering-playbook-for</link>
            <guid>44182188</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://addyo.substack.com/p/the-prompt-engineering-playbook-for">https://addyo.substack.com/p/the-prompt-engineering-playbook-for</a>, See on <a href="https://news.ycombinator.com/item?id=44182188">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>Developers are increasingly relying on AI coding assistants to accelerate our daily workflows. These tools can autocomplete functions, suggest bug fixes, and even generate entire modules or MVPs. Yet, as many of us have learned, the </span><em>quality</em><span> of the AI’s output depends largely on the </span><em>quality of the prompt</em><span> you provide. In other words, </span><strong>prompt engineering</strong><span> has become an essential skill. A poorly phrased request can yield irrelevant or generic answers, while a well-crafted prompt can produce thoughtful, accurate, and even creative code solutions. This write-up takes a practical look at how to systematically craft effective prompts for common development tasks.</span></p><p><span>AI pair programmers are powerful but not magical – they have no prior knowledge of your specific project or intent beyond what you tell them or include as context. The more information you provide, the better the output. We’ll distill key prompt patterns, </span><strong>repeatable frameworks</strong><span>, and memorable examples that have resonated with developers. You’ll see side-by-side comparisons of </span><strong>good vs. bad prompts</strong><span> with actual AI responses, along with commentary to understand why one succeeds where the other falters. </span><strong>Here’s a cheat sheet to get started:</strong></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png" width="1456" height="1915" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/be144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1915,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:520317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbe144e78-4d86-45c9-bc61-28836dee7265_1784x2346.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>Prompting an AI coding tool is somewhat like communicating with a very literal, </span><em>sometimes</em><span> knowledgeable collaborator. To get useful results, you need to set the stage clearly and guide the AI on </span><em>what</em><span> you want and </span><em>how</em><span> you want it. </span></p><p>Below are foundational principles that underpin all examples in this playbook:</p><ul><li><p><strong>Provide rich context.</strong><span> Always assume the AI knows nothing about your project beyond what you provide. Include relevant details such as the programming language, framework, and libraries, as well as the specific function or snippet in question. If there’s an error, provide the exact error message and describe what the code is </span><em>supposed</em><span> to do. </span><strong>Specificity</strong><span> and </span><strong>context</strong><span> make the difference between vague suggestions and precise, actionable solutions . In practice, this means your prompt might include a brief setup like: “I have a Node.js function using Express and Mongoose that should fetch a user by ID, but it throws a TypeError. Here’s the code and error…”. The more setup you give, the less the AI has to guess.</span></p></li><li><p><strong>Be specific about your goal or question.</strong><span> Vague queries lead to vague answers. Instead of asking something like “Why isn’t my code working?”, pinpoint what insight you need. For example: “This JavaScript function is returning undefined instead of the expected result. Given the code below, can you help identify why and how to fix it?” is far more likely to yield a helpful answer. One prompt formula for debugging is: </span><em>“It’s expected to do [expected behavior] but instead it’s doing [current behavior] when given [example input]. Where is the bug?”</em><span> . Similarly, if you want an optimization, ask for a </span><em>specific kind</em><span> of optimization (e.g. </span><em>“How can I improve the runtime performance of this sorting function for 10k items?”</em><span>). Specificity guides the AI’s focus .</span></p></li><li><p><strong>Break down complex tasks.</strong><span> When implementing a new feature or tackling a multi-step problem, don’t feed the entire problem in one gigantic prompt. It’s often more effective to split the work into smaller chunks and iterate. For instance, </span><em>“First, generate a React component skeleton for a product list page. Next, we’ll add state management. Then, we’ll integrate the API call.”</em><span> Each prompt builds on the previous. It’s often not advised to ask for a whole large feature in one go; instead, start with a high-level goal and then iteratively ask for each piece . This approach not only keeps the AI’s responses focused and manageable, but also mirrors how a human would incrementally build a solution.</span></p></li><li><p><strong>Include examples of inputs/outputs or expected behavior.</strong><span> If you can illustrate what you want with an example, do it. For example, </span><em>“Given the array [3,1,4], this function should return [1,3,4].”</em><span> Providing a concrete example in the prompt helps the AI understand your intent and reduces ambiguity . It’s akin to giving a junior developer a quick test case – it clarifies the requirements. In prompt engineering terms, this is sometimes called “</span><strong>few-shot prompting</strong><span>,” where you show the AI a pattern to follow. Even one example of correct behavior can guide the model’s response significantly.</span></p></li><li><p><strong>Leverage roles or personas.</strong><span> A powerful technique popularized in many viral prompt examples is to ask the AI to “act as” a certain persona or role. This can influence the style and depth of the answer. For instance, </span><em>“Act as a senior React developer and review my code for potential bugs”</em><span> or </span><em>“You are a JavaScript performance expert. Optimize the following function.”</em><span> By setting a role, you prime the assistant to adopt the relevant tone – whether it’s being a strict code reviewer, a helpful teacher for a junior dev, or a security analyst looking for vulnerabilities. Community-shared prompts have shown success with this method, such as </span><em>“Act as a JavaScript error handler and debug this function for me. The data isn’t rendering properly from the API call.”</em><span> . In our own usage, we must still provide the code and problem details, but the </span><strong>role-play</strong><span> prompt can yield more structured and expert-level guidance.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png" width="1456" height="971" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3413086,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd499f2e8-258d-4c0b-abe8-11e35f267832_1536x1024.png 1456w" sizes="100vw"></picture></div></a></figure></div><ul><li><p><strong>Iterate and refine the conversation.</strong><span> Prompt engineering is an </span><em>interactive</em><span> process, not a one-shot deal. Developers often need to review the AI’s first answer and then ask follow-up questions or make corrections. If the solution isn’t quite right, you might say, </span><em>“That solution uses recursion, but I’d prefer an iterative approach – can you try again without recursion?”</em><span> Or, </span><em>“Great, now can you improve the variable names and add comments?”</em><span> The AI remembers the context in a chat session, so you can progressively steer it to the desired outcome. The key is to view the AI as a partner you can coach – </span><strong>progress over perfection</strong><span> on the first try .</span></p></li><li><p><strong>Maintain code clarity and consistency.</strong><span> This last principle is a bit indirect but very important for tools that work on your code context. Write clean, well-structured code and comments, even before the AI comes into play. Meaningful function and variable names, consistent formatting, and docstrings not only make your code easier to understand for humans, but also give the AI stronger clues about what you’re doing. If you show a consistent pattern or style, the AI will continue it . Treat these tools as extremely attentive junior developers – they take every cue from your code and comments.</span></p></li></ul><p><span>With these foundational principles in mind, let’s dive into specific scenarios. We’ll start with </span><strong>debugging</strong><span>, perhaps the most immediate use-case: you have code that’s misbehaving, and you want the AI to help figure out why.</span></p><p>Debugging is a natural fit for an AI assistant. It’s like having a rubber-duck that not only listens, but actually talks back with suggestions. However, success largely depends on how you present the problem to the AI. Here’s how to systematically prompt for help in finding and fixing bugs:</p><p><strong>1. Clearly describe the problem and symptoms.</strong><span> Begin your prompt by describing what is going wrong and what the code is supposed to do. Always include the exact error message or incorrect behavior. For example, instead of just saying “My code doesn’t work,” you might prompt: </span><em>“I have a function in JavaScript that should calculate the sum of an array of numbers, but it’s returning NaN (Not a Number) instead of the actual sum. Here is the code: [include code]. It should output a number (the sum) for an array of numbers like [1,2,3], but I’m getting NaN. What could be the cause of this bug?”</em><span> This prompt specifies the language, the intended behavior, the observed wrong output, and provides the code context – all crucial information. Providing a structured context (code + error + expected outcome + what you’ve tried) gives the AI a solid starting point . By contrast, a generic question like “Why isn’t my function working?” yields meager results – the model can only offer the most general guesses without context.</span></p><p><strong>2. Use a step-by-step or line-by-line approach for tricky bugs.</strong><span> For more complex logic bugs (where no obvious error message is thrown but the output is wrong), you can prompt the AI to walk through the code’s execution. For instance: </span><em>“Walk through this function line by line and track the value of total at each step. It’s not accumulating correctly – where does the logic go wrong?”</em><span> This is an example of a </span><strong>rubber duck debugging prompt</strong><span> – you’re essentially asking the AI to simulate the debugging process a human might do with prints or a debugger. Such prompts often reveal subtle issues like variables not resetting or incorrect conditional logic, because the AI will spell out the state at each step. If you suspect a certain part of the code, you can zoom in: </span><em>“Explain what the filter call is doing here, and if it might be excluding more items than it should.”</em><span> Engaging the AI in an explanatory role can surface the bug in the process of explanation.</span></p><p><strong>3. Provide minimal reproducible examples when possible.</strong><span> Sometimes your actual codebase is large, but the bug can be demonstrated in a small snippet. If you can extract or simplify the code that still reproduces the issue, do so and feed that to the AI. This not only makes it easier for the AI to focus, but also forces you to clarify the problem (often a useful exercise in itself). For example, if you’re getting a TypeError in a deeply nested function call, try to reproduce it with a few lines that you can share. Aim to isolate the bug with the minimum code, make an assumption about what’s wrong, test it, and iterate . You can involve the AI in this by saying: </span><em>“Here’s a pared-down example that still triggers the error [include snippet]. Why does this error occur?”</em><span> By simplifying, you remove noise and help the AI pinpoint the issue. (This technique mirrors the advice of many senior engineers: if you can’t immediately find a bug, simplify the problem space. The AI can assist in that analysis if you present a smaller case to it.)</span></p><p><strong>4. Ask focused questions and follow-ups.</strong><span> After providing context, it’s often effective to directly ask what you need, for example: </span><em>“What might be causing this issue, and how can I fix it?”</em><span> . This invites the AI to both diagnose and propose a solution. If the AI’s first answer is unclear or partially helpful, don’t hesitate to ask a follow-up. You could say, </span><em>“That explanation makes sense. Can you show me how to fix the code? Please provide the corrected code.”</em><span> In a chat setting, the AI has the conversation history, so it can directly output the modified code. If you’re using an inline tool like Copilot in VS Code or Cursor without a chat, you might instead write a comment above the code like // BUG: returns NaN, fix this function and see how it autocompletes – but in general, the interactive chat yields more thorough explanations. Another follow-up pattern: if the AI gives a fix but you don’t understand why, ask </span><em>“Can you explain why that change solves the problem?”</em><span> This way you learn for next time, and you double-check that the AI’s reasoning is sound.</span></p><p><span>Now, let’s illustrate these debugging prompt principles with a concrete example, showing a </span><strong>poor prompt vs. improved prompt</strong><span> and the difference in AI responses:</span></p><p>Imagine we have a simple Node.js function meant to convert a list of user objects to a lookup map by user ID. However, it’s throwing an error. Here’s the buggy code:</p><pre><code><code>// Buggy function: converts array of users to a map by ID
function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    // Note: using &lt;= will go out of bounds on the last iteration
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}

// Example usage:
const result = mapUsersById([{ id: 1, name: "Alice" }]);</code></code></pre><p>This code is supposed to iterate through the users array and build an object where keys are user IDs. However, there is a bug: the loop uses i &lt;= users.length instead of i &lt; users.length. This will cause users[i] to be undefined on the last iteration, likely leading to a runtime error (e.g., attempting to access property id of undefined). Let’s see how an AI assistant might respond to two different prompts about this issue.</p><p><strong>❌ Poor Prompt:</strong><span> “Why isn’t my mapUsersById function working?”</span></p><p><em>AI’s response to the poor prompt (simulated):</em></p><blockquote><p><em>It’s hard to say without more information. Are you sure the function is being called with a proper array of users? Perhaps the issue is that users is empty or not an array, causing the loop not to execute properly. It could also be that there’s an error when setting the property on userMap. Make sure that each user has an id property. If users is undefined or null, that would also cause it to not work.</em></p></blockquote><p><strong>Analysis:</strong><span> This answer is quite vague. The prompt gave no details except the function name, so the AI is guessing at generic causes (“maybe the array is empty or not an array”, “make sure each user has an id”). It did not identify the actual bug. This is the kind of response you get when the question is underspecified – the model can only regurgitate common debugging tips. As a developer, this isn’t very helpful because it doesn’t address the real mistake. The </span><strong>lack of context</strong><span> and specificity in the prompt yields a generic answer (essentially the AI equivalent of “have you tried turning it off and on again?”). This aligns with the community advice that vague prompts lead to irrelevant or generic answers .</span></p><p><strong>✅ Improved Prompt:</strong><span> “I have a JavaScript function mapUsersById that should convert an array of user objects into a map (object) keyed by user ID. However, it throws an error when I run it. For example, when I pass [ {id: 1, name: "Alice"} ], I get TypeError: Cannot read property 'id' of undefined. Here is the function code:</span></p><pre><code><code>function mapUsersById(users) {
  const userMap = {};
  for (let i = 0; i &lt;= users.length; i++) {  
    const user = users[i];
    userMap[user.id] = user;
  }
  return userMap;
}</code></code></pre><p>It should return { "1": {id: 1, name: "Alice"} }. What is the bug and how can I fix it?”</p><p><em>AI’s response to the improved prompt (simulated):</em></p><blockquote><p>*The error Cannot read property 'id' of undefined suggests that at some point, user is undefined, and then you try to access user.id. Looking at your loop, the issue is with the loop bounds. You are using i &lt;= users.length. If users.length is 1 (for one user in the array), the loop runs with i = 0 and i = 1. When i is 1, users[1] is undefined (since the only index in the array is 0). Thus user becomes undefined on that iteration, and user.id throws a TypeError. The fix is to change the loop condition to i &lt; users.length so that i goes from 0 to users.length-1. For example:</p></blockquote><pre><code><code>for (let i = 0; i &lt; users.length; i++) {
  const user = users[i];
  userMap[user.id] = user;
}</code></code></pre><blockquote><p>This way, i will be 0 for the first (and only) element, and then the loop stops. With that change, your function will no longer hit an undefined user and should work as expected, returning {"1": {id: 1, name: "Alice"}} for the given input.*</p></blockquote><p><strong>Analysis:</strong><span> This response directly pinpoints the bug (using &lt;= in the loop) and explains why it causes user to be undefined. It then provides the correct code solution (i &lt; users.length) and confirms the expected outcome. The improved prompt succeeded because it followed our principles: it specified the language (JavaScript), described the function’s purpose, included the exact error message and a sample input, and even provided the code snippet in question. Notice how the AI used the error message as a clue to focus on the loop bounds – a targeted prompt enabled the AI to engage in true problem-solving, effectively simulating how a human debugger would think: “where could undefined come from? likely from the loop indexing”. This is a concrete demonstration of the benefit of detailed prompts. </span></p><p><strong>Additional Debugging Tactics:</strong><span> Beyond identifying obvious bugs, you can use prompt engineering for deeper debugging assistance:</span></p><ul><li><p><em>Ask for potential causes.</em><span> If you’re truly stumped, you can broaden the question slightly: </span><em>“What are some possible reasons for a TypeError: cannot read property 'foo' of undefined in this code?”</em><span> along with the code. The model might list a few scenarios (e.g. the object wasn’t initialized, a race condition, wrong variable scoping, etc.). This can give you angles to investigate that you hadn’t considered. It’s like brainstorming with a colleague.</span></p></li><li><p><em>“Ask the Rubber Duck”</em><span> – i.e., explain your code to the AI. This may sound counterintuitive (why explain to the assistant?), but the act of writing an explanation can clarify your own understanding, and you can then have the AI verify or critique it. For example: </span><em>“I will explain what this function is doing: [your explanation]. Given that, is my reasoning correct and does it reveal where the bug is?”</em><span> The AI might catch a flaw in your explanation that points to the actual bug. This technique leverages the AI as an active rubber duck that not only listens but responds.</span></p></li><li><p><em>Have the AI create test cases.</em><span> You can ask: </span><em>“Can you provide a couple of test cases (inputs) that might break this function?”</em><span> The assistant might come up with edge cases you didn’t think of (empty array, extremely large numbers, null values, etc.). This is useful both for debugging and for generating tests for future robustness.</span></p></li><li><p><em>Role-play a code reviewer.</em><span> As an alternative to a direct “debug this” prompt, you can say: </span><em>“Act as a code reviewer. Here’s a snippet that isn’t working as expected. Review it and point out any mistakes or bad practices that could be causing issues: [code]”.</em><span> This sets the AI into a critical mode. Many developers find that phrasing the request as a code review yields a very thorough analysis, because the model will comment on each part of the code (and often, in doing so, it spots the bug). In fact, one prompt engineering tip is to explicitly request the AI to behave like a meticulous reviewer . This can surface not only the bug at hand but also other issues (e.g. potential null checks missing) which might be useful.</span></p></li></ul><p><span>In summary, when debugging with an AI assistant, </span><strong>detail and direction are your friends</strong><span>. Provide the scenario, the symptoms, and then ask pointed questions. The difference between a flailing “it doesn’t work, help!” prompt and a surgical debugging prompt is night and day, as we saw above. Next, we’ll move on to another major use case: refactoring and improving existing code.</span></p><p><span>Refactoring code – making it cleaner, faster, or more idiomatic without changing its functionality – is an area where AI assistants can shine. They’ve been trained on vast amounts of code, which includes many examples of well-structured, optimized solutions. However, to tap into that knowledge effectively, </span><strong>your prompt must clarify what “better” means for your situation</strong><span>. Here’s how to prompt for refactoring tasks:</span></p><p><strong>1. State your refactoring goals explicitly.</strong><span> “Refactor this code” on its own is too open-ended. Do you want to improve readability? Reduce complexity? Optimize performance? Use a different paradigm or library? The AI needs a target. A good prompt frames the task, for example: </span><em>“Refactor the following function to improve its readability and maintainability (reduce repetition, use clearer variable names).”</em><span> Or </span><em>“Optimize this algorithm for speed – it’s too slow on large inputs.”</em><span> By stating </span><strong>specific goals</strong><span>, you help the model decide which transformations to apply . For instance, telling it you care about performance might lead it to use a more efficient sorting algorithm or caching, whereas focusing on readability might lead it to break a function into smaller ones or add comments. If you have multiple goals, list them out. A prompt template from the Strapi guide suggests even enumerating issues: </span><em>“Issues I’d like to address: 1) [performance issue], 2) [code duplication], 3) [outdated API usage].”</em><span> . This way, the AI knows exactly what to fix. Remember, it will not inherently know </span><em>what you consider a problem</em><span> in the code – you must tell it.</span></p><p><strong>2. Provide the necessary code context.</strong><span> When refactoring, you’ll typically include the code snippet that needs improvement in the prompt. It’s important to include the full function or section that you want to be refactored, and sometimes a bit of surrounding context if relevant (like the function’s usage or related code, which could affect how you refactor). Also mention the language and framework, because “idiomatic” code varies between, say, idiomatic Node.js vs. idiomatic Deno, or React class components vs. functional components. For example: </span><em>“I have a React component written as a class. Please refactor it to a functional component using Hooks.”</em><span> The AI will then apply the typical steps (using useState, useEffect, etc.). If you just said “refactor this React component” without clarifying the style, the AI might not know you specifically wanted Hooks.</span></p><ul><li><p><strong>Include version or environment details if relevant.</strong><span> For instance, </span><em>“This is a Node.js v14 codebase”</em><span> or </span><em>“We’re using ES6 modules”</em><span>. This can influence whether the AI uses certain syntax (like import/export vs. require), which is part of a correct refactoring. If you want to ensure it doesn’t introduce something incompatible, mention your constraints.</span></p></li></ul><p><strong>3. Encourage explanations along with the code.</strong><span> A great way to learn from an AI-led refactor (and to verify its correctness) is to ask for an explanation of the changes. For example: </span><em>“Please suggest a refactored version of the code, and explain the improvements you made.”</em><span> This was even built into the prompt template we referenced: </span><em>“…suggest refactored code with explanations for your changes.”</em><span> . When the AI provides an explanation, you can assess if it understood the code and met your objectives. The explanation might say: “I combined two similar loops into one to reduce duplication, and I used a dictionary for faster lookups,” etc. If something sounds off in the explanation, that’s a red flag to examine the code carefully. In short, </span><em>use the AI’s ability to explain as a safeguard</em><span> – it’s like having the AI perform a code review on its own refactor.</span></p><p><strong>4. Use role-play to set a high standard.</strong><span> As mentioned earlier, asking the AI to act as a code reviewer or senior engineer can be very effective. For refactoring, you might say: </span><em>“Act as a seasoned TypeScript expert and refactor this code to align with best practices and modern standards.”</em><span> This often yields not just superficial changes, but more insightful improvements because the AI tries to live up to the “expert” persona. A popular example from a prompt guide is having the AI role-play a mentor: </span><em>“Act like an experienced Python developer mentoring a junior. Provide explanations and write docstrings. Rewrite the code to optimize it.”</em><span> . The result in that case was that the AI used a more efficient data structure (set to remove duplicates) and provided a one-line solution for a function that originally used a loop . The role-play helped it not only refactor but also explain </span><em>why</em><span> the new approach is better (in that case, using a set is a well-known optimization for uniqueness).</span></p><p>Now, let’s walk through an example of refactoring to see how a prompt can influence the outcome. We will use a scenario in JavaScript (Node.js) where we have some less-than-ideal code and we want it improved.</p><p>Suppose we have a function that makes two database calls and does some processing. It works, but it’s not pretty – there’s duplicated code and it’s hard to read. We’d like to refactor it for clarity and efficiency. Here’s the original code:</p><pre><code><code>// Original function: Fetches two lists and processes them (needs refactoring)
async function getCombinedData(apiClient) {
  // Fetch list of users
  const usersResponse = await apiClient.fetch('/users');
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  const users = await usersResponse.json();
  // Fetch list of orders
  const ordersResponse = await apiClient.fetch('/orders');
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }
  const orders = await ordersResponse.json();
  // Combine data (match user to orders)
  const result = [];
  for (let user of users) {
    const userOrders = orders.filter(o =&gt; o.userId === user.id);
    result.push({ user, orders: userOrders });
  }
  return result;
}</code></code></pre><p>Issues in this code: We see duplicated fetch logic for users and orders (could be refactored into a helper or loop), lack of error detail (just a generic error message), and it could perhaps run the fetches in parallel to improve speed. Let’s see two prompting approaches:</p><p><strong>❌ Prompt 1 (Underspecified):</strong><span> “Refactor the above getCombinedData function.”</span></p><p><em>AI’s response to Prompt 1:</em></p><blockquote><p>*Sure. Here’s a refactored version of getCombinedData:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // Fetch users and orders in parallel
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);
  if (!usersResponse.ok || !ordersResponse.ok) {
    throw new Error('Failed to fetch data');
  }
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);
  // Map orders by userId for efficient lookup
  const ordersByUser = {};
  for (const order of orders) {
    if (!ordersByUser[order.userId]) {
      ordersByUser[order.userId] = [];
    }
    ordersByUser[order.userId].push(order);
  }
  // Combine data
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []
  }));
}</code></code></pre><blockquote><p>I fetched users and orders in parallel to improve performance. I also consolidated error handling and used a map (ordersByUser) to group orders by user, which makes the combination step cleaner and potentially faster.*</p></blockquote><p><strong>Analysis:</strong><span> This is actually a pretty good refactor, hitting several improvements: parallel fetching, grouping orders by user for efficiency, cleaner combination using .map and defaulting to an empty array. The AI also explained what it did. So why do we label this prompt as underspecified or “poor”? In this case, the AI happened to make good choices, but it </span><em>assumed</em><span> a lot because the prompt was open-ended. It assumed that performance should be improved (parallel requests, which is a smart change but what if parallel wasn’t safe here?), and it changed error handling to a generic message “Failed to fetch data,” which in combining the errors lost the specificity of which call failed. </span></p><p>The risk with a generic “Refactor this” prompt is that the AI might make changes that alter behavior in unintended ways. Here, combining error handling might be acceptable or not, depending on requirements. The assistant also made a structural change (using a dictionary to group orders) – that’s fine and probably for the better, but if we had specific constraints (like wanting to stream results or keep memory usage low), we didn’t inform it. So while the result looks good, it was somewhat lucky that the AI’s own interpretation of “refactor” aligned with what we wanted. This illustrates that an underspecified prompt can sometimes get good results on simple cases, but for complex code, it might go off in an undesired direction or miss certain improvements you care about.</p><p><strong>✅ Prompt 2 (Goal-Oriented):</strong><span> “Refactor the above getCombinedData function to eliminate duplicate code and improve performance. Specifically: (1) Avoid repeating the fetch logic for users and orders – maybe use a helper or fetch them together. (2) Fetch both lists in parallel if possible. (3) Keep the error handling for each fetch (we want to know which call failed). (4) Improve the combination of data, possibly by using a more efficient structure for lookup instead of a nested loop. Provide the refactored code with comments explaining the changes.”</span></p><p><em>AI’s response to Prompt 2:</em></p><blockquote><p>*Sure! Here’s an improved version of getCombinedData with the requested refactoring:</p></blockquote><pre><code><code>async function getCombinedData(apiClient) {
  // 1. Fetch users and orders in parallel to improve performance
  const [usersResponse, ordersResponse] = await Promise.all([
    apiClient.fetch('/users'),
    apiClient.fetch('/orders')
  ]);

  // 2. Handle errors separately to know which call failed
  if (!usersResponse.ok) {
    throw new Error('Failed to fetch users');
  }
  if (!ordersResponse.ok) {
    throw new Error('Failed to fetch orders');
  }

  // Parse JSON responses in parallel
  const [users, orders] = await Promise.all([
    usersResponse.json(),
    ordersResponse.json()
  ]);

  // 3. Use a lookup map for orders by userId for efficient matching
  const ordersByUser = orders.reduce((map, order) =&gt; {
    if (!map[order.userId]) {
      map[order.userId] = [];
    }
    map[order.userId].push(order);
    return map;
  }, {});

  // 4. Combine data using the lookup map
  return users.map(user =&gt; ({
    user,
    orders: ordersByUser[user.id] || []  // empty array if no orders for user
  }));
}</code></code></pre><blockquote><p><strong>Explanation of changes:</strong></p></blockquote><ul><li><p><em>Parallel fetching:</em><span> I used Promise.all to fetch users and orders at the same time (point 1).</span></p></li><li><p><em>Preserved individual error handling:</em><span> I kept separate checks for usersResponse.ok and ordersResponse.ok so that any error clearly indicates which fetch failed (point 2).</span></p></li><li><p><em>Parallel JSON parsing:</em><span> Also used Promise.all for response.json() calls, which is a minor improvement.</span></p></li><li><p><em>Efficient combination:</em><span> Instead of a nested loop, I created an ordersByUser map (point 3). This groups orders by userId in a single pass. Then I map over users to build the result, pulling each user’s orders from the map in O(1) time (point 4). This is more efficient and concise.*</span></p></li></ul><p><strong>Analysis:</strong><span> This response directly addressed all the specified goals. The code is refactored to be cleaner and faster, and it maintained separate error messages as requested. The AI’s explanation confirms each point we listed, which shows it carefully followed the prompt instructions. This is a great outcome because we, as the prompter, defined what “refactor” meant in this context. By doing so, we guided the AI to produce a solution that matches our needs with minimal back-and-forth. If the AI had overlooked one of the points (say it still merged the error handling), we could easily prompt again: </span><em>“Looks good, but please ensure the error messages remain distinct for users vs orders.”</em><span> – however, in this case it wasn’t needed because our prompt was thorough.</span></p><p><span>This example demonstrates a key lesson: </span><strong>when you know what you want improved, spell it out.</strong><span> AI is good at following instructions, but it won’t read your mind. A broad “make this better” might work for simple things, but for non-trivial code, you’ll get the best results by enumerating what “better” means to you. This aligns with community insights that clear, structured prompts yield significantly improved results .</span></p><p><strong>Additional Refactoring Tips:</strong></p><ul><li><p><em>Refactor in steps:</em><span> If the code is very large or you have a long list of changes, you can tackle them one at a time. For example, first ask the AI to “refactor for readability” (focus on renaming, splitting functions), then later “optimize the algorithm in this function.” This prevents overwhelming the model with too many instructions at once and lets you verify each change stepwise.</span></p></li><li><p><em>Ask for alternative approaches:</em><span> Maybe the AI’s first refactor works but you’re curious about a different angle. You can ask, </span><em>“Can you refactor it in another way, perhaps using functional programming style (e.g. array methods instead of loops)?”</em><span> or </span><em>“How about using recursion here instead of iterative approach, just to compare?”</em><span> This way, you can evaluate different solutions. It’s like brainstorming multiple refactoring options with a colleague.</span></p></li><li><p><em>Combine refactoring with explanation to learn patterns:</em><span> We touched on this, but it’s worth emphasizing – use the AI as a learning tool. If it refactors code in a clever way, study the output and explanation. You might discover a new API or technique (like using reduce to build a map) that you hadn’t used before. This is one reason to ask for explanations: it turns an answer into a mini-tutorial, reinforcing your understanding of best practices.</span></p></li><li><p><em>Validation and testing:</em><span> After any AI-generated refactor, always run your tests or try the code with sample inputs. AI might inadvertently introduce subtle bugs, especially if the prompt didn’t specify an important constraint. For example, in our refactor, if the original code intentionally separated fetch errors for logging but we didn’t mention logging, the combined error might be less useful. It’s our job to catch that in review. The AI can help by writing tests too – you could ask </span><em>“Generate a few unit tests for the refactored function”</em><span> to ensure it behaves the same as before on expected inputs.</span></p></li></ul><p><span>At this point, we’ve covered debugging and refactoring – improving existing code. The next logical step is to use AI assistance for </span><strong>implementing new features</strong><span> or generating new code. We’ll explore how to prompt for that scenario effectively.</span></p><p><strong>❌ Poor Prompt:</strong><span> "My useEffect isn't working right"</span></p><p><strong>✅ Enhanced Prompt:</strong></p><pre><code><code>I have a React component that fetches user data, but it's causing infinite re-renders. Here's my code:


const UserProfile = ({ userId }) =&gt; {
  const [user, setUser] = useState(null);
  const [loading, setLoading] = useState(true);
  
  useEffect(() =&gt; {
    fetchUser(userId).then(setUser).finally(() =&gt; setLoading(false));
  }, [userId, setUser, setLoading]); // Problem is here
  
  return loading ? &lt;div&gt;Loading...&lt;/div&gt; : &lt;div&gt;{user?.name}&lt;/div&gt;;
};</code></code></pre><p><strong>Expected behavior:</strong><span> Should fetch user data once when userId changes Actual behavior: Component re-renders infinitely Error in console: "Warning: Maximum update depth exceeded"</span></p><p>What's causing this infinite loop and how do I fix the dependency array?</p><p><strong>Why this works:</strong><span> Provides exact code, error message, expected vs actual behavior, and focuses on a specific React pattern that's commonly misunderstood.</span></p><p><strong>❌ Poor Prompt:</strong><span> "Build the state management for my Next.js ecommerce app”</span></p><p>✅ Enhanced Prompt:</p><p>I'm building a Next.js 14 e-commerce app and need to design the state management architecture. Here are my requirements:</p><p>Components:</p><ul><li><p>Product listing page (needs: products[], filters, pagination)</p></li><li><p>Shopping cart (needs: cart items, totals, shipping info)</p></li><li><p>User auth (needs: user profile, auth status, preferences)</p></li><li><p>Real-time notifications (needs: toast messages, error states)</p></li></ul><p>Technical constraints:</p><ul><li><p>Next.js 14 with App Router and Server Components</p></li><li><p>TypeScript strict mode</p></li><li><p>Server-side data fetching for SEO</p></li><li><p>Client-side interactivity for cart/user actions</p></li><li><p>State should persist across navigation</p></li></ul><p>Should I use:</p><ol><li><p>Zustand stores for each domain (cart, auth, notifications)</p></li><li><p>React Query/TanStack Query for server state + Zustand for client state</p></li><li><p>A single Zustand store with slices</p></li></ol><p>Please provide a recommended architecture with code examples showing how to structure stores and integrate with Next.js App Router patterns.</p><p><strong>Why this works: </strong><span>Real-world scenario with specific tech stack, clear requirements, and asks for architectural guidance with implementation details.</span></p><p>One of the most exciting uses of AI code assistants is to help you write new code from scratch or integrate a new feature into an existing codebase. This could range from generating a boilerplate for a React component to writing a new API endpoint in an Express app. The challenge here is often that these tasks are open-ended – there are many ways to implement a feature. Prompt engineering for code generation is about guiding the AI to produce code that fits your needs and style. Here are strategies to do that:</p><p><strong>1. Start with high-level instructions, then drill down.</strong><span> Begin by outlining what you want to build in plain language, possibly breaking it into smaller tasks (similar to our advice on breaking down complex tasks earlier). For example, say you want to add a </span><strong>search bar feature</strong><span> to an existing web app. You might first prompt: </span><em>“Outline a plan to add a search feature that filters a list of products by name in my React app. The products are fetched from an API.”</em><span> </span></p><p>The AI might give you a step-by-step plan: “1. Add an input field for the search query. 2. Add state to hold the query. 3. Filter the products list based on the query. 4. Ensure it’s case-insensitive, etc.” Once you have this plan (which you can refine with the AI’s help), you can tackle each bullet with focused prompts. </p><p><span>For instance: </span><em>“Okay, implement step 1: create a SearchBar component with an input that updates a searchQuery state.”</em><span> After that, </span><em>“Implement step 3: given the searchQuery and an array of products, filter the products (case-insensitive match on name).”</em><span> By dividing the feature, you ensure each prompt is specific and the responses are manageable. This also mirrors iterative development – you can test each piece as it’s built.</span></p><p><strong>2. Provide relevant context or reference code.</strong><span> If you’re adding a feature to an existing project, it helps tremendously to show the AI how similar things are done in that project. For example, if you already have a component that is similar to what you want, you can say: </span><em>“Here is an existing UserList component (code…). Now create a ProductList component that is similar but includes a search bar.”</em><span> </span></p><p><span>The AI will see the patterns (maybe you use certain libraries or style conventions) and apply them. Having relevant files open or referencing them in your prompt provides context that leads to more project-specific and consistent code suggestions . Another trick: if your project uses a particular coding style or architecture (say Redux for state or a certain CSS framework), mention that. </span><em>“We use Redux for state management – integrate the search state into Redux store.”</em><span> </span></p><p><span>A well-trained model will then generate code consistent with Redux patterns, etc. Essentially, you are </span><strong>teaching the AI about your project’s environment</strong><span> so it can tailor the output. Some assistants can even use your entire repository as context to draw from; if using those, ensure you point it to similar modules or documentation in your repo.</span></p><ul><li><p><span>If starting something new but you have a preferred approach, you can also mention that: </span><em>“I’d like to implement this using functional programming style (no external state, using array methods).”</em><span> Or, </span><em>“Ensure to follow the MVC pattern and put logic in the controller, not the view.”</em><span> These are the kind of details a senior engineer might remind a junior about, and here </span><strong>you are the senior telling the AI</strong><span>.</span></p></li></ul><p><strong>3. Use comments and TODOs as inline prompts.</strong><span> When working directly in an IDE with Copilot, one effective workflow is writing a comment that describes the next chunk of code you need, then letting the AI autocomplete it. For example, in a Node.js backend, you might write: // TODO: Validate the request payload (ensure name and email are provided) and then start the next line. Copilot often picks up on the intent and generates a block of code performing that validation. This works because your comment is effectively a natural language prompt. However, be prepared to edit the generated code if the AI misinterprets – as always, verify its correctness.</span></p><p><strong>4. Provide examples of expected input/output or usage.</strong><span> Similar to what we discussed before, if you’re asking the AI to implement a new function, include a quick example of how it will be used or a simple test case. For instance: </span><em>“Implement a function formatPrice(amount) in JavaScript that takes a number (like 2.5) and returns a string formatted in USD (like $2.50). For example, formatPrice(2.5) should return '$2.50'.”</em><span> </span></p><p><span>By giving that example, you constrain the AI to produce a function consistent with it. Without the example, the AI might assume some other formatting or currency. The difference could be subtle but important. Another example in a web context: </span><em>“Implement an Express middleware that logs requests. For instance, a GET request to /users should log ‘GET /users’ to the console.”</em><span> This makes it clear what the output should look like. Including expected behavior in the prompt acts as a test the AI will try to satisfy.</span></p><p><strong>5. When the result isn’t what you want, rewrite the prompt with more detail or constraints.</strong><span> It’s common that the first attempt at generating a new feature doesn’t nail it. Maybe the code runs but is not idiomatic, or it missed a requirement. Instead of getting frustrated, treat the AI like a junior dev who gave a first draft – now you need to give feedback. For example, </span><em>“The solution works but I’d prefer if you used the built-in array filter method instead of a for loop.”</em><span> Or, </span><em>“Can you refactor the generated component to use React Hooks for state instead of a class component? Our codebase is all functional components.”</em><span> You can also add new constraints: </span><em>“Also, ensure the function runs in O(n) time or better, because n could be large.”</em><span> This iterative prompting is powerful. A real-world scenario: one developer asked an LLM to generate code to draw an ice cream cone using a JS canvas library, but it kept giving irrelevant output until they refined the prompt with more specifics and context . The lesson is, </span><em>don’t give up after one try</em><span>. Figure out what was lacking or misunderstood in the prompt and clarify it. This is the essence of prompt engineering – each tweak can guide the model closer to what you envision.</span></p><p>Let’s illustrate feature implementation prompting with an example scenario:</p><p>Suppose we have a React application and we want to add a new component: a product list with a search filter and a sort dropdown. This is a fairly typical feature in a web app. We’ll focus on prompting for the search functionality within this component for brevity.</p><p><strong>Initial Prompt (Feature request):</strong><span> “Create a React functional component called ProductList that displays a list of products and includes a text input to filter the products by name. The component should: fetch an array of products from /api/products (assume it returns JSON array of {id, name, ...}), store them in state, and allow the user to type in a search box to filter the displayed products by name (case-insensitive match). Display the filtered list in a simple &lt;ul&gt; with product names. Include basic error handling (e.g. show an error message if the API call fails) and a loading state while fetching.”</span></p><p>That’s a long prompt but it’s specific. Let’s assume we give this to an LLM in a chat.</p><p><em>AI’s response (summary of what it might produce):</em><span> The AI would likely output a ProductList component something like:</span></p><pre><code><code>import React, { useState, useEffect } from 'react';

function ProductList() {
  const [products, setProducts] = useState([]);
  const [search, setSearch] = useState('');
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);

  useEffect(() =&gt; {
    async function fetchProducts() {
      try {
        const res = await fetch('/api/products');
        if (!res.ok) throw new Error('Network response was not ok');
        const data = await res.json();
        setProducts(data);
      } catch (err) {
        console.error('Failed to fetch products:', err);
        setError('Failed to load products.');
      } finally {
        setLoading(false);
      }
    }
    fetchProducts();
  }, []);

  const handleSearchChange = (e) =&gt; {
    setSearch(e.target.value);
  };

  // Filter products by search query (case-insensitive)
  const filteredProducts = products.filter(product =&gt;
    product.name.toLowerCase().includes(search.toLowerCase())
  );

  if (loading) return &lt;div&gt;Loading products...&lt;/div&gt;;
  if (error) return &lt;div&gt;{error}&lt;/div&gt;;

  return (
    &lt;div className="product-list"&gt;
      &lt;input 
        type="text" 
        placeholder="Search products" 
        value={search} 
        onChange={handleSearchChange} 
      /&gt;
      &lt;ul&gt;
        {filteredProducts.map(prod =&gt; (
          &lt;li key={prod.id}&gt;{prod.name}&lt;/li&gt;
        ))}
      &lt;/ul&gt;
    &lt;/div&gt;
  );
}

export default ProductList;</code></code></pre><p><em>(Plus possibly some comments in the code explaining each part, if we asked for them.)</em></p><p>Now, let’s analyze this result and consider prompt refinements:</p><ul><li><p><span>Suppose the above code came back, but our project uses a context or a global store for data fetching instead of fetching inside the component. This is something we didn’t specify. The AI made a choice to fetch in useEffect. If we prefer using a custom hook or context, we should have hinted at that. So we could refine our prompt: </span><em>“Actually, in our app we use a custom hook useProducts() that already handles fetching. Please refactor the component to use useProducts hook instead of directly calling fetch.”</em><span> The AI would then adjust, perhaps assuming useProducts returns { products, loading, error } and simplify the component accordingly.</span></p></li><li><p><span>Another refinement: maybe we realize we also want a sort dropdown (which we didn’t mention initially). We can now extend the conversation: </span><em>“Great, now add a dropdown to sort the products by name (A-Z or Z-A). The dropdown should let the user choose ascending or descending, and the list should sort accordingly in addition to the filtering.”</em><span> Because the AI has the context of the existing code, it can insert a sort state and adjust the rendering. We provided a clear new requirement, and it will attempt to fulfill it, likely by adding something like:</span></p></li></ul><pre><code><code>const [sortOrder, setSortOrder] = useState('asc');
// ... a select input for sortOrder ...
// and sort the filteredProducts before rendering:
const sortedProducts = [...filteredProducts].sort((a, b) =&gt; {
  if (sortOrder === 'asc') return a.name.localeCompare(b.name);
  else return b.name.localeCompare(a.name);
});</code></code></pre><ul><li><p>(plus the dropdown UI).</p><p>By iterating like this, feature by feature, we simulate a development cycle with the AI. This is far more effective than trying to prompt for the entire, complex component with all features in one go initially. It reduces mistakes and allows mid-course corrections as requirements become clearer.</p></li><li><p><span>If the AI makes a subtle mistake (say it forgot to make the search filter case-insensitive), we just point that out: </span><em>“Make the search case-insensitive.”</em><span> It will adjust the filter to use lowercase comparison (which in our pseudo-output it already did, but if not it would fix it).</span></p></li></ul><p><span>This example shows that implementing features with AI is all about </span><strong>incremental development and prompt refinement</strong><span>. A Twitter thread might exclaim how someone built a small app by continually prompting an LLM for each part – that’s essentially the approach: build, review, refine, extend. Each prompt is like a commit in your development process.</span></p><p><strong>Additional tips for feature implementation:</strong></p><ul><li><p><em>Let the AI scaffold, then you fill in specifics:</em><span> Sometimes it’s useful to have the AI generate a rough structure, then you tweak it. For example, </span><em>“Generate the skeleton of a Node.js Express route for user registration with validation and error handling.”</em><span> It might produce a generic route with placeholders. You can then fill in the actual validation rules or database calls which are specific to your app. The AI saves you from writing boilerplate, and you handle the custom logic if it’s sensitive.</span></p></li><li><p><em>Ask for edge case handling:</em><span> When generating a feature, you might prompt the AI to think of edge cases: </span><em>“What edge cases should we consider for this feature (and can you handle them in the code)?”</em><span> For instance, in the search example, an edge case might be “what if the products haven’t loaded yet when the user types?” (though our code handles that via loading state) or “what if two products have the same name” (not a big issue but maybe mention it). The AI could mention things like empty result handling, very large lists (maybe needing debounce for search input), etc. This is a way to leverage the AI’s training on common pitfalls.</span></p></li><li><p><em>Documentation-driven development:</em><span> A nifty approach some have taken is writing a docstring or usage example first and having the AI implement the function to match. For example:</span></p></li></ul><pre><code><code>/**
 * Returns the nth Fibonacci number.
 * @param {number} n - The position in Fibonacci sequence (0-indexed).
 * @returns {number} The nth Fibonacci number.
 * 
 * Example: fibonacci(5) -&gt; 5  (sequence: 0,1,1,2,3,5,…)
 */
function fibonacci(n) {
  // ... implementation
}</code></code></pre><ul><li><p>If you write the above comment and function signature, an LLM might fill in the implementation correctly because the comment describes exactly what to do and even gives an example. This technique ensures you clarify the feature in words first (which is a good practice generally), and then the AI uses that as the spec to write the code.</p></li></ul><p><span>Having covered prompting strategies for debugging, refactoring, and new code generation, let’s turn our attention to some </span><strong>common pitfalls and anti-patterns</strong><span> in prompt engineering for coding. Understanding these will help you avoid wasting time on unproductive interactions and quickly adjust when the AI isn’t giving you what you need.</span></p><p><span>Not all prompts are created equal. By now, we’ve seen numerous examples of effective prompts, but it’s equally instructive to recognize </span><strong>anti-patterns</strong><span> – common mistakes that lead to poor AI responses. </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png" width="1024" height="1077" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1077,&quot;width&quot;:1024,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2173076,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0116373b-99df-416b-9bd0-6840d33fcdd2_1024x1077.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Here are some frequent prompt failures and how to fix them:</p><ul><li><p><strong>Anti-Pattern: The Vague Prompt.</strong><span> This is the classic </span><em>“It doesn’t work, please fix it”</em><span> or </span><em>“Write something that does X”</em><span> without enough detail. We saw an example of this when the question “Why isn’t my function working?” got a useless answer . Vague prompts force the AI to guess the context and often result in generic advice or irrelevant code. The fix is straightforward: </span><strong>add context and specifics</strong><span>. If you find yourself asking a question and the answer feels like a Magic 8-ball response (“Have you tried checking X?”), stop and reframe your query with more details (error messages, code excerpt, expected vs actual outcome, etc.). A good practice is to read your prompt and ask, </span><em>“Could this question apply to dozens of different scenarios?”</em><span> If yes, it’s too vague. Make it so specific that it could </span><em>only</em><span> apply to your scenario.</span></p></li><li><p><strong>Anti-Pattern: The Overloaded Prompt.</strong><span> This is the opposite issue: asking the AI to do too many things at once. For instance, </span><em>“Generate a complete Node.js app with authentication, a front-end in React, and deployment scripts.”</em><span> Or even on a smaller scale, </span><em>“Fix these 5 bugs and also add these 3 features in one go.”</em><span> The AI might attempt it, but you’ll likely get a jumbled or incomplete result, or it might ignore some parts of the request. Even if it addresses everything, the response will be long and harder to verify. The remedy is to </span><strong>split the tasks</strong><span>. Prioritize: do one thing at a time, as we emphasized earlier. This makes it easier to catch mistakes and ensures the model stays focused. If you catch yourself writing a paragraph with multiple “and” in the instructions, consider breaking it into separate prompts or sequential steps.</span></p></li><li><p><strong>Anti-Pattern: Missing the Question.</strong><span> Sometimes users will present a lot of information but never clearly ask a question or specify what they need. For example, dumping a large code snippet and just saying “Here’s my code.” This can confuse the AI – it doesn’t know what you want. Always include a clear ask, such as </span><em>“Identify any bugs in the above code”</em><span>, </span><em>“Explain what this code does”</em><span>, or </span><em>“Complete the TODOs in the code”</em><span>. A prompt should have a </span><em>purpose</em><span>. If you just provide text without a question or instruction, the AI might make incorrect assumptions (like summarizing the code instead of fixing it, etc.). Make sure the AI knows </span><em>why</em><span> you showed it some code. Even a simple addition like, </span><em>“What’s wrong with this code?”</em><span> or </span><em>“Please continue implementing this function.”</em><span> gives it direction.</span></p></li><li><p><strong>Anti-Pattern: Vague Success Criteria.</strong><span> This is a subtle one – sometimes you might ask for an optimization or improvement, but you don’t define what success looks like. For example, </span><em>“Make this function faster.”</em><span> Faster by what metric? If the AI doesn’t know your performance constraints, it might micro-optimize something that doesn’t matter or use an approach that’s theoretically faster but practically negligible. Or </span><em>“make this code cleaner”</em><span> – “cleaner” is subjective. We dealt with this by explicitly stating goals like “reduce duplication” or “improve variable names” etc. The fix: </span><strong>quantify or qualify the improvement</strong><span>. E.g., “optimize this function to run in linear time (current version is quadratic)” or “refactor this to remove global variables and use a class instead.” Basically, </span><em>be explicit about what problem you’re solving with the refactor or feature</em><span>. If you leave it too open, the AI might solve a different problem than the one you care about.</span></p></li><li><p><strong>Anti-Pattern: Ignoring AI’s Clarification or Output.</strong><span> Sometimes the AI might respond with a clarifying question or an assumption. For instance: </span><em>“Are you using React class components or functional components?”</em><span> or </span><em>“I assume the input is a string – please confirm.”</em><span> If you ignore these and just reiterate your request, you’re missing an opportunity to improve the prompt. The AI is signaling that it needs more info. Always answer its questions or refine your prompt to include those details. Additionally, if the AI’s output is clearly off (like it misunderstood the question), </span><em>don’t just retry the same prompt verbatim</em><span>. Take a moment to adjust your wording. Maybe your prompt had an ambiguous phrase or omitted something essential. Treat it like a conversation – if a human misunderstood, you’d explain differently; do the same for the AI.</span></p></li><li><p><strong>Anti-Pattern: Varying Style or Inconsistency.</strong><span> If you keep changing how you ask or mixing different formats in one go, the model can get confused. For example, switching between first-person and third-person in instructions, or mixing pseudocode with actual code in a confusing way. Try to maintain a consistent style within a single prompt. If you provide examples, ensure they are clearly delineated (use Markdown triple backticks for code, quotes for input/output examples, etc.). Consistency helps the model parse your intent correctly. Also, if you have a preferred style (say, ES6 vs ES5 syntax), consistently mention it, otherwise the model might suggest one way in one prompt and another way later.</span></p></li><li><p><strong>Anti-Pattern: Vague references like “above code”.</strong><span> When using chat, if you say “the above function” or “the previous output”, be sure the reference is clear. If the conversation is long and you say “refactor the above code”, the AI might lose track or pick the wrong code snippet to refactor. It’s safer to either quote the code again or specifically name the function you want refactored. Models have a limited attention window, and although many LLMs can refer to prior parts of the conversation, giving it explicit context again can help avoid confusion. This is especially true if some time (or several messages) passed since the code was shown.</span></p></li></ul><p><span>Finally, here’s a </span><strong>tactical approach to rewriting prompts</strong><span> when things go wrong:</span></p><ul><li><p><strong>Identify what was missing or incorrect in the AI’s response.</strong><span> Did it solve a different problem? Did it produce an error or a solution that doesn’t fit? For example, maybe you asked for a solution in TypeScript but it gave plain JavaScript. Or it wrote a recursive solution when you explicitly wanted iterative. Pinpoint the discrepancy.</span></p></li><li><p><strong>Add or emphasize that requirement in a new prompt.</strong><span> You might say, </span><em>“The solution should be in TypeScript, not JavaScript. Please include type annotations.”</em><span> Or, </span><em>“I mentioned I wanted an iterative solution – please avoid recursion and use a loop instead.”</em><span> Sometimes it helps to literally use phrases like </span><em>“Note:”</em><span> or </span><em>“Important:”</em><span> in your prompt to highlight key constraints (the model doesn’t have emotions, but it does weigh certain phrasing as indicating importance). For instance: </span><em><span>“</span><strong>Important:</strong><span> Do not use any external libraries for this.”</span></em><span> or </span><em><span>“</span><strong>Note:</strong><span> The code must run in the browser, so no Node-specific APIs.”</span></em><span>.</span></p></li><li><p><strong>Break down the request further if needed.</strong><span> If the AI repeatedly fails on a complex request, try asking for a smaller piece first. Or ask a question that might enlighten the situation: </span><em>“Do you understand what I mean by X?”</em><span> The model might then paraphrase what it thinks you mean, and you can correct it if it’s wrong. This is meta-prompting – discussing the prompt itself – and can sometimes resolve misunderstandings.</span></p></li><li><p><strong>Consider starting fresh if the thread is stuck.</strong><span> Sometimes after multiple tries, the conversation may reach a confused state. It can help to start a new session (or clear the chat history for a moment) and prompt from scratch with a more refined ask that you’ve formulated based on previous failures. The model doesn’t mind repetition, and a fresh context can eliminate any accumulated confusion from prior messages.</span></p></li></ul><p>By being aware of these anti-patterns and their solutions, you’ll become much faster at adjusting your prompts on the fly. Prompt engineering for developers is very much an iterative, feedback-driven process (as any programming task is!). The good news is, you now have a lot of patterns and examples in your toolkit to draw from.</p><p><span>Prompt engineering is a bit of an art and a bit of a science – and as we’ve seen, it’s quickly becoming a must-have skill for developers working with AI code assistants. By crafting clear, context-rich prompts, you essentially </span><em>teach</em><span> the AI what you need, just as you would onboard a human team member or explain a problem to a peer. Throughout this article, we explored how to systematically approach prompts for debugging, refactoring, and feature implementation:</span></p><ul><li><p>We learned to feed the AI the same information you’d give a colleague when asking for help: what the code is supposed to do, how it’s misbehaving, relevant code snippets, and so on – thereby getting much more targeted help .</p></li><li><p>We saw the power of iterating with the AI, whether it’s stepping through a function’s logic line by line, or refining a solution through multiple prompts (like turning a recursive solution into an iterative one, then improving variable names) . Patience and iteration turn the AI into a true pair programmer rather than a one-shot code generator.</p></li><li><p>We utilized role-playing and personas to up-level the responses – treating the AI as a code reviewer, a mentor, or an expert in a certain stack . This often produces more rigorous and explanation-rich outputs, which not only solve the problem but educate us in the process.</p></li><li><p>For refactoring and optimization, we emphasized defining what “good” looks like (be it faster, cleaner, more idiomatic, etc.) , and the AI showed that it can apply known best practices when guided (like parallelizing calls, removing duplication, handling errors properly). It’s like having access to the collective wisdom of countless code reviewers – but you have to ask the right questions to tap into it.</p></li><li><p>We also demonstrated building new features step by step with AI assistance, showing that even complex tasks can be decomposed and tackled one prompt at a time. The AI can scaffold boilerplate, suggest implementations, and even highlight edge cases if prompted – acting as a knowledgeable co-developer who’s always available.</p></li><li><p>Along the way, we identified pitfalls to avoid: keeping prompts neither too vague nor too overloaded, always specifying our intent and constraints, and being ready to adjust when the AI’s output isn’t on target. We cited concrete examples of bad prompts and saw how minor changes (like including an error message or expected output) can dramatically improve the outcome.</p></li></ul><p><span>As you incorporate these techniques into your workflow, you’ll likely find that working with AI becomes more intuitive. You’ll develop a feel for what phrasing gets the best results and how to guide the model when it goes off course. Remember that the AI is a product of its training data – it has seen many examples of code and problem-solving, but it’s </span><em>you</em><span> who provides direction on which of those examples are relevant now. In essence, </span><strong>you set the context, and the AI follows through</strong><span>.</span></p><p><strong>It’s also worth noting that prompt engineering is an evolving practice.</strong><span> The community of developers is constantly discovering new tricks – a clever one-liner prompt or a structured template can suddenly go viral on social media because it unlocks a capability people didn’t realize was there. Stay tuned to those discussions (on Hacker News, Twitter, etc.) because they can inspire your own techniques. But also, don’t be afraid to experiment yourself. Treat the AI as a flexible tool – if you have an idea (“what if I ask it to draw an ASCII diagram of my architecture?”), just try it. You might be surprised at the results, and if it fails, no harm done – you’ve learned something about the model’s limits or needs.</span></p><p><strong>In summary, prompt engineering empowers developers to get more out of AI assistants.</strong><span> It’s the difference between a frustrating experience (“this tool is useless, it gave me nonsense”) and a productive one (“this feels like pair programming with an expert who writes boilerplate for me”). By applying the playbook of strategies we’ve covered – from providing exhaustive context to nudging the AI’s style and thinking – you can turn these code-focused AI tools into true extensions of your development workflow. The end result is not only that you code faster, but often you pick up new insights and patterns along the way (as the AI explains things or suggests alternatives), leveling up your own skillset.</span></p><p><span>As a final takeaway, remember that </span><strong>prompting is an iterative dialogue</strong><span>. Approach it with the same clarity, patience, and thoroughness you’d use when communicating with another engineer. Do that, and you’ll find that AI assistants can significantly amplify your abilities – helping you debug quicker, refactor smarter, and implement features with greater ease. </span></p><p><strong>Happy prompting, and happy coding!</strong></p><p><strong>Further reading:</strong></p><ul><li><p><em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">How to write better prompts for GitHub Copilot</a></em><a href="https://github.blog/developer-skills/github/how-to-write-better-prompts-for-github-copilot/#:~:text=3%20best%20practices%20for%20prompt,crafting%20with%20GitHub%20Copilot" rel="">. GitHub Blog</a></p></li><li><p><em><a href="https://strapi.io/blog/ChatGPT-Prompt-Engineering-for-Developers#:~:text=1" rel="">ChatGPT Prompt Engineering for Developers: 13 Best Examples</a></em></p></li><li><p><em><a href="https://medium.com/data-science/using-chatgpt-for-efficient-debugging-fc9e065b7856#:~:text=If%20nothing%20comes%20to%20mind%2C,don%E2%80%99t%20be%20afraid%20to%20experiment" rel="">Using ChatGPT for Efficient Debugging</a></em></p></li><li><p><em><a href="https://dev.to/jamesbright/prompt-engineering-for-lazy-programmers-getting-exactly-the-code-you-want-and-even-more-out-of-chatgpt-3plf#:~:text=The%20Trick%3A%20,rewrite%20the%20function%20if%20necessary" rel="">Prompt Engineering for Lazy Programmers: Getting Exactly the Code You Want</a></em></p></li><li><p><em><a href="https://www.linkedin.com/pulse/best-practices-prompting-github-copilot-vs-code-pamela-fox#:~:text=In%20this%20post%2C%20I%27m%20going,provide%20context%20and%20be%20predictable" rel="">Best practices for prompting GitHub Copilot in VS Code</a></em></p></li><li><p><em><a href="https://medium.com/@shamawali/chatgpt-a-new-age-debugger-10-prompts-20ee3e9c63aa#:~:text=2,Debug%20This%20Function%20for%20Me" rel="">ChatGPT: A new-age Debugger, 10 Prompts</a></em></p></li><li><p><em><a href="https://dev.to/techiesdiary/chatgpt-prompts-for-code-review-and-debugging-48j#:~:text=5%20Debug%20Debug%20the%20given,Enter%20your%20code%20here" rel="">ChatGPT Prompts for Code Review and Debugging</a></em></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png" width="1456" height="1456" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1456,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:624626,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://addyo.substack.com/i/164288010?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04b4358b-8b38-4e5b-8d99-22463ecb879e_5246x5246.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[FFmpeg Merges WebRTC Support (502 pts)]]></title>
            <link>https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</link>
            <guid>44182186</guid>
            <pubDate>Wed, 04 Jun 2025 15:58:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00">https://git.ffmpeg.org/gitweb/ffmpeg.git/commit/167e343bbe75515a80db8ee72ffa0c607c944a00</a>, See on <a href="https://news.ycombinator.com/item?id=44182186">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>
avformat/whip:&nbsp;Add&nbsp;WHIP&nbsp;muxer&nbsp;support&nbsp;for&nbsp;subsecond&nbsp;latency&nbsp;streaming</p><p>

0.&nbsp;WHIP&nbsp;Version&nbsp;3.<br>
1.&nbsp;The&nbsp;WHIP&nbsp;muxer&nbsp;has&nbsp;been&nbsp;renamed&nbsp;and&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;improved&nbsp;logging&nbsp;context&nbsp;and&nbsp;error&nbsp;messages&nbsp;for&nbsp;SSL,&nbsp;DTLS,&nbsp;and&nbsp;RTC.<br>
2.&nbsp;Magic&nbsp;numbers&nbsp;have&nbsp;been&nbsp;replaced&nbsp;with&nbsp;macros&nbsp;and&nbsp;extracted&nbsp;to&nbsp;functions,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;log&nbsp;levels&nbsp;have&nbsp;been&nbsp;altered&nbsp;for&nbsp;better&nbsp;clarity.<br>
3.&nbsp;DTLS&nbsp;curve&nbsp;list&nbsp;has&nbsp;been&nbsp;updated,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;SRTP&nbsp;profile&nbsp;names&nbsp;have&nbsp;been&nbsp;refined&nbsp;for&nbsp;FFmpeg&nbsp;and&nbsp;OpenSSL.<br>
4.&nbsp;ICE&nbsp;STUN&nbsp;magic&nbsp;number&nbsp;has&nbsp;been&nbsp;refined,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;RTP&nbsp;payload&nbsp;types&nbsp;have&nbsp;been&nbsp;updated&nbsp;based&nbsp;on&nbsp;Chrome's&nbsp;definition.<br>
5.&nbsp;Fixed&nbsp;frame&nbsp;size&nbsp;has&nbsp;been&nbsp;refined&nbsp;to&nbsp;rtc-&gt;audio_par-&gt;frame_size,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;h264_mp4toannexb&nbsp;is&nbsp;now&nbsp;used&nbsp;to&nbsp;convert&nbsp;MP4/ISOM&nbsp;to&nbsp;annexb.<br>
6.&nbsp;OPUS&nbsp;timestamp&nbsp;issue&nbsp;has&nbsp;been&nbsp;addressed,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;marker&nbsp;setting&nbsp;has&nbsp;been&nbsp;corrected&nbsp;after&nbsp;utilizing&nbsp;BSF.<br>
7.&nbsp;DTLS&nbsp;handshake&nbsp;and&nbsp;ICE&nbsp;handling&nbsp;have&nbsp;been&nbsp;optimized&nbsp;for&nbsp;improved&nbsp;performance,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;single&nbsp;handshake&nbsp;timeout&nbsp;and&nbsp;server&nbsp;role&nbsp;to&nbsp;prevent&nbsp;ARQ.<br>
8.&nbsp;Consolidated&nbsp;ICE&nbsp;request/response&nbsp;handling&nbsp;and&nbsp;DTLS&nbsp;handshake&nbsp;into&nbsp;a&nbsp;single&nbsp;function,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;fixed&nbsp;OpenSSL&nbsp;build&nbsp;errors&nbsp;to&nbsp;work&nbsp;with&nbsp;Pion.<br>
9.&nbsp;Merge&nbsp;TLS&nbsp;&amp;&nbsp;DTLS&nbsp;implementation,&nbsp;shared&nbsp;BIO&nbsp;callbacks,&nbsp;read,&nbsp;write,<br>
&nbsp;&nbsp;&nbsp;&nbsp;print_ssl_error,&nbsp;openssl_init_ca_key_cert,<br>
&nbsp;&nbsp;&nbsp;&nbsp;init_bio_method&nbsp;function&nbsp;and&nbsp;shared&nbsp;same&nbsp;data&nbsp;structure<br>
10.&nbsp;Modify&nbsp;configure&nbsp;that&nbsp;whip&nbsp;is&nbsp;enabled&nbsp;only&nbsp;dtls&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;enabled(just&nbsp;support&nbsp;openssl&nbsp;for&nbsp;now)&nbsp;to&nbsp;fix&nbsp;build&nbsp;error</p><p>

<span>Co-authored-by: winlin &lt;winlinvip@gmail.com&gt;</span><br>
<span>Co-authored-by: yangrtc &lt;yangrtc@aliyun.com&gt;</span><br>
<span>Co-authored-by: cloudwebrtc &lt;duanweiwei1982@gmail.com&gt;</span><br>
<span>Co-authored-by: Haibo Chen &lt;495810242@qq.com&gt;</span><br>
<span>Co-authored-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span><br>
<span>Co-authored-by: Jun Zhao &lt;barryjzhao@tencent.com&gt;</span><br>
<span>Signed-off-by: Jack Lau &lt;jacklau1222@qq.com&gt;</span><br>
<span>Signed-off-by: Steven Liu &lt;lq@chinaffmpeg.org&gt;</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A practical guide to building agents [pdf] (114 pts)]]></title>
            <link>https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf</link>
            <guid>44181700</guid>
            <pubDate>Wed, 04 Jun 2025 15:21:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf">https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=44181700">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[The Right to Repair Is Law in Washington State (334 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</link>
            <guid>44181421</guid>
            <pubDate>Wed, 04 Jun 2025 15:00:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state">https://www.eff.org/deeplinks/2025/06/right-repair-law-washington-state</a>, See on <a href="https://news.ycombinator.com/item?id=44181421">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p>Thanks in part <a href="https://www.eff.org/deeplinks/2025/05/washingtons-right-repair-bill-heads-governor">to your support</a>, the right to repair is now law in Washington.</p>
<p>Gov. Bob Ferguson signed two bills guaranteeing Washingtonians' right to access tools, parts, and information so they can fix personal electronics, appliances, and wheelchairs. This is the epitome of common-sense legislation. When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>When you own something, you should have the final say about who fixes, adapts, or modifies it—and how.</p>
<p>Advocates in Washington have worked for years to pass a strong right-to-repair law in the state. In addition to Washington’s <a href="https://pirg.org/washington/updates/washington-becomes-eighth-state-with-right-to-repair-law-on-the-books/">Public Interest Research Group</a>, the consumer electronics bill moved forward with a growing group of supporting organizations, including environmental advocates, consumer advocates, and manufacturers such as Google and Microsoft. Meanwhile, advocacy from groups including&nbsp; <a href="https://disabilityrightswa.org/">Disability Rights Washington</a>&nbsp;and the <a href="https://www.hereandnowproject.org/">Here and Now Project</a> made the case for the wheelchair's inclusion in the right-to-repair bill, bringing their personal stories to Olympia to show why this bill was so important.</p>
<p>And it’s not just states that recognize the need for people to be able to fix their own stuff.&nbsp; Earlier this month, U.S. Army Secretary Dan Driscoll <a href="https://media.defense.gov/2025/May/01/2003702281/-1/-1/1/ARMY-TRANSFORMATION-AND-ACQUISITION-REFORM.PDF">issued a memo</a> stating that the Army should “[identify] and propose contract modifications for right to repair provisions where intellectual property constraints limit the Army's ability to conduct maintenance and access the appropriate maintenance tools, software, and technical data – while preserving the intellectual capital of American industry.” The memo said that the Army should seek this in future procurement contracts and also to amend existing contracts to include the right to repair.</p>
<p>This is a bedrock of sound procurement with a long history in America. President Lincoln only bought rifles with standardized tooling to outfit the Union Army, for the obvious reason that it would be a little embarrassing for the Commander in Chief to have to pull his troops off the field because the Army’s sole supplier had decided not to ship this week’s delivery of ammo and parts. Somehow, the Department of Defense forgot this lesson over the ensuing centuries, so that today, billions of dollars in public money are spent on material and systems that the US military can only maintain by buying service from a “beltway bandit.”</p>
<p>This recognizes what millions of people have said repeatedly: limiting people’s ability to fix their own stuff stands in the way of needed repairs and maintenance. That’s true whether you’re a farmer with a broken tractor during harvest, a homeowner with a misbehaving washing machine or a cracked smartphone screen, a hospital med-tech trying to fix a ventilator, or a soldier struggling with a broken generator.</p>
<p>The right to repair is gaining serious momentum. All 50 states have now considered some form of right-to-repair legislation. Washington is the eighth state to pass one of these bills into law—let’s keep it up.</p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA["AI Will Replace All the Jobs " Is Just Tech Execs Doing Marketing (173 pts)]]></title>
            <link>https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</link>
            <guid>44181172</guid>
            <pubDate>Wed, 04 Jun 2025 14:38:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/">https://sparktoro.com/blog/ai-will-replace-all-the-jobs-is-just-tech-execs-doing-marketing/</a>, See on <a href="https://news.ycombinator.com/item?id=44181172">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
<p>Over the weekend, I went digging for evidence that AI can, will, or has replaced a large percent of jobs. It doesn’t exist. Worse than that, actually, there’s hundreds of years of evidence and sophisticated analyses from hundreds of sources showing the opposite is true: AI will almost certainly create more jobs than it displaces, just like thousands of remarkable technologies before it.</p>


<div>
<figure data-wp-context="{&quot;imageId&quot;:&quot;68405d5f71052&quot;}" data-wp-interactive="core/image"><img fetchpriority="high" decoding="async" width="1024" height="576" data-wp-class--hide="state.isContentHidden" data-wp-class--show="state.isContentVisible" data-wp-init="callbacks.setButtonStyles" data-wp-on-async--click="actions.showLightbox" data-wp-on-async--load="callbacks.setButtonStyles" data-wp-on-async-window--resize="callbacks.setButtonStyles" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-1024x576.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-300x169.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle-768x432.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/ai-will-take-the-jobs-fear-hype-cycle.png 1280w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<p>I don’t want anyone to think I’m raining on this parade without first attempting to convince myself that the opposite was true, and that AI really would be the first technology in 120 years to displace a massive portion of the workforce. So, dry though it may be, let’s walk through the logic together.</p>



<p>The majority of statements that have received press (and there have been dozens in the last 5 years) center on the claim that AI will destroy 20-50% of the current need for human labor. I’ll attempt to address each of the most robust points inherent in those arguments, rather than trying to argue that any one innovation or upgrade to a model’s capability hasn’t done it yet (or won’t):</p>



<ul>
<li>If AI is going to make such a huge percentage of jobs redundant, there must be historical analogies–i.e. other technologies that massively upended labor markets. What are these and how have they affected jobs in the past?
<ul>
<li><a href="https://www.technologyreview.com/2024/01/27/1087041/technological-unemployment-elon-musk-jobs-ai/">MIT’s Technology Review</a> noted that this “fear of new tech taking jobs,” is far from new. The automation of farm work is the most notable and most labor-impacting example we have from history, rapidly unemploying a huge portion of human beings in the developing economies of the late 19th and 20th centuries. And yet, at the conclusion of this era (~1940s/50s), the conclusion was that “technological unemployment is a myth,” because “technology has created so many new industries” and has expanded the market by “lowering the cost of production to make a price within reach of large masses of purchasers.” In short, technological advances had created more jobs overall.</li>



<li>Last year, Quarterly Journal of Economics <a href="https://academic.oup.com/qje/advance-article/doi/10.1093/qje/qjae008/7630187">published</a> a groundbreaking study on how technological innovations have impacted labor forces across industries since 1980. MIT did a nice <a href="https://news.mit.edu/2024/does-technology-help-or-hurt-employment-0401">summarization</a>: “the number of studies that support the labour replacement effect is more than offset by the number of studies that support the labour-creating/reinstating and real income effects.”</li>



<li><a href="https://www.sciencedirect.com/science/article/pii/S0040162523004353">This 2023 paper</a> looked at 127 previous studies of technology supposedly replacing labor forces from the 18th century to the present, concluding that “the labor displacing effect of technology appears to be more than offset by compensating mechanisms that create or reinstate labor.”</li>



<li>The Economic Policy Institute <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">did a deep dive</a> into what drives labor market demand and unemployment, concluding: “Productivity growth (which technology sometimes enables and other times drives) has not historically been associated with higher unemployment or higher inequality,” and that “Anxieties over widespread technology-driven unemployment lack an empirical base.”</li>



<li>Perhaps the closest analogy to AI is the personal computer revolution of the 1980s. Millions of jobs in communication, documentation, research, analysis, and engineering became obsolete within a decade, and yet, the <a href="https://www.mckinsey.com/featured-insights/future-of-work/what-can-history-teach-us-about-technology-and-jobs">McKinsey Global Institute concluded</a> in 2018 that “We tallied up all the jobs destroyed in the US since 1980 as a result of the rise of personal computing and the Internet, and it’s about 3.5 million,” but “When we add up all the jobs created, we find that over 19 million jobs have been created as a result of the personal computer and Internet. We see a net gain of 15.8 million jobs in the US over the last few decades. That’s about 10 percent of the civilian labor force today.”</li>
</ul>
</li>



<li>If AI is going to have these massive impacts but hasn’t yet, why not?
<ul>
<li>Folks who claim AI will destroy the labor market have claimed this radical change is “only a few years away,” “on the immediate horizon,” or “imminent,” for the last 5 years, yet we’re at historically low unemployment (yes, even accounting for <a href="https://www.fastcompany.com/91341084/functional-unemployment-what-it-means">underemployment</a> and the <a href="https://www.marketplace.org/story/2025/05/23/is-the-unemployment-rate-truly-capturing-whats-happening-in-the-labor-market">way the BLS counts employment</a>). The US labor market is within a single percentage point of its post-war unemployment low, measured in <a href="https://econofact.org/factbrief/did-us-unemployment-fall-to-the-lowest-rate-in-50-years-under-biden">1953 at 3.4%</a>.</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="1024" height="744" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1024x744.png 1024w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-300x218.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-768x558.png 768w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-1536x1116.png 1536w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/b54eb6431c21256713ce6cf3a9d37a6b-1-2048x1488.png 2048w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>


<ul>
<li>If AI is killing jobs, it’s doing so at an imperceptibly slow rate; why could that be? Is it still too early? Did other technologies take a long time to show their impacts on labor markets?
<ul>
<li>The broad consensus from rational industry observers, analysts, economists, and even AI-hyped technologists is that the end of cheap money (i.e. higher US interest rates) <a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">has driven</a> <a href="https://www.theatlantic.com/economy/archive/2025/04/job-market-youth/682641/?gift=o6MjJQpusU9ebnFuymVdsJ1qwI70CnAkjDXBfrYqvHw&amp;utm_source=copy-link&amp;utm_medium=social&amp;utm_campaign=share">most of</a> <a href="https://news.ycombinator.com/item?id=43612448">the</a> <a href="https://www.zdnet.com/article/is-ai-making-it-harder-for-new-college-grads-to-get-hired-in-tech/">lower-than-pre-pandemic-demand</a> for <a href="https://www.techtarget.com/whatis/feature/Tech-sector-layoffs-explained-What-you-need-to-know">entry-level talent</a> (just as it has in times of inflation-fighting interest hikes of the past).</li>



<li>Machine-learning, the technology underpinning AI, <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">has been around for decades</a>, with <a href="https://www.dataversity.net/a-brief-history-of-machine-learning/">widespread adoption</a> in tech companies between 2006-2013. The current generative-AI era, based on the transformer architecture model, kicked off in 2017, with significant public examples and tech adoption from 2018-2020. Most of the current, press-driven AI hype cycle, however, skyrocketed in late 2021 with OpenAI’s release of GPT-3 (longtime readers here will recall that <a href="https://britneymuller.com/">Britney Muller</a> showed off techniques <a href="https://www.linkedin.com/posts/britneymuller_bert-101-state-of-the-art-nlp-model-explained-activity-6907013731067523072-OXsd/">extremely similar</a> to what’s now associated with modern LLMs <a href="https://www.youtube.com/watch?v=47KHP3k2RPo">back in July 2018</a>).</li>



<li>We’ve had <a href="https://courses.cs.washington.edu/courses/cse490h1/19wi/exhibit/machine-learning-1.html">15-20 years</a> of robust machine learning development and adoption, and another <a href="https://toloka.ai/blog/history-of-generative-ai/">5-10 years of broad LLM/generative AI adoption</a>, improvement, and usage, yet labor market fluctuation has been far more dependent on other factors: the Covid pandemic itself, the post-pandemic surge and decline in tech hiring, inflation-fighting tactics by government banks, and (most recently), a renewal of early-20th-century-style tariffs and trade wars. When controlling for these events.</li>



<li>The effects of previous technological advancements also took time, but the most salient examples (of farm equipment in the 1910-1920 era and the personal computer in the 1980s) showed millions of displaced workers <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">within 5 years</a>. AI’s slower changes bode poorly for the argument that it will have a larger impact than those events.</li>



<li>Even if one assumes that AI was the only contributor to labor market changes between 2021-2025, the change has been incredibly slight, *even* in the software engineering market where it supposedly has the greatest impact. There <a href="https://www.reddit.com/r/programming/comments/1di8pe9/us_employment_of_software_developers_is_in/">was a greater loss</a> (nearly 150%) in percentage of software engineering jobs between 2019-2021 than from 2021-2025.</li>



<li>I found it particularly revealing that one of the most commonly cited examples of AI killing labor needs in the software field is the death of StackOverflow, and yet, a <a href="https://www.infoworld.com/article/3993482/ai-didnt-kill-stack-overflow.html">robust analysis of that site’s usage from 2008-2020</a> shows that “What really happened is a parable of human community and experiments in self-governance gone bizarrely wrong.”</li>
</ul>
</li>



<li>However, it seems likely that the perception of AI and its adoption are slowing hiring in the software engineering market in the post-bubble-popping era (2024-25). This thoughtful analysis by <a href="https://substack.com/@pragmaticengineer">Gergely Orosz</a> concludes a <a href="https://newsletter.pragmaticengineer.com/p/software-engineering-job-openings">well-visualized, data-driven walkthrough</a> with: “LLMs are a leading cause of the fall in software developer job postings: there’s uncertainty at large companies about whether to hire as fast as previously, given the productivity hype around AI tooling, and businesses are opting to “wait and see” by slowing down recruitment, as a result.”</li>



<li>It strains credibility to look at the data, history, and analyses and conclude that AI will eventually kill 20-50% of all jobs, when its largest impact in the prior 5-20 years of adoption (depending on one’s starting point) is <a href="https://archive.ph/t8f9X">~10% variation in a job sector that employs ~1% of US workers</a>.</li>



<li>Assuming AI will have an effect similar to 20th Century farm equipment’s on agriculture, why will that labor force behave differently to their 20th Century counterparts (and either refuse to or be prevented from finding new jobs)?
<ul>
<li>This point is hard to find citations for, given that it’s a future-looking, theoretical assertion. We can, however, compare the impact of the tractor (and farm machinery more broadly) on the economy from 1910-1960.</li>



<li>Tractors and farm equipment resulted in the shutdown of a huge number of farms, and a decline in the number of people employed in farming, from ~33% to ~2% of the labor force (notably, even that massive upheaval was less significant than the <a href="https://www.google.com/search?q=ai+will+take+half+of+all+jobs&amp;tbm=nws">prognostications</a> by tech company leaders that AI will displace half of all jobs). Nothing like it has happened in the American economy since, and only the industrial revolution of the 18th/19th centuries can compete in scale of transformation.</li>



<li>A superb breakdown of farm machinery’s impact on a sector that employed more than a quarter of all Americans comes from <a href="https://faculty.econ.ucdavis.edu/faculty/alolmstead/Recent_Publications/Reshaping_the_Landscape.pdf">Olmstead and Rhode at UC Davis</a>:</li>
</ul>
</li>
</ul>


<div>
<figure><img decoding="async" width="657" height="161" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png" alt="The upshot is that by 1960 the tractor reduced annual labor use by at least 3.44 billion man-hours of field and chore labor from the level required using the horse power technology. (The 3.44 billion figure combines the USDA estimate of annual labor savings before 1944 and our lower-bound estimate of saving between 1944 and 1959.) This was the equivalent of approximately 1,720 thousand workers, which represented 24.3 percent of farm employment in 1960 and 27.3 percent of the decline since 1910." srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image.png 657w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-300x74.png 300w" sizes="(max-width: 657px) 100vw, 657px"></figure></div>


<ul>
<li>Is it possible that AI will do to broad sectors of the economy what mechanized farm equipment did to agriculture?
<ul>
<li>Rationally, it’s difficult to fathom generative AI having a greater economic and labor-force impact than the PC revolution of the 1980s. AI makes many tasks more efficient, but evidence that it can wholesale replace entire human functions in a tractor-like way is pure speculation that exists in imagination, not reality.</li>



<li>The core assertion by the “AI will replace 20-50% of all jobs” crowd seems to be that the past 20 years of machine learning and generative AI improvements are not indicative of what will happen in the future: a leap in capability that will enable company management to instruct an AI on a job function (“get us press,” or “optimize our marketing campaign,” or “record and audit our financials” ) and rely on machines to correctly determine what needs to be done, how to do it, and then complete all associated tasks with little to no human supervision, intervention, or additional labor.</li>



<li>It’s impossible to argue against the assertion that AI will do what’s described above, because it’s based not on objective data, but rather on subjective belief about a possible future. Fighting about what someone believes may come about in the future is generally non-productive, so I’ll avoid that to spare us all a lot of wasted time 😉</li>
</ul>
</li>
</ul>



<p>I’ll move on from the dry argument analysis and citation process and attempt to summarize (and opine on) what’s really going on here.</p>



<p>Leaders of AI companies, and some AI proponents, marketers, journalists, and even critics have found that when they make scary predictions about their field destroying the job market, press and media eat it up. This media coverage, because it’s scary and the AI hype cycle is in full swing, draws clicks. Those clicks lead to employees, managers, and leaders at other businesses being scared into learning and adopting AI in their businesses.</p>



<p>Incentive also exists for those who criticize AI, AI companies, or their ethics/models/practices: these folks also benefit directly from the attention they earn when they amplify the message of AI as a job destroying technology.</p>



<p>If you’re feeling like the “AI will take all our jobs” discussion is familiar, you’re in good company. Many others have pointed out the similarities to stories like:</p>


<div>
<figure><a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/"><img loading="lazy" decoding="async" width="816" height="557" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1.png 816w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-300x205.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-1-768x524.png 768w" sizes="auto, (max-width: 816px) 100vw, 816px"></a></figure></div>


<p>Source: <a href="https://www.jalopnik.com/elon-musk-tesla-self-driving-cars-anniversary-autopilot-1850432357/">Jalopnik</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="786" height="631" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2.png 786w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-300x241.png 300w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-2-768x617.png 768w" sizes="auto, (max-width: 786px) 100vw, 786px"></figure></div>


<p>Source: <a href="https://www.insidehook.com/culture/older-generations-kids-too-soft">InsideHook</a></p>


<div>
<figure><img loading="lazy" decoding="async" width="767" height="816" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3.png 767w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-3-282x300.png 282w" sizes="auto, (max-width: 767px) 100vw, 767px"></figure></div>


<p>Source: <a href="https://www.honestjobs.com/post/nobody-wants-to-work-anymore-is-not-new-and-it-s-not-true">Honest Jobs</a></p>



<p>Mechanization really did take jobs from farm workers. Automation took jobs from manual laborers. The PC took jobs from clerical and communication workers. But, all of these resulted in greater productivity, employment, and more optionality for workers. It’s both anti-historic and anti-evidence that AI will somehow prove to be the exception.</p>



<p>Could AI, along with thousands of other impactful technological, political, social, demographic, and black-swan-event changes permanently alter the employment landscape in our lifetimes? Absolutely. In fact, one of my favorite stats from this overly-ambitious weekend of research was MIT’s estimation that <a href="https://www.nber.org/papers/w30389">60% of employment in 2018 was in types of jobs that didn’t exist before 1940</a>.</p>



<p>By the time I’m in my 80s, y’all better have destroyed more than half of all the existing jobs, and that’s just to keep up with the 20th Century’s pace of change. But, don’t expect AI to do it for you in the next decade; that’s just marketing.</p>



<p>p.s. If you’re looking for the TL;DR, <a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o">Ed Zitron on Bluesky</a> has got you:</p>


<div>
<figure><a href="https://bsky.app/profile/edzitron.com/post/3lqaxozwxfc2o" target="_blank" rel=" noreferrer noopener"><img loading="lazy" decoding="async" width="710" height="736" src="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png" alt="" srcset="https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4.png 710w, https://images.sparktoro.com/blog/wp-content/uploads/2025/06/image-4-289x300.png 289w" sizes="auto, (max-width: 710px) 100vw, 710px"></a></figure></div>


<p>p.p.s. I agree there’s evidence that this fear-based marketing campaign has been successful enough to disrupt some hiring, especially for <a href="https://www.reddit.com/r/Futurology/comments/1l100p3/ai_is_breaking_entrylevel_jobs_that_gen_z_workers/">early-stage jobs in a few tech-heavy fields</a>. But squinting at the evidence, it’s &lt;0.1% of jobs (&lt;200,000 total) being affected, and even here, the unbalanced capital vs. labor market is <a href="https://www.epi.org/publication/ai-unbalanced-labor-markets/">a far more compelling explanation</a>.</p>

		
	</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Sky's the limit: AI automation on Mac (113 pts)]]></title>
            <link>https://taoofmac.com/space/blog/2025/06/03/2155</link>
            <guid>44179691</guid>
            <pubDate>Wed, 04 Jun 2025 11:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://taoofmac.com/space/blog/2025/06/03/2155">https://taoofmac.com/space/blog/2025/06/03/2155</a>, See on <a href="https://news.ycombinator.com/item?id=44179691">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://taoofmac.com/space/blog/2025/06/03/2155">Jun 3<sup>rd</sup> 2025</a> · 4 min read
 · <small>
#ai 
#apple 
#automation 
#desktop 
#intelligence 
#macos 
#shortcuts 
#sky 
#wwdc 
</small>
</p><section id="main">
    <p>I’ve been sitting on this draft for a few days now, partly because I thought it would turn down the bitterness, and partly because I kept asking myself whether I should even write it. But I think it is worth getting out of my system, so here goes.</p>
<p>In case you’re not in the Mac community, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is an app that brings <a href="https://taoofmac.com/space/ai" rel="next">AI</a> automation to the Mac that <a href="https://www.macstories.net/stories/sky-for-mac-preview/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Federico Viticci wrote about at length last week</a>, and that not only looks and feels exactly like what I would expect <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">Apple Intelligence</a> to be like, it also completely blows out of the water all the desktop automation tools that have sprung out of the <a href="https://taoofmac.com/space/ai/mcp" rel="next">MCP</a> hype.</p>
<p>I have several questions, some of which I have already sort of asked <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">back in March</a>, but which I think are worth reformulating.</p>
<p>The people who created <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> are the same people who created Workflow and worked on Shortcuts, so here’s my first question:</p>
<p><em>Why wasn’t Apple able to harness their expertise in the first place?</em></p>
<p>I mean, people have free will and all, and can choose to work wherever they want, but this makes my <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">earlier rant about their having neglected automation</a> feel like the first clue to a corporate culture murder scene.</p>
<p>Not having made it possible for them to thrive feels like vanilla corporate politics, but having brilliant people <em>leave</em> Apple and ship something that is, even in preview, <em>much better than anything that Apple Intelligence promised</em> (including the made up bits they paraded as marketing material) is just gross mismanagement (now you know why I held back on this draft).</p>
<p>Which leads me to my second question:</p>
<p><em>Why has Apple failed this badly?</em></p>
<p>Was it just a consequence of their innately siloed nature? The internal decline of John Giannandrea’s team (and the <a href="https://daringfireball.net/linked/2025/03/20/gurman-rockwell-siri?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">rumored hand-over of Siri to Craig Federighi’s team</a>) might have played a role, but <a href="https://taoofmac.com/space/com/apple/macos" rel="next">macOS</a> has been largely stagnant from a UX perspective for ages (and as far as I know it isn’t even being addressed in the upcoming Solarium redesign), so I have to assume the <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> team saw this <em>huge</em> blind spot in terms of improving the desktop experience and just jumped on it.</p>
<p><em>Was it about control? Privacy?</em></p>
<p>I can see Apple balking at doing something like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> (if they ever even considered it) because it not only has to share bits of your screen with an LLM, but also because it would have to open up the Mac to third-party automation in a way that it has never done before, and that would be a huge departure from their current approach.</p>
<p>Which, <a href="https://taoofmac.com/space/blog/2025/03/14/1830" rel="next">again</a>, is pretty much non-existent, so… No, that doesn’t make sense.</p>
<p>But the privacy angle is interesting, because Apple was in a <em>perfect</em> position to do something exactly like <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> and ensure that it was done in a way that respected user privacy. Even though local models are still not quite there yet (remember that RAM requirements are still very high as far as running truly useful models are concerned), they do have the confidential computing tech to run inference in a privacy-preserving way–which might be the only bit of Apple Intelligence that actually works at this point.</p>
<p>But <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a>, despite having cloud inference, is designed to enhance your <em>local</em> Mac experience, and it does so in a way that looks extremely polished, and, above all, <em>feels like the way people always wanted to use computers</em>. Star Trek echoes aside, it has the ability to understand what you want to do, and <em>automates your Mac</em> to achieve that.</p>
<p>Federico’s post also goes into part of the <em>how</em> it does this, and I get the impression that even though <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> can leverage the remnants of Mac automation, for gathering context it is completely bypassing the standard automation APIs and inferring UI structure and content.</p>
<p>Everything I read about it makes me think that Apple has dropped the ball so badly that <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is like a perfect storm of what they could have done, but didn’t.</p>
<p>And now, not only is it a third-party app that is doing what Apple should have done, but it is also doing it in <em>a better way that anything they ever shipped</em>.</p>
<p>And if <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> takes off (let’s face it, the Mac desktop market isn’t really mainstream these days and there is too much AI hype, but just entertain the notion for a bit), that will have the added bonus of highlighting that Apple are completely out of touch with what people want from their computers.</p>
<p>Which leads me to my final question:</p>
<p><em>What will it take for Apple to get its act together?</em></p>
<p>I honestly don’t know. I mean, I have been asking this question for years now, and I have no idea what it will take for Apple to take any sort of integration or automation seriously. I currently have <em>zero</em> expectations towards next week’s WWDC, and not only because of the Mac. We were fooled once, and I don’t think we will be fooled again.</p>
<p>Unless they actually <em>ship</em> something, which seems highly unlikely unless it slots into their yearly release cycle (which they are rumored to be rebranding as “OS 26” because, well, why not ship the org chart <em>and</em> their corporate calendar?).</p>
<p>A case in point (and stop me if you’ve heard this before): Spotlight has been a complete mess for years, and Apple has done <em>nothing</em> to effectively fix it on any of its platforms–and it would be a <em>perfect</em> place to start integrating <a href="https://taoofmac.com/space/ai" rel="next">AI</a> in a way that would actually make sense and be useful to users.</p>
<p>Not to mention that it would be a key component of any sort of retrieval-augmented generation approach, etc.</p>
<p>So yes, <a href="https://sky.app/?utm_source=taoofmac.com&amp;utm_medium=web&amp;utm_campaign=unsolicited_traffic&amp;utm_content=external_link" rel="external">Sky</a> is the limit. Or, at least, one very concrete yardstick by which we can measure how much Apple has failed to deliver on the promise of AI.</p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Just how bad are we at treating age-related diseases? (101 pts)]]></title>
            <link>https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases</link>
            <guid>44179329</guid>
            <pubDate>Wed, 04 Jun 2025 10:51:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases">https://www.ladanuzhna.xyz/writing/just-how-bad-are-we-at-treating-age-related-diseases</a>, See on <a href="https://news.ycombinator.com/item?id=44179329">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-871d4e7f6dd1de2654fa">
  <p><em>This post was last updated in September 2024. I can’t promise this will be up-to-date when you come across it.</em></p><p>Whether you believe in preventing damage (the core aim of the aging field - though that's not the focus of this post) or treating damage as it arises, our current best efforts in both treatment and mechanism selection haven’t yielded promising results. None of the approved drugs for age-related diseases reverse any damage. They don't even halt disease progression. The primary endpoints mainly assess whether the treatment causes a slightly slower rate of decline than would occur otherwise. In some cases, approved drugs show no objective functional benefit at all.  So how did we get here?</p><h3>Geographic atrophy (GA)</h3><p>GA is a gradual loss of central vision due to the breakdown of the retinal cells responsible for detailed sight – retinal pigment epithelium and photoreceptors.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>2 approved drugs, Syfovre&nbsp; (pegcetacoplan) and Izervay (ACP) - both approved in 2023</p></li><li><p>Neither drug halts disease progression. Atrophy progresses at a slightly slower rate (~19% less lesions after 2 years of monthly Syfovre eye injections)</p></li><li><p>Importantly, recent GA trials stopped relying on eyesight (measured through visual acuity score BCVA) as the primary endpoint. After multiple drugs failed to improve vision, the field switched to “lesion growth” as a trial readout</p></li><li><p>As a result, both Syfovre&nbsp; (pegcetacoplan) and Izervay (ACP) treated patients had a decline in vision similar to untreated patients - that is, while results of the trial were statistically significant, you or your family wouldn’t be able to tell as to whether you took the drug</p></li></ul><h3>Idiopathic Pulmonary Fibrosis (IPF)</h3><p>IPF is a disease where the lungs become scarred over time for no clear reason. This scarring makes it harder for your lungs to take in oxygen, so people with IPF often feel short of breath or have a dry cough.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>2 approved drugs, nintedanib, pirfenidone - both approved in 2014</p></li><li><p>The primary endpoint for the trial is forced vital capacity (FVC) – the total amount of air a person can forcibly exhale from their lungs</p></li><li><p>Neither drug improves prognosis or FVC, but rather slightly decreases its decline</p></li><li><p>It's a usual practice to run 2 large Phase 3 trials if the drug is intended for approval. In case of pirfenidone, one of Phase 3 trials (STUDY 006 of CAPACITY Trial) showed no improvement in the primary endpoint relative to placebo. The drug was approved nevertheless.</p></li></ul><h3>MASH</h3><p>MASH is a liver disease that happens when too much fat builds up in the liver, causing hepatocyte damage and acute inflammation. It usually starts with just extra fat in the liver, but over time, leads to fibrosis (scarring), and eventually liver cancer.&nbsp;</p><p>TLDR:</p><ul data-rte-list="default"><li><p>Resmetirom was approved in 2024 and it is the first and only drug approved for MASH</p></li><li><p>MASH drugs are tested through 2 metrics: MASH resolution with no worsening of fibrosis (a scoring metric that focuses on metabolic and inflammatory aspects of the disease) and Fibrosis improvement with no worsening of metabolic aspects of the disease (4 stages of fibrosis: F1-F4)</p></li><li><p>In the high-dose group of resmetirom, MASH resolution was achieved in 29.9% – vs 9.7% resolution in placebo</p></li><li><p>Worth noting that resmetirom and other MASH drugs are primarily focused on earlier stages of liver disease - F1-F3 - and very few drugs are being tested for late-stage fibrotic MASH (F4)</p></li></ul><h3>Alzheimer's</h3><p>Alzheimer's is a neurodegenerative disease, with causes being the source of <a href="https://www.science.org/content/article/potential-fabrication-research-images-threatens-key-theory-alzheimers-disease" target="_blank">many disputes and investigations in the scientific community</a>. It is characterized by cognitive decline, memory loss, and eventually, the inability to perform basic functions.</p><p>TLDR:</p><ul data-rte-list="default"><li><p>The primary endpoint for AD trials is CDR (Clinical Dementia Rating) which measures orientation, memory, problem-solving, interactions with the community, and home life</p></li><li><p>Back in 2021, the FDA approved Aducanumab, targeting beta-amyloid, <a href="https://www.nature.com/articles/d41586-021-01763-9"><span>which was later taken from the market because many believed it not to be efficacious</span></a></p></li><li><p>In 2023, the FDA approved lecanemab (Leqembi), with a mechanism of action identical to Aducanumab. The European Medicines Agency ruled against approving the drug.</p></li><ul data-rte-list="default"><li><p>In the lecanemab trial, both the treatment group and the placebo group declined&nbsp; in Clinical Dementia Rating -&nbsp; all showing progressive cognitive impairment, amyloid deposition by PET scans or in their cerebrospinal fluid. The treatment group declined less, with the result being “statistically significant” - and <a href="https://www.nytimes.com/2022/11/29/health/lecanemab-alzheimers-drug.html"><span>many clinicians pointed out that statistically significant is not the same as clinically meaningful</span></a></p></li></ul><li><p>Donanemab (Kisunla) is yet another anti-amyloid antibody approved recently, which had a slightly different trial design by stratifying patients based on tau levels (another protein accumulated in AD) – with low-tau patients&nbsp;showing slightly better response to treatment</p></li><ul data-rte-list="default"><li><p>Unfortunately, treated patients also showed cerebral edema (ARIA-E), 2.1% of the placebo group as opposed to 24% of the treatment group; and brain microhemorrhages (ARIA-H), 13.6% in placebo and 31.4% in the treatment</p></li></ul><li><p>Many of the AD have a pretty bad side-effect profile, including <a href="https://www.neurology.org/doi/10.1212/WNL.0000000000207156"><span>cerebral shrinkage</span></a> (a shared feature of beta-amyloid drugs)</p></li></ul><h3>Data</h3><h4>Geographic Atrophy</h4>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why I Wrote the BEAM Book (396 pts)]]></title>
            <link>https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</link>
            <guid>44179257</guid>
            <pubDate>Wed, 04 Jun 2025 10:36:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/">https://happihacking.com/blog/posts/2025/why_I_wrote_theBEAMBook/</a>, See on <a href="https://news.ycombinator.com/item?id=44179257">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	
	<h3> Post-mortems, coffee, and a decade of stubborn curiosity </h3><p>

	Posted:  2025-06-03</p>

	<h2>Why I wrote the Beam Book</h2>
<p>After ten years of keeping Klarna’s core system upright I know this: a 15
millisecond pause in the BEAM can stall millions of peak-shopping payments, trigger a 3 a.m. Christmas-Eve post-mortem, and earn you a very awake call from the CEO. I wrote <em>The BEAM Book</em> so the next engineer fixes that pause before the coffee cools.</p>
<p><img src="https://happihacking.com/images/thebeambooks.jpg" alt="A picture of two printed BEAM Books."></p><h3>Origins</h3>
<p>I opened the project on 12 October 2012 with a lone DocBook file with four lines of text and an oversized sense of optimism.
After two weeks, the commit log is mostly me adding structure, moving
headings, and updating metadata. Most of it is scaffolding. The actual
content is still just a few hopeful lines.</p>
<p>By November I had abandoned DocBook for AsciiDoc, written a custom build
script, and convinced myself the book could be wrapped up in six months.
Those early commits glow with energy: adds, rewrites, then more
rewrites to fix the rewrites.
Delusion is underrated.</p>
<p>In 2013 I managed to convince O’Reilly to publish. Moving the repo to their
Atlas system sounded simple until Atlas began hiding my main file and
overwriting half-finished chapters.</p>
<p>The Git history reads like a diary of frustration:
“Moving files to top level to cope with Atlas,” “Atlas seems to be
overwriting book.asciidoc”. Word count shot past 120 000 while actual
progress crawled. On 10 March 2015 I was literally “Smashing chapters into sections” just to keep the build green.</p>
<p>The quiet cancellation came two months later. No drama, just a polite call and a line through the contract. Relief mingled with embarrassment, I had spent two years rearranging files rather than finishing sentences.</p>
<p>Pragmatic Bookshelf took over that same year. I kept working in CVS for
their production system, but progress was slow. Eventually, they cancelled
too. On 20 January 2017, I imported everything into a new repo in one
massive commit: 6,622 files, over a million lines.
The rewrite stalled, and so did the project.</p>
<p>On 23 March 2017 I started fresh with Asciidoctor in a private GitHub repo, copy-pasting
only the parts that still made sense. Two weeks later, on April 7, minutes before
a lecture at Chalmers, I flipped the repository public. Within twenty-four
hours strangers fixed typos, added diagrams, and merged a Creative Commons
BY-4.0 license.</p>
<h3>What Kept Me Going</h3>
<p><img src="https://happihacking.com/images/star-history.svg" alt="A picture of the stars on GitHub passing 3000."></p><p>I kept going because I wanted to understand the BEAM properly. There’s
value in following the real logic, not just the surface explanations.</p>
<p>Community feedback made a difference. As soon as the repo was public,
people began sending corrections, examples, and improvements.</p>
<p>Seeing the numbers of people starring the repo on GitHub kept me going.
One highlight: <strong>Issue #113 – “Please continue being awesome.”</strong>
That emoji-laced drive-by encouragement (August 2018) still pops into my
head whenever motivation dips.</p>
<p><img src="https://happihacking.com/images/issue113.png" alt="Issue 113: This book
is ridiculously good. I have only read a few bits of it so far and have
learned a lot already. Please continue being awesome!"></p>
<p>The book started showing up as a reference in Erlang and BEAM conference
talks, sometimes several times in the same event. That was a clear signal
that others needed this as much as I did.</p>
<p>Even Twitter (in the good old days of Twitter) played a role. Whenever
someone mentioned the book or shared a
link, it was an extra nudge to keep at it.</p>
<p>Mostly, I just wanted a manual I could trust myself, a reference for the
parts of the VM that matter when things go wrong. That’s reason enough to
keep writing, even after the third rewrite.</p>
<h3>What’s Inside the Book &amp; Who It Helps</h3>
<p>The book covers what I wish I’d had when building and operating large
Erlang systems:</p>
<ul>
<li>Schedulers and process management: How the BEAM schedules,
prioritizes, and balances processes under real load.</li>
<li>Processes and their memory: How process heaps,
stack, messages, and binaries are managed and
why these details matter in production.</li>
<li>Garbage collection and memory: What actually happens
with per-process and global garbage collectors, binary references,
and memory leaks.</li>
<li>Tagging schemes and terms: How the BEAM represents data—integers,
floats, tuples, binaries, references—down to the tagging bits.</li>
<li>The compiler and the VM: How code is turned into instructions,
what the compiler does (and doesn’t do), and how the emulator executes it.</li>
<li>Tracing and debugging: Practical use of dbg, erlang:trace,
and other tools to follow messages, events, and identify bottlenecks.</li>
<li>Performance tuning: What matters when profiling real code,
understanding reductions, and tracking down real-world latency problems.</li>
<li>System architecture: How ERTS, the BEAM VM, and their subsystems
actually work together in a running node.</li>
</ul>
<p>If you build or operate Erlang or Elixir systems, especially under any kind
of scale—this book is for you. It saves you from hunting through mailing
lists, scattered docs, and code comments just to answer, “Why is the VM
behaving like this?”</p>
<h3>Lessons Learned</h3>
<p>Persistence beats perfection. Two cancelled publishing deals look bad on a
résumé, but an unfinished idea looks worse.</p>
<p>Boundaries matter. I made progress by blocking time for writing, turning
off notifications, and treating focus like a real deadline. Fika at 14:30
is non-negotiable.</p>
<p>The crowd helps. Making the repo public brought in corrections,
encouragement, and the occasional nudge when motivation was low.</p>
<p>Scope is everything. I cut the details on dirty schedulers, the new JIT,
and the debugger. Maybe those will end up in an appendix, but not in the
core.</p>
<p>Ship, then iterate. The BEAM changes every year. A living Git repo keeps
up.</p>
<p>A real deadline helps. This January, during my yearly review, I
decided to print the book in time for Code Beam Stockholm. I thought I had
until autumn, turns out the conference was June 2. That’s how you find out
what’s truly essential.</p>
<h3>Definition of Done</h3>
<p>Holding the print in my hands, it finally feels finished, at least for now. Years of scattered commits are bound into something real, so I’m calling it done.</p>
<h3>Get Involved</h3>
<p>You can now get the paperback—The BEAM Book 1.0 is live on Amazon. Buy it
here.&nbsp;<a href="https://www.amazon.com/dp/9153142535">Amazon</a></p>
<p>If you spot an error, want to improve something, or just want to see how it
works under the hood, star or fork the repo. File an issue or, even better,
submit a pull request. Contributors are credited in the acknowledgments.
<a href="https://github.com/happi/theBeamBook">GitHub: theBeamBook</a></p>
<p>If you read the book, please leave an honest review.
Algorithms notice real feedback more than marketing copy.</p>
<p>If your team wants a deep dive, I run hands-on BEAM internals
workshops, tailored for real systems, not just hello world.
Email me if that’s what you need.
<a href="mailto:happi@happihacking.com">happi@happihacking.com</a></p>


	<p>
	  - Happi
  </p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cockatoos have learned to operate drinking fountains in Australia (282 pts)]]></title>
            <link>https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</link>
            <guid>44178902</guid>
            <pubDate>Wed, 04 Jun 2025 09:42:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia">https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia</a>, See on <a href="https://news.ycombinator.com/item?id=44178902">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/cockatoos-have-learned-operate-drinking-fountains-australia: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Cloud Run GPUs, now GA, makes running AI workloads easier for everyone (288 pts)]]></title>
            <link>https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</link>
            <guid>44178468</guid>
            <pubDate>Wed, 04 Jun 2025 08:28:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available">https://cloud.google.com/blog/products/serverless/cloud-run-gpus-are-now-generally-available</a>, See on <a href="https://news.ycombinator.com/item?id=44178468">Hacker News</a></p>
<div id="readability-page-1" class="page"><div jsname="tx2NYc"><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><p><span>Developers love </span><a href="https://cloud.google.com/run"><span>Cloud Run</span><span>, Google Cloud’s serverless runtime, </span></a><span>for its simplicity, flexibility, and scalability. And today, we’re thrilled to announce that NVIDIA GPU support for Cloud Run is now generally available, offering a powerful runtime for a variety of use cases that’s also remarkably cost-efficient.&nbsp;</span></p>
<p><span>Now, you can enjoy the following benefits across both GPUs and CPUs:</span></p>
<ul>
<li>
<p><strong>Pay-per-second billing</strong><span>: You are only charged for the GPU resources you consume, down to the second.</span></p>
</li>
<li>
<p><strong>Scale to zero</strong><span>: Cloud Run automatically scales your GPU instances down to zero when no requests are received, eliminating idle costs. This is a game-changer for sporadic or unpredictable workloads.</span></p>
</li>
<li>
<p><strong>Rapid startup and scaling</strong><span> Go from zero to an instance with a GPU and drivers installed in under 5 seconds, allowing your applications to respond to demand very quickly. For example, when scaling from zero (cold start), we achieved an impressive Time-to-First-Token of approximately 19 seconds for a gemma3:4b model (this includes startup time, model loading time, and running the inference)</span></p>
</li>
<li>
<p><strong>Full streaming support</strong><span>: Build truly interactive applications with out-of-the box support for HTTP and WebSocket streaming, allowing you to provide LLM responses to your users as they are generated.</span></p>
</li>
</ul>
<p><span>Support for GPUs in Cloud Run is a significant milestone, underscoring our leadership in making GPU-accelerated applications simpler, faster, and more cost-effective than ever before.</span></p>
<p><span>“Serverless GPU acceleration represents a major advancement in making cutting-edge AI computing more accessible. With seamless access to NVIDIA L4 GPUs, developers can now bring AI applications to production faster and more cost-effectively than ever before.” </span><span>- Dave Salvator, director of accelerated computing products, NVIDIA</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>AI inference for everyone</strong></h3>
<p><span>One of the most exciting aspects of this GA release is that Cloud Run GPUs are now available to everyone for NVIDIA L4 GPUs, with </span><strong>no quota request required</strong><span>.This removes a significant barrier to entry, allowing you to immediately tap into GPU acceleration for your Cloud Run services. Simply use </span><code>--gpu 1</code><span> from the Cloud Run command line, or check the "GPU" checkbox in the console, no need to request quota:</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/1_XkZEV9U.max-1000x1000.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Production-ready</strong></h3>
<p><span>With general availability, Cloud Run with GPU support is now covered by Cloud Run's </span><a href="https://cloud.google.com/run/sla"><span>Service Level Agreement (SLA)</span></a><span>, providing you with assurances for reliability and uptime. By default, Cloud Run offers </span><a href="https://cloud.google.com/run/docs/zonal-redundancy"><span>zonal redundancy</span></a><span>, helping to ensure enough capacity for your service to be resilient to a zonal outage; this also applies to Cloud Run with GPUs. Alternatively, you can turn off zonal redundancy and benefit from a </span><a href="https://cloud.google.com/run/pricing"><span>lower price</span></a><span> for best-effort failover of your GPU workloads in case of a zonal outage.</span></p>
<h3><strong>Multi-regional GPUs</strong></h3>
<p><span>To support global applications, Cloud Run GPUs are available in five Google Cloud </span><a href="https://cloud.google.com/run/docs/locations#gpu"><span>regions</span></a><span>: us-central1 (Iowa, USA), europe-west1 (Belgium), europe-west4 (Netherlands), asia-southeast1 (Singapore), and asia-south1 (Mumbai, India), with more to come.</span></p>
<p><span>Cloud Run also </span><a href="https://cloud.google.com/run/docs/multiple-regions"><span>simplifies deploying your services across multiple regions</span></a><span>. For instance, you can deploy a service across the US, Europe and Asia with a single command, providing global users with lower latency and higher availability. For instance, here’s how to deploy </span><a href="https://ollama.com/" rel="noopener" target="_blank"><span>Ollama</span></a><span>, one of the easiest way to run open models, on Cloud Run across three regions:</span></p></div><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>See it in action: 0 to 100 NVIDIA GPUs in four minutes</strong></h3>
<p><span>You can witness the incredible scalability of Cloud Run with GPUs for yourself with </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2140" rel="noopener" target="_blank"><span>this live demo</span></a><span> from Google Cloud Next 25, showcasing how we scaled from 0 to 100 GPUs in just four minutes.</span></p></div><section><figure><section jscontroller="SCGBie" jsaction="rcuQ6b:npT2md"><img src="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" alt="https://storage.googleapis.com/gweb-cloudblog-publish/images/2_SrvmWli.max-1600x1600.png" jsname="P3Vluc" jsaction="click:HTIlC" loading="lazy"></section></figure><div><p>Load testing a Stable Diffusion service running on Cloud Run GPUs to 100 GPU instances in four minutes.</p></div></section><div jsaction="rcuQ6b:npT2md" jscontroller="M0Q3Qb"><h3><strong>Unlock new use cases with NVIDIA GPUs on Cloud Run jobs</strong></h3>
<p><span>The power of Cloud Run with GPUs isn't just for real-time inference using request-driven Cloud Run services. We're also excited to announce the availability of GPUs on </span><a href="https://cloud.google.com/run/docs/overview/what-is-cloud-run#cloud-run-jobs"><span>Cloud Run jobs</span></a><span>, unlocking new use cases, particularly for batch processing and asynchronous tasks:</span></p>
<ul>
<li>
<p><strong>Model fine-tuning</strong><span>: Easily fine-tune a pre-trained model on specific datasets without having to manage the underlying infrastructure. Spin up a GPU-powered job, process your data, and scale down to zero when it’s complete.</span></p>
</li>
<li>
<p><strong>Batch AI inferencing</strong><span>: Run large-scale batch inference tasks efficiently. Whether you're analyzing images, processing natural language, or generating recommendations, Cloud Run jobs with GPUs can handle the load.</span></p>
</li>
<li>
<p><strong>Batch media processing</strong><span>: Transcode videos, generate thumbnails, or perform complex image manipulations at scale.</span></p>
</li>
</ul>
<p><span><a href="https://docs.google.com/forms/d/e/1FAIpQLSe_-u-ZSxVLhRMZ3p4ZSk2CkgL_URKqNgyM8rfMGUrTbpqYJQ/viewform?usp=dialog" rel="noopener" target="_blank"><span>Sign up</span></a><span> for the private preview of GPUs on Cloud Run jobs.</span></span></p>
<h3><strong>What Cloud Run customers are saying</strong></h3>
<p><span>Don't just take our word for it. Here's what some early adopters of Cloud Run GPUs are saying:</span></p>
<p><span>"Cloud Run helps vivo quickly iterate AI applications and greatly reduces our operation and maintenance costs. The automatically scalable GPU service also greatly improves the efficiency of our AI going overseas.”</span><span> - Guangchao Li, AI Architect, vivo</span></p>
<p><span>"L4 GPUs offer really strong performance at a reasonable cost profile. Combined with the fast auto scaling, we were really able to optimize our costs and saw an 85% reduction in cost. We've been very excited about the availability of GPUs on Cloud Run."</span><span> - John Gill at </span><a href="https://youtu.be/PWPvX25R6dM?feature=shared&amp;t=2496" rel="noopener" target="_blank"><span>Next'25</span></a><span>, Sr. Software Engineer, Wayfair</span></p>
<p><span><span>"At Midjourney, we have found Cloud Run GPUs to be incredibly valuable for our image processing tasks. Cloud Run has a simple developer experience that lets us focus more on innovation and less on infrastructure management. Cloud Run GPU’s scalability also lets us easily analyze and process millions of images.</span><span>" - Sam Schickler, Data Team Lead, Midjourney</span></span></p>
<h3><strong>Get started today</strong></h3>
<p><span>Cloud Run with GPU is ready to power your next generation of applications. Dive into the </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu"><span>documentation</span></a><span>, explore our </span><a href="https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama"><span>quickstarts</span></a><span>, and review our </span><a href="https://cloud.google.com/run/docs/configuring/services/gpu-best-practices"><span>best practices for optimizing model loading</span></a><span>. We can't wait to see what you build!</span></p></div><section><span>Posted in</span><ul><li><a href="https://cloud.google.com/blog/products/serverless" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/serverless" track-metadata-module="tag list" track-metadata-module_headline="posted in">Serverless</a></li><li><a href="https://cloud.google.com/blog/products/application-modernization" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/application-modernization" track-metadata-module="tag list" track-metadata-module_headline="posted in">Application Modernization</a></li><li><a href="https://cloud.google.com/blog/products/compute" track-metadata-position="body" track-metadata-eventdetail="cloud.google.com/blog/products/compute" track-metadata-module="tag list" track-metadata-module_headline="posted in">Compute</a></li></ul></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Machine Code Isn't Scary (156 pts)]]></title>
            <link>https://jimmyhmiller.com/machine-code-isnt-scary</link>
            <guid>44177446</guid>
            <pubDate>Wed, 04 Jun 2025 05:19:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jimmyhmiller.com/machine-code-isnt-scary">https://jimmyhmiller.com/machine-code-isnt-scary</a>, See on <a href="https://news.ycombinator.com/item?id=44177446">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<p>The first programming language I ever learned was ActionScript. Writing code for Macromedia's Flash might be the furthest away from "bare metal" as you can possibly get. As I continued learning new languages, this starting heritage stuck with me. I was mostly interested in high-level, "web languages". Low-level languages felt impenetrable. Over time, I learned a bit more about them here and there, but for some reason, this notion stuck with me. Low-level things are scary, and machine code epitomized that most directly. When I Googled things asking about writing in "straight machine code", I was met with discouraging messages rather than learning.</p>
<p>Eventually, I decided I needed to overcome this barrier if I was going to achieve my goals. In doing so, I learned something I didn't expect.</p>
<blockquote>
<p>Machine code isn't scary. If you can make sure your JSON conforms to a JSON schema, you can write machine code.</p>
</blockquote>
<h3>Which Machine Code?</h3>
<p>One problem with machine code is that there isn't simply one standard. There are many different "instruction sets" depending on the processor. Most modern PCs use x86-64 machine code, but newer Macs, Raspberry Pis, and most mobile devices use ARM. There are other architectures out there, especially as you go back in time. The goal of this article won't be to give you a deep understanding of any particular instruction set, but instead, to give you enough information about how machine code typically works so you cannot be afraid of machine code. So we will start by having our examples be in ARM 64-bit (also written as aarch64). Once we have a decent understanding of that, we will talk a bit about x86-64.</p>
<h2>Machine Code Basics</h2>
<p>To understand the basics of machine code, you need three concepts:</p>
<ol>
<li>Instructions</li>
<li>Registers</li>
<li>Memory</li>
</ol>
<p>Instructions are exactly what they sound like; they are the code that will run. Machine code instructions are just numbers. In fact, in AArch64, every instruction is a 32-bit number. Instructions encode what operation the machine should run (add, move, subtract, jump, etc.) and accept some arguments for what data to operate on. These arguments might be constants (meaning like the number 2; these constants are often called "immediates"), but they can also be registers or a memory address. For now, just think of a register as a variable and memory as a list.</p>
<h3>Arm Instructions</h3>
<p>Here is an example of the instruction <a href="https://developer.arm.com/documentation/ddi0596/2021-03/Base-Instructions/ADD--immediate---Add--immediate--">add immediate</a>.</p>
<div><table><thead><tr><th>31</th><th>30</th><th>29</th><th>28</th><th>27</th><th>26</th><th>25</th><th>24</th><th>23</th><th>22</th><th>21</th><th>20</th><th>19</th><th>18</th><th>17</th><th>16</th><th>15</th><th>14</th><th>13</th><th>12</th><th>11</th><th>10</th><th>9</th><th>8</th><th>7</th><th>6</th><th>5</th><th>4</th><th>3</th><th>2</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>sf</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>sh</td><td colspan="12">imm12</td><td colspan="5">Rn</td><td colspan="5">Rd</td></tr></tbody></table></div>
<p>Now this might look a bit confusing, but once you've seen these tables long enough, they start to be fairly straightforward. Each column in this table represents a single bit in a 32-bit number. If the value is a 0 or 1, that just means it is already filled in. If it has a label, it is a variable that needs to be filled in. <code>sf</code> tells us whether the registers we are going to use are 64-bit or 32-bit registers. <code>sh</code> stands for shift. <code>sh</code> goes in conjunction with imm12, which stands for a 12-bit immediate (constant). So if we want to add <code>42</code> to something, we would put <code>000000101010</code> in for <code>imm12</code> and set sh to 0 (meaning we aren't shifting the number). But what if we want to represent a number larger than 12 bits? Well, the add instruction doesn't let us represent all such numbers; but setting sh to 1 lets us shift our number by 12 bits. So for example we can represent <code>172032172032</code> by leaving our 42 alone and setting sh to 1. This is a clever technique for encoding larger numbers in a small space. Variables that start with R are registers, in this case, Rn is our argument to add, and Rd is our destination.</p>
<p>So the above instruction can be thought of like this:</p>
<pre><code><span>struct</span> Add <span>{</span>
 is_sixty_four_bit<span>:</span> boolean<span>,</span>
 shift<span>:</span> boolean<span>,</span>
 immediate<span>:</span> u12<span>,</span>
 n<span>:</span> Register<span>,</span>
 destination<span>:</span> Register<span>,</span>
<span>}</span>
</code></pre>
<p>Our add instruction is really just a data structure where we put the right parts in the right places.</p>
<h2>Registers</h2>
<p>Registers are small places to store values. Every instruction set will have a different number of these registers, different sizes of registers, different kinds of registers, and different naming conventions for registers. For AArch64, there are 31 general-purpose registers numbered X0 through X30 for 64-bit registers. Let's say we want to add 42 to register X0 and store the result in X1; we use this binary number.</p>
<div><table><thead><tr><th>sf</th><th colspan="8">operation</th><th>sh</th><th colspan="12">imm12</th><th colspan="5">Rn</th><th colspan="5">Rd</th></tr></thead><tbody><tr><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div>
<p>To encode our registers into our instruction, we just use their number. So register X0 would be 00000 and register X18 would be <code>10010</code>. Registers are simply places where we can store values. But by convention, registers can be used for different things. These are called calling conventions and they are how "higher" level languages like C encode function calls.</p>
<p>Writing out all these binary numbers all the time (or even converting them to hex) can often be tedious. So instead, we usually talk about instructions in a simple text format called assembly.</p>
<pre><code>add x1, x0, #0x2a 
</code></pre>
<p>In order to feel cool, people usually write numbers in assembly as hex values. This is just the number 42. You can see that assembly hides some of the details of the encoding we just made. We don't think about sf, sh, what size our number is, that a register is Rn vs Rd. Instead, the destination comes first and the arguments after. Because of this lack of detail, a single assembly instruction <code>add</code> might actually map to many different machine code instructions depending on its arguments.</p>
<h2>Memory</h2>
<p>The last piece we have to understand for machine code is memory. To understand what is going on with memory, we will look at an instruction that lets us store things in memory. This instruction is called <a href="https://developer.arm.com/documentation/ddi0602/2025-03/Base-Instructions/STR--immediate---Store-register--immediate--?lang=en#iclass_unsigned_offset">STR</a> or not written in shorthand, store.</p>
<div><table><thead><tr><th>31</th><th>30</th><th>29</th><th>28</th><th>27</th><th>26</th><th>25</th><th>24</th><th>23</th><th>22</th><th>21</th><th>20</th><th>19</th><th>18</th><th>17</th><th>16</th><th>15</th><th>14</th><th>13</th><th>12</th><th>11</th><th>10</th><th>9</th><th>8</th><th>7</th><th>6</th><th>5</th><th>4</th><th>3</th><th>2</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>1</td><td>x</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td colspan="12">imm12</td><td colspan="5">Rn</td><td colspan="5">Rt</td></tr></tbody></table></div>
<p>Using this instruction, we are going to store some value (RT) into the address (RN) + some offset (imm12). So if we think about memory as a big array, this instruction is like writing into that array. <code>array[offset] = value</code>. The x here is like our sf before, it controls whether we are using 64-bit values or not. If we want to make this concrete, let's say we have a value in X2, we have an address of memory in X1 and we want to store a value 2 bytes offset from that. We would get this structure:</p>
<div><table><thead><tr><th></th><th>x</th><th colspan="8">operation</th><th colspan="12">imm12</th><th colspan="5">Rn</th><th colspan="5">Rt</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>
<p>Since writing that all is tedious, we often just write the assembly notation. We are storing the value in x2 based on the address stored in x1 + 2.</p>
<pre><code>str x2, [x1, #0x2]
</code></pre>
<h2>X86-64</h2>
<p>X86 encoding is a bit different, but it more or less has the same parts. We are still working with instructions, registers, and memory. Some names are a bit different. Instead of the consistent 0-30 naming, we get the historical baggage of the following 64-bit registers: rax, rbx, rcx, rdx, rsi, rdi, rbp, rsp, r8-r15). However, the biggest difference is that x86 is not a fixed width instruction set. We can't simply give a nice little diagram of every instruction using 32 bits. Instead, instructions are assembled from parts. These parts are given different names; when you see an instruction encoding, it tells you how to put the parts together.</p>
<h3>REX</h3>

<p>The first part is called the REX. This is a prefix that we can use to help us with 64-bit operations. Not sure if there is an official justification for the name REX, but my understanding is that it is the "Register Extension Prefix". Unfortunately, because the REX is a prefix, it will only make sense when we see what comes later. REX is there for backward compatibility. The W in REX lets us signal that we are using 64-bit or not for certain operations. The R and B will "extend" our registers in certain operations.  In other words, it allows more registers than you used to be able to (These are those r8-r15 registers with a different naming convention than the older registers). We need these because, before 64-bit x86, we had fewer registers and our instructions only had 3 bits per register. With 16 registers, we need an extra bit. (X is for the SIB structure, which we don't cover here).</p>
<h3>ModR/M</h3>

<p>Our next part is ModR/M. ModR/M keeps up with the tradition of naming things incredibly short and confusing names. <code>mod</code> actually means Mode. <code>mod</code> tells us if <code>rm</code> is acting as a register or if it is a pointer to memory. If <code>mod == 11</code> then rm is being used as a register, otherwise, it is being used as a pointer. <code>reg</code> just is a register.</p>
<h3>OpCode</h3>
<p><code>OpCode</code> is simple, it is a number. It can be 1-3 bytes long.</p>
<h2>Putting It Together</h2>
<p>There are other parts, but we won't cover them here. With just these parts, we can build up an instruction. Let's say we want to move a 32-bit signed immediate to a 64-bit register. We can consult <a href="https://www.felixcloutier.com/x86/mov">a table of instruction encodings</a> and we will get this:</p>
<pre><code>REX.W + C7 /0 id
</code></pre>
<p>So now we can assemble our parts and make our instruction. Let's start with REX.W. This notation just means REX with W set to 1. Then there’s B8, which is just a number written in hex. <code>/0</code> is yet more shorthand for using the ModR/M but setting the reg to 0. Finally, <code>id</code> means "immediate doubleword", in other words, a constant number that is 32 bits long. So given all that, we can write our instruction. So let's move the number 42 to the rbx register.</p>
<table><thead><tr><th>Byte Index</th><th>Bits</th><th>Description</th></tr></thead><tbody><tr><td>Byte 0</td><td>55–48</td><td>01001000      REX.W = 1</td></tr><tr><td>Byte 1</td><td>47–40</td><td>11000111      Opcode C7</td></tr><tr><td>Byte 2</td><td>39–32</td><td>11000011      ModR/M: reg=000, r/m=011 (RBX)</td></tr><tr><td>Byte 3</td><td>31–24</td><td>00101010      42</td></tr><tr><td>Byte 4</td><td>23–16</td><td>00000000      the rest of 42</td></tr><tr><td>Byte 5</td><td>15–8</td><td>00000000      ...</td></tr><tr><td>Byte 6</td><td>7–0</td><td>00000000      ...</td></tr></tbody></table>
<p>Why is RBX 011? Well, because <a href="https://wiki.osdev.org/X86-64_Instruction_Encoding#Registers">the table</a> says so. Yeah, I did say that x86 is a bit weird.</p>
<h2>The Rest of It</h2>
<p>I won't pretend that this is all you need. But I will say that starting here can get you further than you think. There are some other things to learn, like various flags for things like overflow, there’s also calling conventions, which are about which registers you use when for things like function calls. We haven't really talked about the stack here, but that's memory that you write to to keep track of things. Nor have we talked about jumps, or how to encode larger immediates in ARM, but you’ve gotten the basics. It’s easier than you would think to hop on <a href="https://godbolt.org/">compiler explorer</a> and learn how things are done.</p>
<p>Learning machine code and writing things at this low level has unlocked so many things that were mental blocks for me before. Relying on libraries made by others to do these low-level things always left a gap in my knowledge that made me doubt my understanding. Even if I intellectually could explain things, actually doing them has made a huge difference for me. So if you, like me, find low-level things intimidating, I can't recommend enough starting from scratch, at the lowest possible level for your task. What I've found over and over again with low-level details, their not hard, their just poorly documented and poorly explained.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Merlin Bird ID (543 pts)]]></title>
            <link>https://merlin.allaboutbirds.org/</link>
            <guid>44176829</guid>
            <pubDate>Wed, 04 Jun 2025 02:58:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://merlin.allaboutbirds.org/">https://merlin.allaboutbirds.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="hero-wrapper">
              <h2>Identify the birds you see or hear with Merlin Bird ID</h2><p>Free global bird guide with photos, <br>sounds, maps, and more.</p><p><a href="https://itunes.apple.com/app/apple-store/id773457673?pt=401711&amp;ct=marketingwebsite&amp;mt=8"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/01/Download_App_Store_en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a> <a href="https://play.google.com/store/apps/details?id=com.labs.merlinbirdid.app&amp;pli=1"><img data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2020/02/google-play-badge-en.png" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></a></p>            </div><section id="content" aria-label="Main content" data-sticky-container="">
 
      
<div>
<div>
<h2>Identify Bird Songs and Calls</h2>



<p><strong>Sound ID</strong> listens to the birds around you&nbsp;and shows real-time suggestions for who’s singing. Compare your recording to the songs and calls in Merlin to confirm what you heard. Sound ID works completely offline, so you can identify birds you hear no matter where you&nbsp;are. </p>



<p>Available for birds in the US, Canada, Europe, with some common birds of Central and South America, and India. More species and regions coming soon.&nbsp;</p>




</div>



<div>
<figure><img fetchpriority="high" decoding="async" width="2000" height="1250" src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png" alt="" srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Merlin-Website-Screens-1-480x300.png 480w" sizes="(max-width: 2000px) 100vw, 2000px"></figure>
</div>
</div>



<hr>



<div>
<div>
<figure><img decoding="async" width="1280" height="800" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1-480x300.png 480w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Photo-ID-1.png 2000w" data-sizes="(max-width: 1280px) 100vw, 1280px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>



<div>
<h2>Identify Birds in a Photo</h2>



<p>Snap a photo of a bird, or pull one in from your camera roll, and <strong>Photo ID</strong> will offer a short list of possible matches. Photo ID works completely offline, so you can identify birds in the photos you take no matter where you&nbsp;are. </p>




</div>
</div>



<hr>



<h2>Bird ID Wizard—Step-by-step</h2>



<p>Answer three simple questions about a bird you are trying to identify and Merlin will give you a list of possible matches.&nbsp;Merlin offers quick identification help for all levels of bird watchers and outdoor enthusiasts to help you learn about the birds in any country in the world. </p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Step-by-Step-1-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>











<hr>



<div>
<div>
<h2>Save Birds to Your Life List</h2>



<p>Build a&nbsp;digital&nbsp;scrapbook of your birding memories with Save My Bird. Tap “This is my bird!” each time you identify a bird, and&nbsp;Merlin will add it to your growing life&nbsp;list.</p>




</div>



<div>
<figure><img decoding="async" width="2000" height="1250" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List.png 2000w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-720x450.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1280x800.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-768x480.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-1536x960.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Life-List-480x300.png 480w" data-sizes="(max-width: 2000px) 100vw, 2000px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>
</div>
</div>



<hr>



<h2>Explore Lists of Birds Near You</h2>



<p>Merlin is powered by&nbsp;<a href="http://ebird.org/">eBird</a>, allowing you to&nbsp;build custom lists of&nbsp;the birds you’re likely to spot wherever you are. Use the filter options to&nbsp;explore birds for different locations or time of year, or switch to show all the Offline Birds you’ve downloaded.&nbsp;Get more from the app with these <a href="https://support.ebird.org/en/support/solutions/articles/48000966225-merlin-tips-and-tricks#anchorCustomList">Merlin Tips and Tricks</a>.</p>



<figure><img decoding="async" width="1600" height="600" data-src="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png" alt="" data-srcset="https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore.png 1600w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-720x270.png 720w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1280x480.png 1280w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-768x288.png 768w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-1536x576.png 1536w, https://merlin.allaboutbirds.org/wp-content/uploads/2025/05/Explore-480x180.png 480w" data-sizes="(max-width: 1600px) 100vw, 1600px" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg=="></figure>



<hr>



<h2>See How Merlin Can Help You ID Birds</h2>



<figure><p>
<iframe title="Merlin Bird ID Demo from the Cornell Lab of Ornithology" width="1200" height="675" data-src="https://www.youtube.com/embed/xmSUOLxyatY?feature=oembed&amp;modestbranding=1&amp;showinfo=0&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen="" src="data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMSIgaGVpZ2h0PSIxIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==" data-load-mode="1"></iframe>
</p></figure>



<hr>



<h2>The Best Birding App, Powered By You</h2>



<p>Merlin features the best of community contributed photos, songs, and calls, tips from experts around the world to help you ID the birds you see, and range maps from <a href="https://birdsoftheworld.org/bow/home">Birds of the World</a>—all powered by billions of bird observations submitted to <a href="https://ebird.org/home">eBird</a>.</p>





      
        </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Binary Wordle (244 pts)]]></title>
            <link>https://wordle.chengeric.com/</link>
            <guid>44176825</guid>
            <pubDate>Wed, 04 Jun 2025 02:57:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wordle.chengeric.com/">https://wordle.chengeric.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44176825">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Use keyboard (0, 1, Enter, Backspace) or buttons to play</p></div><div><p>Sponsor: Don't like waiting on hold? Try</p><!-- --> <p><a href="https://altodial.com/?wordle"><img alt="" loading="lazy" width="16" height="16" decoding="async" data-nimg="1" src="https://wordle.chengeric.com/square.svg">altodial.com</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DiffX – Next-Generation Extensible Diff Format (343 pts)]]></title>
            <link>https://diffx.org/</link>
            <guid>44176737</guid>
            <pubDate>Wed, 04 Jun 2025 02:38:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://diffx.org/">https://diffx.org/</a>, See on <a href="https://news.ycombinator.com/item?id=44176737">Hacker News</a></p>
Couldn't get https://diffx.org/: Error: getaddrinfo ENOTFOUND diffx.org]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Has anybody built search on top of Anna's Archive? (256 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176514</link>
            <guid>44176514</guid>
            <pubDate>Wed, 04 Jun 2025 01:47:26 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176514">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44176767"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176767" href="https://news.ycombinator.com/vote?id=44176767&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Honestly I don't think it would be that costly, but it would take a pretty long time to put together. I have a (few years old) copy of Library Genesis converted to plaintext and it's around 1TB. I think libgen proper was 50-100TB at the time, so we can probably assume that AA (~1PB) would be around 10-20TB when converted to plaintext. You'd probably spend several weeks torrenting a chunk of the archive, converting everything in it to plaintext, deleting the originals, then repeating with a new chunk until you have plaintext versions of everything in the archive. Then indexing all that for full text search would take even more storage and even more time, but still perfectly doable on commodity hardware.</p><p>The main barriers are going to be reliably extracting plaintext from the myriad of formats in the archive, cleaning up the data, and selecting a decent full text search database (god help you if you pick wrong and decide you want to switch and re-index everything later).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177936"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177936" href="https://news.ycombinator.com/vote?id=44177936&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>The main barriers for me would be:</p><p>1. Why? Who would use that? What’s the problem with the other search engines? How will it be paid for?</p><p>2. Potential legal issues.</p><p>The technical barriers are at least challenging and interesting.</p><p>Providing a service with significant upfront investment needs with no product or service vision that I’ll likely to be sued for a couple of times a year, probably losing with who knows what kind of punishment… I’ll have to pass unfortunately.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178200"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178200" href="https://news.ycombinator.com/vote?id=44178200&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It would be incredible for LLMs. Searching it, using it as training data, etc. Would probably have to be done in Russia or some other country that doesn't respect international copyright though.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178241"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178241" href="https://news.ycombinator.com/vote?id=44178241&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Do you have a reason to believe this ain't already being done? I would assume that the big guys like openai are already training on basically all text in existence.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178453"><td></td></tr>
                  <tr id="44178237"><td></td></tr>
                        <tr id="44177444"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177444" href="https://news.ycombinator.com/vote?id=44177444&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I think there’s a couple ways to improve it:</p><p>1. There’s a lot of variants of the same book. We only need one for the index. Perhaps for each ISBN, select the format easiest to parse.</p><p>2. We can download, convert and index top 100K books first, launch with these, and then continue indexing and adding other books.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177679"><td></td></tr>
                <tr id="44178316"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178316" href="https://news.ycombinator.com/vote?id=44178316&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>From a good search perspective though you probably dont want 500 different versions of the same book popping up for a query</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44177996"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177996" href="https://news.ycombinator.com/vote?id=44177996&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I wonder if you could implement it with only static hosting?</p><p>We would need to split the index into a lot of smaller files that can be practically downloaded by browsers, maybe 20 MB each.
The user types in a search query, the browser hashes the query and downloads the corresponding index file which contains only results for that hashed query. Then the browser sifts quickly through that file and gives you the result.</p><p>Hosting this would be cheap, but the main barriers remain..</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177999"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177999" href="https://news.ycombinator.com/vote?id=44177999&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>It's trivial to normalise the various formats, and there were a few libraries and ML models to help parse PDFs. I was tinkering around with something like this for academic papers in Zotero, and the main issue I ran into was words spilling over to the next page, and footnotes. I totally gave up on that endeavour several years ago, but the tooling has probably matured exponentially since then.</p><p>As an example, all the academic paper hubs have been using this technology for decades.</p><p>I'd wager that <i>all</i> of the big Gen AI companies have planned to use this exact dataset, and many or them probably have already.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178133"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178133" href="https://news.ycombinator.com/vote?id=44178133&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>&gt; It's trivial to normalise the various formats,</p><p>Ha. Ha. ha ha ha.</p><p>As someone who as pretty broadly tried to normalize a pile of books and documents I have legitimate access to, <i>no it is not</i>.</p><p>You can get good results 80% of the time, usable but messy results 18% of the time, and complete garbage the remaining 2%. More effort seems to only result in marginal improvements.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178321"><td></td></tr>
                              <tr id="44178333"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178333" href="https://news.ycombinator.com/vote?id=44178333&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There is a search solution for zipped fb2 files. Not exactly what you need, but it has potential.</p><p>The project has similar story to Anna's archive. There is 0.5 TB of archived books, and the project creates index of all the books with text, title and aruthor search capabilities, gives html UI for search and reading. On weak machine it takes about 2 hours to build that index.</p><p>So if you have zipped archives of fb2, you can use the project to create web UI with search for those files. Without need of enough space to unpack all the files.</p><p>You'll have to translate some russian though to get instructions on how to set it up.</p><p><a href="https://gitlab.com/opennota/fb2index/-/blob/master/README.ru.md" rel="nofollow">https://gitlab.com/opennota/fb2index/-/blob/master/README.ru...</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178203"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178203" href="https://news.ycombinator.com/vote?id=44178203&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>There’s an android app called OpenLip. [1]</p><p>Description:</p><p>Openlib is an open source app to download and read books from shadow library (Anna’s Archive). The App Has Built In Reader to Read Books.</p><p>As Anna’s Archive doesn't have an API, the app works by sending requests to Anna’s Archive and parses the response to objects. The app extracts the mirrors from the responses, downloads the book and stores it in the application's document directory.</p><p>Note :
The app requires VPN to function properly . Without VPN the might show the captcha required page even after completing the captcha</p><p>Main Features:</p><p>Trending Books</p><p>Download And Read Books With In-Built Viewer</p><p>Supports Epub And Pdf Formats</p><p>Open Books With Your Favourite Ebooks Reader</p><p>Filter Books</p><p>Sort Books</p><p>[1]: <a href="https://f-droid.org/de/packages/com.app.openlib/" rel="nofollow">https://f-droid.org/de/packages/com.app.openlib/</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44176569"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176569" href="https://news.ycombinator.com/vote?id=44176569&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>You must mean free text search and page level return, because it already has full metadata indexing.</p><p>The thing is AA doesn't hold the texts. They're disputable IPR and even a derived work would be a legal target.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178214"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178214" href="https://news.ycombinator.com/vote?id=44178214&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Z-Library has a keyword search. Personally i didn't find it too useful, especially given Google Books exists. It's not easy to create a quality book search engine.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44177457"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177457" href="https://news.ycombinator.com/vote?id=44177457&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>As far as I know, no one has fully implemented full-text search directly over Anna's Archive. Technically it’s feasible with tools like Meilisearch, Elasticsearch, or Lucene, but the main challenges are:</p><pre><code>    Converting all documents (PDFs, EPUBs, etc.) to clean plaintext.

    Indexing at scale efficiently.

    Managing potential legal issues.
</code></pre><p>
Z-Library does something similar, but it’s smaller in scope and doesn't integrate AA’s full catalog.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177592"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44177592" href="https://news.ycombinator.com/vote?id=44177592&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I’ve done something like this before. Meilisearch will not be viable, because it indexes very slow and it takes up a lot of space.</p><p>In my experience only Tantivy can index this much data. Check out Lnx.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178248"><td></td></tr>
                        <tr id="44177894"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177894" href="https://news.ycombinator.com/vote?id=44177894&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Related question, has Anna's archive been thoroughly filtered for non-copyright-related illegal material? Pedo, terrorism, etc. I've considered downloading a few chunks of it but I'm worried of ending up with content I really don't want to be anywhere near from.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178086" href="https://news.ycombinator.com/vote?id=44178086&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>This is a really strange question to be honest you could ask this literally about any download let alone simply torrents of documents.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178012"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178012" href="https://news.ycombinator.com/vote?id=44178012&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>How might you inadvertently download illegal content while searching for legal content?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178070"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44178070" href="https://news.ycombinator.com/vote?id=44178070&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>He said he wants to download lots of it in general, not specifical. Legit question, if you end up with dark material.</p><p>I would assume pedo stuff is not really there, but the anarchist cookbook and alike likely will be.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178167"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178167" href="https://news.ycombinator.com/vote?id=44178167&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I'm still not sure the question makes much sense, if it's a general: "I want to support the project and so I want to seed a large chunk" Okay, I guess it's your due diligence to check, but there is a reporting feature built in, if something is found, report it.</p><p>Aside from that, if you're searching for specific content, the question is moot I guess.</p><p>I guess my confusion is what distinguishes this apart from any other torrent ? That is, if the submitted content is submitted at all.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178210"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178210" href="https://news.ycombinator.com/vote?id=44178210&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>I understood it as he or she wants to download large chunks of potentially interesting books for offline use, or once Anna goes down. So a broad filter. Not for seeding.</p><p>But thanks for the explanation that there is a report build in.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178137"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_44178137" href="https://news.ycombinator.com/vote?id=44178137&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Considering the anarchist cookbook is just a rebranded selection of freely-available US Army Field Manuals, ... I don't see the problem.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178201"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178201" href="https://news.ycombinator.com/vote?id=44178201&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>I don't either, but many states have laws regarding books on how to build bombs and they might get enforced more than copyright.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44178127"><td></td></tr>
                <tr id="44178218"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_44178218" href="https://news.ycombinator.com/vote?id=44178218&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Well, I won't. But does it contain just text or real pictures? That would make a big legal difference I assume.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178277"><td></td></tr>
                                    <tr id="44176864"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44176864" href="https://news.ycombinator.com/vote?id=44176864&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>AFAIK, Z-Library already does this, to some extent. Basic full-text queries do search inside the body of books and articles.</p><p>It's a bit smaller than Anna's Archive, as they do host their own collections. From some locations, it's only easy to access through Tor.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44178107"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44178107" href="https://news.ycombinator.com/vote?id=44178107&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>Has anyone explored a different angle — like mapping out the 1,000 most frequently mentioned or cited books (across HN, Substack, Twitter, etc.), then turning their raw content into clean, structured data optimized for LLMs? Imagine curating these into thematic shelves — say, “Bill Gates’ Bookshelf” or “HN Canon” — and building an indie portal where anyone can semantically search across these high-signal texts. Kind of like an AI-searchable personal library of the internet’s favorite books.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44178130"><td></td></tr>
                  <tr id="44177619"><td></td></tr>
            <tr id="44177170"><td></td></tr>
                <tr id="44178025"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44178025" href="https://news.ycombinator.com/vote?id=44178025&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div>
                  <p>It's not exactly clear, but OP is asking about indexing the content of all the documents, not the metadata (e.g. titles etc)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44177542"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44177542" href="https://news.ycombinator.com/vote?id=44177542&amp;how=up&amp;goto=item%3Fid%3D44176514"></a></center>    </td><td><br><div><p>Mebbe easier to just search Amazon or Goodreads. Like site:amazon.ca &lt;query words&gt; as someone has mentioned below.</p><p>Every book has an ISBN 10 or 13 digit ISBN number to identify them. Unless it's some self-pub/amateur-hour situation by some paranoid prepper living in a faraday-cage-protected cage in Arkansas or Florida it's likely a publication with a title, an author and an ISBN number.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44177831"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: Startup getting spammed with PayPal disputes, what should we do? (213 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44176510</link>
            <guid>44176510</guid>
            <pubDate>Wed, 04 Jun 2025 01:46:49 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44176510">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44176510">
      <td><span></span></td>      <td><center><a id="up_44176510" href="https://news.ycombinator.com/vote?id=44176510&amp;how=up&amp;goto=item%3Fid%3D44176510"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44176510">Ask HN: Startup getting spammed with PayPal disputes, what should we do?</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44176510">115 points</span> by <a href="https://news.ycombinator.com/user?id=june3739">june3739</a> <span title="2025-06-04T01:46:49 1749001609"><a href="https://news.ycombinator.com/item?id=44176510">12 hours ago</a></span> <span id="unv_44176510"></span> | <a href="https://news.ycombinator.com/hide?id=44176510&amp;goto=item%3Fid%3D44176510">hide</a> | <a href="https://hn.algolia.com/?query=Ask%20HN%3A%20Startup%20getting%20spammed%20with%20PayPal%20disputes%2C%20what%20should%20we%20do%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44176510&amp;auth=a5e7334e20b47144b4b9f3ad251dd0ab5bc8265e">favorite</a> | <a href="https://news.ycombinator.com/item?id=44176510">92&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>Longtime user posting from a new account out of an abundance of caution.</p><p>I founded an e-commerce marketplace startup. We use PayPal's Multiparty APIs (PayPal Commerce Platform) for checkout. For the 10 days, someone has been bombarding us with purchases that they later dispute. There's consistent pattern to it:</p><p>* They use an email address that has no footprint online, always from the same two domains
* They use an unverified PayPal account to pay
* They pay a low amount, not always the same, in a narrow range for a digital item
* All of the charges were disputed within a few hours</p><p>They're not doing this through our API. The purchase process requires a browser because of the way our payment form is configured. There's an amount of variation to each purchase that tells us they're automating a browser. Logs indicate that they're changing IP each time. The events come in bursts and seem to be spaced to avoid automated detection.</p><p>We added the typical mitigations to our network stack and code. A few are still slipping through. Logs indicate a high amount of bot traffic.</p><p>PayPal does not seem equipped to deal with this. Their support is always extremely slow, relies on canned responses, and to date has a very limited understanding of how their own Multiparty APIs work. Their phone support people will not talk with me, they see no indication that my PayPal account is affiliated with these purchases in any way. They want each of our sellers to contact them independently, which we know will result in disparate cases that don't tell the complete story or offer any assistance.</p><p>Has anyone encountered anything like this before? We're struggling to find the motive or intended outcome by the attacker(s). We're a small company with a niche audience, we've never had a conflict with anyone that got serious enough that we'd expect them to come after us like this.</p><p>Any thoughts and recommendations would be greatly appreciated. We feel like we are on our own here and are unsure of how to handle it.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A manager is not your best friend (182 pts)]]></title>
            <link>https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</link>
            <guid>44176425</guid>
            <pubDate>Wed, 04 Jun 2025 01:29:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html">https://staysaasy.com/management/2025/06/02/your-manager-is-not-your-best-friend.html</a>, See on <a href="https://news.ycombinator.com/item?id=44176425">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>As people become managers, it’s quite common for their team members to want to commiserate with them. This is especially true for friendly, competent, reasonable-seeming managers – people want to commiserate with <em>winners</em>. This makes commiseration extra dangerous, as it comes with a hint of flattery (“I respect your opinion and trust your discretion”).</p>

<p>But commiseration, especially with your direct reports, is organizational poison. It erodes the fabric of an organization and builds factions. It leads to feelings of superiority and creates a low-trust environment – <em>even if what you’re complaining about is made up!</em> Worst of all, it doesn’t give other teams an opportunity to improve. If I think that HR sucks, and I commiserate with my directs about it, my team is going to treat them poorly. HR will never know why, will never fix the problem, and will just think that my team are jerks (and they’ll arguably be right). Commiseration is self-fulfilling because it’s a form of victimhood: The world is conspiring against us, the only truly virtuous team.</p>

<p>Commiseration comes naturally to most people, because it happens all the time in real life. As your best friend, if you come to me saying that you were just dumped by your girlfriend, you will get unconditional sympathy beyond words. You’re the best, she didn’t deserve you, you can do so much better, everyone knows you’re the man, I have no idea why you ever spent time with her. There will be no questions, there will be no need for you to explain anything about the situation, even if all you did for the last 2 years was sit on your couch smoking weed and playing Warzone, <em>Greg</em>. Unless you did something totally crazy or illegal, my loyalty will be immediate and unconditional, because – let’s be real – none of it matters. You’re going your way, she’s going hers, and it’s over.</p>

<p>As a manager, your empathy needs to be highly conditional. Your job is to get to the truth of a matter in a respectful way, not make your team feel good. You are largely stuck with your coworkers, and you need to get stuff done together or everyone suffers. If you break up with your girlfriend you get unconditional sympathy. But if you break up with your girlfriend, and the 3 of us were trying to climb Mt. Everest together, I’m going to be a lot more measured in how I communicate and balance your relationship so that we can all survive the next few days.</p>

<p>Commiseration is generally a sensitive topic, so I’ve tried to boil down how to handle situations when a direct comes to you with grievances via some heuristics:</p>

<ul>
  <li>You don’t really want to debate your team in every situation, but your job is to essentially be a scientist and get to the bottom of what’s going on. If someone wants to commiserate about some other team, your first job is to ask a bunch of questions about what’s going on. In my experience, 90% of the time the situation is a grey area, and probably 30% of the time the person who wants to commiserate is actually in the wrong, on balance.</li>
  <li>Your role as a manager is also to be a perspective-creator. Sure, that salesperson was overly optimistic on how impactful this custom feature was for a prospect. And sure, the deal wasn’t as large and didn’t close as quickly as they said it would. But sales incentives are a law of the universe, and sales directors need to manage around them just as much as product teams do, because <em>not</em> having sales incentives is even worse. And by the way, we’re not so great at estimating development timelines either. Everyone’s blood pressure should always be lower after they’ve spoken to you.</li>
  <li>Bad therapists just let you rant. Good therapists let you vent, but they ask clarifying questions, and they sometimes push back. The phrase “is that actually true?” or “Can you explain that more?” are your friends. Good therapists validate feelings but they don’t necessarily validate <em>facts</em>. “I know you feel like you’re being a good daughter” is not the same as “you are the best daughter.” You want to be a good therapist.</li>
  <li>Remove the phrase “I don’t know why they…” from your lexicon. No matter how you end this sentence, the subtext will be clear: “I don’t know why they’re so incompetent.” Instead, it’s often better to give the most optimistic view for why another team is behaving the way that they are. It might not be <em>right</em>, but it builds empathy which is the bedrock on which productive collaboration is built.</li>
  <li>If someone is trying to get you to commiserate with them, try to speak in terms of reiterating a decision framework. Rather than “marketing doesn’t know what they’re doing,” you want to say something like “our role is to build the product and have a strong POV for marketing, and their role is to make sure that our launch generates enough pipeline. If you don’t think that’s going to happen then let’s talk to them.” The goal is to focus on objective truths rather than disparaging opinions.</li>
  <li>When it comes to commiseration, people are highly attuned to nuanced communication – especially from their boss. “Well guys, we’ve got this” plus that little head nod and eyeroll is functionally the equivalent of saying “it’s all on us, the protagonists, because everyone else is a fucking idiot <strong>again</strong>.” Those words didn’t literally leave your mouth, but you effectively said it, and as a manager that’s 100% on you. As a manager your implicit communication is just as important as your explicit communication – this is not a courtroom, this is real life, and non-verbal actions can still have consequences.</li>
  <li>One of the most common people to commiserate about is your own boss, or the company’s CEO. This can get highly toxic fast, and is rarely actually productive – cases of teams changing their CEO’s behavior through commiseration are vanishingly rare. The right way to pivot this conversation (or at least, the only way I’ve ever seen this play out positively) is to discuss how you can most effectively work with your boss. This is significantly more productive, and even if you still think they’re being dumb, at least you’re tackling that problem constructively.</li>
</ul>

<p>Of course sometimes people really are AAA grade idiots. When this happens, your communication should typically address the issue, not the other team. For most situations, it’s best to say “let me follow up,” rather than “I agree that they’re dumb.” In particularly egregious cases, you can go with “I know this is a problem and I’ll get on it” or “I hear you, I’m working on it, but I can’t give you every detail on how and don’t expect ongoing updates” – this avoids gaslighting them that everything is fine, but it also stops the vent session. When the door opens to commiseration with your team, you must slam it shut.</p>

<p>Either way, following up is the ideal next step because it commits you to respond but separates the emotion from the action. Accumulated strong emotions leave a strong impression, especially when they’re negative emotions like bitterness, so it’s best to suck the emotion out of the conversation as fast as possible. For evidence of this, light autists often make very effective managers.</p>

<p>Finally – we’re all human here, and sometimes you need to commiserate with <em>someone</em> before your head explodes. If you must commiserate, it’s almost always best if they’re a peer / near peer, and they’re not on your direct team (you don’t share a boss). This at least dodges the situation where a manager complains alongside their team, and thereby implicitly blesses their most negative views.</p>


    

    

    

    




  </div></div>]]></description>
        </item>
    </channel>
</rss>