<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 01 Feb 2024 08:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Making a PDF that's larger than Germany (387 pts)]]></title>
            <link>https://alexwlchan.net/2024/big-pdf/</link>
            <guid>39210507</guid>
            <pubDate>Wed, 31 Jan 2024 22:47:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alexwlchan.net/2024/big-pdf/">https://alexwlchan.net/2024/big-pdf/</a>, See on <a href="https://news.ycombinator.com/item?id=39210507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="main" tabindex="-1"> <article>   <p>I was browsing social media this morning, and I saw a claim I’ve seen go past a few times now – that there’s a maximum size for a PDF document:</p>  <p>Some version of this has been floating around the Internet <a href="https://twitter.com/hassmanm/status/303086542">since 2007</a>, probably earlier. This tweet is pretty emblematic of posts about this claim: it’s stated as pure fact, with no supporting evidence or explanation. We’re meant to just accept that a single PDF can only cover about half the area of Germany, and we’re not given any reason why 381&nbsp;kilometres is the magic limit.</p> <p>I started wondering: has anybody made a PDF this big? How hard would it be? Can you make a PDF that’s even bigger?</p> <p>A few years ago I did some <a href="https://codegolf.stackexchange.com/a/48938/13285">silly noodling into PostScript</a>, the precursor to PDF, and it was a lot of fun. I’ve never actually dived into the internals of PDF, and this seems like a good opportunity.</p> <p>Let’s dig in.</p> <h2 id="where-does-the-claim-come-from">Where does the claim come from?</h2> <p>These posts are often accompanied by a “well, actually” where people in the replies explain this is a limitation of a particular PDF reader app, not a limitation of PDF itself. They usually link to something like <a href="https://en.wikipedia.org/wiki/PDF#:~:text=Page%20dimensions%20are%20not%20limited%20by%20the%20format%20itself">the Wikipedia article for PDF</a>, which explains:</p> <blockquote> <p>Page dimensions are not limited by the format itself. However, Adobe Acrobat imposes a limit of 15&nbsp;million by 15&nbsp;million inches, or 225 trillion in<sup>2</sup> (145,161 km<sup>2</sup>).<a href="https://en.wikipedia.org/wiki/PDF#cite_note-pdf-ref-1.7-2"><sup>[2]</sup></a></p> </blockquote> <p>If you follow the reference link, you find the <a href="https://web.archive.org/web/20081001170454/https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf">specification for PDF&nbsp;1.7</a>, where an appendix item explains in more detail (emphasis mine):</p> <blockquote> <p>In PDF versions earlier than PDF&nbsp;1.6, the size of the default user space unit is fixed at 1/72&nbsp;inch. In Acrobat viewers earlier than version 4.0, the minimum allowed page size is 72 by 72&nbsp;units in default user space (1 by 1&nbsp;inch); the maximum is 3240 by 3240&nbsp;units (45 by 45&nbsp;inches). In Acrobat versions 5.0 and later, the minimum allowed page size is 3 by 3&nbsp;units (approximately 0.04 by 0.04&nbsp;inch); the maximum is 14,400 by 14,400&nbsp;units (200 by 200&nbsp;inches).</p> <p>Beginning with PDF&nbsp;1.6, the size of the default user space unit may be set with the UserUnit entry of the page dictionary. <strong>Acrobat 7.0 supports a maximum UserUnit value of 75,000, which gives a maximum page dimension of 15,000,000&nbsp;inches (14,400 * 75,000 * 1 ⁄ 72).</strong> The minimum UserUnit value is 1.0 (the default).</p> </blockquote> <p>15&nbsp;million inches is exactly 381&nbsp;kilometres, matching the number in the original tweet. And although this limit first appeared in PDF&nbsp;1.6, it’s “version 7” of Adobe Acrobat. This is probably where the original claim comes from.</p> <p>What if we make a PDF that exceeds these “maximum” values?</p> <h2 id="the-inner-structure-of-pdfs">The inner structure of PDFs</h2> <p>I’ve never dived into the internals of a PDF document – I’ve occasionally glimpsed some bits in a hex editor, but I’ve never really understood how they work. If I’m going to be futzing around for fun, this is a good opportunity to learn how to edit the PDF directly, rather than going through a library.</p> <p>I found <a href="https://help.callassoftware.com/a/798383-how-to-create-a-simple-pdf-file">a good article</a> which explains the internal structure of a PDF, and combined with asking ChatGPT a few questions, I was able to get enough to write some simple files by hand.</p> <p>I know that PDFs support a huge number of features, so this is probably a gross oversimplification, but this is the mental picture I created:</p> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 560" role="img" aria-labelledby="svg_pdf_layout" style="width: 300px;"> <defs> <marker id="arrowhead" markerWidth="7" markerHeight="4.9" refX="0" refY="2.45" orient="auto"> <polygon points="0 0, 7 2.45, 0 4.9"></polygon> </marker> </defs> <path d="M 230 470 L 270 470 L 270 350 L 250 350" marker-end="url(#arrowhead)"></path> <rect x="50" y="10" width="180" height="40"></rect> <text x="140" y="30">%PDF-1.6</text> <rect x="50" y="70" width="180" height="240"></rect> <text x="140" y="85" text-anchor="middle" dominant-baseline="middle" font-size="10pt">objects</text> <rect x="80" y="100" width="120" height="40"></rect> <text x="140" y="120">object 1</text> <rect x="80" y="150" width="120" height="40"></rect> <text x="140" y="170">object 2</text> <path d="M 140 195 L 140 245"></path> <rect x="80" y="250" width="120" height="40"></rect> <text x="140" y="270">object N</text> <path d="M 50 350 L 10 350 L 10 270 L 62 270" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 220 L 62 220" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 170 L 62 170" marker-end="url(#arrowhead)"></path> <path d="M 50 350 L 10 350 L 10 120 L 62 120" marker-end="url(#arrowhead)"></path> <rect x="50" y="330" width="180" height="40"></rect> <text x="140" y="350">xref</text> <rect x="50" y="390" width="180" height="40"></rect> <text x="140" y="410">trailer</text> <rect x="50" y="450" width="180" height="40"></rect> <text x="140" y="470">startxref</text> <rect x="50" y="510" width="180" height="40"></rect> <text x="140" y="530">%%EOF</text> <title id="svg_pdf_layout">tbc</title></svg> <p>The start and end of a PDF file are always the same: a version number (<code>%PDF-1.6</code>) and an end-of-file marker (<code>%%EOF</code>).</p> <p>After the version number comes a long list of objects. There are lots of types of objects, for all the various things you can find in a PDF, including the pages, the text, and the graphics.</p> <p>After that list comes the <code>xref</code> or cross-reference table, which is a lookup table for the objects. It points to all the objects in the file: it tells you that object 1 is 10&nbsp;bytes after the start, object 2 is after 20&nbsp;bytes, object 3 is after 30&nbsp;bytes, and so on. By looking at this table, a PDF reading app knows how many objects there are in the file, and where to find them.</p> <p>The <code>trailer</code> contains some metadata about the overall document, like the number of pages and whether it’s encrypted.</p> <p>Finally, the <code>startxref</code> value is a pointer to the start of the <code>xref</code> table. This is where a PDF reading app starts: it works from the end of the file until it finds the <code>startxref</code> value, then it can go and read the <code>xref</code> table and learn about all the objects.</p> <p>With this knowledge, I was able to write my first PDF by hand. If you save this code into a file named <code>myexample.pdf</code>, it should open and show a page with a red square in a PDF reading app:</p><pre><code><span>%PDF-1.6

% The first object.  The start of every object is marked by:
%
%     &lt;object number&gt; &lt;generation number&gt; obj
%
% (The generation number is used for versioning, and is usually 0.)
%
% This is object 1, so it starts as `1 0 obj`.  The second object will
% start with `2 0 obj`, then `3 0 obj`, and so on.  The end of each object
% is marked by `endobj`.
%
% This is a "stream" object that draws a shape.  First I specify the
% length of the stream (54&nbsp;bytes).  Then I select a colour as an
% RGB value (`1 0 0 RG` = red), then I set a line width (`5 w`) and
% finally I give it a series of coordinates for drawing the square:
%
%     (100, 100) ----&gt; (200, 100)
%                          |
%     [s = start]          |
%         ^                |
%         |                |
%         |                v
%     (100, 200) &lt;---- (200, 200)
%</span>
1 0 obj
&lt;&lt;
	/Length 54
&gt;&gt;
stream
1 0 0 RG
5 w
100 100 m
200 100 l
200 200 l
100 200 l
s
endstream
endobj

<span>% The second object.
%
% This is a "Page" object that defines a single page.  It contains a
% single object: object 1, the red square.  This is the line `1 0 R`.
%
% The "R" means "Reference", and `1 0 R` is saying "look at object number 1
% with generation number 0" -- and object 1 is the red square.
%
% It also points to a "Pages" object that contains the information about
% all the pages in the PDF -- this is the reference `3 0 R`.</span>
2 0 obj
&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	/MediaBox [0 0 300 300]
	/Contents 1 0 R
&gt;&gt;
endobj

<span>% The third object.
%
% This is a "Pages" object that contains information about the different
% pages.  The `2 0 R` is reference to the "Page" object, defined above.</span>
3 0 obj
&lt;&lt;
	/Type /Pages
	/Kids [2 0 R ]
	/Count 1
&gt;&gt;
endobj

<span>% The fourth object.
%
% This is a "Catalog" object that provides the main structure of the PDF.
% It points to a "Pages" object that contains information about the
% different pages -- this is the reference `3 0 R`.</span>
4 0 obj
&lt;&lt;
	/Type /Catalog
	/Pages 3 0 R
&gt;&gt;
endobj

<span>% The xref table.  This is a lookup table for all the objects.
%
% I'm not entirely sure what the first entry is for, but it seems to be
% important.  The remaining entries correspond to the objects I created.</span>
xref
0 4
0000000000 65535 f
0000000851 00000 n
0000001396 00000 n
0000001655 00000 n
0000001934 00000 n

<span>% The trailer.  This contains some metadata about the PDF.  Here there
% are two entries, which tell us that:
%
%   - There are 4 entries in the `xref` table.
%   - The root of the document is object 4 (the "Catalog" object)
%</span>
trailer
&lt;&lt;
	/Size 4
	/Root 4 0 R
&gt;&gt;

<span>% The startxref marker tells us that we can find the xref table 2196&nbsp;bytes
% after the start of the file.</span>
startxref
2196

<span>% The end-of-file marker.
%%EOF</span></code></pre><p>I played with this file for a while, just doing simple things like adding extra shapes, changing how the shapes appeared, and putting different shapes on different pages. I tried for a while to get text working, but that was a bit beyond me.</p> <p>It quickly became apparent why nobody writes PDFs by hand – it got very fiddly to redo all the lookup tables! But I’m glad I did it; manipulating all the PDF objects and their references really helped me feel like I understand the basic model of PDFs. I opened some “real” PDFs created by other apps, and they have many more objects and types of object – but now I could at least follow some of what’s going on.</p> <p>With this newfound ability to edit PDFs by hand, how can I create monstrously big ones?</p> <h2 id="changing-the-page-size-mediabox-and-userunit">Changing the page size: /MediaBox and /UserUnit</h2> <p>Within a PDF, the size of each page is set on the individual “Page” objects – this allows different pages to be different sizes. We’ve already seen this once:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 300 300]</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>Here, the <code>MediaBox</code> is setting the width and height of the page – in this case, a square of 300 × 300&nbsp;units. The default unit size is 1/72&nbsp;inch, so the page is 300 × 72 = 4.17&nbsp;inches. And indeed, if I open this PDF in Adobe Acrobat, that’s what it reports:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in.dark_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.avif 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.avif 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.webp 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.webp 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png 724w, https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_2x.png 1448w" sizes="(max-width: 724px) 100vw, 724px" type="image/png"> <img src="https://alexwlchan.net/images/2024/adobe-acrobat-pdf-4in_1x.png" alt="Screenshot of Acrobat’s ‘Document Properties’ panel, showing the page size of 4.17 x 4.17 in." width="724"> </picture> <p>By changing the <code>MediaBox</code> value, we can make the page bigger. For example, if we change the value to <code>600 600</code>, Acrobat says it’s now <code>8.33 x 8.33 in</code>. Nice!</p> <p>We can increase it all the way to <code>14400 14400</code>, the max allowed by Acrobat, and then it says the page is now <code>200.00 x 200.00in</code>. (You <a href="https://alexwlchan.net/images/2024/acrobat-error.png">get a warning</a> if you try to push past that limit.)</p> <p>But 200&nbsp;inches is far short of 381&nbsp;kilometres – and that’s because we’re using the default unit of 1/72&nbsp;inch. We can increase the unit size by adding a <code>/UserUnit</code> value. For exaple, setting the value to 2 will double the page in both dimensions:</p><pre><code>&lt;&lt;
	/Type /Page
	/Parent 3 0 R
	<span>/MediaBox [0 0 14400 14400]</span>
	<span>/UserUnit 2</span>
	/Contents 1 0 R
&gt;&gt;</code></pre><p>And now Acrobat reports the size of the page as <code>400.00 x 400.00 in</code>.</p> <p>If we crank it all the way up to the maximum of <code>UserUnit 75000</code>, Acrobat now reports the size of our page as <code>15,000,000,000.00 x 15,000,000,000.00 in</code> – 381 km along both sides, matching the original claim. If you’re curious, you can <a href="https://alexwlchan.net/files/2024/biggest.pdf">download the PDF</a>.</p> <p>If you try to create a page with a larger size, either by increasing the <code>MediaBox</code> or <code>UserUnit</code> values, Acrobat just ignores it. It keeps saying that the size of a page is 15 billion inches, even if the page metadata says it’s higher. (And if you increase the <code>UserUnit</code> past <code>75000</code>, this happens silently – there’s no warning or error to suggest the size of the page is being capped.)</p> <p>This probably isn’t an issue – I don’t think the <code>UserUnit</code> value is widely used in practice. I found <a href="https://stackoverflow.com/a/59927201/1558022">one Stack Overflow answer</a> saying as such, and I couldn’t find any examples of it online. The builtin macOS Preview.app doesn’t even support it – it completely ignores the value, and treats all PDFs as if the unit size is 1/72&nbsp;inch.</p> <p>But unlike Acrobat, the Preview app doesn’t have an upper limit on what we can put in <code>MediaBox</code>. It’s perfectly happy for me to write a width which is a 1 followed by twelve 0s:</p> <picture> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide.dark_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide.dark_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png" media="(prefers-color-scheme: dark)"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.avif 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.avif 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/avif"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.webp 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.webp 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/webp"> <source srcset="https://alexwlchan.net/images/2024/preview-megawide_1x.png 366w, https://alexwlchan.net/images/2024/preview-megawide_2x.png 732w" sizes="(max-width: 366px) 100vw, 366px" type="image/png"> <img src="https://alexwlchan.net/images/2024/preview-megawide_1x.png" alt="Screenshot of Preview’s Document inspector, showing the page size of 352777777777.78 x 10.59 cm." width="366"> </picture> <p>If you’re curious, that width is approximately the distance between the Earth and the Moon. I’d have to get my ruler to check, but I’m pretty sure that’s larger than Germany.</p> <p>I could keep going. And I did. Eventually I ended up with a PDF that Preview claimed is larger than the entire universe – approximately 37 trillion light years square. Admittedly it’s mostly empty space, but so is the universe. If you’d like to play with that PDF, you can <a href="https://alexwlchan.net/files/2024/universe.pdf">get it here</a>.</p> <p>Please don’t try to print it.</p> </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[MobileDiffusion: Rapid text-to-image generation on-device (139 pts)]]></title>
            <link>https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</link>
            <guid>39210458</guid>
            <pubDate>Wed, 31 Jan 2024 22:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html">https://blog.research.google/2024/01/mobilediffusion-rapid-text-to-image.html</a>, See on <a href="https://news.ycombinator.com/item?id=39210458">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-5966553114967673984">
<p><span>Posted by Yang Zhao, Senior Software Engineer, and Tingbo Hou, Senior Staff Software Engineer, Core ML</span>

</p><p>
Text-to-image <a href="https://arxiv.org/abs/2006.11239">diffusion models</a> have shown exceptional capabilities in generating high-quality images from text prompts. However, leading models feature billions of parameters and are consequently expensive to run, requiring powerful desktops or servers (e.g., <a href="https://stability.ai/news/stable-diffusion-public-release">Stable Diffusion</a>, <a href="https://openai.com/research/dall-e">DALL·E</a>, and <a href="https://imagen.research.google/">Imagen</a>). While recent advancements in inference solutions on <a href="https://blog.research.google/2023/06/speed-is-all-you-need-on-device.html">Android</a> via MediaPipe and <a href="https://github.com/apple/ml-stable-diffusion">iOS</a> via Core ML have been made in the past year, rapid (sub-second) text-to-image generation on mobile devices has remained out of reach.
</p> <p>
To that end, in “<a href="https://arxiv.org/abs/2311.16567">MobileDiffusion: Subsecond Text-to-Image Generation on Mobile Devices</a>”, we introduce a novel approach with the potential for rapid text-to-image generation on-device. MobileDiffusion is an efficient latent diffusion model specifically designed for mobile devices. We also adopt <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> to achieve one-step sampling during inference, which fine-tunes a pre-trained diffusion model while leveraging a GAN to model the denoising step. We have tested MobileDiffusion on iOS and Android premium devices, and it can run in half a second to generate a 512x512 high-quality image. Its comparably small model size of just 520M parameters makes it uniquely suited for mobile deployment.
</p>




<p>Rapid text-to-image generation on-device.</p>





<h2>Background</h2>


<p>
The relative inefficiency of text-to-image diffusion models arises from two primary challenges. First, the inherent design of diffusion models requires <a href="https://blog.research.google/2023/06/on-device-diffusion-plugins-for.html">iterative denoising</a> to generate images, necessitating multiple evaluations of the model. Second, the complexity of the network architecture in text-to-image diffusion models involves a substantial number of parameters, regularly reaching into the billions and resulting in computationally expensive evaluations. As a result, despite the potential benefits of deploying generative models on mobile devices, such as enhancing user experience and addressing emerging privacy concerns, it remains relatively unexplored within the current literature.
</p>
<p>
The optimization of inference efficiency in text-to-image diffusion models has been an active research area. Previous studies predominantly concentrate on addressing the first challenge, seeking to reduce the number of function evaluations (NFEs). Leveraging advanced numerical solvers (e.g., <a href="https://arxiv.org/abs/2206.00927">DPM</a>) or distillation techniques (e.g., <a href="https://arxiv.org/abs/2202.00512">progressive distillation</a>, <a href="https://arxiv.org/abs/2303.01469">consistency distillation</a>), the number of necessary sampling steps have significantly reduced from several hundreds to single digits. Some recent techniques, like <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN</a> and <a href="https://arxiv.org/abs/2311.17042#:~:text=We%20introduce%20Adversarial%20Diffusion%20Distillation,while%20maintaining%20high%20image%20quality.">Adversarial Diffusion Distillation</a>, even reduce to a single necessary step. 
</p>
<p>
However, on mobile devices, even a small number of evaluation steps can be slow due to the complexity of model architecture. Thus far, the architectural efficiency of text-to-image diffusion models has received comparatively less attention. A handful of earlier works briefly touches upon this matter, involving the removal of redundant neural network blocks (e.g., <a href="https://snap-research.github.io/SnapFusion/">SnapFusion</a>). However, these efforts lack a comprehensive analysis of each component within the model architecture, thereby falling short of providing a holistic guide for designing highly efficient architectures.
</p>





<h2>MobileDiffusion</h2>


<p>
Effectively overcoming the challenges imposed by the limited computational power of mobile devices requires an in-depth and holistic exploration of the model's architectural efficiency. In pursuit of this objective, our research undertakes a detailed examination of each constituent and computational operation within Stable Diffusion’s <a href="https://arxiv.org/abs/2112.10752">UNet architecture</a>. We present a comprehensive guide for crafting highly efficient text-to-image diffusion models culminating in the MobileDiffusion.
</p>
<p>
The design of MobileDiffusion follows that of <a href="https://arxiv.org/abs/2112.10752">latent diffusion models</a>. It contains three components: a text encoder, a diffusion UNet, and an image decoder. For the text encoder, we use <a href="https://arxiv.org/abs/2103.00020">CLIP-ViT/L14</a>, which is a small model (125M parameters) suitable for mobile. We then turn our focus to the diffusion UNet and image decoder. 
</p>




<h3>Diffusion UNet</h3>


<p>
As illustrated in the figure below, diffusion UNets commonly interleave transformer blocks and convolution blocks. We conduct a comprehensive investigation of these two fundamental building blocks. Throughout the study, we control the training pipeline (e.g., data, optimizer) to study the effects of different architectures.
</p>
<p>
In classic text-to-image diffusion models, a transformer block consists of a self-attention layer (SA) for modeling long-range dependencies among visual features, a cross-attention layer (CA) to capture interactions between text conditioning and visual features, and a feed-forward layer (FF) to post-process the output of attention layers. These transformer blocks hold a pivotal role in text-to-image diffusion models, serving as the primary components responsible for text comprehension. However, they also pose a significant efficiency challenge, given the computational expense of the attention operation, which is quadratic to the sequence length. We follow the idea of <a href="https://arxiv.org/abs/2301.11093">UViT</a> architecture, which places more transformer blocks at the bottleneck of the UNet. This design choice is motivated by the fact that the attention computation is less resource-intensive at the bottleneck due to its lower dimensionality. 
</p>





<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s915/image4.png"><img data-original-height="249" data-original-width="915" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgsshK53k6noqIbabpGMBzYIBCdviXisDoBsD3Houk-lXzN8pZQcusKYBvjWwcwA1Aq5DnWyk01YM9B2RyRZx6HcGgTP-LrW-tnwFwByzlBACN3WggyPYM0Mpyr2OVGVLFhx1uN48aR1g9P4o0joN2STli9VpA_tFMdQ-ikRXVrNpawzB793-unSENR-PIV/s16000/image4.png"></a></td></tr><tr><td>Our UNet architecture incorporates more transformers in the middle, and skips self-attention (SA) layers at higher resolutions.</td></tr></tbody></table>




<p>
Convolution blocks, in particular <a href="https://arxiv.org/abs/1512.03385">ResNet</a> blocks, are deployed at each level of the UNet. While these blocks are instrumental for feature extraction and information flow, the associated computational costs, especially at high-resolution levels, can be substantial. One proven approach in this context is <a href="https://arxiv.org/abs/1704.04861">separable convolution</a>. We observed that replacing regular convolution layers with lightweight separable convolution layers in the deeper segments of the UNet yields similar performance.
</p>
<p>
In the figure below, we compare the UNets of several diffusion models. Our MobileDiffusion exhibits superior efficiency in terms of <a href="https://arxiv.org/pdf/2110.12894.pdf">FLOPs</a> (floating-point operations) and number of parameters. 
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s1200/image3.png"><img data-original-height="742" data-original-width="1200" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXYleITSssbZnLffeh3BzG3tX2qNQNeB__xc-ySks0SPnXsMb2kTLZ0PcE2KWJ4I9FX_QMP32pXd06IuV1kJJSlgp7CuV6dqkXJsiFqo_6xqWXZ1-65p_EPU9gk7G9B4-L2TaKGiD5cahwg428CTmV1dcuQQ_vBTVmP8543IJigIF0qHo8_JaB8h5EuVvl/s16000/image3.png"></a></td></tr><tr><td>Comparison of some diffusion UNets.</td></tr></tbody></table>





<h3>Image decoder</h3>


<p>
In addition to the UNet, we also optimized the image decoder. We trained a <a href="https://arxiv.org/abs/2012.03715">variational autoencoder</a> (VAE) to encode an <a href="https://en.wikipedia.org/wiki/RGB_color_model">RGB</a> image to an 8-channel latent variable, with 8× smaller spatial size of the image. A latent variable can be decoded to an image and gets 8× larger in size.  To further enhance efficiency, we design a lightweight decoder architecture by pruning the original’s width and depth. The resulting lightweight decoder leads to a significant performance boost, with nearly 50% latency improvement and better quality. For more details, please refer to our <a href="https://arxiv.org/abs/2311.16567">paper</a>.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s1124/image6.png"><img data-original-height="789" data-original-width="1124" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjT2Nmo7GjGdN0_2dqevJB52RogqnWFDVmFsrusHHxnVf9YQYsdbVkAQvBI3h9SzKZ0TqOQOmnxaZ6z2kdix12tei5oMpD17SY1LoBWqxD1EHgV0ygTb9TV0IFZQtv4dAix378lb8WGv5GGPQIuyStX3gWqn0pjTTXbpIlA0VzYSeiGpkO5bsHhZfjbkR07/s16000/image6.png"></a></td></tr><tr><td>VAE reconstruction. Our VAE decoders have better visual quality than SD (Stable Diffusion).</td></tr></tbody></table>



<br>

<table>
  <tbody><tr>
   <td><b>Decoder</b>
   </td>
   <td><b>&nbsp;&nbsp;#Params (M)&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;PSNR↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;SSIM↑&nbsp;&nbsp;</b>
   </td>
   <td><b>&nbsp;&nbsp;LPIPS↓&nbsp;&nbsp;</b>
   </td>
  </tr>
  <tr>
   <td><b>SD</b>
   </td>
   <td>49.5
   </td>
   <td>26.7
   </td>
   <td>0.76
   </td>
   <td>0.037
   </td>
  </tr>
  <tr>
   <td><b>Ours</b>
   </td>
   <td>39.3
   </td>
   <td>30.0
   </td>
   <td>0.83
   </td>
   <td>0.032
   </td>
  </tr>
  <tr>
   <td><b>Ours-Lite&nbsp;&nbsp;&nbsp;&nbsp;</b>
   </td>
   <td>9.8
   </td>
   <td>30.2
   </td>
   <td>0.84
   </td>
   <td>0.032
   </td>
  </tr>
</tbody></table>
<br>





<h3>One-step sampling</h3>


<p>
In addition to optimizing the model architecture, we adopt a <a href="https://arxiv.org/abs/2311.09257">DiffusionGAN hybrid</a> to achieve one-step sampling. Training DiffusionGAN hybrid models for text-to-image generation encounters several intricacies. Notably, the discriminator, a classifier distinguishing real data and generated data, must make judgments based on both texture and semantics. Moreover, the cost of training text-to-image models can be extremely high, particularly in the case of GAN-based models, where the discriminator introduces additional parameters. Purely GAN-based text-to-image models (e.g., <a href="https://arxiv.org/abs/2301.09515">StyleGAN-T</a>, <a href="https://arxiv.org/abs/2303.05511">GigaGAN</a>) confront similar complexities, resulting in highly intricate and expensive training.
</p>
<p>
To overcome these challenges, we use a pre-trained diffusion UNet to initialize the generator and discriminator. This design enables seamless initialization with the pre-trained diffusion model. We postulate that the internal features within the diffusion model contain rich information of the intricate interplay between textual and visual data. This initialization strategy significantly streamlines the training.
</p>
<p>
The figure below illustrates the training procedure. After initialization, a noisy image is sent to the generator for one-step diffusion. The result is evaluated against ground truth with a reconstruction loss, similar to diffusion model training. We then add noise to the output and send it to the discriminator, whose result is evaluated with a GAN loss, effectively adopting the GAN to model a denoising step. By using pre-trained weights to initialize the generator and the discriminator, the training becomes a fine-tuning process, which converges in less than 10K iterations.  
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s960/image7.jpg"><img data-original-height="576" data-original-width="960" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjnK7SE2-cHSlP-PDmkl_xfjp3sP-kB41r6OvC8Wg6miXnYwdES0INwN19BHWQ_uyXtcBT-872U5J6jLY8yXVtA_W96qkRRPh6Pjvw0n-ZJvjJK91kYTh7H1n4nzy8z1TyrQZlZoZrQUDTo5Qm-6a_2vIVye3aqm7o32qOOXiWXwxDzw_J6cQsOrJ-UILKw/s16000/image7.jpg"></a></td></tr><tr><td>Illustration of DiffusionGAN fine-tuning.</td></tr></tbody></table>




<h2>Results</h2>


<p>
Below we show example images generated by our MobileDiffusion with DiffusionGAN one-step sampling. With such a compact model (520M parameters in total), MobileDiffusion can generate high-quality diverse images for various domains.
</p>




<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s1728/image1.png"><img data-original-height="1296" data-original-width="1728" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiyDLq1NW7Qvy4_oEqg1pHAMzeBfuei3VadIKZRNkv6ZHnzewVWQU5x76e0bm-QqWVr-_q1W4axBJeyqyCbdRFoUFBYxRxDj3qo7I4-Du6TS2Bez_-mmXzYoHLJk7y5fiKl9PPkHNk_dsvy7ezuAFavW4sYIeYTxhAPAH35FYP5YOceS8NfJey0gpvHUwza/s16000/image1.png"></a></td></tr><tr><td>Images generated by our MobileDiffusion</td></tr></tbody></table>



<p>
We measured the performance of our MobileDiffusion on both iOS and Android devices, using different runtime optimizers. The latency numbers are reported below. We see that MobileDiffusion is very efficient and can run within half a second to generate a 512x512 image. This lightning speed potentially enables many interesting use cases on mobile devices.
</p>



<table><tbody><tr><td><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s1184/image8.png" imageanchor="1"><img data-original-height="742" data-original-width="1184" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiFkcI7kibwRFhpxTsVmUkAzK38MCeBoTR6fOWyhjnqwPm7x8TwrVn_O0OipsXCbgS4qTtcbtm41Fxi7U_IJjpeuZadWO7cBKkcdrXHniAJgQP4Qk-wOBfnhtwNPxDbzxtM0uxVba3BjwzLa3Lw13-03FoRQbWwf_25KR9GLLkSqIFpnU5aE-6hnomY5IuK/s16000/image8.png"></a></td></tr><tr><td>Latency measurements (<b>s</b>) on mobile devices.</td></tr></tbody></table>




<h2>Conclusion</h2>


<p>
With superior efficiency in terms of latency and size, MobileDiffusion has the potential to be a very friendly option for mobile deployments given its capability to enable a rapid image generation experience while typing text prompts. And we will ensure any application of this technology will be in-line with Google’s <a href="https://ai.google/responsibility/responsible-ai-practices/">responsible AI practices</a>.
</p>




<h2>Acknowledgments</h2>


<p>
<em>We like to thank our collaborators and contributors that helped bring MobileDiffusion to on-device: Zhisheng Xiao, Yanwu Xu, Jiuqiang Tang, Haolin Jia, Lutz Justen, Daniel Fenner, Ronald Wotzlaw, Jianing Wei, Raman Sarokin, Juhyun Lee, Andrei Kulik, Chuo-Ling Chang, and Matthias Grundmann.</em>
</p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Visual Analysis of Binary Files (107 pts)]]></title>
            <link>http://binvis.io/#/</link>
            <guid>39210436</guid>
            <pubDate>Wed, 31 Jan 2024 22:39:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://binvis.io/#/">http://binvis.io/#/</a>, See on <a href="https://news.ycombinator.com/item?id=39210436">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[I don't want anything your AI generates (154 pts)]]></title>
            <link>https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</link>
            <guid>39210126</guid>
            <pubDate>Wed, 31 Jan 2024 22:11:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/">https://coryd.dev/posts/2024/i-dont-want-anything-your-ai-generates/</a>, See on <a href="https://news.ycombinator.com/item?id=39210126">Hacker News</a></p>
<div id="readability-page-1" class="page"><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><div><p>I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).</p><p>I can't trust the answers it provides or the text it generates. It's not a replacement for search, it simply makes search worse.</p><p>The images it generates are, at best, a polished regression to the mean. If you want custom art, pay an artist.</p><p>I want to talk to a person, not a chatbot. The chatbot wastes time while you wait for a person that can actually help.</p><p>I don't want music recommendations from something that can't appreciate or understand music. Human recommendations will always be better.</p><p>I don't want AI mediating social interactions that it cannot and does not understand (though it may appear to). If I'm weary of too much volume on any social platform or in any news feed, I'll cut back on what I'm following.</p><p>If you're having AI attend a meeting for you, it probably wasn't that important. If you're having AI write your email, it probably wasn't that important. If it's screening job candidates for you, you're missing quality candidates.</p><p>I don't like the idea of it being trained on anything I've written or created<sup><a href="#fn1" id="fnref1">[1]</a></sup>.</p><p>I'll concede that Copilot is better than traditional autocomplete, but that feels like a pretty low bar<sup><a href="#fn2" id="fnref2">[2]</a></sup>.</p><p>These tools will improve<sup><a href="#fn3" id="fnref3">[3]</a></sup>. The cost to the environment will increase, it will help companies deskill jobs and muddy public discourse. The "benefits" it provides to end users are, at best, dubious — though everyone responsible for creating it will most certainly enrich themselves.</p><p>This all portends a future I can't imagine wanting or even being interested in.</p><hr><section><ol><li id="fn1"><p>Although consuming that may well be to its detriment. Nonetheless, my <code>robots.txt</code> file reflects this stance. <a href="#fnref1">↩︎</a></p></li><li id="fn2"><p>My first experience with Copilot chat was it getting an answer wrong, apologizing when that was pointed out and then repeating the same or similar, but still incorrect, answer. <a href="#fnref2">↩︎</a></p></li><li id="fn3"><p>I don't think this is a good thing. <a href="#fnref3">↩︎</a></p></li></ol></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DeepSeek Coder: Let the Code Write Itself (171 pts)]]></title>
            <link>https://deepseekcoder.github.io/</link>
            <guid>39209814</guid>
            <pubDate>Wed, 31 Jan 2024 21:43:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepseekcoder.github.io/">https://deepseekcoder.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39209814">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p>
            DeepSeek Coder comprises a series of code language models trained from scratch on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.
            We provide various sizes of the code model, ranging from 1B to 33B versions.
            Each model is pre-trained on repo-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, resulting in foundational models (DeepSeek-Coder-Base).
            We further fine-tune the base model with 2B tokens of instruction data to get instruction-tuned models, namedly DeepSeek-Coder-Instruct.
          </p>
          <!-- <p>
            For coding capabilities, DeepSeek-Coder-Base achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.
            And Compared to GPT35-turbo, DeepSeek-Coder-Instruct demonstrates superior performance in human evaluation while maintaining comparable performance in MBPP.
          </p> -->
          <ul>
            <li>
              Pretrained on <b>2 Trillion</b> tokens over more than 80 programming languages.
            </li>
            <li>
              Various model sizes (<b>1.3B</b>, <b>5.7B</b>, <b>6.7B</b> and <b>33B</b>) to support different requirements.
            </li>
            <li>
              A window size of <b>16K window </b>size, supporting <b>project-level</b> code completion and infilling.
            </li>
            <li>
              <b>State-of-the-Art</b> performance among open code models. 
            </li>
            <li>
              <b>Open source and free for research and commercial use</b>.
            </li>
          </ul>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Generating Fantasy Maps (2016) (110 pts)]]></title>
            <link>http://mewo2.com/notes/terrain/</link>
            <guid>39209687</guid>
            <pubDate>Wed, 31 Jan 2024 21:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://mewo2.com/notes/terrain/">http://mewo2.com/notes/terrain/</a>, See on <a href="https://news.ycombinator.com/item?id=39209687">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="container">
    
    





<p><img src="http://mewo2.com/deserts/map.png" alt="An example map from the generator" width="60%"></p>
<p>These are some notes on how I generate the maps for my Twitter bot
<a href="https://twitter.com/unchartedatlas">@unchartedatlas</a>, which is based on a generator I originally produced
during <a href="https://github.com/dariusk/NaNoGenMo-2015/issues/156">NaNoGenMo 2015</a>. There's JavaScript code for the generator
on Github <a href="https://github.com/mewo2/terrain">here</a>, and the original messy Python generator code can be
seen <a href="https://github.com/mewo2/deserts">here</a>.</p>
<p>You may also be interested in this <a href="http://mewo2.com/notes/naming-language/">companion piece</a>, which describes
the placename generation.</p>
<h3>Inspiration</h3>
<p>I wanted to make maps that look like something you'd find at the back of one of
the cheap paperback fantasy novels of my youth. I always had a fascination with
these imagined worlds, which were often much more interesting than whatever
luke-warm sub-Tolkien tale they were attached to.</p>
<p>At the same time, I wanted to play with terrain generation with a physical
basis. There are loads of articles on the internet which describe terrain
generation, and they almost all use some variation on a fractal noise approach,
either directly (by adding layers of noise functions), or indirectly (e.g.
through midpoint displacement). These methods produce lots of fine detail, but
the large-scale structure always looks a bit off. Features are attached in
random ways, with no thought to the processes which form landscapes. I wanted
to try something a little bit different.</p>
<p>There are a few different stages to the generator. First we build up a
height-map of the terrain, and do things like routing water flow over the
surface. Then we can render the 'physical' portion of the map. Finally we can
place cities and 'regions' on the map, and place their labels.</p>
<h3>Grids</h3>
<p>To represent the heightmap, first we need a grid of points. Although it can be
simpler to work on a regular square grid, I much prefer to work on an irregular
set of points for something like this. With a regular grid, it's very easy to
run into weird artifacts, and you often have to do a lot of postprocessing to
hide the effects of the grid. If you use an irregular grid, then there are a
few things which are more complicated, but the structure of the grid helps to
give the map a rough, organic feel, and you never have to worry about nasty
linear artifacts in the finished product.</p>
<p>The approach I use is the same as in <a href="http://www-cs-students.stanford.edu/~amitp/game-programming/polygon-map-generation/">this article</a>, which is one of the
better references out there on how to do non-fractal terrain generation. I
won't go into too much detail here because that article explains it very
clearly, with lots of diagrams.</p>
<p>I start by selecting points at random within the map. These points tend to be a
bit clumpy and uneven, so I use <a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd relaxation</a> to improve the point
set. For speed, I only use one iteration of this process, but you can repeat it
as many times as you like. There are rapidly diminishing returns after a few
iterations though.</p>
<p>All of the calculations are actually carried out on the 'dual points' of the
original point set, which correspond to the corners of the Voronoi polygons.
This has the advantage that the number of neighbours per node is fixed at
three, which helps in some parts of the code.</p>
<p>Note: this shows 256 (2<sup>8</sup>) points, to make viewing easier, but the real generator uses 16,384 (2<sup>14</sup>) points. I have a programmer's superstitions about always using powers of 2, which are more pleasing to the spirit of the machine.</p>

<h3>Rough outlines</h3>
<p>One of the difficulties of creating landscapes in a realistic way is that real
landscapes aren't created all at once. Instead, they evolve from earlier
landscapes, which in turn evolved from even earlier landscapes, and so on back
for billions of years. There's no good way to simulate this process in a
reasonable amount of time, so we need to cheat slightly.</p>
<p>Rather than an infinite regress of older landscapes, I start with a simple
'proto-landscape', built with geometric primitives. This lets me control the
broad outlines of the terrain, while leaving the details for the more physical
processes to fill in later.</p>
<p>Some useful primitives which we can add together:</p>
<ul>
<li>Constant slope - if you want to pretend this is physically motivated, think of it as tectonic uplift on one side of the map</li>
<li>Cone shapes - these can be islands or mountains, or if inverted, lakes or seas</li>
<li>Rounded blobs - these make better hills, and can be scattered all around to make a noisy surface</li>
</ul>
<p>We also have a few operations which are handy:</p>
<ul>
<li>Normalize - rescale the heights to lie in the range 0-1</li>
<li>Round - normalize, then take the square root of the height value, to round off the tops of hills</li>
<li>Relax - replace each height value with the average of its neighbours, to smooth the surface</li>
<li>Set sea level - translate the heightmap up or down so that a particular quantile is at zero</li>
</ul>
<p>The particular sequence of primitives and operations used can be varied to produce different kinds of landscape, such as coastlines, islands and mountain ranges.</p>
<p>Note: the black line indicates the zero contour, which we treat as 'sea level'. Also, this map uses 4,096 (2<sup>12</sup>) points, for speed.</p>

<h3>Erosion</h3>
<p>The results of this process can be a little bit on the blobby side, which means
they rarely look good on their own. We want to scuff them up a bit, so they
look more like real landscapes. We do this by applying an erosion operation.</p>
<p>In most of the world, by far the largest influence on the shape of landforms is
fluvial (water-based) erosion. Water flows downhill, carrying sediment along
with it, carving out valleys and river basins. This is a massively complex
phenomenon, and modelling it correctly is a very active research area, but we
can get a long way by sketching a simple version of the process.</p>
<p>We need to start by tracing the routes that water would take over the grid. For
each grid point, we say that water flows to its lowest neighbour, and so on
down until we reach the edge of the map. This gives a map of water flow.</p>
<p>There's an obvious problem when we reach gridpoints which are lower than all of
their neighbours. Do we route the water back uphill? This will probably lead to
cycles in the water system, which are trouble. Instead, we want to fill in
these gaps (often called sinks or depressions), so that the water always runs
downhill all the way to the edge.</p>
<p>It's easy to see how to fill in a single gridpoint, but as the depression gets
bigger, and possibly links up with other depressions, the number of possible
cases multiplies enormously. Luckily, there's an algorithm for filling
depressions, called the Planchon-Darboux algorithm.</p>
<div>

<h4>Aside: the Planchon-Darboux algorithm</h4>

<p>The algorithm works by finding the lowest surface with the following two properties:</p>

<ul>
<li>The surface is everywhere at least as high as the input surface</li>
<li>Every non-edge point has a neighbour which is lower than it</li>
</ul>

<p>To calculate this, we start with an infinitely high surface everywhere
except on the edge, where we use the original heights. Then, on each iteration,
we find points which have a neighbour which is lower than them, and set their
height to their original height, or the height of their lowest neighbour (plus
a small amount), whichever is higher. We halt when we can go a full iteration
without changing any point.</p>

<p>There are various ways of speeding up this algorithm, mostly by tweaking the
order in which points are visited. For more details, and a proof of
correctness, you can read <a href="http://horizon.documentation.ird.fr/exl-doc/pleins_textes/pleins_textes_7/sous_copyright/010031925.pdf">the original paper</a>.</p>

</div>

<p>With the water routing calculated, we can work out how much water is flowing
through each point. I assume that rainfall is constant across the whole map,
and iterate through the points in descending order, passing the rainfall, plus
the accumulated water flux, from each point to its 'downhill point'. This gives
a map of water flux, which usually converges into a nice branching river
structure, with lots of small streams feeding a larger central channel.</p>
<p>To calculate erosion, I combine the water flux with the slope at each point, as
calculated based on the triangle of its neighbours. The exact formula I use is
the product of the slope with the square root of the water flux. This isn't
necessarily very physical, but it does give nice-looking results. I also add a
small term which is proportional to the slope squared. This prevents deep
gorges from forming, which might be physically realistic, but don't look good
in the graphical style I've chosen.</p>
<p>I find it's very important to cap the erosion rate, otherwise strange things
can happen. A little goes a very long way with this. Also, erosion always
lowers the surface, so it usually helps to drop the sea level afterwards to match.</p>
<p>A final tweak to the heightmap is to smooth out the coastlines slightly. The
erosion tends to produce quite rough terrain, which becomes tiny islands when
cut off by sea level. A few of these can look good, but too many just looks
messy. I repeatedly apply a filter where points which are below sea level, but
a majority of whose neighbours are above sea level, get pulled up, and vice
versa for points which are above sea level and have undersea neighbours. A
couple of repeats of this produces a much cleaner coastline.</p>


<h3>Rendering terrain</h3>
<p>Now comes the question of drawing the map (at least the physical portion). The
easy part is the coastline - we've been doing this already. It's just a matter
of drawing line segments where the heightmap crosses zero. There's not a lot
extra to do about this.</p>
<p>The next component is the rivers. We don't want to display the entire drainage
network, because that would cover the whole map. Instead, we only show the
drainage from points with above a certain threshold of water flux. By
connecting these points to their downstream neighbours, we can trace out the
river paths.</p>
<p>One problem with this approach is that the rivers tend to zigzag from grid
point to grid point, rather than following a smooth path. To solve this, I
relax the points in the middle of the path towards their upstream and
downstream neighbours (keeping the top and bottom fixed, so that intersections
work properly). This smooths things out beautifully.</p>
<p>The final part of this is the shading on the sides of hills, which helps
indicate the topography. It is a central principle of cartography that we tend
to interpret maps as though viewing the terrain from the bottom of the map,
looking towards the top. So we want to draw strokes which go up and right if
the terrain slopes upwards from left to right, and down and right if the
terrain slopes downwards. Similarly, the strokes on the 'near' side of hills
should be longer than those on the 'far' side.</p>
<p>For each grid point, I calculate the slope, and ignore the point if it is less
than a random threshold. For points which pass this test, I draw a short stroke
with slope proportional to the horizontal component of the heightmap slope,
with a small modifier for the vertical component. If the stroke would be too
steep, I split it into several shorter strokes, at the maximum slope, drawn at
random around the point.</p>


<h3>Cities, borders</h3>
<p>Now that we have the 'physical' portion of the map sorted, we can move to
looking at the 'political'. We want to place cities and towns on the map in
feasible-looking locations. At the same time, we want the cities to be spread
out enough that we can put labels on them without worrying too much about
overlap.</p>
<p>To place a city, I generate a score for each point, which is a combination of three things:</p>
<ul>
<li>Water flux - we want cities to be preferentially located on rivers, so high water flux gets a bonus</li>
<li>Distance from other cities - we want cities to be spread out, so penalize locations which are too close to an existing city</li>
<li>Distance from the edge of the map - the other two criteria alone tend to push cities to the map edge, which isn't ideal, so penalize locations too close to the edge</li>
</ul>
<p>Every time I add a city, I choose the point with the highest score, then
recalculate the scores for all points. I make a distinction between cities,
which have a 'region' associated with them, and towns, which don't. The cities
are placed first, but otherwise there's no distinction in the code.</p>
<p>The next step is to mark out the regions. We want the borders between regions
to seem fairly reasonable, following natural borders like rivers and mountain
ranges. The way I approach this is to expand regions outwards from each city,
so that each region consists of the points which are 'closest' to its city,
according to a particular distance measure. This distance measure is calculated
by adding up the cost of the route, based on these criteria:</p>
<ul>
<li>Horizontal distance</li>
<li>Slope - uphill is much cheaper than downhill, so regions expand until they hit the top of ridges, then stop</li>
<li>Water flux - crossing a river is expensive</li>
<li>Shorelines - there is a large penalty for going from land to water (or vice versa), and a smaller penalty for travelling by water</li>
</ul>
<p>Finally, the borders are drawn, using the same smoothing technique as the rivers.</p>


<h3>Placing labels</h3>
<p>At this point we can start naming things, using the process described in <a href="http://mewo2.com/notes/naming-language/">these
notes</a>. I generate names for cities, towns and regions, using a
consistent language for the whole map.</p>
<p>The very last part of the process is to place the labels, avoiding overlaps,
obscured cities, labels going off the edge of the map, etc. This sounds easy,
but it really really isn't. Ideally, some sort of intelligent layout algorithm
would place all the labels, rearranging as necessary. Instead, I have a few
hundred lines of spaghetti code which seems to get the right answer most of the
time, but which is packed full of magic numbers.</p>
<p>In rough outline, what happens is this: the labels are placed in the order 1.
cities, 2. towns, 3. regions. For cities and towns, there are four possible
slots for the label, above, below, and to each side of the marker. Each label
is placed, attempting to avoid the following overlaps (in rough order of
importance):</p>
<ul>
<li>City markers</li>
<li>Other city labels</li>
<li>The map edge</li>
<li>Borders</li>
<li>Coastlines</li>
<li>Rivers</li>
</ul>
<p>Obviously it's not usually possible to avoid all of these, so the least bad
solution is chosen.</p>
<p>For regions, there's a bit more freedom in where to place the label, but the
labels are also bigger, and the map is more cluttered at this point. The
scoring system rates positions based on proximity to the center of the region,
as well as being over land, and penalties for all the overlaps mentioned
before.</p>
<p>I wanted to make a nice interactive example for this, but trying to separate out the label placement code made me feel physically unwell. Sorry about that.</p>

<p>So that's the algorithm, at least in rough outline. The actual code running on
the bot has a few other little bits and pieces, which honestly don't do much,
but removing them is more trouble than it's worth when the code is working as
it stands. The JavaScript code behind this page is available <a href="https://github.com/mewo2/terrain">on
GitHub</a>, and if you're really really brave you can look at the original
<a href="https://github.com/mewo2/deserts">Python code</a> which was written while I was figuring all this out.</p>
<p>There's obviously lots that could be done to improve this. Erosion is just one
process, and the next obvious thing to add would be fluvial deposition, which
would allow for flood plains, river deltas, etc to form. If you wanted more
realistic mountains, then glacial processes would be worth looking at. Volcanic
stuff could also be fun.</p>
<p>On the graphical side, it could be fun to try to sketch more interesting
textures on the map, such as forests or fields. Or you could fill the oceans
with sea monsters and lost ships. If you're really brave, you could even look
at labelling more features, like mountain ranges and rivers.</p>
<p>As always, if you do any of these things, or make anything else interesting
with this code or these ideas, please get in touch and let me know.</p>
<p>If you would like to buy a print of one of these maps, please visit <a href="https://shop.mewo2.com/">my web shop</a>.</p>

    <hr><p>
    
    If you've enjoyed this piece, please consider <a href="https://www.patreon.com/mewo2">contributing on Patreon</a> so I can do more things like this.
    
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Comcast reluctantly agrees to stop its misleading "10G Network" claims (167 pts)]]></title>
            <link>https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</link>
            <guid>39209469</guid>
            <pubDate>Wed, 31 Jan 2024 21:10:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/">https://arstechnica.com/tech-policy/2024/01/comcast-to-stop-calling-entire-network-10g-as-name-is-ruled-misleading/</a>, See on <a href="https://news.ycombinator.com/item?id=39209469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      10G or not 10G    —
</h4>
            
            <h2 itemprop="description">Comcast said it will drop "Xfinity 10G Network" brand name after losing appeal. </h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2024/01/comcast-10g-800x528.jpg" alt="A Comcast router/modem gateway.">
      <figcaption><p>Comcast</p></figcaption>  </figure>

  




<!-- cache hit 181:single/related:5927e4533beb5ac868f3f9b9c0ce65dd --><!-- empty -->
<p>Comcast has reluctantly agreed to discontinue its "Xfinity 10G Network" brand name after losing an appeal of a ruling that found the marketing term was misleading. It will keep using the term 10G in other ways, however.</p>
<p>Verizon and T-Mobile both challenged Comcast's advertising of 10G, a term used by cable companies since it was unveiled in January 2019 by industry lobby group NCTA-The Internet &amp; Television Association. We <a href="https://arstechnica.com/information-technology/2019/01/envious-of-5g-hype-cable-cos-unveil-potentially-confusing-10g-trademark/">wrote in 2019</a> that the cable industry's 10G marketing was likely to confuse consumers and seemed to be a way of countering 5G hype generated by wireless companies.</p>
<p>10G doesn't refer to the 10th generation of a technology. It is a reference to potential 10Gbps broadband connections, which would be much faster than the actual speeds on standard cable networks today.</p>
<p>The challenges lodged against Comcast marketing were filed with the advertising industry's self-regulatory system run by BBB National Programs. BBB's National Advertising Division (NAD) <a href="https://arstechnica.com/tech-policy/2023/10/comcast-should-stop-advertising-slower-speeds-as-10g-industry-group-says/">ruled against Comcast</a> in October 2023, but Comcast appealed to the National Advertising Review Board (NARB).</p>
<p>The NARB <a href="https://bbbprograms.org/media-center/dd/comcast-10g-xfinity">announced its ruling</a> today, agreeing with the NAD that "Comcast should discontinue use of the term 10G, both when used in the name of the service itself ('Xfinity 10G Network') as well as when used to describe the Xfinity network. The use of 10G in a manner that is not false or misleading and is consistent with the panel decision is not precluded by the panel recommendations."</p>
<h2>“Comcast will discontinue brand name”</h2>
<p>Comcast agreed to make the change in an advertiser's statement that it provided to the NARB. "Although Comcast strongly disagrees with NARB's analysis and approach, Comcast will discontinue use of the brand name 'Xfinity 10G Network' and will not use the term '10G' in a manner that misleadingly describes the Xfinity network itself," Comcast said.</p>
<p>Comcast said it disagrees with "the recommendation to discontinue the brand name" because the company "makes available 10Gbps of Internet speed to 98 percent of its subscribers upon request." But those 10Gbps speeds aren't available in Comcast's typical service plans and require a fiber-to-the-home connection instead of a standard cable installation.</p>                                            
                                                        
<p>The Comcast "Gigabit Pro" fiber connection that provides 10Gbps speeds costs $299.95 a month plus a $19.95 modem lease fee. It also requires a $500 installation charge and a $500 activation charge.</p>
<p>Comcast said it may still use 10G in ways that are less likely to confuse consumers. "Consistent with the panel's recommendation... Comcast reserves the right to use the term '10G' or 'Xfinity 10G' in a manner that does not misleadingly describe the Xfinity network itself," the company said.</p>
<p>When contacted by Ars, a Comcast spokesperson said, "We disagree with the decision but are pleased that we have confirmed our continued use of 10G in advertising."</p>
<h2>Comcast claims “not supported”</h2>
<p>The NARB said the "recent availability of 10G speeds through [the Gigabit Pro] service tier does not support the superior speed claim (or a 10Gbps claim) for the Xfinity network as a whole." As the NARB noted, there is an "absence" of data showing how many Comcast customers actually use that service.</p>
<p>The NARB also said that 10G is misleading because of the implied comparison to 5G wireless networks.&nbsp;"The NARB panel concluded that 10G expressly communicates at a minimum that users of the Xfinity network will experience significantly faster speeds than are available on 5G networks," the announcement of the ruling said. "This express claim is not supported because the record does not contain any data comparing speeds experienced by Xfinity network users with speeds experienced by subscribers to 5G networks."</p>
<p>As the NAD has <a href="https://bbbprograms.org/media-center/dd/comcast-10g">previously stated</a>, 10G is more of an "aspirational" term rather than something that's offered over today's cable networks. Over the past five years, the NCTA has been using the term 10G to describe just about any improvement to cable networks, regardless of the actual speeds.</p>
<p>The NCTA coincidentally issued a <a href="https://www.ncta.com/whats-new/10g-five-years-on-the-future-is-even-closer">press release</a> yesterday hailing the fifth anniversary of its first 10G announcement. "Five years on, the future is even closer... Here in 2024, the promise of 10G is becoming more and more of a reality," the NCTA said.</p>
<p>The announcement listed some examples of multi-gigabit (but not 10-gigabit) cable speeds, some of which were only achieved in lab testing or demos. NCTA claimed that "10G can change lives" and that the "10G platform will facilitate the next great technological advancements in the coming decades, ensuring fast, reliable, and safe networks continue to power the American economy."</p>
<p>For all of you cable broadband users, just remember to ignore "10G" in cable-company marketing and check the actual speeds you're paying for.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Moved My Blog from IPFS to a Server (226 pts)]]></title>
            <link>https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</link>
            <guid>39208673</guid>
            <pubDate>Wed, 31 Jan 2024 20:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html">https://neimanslab.org/2024-01-31/why-i-moved-my-blog-ipfs-to-server.html</a>, See on <a href="https://news.ycombinator.com/item?id=39208673">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
  <p><span>Written by</span>
    
        Neiman
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2024-01-31 00:00:33 +0100">January 31, 2024</time>
    
  </p>

  
  

  <p>It’s safe to say I was a pioneer of IPFS + ENS websites. When I set up my first ENS+IPFS website in March 2019 there were no more than 15 others. Between 2019 to 2022 I co-built an IPFS+ENS browser extension (Almonit), an IPFS+ENS search engine (Esteroids), and of course, my personal blog was available only in IPFS+ENS.</p>

<p>But today I moved my blog back to a server, and I’d like to discuss why.</p>

<p>What got me excited about peer-to-peer websites like IPFS is that, theoretically, the more visitors a website has, the more robust, censorship-resistant, and scalable it is. Again, theoretically.</p>

<p>Do you know how popular torrent files seem to live forever? I wanted the same but for websites. I imagined a website that is hard to ddos (Robust), difficult to block (Censorship-resistant), and the more readers it has, the faster it is to use it since some readers help to spread the content (Scalable).</p>

<p>I imagined a website with a big “Pin Me” button (pinning in IPFS is like seeding in BitTorrent). If a reader presses the button they wil help serve the website.</p>

<p>In practice this didn’t work out really, and for several reasons.</p>

<ol>
  <li>
    <p>IPFS users mostly don’t run their own nodes or software. Instead, they use gateways. It’s an educated guess I’m making based on what I see in the community, and based on the fact that it’s quite an inconvenience to run your own IPFS node. But even if you do run your own node, the fact you access a website doesn’t mean you pin it. Not at all.</p>

    <p>This is a huge difference from BitTorrent where the only way to get content is to run your own software, and when you download something you also share it, by default.</p>

    <p>Hence, most readers will not help share the website, but even for the ones who will there are still extra complications:</p>
  </li>
  <li>
    <p>Websites are dynamic objects. Their content is being updated all the time. If you just pin the content of the current version of a website, that’s not much help.</p>

    <p>What most IPFS websites do is use a name system that points to the latest version of its content. It’s usually either IPNS, the internal name system of IPFS, or ENS, Ethereum Name System. But IPFS doesn’t include yet an easy command to always pin the latest content of IPNS, and if someone uses ENS, it means that whoever pins it also needs to listen to Ethereum blockchain events, a huge extra challenge on its own to do without a centralized service.</p>
  </li>
  <li>
    <p>To make things worse, it’s actually quite hard to get IPFS content to be available in the browser in a reliable way!</p>

    <p>For example, I wanted my IPFS blog to be available to all major gateways, all IPFS nodes, Brave browser (which supports IPFS natively), and js-libp2p &amp; helia (the js libraries of IPFS). I didn’t find a reliable way to achieve that on my own.</p>

    <p><strong>Long rant:</strong> I pinned content from my own server and played forever with settings and definitions, but couldn’t get the content to be available everywhere. Worse was with Helia, where I just couldn’t manage to get my content accessible from Helia within the browser without connecting directly to my own node. But then what’s p2p network about it?</p>

    <p>I found out that there’s a service, <a href="https://cid.contact/">cid.contact</a>, called a “Content Routing” service. It’s written in the “about section” of cid.contact it’s related to Filecoin, but for some reason, it holds routing data for IPFS, which as far as I know is a different network. The address cid.contact is hard-coded into Helia’s code in the version I used at least, and it was clear that if a content is indexed by cid.contact, then it’s reachable almost everywhere, but if it’s not indexed - it’s not reachable always.</p>

    <p>I couldn’t figure out how to index my content in cid.contact. Honestly, I’m not sure I wanted to. Because what’s the point? it seems to just add a dependency on a centralized service. I could try to run my own indexer and define it in my website, but again, centralization. What’s the economic model for these indexers? Which actors do we expect to run them in the long term?</p>

    <p>The text in cid.contact says that at the current size of IPFS, it’s unreasonable to expect the DHT to handle routing efficiently on its own. This kind of makes sense, but what’s the alternative to that, that doesn’t break up the technology pros?</p>
  </li>
</ol>

<p>By now I got tired of the constant struggle for my IPFS blog to function well. At least for a short while, I want a simple, classic working solution. The blog you’re reading now is built with Jekyll and is hosted on my own 10$ server.</p>

<p>don’t get me wrong, I’m still an IPFS fanboy. It’s a great project managed very well. It just doesn’t fit a personal blog needs yet.</p>

<p>That said, It’s difficult to follow the constant development and innovation of IPFS or Filecoin without this becoming a day job. Did I miss some trivial solution or a recent innovation? If yes, let me know. There are no comments here yet, but I am available via old-style email (neiman@hackerspace.pl), or in Mastodon (@neiman@mastodon.social).</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: Some blind hackers are bridging IRC to LMMs running locally (140 pts)]]></title>
            <link>https://2mb.codes/~cmb/ollama-bot/</link>
            <guid>39208451</guid>
            <pubDate>Wed, 31 Jan 2024 19:50:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://2mb.codes/~cmb/ollama-bot/">https://2mb.codes/~cmb/ollama-bot/</a>, See on <a href="https://news.ycombinator.com/item?id=39208451">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>This is a rudimentary IRC bot that communicates with a local instance
of <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<h3 id="installation"><a href="#installation" rel="nofollow noopener">#</a>Installation</h3>
<p>You'll need to have <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a> installed in order to
actually use the bot.  Instructions for doing that are out of scope for
this document.  Then, build the bot from source:</p>
<pre><code>git clone 'https://git.2mb.codes/~cmb/ollama-bot
cd ollama-bot
go build
</code></pre>
<p>Put the ollama-bot binary on your <code>$PATH</code>.
We run this on a Void Linux host.  Runit services for both the bot and
ollama itself are included in the ollama-bot repository, along with some
instructions for getting it running under runit.  If you use something else,
you're on your own, but of course, patches are welcome.</p>
<h3 id="configuration"><a href="#configuration" rel="nofollow noopener">#</a>Configuration</h3>
<p>The program searches for its configuration file in one of several places,
in the following order, and uses the first file it finds:</p>
<pre><code>~/.config/ollama-bot/ollama-bot.toml
/usr/local/etc/ollama-bot/ollama-bot.toml
/etc/ollama-bot/ollama-bot.toml
</code></pre>
<p>The configuration file should contain one or more network definitions.
I.E., it is an array of tables using toml notation.  Look at the file
<code>config.toml.sample</code> for a sample configuration.</p>
<h3 id="usage"><a href="#usage" rel="nofollow noopener">#</a>Usage</h3>
<p>Once the bot is connected to one or more IRC networks, you can converse
with it, either by highlighting it in a channel, or by sending it a private
message.  For the most part, you just send it free-form text, and it will
respond with free-form text.  However, it also accepts various commands,
preceded by an exclamation mark <code>!</code>.  Send <code>!help</code> for a list of commands.
Use of the commands is best demonstrated by an example chat transcript.</p>
<h4 id="chat-transcript"><a href="#chat-transcript" rel="nofollow noopener">#</a>Chat Transcript</h4>
<pre><code>&lt;chris&gt; ai-bot: !help
&lt;ai-bot&gt; 
&lt;ai-bot&gt; This bot supports the following commands:
&lt;ai-bot&gt; !describe URL [model] -- Describe image at URL, optionally choosing model
&lt;ai-bot&gt; !switch-text-model -- Start a conversation using a different text model
&lt;ai-bot&gt; !list-text-models -- List models usable for text conversation
&lt;ai-bot&gt; !list-image-models -- List models usable for image description
&lt;ai-bot&gt; !help -- This command.
&lt;chris&gt; ai-bot: !list-image-models
&lt;ai-bot&gt; This bot supports the following image models: llava, llava:13b, bakllava
&lt;chris&gt; ai-bot: !list-text-models
&lt;ai-bot&gt; This bot supports the following text models: mistral:instruct, llama2, llama2-uncensored
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg llava:13b
&lt;ai-bot&gt;  The image features a bearded man with a bird perched on his shoulder. The bird is small, likely a parrot or a similar type of pet bird. The man appears to be in a relaxed setting, possibly sitting down while enjoying the company with his feathered companion.
&lt;chris&gt; ai-bot: What kind of beard does he have?
&lt;ai-bot&gt;  While I cannot precisely describe the man's beard style without more specific information, it is safe to say that he has some form of facial hair on his face. This could range from a full beard to a goatee or even just a five o'clock shadow.
# Use the default model, bakllava.  This resets conversation state.
&lt;chris&gt; ai-bot: !describe https://the-brannons.com/chris-and-bird.jpg
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The image features a bearded man with a bird perched on his shoulder. The man has an odd facial expression, almost as if he is looking into the camera. The bird appears to be a parrot and is sitting close to the man's ear.
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The scene takes place in a room furnished with a couch and a chair. The couch is positioned in the background, while the chair occupies a smaller portion of the frame on the left side.
&lt;chris&gt; ai-bot: Tell me about his beard?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; The bearded man in the image has an odd facial expression, making his appearance quite distinctive. His beard seems to be well-groomed and likely represents a fashionable or stylish choice in his personal grooming.
&lt;chris&gt; ai-bot: What do you mean by odd facial expression?
&lt;ai-bot&gt; 
&lt;ai-bot&gt; By odd facial expression, I refer to the man's facial features looking unnatural, exaggerated, or distorted. This may include an awkward smile, squinting, or an unusual or contorted countenance. The phrase "what the heck" implies that the person's appearance seems out of the ordinary or unexpected, which adds a humorous and quirky touch to the image.
# Switch back to text chat, selecting a model and resetting conversation state:
&lt;chris&gt; ai-bot: !switch-to-text-model llama2-uncensored
&lt;ai-bot&gt; Text model switched.
&lt;chris&gt; ai-bot: Somebody on the Internet told me that I can believe everything I read on the Internet?  Were they full of shit?
&lt;ai-bot&gt; Unfortunately, no. This statement is not true. There are many websites and sources on the internet that provide incorrect or misleading information. It's important to be careful when using any online source for information and to verify it by checking multiple sources or contacting a reliable expert.
&lt;chris&gt; ai-bot: But Wikipedia is always reliable!
&lt;ai-bot&gt; Wikipedia is an excellent resource, but it's not infallible. Like any encyclopedia, it can be edited and updated by anyone with access to the website. While most of the information on Wikipedia is accurate, there are some inaccuracies and errors that may exist due to human error or deliberate vandalism. It's always a good idea to cross-reference information from different sources before relying solely on any one source.
&lt;chris&gt; ai-bot: Could you tell me about some known examples of errors in Wikipedia?  Otherwise, I'm just gonna assume you're full of it.
&lt;ai-bot&gt; Sure! One well-known example of an error in Wikipedia is the article on Abraham Lincoln. The entry stated that Lincoln was the first president to receive a patent for his invention - which was actually issued after he left office. Another example is the article on the Apollo 11 mission, which claimed that Neil Armstrong's quote "One small step" was misquoted from an interview with President John F. Kennedy. However, NASA confirmed that the quote used in Wikipedia w
&lt;ai-bot&gt; as
&lt;ai-bot&gt; accurate and had been recorded during the mission.
</code></pre>
<h3 id="why"><a href="#why" rel="nofollow noopener">#</a>Why?</h3>
<p>Because most of the people in the group behind this are hard-core free software
people to some degree or other.  We want to see access to AI liberated and
democratized, I.E., free as in freedom.  And almost as importantly, it sounded
like fun when we were bored on a Friday afternoon.</p>
<h3 id="legal"><a href="#legal" rel="nofollow noopener">#</a>Legal</h3>
<p>Licensed as GPL-2 to match the license of <code>irc-go</code>.</p>
<h3 id="thanks"><a href="#thanks" rel="nofollow noopener">#</a>Thanks</h3>
<p>The group behind ollama-bot consists of Kirk, Tyler, Mike, and Chris, plus the peanut gallery on IRC.</p>
<p>We use the <a href="https://git.zx2c4.com/irc-go" rel="nofollow noopener">irc-go</a> package from Jason Donenfeld.
This seems to have been an excellent choice.</p>
<p>This project would not be possible without <a href="https://ollama.ai/" rel="nofollow noopener">ollama</a>.</p>
<p>Code hosting is graciously provided by the fine people at <a href="https://2mb.solutions/" rel="nofollow noopener">2MB Solutions</a>.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral CEO confirms 'leak' of new open source AI model nearing GPT4 performance (337 pts)]]></title>
            <link>https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</link>
            <guid>39208213</guid>
            <pubDate>Wed, 31 Jan 2024 19:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/">https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/</a>, See on <a href="https://news.ycombinator.com/item?id=39208213">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
		
		<section>
			
			<p><time title="2024-01-31T18:44:56+00:00" datetime="2024-01-31T18:44:56+00:00">January 31, 2024 10:44 AM</time>
			</p>
			
		</section>
		<div>
					<p><img width="750" height="420" src="https://venturebeat.com/wp-content/uploads/2024/01/cfr0z3n_overhead_view_of_the_eiffel_tower_rising_from_a_landsca_d374695a-bca7-4db7-abcd-d30ef5fd0e04.png?fit=750%2C420&amp;strip=all" alt="Overhead view of Eiffel tower in a Paris made of circuit boards."></p><p><span>Credit: VentureBeat made with Midjourney V6</span></p>		</div><!-- .article-media-header -->
	</div><div id="primary" role="main">

			<article id="post-2932274">
				<div>
					<p>The past few days have been a wild ride for the growing open source AI community — even by its fast-moving and freewheeling standards.</p>



<p>Here’s the quick chronology: on or about January 28, a user with the handle “Miqu Dev” <a href="https://huggingface.co/miqudev/miqu-1-70b" target="_blank" rel="noreferrer noopener">posted a set of files on HuggingFace</a>, the leading open source AI model and code sharing platform, that together comprised a seemingly new open source large language model (LLM) labeled “miqu-1-70b.”</p>



<p>The HuggingFace entry, which is still up at the time of this article’s posting, noted that new LLM’s “Prompt format,” how users interact with it, was the same as <a href="https://mistral.ai/">Mistral</a>, the <a href="https://www.ft.com/content/cf939ea4-d96c-4908-896a-48a74381f251" target="_blank" rel="noreferrer noopener">well-funded open source Parisian AI company</a> behind <a href="https://venturebeat.com/ai/mistral-shocks-ai-community-as-latest-open-source-model-eclipses-gpt-3-5-performance/" target="_blank" rel="noreferrer noopener">Mixtral 8x7b</a>, viewed by many to be the top performing open source LLM presently available, a fine-tuned and retrained version of Meta’s Llama 2. </p>



<h2 id="h-posted-on-4chan">Posted on 4chan</h2>



<p>The same day, an anonymous user on 4chan (possibly “Miqu Dev”) posted a <a href="https://boards.4chan.org/g/thread/98696032#p98697258" target="_blank" rel="noreferrer noopener">link to the miqu-1-70b files on 4chan</a>, the notoriously longstanding haven of online memes and toxicity, where users began to notice it.</p>



<p>Some took to X, Elon Musk’s social network formerly known as Twitter, to share the discovery of the model and what appeared to be its exceptionally high performance at common LLM tasks (measured by tests known as benchmarks), approaching the previous leader, OpenAI’s GPT-4 on the <a href="https://eqbench.com/" target="_blank" rel="noreferrer noopener">EQ-Bench</a>. </p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Whatever Miqu is, it has some sort of special sauce. It gets an 83.5 on EQ-Bench (evaluated locally), surpassing *every other LLM in the world except GPT-4*. EQ-Bench has a 0.97 correlation w/ MMLU, and a 0.94 correlation w/ Arena Elo. It *beats* Mistral Medium – at Q4_K_M. I… <a href="https://t.co/0gOOPjxjPD">pic.twitter.com/0gOOPjxjPD</a></p>— N8 Programs (@N8Programs) <a href="https://twitter.com/N8Programs/status/1752441060133892503?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<figure><div>
<blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Might be late but I am now 100% convinced that Miqu is the same model that's accessible as Mistral-Medium on Perplexity Labs. It was plausible that it knows standard puzzles, but there ain't no way in Hell a pranker has tuned it to identically phrase the responses in Russian too. <a href="https://t.co/zZMcpspXch">pic.twitter.com/zZMcpspXch</a></p>— Teortaxes▶️ (@teortaxesTex) <a href="https://twitter.com/teortaxesTex/status/1752427812466593975?ref_src=twsrc%5Etfw">January 30, 2024</a></blockquote>
</div></figure>



<h2 id="h-mistral-quantized">Mistral quantized?</h2>



<p>Machine learning (ML) researchers took notice on <a href="https://www.linkedin.com/posts/maxime-labonne_new-open-source-llm-competes-with-gpt-activity-7158414579671199744-MFfA/" target="_blank" rel="noreferrer noopener">LinkedIn</a>, as well. </p>



<div><p><em>“Does ‘miqu’ stand for MIstral QUantized? We don’t know for sure, but this quickly became one of, if not the best open-source LLM,”</em> wrote Maxime Labonne, an ML scientist at JP Morgan &amp; Chase, one of the world’s largest banking and financial companies.<em> “Thanks to @152334H, we also now have a good unquantized version of miqu here: <a href="https://lnkd.in/g8XzhGSM">https://lnkd.in/g8XzhGSM</a></em></p><p><em>The investigation continues. Meanwhile, we might see fine-tuned versions of miqu outperforming GPT-4 pretty soon.</em>“</p></div>



<p><a href="https://huggingface.co/docs/optimum/concept_guides/quantization" target="_blank" rel="noreferrer noopener">Quantization</a> in ML refers to a technique used to make it possible to run certain AI models on less powerful computers and chips by replacing specific long numeric sequences in a model’s architecture with shorter ones. </p>



<p>Users speculated “Miqu” might be a new Mistral model being covertly “leaked” by the company itself into the world — especially since Mistral is known for <a href="https://venturebeat.com/ai/mistral-ai-bucks-release-trend-by-dropping-torrent-link-to-new-open-source-llm/" target="_blank" rel="noreferrer noopener">dropping new models and updates without fanfare</a> through esoteric and technical means — or perhaps an employee or customer gone rouge.</p>



<h2 id="h-confirmation-from-the-top">Confirmation from the top</h2>



<p>Well, today it appears we finally have confirmation of the latter of those possibilities: Mistral co-founder and CEO Arthur Mensch took to X to clarify: <em>“An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly…</em></p>



<p><em>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got access to our entire cluster — the pretraining finished on the day of Mistral 7B release. We’ve made good progress since — stay tuned!<a href="https://twitter.com/arthurmensch/status/1752737462663684344/history"></a>“</em></p>



<figure><div>
<blockquote data-width="500" data-dnt="true"><div lang="en" dir="ltr"><p>An over-enthusiastic employee of one of our early access customers leaked a quantised (and watermarked) version of an old model we trained and distributed quite openly.</p><p>To quickly start working with a few selected customers, we retrained this model from Llama 2 the minute we got…</p></div>— Arthur Mensch (@arthurmensch) <a href="https://twitter.com/arthurmensch/status/1752737462663684344?ref_src=twsrc%5Etfw">January 31, 2024</a></blockquote>
</div></figure>



<p>Hilariously, Mensch also appears to have taken to the illicit HuggingFace post not to demand a takedown, but leaving a comment that the poster “might consider attribution.”</p>



<figure></figure>



<p>Still, with Mensch’s note to “stay tuned!” it appears that not only is Mistral training a version of this so-called “Miqu” model that approaches GPT-4 level performance, but it may, in fact, match or exceed it, if his comments are to be interpreted generously. </p>



<h2 id="h-a-pivotal-moment-in-open-source-ai-and-beyond">A pivotal moment in open source AI and beyond?</h2>



<p>That would be a watershed moment not just for open source generative AI but the entire field of AI and computer science: <a href="https://openai.com/research/gpt-4" target="_blank" rel="noreferrer noopener">since its release back in March 2023</a>, GPT-4 has remained the most powerful and highest performing LLM in the world by most benchmarks. Not even any of <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">Google’s presently available, long-rumored Gemini models</a> have been able to eclipse it — yet (according to some measures, the current Gemini models are <a href="https://venturebeat.com/ai/google-gemini-is-not-even-as-good-as-gpt-3-5-turbo-researchers-find/" target="_blank" rel="noreferrer noopener">actually worse than the older OpenAI GPT-3.5 mode</a>l). </p>



<p>The release of an open source GPT-4 class model, which would presumably be functionally free to use, would likely place enormous competitive pressure on OpenAI and its subscription tiers, especially as more enterprises look to open source models, or a mixture of open source and closed source, to power their applications, <a href="https://venturebeat.com/ai/how-enterprises-are-using-open-source-llms-16-examples/">as VentureBeat’s founder and CEO Matt Marshall recently reported</a>. OpenAI may retain the edge with its faster GPT-4 Turbo and GPT-4V (vision), but the writing on the wall is pretty clear: the open source AI community is catching up fast. Will OpenAI have enough of a head start, and a metaphorical “moat” with its <a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come/">GPT Store</a> and other features, to remain in the top spot for LLMs? </p>
<p><strong>VentureBeat's mission</strong> is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. <a href="https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=bottomBoilerplate" data-type="URL" data-id="/newsletters/">Discover our Briefings.</a></p><!-- Boilerplate CSS for "after" -->				</div><!-- .article-content -->

									
				
			</article><!-- #post-2932274 .article-wrapper -->


		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Kando: The Cross-Platform Pie Menu (129 pts)]]></title>
            <link>https://github.com/kando-menu/kando</link>
            <guid>39206966</guid>
            <pubDate>Wed, 31 Jan 2024 17:51:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/kando-menu/kando">https://github.com/kando-menu/kando</a>, See on <a href="https://news.ycombinator.com/item?id=39206966">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:kando-menu/kando" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="ZjgF1FIoJ3Gm7wxtqnKw8WzmM6JDZjaI7SjhtwmwHx649CPXFc08BGilSfuY00Y5ms6f1tyLljUiyYpRUrrTKQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="kando-menu/kando" data-current-org="kando-menu" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=FCltbRivJMkrLPIMV7rr6Q2kg6xiv2TeI%2BTOV043onkp1lbR8qELatdabL0UHFNXyqqrj49KAqn6E5syczmB18ZESFRFMDU5n4wISzeG%2FeqN3%2BYM4cWXFkbuacAy1pOr25L2%2BJq8QeJPCvoEyXiyl4t1wvfJzah5y0oqwUQYsp7KNGlAccxSwMWiMSpwDnQkShsEYHIxSa5a9V%2B7S2mpQbe42DE%2B%2BGgKuBERiRvnVhXjtqlJF12PTgc5hTDN9sC%2Fwx9B8izY3%2FqC2EDzIX%2BS0XYmjIzWUBGLn%2BKUq4QWSRByush9himQxNT4UmXbxhShw2bPveHXDx0FPPRaGkgIvVwL9WWSGNRuLtdJkMestC9lfCEUbWMeLGwmLgUCYSO9dmHTyVII7C%2F9nWtshvzQfPEuC9jayF6h%2Fpn%2BYh0FbTDmEp0rR2Gdxx5mQiqmXc%2F%2FBH6tBKbO2SgNVY24PyQG55kvu6DZcAaZLDGDG59BS2f6oAN6sUlNwwzLvBsHl3PCWRhg49cWDPOltTLTKxA%3D--5%2FaNfF%2F%2BXue6MB0N--HRT%2BZRbFbvbxVReIj5Hpjw%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=kando-menu%2Fkando" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/kando-menu/kando&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="71bbb3ccf2095432fdae0dfca361a935a2c104a0221d8343d9f88b26f6fcb68c" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hearing aids slow cognitive decline in people at high risk (155 pts)]]></title>
            <link>https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk</link>
            <guid>39206911</guid>
            <pubDate>Wed, 31 Jan 2024 17:47:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk">https://www.nih.gov/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk</a>, See on <a href="https://news.ycombinator.com/item?id=39206911">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content">
                <h2>You are here</h2>          
                <div about="/news-events/nih-research-matters/hearing-aids-slow-cognitive-decline-people-high-risk" typeof="sioc:Item foaf:Document" role="main">

  
  <p><span property="dc:date" datatype="xsd:dateTime" content="2023-08-08T00:00:00-04:00">August 8, 2023</span></p><div><div><h3>At a Glance</h3>

<ul>
	<li>Hearing aids reduced the rate of cognitive decline in older adults at high risk of dementia by almost 50% over a three-year period.</li>
	<li>Treating hearing loss may be a safe way to lower the risk of dementia in vulnerable populations.</li>
</ul></div><p>As the world population ages, the number of people living with dementia and other types of cognitive impairment continues to rise. Safe and affordable interventions to prevent or slow age-related cognitive decline are greatly needed.</p>

<p>Studies have found an association between hearing loss and the development of dementia in older adults. Research also suggests that using hearing aids to treat hearing loss may help slow the development of cognitive problems. But potential links between hearing loss treatment and improved cognition hadn’t been tested in a large, randomized study.</p>

<p>To close this gap, an NIH-funded research team co-led by Dr. Frank Lin from Johns Hopkins University enrolled nearly 1,000 adults, ages 70 to 84, into a clinical trial. They compared the rate of cognitive decline over a three-year period between people who did and didn’t receive hearing aids.</p>

<p>The researchers recruited participants with substantial hearing loss from two different study populations. About 250 healthy older adults came from a long-term study of heart health at four sites in the U.S. Another 739 people were newly recruited from the surrounding communities at the same four sites. Participants recruited from the heart-health study had more risk factors for developing dementia, including being older and having faster rates of cognitive decline than those from the general community.</p>

<p>The researchers randomly assigned participants to one of two interventions. About half received hearing aids and instruction in how to use them. The other half were assigned to a health education program focused on promoting healthy aging. Both groups received follow-up visits every six months to reinforce the training.</p>

<p>Before the start of the study and annually for three years, all participants took a battery of tests to measure cognitive functioning. The results were published on July 17, 2023, in <em>The Lancet</em>.</p>

<p>People who received the hearing aids reported substantial improvement in communication abilities over the course of the study. Those in the health-education group, as expected, did not experience improvement in hearing and communication.</p>

<p>In the main analysis of all study participants, the researchers saw no difference in the rate of change in cognitive functioning between people who received the hearing aids and those who didn’t.</p>

<p>However, when the analysis focused on people from the heart-health study, who had a higher risk of dementia, the benefit of the hearing aids was substantial. Those who received hearing aids had an almost 50% reduction in the rate of cognitive decline compared with people in the health-education group.</p>

<p>“Hearing loss is very treatable in later life, which makes it an important public health target to reduce risk of cognitive decline and dementia,” Lin says.</p>

<p>The researchers are continuing to follow the study participants to see how changes in cognition develop over time. They’re also looking at brain scans and data on social engagement to better understand how protecting hearing may prevent cognitive decline in vulnerable older adults.</p>

<p>“Until we know more,” Lin adds, “we recommend for general health and well-being that older adults have their hearing checked regularly and any hearing issues properly addressed.”</p>

<p>—by Sharon Reynolds</p></div><h2>Related Links</h2><ul><li><a href="https://www.nih.gov/news-events/nih-research-matters/diagnosing-hidden-hearing-loss" target="_top">Diagnosing Hidden Hearing Loss</a></li><li><a href="https://www.nih.gov/news-events/nih-research-matters/healthy-diets-linked-lower-risk-hearing-loss-women" target="_top">Healthy Diets Linked to Lower Risk of Hearing Loss in Women</a></li><li><a href="https://www.nih.gov/news-events/nih-research-matters/protein-involved-hearing-loss-recovery" target="_top">Protein Involved in Hearing Loss Recovery</a></li><li><a href="https://newsinhealth.nih.gov/2018/05/struggling-hear" target="_top">Struggling to Hear? Tiny Devices Can Keep You Connected</a></li><li><a href="https://www.nidcd.nih.gov/health/age-related-hearing-loss" target="_top">Age-Related Hearing Loss (Presbycusis)</a></li><li><a href="https://www.nidcd.nih.gov/health/hearing-aids" target="_top">Hearing Aids</a></li><li><a href="https://www.nia.nih.gov/health/topics/hearing-and-hearing-loss" target="_top">Hearing and Hearing Loss</a></li><li><a href="https://www.nia.nih.gov/health/alzheimers" target="_top">Alzheimer's Disease &amp; Related Dementias</a></li><li><a href="https://www.nia.nih.gov/health/cognitive-health-and-older-adults" target="_top">Cognitive Health and Older Adults</a></li></ul><p><strong>References:&nbsp;</strong><a href="https://pubmed.ncbi.nlm.nih.gov/37478886/">Hearing intervention versus health education control to reduce cognitive decline in older adults with hearing loss in the USA (ACHIEVE): a multicentre, randomised controlled trial.</a> Lin FR, Pike JR, Albert MS, Arnold M, Burgard S, Chisolm T, Couper D, Deal JA, Goman AM, Glynn NW, Gmelin T, Gravens-Mueller L, Hayden KM, Huang AR, Knopman D, Mitchell CM, Mosley T, Pankow JS, Reed NS, Sanchez V, Schrack JA, Windham BG, Coresh J; ACHIEVE Collaborative Research Group. <em>Lancet</em>. 2023 Jul 17:S0140-6736(23)01406-X. doi: 10.1016/S0140-6736(23)01406-X. Online ahead of print. PMID: 37478886.</p><p><strong>Funding:&nbsp;</strong>NIH’s National Institute on Aging (NIA) and National Heart, Lung, and Blood Institute (NHLBI); Eleanor Schwartz Charitable Foundation.</p>  
  
</div>
                
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Testing how hard it is to cheat with ChatGPT in interviews (169 pts)]]></title>
            <link>https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</link>
            <guid>39206731</guid>
            <pubDate>Wed, 31 Jan 2024 17:35:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews">https://interviewing.io/blog/how-hard-is-it-to-cheat-with-chatgpt-in-technical-interviews</a>, See on <a href="https://news.ycombinator.com/item?id=39206731">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>ChatGPT has revolutionized work as we know it. From helping small businesses automate their administrative tasks to coding entire React components for web developers, its usefulness is hard to overstate.</p>
<p>At interviewing.io, we've been thinking a lot about how ChatGPT will change technical interviewing. <strong>One big question is: Does ChatGPT make it easy to cheat in interviews?</strong> To decide for yourself, check out this 45-second video. In it, an engineer gets ChatGPT to show him exactly how to respond to an interviewer’s questions.</p>
<a href="https://www.tiktok.com/@kagehiromitsuyami/video/7296566544602123566">
<blockquote cite="https://www.tiktok.com/@kagehiromitsuyami/video/7296566544602123566" data-video-id="7296566544602123566"> </blockquote> 
</a>
<p>ChatGPT integration to help you cheat in interviews!</p>
<p>Crazy, right? Initial responses to cheating software like this have been pretty in line with what you’d expect:</p>

<p>It seems clear that ChatGPT can assist people during their interviews, but we wanted to know:</p>
<ul>
<li>How much can it help?</li>
<li>How easy is it to cheat (and get away with it)?</li>
<li>Will companies that ask LeetCode questions need to make significant changes to their interview process?</li>
</ul>
<p>To answer these questions, we recruited some of our professional interviewers and users for a cheating experiment! Below, we’ll share everything we discovered and explain what it means for you. As a little preview, just know this: companies need to change the types of interview questions they are asking—immediately.</p>
<h2>The experiment</h2>
<p>interviewing.io is an interview practice platform and recruiting marketplace for engineers. Engineers use us for mock interviews. Companies use us to hire top performers. We have thousands of professional interviewers in our ecosystem, and hundreds of thousands of engineers have used our platform to prepare for interviews.<sup><a href="#user-content-fn-1" id="user-content-fnref-1" data-footnote-ref="" aria-describedby="footnote-label">1</a></sup></p>
<h3>Interviewers</h3>
<p>Interviewers came from our pool of professional interviewers. They were broken into three groups, with each group asking a different type of question.  <strong>The interviewers had no idea that the experiment was about ChatGPT or cheating; we told them that "[this] research study aims to understand the trends in the predictability of an interviewer’s decisions over time – especially when asking standard vs. non-standard interview questions."</strong></p>
<p>These were the three question types:</p>
<ol>
<li><strong>Verbatim LeetCode questions</strong>: questions pulled directly from LeetCode at the interviewer's discretion with no modifications to the question.</li>
</ol>
<p>Example: The <a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a> LeetCode question is asked exactly as it is written.</p>
<ol start="2">
<li><strong>Modified LeetCode questions</strong>: questions pulled from LeetCode and then modified to be similar to the original but still notably different from it.</li>
</ol>
<p>Example: The <a href="https://leetcode.com/problems/sort-colors/">Sort Colors</a> question above but modified to have four integers (0,1,2,3) instead of just three integers (0,1,2) in the input.</p>
<ol start="3">
<li><strong>Custom questions</strong>: questions that aren’t directly tied to any question that exists online.</li>
</ol>
<p>Example: You are given a log file with the following format:
- <code>&lt;username&gt;: &lt;text&gt; - &lt;contribution score&gt;</code>
- Your task is to identify the user who represents the median level of engagement in a conversation. Only consider users with a contribution score greater than 50%. Assume that the number of such users is odd, and you need to find the one right in the middle when sorted by their contribution scores. Given the file below, the correct answer is SyntaxSorcerer.</p>
<div><pre><code>LOG FILE START
<span>NullPointerNinja:</span> <span>"who's going to the event tomorrow night?"</span> - <span>100%</span>
<span>LambdaLancer:</span> <span>"wat?"</span> - <span>5%</span>
<span>NullPointerNinja:</span> <span>"the event which is on 123 avenue!"</span> - <span>100%</span>
<span>SyntaxSorcerer:</span> <span>"I'm coming! I'll bring chips!"</span> - <span>80%</span>
<span>SyntaxSorcerer:</span> <span>"and something to drink!"</span> - <span>80%</span>
<span>LambdaLancer:</span> <span>"I can't make it"</span> - <span>25%</span>
<span>LambdaLancer:</span> <span>"🙁"</span> - <span>25%</span>
<span>LambdaLancer:</span> <span>"I really wanted to come too!"</span> - <span>25%</span>
<span>BitwiseBard:</span> <span>"I'll be there!"</span> - <span>25%</span>
<span>CodeMystic:</span> <span>"me too and I'll brink some dip"</span> - <span>75%</span>
LOG FILE <span>END</span>
</code></pre></div>
<p>For more information about question types and about how we designed this experiment, please read the <a href="https://docs.google.com/document/u/0/d/1UdWZHUQfeLR8oUiNY4JfwgES42HTlAQL5z_VfQJPPKk/edit">Interviewer Experiment Guidelines</a> doc that we shared with participating interviewers.</p>
<h3>Interviewees</h3>
<p>Interviewees came from our pool of active users and were invited to participate in a short survey. We selected interviewees who:</p>
<ul>
<li>Were actively looking for a job in today's market</li>
<li>Had 4+ years of experience and were applying to senior-level positions</li>
<li>Rated their “ChatGPT while coding” familiarity as moderate to high</li>
<li>Identified themselves as someone who thought they could cheat in an interview without getting caught</li>
</ul>
<p>This selection helped us skew the candidates toward people who could feasibly cheat in an interview, had the motivation to do so, and were already reasonably familiar with ChatGPT and coding interviews.</p>
<p><strong>We told interviewees that they had to use ChatGPT in the interview, and the goal was to test their ability to cheat with ChatGPT.</strong> They were also told not to try to pass the interview with their own skills — the point was to rely on ChatGPT.</p>
<p>We ended up conducting 37 interviews overall, 32 of which we were able to use (we had to remove 5 because participants didn’t follow directions):</p>
<ul>
<li>11 with the “verbatim” treatment</li>
<li>9 with the “modified” treatment</li>
<li>12 with the “custom” treatment</li>
</ul>
<p>A quick disclaimer. Because our platform allows for anonymity, our interviews have audio but no video. We’re anonymous because we want to create a safe space for our users to fail and learn quickly without judgment. It’s great for our users, but we acknowledge that not having video in these interviews makes our experiment less realistic. In a real interview, you will be on camera with a job on the line, which makes cheating harder — but does not eliminate it (watch the TikTok above if you disagree!).</p>
<p>After the interviews, both interviewers and interviewees had to complete an exit survey. We asked interviewees about the difficulties of using ChatGPT during the interview, and interviewers were given multiple chances to express concerns about the interview — we wanted to see how many interviewers would flag their interviews as problematic and report that they suspected cheating.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewee_Collage_Survey_88a5b0a095.jpg" alt="Post-survey interviewee questions" width="600" height="500"></p><p>Post-survey interviewee questions</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_Collage_Survey_a0e0cf80fb.jpg" alt="Post-survey interviewer questions" width="600" height="500"></p><p>Post-survey interviewer questions</p>
<p>We had no idea what would happen in this experiment, but we assumed that if half the candidates that cheated got away with it and passed the interview, it would be a telling result for our industry.</p>
<h2>Results</h2>
<p>After removing interviews where participants did not follow instructions<sup><a href="#user-content-fn-2" id="user-content-fnref-2" data-footnote-ref="" aria-describedby="footnote-label">2</a></sup>, we got the following results. Our control was how candidates performed in interviewing.io mock interviews outside the study: 53%.<sup><a href="#user-content-fn-3" id="user-content-fnref-3" data-footnote-ref="" aria-describedby="footnote-label">3</a></sup> Note that most mock interviews on our platform are LeetCode-style questions, which makes sense because that's primarily what FAANG companies ask. We'll come back to this in a moment.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Pass_rate_by_question_type_when_trying_to_cheat_with_Chat_GPT_c20c87712e.png" alt="Pass Rate by Question Type with the Control group at 53%, the Verbatim group at 73%, the Modified group at 67%, and the Custom group at 25%" width="650"></p><p>"Verbatim" questions passed significantly more often, compared to both our platform average and to "custom" questions. "Verbatim" and "modified" questions were not statistically significantly differnt from each other. "Custom" questions had a significantly lower pass rate than any of the other groups.</p>
<h3>“Verbatim” questions</h3>
<p>Predictably, the verbatim group performed the best, passing 73% of their interviews. Interviewees reported that they got the perfect solution from ChatGPT.</p>
<p>The most notable comment from the post-interview survey for this group is below — we think it is particularly telling of what was going on in many of the interviewers’ minds:</p>
<blockquote>
<p><em>“It's tough to determine if the candidate breezed through the question because they're actually good or if they've heard this question before. Normally, I add 1-2 unique twists to the problem to ascertain the difference.”</em></p>
</blockquote>
<p>Normally, this interviewer would have followed up with a modified question to get more signal, so let’s examine the “modified” group next to see if interviewers actually got more signal by adding a twist to their questions.</p>
<h3>“Modified” questions</h3>
<p>Remember, this group may have had a LeetCode question given to them, which was standard but modified in a way that was not directly available online. This means ChatGPT couldn’t have had a direct answer to this question. Hence, the interviewees were much more dependent on ChatGPT's actual problem-solving abilities than its ability to regurgitate LeetCode tutorials.</p>
<p>As predicted, the results for this group weren’t too different from the “verbatim” group, with 67% of candidates passing their interviews. As it turns out, this difference was not statistically significantly different from the "verbatim" group, i.e., “modified” and “verbatim” are essentially the same. This result suggests that ChatGPT can handle minor modifications to questions without much trouble. Interviewees did notice, however, that it took more prompting to get ChatGPT to solve the modified questions. As one of our interviewees said:</p>
<blockquote>
<p><em>“Questions that are lifted directly from LeetCode were no problem at all. A follow-up question that was not so much directly LeetCode-style was much harder to get ChatGPT to answer.”</em></p>
</blockquote>
<h3>“Custom” questions</h3>
<p>As expected, the “custom” question group had the lowest pass rate, with only 25% of candidates passing. <strong>Not only is it statistically significantly smaller than the other two treatment groups, it's significantly lower than the control! When you ask candidates fully custom questions, they perform worse than they do when they're not cheating (and getting asked LeetCode-style questions)!</strong></p>
<p>Note that this number, when initially calculated, was marginally higher, but after reviewing the custom questions in detail, we discovered a problem with this question type we hadn’t anticipated, which had skewed the results minorly toward a higher pass rate. Read the section below called "Companies: Change the questions you are asking immediately!" to find out what that problem was.</p>
<h2>No one was caught cheating!</h2>
<p>In our experiment, interviewers were not aware that the interviewees were being asked to cheat. As you recall, after each interview, we had interviewers complete a survey in which they had to describe how confident they were in their assessments of candidates.</p>
<p><strong>Interviewer confidence in the correctness of their assessments was high, with 72% saying they were confident in their hiring decision.</strong> One interviewer felt so strongly about an interviewee's performance that they concluded we should invite them to be an interviewer on the platform!</p>
<blockquote>
<p><em>“The candidate performed very well and demonstrated knowledge of a strong Amazon L6 (Google L5) SWE... and could also be considered to be an interviewer/mentor on interviewing.io.”</em></p>
</blockquote>
<p>That is a lot of confidence after just one interview — probably too much!</p>
<p>We’ve long known that <a href="https://interviewing.io/blog/own-interview-performance">engineers are bad at gauging their own performance</a>, so perhaps it shouldn’t come as a shock to find that interviewers also overestimate the effectiveness of the questions that they ask.</p>
<p>Of the interviewers who were not confident in their hiring choice (28%), we asked them why. This was the frequency distribution of their reasons.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewer_confidence_61b40b9ce1.png" alt="Interviewers weren't confident in their assessment because the candidate gave suboptimal solutions, missed edge cases, had messy code, was inarticulate, or occasionally had other niche issues, but no interviewer was called out cheating as a concern" width="650"></p><p>Note that cheating isn’t mentioned anywhere!</p>
<p>Most interviewers concerned about their hiring decision expressed specific reasons for their lack of confidence. These issues typically included suboptimal solutions, missed edge cases, messy code, or poor communication. We specifically included an “Other Issue” category to see if they would express a concern that the interviewee was cheating, but digging deeper revealed only minor grievances like “personality issues” and “they need to speed up their coding.”</p>
<p>In addition to having this opportunity to call out cheating, interviewers were prompted three additional times to note any other concerns they had with the interview, including free-form text boxes and several multiple-choice questions with options to explain their concerns.</p>
<p>When an interviewee bombed because they didn’t understand the ChatGPT response, the interviewer chalked up the interviewee’s odd behavior and stilted responses to a lack of practice — not cheating. One interviewer thought the candidate's problem-solving was fine but commented that they were slow and needed to consider edge cases more carefully.</p>
<blockquote>
<p><em>“Candidate did not seem prepared for any LeetCode questions."</em></p>
</blockquote>
<blockquote>
<p><em>“Candidate's approach lacked clarity, and they jumped into the coding too early.”</em></p>
</blockquote>
<blockquote>
<p><em>“The candidate was not prepared to tackle even the most basic coding questions on LeetCode.”</em></p>
</blockquote>
<blockquote>
<p><em>“Good problem solving in general, but the candidate needs to be faster at coding and identifying critical edge cases.”</em></p>
</blockquote>
<p>So, who reported concerns about cheating? And who got caught?</p>
<p><strong>As it turns out, not a single interviewer mentioned concerns about any of the candidates cheating!</strong></p>
<p>We were stunned to discover that interviewers reported no suspicions of cheating, and interestingly, interviewees were largely confident that they were getting away with it, too. 81% reported no concerns about being caught, 13% thought they might have tipped off the interviewer, and an astonishingly small 6% of participants thought the interviewer suspected them of cheating.</p>
<p><img src="https://strapi-iio.s3.us-west-2.amazonaws.com/Interviewee_worry_level_6d1445e631.png" alt="81% of interviewees were not worried, 13% were slightly worried, and just 6% were very worried about being caught" width="650"></p><p>Interviewees were mostly sure that their cheating went undetected</p>
<p>The candidates who worried they were caught did have abnormal comments from the interviewers in the post-survey analysis, but they still were not suspected of cheating. <strong>To summarize, most candidates thought they were getting away with cheating — and they were right!</strong></p>
<h2>Companies: Change the questions you are asking immediately!</h2>
<p>The obvious conclusion from these results is that <strong>companies need to start asking custom questions immediately, or they are at serious risk of candidates cheating during interviews (and ultimately not getting useful signal from their interviews)!</strong></p>
<p>ChatGPT has made verbatim questions obsolete; anyone relying on them will be naively leaving their hiring processes up to chance. Hiring is <a href="https://interviewing.io/blog/we-ran-the-numbers-and-there-really-is-a-pipeline-problem-in-eng-hiring">already tricky enough</a> without worrying about cheating. If you’re part of a company that uses verbatim LeetCode questions, please share this post internally!</p>
<p>Using custom questions isn’t just a good way to prevent cheating. It filters out candidates who have memorized a bunch of LeetCode solutions (as you saw, our custom question pass rate was significantly lower than our control). It also meaningfully improves candidate experience, which makes people way more likely to want to work for you. A while ago, we did an <a href="https://interviewing.io/blog/best-technical-interviews-common">analysis of what makes good interviewers good</a>. Not surprisingly, asking good questions was one of the hallmarks, and our best-rated interviewers were the ones who tended to ask custom questions! Question quality was extremely significant in our study, regarding whether the candidate wanted to move forward with the company. It was much more important than the company’s brand strength, which mattered for getting candidates in the door but didn’t matter relative to question quality once they were in process.</p>
<p>As some of our interviewees said…</p>
<blockquote>
<p><em>“Always nice to get questions that are more than just plain algorithms.”</em></p>
</blockquote>
<blockquote>
<p><em>“I liked the question — it takes a relatively simple algorithms problem (build and traverse a tree) and adds some depth. I also liked that the interviewer connected the problem to a real product at [Redacted], which made it feel less like a toy problem and more like a pared-down version of a real problem.”</em></p>
</blockquote>
<blockquote>
<p><em>“This is my favorite question that I’ve encountered on this site. It was one of the only ones that seemed to have real-life applicability and was drawn from a real (or potentially real) business challenge. And it also nicely wove in challenges like complexity, efficiency, and blocking.”</em></p>
</blockquote>
<p>One more somewhat subtle piece of advice for companies who decide to move to more custom questions. You might be tempted to take verbatim LeetCode questions and change up the wording or some of the window dressing. That makes sense, because it’s certainly easier than coming up with questions from scratch. Unfortunately, that doesn’t work.</p>
<p>As we mentioned earlier, we discovered in this experiment that just because a question looks like a custom question, doesn’t mean it is one. Questions can appear custom and still be identical to an existing LeetCode question. <strong>When making questions to ask candidates, it isn’t enough to obscure an existing problem.</strong> You need to ensure that the problem has unique inputs and outputs to be effective at stopping ChatGPT from recognizing it!</p>
<p>The questions that interviewers ask are confidential, and we cannot share the exact questions that our interviewers used in the experiment. However, we can give you an indicative example.  Below is a “custom question” with this critical flaw, which is easy for ChatGPT to beat:</p>
<div><pre><code><span>For</span> her birthday, Mia received a mysterious box containing numbered cards 
<span>and</span> a note saying, <span>"Combine two cards that add up to 18 to unlock your gift!"</span> 
Help Mia find the right pair <span>of</span> cards <span>to</span> reveal her surprise.

<span>Input:</span> An array <span>of</span> integers (the numbers <span>on</span> the cards), <span>and</span> the target sum (<span>18</span>). 
arr = [<span>1</span>, <span>3</span>, <span>5</span>, <span>10</span>, <span>8</span>], target = <span>18</span>

<span>Output:</span> The indices <span>of</span> the two cards that add up <span>to</span> the target sum. 
<span>In</span> this <span>case</span>, [<span>3</span>, <span>4</span>] because index <span>3</span> <span>and</span> <span>4</span> add <span>to</span> <span>18</span> (<span>10</span>+<span>8</span>).
</code></pre></div>
<p>Did you spot the issue? While this question appears “custom” at first glance, its objective is identical to the popular <a href="https://leetcode.com/problems/two-sum/">TwoSum</a> question: finding two numbers that sum to a given target. The inputs and outputs are identical; the only thing “custom” about the question is the story added to the problem.</p>
<p>Seeing that this is identical to known problems, it shouldn’t be a surprise to learn that ChatGPT does well on questions that have inputs and outputs identical to existing known problems — even when they have a unique story added to them.</p>
<h3>How to actually create good custom questions</h3>
<p>One thing we’ve found incredibly useful for coming up with good, original questions is to start a shared doc with your team where every time someone solves a problem they think is interesting, no matter how small, they jot down a quick note. These notes don’t have to be fleshed out at all, but they can be the seeds for unique interview questions that give candidates insight into the day-to-day at your company. Turning these disjointed seeds into interview questions takes thought and effort — you have to prune a lot of the details and distill the essence of the problem into something that doesn’t take the candidate a lot of work/setup to grok. You’ll also likely have to iterate on these home-grown questions a few times before you get them right — but the payoff can be huge.</p>
<p>To be clear, we’re not advocating the removal of data structures and algorithms from technical interviews. DS&amp;A questions have gotten a bad reputation because of bad, unengaged interviewers and because of companies lazily rehashing LeetCode problems, many of them bad, which have nothing to do with their work. In the hands of good interviewers, those questions are powerful and useful. If you use the approach above, you’ll be able to come up with new data structure &amp; algorithmic questions that have a practical foundation and component that will engage candidates and get them excited about the work you’re doing.</p>
<p>You’ll also be moving our industry forward. It’s not OK that memorizing a bunch of LeetCode questions gives candidates an edge in today’s interview process, nor is it OK that interviews have gotten to a state where cheating starts to look like a rational choice. The solution is more work on the employer’s part to come up with better questions. Let’s all do it together.</p>
<h2>Real talk for job seekers</h2>
<p>All right, now, for all of you who are actively looking for work, listen up! Yes, a subset of your peers will now be using ChatGPT to cheat in interviews, and at companies that ask LeetCode questions (sadly, many of them), those peers will have an edge… for a short while.</p>
<p><strong>Right now, we’re in a liminal state where companies’ processes have not caught up to reality. They will, soon enough, either by moving away from using verbatim LeetCode questions entirely (which will be a boon for our entire industry) or by returning to in-person onsites (which will make cheating largely impossible past the technical screen) or both.</strong></p>
<p>It sucks that other candidates cheating is another thing to worry about in an <a href="https://interviewing.io/blog/you-now-need-to-do-15-percent-better-in-technical-interviews">already difficult climate</a>, but we cannot, in good conscience, endorse cheating to “level the playing field.”</p>
<p>In addition, interviewees who used ChatGPT uniformly reported how much more difficult the interview was to complete while juggling the AI.</p>
<p>Below, you can view one interviewee stumbling through their time complexity analysis after giving a perfect answer to an interview question. The interviewer is confused as the interviewee scrambles to explain how they got to their incorrect time complexity (secretly provided by ChatGPT).</p>
<p><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" height="360px" src="https://www.youtube.com/embed/jtcCK0yr9Bg" title="Interviewee stumbles after ChatGPT feeds them a wrong answer" width="640px"></iframe></p>
<p>While no one was caught during the study, their cameras were off, and cheating was still difficult for many of our skilled candidates, as evidenced by this clip.</p>
<p>Ethics aside, cheating is difficult, stressful, and not entirely straightforward to implement. Instead, we advise investing that effort into practice, which will serve you well once companies change their processes, which hopefully should be soon. Ultimately, we hope the advent of ChatGPT will be the catalyst that finally moves our industry’s interview standards away from grinding and memorizing to actually testing engineering ability.</p>
<div><p><img alt="Author avatar" title="Michael Mroczka" loading="lazy" width="150" height="150" decoding="async" data-nimg="1" srcset="https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=256&amp;q=75 1x, https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=384&amp;q=75 2x" src="https://interviewing.io/_next/image?url=https%3A%2F%2Fstrapi-iio.s3.us-west-2.amazonaws.com%2FMike_Headshot_actual_size_2990c50d7a.png&amp;w=384&amp;q=75"></p><div><p><span>Michael Mroczka</span></p><p>Michael Mroczka, an ex-Google SWE, is one of the highest-rated mentors at interviewing.io and primarily works on the Dedicated Coaching program. He has a decade of coaching experience, having personally helped 100+ engineers get into Google and other desirable tech companies. After receiving multiple offers from tech companies early in his career, he enjoys teaching others proven techniques to pass technical interviews.</p>
<p>He also sometimes writes technical content for interviewing.io (like this piece) and was one of the authors of interviewing.io’s <a href="https://interviewing.io/guides/system-design-interview">A Senior Engineer's Guide to the System Design Interview</a>.</p></div></div>
<p><em>Special thanks to Dwight Gunning and Liz Graves for their help with this experiment. And of course a big thank you to all the interviewees and interviewers who participated!</em></p>
<h2>Footnotes:</h2>
<section data-footnotes="">
<ol>
<li id="user-content-fn-1">
<p>To be an interviewer on our platform, you have to have at least 4 years of experience and have conducted at least 20 interviews on behalf of a FAANG or FAANG-adjacent company). <a href="#user-content-fnref-1" data-footnote-backref="" aria-label="Back to reference 1">↩</a></p>
</li>
<li id="user-content-fn-2">
<p>Five interviews needed to be removed because they did not meaningfully use ChatGPT. In two instances, the interviewee was familiar with the question and chose to solve the problem themselves. In one interview, the interviewee wanted to just try the question on their own and didn't prompt ChatGPT, ignoring our instructions. The last two interviews were "custom" interview questions that were problematic for reasons we’ll outline later in this article. <a href="#user-content-fnref-2" data-footnote-backref="" aria-label="Back to reference 2">↩</a></p>
</li>
<li id="user-content-fn-3">
<p>This is a higher passthrough rate than you'd see in the wild. We think it comes down to two factors: selection bias and pre-interview prep. The users who invest in interview prep are a specific, self-selected slice of the population. Moreover, many of our users practice on their own before practicing with a human. <a href="#user-content-fnref-3" data-footnote-backref="" aria-label="Back to reference 3">↩</a></p>
</li>
</ol>
</section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Sega AI (241 pts)]]></title>
            <link>https://www.smspower.org/SegaAI/Index</link>
            <guid>39206529</guid>
            <pubDate>Wed, 31 Jan 2024 17:22:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.smspower.org/SegaAI/Index">https://www.smspower.org/SegaAI/Index</a>, See on <a href="https://news.ycombinator.com/item?id=39206529">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="wikitext">
<p><span>Sega AI Computer (セガＡＩコンピューター)</span>
</p>
<div id="tocid1">
<ul><li><a href="#Introduction">Introduction</a>
</li><li><a href="#SystemSpecs">System Specs</a>
<ul><li><a href="#TouchSurface">Touch surface</a>
</li><li><a href="#AudioSpecs">Audio Specs</a>
</li><li><a href="#SoftwareStorage">Software Storage</a>
</li></ul></li><li><a href="#ArtificialIntelligence">Artificial Intelligence?</a>
</li><li><a href="#WeEventuallyFoundOne">We eventually found one...</a>
</li><li><a href="#AnUsuallyLong19861989LifeCycle">An usually long 1986-1989 life cycle?</a>
</li><li><a href="#WhoWorkedOnTheSegaAIComputer">Who worked on the Sega AI Computer?</a>
</li><li><a href="#Emulation">Emulation</a>
<ul><li><a href="#InstructionsToRunMAME">Instructions to run MAME</a>
</li><li><a href="#Controls">Controls</a>
</li><li><a href="#KnownEmulationBugsAsOfJanuary2024">Known emulation bugs as of January 2024</a>
</li></ul></li><li><a href="#SoftwareTitles">Software Titles</a>
<ul><li><a href="#WhatWeFoundSoFar">What we found so far</a>
</li><li><a href="#WhatWeHaventFoundSoFar">What we haven't found so far</a>
</li></ul></li><li><a href="#Downloads">Downloads</a>
<ul><li><a href="#SystemRoms">System Roms</a>
</li><li><a href="#SoftwareRomsAmpOverlay">Software Roms &amp; Overlay</a>
</li><li><a href="#MAMEReadyRomsFolder">MAME ready Roms folder</a>
</li><li><a href="#TapesRecordings">Tapes recordings</a>
</li><li><a href="#HardwareManuals">Hardware Manuals</a>
</li><li><a href="#SoftwareScans">Software Scans</a>
</li><li><a href="#FlyersAmpAds">Flyers &amp; Ads</a>
</li><li><a href="#Photos">Photos</a>
</li><li><a href="#MagazineScans">Magazine Scans</a>
</li><li><a href="#TechnicalDocumentations">Technical documentations</a>
</li></ul></li><li><a href="#Links">Links</a>
<ul><li><a href="#OurBigSharedFolder">Our big shared folder</a>
</li><li><a href="#ExternalResources">External resources</a>
</li><li><a href="#SiteSections">Site sections</a>
</li><li><a href="#ForumThreads">Forum threads</a>
</li></ul></li><li><a href="#ResearchCredits">Research credits</a>
<ul><li><a href="#Index">Credits</a>
</li><li><a href="#SpecialThanks">Special Thanks</a>
</li><li><a href="#AboutSMSPower">About SMS Power!</a>
</li></ul></li></ul></div>
<h2><a name="Introduction" id="Introduction"></a> Introduction</h2>
<p>Around late 1986, Sega released the “Sega AI Computer”. <strong>This is one of Sega’s least well known and rarest systems.</strong> Not much is known about this system apart from a small amount of information in Japanese and American flyers and press articles. The information we have is still piecemeal and may be partly inaccurate.
</p>
<p><strong>Today we are making public, for the first time: all system roms extracted from the Sega AI Computer, data dumps from 26 my-cards and 14 tapes, many scans and photographs, and in collaboration with <a href="https://www.mamedev.org/" rel="nofollow">MAME developers</a>, an early working MAME driver allowing this computer to be emulated.</strong>
</p>
<p>The majority of these software titles had zero information about them on the internet prior to us publishing them: no screenshots, no photos or scans of actual software. Considering the elusive nature of this machine, it is possible that some games have never been seen or completed by anyone outside of their original development teams.
</p>
<p>We hope that this release will be interesting to obscure game and computer historians and hobbyists alike. We will further amend it over time by releasing extra scans, hopefully improving emulation and publishing/discovering new information.
</p>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-Cover2.jpg" alt="" title=""><br>Extract from a US ad (1986)</p>
<hr>
<h2><a name="SystemSpecs" id="SystemSpecs"></a> System Specs</h2>
<ul><li>Year: 1986
</li><li>CPU: 16-bit NEC V20 at 5 Mhz
</li><li>ROM: 512 KB total (as actually found in system)
<ul><li>2x64 KB system ROM (“Operating System, Prolog”)
</li><li>1x128 KB character ROM
</li><li>2x128 KB speech ROM
</li></ul></li><li>RAM: 128 KB
</li><li>Software on Sega My Cards (128 KB to 256 KB).
</li><li>Software on Audio Cassettes.
</li><li>Video : Yamaha V9938 (Resolution 256x212) with 64 KB VRAM.
</li><li>Sound: SN76489 PSG
</li><li>Inputs: Touch surface with overlays, 8-directions pad + 3 buttons.
</li><li>Inputs: Microphone input.
</li><li>Inputs/Outputs: a RS232 Centronics port
</li><li>Cassette Drive : 9600bps.
</li><li>Optional Keyboard peripheral.
</li><li>Optional Sound Box peripheral with a YM2151 FM chip.
</li></ul><p>An early flyer suggest the possible planning or existence of: a BASIC programming ROM Card, a Disk Drive extension, a Bridge Unit featuring an 8-bit CPU, joystick peripherals and a microphone input.
</p>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-Peripherals.jpg" alt="" title=""><br>Extract from a 1986 Japanese pamphlet</p>
<p>Quick comparisons with other pieces of hardware:
</p><ul><li>The Sega SC-3000 (1983) used a Z80 8-bit CPU, 2 KB of RAM, 16 KB of RAM and no system ROM.
</li><li>The Sega Master System (1985+) increased this to 8 KB of RAM and included a better video chip.
</li><li>The video portion of Sega AI Computer satisfies MSX2 specs.
</li></ul><h3><a name="TouchSurface" id="TouchSurface"></a> Touch surface</h3>
<p>The system has a large touch surface. Most software titles are shipped with an overlay, providing a custom touch interface for each title. Some titles use the touch surface without an overlay to provide drawing capabilities.
</p>
<h3><a name="AudioSpecs" id="AudioSpecs"></a> Audio Specs</h3>
<p>The system is capable of playing <strong>PSG audio</strong>, and <strong>FM audio</strong> when equipped with the Sound Box extension. It is also equipped with a <strong>“speech synthesiser”</strong> in the form of a chip capable of decoding ADPCM data. One of the 128 KB ROM contains speech samples for the 46 common sounds of the Japanese language, the other 128 KB ROM contains entire sentences which are used by the system. In addition, the system is also able to seek and play audio from the cassette drive.
</p>
<p>So up to 4 types of audio can be mixed together: <em>“I tried out a sampling of the software with the Sound Box, and many of them produced FM music instead of simple PSG without the box. All FM is mixed in with everything else, including the PSG, which is still used for rhythm hits; the speech chip, which is used in "Popo's Adventure" ("take a big hamburger" it says, eg.); and the sound from the cassette, which is mostly narration and hints.”</em>.
</p>

<h3><a name="SoftwareStorage" id="SoftwareStorage"></a> Software Storage</h3>
<p>The system can boot software from small-sized cards similar to Master System and PC Engine cards. Unlike Master System-era cards, which could carry a maximum of 32 KB of data, the Sega AI Computer cards can carry 128 KB and 256 KB of data.
</p>
<p>The 128 KB cards have their whole memory space mapped through card pinouts (17 pins). The 256 KB cards have a simple Master System-style bank-switching mapper embedded.
</p>
<p>The system can also boot software from its cassette drive. Sega AI Computer cassettes are stereo, with one channel dedicated to storing encoded data, and the other to storing regular audio. This allows certain loading operations to be playing background music during the load, including snippets from Vivaldi’s Four Seasons.
</p>
<h2><a name="ArtificialIntelligence" id="ArtificialIntelligence"></a> Artificial Intelligence? </h2>
<p><span><img src="https://www.smspower.org/uploads/SegaAI/CloseUp-01.jpg" alt="" title=""></span></p>
<p>Documents describe it as a full-featured computer with an educational twist. The system itself sports markings with a promising <em>“SEGA PROLOG…. Bringing you into the world of artificial intelligence”</em>.
</p>
<p>Effectively all the software we found so far is educational and mostly aimed at kids. The system hardware was definitely not used to its maximum potential, although later 1988-1989 software titles are of higher quality than earlier ones. A US prototype suggests that some form of LISP exists, but we weren’t able to get access to it yet.
</p>
<p>The Prolog interpreter appears to be used by existing software to allow some form of natural language processing. It doesn't seem accessible to the end-user.
</p>

<p>One of the most important English sources is a July 24, 1986 article from “Electronics” magazine, which we are reproducing here in its entirety:
</p>
<div><p>“CHILDREN'S COMPUTER BRINGS AI INTO THE HOME
</p></div><div><p>TOKYO Recent advances in logic-programming languages and processor and peripheral chips are making artificial-intelligence applications practical in the low-end home computer market. The first product that will seek to prove this point is the AI Computer, which Sega Enterprises Ltd. of Tokyo will start selling in Japan next month for $547.
</p></div><div><p>The Sega AI Computer is built to run programs written in the Prolog AI language. It is strongly oriented toward computer-aided-instruction, especially for children, rather than the catchall hobbyist-to-professional target of other home computers. Perhaps its simplest AI application is a personal diary program that can be used two ways: in a simple word-processing mode for children with some writing facility and in a prompt mode.
</p></div><div><p>In the prompt mode, the child is asked about his or her activities during the day and replies with one- and two-word answers. The computer program then writes a grammatically correct diary entry based on those replies. In more advanced CAI applications, the computer is more flexible than previous systems. It can parse a user's natural-language inputs and evaluate the person's ability level. It can then proceed to material of appropriate difficulty, rather than simply advancing one level at a time.
</p></div><div><p>The success of the AI Computer will depend almost totally on the software available for it, and Sega has already forged ties with the educational community to develop courseware for 3- to 8- year-olds. Its partner in this endeavor is general trader Marubeni Corp.'s Visual Information Section, Corporate Development Department, Tokyo, which is working with various educational organizations to create the programs.
</p></div><div><p>Sega plans to work also with Linguaphone Institute (Japan) Ltd., Tokyo, to develop English-language instruction programs. Courseware in English should be ready next year, and the Sega AI machine then will be introduced in the U. S.
</p></div><div><p>Rather than employing the Basic-language interpreters of most personal computers, Sega's AI machine uses a run-time Prolog-language interpreter residing in 128-K bytes of read-only memory. The Prolog interpreter is for running applications only — it cannot be used for programming. The company chose the Prolog AI language because of its ability to handle unformatted input and to parse natural-language input. Prolog is not especially suitable for driving displays and controlling peripherals, so Prolog functions call up fast, efficient assembly-language subroutines for these tasks.
</p></div><div><p>ENHANCED MATH. Sega Prolog was developed jointly with CSK Research Institute, the AI lab of software house Computer Services Corp., Tokyo. In addition to the assembly-language calls, the language's mathematical performance was enhanced for the AI Computer.
</p></div><div><p>The computer's hardware is designed for high performance at low cost. It is built under contract by Nippon Gakki Co., Shizuoka, a leading maker of MSX- compatible computers and electronic musical instruments. The microprocessor is NEC Corp.'s V-20, an enhanced version of the Intel 8088 implemented in CMOS. The Prolog interpreter is stored in 128-K bytes of read-only memory, and main memory is 128K bytes of dynamic random-access memory. Video memory is 64-K bytes of DRAM, expandable to 128-K bytes. A Centronics compatible printer interface is standard. The system's power supply has sufficient capacity to run an optional 3 1/2-in. floppy-disk drive. The machine also comes with a tape recorder for digital input or natural-language audio output, and an eight-direction cursor controller.
</p></div><div><p>The Sega Computer has a tablet on its sloping upper surface that takes overlays for various applications. One that will be used in many children's programs is a Japanese-language touchpad, which includes all phonetic-syllabary characters along with variations annotated with one of two standard marks. The technique was designed to meet the needs of young users, who might not be adept at using a traditional Japanese keyboard where the operator strikes character and mark-only keys in succession to obtain the desired syllable. For older children and adults, the computer also provides the new Japanese industrial-standard keyboard,, which fits on top of the tablet to save desk space. Applications programs are usually supplied as plug-in ROM cards with a capacity off up to 128-K bytes, although the built-in cassette tape recorder and optional 3 1/2-in floppy disk drive can also be used.
</p></div><div><p>ROM versions of Basic and Logo are available for users who want to write programs in those languages. A I28-K RAM expansion kit and a I28-K RAM card are available for disk-based applications or languages that require extra memory.
</p></div><p>-Charles L Cohen
</p><p>Electronics/ July 24, 1986“
</p><h2><a name="WeEventuallyFoundOne" id="WeEventuallyFoundOne"></a> We eventually found one...</h2>
<p>In September 2014, more than a decade after the last public sighting, a user of Yahoo! Auctions in Japan put a Sega AI Computer unit for sale along with 15 software titles, all listed separately. With help from a few generous individuals and associations, we decided to buy everything and work on it. Since that time, several other units have appeared on Yahoo Auctions, but rarely ever came with software again. We however slowly managed to gather more software from other sources. At the end of 2022, another meaningful set of auctions including a boxed keyboard and 10 earlier iterations of known software were sold, which we also managed to acquire a year later from the initial buyer.
</p>
<p>The hardware and software acquired in the first batch were stamped with mention of the Aomori Minami Hoikuen (青森南保育園) in Tokyo. The Sega AI Computer appears to have been mostly sold to Japanese schools. It is unknown how many were manufactured, how many were sold, and if Sega had further plans to market the machine to a wider audience. The December 1986 and January 1987 issues of “Copel 21”, a Japanese science magazine for kids, included an order form to purchase the Sega AI Computer for 87500 yens, and the Kumon Wonderschool set in 17 installments of 9990 yens.
</p>

<h2><a name="AnUsuallyLong19861989LifeCycle" id="AnUsuallyLong19861989LifeCycle"></a> An usually long 1986-1989 life cycle?</h2>
<p>Many software titles have a date printed in their manual. They range from August 1st, 1986 to May 1st, 1989. There are two versions of the My Card for “AI Enikki”: one dated 1986 and one dated 1989. At least 10 titles were released in 1986 on cassettes, then re-released in 1987-1988 on cards. In both instances, the later software versions have been updated and are more advanced.
</p>
<p>The presumed software release timespan (at least 33 months between August 1986 and May 1989) is particularly odd considering this is an extremely rare and unknown system. Why would Sega update and re-release software? Did they do an early confidential launch, then decided to go back to the drawing board to improve software, only to do a similarly confidential relaunch? Did Sega have contracts in place with customers e.g. schools to keep on delivering software for a certain time? We're not sure!
</p>
<p>The ad and order form from COPEL 21 December 1986 issue suggest that at least it was possible to purchase the Sega AI Computer from that time.
</p>
<h2><a name="WhoWorkedOnTheSegaAIComputer" id="WhoWorkedOnTheSegaAIComputer"></a> Who worked on the Sega AI Computer?</h2>
<p><em>Caution: Information here is extremely piecemeal and may be inaccurate.</em>
</p>
<ul><li>The project was seemingly led by Sega R&amp;D Dept 6 (第6研究開発部).
</li><li>Masami Ishikawa (Sega) is among the people who worked on the Sega AI Computer project.
</li><li>Mika Okada (Sega) is among the people who worked on art for games.
</li><li>Very likely many other people were involved but we don’t have many names for now.
</li><li>Some games were developed by Marubeni Corp (according to “Electronics” July 1986 article).
</li><li>The “Pinpon Pasokon” series of 3 software are credited to a company called Kamiya.
</li><li>Sega Prolog was developed partly by the CSK Research Institute (CRI). An early prototype unit aimed at market evaluation in the USA seemingly makes many references to CRI, but we didn’t get access to this prototype.
</li><li>Sega worked with Linguaphone Institute to develop the English-language learning software.
</li><li>Hardware was built by Nippon Gakki Co., Shizuoka (according to “Electronics” July 1986 article).
</li><li>Spotted in the data following a simple search for ASCII strings:
<ul><li>One system ROM contains the following ASCII string: <code>“07/19-1986”.</code>
</li><li>The Sound Box ROM contains the following ASCII strings: <code>“Programmed by SHUN ARAI. Produced by YASUHIGE KOBAYASHI. Special Thanks: KAMIYA STUDIO &amp; PHILOS” “R&amp;D No. 6 SOUND BOARD v1.1 87/08/12 SEGA”</code>
</li></ul></li></ul><p>Black Squirrel thoughts: <em>“It is likely a CSK product with Sega branding. CSK was Sega's parent company at the time (they bought Sega in 1983) and seem to have used the AI Computer as an attempt to get into the educational market. CSK also had subsidiaries producing hardware and software - CRI (CSK Research Institute) seems to be responsible for "Sega Prolog", the closest this thing gets to an operating system. I don't know if this means CRI were secretly producing the software on cards and cassettes under the Sega name, or if there were genuine Sega people involved (I noticed a credit for Shun Arai on the sound board - he was a Sega man). Perhaps they thought kids would engage more if they saw a Sega logo in 1986 rather than a CSK/CRI one?”</em>
</p>
<p><em>“Sega's use of the term "AI" or "artificial intelligence" is a bit loose, even for 1986. It's a far cry from the likes of ChatGPT - I think the Sega AI Computer is giving the illusion of intelligence by recognising and responding in real human languages (English? Japanese? Both?) rather than 20 GOTO 10. The selling point seems to be "it's not BASIC, it's not assembly, it's Prolog!"”</em>
</p>
<p><em>“The promotional material is hinting at modems and printers and microphones and mysterious boxes - not sure how much of that came to pass. There's talk of an "8-bit CPU board" - maybe that's where MSX2 compatibility comes in? I don't know what the plan was there - perhaps it was a fallback if AI Computer software didn't take off.”</em>
</p>
<p><em>“It does have some legacy though: starting with the Mega Drive, games on Sega platforms were required to have a ROM header: <a href="https://segaretro.org/ROM_header" rel="nofollow">https://segaretro.org/ROM_header</a>. If it was a game, it was designated "GM", but educational software was designated "AI". ”</em>
</p>
<h2><a name="Emulation" id="Emulation"></a> Emulation</h2>
<p><img src="https://www.smspower.org/uploads/SegaAI/SegaAI-BootScreen.png" alt="" title=""></p>
<p>A MAME driver has been developed by Wilbert Pol and Fabio Priuli, based on hardware research by Chris Covell. Code has been merged, and 2024/01/31 release will partial emulation of the Sega AI Computer. If you want to try it today:
</p>
<ul><li><a href="https://www.mamedev.org/release.html" rel="nofollow">MAME Downloads</a>.
</li><li><a href="https://www.mamedev.org/?p=533" rel="nofollow">MAME 0.262 release note</a>.
</li><li>Download our <a href="#MAMEReadyRomsFolder">MAME ready roms folder</a> and unzip to <code>roms/segaai/</code>
</li></ul><h3><a name="InstructionsToRunMAME" id="InstructionsToRunMAME"></a> Instructions to run MAME</h3>
<ul><li>From the UI:
<ul><li>Run MAME and browse to AI system.
</li><li>Press TAB to access configuration menu.
</li></ul></li><li>From the command-line:
<ul><li>Copy system roms, games and overlay scans from xxxx.zip into roms/segai/ folder (use use "-rompath /path/to/extract/to/segaai" command-line parameter).
</li><li>Run with <code>“mame -nodebug -window segaai -card XXXX”</code> where “XXXX” stands for a game identifier in MAME’s format.
</li><li>To enable the Sound Box emulation, add the <code>“-exp soundbox”</code> flag.
</li><li>Multi-cards games may be booted from the second card using <code>"-card XXXX:card2"</code>, e.g. <code>"-card okeihana:card2"</code>.
</li></ul></li></ul><p>The game identifiers in MAME’s format are: <code>aienikki, alicewor, andersen, arabiann, cinderel, columbus, cosmictr, eigogame, eigoohan, gulliver, henshin, mozartac, oceanfan, okeihana, pinponmm, pinponmr, pinponnm, ranranme, robinson, runrunmu, tantanrh, wakuwaku</code>.
</p>
<h3><a name="Controls" id="Controls"></a> Controls</h3>
<ul><li>Because the system has both a D-Pad and an emulated touch surface, it is recommended that you reconfigure MAME inputs.
</li><li>Press TAB and go to 'Input Settings'-&gt;'Input Assignments (this system')
</li><li>You can leave P1 Up/Down/Left/Right mapped to Keyboard arrow keys.
</li><li>Change Touchpad X Analog Inc/Dec to e.g. D and A, and Touchpad Y Analog Inc/Dec to e.g. S and W.
</li><li>The controls becomes:
<ul><li>Touchpad: move cursor with WASD, press with SHIFT.
</li><li>D-Pad: Arrow keys
</li><li>Grey button: Space
</li><li>RL/PL: Ctrl, Alt
</li></ul></li></ul><h3><a name="KnownEmulationBugsAsOfJanuary2024" id="KnownEmulationBugsAsOfJanuary2024"></a> Known emulation bugs as of January 2024:</h3>
<ul><li>Speech emulation seems to occasionally refer to samples from the wrong ROM.
</li><li>Tape drive is not emulated yet.
</li><li>Keyboard is not emulated yet.
</li><li>Likely other emulation bugs and issues.
</li></ul><h2><a name="SoftwareTitles" id="SoftwareTitles"></a> Software Titles</h2>
<h3><a name="WhatWeFoundSoFar" id="WhatWeFoundSoFar"></a> What we found so far</h3>
<p>See our site listings (every game has its own page):
</p><ul><li><a href="https://www.smspower.org/Games-Releases/AI-JP" rel="nofollow">Our list of known Sega AI Computer unique releases</a>
</li><li><a href="https://www.smspower.org/Games/AI-All" rel="nofollow">Our list of known Sega AI Computer unique titles</a>.
</li><li><a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">Our list of Sega AI Computer related scans</a>.
</li></ul><p>All software we found so far is mostly in the Japanese language and tends to be text-heavy, making them difficult to play for non-Japanese readers. We hope that our scans and screenshots can still give you a sense of what they are.
</p>
<p><strong>AI Enikki</strong>
</p><ul><li><a href="https://www.smspower.org/Games/AIEnikki-AI">AI Enikki</a> (ＡＩえにっき) (early card version, later card version)
</li></ul>
<p><strong>Kumon Wonderschool (two sets)</strong>
</p>
<p>Two sets exists: an earlier tape-only set, and a later card-only set.
Only the card versions are emulated so far. The tape version were released 1-2 years before and appear to be different software with different contents.
</p>

<ul><li><a href="https://www.smspower.org/Games/AliceWorld-AI">Alice World</a> (ありす・わーるど) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/RobinsonLand-AI">Robinson Land</a> (ろびんそん・らんど) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/CosmicTrain-AI">Cosmic Train</a> (こすみっく・とれいん) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/CinderellaLabyrinth-AI">Cinderella Labyrinth</a> (しんでれら・らびりんす) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/GulliverPocket-AI">Gulliver Pocket</a> (がりばー・ぽけっと) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/MozartAcademy-AI">Mozart Academy</a> (もーつぁると・あかでみー) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/ArabianNight-AI">Arabian Night</a> (あらびあん・ないと) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/AndersenDream-AI">Andersen Dream</a> (あんでるせん・どりーむ) (early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/OceanFantasy-AI">Ocean Fantasy</a> (おーしゃん・ふぁんたじー) early tape version, later card version)
</li><li><a href="https://www.smspower.org/Games/ColumbusMap-AI">Columbus Map</a> (ころんぶす・まっぷ) early tape version, later card version)
</li></ul><table><tbody><tr><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-04.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AliceWorld-AI-Card-11.png" alt="" title=""></td></tr>
<tr><td>Alice World [Card version]</td><td>Alice World [Card version]</td><td>Alice World [Card version]</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/CosmicTrain-AI-Card-02.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/CosmicTrain-AI-Card-14.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/AndersenDream-AI-Card-TitleScreen.png" alt="" title=""></td></tr>
<tr><td>Cosmic Train [Card version]</td><td>Cosmic Train [Card version]</td><td>Andersen Dream [Card version]</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/ArabianNight-AI-Card-01.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/CinderellaLabyrinth-AI-Card-02.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/ColumbusMap-AI-Card-08.png" alt="" title=""></td></tr>
<tr><td>Arabian Night [Card version]</td><td>Cinderella Labyrinth [Card version]</td><td>Columbus Map [Card version]</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/GulliverPocket-AI-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/GulliverPocket-AI-03.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/MozartAcademy-AI-Card-06.png" alt="" title=""></td><td>&nbsp;</td></tr>
<tr><td>Gulliver Pocket [Card version]</td><td>Gulliver Pocket [Card version]</td><td>Mozart Academy [Card version]</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/OceanFantasy-AI-Card-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/OceanFantasy-AI-Card-08.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/RobinsonLand-AI-Card-TitleScreen.png" alt="" title=""></td></tr>
<tr><td>Ocean Fantasy [Card version]</td><td>Ocean Fantasy [Card version]</td><td>Robinson Land [Card version]]</td></tr>
</tbody></table>
<p><strong>Ongaku Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/RunrunMusic-AI">Runrun Music</a> (るんるん・みゅーじっく) (card)
</li><li><a href="https://www.smspower.org/Games/TantanRhythm-AI">Tantan Rhythm</a> (たんたん・りずむ) (card)
</li><li><a href="https://www.smspower.org/Games/RanranMelody-AI">Ranran Melody</a> (らんらん・めろでぃー) (card)
</li></ul>
<p><strong>English Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/FolkAndFairyTales-AI">Folks &amp; Fairy Tales - Eigo de Hanashi</a> (Folk &amp; Fairy Tales - えいごでおはなし) (two cards + two tapes)
</li><li><a href="https://www.smspower.org/Games/PoposAdventure-AI">Popo Adventure's - Eigo de Game</a> (Popo's Adventure - えいごでげーむ) (two cards + two tapes)
</li></ul><table><tbody><tr><td><img src="https://www.smspower.org/uploads/Screenshots/PoposAdventure-AI-PopoNoYume-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/PoposAdventure-AI-PopoNoYume-02.png" alt="" title=""></td><td>&nbsp;</td></tr>
<tr><td>Popo's Adventure: Popo no Yume</td><td>Popo's Adventure: Popo no Yume</td><td>&nbsp;</td></tr>
<tr><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-TitleScreen.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-01.png" alt="" title=""></td><td><img src="https://www.smspower.org/uploads/Screenshots/FolkAndFairyTales-AI-EigoDeOhanashi1-02.png" alt="" title=""></td></tr>
<tr><td>Folk &amp; Fairy Tales</td><td>Folk &amp; Fairy Tales</td><td>Folk &amp; Fairy Tales</td></tr>
</tbody></table>
<p><strong>Surasura Moji Wonder School</strong>
</p><ul><li><a href="https://www.smspower.org/Games/HenshinKanji-AI">Henshin Kanji</a> (へんしんかんじ) (card)
</li><li><a href="https://www.smspower.org/Games/OkeikoHanamaruAiueo-AI">Okeiko/Hanamaru Aiueo</a> (おけいこ・はなまる　あいえうお) (two cards)
</li><li><a href="https://www.smspower.org/Games/WakuWakuABCTo123-AI">Waku Waku ABC To 123</a> (わくわくＡＢＣと１２３) (card)
</li></ul>
<p><strong>Pinpon Pasokon</strong>
</p><ul><li><a href="https://www.smspower.org/Games/PinponNumbers-AI">Pinpon Numbers</a> (ピンポン　ナンバーズ) (card)
</li><li><a href="https://www.smspower.org/Games/PinponMusicRhythm-AI">Pinpon Music Rhythm</a> (ピンポン　ミュージック　リズム) (card)
</li><li><a href="https://www.smspower.org/Games/PinponMusicMelody-AI">Pinpon Music Melody</a> (ピンポン　ミュージック　メロディー) (card)
</li></ul>
<h3><a name="WhatWeHaventFoundSoFar" id="WhatWeHaventFoundSoFar"></a> What we haven't found so far</h3>
<p>AI Enikki: Cassette version was spotted in an auction, we couldn’t acquire it.
</p>
<p>Edison Labo: mentioned in Kumon Wonder School Japanese flyer, with screenshots.
</p>
<p>Mentions of other titles at <a href="http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html" rel="nofollow">SukaSega's AI page</a> (exact source unknown)
</p><ul><li>Edison’s Labo (エジソンラボ)
</li><li>Lincoln Freedom (リンカーン・フリーダム)
</li><li>Grimm House (グリム・ハウス)
</li><li>Holmes Mystery (ホームズ・ミステリー)
</li><li>Anne Diary (アンネ・ダイアリー)
</li><li>Columbus Egg (コロンブス・エッグ) → Likely same as “Columbus Map”
</li><li>Takara Jima Pirates (宝島・パイレーツ)
</li><li>Space Fantasy (スペース・ファンタジー)
</li><li>Safari Fantasy (サファリ・ファンタジー)
</li><li>Mothers Book (マザーズブック)
</li></ul><p><a href="#MagazineScans">COPEL 21 December 1986 and January 1987 issues</a> contains an ad showing 18 boxes (likely mockups) with readable names:
</p><ul><li>Alice World (アリス　ワールド）→ Released
</li><li>Robinson World (ロビンソン　ワールド）→ Released as “Robinson Land”
</li><li>Cinderella World (シンデレラ ワールド）→ Released as “Cinderella Labyrinth”
</li><li>Aesop World (イソップ ワールド）
</li><li>Hansel &amp; Gretel (へんセル＆グレーテル　ワールド）
</li><li>Goku World (ゴクウ　ワールド）
</li><li>Taka Jima World (高島　ワールド）
</li><li>Helen Keller World (ヘレン ケラー ワールド）
</li><li>Nightingale World（ナイチンゲール ワールド）
</li><li>Jeanne d’Arc World (ジャンヌ・ダルク　ワールド）
</li><li>Edison World (エジソン　ワールド）→ Later referred to as “Edison’s Labo”
</li><li>Lincoln World (リンカーン ワールド）→ Later referred to as “Lincoln Freedom”
</li><li>Space World (スペース（宇宙）ワールド）
</li><li>Safari World (サファリ（？？）ワールド）
</li><li>Robot World (ロボット　ワールド）
</li><li>Mother World 1 (マザー ワールド１）
</li><li>Mother World 2 (マザー ワールド２）
</li><li>Mother World 3 (マザー ワールド３）
</li></ul><p>It is unlikely that those titles were all finished or released, but we have screenshots for “Edison’s Labo” in a flyer so this one was likely developed.
</p>
<h2><a name="Downloads" id="Downloads"></a> Downloads</h2>
<h3><a name="SystemRoms" id="SystemRoms"></a> System Roms</h3>
<p>The system dumps amount to a total of 590 KB of uncompressed data.
</p>
<ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20System%20Roms.zip">Sega AI Computer - System Roms.zip</a></strong> (382 KB)
<ul><li>AI-E000.bin: system EPROM 0, marked 8/24 (64 kb)
</li><li>AI-F000.bin: system EPROM 1, marked 7/21 (64 kb), contains string: <code>7/19/86</code>
</li><li>AI-MPR-7619.bin: speech ROM 0 (128 kb)
</li><li>AI-MPR-7620.bin: speech ROM 1 (128 kb)
</li><li>AI-MPR-7689.bin: character ROM (128 kb)
</li><li>AI-SND-AI2002.bin: sound box EPROM (128 kb), contains strings: <code>Programed by SHUN ARAI.Produced by YASUHIGE KOBAYASHI.Special Thanks KAMIYA STUDIO &amp; PHILOS [...] R&amp;D No.6SOUND BOARD V1.187/08/12 SEGA</code>
</li></ul></li></ul><h3><a name="SoftwareRomsAmpOverlay" id="SoftwareRomsAmpOverlay"></a> Software Roms &amp; Overlay</h3>
<p>The 26 Sega My Card dumps amount to a total of 4.2 MB of uncompressed data.
We also scan all overlays at had at our disposal.
</p>
<ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%2026%20Cards%20Roms.zip">Sega AI Computer - 26 Cards Roms.zip</a></strong> (1941 kB)
</li><li><strong><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%2026%20Overlay%20Scans.zip">Sega AI Computer - 26 Overlay Scans.zip</a></strong> (10 MB)
</li></ul><h3><a name="MAMEReadyRomsFolder" id="MAMEReadyRomsFolder"></a> MAME ready Roms folder</h3>
<p>As MAME use specific naming convention for its files, we provide a readily usable folder:
</p><ul><li><strong><a href="https://www.smspower.org/uploads/SegaAI/MAME%20ready%20roms%20folder%20for%20Sega%20AI%20Computer%20no%20tapes.zip">MAME ready roms folder for Sega AI Computer (no tapes).zip</a></strong> (10 MB)
</li></ul><h3><a name="TapesRecordings" id="TapesRecordings"></a> Tapes recordings</h3>
<p>Important: Very large data. The 14 cassettes (16 sides in total) currently amount to about ~1.2 GB of data. Please don't download unless you have use for this: the cassette system is NOT emulated a the time of writing, thus the data from our cassette dumps was not yet certified as “correct” (as in, passing the Sega AI Computer loading routines and probable checksum checks). According to early research, tapes are using a FSK modulation scheme, with a theoretical maximum bit-rate of 9600 BPS which is very high for its time. Actual bit-rate used seems at least two times lower. We are currently releasing 48 Khz, 24-bit stereo lossless FLAC. It is probable that once emulated, we can settle on a compact format for the data segments, or can tell what level of audio compression is adequate for loading in a real system and emulators.
</p>
<p>Tape downloads are available from our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Our Big Shared Folder</a>.
</p>
<ul><li>Alice World [Tape version] (Tape).flac (~5 minutes)
</li><li>Andersen Dream [Tape version] (Tape).flac (~9 minutes)
</li><li>Arabian Night [Tape version] (Tape).flac (~10 minutes)
</li><li>Cinderella Labyrinth [Tape version] (Tape).flac (~5 minutes)
</li><li>Columbus Map [Tape version] (Tape).flac (~5 minutes)
</li><li>Cosmic Train [Tape version] (Tape).flac (~9 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side A).flac (~15 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 1 (Tape Side B).flac (~15 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side A).flac (~16 minutes)
</li><li>Folk and Fairy Tales - Eigo de Ohanashi 2 (Tape Side B).flac (~16 minutes)
</li><li>Gulliver Pocket [Tape version] (Tape).flac (~5 minutes)
</li><li>Mozart Academy [Tape version] (Tape).flac (~5 minutes)
</li><li>Ocean Fantasy [Tape version] (Tape).flac (~11 minutes)
</li><li>Popo's Adventure - Eigo de Game 1 - Popo no Yume (Tape).flac (~16 minutes)
</li><li>Popo's Adventure - Eigo de Game 2 - Popoland no Himitsu (Tape).flac (~18 minutes)
</li><li>Robinson Land [Tape version] (Tape).flac (~5 minutes)
</li></ul><h3><a name="HardwareManuals" id="HardwareManuals"></a> Hardware Manuals</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Main%20Unit%20Manual.pdf">Sega AI Computer - Main Unit Manual.pdf</a> (15 MB)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Sound%20Box%20Manual.pdf">Sega AI Computer - Sound Box Manual.pdf</a> (9 MB)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Keyboard%20Manual.pdf">Sega AI Computer - Keyboard Manual.pdf</a> (15 MB)
</li></ul>
<h3><a name="SoftwareScans" id="SoftwareScans"></a> Software Scans</h3>
<ul><li>See <a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">Our list of Sega AI Computer related scans</a>.
</li></ul><h3><a name="FlyersAmpAds" id="FlyersAmpAds"></a> Flyers &amp; Ads</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Flyer%20-%20CAI-1%201986-04%20Japan.pdf">Flyer - CAI-1 (1986-04) (Japan).pdf</a> (26 MB) (courtesy of <a href="https://sudden-desu.net/" rel="nofollow">Sudden-Desu</a>)
</li><li><a href="https://www.smspower.org/uploads/SegaAI/Flyer%20-%20Sega%20AI%20Computer%20Japan.pdf">Flyer - Sega AI Computer (Japan).pdf</a> (20 MB) (courtesy of <a href="https://sudden-desu.net/" rel="nofollow">Sudden-Desu</a>)
</li><li>see our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for many more.
</li></ul>

<h3><a name="Photos" id="Photos"></a> Photos</h3>
<p>See our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for:
</p><ul><li>about 150 photos from all past known auctions/sales of Sega AI Computer contents.
</li><li>close up photos of Sega AI Main Unit
</li><li>close up photos of Sega AI Main Unit board
</li><li>close up photos of Sega AI Keyboard
</li><li>close up photos of Sega AI Sound Box board
</li><li>photos of software packaging
</li><li>archived webpages about an unreleased US prototype
</li><li>and more...
</li></ul>



<h3><a name="MagazineScans" id="MagazineScans"></a> Magazine Scans</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/COPEL%2021%201986-12%20Article%20amp%20Ad%20Japan.pdf">COPEL 21 - 1986-12 - Article and Ad (Japan).pdf</a> (30 MB)
</li><li>see our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> for many more.
</li></ul><h3><a name="TechnicalDocumentations" id="TechnicalDocumentations"></a> Technical documentations</h3>
<ul><li><a href="https://www.smspower.org/uploads/SegaAI/Sega%20AI%20Computer%20-%20Misc%20Technical%20Docs%202024-01-29.zip">Sega AI Computer - Misc Technical Docs (2024-01-29).zip</a> (122 KB) various notes and schematics by Chris Covell.
</li><li>Also see <a href="https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread" rel="nofollow">Reverse engineering thread</a>.
</li></ul><h2><a name="Links" id="Links"></a> Links</h2>
<h3><a name="OurBigSharedFolder" id="OurBigSharedFolder"></a> Our big shared folder</h3>
<ul><li>Our <a href="https://drive.google.com/drive/folders/1zCtETqhax-0UHjtOrBd4nWcDC5WB1gSF" rel="nofollow">Big Shared Folder</a> has about 700 files, 1.6 GB of data.
</li></ul><h3><a name="ExternalResources" id="ExternalResources"></a> External resources</h3>
<ul><li><a href="http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html" rel="nofollow">http://park21.wakwak.com/~suka/sega/catalog/AIcom/AI.html</a>
</li><li><a href="https://segaretro.org/Sega_AI_Computer" rel="nofollow">https://segaretro.org/Sega_AI_Computer</a>
</li><li><a href="https://archive.org/search?query=%22%E3%82%BB%E3%82%ACAI%22&amp;sin=TXT" rel="nofollow">https://archive.org/search?query=%22%E3%82%BB%E3%82%ACAI%22&amp;sin=TXT</a>
</li></ul><h3><a name="SiteSections" id="SiteSections"></a> Site sections</h3>
<ul><li><a href="https://www.smspower.org/Games-Releases/AI-JP" rel="nofollow">https://www.smspower.org/Games-Releases/AI-JP</a>
</li><li><a href="https://www.smspower.org/Games/AI-All" rel="nofollow">https://www.smspower.org/Games/AI-All</a>
</li><li><a href="https://www.smspower.org/Scans/AI-All" rel="nofollow">https://www.smspower.org/Scans/AI-All</a>
</li></ul><h3><a name="ForumThreads" id="ForumThreads"></a> Forum threads</h3>
<ul><li><a href="https://www.smspower.org/forums/15004-SegaAIComputerSoftware" rel="nofollow">https://www.smspower.org/forums/15004-SegaAIComputerSoftware</a>
</li><li><a href="https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread" rel="nofollow">https://www.smspower.org/forums/15244-SegaAIComputerReverseEngineeringThread</a>
</li><li><a href="https://www.smspower.org/forums/19976-SegaAIComputerFirstDumpsSystemRomsScansRecordingsMore" rel="nofollow">https://www.smspower.org/forums/19976-SegaAIComputerFirstDumpsSystemRomsScansRecordingsMore</a>
</li></ul><h2><a name="ResearchCredits" id="ResearchCredits"></a> Research credits</h2>
<h3><a name="Index" id="Index"></a> Credits</h3>
<ul><li>Chris Covell (<a href="https://www.chrismcovell.com/" rel="nofollow">https://www.chrismcovell.com</a>) did the majority of the early hardware reverse engineering, documentation and software dumping.
</li><li>Wilbert Pol and Fabio Priuli went on to create a MAME driver. Early work was actually done in 2015. As we uncovered new software and received it in late 2023, we decided to resume work on this release and Wilbert took care of further emulation work. A first version of the driver was merged into MAME’s codebase on January 23, 2024 (<a href="https://github.com/mamedev/mame/pull/11747" rel="nofollow">pull request</a>).
</li><li>Omar Cornut acquired hardware and software, dumped software cards and tapes acquired later, scanned contents, researched information and organized other work.
</li><li>Charles MacDonald helped build a custom My Card adapter with configurable pinouts to facilitate dumping later finds.
</li><li>Game Preservation Society received the hardware and made the first pass on trying to dump the data. It was found that the Sega AI cards share the pinouts of Sega MyCards used for the SG-1000 II and Master System. A first set of (incomplete) dumps were made which would help moving emulation forward until the unit and software were passed to Chris.
</li></ul>
<h3><a name="SpecialThanks" id="SpecialThanks"></a> Special Thanks</h3>
<ul><li>Thanks to Odin Ozdil for helping acquire extra software and scanning the Sega AI Computer manual.
</li><li>Thanks to Masato Ozaki, BEEP Shop &amp; Takayuki Komabayashi for helping me acquire extra software.
</li><li>Thanks to Black Squirrel and Gorefestor for help writing this article.
</li><li>Thanks to KailoKyra for scan processing, assistance with tape dumping &amp; more.
</li><li>Thanks to Balrog for assistance with tape dumping.
</li><li>Thanks to <a href="http://park21.wakwak.com/~suka/sega" rel="nofollow">SukaSega</a> for many early scans and research.
</li><li>Thanks to BKK for general research.
</li><li>Thanks to <a href="https://sudden-desu.net/" rel="nofollow">Sudden-desu</a> for high-resolution scans of two Japanese flyers.
</li><li>Thanks to Maxim for general support, corrections, web work &amp; many more.
</li><li>Thanks to generous donors helping buying the first batch: initial purchase received funds from: Adrien Duchemole, Bock, Chris Covell, David Papazian (Mobigame), Frank Cifaldi (Video Game History Foundation), Kusfo, Game Preservation Society, GlobZ, Gregg "Greggman" Tavares, Olivier L, Vitorio Miliano.
</li></ul><h3><a name="AboutSMSPower" id="AboutSMSPower"></a> About SMS Power!</h3>
<ul><li>Active since 1997, <a href="https://www.smspower.org/" rel="nofollow">SMS Power!</a> has been researching, gathering and publishing all sort of information and data related to Sega 8-bit systems. We unveiled hundreds of prototypes, and have an active community of hobbyists contributors creating maps, soundtracks, game modifications, translations and more. You may if you can <a href="https://www.smspower.org/Home/Donate" rel="nofollow">Donate to facilitate purchasing prototypes and rare software</a>.
</li></ul>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLaVA-1.6: Improved reasoning, OCR, and world knowledge (143 pts)]]></title>
            <link>https://llava-vl.github.io/blog/2024-01-30-llava-1-6/</link>
            <guid>39206375</guid>
            <pubDate>Wed, 31 Jan 2024 17:13:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://llava-vl.github.io/blog/2024-01-30-llava-1-6/">https://llava-vl.github.io/blog/2024-01-30-llava-1-6/</a>, See on <a href="https://news.ycombinator.com/item?id=39206375">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <!-- for mathjax support -->





<p>In October 2023, we released <a href="https://arxiv.org/abs/2310.03744">LLaVA-1.5</a> with a simple and efficient design along with great performance on a benchmark suite of 12 datasets. It has since served as the foundation of many comprehensive studies of data, model, and capabilities of large multimodal models (LMM), and has enabled various new applications.</p>

<p><strong>Today, we are thrilled to present LLaVA-1.6, with improved reasoning, OCR, and world knowledge. LLaVA-1.6 even exceeds Gemini Pro on several benchmarks.</strong></p>

<p>Compared with LLaVA-1.5, LLaVA-1.6 has several improvements:</p>

<ol>
  <li><strong>Increasing the input image resolution</strong> to 4x more pixels. This allows it to grasp more visual details. It supports three aspect ratios, up to 672x672, 336x1344, 1344x336 resolution.</li>
  <li><strong>Better visual reasoning and OCR capability</strong> with an improved visual instruction tuning data mixture.</li>
  <li><strong>Better visual conversation for more scenarios</strong>, covering different applications. Better world knowledge and logical reasoning.</li>
  <li><strong>Efficient deployment and inference</strong> with <a href="https://github.com/sgl-project/sglang">SGLang</a>.</li>
</ol>

<p>Along with performance improvements, <strong>LLaVA-1.6 maintains the minimalist design and data efficiency of LLaVA-1.5</strong>. It re-uses the pretrained connector of LLaVA-1.5, and still uses less than 1M visual instruction tuning samples. The largest 34B variant finishes training in ~1 day with 32 A100s. <strong>Code, data, model will be made publicly available.</strong></p>

<h2 id="open-source-release">Open-Source Release</h2>

<p>We open-source the LLaVA-1.6 to facilitate future development of LMM in the community. Code, data, model will be made publicly available.</p>

<ul>
  <li><a href="https://llava.hliu.cc/" target="_blank">Demo</a></li>
  <li><a href="https://github.com/haotian-liu/LLaVA" target="_blank">Code</a> (Training code coming soon)</li>
  <li><a href="https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md#llava-v16" target="_blank">Model</a></li>
  <li>Data: Coming soon.</li>
</ul>

<h2 id="results">Results</h2>

<p><span>Open-Source</span>
  <span>Proprietary</span>
</p>

<table>
  <tbody><tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>Math-Vista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th></tr>
  <tr><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td></tr>
  <tr><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>43.3</td><td>-</td><td>-</td><td>55.7</td><td>-</td><td>65.7</td></tr>
  <tr><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td></tr>
  <tr><td>125M</td><td>~1M</td><td>Yi-VL-34B</td><td>45.9</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>558K</td><td>665K</td><td>LLaVA-1.5-13B</td><td>36.4</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-34B</td><td>51.1</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td></tr>
</tbody></table>

<details>
<summary><b><em>For more results, please unfold to see expanded tables</em></b></summary>

<i>More benchmarks will be added soon.</i>

<p><span>$^\dagger$specialist</span>
  <span>Grey: supervised finetuned</span>
  <span>$^*$training <u>image</u> observed</span>
  <span>Normal text: zero-shot</span>
</p>

<table>
  <tbody><tr><th>Model</th><th>VQAv2</th><th>GQA</th><th>VisWiz</th><th>TextVQA</th><th>ScienceQA</th></tr>
  <tr><td>GPT-4V</td><td>77.2</td><td>-</td><td>-</td><td>78.0</td><td>-</td></tr>
  <tr><td>Gemini Ultra</td><td>77.8</td><td>-</td><td>-</td><td>82.3</td><td>-</td></tr>
  <tr><td>Gemini Pro</td><td>71.2</td><td>-</td><td>-</td><td>74.6</td><td>-</td></tr>
  <tr><td>PALI-X</td><td>86.0$^\dagger$</td><td>-</td><td>70.9$^\dagger$</td><td>71.4$^\dagger$</td><td>-</td></tr>
  <tr><td>CogVLM-30B</td><td>83.4 (84.7$^\dagger$)</td><td>65.2$^\dagger$</td><td>76.4$^\dagger$</td><td>68.1 (69.3$^\dagger$)</td><td>92.7$^\dagger$</td></tr>
  <tr><td>LLaVA-1.5-13B</td><td>80</td><td>63.3</td><td>53.6</td><td>61.3$^*$</td><td>71.6</td></tr>
  <tr><td>LLaVA-1.6-Vicuna-7B</td><td>81.8</td><td>64.2</td><td>57.6</td><td>64.9</td><td>70.1</td></tr>
  <tr><td>LLaVA-1.6-Vicuna-13B</td><td>82.8</td><td>65.4</td><td>60.5</td><td>67.1</td><td>73.6</td></tr>
  <tr><td>LLaVA-1.6-Mistral-7B</td><td>82.2</td><td>64.8</td><td>60.0</td><td>65.7</td><td>72.8</td></tr>
  <tr><td>LLaVA-1.6-34B</td><td>83.7</td><td>67.1</td><td>63.8</td><td>69.5</td><td>81.8</td></tr>
</tbody></table>


<table>
  <tbody><tr><th>Data (PT)</th><th>Data (IT)</th><th>Model</th><th>MMMU (val)</th><th>MMMU (test)</th><th>MathVista</th><th>MMB-ENG</th><th>MMB-CN</th><th>MM-Vet</th><th>LLaVA-Wild</th><th>SEED-IMG</th><th>MME</th><th>POPE</th></tr>
  <tr><td>N/A</td><td>N/A</td><td>GPT-4V</td><td>56.8</td><td>55.7</td><td>49.9</td><td>75.8</td><td>73.9</td><td>67.6</td><td>-</td><td>71.6</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Ultra</td><td>59.4</td><td>-</td><td>53</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>N/A</td><td>N/A</td><td>Gemini Pro</td><td>47.9</td><td>-</td><td>45.2</td><td>73.6</td><td>74.3</td><td>64.3</td><td>-</td><td>70.7</td><td>-</td><td>-</td></tr>
   <tr><td>1.4B</td><td>50M</td><td>Qwen-VL-Plus</td><td>45.2</td><td>40.8</td><td>43.3</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr> 
  <tr><td>1.5B</td><td>5.12M</td><td>CogVLM-30B</td><td>32.1</td><td>30.1</td><td>-</td><td>-</td><td>-</td><td>56.8</td><td>-</td><td>-</td><td>-</td><td>-</td></tr>
  <tr><td>558K</td><td>665K</td><td>LLaVA-1.5-13B (336$^2$)</td><td>36.4</td><td>33.6</td><td>27.6</td><td>67.8</td><td>63.3</td><td>36.3</td><td>72.5</td><td>68.2</td><td>1531/295</td><td>85.93</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-Vicuna-7B (672$^2$)</td><td>35.8</td><td>-</td><td>34.6</td><td>67.4</td><td>60.6</td><td>43.9</td><td>81.6</td><td>70.2</td><td>1519/332</td><td>86.53</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-Mistral-7B (672$^2$)</td><td>35.3</td><td>-</td><td>37.7</td><td>68.7</td><td>61.2</td><td>47.3</td><td>83.2</td><td>72.2</td><td>1498/321</td><td>86.73</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-13B (672$^2$)</td><td>36.2</td><td>-</td><td>35.3</td><td>70</td><td>64.4</td><td>48.4</td><td>87.3</td><td>71.9</td><td>1575/326</td><td>86.23</td></tr>
  <tr><td>558K</td><td>760K</td><td>LLaVA-1.6-34B (672$^2$)</td><td>51.1</td><td>44.7</td><td>46.5</td><td>79.3</td><td>79</td><td>57.4</td><td>89.6</td><td>75.9</td><td>1631/397</td><td>87.73</td></tr>
</tbody></table>

</details>



<p>✨ Highlights:</p>
<ul>
  <li><strong>SoTA Performance!</strong> LLaVA-1.6 achieves the best performance compared with open-source LMMs such as <a href="https://github.com/THUDM/CogVLM" target="_blank">CogVLM</a> or <a href="https://huggingface.co/01-ai/Yi-VL-34B" target="_blank">Yi-VL</a>. Compared with commercial ones, it catches up to Gemini Pro and outperforms <a href="https://huggingface.co/spaces/Qwen/Qwen-VL-Plus" target="_blank">Qwen-VL-Plus</a> on selected benchmarks.</li>
  <li><strong>Zero-shot Chinese capability</strong>. LLaVA-1.6’s Chinese capability is an emerging zero-shot capability (i.e., only English multimodal data is considered). Its performance on Chinese multimodal scenarios is surprisingly good, e.g., SoTA on MMBench-CN.</li>
  <li><strong>Low Training Cost</strong>. LLaVA-1.6 is trained with 32 GPUs for ~1 day, with 1.3M data samples in total. The compute / training data cost is 100-1000 times smaller than others.</li>
</ul>

<h3 id="qualitative-results">Qualitative Results</h3>

<details>
<summary><b>Mark Zuckerberg talks about Llama-3 and 600K H100s</b></summary>

<div>

  <h4>User: Explain this to me in detail.</h4>

  <p><img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/zuck_llama3.jpg">

</p></div>

<div>

  <h4>LLaVA-1.6-34B</h4>

  <p>The image you've provided appears to be a screenshot of a social media post, specifically from Instagram, featuring a man speaking and a text overlay. The man in the image is wearing a black t-shirt and appears to be in the middle of a conversation or presentation. The text overlay is a quote from a post by a user named "zuck," which is presumably a reference to Mark Zuckerberg, the CEO of Meta (formerly known as Facebook).</p>

  <p>The text overlay reads:</p>

  <p>"Some updates on our AI efforts. Our long term vision is to build general intelligence, open source it responsibly, and make it widely available so everyone can benefit. We're bringing our two major AI research efforts (FAIR and GenAI) closer together to support this. We're currently training our next-gen model Llama 3, and we're building massive compute infrastructure to support our future roadmap, including 35k H100s by the end of this year — and overall almost 600k H100s equivalents of compute if you include other GPUs. Also really excited about our progress building new AI-centric computing devices like Ray Ban Meta smart glasses. Lots more to come."</p>

  <p>This text provides insights into Meta's artificial intelligence (AI) initiatives. It mentions that the company's long-term goal is to develop general intelligence and make it accessible to everyone. The post also discusses the integration of Meta's two AI research efforts, FAIR (Facebook AI Research) and GenAI, to support this goal.</p>

  <p>The text also mentions the training of a next-generation AI model called Llama 3 and the construction of massive compute infrastructure to support Meta's future plans. It highlights the company's investment in NVIDIA's H100 GPUs, with a total of 35,000 H100s by the end of the year and almost 600,000 H100s equivalents of compute if other GPUs are included.</p>

  <p>Lastly, the post expresses excitement about Meta's progress in developing AI-centric computing devices, specifically Ray Ban Meta smart glasses. The post concludes with a statement that there is more to come, suggesting that Meta has additional AI-related announcements or developments in the pipeline.</p>

  <p>The image of the man speaking could be from a video or a live stream where he is discussing these AI updates. The context of the image suggests that it is part of a larger conversation or announcement related to Meta's AI strategy and advancements.</p>

</div>

</details>

<details>
<summary><b>When to leave to pickup based on flight info?</b></summary>

<div>

  <h4>User: I need to pick up my wife. I live in San Jose. When should I leave?</h4>

  <p><img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/ewr_sfo_flight.png">

</p></div>

<div>

  <h4>LLaVA-1.6-34B</h4>

  <p>Based on the information provided in the image, the flight is scheduled to arrive at 11:51 AM at San Francisco International Airport (SFO). If you live in San Jose, you should consider the travel time between San Jose and San Francisco, which is approximately 45-60 minutes depending on traffic conditions.</p>

  <p>To ensure you have enough time to pick up your wife, you should leave San Jose no later than 11:00 AM to account for traffic and any unexpected delays. However, it's always a good idea to leave earlier than necessary to ensure you arrive on time.</p>

</div>

</details>


<h2 id="detailed-technical-improvement">Detailed Technical Improvement</h2>

<p>We detail the findings we have and improvements we make to LLaVA-1.5. <em>More implementation details will be released and documented here in the coming days.</em></p>

<h3 id="1-dynamic-high-resolution">(1) Dynamic High Resolution</h3>

<p>We design our model at high resolution with an aim to <strong>preserve its data efficiency</strong>.
When provided with high-resolution images and representations that preserve these details, the model’s capacity to perceive intricate details in an image is significantly improved. It reduces the model hallucination that conjectures the imagined visual content when confronted with low-resolution images. Our ‘AnyRes’ technique is designed to accommodate images of various high resolutions. We employ a grid configuration of $\{2 \times 2, 1 \times \{2,3,4\}, \{2,3,4\} \times 1\}$, balancing performance efficiency with operational costs. See our <a href="https://static.hliu.cc/files/llava/improved_llava.pdf">updated LLaVA-1.5 technical report</a> for more details.</p>

<p>
  <img width="90%" src="https://llava-vl.github.io/blog/assets/images/llava-1-6/high_res_arch_v2.png">
  <br>
  <em>Illustration of dynamic high resolution scheme: a grid configuration of $\\{2 \times 2\\}$</em>
</p>

<h3 id="2-data-mixture">(2) Data Mixture</h3>

<ul>
  <li><strong>High-quality User Instruct Data</strong>. Our definition of high-quality visual instruction-following data hinges on two principal criteria: First, the diversity of task instructions, ensuring adequately represent a broad spectrum of user intents that are likely to be encountered in real-world scenarios, particularly during the model’s deployment phase. Second, the superiority of responses is critical, with the objective of soliciting favorable user feedback. To achieve this, we consider two data sources: (1) Existing GPT-V data. <a href="https://huggingface.co/datasets/laion/gpt4v-dataset">LAION-GPT-V</a> and <a href="https://sharegpt4v.github.io//">ShareGPT-4V</a>. (2) To further facilitate better visual conversation for more scenarios, we collect a small 15K visual instruction tuning dataset covering different applications. The instructions and images come from <a href="https://llava-vl.github.io/">LLaVA demo</a>, which are real-world users requests. We carefully filter samples that may have privacy concerns or are potentially harmful, and generate the response with GPT-4V.</li>
  <li><strong>Multimodal Document/Chart Data</strong>. (1) We remove <a href="https://textvqa.org/textcaps/">TextCaps</a> from our training data as we realize that TextCaps uses the same set of training images as <a href="https://textvqa.org/">TextVQA</a>. This allows us to better understand the zero-shot OCR capability of our model when evaluating TextVQA during development. To maintain and further improve our model’s OCR capability, we replace TextCaps with DocVQA and SynDog-EN. (2) Motivated by <a href="https://huggingface.co/Qwen/Qwen-VL">Qwen-VL-7B-Chat</a>, we further add ChartQA, DVQA, and AI2D for better chart and diagram understanding.</li>
</ul>

<h3 id="3-scaling-llm-backbone">(3) Scaling LLM backbone</h3>

<p>In addition to Vicuna-1.5 (<a href="https://huggingface.co/lmsys/vicuna-7b-v1.5">7B</a> and <a href="https://huggingface.co/lmsys/vicuna-13b-v1.5">13B</a>), we consider more LLMs, including <a href="https://mistral.ai/news/announcing-mistral-7b/">Mistral-7B</a> and <a href="https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B">Nous-Hermes-2-Yi-34B</a>. These LLMs possess nice properties, flexible commercial use terms, strong bilingual support and larger language model capacity. It allows LLaVA to support a wider spectrum of users and more scenarios in the community. The LLaVA recipe works well with various LLMs, and scales up smoothly with the LLM up to 34B.</p>

<h2 id="model-card">Model Card</h2>

<table>
  <tbody><tr><th colspan="2">Name</th><th>LLaVA-1.6-7B</th><th>LLaVA-1.6-13B</th><th>LLaVA-1.6-34B</th></tr>
  <tr><th rowspan="4">Model Size</th><td>Total</td><td><b>7.06B</b></td><td><b>13.35B</b></td><td><b>34.75B</b></td></tr>
  <tr><td>Vision Encoder</td><td>303.5M</td><td>303.5M</td><td>303.5M</td></tr>
  <tr><td>Connector</td><td>21M</td><td>31.5M</td><td>58.7M</td></tr>
  <tr><td>LLM</td><td>6.74B</td><td>13B</td><td>34.39B</td></tr>
  <tr><th colspan="2">Resolution</th><td colspan="3">336 x [(2,2), (1,2), (2,1), (1,3), (3,1), (1,4), (4,1)]</td></tr>
  <tr><th>Stage-1</th><th>Training Data</th><td colspan="3">558K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Connector</td></tr>
  <tr><th>Stage-2</th><th>Training Data</th><td colspan="3">760K</td></tr>
  <tr><th></th><th>Trainable Module</th><td colspan="3">Full model</td></tr>
  <tr><th colspan="2">Compute (#GPU x #Hours)</th><td>8x20</td><td>16x24</td><td>32x30</td></tr>
  <tr><th colspan="2">Training Data (#Samples)</th><td colspan="3">1318K</td></tr>
</tbody></table>

<h2 id="team">Team</h2>



<h2 id="acknowledgement">Acknowledgement</h2>

<ul>
  <li>A16Z Open Source AI Grants Program.</li>
  <li>We thank Lianmin Zheng, Ying Sheng, Shiyi Cao for the integration of LLaVA to SGLang.</li>
  <li>This work was supported in part by NSF CAREER IIS2150012, Microsoft Accelerate Foundation Models Research, and Institute of Information &amp; communications Technology Planning &amp; Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).</li>
</ul>

<h2 id="citation">Citation</h2>

<div><pre><code><span>@misc</span><span>{</span><span>liu2024llava16</span><span>,</span>
    <span>title</span><span>=</span><span>{LLaVA-1.6: Improved reasoning, OCR, and world knowledge}</span><span>,</span>
    <span>url</span><span>=</span><span>{https://llava-vl.github.io/blog/2024-01-30-llava-1-6/}</span><span>,</span>
    <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae}</span><span>,</span>
    <span>month</span><span>=</span><span>{January}</span><span>,</span>
    <span>year</span><span>=</span><span>{2024}</span>
<span>}</span>

<span>@misc</span><span>{</span><span>liu2023improvedllava</span><span>,</span>
      <span>title</span><span>=</span><span>{Improved Baselines with Visual Instruction Tuning}</span><span>,</span> 
      <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae}</span><span>,</span>
      <span>publisher</span><span>=</span><span>{arXiv:2310.03744}</span><span>,</span>
      <span>year</span><span>=</span><span>{2023}</span><span>,</span>
<span>}</span>

<span>@misc</span><span>{</span><span>liu2023llava</span><span>,</span>
      <span>title</span><span>=</span><span>{Visual Instruction Tuning}</span><span>,</span> 
      <span>author</span><span>=</span><span>{Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae}</span><span>,</span>
      <span>publisher</span><span>=</span><span>{NeurIPS}</span><span>,</span>
      <span>year</span><span>=</span><span>{2023}</span><span>,</span>
<span>}</span>
</code></pre></div>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Differ: Tool for testing and validating transformed programs (109 pts)]]></title>
            <link>https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/</link>
            <guid>39205784</guid>
            <pubDate>Wed, 31 Jan 2024 16:33:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/">https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/</a>, See on <a href="https://news.ycombinator.com/item?id=39205784">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">

			
				
<article id="post-106148">
	<!-- .entry-header -->

	<div>
		<p><em>By Michael Brown</em></p>
<p>We recently released a new differential testing tool, called DIFFER, for finding bugs and soundness violations in transformed programs. DIFFER combines elements from differential, regression, and fuzz testing to help users find bugs in programs that have been altered by software rewriting, debloating, and hardening tools. We used DIFFER to evaluate 10 software debloating tools, and it discovered debloating failures or soundness violations in 71% of the transformed programs produced by these tools.</p>
<p>DIFFER fills a critical need in post-transformation software validation. Program transformation tools usually leave this task entirely to users, who typically have few (if any) tools beyond regression testing via existing unit/integration tests and fuzzers. These approaches do not naturally support testing transformed programs against their original versions, which can allow subtle and novel bugs to find their way into the modified programs.</p>
<p>We’ll provide some background research that motivated us to create DIFFER, describe how it works in more detail, and discuss its future.</p>
<p>If you prefer to go straight to the code, check out <a href="https://github.com/trailofbits/differ" target="_blank" rel="noopener">DIFFER on GitHub</a>.</p>
<p><a href="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?ssl=1"><img fetchpriority="high" decoding="async" data-attachment-id="106163" data-permalink="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/differ-tool-design-2/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=1000%2C820&amp;ssl=1" data-orig-size="1000,820" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Differ Tool Design (2)" data-image-description="" data-image-caption="" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=300%2C246&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?fit=690%2C566&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=690%2C566&amp;ssl=1" alt="" width="690" height="566" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?w=1000&amp;ssl=1 1000w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=300%2C246&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/Differ-Tool-Design-2.png?resize=768%2C630&amp;ssl=1 768w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></a></p>
<h2>Background</h2>
<p>Software transformation has been a hot research area over the past decade and has primarily been motivated by the need to secure legacy software. In many cases, this must be done without the software’s source code (binary only) because it has been lost, is vendor-locked, or cannot be rebuilt due to an obsolete build chain. Among the more popular research topics that have emerged in this area are binary lifting, recompiling, rewriting, patching, hardening, and debloating.</p>
<p>While tools built to accomplish these goals have demonstrated some successes, they carry significant risks. When compilers lower source code to binaries, they discard contextual information once it is no longer needed. Once a program has been lowered to binary, the contextual information necessary to safely modify the original program generally cannot be fully recovered. As a result, tools that modify program binaries directly may inadvertently break them and introduce new bugs and vulnerabilities.</p>
<p>While DIFFER is application-agnostic, we originally built this tool to help us find bugs in programs that have had unnecessary features removed with a debloating tool (e.g., <a href="https://github.com/michaelbrownuc/CARVE" target="_blank" rel="noopener">Carve</a>, <a href="https://github.com/ashish-gehani/Trimmer" target="_blank" rel="noopener">Trimmer</a>, <a href="https://github.com/cxreet/razor" target="_blank" rel="noopener">Razor</a>). In general, software debloaters try to minimize a program’s attack surface by removing unnecessary code that may contain latent vulnerabilities or be reused by an attacker using code-reuse exploit patterns. Debloating tools typically perform an analysis pass over the program to map features to the code necessary to execute them. These mappings are then used to cut code that corresponds to features the user doesn’t want. However, these cuts will likely be imprecise because generating the mappings relies on imprecise analysis steps like binary recovery. As a result, new bugs and vulnerabilities can be introduced into debloated programs during cutting, which is exactly what we have designed DIFFER to detect.</p>
<h2>How does DIFFER work?</h2>
<p>At a high level, DIFFER (shown in figure 1) is used to test an unmodified version of the program against one or more modified variants of the program. DIFFER allows users to specify seed inputs that correspond to both unmodified and modified program behaviors and features. It then runs the original program and the transformed variants with these inputs and compares the outputs. Additionally, DIFFER supports template-based mutation fuzzing of these seed inputs. By providing mutation templates, DIFFER can maximize its coverage of the input space and avoid missing bugs (i.e., false negatives).</p>
<p>DIFFER expects to see the same outputs for the original and variant programs when given inputs that correspond to unmodified features. Conversely, it expects to see different outputs when it executes the programs with inputs corresponding to modified features. If DIFFER detects unexpected matching, differing, or crashing outputs, it reports them to the user. These reports help the user identify errors in the modified program resulting from the transformation process or its configuration.</p>
<div id="attachment_106156"><p><a href="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?ssl=1"><img decoding="async" aria-describedby="caption-attachment-106156" data-attachment-id="106156" data-permalink="https://blog.trailofbits.com/2024/01/31/introducing-differ-a-new-tool-for-testing-and-validating-transformed-programs/differ-overview/" data-orig-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=1999%2C1480&amp;ssl=1" data-orig-size="1999,1480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="DIFFER Overview" data-image-description="" data-image-caption="<p>Figure 1: Overview of DIFFER</p>
" data-medium-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=300%2C222&amp;ssl=1" data-large-file="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?fit=690%2C511&amp;ssl=1" src="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=690%2C511&amp;ssl=1" alt="" width="690" height="511" srcset="https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?w=1999&amp;ssl=1 1999w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=300%2C222&amp;ssl=1 300w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1024%2C758&amp;ssl=1 1024w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=768%2C569&amp;ssl=1 768w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1536%2C1137&amp;ssl=1 1536w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?resize=1200%2C888&amp;ssl=1 1200w, https://i0.wp.com/blog.trailofbits.com/wp-content/uploads/2024/01/DIFFER-Overview.png?w=1380&amp;ssl=1 1380w" sizes="(max-width: 690px) 100vw, 690px" data-recalc-dims="1"></a></p><p id="caption-attachment-106156">Figure 1: Overview of DIFFER</p></div>
<p>When configuring DIFFER, the user selects one or more comparators to use when comparing outputs. While DIFFER provides many built-in comparators that check basic outputs such as return codes, console text, and output files, more advanced comparators are often needed. For this purpose, DIFFER allows users to add custom comparators for complex outputs like packet captures. Custom comparators are also useful for reducing false-positive reports by defining allowable differences in outputs (such as timestamps in console output). Our open-source release of DIFFER contains many useful comparator implementations to help users easily write their own comparators.</p>
<p>However, DIFFER does not and cannot provide formal guarantees of soundness in transformation tools or the modified programs they produce. Like other dynamic analysis testing approaches, DIFFER cannot exhaustively test the input space for complex programs in the general case.</p>
<h2>Use case: evaluating software debloaters</h2>
<p>In a recent <a href="https://arxiv.org/abs/2312.13274" target="_blank" rel="noopener">research study</a> we conducted in collaboration with our friends at GrammaTech, we used DIFFER to evaluate debloated programs created by 10 different software debloating tools. We used these tools to remove unnecessary features from 20 different programs of varying size, complexity, and purpose. Collectively, the tools created 90 debloated variant programs that we then validated with DIFFER. DIFFER discovered that 39 (~43%) of these variants still had features that debloating tools failed to remove. Even worse, DIFFER found that 25 (~28%) of the variants either crashed or produced incorrect outputs in retained features after debloating.</p>
<p>By discovering these failures, DIFFER has proven itself as a useful post-transformation validation tool. Although this study was focused on debloating transformations, we want to emphasize that DIFFER is general enough to test other transformation tools such as those used for software hardening (e.g., CFI, stack protections), translation (e.g., C-to-Rust transformers), and surrogacy (e.g., ML surrogate generators).</p>
<h2>What’s next?</h2>
<p>With DIFFER now available as open-source software, we invite the security research community to use, extend, and help maintain DIFFER via pull requests. We have several specific improvements planned as we continue to research and develop DIFFER, including the following:</p>
<ul>
<li>Support running binaries in Docker containers to reduce environmental burdens.</li>
<li>Add new built-in comparators.</li>
<li>Add support for targets that require superuser privileges.</li>
<li>Support monitoring multiple processes that make up distributed systems.</li>
<li>Add runtime comparators (via instrumentation, etc.) for “deep” equivalence checks.</li>
</ul>
<h3>Acknowledgements</h3>
<p>This material is based on work supported by the Office of Naval Research (ONR) under Contract No. N00014-21-C-1032. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the author and do not necessarily reflect the views of the ONR.</p>

			</div><!-- .entry-content -->

	
</article><!-- #post-106148 -->
						<!-- #nav-below -->
		
					<!-- #comments .comments-area -->

			
			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Y Combinator CEO Garry Tan's online rant spurs threats to supes, police reports (408 pts)]]></title>
            <link>https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/</link>
            <guid>39205676</guid>
            <pubDate>Wed, 31 Jan 2024 16:26:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/">https://missionlocal.org/2024/01/y-combinator-ceo-garry-tans-online-rant-spurs-threat-to-supe-police-reports/</a>, See on <a href="https://news.ycombinator.com/item?id=39205676">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

		
		
<p>At least two San Francisco supervisors received a threatening letter at their homes on Tuesday and two more filed police reports after Garry Tan, the CEO of Y Combinator and a heavy campaign donor to efforts to oust progressive politicians, <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">posted online that seven supervisors should die a slow death</a>.</p>



<p>Tan wrote “Die slow motherfuckers” in reference to Supervisors Aaron Peskin, Connie Chan, Myrna Melgar, Shamann Walton, Hillary Ronen, Dean Preston and Ahsha Safaí in a <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">late-night rant Saturday</a>. During his online tirade, Tan <a href="https://missionlocal.org/2024/01/garry-tan-death-wish-sf-supervisors/">posted photos of his private liquor stash, and indicated to a fellow Twitter-user that he was inebriated</a>. </p>



<p>The “die slow, motherfucker” line was a reference to a Tupac Shakur song, and Tan later apologized. That 1996 song, “<a href="https://www.youtube.com/watch?v=41qC3w3UUkU">Hit ‘Em Up</a>,” escalated the simmering East Coast-West Coast rap rivalry into a lethal feud; Shakur was gunned down three months after its release.&nbsp;</p>




<p>The named supervisors did not take the incident lightly:<a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php"> Peskin and Chan</a> on Tuesday <a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php">filed police reports</a>, Melgar and Safaí have pledged to do the same and both Peskin and Preston today received a letter to their homes citing Tan’s diatribe and wishing death upon them and their families.</p>



<p>When asked if she felt personally threatened by Tan’s behavior, Chan responded “Seeing what my colleague[s] received in the mail? Yes, absolutely. I have a 10-year-old. I do not tell people where my child attends school.”</p>



<p><a href="https://www.sfchronicle.com/sf/article/sf-garry-tan-x-threatening-peskin-police-report-18637813.php">The <em>San Francisco Chronicle</em> first reported </a>the hate mail sent to Preston and the police reports filed by Chan and Peskin. Peskin at 8:40 p.m. confirmed to Mission Local that he, too, received a threatening letter at his home. The envelope was emblazoned with a smiling portrait of Tan.</p>



<p>“Garry Tan is right!” the letter sent to Peskin, Preston and, perhaps, others read. “I wish a slow and painful death for you and your loved ones.”</p>



<figure><img decoding="async" width="853" height="640" src="https://missionloca.s3.amazonaws.com/mission/wp-content/uploads/2024/01/IMG_0761-853x640.jpeg" alt="Gary Tan is the right poster."><figcaption>The letter received today by Supervisors Aaron Peskin and Dean Preston</figcaption></figure>



<p>Preston has in particular drawn the venom of Tan: Preston is the lone democratic socialist on the Board of Supervisors, an odious position to Tan, and Tan has donated $5,000 to Preston’s opponent and <a href="https://sfstandard.com/2023/07/31/dump-dean-preston-pac-growsf-declares-war-sf-supervisor-election/">pledged</a> another $50,000 to the effort to unseat him.</p>




<p>Bizarrely, the letter to Preston and Peskin concluded: “This mail was sent to communicate a political opinion. No threats were intended.”</p>



<p>The board’s five Jewish members —&nbsp;Peskin, Ronen, Melgar, Preston and Rafael Mandelman — <a href="https://sfstandard.com/2023/10/20/sickening-antisemitic-postcard-mailed-to-homes-of-5-san-francisco-supervisors/">in October received antisemitic postcards</a> at their homes. Peskin said multiple supervisors have received as many as four more antisemitic letters or postcards since then. </p>



<p>He noted that the exact phrase “This mail was sent to communicate a political opinion. No threats were intended” was written on one of the disturbing communiques he received earlier.&nbsp;</p>



<p>Chan says she received a letter in December at her home accusing her of “white genocide.” Chan, who is Chinese, is married to a white man and has&nbsp;a mixed-race child.&nbsp;</p>



<p>While she first thought to “brush off” Tan’s comments, Chan says she was initially unaware how tied in he is politically. She then learned that Tan has hosted numerous moderate supervisor candidates at his home, including her major challenger, Marjan Philhour.</p>




<p>Chan, in her police report, notes that she had filed a prior report last year against Philhour’s staffer, Forrest Liu, whom she accuses of twice physically threatening her — publicly and in person.</p>



<p>After realizing the association of Tan to Philhour to Liu, she said “it then dawned on me that these unhinged individuals who have wished me death online as well as physically threaten me are connected, it became alarming to me that the threats are potentially violent and imminent.” </p>



<p>Liu did not respond to requests for comment. </p>



<p>Tan has indeed hosted San Francisco moderate political figures at his Mission District home, including District Attorney Brooke Jenkins, Philhour and District 9 candidate Trevor Chandler.</p>



<p>Philhour and Chandler both criticized their backer Tan in light of his comments.</p>



<p>“Threats to one’s life or safety have no place in political discourse,” Philhour said.</p>



<p>“It is never okay to advocate violence, be it seriously or in jest,” said Chandler.&nbsp;</p>



<p>Jenkins did not respond to requests for comment.</p>



<p>Peskin today asked the City Attorney’s Office to look into requiring public disclosures from recipients of political donations from “purveyors of hate and violence.” </p>



<p>“When individuals in our society, especially and most concerning those with money,<br>power and influence, call for public servants to be tortured or killed, it is crossing a bright line — and must not be condoned by society,” Peskin said today in Board chambers. “To ignore it is dangerous and a disservice to democracy.”</p>



<p>But tying this potential legislation to the message Tan communicated to his 408,000 Twitter followers would appear to be a serious legal challenge: Half a dozen lawyers and judges told Mission Local that, however ill-advised, Tan’s comments do not rise to the legal definition of a death threat.&nbsp;</p>



<p><a href="https://leginfo.legislature.ca.gov/faces/codes_displaySection.xhtml?lawCode=PEN&amp;sectionNum=422.">Under Penal Code 422</a>, a person making a criminal threat must harbor “specific intent that the statement, made verbally, in writing, or by means of an electronic communication device, is to be taken as a threat…”&nbsp;</p>



<p>“It is offensive, but it is speech protected by the First Amendment,” said Berkeley School of Law dean Erwin Chemerinsky. “It does not meet the standard for incitement.”&nbsp;</p>



<p>Tan is a well-heeled donor for San Francisco’s moderate causes and candidates. He sits on the board of Grow SF, a political pressure group favoring moderate causes and candidates and targeting progressives. Tan gave more than $100,000 to the <a href="https://missionlocal.org/2022/05/explore-almost-9-million-donated-for-and-against-boudin-recall/">2022 campaign to recall then-District Attorney Chesa Boudin</a>. He gave at least $20,000 to the <a href="https://missionlocal.org/2022/01/who-is-funding-the-school-board-recall/">2021 school board recall</a>, too.</p>



<p>Tan could not be reached for comment. Neither Y Combinator nor Grow SF immediately responded to requests for comment.</p>


	<div data-posts="" data-current-post-id="581806">
							<h2>
					<span>See Also</span>
				</h2>
						
	<article data-post-id="581443">
							<figure>
								
				
							</figure><!-- .featured-image -->
		
		<!-- .entry-wrapper -->
	</article>

				</div>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Infinite Craft (872 pts)]]></title>
            <link>https://neal.fun/infinite-craft/</link>
            <guid>39205020</guid>
            <pubDate>Wed, 31 Jan 2024 15:34:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://neal.fun/infinite-craft/">https://neal.fun/infinite-craft/</a>, See on <a href="https://news.ycombinator.com/item?id=39205020">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-v-25c6935a="" id="__nuxt" data-server-rendered="true"><p><a href="https://neal.fun/" data-v-25c6935a=""><img src="https://neal.fun/general/logo.svg" data-v-25c6935a=""></a></p><p><img src="https://neal.fun/infinite-craft/clear.svg" data-v-25c6935a=""> <img src="https://neal.fun/infinite-craft/sound.svg" data-v-25c6935a=""></p> <canvas data-v-25c6935a=""></canvas> <div data-v-25c6935a=""><div data-v-25c6935a=""><p><span data-v-25c6935a="">💧</span>
Water
</p><p><span data-v-25c6935a="">🔥</span>
Fire
</p><p><span data-v-25c6935a="">🌬️</span>
Wind
</p><p><span data-v-25c6935a="">🌍</span>
Earth
</p> <p>Drag elements to craft</p></div> <div data-v-25c6935a=""><p><img src="https://neal.fun/infinite-craft/sort.svg" data-v-25c6935a="">
Sort by time
</p></div></div> <div data-v-25c6935a=""><p><span data-v-25c6935a="">💧</span>
Water
</p></div>  <p><img src="https://neal.fun/infinite-craft/sound.svg" data-v-25c6935a=""></p> <p><img src="https://neal.fun/infinite-craft/logo.svg" data-v-25c6935a=""></p><div data-v-25c6935a=""><div data-v-25c6935a=""><p><span data-v-25c6935a="">💧</span>
Water
</p></div><div data-v-25c6935a=""><p><span data-v-25c6935a="">🔥</span>
Fire
</p></div><div data-v-25c6935a=""><p><span data-v-25c6935a="">🌬️</span>
Wind
</p></div><div data-v-25c6935a=""><p><span data-v-25c6935a="">🌍</span>
Earth
</p></div></div> <p>
Tap two elements to craft
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Adobe gives up on web-design product to rival Figma after deal collapse (223 pts)]]></title>
            <link>https://www.bloomberg.com/news/articles/2024-01-30/adobe-gives-up-on-xd-after-figma-deal-collapse</link>
            <guid>39204955</guid>
            <pubDate>Wed, 31 Jan 2024 15:29:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bloomberg.com/news/articles/2024-01-30/adobe-gives-up-on-xd-after-figma-deal-collapse">https://www.bloomberg.com/news/articles/2024-01-30/adobe-gives-up-on-xd-after-figma-deal-collapse</a>, See on <a href="https://news.ycombinator.com/item?id=39204955">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
    <section>
        <h3>Why did this happen?</h3>
        <p>Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our <a href="https://www.bloomberg.com/notices/tos">Terms of
                Service</a> and <a href="https://www.bloomberg.com/notices/tos">Cookie Policy</a>.</p>
    </section>
    <section>
        <h3>Need Help?</h3>
        <p>For inquiries related to this message please <a href="https://www.bloomberg.com/feedback">contact
            our support team</a> and provide the reference ID below.</p>
        <p>Block reference ID:</p>
    </section>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Far Side – By Gary Larson (322 pts)]]></title>
            <link>https://www.thefarside.com/</link>
            <guid>39204918</guid>
            <pubDate>Wed, 31 Jan 2024 15:27:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.thefarside.com/">https://www.thefarside.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39204918">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

      <p><img data-src="https://assets.thefarside.com/packs/media/images/brand/logo/farworks_amoeba-8d3feb7d97c372bd52b1f53edd7c1d61.png" data-width="678" data-height="922" data-timing="300" alt="FarWorks, Inc. Registered logo" src="https://assets.thefarside.com/packs/media/images/brand/logo/farworks_amoeba-8d3feb7d97c372bd52b1f53edd7c1d61.png">
      </p>

      <div>
        <p>Website hosted and operated by Andrews McMeel Universal.</p>
<p>Copyright © 2019-2024 FarWorks, Inc. All rights reserved. Used with permission.</p>
<p><em>The Far Side</em>®, <em>FarWorks, Inc.</em>®, <em>Tales From The Far Side</em>®, the <em>Larson</em>® signature, the Amoeba® logo, and the Cow® logo are registered trademarks of FarWorks, Inc. in certain countries.</p>
<p>No images or other content displayed on this Website may be reproduced, digitized, stored in a retrieval system, made available via any computer or wireless networks, transmitted or circulated in any form or by any means, without prior written permission of the copyright owner.</p>
<p><span><a href="https://www.thefarside.com/terms">Terms of Service</a> | <a href="https://www.thefarside.com/privacy-policy">Privacy Policy</a></span> | <a id="compliance_link" target="_blank" rel="noopener" href="https://www.andrewsmcmeel.com/privacy-requests?referrer=https://www.thefarside.com">Do Not Sell or Share my Personal Information</a>

      </p></div>


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[RavenDB 6.0.2 (A Jepsen Report) (175 pts)]]></title>
            <link>https://jepsen.io/analyses/ravendb-6.0.2</link>
            <guid>39204622</guid>
            <pubDate>Wed, 31 Jan 2024 15:04:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jepsen.io/analyses/ravendb-6.0.2">https://jepsen.io/analyses/ravendb-6.0.2</a>, See on <a href="https://news.ycombinator.com/item?id=39204622">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><a href="https://ravendb.net/">RavenDB</a> is a document database which claims to offer ACID transactions, including Snapshot Isolation by default and Serializability with the strongest settings. Following the documentation’s claim that a session “represents a single business transaction,” we tested RavenDB 6.0.2 and found surprising behavior even in healthy, single-node clusters. Transactions lose updates by default. Both the optimistic concurrency and cluster-wide transaction modes allow fractured read: a serious anomaly forbidden by Snapshot Isolation and several weaker consistency models. Alternatively, RavenDB may not have interactive transactions at all. This work was performed independently without compensation, and conducted in accordance with the <a href="https://jepsen.io/ethics">Jepsen ethics policy</a>.</p><article>
  <div>
<h2 data-number="1" id="background"> Background</h2>
<p><a href="https://ravendb.net/">RavenDB</a> is a distributed document database which <a href="https://ravendb.net/why-ravendb">repeatedly advertises</a> its support for ACID transactions.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> It’s intended for OLTP workloads, and offers a variety of <a href="https://ravendb.net/docs/article-page/6.0/csharp/server/ongoing-tasks/etl/basics">ETL paths</a> for exporting data to other systems. Its transactional API revolves around a <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/what-is-a-session-and-how-does-it-work">session</a> handle, which “represents a single business transaction on a particular database.” Users create a session, perform operations like reads and writes, and finally call <code>session.saveChanges()</code> to commit their writes as an atomic unit.</p>
<p>RavenDB can <a href="https://ravendb.net/why-ravendb/high-availability">replicate data across a set of nodes with automated failover</a>. Sharding is either a <a href="https://issues.hibernatingrhinos.com/issue/RavenDB-8115">work in progress</a> or <a href="https://ravendb.net/features/clusters/sharding">ready in 6.0</a>, depending on which part of the documentation you’re reading. RavenDB includes secondary indices wrapped with a homegrown <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/querying/how-to-query">query language</a>, <a href="https://ravendb.net/docs/article-page/6.0/csharp/document-extensions/revisions/overview">multiple revisions of documents</a>, <a href="https://ravendb.net/docs/article-page/6.0/csharp/document-extensions/timeseries/overview">time series</a> datatypes, and <a href="https://crdt.tech/">CRDT</a>-based <a href="https://ravendb.net/why-ravendb/multi-model">counters</a>. In this text, we’ll focus on RavenDB’s transactional key-value operations.</p>
<h2 data-number="1.1" id="replication"> Replication</h2>
<p>Per RavenDB’s <a href="https://ravendb.net/why-ravendb/high-availability">High Availability</a> page, the database accepts writes and reads across all nodes in the cluster. It uses the <a href="https://raft.github.io/">Raft</a> consensus algorithm, which should theoretically allow RavenDB to provide consistency models up to <a href="https://jepsen.io/consistency/models/strict-serializable">Strong Serializability</a>. However, that page goes on to say operators can “easily setup a topology in which end points operate … independently in case the network is disrupted.” In a blog post, CEO Oren Eini repeats this claim:</p>
<blockquote>
<p>If a node is located in a place where the internet connectivity goes down, that node can continue to operate offline, taking in data locally. Once the connection is restored, the node will take the data it processed and replicate it throughout your cluster.</p>
</blockquote>
<p>This would make ACID transactions impossible. The “I” in ACID refers to “Isolation”: transactions must appear to execute independently, without interference from other transactions. This property is formalized as <a href="https://jepsen.io/consistency/models/serializable">Serializability</a>: equivalence to some totally ordered, non-concurrent execution of transactions. We know that totally available systems <a href="https://jepsen.io/consistency">cannot offer</a> Serializability or even <a href="https://jepsen.io/consistency/models/snapshot-isolation">Snapshot Isolation</a>. RavenDB might offer <a href="https://jepsen.io/consistency/models/causal">Causal</a> or <a href="https://jepsen.io/consistency/models/read-committed">Read Committed</a>, but the stronger consistency models are theoretically off-limits.</p>
<p>A <a href="https://ravendb.net/features/clusters/high-availability">second page on high availability</a> explains that there are two layers within RavenDB, and that Raft is used only for cluster metadata:</p>
<blockquote>
<ul>
<li><p>First, the cluster layer is managed by a consensus protocol called Raft. In CAP theorem it is CP (consistent and partition tolerant).</p></li>
<li><p>The second layer, the database layer, is AP (it is always available, even if there is a partition, and it’s eventually consistent) and is handled by a gossip protocol between the databases on different nodes, forming multi-master mesh and replicating data between each other.</p></li>
</ul>
<p>RavenDB utilizes the different layers for different purposes. At the cluster layer, the consensus protocol ensures that operators have the peace of mind of knowing that their commands are accepted and followed. At the database layer you know that RavenDB will never lose writes and will always keep your data safe.</p>
</blockquote>
<p>This is also confusing. AP systems are known for availability, not safety; lost update is a well-understood problem in AP registers. RavenDB claims to offer transactions with ACID guarantees. However, these transactions are apparently routed through an eventually-consistent, totally available replication system. There are <a href="https://docs.datomic.com/pro/getting-started/brief-overview.html">databases which couple</a> an (e.g.) Sequential transaction coordinator to an eventually-consistent datastore to provide Serializability, but it’s not clear from this documentation how RavenDB links cluster and database layers together to ensure safety.</p>
<p>The <a href="https://ravendb.net/learn/inside-ravendb-book/reader/4.0/6-ravendb-clusters">Inside RavenDB chapter on cluster design</a> confirms that Raft is used only for cluster metadata. Writes are allowed on every node, and are totally available:</p>
<blockquote>
<p>RavenDB uses multi-master replication inside a database, and it’s always able to accept writes.</p>
<p>In other words, even if the majority of the cluster is down, as long as a single node is available, we can still process reads and writes.</p>
</blockquote>
<p>On the other hand, RavenDB’s <a href="https://ravendb.net/articles/acid-transactions-in-nosql-ravendb-vs-mongodb">ACID Transactions in NoSQL</a> post claims the opposite:</p>
<blockquote>
<p>As in the single node version, RavenDB commits a transaction with just one round of Raft consensus.</p>
</blockquote>
<p>If Raft <em>is</em> involved in the transactions, RavenDB can offer up to Strong Serializability—but transactions cannot be totally available. Indeed, RavenDB’s <a href="https://ravendb.net/docs/article-page/6.0/Csharp/server/clustering/cluster-transactions">clustering documentation</a> clarifies there are actually two separate transaction paths. The default mode is called a <em>single-node</em> transaction, which allows conflicts “when two clients try to modify the same set of documents on two different database nodes.” A <em>cluster-wide transaction</em> uses Raft to prevent conflicts, allowing transactions to “favor consistency over availability.” To execute a cluster-wide transaction, one <a href="https://ravendb.net/docs/article-page/6.0/java/client-api/session/saving-changes#transaction-mode---cluster-wide">must set</a> <code>TransactionMode = CLUSTER_WIDE</code>.</p>
<p>What safety properties do these transaction paths guarantee? For this, we need to consider RavenDB’s ACID claims in detail.</p>
<h2 data-number="1.2" id="acid"> ACID</h2>
<p>RavenDB’s <a href="https://ravendb.net/why-ravendb">home page</a> prominently advertises “ACID database transactions” “across multiple documents and across your entire cluster.” Its <a href="https://ravendb.net/why-ravendb/acid-transactions">ACID Database Transactions</a> page explains that a database without transactions is “not much of a database.” It boasts that RavenDB “guarantee[s] ACID without sacrificing performance” and notes that because of its distributed ACID guarantees, “developers are exempt from handling the numerous scenarios of partial data transfers and the intricacies of data storage.”</p>
<p>RavenDB’s <a href="https://ravendb.net/articles/acid-transactions-in-nosql-ravendb-vs-mongodb">ACID Transactions in NoSQL</a> article explains that RavenDB “was capable of multi-document transactions since version 1.0”:</p>
<blockquote>
<p>Because it was optimized with this in mind, there wasn’t even a need for a non-ACID option. Any combination of database operations can be combined into an ACID transaction. As a user you never needed to implement ACID guarantees yourself, and you were free to design documents around your own requirements….</p>
<p>RavenDB was designed to make one and only one round trip to the server per transaction. RavenDB’s version of the session object tracks a series of commands, collects them as a batch, and sends them all to the server in a single round-trip when the method <code>session.saveChanges()</code> is called.</p>
</blockquote>
<p>Again, <a href="https://ravendb.net/articles/acid-cluster-distributed-nonrelational-database">RavenDB claims</a> to have been “the pioneer database to offer ACID in a nonrelational context. In 2010, RavenDB offered ACID consistency across multiple documents.” However, these guarantees held only on a single node: concurrent clients on different nodes could violate isolation. RavenDB 4.0, released in fall 2020, introduced the cluster-wide transaction path, which took transactions “from being ACID over multiple documents to being ACID over your entire cluster.”</p>
<p>The <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/faq/transaction-support">Transaction FAQ</a> says “all actions performed on documents are fully ACID” but contradicts itself immediately, saying “in a single transaction, all operations operate under snapshot isolation.” <a href="https://dbdb.io/db/ravendb">DBDB</a> takes this to mean that RavenDB offers Snapshot Isolation by default.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> In a <a href="https://www.youtube.com/watch?v=5ZXBR3croMA&amp;t=39m">2020 webinar</a>, CEO Oren Eini confirmed this position: “RavenDB uses Snapshot Isolation by default, and transactions are effectively going to observe Serializable between operations that happen on the same node.”</p>
<p>There are hints that RavenDB might provide something much weaker than Snapshot Isolation. Buried in the <em>Inside RavenDB</em> book, in the chapter on document modeling, is a <a href="https://ravendb.net/learn/inside-ravendb-book/reader/4.0/3-document-modeling#concurrency-control">section on concurrency control</a>. This section explains that RavenDB (at least in version 4.0) performed no concurrency control, and instead used Last Write Wins conflict resolution by default.</p>
<blockquote>
<p>What happens if two requests are trying to modify the same document at the same time? That depends on what, exactly, you asked RavenDB to do. If you didn’t do anything, RavenDB will execute those two modifications one at a time, and the last one will win. There’s no way to control which would be last. Note that both operations will execute.</p>
</blockquote>
<p>This is the <em>opposite</em> of ACID isolation. Isolated transactions appear to execute sequentially, not concurrently. It also contradicts claims of Snapshot Isolation: Last Write Wins registers allow all kinds of anomalies which would be prohibited under Snapshot Isolation, including <a href="https://dzone.com/articles/conflict-resolution-using-last-write-wins-vs-crdts">lost update</a>. However, this book is two major releases out of date; it may not apply to 6.0.2.</p>
<p>What about cluster-wide transactions? The <a href="https://ravendb.net/docs/article-page/6.0/java/server/clustering/cluster-transactions#concurrent-cluster-wide-and-single-node-transactions">Cluster Transactions</a> page seems definitive. “Concurrent cluster-wide transactions are guaranteed to appear as if they are run one at a time (<code>serializable</code> isolation level).”<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>From this, Jepsen infers that RavenDB’s default transaction settings should ensure Snapshot Isolation by default and Serializability in a single-node system. Cluster-wide transactions should ensure Serializability globally.</p>
<h2 data-number="2" id="test-design"> Test Design</h2>
<p>In 2020 RavenDB <a href="https://github.com/ml054/jepsen/">wrote their own Jepsen test</a> and declared in <a href="https://www.youtube.com/watch?v=5ZXBR3croMA&amp;">a webinar</a> that per that test, “everything works.” Their test checked the linearizability of individual <a href="https://github.com/ml054/jepsen/blob/08960da98ab9bf7959fce014c63b146b903cbe6c/ravendb/src/jepsen/ravendb.clj#L164-L196">reads and writes</a> against a single document. It did not evaluate multi-operation or multi-document transactions.<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>We designed a <a href="https://github.com/jepsen-io/ravendb/tree/4431310402f334ffdb18a5a7ec819316847b642b">new test harness</a> for RavenDB 6.0.2 running on a single Debian Bookworm node. Our test used RavenDB’s JVM client library at version 5.0.4. We did not evaluate multi-node clusters or any kind of faults.</p>
<p>We wrote a single list-append workload using <a href="https://github.com/jepsen-io/elle">Elle</a> to verify transactional isolation. This workload performs transactions over lists, each list identified by a unique integer ID. Each transaction consists of reads and/or appends of unique integers to those lists. Each worker thread in the test <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/client.clj#L26-L31C15">opens a single DocumentStore</a> connected to the same node. Each transaction creates a new <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/client.clj#L54-L62">session</a>, performs <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/append.clj#L25-L41">reads and/or appends</a>, then calls <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/client.clj#L90"><code>session.saveChanges()</code></a> to commit. Reads are encoded as a <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/append.clj#L26-L28">single call</a> to <code>session.load(java.util.Map, id)</code>. Appends call <code>session.load</code> to read the current value, add their integer element to the end of the list, then call <a href="https://github.com/jepsen-io/ravendb/blob/8315d6053bf022203a24db74812d2fdfe89b56b7/src/jepsen/ravendb/append.clj#L40"><code>session.store(map, key)</code></a>.</p>
<p>RavenDB offers a few knobs for tuning transaction safety: <code>transactionMode</code> and <code>optimisticConcurrency</code>. We ran our tests using the defaults (single-node transactions, no optimistic concurrency), with single-node transactions and optimistic concurrency, and finally with cluster-wide transactions. Cluster-wide transactions cannot be combined with optimistic concurrency.</p>
<h2 data-number="3" id="results"> Results</h2>
<p>We found surprising safety errors in all three transaction modes.</p>
<h2 data-number="3.1" id="lost-update-with-single-node-transactions-17927"> Lost Update with Single-Node Transactions (#17927)</h2>
<p>By default, RavenDB executes transactions with <code>transactionMode = SINGLE_NODE</code> and <code>optimisticConcurrency = false</code>. One might assume that <code>SINGLE_NODE</code> transactions are safe on single-node clusters. However, we found the default settings caused RavenDB to lose updates constantly, even in single-node clusters without faults.</p>
<p>For instance, in this <a href="https://github.com/ravendb/ravendb/files/13794868/20231229T102201.960-0600.zip">five-second test run</a> we performed 12,886 transactions over 975 keys. 81 of those keys exhibited a provable lost update. Here are two committed transactions involving key 830:</p>
<div id="cb1"><pre><code><span id="cb1-1">[[<span>:r</span> <span>830</span> [<span>1</span> <span>2</span>]]</span>
<span id="cb1-2"> [<span>:append</span> <span>824</span> <span>11</span>]</span>
<span id="cb1-3"> [<span>:append</span> <span>807</span> <span>14</span>]</span>
<span id="cb1-4"> [<span>:append</span> <span>830</span> <span>3</span>]]</span>
<span id="cb1-5"></span>
<span id="cb1-6">[[<span>:r</span> <span>830</span> [<span>1</span> <span>2</span>]]</span>
<span id="cb1-7"> [<span>:r</span> <span>831</span> <span>nil</span>]</span>
<span id="cb1-8"> [<span>:r</span> <span>831</span> <span>nil</span>]</span>
<span id="cb1-9"> [<span>:append</span> <span>830</span> <span>4</span>]]}]}</span></code></pre></div>
<p>Both of these transactions read key 830’s value as the list <code>[1, 2]</code>. Both went on to append a value to key 830: the first transaction appended <code>3</code>, and the second transaction appended <code>4</code>. Neither saw the other’s effects. In a Snapshot Isolated system, the <em>first-committer-wins</em> rule demands that one of these transactions must abort. RavenDB, however, allowed both transactions to commit. This is the definition of a lost update anomaly.</p>
<p>In an isolated transaction system which only ever appends elements to lists, every observed version of a single list must be a prefix of the longest version of that list. However, 454 of the keys in this test violated this prefix property, exhibiting <em>incompatible orders</em>. For example, here are all the reads of key 116:</p>
<table>
<thead>
<tr>
<th>Time (s)</th>
<th>Process</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.01</td>
<td>1</td>
<td>[1 2]</td>
</tr>
<tr>
<td>3.01</td>
<td>1</td>
<td>[1 2]</td>
</tr>
<tr>
<td>3.01</td>
<td>0</td>
<td>[1 2]</td>
</tr>
<tr>
<td>3.01</td>
<td>1</td>
<td>[1 2 3]</td>
</tr>
<tr>
<td>3.01</td>
<td>0</td>
<td>[1 2 4]</td>
</tr>
<tr>
<td>3.01</td>
<td>1</td>
<td>[1 2 4 5]</td>
</tr>
<tr>
<td>3.01</td>
<td>0</td>
<td>[1 2 4]</td>
</tr>
<tr>
<td>3.02</td>
<td>1</td>
<td>[1 2 4 6]</td>
</tr>
<tr>
<td>3.02</td>
<td>1</td>
<td>[1 2 4 6]</td>
</tr>
<tr>
<td>3.02</td>
<td>1</td>
<td>[1 2 4 6 9]</td>
</tr>
<tr>
<td>3.02</td>
<td>1</td>
<td>[1 2 4 6 9 10]</td>
</tr>
<tr>
<td>3.02</td>
<td>1</td>
<td>[1 2 4 6 9 10 11 15]</td>
</tr>
<tr>
<td>3.02</td>
<td>0</td>
<td>[1 2 4 6 9 10 11 15]</td>
</tr>
</tbody>
</table>
<p>Just over three seconds into the test, process 1 observed key 116’s state as <code>[1 2 3]</code>. However, an immediately following read by process 0 saw <code>[1 2 4]</code>, and the write of <code>3</code> never appeared again. Process 1 then observed <code>[1 2 4 5]</code>. This write of <code>5</code> was replaced by <code>6</code> and never seen again.</p>
<p>Our lost update checker is conservative: it only infers an anomaly if two transactions read the same version of some key <em>and</em> both write to it. However, our <code>append</code> operations are performed by reading the value, then writing back changes. This means these transactions contain reads which are effectively invisible to the checker. It seems likely that these cases of incompatible order also represent lost updates.</p>
<p>In total, 481 out of the 975 keys in this test exhibited lost updates or incompatible orders. These phenomena are prohibited by Serializability, Snapshot Isolation, and Repeatable Read. We’ve reported this as <a href="https://github.com/ravendb/ravendb/issues/17927">issue 17927</a> in RavenDB’s issue tracker.</p>
<h2 data-number="3.2" id="fractured-reads-with-optimistic-concurrency-17929"> Fractured Reads with Optimistic Concurrency (#17929)</h2>
<p>With the <em><a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/configuration/how-to-enable-optimistic-concurrency">optimistic concurrency</a></em> feature enabled, RavenDB promises to “generate a concurrency exception (and abort all modifications in the current transaction) when the document has been modified on the server side after the client received and modified it.” When running on a single node, this setting does appear to prevent lost updates. However, it allows fractured reads—as well as various flavors of G-Single, G-nonadjacent, and G2-item. Again, these anomalies occurred in a healthy single-node system.</p>
<p>For instance, consider this <a href="https://s3.amazonaws.com/jepsen.io/analyses/ravendb-6.0.2/20231229T140050.249-0600.zip">ten-second test run</a> in which every transaction enabled optimistic concurrency. Our checker found hundreds of anomalies like this:</p>

<p>In this diagram the top transaction <span><em>T</em><sub>1</sub></span> appended <code>3</code> to key 271, then appended <code>2</code> to key 279. The bottom transaction <span><em>T</em><sub>2</sub></span> read key 271 and found nothing, appended 6 to key 276, and finally read key 279’s value as <code>[2]</code>. Because <span><em>T</em><sub>2</sub></span> failed to observe <span><em>T</em><sub>1</sub></span>’s append to key 271, we have a read-write anti-dependency, denoted <code>rw</code>. Because <span><em>T</em><sub>2</sub></span> observed <span><em>T</em><sub>1</sub></span>’s append to key 279, we have a write-read dependency, denoted <code>wr</code>. In short, <span><em>T</em><sub>2</sub></span> observed some, but not all, of the effects of <span><em>T</em><sub>1</sub></span>.</p>
<p>This anomaly is called <em>fractured read</em>, and it is prohibited under Read Atomic, Update Atomic, Causal, Prefix, Parallel Snapshot Isolation, Snapshot Isolation, Repeatable Read, and Serializable. RavenDB’s <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/faq/transaction-support">Transaction FAQ</a> promises Snapshot Isolation: “even if you access multiple documents, you’ll get all of their state as it was in the beginning of the request.” In all Snapshot Isolated databases Jepsen is familiar with, snapshots extend across multiple reads. In RavenDB, it appears each read can observe a different state. We’ve reported this as <a href="https://github.com/ravendb/ravendb/issues/17929">issue #17929</a> to RavenDB.</p>
<h2 data-number="3.3" id="fractured-read-with-cluster-wide-transactions-17928"> Fractured Read with Cluster-Wide Transactions (#17928)</h2>
<p>Cluster-wide transactions are <a href="https://ravendb.net/docs/article-page/6.0/csharp/server/clustering/cluster-transactions">supposed to be Serializable</a>. However, we found that even healthy, single-node clusters in which every transaction used <code>CLUSTER_WIDE</code> mode routinely exhibited fractured reads, as well as G-single, G-nonadjacent, G2-item, and more. Consider this <a href="https://s3.amazonaws.com/jepsen.io/analyses/ravendb-6.0.2/20231229T145350.284-0600.zip">five second test run</a>, which contained hundreds of serializability violations. Here is one of those anomalies:</p>

<p>Here, the bottom transaction <span><em>T</em><sub>2</sub></span> appended <code>6</code> to key 146 and <code>1</code> to key 149. The top transaction <span><em>T</em><sub>1</sub></span> failed to observe <span><em>T</em><sub>2</sub></span>’s append to key 149, but <em>did</em> observe its append to key 146. This is another instance of fractured read. As before, this behavior appears to be proscribed by RavenDB’s documentation, as well as all consistency models above Read Atomic.</p>
<p>We’ve reported this to RavenDB as <a href="https://github.com/ravendb/ravendb/issues/17928">issue #17928</a>.</p>
<table>
<thead>
<tr>
<th>№</th>
<th>Summary</th>
<th>Event Required</th>
<th>Fixed in</th>
</tr>
</thead>
<tbody>
<tr>
<td>17927</td>
<td>Lost update with single-node transactions</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>17929</td>
<td>Fractured read with optimistic concurrency</td>
<td>None</td>
<td>Unresolved</td>
</tr>
<tr>
<td>17928</td>
<td>Fractured read with cluster-wide transactions</td>
<td>None</td>
<td>Unresolved</td>
</tr>
</tbody>
</table>
<h2 data-number="4" id="discussion"> Discussion</h2>
<p>RavenDB variously claims to offer “fully ACID” transactions, Serializability, or at least Snapshot Isolation. All of these claims appear false. RavenDB 6.0.2’s default settings allowed lost updates. Even cluster-wide transactions exhibited fractured reads: a serious anomaly prohibited under Snapshot Isolation, as well as several weaker models. These behaviors occur even in healthy, single-node, single-shard systems, in which all access occurs via primary key.</p>
<p>RavenDB’s strongest safety settings violate Read Atomic. It therefore cannot satisfy Update Atomic, Causal, Prefix, Parallel Snapshot Isolation, Snapshot Isolation, Repeatable Read, or Serializable. RavenDB might offer Read Committed or Monotonic Atomic View, but without more rigorous testing, Jepsen is hesitant to make this claim.</p>
<p>RavenDB’s weak default behavior is surprising given RavenDB’s repeated emphasis on safety. As CEO Oren Eini <a href="https://www.youtube.com/watch?v=5ZXBR3croMA&amp;t=850s">remarked</a> on MongoDB’s transaction safety settings:</p>
<blockquote>
<p>[Default] values matter. They matter quite a lot. Why is that? Because if you choose the bad values, you’re absolutely going to get some great numbers in benchmark performance. But then you are going to be hitting those [safety] issues in production. And then there is this classic response: “Oh, you should have read the docs and used the proper configuration.”</p>
</blockquote>
<p>One wonders: if ACID properties are so important for RavenDB’s users, why do the default settings allow lost updates, even on single-key operations? Do users realize their updates can be silently discarded? How many are taking care to use cluster-wide transactions where lost updates would violate safety? Do they know that even cluster-wide transactions allow fractured read?</p>
<p>This report follows a cursory investigation into RavenDB’s behavior—it is by no means exhaustive. As always, we caution that Jepsen takes an experimental approach to safety verification: we can prove the presence of bugs, but not their absence. There may be other anomalies in RavenDB.</p>
<h2 data-number="4.1" id="does-ravendb-even-have-transactions"> Does RavenDB Even Have Transactions?</h2>
<p>The first sentence of RavenDB’s <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/cluster-transaction/overview">cluster transaction documentation</a> appears quite clear:</p>
<blockquote>
<p>A session represents a single business transaction.</p>
</blockquote>
<p>This is echoed by the first sentence of RavenDB’s <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/what-is-a-session-and-how-does-it-work">session documentation</a>:</p>
<blockquote>
<p>The Session, which is obtained from the Document Store, is a Unit of Work that represents a single business transaction on a particular database.</p>
</blockquote>
<p>… which goes on to say:</p>
<blockquote>
<p>The batched operations that are sent in the <code>SaveChanges()</code> will complete transactionally. In other words, either all changes are saved as a Single Atomic Transaction or none of them are. So once SaveChanges returns successfully, it is guaranteed that all changes are persisted to the database.</p>
</blockquote>
<p>RavenDB sessions are clearly not intended to work like sessions in typical databases, which are (roughly speaking) <a href="https://www.postgresql.org/docs/current/tutorial-arch.html">one-to-one with client connections</a>.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a> They come with a default limit of <a href="https://ravendb.net/docs/article-page/5.4/java/client-api/session/what-is-a-session-and-how-does-it-work#remarks">30 network requests</a>; typical database sessions are unbounded. They buffer writes; Jepsen is unaware of any other database whose sessions do this. They cache reads; most sessions do not. They include concurrency control mechanisms like lost update prevention; Jepsen is unaware of any other database which does this at the session level. These are all hallmarks of what most databases would call a transaction.</p>
<p>RavenDB’s article <a href="https://ravendb.net/articles/acid-transactions-in-nosql-ravendb-vs-mongodb">ACID Transactions in NoSQL? RavenDB vs MongoDB</a> is emphatic: RavenDB has supported ACID transactions over “any combination of database operations” for over a decade. It certainly appears as if RavenDB sessions are intended for this role!</p>
<p>However, in a <a href="https://github.com/ravendb/ravendb/issues/17927#issuecomment-1872912239">response to issue 17927</a>, Eini (a.k.a. Ayende Rahien) explained that sessions are <em>not</em> in fact transactions:<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<blockquote>
<p>Crucially, RavenDB does not attempt to provide transactional semantics over the entire session, rather it provide[s] transactions over individual requests.</p>
</blockquote>
<p>And in <a href="https://github.com/ravendb/ravendb/issues/17928#issuecomment-1872916841">response to issue #17928</a>, Eini affirms:</p>
<blockquote>
<p>A transaction in RavenDB is a request - so TX1 and TX2 above aren’t actually single transactions, instead, each of them represent 3 independent transactions.</p>
</blockquote>
<p>This is a striking viewpoint: the point of transactions is generally to provide isolation across multiple requests.<a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a> Moreover, RavenDB’s optimistic concurrency and cluster-wide transaction mechanisms are clearly intended to provide transactional isolation which spans from a session’s reads to its writes. Furthermore, Eini <a href="https://www.youtube.com/watch?v=5ZXBR3croMA&amp;t=23m40s">directly compared RavenDB sessions to MongoDB transactions</a> (which offer typical interactive transaction semantics) and claimed that unlike MongoDB, RavenDB sessions actually satisfied Snapshot Isolation. Yet per Eini’s comments, RavenDB <em>does not have interactive transactions at all</em>.</p>
<p>Repeatedly advertising “ACID transactions” across “any combination of database operations,” telling users that a “session represents a single business transaction,” comparing RavenDB sessions to interactive transactions in other databases, offering concurrency control mechanisms whose scope extends across an entire session, and finally expecting users to realize that sessions are not transactions at all—that a transaction is actually limited to a single HTTP request—stretches credulity.</p>
<p>Jepsen strives to evaluate databases in the context of their marketing and documentation. Although RavenDB’s CEO <a href="https://github.com/ravendb/ravendb/issues/17928#issuecomment-1874064897">now states</a> “we don’t support a transaction over more than a single HTTP request,” RavenDB’s documentation and marketing give every appearance that a session is intended to be a transaction. Jepsen has consulted with several software engineers on their interpretation of these claims, and believes typical database users would come to the same conclusion: RavenDB sessions are transactions. We continue this interpretation throughout this report.</p>
<h2 data-number="4.2" id="recommendations"> Recommendations</h2>
<p>RavenDB users should be aware RavenDB transactions are not ACID in any meaningful sense.<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a> This holds even in single-node, single-shard deployments. The defaults allow lost updates: you should expect some of your writes to be silently discarded. The strongest safety settings allow fractured read: you might observe some, but not all, of another transaction’s effects. You could appear to write “into the middle” of another transaction. The two isolation levels RavenDB advertises—Snapshot Isolation and Serializable—appear impossible to obtain.</p>
<p>Users who designed their applications assuming RavenDB provided interactive ACID transactions—or even Snapshot Isolation—should carefully reevaluate their transactions to ensure they are safe in the presence of these anomalies. Consider writing simple tests to verify application invariants are preserved under concurrent execution: the issues in this report are easy to reproduce.</p>
<p>Jepsen recommends RavenDB remove claims of “ACID”, “Serializable”, and “Snapshot Isolation” from their marketing materials and documentation. RavenDB should instead make specific, accurate, and internally consistent claims about safety properties. For instance, RavenDB might say “transactions offer Read Committed by default, plus internal consistency within the scope of a transaction: once a transaction reads a key, subsequent reads and writes of that key observe the originally read state, plus the effects of that particular transaction’s writes. Transactions allow lost update by default. Enabling cluster-wide transactions prevents lost update, but still allows fractured read,” and so on.</p>
<p>RavenDB’s documentation is remarkably confusing. It repeatedly claims to offer <a href="https://ravendb.net/why-ravendb/acid-transactions">ACID transactions</a>, which implies Serializability. There are specific claims that RavenDB ensures either <a href="https://ravendb.net/docs/article-page/6.0/csharp/server/clustering/cluster-transactions#case-1-multiple-concurrent-cluster-transactions">Serializability</a> or <a href="https://www.youtube.com/watch?v=5ZXBR3croMA">Snapshot Isolation</a>. However, the documentation also says that RavenDB’s database layer is an <a href="https://ravendb.net/learn/inside-ravendb-book/reader/4.0/6-ravendb-clusters">AP system</a> based on Last Write Wins, and the marketing material claims isolated nodes can operate <a href="https://ravendb.net/why-ravendb/high-availability">independently</a>. This is impossible: totally available systems <a href="https://jepsen.io/consistency">cannot provide</a> Serializability or Snapshot Isolation.<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>There are systems (like <a href="https://riak.com/index.html">Riak</a> &amp; <a href="https://cassandra.apache.org/_/index.html">Cassandra</a>) which allow clients to execute either totally available operations with weak consistency, or majority available operations with stronger guarantees, like Linearizability. If RavenDB intends to build a system which supports both modes, they should clearly distinguish those modes throughout marketing and documentation. They have completely different availability, latency, and safety characteristics. Repeated claims that RavenDB provides ACID “<a href="https://ravendb.net/why-ravendb/acid-transactions">without sacrificing performance</a>” are <a href="https://lamport.azurewebsites.net/pubs/lower-bound.pdf">provably impossible</a>, and should be rewritten to clearly explain the tradeoffs involved.</p>
<p>ACID transactions are clearly important to RavenDB. It is therefore alarming that RavenDB’s documentation and GitHub comments fundamentally disagree on what a transaction <em>is</em>. In one interpretation, RavenDB offers interactive transactions, represented by the session API, which provide relatively weak isolation—certainly not ACID. In another interpretation, RavenDB lacks interactive transactions altogether. Instead, it offers a sort of micro-transaction which (e.g.) writes multiple documents in a single network request. In this world, sessions offer varying, weak consistency constraints that extend <em>between</em> micro-transactions.</p>
<p>To resolve this confusion, RavenDB should pick a single definition of “transaction” and stick with it. The equivalence or difference between a transaction and session should be clearly explained, and these terms used consistently throughout marketing and documentation. RavenDB should provide guidance as to the boundaries of each unit: when are multiple calls to <code>load</code> performed in a single transaction? What about <code>store</code>? Can a single transaction encompass both a <code>load</code> and <code>store</code>? The consistency properties of both transactions and sessions should be clearly and formally defined. Are transactions Serializable? Do sessions ensure Monotonic Atomic View? When does a session preclude lost update, and when does it allow it? Above all, do not <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/cluster-transaction/overview">tell</a> <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/session/what-is-a-session-and-how-does-it-work">users</a> that sessions “represent a single business transaction” if they are, in point of fact, not transactions at all.</p>
<p>Finally, if RavenDB transactions are truly intended to cover only a single network request, consider using a different term altogether, and avoid comparisons to databases which do have interactive transactions. Some databases call these “mini-” or “micro-transactions,” which provides an obvious hint of their limited scope.</p>
<h2 data-number="4.3" id="future-work"> Future Work</h2>
<p>This work evaluated only single-node RavenDB clusters without faults. Future research could expand tests across multiple nodes, as well as introducing network, process, and disk faults. We dealt only with key-value operations, and did not evaluate RavenDB’s secondary indices. These indices are described as eventually consistent, which raises questions around the integrity of predicate reads. RavenDB also offers <a href="https://ravendb.net/docs/article-page/6.0/csharp/client-api/operations/patching/single-document">server-side transactions</a> using Javascript or a library of built-in patch operations. These might offer different safety characteristics than the interactive transactions we used in this report. Finally, cross-shard transactions are a notoriously challenging problem and deserve careful testing.</p>
<p><em>Jepsen wishes to thank <a href="https://www.irenekannyo.com/">Irene Kannyo</a> for her invaluable editorial support. Thanks as well to C. Scott Andreas, Taber Bain, Silvia Botros, Coda Hale, Ben Linsay, Kelly Shortridge, Nathan Taylor, Zach Tellman, and Leif Walsh for their comments on early versions of this manuscript. This work was performed independently without compensation, in accordance with the <a href="https://jepsen.io/ethics">Jepsen ethics policy</a>.</em></p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p>As discussed in section 4.1, the interpretation of a “transaction” in RavenDB is complicated. In this report, we identify RavenDB’s session API as an interactive transaction.<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Technically, this isn’t clear from the documentation alone: Snapshot Isolation is a property of <em>histories</em> of transactions, but the docs discuss behavior only within the scope of a single transaction. RavenDB might have meant some weaker property here—for instance, Prefix Consistency. However, the CEO’s claims of Snapshot Isolation by default seem authoritative.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>It is tempting to believe that cluster-wide transactions are the ACID transactions RavenDB’s marketing boasts, but this cannot be the case. RavenDB says it’s offered ACID transactions since 2010, and cluster-wide transactions weren’t introduced until roughly a decade later. This may reflect confusion over what “ACID” means.<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>RavenDB’s Jepsen test may not have measured anything at all: at least in the most recent revision, the <a href="https://github.com/ml054/jepsen/blob/08960da98ab9bf7959fce014c63b146b903cbe6c/ravendb/src/jepsen/ravendb.clj#L308-L316">generator</a> included no client operations of any kind.<a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>There are scenarios in which a database multiplexes multiple sessions onto a single connection, or migrates a session across connections. It might be more apt to think of a typical database session as “a logical, single-threaded connection.”<a href="#fnref5" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>In the same thread, Eini <a href="https://github.com/ravendb/ravendb/issues/17927#issuecomment-1872912239">remarks</a> that “we only consider transactions to be the calls to SaveChanges or other data mutation operations.” This is somewhat alarming: it implies that RavenDB transactions don’t encompass reads at all. On the <a href="https://github.com/ravendb/ravendb/issues/17928#issuecomment-1874064897">other hand</a>, a transaction involving a single HTTP request “applies to reads as well,” so it’s not quite clear what RavenDB’s read safety semantics are.<a href="#fnref6" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>There are databases, like <a href="https://fauna.com/">FaunaDB</a>, where transactions are written as small programs and submitted to the database in a single request. RavenDB, like most databases, provides interactive sessions: clients make calls to <code>load</code> and <code>store</code> interspersed with arbitrary local computation, and call <code>saveChanges()</code> to commit their effects. In these kinds of systems, transactions typically encompass multiple read and write requests.<a href="#fnref7" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>Alternatively, “RavenDB does not have interactive transactions at all.” Readers may select their favorite interpretation throughout this report.<a href="#fnref8" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>At least, not in a network which can partition.<a href="#fnref9" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
  </div>
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Macaroons Escalated Quickly (231 pts)]]></title>
            <link>https://fly.io/blog/macaroons-escalated-quickly/</link>
            <guid>39204314</guid>
            <pubDate>Wed, 31 Jan 2024 14:39:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fly.io/blog/macaroons-escalated-quickly/">https://fly.io/blog/macaroons-escalated-quickly/</a>, See on <a href="https://news.ycombinator.com/item?id=39204314">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
         <dl>
             <dt>Author</dt>
             <dd>
                 <img alt="Thomas Ptacek" src="https://fly.io/static/images/thomas.webp">
               <dl>
                 <dt>Name</dt>
                 <dd>
                   Thomas Ptacek
                 </dd>
                  <dt>@tqbf</dt>
                  <dd>
                    <a href="https://twitter.com/tqbf" target="_blank">
                      @tqbf
                    </a>
                  </dd>
               </dl>
             </dd>
         </dl>

        <section>
            <figure>
                <img src="https://fly.io/blog/macaroons-escalated-quickly/assets/evil-cookies-cover.webp" alt="Evil cookies!">
                <figcaption>
                  <span>Image by</span>
                  
<svg role="img" style="pointer-events: none; width: 17px; height: 17px;" viewBox="0 0 20 20" fill="currentColor" fill-rule="evenodd">
  <g buffered-rendering="static">
    <path fill-rule="evenodd" d="M1 8a2 2 0 012-2h.93a2 2 0 001.664-.89l.812-1.22A2 2 0 018.07 3h3.86a2 2 0 011.664.89l.812 1.22A2 2 0 0016.07 6H17a2 2 0 012 2v7a2 2 0 01-2 2H3a2 2 0 01-2-2V8zm13.5 3a4.5 4.5 0 11-9 0 4.5 4.5 0 019 0zM10 14a3 3 0 100-6 3 3 0 000 6z" clip-rule="evenodd"></path>
  </g>
</svg>

                    <a href="https://annieruygtillustration.com/" target="_blank">
                      Annie Ruygt
                    </a>
                </figcaption>
            </figure>
          <p>We’re Fly.io and we transmute containers into VMs, running them on our hardware around the world. We built a new security token system, and can I tell you the good news about our lord and savior the Macaroon?</p><h2 id="1"><a href="#1" aria-label="Anchor"></a>1</h2>
<p>Let’s implement an API token together. It’s a design called “Macaroons”, but don’t get hung up on that yet.</p>

<p>First some . Then:</p>
<div id="includes" toggle-content="" aria-label="show very boring code">
  <pre><code><span>import</span> <span>sys</span>
<span>import</span> <span>os</span>
<span>import</span> <span>json</span>
<span>import</span> <span>hmac</span> <span>as</span> <span>hm</span>
<span>from</span> <span>base64</span> <span>import</span> <span>b64encode</span><span>,</span> <span>b64decode</span>
<span>from</span> <span>hashlib</span> <span>import</span> <span>sha256</span>

<span>def</span> <span>hmac</span><span>(</span><span>k</span><span>,</span> <span>v</span><span>):</span> <span>return</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>,</span> <span>v</span><span>,</span> <span>sha256</span><span>).</span><span>digest</span><span>()</span>
<span>def</span> <span>enc</span><span>(</span><span>x</span><span>):</span> <span>return</span> <span>b64encode</span><span>(</span><span>x</span><span>)</span>
<span>def</span> <span>dec</span><span>(</span><span>x</span><span>):</span> <span>return</span> <span>b64decode</span><span>(</span><span>x</span><span>)</span>
</code></pre>
</div><div>
  <pre><code><span>def</span> <span>blank_token</span><span>(</span><span>uid</span><span>,</span> <span>key</span><span>):</span>
  <span>nonce</span> <span>=</span> <span>enc</span><span>(</span><span>":"</span><span>.</span><span>join</span><span>([</span><span>str</span><span>(</span><span>uid</span><span>),</span> <span>os</span><span>.</span><span>urandom</span><span>(</span><span>16</span><span>)]))</span>
  <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>([</span><span>nonce</span><span>,</span> <span>enc</span><span>(</span><span>hmac</span><span>(</span><span>key</span><span>,</span> <span>nonce</span><span>))])</span>
</code></pre>
</div>
<p>Bearer tokens: like cookies, blobs you attach to a request (usually in an HTTP header).</p>
<p>We’re going to build a minimally-stateful bearer token, a blob signed with HMAC. Nothing fancy so far. <a href="https://api.rubyonrails.org/classes/ActiveSupport/MessageVerifier.html">Rails has done this</a> for a decade and a half.</p>

<p>There’s a <a href="http://cryto.net/%7Ejoepie91/blog/2016/06/13/stop-using-jwt-for-sessions/">fashion in API security for stateless tokens</a>, which encode all the data you’d need to check any request accompanied by that token – without a database lookup. Stateless tokens have some nice properties, and some less-nice. Our tokens won’t be stateless: they carry a user ID, with which we’ll look up the HMAC key to verify it. But they’ll stake out a sort of middle ground.</p>
<div>
  <pre><code><span>def</span> <span>attenuate</span><span>(</span><span>macStr</span><span>,</span> <span>cav</span><span>):</span>
    <span>mac</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>macStr</span><span>)</span>
    <span>cavStr</span> <span>=</span> <span>json</span><span>.</span><span>dumps</span><span>(</span><span>cav</span><span>)</span>
    <span>oldTail</span> <span>=</span> <span>dec</span><span>(</span><span>mac</span><span>[</span><span>-</span><span>1</span><span>])</span>         
    <span>newTail</span> <span>=</span> <span>enc</span><span>(</span><span>hmac</span><span>(</span><span>oldTail</span><span>,</span> <span>cavStr</span><span>))</span>    
    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>(</span><span>mac</span><span>[</span><span>0</span><span>:</span><span>-</span><span>1</span><span>]</span> <span>+</span> <span>[</span><span>cavStr</span><span>,</span> <span>newTail</span><span>])</span>

<span>m0</span> <span>=</span> <span>blank_token</span><span>(</span><span>10</span><span>,</span> <span>keys</span><span>[</span><span>10</span><span>])</span>
<span>m1</span> <span>=</span> <span>attenuate</span><span>(</span><span>m0</span><span>,</span> <span>{</span><span>'path'</span><span>:</span> <span>'/images'</span><span>})</span>
<span>m2</span> <span>=</span> <span>attenuate</span><span>(</span><span>m1</span><span>,</span> <span>{</span><span>'op'</span><span>:</span> <span>'read'</span><span>})</span>
</code></pre>
</div>

<p>Let’s add some stuff.</p>

<p>The meat of our tokens will be a series of claims we call “caveats”. We call them that because each claim restricts further what the token authorizes. After <code>{'path': '/images'}</code>, this token only allows operations that happen underneath the <code>/images</code> directory. Then, after <code>{'op': 'read'}</code>, it allows only reads, not writes.</p>

<p>(I guess we’re building a file sharing system. Whatever.)</p>

<p>Some important things about things about this design. First: by implication from the fact that caveats further restrict tokens, a token with no caveats restricts nothing. It’s a god-mode token. Don’t honor it.</p>
<p>In other words: the ordering of caveats doesn’t matter.</p>
<p>Second: the rule of checking caveats is very simple: every single caveat must pass, evaluating <code>True</code> against the request that carries it, in isolation and without reference to any other caveat. If any caveat evaluates <code>False</code>, the request fails. In that way, we ensure that adding caveats to a token can only ever weaken it.</p>

<p>With that in mind, take a closer look at this code:</p>
<div>
  <pre><code><span>oldTail</span> <span>=</span> <span>dec</span><span>(</span><span>mac</span><span>[</span><span>-</span><span>1</span><span>])</span>         
<span>newTail</span> <span>=</span> <span>enc</span><span>(</span><span>hmac</span><span>(</span><span>oldTail</span><span>,</span> <span>cavStr</span><span>))</span>    
</code></pre>
</div>

<p>Every caveat is HMAC-signed independently, which is weird. Weirder still, the key for that HMAC is the output of the last HMAC. The caveats chain together, and the HMAC of the last caveat becomes the “tail” of the token.</p>

<p>Creating a new blank token for a particular user requires a key that the server (and probably only the server) knows. But adding a caveat doesn’t! Anybody can add a caveat. In our design, you, the user, can edit your own API token.</p>
<div>
  <pre><code><span>def</span> <span>verify</span><span>(</span><span>macStr</span><span>,</span> <span>keys</span><span>):</span>
    <span>mac</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>macStr</span><span>)</span>
    <span>nonce</span> <span>=</span> <span>dec</span><span>(</span><span>mac</span><span>[</span><span>0</span><span>]).</span><span>split</span><span>(</span><span>":"</span><span>)</span>
    <span>key</span> <span>=</span> <span>keys</span><span>[</span><span>int</span><span>(</span><span>nonce</span><span>[</span><span>0</span><span>])]</span>
    <span>tail</span> <span>=</span> <span>""</span>
    <span>for</span> <span>cav</span> <span>in</span> <span>mac</span><span>[:</span><span>-</span><span>1</span><span>]:</span>
        <span>tail</span> <span>=</span> <span>hmac</span><span>(</span><span>key</span><span>,</span> <span>cav</span><span>)</span>
        <span>key</span> <span>=</span> <span>tail</span>
    <span>return</span> <span>hm</span><span>.</span><span>compare_digest</span><span>(</span><span>tail</span><span>,</span> <span>dec</span><span>(</span><span>mac</span><span>[</span><span>-</span><span>1</span><span>]))</span>

<span>verify</span><span>(</span><span>m2</span><span>,</span> <span>keys</span><span>)</span> <span># =&gt; True
</span></code></pre>
</div>

<p>For completeness, and to make a point, there’s the verification code. Look up the original secret key from the user ID,  and then it’s chained HMAC all the way down. The point I’m making is that Macaroons are very simple.</p>
<h2 id="2"><a href="#2" aria-label="Anchor"></a>2</h2>
<p>Back in 2014, Google published <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41892.pdf">a paper at NDSS</a> introducing “Macaroons”, a new kind of cookie. Since then, they’ve become a sort of hipster shibboleth. But they’re more talked about than implemented, which is a nice way to say that practically nobody uses them.</p>

<p>Until now! I dragged Fly.io into implementing them. Suckers!</p>

<p>We had a problem: our API tokens were much too powerful. We needed to scope them down and let them express roles, and I scoped up that project to replace OAuth2 tokens altogether. We now have what I think is one of the more expansive Macaroon implementations on the Internet.</p>

<p>I dragged us into using Macaroons because I wanted us to use a hipster token format. Google designed Macaroons for a bigger reason: they hoped to replace browser cookies with something much more powerful.</p>

<p>The problem with simple bearer tokens, like browser cookies or JWTs, is that they’re prone to being stolen and replayed by attackers.</p>
<p>game-over: pentest jargon for “very bad”</p>
<p>Worse, a stolen token is usually a game-over condition. In most schemes, a bearer token is an all-access pass for the associated user. For some applications this isn’t that big a deal, but then, <a href="https://neilmadden.blog/2020/09/09/macaroon-access-tokens-for-oauth-part-2-transactional-auth/">think about banking</a>. A banking app token that authorizes arbitrary transactions is a recipe for having a small heart attack on every HTTP request.</p>
<p>(Perfectly minimized API tokens: a software security holy grail)</p>
<p>Macaroons are user-editable tokens that enable JIT-generated least-privilege tokens. With minimal ceremony and no additional API requests, a banking app Macaroon lets you authorize a request with a caveat like, I don’t know, <code>{'maxAmount': '$5'}</code>. I mean, something way better than that, probably lots of caveats, not just one, but you get the idea: a token so minimized you feel safe sending it with your request. Ideally, a token that only authorizes that single, intended request.</p>
<h2 id="3"><a href="#3" aria-label="Anchor"></a>3</h2>
<p>That’s not why we like Macaroons. We already assume our tokens aren’t being stolen.</p>

<p>In most systems, the developers come up with a permissions system, and you’re stuck with it. We run a public cloud platform, and people want a lot of different things from our permissions. The dream is, we (the low-level platform developers on the team) design a single permission system, one time, and go about our jobs never thinking about this problem again.</p>

<p>Instead of thinking of all of our “roles” in advance, we just model our platform with caveats:</p>

<ol>
<li>Users belong to <code>Organizations</code>.
</li><li><code>Organizations</code> own <code>Apps</code>.
</li><li><code>Apps</code> contain <code>Machines</code> and <code>Volumes</code>.
</li><li>To any of these things, you can <code>Read</code>, <code>Write</code>, <code>Create</code>, <code>Delete</code>, and/or <code>Control</code> .
</li><li>Some administrivia, like expiration (<code>ValidityWindow</code>), locking tokens to specific Fly Machines (<code>FromMachineSource</code>), and escape hatches like <code>Mutation</code> (for our GraphQL API).
</li></ol>
<p>(this is a vibes-based notation, don’t think too hard about it)</p>
<p>Simplistic. But it expresses admin tokens: </p>
<div>
  <pre><code>Organization 4721, mask=*
</code></pre>
</div>

<p>And it expresses normal user tokens:</p>
<div>
  <pre><code>Organization 4721, mask=read,write,control
(App 123, mask=control), (App 345, mask=read, write, control)
</code></pre>
</div>

<p>And also an auditor-only token for that user:</p>
<div>
  <pre><code>Organization 4721, mask=read,write,control
(App 123, mask=control), (App 345, mask=read, write, control)
Organization 4721, mask=read
</code></pre>
</div>
<p>(our deploy tokens are more complicated than this)</p>
<p>Or a deployment-only token, for a CI/CD system:</p>
<div>
  <pre><code>Organization 4721, mask=write,control
(App 123, mask=*)
</code></pre>
</div>

<p>Those are just the roles we came up with. Users can invent others. The important thing is that they don’t have to bother me about them.</p>
<h2 id="4"><a href="#4" aria-label="Anchor"></a>4</h2>
<p>Astute readers will have noticed by now that we haven’t shown any code that actually evaluates a caveat. That’s because it’s boring, and I’m too lazy to write it out. Got an <code>Organization</code> token for <code>image-hosting</code> that allows <code>Reads</code>? Ok; check and make sure the incoming request is for an asset of <code>image-hosting</code>, and that it’s a <code>Read</code>. Whatever code you came up with, it’d be fine.</p>

<p>These straightforward restrictions are called “first party caveats”. The first party is us, the platform. We’ve got all the information we need to check them.</p>

<p>Let’s kit out our token format some more.</p>
<div>
  <pre><code><span>def</span> <span>third_party_caveat</span><span>(</span><span>ka</span><span>,</span> <span>msg</span><span>,</span> <span>url</span><span>):</span>
    <span>crk</span> <span>=</span> <span>os</span><span>.</span><span>urandom</span><span>(</span><span>16</span><span>)</span>
    <span>ticket</span> <span>=</span> <span>enc</span><span>(</span><span>encrypt</span><span>(</span><span>ka</span><span>,</span> <span>json</span><span>.</span><span>dumps</span><span>({</span>
        <span>'crk'</span><span>:</span> <span>enc</span><span>(</span><span>crk</span><span>),</span>
        <span>'msg'</span><span>:</span> <span>msg</span>
    <span>})))</span>
    <span>challenge</span> <span>=</span> <span>enc</span><span>(</span><span>encrypt</span><span>(</span><span>tag</span><span>,</span> <span>crk</span><span>))</span>
    <span>return</span> <span>{</span> <span>'url'</span><span>:</span> <span>url</span><span>,</span> <span>'ticket'</span><span>:</span> <span>ticket</span><span>,</span> <span>'challenge'</span> <span>:</span> <span>challenge</span> <span>}</span>

<span>key</span> <span>=</span> <span>bytes</span><span>(</span><span>"YELLOWSUBMARINE"</span><span>)</span>
<span>url</span> <span>=</span> <span>"https://canary.service"</span>
<span>c3</span> <span>=</span> <span>third_party_caveat</span><span>(</span><span>key</span><span>,</span> <span>url</span><span>,</span> <span>json</span><span>.</span><span>dumps</span><span>({</span><span>'user'</span><span>:</span> <span>'bobson.dugnutt'</span><span>}))</span>
<span>m3</span> <span>=</span> <span>attenuate</span><span>(</span><span>m2</span><span>,</span> <span>c3</span><span>)</span>
</code></pre>
</div>

<p>Up till now, we’ve gotten by with nothing but HMAC, which is one of the great charms of the design. Now we need to encrypt. There’s no authenticated encryption in the Python standard library, but that won’t stop us. </p>
<div id="hmac-ctr" toggle-content="" aria-label="show very silly code">
  <pre><code><span># do i really need to say that i'm not serious about this?
</span>
<span>def</span> <span>hmactr</span><span>(</span><span>k</span><span>,</span> <span>n</span><span>):</span>
     <span>ks</span> <span>=</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>+</span><span>n</span><span>)</span>
     <span>for</span> <span>counter</span> <span>in</span> <span>xrange</span><span>(</span><span>sys</span><span>.</span><span>maxint</span><span>):</span>
         <span>ks</span><span>.</span><span>update</span><span>(</span><span>str</span><span>(</span><span>counter</span><span>))</span>
         <span>kbs</span> <span>=</span> <span>ks</span><span>.</span><span>digest</span><span>()</span>
         <span>for</span> <span>i</span> <span>in</span> <span>xrange</span><span>(</span><span>16</span><span>):</span> <span>yield</span> <span>kbs</span><span>[</span><span>i</span><span>]</span>

<span>def</span> <span>encrypt</span><span>(</span><span>k</span><span>,</span> <span>buf</span><span>):</span>
    <span>ak</span> <span>=</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>,</span> <span>'auth'</span><span>).</span><span>digest</span><span>()</span>
    <span>nonce</span> <span>=</span> <span>os</span><span>.</span><span>urandom</span><span>(</span><span>16</span><span>)</span>
    <span>cipher</span> <span>=</span> <span>hmactr</span><span>(</span><span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>,</span> <span>'enc'</span><span>).</span><span>digest</span><span>(),</span> <span>nonce</span><span>)</span>
    <span>ctxt</span> <span>=</span> <span>bytearray</span><span>(</span><span>buf</span><span>)</span>
    <span>for</span> <span>i</span> <span>in</span> <span>xrange</span><span>(</span><span>len</span><span>(</span><span>buf</span><span>)):</span>
        <span>ctxt</span><span>[</span><span>i</span><span>]</span> <span>^=</span> <span>ord</span><span>(</span><span>cipher</span><span>.</span><span>next</span><span>())</span>
    <span>res</span> <span>=</span> <span>nonce</span> <span>+</span> <span>str</span><span>(</span><span>ctxt</span><span>)</span>
    <span>return</span> <span>res</span> <span>+</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>ak</span><span>,</span> <span>res</span><span>).</span><span>digest</span><span>()</span>

<span>def</span> <span>decrypt</span><span>(</span><span>k</span><span>,</span> <span>buf</span><span>):</span>
    <span>ak</span> <span>=</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>,</span> <span>'auth'</span><span>).</span><span>digest</span><span>()</span>
    <span>if</span> <span>not</span> <span>hm</span><span>.</span><span>compare_digest</span><span>(</span><span>buf</span><span>[</span><span>-</span><span>16</span><span>:],</span> <span>hm</span><span>.</span><span>new</span><span>(</span><span>ak</span><span>,</span> <span>buf</span><span>[:</span><span>-</span><span>16</span><span>]).</span><span>digest</span><span>()):</span>
        <span>return</span> <span>False</span>
    <span>nonce</span> <span>=</span> <span>buf</span><span>[:</span><span>16</span><span>]</span>
    <span>cipher</span> <span>=</span> <span>hmactr</span><span>(</span><span>hm</span><span>.</span><span>new</span><span>(</span><span>k</span><span>,</span> <span>'enc'</span><span>).</span><span>digest</span><span>(),</span> <span>nonce</span><span>)</span>
    <span>ptxt</span> <span>=</span> <span>bytearray</span><span>(</span><span>buf</span><span>[</span><span>16</span><span>:</span><span>-</span><span>16</span><span>])</span>
    <span>for</span> <span>i</span> <span>in</span> <span>xrange</span><span>(</span><span>len</span><span>(</span><span>buf</span><span>[</span><span>16</span><span>:</span><span>-</span><span>16</span><span>])):</span>
        <span>ptxt</span><span>[</span><span>i</span><span>]</span> <span>^=</span> <span>ord</span><span>(</span><span>cipher</span><span>.</span><span>next</span><span>())</span>
    <span>return</span> <span>str</span><span>(</span><span>ptxt</span><span>)</span>
</code></pre>
</div>
<p>With “third-party” caveats comes a cast of characters. We’re still the first party. You’ll play the second party. The third party is any other system in the world that you trust: an SSO system, an audit log, a revocation checker, whatever.</p>

<p>Here’s the trick of the third-party caveat: our platform doesn’t know what your caveat means, and it doesn’t have to. Instead, when you see a third-party caveat in your token, you tear a ticket off it and exchange it for a “discharge Macaroon” with that third party. You submit both Macaroons together to us.</p>

<p>Let’s attenuate our token with a third-party caveat hooking it up to a “canary” service that generates a notice approximately any time the token is used.</p>

<p><img src="https://fly.io/blog/macaroons-escalated-quickly/assets/third-party.png?1/2&amp;wrap-left"></p>

<p>To build that canary caveat, you first make a <code>ticket</code> that users of the token will hand to your canary, and then a <code>challenge</code> that Fly.io will use to verify discharges your checker spits out. The ticket and the challenge are both encrypted. The ticket is encrypted under <code>KA</code>, so your service can read it. The challenge is encrypted under the previous Macaroon tail, so only Fly.io can read it. Both hide yet another key, the random HMAC key <code>CRK</code> (“caveat root key”).</p>

<p>In addition to <code>CRK</code>, the ticket contains a message, which says whatever you want it to; Fly.io doesn’t care. Typically, the message describes some kind of additional checking you want your service to perform before spitting out a discharge token.</p>
<div>
  <pre><code><span>def</span> <span>discharge</span><span>(</span><span>ka</span><span>,</span> <span>ticket</span><span>):</span>
    <span>ptxt</span> <span>=</span> <span>decrypt</span><span>(</span><span>ka</span><span>,</span> <span>dec</span><span>(</span><span>ticket</span><span>))</span>
    <span>if</span> <span>ptxt</span> <span>==</span> <span>False</span><span>:</span> <span>return</span> <span>False</span>
    <span>tbody</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>ptxt</span><span>)</span>
    <span># not shown: do something with tbody['msg']
</span>    <span>return</span> <span>json</span><span>.</span><span>dumps</span><span>([</span><span>ticket</span><span>,</span> <span>enc</span><span>(</span><span>hmac</span><span>(</span><span>dec</span><span>(</span><span>tbody</span><span>[</span><span>'crk'</span><span>]),</span> <span>ticket</span><span>))])</span>
</code></pre>
</div>

<p>To authorize a request with a token that includes a third-party caveat for the canary service, you need to get your hands on a corresponding discharge Macaroon. Normally, you do that by <code>POST</code>ing the ticket from the caveat to the service.</p>

<p>Discharging is simple. The service, which holds <code>KA</code>, uses it to decrypt the ticket. It checks the message and makes some decisions. Finally, it mints a new macaroon, using <code>CRK</code>, recovered from the ticket, as the root key. The ticket itself is the nonce.</p>

<p>If it wants, the third-party service can slap on a bunch of first-party caveats of its own. When we verify the Macaroon, we’ll copy those caveats out and enforce them. Attenuation of a third-party discharge macaroon works like a normal macaroon.</p>
<div>
  <pre><code><span>def</span> <span>verify_third_party</span><span>(</span><span>tag</span><span>,</span> <span>cav</span><span>,</span> <span>discharges</span><span>=</span><span>[]):</span>
    <span>crk</span> <span>=</span> <span>decrypt</span><span>(</span><span>tag</span><span>,</span> <span>dec</span><span>(</span><span>cav</span><span>[</span><span>'challenge'</span><span>]))</span>
    <span>if</span> <span>crk</span> <span>==</span> <span>False</span><span>:</span> <span>return</span> <span>False</span>
    <span>discharge</span> <span>=</span> <span>None</span>
    <span>for</span> <span>dcs</span> <span>in</span> <span>discharges</span><span>:</span>
        <span>if</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>dcs</span><span>)[</span><span>0</span><span>]</span> <span>==</span> <span>cav</span><span>[</span><span>'ticket'</span><span>]:</span>
            <span>discharge</span> <span>=</span> <span>dcs</span>
            <span>break</span>
    <span>if</span> <span>not</span> <span>discharge</span><span>:</span> <span>return</span> <span>False</span>
    <span>mac</span> <span>=</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>discharge</span><span>)</span>
    <span>key</span> <span>=</span> <span>crk</span>
    <span># boring old stuff ---------------------
</span>    <span>tag</span> <span>=</span> <span>""</span>
    <span>for</span> <span>cav</span> <span>in</span> <span>mac</span><span>[:</span><span>-</span><span>1</span><span>]:</span>
        <span>tag</span> <span>=</span> <span>hmac</span><span>(</span><span>key</span><span>,</span> <span>cav</span><span>)</span>
        <span>key</span> <span>=</span> <span>tag</span>
    <span>return</span> <span>hm</span><span>.</span><span>compare_digest</span><span>(</span><span>tag</span><span>,</span> <span>dec</span><span>(</span><span>mac</span><span>[</span><span>-</span><span>1</span><span>]))</span>
</code></pre>
</div>

<p>To verify tokens that have third-party caveats, start with the root Macaroon, walking the caveats like usual. At each third-party caveat, match the <code>ticket</code> from the caveat with the <code>nonce</code> on the discharge Macaroon. The key for root Macaroon decrypts the <code>challenge</code> in the caveat, recovering <code>CRK</code>, which cryptographically verifies the discharge.</p>

<p>(The Macaroons paper uses different terms: “caveat identifier” or <code>cId</code> for “ticket”, and “verification-key identifier” or <code>vId</code> for “challenge”. These names are self-evidently bad and our contribution to the state of the art is to replace them.)</p>

<p>There’s two big applications for third-party caveats in Popular Macaroon Thought. First, they facilitate microservice-izing your auth logic, because you can stitch arbitrary policies together out of third-party caveats. And, they seem like <a href="https://github.com/go-macaroon-bakery/macaroon-bakery">fertile ground for an ecosystem of interoperable Macaroon services</a>: Okta and Google could stand up SSO dischargers, for instance, or someone can do a really good revocation service.</p>

<p>Neither of these light us up. We’re allergic to microservices. As for public protocols, well, it’s good to want things. So we almost didn’t even implement third-party caveats.</p>
<h2 id="5"><a href="#5" aria-label="Anchor"></a>5</h2>
<p>I’m glad we did though, because they’ve been pretty great.</p>

<p>The first problem third-party caveats solved for us was hazmat tokens. To the extent possible, we want Macaroon tokens to be safe to transmit between users. Our Macaroons express permissions, but not authentication, so it’s almost safe to email them.</p>

<p>The way it works is, our Macaroons all have a third-party caveat pointing to a “login service”, either identifying the proper bearer as a particular Fly.io user or as a member of some <code>Organization</code>. To allow a request with your token, you first need to collect the discharge from the login service, which requires authentication.</p>

<p>The login discharge is very sensitive, but there isn’t much reason to pass it around. The original permissions token is where all the interesting stuff is, and it’s not scary. So that’s nice.</p>

<p><img src="https://fly.io/blog/macaroons-escalated-quickly/assets/fly-sso.png?1/3&amp;wrap-left"></p>

<p>Ben then came up with <a href="https://community.fly.io/t/organization-required-sso/17560">third-party caveats that require Google or Github SSO logins.</a> If your token has one of those caveats, when you run <code>flyctl deploy</code>, a browser will pop up to log you into your SSO IdP (if you haven’t done so recently already).</p>

<p>We’ve put a <a href="https://fly.io/blog/tokenized-tokens/#tokenizer-the-fabled-4th-way">bunch of work into getting the guts of our SSO system working</a>, but that work has mostly been invisible to customers. But Macaroon-ized SSO has a subtle benefit: you can configure <a href="http://fly.io/">Fly.io</a> to automatically add SSO requirements to specific <code>Organizations</code> (so, for instance, a dev environment might not need SSO at all, and prod might need two).</p>

<p>SSO requirements in most applications are a brittle pain in the ass. Ours are flexible and straightforward, and that happened almost by accident. Macaroons, baby!</p>

<p>Here’s a fun thing you can do with a Macaroon system: stand up a Slack bot, and give it an HTTP <code>POST</code> handler that accepts third-party tickets. Then:</p>

<p><img src="https://fly.io/blog/macaroons-escalated-quickly/assets/bot-ok.png?1/2&amp;center&amp;border"></p>

<p>So, the bot is cute, but any platform could do that. What’s cool is the way our platform <em>doesn’t</em> work with Slack; in fact, nothing on our platform knows anything about Slack, and Slack doesn’t know anything about us. We didn’t reach out to a Slack endpoint. Everything was purely cryptographic.</p>

<p>That bot could, if I sunk some time into it, enforce arbitrary rules: it could selectively add caveats for the requests it authorizes, based on lookups of the users requesting them, at specific times of day, with specific logging. Theoretically, it could add third-party caveats of its own.</p>

<p>The win for us for third-party caveats is that they create a plugin system for our security tokens. That’s an unusual place to see a plugin interface! But Macaroons are easy to understand and keep in your head, so we’re pretty confident about the security issues.</p>
<h2 id="6"><a href="#6" aria-label="Anchor"></a>6</h2>
<p>Obviously, we didn’t write our Macaroon code in Python, or with HMAC-SHA256-CTR.</p>

<p>We landed on a primary implementation Golang (Ben subsequently wrote an Elixir implementation). Our hash is SHA256, our cipher is Chapoly. We encode in MsgPack.</p>
<p>We didn’t use the pre-existing public implementation because <a href="https://securitycryptographywhatever.com/2021/08/12/what-do-we-do-about-jwt-with-jonathan-rudenberg/">we were warned not to</a>. The Macaroon idea is simple, and it exists mostly as an academic paper, not a standard. The community that formed around building open source “standard” Macaroons decided to use untyped opaque blobs to represent candidates. We need things to be as rigidly unambiguous as they can be.</p>
<p><img src="https://fly.io/blog/macaroons-escalated-quickly/assets/verifier-service.png?2/3&amp;center"></p>

<p>The big strength of Macaroons as a cryptographic design — that it’s based almost entirely on HMAC — makes it a challenge to deploy. If you can verify a Macaroon, you can generate one.  We have thousands of servers. They can’t all be allowed to generate tokens.</p>

<p>What we did instead:</p>

<ul>
<li>We split token checking into “verification” of token HMAC tags and “clearing” of token caveats.
</li><li>Verification occurs only on a physically isolated token-verification service; to verify a token’s tag, you  HTTP <code>POST</code> the token to the verifier.
</li><li>Clearing of token caveats can happen anywhere. Token caveat clearing is domain-specific and subject to change; token verification is simple cryptography and  changes rarely.
</li><li>A token verification is cacheable. The client library for the token verifier does that, which speeds things up by exploiting the locality of token submissions.
</li><li>The verification service is backed by a <a href="https://fly.io/docs/litefs/">LiteFS-distributed SQLite database</a>, so verification is fast globally — a major step forward from our legacy OAuth2 tokens, which are only fast in Ashburn, VA.
</li></ul>

<p><img src="https://fly.io/blog/macaroons-escalated-quickly/assets/service-token.png?2/3&amp;center"></p>

<p>Now buckle up, because I’m about to try to get you to care about service tokens.</p>

<p>We operate “worker servers” all over the world to host apps for our customers. To do that, those workers need access to customer secrets, like the key to decrypt a customer volume. To retrieve those secrets, the workers have to talk to secrets management servers.</p>

<p>We manage a lot of workers. We trust them. But we don’t trust them that much, if you get my drift. You don’t want to just leave it up to the servers to decide which secrets they can access. The blast radius of a problem with a single worker should be no greater than the apps that are supposed to run there.</p>

<p>The gold standard for approving access to customer information is, naturally, explicit customer authorization. We almost have that with Macaroons! The first time an app runs on a worker, <a href="https://fly.io/blog/carving-the-scheduler-out-of-our-orchestrator/">the orchestrator code</a> has a token, and it can pass that along to the secret stores.</p>

<p>The problem is, you need that token more than once; not just when the user does a deploy, but potentially any time you restart the app or migrate it to a new worker. And you can’t just store and replay user Macaroons. They have expirations.</p>

<p>So our token verification service exposes an API that transforms a user token into a “service token”, which is just the token with the authentication caveat and expiration “stripped off”.</p>

<p>What’s cool is: components that receive service tokens can attenuate them. For instance, we could lock a token to a particular worker, or even a particular Fly Machine. Then we can expose the whole <a href="https://fly.io/docs/machines/working-with-machines/">Fly Machines API</a> to customer VMs while keeping access traceable to specific customer tokens. Stealing the token from a Fly Machine doesn’t help you since it’s locked to that Fly Machine by a caveat attackers can’t strip.</p>
<h2 id="7"><a href="#7" aria-label="Anchor"></a>7</h2>
<p>If a customer loses their tokens to an attacker, we can’t just blow that off and let the attacker keep compromising the account!</p>
<p>This cancels every token derived through attenuation by that nonce.</p>
<p>Every Macaroon we issue is identified by a unique nonce, and we can revoke tokens by that nonce. This is just a basic function of the token verification service we just described.</p>

<p>We host token caches all over our fleet. Token revocation invalidates the caches. Anything with a cache checks frequently whether to invalidate. Revocation is rare, so just keeping a revocation list and invalidating caches wholesale seems fine.</p>
<h2 id="8"><a href="#8" aria-label="Anchor"></a>8</h2>
<p>I get it, it’s tough to get me to shut up about Macaroons.</p>

<p>A couple years ago, I <a href="https://fly.io/blog/api-tokens-a-tedious-survey/">wrote a long survey of API token designs</a>, from JWTs (never!) to Biscuits. I had a <a href="https://fly.io/blog/api-tokens-a-tedious-survey/#macaroons">bunch to say about Macaroons</a>, not all of it positive, and said we’d be plowing forward with them at Fly.io.</p>

<p>My plan had been to follow up soon after with a deep dive on Macaroons as we planned them for Fly.io. I’m glad I didn’t do that, not just because it would’ve been embarrassing to announce a feature that took us over 2 years to launch, but also because the process of working on this with Ben Toews changed a lot of my thinking about them.</p>

<p>I think if you asked Ben, he’d say he had mixed feelings about how much complexity we wrangled to get this launched. On the other hand: we got a lot of things out of them without trying very hard:</p>

<ul>
<li>Security tokens you can (almost) email to your users and partners without putting your account at risk.
</li><li>A flexible permission system, encoded directly into the tokens, that users can drive without talking to our servers.
</li><li>A plugin system that users can (when we clean up the tooling) use themselves, to add things like Passkeys or two-person-approval rules or audit logging, without us getting in the middle.
</li><li>An SSO system that can stack different IdPs, mandate SSO login, and do that on a per-<code>Organization</code> basis. 
</li><li><a href="https://www.latacora.com/blog/2018/06/12/a-childs-garden/">Inter-service authorization</a> that is traceable back to customer actions, so our servers can’t just make up which apps they’re allowed to look at.
</li><li>An elegant way of exposing our own APIs to customer Fly Machines with ambient authentication, but without the <a href="https://github.com/SummitRoute/imdsv2_wall_of_shame/blob/main/README.md">AWS IMDSv1 credential theft problem</a>.
</li></ul>

<p>There are downsides and warts! I’m mostly not telling you about them! Pure restrictive caveats are an awkward way to express some roles. And, blinded by my hunger to get Macaroons deployed, I spat in the face of science and used internal database IDs as our public caveat format, an act for which JP will never forgive me.</p>

<p>If i’ve piqued your interest, <a href="https://github.com/superfly/macaroon">the code for this stuff is public</a>, along with some more <a href="https://github.com/superfly/macaroon/blob/main/macaroon-thought.md">detailed technical documentation</a>.</p>

          
        </section>
        <dl>
            <dt>
              Previous post  ↓
            </dt>
            <dd>
              <a href="https://fly.io/blog/how-i-fly-yoko-li/">
                How Yoko Li makes towns, tamagoes, and tools for local AI
              </a>
            </dd>
        </dl>
      </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Don't use NameCheap for the .fr TLD (132 pts)]]></title>
            <link>https://reddit.com/r/webdev/comments/1aesrm3/dont_use_namecheap_for_the_fr_tld/</link>
            <guid>39203315</guid>
            <pubDate>Wed, 31 Jan 2024 13:10:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reddit.com/r/webdev/comments/1aesrm3/dont_use_namecheap_for_the_fr_tld/">https://reddit.com/r/webdev/comments/1aesrm3/dont_use_namecheap_for_the_fr_tld/</a>, See on <a href="https://news.ycombinator.com/item?id=39203315">Hacker News</a></p>
<div id="readability-page-1" class="page"><p><span>
      <span>
        <faceplate-tracker source="post_credit_bar" action="click" noun="community">
          <a href="https://reddit.com/r/webdev/">
            
            <faceplate-screen-reader-content>
              Go to webdev
            </faceplate-screen-reader-content>
          </a>
        </faceplate-tracker>
      </span>

      
    </span>
    <span>
      
    <shreddit-async-loader bundlename="status_icons">
      <shreddit-status-icons thing-id="t3_1aesrm3">
        <svg rpl="" aria-label="Locked post" fill="currentColor" height="12" icon-name="lock-fill" viewBox="0 0 20 20" width="12" xmlns="http://www.w3.org/2000/svg">
      <path d="M16.375 8H15V5.312A5.17 5.17 0 0 0 10 0a5.169 5.169 0 0 0-5 5.312V8H3.625A1.629 1.629 0 0 0 2 9.63v7.74A1.629 1.629 0 0 0 3.625 19h12.75A1.629 1.629 0 0 0 18 17.37V9.63A1.629 1.629 0 0 0 16.375 8Zm-3.25 0h-6.25V5.313A3.3 3.3 0 0 1 10 1.875a3.3 3.3 0 0 1 3.125 3.438V8Z"></path>
    </svg>
        <svg rpl="" aria-label="Sticked post" fill="currentColor" height="12" icon-name="pin-fill" viewBox="0 0 20 20" width="12" xmlns="http://www.w3.org/2000/svg">
      <path d="M19.6 6.876 13.124.4a1.132 1.132 0 0 0-1.606 0l-.665.665a2.083 2.083 0 0 0-.379 2.435L6.68 7.292a2.085 2.085 0 0 0-2.432.374l-.665.665a1.14 1.14 0 0 0 0 1.612l2.53 2.53-5.82 5.82 1.414 1.414 5.82-5.82 2.53 2.53a1.132 1.132 0 0 0 1.606 0l.665-.665a2.084 2.084 0 0 0 .375-2.432L16.5 9.528a2.126 2.126 0 0 0 2.433-.375l.666-.666a1.142 1.142 0 0 0 .001-1.611Z"></path>
    </svg>
        <svg rpl="" aria-label="Archived post" fill="currentColor" height="12" icon-name="archived-fill" viewBox="0 0 20 20" width="12" xmlns="http://www.w3.org/2000/svg">
      <path d="M19 2.125v3.75A1.127 1.127 0 0 1 17.875 7H2.125A1.127 1.127 0 0 1 1 5.875v-3.75A1.127 1.127 0 0 1 2.125 1h15.75A1.127 1.127 0 0 1 19 2.125Zm-1 6.287v9.453a1.19 1.19 0 0 1-1.192 1.185H3.192A1.19 1.19 0 0 1 2 17.865V8.412c.042 0 .082.013.125.013h15.75c.043 0 .083-.011.125-.013ZM13.35 11h-6.7v1.5h6.7V11Z"></path>
    </svg>
      </shreddit-status-icons>
    </shreddit-async-loader>
  
      
      <shreddit-async-loader bundlename="shreddit_post_overflow_menu">
    <shreddit-post-overflow-menu is-desktop="" is-report-enabled="" subreddit-id="t5_2qs0q" post-permalink="/r/webdev/comments/1aesrm3/dont_use_namecheap_for_the_fr_tld/" subreddit-prefixed-name="r/webdev" can-update-commercial-communication-status="">
      
            
          
    </shreddit-post-overflow-menu>
  </shreddit-async-loader>
      
    </span>
  </p><div id="t3_1aesrm3-post-rtjson-content" data-post-click-location="text-body" slot="text-body">
    <p>
    Quick backstory,
  </p><p>
    I had i domain in .fr with them for the last 3 years. It's my main email address, very important to me. In the past, I had always renewed my domain in advance. But my main phone was broken for a bit this month and I realized my domain would be expiring in 10 day (this is important).
  </p><p>
    So I went on my NameCheap account to see that "Error Info: Renew not supported later than 12 days before expiry date for this TLD. ". So I hopped in a support chat, the agent told me to credit my account of the $18.56 for a renewal and that they would do it on their end.
  </p><p>
    Later that day, I received this email:
  </p><blockquote>
    <p>
    Hello,
  </p><p>
    We are contacting regarding your xxxxxx.fr domain name.
  </p><p>
    To our regret, your domain is now in the redemption state. In order to reinstate it in this phase, there is an redemption fee plus renewal price. The total amount, in this case, would be $98.98.
  </p>
  </blockquote><p>
    So my not yet expired domain is in redemption state ?!
  </p><p>
    After a bit a search, I found that my actual registra is 1API (WHOIS info). Let's see why they have this rule in place... <a href="https://wiki.hexonet.net/wiki/FR" rel="noopener nofollow ugc" target="_blank">https://wiki.hexonet.net/wiki/FR</a> ( note : hexonet and 1API are the same company)
  </p><p>
    And .... They don't, I start another support chat and ask the dicey question.
  </p><p>
    Me : What is the technical reason ?
  </p><p>
    Agent 1 : Allow me 5-10 minutes to contact our team in this regard.
  </p><p>
    Agent 2 : Thank you for waiting. The rule about the renewal of .FR domains are set up not by Namecheap, but by our upstream provider. The "Domain renew by" date is indicated in the Namecheap account, you should renew (have the domain auto-renewed) before that date. If not renewed. it enters the redemption period and will require a redemption fee to be paid along with the regular renewal cost. The domains that were not renewed on time will have "Alert" status and are removed from the Namecheap account the next day after the expiration. If you'd like to transfer the domain, our Concierge department will help you with this.
  </p><p>
    Me : is your upstream provider 1API ?
  </p><p>
    Agent 2 : No, it is not our upstream provider.
  </p><p>
    Agent 2 : I cannot disclose our internal information, regrettably.
  </p><p>
    Yes, a 10 million customer company use a company that use other company for one of the main TLD.
  </p><p>
    To end this story, I have transferred my domain to OVH for 6€, and they renewed it for free!
  </p><p>
    TLDR: Don't use NameCheap for <a href="https://www.namecheap.com/domains/registration/cctld/ch/" rel="noopener nofollow ugc" target="_blank">.CH</a>, <a href="https://www.namecheap.com/domains/registration/cctld/es/" rel="noopener nofollow ugc" target="_blank">.ES</a>, <a href="https://www.namecheap.com/domains/registration/cctld/fr/" rel="noopener nofollow ugc" target="_blank">.FR</a>, <a href="https://www.namecheap.com/domains/registration/cctld/li/" rel="noopener nofollow ugc" target="_blank">.LI</a>, <a href="https://www.namecheap.com/domains/registration/cctld/pe/" rel="noopener nofollow ugc" target="_blank">.PE</a>, <a href="https://www.namecheap.com/domains/registration/cctld/sg/" rel="noopener nofollow ugc" target="_blank">.SG</a>, <a href="https://www.namecheap.com/domains/registration/cctld/com-sg/" rel="noopener nofollow ugc" target="_blank">.COM.SG</a>, <a href="https://www.namecheap.com/domains/registration/cctld/com-au/" rel="noopener nofollow ugc" target="_blank">.COM.AU</a>, <a href="https://www.namecheap.com/domains/registration/cctld/org-au/" rel="noopener nofollow ugc" target="_blank">.ORG.AU</a>, <a href="https://www.namecheap.com/domains/registration/cctld/net-au/" rel="noopener nofollow ugc" target="_blank">.NET.AU</a> domains.
  </p><p>
    <a href="https://www.namecheap.com/support/knowledgebase/article.aspx/9916/2207/tlds-grace-periods/" rel="noopener nofollow ugc" target="_blank">https://www.namecheap.com/support/knowledgebase/article.aspx/9916/2207/tlds-grace-periods/</a>
  </p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[There's More Proof That Return to Office Is Pointless (108 pts)]]></title>
            <link>https://gizmodo.com/more-proof-that-return-to-office-is-pointless-1851209231</link>
            <guid>39202992</guid>
            <pubDate>Wed, 31 Jan 2024 12:34:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gizmodo.com/more-proof-that-return-to-office-is-pointless-1851209231">https://gizmodo.com/more-proof-that-return-to-office-is-pointless-1851209231</a>, See on <a href="https://news.ycombinator.com/item?id=39202992">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure data-id="376aa324f0c8fc12f5436642a370b0e6" data-recommend-id="image://376aa324f0c8fc12f5436642a370b0e6" data-format="jpg" data-width="5617" data-height="3153" data-lightbox="true" data-recommended="true" data-hide="false" contenteditable="false" draggable="false"><div contenteditable="false" data-link-reference="" data-link-target="" data-syndicationrights="true" data-imagerights="shutterstock" data-hide="false" data-hidecredit="false"><p><span><div><picture><source media="(max-width: 37.31em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/376aa324f0c8fc12f5436642a370b0e6.jpg"><source media="(min-width: 37.37em)" type="image/jpeg" srcset="https://i.kinja-img.com/image/upload/c_fit,q_60,w_1315/376aa324f0c8fc12f5436642a370b0e6.jpg"><img alt="Image for article titled There’s More Proof That Return to Office Is Pointless" data-chomp-id="376aa324f0c8fc12f5436642a370b0e6" data-format="jpg" data-alt="Image for article titled There’s More Proof That Return to Office Is Pointless" data-anim-src="" src="https://i.kinja-img.com/image/upload/c_fit,q_60,w_645/376aa324f0c8fc12f5436642a370b0e6.jpg"></picture></div></span></p><p><figcaption>Photo<!-- -->: <!-- -->Dean Drobot<!-- --> (<!-- -->Shutterstock<!-- -->)</figcaption></p></div><span data-id="376aa324f0c8fc12f5436642a370b0e6" data-recommend-id="image://376aa324f0c8fc12f5436642a370b0e6" data-format="jpg" data-width="5617" data-height="3153" data-lightbox="true" data-recommended="true" data-hide="false"></span></figure><div><p>Your manager may suggest that <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/amazon-ceo-tells-workers-to-return-to-office-or-fired-1850783235&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/amazon-ceo-tells-workers-to-return-to-office-or-fired-1850783235">returning to the office</a></span> is imperative for the company’s success, workplace culture, and overall productivity. However, there’s a <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/four-day-work-week-work-from-home-return-to-office-1849562791&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/four-day-work-week-work-from-home-return-to-office-1849562791">growing body of evidence</a></span> suggesting that’s bullshit. New <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://deliverypdf.ssrn.com/delivery.php?ID=011106009067112120102007114118111024038039000065003034103087106087095103085122071078010118005034010099113069111020006086125114039035093009046118064127089020088111025086069046115071066088115108002030121095001081025031094123064095019127083112075017103066&amp;EXT=pdf&amp;INDEX=TRUE&quot;,{&quot;metric25&quot;:1}]]" href="https://deliverypdf.ssrn.com/delivery.php?ID=011106009067112120102007114118111024038039000065003034103087106087095103085122071078010118005034010099113069111020006086125114039035093009046118064127089020088111025086069046115071066088115108002030121095001081025031094123064095019127083112075017103066&amp;EXT=pdf&amp;INDEX=TRUE" target="_blank" rel="noopener noreferrer">research</a></span> out of the University of Pittsburgh examined 137 of America’s largest corporations and found that return-to-office mandates did not result in significant improvements to firm performance.</p><div data-video-id="196376" data-monetizable="true" data-position="sidebar" data-video-title="Alex Winter on the Most Important Modern Horror Movie " data-video-blog-id="4" data-video-network="gizmodo" data-video-duration="193" data-playlist="196376,196371,196355" data-current="196376"><div><p>Alex Winter on the Most Important Modern Horror Movie </p></div><video disablepictureinpicture="" muted="" playsinline="" width="100%" height="100%" crossorigin="anonymous" preload="none"><source data-src="https://vid.kinja.com/prod/196376/196376_240p.mp4" label="240p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196376/196376_480p.mp4" label="480p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196376/196376_720p.mp4" label="720p" type="video/mp4"><source data-src="https://vid.kinja.com/prod/196376/196376_1080p.mp4" label="1080p" type="video/mp4"><track kind="captions" label="English" src="https://kinja.com/api/videoupload/caption/21923.vtt" srclang="en"></video><div><ul><li data-label="">Off</li><li data-label="English">English</li></ul></div></div><p>“Using a sample of S&amp;P 500 firms, we examine determinants and consequences of U.S. firms’ return-to-office (RTO) mandates,” said researchers from the Katz Graduate School of Business at the University of Pittsburgh. The study found that managers use RTO mandates “to reassert control over employees and blame employees as a scapegoat,” and concluded that “we do not find significant changes in firm performance in terms of profitability and stock market valuation after the RTO mandates.”</p><p>The study analyzed all 137 firms on the S&amp;P 500 that have publicly announced RTO plans since the coronavirus pandemic introduced remote work. Researchers also looked into the motivation behind RTO mandates and found that firms led by male CEOs with greater power are more inclined to enforce an RTO policy. Power was defined as the CEO’s total compensation divided by the average compensation of the firm’s highest-paid executives. There was no correlation between a CEO’s political affiliation or percentage of stake in the company and an RTO mandate.</p><p>Unsurprisingly, employee satisfaction plummeted after RTO mandates. Employee ratings of work-life balance, senior management, and cultural values all had significant decreases in the face of return-to-office policies.</p><p>In the post-pandemic era, corporate management has become consumed with <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/work-from-home-jobs-remote-work-tech-jobs-1850269974&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/work-from-home-jobs-remote-work-tech-jobs-1850269974">getting employees back in the office</a></span> making it one of the most hotly contested debates in workplace culture. Even <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;Internal link&quot;,&quot;https://gizmodo.com/zoom-ceo-return-to-office-leaked-audio-1850769660&quot;,{&quot;metric25&quot;:1}]]" href="https://gizmodo.com/zoom-ceo-return-to-office-leaked-audio-1850769660">Zoom’s CEO asked employees to get back to the office</a></span>, saying it’s hard to build trust over Zoom. This study, published Jan. 18, adds to several other studies that have shown <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.zdnet.com/home-and-office/work-life/no-working-from-home-doesnt-harm-productivity-says-study/&quot;,{&quot;metric25&quot;:1}]]" href="https://www.zdnet.com/home-and-office/work-life/no-working-from-home-doesnt-harm-productivity-says-study/" target="_blank" rel="noopener noreferrer">remote work is very productive</a></span> and <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://fortune.com/2023/02/16/return-office-real-reason-slump-productivity-data-careers-gleb-tsipursky/&quot;,{&quot;metric25&quot;:1}]]" href="https://fortune.com/2023/02/16/return-office-real-reason-slump-productivity-data-careers-gleb-tsipursky/" target="_blank" rel="noopener noreferrer">RTO mandates could be causing a productivity slump</a></span>.</p><p>Return to office mandates are still going strong. The <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.wsj.com/business/earnings/ups-to-cut-12-000-jobs-as-delivery-business-slows-d832f1d2&quot;,{&quot;metric25&quot;:1}]]" href="https://www.wsj.com/business/earnings/ups-to-cut-12-000-jobs-as-delivery-business-slows-d832f1d2" target="_blank" rel="noopener noreferrer">United Parcel Service is mandating staff return to the office</a></span> five days a week, according to the Wall Street Journal on Tuesday. The package shipper hopes this change, and 12,000 layoffs, will help the company bounce back from a slowdown in shipments. IBM also delivered managers still working remotely a stern message on Monday: <span><a data-ga="[[&quot;Embedded Url&quot;,&quot;External link&quot;,&quot;https://www.bloomberg.com/news/articles/2024-01-29/ibm-to-managers-move-near-an-office-or-leave-company?sref=P6Q0mxvj&quot;,{&quot;metric25&quot;:1}]]" href="https://www.bloomberg.com/news/articles/2024-01-29/ibm-to-managers-move-near-an-office-or-leave-company?sref=P6Q0mxvj" target="_blank" rel="noopener noreferrer">move near an office or leave the company</a></span>.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[23andMe's Fall (283 pts)]]></title>
            <link>https://www.wsj.com/health/healthcare/23andme-anne-wojcicki-healthcare-stock-913468f4</link>
            <guid>39202583</guid>
            <pubDate>Wed, 31 Jan 2024 11:36:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/health/healthcare/23andme-anne-wojcicki-healthcare-stock-913468f4">https://www.wsj.com/health/healthcare/23andme-anne-wojcicki-healthcare-stock-913468f4</a>, See on <a href="https://news.ycombinator.com/item?id=39202583">Hacker News</a></p>
Couldn't get https://www.wsj.com/health/healthcare/23andme-anne-wojcicki-healthcare-stock-913468f4: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Citation cartels help mathematicians-and their universities-climb the rankings (192 pts)]]></title>
            <link>https://www.science.org/content/article/citation-cartels-help-some-mathematicians-and-their-universities-climb-rankings</link>
            <guid>39202571</guid>
            <pubDate>Wed, 31 Jan 2024 11:34:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.science.org/content/article/citation-cartels-help-some-mathematicians-and-their-universities-climb-rankings">https://www.science.org/content/article/citation-cartels-help-some-mathematicians-and-their-universities-climb-rankings</a>, See on <a href="https://news.ycombinator.com/item?id=39202571">Hacker News</a></p>
Couldn't get https://www.science.org/content/article/citation-cartels-help-some-mathematicians-and-their-universities-climb-rankings: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[The Pentagon tried to hide that it bought Americans' data without a warrant (131 pts)]]></title>
            <link>https://www.wired.com/story/pentagon-data-purchases-wyden-letter/</link>
            <guid>39201660</guid>
            <pubDate>Wed, 31 Jan 2024 09:11:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/pentagon-data-purchases-wyden-letter/">https://www.wired.com/story/pentagon-data-purchases-wyden-letter/</a>, See on <a href="https://news.ycombinator.com/item?id=39201660">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>United States officials fought to conceal details of arrangements between US spy agencies and private companies tracking the whereabouts of Americans via their cell phones. Obtaining location data from US phones normally requires a warrant, but police and intelligence agencies routinely pay companies instead for the data, effectively circumventing the courts.</p><p>Ron Wyden, the US senator from Oregon, informed the nation’s intelligence chief, Avril Haines, on Thursday that the Pentagon only agreed to release details about the data purchases, which had always been unclassified, after Wyden hindered the Senate's efforts to appoint a new director of the National Security Agency. “The secrecy around data purchases was amplified,” Wyden wrote, “because intelligence agencies have sought to keep the American people in the dark."</p><p>Wyden's office says it’s been investigating sales of location data to the government for years, uncovering multiple ties between the Department of Defense and what the senator refers to as “shady companies" committing “flagrant violations” of people's privacy. The companies' practices are “not just unethical, but illegal,” he says.</p><p>Pentagon offices known to have purchased location data from these companies include the Defense Intelligence Agency and the NSA, among others. Wyden's letter, first reported by <a href="https://www.nytimes.com/2024/01/25/us/politics/nsa-internet-privacy-warrant.html?unlocked_article_code=1.QU0.YdWh.n51FMReztrOH"><em>The New York Times</em></a>, indicates that the NSA is also “buying Americans' domestic internet metadata.”</p><p>Wyden's disclosure comes amid a fight in the US House of Representatives over efforts to outlaw the purchases entirely. Last month, members of the House Judiciary Committee attached legislation doing so, known as the <em>Fourth Amendment Is Not For Sale Act</em>, to a bill reauthorizing a contentious surveillance program known as Section 702.</p><p>The bill, originally coauthored by Wyden, nearly received a vote last month during a showdown with rival legislation introduced by the House Intelligence Committee that does not seek to ban the purchases. Congressional sources tell WIRED the vote was called off at the last minute after Biden administration officials and members of the intelligence committee staged a campaign against the privacy-enhancing measures.</p><p>Intelligence officials in the House held separate meetings with members and their aides aiming to discourage support for the judiciary bill—the <em>Protect Liberty Act</em>—alleging that new warrant requirements would be overly burdensome for law enforcement, despite a slew of exemptions for cyberwarfare, terrorism, and espionage threats.</p><p>Six sources who attended the meetings told WIRED that intelligence committee members used images of Hamas militants in presentations to drive home its argument for relaxing limits on domestic surveillance. The message, Republican aides said, was, “it could happen here.” Three Democrats who attended meetings with representatives from the FBI, CIA, and NSA, among other agencies, described the presentation as a “scare tactic.”</p><p>The home surveillance debate, which has exploded in recent months, hampering the passing of routine legislation, has largely focused on Section 702, an authority under which the government monitors the calls, texts, and emails of foreign nationals. Section 702 is set to expire in under four months.</p><p>Both the <em>Protect Liberty Act</em> and its intelligence committee rival—the <em>FISA Reform and Reauthorization Act</em>—aim to reauthorize Section 702 into the future. In how that's accomplished the bills are radically different. With access by the FBI to foreign intelligence for domestic investigations being the biggest point of contention, federal lawmakers can now effectively be divided into two factions: people who support surveillance warrants and people who don't.</p><p>The pro-warrant <em>Protect Liberty Ac</em>t could receive a vote as early as next month, with its provisions banning the government from buying data as a means of evading warrant requirements. Republicans on the Hill say they can't be sure whether House Speaker Mike Johnson will allow a vote, however, due to the intense amount of pressure he faces from the intelligence system.</p><p>“There is a lot of baloney going around about surveillance reform," Wyden says. “Probably because some surveillance supporters are worried they won’t win an honest debate.”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gitlab's ActivityPub architecture blueprint (207 pts)]]></title>
            <link>https://docs.gitlab.com/ee/architecture/blueprints/activity_pub/</link>
            <guid>39201453</guid>
            <pubDate>Wed, 31 Jan 2024 08:37:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.gitlab.com/ee/architecture/blueprints/activity_pub/">https://docs.gitlab.com/ee/architecture/blueprints/activity_pub/</a>, See on <a href="https://news.ycombinator.com/item?id=39201453">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" itemscope="" itemprop="mainContentOfPage">

<table>
<thead>
<tr>
<th>Status
</th><th>Authors
</th><th>Coach
</th><th>DRIs
</th><th>Owning Stage
</th><th>Created
</th></tr></thead><tbody>
<tr>
<td>
<span>
<span>proposed</span>
</span>
</td><td>
<code><a href="https://gitlab.com/oelmekki">@oelmekki</a></code>
<code><a href="https://gitlab.com/jpcyiza">@jpcyiza</a></code>
</td><td>
<code><a href="https://gitlab.com/tkuah">@tkuah</a></code>
</td><td>
<code><a href="https://gitlab.com/derekferguson">@derekferguson</a></code>
</td><td>
<span>
<span></span>
</span>
</td><td>2023-09-12
</td></tr></tbody></table>
<ul id="markdown-toc">
<li><a href="#summary" id="markdown-toc-summary">Summary</a>
</li><li>
<a href="#description-of-the-related-tech-and-terms" id="markdown-toc-description-of-the-related-tech-and-terms">Description of the related tech and terms</a> <ul>
<li><a href="#the-fediverse" id="markdown-toc-the-fediverse">The Fediverse</a>
</li><li><a href="#activitypub" id="markdown-toc-activitypub">ActivityPub</a>
</li></ul>
</li><li>
<a href="#motivation" id="markdown-toc-motivation">Motivation</a> <ul>
<li><a href="#goals" id="markdown-toc-goals">Goals</a>
</li><li><a href="#non-goals" id="markdown-toc-non-goals">Non-Goals</a>
</li></ul>
</li><li><a href="#proposal" id="markdown-toc-proposal">Proposal</a>
</li><li>
<a href="#design-and-implementation-details" id="markdown-toc-design-and-implementation-details">Design and implementation details</a> <ul>
<li><a href="#production-readiness" id="markdown-toc-production-readiness">Production readiness</a>
</li><li>
<a href="#the-social-following-part" id="markdown-toc-the-social-following-part">The social following part</a> <ul>
<li><a href="#endpoints" id="markdown-toc-endpoints">Endpoints</a>
</li><li><a href="#serializers" id="markdown-toc-serializers">Serializers</a>
</li><li><a href="#subscription" id="markdown-toc-subscription">Subscription</a>
</li><li><a href="#unfollow" id="markdown-toc-unfollow">Unfollow</a>
</li><li><a href="#sending-activities-out" id="markdown-toc-sending-activities-out">Sending activities out</a>
</li><li><a href="#webfinger" id="markdown-toc-webfinger">Webfinger</a>
</li><li><a href="#http-signatures" id="markdown-toc-http-signatures">HTTP signatures</a>
</li></ul>
</li><li><a href="#host-allowlist-and-denylist" id="markdown-toc-host-allowlist-and-denylist">Host allowlist and denylist</a>
</li><li><a href="#limits-and-rollout" id="markdown-toc-limits-and-rollout">Limits and rollout</a>
</li><li><a href="#the-cross-instance-issues-and-merge-requests-part" id="markdown-toc-the-cross-instance-issues-and-merge-requests-part">The cross-instance issues and merge requests part</a>
</li></ul>
</li></ul>

<h2 id="summary">Summary<a href="#summary" title="Permalink"></a>
</h2>
<p>The end goal of this proposal is to build interoperability features into
GitLab so that it’s possible on one instance of GitLab to open a merge
request to a project hosted on an other instance, merging all willing
instances in a global network.
</p><p>To achieve that, we propose to use ActivityPub, the w3c standard used by
the Fediverse. This will allow us to build upon a robust and battle-tested
protocol, and it will open GitLab to a wider community.
</p><p>Before starting implementing cross-instance merge requests, we want to
start with smaller steps, helping us to build up domain knowledge about
ActivityPub and creating the underlying architecture that will support the
more advanced features. For that reason, we propose to start with
implementing social features, allowing people on the Fediverse to subscribe
to activities on GitLab, for example to be notified on their social network
of choice when their favorite project hosted on GitLab makes a new release.
As a bonus, this is an opportunity to make GitLab more social and grow its
audience.
</p>
<p>Feel free to jump to <a href="#motivation">Motivation</a> if you already know what
ActivityPub and the Fediverse are.
</p><p>Among the push for <a href="https://en.wikipedia.org/wiki/Decentralized_web">decentralization of the web</a>,
several projects tried different protocols with different ideals behind their reasoning.
Some examples:
</p><ul>
<li>
<a href="https://en.wikipedia.org/wiki/Secure_Scuttlebutt">Secure Scuttlebutt</a> (or SSB for short)
</li><li><a href="https://en.wikipedia.org/wiki/Dat_%28software%29">Dat</a>
</li><li>
<a href="https://en.wikipedia.org/wiki/InterPlanetary_File_System">IPFS</a>,
</li><li><a href="https://en.wikipedia.org/wiki/Solid_%28web_decentralization_project%29">Solid</a>
</li></ul>
<p>One gained traction recently: <a href="https://en.wikipedia.org/wiki/ActivityPub">ActivityPub</a>,
better known for the colloquial <a href="https://en.wikipedia.org/wiki/Fediverse">Fediverse</a> built
on top of it, through applications like
<a href="https://en.wikipedia.org/wiki/Mastodon_%28social_network%29">Mastodon</a>
(which could be described as some sort of decentralized Facebook) or
<a href="https://en.wikipedia.org/wiki/Lemmy_%28software%29">Lemmy</a> (which could be
described as some sort of decentralized Reddit).
</p><p>ActivityPub has several advantages that makes it attractive
to implementers and could explain its current success:
</p><ul>
<li>
<strong>It’s built on top of HTTP</strong>. You don’t need to install new software or
to tinker with TCP/UDP to implement ActivityPub, if you have a webserver
or an application that provides an HTTP API (like a rails application),
you already have everything you need.
</li><li>
<strong>It’s built on top of JSON</strong>. All communications are basically JSON
objects, which web developers are already used to, which simplifies adoption.
</li><li>
<strong>It’s a W3C standard and already has multiple implementations</strong>. Being
piloted by the W3C is a guarantee of stability and quality work. They
have profusely demonstrated in the past through their work on HTML, CSS
or other web standards that we can build on top of their work without
the fear of it becoming deprecated or irrelevant after a few years.
</li></ul>
<h3 id="the-fediverse">The Fediverse<a href="#the-fediverse" title="Permalink"></a>
</h3>
<p>The core idea behind Mastodon and Lemmy is called the Fediverse. Rather
than full decentralization, those applications rely on federation, in the
sense that there still are servers and clients. It’s not P2P like SSB,
Dat and IPFS, but instead a galaxy of servers chatting with each other
instead of having central servers controlled by a single entity.
</p><p>The user signs up to one of those servers (called <strong>instances</strong>), and they
can then interact with users either on this instance, or on other ones.
From the perspective of the user, they access a global network, and not
only their instance. They see the articles posted on other instances, they
can comment on them, upvote them, etc.
</p><p>What happens behind the scenes:
their instance knows where the user they reply to is hosted. It
contacts that other instance to let them know there is a message for them -
somewhat similar to SMTP. Similarly, when a user subscribes
to a feed, their instance informs the instance where the feed is
hosted of this subscription. That target instance then posts back
messages when new activities are created. This allows for a push model, rather
than a constant poll model like RSS. Of course, what was just described is
the happy path; there is moderation, validation and fault tolerance
happening all the way.
</p><h3 id="activitypub">ActivityPub<a href="#activitypub" title="Permalink"></a>
</h3>
<p>Behind the Fediverse is the ActivityPub protocol. It’s a HTTP API
attempting to be as general a social network implementation as possible,
while giving options to be extendable.
</p><p>The basic idea is that an <code>actor</code> sends and receives <code>activities</code>. Activities
are structured JSON messages with well-defined properties, but are extensible
to cover any need. An actor is defined by four endpoints, which are
contacted with the
<code>application/ld+json; profile="https://www.w3.org/ns/activitystreams"</code> HTTP Accept header:
</p><ul>
<li>
<code>GET /inbox</code>: used by the actor to find new activities intended for them.
</li><li>
<code>POST /inbox</code>: used by instances to push new activities intended for the actor.
</li><li>
<code>GET /outbox</code>: used by anyone to read the activities created by the actor.
</li><li>
<code>POST /outbox</code>: used by the actor to publish new activities.
</li></ul>
<p>Among those, Mastodon and Lemmy only use <code>POST /inbox</code> and <code>GET /outbox</code>, which
are the minimum needed to implement federation:
</p><ul>
<li>Instances push new activities for the actor on the inbox.
</li><li>Reading the outbox allows reading the feed of an actor.
</li></ul>
<p>Additionally, Mastodon and Lemmy implement a <code>GET /</code> endpoint (with the
mentioned Accept header). This endpoint responds with general information about the
actor, like name and URL of the inbox and outbox. While not required by the
standard, it makes discovery easier.
</p><p>While a person is the main use case for an actor, an actor does not
necessarily map to a person. Anything can be an actor: a topic, a
subreddit, a group, an event. For GitLab, anything with activities (in the sense
of what GitLab means by “activity”) can be an ActivityPub actor. This includes
items like projects, groups, and releases. In those more abstract examples,
an actor can be thought of as an actionable feed.
</p><p>ActivityPub by itself does not cover everything that is needed to implement
the Fediverse. Most notably, these are left for the implementers to figure out:
</p><ul>
<li>Finding a way to deal with spam. Spam is handled by authorizing or
blocking (“defederating”) other instances.
</li><li>Discovering new instances.
</li><li>Performing network-wide searches.
</li></ul>
<h2 id="motivation">Motivation<a href="#motivation" title="Permalink"></a>
</h2>
<p>Why would a social media protocol be useful for GitLab? People want a single,
global GitLab network to interact between various projects, without having to
register on each of their hosts.
</p><p>Several very popular discussions around this have already happened:
</p><ul>
<li><a href="https://gitlab.com/gitlab-org/gitlab/-/issues/21582">Share events externally via ActivityPub</a>
</li><li><a href="https://gitlab.com/gitlab-org/gitlab/-/issues/14116">Implement cross-server (federated) merge requests</a>
</li><li>
<a href="https://gitlab.com/groups/gitlab-org/-/epics/260">Distributed merge requests</a>.
</li></ul>
<p>The ideal workflow would be:
</p><ol>
<li>Alice registers to her favorite GitLab instance, like <code>gitlab.example.org</code>.
</li><li>She looks for a project on a given topic, and sees Bob’s project, even though
Bob is on <code>gitlab.com</code>.
</li><li>Alice selects <strong>Fork</strong>, and the <code>gitlab.com/Bob/project.git</code> is
forked to <code>gitlab.example.org/Alice/project.git</code>.
</li><li>She makes her edits, and opens a merge request, which appears in Bob’s
project on <code>gitlab.com</code>.
</li><li>Alice and Bob discuss the merge request, each one from their own GitLab
instance.
</li><li>Bob can send additional commits, which are picked up by Alice’s instance.
</li><li>When Bob accepts the merge request, his instance picks up the code from
Alice’s instance.
</li></ol>
<p>In this process, ActivityPub would help in:
</p><ul>
<li>Letting Bob know a fork happened.
</li><li>Sending the merge request to Bob.
</li><li>Enabling Alice and Bob to discuss the merge request.
</li><li>Letting Alice know the code was merged.
</li></ul>
<p>It does <em>not</em> help in these cases, which need specific implementations:
</p><ul>
<li>Implementing a network-wide search.
</li><li>Implementing cross-instance forks. (Not needed, thanks to Git.)
</li></ul>
<p>Why use ActivityPub here rather than implementing cross-instance merge requests
in a custom way? Two reasons:
</p><ol>
<li>
<strong>Building on top of a standard helps reach beyond GitLab</strong>.
While the workflow presented above only mentions GitLab, building on top
of a W3C standard means other forges can follow GitLab
there, and build a massive Fediverse of code sharing.
</li><li>
<strong>An opportunity to make GitLab more social</strong>. To prepare the
architecture for the workflow above, smaller steps can be taken, allowing
people to subscribe to activity feeds from their Fediverse social
network. Anything that has a RSS feed could become an ActivityPub feed.
People on Mastodon could follow their favorite developer, project, or topic
from GitLab and see the news in their feed on Mastodon, hopefully raising
engagement with GitLab.
</li></ol>
<h3 id="goals">Goals<a href="#goals" title="Permalink"></a>
</h3>
<ul>
<li>allowing to share interesting events on ActivityPub based social media
</li><li>allowing to open an issue and discuss it from one instance to an other
</li><li>allowing to fork a project from one instance to an other
</li><li>allowing to open a merge request, discuss it and merge it from one instance to an other
</li><li>allowing to perform a network wide search?
</li></ul>
<h3 id="non-goals">Non-Goals<a href="#non-goals" title="Permalink"></a>
</h3>
<ul>
<li>federation of private resources
</li><li>allowing to perform a network wide search?
</li></ul>
<h2 id="proposal">Proposal<a href="#proposal" title="Permalink"></a>
</h2>
<p>The idea of this implementation path is not to take the fastest route to
the feature with the most value added (cross-instance merge requests), but
to go on with the smallest useful step at each iteration, making sure each step
brings something immediately useful.
</p><ol>
<li>
<strong>Implement ActivityPub for social following</strong>.
After this, the Fediverse can follow activities on GitLab instances.
<ol>
<li>ActivityPub to subscribe to project releases.
</li><li>ActivityPub to subscribe to project creation in topics.
</li><li>ActivityPub to subscribe to project activities.
</li><li>ActivityPub to subscribe to group activities.
</li><li>ActivityPub to subscribe to user activities.
</li></ol>
</li><li>
<strong>Implement cross-instance forks</strong> to enable forking a project from an other instance.
</li><li>
<strong>Implement ActivityPub for cross-instance discussions</strong> to enable discussing
issues and merge requests from another instance:
<ol>
<li>In issues.
</li><li>In merge requests.
</li></ol>
</li><li>
<strong>Implement ActivityPub to submit cross-instance merge requests</strong> to enable
submitting merge requests to other instances.
</li><li>
<strong>Implement cross-instance search</strong> to enable discovering projects on other instances.
</li></ol>
<p>It’s open to discussion if this last step should be included at all.
Currently, in most Fediverse apps, when you want to display a resource from
an instance that your instance does not know about (typically a user you
want to follow), you paste the URL of the resource in the search box of
your instance, and it fetches and displays the remote resource, now
actionable from your instance. We plan to do that at first.
</p><p>The question is : do we keep it at that? This UX has severe frictions,
especially for users not used to Fediverse UX patterns (which is probably
most GitLab users). On the other hand, distributed search is a subject
complicated enough to deserve its own blueprint (although it’s not as
complicated as it used to be, now that decentralization protocols and
applications worked on it for a while).
</p><h2 id="design-and-implementation-details">Design and implementation details<a href="#design-and-implementation-details" title="Permalink"></a>
</h2>
<p>First, it’s a good idea to get familiar with the specifications of the
three standards we’re going to use:
</p><ul>
<li>
<a href="https://www.w3.org/TR/activitypub/">ActivityPub</a> defines the HTTP
requests happening to implement federation.
</li><li>
<a href="https://www.w3.org/TR/activitystreams-core/">ActivityStreams</a> defines the
format of the JSON messages exchanged by the users of the protocol.
</li><li>
<a href="https://www.w3.org/TR/activitystreams-vocabulary/">Activity Vocabulary</a>
defines the various messages recognized by default.
</li></ul>
<p>Feel free to ping <a href="https://gitlab.com/oelmekki">@oelmekki</a> if you have questions or find the documents too
dense to follow.
</p><h3 id="production-readiness">Production readiness<a href="#production-readiness" title="Permalink"></a>
</h3>
<p>TBC
</p>
<p>This part is laying the ground work allowing to
<a href="https://docs.gitlab.com/ee/development/activitypub/actors/index.html">add new ActivityPub actors</a> to
GitLab.
</p><p>There are 5 actors we want to implement:
</p><ul>
<li>the <code>releases</code> actor, to be notified when given project makes a new
release
</li><li>the <code>topic</code> actor, to be notified when a new project is added to a topic
</li><li>the <code>project</code> actor, regarding all activities from a project
</li><li>the <code>group</code> actor, regarding all activities from a group
</li><li>the <code>user</code> actor, regarding all activities from a user
</li></ul>
<p>We’re only dealing with public resources for now. Allowing federation of
private resources is a tricky subject that will be solved later, if it’s
possible at all.
</p><h4 id="endpoints">Endpoints<a href="#endpoints" title="Permalink"></a>
</h4>
<p>Each actor needs 3 endpoints:
</p><ul>
<li>the profile endpoint, containing basic info, like name, description, but
also including links to the inbox and outbox
</li><li>the outbox endpoint, allowing to show previous activities for an actor
</li><li>the inbox endpoint, on which to post to submit follow and unfollow
requests (among other things we won’t use for now).
</li></ul>
<p>The controllers providing those endpoints are in
<code>app/controllers/activity_pub/</code>. It’s been decided to use this namespace to
avoid mixing the ActivityPub JSON responses with the ones meant for the
frontend, and also because we may need further namespacing later, as the
way we format activities may be different for one Fediverse app, for an
other, and for our later cross-instance features. Also, this namespace
allow us to easily toggle what we need on all endpoints, like making sure
no private project can be accessed.
</p><h4 id="serializers">Serializers<a href="#serializers" title="Permalink"></a>
</h4>
<p>The serializers in <code>app/serializers/activity_pub/</code> are the meat of our
implementation, are they provide the ActivityStreams objects. The abstract
class <code>ActivityPub::ActivityStreamsSerializer</code> does all the heavy lifting
of validating developer provided data, setting up the common fields and
providing pagination.
</p><p>That pagination part is done through <code>Gitlab::Serializer::Pagination</code>, which
uses offset pagination.
<a href="https://gitlab.com/gitlab-org/gitlab/-/issues/424148">We need to allow it to do keyset pagination</a>.
</p><h4 id="subscription">Subscription<a href="#subscription" title="Permalink"></a>
</h4>
<p>Subscription to a resource is done by posting a
<a href="https://www.w3.org/TR/activitystreams-vocabulary/#dfn-follow">Follow activity</a>
to the actor inbox. When receiving a Follow activity,
<a href="https://www.w3.org/TR/activitypub/#follow-activity-inbox">we should generate an Accept or Reject activity in return</a>,
sent to the subscriber’s inbox.
</p><p>The general workflow of the implementation is as following:
</p><ul>
<li>A POST request is made to the inbox endpoint, with the Follow activity
encoded as JSON
</li><li>if the activity received is not of a supported type (e.g. someone tries to
comment on the activity), we ignore it ; otherwise:
</li><li>we create an <code>ActivityPub::Subscription</code> with the profile URL of the
subscriber
</li><li>we queue a job to resolve the subscriber’s inbox URL
<ul>
<li>in which we perform a HTTP request to the subscriber profile to find
their inbox URL (and the shared inbox URL if any)
</li><li>we store that URL in the subscription record
</li></ul>
</li><li>we queue a job to accept the subscription
<ul>
<li>in which we perform a HTTP request to the subscriber inbox to post an
Accept activity
</li><li>we update the state of the subscription to <code>:accepted</code>
</li></ul>
</li></ul>
<p><code>ActivityPub::Subscription</code> is a new abstract model, from which inherit
models related to our actors, each with their own table:
</p><ul>
<li>ActivityPub::ReleasesSubscription, table <code>activity_pub_releases_subscriptions</code>
</li><li>ActivityPub::TopicSubscription, table <code>activity_pub_topic_subscriptions</code>
</li><li>ActivityPub::ProjectSubscription, table <code>activity_pub_project_subscriptions</code>
</li><li>ActivityPub::GroupSubscription, table <code>activity_pub_group_subscriptions</code>
</li><li>ActivityPub::UserSubscription, table <code>activity_pub_user_subscriptions</code>
</li></ul>
<p>The reason to go with a multiple models rather than, say, a simpler <code>actor</code>
enum in the Subscription model with a single table is because we needs
specific associations and validations for each (an
<code>ActivityPub::ProjectSubscription</code> belongs to a Project, an
<code>ActivityPub::UserSubscription</code> does not). It also gives us more room for
extensibility in the future.
</p><h4 id="unfollow">Unfollow<a href="#unfollow" title="Permalink"></a>
</h4>
<p>When receiving
<a href="https://www.w3.org/TR/activitypub/#undo-activity-inbox">an Undo activity</a>
mentioning previous Follow, we remove the subscription from our database.
</p><p>We are not required to send back any activity, so we don’t need any worker
here, we can directly remove the record from database.
</p><h4 id="sending-activities-out">Sending activities out<a href="#sending-activities-out" title="Permalink"></a>
</h4>
<p>When specific events (which ones?) happen related to our actors, we should
queue events to issue activities on the subscribers inboxes (the activities
are the same than we display in the actor’s outbox).
</p><p>We’re supposed to deduplicate the subscriber list to make sure we don’t
send an activity twice to the same person - although it’s probably better
handled by a uniqueness validation from the model when receiving the Follow
activity.
</p><p>More importantly, we should group requests for a same host : if ten users
are all on <code>https://mastodon.social/</code>, we should issue a single request on
the shared inbox provided, adding all the users as recipients, rather than
sending one request per user.
</p><h4 id="webfinger">
<a href="https://gitlab.com/gitlab-org/gitlab/-/issues/423079">Webfinger</a><a href="#webfinger" title="Permalink"></a>
</h4>
<p>Mastodon
<a href="https://docs.joinmastodon.org/spec/webfinger/">requires instance to implement the Webfinger protocol</a>.
This protocol is about adding an endpoint at a well known location which
allows to query for a resource name and have it mapped to whatever URL we
want (so basically, it’s used for discovery). Mastodon uses this to query
other fediverse apps for actor names, in order to find their profile URLs.
</p><p>Actually, GitLab already implements the Webfinger protocol endpoint through
Doorkeeper
(<a href="https://github.com/doorkeeper-gem/doorkeeper-openid_connect/blob/5987683ccc22262beb6e44c76ca4b65288d6067a/app/controllers/doorkeeper/openid_connect/discovery_controller.rb#L14-L16">this is the action that maps to its route</a>),
implemented in GitLab
<a href="https://gitlab.com/gitlab-org/gitlab/-/blob/efa76816bd0603ba3acdb8a0f92f54abfbf5cc02/app/controllers/jwks_controller.rb">in JwksController</a>.
</p><p>There is no incompatibility here, we can just extend this controller.
Although, we’ll probably have to rename it, as it won’t be related to Jwks
alone anymore.
</p><p>One difficulty we may have is that contrary to Mastodon, we don’t only deal
with users. So we need to figure something to differentiate asking for a
user from asking for a project, for example. One obvious way would be to
use a prefix, like <code>user-&lt;username&gt;</code>, <code>project-&lt;project_name&gt;</code>, etc. I’m
pondering that from afar, while we haven’t implemented much code in the
epic and I haven’t dig deep into Webfinger’s specs, this remark may be
deprecated when we reach actual implementation.
</p><h4 id="http-signatures">
<a href="https://gitlab.com/gitlab-org/gitlab/-/issues/423083">HTTP signatures</a><a href="#http-signatures" title="Permalink"></a>
</h4>
<p>Mastodon
<a href="https://docs.joinmastodon.org/spec/security/#http">requires HTTP signatures</a>,
which is yet an other standard, in order to make sure no spammer tries to
impersonate a given server.
</p><p>This is asymmetrical cryptography, with a private key and a public key,
like SSH or PGP. We will need to implement both signing requests, and
verifying them. This will be of considerable help when we’ll want to have
various GitLab instances communicate later in the epic.
</p><h3 id="host-allowlist-and-denylist">Host allowlist and denylist<a href="#host-allowlist-and-denylist" title="Permalink"></a>
</h3>
<p>To give GitLab instance owners control over potential spam, we need to
allow to maintain two mutually exclusive lists of hosts:
</p><ul>
<li>the allowlist : only hosts mentioned in this list can be federated with.
</li><li>the denylist : all hosts can be federated with but the ones mentioned in
that list.
</li></ul>
<p>A setting should allow the owner to switch between the allowlist and the denylist.
In the beginning, this can be managed in rails console, but it will
ultimately need a section in the admin interface.
</p><h3 id="limits-and-rollout">Limits and rollout<a href="#limits-and-rollout" title="Permalink"></a>
</h3>
<p>In order to control the load when releasing the feature in the first
months, we’re going to set <code>gitlab.com</code> to use the allowlist and rollout
federation to a few Fediverse servers at a time, so that we can see how it
takes the load progressively, before ultimately switching to denylist
(note: there are
<a href="https://gitlab.com/gitlab-org/gitlab/-/issues/426373#note_1584232842">some ongoing discussions</a>
regarding if federation should be activated on <code>gitlab.com</code> or not).
</p><p>We also need to implement limits to make sure the federation is not abused:
</p><ul>
<li>limit to the number of subscriptions a resource can receive.
</li><li>limit to the number of subscriptions a third party server can generate.
</li></ul>
<h3 id="the-cross-instance-issues-and-merge-requests-part">The cross-instance issues and merge requests part<a href="#the-cross-instance-issues-and-merge-requests-part" title="Permalink"></a>
</h3>
<p>We’ll wait to be done with the social following part before designing this
part, to have ground experience with ActivityPub.
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Constructing a Four-Point Egg (115 pts)]]></title>
            <link>https://dotat.at/@/2024-01-29-four-point-egg.html</link>
            <guid>39201332</guid>
            <pubDate>Wed, 31 Jan 2024 08:15:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://dotat.at/@/2024-01-29-four-point-egg.html">https://dotat.at/@/2024-01-29-four-point-egg.html</a>, See on <a href="https://news.ycombinator.com/item?id=39201332">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <p>For reasons beyond the scope of this entry, I have been investigating
elliptical and ovoid shapes. The <a href="https://en.wikipedia.org/wiki/Moss%27s_egg">Wikipedia article for Moss’s
egg</a> has a link to <a href="https://web.archive.org/web/20200618202007/https://www.dynamat.oriw.eu/upload_pdf/20121022_154322__0.pdf">a tutorial on Euclidean Eggs by Freyja
Hreinsdóttir</a> which (amongst other things) describes how to
construct the “four point egg”. I think it is a nicer shape than
Moss’s egg.</p>

<p>Freyja’s construction uses a straight edge and compasses in classical
style, so it lacks dimensions. Below is my version, with the numbers
needed to draw it on a computer.</p>
<p>At first this construction might seem fairly rigid, but some of its
choices are more arbitrary than they might appear to be. I have made
<a href="https://dotat.at/@/2024-01-eggsperiment.html">an interactive four-point egg</a> so you can drag the
points around and observe how its shape changes.</p>
<p>In the following, I will measure angles in fractions of a turn <code>𝜏</code>,
clockwise from noon because the egg’s pointy end is upwards.</p>
<ol>
<li>Start with axes and a unit circle.</li>
</ol>

<ol start="2">
<li>Imagine two diagonal scaffolding lines: one through the unit
circle’s south and east points; and another through its south and
west points.</li>
</ol>

<ol start="3">
<li>Draw an arc centred on the unit circle’s south point and
equidistant from its north point, from one scaffolding line to the
other. This forms the big end of the egg.</li>
</ol>
<pre>x: 0 y: -1 radius: 2 from: 𝜏*3/8 to: 𝜏*5/8
</pre>

<ol start="4">
<li>Draw the egg’s lower right arc, centred on the unit circle’s west
point, from the right end of the bottom arc up to the positive X
axis.</li>
</ol>
<pre>x: -1 y: 0 radius: 2+√2 from: +𝜏*2/8 to: +𝜏*3/8
</pre>

<ol start="5">
<li>Draw the egg’s lower left arc mirroring the lower right arc.</li>
</ol>
<pre>x: +1 y: 0 radius: 2+√2 from: -𝜏*3/8 to: -𝜏*2/8
</pre>

<ol start="6">
<li>The lower right and lower left arcs meet the X axis at the egg’s
east and west points. Imagine a scaffolding circle centred at the
origin joining these points.</li>
</ol>

<ol start="7">
<li>
<p>The egg’s north centre is where this scaffolding circle meets the
Y axis at <code>+1+√2</code>.</p>
<p>Imagine two new diagonal scaffolding lines: one through the egg’s
west point and north centre; and another through its east point
and north centre.</p>
</li>
</ol>

<ol start="8">
<li>Draw the egg’s upper right arc, centred on the egg’s west point,
from the egg’s east point up to a scaffolding line.</li>
</ol>
<pre>x: -1-√2 y: 0 radius: 2+2√2 from: +𝜏*1/8 to: +𝜏*2/8
</pre>

<ol start="9">
<li>Draw the egg’s upper left arc mirroring the upper right arc.</li>
</ol>
<pre>x: +1+√2 y: 0 radius: 2+2√2 from: -𝜏*2/8 to: -𝜏*1/8
</pre>

<ol start="10">
<li>Draw an arc on the egg’s north centre joining the ends of the
upper right and upper left arcs. This arc forms the little end of
the egg.</li>
</ol>
<pre>x: 0 y: 1+√2 radius: √2 from: -𝜏*1/8 to: +𝜏*1/8
</pre>

<p>The six coloured points marked in the diagram above are the centres of
the six arcs that make the egg. These comprise the four points the egg
is named after, plus their mirror images.</p>

<p>Moss’s egg is a three-point egg. It has the same top half as this
four-point egg, but its bottom half is a simple semi-circle.</p>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Microsoft Edge ignores user wishes, slurps tabs from Chrome without permission (320 pts)]]></title>
            <link>https://www.theregister.com/2024/01/30/microsoft_edge_tabs/</link>
            <guid>39201317</guid>
            <pubDate>Wed, 31 Jan 2024 08:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/01/30/microsoft_edge_tabs/">https://www.theregister.com/2024/01/30/microsoft_edge_tabs/</a>, See on <a href="https://news.ycombinator.com/item?id=39201317">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p><span>Updated</span> Windows users, take notice: Microsoft's Edge browser is said to be actively importing open Chrome tabs and slurping other data from Google's browser without permission and even if the "feature" that makes that happen is disabled.&nbsp;</p>
<p>Buried within Edge's browser settings on Windows PCs is code that goes well beyond a one-time import of favorites and stored passwords, and has been present in some form as far back as <a href="https://www.windowslatest.com/2022/05/30/microsoft-edge-new-feature-will-constantly-pull-data-from-chrome/" rel="nofollow">mid-2022</a> at least. It gives gives Edge the ability to import <a href="https://support.microsoft.com/en-us/microsoft-edge/what-s-imported-to-microsoft-edge-ab7d9fa1-4586-23ce-8116-e46f44987ac2#:~:text=What%27s%20imported%20from%20Google%20Chrome%3F" rel="nofollow">practically</a> all browser data from Chrome each time Redmond's browser is launched.&nbsp;</p>
<p>Ostensibly a way for Microsoft to simplify the process of getting Windows users to switch to Edge, the feature has a <a href="https://www.theregister.com/2015/09/28/windows_10_data_collection/">classic</a> Microsoft problem: it's right now doing so without full permission, according to users. As the Windows maker is wont to do, it'll also <a href="https://support.microsoft.com/en-us/microsoft-edge/change-and-customize-sync-settings-in-microsoft-edge-be529080-f2e9-b642-538f-976956b8da6b" rel="nofollow">sync</a> that data to the cloud too, provided users are signed into a Microsoft account - not great if you had intended to keep your Chrome and Edge environments separated.&nbsp;</p>

    

<p><a href="https://www.theregister.com/2023/11/15/google_amazon_microsoft_mozilla/">More data</a> for Redmond; even less privacy for users, as we've covered in the past.&nbsp;</p>

        


        

<p>Windows users – even those who don't make a habit of using Edge – have been reporting problems and unexpected surprises with the import feature since last year. Other outlets have <a href="https://www.ghacks.net/2023/10/13/how-to-stop-microsoft-edge-from-importing-google-chrome-browsing-data-at-launch/" rel="nofollow">covered</a> the feature's greedy nature and suggested steps to stop it dead (for example by opening up <code>edge://settings/profiles/importBrowsingData</code> and making sure it's off), but reader responses to those stories suggest settings don't matter, and the import happens regardless.</p>
<p>The issue was most recently flagged up again this week by The Verge's Tom Warren, who <a href="https://www.theverge.com/24054329/microsoft-edge-automatic-chrome-import-data-feature" rel="nofollow">said</a> he noticed Edge opening on a system reboot in the same state that Chrome was in before the restart. Warren claimed he neither imported Chrome data to Edge nor gave it permission to import open browser tabs from Chrome, yet there it was. Warren said he never toggled the setting on, and that it was off when he checked.</p>

        

<p>Taking a quick spin around the internet, it's clear that this is happening to plenty of people – data being pulled into Edge by default, without permission – though it doesn't appear universal.</p>
<ul>

<li><a href="https://www.theregister.com/2024/01/24/microsoft_edge_bing_dma/">Microsoft's plucky challengers, Bing and Edge, might gain DMA exemptions</a></li>

<li><a href="https://www.theregister.com/2024/01/18/bing_ai_search/">Did all that AI chatbot hype boost Bing's market share? Oh, wait, never mind</a></li>

<li><a href="https://www.theregister.com/2024/01/29/microsoft_confirms_windows_server_2025/">Microsoft confirms Windows Server 2025 is on the way</a></li>

<li><a href="https://www.theregister.com/2023/11/18/google_kills_legacy_extensions/">What's really going on with Chrome's June crackdown on extensions – and why your ad blocker may or may not work</a></li>
</ul>
<p><a href="https://answers.microsoft.com/en-us/microsoftedge/forum/all/how-to-make-edge-stop-importing-from-chrome/4bcb43bb-b54a-4413-b24c-ea43b49cea54" rel="nofollow">Multiple</a> Microsoft <a href="https://answers.microsoft.com/en-us/microsoftedge/forum/all/why-edge-keep-syncing-all-data-from-chrome-even/1efde161-5c8b-411b-a59a-1bfa2b8cedcc" rel="nofollow">support</a> <a href="https://answers.microsoft.com/en-us/microsoftedge/forum/all/how-do-i-stop-edge-from-importing-bookmarks-and/22d7af7f-d111-481d-9ef3-13a703e61c18" rel="nofollow">threads</a> from the past few months allege the same issue – that Edge is importing data from Chrome unexpectedly, even when the setting is toggled off. Several posts on Reddit <a href="https://www.reddit.com/r/MicrosoftEdge/comments/17xglpc/edge_keeps_importing_chrome_bookmarks_on_startup/" rel="nofollow">allege</a> the same.</p>
<p>Responses from Microsoft staff and independent advisers suggest checking <code>ImportBrowsingData</code> settings as well as manually disabling auto import by manually adding a registry key to Group Policy settings, but responses are universal: Nothing seems to turn it off.&nbsp;</p>
<p>Reports that Edge is opening on startup populated with Chrome tabs that were open before reboot appear to have flared up following a Windows 11 update, <a href="https://support.microsoft.com/en-au/topic/january-23-2024-kb5034204-os-builds-22621-3085-and-22631-3085-preview-7652acf2-56dc-430e-b8ef-ec8f56ec1028" rel="nofollow">KB5034204</a>, released on January 23. This update seems to be heavily forcing the feature on users in one way or another.</p>
<p><a href="https://www.reddit.com/r/Windows11/comments/19e118i/january_23_2024kb5034204_os_builds_226213085_and/" rel="nofollow">Other folks</a> have reported similar issues with Edge asserting itself dressed in Chrome's data since installing the update, though security researcher Zach Edwards <a href="https://twitter.com/thezedwards/status/1750952950598672455" rel="nofollow">noted</a> on X/Twitter that Microsoft may have an excuse this time. Screens in the update installation steps mention Edge will "regularly bring in data from other browsers available on your Windows device."&nbsp;</p>

        

<p>It's all out in the open – somewhat – since Microsoft stuck a brief mention of it in splash screens with the installation of the infamous KB5034204 update.</p>
<p>Beyond the issues with Edge, KB5034204 appears to have a number of issues affecting lots of Windows 11 users, as evidenced by a <a target="_blank" rel="nofollow" href="https://www.reddit.com/r/WindowsHelp/comments/1abedvs/update_kb5034204_fails_to_install/">Reddit thread</a> posted a few days after its rollout. Users have reported failed installations and boot issues, while other sites have <a target="_blank" rel="nofollow" href="https://windowsreport.com/windows-11-kb5034204-issues/">reported</a> broken apps, file explorer and taskbar <a target="_blank" rel="nofollow" href="https://www.windowslatest.com/2024/01/28/windows-11-kb5034204-is-crashing-file-explorer-taskbar-and-wont-install-for-some/">trouble</a> and other issues. We've reached out to Microsoft again to inquire about broader problems with KB5034204, but haven't heard back about that, either.</p>
<p>Regardless of whether Microsoft asked permission for Edge to ingest data from other browsers in that update, the issue isn't new - it's well documented in forum posts and help requests. We've asked Microsoft for an explanation, and while we're told they're looking into it, we have yet to receive an actual response.</p>
<p>In the meantime, best to hold off installing the latest round of Windows 11 updates until Microsoft patches the patch. Or explains itself. Ah, who are we kidding. We've asked for comment. ®</p>
<h3>Updated to add</h3>
<p>Microsoft has so far declined to comment. A person familiar with the kerfuffle who has visibility into the Windows giant, though who did not want to be identified, told us it appears that "if a user chose continuous import in the Edge first run experience on some other device, this state may be syncing incorrectly across their devices. This is not the intended feature experience." We're assured that Microsoft is addressing it for the next Edge Stable release.</p>                                
                    </div></div>]]></description>
        </item>
    </channel>
</rss>