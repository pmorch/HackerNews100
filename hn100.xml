<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Sat, 10 Feb 2024 22:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Aho – a Git implementation in Awk (147 pts)]]></title>
            <link>https://github.com/djanderson/aho</link>
            <guid>39327192</guid>
            <pubDate>Sat, 10 Feb 2024 16:05:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/djanderson/aho">https://github.com/djanderson/aho</a>, See on <a href="https://news.ycombinator.com/item?id=39327192">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:djanderson/aho" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="Nmpn50ir0StXweXHuVsrtlmmJvmSorNITkWih1jrJ2GSemEy9cFZljHpbr8tIMKsO8F8kfhday2xWun5bUZ4-Q" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="djanderson/aho" data-current-org="" data-current-owner="djanderson" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=XJP5HDWsGaFiecy2C53wUG%2FBYLJsEEieTIjStqxZUvOwORLDSv7RPjmCuoBvAjX96a28gy8YLSv4RK4GFs%2FJKtUFNliNciA2hkloHiaU%2FEb%2BR251OyAozpvugAF9peZjpTNRu%2BwIPNuyz2Ce78aIiVd5loz%2FUCMEUYHZ91TAOYMX52p8OgR%2B8QjbMJSovQGpXnG%2BElDIqBS8cC5wHlBUmLOkCrJv3EbKN0xZYrOVWj1oFYuZVuTZnDuTWFACzW48%2FSYomu21s8ryZ72MarnxfAOgAwDsMbPRNpD%2B7F9qIFD%2FNQDBkzZMkDFQpzutOA0OyytoruX15qCt1i6FTbuUY%2B%2BJwo0G6hgrOD2LtbweqPy4PASw0IeaZrNau6bNf3KeIOniCwAqnh2ZuYgU5OAG%2Bo6yEasyUSFiN3ekUv2fLEFMhCuXJqg4FkWLsLXbrZt%2BJSpOQwyLSGZt2%2FbP3MpMfGSIo7mUH6cIfLCx4Na%2BLrrhV4z%2B%2BNw95%2B%2FdDIZSGBCQbDYHl%2FDB8XSWtw%3D%3D--%2B4gQrgZ9%2BEJBXYfn--9GZAXi9h8oH9hKpFPBb%2BOQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=djanderson%2Faho" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/djanderson/aho&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d6cc66d19a471110c58d1db9877d5425be09537f9dc63fbe23c1ec6f93fefff4" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tenets (163 pts)]]></title>
            <link>https://github.com/sveltejs/svelte/discussions/10085</link>
            <guid>39327113</guid>
            <pubDate>Sat, 10 Feb 2024 15:57:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/sveltejs/svelte/discussions/10085">https://github.com/sveltejs/svelte/discussions/10085</a>, See on <a href="https://news.ycombinator.com/item?id=39327113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="presentation" data-paste-markdown-skip="">
    <tbody data-target-translation-id="6029409" data-target-translation-type="discussion">
        <tr>
    <td>
        <p dir="auto">This is an attempt to articulate the Svelte <em>philosophy</em> — our bedrock principles, that guide our design decisions.</p>
<p dir="auto"><em>Edit: for visitors from Hacker News, and anyone else who is curious, <a href="https://www.youtube.com/watch?v=eswNQiq4T2w&amp;t=5211s" rel="nofollow">here is the video</a> that this list came out of</em></p>
<h2 dir="auto">The web matters</h2>
<p dir="auto">We work on Svelte because we believe that the web is a critically important technology, and that its continued survival is not guaranteed.</p>
<h2 dir="auto">Optimise for vibes</h2>
<p dir="auto">People use Svelte because they <em>like</em> Svelte. They like it because it aligns with their aesthetic sensibilities.</p>
<p dir="auto">Instead of striving to be the fastest or smallest or whateverest, we explicitly aim to be the framework with the best vibes.</p>
<h2 dir="auto">Don't optimise for adoption</h2>
<p dir="auto">We're not trying to be the most popular framework, we're trying to be the best framework. Sometimes that means making choices that we believe in but that go against the grain of web development trends.</p>
<h2 dir="auto">HTML, The Mother Language</h2>
<p dir="auto">HTML is a really good language for describing UI. Svelte augments HTML in a way that makes it a really good language for describing <em>interactive</em> UI.</p>
<p dir="auto">Most frameworks are JS-centric, because JS is the most powerful language. But then they find themselves jumping through hoops to make it feel like you're writing HTML. We think both options are valid, but the HTML-first approach ends up feeling more natural.</p>
<h2 dir="auto">Embrace progress</h2>
<p dir="auto">There is a tendency in the web developer community towards a harmful form of pessimistic nostalgia — the idea that things were better in the prelapsarian age before bundlers, TypeScript, client-side routing and other trappings of modernity.</p>
<p dir="auto">This is nonsense. As a community our default position is one of optimism about technology — the platform is getting better, our tools are getting better, our devices are getting better, and if we embrace that fact we can make better stuff.</p>
<p dir="auto">And when other frameworks introduce new ideas like signals or server components, we look at them with interest and jealousy, and try to work out how we can incorporate good ideas, instead of resting on our laurels. There is always room for improvement.</p>
<h2 dir="auto">Numbers lie</h2>
<p dir="auto">Lighthouse has broken the brains of a generation of web developers. We have replaced good judgment with subservience to metrics that were only ever intended to be used as a diagnostic tool.</p>
<p dir="auto">Goodhart's Law states that</p>
<blockquote>
<p dir="auto">When a measure becomes a target, it ceases to be a good measure</p>
</blockquote>
<p dir="auto">and this is very true in web development. Numerical rigour is good, and we pay attention to the various numbers, but when designing Svelte we think qualitatively, not quantitatively.</p>
<h2 dir="auto">Magical, not magic</h2>
<p dir="auto">There's a subtle line between something feeling <em>magical</em>, and something feeling like <em>magic</em>. We want Svelte to feel magical — we want you to feel like a wizard when you're writing Svelte code. Historically I think Svelte went too far into magic territory, where it's not 100% clear why things work a certain way, and that's something that we're rectifying with Svelte 5.</p>
<h2 dir="auto">Dream big</h2>
<p dir="auto">'Choose the right tool for the job' is sensible but boring advice.</p>
<p dir="auto">It makes us small in our ambitions. I want us to dream bigger. I don't want to feel like my tools can't handle evolving requirements, or that if I want to dabble in a new field I need to learn an entirely new way of working first.</p>
<p dir="auto">Even if it turns out to be unachievable, I find it valuable to ask the question 'what would it take for SvelteKit to be the best framework for <em>any</em> app?', whether it's purely static content, or a realtime multiplayer app, or an offline-first productivity app, or even something built for an augmented reality headset.</p>
<h2 dir="auto">No-one cares</h2>
<p dir="auto">Most people do not care about frameworks. They just want to build something cool, and Svelte is for those people too.</p>
<p dir="auto">So when we design things we need to think about the people who haven't read the docs in a while, if at all, and don't care about things like fine-grained rendering or configuring their build tool. This means that things need to be intuitive, that we shouldn't need to worry about manual optimisations like memoisation, that we should have as few APIs as possible, and that things need to be discoverable — for example you should be able to hover over a rune and get a link to comprehensive documentation.</p>
<p dir="auto">This also informs our approach to documentation and tutorials — it should be possible to build what you want by just learning the concepts that you need, and worrying about the other stuff for another day.</p>
<h2 dir="auto">Design by consensus</h2>
<p dir="auto">Svelte is a community-driven and consensus-led project. It's important that the community — that's you — has a stake in the project's future. Many of Svelte's best ideas originated outside the core team.</p>
<p dir="auto">When we introduce new plans, we want to communicate them openly and provide everyone with good opportunities to offer their feedback.</p>
<p dir="auto">Those of us who work on the project every day have a vision for what kind of thing we want it to be, but we don't want to foist that vision on people against their will. And so while we can't get unanimous agreement on every change, we can at least say that dissenting voices have been heard and considered.</p>
    </td>
  </tr>

    </tbody>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A rent-stabilized 1 bedroom apartment for $1,100 In NYC? broker's fee is $15K (155 pts)]]></title>
            <link>https://gothamist.com/news/a-rent-stabilized-1-bedroom-apartment-for-1100-in-nyc-the-brokers-fee-is-15k</link>
            <guid>39326675</guid>
            <pubDate>Sat, 10 Feb 2024 15:12:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gothamist.com/news/a-rent-stabilized-1-bedroom-apartment-for-1100-in-nyc-the-brokers-fee-is-15k">https://gothamist.com/news/a-rent-stabilized-1-bedroom-apartment-for-1100-in-nyc-the-brokers-fee-is-15k</a>, See on <a href="https://news.ycombinator.com/item?id=39326675">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>How much would you pay a broker for a $1,100-a-month apartment if you knew the rent wouldn’t be jacked up year after year?</p><p>A broker in Queens was offering apartment hunters the deal of a lifetime: a one-bedroom, rent-stabilized unit in Flushing, well below market rate for the area. The only catch was a $15,000 broker fee to secure the unit. Under rent stabilization, modest annual increases are <a href="https://gothamist.com/news/nyc-board-votes-to-increase-rents-on-1-million-rent-stabilized-units-by-3" rel="noopener" target="_blank">set by a city panel</a>, shielding tenants from dramatic hikes.</p><p>That was the predicament facing 27-year-old Christian Garbutt while he was searching for an apartment last month, he told Gothamist. The apartment seemed great, but he couldn’t afford the fee — nor did he want to pay it.</p><p>But the $15,000 broker fee levied by Miguel Silva, a broker with the New York City branch of the real estate company Keller Williams, was too high for even his employers. In response to questions from Gothamist, they said they are returning some of the money to the tenant who landed the apartment.</p><p>Broker fees are standard procedure in the city’s highly competitive real estate market, even when tenants find the units online and do most of the work themselves. Typical rates range from a month’s rent to a higher percentage of the yearly total.</p><p>But as affordable housing dwindles, broker’s fees are rising along with rents, making moves more difficult and dependent on upfront cash, in a sector where laws and regulations remain cloudy.</p><p>Last month, Gov. Kathy Hochul criticized “<a href="https://gothamist.com/news/nyc-brokers-charging-exorbitant-fees-forced-to-pay-260k-in-penalties" rel="noopener" target="_blank">excessive</a>” fees when she announced a $260,000 penalty against a brokerage firm found to be demanding $20,000 from prospective tenants trying to secure a rent-stabilized apartment, a rare rebuke of the practice.</p><p>The New York Department of State, which licenses brokers, said in an email that it determines that fees are too high when they exceed “industry norms and standards” and do not relate to actual services.</p><p>The rules are murky, and no laws explicitly cap broker’s fees or define the difference between reasonable costs and extortion — similar to a once-common arrangement known as “<a href="https://www.brickunderground.com/blog/2015/11/what_is_key_money" rel="noopener" target="_blank">key money</a>.”</p><h4><b>‘That’s insane’</b></h4><p>Garbutt said he was looking for an apartment in Queens when he came across a place in Flushing priced at $1,450 a month and advertised as a “newly renovated rent-stabilized” one-bedroom on the listings website StreetEasy. He said he toured the unit and “fell in love with it,” but soon found out there was a catch: The broker was asking for an $8,000 fee.</p><p>Garbutt said he decided to pass on the place because the upfront cost would have wiped out his savings. He said he found another one-bedroom in the same building — this one listed online at $1,100 a month — and again texted the broker.</p><p>According to text messages reviewed by Gothamist, the broker warned “the fee is also high. It’s much higher than the other one.” He wanted $15,000.</p><p>Garbutt said he was “shocked” by the figure and could not afford it. He said he stopped responding, even after the broker offered to negotiate the fee and updated the listing to $1,800 a month.</p><p>“That’s insane," Garbutt said of the fee. "That’s totally not fair. Normal New Yorkers, a lot of people, don't have that type of money saved up.”</p><p>He said he’s still looking for a place.</p><p>Silva, the broker, did not respond to requests for comment.</p><p>Keller Williams said it would return any money over 15% of the annual rent to the tenant who got the apartment.</p><p>"Although there are no standard or typical or legal caps on broker fees, Keller Williams NYC adheres to real property law and its guidance in the commission fees that are charged to reasonably relate to legitimate services provided to renters by our agents and firm,” said Richard Amato, an operating principal at the firm.</p><p>Here’s what you need to know about broker’s fees and what to do if you think you’re being overcharged.</p><h4><b>What are the upfront costs for renting an apartment?</b></h4><p>Renting an apartment in New York City is hard. The broker’s fee, a uniquely New York inconvenience that has <a href="https://www.northjersey.com/story/news/state/2022/09/09/broker-fees-nyc-nj-renters/65474656007/" rel="noopener" target="_blank">spread</a> to a <a href="https://therealdeal.com/new-york/2020/02/07/broker-fees-for-nyc-rentals-mystified-outsiders-heres-how-other-us-cities-do-it/" rel="noopener" target="_blank">few other places</a>, can make it even harder.</p><p>Along with the first month’s rent, security deposit and moving expenses, apartment hunters in the five boroughs typically have to fork over even more money to a middleman — a broker — usually hired by a landlord to list and show apartments.</p><p>Brokers typically charge a one-time fee of around 8% to 15% of the annual rent, or roughly $5,400 on a $3,000-a-month apartment.</p><p>In most cases, there is no way of getting around it, said Allia Mohamed, CEO of listings and landlord review website openigloo.</p><p>“It’s not really fair, but if you want to get that specific apartment you have to be prepared to pay up,” Mohamed said.</p><h4><b>What is a broker anyway?</b></h4><p>A real estate broker is a person who facilitates the sale or rental of a property, typically for a fee.</p><p>Real estate brokers in the Empire State <a href="https://dos.ny.gov/real-estate-broker" rel="noopener" target="_blank">are regulated by the New York Department of State</a>, which grants them a license if they meet the requirements and pass an exam.</p><p>Normally, people hire brokers to survey the market and find prospective options to buy or rent a place.</p><p>But in New York City, it's more common for landlords to hire the brokers to list the apartments on StreetEasy, Trulia and other platforms that charge a fee, and arrange visits with applicants. It’s then left to tenants to pay their commission.</p><h4><b>How much is too much?</b></h4><p>There’s a limit to what’s considered reasonable, according to industry regulators and lawmakers. Even many brokers agree that some fees are truly exorbitant. But where brokers cross the line isn’t clearly spelled out.</p><p>Regulators from the Department of State didn’t respond to a question about what they consider the industry standard for New York City broker fees. Industry groups generally agree that 15% of the annual rent is the norm.</p><p>Yet brokers know that someone with disposable income may be willing to pony up for a rent-stabilized apartment, since they could save money down the road. That’s the argument Garbutt said his broker made.</p><p>“He said it’s a lot of money, but you’re going to see it in the long run,” Garbutt recalled.</p><p>Still, that arrangement locks out the vast majority of New Yorkers who can only dream of paying that kind of fee.</p><p>The median household income in New York City is around $76,600, <a href="https://www.census.gov/quickfacts/fact/table/newyorkcitynewyork" rel="noopener" target="_blank">according to the U.S. Census Bureau</a>. Median rents are about $4,100 in Manhattan and $2,500 in Queens, according to the latest <a href="https://streeteasy.com/blog/data-dashboard/[object%20Object]?agg=Median&amp;metric=Asking%20Rent&amp;type=Rentals&amp;bedrooms=One%20Bedroom&amp;property=Any%20Property%20Type&amp;minDate=2010-01-01&amp;maxDate=2024-01-01&amp;area=Flatiron,Brooklyn%20Heights" rel="noopener" target="_blank">StreetEasy review of listings</a>. And most New Yorkers are considered rent-burdened, as they pay more than 30% of their income on rent, <a href="https://gothamist.com/news/1-in-3-nyc-tenants-spend-half-their-income-on-rent-as-affordability-crisis-deepens" rel="noopener" target="_blank">city housing data</a> shows.</p><p>For a $3,000-a-month apartment, that means you would need $11,400 upfront to cover the security deposit, first month of rent and 15% broker’s fee.</p><p>If you suspect you’re being overcharged, you can file a complaint <a href="https://dos.ny.gov/preliminary-statement-complaint-0" rel="noopener" target="_blank">with the Department of State.</a></p><h4><b>So is anyone doing anything about these fees?</b></h4><p>Past efforts to limit the fees have all failed.</p><p>The Real Estate Board of New York, or REBNY, successfully sued to <a href="https://gothamist.com/news/those-hefty-brokers-fees-are-returning-following-state-judges-ruling" rel="noopener" target="_blank">end a brief cap on broker’s fees for tenants</a>, based on an interpretation of <a href="https://gothamist.com/news/nyc-real-estate-brokers-are-already-exploiting-a-perceived-loophole-in-new-rent-laws" rel="noopener" target="_blank">state rent laws</a> enacted in 2019.</p><p>A City Council <a href="https://gothamist.com/news/city-council-proposals-would-slash-obscene-brokers-fees-and-limit-security-deposits" rel="noopener" target="_blank">bill limiting broker’s fees</a> to one month’s rent also died in 2019 amid <a href="https://ny.curbed.com/2019/6/28/18761629/nyc-broker-fee-reform-bill-city-council-real-estate" rel="noopener" target="_blank">opposition from the real estate industry</a>.</p><p>The latest attempt? A <a href="https://gothamist.com/news/nyc-broker-fees-are-skyrocketing-the-city-council-is-trying-to-put-the-fees-on-landlords" rel="noopener" target="_blank">City Council bill</a> that would require whoever hired the broker — such as the landlord, property manager or tenant — to cover the cost.</p><p>“I don't think anyone should be forced to pay a fee to someone who they never hired,” said Councilmember Chi Ossé of Brooklyn, who introduced the measure. “It's kind of replicating how we do transactions in every other industry within the city.”</p><p>REBNY and other critics of the bill say it will lead to higher rents on non-rent-stabilized apartments because landlords will spread the cost across the monthly rent payments for as long as the tenant lives there, instead of a one-time fee. Owners of rent-stabilized units would have to eat the cost.</p><p>“This bill would make the process of renting an apartment more costly and challenging for New Yorkers while negatively impacting the livelihood of hardworking agents,” said REBNY spokesperson Christopher Santarelli.</p><p>Veteran broker <a href="https://sammoritz.medium.com/thoughts-on-broker-fees-in-nyc-from-a-new-york-city-real-estate-agent-c7ef0802a68" rel="noopener" target="_blank">Sam Moritz</a> agreed.</p><p>“The tenant paying the broker fee is what it is,” he said. “If you eliminate the tenant-paid broker fee for apartments, rents are going to increase.”</p><p>Nikki Thomas, a broker and real estate agent for the Corcoran Group, declined to take a position on the legislation but said nixing the fee could be good business for property owners.</p><p>Thomas said she often encourages landlords to pay her commission and then add the price to the rent, since people get alienated by “junk fees.”</p><p>“I usually tell them to adjust the rent to factor that in, because I know people psychologically don’t like additional upfront costs,” she said. “You’re starting off with a happier tenant.”</p><h4><b>So what are you paying for exactly?</b></h4><p>Convenience — usually for the landlord — said Mohamed, the openigloo executive.</p><p>The brokers advertise the apartments, arrange showings and put together applications for the property owner, she said.</p><p>“When the landlord is hiring the broker, they’re the ones benefiting from the headache the broker is taking off their shoulders,” Mohamed said. “They definitely put in a ton of work in helping the landlord sift through applicants and prepare that space.”</p><p>Moritz, the longtime broker, said the services are especially important when the landlord lives outside the city or has a busy schedule that prevents them from continuously showing the apartment.</p><p>“They want to hire a professional, someone who does this full time and closes deals efficiently and professionally,” he said.</p><p>The brokers also have to cover some costs. Posting the apartment on StreetEasy costs $7 a day, or more than $200 a month, for example.</p><p>Moritz said he charges tenants the equivalent of one month’s rent so that he can get apartments leased up as quickly as possible. He said he figures he needs to earn about $6,500 a month to cover his own rent and expenses in Bushwick, which equates to at least three or four broker fees a month, he said.</p><p>He criticized brokers who charge exorbitant fees and said they are taking advantage of tenants and owners.</p><p>“I think that is a little greedy because this is the landlord’s asset,” Moritz said. “I personally think that if a landlord gives me a listing, then I am working for the landlord.”</p><p>Plenty of tenants also end up hiring brokers to help them find an apartment, especially if they have disposable income, a busy schedule or are moving to New York City from somewhere else.</p><p>“A broker can accomplish in a few days what might take someone several weeks,” Thomas of the Corcoran Group said. “Like so many things, you can do it on your own, but is it actually worth it?”</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I Was Illegally Fired by Amazon for Speaking Out About a Coworker's Death (2023) (500 pts)]]></title>
            <link>https://jacobin.com/2023/11/i-was-illegally-fired-by-amazon-for-speaking-out-about-a-coworkers-death/</link>
            <guid>39326559</guid>
            <pubDate>Sat, 10 Feb 2024 14:56:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jacobin.com/2023/11/i-was-illegally-fired-by-amazon-for-speaking-out-about-a-coworkers-death/">https://jacobin.com/2023/11/i-was-illegally-fired-by-amazon-for-speaking-out-about-a-coworkers-death/</a>, See on <a href="https://news.ycombinator.com/item?id=39326559">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">

                <!-- Antescript -->

                
                  
                

                <!-- Intro -->
                
                  
                    
    <dl>
      <dt>Interview by</dt>
      
        <dd>
          <a href="https://jacobin.com/author/caspar-shaller">Caspar Shaller</a>
        </dd>
      
    </dl>
  
                  
                

                
                  
                    <section id="ch-0">
                      <p>Two years ago, union organizer Magda Malinowska was fired from her job at an Amazon warehouse near Poznań, in western Poland. Her sacking came as retaliation for her efforts to speak out against poor working conditions — a conflict catalyzed by the September 2021 death of forty-nine-year-old Dariusz Dziamski, a colleague of hers who died on the shop floor. Fast forward to October 2023, and a Polish court has ruled that Amazon fired Malinowska illegally.</p>
<p>The ruling provides a small step toward justice for Malinowska, albeit only after a two-year delay, and a period in which many aspects of Amazon workers’ rights have worsened. In an interview, Malinowska spoke to <i>Jacobin</i>’s Caspar Shaller about Amazon’s inhumane treatment of staff, its violation of basic union rights, and workers’ efforts to stand up against corporate tyranny.</p>

                    </section>
                  
                

                <!-- Main Content -->

                
    
      
        <section id="ch-1">
          
            
            <hr>
          
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>A court has decided that Amazon fired you illegally and reinstated your job …</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>… but they still have three weeks to appeal. I assume they will.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>What was the judge’s reasoning?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>That was the most interesting part: it was actually pretty clear that Amazon violated union rights. So, from a formal, legal perspective it was clear that I should win the case.</p>
<p>In Poland, however, it is possible to win the case, but without reinstatement. Some judges believe that if there is a major conflict between the trade unionists and the company it will be better this way. I was afraid that I would win the case but would not be reinstated — all the more so because the judge called more witnesses and delved deeply into the case and the relationship between the union and Amazon. The HR employees who fired me were questioned twice. Finally, the judge stated that my dismissal could have been caused by my activity. What I was accused of in terminating the contract was not proven during the case. Amazon didn’t manage to provide any evidence to support their position.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>What was the reason Amazon gave for firing you?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>They fired me for allegedly taking pictures or videos — but they didn’t know, because no one saw it — when the body of Dariusz, a colleague of mine who had died during his shift, was moved to the hearse, which they considered inconsistent with their values and social standards.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>How did Dariusz die?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>Dariusz had all the signs of experiencing a heart attack, but apparently that was not the medical reason. His wife told me his heart was broken. That’s why this was such an important case for us. Dariusz worked so hard. His job used to be done by a few people during a shift, but then they made him do all that work alone, pushing around trolleys with heavy boxes.</p>
<p>Dariusz had been complaining that his job was too exhausting for him and he wanted a different task. But they wouldn’t give him another position. They did the exact opposite: they reduced the number of people working the same job, so his work got even more exhausting.</p>
<p>Before Dariusz died, we’d long been trying to get Amazon to carefully examine energy expenditure and how hard the work we do is, including Dariusz’s. The company is required to do so by law. However, Amazon only roughly estimates how much energy workers put in, and doesn’t use an appropriate method to calculate it. A few years ago, we managed to persuade the Labor Inspectorate and CIOP (Central Institute for Labor Protection) to investigate the cases of several employees; some of them were putting in up to twice as much as the allowed levels.</p>
<p>We have court judgments that say that Amazon does not examine this properly. But this is dangerous, especially as each of us has some health issues, and some of us have been doing hard physical work for years. After Dariusz’s death, I told the media that [this past dispute] was probably the real reason for my dismissal.</p>
<p>On the Sunday before Dariusz died, he asked the supervisor to transfer him to another department because he was fed up with working too hard. Despite the requests, he continued to work alone at his post for about five hours. The next morning Dariusz collapsed and died on the floor of the warehouse. They didn’t really help him, despite his symptoms. He was told to go through the warehouse — the size of a dozen or so soccer fields — on his own, to go down the stairs to the medical room. When he got there, he died.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>Does this kind of thing happen a lot? Are there a lot of people who die in Amazon warehouses?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>It’s not like a construction site or a mine, where workers die all the time or have severe accidents. Amazon management actually gets pretty angry when there’s an accident. The work is dangerous in another way: the repetition and the strain get to you. That does affect your body and your health. That’s why we want to be able to do a labor inspection, to see if someone’s health is being affected by their work. And we want to change the definition of a workplace accident. Because if you do a super heavy job for some time and then you get a heart attack, formally it’s not connected to the job.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>Amazon is known for spearheading new types of labor surveillance. How is that connected to this case?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>One reason for the surveillance is that Amazon is trying to optimize all steps of the work process. They are not the only ones to try out new forms of labor management, but they are very quick at implementing changes. They are constantly implementing new ways of controlling workers’ bodies and movements. You can’t even leave your workstation for more than three minutes. You have scanners and cameras everywhere checking on your every movement.</p>
<p>So, surveillance is really the big issue at Amazon. Through all this, they force you to do very simple movements in a very repetitive way. Because they broke down the labor process into these simple movements, they can easily exchange workers, they don’t need experienced workers, they can just train new ones to do these simple movements. But having to do the same movements over and over and faster and faster is totally destroying our bodies.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>What are you going to do, now that you can go back to the warehouse?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>Well, for me, this is a completely new situation. I’ve never been fired and then reinstated. It’s going to be interesting what Amazon will do now. And I’m wondering if, after this ruling and the explanation that the judge gave us, they will be willing to change their relationship with us on the union side. That is actually what the judge said: that there is a conflict between labor and employer, but the conflict cannot be an argument to dismiss union members.</p>
<p>Firing all union members would mean there’s no union in the workplace anymore, but Polish law guarantees the right to a union. The juge even said she can see that Amazon’s attitude toward unions is very negative. It’s a global problem with Amazon: they don’t want to recognize unions anywhere. So, I think it was very important that this judge confirms what we’ve been saying, someone who’s not like us — some militant activist, who’s always up for a fight — but a representative of the state.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>Do you think this ruling will change anything in the immediate future?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>Unfortunately, I’m afraid they will not change their policy. They behave like they’re not scared of anything and try to break as many rules and regulations as possible. They’re just smashing everything on their way to making more and more profit. They’re always checking how far they can go, until finally there will be nothing left that can stop them. So this small victory is a step toward stoping them.</p>
<p>Amazon has used Poland as a base from which to attack German unions right next door, either by importing Polish workers as strikebreakers when German warehouses were being picketed or then by simply building warehouses along the German-Polish border to serve the German market and get around German labor laws and unions.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>How can workers cooperate over national lines and stop multinationals from playing workers off each other?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>The current situation is amazing for Amazon. There are so many different legal systems, even within Germany for example, different regions have different regulations. So they try to use it against us and to make bigger profits, pay less taxes, and so on. They move orders from one warehouse to another warehouse, if there is slack in one location – or when we’re organizing a blockade, for example for our campaign “Make Amazon Pay.”</p>
<p>When the Polish warehouses were opened, German workers were on strike, and the Amazon moved orders from German warehouses to Polish warehouses, forcing people Polish to work longer shifts. When Polish workers heard about the reason why they had to work longer shifts, they said, hey we don’t want to be scabs, and organized a go-slow. That was the beginning of our cooperation with German workers.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>How has organizing workers made progress over the recent years?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>Unfortunately, it’s become more difficult to organize. I think people are more scared. After previous actions, some people got fired. And there’s been a shift in company policy. When Amazon started out in Poland, they hired people on permanent contracts. That gave people a feeling of stability and legal protection, which allowed them to fight back.</p>
<p>But now, there are a lot of people who are employed by agencies. The split among the workers is pretty bad. I’ve seen discussions online about how only certain types of workers are allowed to do overtime. Pay is so low that everyone wants to pick up more shifts, so weirdly being allowed to work even more becomes a matter of prestige. So, working overtime isn’t a penalty, it’s a prize. That situation is totally sick.</p>

              </div>
          
            <div>
                <p><span>
                  
                    
                      Caspar Shaller
                </span></p><p>What are you doing to change that?</p>

              </div>
            <div>
                <p><span>
                  
                    
                      Magda Malinowska
                </span></p><p>Last year, we started a campaign about the firing of shop stewards and union representatives. Indeed, I wasn’t the only one who was sacked, there were others, too. And together with other unions, we organized a huge campaign to change the law. And it was successful: the government changed the law. Since the law came into effect in September it’s basically pointless to fire union officials. It’s still happening illegally, of course, but maybe it changes workers’ mentality a bit to know they have more legal rights. Maybe unions will get a little bit stronger. Or at least there won’t be a repetition of 2021 when so many people were sacked out of the blue.</p>
<p>Recently there was an election and the far-right government <a href="https://jacobin.com/2023/10/poland-lewica-left-wing-alliance-opposition-donald-tusk-law-and-justice">lost</a>. So maybe there might be some change in how the state handles these cases. But we are not focused only on the law, we’re also trying to figure out how to strengthen our position in warehouses. And we will continue to educate workers and exchange ideas and experiences among people from my union and others. One important focus will be on inspecting labor standards and how workers can do that themselves. We will try to use health and safety as tool to make working conditions better.</p>

              </div>
          
        </section>
      
    
  

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Walmart, Delta, and Starbucks are using AI to monitor employee messages (117 pts)]]></title>
            <link>https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html</link>
            <guid>39326201</guid>
            <pubDate>Sat, 10 Feb 2024 14:06:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html">https://www.cnbc.com/2024/02/09/ai-might-be-reading-your-slack-teams-messages-using-tech-from-aware.html</a>, See on <a href="https://news.ycombinator.com/item?id=39326201">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107233563" data-test="InlineImage"><p>Klaus Vedfelt | Digitalvision | Getty Images</p></div><div><p>Cue the George Orwell reference.</p><p>Depending on where you work, there's a significant chance that artificial intelligence is analyzing your messages on Slack, Microsoft Teams, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/ZM/">Zoom</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and other popular apps.</p><p>Huge U.S. employers such as <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/WMT/">Walmart</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/DAL/">Delta Air Lines</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-4"><a href="https://www.cnbc.com/quotes/TMUS/">T-Mobile</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-5"><a href="https://www.cnbc.com/quotes/CVX/">Chevron</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> and <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/SBUX/">Starbucks</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, as well as European brands including Nestle and <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-7"><a href="https://www.cnbc.com/quotes/AZN-GB/">AstraZeneca</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, have turned to a seven-year-old startup, Aware, to monitor chatter among their rank and file, according to the company.</p><p>Jeff Schumann, co-founder and CEO of the Columbus, Ohio-based startup, says the AI helps companies "understand the risk within their communications," getting a read on employee sentiment in real time, rather than depending on an annual or twice-per-year survey.</p><p>Using the anonymized data in Aware's analytics product, clients can see how employees of a certain age group or in a particular geography are responding to a new corporate policy or marketing campaign, according to Schumann. Aware's dozens of AI models, built to read text and process images, can also identify bullying, harassment, discrimination, noncompliance, pornography, nudity and other behaviors, he said.</p><p>Aware's analytics tool — the one that monitors employee sentiment and toxicity — doesn't have the ability to flag individual employee names, according to Schumann. But its separate eDiscovery tool can, in the event of extreme threats or other risk behaviors that are predetermined by the client, he added.</p><p>Aware said Walmart, T-Mobile, Chevron and Starbucks use its technology for governance risk and compliance, and that type of work accounts for about 80% of the company's business. </p><p>CNBC didn't receive a response from Walmart, T-Mobile, Chevron, Starbucks or Nestle regarding their use of Aware. A representative from AstraZeneca said the company uses the eDiscovery product but that it doesn't use analytics to monitor sentiment or toxicity. Delta told CNBC that it uses Aware's analytics and eDiscovery for monitoring trends and sentiment as a way to gather feedback from employees and other stakeholders, and for legal records retention in its social media platform.</p><p>It doesn't take a dystopian novel enthusiast to see where it could all go very wrong.</p></div><div id="Placeholder-ArticleBody-Video-107365190" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000329724" aria-labelledby="Placeholder-ArticleBody-Video-107365190"><p><img src="https://image.cnbcfm.com/api/v1/image/107365191-17062918051706291803-33066365856-1080pnbcnews.jpg?v=1706291805&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Generative AI is coming to wealth management in a very big way, says Ritholtz's Josh Brown"><span></span><span></span></p></div><div><p>Jutta Williams, co-founder of AI accountability nonprofit Humane Intelligence, said AI adds a new and potentially problematic wrinkle to so-called insider risk programs, which have existed for years to evaluate things like corporate espionage, especially within email communications.</p><p>Speaking broadly about employee surveillance AI rather than Aware's technology specifically, Williams told CNBC: "A lot of this becomes thought crime." She added, "This is treating people like inventory in a way I've not seen."</p><p>Employee surveillance AI is a rapidly expanding but niche piece of a larger AI market that's exploded in the past year, following the launch of OpenAI's ChatGPT chatbot in late 2022. Generative AI quickly became the buzzy phrase for corporate earnings calls, and some form of the technology is automating tasks in just about every industry, from financial services and biomedical research to logistics, online travel and <a href="https://prod.ucwe.capgemini.com/wp-content/uploads/2023/07/Final-Web-Version-Report-Harnessing-the-Value-of-Gen-AI.1.pdf" target="_blank">utilities</a>.</p><p>Aware's revenue has jumped 150% per year on average over the past five years, Schumann told CNBC, and its typical customer has about 30,000 employees. Top competitors include Qualtrics, Relativity, Proofpoint, Smarsh and Netskope.</p><p>By industry standards, Aware is staying quite lean. The company last raised money in 2021, when it pulled in $60 million in a <a href="https://www.prnewswire.com/news-releases/aware-raises-60-million-in-series-c-funding-led-by-goldman-sachs-asset-management-301398993.html" target="_blank">round</a> led by <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/GS/">Goldman Sachs</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Asset Management. Compare that with large language model, or LLM, companies such as OpenAI and Anthropic, which have raised billions of dollars each, largely from strategic partners.</p></div><h2><a id="headline0"></a>'Tracking real-time toxicity'</h2><div><p>Schumann started the company in 2017 after spending almost eight years working on enterprise collaboration at insurance company Nationwide.</p><p>Before that, he was an entrepreneur. And Aware isn't the first company he's started that's elicited thoughts of Orwell.</p><p>In 2005, Schumann founded a company called BigBrotherLite.com. According to his <a href="https://www.linkedin.com/in/jeffreyschumann/" target="_blank">LinkedIn profile</a>, the business developed software that&nbsp;"enhanced the digital and mobile viewing experience" of the CBS reality series "Big Brother." In Orwell's classic novel "1984," Big Brother was the leader of a totalitarian state in which citizens were under perpetual surveillance.</p><p><strong>"</strong>I built a simple player focused on a cleaner and easier consumer experience for people to watch the TV show on their computer," Schumann said in an email.</p><p>At Aware, he's doing something very different.</p><p>Every year, the company puts out a report aggregating insights from the billions — in 2023, the number was 6.5 billion — of messages sent across large companies, tabulating perceived risk factors and workplace sentiment scores. Schumann refers to the trillions of messages sent across workplace communication platforms every year as "the fastest-growing unstructured data set in the world."&nbsp;</p><p>When including other types of content being shared, such as images and videos, Aware's analytics AI analyzes more than 100 million pieces of content every day. In so doing, the technology creates a company social graph, looking at which teams internally talk to each other more than others.</p><p>"It's always tracking real-time employee sentiment, and it's always tracking real-time toxicity," Schumann said of the analytics tool. "If you were a bank using Aware and the sentiment of the workforce spiked in the last 20 minutes, it's because they're talking about something positively, collectively. The technology would be able to tell them whatever it was."</p><p>Aware confirmed to CNBC that it uses data from its enterprise clients to train its machine-learning models. The company's data repository contains about 6.5 billion messages, representing about 20 billion individual interactions across more than 3 million unique employees, the company said.&nbsp;</p><p>When a new client signs up for the analytics tool, it takes Aware's AI models about two weeks to train on employee messages and get to know the patterns of emotion and sentiment within the company so it can see what's normal versus abnormal, Schumann said.</p><p>"It won't have names of people, to protect the privacy," Schumann said. Rather, he said, clients will see that "maybe the workforce over the age of 40 in this part of the United States is seeing the changes to [a] policy very negatively because of the cost, but everybody else outside of that age group and location sees it positively because it impacts them in a different way."</p></div><div id="Placeholder-ArticleBody-Video-107364640" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000329612" aria-labelledby="Placeholder-ArticleBody-Video-107364640"><p><img src="https://image.cnbcfm.com/api/v1/image/107364641-17062107841706210781-33053162930-1080pnbcnews.jpg?v=1706210784&amp;w=750&amp;h=422&amp;vtcrop=y" alt="FTC scrutinizes megacap's AI deals"><span></span><span></span></p></div><div><p>But Aware's eDiscovery tool operates differently. A company can set up role-based access to employee names depending on the "extreme risk" category of the company's choice, which instructs Aware's technology to pull an individual's name, in certain cases, for human resources or another company representative.</p><p>"Some of the common ones are extreme violence, extreme bullying, harassment, but it does vary by industry," Schumann said, adding that in financial services, suspected insider trading would be tracked.</p><p>For instance, a client can specify a "violent threats" policy, or any other category, using Aware's technology, Schumann said, and have the AI models monitor for violations in Slack, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-12"><a href="https://www.cnbc.com/quotes/MSFT/">Microsoft</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Teams and Workplace by <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-13"><a href="https://www.cnbc.com/quotes/META/">Meta</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>. The client could also couple that with rule-based flags for certain phrases, statements and more. If the AI found something that violated a company's specified policies, it could provide the employee's name to the client's designated representative.</p><p>This type of practice has been used for years within email communications. What's new is the use of AI and its application across workplace messaging platforms such as Slack and Teams.</p><p>Amba Kak, executive director of the AI Now Institute at New York University, worries about using AI to help determine what's considered risky behavior.</p><p>"It results in a chilling effect on what people are saying in the workplace," said Kak, adding that the Federal Trade Commission, Justice Department and Equal Employment Opportunity Commission have all expressed concerns on the matter, though she wasn't speaking specifically about Aware's technology. "These are as much worker rights issues as they are privacy issues."&nbsp;</p><p>Schumann said that though Aware's eDiscovery tool allows security or HR investigations teams to use AI to search through massive amounts of data, a "similar but basic capability already exists today" in Slack, Teams and other platforms.</p><p>"A key distinction here is that Aware and its AI models are not making decisions," Schumann said. "Our AI simply makes it easier to comb through this new data set to identify potential risks or policy violations."</p></div><h2><a id="headline1"></a>Privacy concerns</h2><div><p>Even if data is aggregated or anonymized, <a href="https://crypto.stanford.edu/~pgolle/papers/census.pdf" target="_blank">research suggests</a>, it's a flawed concept. A <a href="https://dataprivacylab.org/projects/identifiability/paper1.pdf" target="_blank">landmark study</a> on data privacy using 1990 U.S. Census data showed that 87% of Americans could be identified solely by using ZIP code, birth date and gender. Aware clients using its analytics tool have the power to add metadata to message tracking, such as employee age, location, division, tenure or job function.&nbsp;</p><p>"What they're saying is relying on a very outdated and, I would say, entirely debunked notion at this point that anonymization or aggregation is like a magic bullet through the privacy concern," Kak said.</p><p>Additionally, the type of AI model Aware uses can be effective at generating inferences from aggregate data, making accurate guesses, for instance, about personal identifiers based on language, context, slang terms and more, according to <a href="https://openreview.net/pdf?id=kmn0BhQk7p" target="_blank">recent research</a>.</p><p>"No company is essentially in a position to make any sweeping assurances about the privacy and security of LLMs and these kinds of systems," Kak said. "There is no one who can tell you with a straight face that these challenges are solved."</p><p>And what about employee recourse? If an interaction is flagged and a worker is disciplined or fired, it's difficult for them to offer a defense if they're not privy to all of the data involved, Williams said.</p><p>"How do you face your accuser when we know that AI explainability is still immature?" Williams said.</p><p>Schumann said in response: "None of our AI models make decisions or recommendations regarding employee discipline."</p><p>"When the model flags an interaction," Schumann said, "it provides full context around what happened and what policy it triggered, giving investigation teams the information they need to decide next steps consistent with company policies and the law."</p><p><strong>WATCH:</strong> <a href="https://www.cnbc.com/video/2024/01/25/ai-is-really-at-play-here-with-the-recent-tech-layoffs-says-jason-greer.html">AI is 'really at play here' with the recent tech layoffs</a></p></div><div id="Placeholder-ArticleBody-Video-107364334" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000329554" aria-labelledby="Placeholder-ArticleBody-Video-107364334"><p><img src="https://image.cnbcfm.com/api/v1/image/107364335-17061866781706186675-33048845410-1080pnbcnews.jpg?v=1706186678&amp;w=750&amp;h=422&amp;vtcrop=y" alt="AI is 'really at play here' with the recent tech layoffs, says Jason Greer"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In 2023 operations for the .GOV TLD transitioned from Verisign to Cloudflare (116 pts)]]></title>
            <link>https://indico.dns-oarc.net/event/48/contributions/1038/</link>
            <guid>39326092</guid>
            <pubDate>Sat, 10 Feb 2024 13:46:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://indico.dns-oarc.net/event/48/contributions/1038/">https://indico.dns-oarc.net/event/48/contributions/1038/</a>, See on <a href="https://news.ycombinator.com/item?id=39326092">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            
    <section>
        
        <p><span>
    <span>Christian Elmerot</span>
        <span>
            <span>(<span>Cloudflare</span>)</span></span></span>
        </p>
    </section>


    
        <div>
                <p>In 2023 operations for the .GOV TLD transitioned from Verisign to Cloudflare. One interesting aspect of this transition was the different approaches to DNSSEC signing by Verisign and Cloudflare. Whereas Verisign uses offline signing with RSA (algorithm 8) and NSEC3, Cloudflare generally uses online signing with ECDSA (algorithm 13) and NSEC.</p>
<p>Although the parties agreed to transition using only RSA, we wanted to test the statement in RFC 8901 ("Multi-Signer DNSSEC Models") that says "NSEC and NSEC3 can be used by different providers to serve the same zone." After extensive testing by both parties, we found no reasons why it shouldn't work, and this approach was used for the transition. To the best of our knowledge, this is likely to be the first time that a signed zone of such significance was operated using NSEC and NSEC3 at the same time.</p>
            </div>
    

    
        
        
    
        
        
    

    

    
        <section>
            
            
    <p><span itemprop="performers" itemscope="" itemtype="http://schema.org/Person">
                
                
                    <span>Christian Elmerot</span>
                
                <span>
                        <span>(<span>Cloudflare</span>)</span>
                    </span>
            </span>
        
    </p>

        </section>
    

    

    
    


    

    
    
        
    
    
    
    
    
    
    
    
    

    

    
    
        
    
    
    
    

    

        
        
    
    
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Memray – A Memory Profiler for Python (116 pts)]]></title>
            <link>https://github.com/bloomberg/memray</link>
            <guid>39325983</guid>
            <pubDate>Sat, 10 Feb 2024 13:28:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/bloomberg/memray">https://github.com/bloomberg/memray</a>, See on <a href="https://news.ycombinator.com/item?id=39325983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:bloomberg/memray" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="iVNVeL11Ne4eZZduspHHZq__lLvIN97ZtqkCG4kasqHqYAwDRNNovc-G_YMSb13WxbNm41z3nMBkhcmHJUGBkQ" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="bloomberg/memray" data-current-org="bloomberg" data-current-owner="" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=hJMDbBKF%2FZk%2Fu7%2FzhksoRR0nkCJ81U1FbCPyUMprfnWcmK%2FOBfgTDBt6MIpxPVyLnm7k46l8sBphd5ByPUuykwepgGzg45IbJOWuSUzSYfIiss6tfftTzfCEijP7rXOt07Futo7bDM3x%2BKdGf7ApqGXkkBCl6o5thPCk%2B4MKCjpwwrE2GFnEyq6LYl2Y%2FcVjWmSbE%2FbDoMsd7MfNkfK%2BN8SyNQcMeNA%2FE4aSHvaPzVu7ByLD1FoOcqIx6fRvyXhNqjnjx%2BlWmaUKoaa9wRSAHB96y5%2BHrR7uOlW6rUpdR4X%2B%2BmfOP54p2i%2FTlVst%2BZLx3dWikpm0usGPdRj26yWMlFal7IJIWfbRMN14rN9VPPMGOrdio7JydNrSLHasm1R2hC26aA12dwrNUnOBaDU4w1AcFceZyU9AhWX21%2FAvArgkKlV5zAUZj5sXfFHjZfE%2FPFkymoieonyxJQfV0DcNdG4aiU4TPalvOZ%2BOWgdpCKl%2B2Hpi1k3o9nNslpy4EfoFslCgCXSbT2LJOga5un0%3D--%2B96EEylELqg9NdPp--X73HRzcrjaMHYUaLwdsorQ%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-disabled="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=bloomberg%2Fmemray" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/bloomberg/memray&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="92d30e7bde27f0903164d388b5d768aa9f5f52f3f0a26850c59693e1e2bc2baa" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Tears of a Clown: Probing the comedian's psyche (2008) (107 pts)]]></title>
            <link>https://www.psychologytoday.com/gb/blog/beautiful-minds/200812/the-tears-clown</link>
            <guid>39325700</guid>
            <pubDate>Sat, 10 Feb 2024 12:32:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.psychologytoday.com/gb/blog/beautiful-minds/200812/the-tears-clown">https://www.psychologytoday.com/gb/blog/beautiful-minds/200812/the-tears-clown</a>, See on <a href="https://news.ycombinator.com/item?id=39325700">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                    <div>
<p><img src="https://cdn2.psychologytoday.com/sites/default/files/styles/article-inline-half/public/blogs/624/2008/12/2752-75861.jpg?itok=ZVcFVqr3" alt="" title=""></p>
</div>
<p>Everyone likes a good comic. Comedy, done well, adds much needed soul to life-exposing and magnifying the absurdities of life, reducing <a href="https://www.psychologytoday.com/gb/basics/stress" title="Psychology Today looks at stress" hreflang="en-GB">stress</a>, defusing social tensions, and generally increasing the quality of life. Of course, comedy, done poorly, is about as awkward as watching George W. Bush tap dance while waiting to endorse John McCain (Time magazines<a href="http://www.time.com/time/specials/2008/top10/article/0,30583,1855948_1864014,00.html"> #1 most awkward moment of 2008</a>).
</p>

<p>But behind every punch line is a real live human sweating profusely to get a laugh. What are these people like? How did the comic become this way, constantly pushing the limit to get a reaction from others? What motivates the comic? How do comics perceive themselves? Let's start at the beginning. </p>
<p><strong>Origins of the Comic </strong></p>

<p>Samuel Janus and his colleagues studied the <a href="https://www.psychologytoday.com/gb/basics/intelligence" title="Psychology Today looks at intelligence" hreflang="en-GB">intelligence</a>, educational level, family background, and <a href="https://www.psychologytoday.com/gb/basics/personality" title="Psychology Today looks at personality" hreflang="en-GB">personality</a> of 69 comedians, all of whom were said to be famous and successful. Data were collected using a variety of methods: clinical interviews, accounts of early memories, <a href="https://www.psychologytoday.com/gb/basics/dreaming" title="Psychology Today looks at dreams" hreflang="en-GB">dreams</a>, handwriting analyses, projective tests, and the Weschsler Adult Intelligence Scale (WAIS).</p>

<p>Janus concluded that comedians tended to be superior in intelligence, but also felt misunderstood, angry, anxious, suspicious, depressed, and concerned with approval. Their early lives were frequently characterized by suffering, isolation, and feelings of deprivation; in many cases, the comedians learned to use <a href="https://www.psychologytoday.com/gb/basics/humor" title="Psychology Today looks at humor" hreflang="en-GB">humor</a> as a defense against <a href="https://www.psychologytoday.com/gb/basics/anxiety" title="Psychology Today looks at anxiety" hreflang="en-GB">anxiety</a>, converting their feelings of suppressed rage from physical to verbal <a href="https://www.psychologytoday.com/gb/basics/anger" title="Psychology Today looks at aggression" hreflang="en-GB">aggression</a>. However, many comedians were also <a href="https://www.psychologytoday.com/gb/basics/shyness" title="Psychology Today looks at shy" hreflang="en-GB">shy</a>, sensitive, and <a href="https://www.psychologytoday.com/gb/basics/empathy" title="Psychology Today looks at empathic" hreflang="en-GB">empathic</a> individuals whose comedic success was apparently due partly to an ability to accurately perceive the fears and needs of their audiences.</p>

<p>Seymour and Rhoda Fisher conducted a better-controlled study of professional humor producers, and summarized their results in a delightful book called <a href="http://www.amazon.com/Pretend-World-Funny-Forever-Psychological/dp/0898590736/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1229806854&amp;sr=1-1">Pretend the World is Funny and Forever: A Psychological Analysis of Comedians, Clowns, and Actors</a>. They assessed the personality, motivations, and <a href="https://www.psychologytoday.com/gb/basics/child-development" title="Psychology Today looks at childhood" hreflang="en-GB">childhood</a> recollections of 43 professional comedians (including 15 circus clowns) and scoured published biographical and autobiographical accounts of 40 comedians and clowns, from <a href="http://www.woodyallen.com/">Woody Allen</a> and <a href="http://www.jackiegleason.com/">Jackie Gleason</a> to <a href="http://www.jerrylewiscomedy.com/">Jerry Lewis</a> and <a href="http://en.wikipedia.org/wiki/Beatrice_Lillie">Beatrice Lillie</a> (once dubbed the "Funniest Women in the World"). They also administered the <a href="https://www.psychologytoday.com/gb/basics/rorschach-test" title="Psychology Today looks at Rorschach inkblot test" hreflang="en-GB">Rorschach inkblot test</a> and the Thematic Apperception Test (TAT) to identify themes and preoccupations in the thoughts of the comedians. As a control, they included a sample of 41 professional actors.</p>

<p>Fisher and Fisher found that professional comedians did not differ from actors in <a href="https://www.psychologytoday.com/gb/basics/depression" title="Psychology Today looks at depression" hreflang="en-GB">depression</a> or overall psychological health, but did uncover several differences between the groups. The majority of the comics came from lower socioeconomic strata. Quite early, they displayed talent for being funny, often acting as the "class clown" in school. Many in the sample entered comedy professionally through their interest in music. Compared to the actors, the professional comedians had to take on considerably more responsibility at an early age. They were also more likely than the actors to describe the father in highly positive terms, and were more inclined to refer to the mother as a disciplinarian, an aggressive critic, non-nurturing, and non-maternal. This finding was also discovered in a sample of amateur humor producers: the more college students considered themselves to be comics the more they saw their mother as controlling and the father as softer in their childrearing practices.</p>

<p>Along similar lines, Steven Prasinos and Bennett Tittler investigated 88 adolescent Boy Scouts and found that the funnier Boy Scouts (assessed through peer nomination) tended to be more distanced from their families, reported less cohesion in their families than what was reported by their peers, and reported greater family conflict. The authors interpret these findings as suggesting that humor represents an attempt to relate from a distance. They also raise the interesting possibility that this distancing is precisely what enables these individuals to be funny, since it allows for a fresh perspective.</p>

<p>The Fishers present another intriguing explanation for the link between early life experiences and the development of the comic. They draw on Alfred Heilbrum's work who found a tendency for individuals who are raised by controlling, non-nurturing mothers to develop schizophrenia. Heilbrun found two types of men (there really needs to be more research on female comics!) who are raised by a non-nurturing mother. The "closed style" type was characterized by defenses such as isolation from social interactions and depression, while the "open style" type was characterized as <a href="https://www.psychologytoday.com/gb/basics/extroversion" title="Psychology Today looks at extroverted" hreflang="en-GB">extroverted</a> and alert to ways of winning social approval-"broad scanners" of <a href="https://www.psychologytoday.com/gb/basics/environment" title="Psychology Today looks at the environment" hreflang="en-GB">the environment</a>, always looking for cues of what people expect of them.</p>

<p>The Fishers related this open style personality to the comics in their sample and argued that the comic's style of relating to people may partly mirror their early adventures with their mother. Their idea is that he or she becomes an expert in "reading" his or her mother, and then later learns how to "scan the world in a very sensitive way, looking for contradictions to decode and reconcile, hunting out cues as to how to win approval and support" (p. 207).</p>



  


<p><strong>Motivations</strong></p>
<p>What motivates the comic? In their study, the Fishers noticed that professional comics displayed significantly more themes of contrasts and opposites. The Fishers hypothesized that the comedian learns, through early life experiences, that life is absurd. They then spend their lives telling jokes to help them understand the absurdity of their own position. They note that much of humor involves spotting and giving meaning to ambiguities, and that comedians are obsessed with instability. They hypothesized that this focus on inconstancy may represent an effort at mastery, and that the comedian seeks to adapt to a threat that was of painful intensity in their early childhood.</p>

<p>The researchers also noted that the professional comic frequently seemed to put up a screen by retreating behind a barrage of jokes, as suggested in interviews and inkblot responses, where the professional comics conjured up images about concealment. Compared to actors, they were more likely to refer to people wearing masks, creatures hiding, and objects that can't be distinguished properly because they are obscured by darkness.</p>

<p>Fisher and Fisher also found that the majority of professional comedians in their sample had imagery of smallness. The comics tended to have lower self-esteems and to say bad things about themselves. They argue that the comedian's focus on his or her smallness may be a result of the reduced significance he or she felt as a child and that much comic behavior is aimed at reducing the discrepancy of smallness between themselves and others. "There is no question but that size strategies pervade the comic's codes and metaphors...He is forever reducing or magnifying. He never reports things in their immediate proportions" (p. 216). They also note that the low <a href="https://www.psychologytoday.com/gb/basics/self-esteem" title="Psychology Today looks at self-esteem" hreflang="en-GB">self-esteem</a> and feeling of smallness existent among the professional comics may actually set the comic on a unique path. "We would emphasize...the possibility that in some paradoxical way these negative self-feelings provide a durable base for shaping one's <a href="https://www.psychologytoday.com/gb/basics/identity" title="Psychology Today looks at identity" hreflang="en-GB">identity</a> and going off on an independent trajectory" (p. 200).</p>

<p><strong>Self-Perception </strong></p>
<p>How does the comic view himself or herself? The Fishers found that they viewed themselves as healers. Many of the professional comedians expressed a dedication to being <a href="https://www.psychologytoday.com/gb/basics/altruism" title="Psychology Today looks at altruistic" hreflang="en-GB">altruistic</a>. The comic sees his or her central duty as that of making people feel that events are funny. At the same time, the professional comics also viewed humor as a technique for controlling and dominating the audience. Indeed, Fisher and Fisher were impressed at how this view of the comic as a fool-priest is consistent with scholarly reviews of the history of the clown, the court jester, and the fool. They also noted how the contemporary comic serves a similar function as the court jester in earlier times. On the one hand, the comic presents himself or herself as the silly fellow that jokes, amuses and entertains. On the other hand, the comic initiates opposing currents, uncovering truths that many people usually try to banish from awareness.</p>

<p><strong>Conclusion</strong></p>
<div>
<p><img src="https://cdn2.psychologytoday.com/sites/default/files/styles/article-inline-half/public/blogs/624/2008/12/2752-75862.jpg?itok=-Stcx8DI" alt="" title=""></p>
</div>
<p>This research suggests that humor in professional comedians serves as a defense or coping mechanisms in dealing with his or her early family experiences, and the burden of having to take care of oneself. This may motivate the comic to make people laugh in order to gain their acceptance, as well as drive the comic to reveal the absurdity of life to make sense of their own lives. As Mark Runco has noted, writing is often a form of problem-solving; many writers are motivated to write to solve problems in their lives. Comedy writers may use comedy in their writing to help them understand themselves and the world, and to do so in a fashion that controls the reader's emotions. Enough analysis. Whatever the reason for comics, thank goodness for them.</p>


<p> © 2008 by <a title="scottbarrykaufman.com" href="http://www.scottbarrykaufman.com/">Scott Barry Kaufman</a> </p>
<p>--</p>
<p>Note. Some of the above was excerpted from <em>The Tears of a Clown: Understanding Comedy Writers</em>, co-authored with <span>Aaron Kozbelt</span>. For references and more, download:</p>
<p>Kaufman, S.B., &amp; Kozbelt, A. (in press). The Tears of a Clown: Understanding Comedy Writers. To appear in S.B. Kaufman &amp; J.C. Kaufman (Eds.), <em><a href="http://www.cambridge.org/us/catalogue/catalogue.asp?isbn=052170782X">The Psychology of Creative Writing</a></em>. Cambridge, UK: Cambridge University Press. [<a href="http://web.me.com/scottbarrykaufman/Scott_Barry_Kaufman/Publications_files/Kaufman%20%26%20Kozbelt%20%282009%29%20-%20The%20tears%20of%20a%20clown.pdf">pdf</a>]</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OPML Is Underrated (151 pts)]]></title>
            <link>https://kmaasrud.com/blog/opml-is-underrated.html</link>
            <guid>39324847</guid>
            <pubDate>Sat, 10 Feb 2024 09:31:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kmaasrud.com/blog/opml-is-underrated.html">https://kmaasrud.com/blog/opml-is-underrated.html</a>, See on <a href="https://news.ycombinator.com/item?id=39324847">Hacker News</a></p>
<div id="readability-page-1" class="page"><h2 id="opml-is-underrated">OPML is underrated</h2>
<!-- atom-id: a23474ca-0fc0-4444-93ad-21d1d08cded9 -->

<p>As a response to the general <a href="https://doctorow.medium.com/social-quitting-1ce85b67b456">enshittification</a>
of major platforms, I would say we are seeing a resurgence of the old
web’s ethos, with personal blogs gaining traction and the concept of the
<a href="https://neustadt.fr/essays/the-small-web/">small web</a> on the
rise. That might be colored by the digital communities I hang around in
(which are mostly dominated by programmers) but it does at least
empirically feel like a trend<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>. That brings along with
it a new interest in open web standards. Among them is RSS<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>,
which both I, and I think a lot others, have increasingly integrated
into our digital routines to keep track of posts from people and sources
we’re following. RSS aligns perfectly with the movement towards more
personalized and controlled content consumption. Unlike the
algorithm-driven feeds of <q>other platforms</q> — which often
prioritize engagement over relevance or quality — RSS allows me to
curate my own information stream. This feels important to me, as it
gives me a level of autonomy over the content that shapes my views and
knowledge, as opposed to handing that power over to advertisers.</p>
<p>There is an issue, though. RSS feeds can be a bit clunky to manage
and keep track of. Their decentralized nature also makes discoverability
an issue. Enter <a href="http://opml.org/spec2.opml">OPML</a>, which is
an outliner format that is most commonly used to store <a href="http://opml.org/spec2.opml#subscriptionLists">a list of feed
subscriptions</a>. I promise you; having a single file that stores all
the feeds you’re interested in is a gamechanger, as it makes it
significantly easier to organize, migrate, and share those feeds across
different platforms and devices. Here’s an example:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>&lt;?xml</span><span> version=</span><span>"1.0"</span><span> encoding=</span><span>"utf-8"</span><span>?&gt;</span></span>
<span id="cb1-2">&lt;<span>opml</span><span> version=</span><span>"2.0"</span>&gt;</span>
<span id="cb1-3">  &lt;<span>head</span>&gt;</span>
<span id="cb1-4">    &lt;<span>title</span>&gt;A list of feeds I follow&lt;/<span>title</span>&gt;</span>
<span id="cb1-5">  &lt;/<span>head</span>&gt;</span>
<span id="cb1-6">  &lt;<span>body</span>&gt;</span>
<span id="cb1-7">    &lt;<span>outline</span><span> text=</span><span>"My favorite blog"</span><span> xmlUrl=</span><span>"https://a-cool-blog.tld/blog/feed.xml"</span><span> type=</span><span>"rss"</span></span>
<span id="cb1-8"><span>      htmlUrl=</span><span>"https://a-cool-blog.tld/blog"</span><span> description=</span><span>"You can also add a description"</span> /&gt;</span>
<span id="cb1-9">    <span>&lt;!-- more outline elements with feeds... --&gt;</span></span>
<span id="cb1-10">  &lt;/<span>body</span>&gt;</span>
<span id="cb1-11">&lt;/<span>opml</span>&gt;</span></code></pre></div>
<p>Each outline item must have the type <code>rss</code> (that goes for
both RSS and Atom feeds) and must include the <code>xmlUrl</code>
attribute. Optionally, you can specify some more attributes, like adding
a title with <code>text</code>, a description with
<code>description</code> and a link to the blog front page with
<code>htmlUrl</code> — that added metadata can be very useful. Yes, it
is XML-based, which I admit isn’t exactly the easiest format to work
with, but it has a few advantages, which we’ll get back to.</p>
<p>With OPML, you don’t need separate applications or services to
categorize feeds. Categorization can be achieved within a single OPML
file through its outlining capabilities or by managing multiple OPML
files, each dedicated to a different category or use-case. It is a very
viable workflow to have one OPML file for your YouTube subscriptions,
another for your favorite Twitter/X and Mastodon users, one more for
news sites, and yet another for personal blogs — the world’s your
oyster. However, there aren’t many application that support nested OPML
outlines or categorizing based on different files, sadly, but there
should be! This is a call to action, developers: Perfect
side-project!</p>
<p>Beyond personal convenience, OPML has the potential to better the
<em>ecosystem</em> of the <q>small web.</q> By not only sharing an RSS
feed on your personal website, but also your list of subscribed feeds,
we’re effectively creating a recommendation system that is based on
concious curation, not statistical metrics. Your OPML file is now called
a <em><a href="https://blogroll.org/what-are-blogrolls/">blogroll</a></em>, and
you officially get to call yourself a <strong>90s web
developer</strong>. Jokes aside, I believe the simple fact that there is
a known person behind each recommendation is advantageous. Yes, this
might promote smaller digital social circles, but I personally think the
transparency of a known source is the best way to combat <a href="https://en.wikipedia.org/wiki/Filter_bubble">filter bubbles</a>.
That part is a whole sociological discussion in itself, so if you would
like to discuss it further, I would love chatting about it on <a href="https://lists.sr.ht/~kmaasrud/inbox">my mailing list</a>.</p>
<p>Now, getting back to the fact that OPML is XML-based; I’d like to
highlight an often-overlooked feature of this: The ability to use an XSL
stylesheet to display the OPML file rendered through a HTML template
when loaded in a browser. With this, you can add a short introduction
and guide to the format, making the blogroll more accessible to those
unfamiliar with it. It also opens the possibility to showcase each feed
with added context or descriptions.</p>
<details>
<summary>
Here is an example XSL stylesheet you can use:
</summary>
<div id="cb2"><pre><code><span id="cb2-1"><span>&lt;?</span><span>xml</span><span> version=</span><span>"1.0"</span><span> encoding=</span><span>"UTF-8"</span><span>?&gt;</span></span>
<span id="cb2-2"><span>&lt;</span><span>xsl:stylesheet</span><span> version=</span><span>"1.0"</span><span> xmlns:xsl=</span><span>"http://www.w3.org/1999/XSL/Transform"</span><span>&gt;</span></span>
<span id="cb2-3">  <span>&lt;</span><span>xsl:template</span><span> match=</span><span>"/opml"</span><span>&gt;</span></span>
<span id="cb2-4">    <span>&lt;html</span><span> xmlns=</span><span>"http://www.w3.org/1999/xhtml"</span><span> xml:lang=</span><span>"en"</span><span> lang=</span><span>"en"</span><span>&gt;</span></span>
<span id="cb2-5">      <span>&lt;head&gt;</span></span>
<span id="cb2-6">        <span>&lt;title&gt;</span></span>
<span id="cb2-7">          <span>&lt;</span><span>xsl:value-of</span><span> select=</span><span>"head/title"</span><span>/&gt;</span></span>
<span id="cb2-8">        <span>&lt;/title&gt;</span></span>
<span id="cb2-9">        <span>&lt;meta</span><span> name=</span><span>"viewport"</span><span> content=</span><span>"width=device-width, initial-scale=1"</span><span>/&gt;</span></span>
<span id="cb2-10">        <span>&lt;style&gt;</span> /* Insert CSS here */ <span>&lt;/style&gt;</span></span>
<span id="cb2-11">      <span>&lt;/head&gt;</span></span>
<span id="cb2-12">      <span>&lt;body&gt;</span></span>
<span id="cb2-13">        <span>&lt;p&gt;</span></span>
<span id="cb2-14">          This is a list of blogs and news sources I follow. The page</span>
<span id="cb2-15">          is itself an <span>&lt;a</span><span> href=</span><span>"http://opml.org/"</span><span>&gt;</span>OPML<span>&lt;/a&gt;</span> file, which</span>
<span id="cb2-16">          means you can copy the link into your RSS reader to</span>
<span id="cb2-17">          subscribe to all the feeds listed below.</span>
<span id="cb2-18">        <span>&lt;/p&gt;</span></span>
<span id="cb2-19">        <span>&lt;ul&gt;</span></span>
<span id="cb2-20">          <span>&lt;</span><span>xsl:apply-templates</span><span> select=</span><span>"body/outline"</span><span>/&gt;</span></span>
<span id="cb2-21">        <span>&lt;/ul&gt;</span></span>
<span id="cb2-22">      <span>&lt;/body&gt;</span></span>
<span id="cb2-23">    <span>&lt;/html&gt;</span></span>
<span id="cb2-24">  <span>&lt;/</span><span>xsl:template</span><span>&gt;</span></span>
<span id="cb2-25">  <span>&lt;</span><span>xsl:template</span><span> match=</span><span>"outline"</span><span> xmlns=</span><span>"http://www.w3.org/1999/xhtml"</span><span>&gt;</span></span>
<span id="cb2-26">    <span>&lt;</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-27">      <span>&lt;</span><span>xsl:when</span><span> test=</span><span>"@type"</span><span>&gt;</span></span>
<span id="cb2-28">        <span>&lt;</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-29">          <span>&lt;</span><span>xsl:when</span><span> test=</span><span>"@xmlUrl"</span><span>&gt;</span></span>
<span id="cb2-30">            <span>&lt;li&gt;</span></span>
<span id="cb2-31">              <span>&lt;strong&gt;</span></span>
<span id="cb2-32">                <span>&lt;a</span><span> href=</span><span>"</span><span>{@htmlUrl}</span><span>"</span><span>&gt;&lt;</span><span>xsl:value-of</span><span> select=</span><span>"@text"</span><span>/&gt;&lt;/a&gt;</span></span>
<span id="cb2-33">                (<span>&lt;a</span><span> class=</span><span>"feed"</span><span> href=</span><span>"</span><span>{@xmlUrl}</span><span>"</span><span>&gt;</span>feed<span>&lt;/a&gt;</span>)</span>
<span id="cb2-34">              <span>&lt;/strong&gt;</span></span>
<span id="cb2-35">              <span>&lt;</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-36">                <span>&lt;</span><span>xsl:when</span><span> test=</span><span>"@description != </span><span>''</span><span>"</span><span>&gt;</span></span>
<span id="cb2-37">                  <span>&lt;br/&gt;&lt;</span><span>xsl:value-of</span><span> select=</span><span>"@description"</span><span>/&gt;</span></span>
<span id="cb2-38">                <span>&lt;/</span><span>xsl:when</span><span>&gt;</span></span>
<span id="cb2-39">              <span>&lt;/</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-40">            <span>&lt;/li&gt;</span></span>
<span id="cb2-41">          <span>&lt;/</span><span>xsl:when</span><span>&gt;</span></span>
<span id="cb2-42">        <span>&lt;/</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-43">      <span>&lt;/</span><span>xsl:when</span><span>&gt;</span></span>
<span id="cb2-44">    <span>&lt;/</span><span>xsl:choose</span><span>&gt;</span></span>
<span id="cb2-45">  <span>&lt;/</span><span>xsl:template</span><span>&gt;</span></span>
<span id="cb2-46"><span>&lt;/</span><span>xsl:stylesheet</span><span>&gt;</span></span></code></pre></div>
</details>
<p>You can link to the stylesheet in your OPML file by adding
<code>&lt;?xml-stylesheet type="text/xsl" href="path/to/stylesheet.xsl"?&gt;</code>
at the top. I actually do this on <a href="https://kmaasrud.com/blogroll.xml">my own blogroll</a>, so check
that out if you want some inspiration.</p>
<p>While we’re all getting a bit fed up with the big platforms, OPML is
like a breath of fresh air from the old web days. It’s all about making
life easier when managing feeds, sharing cool finds, and stumbling upon
new stuff. So I encourage you to create your own blogroll, slap it on
your website, and share what you’re into. It’s a simple move, but it
could spark some real connections and bring back a bit of that community
vibe we miss.</p>


<section>
<h3>Here are some posts from sites I follow</h3>
<section>

<div>
<h4 dir="auto">
<a href="https://blog.rust-lang.org/2024/02/06/crates-io-status-codes.html" target="_blank" rel="noopener">crates.io: API status code changes</a>
</h4>
<p dir="auto">Cargo and crates.io were developed in the rush leading up to the Rust 1.0 release to fill the needs for a tool to manage dependencies and a registry that people could use to share code. This rapid work resulted in these tools being connected with an API t…</p>
<p>via <a href="https://blog.rust-lang.org/">Rust Blog</a>
February 6, 2024</p>
</div>

<div>
<h4 dir="auto">
<a href="https://sourcehut.org/blog/2024-01-19-outage-post-mortem/" target="_blank" rel="noopener">SourceHut network outage post-mortem</a>
</h4>
<p dir="auto">It’s been a busy couple of weeks here at SourceHut. At the time of writing, we
have restored SourceHut to full service following an unprecedented 170 hour
outage, and while we still have numerous kinks to sort out following an
unscheduled emergency migration…</p>
<p>via <a href="https://sourcehut.org/blog/">Blogs on Sourcehut</a>
January 19, 2024</p>
</div>

<div>
<h4 dir="auto">
<a href="https://nutcroft.mataroa.blog/blog/when-hope-and-gloom-unite/" target="_blank" rel="noopener">When hope and gloom unite</a>
</h4>
<p dir="auto">This is the Matrix and I am Neo. Not because I am the One — I'm not. The reason I am Neo is because I really can fly, I really can stop bullets, I really can download knowledge to my brain.
The reader might expect that I justify the above but instead I…</p>
<p>via <a href="https://nutcroft.com/">nutcroft</a>
January 16, 2024</p>
</div>

</section>
</section>
</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Building the DirectX shader compiler better than Microsoft? (253 pts)]]></title>
            <link>https://devlog.hexops.com/2024/building-the-directx-shader-compiler-better-than-microsoft/</link>
            <guid>39324800</guid>
            <pubDate>Sat, 10 Feb 2024 09:22:33 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://devlog.hexops.com/2024/building-the-directx-shader-compiler-better-than-microsoft/">https://devlog.hexops.com/2024/building-the-directx-shader-compiler-better-than-microsoft/</a>, See on <a href="https://news.ycombinator.com/item?id=39324800">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><main aria-role="main"><div><p>This is a <del>story</del> nightmare about the messy state of Microsoft’s DirectX shader compiler, and trying to wrangle it into a nicer experience for game developers. In some respects, we now build the DXC compiler better than how Microsoft does.</p><h2 id="setting-the-stage">Setting the stage</h2><p>For <a href="https://machengine.org/">Mach engine</a> we’ve been building an <a href="https://devlog.hexops.com/2024/mach-v0.3-released/#sysgpu">experimental graphics API called sysgpu</a> using Zig, aiming to be a <em>successor</em> and <em>descendant</em> of WebGPU for native graphics. It will support Metal, Vulkan, Direct3D, and OpenGL backends. As part of this, we need to compile shader programs into something that Direct3D 12 can consume. But what does it consume?</p><h2 id="a-brief-history-lesson">A brief history lesson</h2><p>The DirectX graphics API uses HLSL as its shading language of choice. In the past, with Direct3D 11 and earlier, this compiler was called ‘FXC’ (the ‘effects compiler’)</p><h3 id="fxc-is-deprecated-dxc-enters-the-scene-with-direct3d-12">FXC is deprecated, DXC enters the scene with Direct3D 12</h3><p>Unfortunately, FXC as a compiler is rather notoriously slow among game developers, with suboptimal code generation - meaning shaders often both compile and execute fairly suboptimally.</p><p>With the release of Direct3D 12 and Shader Model 6.0 (SM6), Microsoft officially deprecated the FXC compiler distributed as part of the Windows OS in favor of a new compiler called ‘DXC’ (‘directx compiler’), which exists as a public Microsoft-official fork of LLVM/Clang v3.7 <a href="https://github.com/microsoft/DirectXShaderCompiler">Microsoft/DirectXShaderCompiler</a> and prebuilt binaries you can download.</p><p>In this Microsoft fork of LLVM, changes are meticulously annotated via <code>// HLSL Change Start</code> and <code>// HLSL Change End</code> comments making it clear who owns what code:</p><p><a href="https://devlog.hexops.com/img/2024/hlsl-change-start"><img src="https://devlog.hexops.com/img/2024/hlsl-change-start.png"></a></p><h3 id="what-a-directx-driver-eats-for-breakfast-dxbc-or-dxil">What a DirectX driver eats for breakfast: DXBC or DXIL</h3><p>Although HLSL is the language of choice for Direct3D programming, at the end of the day GPUs under the hood all have different compute architectures and requirements: the compiled binary form of a shader program that an Intel GPU needs is going to be different from what an NVIDIA GPU needs, same goes for AMD.</p><p>Microsoft’s role is to provide the nice game developer frontend APIs (like Direct3D, and the HLSL shanding language), while working with independent hardware vendors (IHV’s) like Intel/AMD/NVIDIA who write the drivers - bridging those nice frontend APIs to whatever is hopefully closest to hardware manufacturer’s instruction set architecture (ISA) under the hood. You can think of it like web browsers making sure JavaScript can run on both a Windows PC and a macOS Apple Silicon device, though graphics developers would spit at the suggested comparison.</p><p>DirectX versions 9-11 had driver manufacturers consuming what is called DXBC (DirectX Byte Code) - game developers would produce DXBC either using a CLI tool to compile their HLSL programs like <code>fxc.exe</code>, or at runtime using the <code>d3dcompiler</code> APIs, and then the driver’s job was to take that decently-optimized shader bytecode and turn it into the actual binary that the GPU would run. This bytecode was an undocumented, proprietary format really only shared between Microsoft and GPU driver manufacturers - excluding a few odd-ball Linux developers who cared to reverse engineer it for Proton.</p><p>With the advent of DirectX 12 and Shader Model 6.0, Microsoft aspirationally had intended to create their own standard IR called DXIR, but in 2021 they <a href="https://github.com/microsoft/DirectXShaderCompiler/commit/61c6573842be58a14e1dfc6b1b3def03d39d9988">removed all language suggesting they might do this</a>. The intent <em>was</em> for DXIR to be the ‘high level’, ‘unoptimized’ IR form which compilers (think: Rust) could target, and then the DXC compiler could lower DXIR into the optimized DXIL bytecode form, a new ‘low level’ post-optimization IR format, before handing it off to graphics drivers to muck with as they please before it gets translated to run on the actual hardware.</p><p>Asked about DXIR documentation, a <a href="https://github.com/microsoft/DirectXShaderCompiler/issues/2389#issuecomment-517076643">Microsoft employee</a> had noted this in 2019:</p><blockquote><p>Unfortunately, documentation on the lowering process [from DXIR to DXIL] is mostly non-existent. […]</p><p>Oh, and DXIR isn’t anything official, but just the first LLVM IR after CodeGen.</p></blockquote><p>As you’ll soon see, this theme of ‘there are no docs, just whatever our compiler actually does’ will become a common pattern.</p><h3 id="dxil">DXIL</h3><p>DXIL (pronunciation?) is the official format that DirectX 12 driver manufacturers consume <em>today</em>.</p><p>A game developer produces DXIL bytecode using the DXC compiler, which is a fork of LLVM/clang heavily modified to support HLSL compilation, and the DirectX APIs hand that DXIL over to the graphics driver which then converts the IR into their own intermediate languages, performing any secret sauce optimization passes on it, and ultimately boiling down to the actual machine code that will run on the GPU hardware.</p><p>Much like the old bytecode format DXBC which DXIL replaced, it is <em>also</em> an undocumented bytecode format, specifically it is LLVM’s version 3.7 post-codegen post-optimization-passes bitecode format. It is undocumented not because nobody wants to document it, but rather because the documentation is literally ‘whatever the Microsoft fork of LLVM v3.7 with all the HLSL changes we made, after CodeGen and optimization passes have occurred, actually emits as LLVM bitcode - plus a small custom container/wrapper file format on top.’</p><h3 id="correcting-the-microsoft-fork-of-llvm">Correcting the Microsoft fork of LLVM</h3><p>Microsoft themselves are well aware that a bunch of independent driver manufacturers relying on and expecting to consume a hyper-specific undocumented LLVM bitcode format specifically produced by their fork is, well, less than ideal - and also aware that their fork of LLVM is not super fun to maintain, either. Quoting <a href="https://github.com/microsoft/DirectXShaderCompiler/issues/5773#issuecomment-1735794551">another Microsoft employee</a> (Sep 2023) who was asked about the potential of adding DirectX 9/10/11 support to the new/better DXC compiler, they stated:</p><blockquote><p>DXC’s fork of LLVM removed and/or damaged much of the code generation layer and infrastructure [of LLVM]. Given that, supporting DXBC generation in DXC would be a massive task to fix and restore broken LLVM functionality. Due to the large scale of this issue and resource constraints on our team we’re not going to address this issue in [the new] DXC [compiler] ever.</p><p>We may support DXBC generation in Clang in the future (we mentioned that in the original proposal to LLVM). That work is unlikely to begin for a few years as our focus will be on supporting DXIL and SPIR-V generation first.</p></blockquote><p>As noted above, in March of 2022, Microsoft had proposed and begun work on <a href="https://discourse.llvm.org/t/rfc-adding-hlsl-and-directx-support-to-clang-llvm/60783">upstreaming HLSL compilation support directly into LLVM/clang proper</a> - work that is still ongoing today - and involved <em>adding back</em> legacy LLVM v3.7 bitcode writing support to modern LLVM/clang versions:</p><blockquote><p>By isolating as much of the DXIL-specific code as possible into a target we hope to minimize the cost on the community to maintain our legacy bitcode writing support.</p></blockquote><p>i.e. the plan to get away fromn their fork is to upstream HLSL and DXIL support to LLVM/clang proper.</p><h2 id="the-challenge-for-gamedevs-webgpu-etc">The challenge for gamedevs, WebGPU, etc.</h2><p>Graphics abstraction layers which aim to provide a unified interface to modern graphics APIs like Metal, Direct3D 12, and Vulkan.. ultimately need to provide a unified shading language as well. If you look today, you’ll find most WebGPU implementations which do this have had a goal of ‘in the future we might be able to emit DXIL directly..’ but in practice, none actually do.</p><p>Instead, basically every WebGPU implementation today behaves as follows:</p><ul><li>The WGSL textual language first gets translated to HLSL at runtime</li><li>HLSL is compiled into DXBC or DXIL using an HLSL compiler</li><li>The optimized DXBC/DXIL is handed to the graphics driver, which then gets converted to the various vendor-specific ILs before finally becoming machine code that runs on the GPU.</li></ul><h3 id="a-quick-detour-spir-v">A quick detour: SPIR-V</h3><p>Vulkan/SPIR-V does much the same as the above, in fact most drivers cannot assume SPIR-V is optimized at all - though some do, and this varies by mobile/desktop GPUs - and have more work to perform to get SPIR-V turned into a <em>driver-compiled</em> native binary.</p><p>Valve has <a href="https://github.com/ValveSoftware/Fossilize">Fossilize</a> and maintains caches of each specific (GPU, driver version, etc.) pairing along with the <em>actual</em> driver-compiled binary for a SPIR-V blob, to enable downloading ‘pre-cached shaders’ from Valve servers ahead of playing games for this reason: so that you don’t spend all day waiting for your computer to go brrr compiling and optimizing SPIR-V shaders into actual native code your GPU understands.</p><p>In other words, DXIL is always post-optimization-passes LLVM <em>bitcode</em>, while SPIR-V can or cannot be an an optimized form, and GPU manufacturers write their drivers based on what SPIR-V looks like in the wild - which may or may not be a pre-optimized form. SPIR-V is closer to hardware than a textual shading language, but still very far from native machine code a GPU understands.</p><p>Only Apple’s Metal graphics API supports compiling directly to the actual target hardware’s native binary format (thanks to that iron fist they hold over their hardware, I guess.)</p><h2 id="to-use-dxcompilerdll-or-not">To use dxcompiler.dll or not?</h2><p>Since WGSL-&gt;HLSL-&gt;DXIL is happening at runtime, WebGPU runtimes are faced with a challenge: do we use the new DXC HLSL compiler, or the old, officially deprecated FXC compiler which has worse performance and codegen quality? On the surface, this hardly sounds like a difficult choice!</p><p>However, despite this, many indie devs and game engines choose to use FXC by default. <a href="https://docs.rs/bevy/latest/i686-pc-windows-msvc/bevy/render/settings/enum.Dx12Compiler.html">Bevy game engine’s documentation</a> puts it really well:</p><blockquote><p>The Fxc compiler (default) is old, slow and unmaintained. However, it doesn’t require any additional .dlls to be shipped with the application.</p><p>The Dxc compiler is new, fast and maintained. However, it requires both <code>dxcompiler.dll</code> and <code>dxil.dll</code> to be shipped with the application. These files can be downloaded from <a href="https://github.com/microsoft/DirectXShaderCompiler/releases">https://github.com/microsoft/DirectXShaderCompiler/releases</a>.</p></blockquote><p>As a result, much software defaults to the old, slow and unmaintained compiler. And it’s not just Bevy: <code>wgpu</code> Rust users, Dawn WebGPU users, etc. are all faced with this same question. It’s likely one of the reasons WebGPU does not support Shader Model 6.0+ functionality today - using the DXC compiler is not so pleasant: it is after all a large, clunky Microsoft fork of a C++ codebase from nearly a decade ago!</p><h2 id="well-why-not-just-statically-link-against-it">Well, why not just statically link against it?</h2><p>You can’t.</p><p>Firstly, there is the issue that <a href="https://github.com/microsoft/DirectXShaderCompiler/issues/4766">Microsoft’s fork of LLVM doesn’t support statically linking</a>. On the surface, this appears just to be due to some cmake files assuming <code>SHARED</code> instead of <code>STATIC</code> when creating libraries, but if you dig into it - as I did - you’ll soon find it is <em>much</em> more involved than that.</p><p>Switching <code>SHARED</code> to <code>STATIC</code> everywhere in CMake files will appear to get you a build with ~15 different static libraries to link against (not pleasant compared to just one.) You might think using cmake <code>OBJECT</code> libraries could solve this, but with this you will quickly encounter an issue where although the cmake files are structured logically as dependants, they actually have implicit dependencies on eachother due to the HLSL changes Microsoft made. I am 80% sure you would need to rewrite every cmake file in the repository to support OBJECT libraries. I can say this, because I tried!</p><p>You might be thinking, linking against ~15 static libraries isn’t SO bad as long as the final executable is static, right?</p><p>Not so fast - many parts of DXC’s COM interface implementation is also explicitly designed to load itself as a DLL, i.e. to load <code>dxcompiler.dll</code> and <code>dxil.dll</code> as dynamic libraries and self-invoke methods.</p><p>OK, we just need to patch the implementation to not call <code>LoadLibraryW</code> then, basically, right?</p><h2 id="introducing-dxildll---the-proprietary-code-signing-blob-for-directx-shaders">Introducing dxil.dll - the proprietary code signing blob for DirectX shaders</h2><p>If you’ve ever built DirectXShaderCompiler from source, you might notice something: dxil.dll doesn’t get built. Why? It’s distributed in every release on GitHub, both for Windows (x86/arm) and Linux (x86 only).</p><p>Strange, I thought the compiler was supposed to be open source? Well, it wouldn’t be the first time<a href="https://github.com/microsoft/win32metadata/issues/766#issuecomment-1150271300">[0]</a><a href="https://github.com/microsoft/Azure-Kinect-Sensor-SDK/issues/1521">[1]</a> I’ve encountered a Microsoft ‘open source’ repository that actually completely depends on some proprietary platform-specific code blobs behind the scenes.</p><p>Incidentally, I stumbled across the <a href="https://microsoft.github.io/DirectX-Specs/d3d/ShaderCache.html">D3D12 Shader Cache API specification</a> which mentions the existence of this proprietary code signing blob as a ‘good reason for invoking the shader compiler at runtime’:</p><blockquote><p>D3D12 will only accept signed shaders. That means that if any patching or runtime optimizations are performed, such as constant folding, the shader must be re-validated and re-signed, which is non-trivial.</p></blockquote><p>And in the recent <a href="https://github.com/microsoft/DirectXShaderCompiler/releases/tag/v1.8.2306-preview">‘preview release’ for Shader Model 6.8 functionality</a>, Microsoft notes how they appear to leverage this DLL to restrict new experimental shader functionality:</p><blockquote><p>The DXIL signing library (dxil.dll/libdxil.so) is not provided with this preview release. DXIL generated with this compiler targeting Shader Model 6.8 is not final, cannot be validated, and is not supported for distribution or execution on machines not running in developer mode.</p></blockquote><p>In other words: if you do not have dxil.dll, then your shaders will not be signed/validated. If your shaders are not signed/validated, then they cannot run on a Windows machine unless it is running in Developer Mode.</p><h2 id="platform-support-challenges">Platform support challenges</h2><p>For a second, I’d like to go back to something I wrote at the start of this article:</p><blockquote><p>For <a href="https://machengine.org/">Mach engine</a> […] we need to compile shader programs into something that Direct3D 12 can consume.</p></blockquote><p>I’d like for us to be able to perform offline shader compilation, and skip out on distributing the heavy DXC dependency, when desired.</p><p>But Microsoft only distributes a copy of dxil.dll for Windows (x86/arm) and Linux (x86). There’s no Linux aarch64 binary. There’s no macOS binary. In other words, you can’t produce builds of your cross-platform game for Windows using offline shader compilation on a mac, or in your Arm Linux CI pipeline. You need a Windows or x86_64 Linux machine to run the proprietary blob.</p><h2 id="recap">Recap</h2><p>To recap:</p><ul><li>We cannot build DXC as a static library, because the decades-old Microsoft fork of LLVM v3.7 has a very messy build-system.</li><li>Even if we could, we cannot build DXC as a static library <strong>because of the proprietary code-signing blob</strong>.</li><li>We cannot compile DirectX HLSL shaders offline on a Mac, or build our cross-platform game in an arm Linux CI pipeline, because Microsoft doesn’t distribute copies of <strong>the proprietary code signing blob</strong> for those platforms.</li></ul><h2 id="going-deeper">Going deeper</h2><h3 id="un-the-build-system">Un#$@&amp;%*! the build system</h3><p>The first problem I wanted to address was how to actually build this codebase into a single static library.</p><p>After several days of attempting to fix the implicit dependencies that changing the cmake virtual libraries from <code>DYNAMIC</code> -&gt; <code>OBJECT</code> surfaces, I gave up. Originally, my intent was to use their existing cmake build system (so as to not diverge from their codebase too much) and just swap the compiler with <code>zig cc</code> as the build toolchain for cross-compilation.</p><p>After it slowly and painfully became apparent that direction was not going to be <em>any</em> better than maintaining the entire buildsystem myself, I decided to just bite the bullet and rewrite the entire CMake build system they had, some ~10.5k lines of code, using <code>build.zig</code> instead. To make things simpler, I chose to build only the two parts we (and others) really care about as consumers of the code: the <code>dxcompiler.dll</code> library, and <code>dxc.exe</code> binary for offline compilation / testing. (we’ll deal with <code>dxil.dll</code> later.)</p><p>This resulted in somewhere around <a href="https://github.com/hexops/mach-dxcompiler/blob/bd0cfbe4230133d8d3b50eedf1a0d0c4a00f47d7/build.zig#L1-L956">~1k lines of build.zig logic</a>, and in practice it’s less than that because much of it is just related to running <code>git clone</code> on the source repository, having the ability for Zig package consumers to use a prebuilt binary instead of building the large C++ library from source, and header/source generation (though we’re still not done with that, thanks to llvm-tablegen)</p><h3 id="un-the-dynamic-library-dependency">Un#$@&amp;%*! the dynamic library dependency</h3><p>As mentioned earlier, DXC is written with the expectation that <code>dxcompiler.dll</code> and <code>dxil.dll</code> exist. Reading the code, it almost appears as if the COM API implementation invokes the DLL, which then invokes itself dynamically depending on which is available.</p><p>Taking some advice from Microsoft, I got my hands dirty, <em>forked their codebase</em> and got to work on the actual C++ code. I began annotating my changes with cute <code>// Mach change start</code> and <code>// Mach change end</code> comments, to know who owns what code. All of this existing as a choice that I hope will come back to haunt my dreams in the future as much as Microsoft’s own choice to underemploy the HLSL team and fork LLVM 3.7 originally.</p><p>I was off to the races: <a href="https://github.com/hexops/DirectXShaderCompiler/blob/4190bb0c90d374c6b4d0b0f2c7b45b604eda24b6/tools/clang/tools/dxcompiler/DXCompiler.cpp#L88">simulating dllmain</a> entrypoints, <a href="https://github.com/hexops/DirectXShaderCompiler/blob/4190bb0c90d374c6b4d0b0f2c7b45b604eda24b6/tools/clang/tools/dxclib/dxc.cpp#L1258">disabling</a> the ability to print the compiler version info derived from the dlls, and <a href="https://github.com/hexops/DirectXShaderCompiler/blob/4190bb0c90d374c6b4d0b0f2c7b45b604eda24b6/include/dxc/Support/dxcapi.use.h#L17">emulating dynamic library function pointer loads</a>.</p><h3 id="un-the-proprietary-code-signing">Un#$@&amp;%*! the proprietary code signing</h3><p>All that was left was that pesky <code>dxil.dll</code> - what sort of magic might Microsoft be employing in that library to “sign shaders”? How can they prevent unsigned shaders from running on Windows machines that aren’t in developer mode? How are they able to distribute that binary on Linux, too?</p><p>I won’t comment on any of those questions, but will say that <a href="https://github.com/hexops/DirectXShaderCompiler/blob/4190bb0c90d374c6b4d0b0f2c7b45b604eda24b6/tools/clang/tools/dxcompiler/MachSiegbertVogtDXCSA.cpp#L178">you’ll find dxil.dll is NOT a dependency of mach-dxcompiler in any form</a>. You can compile an HLSL shader on a macOS machine using mach-dxcompiler, without the proprietary <code>dxil.dll</code> blob - and end up with a DXIL bytecode file that is byte-for-byte equal to one which runs it on a standard Windows box. Enjoy!</p><h2 id="results">Results</h2><p>We now have prebuilt, static binaries of the <code>dxcompiler</code> library, as well as the <code>dxc</code> CLI <a href="https://github.com/hexops/mach-dxcompiler/releases/tag/2024.02.10%2B2c3635c.1">here</a>, with zero dependency on the proprietary <code>dxil.dll</code>. At the time of writing, we have binaries building in our CI pipeline for:</p><ul><li>macOS (the first ever in history), both Apple Silicon (aarch64) and Intel (x86_64).</li><li>Linux, including musl and glibc, as well as aarch64 (first ever in history) and x86_64.</li><li>Windows, x86_64 and aarch64, including for MinGW/GNU ABI (first ever in history?)</li></ul><p>Additionally included is a <a href="https://github.com/hexops/mach-dxcompiler/blob/main/src/mach_dxc.h">small C API</a> the library now exposes, as an alternative to the COM API traditionally required.</p><p>Zig game developers will find the repository also includes a Zig API, see <a href="https://github.com/hexops/mach-dxcompiler/blob/main/src/main.zig"><code>src/main.zig</code></a> tests for usage. By default prebuilt binaries are downloaded/used.</p><p>You can <a href="https://github.com/hexops/mach-dxcompiler">build from source yourself</a> for any OS/arch with only <code>zig</code> and <code>git</code>, just make sure you have <a href="https://machengine.org/about/zig-version/">the right Zig version</a>:</p><pre><code>git clone https://github.com/hexops/mach-dxcompiler
cd mach-dxcompiler/
zig build -Dfrom_source -Dtarget=aarch64-macos
zig build -Dfrom_source -Dtarget=x86_64-windows-gnu
zig build -Dfrom_source -Dtarget=x86_64-linux-gnu
</code></pre><h2 id="caveats">Caveats</h2><p>It’s not all roses - there are some drawbacks:</p><ul><li>Windows MSVC ABI binaries are currently not building due to a small bug in the C bindings - will fix it quickly if important for you, otherwise at our own pace.</li><li>Linux musl binaries are untested, they build fine and I’d be curious to know if they run fine!</li><li>With Mach engine, we plan to use Zig itself as our shading language, not HLSL, so I do not build SPIRV-output support, sorry! I have no plans to add it.</li><li>No plans to update this to support SM6.7 currently (released very recently), though perhaps in the future.</li><li>LLVM’s cmake build system is not trivial, there are some aspects yet-to-be-translated. See <code>generated-include/</code> for specifics which come from the cmake build system still.</li><li>If you use this, you’ll be relying on myself to fix/address any issues. I am the only person working on this, and it exists solely to solve Mach’s own problems. If it works for you, great - but there may be a time we find a better path forward for us and it could get deprecated, so keep that in mind.</li></ul><h2 id="on-a-personal-note">On a personal note</h2><div><p><a href="https://github.com/slimsag"><img src="https://machengine.org/img/slimsag-profile.png"></a></p><div><p>My name is Stephen, I work a normal tech job, and after signing off from work at the end of the day I go online to build <a href="https://machengine.org/">Mach engine</a>. I've been dreaming of being able to build a game engine like this for a long time, and I'm finally doing it!</p><p>FOSS <a href="https://devlog.hexops.com/2021/increasing-my-contribution-to-zig-to-200-a-month#i-grew-up-playing-linux-games-like-mania-drive">is in my roots</a>, I believe we should own our tools, they should empower <em>us</em>-not be part of <a href="https://kristoff.it/blog/the-open-source-game/">the 'open source' game</a> which is all too prevelant today (even among 'open source' engines.) I <em>need</em> Mach to genuinely be <a href="https://softwareyoucan.love/">software you can love</a>.</p><p>My dream is one day to live a simple, modest, life earning a living building Mach for everyone and selling high-quality games. Please consider <a href="https://github.com/sponsors/slimsag">sponsoring my work</a> if you believe in my vision. It means the world to me!</p></div></div><h2 id="thanks-for-reading">Thanks for reading</h2><div><p><img src="https://user-images.githubusercontent.com/3173176/187348488-0b52e87d-3a48-421c-9402-be78e32b5a20.png"></p><ul><li>Check out <a href="https://machengine.org/">machengine.org</a></li><li>Consider <a href="https://github.com/sponsors/slimsag">sponsoring development</a> so we can do more of it!</li><li>Join the <a href="https://discord.gg/XNG3NZgCqp">Mach Discord server</a></li></ul></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[ZX – A tool for writing better scripts (153 pts)]]></title>
            <link>https://github.com/google/zx</link>
            <guid>39323986</guid>
            <pubDate>Sat, 10 Feb 2024 06:01:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google/zx">https://github.com/google/zx</a>, See on <a href="https://news.ycombinator.com/item?id=39323986">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><h2 tabindex="-1" dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5f714c65dc258cb17ef40c5ac0a477ebf22dfcde8507c93ab8870219d500dcd0/68747470733a2f2f676f6f676c652e6769746875622e696f2f7a782f696d672f6c6f676f2e737667"><img src="https://camo.githubusercontent.com/5f714c65dc258cb17ef40c5ac0a477ebf22dfcde8507c93ab8870219d500dcd0/68747470733a2f2f676f6f676c652e6769746875622e696f2f7a782f696d672f6c6f676f2e737667" alt="Zx logo" height="32" data-canonical-src="https://google.github.io/zx/img/logo.svg"></a> zx</h2>
<div dir="auto" data-snippet-clipboard-copy-content="#!/usr/bin/env zx

await $`cat package.json | grep name`

let branch = await $`git branch --show-current`
await $`dep deploy --branch=${branch}`

await Promise.all([
  $`sleep 1; echo 1`,
  $`sleep 2; echo 2`,
  $`sleep 3; echo 3`,
])

let name = 'foo bar'
await $`mkdir /tmp/${name}`"><pre>#!/usr/bin/env zx

<span>await</span> <span>$</span><span>`cat package.json | grep name`</span>

<span>let</span> <span>branch</span> <span>=</span> <span>await</span> <span>$</span><span>`git branch --show-current`</span>
<span>await</span> <span>$</span><span>`dep deploy --branch=<span><span>${</span><span>branch</span><span>}</span></span>`</span>

<span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>[</span>
  <span>$</span><span>`sleep 1; echo 1`</span><span>,</span>
  <span>$</span><span>`sleep 2; echo 2`</span><span>,</span>
  <span>$</span><span>`sleep 3; echo 3`</span><span>,</span>
<span>]</span><span>)</span>

<span>let</span> <span>name</span> <span>=</span> <span>'foo bar'</span>
<span>await</span> <span>$</span><span>`mkdir /tmp/<span><span>${</span><span>name</span><span>}</span></span>`</span></pre></div>
<p dir="auto">Bash is great, but when it comes to writing more complex scripts,
many people prefer a more convenient programming language.
JavaScript is a perfect choice, but the Node.js standard library
requires additional hassle before using. The <code>zx</code> package provides
useful wrappers around <code>child_process</code>, escapes arguments and
gives sensible defaults.</p>
<h2 tabindex="-1" dir="auto">Install</h2>

<h2 tabindex="-1" dir="auto">Documentation</h2>
<p dir="auto">Read documentation on <a href="https://google.github.io/zx/" rel="nofollow">google.github.io/zx</a>.</p>
<h2 tabindex="-1" dir="auto">License</h2>
<p dir="auto"><a href="https://github.com/google/zx/blob/main/LICENSE">Apache-2.0</a></p>
<p dir="auto">Disclaimer: <em>This is not an officially supported Google product.</em></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[BirdLingo: A birdsong learning game (144 pts)]]></title>
            <link>https://jessicalieb.itch.io/birdlingo</link>
            <guid>39323893</guid>
            <pubDate>Sat, 10 Feb 2024 05:37:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://jessicalieb.itch.io/birdlingo">https://jessicalieb.itch.io/birdlingo</a>, See on <a href="https://news.ycombinator.com/item?id=39323893">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="community_topic_posts_1071339"><div id="post-9340610" data-post="{&quot;id&quot;:9340610,&quot;user_id&quot;:1986376}"><a href="https://itch.io/profile/ironarachne"></a><div><p>This is awesome! I would pay for a more thorough version of this.</p></div></div><div id="post-9335184" data-post="{&quot;id&quot;:9335184,&quot;user_id&quot;:7197833}"><a href="https://itch.io/profile/smoothiefemale696"></a><div dir="auto"><p>hey i ranked your game and 19 others here:</p>

<p>ur ranking was ok. check out my spreadsheet linked below the video for some feedback. if you would like more coverage i am making another video for subscriber games only. so far i only have 1 subscriber who is a game dev so rankings would be a lot less competitive and i would spend longer on each game.</p></div></div><div id="post-9325027" data-post="{&quot;id&quot;:9325027,&quot;user_id&quot;:2113943}"><a href="https://itch.io/profile/tunegoro"></a><div dir="auto"><p>I had never been aware of the different types of birdsong, but with careful repetition over three levels, I was able to learn the birdsongs beautifully!</p>
<p>A very good learning software full of love for wild birds, meow!</p></div></div><div id="post-9326514" data-post="{&quot;id&quot;:9326514,&quot;user_id&quot;:9854420}"><a href="https://itch.io/profile/jessicalieb"></a><div><p>Wow, that's a great video!&nbsp;Thank you so much for testing my game, I'm glad you like it :)&nbsp;&nbsp;ありがとう！</p></div></div><div id="post-9294954" data-post="{&quot;id&quot;:9294954,&quot;user_id&quot;:4858078}"><a href="https://itch.io/profile/infinity-kitsune27"></a><div><p>This game is really cute :3 Also it's really educational as well so</p></div></div><div id="post-9293211" data-post="{&quot;id&quot;:9293211,&quot;user_id&quot;:9038779}"><a href="https://itch.io/profile/ryocotori"></a><div><p>I'm Japanese and my English isn't very good, but I was able to enjoy this.<br>I love birds, and whenever I hear a bird chirping, I always wonder what kind of bird it is.<br>The types of birds were a little different from those seen in Japan, but the birds that looked similar had similar voices.<br>I want to memorize all the bird calls.</p></div></div><div id="post-9297592" data-post="{&quot;id&quot;:9297592,&quot;user_id&quot;:9854420}"><a href="https://itch.io/profile/jessicalieb"></a><div><p>Thank you! Maybe one day I'll make a version with Japanese birds ;)</p></div></div><div id="post-9290120" data-post="{&quot;id&quot;:9290120,&quot;user_id&quot;:7197833}"><a href="https://itch.io/profile/smoothiefemale696"></a><div dir="auto"><p>hey i really like this a lot. really excellent educational software that plays like a game. im just curious cause i lived in france for 12 months is your first language english or french?? i got past level 1 and part way through level 2 of identifying bird songs.&nbsp;</p><p>also i make YT videos about itch games like this one:</p>

<p>do you want your game reviewed in my next video???</p></div></div><div><div id="post-9291803" data-post="{&quot;id&quot;:9291803,&quot;user_id&quot;:9854420}"><a href="https://itch.io/profile/jessicalieb"></a><div><p>Hi, thank you! My first language is French. It would be really cool to be featured in your video :)</p></div></div><div id="post-9304071" data-post="{&quot;id&quot;:9304071,&quot;user_id&quot;:7197833}"><a href="https://itch.io/profile/smoothiefemale696"></a><div><p>wonderful ill take some gameplay footage now</p></div></div></div><div id="post-9286807" data-post="{&quot;id&quot;:9286807,&quot;user_id&quot;:2595742}"><a href="https://itch.io/profile/queerdisagreeable"></a><div><p>this is exactly scratching the itch of wanting to birdwatch but being too dumb to remember the different calls. i love this. thank you.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LubeLogger: Self-hosted, open-source vehicle service records and tracker (102 pts)]]></title>
            <link>https://github.com/hargata/lubelog</link>
            <guid>39323601</guid>
            <pubDate>Sat, 10 Feb 2024 04:29:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/hargata/lubelog">https://github.com/hargata/lubelog</a>, See on <a href="https://news.ycombinator.com/item?id=39323601">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/155338622/293981546-545debcd-d80a-44da-b892-4c652ab0384a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDc1OTkxMDUsIm5iZiI6MTcwNzU5ODgwNSwicGF0aCI6Ii8xNTUzMzg2MjIvMjkzOTgxNTQ2LTU0NWRlYmNkLWQ4MGEtNDRkYS1iODkyLTRjNjUyYWIwMzg0YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMjEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDIxMFQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMzA4ZTZhMzdiM2E3NzBjZjk3NTlmZDhkMTY2NzIxZTAxOWI5YjJkY2RjYjBiYWQ3ODg1MTJiNWUyY2JhMTMzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.wPkrV0t8p6e2SNPZ73coHxXmzmUQxkrKWE7jmByp-vE"><img src="https://private-user-images.githubusercontent.com/155338622/293981546-545debcd-d80a-44da-b892-4c652ab0384a.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDc1OTkxMDUsIm5iZiI6MTcwNzU5ODgwNSwicGF0aCI6Ii8xNTUzMzg2MjIvMjkzOTgxNTQ2LTU0NWRlYmNkLWQ4MGEtNDRkYS1iODkyLTRjNjUyYWIwMzg0YS5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQwMjEwJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MDIxMFQyMTAwMDVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1jMzA4ZTZhMzdiM2E3NzBjZjk3NTlmZDhkMTY2NzIxZTAxOWI5YjJkY2RjYjBiYWQ3ODg1MTJiNWUyY2JhMTMzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCZhY3Rvcl9pZD0wJmtleV9pZD0wJnJlcG9faWQ9MCJ9.wPkrV0t8p6e2SNPZ73coHxXmzmUQxkrKWE7jmByp-vE" alt="image"></a></p>
<p dir="auto">A self-hosted, open-source vehicle service records and maintainence tracker.</p>
<p dir="auto">Visit our website: <a href="https://lubelogger.com/" rel="nofollow">https://lubelogger.com</a></p>
<p dir="auto">Support this project by <a href="https://patreon.com/LubeLogger" rel="nofollow">Subscribing on Patreon</a> or <a href="https://buy.stripe.com/aEU9Egc8DdMc9bO144" rel="nofollow">Making a Donation</a></p>
<h2 tabindex="-1" dir="auto">Why</h2>
<p dir="auto">Because nobody should have to deal with a homemade spreadsheet or a shoebox full of receipts when it comes to vehicle maintainence.</p>
<h2 tabindex="-1" dir="auto">Screenshots</h2>
<p dir="auto"><a href="https://github.com/hargata/lubelog/blob/main/docs/screenshots.md">Screenshots</a></p>
<h2 tabindex="-1" dir="auto">Demo</h2>
<p dir="auto">Try it out before you download it! The live demo resets every 20 minutes.</p>
<p dir="auto"><a href="https://demo.lubelogger.com/" rel="nofollow">Live Demo</a> Login using username "test" and password "1234"</p>
<h2 tabindex="-1" dir="auto">Dependencies</h2>
<ul dir="auto">
<li>Bootstrap</li>
<li>LiteDB</li>
<li>Bootstrap-DatePicker</li>
<li>SweetAlert2</li>
<li>CsvHelper</li>
<li>Chart.js</li>
<li>Drawdown</li>
</ul>
<h2 tabindex="-1" dir="auto">Docker Setup (GHCR)</h2>
<ol dir="auto">
<li>Install Docker</li>
<li>Run <code>docker pull ghcr.io/hargata/lubelogger:latest</code></li>
<li>CHECK culture in .env file, default is en_US, this will change the currency and date formats. You can also setup SMTP Config here.</li>
<li>If using traefik, use docker-compose.traefik.yml</li>
<li>Run <code>docker-compose up</code></li>
</ol>
<h2 tabindex="-1" dir="auto">Docker Setup (Manual Build)</h2>
<ol dir="auto">
<li>Install Docker</li>
<li>Clone this repo</li>
<li>CHECK culture in .env file, default is en_US, also setup SMTP for user management if you want that.</li>
<li>Run <code>docker build -t lubelogger -f Dockerfile .</code></li>
<li>CHECK docker-compose.yml and make sure the mounting directories look correct.</li>
<li>If using traefik, use docker-compose.traefik.yml</li>
<li>Run <code>docker-compose up</code></li>
</ol>
<h2 tabindex="-1" dir="auto">Additional Docker Instructions</h2>
<h3 tabindex="-1" dir="auto">manual</h3>
<ul dir="auto">
<li>build</li>
</ul>
<div data-snippet-clipboard-copy-content="docker build -t hargata/lubelog:latest ."><pre><code>docker build -t hargata/lubelog:latest .
</code></pre></div>
<ul dir="auto">
<li>run</li>
</ul>
<div data-snippet-clipboard-copy-content="docker run -d hargata/lubelog:latest"><pre><code>docker run -d hargata/lubelog:latest
</code></pre></div>
<p dir="auto">add <code>-v</code> for persistent volumes as needed. Have a look at the docker-compose.yml for examples.</p>
<h2 tabindex="-1" dir="auto">docker-compose</h2>
<ul dir="auto">
<li>build image</li>
</ul>

<ul dir="auto">
<li>run</li>
</ul>
<div data-snippet-clipboard-copy-content="docker compose up

# or variant with traefik labels:

docker compose -f docker-compose.traefik.yml up"><pre><code>docker compose up

# or variant with traefik labels:

docker compose -f docker-compose.traefik.yml up
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Privacy focused platform Skiff is joining Notion, Skiff to be sunset (110 pts)]]></title>
            <link>https://www.notion.so/blog/meet-skiff-the-newest-member-of-the-notion-family</link>
            <guid>39322173</guid>
            <pubDate>Sat, 10 Feb 2024 00:22:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.notion.so/blog/meet-skiff-the-newest-member-of-the-notion-family">https://www.notion.so/blog/meet-skiff-the-newest-member-of-the-notion-family</a>, See on <a href="https://news.ycombinator.com/item?id=39322173">Hacker News</a></p>
Couldn't get https://www.notion.so/blog/meet-skiff-the-newest-member-of-the-notion-family: Error: Parse Error: Header overflow]]></description>
        </item>
        <item>
            <title><![CDATA[All my thoughts after 40 hours in the Vision Pro (112 pts)]]></title>
            <link>https://waitbutwhy.com/2024/02/vision-pro.html</link>
            <guid>39321395</guid>
            <pubDate>Fri, 09 Feb 2024 22:43:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://waitbutwhy.com/2024/02/vision-pro.html">https://waitbutwhy.com/2024/02/vision-pro.html</a>, See on <a href="https://news.ycombinator.com/item?id=39321395">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-10470">

		

		<div>
				
<p>
  I’m writing this on a 30-foot screen on top of a 10,000-foot mountain in Hawaii, at a table in an Austin coffee shop where I’m pretty sure other people are taking photos of me to send to their friends so they can all call me a piece of shit. In the last week, life has gotten weird.
</p>



<p>
  My journey to the Haleakalā shield volcano Austin coffee shop began more than 30 years ago, in 1990, when my parents brought me to something called a “virtual reality exhibit” at the Seaport World Trade Center in Boston. I stood on a little circular pedestal, and the guy handed me a plastic gun and put a big headset on me. Suddenly I was in some cartoon world, in a military uniform, holding a real gun. The person on the pedestal next to me was there, also a cartoon, also holding a gun. After some janky waving and shooting, they kicked me out for the next person in line. 
</p>



<p>
  I had recently read <em>The Phantom Tollbooth</em>, where a kid in the real world crosses a magic threshold and enters a cartoon world.<em> </em>This felt like that. I wanted more.
</p>



<p>
  Then VR disappeared for 25 years. Throughout the ‘90s and 2000s, “virtual reality” was a forgotten dream—a cool concept that never made it. But in the mid-2010s, VR made an unexpected comeback. 20-year-old Palmer Luckey’s duct-taped headset prototype had impressed enough investors for Oculus to become a real company. In 2014, Facebook bought Oculus. Google and Sony got involved. It was all finally happening.
</p>



<p>   In 2016, I decided to write about the VR revolution. I went around Silicon Valley, interviewing people at Google and Facebook to get the full scoop on VR. I even sat down with Mark Zuckerberg.</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck.png"><img fetchpriority="high" decoding="async" width="1139" height="797" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck.png 1139w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck-600x420.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck-750x525.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Zuck-768x537.png 768w " sizes="(max-width: 1139px) 100vw, 1139px"></a></figure>



<p>
  I demoed everything. It was mind-blowing. VR was about to take over the world. And I was gonna be the one to tell everyone.
</p>



<p>
  Then two things happened:
</p>



<ol>
<li>
  VR didn’t take over the world.
</li>



<li>   I didn’t write a VR post because I fell into a six-year <a href="https://waitbutwhy.com/whatsourproblem" target="_blank" rel="noreferrer noopener">book</a> hole instead. </li>
</ol>



<p>   From the bottom of my book hole, I kept following the story. At Facebook’s 2016 developer conference, Zuckerberg had demoed a new bleeding-edge kind of “standalone, inside out” headset. Up until then, there were two ways to do VR: The first was with a cheap headset, maybe using your phone as a screen, that could do primitive head-movement tracking but had no way to see the environment around you. The second was with external sensors on the walls and a headset that attached to a high-powered PC with a cord. “Standalone” meant the new headset would have the computer inside, with no need for a cable. “Inside out” meant the headset could see the room around you, so you didn’t need external sensors. In 2016, this was just a prototype. Three years later, Facebook launched Oculus Quest.  </p>



<p>   In 2020, standing around during Covid with my dick in my hand like everyone else, I got myself a Quest 2. It was amazing. I loved it. It was my daily post-writing reward activity. I made <a href="https://www.tiltbrush.com/" target="_blank" rel="noreferrer noopener">3D art</a>. I <a href="https://www.meta.com/experiences/2134272053250863/?ranking_trace=0_2134272053250863_SKYLINEWEBQUESTSEARCH_1vKvJ1edv2vQxMeJK" target="_blank" rel="noreferrer noopener">swam with whales</a>. I went on <a href="https://vacationsimulatorgame.com/" target="_blank" rel="noreferrer noopener">cartoon vacations</a>. I exercised by <a href="https://www.meta.com/experiences/2448060205267927/" target="_blank" rel="noreferrer noopener">slashing music</a>. I beat <a href="https://www.meta.com/experiences/2718107161580827/" target="_blank" rel="noreferrer noopener">Trover</a>. </p>



<p>   Then, for some reason, I stopped. I can’t really explain why. I really loved being in the Quest 2. I recently dusted it off to give friends a demo and they were floored, reminding me how great it is. It just didn’t <em>hook </em>me. Maybe it was the solo aspect. I don’t have friends who do VR so there’s no one else to play with. Maybe it’s the friction. It’s minor, but charging the headset, putting it on, and creating a boundary<a href="#footnote-1-10470" id="note-1-10470" rel="footnote">1</a> is still a lot more friction than picking up my phone. Maybe my delight relied more on novelty than I realized. </p>



<p>   It’s not just me. VR blows everyone away when they try it, but it seems to have a hard time hooking people for the long run. After a major wave of hype in the mid-2010s, VR receded into the land of subcultures. </p>



<p>
  And the question is: Is there some fatal flaw to the concept of VR that will always prevent it from achieving mass adoption? Or are we some tipping point away from VR exploding into the stratosphere like the computer and smartphone?
</p>



<h3><strong>Enter Apple</strong>
</h3>



<p>   Everyone remembers where they were when they learned that JFK was shot, a man had landed on the moon, or airplanes had flown into the Twin Towers. I remember where I was when I saw Steve Jobs <a href="https://www.youtube.com/watch?v=VQKMoT-6XSg&amp;ab_channel=ProtectstarInc." target="_blank" rel="noreferrer noopener">unveil the first iPhone</a>. </p>



<p>
  I didn’t always like Apple. My family’s first computer was an Apple 2GS. But then, like many early Apple computer users, I became a PC person. I used an IBM ThinkPad in college and thought Apple people were annoying.
</p>



<p>
  Then Steve Jobs came back to Apple and started Making Apple Great Again. My post-college music composing path forced me to get a 2004 PowerBook G4. After getting used to the interface (<em>why</em> <em>the fuck is there no start button?</em>), I realized that Macs were amazing, and I’ve been an annoying Apple person ever since. But it wasn’t until 2007 that I became a <em>fanboy.</em>
</p>



<p>   In the presentation, when Jobs did the world’s first “swipe to unlock,” the audience made an <a href="https://youtu.be/VQKMoT-6XSg?si=_ruGaoE1NWaLBrjC&amp;t=933" target="_blank" rel="noreferrer noopener">audible gasp</a>. A minute later, he brought up a list of artists in the phone’s “iPod” app and <a href="https://youtu.be/VQKMoT-6XSg?si=WXikR3ntj4j3EXd-&amp;t=972" target="_blank" rel="noreferrer noopener">asked</a>, “Well, how do I scroll through my list of artists? I just take my finger and scroll.” Another audible gasp. It’s weird that something so normal today was jaw-dropping 17 years ago. </p>



<p>   The feeling I had watching that presentation had happened before. I felt it when I was five years old and tried Nintendo for the first time at a friend’s house (<em>I can make something on the TV move by clicking this button??</em>). I felt it in the early ‘90s when my friend showed me how to send an email (<em>You can type something on your computer, hit a button, and it shows up on mine??)</em>.<em> </em>I felt it the first time I test-drove a Tesla (<em>Why is this car accelerating so futuristically?</em>).</p>



<p>
  I’ve learned to see a lot of meaning in these holy shit moments. In most cases, they’ve been followed by an entirely new industry sweeping the world—like the smartphone, video game, internet, and electric car revolutions.
</p>



<p>   In June 2023, Apple <a href="https://www.youtube.com/watch?v=GYkq9Rgoj8E" target="_blank" rel="noreferrer noopener">announced</a> the VR—sorry, <a href="https://www.techradar.com/computing/software/apple-tells-developers-not-to-use-virtual-reality-when-talking-about-vision-pro" target="_blank" rel="noreferrer noopener"><em>spatial computing</em></a><em>—</em>headset that had been long rumored: the Vision Pro.  </p>



<p>
  I watched the presentation, but it wasn’t quite like my experience in 2007. First, I had gotten excited about VR multiple times in the past and ended up disappointed. Second, unlike demoing an iPhone, watching a VR demo on a 2D screen just doesn’t show you what it’s actually like. Oh, also, it was <em>$3,500. </em>I happily shelled out $600 for the first iPhone. But $3,500? For a V1 product that will get way better (and cheaper) in the next few years? When I already have a Meta Quest? Nah. I might be a fanboy but I’m not a chump. It was the obvious grown-up decision to wait it out. Then I ordered one in the first minute after preorders started.
</p>



<p>
  This Monday morning, I went to the Apple Store to put the Vision Pro on my chump face for the first time. The staff member guided me through a demo. And there it was: the holy shit moment.
</p>



<p>
  But it was a holy shit moment with an asterisk. I had experienced full holy shit moments both in 1990 and in 2016 with VR, and these were the notable exceptions to the “holy shit moments are a surefire omen of an industry about to blow up” rule. Was this time different or would history repeat itself?
</p>



<p>
  What I did know was that it was finally time to write a VR post. I wanted to post this week while everyone was hyped up about the Vision Pro. But I didn’t want to write about it before I had used it a <em>lot</em>, so I could experience not only the honeymoon phase but also what it was like to get thoroughly sick of it.
</p>



<p>
  The plan was clear. I went home, told my wife that I would be deeply ignoring her and our baby for the week, and spent twelve hours a day in the headset for four straight days. I’m writing this on Thursday afternoon, having already logged over forty hours. Here are my thoughts.
</p>







<p>
  There are three elements of any VR system: hardware, operating system, and applications. Let’s talk about each.
</p>



<h3><strong>Hardware</strong></h3>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset.jpg"><img decoding="async" width="600" height="337" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset-600x337.jpg" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset-600x337.jpg 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset-750x422.jpg 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset-768x432.jpg 768w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/headset.jpg 980w " sizes="(max-width: 600px) 100vw, 600px"></a></figure>



<p>   Apple Vision Pro (AVP)<a href="#footnote2-1-10470" id="note2-1-10470" rel="footnote2">1</a> is heavy—a decent amount heavier (650 grams) than Meta’s Quest 3 (515 grams). It comes with a fancy band that goes on easily and you tighten with a little knob. It’s awesome. For 12 minutes. Then it started killing my face. With 3,500 regrets, I switched to the other band it comes with, which includes a loop that goes over the top of your head, and thank god for that because it was <em>way </em>better—so good that I am shocked to say that even at the end of a full day wearing it, I didn’t feel a euphoric “ahhh” relief taking it off. At least right now, it seems only a little more uncomfortable than wearing over-ear headphones for long periods of time. This might not apply to everyone, but I have not felt nauseous once while wearing it. </p>



<p>
  That doesn’t mean there’s nothing that sucks about wearing it. The “field of view” isn’t great, meaning there are thick black walls where your peripheral vision is supposed to be, which is a bummer. I can’t imagine it’s great for your eyes. And there’s no way around the fact that you feel like an asshole when other people are in the room. 
</p>



<p>
  There’s an external battery pack that connects to the headset with a cord and typically lives in my pocket. The battery lasts about three hours, but you can plug in the battery to make it last forever, like a computer if the battery only lasted for three hours. (You’re often using it in conjunction with your computer, which makes it a non-issue because you can plug the battery into the computer.)
</p>



<p>   When you put the headset on, it does the AVP version of Face ID: scanning your irises. This is seamless and very futuristic. Then, you see exactly what you saw before putting the headset on. <a href="https://www.youtube.com/watch?v=86Gy035z_KA&amp;ab_channel=MarquesBrownlee" target="_blank" rel="noreferrer noopener">Lots</a> <a href="https://www.youtube.com/watch?v=n7hJlyVDEc8&amp;ab_channel=CleoAbram" target="_blank" rel="noreferrer noopener">of</a> <a href="https://daringfireball.net/2024/01/the_vision_pro" target="_blank" rel="noreferrer noopener">reviewers</a> have marveled over AVP’s “pass-through” capabilities, and the second I put it on, I understood why. While it’s not perfect, it’s <em>almost </em>like you’re wearing a transparent snorkeling mask. The headset is in fact opaque—cameras on the outside transmit the world onto screens on the inside. But the screens are so good and the latency so low that it really seems transparent. Then there’s the much less successful attempt to make it look transparent from the outside as well, using cameras on the inside to broadcast your eyes onto the front of the headset. The goal is that if you’re talking to someone while wearing the headset, it feels to <em>both </em>people like you’re wearing a transparent snorkeling mask. But at least in V1, the eyes don’t show up nearly as well as <a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/applead-eyes.png">advertised</a>. </p>



<p>   The internal screens save energy by doing something clever called “foveated rendering”—i.e. only putting the exact place you’re looking in perfect focus while making the rest of the view lower-res. This is what your actual eyes do, which is why your peripheral vision is blurry. If you watch this <a href="https://x.com/waitbutwhy/status/1755000159698612346?s=20" target="_blank" rel="noreferrer noopener">viewcast</a> I made, you’ll see that most of it is blurry (the sharp part was where I happened to be looking while taking it)—but as the person in the headset, I only ever saw perfect sharpness. </p>



<p>   The way Vision Pro does audio is also cool. There have always been two sound options for me while on my phone or computer: play from the speaker and everyone can hear it or put on headphones and no one can hear it. AVP speakers are somewhere in between. The speakers (which sound great) are small and right above your ears, and while people right next to you can hear what you’re hearing, people in the next room cannot. So in a coffee shop or on an airplane, you still need headphones, but I do a lot of my work in an office in our house with the door open, and it’s been nice to work both without headphones <em>and </em>without bothering anyone in the other room. </p>



<h3><strong>Operating System</strong>
</h3>



<p>
  This was the biggest holy shit of my holy shit moment. Apple is the king of simple intuitive interfaces. Part of what drew those gasps in 2007 was how <em>natural </em>the iPhone’s interface was. You scrolled down by pushing the page up, just like you would in real life. You zoomed by pinching with two fingers. It seemed like magic. AVP’s interface is gaspworthy for the same reason. The main gesture is what I’ve been calling the “eye pinch.”
</p>



<p>   When you press the button at the top of the headset, your apps come up, floating in the room in front of you, looking as real as any other object in the room. They’re fixed in space. You can walk right up to them, and the detail is amazing.<a href="#footnote2-2-10470" id="note2-2-10470" rel="footnote2">2</a>



</p><figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom.png"><img decoding="async" width="835" height="511" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom.png 835w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom-600x367.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom-750x459.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/appsinroom-768x470.png 768w " sizes="(max-width: 835px) 100vw, 835px"></a></figure>



<p>
  Vision Pro’s eye tracking is outrageously good. It knows <em>precisely </em>where you’re looking. So all you do to select an app is look at it and tap your thumb and index finger together. Your hand doesn’t need to move up to do this, just somewhere the headset can see it. Watching the demo, it seemed like this might be annoying to do, but it’s every bit as easy and intuitive as opening an app on a smartphone. 
</p>



<p>   No matter what you’re doing, the eye pinch is the equivalent of touching a finger to a smartphone screen. To scroll, look anywhere in the window, pinch, and move your hand up. To move a window, look at the little bar below the window, pinch, and move it where you want to. To resize the window, look at the window’s corner, pinch, and resize. </p>



<p>   As John Gruber put it in his <a href="https://daringfireball.net/2024/01/the_vision_pro" target="_blank" rel="noreferrer noopener">review</a>: </p>



<p><em>The fundamental interaction model in VisionOS feels like it will be copied by all future VR/AR headsets, in the same way that all desktop computers work like the Mac, and all phones and tablets now work like the iPhone. And when that happens, some will argue that of course they all work that way, because how else could they work? But personal computers didn’t have point-and-click GUIs before the Mac, and phones didn’t have “it’s all just a big touchscreen” interfaces before the iPhone. No other headset today has a “just look at a target, and tap your finger and thumb” interface today. I suspect in a few years they all will.</em>
</p>



<p>   Then there’s the fact that everything you see in front of you is available desktop to work with. On my computer, I’m used to my applications being stacked, and I toggle between them. Or maybe I put a few vertical windows side by side. In AVP, I can put one eight-foot window in front of me, two more on either side of it, and a couple more <em>above </em>them in the sky. Then, if I get up to go to the other room, the windows all stay exactly where they are, waiting for me to come back. If I want to switch work spots, I just hold the headset button and the whole configuration jumps to the new location. This is all way cooler than I’m making it sound, so I made a video to show you how it works:</p>



<figure><p>
<iframe loading="lazy" title="My Work Day in a Vision Pro" width="500" height="281" src="https://www.youtube.com/embed/F3CZD-K04KI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</p></figure>



<p>   One thing you’ll notice in the video is that I routinely spin the digital crown on the headset to slide between being entirely in reality, partially in reality, and entirely in a virtual landscape. This is ridiculously fun to do. And it’s a general reminder that AR and VR<a href="#footnote-2-10470" id="note-2-10470" rel="footnote">2</a> being separate categories is a thing of the past. In the Vision Pro, the Quest 3, and any future headset, you can be 100% in the real world (when there’s nothing on the screen and it seems like you’re wearing a snorkeling mask), you can be mostly in the real world except there’s a virtual <a href="https://youtu.be/-dJu9VyIw64?si=CXyrl8KvUcYQ5uYZ&amp;t=449" target="_blank" rel="noreferrer noopener">game board</a> on your kitchen table or a little virtual butterfly fluttering around. You can be halfway between reality and virtual when, say, <a href="https://youtu.be/-dJu9VyIw64?si=N85diDS1-4EDe5-1&amp;t=476" target="_blank" rel="noreferrer noopener">portals open up</a> in the walls around you during a game. Or you can go full virtual. </p>



<h3><strong>Apps</strong>
</h3>



<p>
  There are many categories of spatial computing apps—productivity, entertainment, social, gaming, creative, fitness—and for most of them today, you’ll need a Meta Quest or some other non-Apple headset. There are a small handful of astounding<em> </em>apps for AVP, but they’re more a sampling of what’s possible than an actual app store.
</p>



<p>
  The most “you can absolutely not do this anywhere but a VR headset” thing I did was their little taster menu of immersive entertainment. Entertainment on a headset runs on a spectrum of immersion. The least immersive is watching a normal movie on a massive screen in a virtual space like the moon or a giant theater. Those movies you missed that everyone says are best seen on the big screen—you can see those on a big screen now. 
</p>



<p>
  Next are movies that are framed in a normal rectangle, but they’re 3D looking—like when we used to wear those stupid paper glasses but much, much better. Sometimes, these surprise you when something comes <em>out </em>of the screen to fly through the air or stand on the floor between you and the screen. The AVR comes with one of these—“Encounter Dinosaurs”—and it’s delightful. 
</p>



<p>
  Finally, there’s full immersion, where the scene entirely surrounds you and you actually feel like you’re there. These are better described as “experiences” than “entertainment.” I saw rhinos up close in person last year. Then, this week, I did one of the Vision Pro experiences that’s an up-close hang with rhinos. These two experiences were <em>very </em>similar. Another experience lets you sit in on an Alicia Keys rehearsal where she sings some songs standing two feet away from you. You can watch her for a while, then look over at what the drummer or keyboardist is doing for a while—just like you would if you were actually there.
</p>



<p>
  Photos and videos are also cool. When you take a panoramic photo, you sweep your phone around in a C-shaped arc—but on a flat phone screen, the result is a flat photo. In AVP, panos are C-shaped, like the photo you actually took. The C wraps around you, which I quickly learned brings the memory back way better than the flat version. You can also turn the headset into a camera and record photos and videos, both of which are immersive. When you later view them in the headset, they’re 3D, putting you right back into the actual scene.
</p>



<p>
  Then there’s the infamous Vision Pro avatars. You get one of these by flipping the headset around and letting it take pictures of you from different angles. Then when you FaceTime someone, your avatar mimics whatever facial expressions you’re making. Here’s mine:
</p>



<figure><img loading="lazy" decoding="async" width="636" height="586" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/timavatar.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/timavatar.png 636w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/timavatar-600x553.png 600w " sizes="(max-width: 636px) 100vw, 636px"></figure>



<p>
  The first person I tested it out on was my wife, who immediately gasped in horror, saying I had “little uncanny valley snake eyes rolling around in my skull,” whatever the fuck that means.
</p>



<p>   The uncanny valley she’s upset about is this:<a href="#footnote2-3-10470" id="note2-3-10470" rel="footnote2">3</a>



</p><figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley.png"><img loading="lazy" decoding="async" width="600" height="324" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-600x324.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-600x324.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-750x405.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-768x415.png 768w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-1536x830.png 1536w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley-239x130.png 239w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/uncannyvalley.png 2001w " sizes="(max-width: 600px) 100vw, 600px"></a></figure>



<p>
  The idea is that we like faces that are somewhat humanlike, and we like faces that are totally humanlike, but we <em>hate </em>faces that are almost-but-not-totally humanlike. Faces that fall just short of being human give us the collective willies. 
</p>



<p>
  Avatars used to suck. Then they got better. Now they’ve gotten so good they’ve plunged into the uncanny valley. This was always gonna happen at some point on the road to perfect avatars and that time is now.
</p>



<p>   To test it out for myself, I FaceTimed my friend <a href="https://www.tiktok.com/@julesterpak" target="_blank" rel="noreferrer noopener">Jules Terpak</a>, who also has a Vision Pro. First I put her across from me at this table while we sat around with each other’s uncanny valley faces for a few minutes.</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office.jpg"><img loading="lazy" decoding="async" width="2000" height="1082" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office.jpg" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office.jpg 2000w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office-600x325.jpg 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office-750x406.jpg 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office-768x415.jpg 768w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office-1536x831.jpg 1536w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Office-239x130.jpg 239w " sizes="(max-width: 2000px) 100vw, 2000px"></a></figure>



<p>
  One very cool thing is that when I moved her window to a different seat at the table, her voice shifted locations to that spot. We concluded that this activity was not actually an upgrade over FaceTime, but that if there were more than two people, it could feel like everyone was sitting around a table together, which would be better than talking to a group FaceTime or Zoom.
</p>



<p>
  Then we shifted locations to Mount Hood.
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood.jpg"><img loading="lazy" decoding="async" width="2000" height="1082" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood.jpg" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood.jpg 2000w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood-600x325.jpg 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood-750x406.jpg 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood-768x415.jpg 768w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood-1536x831.jpg 1536w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-Mt-Hood-239x130.jpg 239w " sizes="(max-width: 2000px) 100vw, 2000px"></a></figure>



<p>
  This felt more like we were actually hanging out somewhere, which is an effect you can’t get on FaceTime. 
</p>



<p>
  When we started going into apps together, it felt even more like we were actually doing an activity together, in a way you normally can’t do without being in person. 
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps.png"><img loading="lazy" decoding="async" width="2130" height="1592" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps.png 2130w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps-600x448.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps-750x561.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps-768x574.png 768w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps-1536x1148.png 1536w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Jules-apps-2048x1531.png 2048w " sizes="(max-width: 2130px) 100vw, 2130px"></a></figure>



<p>   It’s very crude right now, but it’s a primitive version of something we’ll probably all be doing constantly in the 2030s. It’s the next step in a centuries-long human mission to conquer long-distance. First there were letters, then phone calls, then mobile phones<a href="#footnote-3-10470" id="note-3-10470" rel="footnote">3</a> and video calls. The next step is VR hangouts. </p>



<p>   By far the thing I spent the most time doing in the Vision Pro was exactly what I normally do, but the AVP version. When you’re sitting down in front of your computer while wearing the headset, you can open your computer screen as a giant virtual window (which you still control with your normal keyboard and trackpad). Whatever screen you’re used to working on is now much, much bigger. It’s also much more mobile. I don’t usually work on the couch because I prefer my big monitor over my laptop screen. This week, I spent a lot of time working on my couch on a 100-inch monitor. I don’t normally work lying flat in bed because the laptop screen isn’t directly above me. This week I did, putting the screen up on the ceiling. I did some work outside on the porch and some more under a tree. Sometimes I saw the room around me, only with a big screen floating in it. Other times, I went fully immersive, writing on a mountain top, a sand dune, or the moon. And as I mentioned at the beginning of the post, I’m <a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/timincafe-scaled.jpeg">currently</a> using the AVP in a coffee shop, which is officially <a href="https://x.com/waitbutwhy/status/1755672453999710482?s=20" target="_blank" rel="noreferrer noopener">embarrassing</a>. </p>



<p>
  For some odd reason, you can’t open multiple desktops (yet), but you <em>can </em>open some of the things on your desktop as their own apps in separate windows. There’s an AVP iMessage app, so I closed iMessage on my desktop and opened it in an adjacent window. I often remotely cowork with Alicia (WBW’s Manager of Lots of Things), putting her in a little window in the corner of my screen. Now, she’s in her own window. If I’m willing to bite the bullet and switch from Chrome to Safari, I can pull my research and web browsing off the desktop too. The end result is that a single small, immobile computer screen has been replaced with a giant mosaic of screens, for the small price of having a snorkeling mask on my face all day. It kind of feels like you stepped into your computer screen, into the beautiful wallpaper landscape, amongst the windows. Very surreal. I wrote this entire post in the headset and found myself enjoying writing more—and being more focused—than normal.
</p>



<h3><strong>My overall feelings</strong>
</h3>



<p>
  The best way I can describe how I feel about the Vision Pro is a strange combination of utterly thunderstruck and mildly underwhelmed. 
</p>



<p>
  The magical interface, the giant screens, the immersive experiences—they’re just unfathomably cool and awe-inspiring. It feels like a sneak peek at the 2030s.
</p>



<p>
  But after a couple of days, I found myself thinking, “Is that…it?” I had done the small handful of immersive experiences, played some of the small selection of games, looked at a bunch of my panoramic photos, and tried avatar FaceTime—and at the moment, there’s just not that much else to do in the Vision Pro.
</p>



<p>
  The first iPhone left me feeling the same combination of blown away and bored. The phone and I had a torrid honeymoon, but after the novelty of the interface wore off, all it had to offer was the same 16 practical apps.
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/iphone-os1.jpeg"><img loading="lazy" decoding="async" width="259" height="384" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/iphone-os1.jpeg" alt=""></a></figure>



<p>
  There was no app store yet, it dropped calls constantly, and the cellular internet (which you couldn’t use while on a call) was painfully slow. The iPhone wasn’t a world-changing device yet. It was the seed from which a world-changing device would grow.
</p>



<p>
  If you zoom out on a story of technology, you usually see a big exponential curve.
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1.png"><img loading="lazy" decoding="async" width="1187" height="1124" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1.png 1187w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1-600x568.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1-750x710.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-1-768x727.png 768w " sizes="(max-width: 1187px) 100vw, 1187px"></a></figure>



<p>
  But if you look at the curve up close, you see that it’s wavy, made of S-curves.
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2.png"><img loading="lazy" decoding="async" width="1249" height="1124" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2.png 1249w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2-600x540.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2-750x675.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-2-768x691.png 768w " sizes="(max-width: 1249px) 100vw, 1249px"></a></figure>



<p>
  The first iPhone was such a big deal because it launched a new S. Investors had a new place to pour their money. Developers had a new place to pour their efforts. Creators had a new place to pour their talents. As millions of human hours worked on the collective human project, the next five years were a whirlwind of innovation and excitement. Apple’s keynotes became a must-watch for anyone interested in tech, as each jump between the iPhone 1 &gt; 3G &gt; 3GS &gt; 4 &gt; 4S &gt; 5 was a major leap in hardware and software. It was the steep part of the S.
</p>



<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3.png"><img loading="lazy" decoding="async" width="1121" height="986" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3.png 1121w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3-600x528.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3-750x660.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/S-Curves-3-768x676.png 768w " sizes="(max-width: 1121px) 100vw, 1121px"></a></figure>



<p>
  Then, the keynotes got boring. The changes were incremental. Apple stopped innovating and started refining. This coincided with Tim Cook taking over, but it isn’t his fault. The steep part of the S-curve doesn’t go on forever, and companies often reap the biggest rewards in the boring, top part of the S once the industry matures.
</p>



<p>
  Maybe the reason VR has been slow to take off isn’t because there’s something fundamentally wrong with VR. Maybe it’s because, for the last decade, we’ve been working our way through the very early part of the VR S-curve—the slow part where foundational technology is researched and built. My Vision Pro is highly imperfect—overpriced, heavy, slightly glitchy, very limited, creepy-avatared—because that’s exactly what products are like at the bottom of the S. Consumer products aren’t ready for mass adoption during this stage. But it’s the breakthroughs made during these years that set the stage for the explosive exponential phase of the curve.
</p>



<p>   The lesson from past VR hype cycles is to temper expectations. The VR S-Curve explosion may be many years away or never come at all. But the lesson from past Apple launches is don’t bet against Apple, and Apple’s bet is that the Vision Pro could be a seed like the first iPhone—a platform for innovation that kicks a new S-curve into high gear.<a href="#footnote-4-10470" id="note-4-10470" rel="footnote">4</a>  </p>



<h2><strong>Vision Pro, V2 – V10</strong>
</h2>



<p>
  For someone to regularly use a piece of technology, the benefits have to outweigh the costs. Right now, the Vision Pro benefits are probably less than the costs.
</p>


<div>
<figure><a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale.png"><img loading="lazy" decoding="async" width="1250" height="807" src="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale.png" alt="" srcset="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale.png 1250w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale-600x387.png 600w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale-750x484.png 750w , https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/Scale-768x496.png 768w " sizes="(max-width: 1250px) 100vw, 1250px"></a></figure></div>


<p>   I’ve already paid for mine, which removes one of the costs, and it’s <em>still </em>a question to what extent I’ll choose it over my computer in the long run. In that regard, the AVP might currently be more like those <a href="https://www.seeclearfield.com/assets/images/blogs/bag-phone.jpg">first cell phones</a> you had to carry around with a briefcase than the first iPhone. Would you get a cell phone if the only way they came was attached a briefcase? Maybe, but it’s a close call. </p>



<p>
  For VR to achieve mass adoption, the good needs to be better and the bad needs to be less bad. It’s easy to imagine a pathway to both.
</p>



<p>
  The operating system will get better each year. The two-finger pinch is currently the only gesture. More will be added. Eventually, there may be dozens of ways to make gestures with our fingers, each one a different command, like today’s keyboard shortcuts. When you spend ten minutes setting up an elaborate configuration of windows, you’ll be able to save (and share) it.
</p>



<p>
  Avatars will go from uncanny valley to indistinguishable from your normal face. When you go into immersive environments, you can currently see only your hands. In the future, you’ll be able to identify other objects to remain visible (like a coffee mug). The environments around you will expand from the six current options to hundreds, including delightful fantasy worlds, and they’ll be interactive, allowing you to change things like the weather.
</p>



<p>   The hardware will get continually smaller and more comfortable. The resolution and frame rate will become as advanced as the latency. The battery will get way better. So will the look from the outside: to people in the room, the headset will come to look totally transparent. (My personal fantasy: The computer itself becomes detachable, allowing the headset to be a light, sleek, cool-looking visor. The computer and battery snap together into something the size of a smartphone. You’ll be able to snap it to the back of your visor if you don’t want the cord, but most people will prefer the weight to be somewhere other than their heads. The computer/battery rectangle will also have a screen and function as a smartphone for times when you want to do something with the visor off. The visor will fold neatly onto the rectangle to make the whole thing a single compact object.)</p>



<p>
  Finally, the amount of content, applications, and experiences will multiply by 1,000-fold, just like the apps in the app store did from 2008 to today. There will be a wide array of immersive games and entertainment. People will watch sports from one of many vantage points on the field, sideline, stands, or overhead—next to their friends, who will be able to look at each other and talk as well as if they were actually together in person. Pop stars will play in front of 50,000 people in person and 5 million people virtually. Fitness will become fun, interactive, and social. The best teachers and coaches will reach millions of people. Amazing AI teachers could reach billions. Distance will melt away, allowing people to spend high-quality time with their loved ones, no matter where they are. People who couldn’t dream of traveling the world today will get to enjoy vivid experiences anywhere on the globe. Of course, my silly 2024 imagination can’t scratch the surface any more than people in the briefcase phone days could have predicted Uber, TikTok, or Tinder.
</p>



<p>
  Over time, the price will come down, with some companies making headsets dirt cheap the way they have for smartphones today. As the value proposition gets better and better, more people will have them, enhancing the social component and eradicating any stigma. Mass adoption seems like a very real future possibility.
</p>



<p>   I know what many of you are thinking: A world where everyone is in VR headsets (or visors, or glasses, or contact lenses) sounds<em> </em>dystopian and <a href="https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/goldblum.jpg">awful</a>. And granted, this is coming from a guy who thought that world of glazed over people in moving chairs in Wall-E looked like a great place to live—but I’m excited.</p>



<p>
  K can I take this thing off my face now?
</p>



<p>_______</p>


<p><strong>What to read next:</strong></p>
<p><a href="https://waitbutwhy.com/2017/04/neuralink.html" target="_blank" rel="noopener">A post about a technology even more intense than VR</a></p>
<p><a href="https://waitbutwhy.com/2016/03/cryonics.html" target="_blank" rel="noopener">A post about a different technology that’s also even more intense than VR</a></p>
<p><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html" target="_blank" rel="noopener">A post about a third technology that’s even more intense than VR</a></p>


<p>_______</p>







<p>If you like Wait But Why, sign up for our <strong><a data-formkit-toggle="047cbbd566" href="https://newsletter.waitbutwhy.com/join">email list</a></strong> and we’ll send you new posts when they come out.</p>



<p>To support Wait But Why, visit our <strong><a data-type="URL" data-id="https://patreon.com/waitbutwhy" href="https://patreon.com/waitbutwhy" target="_blank" rel="noreferrer noopener">Patreon page</a></strong>.</p>
<!--/#footnotes--><!--/#footnotes2-->				
				
				
			<!-- /entry-content-wrap -->

			
	</div></article></div>]]></description>
        </item>
    </channel>
</rss>