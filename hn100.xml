<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 06 Sep 2024 15:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: Wealthfolio: A Private, Open-Source Investment Tracker (179 pts)]]></title>
            <link>https://wealthfolio.app</link>
            <guid>41465735</guid>
            <pubDate>Fri, 06 Sep 2024 12:56:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://wealthfolio.app">https://wealthfolio.app</a>, See on <a href="https://news.ycombinator.com/item?id=41465735">Hacker News</a></p>
Couldn't get https://wealthfolio.app: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Swift is a more convenient Rust (213 pts)]]></title>
            <link>http://blog.namangoel.com/swift-is-the-more-convenient-rust</link>
            <guid>41464371</guid>
            <pubDate>Fri, 06 Sep 2024 09:10:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://blog.namangoel.com/swift-is-the-more-convenient-rust">http://blog.namangoel.com/swift-is-the-more-convenient-rust</a>, See on <a href="https://news.ycombinator.com/item?id=41464371">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="container">
  <article id="kf5pNR8Jwc8n6dk7svFLSk">
	<time datetime="2023-10-02">October  2, 2023</time>
  <h2>
    <a href="http://blog.namangoel.com/swift-is-the-more-convenient-rust">Swift is a more convenient Rust</a>
  </h2>
	<p>I’ve been learning Rust lately. </p>

<p>Rust is one of the most loved languages out there, is fast, and has an amazing community. Rust invented the concept of ownership as a solution memory management issues without resorting to something slower like Garbage Collection or Reference Counting. But, when you don’t need to be quite as low level, it gives you utilities such as <code>Rc</code>, <code>Arc</code> and <code>Cow</code> to do reference counting and “clone-on-right” in your code. And, when you need to go lower-level still, you can use the <code>unsafe</code> system  and access raw C pointers.</p>

<p>Rust also has a bunch of awesome features from functional languages like tagged enums, match expressions, first class functions and a powerful type system with generics.</p>

<p>Rust has an LLVM-based compiler which lets it compile to native code and WASM.</p>

<p>I’ve also been doing a bit of Swift programming for a couple of years now. And the more I learn Rust, the more I see a reflection of Swift. (I know that Swift stole a lot of ideas from Rust, I’m talking about my own perspective here).</p>

<p>Swift, too, has awesome features from functional languages like tagged enums, match expressions and first-class functions. It too has a very powerful type system with generics.</p>

<p>Swift too gives you complete type-safety without a garbage collector. By default, everything is a value type with “copy-on-write” semantics. But when you need extra speed you can opt into an ownership system and “move” values to avoid copying. And if you need to go even lower level, you can use the unsafe system and access raw C pointers.</p>

<p>Swift has an LLVM-based compiler which lets it compile to native code and WASM.</p>
<h2 id="deja-vu_2">Deja Vu? <a href="#deja-vu_2">#</a></h2>
<p>You’re probably feeling like you just read the same paragraphs twice. This is no accident. Swift is extremely similar to Rust and has most of the same feature-set. But there is a very big difference is <em>perspective</em>. If you consider the default memory model, this will start to make a lot of sense.</p>
<h2 id="rust-is-bottomup-swift-is-topdown_2">Rust is bottom-up, Swift is top-down. <a href="#rust-is-bottomup-swift-is-topdown_2">#</a></h2>
<p>Rust is a low-level systems language at heart, but it gives you the tools to go higher level. Swift starts at a high level and gives you the ability to go low-level.</p>

<p>The most obvious example of this is the memory management model. Swift use value-types by default with <code>copy-on-write</code> semantics. This is the equivalent of using <code>Cow&lt;&gt;</code> for all your values in Rust. But defaults matter. Rust makes it easy to use “moved” and “borrowed” values but requires extra ceremony to use <code>Cow&lt;&gt;</code> values as you need to “unwrap” them <code>.as_mutable()</code> to actually use the value within. Swift makes these Copy-on-Write values easy to use and instead requires extra ceremony to use borrowing and moving instead. Rust is faster by default, Swift is simpler and easier by default.</p>
<h2 id="swift-takes-rust39s-ideas-and-hides-them-in-c_2">Swift takes Rust’s ideas and hides them in C-like syntax. <a href="#swift-takes-rust39s-ideas-and-hides-them-in-c_2">#</a></h2>
<p>Swift’s syntax is a masterclass in taking awesome functional language concepts and hiding them in C-like syntax to trick the developers into accepting them.</p>

<p>Consider <code>match</code> statements. This is what a match statement looks like in Rust:</p>

<pre><code>enum Coin {
    Penny,
    Nickel,
    Dime,
    Quarter,
}

fn value_in_cents(coin: Coin) -&gt; u8 {
    match coin {
        Coin::Penny =&gt; 1,
        Coin::Nickel =&gt; 5,
        Coin::Dime =&gt; 10,
        Coin::Quarter =&gt; 25,
    }
}
</code></pre>

<p>Here’s how that same code would be written in Swift:</p>

<pre><code>enum Coin {
    case penny
    case nickel
    case dime
    case quarter
}
func valueInCents(coin: Coin) -&gt; Int {
    switch coin {
    case .penny: 1
    case .nickel: 5
    case .dime: 10
    case .quarter: 25
    }
}
</code></pre>

<p>Swift doesn’t have a <code>match</code> statement or expression. It has a <code>switch</code> statement that developers are already familiar with. Except this <code>switch</code> statement is actually not a <code>switch</code> statement at all. It’s an expression. It doesn’t “fallthrough”. It does pattern matching. It’s just a <code>match</code> expression with a different name and syntax.</p>

<p>In fact, Swift treats <code>enums</code> as more than <em>just</em> types and lets you put methods directly on it:</p>

<pre><code>enum Coin {
    case penny
    case nickel
    case dime
    case quarter

    func valueInCents() -&gt; Int {
        switch self {
        case .penny: 1
        case .nickel: 5
        case .dime: 10
        case .quarter: 25
        }
    }
}
</code></pre>
<h3 id="optional-types_3">Optional Types <a href="#optional-types_3">#</a></h3>
<p>Rust doesn’t have <code>null</code>, but it does have <code>None</code>. Swift has a <code>nil</code>, but it’s really just a <code>None</code> in hiding. Instead of an <code>Option&lt;T&gt;</code>, Swift let’s you use <code>T?</code>, but the compiler still forces you to check that the value is not <code>nil</code> before you can use it.</p>

<p>You get the same safety with more convenience since you can do this in Swift with an optional type:</p>

<pre><code>let val: T?

if let val {
  // val is now of type `T`.
}
</code></pre>

<p>Also, you’re not forced to wrap every value with a <code>Some(val)</code> before returning it. The Swift compiler takes care of that for you. A <code>T</code> will transparently be converted into a <code>T?</code> when needed.</p>
<h3 id="error-handling_3">Error Handling <a href="#error-handling_3">#</a></h3>
<p>Rust doesn’t have <code>try-catch</code>. Instead it has a <code>Result</code> type which contains the success and error types.</p>

<p>Swift doesn’t have a <code>try-catch</code> either, but it does have <code>do-catch</code> and you have to use <code>try</code> before calling a function that could throw. Again, this is just deception for those developers coming from C-like languages. Swift’s error handling works exactly like Rust’s behind the scenes, but it is hidden in a clever, familiar syntax. </p>

<pre><code>func usesErrorThrowingFunction() throws {
  let x = try thisFnCanThrow()
}

func handlesErrors() {
  do {
    let x = try thisFnCanThrow()
  } catch err {
    // handle the `err` here.
  }
}
</code></pre>

<p>This is very similar to how Rust let’s you use <code>?</code> at the end of statements to automatically forward errors, but you don’t have to wrap your success values in <code>Ok()</code>.</p>
<h2 id="rust39s-compiler-catches-problems-swift39s-co_2">Rust’s compiler catches problems. Swift’s compiler solves some of them <a href="#rust39s-compiler-catches-problems-swift39s-co_2">#</a></h2>
<p>There are many common problems that Rust’s compiler will catch at compile time and even suggest solutions for you. The example that portrays this well is self-referencing enums.</p>

<p>Consider an enum that represents a tree. Since, it is a recursive type, Rust will force you to use something like <code>Box&lt;&gt;</code> for referencing a type within itself.</p>

<pre><code>enum TreeNode&lt;T&gt; {
    Leaf(T),
    Branch(Vec&lt;Box&lt;TreeNode&lt;T&gt;&gt;&gt;),
}
</code></pre>

<p>(You could also us <code>Box&lt;Vec&lt;TreeNode&lt;T&gt;&gt;&gt;</code> instead)</p>

<p>This makes the problem explicit and forces you to deal with it directly. Swift is a little more, <em>automatic</em>.</p>

<pre><code>indirect enum TreeNode&lt;T&gt; {
    case leaf(T)
    case branch([TreeNode&lt;T&gt;])
}
</code></pre>

<p><strong>Note</strong>: that you still have to annotate this <code>enum</code> with the <code>indirect</code> keyword to indicate that it is recursive. But once you’ve done that, Swift’s compiler takes care of the rest. You don’t have to think about <code>Box&lt;&gt;</code> or <code>Rc&lt;&gt;</code>. The values just work normally.</p>
<h2 id="swift-is-less-quotpurequot_2">Swift is less “pure” <a href="#swift-is-less-quotpurequot_2">#</a></h2>
<p>Swift was designed to replace Objective-C and needed to be able to interface with existing code. So, it has made a lot of pragmatic choices that makes it a much less “pure” and “minimalist” language. Swift is a pretty big language compared to Rust and has many more features built-in. However, Swift is designed with “progressive disclosure” in mind which means that just as soon as you think you’ve learned the language a little more of the iceberg pops out of the water.</p>

<p>Here are just <em>some</em> of the language features:</p>

<ul>
<li>Classes / Inhertence</li>
<li>async-await</li>
<li>async-sequences</li>
<li>actors</li>
<li>getters and setters</li>
<li>lazy properties</li>
<li>property wrappers</li>
<li>Result Builders (for building tree-like structures. e.g. HTML / SwiftUI)</li>
</ul>
<h2 id="convenience-has-its-costs_2">Convenience has its costs <a href="#convenience-has-its-costs_2">#</a></h2>
<p>Swift is a far easier language to get started and productive with. The syntax is more familiar and a lot more is done for you automatically. But this really just makes Swift a higher-level language and it comes with the same tradeoffs.</p>

<p>By default, a Rust program is much faster than a Swift program. This is because Rust is fast by default, and <em>lets</em> you be slow, while Swift is easy by default and <em>lets</em> you be fast.</p>

<p>Based on this, I would say both languages have their uses. Rust is better for systems and embedded programming. It’s better for writing compilers and browser engines (Servo) and it’s better for writing entire operating systems.</p>

<p>Swift is better for writing UI and servers and some parts of compilers and operating systems. Over time I expect to see the overlap get bigger.</p>

  <figure id="kudo_kf5pNR8Jwc8n6dk7svFLSk">
    <a href="#kudo">
      
    </a>
    <p>198</p>
    <p>Kudos</p>
  </figure>
  <figure id="kudo_side_kf5pNR8Jwc8n6dk7svFLSk">
    <a href="#kudo">
      
    </a>
    <p>198</p>
    <p>Kudos</p>
  </figure>
</article>

</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Intent to unship: HTTP/2 Push (114 pts)]]></title>
            <link>https://groups.google.com/a/mozilla.org/g/dev-platform/c/vU9hJg343U8/m/4cZsHz7TAQAJ</link>
            <guid>41464334</guid>
            <pubDate>Fri, 06 Sep 2024 09:03:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://groups.google.com/a/mozilla.org/g/dev-platform/c/vU9hJg343U8/m/4cZsHz7TAQAJ">https://groups.google.com/a/mozilla.org/g/dev-platform/c/vU9hJg343U8/m/4cZsHz7TAQAJ</a>, See on <a href="https://news.ycombinator.com/item?id=41464334">Hacker News</a></p>
Couldn't get https://groups.google.com/a/mozilla.org/g/dev-platform/c/vU9hJg343U8/m/4cZsHz7TAQAJ: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Did Sandia use a thermonuclear secondary in a product logo? (375 pts)]]></title>
            <link>https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/</link>
            <guid>41463809</guid>
            <pubDate>Fri, 06 Sep 2024 07:27:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/">https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/</a>, See on <a href="https://news.ycombinator.com/item?id=41463809">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p>I happened to look at a slide deck from Sandia National Laboratories from 2007 that someone had posted on Reddit late last night (you know, as one does, instead of <em>sleeping</em>), and one particular slide jumped out at me:&nbsp;</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635.jpg"><img fetchpriority="high" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635-600x450.jpg" alt="" width="600" height="450" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635-600x450.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635-1024x769.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635-768x576.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635-1536x1153.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sandia-Sierra-Mechanics-Package-Logo-Slide-1716635.jpg 2044w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>It’s a little graphic advertising the different kinds of modeling software that are part of something called the SIERRA framework, as part of a pretty standard “overview” presentation on computer modeling at Sandia that was given at a meeting in Luxembourg.<sup><a href="#footnote_0_7400" id="identifier_0_7400" title="Harold Morgan, “Sandia National Laboratories and Engineering Sciences Overview,” SAND2007-6636P, Presentation to Goodyear/Sandia CRADA Meeting Colmar-Berg, Luxembourg (22 October 2007).">1</a></sup></p>
<p>Did you catch the part that made me stop and audibly say, “<em>uhhhhh</em>“? Look at the lower right:</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail.jpg"><img decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-600x323.jpg" alt="" width="600" height="323" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-600x323.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-1024x552.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-768x414.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-1536x828.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-detail-2048x1104.jpg 2048w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>So, that looks an&nbsp;<em>awful lot</em> like the cutaway of a compact thermonuclear weapon design. I immediately wondered if I couldn’t find a better resolution version of the same graphic, so I went onto OSTI.gov and starting plugging in terms that seemed relevant. Searching for “Sierra” and “Salinas” and restricting to “Conference presentations” turned up a bunch of other instances of it from the 2007-2011 or so timeframe. The one with the highest resolution came from another presentation, from 2008:<sup><a href="#footnote_1_7400" id="identifier_1_7400" title="Heidi Ammerlahn, Richard Griffith, and Paul Nielan, “Modeling and Simulation at Sandia: An Overview,” SAND2008-3315P (23 April 2008).">2</a></sup></p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail.jpg"><img decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail-600x387.jpg" alt="" width="600" height="387" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail-600x387.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail-1024x660.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail-768x495.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail-1536x991.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2008-Sierra-Salinas-warhead-1709042-detail.jpg 1814w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>So this is <em>awfully strange</em>. We’ve got something here that looks like a plausible reentry vehicle for a nuclear warhead. The bits in red, yellow, perhaps fuscia at the “tip” are in the position (and about the right size) to be the arming, fuzing, and firing system. The bits below that — the green, the blue, etc. — look like a thermonuclear warhead. The green part looks like it is meant to represent the location of the “primary,” while the the cylinders-within-cylinders are a classic representation of a thermonuclear “secondary.” One could debate about the exact identity of each color, but it looks a lot like it is meant to represent a radiation case, an interstage medium, a tamper, fusion fuel, and a “sparkplug.” You’ve even got an interesting little “dip” into the central cylinder which looks like a channel to get neutrons into the “sparkplug.”&nbsp;</p>
<p>By comparison, this image from later in the presentation looks a lot more like what one would expect them to release about a reentry vehicle in a public document — just the arming, fuzing, and firing system (the top part, with the detail at right), and then the “warhead” section depicted as a featureless blank:</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23.jpg"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23-600x407.jpg" alt="" width="600" height="407" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23-600x407.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23-1024x694.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23-768x521.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23-1536x1041.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2007-Sierra-Salinas-warhead-1716635-page-23.jpg 1720w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>Even&nbsp;<em>that</em> is a little more revealing than usual, as it gives pretty precise dimensions. So seeing something that looks like it is meant to represent the warhead itself is… <em>pretty</em> <em>surprising</em>!<sup><a href="#footnote_2_7400" id="identifier_2_7400" title="And, just to be very clear about it, that complicated set of machinery in the render is the arming, fuzing, and firing (AFF) system. The basic shapes of such systems have been declassified for a long time. It is the system that causes the warhead firing signal to be sent if the right conditions are met. It is not the warhead itself and is a separate component.">3</a></sup></p>
<p>This isn’t some one-off slip up kind of thing. This particular graphic is present in at least half-a-dozen conference presentations on OSTI.gov, and even some on a few other government websites (like <a href="https://tfaws.nasa.gov/TFAWS11/Proceedings/TFAWS2011-IN-002.pdf">this presentation given to NASA</a>). It’s literally the logo they use for this particular software package. And it’s not some kind of redaction error, <a href="https://blog.nuclearsecrecy.com/2021/05/17/how-not-to-redact-a-warhead/">like the ones I wrote about previously</a>, in which things <em>not dissimilar from the above</em> were very clearly <em>intended</em> to be redacted, but were done so poorly that you could in fact see some aspects of them. This is literally the logo for this particular software framework, and it has been used in lots of presentations (including those done overseas), and is posted all over unclassified, public-facing databases hosted by the federal government.</p>
<p>It took me a little more searching, but I eventually tracked down an isolated version of the image from yet another Sandia presentation:</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model.jpg"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-600x265.jpg" alt="" width="600" height="265" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-600x265.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-1024x452.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-768x339.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-1536x678.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/SAND2007-6128P-Model-2048x904.jpg 2048w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>The slide doesn’t give any clarification as to what we’re looking at, here, other than indicating that it part of modeling work for the purposes of structural dynamics, and is clearly part of a nuclear weapons context.<sup><a href="#footnote_3_7400" id="identifier_3_7400" title="Thomas M. Baca, “1523 General Capability Overview,” SAND2007-6128P (1 September 2007).">4</a></sup></p>
<p>The SIERRA software framework, I gather, is a simulation/modeling toolkit that allowed scientists to basically simulate a relatively “full spectrum” of weapons safety issues. This is Sandia’s bread and butter: making sure that your weapon won’t go off if, say, you drop it, or set it on fire, or let it get hit by lightning. Things which have happened <em>a number of times</em> over the years.<sup><a href="#footnote_4_7400" id="identifier_4_7400" title="Sandia made (and has since put online) a very informative, well-produced, three-part documentary about their work on the technical side of “command and control” of nuclear weapons, titled Always/Never: The Quest for Safety, Control, and Survivability. It’s worth a watch if you haven’t seen it. Separately, one of my favorite bits of weapons jargon is the term “mechanical insult,” which means denting your warhead in some way.">5</a></sup> The “Salinas” package in particular seems to be about modeling mechanical aspects of materials. Which is to say, this demonstration of its “capabilities” is not about showing you that it is modeling how a nuclear weapon would detonate. It is showing you, “look, we can model a lot of different materials — steel, uranium, lithium, etc. — and could probably tell you whether they would crack or strain or shatter or whatever if you, say, dropped this weapon.” That’s my quick gloss on the various presentations, anyway.</p>
<p>To give a sense of how strange this is, here is the only “officially sanctioned” way to represent a multistage thermonuclear weapon, according to US Department of Energy guidance since the 1990s:</p>

<p>Two circles in a box, maybe inside of a reentry vehicle. That’s it. Nothing that gives any actual sense of size, location, materials, <em>physicality</em>. One can compare this with the images of more speculative thermonuclear weapon designs in the public domain for a sense of how&nbsp;<em>limited</em> the official release is compared with what is “believed to be known” about such things:</p>

<p>Incidentally, I submitted a FOIA request on that particular guidance document (TCG-NAS-2) some time back, and the document that I got back was hilariously redacted to the point that even&nbsp;<em>terms</em> like “gun-type” and “implosion” were redacted, much less&nbsp;<em>any and all images</em>, despite that document apparently containing examples of what actually <em>could</em> be said publicly about these things.<sup><a href="#footnote_5_7400" id="identifier_5_7400" title="U.S. Department of Energy, “Joint DOE/DoD Topical Classification Guide for Nuclear Assembly Systems,” TCG-NAS-2 (March 1997), received in 2021 in response to FOIA request HQ-2020-00067-F.">6</a></sup> Which is just to emphasize, it’s not like the DOE is particularly <em>loose</em> about even as vaguely representational an image as is that one — if anything, the err in the other direction.&nbsp;</p>
<p>Why are they so uptight about thermonuclear weapon design “shapes”? The official reason, of course, is because of proliferation concerns. But there’s another reason: even the&nbsp;<em>appearance</em> of giving away “secrets” can generate unwanted publicity and political scandal.&nbsp;</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos.jpg"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos-600x496.jpg" alt="" width="600" height="496" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos-600x496.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos-1024x846.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos-768x634.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos-1536x1269.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1999-Cox-Report-Los-Alamos.jpg 1666w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>In 1999, the <a href="https://www.govinfo.gov/content/pkg/GPO-CRPT-105hrpt851/pdf/GPO-CRPT-105hrpt851.pdf">Cox Committee’s report</a> on Chinese nuclear espionage made some hay out of publicly-available depictions of H-bombs, and featured an entire spread dedicated to the fact that “visitors to Los Alamos National Laboratory are provided a 72-page publication that provides, among other things, a primer on the design of thermonuclear weapons.” It sensationalized that very two-circles-in-a-box image that I showed above, and weaponized it. How dare Los Alamos give that away! Despite it being unclassified. But that’s what I mean by unwanted political scandal — lots of scandals about the release of “secrets” involve non-secrets. (There’s a lot on this sort of thing <a href="https://book.nuclearsecrecy.com/">in my book</a>, of course.)</p>
<p>Which leads us to an interesting puzzle: why would the censors <em>repeatedly </em>allow Sandia to use what appears to be a thermonuclear weapon cutaway as part of a promotional diagram for a software package? There are a few possibilities that come to my mind.</p>
<div id="attachment_7408"><p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-scaled.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-7408" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-600x384.jpg" alt="" width="600" height="384" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-600x384.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-1024x655.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-768x491.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-1536x982.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Sandia-UNCLEARED-badge-2048x1310.jpg 2048w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-7408">I gave a talk at Sandia this summer, and they made me wear this badge (and another one with my face on it, which I wasn’t allowed to photograph or keep) everywhere I went. Presumably so nobody would tell me secrets, but also, perhaps, to indicate my willingness to play Checkers.</p></div>
<p>One is the idea that this is an accident, a leak, an <em>oopsie</em>. I find this unlikely to the point of near impossibility. Not because the classification officers are perfect. But this is <em>so obviously not something you would authorize for release</em> if you thought it was representing something classified. To have approved&nbsp;<em>many</em> presentations with this graphic in it to go out into the world, to be posted on the websites of multiple government agencies… they’re not <em>perfect</em>, but&nbsp;they’re not<em> fools</em>. Again, if anything, they tend to err on the side&nbsp;not releasing&nbsp;<em>enough</em>. So I find it hard to believe that they’d have messed this up, again and again, when it is the most&nbsp;<em>blatant thing in the world</em>. This isn’t some subtle technical thing. Anyone who thinks about weapons information and secrecy is going to know what a cylindrical secondary looks like. I mean, this thing&nbsp;<em>jumps off the page</em> if you are that kind of person. Which I am, of course, but so are redactors. If this were the case, it would be an&nbsp;<em>incredible</em> and <em>repeat </em>failure of the classification system at many points, in the&nbsp;<em>same way</em>, over several years. One can’t say such a thing is&nbsp;<em>impossible</em> but I find that&nbsp;<em>extremely unlikely.</em></p>
<p>Another easily dismissible possibility is that this is some kind of deliberate release of classified information. Again, there is an entire infrastructure devoted to&nbsp;<em>not</em> letting this happen. With peoples’ jobs, security clearances, and personal freedoms on the line. Plus the fact that the people who tend to work in these jobs take for granted that secrecy translates to security. Even actual spies wouldn’t do it this way — they’re not about releasing secrets to the public, they’re about channeling them to the people they are spying to, quiet-like.&nbsp;</p>
<p>So we’re left with much more plausible conclusion that <em>they consider this to be unclassifiable and benign.</em> But <em>why</em> would they think that, given what we know about how sensitive they are to anything that comes even <em>remotely</em> close to representing internal weapon components?&nbsp;</p>
<div id="attachment_7418"><p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object.jpg"><img loading="lazy" decoding="async" aria-describedby="caption-attachment-7418" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object-600x414.jpg" alt="" width="600" height="414" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object-600x414.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object-1024x707.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object-768x530.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object-1536x1060.jpg 1536w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-DOE-Multipurpose-Test-Object.jpg 1828w" sizes="(max-width: 600px) 100vw, 600px"></a></p><p id="caption-attachment-7418">This “multipurpose test object” (taken from the aforementioned TGC-NAS-2 report from 1997) is an example of what I mean by a deliberately “unclassified shape”: something specified by the DOE as being <em>evocative</em> of the kinds of physical shapes and materials that are involved in nuclear weapons designs, but are <em>explicitly</em> indicated as being not actually relevant to weapons design. So this kind of “shape” is something you could use to validate simulation codes on which would probably work with actual weapons materials/designs, but would not actually reveal any weapons materials/designs information other than what has already been declassified.</p></div>
<p>The “obvious” answer, if my above assertions are true, is that it <em>must not actually represent a thermonuclear secondary</em>. What else could it be? It could be some kind of pre-approved “unclassified shape” which is used for diagnostics and model verification, for example. There are other examples of this kind of thing that the labs have used over time. That is entirely a possibility. What would be bizarre about this being the answer is that a) “unclassified shapes” generally don’t look like actual, plausible weapon designs, and this thing looks “close-enough”; b) it still gives off <em>the appearance</em> of a classified shape, which as noted, is dangerous in and of itself from a political standpoint; and c) if the goal is just to show off modeling capabilities in a very superficial way (this is essentially an advertising logo) they surely could have picked a million&nbsp;<em>less provocative</em> (from a classification standpoint) examples.&nbsp;</p>
<p>It’s also possible that it isn’t even meant to be a nuclear weapon at all. Sure, it looks like a reentry vehicle. Yeah… it seems awfully nuke-shaped. But there are other things that can look like nukes but at really meant to be something else. Maybe I’m seeing a “secondary” because I’m primed to see one, by the context? It’s… possible. Neither spheres-within-sphere nor cylinders-within-cylinders are <em>inherently</em> related to nuclear weapons components. But when you place them like that, in a reentry vehicle, in that order… it looks <em>very much</em> like a fusing system, a primary, a secondary… It would be quite surprising to me if it was <em>not</em> meant to be representative of those things, but something totally different. And, again, the original context of that model appears to be very firmly rooted in nuclear weapons development.<sup><a href="#footnote_6_7400" id="identifier_6_7400" title="For example, I don’t know exactly what this is meant to be — an example used in a Los Alamos presentation on computer modeling — but it’s not a nuclear weapon.">7</a></sup></p>
<p>Another possibility is that it is some kind of “deliberate disinformation” or “misinformation.” This is the kind of thing that I think people assume the government labs might do, but in my experience, is pretty unusual and pretty unlikely. In general, you have to remember that the national laboratories are pretty, well, <em>boring</em>, when it comes to classified information. They&nbsp;<em>want</em> to be boring in this respect. They are not doing cloak-and-dagger stuff on the regular. They’re scientists and engineers for the most part. These are not James Bond-wannabes. They don’t parachute behind enemy lines to set up palace coups. They are&nbsp;<em>extremely rule-abiding</em> for the most part. There are lots of social and historical reasons for this (again, <a href="https://book.nuclearsecrecy.com/">my book</a> goes into the historical ones — the anxiety about “nuclear secrets” always made the Atomic Energy Commission and its successor organizations very anxious about being accused of being lax about them).&nbsp;</p>
<p>And beyond the institutional culture aspects, the idea that a bunch of engineers at Sandia are going to be using a software package logo to deliberate leak out misinformation, just&nbsp;<em>waiting</em> for someone to notice it, seems a little unlikely to me on the face of it. I mean, really. What is the “operation” here? Who is meant to be “fooled”? Me? You? The North&nbsp; Koreans? It doesn’t feel very realistic.<sup><a href="#footnote_7_7400" id="identifier_7_7400" title="And nor does taking it one level “deeper”: the idea that they’d put out real information to make us think it must be fake information, because why else would they put it out? This is an amusing idea but, I assure you, is not how bureaucrats think, and we are talking, for better or worse, about bureaucrats here.">8</a></sup></p>
<p>And one can add to the above the fact that, at least historically, the Atomic Energy Commission and its successor organizations have frowned on disinformation and misinformation for other very practical reasons. If you release a lie, you run the risk of someone noticing it is a lie, which can draw more attention to the reality. And even misinformation/inaccuracy <a href="https://blog.nuclearsecrecy.com/2011/12/28/weekly-document-7-is-inaccuracy-classified-1963/">can put “brackets” around the possibilities of truth</a>. The goal of these organizations is to leave a&nbsp;<em>total blank</em> in the areas that they don’t want people to know about, and misinformation/disinformation/inaccuracy is something other than a <em>total blank</em>.&nbsp;</p>
<p>That’s where I’ve ended up, in thinking about what this “means” and what possibly accounts for it. But it’s still bizarre that anyone would allow something that looks so&nbsp;<em>suggestive</em>, even if it is not&nbsp;<em>accurate</em>, to be released as an official product of a national laboratory. It seems like a bad idea, anyway. And yet — I can’t come up with an explanation for this that isn’t one kind of bad idea or another. But I think this is the “most plausible bad idea” of the set.</p>
<p>One last thing. In more recent presentations on the SIERRA Mechanics framework, they changed the diagram somewhat:<sup><a href="#footnote_8_7400" id="identifier_8_7400" title="E.g., Timothy Walsh, Greg Bunting, Andrew Kurzawski, Ellen Le, and Kevin Dowding, “Large-Scale Inverse Capabilities in Sierra Mechanics,” SAND2019-6059C (May 29, 2019).">9</a></sup></p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699.jpg"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699-600x472.jpg" alt="" width="600" height="472" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699-600x472.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699-1024x806.jpg 1024w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699-768x604.jpg 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/2019-Sierra-Mechanics-logo-1640699.jpg 1416w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>The resolution isn’t great, but you can see that the potentially problematic part is much more obscured. But <em>it’s still there</em>, so I don’t think that is really an attempt to draw attention from it, so much as it is an artifact of somewhat careless graphic design. In general, it’s not a <em>great logo</em> by any means — too busy, too complicated, too much information, does not reproduce well at small sizes or low resolutions, etc. — but, as discussed, that is not even close to the most potentially problematic aspect of it!</p>
<p>I saw this and couldn’t resist quickly writing something up about it. That’s all I’ve got. If you’ve got thoughts on it, let me know. And if you haven’t already signed up for it, I am much more active on my other blog, <a href="http://doomsdaymachines.net/"><em>Doomsday Machines</em></a>, as of late!</p>
<hr>
<p>I’ve updated this post a few times since I first put it up this afternoon, but just stumbled across something even more helpful. Here’s an image from a 2014 article about computational science at Sandia that looks awfully similar to the one above:<sup><a href="#footnote_9_7400" id="identifier_9_7400" title="Monte Basgall, “Joint venture,” DEIXIS Magazine (September 2014).">10</a></sup></p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/LargeRvMesh.png"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/LargeRvMesh-600x409.png" alt="" width="600" height="409" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/LargeRvMesh-600x409.png 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/LargeRvMesh-768x524.png 768w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/LargeRvMesh.png 1024w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>Unlike the others, it comes with a caption: “The multiple components of a nuclear weapon body are highlighted in this intentionally simplified mesh. Each part is comprised of numerous subcomponents, fastened together with screws, nuts, bolts, jar-lid-like fittings and more.” Which is just to say, it is pretty clearly saying that this “thing” is meant to be some kind of representation of a nuclear weapon, albeit “intentionally simplified.” Which doesn’t really solve the mystery — if anything, it just highlights why I still find it so odd that this thing got approved for released at all! Not in the sense that it contains “secrets” — but in the sense that it is just not the kind of image the national labs tend to release.&nbsp;</p>
<p>Someone reminded me of something I had seen years ago: the British nuclear program at Aldermaston, when it has published on its own computer modeling in the past, used a sort of “bomb mockup” that looks far more deliberately “fake” than this Sandia one. I offer this up as what I would think is a more&nbsp; “safe”&nbsp; approach than something that looks, even superficially, like a “real” secondary design:</p>
<p><a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Aldermaston-test-object.jpg"><img loading="lazy" decoding="async" src="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Aldermaston-test-object-600x520.jpg" alt="" width="600" height="520" srcset="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Aldermaston-test-object-600x520.jpg 600w, https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Aldermaston-test-object.jpg 621w" sizes="(max-width: 600px) 100vw, 600px"></a></p>
<p>This is called the MACE (Modal Analysis Correlation Exercise) assembly, and was created by the UK Atomic Weapons Research Establishment in the 1990s to serve as a sort of a <a href="https://en.wikipedia.org/wiki/Utah_teapot">Utah Teapot</a> of weapons structural modeling: a benign shape that could be used to test aspects of the code that would nonetheless tell you if the code would work for real weapons assemblies.<sup><a href="#footnote_10_7400" id="identifier_10_7400" title="Some more info on the MACE assembly can be found in this PhD thesis from 2004: Philip Ind, “The Non-Intrusive Model Testing of Delicate and Critical Structures” (Imperial College of Science, Technology, and Medicine, University of London, 2004). The screen cap image comes from an in-house AWRE publication (Discovery) from 2000.">11</a></sup></p>
<p>Anyway, I’m just surprised the DOE would release&nbsp;<em>any</em> image that gave really&nbsp;<em>any</em> implied graphical structure of a thermonuclear secondary, even if it is clearly schematic and meant to be only somewhat representative. It’s more than they usually allow!</p>
<ol><li id="footnote_0_7400">Harold Morgan, “<a href="https://www.osti.gov/biblio/1716635">Sandia National Laboratories and Engineering Sciences Overview</a>,” SAND2007-6636P, Presentation to Goodyear/Sandia CRADA Meeting Colmar-Berg, Luxembourg (22 October 2007).<span> [<a href="#identifier_0_7400">↩</a>]</span></li><li id="footnote_1_7400">Heidi Ammerlahn, Richard Griffith, and Paul Nielan, “<a href="https://www.osti.gov/biblio/1709042">Modeling and Simulation at Sandia: An Overview</a>,” SAND2008-3315P (23 April 2008).<span> [<a href="#identifier_1_7400">↩</a>]</span></li><li id="footnote_2_7400">And, just to be very clear about it, that complicated set of machinery in the render is the arming, fuzing, and firing (AFF) system. The basic shapes of such systems have been declassified for a long time. It is the system that causes the warhead firing signal to be sent if the right conditions are met. It is not the warhead itself and is a separate component.<span> [<a href="#identifier_2_7400">↩</a>]</span></li><li id="footnote_3_7400">Thomas M. Baca, “<a href="https://www.osti.gov/biblio/1719007">1523 General Capability Overview</a>,” SAND2007-6128P (1 September 2007).<span> [<a href="#identifier_3_7400">↩</a>]</span></li><li id="footnote_4_7400">Sandia made (and has since put online) a very informative, well-produced, three-part documentary about their work on the technical side of “command and control” of nuclear weapons, titled <a href="https://www.youtube.com/watch?v=DQEB3LJ5psk"><em>Always/Never: The Quest for Safety, Control, and Survivability</em></a>. It’s worth a watch if you haven’t seen it. Separately, one of my favorite bits of weapons jargon is the term “mechanical insult,” which means denting your warhead in some way.<span> [<a href="#identifier_4_7400">↩</a>]</span></li><li id="footnote_5_7400">U.S. Department of Energy, “<a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/1997-TCG-NAS-2-Joint-DOE-DOD-Topical-Classi%EF%AC%81cation-Classification-Guide-for-Nuclear-Assembly-Systems-FOIA-HQ-2020-00067-F.pdf">Joint DOE/DoD Topical Classification Guide for Nuclear Assembly Systems</a>,” TCG-NAS-2 (March 1997), received in 2021 in response to FOIA request HQ-2020-00067-F.<span> [<a href="#identifier_5_7400">↩</a>]</span></li><li id="footnote_6_7400">For example, I don’t know exactly what <a href="https://blog.nuclearsecrecy.com/wp-content/uploads/2024/09/Los-Alamos-odd-device.jpg">this</a> is meant to be — an example used in <a href="https://www.osti.gov/biblio/1983836">a Los Alamos presentation on computer modeling</a> — but it’s not a nuclear weapon.<span> [<a href="#identifier_6_7400">↩</a>]</span></li><li id="footnote_7_7400">And nor does taking it one level “deeper”: the idea that they’d put out <em>real</em> information to make us think it must be <em>fake</em> information, because why else would they put it out? This is an amusing idea but, I assure you, is not how bureaucrats think, and we are talking, for better or worse, about bureaucrats here.<span> [<a href="#identifier_7_7400">↩</a>]</span></li><li id="footnote_8_7400">E.g., <span>Timothy Walsh, Greg Bunting, Andrew Kurzawski, Ellen Le, and Kevin Dowding, “<a href="https://www.osti.gov/servlets/purl/1640699">Large-Scale Inverse Capabilities in Sierra Mechanics</a>,” SAND2019-6059C (May 29, 2019).<span> [<a href="#identifier_8_7400">↩</a>]</span></span></li><li id="footnote_9_7400">Monte Basgall, “<a href="https://deixismagazine.org/2014/09/joint-venture/">Joint venture</a>,” <em>DEIXIS Magazine</em> (September 2014).<span> [<a href="#identifier_9_7400">↩</a>]</span></li><li id="footnote_10_7400">Some more info on the MACE assembly can be found in this PhD thesis from 2004: Philip Ind, “<a href="https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dynamics/7289238.PDF">The Non-Intrusive Model Testing of Delicate and Critical Structures</a>” (Imperial College of Science, Technology, and Medicine, University of London, 2004). The screen cap image comes from <a href="http://web.archive.org/web/20060314131511/http://www.awe.co.uk/Images/joint_test_prog_tcm6-1953.pdf">an in-house AWRE publication (<em>Discovery</em>) from 2000</a>.<span> [<a href="#identifier_10_7400">↩</a>]</span></li></ol>
				

								<p>Tags: <a href="https://blog.nuclearsecrecy.com/tag/2000s/" rel="tag">2000s</a>, <a href="https://blog.nuclearsecrecy.com/tag/2010s/" rel="tag">2010s</a>, <a href="https://blog.nuclearsecrecy.com/tag/bad-ideas/" rel="tag">Bad ideas</a>, <a href="https://blog.nuclearsecrecy.com/tag/bomb-design/" rel="tag">Bomb design</a>, <a href="https://blog.nuclearsecrecy.com/tag/graphic-design/" rel="tag">Graphic design</a>, <a href="https://blog.nuclearsecrecy.com/tag/hydrogen-bomb/" rel="tag">Hydrogen bomb</a>, <a href="https://blog.nuclearsecrecy.com/tag/sandia-national-laboratories/" rel="tag">Sandia National Laboratories</a></p>
				<p>
					<small>
						This entry was posted
												on Wednesday, September 4th, 2024 at 1:21 pm						and is filed under <a href="https://blog.nuclearsecrecy.com/category/redactions/" rel="category tag">Redactions</a>.
						You can follow any responses to this entry through the <a href="https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/feed/">RSS 2.0</a> feed.

													You can <a href="#respond">leave a response</a>, or <a href="https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/trackback/" rel="trackback">trackback</a> from your own site.

						 <p>
<b>Citation:</b> Alex Wellerstein, "Did Sandia use a thermonuclear secondary in a product logo?," <em>Restricted Data: A Nuclear History Blog</em>, September 4, 2024, accessed September 6, 2024, <a href="https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/">https://blog.nuclearsecrecy.com/2024/09/04/did-sandia-use-a-thermonuclear-secondary-in-a-product-logo/</a>.

					</p></small>
				</p>

		<br>


			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exercise has 2M users but no money in the bank (114 pts)]]></title>
            <link>https://exercism.org/blog/september-2024-restructure</link>
            <guid>41463734</guid>
            <pubDate>Fri, 06 Sep 2024 07:16:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://exercism.org/blog/september-2024-restructure">https://exercism.org/blog/september-2024-restructure</a>, See on <a href="https://news.ycombinator.com/item?id=41463734">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>Hi everyone!</p>
<p>Last week we hit the huge milestone of <strong>two million users</strong>.
Within a few hours, we also hit <strong>45 million exercise submissions</strong>.</p>
<p>A day later, I paid the <strong>final payroll</strong> for me, Erik and Aron, and our bank account reduced down to the point we can't afford to pay another.</p>
<p>I think this sums up Exercism's story pretty well.
Over <strong>1,200 people per day sign up</strong> to Exercism.
Tens of thousands solve exercises each day.
But we don't have enough money to continue to work on the platform.</p>
<p>We've tried a lot of things to change that.
We've spoken to hundreds of funders and companies, but Exercism isn't the right fit for their support.
It doesn't fit a niche that makes sense for them.
By serving people everywhere, it seems we don't serve a narrow enough demographic that we align to funders' often narrow missions!</p>
<p>The one area we have had some promising success is in advertising on the site.
But the effort it takes to find advertisers and manage them, and my general desire not to flood Exercism with adverts, has meant that I feel this isn't a very sustainable strategy.</p>
<p>I think it's fair to say that at this stage <strong>I've lost faith in the nonprofit business model</strong> working in a way that allows Exercism to reach any of its potential.
Keeping something free for everyone relies on either the user being the product, or on significant donations, and without either, it's very hard to grow.</p>
<h2 id="h-erik-oss-restructure">Erik + OSS Restructure</h2>
<p>Probably the hardest thing about the situation right now, is that <strong>we can no longer afford to pay Erik</strong>, so he's leaving as an employee at the end of this week.</p>
<p>Erik has been an absolute critical part of Exercism's growth and success over the last few years.
He's also been a wonderful colleague and friend, and I'll really miss working alongside him.
It goes without saying that I'm incredibly grateful to Erik for all his hard work and support.
And I know many of you will feel the same (if so, please reach out and tell him!)</p>
<p>Erik's a die-hard Exercism fan, and he's going to continue as a senior maintainer of a few tracks, and he'll hold onto his super-admin privileges, but the plethora of hidden (and often a bit boring) things that he does day-to-day need to get spread across the organisation.</p>
<p>The one key thing we're enforcing is that <strong>every PR in a live Exercism repository should get a review before it can be merged</strong> (with only one exception outlined below).
This has generally been the case anyway for a long time, but there are places where it's fallen through the cracks, so we've now scripted things to ensure this is always the case.</p>
<p>To do this, we've come up with a new classification system for repos, and specific rules for each type:</p>
<ul>
<li>
<code>maintained</code>: A repo that has multiple maintainers. All PRs require reviews from a track maintainer.</li>
<li>
<code>maintained-solitary</code>: A repo that has one maintainer. A new cross-track-maintainers team will get pinged to review all PRs.</li>
<li>
<code>unmaintained</code>: A repo that has no maintainers. The cross-track-maintainers team will get pinged here too.</li>
<li>
<code>maintained-autonomous</code>: A repo where all maintainers are also in the cross-track-maintainer team. This is the exception, where they can merge their own PRs.</li>
<li>
<code>wip-track</code>: An unlaunched track. As it's not "live" yet, it doesn't have restrictions.</li>
</ul>
<p>We've created two new GitHub teams that enforce this.</p>
<ul>
<li>
<code>@exercism/guardians</code>: A team to check the safety of PRs to tooling repositories (test runners, analyzers, etc). Made up of a few longstanding polyglots.</li>
<li>
<code>@exercism/cross-track-maintainers</code>: A new team made up of longstanding polyglots who are active on the site on a day-to-day basis, and who have the same level of reviewing-pedanticism that I do (ie they're not more strict or more flexible than me). This is important, as I want a consistent, responsive experience from this team.</li>
</ul>
<p>Both teams are invite-only.
I'll review them sporadically.</p>
<p>We've also invited new maintainers and "pruned" a lot of inactive maintainers as part of this.
To those people who have been removed - thank you for all you've done, and please know you're very welcome back if you find the time/desire to contribute in earnest to Exercism again!</p>
<h2 id="h-so-what-s-next">So what's next?</h2>
<p>So this has all been a bit gloom and doom so far.
Things don't always go as you hope in life, but you have to make lemonade from the lemons!</p>
<p>Right now, we have about 800 monthly donors and about $7,500 in monthly donations.
That <strong>covers our server costs</strong> pretty much exactly.
So if you're donating right now, thank you.
You are literally keeping our servers on.
Our donor base is generally quite stable, so I'm not too worried about Exercism's existential prospects.</p>
<p>(It would be really good to build a bit of a financial buffer, so if you can afford it, please consider making a <a href="https://exercism.org/donate">recurring or one-off donation</a> 💙)</p>
<p>We also have an <strong>amazing community, maintainer team, and group of mentors</strong> who keep adding new exercises (and tracks!), helping students, and numerous other things.
Exercism has probably never been healthier as an organisation.
It's <strong>growing faster than ever</strong>, more people are using it than ever, and I think the product and educational experience is better than ever.
So I'm still deeply dedicated to growing and nurturing Exercism.</p>
<p>For the last few months, I've been working on a new educational product teaching coding fundamentals that I'm going to launch in 2025.
96% of people who try to learn coding give up - which I find unacceptable, so I'm aiming to change that.
My plan is to give beginners a rock-solid base, then funnel them into Exercism.
I'm creating a new for-profit company for the new company, and going to use proceeds from that to keep Exercism growing.
I've raised a little investment for this, which means I can continue to pay Aron's salary, so he'll be staying around, working on that with me, and tweaking Exercism as needed.</p>
<p>I'm also planning (probably 90% certain) of running <strong>a dedicated learn-to-code course</strong> from Jan-March 2025, where I can test out some of what we've been building, and I can get my hands dirty working with the students who existing platforms don't serve.
So keep out for an announcement about this soon.</p>
<p>I'm exploring launching <strong>a basic version of Exercism Teams</strong>, as a way of making some extra revenue.
There's <a href="https://forum.exercism.org/t/exercism-teams-coming-soon/12667" target="_blank" rel="noreferrer">a forum post</a> here where you can leave thoughts.
I'd really appreciate any you have!</p>
<p>But first, <strong>I need a breather!</strong>
I'll be taking the next two weeks off, unplugging and recovering a little.</p>
<p>Thanks for all your support - emotional and financial.
Exercism's community is phenomenal and <strong>I'm deeply grateful</strong> to everyone involved in the project.</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The expected value of the game is positive regardless of Ballmer’s strategy (182 pts)]]></title>
            <link>https://gukov.dev/puzzles/math/2024/09/05/steve-ballmer-was-wrong.html</link>
            <guid>41463330</guid>
            <pubDate>Fri, 06 Sep 2024 06:08:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gukov.dev/puzzles/math/2024/09/05/steve-ballmer-was-wrong.html">https://gukov.dev/puzzles/math/2024/09/05/steve-ballmer-was-wrong.html</a>, See on <a href="https://news.ycombinator.com/item?id=41463330">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>A few days ago John Graham-Cumming posted about <a href="https://blog.jgc.org/2024/09/steve-ballmers-binary-search-interview.html">“Steve Ballmer’s incorrect binary search interview question”</a> which <a href="https://news.ycombinator.com/item?id=41434637">drew a lot of attention</a> on Hacker News. The Ballmer’s favorite brain teaser goes like this:</p>

<blockquote>
  <p><em>I’m thinking of a number between 1 and 100. You can guess, after each guess I’ll tell you whether you’re high or low. You get it the first guess - I’ll give you five bucks. Four bucks, three, two, one, zero, you pay me a buck, you pay me two, you pay me three</em>.</p>

  <p><em>Should you accept to play this game?</em></p>
</blockquote>

<p>Steve Ballmer argues in <a href="https://youtu.be/svCYbkS0Sjk?si=89kJu8Ukkr9QpkFX&amp;t=34">this YouTube interview</a> that there are two reasons why you should not play this game:</p>

<ol>
  <li>There are many numbers that would result in a loss, making the expected value negative even if he randomly picks numbers between 1 and 100.</li>
  <li>He can strategically pick numbers that would require the longest time for you to find using binary search.</li>
</ol>

<p>However, John counters Ballmer’s first point in his <a href="https://blog.jgc.org/2024/09/steve-ballmers-binary-search-interview.html">blog post</a> by demonstrating that if Ballmer selects the number randomly, the expected value of the game is actually positive: <strong>$0.20</strong>.</p>

<p>I will refute the second point and demonstrate that the expected value of the game is positive regardless of Ballmer’s strategy.</p>

<h2 id="how-can-ballmer-pick-numbers-adversarially">How can Ballmer pick numbers adversarially?</h2>

<p>Let’s assume you always employ the binary search strategy to find the number. Out of the 100 numbers, there are 37 that would require you to ask 6 questions to make a guess.</p>

<p>If Ballmer is aware of your strategy, he can always select one of these “losing” numbers, resulting in a loss for you in every game.</p>

<p>This holds true for any “fixed” search pattern. There will always be at least 37 numbers that would result in a loss, and Ballmer can choose one of them.</p>

<h2 id="how-can-you-counter">How can you counter?</h2>

<p>Here we’re getting into the <strong>game theory</strong> territory.</p>

<p>Instead of using a single fixed search pattern, you can prepare a set of different search patterns. Then at the beginning of the game, draw one of these patterns with some probability and stick to it during the game.</p>

<blockquote>
  <p><a href="https://en.wikipedia.org/wiki/Strategy_(game_theory)#Pure_and_mixed_strategies">In game theory</a>, you call it a <strong>mixed strategy</strong> based on the <strong>strategy set</strong> of multiple <strong>pure strategies</strong>.</p>
</blockquote>

<p>Because the same number could be “winning” for one search pattern and “losing” for another, such a mixed strategy could “even out” the expected winnings for each number.</p>

<p>Potentially, a mixed strategy could even make every number “winning”, i.e. have a positive expected value of the win for every number.</p>

<p>And this is exactly what we’re looking for!</p>

<h2 id="how-to-find-the-winning-mixed-strategy">How to find the winning mixed strategy?</h2>

<blockquote>
  <p>Note: We are looking for <em>any</em> winning strategy, not the <em>best</em> winning strategy that has the maximum expected value in the worst case, i.e., the <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a>. If you are curious about the Nash equilibrium, Arthur O’Dwyer explored it for the game <a href="https://quuxplusone.github.io/blog/2024/09/04/the-game-is-flawed/">up to 5 numbers</a>.</p>
</blockquote>

<p>Finding the mixed strategy that wins on every number can be viewed as a mathematical optimization problem.</p>

<p>Every strategy can be described as a “win” vector $V = (v_1, .., v_{100})$, where $v_k$ is the expected win if Ballmer picks the number $k$. For example, the binary search could correspond to a vector with $v_{50} = 5$, $v_{25} = 4$, and $v_{0} = -1$.</p>

<p>Suppose we have a set of pure strategies ${V_1, V_2, …, V_n}$, and our mixed strategy chooses the strategy $V_k$ with probability $p_k$.</p>

<p>Then the corresponding “win” vector for the mixed strategy is just a linear combination: $V_{mixed}=\sum_{i=1}^{n}{p_iV_i}$.</p>

<p>In this interpretation, finding the winning strategy means finding some linear combination of the given vectors with two constraints:</p>

<ul>
  <li>Each element of the linear combination is positive (the strategy wins money, on average, for each number).</li>
  <li>The coefficients of this linear combination are non-negative (as they correspond to probabilities).</li>
</ul>

<p>This is a typical <a href="https://en.wikipedia.org/wiki/Linear_programming">linear programming</a> problem, and scipy <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html">has an efficient solver</a> for it.</p>

<p>To find a mixed strategy, I thought of a set of pure strategies (various binary searches), <a href="https://github.com/gukoff/ballmer_puzzle/blob/main/main.py#L98">fed it</a> into <code>scipy.linprog()</code>, and voilà - the solver came up with a winning strategy!</p>

<h2 id="example-winning-strategy">Example winning strategy</h2>

<blockquote>
  <p>Full code is at <a href="https://github.com/gukoff/ballmer_puzzle#winning-strategy">gukoff/ballmer_puzzle</a>.</p>
</blockquote>

<ul>
  <li>Average win if Ballmer chooses randomly: <strong>$0.12</strong></li>
  <li>Worst win if Ballmer chooses adversarially: <strong>$0.07</strong></li>
</ul>

<p>The resulting mixed strategy goes like this:</p>

<div><pre><code>- With probability 0.2120%: Binary search, first guess is 29. On each step, guess the middle element in the interval, in case of tie guess the left one.
- With probability 0.0450%: Binary search, first guess is 33. On each step, guess the middle element in the interval, in case of tie guess the left one.
- With probability 0.9843%: Binary search, first guess is 26. On each step, guess the middle element in the interval, in case of tie guess the right one.
- With probability 0.6910%: Binary search, first guess is 28. On each step, guess the middle element in the interval, in case of tie guess the right one.
- With probability 0.9686%: Binary search, first guess is 1. On each step, guess the rightmost element in the interval that won't increase the worst-case complexity.
- With probability 0.7134%: Binary search, first guess is 18. On each step, guess the rightmost element in the interval that won't increase the worst-case complexity.
- With probability 2.7288%: Binary search, first guess is 46. On each step, guess the rightmost element in the interval that won't increase the worst-case complexity.
- With probability 2.6411%: Binary search, first guess is 36. On each step, guess the leftmost element in the interval that won't increase the worst-case complexity.
- With probability 5.2209%: Binary search, first guess is 40. On each step, guess the leftmost element in the interval that won't increase the worst-case complexity.

...
</code></pre></div>

<p>The complete strategy consists of 60 lines, which I have omitted for brevity. If you are interested, you can <a href="https://github.com/gukoff/ballmer_puzzle?tab=readme-ov-file#winning-strategy">view it on GitHub</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>If you find winning (on average) 7 cents per game worth your time, then you should definitely play this game with Steve Ballmer the next time he offers.</p>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What happens when you touch a pickle to an AM radio tower (228 pts)]]></title>
            <link>https://www.jeffgeerling.com/blog/2024/what-happens-when-you-touch-pickle-am-radio-tower</link>
            <guid>41462574</guid>
            <pubDate>Fri, 06 Sep 2024 03:16:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.jeffgeerling.com/blog/2024/what-happens-when-you-touch-pickle-am-radio-tower">https://www.jeffgeerling.com/blog/2024/what-happens-when-you-touch-pickle-am-radio-tower</a>, See on <a href="https://news.ycombinator.com/item?id=41462574">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>A few months ago, our <a href="https://www.instagram.com/insights/media/3333127873529510767/">AM radio hot dog experiment</a> went mildly viral. That was a result of me asking my Dad 'what would happen if you ground a hot dog to one of your AM radio towers?' He didn't know, so one night on the way to my son's volleyball practice, we tested it. And it was <em>awesome</em>.</p>

<p>There's a video and some pictures in my <a href="https://www.jeffgeerling.com/blog/2024/talking-hot-dog-gives-new-meaning-ham-radio">hot dog radio blog post</a> from back in March.</p>

<p>
<video preload="" autoplay="" loop="" playsinline="" muted="">
  <source src="https://www.jeffgeerling.com/sites/default/files/bratwurst-am-tower.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</p>

<p>Fast forward a few months and one Open Sauce later, and Jay from <a href="https://www.youtube.com/@PlasmaChannel">Plasma Channel</a> visited us in St. Charles, MO, for round two—where my Dad and I were prepared to measure (almost) everything: SWR, RF forward power, SDR on site, AM field intensity 25km (16mi) away, meat thermals, and—courtesy of Jay—some taste testing!</p>

<p>Our full experience is documented in today's Geerling Engineering video:</p>

<div>
<p><iframe src="https://www.youtube.com/embed/wzDEIBpbLRk" frameborder="0" allowfullscreen=""></iframe></p>
</div>

<p>But I'll also summarize all our test results in this blog post, for easier reference:</p>

<h2>Test Setup and Safety Precautions</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-hotdog.jpg" alt="RF Safety - Hot Dog AM radio tower"></p>

<p><strong>DO NOT ATTEMPT</strong>. Don't mess with towers, especially AM broadcast towers.</p>

<p>We consulted with an experienced broadcast antenna designer before any testing. Using <a href="https://transition.fcc.gov/Bureaus/Engineering_Technology/Documents/bulletins/oet65/oet65.pdf">conservative FCC guidelines</a> (also see <a href="https://transition.fcc.gov/oet/info/documents/bulletins/oet65/oet65a.pdf">Supplement A</a>), we determined a safe exposure distance <em>for the tower and transmitter at this single tower site</em>. Every tower, station, and frequency will be different, so again, <em>do not attempt</em> what we did. It was for educational purposes only. And science.</p>

<h2>Experiment 1 - Hot Dog</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-hotdog-measurements.jpg" alt="RF Safety - Hot Dog device measurements"></p>

<p><strong>Hypothesis</strong>: Well, we already knew the hot dog would make some noise. This time, we brought all our instrumentation, and measured how it affected the signal.</p>

<p><strong>Observations</strong>: The hot dog did, indeed, produce copious noise through plasma-air interaction. It did an excellent job demodulating the AM signal into audible sound, and the entire hot dog was heated to around 80°C—which is luckily a safe internal temperature for eating.</p>

<p>What may be <em>less</em> safe is eating whatever charred remains were left on the end of the hot dog. The transmitter RF output (as measured on the control panel) rose to 14 kW (from a nominal 12) briefly, before the internal <a href="https://electronics.stackexchange.com/a/2936/9952">foldback protection</a> cut power to around 6 kW (until we stopped shorting the hot dog to ground, after which the Nautel XR12 raised power back to 12 kW).</p>

<h2>Experiment 2 - Pickle (Gherkin)</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-pickle-plasma-shockwave.jpg" alt="RF Safety - Plasma shockwave coming off gherkin pickle on AM tower"></p>

<p><strong>Hypothesis</strong>: Some commenters believed the salt content of a fresh gherkin would cause the arc to change from orange-ish (hot dog) to green-ish (pickle). We speculated it may turn more reddish...</p>

<p><strong>Observations</strong>: The gherkin was quite a pickle. While testing, there was a loud spark, then the transmitter quickly got <em>very</em> quiet. We originally thought it cauterized itself and caused less conduction, but were very wrong. As it turns out, the pickle was an <em>excellent</em> conductor, with much lower internal resistance than any of the meats we tested. The salt-saturated watery interior provided an excellent path from tower to ground!</p>

<p>This was the only object we tested which caused the transmitter to completely disable its RF output, if only momentarily. The end of the pickle glowed orange, and we also observed a plasma <em>shockwave</em> (pictured above) in a few frames. Would love to see this with a high-speed camera!</p>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-jay-pickle-eat.jpg" alt="Jay from Plasma Channel Taste-Tests a Pickle"></p>

<p>In the category of 'what-were-you-thinking' comes Jay from the <a href="https://www.youtube.com/@PlasmaChannel">Plasma Channel</a>. He decided to taste-test the pickle, and immediately spat it out as he said it tasted strongly of copper. He speculates the taste may have resulted from the electrolysis of the copper we used to ground the pickle. We speculate he may be a little crazy—our kind of crazy. Jay has his own video on the experience here: <a href="https://www.youtube.com/watch?v=NowhPAMDOTo">Creating A Plasma Shockwave Using Wireless Energy</a>.</p>

<h2>Experiment 3 - Bratwurst</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-bratwurst.jpg" alt="RF Safety - Bratwurst on AM radio tower"></p>

<p><strong>Hypothesis</strong>: Many commenters speculated a bratwurst would translate the normally-English radio signal into German. We had our doubts.</p>

<p><strong>Observations</strong>: The bratwurst did indeed translate the signal into German! Or, well... it broadcast the german phrases the <a href="https://adamwrightstl.com/about/">on-air talent</a> spoke during his morning show.</p>

<p>The bratwurst was the floppiest of the tested meats, and had a rather phallic look as it was hanging off the end of our probe. The 'droop' resulted in a large contact patch, which produced more smoke than the hot dog, but not any perceptible difference in sound. The transmitter reacted about the same as it did with the hot dog.</p>

<h2>Experiment 4 - Vegan Hot Dog</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-vegan-hot-dog.jpg" alt="RF Safety - vegan hot dot"></p>

<p><strong>Hypothesis</strong>: We expected the vegan hot dog to perform similarly to the all-beef hot dog, though were wondering if it could surprise us. <a href="https://lightlife.com/product/jumbo-smart-dogs/">Soybeans and sugar</a> may react differently than beef or turkey!</p>

<p><strong>Observations</strong>: The sound was a bit louder, the vegan hot dog was a bit cooler, and the end burned off a bit more quickly. The more disgusting bit was the end near the copper insertion point—some white substance oozed out the backside and did not look very appetizing (see photo above).</p>

<p>Jay taste-tested the cooked vegan hot dog (this time biting off a section from the middle, not a part that came in contact with the copper) and described the taste as 'pretty good'.</p>

<h2>Experiment 5 - Corn Dog</h2>

<p>
<video preload="" autoplay="" loop="" playsinline="" muted="">
  <source src="https://www.jeffgeerling.com/sites/default/files/corn-dog-am-tower-shrunk.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</p>

<p><strong>Hypothesis</strong>: We speculated the corn layer surrounding the hot dog may provide enough insulation to prevent a serious reaction with the tower. This item also was a physical manifestation of our sometimes 'corny' dad jokes.</p>

<p><strong>Observations</strong>: Copious amounts of smoke, followed by large bursts of flame. Honestly, this was the most surprising of the bunch. The smoke seemed to follow the leg of the tower at a fairly large distance from the point of contact!</p>

<p>Unlike the other meats, the smell of the burnt corn dog was pleasant–almost sugary. The sound was not that loud, and apparently the corn layer provided enough resistance the transmitter's foldback protection circuit never activated; the transmitter continued at 12 kW power throughout our corn dog test.</p>

<p>Nobody was brave enough to taste-test the corn dog.</p>

<h2>Experiment 6 - Breakfast Sausage</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-breakfast-sausage.jpg" alt="RF Safety - AM breakfast sausage"></p>

<p><strong>Hypothesis</strong>: Will a meat meant for the 'a.m.' perform any better on an AM radio tower?</p>

<p><strong>Observations</strong>: Yes, in fact—this smaller bit of meat was louder than the rest, and seemed to burn <em>very</em> evenly. The breakfast sausage stayed under 70°C, and it quickly kicked the transmitter back to 6 kW (half) output, but was stable at that power output.</p>

<p>The end was quite crispy.</p>

<h2>Experiment 7 - Hot Dog Warmer</h2>

<p>We also tested holding a hot dog on the stick within about 1" of the base of the tower for approximately 60 seconds, but found there to be no significant heating at that distance. Part of the hot dog had to be touching the tower, creating the plasma arcs, to heat the hot dog at the 1460 kHz frequency of this broadcast station. At least at the 7 kW or so this tower was putting out.</p>

<h2>Conclusion</h2>

<p><img width="700" height="auto" src="https://www.jeffgeerling.com/sites/default/files/images/rf-safety-transmitter-status-display.jpg" alt="RF Safety - transmitter status log"></p>

<p>If we ran the tests again, I would very much like to bring a sound pressure level meter. It would be interesting to more quantitatively measure the sound of each object.</p>

<p>Some have also suggested using a better insulating rod—something like <a href="https://amzn.to/3z41UKv">this $300 fiberglass rod</a>. It would be an improvement over our wooden stick, though with the power at <em>this</em> antenna, the risk is extremely small that RF would arc through the stick, into a human holding it (especially with our thick rubber gloves), versus through the copper in the end. The bigger risk is heating and near-field RF exposure, which follows the <a href="https://en.wikipedia.org/wiki/Inverse-square_law">inverse-square law</a>. Distance is safety.</p>

<p>I would love to get a high speed camera (capable of at least 10,000 fps) to capture the plasma interaction between the tower and the meat to see if we could visualize the amplitude modulation in plasma. Maybe also repeating the experiment at dusk, so the plasma is brighter.</p>

<p>As it is, there are just lots of bright bursts of plasma that look interesting but mask the hidden modulation causing them! (If you're a Slow Mo Guy and you're reading this... DMs are open, lol.)</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LwIP – Lightweight IP Stack (130 pts)]]></title>
            <link>https://www.nongnu.org/lwip/2_1_x/index.html</link>
            <guid>41461850</guid>
            <pubDate>Fri, 06 Sep 2024 01:00:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nongnu.org/lwip/2_1_x/index.html">https://www.nongnu.org/lwip/2_1_x/index.html</a>, See on <a href="https://news.ycombinator.com/item?id=41461850">Hacker News</a></p>
Couldn't get https://www.nongnu.org/lwip/2_1_x/index.html: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[Why Don't Tech Companies Pay Their Engineers to Stay? (156 pts)]]></title>
            <link>https://www.goethena.com/post/why-dont-tech-companies-pay-engineers-more-to-stay/</link>
            <guid>41461747</guid>
            <pubDate>Fri, 06 Sep 2024 00:41:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.goethena.com/post/why-dont-tech-companies-pay-engineers-more-to-stay/">https://www.goethena.com/post/why-dont-tech-companies-pay-engineers-more-to-stay/</a>, See on <a href="https://news.ycombinator.com/item?id=41461747">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
                    
<p><em>Staying in a role builds valuable, company-specific domain knowledge. Leaving often results in a payday—so, why aren’t tech companies paying more for their engineers to stay?</em></p>



<p id="viewer-3sprm"><em>This article was originally published to </em><a href="https://marker.medium.com/why-dont-tech-companies-pay-their-engineers-to-stay-b9c7e4b751e9" target="_blank" rel="noreferrer noopener"><em><u>Marker</u></em></a><em>.</em></p>



<h2 id="viewer-eutfd"><strong>Why don’t companies pay engineers more?</strong></h2>



<p id="viewer-tnit">When news broke that breakout star Regé-Jean Page was <a href="https://www.usatoday.com/story/entertainment/tv/2021/04/02/bridgerton-rege-jean-page-wont-return-simon-basset-season-2/7063065002/" target="_blank" rel="noreferrer noopener"><u>leaving Bridgerton after only one season</u></a>, my initial reaction was that of shock and heartbreak. I couldn’t fathom why Netflix and The Duke of Hastings weren’t able to come to <em>some </em>sort of agreement.</p>



<p id="viewer-16onb">But then it hit me: my entire career, I’ve watched talented engineers leave companies for greener pastures after painfully short tenures. I’m even guilty of this myself — I left an amazing company after less than two years to join <a href="https://www.goethena.com/" target="_blank" rel="noreferrer noopener"><u>Ethena</u></a> as VP of Engineering.</p>



<p id="viewer-296pr">My example aside, I think the larger phenomena is a problem with organizations, not individuals. When hiring someone new, companies are forced to play in the open market, competing for top talent. But internally, they create opaque and informationally asymmetric compensation structures designed to minimize growth for existing employees to save the company’s bottom line.</p>



<p id="viewer-4lru8">That’s not the type of company I want to be a part of, so at Ethena, I’m working to create career paths and compensation structures that encourage long and happy tenures by <strong>paying our engineers generously and in proportion to their impact</strong>.</p>



<p id="viewer-4sd7n">Additionally, I’ll be working with our not yet hired <a href="https://jobs.lever.co/ethena/" target="_blank" rel="noreferrer noopener"><u>Head of People</u></a> to <strong>publish our compensation formula publically</strong>.</p>



<p id="viewer-ncd7">Let’s break down why this strategy makes sense, and why I believe it will be fundamental to Ethena Engineering’s long-term success.</p>







<h2 id="viewer-2qgeg"><strong>Tech compensation is all wrong</strong></h2>



<p id="viewer-9it7m">With a market this hot, software developers are hopping between companies and raking in enormous pay raises in the process. But let’s think like economists for a second — in a perfect market, this shouldn’t be possible! If a developer of a certain caliber is able to demand a given salary on the open market, why isn’t their existing company paying them that very same amount? In our imperfect world, engineers join a company at a certain level of expertise, spend time learning and growing that expertise within that company, and then for whatever reason find that it’s much easier to get recognition for that growth <em>outside</em> that company.</p>



<p id="viewer-f9b2u">A graph or two might help illustrate the problem. First, market salary increases with years of experience, assuming an engineer is improving their skills of course!</p>



<figure><img decoding="async" width="712" height="424" src="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_7eb09b3b3db64c38b017a22f173c9088_mv2.webp" alt="" srcset="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_7eb09b3b3db64c38b017a22f173c9088_mv2.webp 712w, https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_7eb09b3b3db64c38b017a22f173c9088_mv2-300x179.webp 300w" sizes="(max-width: 712px) 100vw, 712px"></figure>



<p id="viewer-fhu06">That one is a bit obvious, huh? Okay, well, let’s look at an engineer’s salary in their job at Acme Corp. Notice the slope of the line is a bit less . . . exciting? In my opinion, it’s likely the short-sighted desire to save money that defines the slope of this line at many companies.</p>



<figure><img decoding="async" width="712" height="406" src="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_b7f90219012d4bf4be4f863dd68ea6c7_mv2.webp" alt="" srcset="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_b7f90219012d4bf4be4f863dd68ea6c7_mv2.webp 712w, https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_b7f90219012d4bf4be4f863dd68ea6c7_mv2-300x171.webp 300w" sizes="(max-width: 712px) 100vw, 712px"></figure>



<p id="viewer-faf14">Well, if we’re talking about tech compensation over time, what about impact? Check out this steep angle!</p>



<figure><img loading="lazy" decoding="async" width="708" height="374" src="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_ab652db952a44b3d89397224345b2e81_mv2.webp" alt="" srcset="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_ab652db952a44b3d89397224345b2e81_mv2.webp 708w, https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_ab652db952a44b3d89397224345b2e81_mv2-300x158.webp 300w" sizes="(max-width: 708px) 100vw, 708px"></figure>



<p id="viewer-9e72g">See, developers don’t just improve their transferable skills while working at their company—they gain domain knowledge that is<em> specific to that company</em>, and that stuff is incredibly valuable. The wild thing is that it’s only valuable to that one company! Let’s put it all together in one graph, and please don’t judge my blending of the units here . . .</p>



<figure><img loading="lazy" decoding="async" width="925" height="380" src="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_56a1342296554847826f31397da0d476_mv2.webp" alt="" srcset="https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_56a1342296554847826f31397da0d476_mv2.webp 925w, https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_56a1342296554847826f31397da0d476_mv2-300x123.webp 300w, https://www.goethena.com/wp-content/uploads/2023/02/4d2dc5_56a1342296554847826f31397da0d476_mv2-768x316.webp 768w" sizes="(max-width: 925px) 100vw, 925px"></figure>



<p id="viewer-f39g5">The hard truth that many companies struggle to wrap their heads around is that they should be paying their long-tenured engineers <em>above market</em> <em>rate</em>. This is because an engineer that’s been working at a company for a long time will be more impactful <em>specifically at that company than at any other company</em>. Bridgerton casting aside, a company like Netflix <a href="https://hbr.org/2014/01/how-netflix-reinvented-hr" target="_blank" rel="noreferrer noopener"><u>understands this</u></a>, and will always outbid the competition for their top talent.</p>



<h2 id="viewer-36kfk"><strong>Paying engineers more is easier said than done…</strong></h2>



<p id="viewer-ak6jm">It’s easy to say “pay your developers more,” but it’s never that simple. For one thing, this type of compensation structure is unrealistic for very early-stage startups where paying your first few engineers a market salary is usually impossible.</p>



<p id="viewer-evo0q">Additionally, without an HR infrastructure in place, simplicity and equality can help early-stage companies thrive. This is why, so far at Ethena, the entire engineering team, including myself, has made the same salary.</p>



<p id="viewer-bvn89">The other challenge with a compensation strategy like I’m proposing is measuring everyone’s unique impact accurately and without bias. What happens when the new hotshot junior engineer suddenly starts outperforming the old guard? With an impact-based compensation structure, she would deserve a sizeable raise, but is the company willing to go through with it and risk upsetting the other team members?</p>



<p id="viewer-2gqd9">Plus, not all engineers necessarily increase their level of impact over time. How are these engineers, who still add value every day, compensated in this type of structure? I believe the key to navigating these hard problems is organizational maturity.</p>



<p id="viewer-dq9d3">At Ethena, our track record of transparency, open feedback channels, and an experienced management team put us in a position to make these types of bold policies that other companies may struggle to operationalize.</p>



<h2 id="viewer-1din5"><strong>So, what’s the plan for compensating engineers?</strong></h2>



<p id="viewer-d3elk">With the help of our soon-to-be Head of People, we intend to create a formula that takes into account title, years of tenure, and performance to create a compensation graph that scales with company impact as closely as possible. By making it predictable and public, both existing team members and potential hires can know in advance what they should expect to make at Ethena in the short- and long-term.</p>



<p id="viewer-fdnj">It also means being comfortable with hard truths — not everyone’s impact grows in the same way and at the same time scale and maintaining talent density means being willing to part ways with underperformers. At the end of the day, if any engineer at Ethena demonstrates they are performing at a certain level, regardless of how many years of experience they have in or outside of the company, they should be rewarded with a title and base salary that matches their impact (in addition to their tenure and performance multipliers!).</p>



<p id="viewer-dqi34">So stay tuned for more! I hope the future Head of People knows what they’ll be getting themselves into . . .</p>



<hr>



<p id="viewer-dmngh">Speaking of compensating engineers, <a href="https://jobs.lever.co/ethena?team=Software%20Development" target="_blank" rel="noreferrer noopener"><u>we’re hiring</u></a>!</p>
                </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: Burnout is bad to your brain, take care (575 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=41461499</link>
            <guid>41461499</guid>
            <pubDate>Thu, 05 Sep 2024 23:59:02 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=41461499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="41461499">
      <td><span></span></td>      <td><center><a id="up_41461499" href="https://news.ycombinator.com/vote?id=41461499&amp;how=up&amp;goto=item%3Fid%3D41461499"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=41461499">Tell HN: Burnout is bad to your brain, take care</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_41461499">165 points</span> by <a href="https://news.ycombinator.com/user?id=tuyguntn">tuyguntn</a> <span title="2024-09-05T23:59:02.000000Z"><a href="https://news.ycombinator.com/item?id=41461499">1 hour ago</a></span> <span id="unv_41461499"></span> | <a href="https://news.ycombinator.com/hide?id=41461499&amp;goto=item%3Fid%3D41461499">hide</a> | <a href="https://hn.algolia.com/?query=Tell%20HN%3A%20Burnout%20is%20bad%20to%20your%20brain%2C%20take%20care&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=41461499&amp;auth=65296f0963350844d44e2445b8720b797b03478f">favorite</a> | <a href="https://news.ycombinator.com/item?id=41461499">77&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I am depressed and burned out for quite some time already, unfortunately my brain still couldn't recover from it.</p><p>If I summarize the impact of burnout to my brain:</p><p>- Before: I could learn things pretty quickly, come up with solutions to the problems, even be able to see common patterns and see bigger underlying problems</p><p>- After: can't learn, can't work, can't remember, can't see solutions for trivial problems (e.g. if your shirt is wet, you can change it, but I stare at it thinking when it is going to get dried up)</p><p>Take care of your mental health</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Early Days of Valve from a Woman Inside (199 pts)]]></title>
            <link>https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961</link>
            <guid>41460276</guid>
            <pubDate>Thu, 05 Sep 2024 20:47:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961">https://medium.com/@monicah428/the-early-days-of-valve-from-a-woman-inside-bf80c6b47961</a>, See on <a href="https://news.ycombinator.com/item?id=41460276">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><figure><figcaption>Valve’s Half-Life on CD-ROM, still available on Amazon 26 years after its original release. The author worked side-by-side with Gabe Newell and Mike Harrington to launch Valve’s debut product and establish Valve as a major player in the game industry.</figcaption></figure><div><a rel="noopener follow" href="https://medium.com/@monicah428?source=post_page-----bf80c6b47961--------------------------------"><div aria-hidden="false"><p><img alt="Monica Harrington" src="https://miro.medium.com/v2/resize:fill:88:88/1*EoXOOMWLEiPwBBaYA1HQXA.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a></div><p id="ce96"><em>Almost 30 years ago, Monica Harrington guided the marketing and business development efforts for </em><a href="https://www.valvesoftware.com/en/about" rel="noopener ugc nofollow" target="_blank"><em>Valve</em></a><em>, which has become the biggest player in the PC game industry. This is her story.</em></p><p id="f68c">Almost 30 years ago, a small company was founded near Seattle WA. <a href="https://www.washingtonpost.com/news/the-switch/wp/2014/01/06/gabe-newell-on-valves-intimate-relationship-with-its-customers/" rel="noopener ugc nofollow" target="_blank">Gabe Newell</a> and my now ex <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=Mike+Harrington&amp;ie=UTF-8&amp;oe=UTF-8&amp;si=ACC90nzhFW-fOieS-I060ZQl1KsOy-txPmX_lTcmXNvaH1ksSP08B9BLNakMqXf1x-9DnuejP1RHVsRWyYDblVs8NZmZbkCBQ_Zo0glVi6BIas5v1jUQMw8%3D&amp;ictx=1&amp;ved=2ahUKEwj4p5bw0YKIAxWAJTQIHVghKf4Q_coHegQILBAD" rel="noopener ugc nofollow" target="_blank">Mike Harrington</a> were the official cofounders.</p><p id="1988">I was on a two-month leave from my job at Microsoft, where I was a group marketing manager in the Consumer Division, overseeing a product portfolio that included Microsoft Games.</p><figure><figcaption>The Microsoft Market Maker Award, chipped but still standing</figcaption></figure><p id="0799">I’d worked for Microsoft for nine years by then. My career was in high gear. A year earlier I’d been honored with the Market Maker award as the marketer who “added the most to the (1600+-person) Consumer Division’s bottom line.” I loved my job but I was also tired, so when Microsoft announced a leave program in the Spring of 1996, I was all in, hoping that a couple of months of paid leave would be invigorating, ideally with some travel and a break from the remodeling project that had left our new home a mess.</p><p id="28dd">My husband Mike had other plans.</p><p id="919c">He’d worked in the game industry before joining Microsoft the same year I did, and he had a dream of starting a game company. Mike decided to use his break to figure out if he wanted to stay at Microsoft or embark on a huge new adventure doing something else. During the break he started to solidify plans with Gabe Newell to really make it happen. Our travel plans quickly disappeared.</p><p id="6844">Gabe and Mike had met when Mike was working on printer drivers for the OS/2 operating system and the three of us had hung around together beginning a few years earlier, including a weekend in Eastern WA with Gabe’s then girlfriend. Mike was also friends with <a href="https://en.wikipedia.org/wiki/Michael_Abrash" rel="noopener ugc nofollow" target="_blank">Mike Abrash</a>, an industry luminary who was then working at <a href="https://en.wikipedia.org/wiki/Id_Software" rel="noopener ugc nofollow" target="_blank">id Software</a>, the independent studio behind the hugely popular Doom franchise.</p><p id="f1a5">Because of his relationship with Abrash, Mike was able to quickly secure agreement to build Valve’s first product using id’s game engine. That was a huge deal and essentially catapulted Valve and Gabe and Mike into serious company and game development.</p><p id="babf">When I returned to Microsoft, Mike and Gabe and the small team they quickly assembled were well on their way to planning their first product.</p><p id="727c">The original idea Mike had was that Valve would create a product that would be launched by Microsoft. It turned out that Microsoft had little appetite for doing a deal with its own former employees, which meant that Valve would need to sign a publishing agreement with one of Microsoft’s competitors.</p><p id="b469">Because of my role overseeing Games Marketing for Microsoft, the interplay with Valve was complicated. The investment Mike and I were making in Valve was substantial so I knew I couldn’t not be involved, but I also needed boundaries. When I returned to Microsoft, I went directly to the head of games and to the VP of our division to say that my husband and Gabe Newell were starting a games business, and that I would be helping them, and that I’d be open to any changes Microsoft might want to make in terms of my role as a result. Both <a href="https://en.wikipedia.org/wiki/Ed_Fries" rel="noopener ugc nofollow" target="_blank">Ed Fries</a> and my managers were friendly and reassuring. At the time, there were literally hundreds of games published each year that vanished into the ether. Valve was a tiny player, and I’m sure from Ed’s and everyone else’s perspective, likely to remain so.</p><p id="2637">I was traveling for Microsoft on a cold wintry day in Seattle when Mike called to tell me that he and the team had just finished meeting with the head of <a href="https://www.wired.com/story/sierra-online-ken-williams-interview-memoir/" rel="noopener ugc nofollow" target="_blank">Sierra Online</a>, which was a major force in PC Games and one of our biggest competitors. Their office was in Bellevue WA, just twenty-five minutes or so from our home. Because of the icy conditions, Ken Williams, their CEO, was the only one from Sierra who made it into the office. By the end of the meeting Ken was essentially saying to the team, “I want to work with you…let’s make it happen.”</p><p id="5453">Because of my role at Microsoft, I decided not to get involved in the contract negotiations. It was up to Mike and Gabe to figure that out with Sierra. At the time, standard industry practice for an unproven game company was for an upfront advance against royalties, with additional royalties due only if the game were successful.</p><p id="996a">The Sierra advance was for approximately a million dollars, which along with the hundreds of thousands Mike and I and Gabe had already invested, meant that Valve had enough to fund the company through the launch of the first product.</p><p id="b6b1">The first product was going to be a first-person shooter, in the same category as id’s category-defining game <a href="https://en.wikipedia.org/wiki/Doom_(franchise)" rel="noopener ugc nofollow" target="_blank">Doom.</a></p><p id="c144">Gabe and Mike and I were meeting regularly and one of the things I told them was that the Games business was a hit-based business, where only the top 10 games made any real money. At a time when thousands of games were being introduced each year, it meant that Valve’s first game was going to be a hit or it wasn’t, and given the dynamics of the games industry, if the first game wasn’t a hit, there likely wouldn’t be a second Valve game. Gabe and Mike had been talking about trying to launch a B product, something that wasn’t a hit, but that would be successful enough to support the company as it established itself, and I knew and made clear that simply wouldn’t work.</p><p id="367e">For the next ten months or so, the activity at Valve was all about hiring and onboarding people, building out appropriate office space, and fleshing out the game concept that had hooked Sierra’s Ken Williams.</p><p id="22bb">While I continued to consult with Valve, my day job at Microsoft was invigorating. One of the projects we had underway was the first global launch of a new game, which meant a simultaneous launch in all of the markets where Microsoft had a consumer presence. My team and I had been working for months to make sure we had the launch and marketing strategies ready to go to support Microsoft’s next big hit game in markets around the world. At the time, the game group’s biggest hits to date were the strategy game <a href="https://www.ageofempires.com/" rel="noopener ugc nofollow" target="_blank">Age of Empires</a> and <a href="https://www.flightsimulator.com/" rel="noopener ugc nofollow" target="_blank">Flight Simulator,</a> a product that mostly appealed to pilots and flying enthusiasts.</p><p id="2033">In the background, I was still working with Valve, something I reminded Microsoft folks about frequently. No one seemed worried. My general rule was that I wouldn’t tell Microsoft anything about Valve, and I would tell Valve nothing about what was happening at Microsoft. However, all of the learning I was doing about the games industry in my work and personal time was definitely put to work for both Microsoft and Valve. Mike and I were each working 10 to 12-hour days or more.</p><p id="54df">At one time, I was in a meeting with some well-known game developers who had signed a long-term, multi-game development deal with Microsoft and the subject of Valve’s new deal, using the id engine came up. The leads of that company, not having any idea about my ties to Valve said something like, “what a joke — a couple of Microsoft developers license the id engine and think they can build a game.”</p><p id="a4e6">Inside, I thought, “Oh man…this is going to be tough.”</p><p id="59c1">One of the roles I played with Mike and Gabe in those days was to help them understand what it would take to be successful from a marketing and business standpoint. At the time, the key marketing strategies for games involved PR and outreach to influentials, advertising in game-specific publications, and working with the retail channel on strategies to get prominent display space so a consumer walking in would feel compelled to pick up and purchase your game. Once a game shipped, you had to feed and nurture the early adopters, supporting them so they could tell their friends about your game. The broad strategy I’d learned and fine-tuned over a period of years was essentially “Arm and Activate (early adopters).”</p><p id="2ae2">From a PR perspective, Microsoft was a phenomenon within the industry. By the time I worked on the games business, I’d spent years working directly with consumer and technical press for products ranging from Microsoft Word and Office to Expedia and Encarta. I’d also managed large product marketing and communication teams, and had been on press tours meeting reviewers around the country, from New York to New Hampshire to Austin, Texas, and Eugene, Oregon. The games business shared a lot in common with some of the other businesses I worked on, but there was also a key difference. In a business like Word or Office, you basically run your marketing with a winner-take-all perspective, and that’s because in those businesses, there’s likely to be one market leader, and as that leader gains more market share, its advantages continue to grow and become self-reinforcing as companies and the broader industry standardize around one product. At the time, my experience in fighting market battles with Lotus and WordPerfect was that a given software category will only support one market leader.</p><p id="0e3b">The PC games business in the late 90s was much more like the music business, where there were lots of independent studios, and where the developers treated each other much as musicians do. There’s mutual respect, there’s competition in a given category, but also the understanding that gamers are going to buy multiple games. If they love your game, they also have room to love your competitor’s game and in fact someone who likes action games is likely to be a connoisseur of the category, with multiple favorites. The most influential people in the games market were not press, but game developers themselves.</p><p id="8f99">For the publishers and major players, it was a different story. Microsoft was competing against Sierra and Electronic Arts to attract game developers, much like music labels at the time competed to sign top musical talent. The typical 15 percent royalty margins for new games developers mimicked other content deals in music and book publishing.</p><p id="58b9">Of course, the huge difference between the games business and the book or music industry was that the costs of producing a game were much higher and starting to climb exponentially.</p><p id="3ecb">At Valve, costs were growing rapidly as the team was being built out. Soon it became clear that the initial investment Gabe and Mike and I had made plus the advance Sierra had made were not going to cover the actual cost of producing Valve’s first game.</p><p id="64b4">One evening Mike and I hosted a potential new Valve hire, Yahn Bernier, and his fiancé at our newly remodeled home. I remember thinking that “if we hire you, Mike and I and Gabe are going to be paying your salary from our own pockets.”</p><p id="cd6e">We pitched Yahn and his fiancé Beth hard. I remember that it was only a few days earlier that Beth had learned Yahn even did software development, as his day job was as a patent attorney in Atlanta. Valve’s strength in those days was finding talent around the world who had done amazing things — the type of things that might not show up on a typical resume but could be discovered on the Internet, which in many ways was still in its infancy. At one point Gabe tracked down and recruited two creators of game-related software that was becoming popular on the Internet only to discover that they delivered pizza for a living and they thought Gabe’s phone call was part of some elaborate prank.</p><p id="0d01">Fortunately, while the Valve development team was working to build Half-Life, my Microsoft options were continuing to vest, which meant that Mike’s and my net worth continued to climb. It was stressful, but in the overall context, the stress was manageable.</p><p id="4009">Somewhere in the first year of Half-Life’s development, I officially wrote up the marketing plan for launching Half-Life. Key to our strategy was positioning Half-Life as a game that was worthy of Game of the Year honors. We wanted to earn the respect of the industry influentials, which included the gaming press and other game developers. As part of the strategy, Valve’s developers went to industry conferences to talk about some of the work they were doing in key technical areas including AI, skeletal animation, and physics. In support of the effort, I wrote up backgrounders on each of these topics for the press based on interviews with developers Ken Birdwell, Jay Stelly, Yahn Bernier and others.</p><p id="7d8d">In the Spring of 1997, I was in a meeting at Microsoft about the upcoming E3 show, which was the biggest and most important industry tradeshow for electronic entertainment companies. One of my team members made the recommendation for Microsoft not to attend the show, and during her pitch to me and others, part of her recommendation was clearly based on the idea that our competitors weren’t likely to have a major presence there. I knew Sierra was going to be there and Valve would have a big presence.</p><p id="df2f">Meanwhile, the game I’d been focused on for Microsoft, which would have been at the show, had faced a huge setback. A month or two earlier, I had initiated a meeting with Ed Fries where I told him point blank, “I’m not hearing great things about our game and am losing confidence. Are you sure?”</p><p id="5fdd">My conversation was based on hallway talks with various gamers within the division, who had had exposure to the game and weren’t excited. Basically, I couldn’t find anyone who really believed in the game. I also knew from the work I was doing for Valve what it felt like when developers who are also gamers are hugely excited about a new project</p><p id="1ce1">By that time, Microsoft’s Consumer Division had already laid a huge egg with <a href="https://www.technewsworld.com/story/microsofts-copilot-rises-from-the-ashes-of-bob-and-clippy-178799.html" rel="noopener ugc nofollow" target="_blank">Microsoft Bob</a>, and I knew that if we launched another consumer product that didn’t live up to the expectations we’d raised, the broader consumer effort would be hugely damaged. After some soul searching, Ed decided to cancel the launch, and the huge plans we’d made were quietly set aside.</p><p id="ed13">By that time, I knew that Sierra was going to be teasing Half-Life and that it would be the star of their booth at E3. I remembered thinking, “Damn, I know too much. Something has to change.”</p><p id="9790">I had another conversation with Microsoft execs about my role and the conflict with Valve, and again I was essentially told, “it’s fine, we’re OK, we like where you’re at, don’t worry.”</p><p id="8fb0">A couple of months later, Valve’s Half-Life premiered publicly in the Sierra booth at E3 in Las Vegas. The demos Valve showed were so well received that Valve earned Best Action Game honors at the show.</p><p id="b308">When the Valve team returned from Las Vegas, and I got the full update, I asked for a meeting with the division’s senior exec, and said, “this may not be a conflict for you, but it is for me. I need a new assignment.”</p><p id="5df5">Shortly thereafter, I started a new role within the company, completely unrelated to the games business.</p><p id="c971">As the months went on and Valve’s costs continued to escalate, it became clear that Mike and I were maxing out on our financial commitment. Rather than renegotiate the contract with Sierra, Gabe, who had started at Microsoft much earlier than Mike and me, began funding the ongoing development costs, set up as a loan against future company profits.</p><p id="a66a">Over that summer and in the months to follow, the “game” that Valve had shown, which wasn’t actually a game, but instead was some elaborate demos, went into early playtesting, which involved bringing gamers into Valve, having them play with the game elements and giving us their feedback. The feedback was OK. Just OK. Which for a company that needed a hit was devastating.</p><p id="1255">Gabe and Mike and I all knew that we couldn’t stay the course. If Valve shipped the game we had, it would launch and quietly disappear, and all of the work we’d all done would account for nothing. All of the people we’d hired would lose their jobs, we’d lose the money we’d invested. It was a disaster.</p><p id="b363">There was no choice. Ultimately the decision was made to essentially toss out what we had, and use everything the team had learned to that point to start fresh. Unfortunately, Sierra was not on board with the plan. They wouldn’t invest any more to make Valve’s first game a hit. We were on our own. Gabe’s deep pockets became more important.</p><p id="d83e">It would take more than a year for Valve to rebuild Half-Life in a way that put us back into the position which everyone assumed was already the case– with a game that could be launched in just a few months. The new target become launching for the Christmas season of 1998.</p><p id="50d1">In the Spring of 1998, Gabe was essentially saying, “When can you be here full-time? We need you now.”</p><p id="2c5b">I’d already written the marketing and launch plan for Half-Life, I’d written all of the press materials and copy for the web site, I’d written the backgrounders we shared with key press, but I was not focused on some of the larger business fundamentals. As a huge example, I still had not read the publishing agreement Gabe and Mike signed with Sierra.</p><p id="0988">I went to my bosses at Microsoft and essentially said, “I’m ready to leave.”</p><p id="77f4">In my closing interview with Pete Higgins, then the VP of Microsoft’s Consumer Division, he started by saying, “Is there anything that would get you to stay?” and I knew the answer was No.</p><p id="548b">I needed to move on. Valve needed me. We had had too much invested. It was time.</p><p id="e7ea">For the next few months, I worked furiously laying the groundwork for Half-Life’s launch. Among the projects I worked on was seeding a story with the Wall St. Journal. Basically, my idea was to build retailer buzz so that the retailers who were going to be putting in orders for the Christmas season would order more Half-Life and feature it prominently. For several weeks, I sent the reporter emails with updates about all of the great Half-Life news, from developer comments to industry buzz that was appearing in the gaming press.</p><p id="7014">By that time, most of my Valve work was with Gabe or the other developers or with Sierra’s own marketing team. Mike was furiously heads-down on the final work needed to finish Half-Life so we could send it off for duplication. One of the key issues we worried about was piracy. One of my nephews had recently bought a CD duplicator with a monetary gift I’d sent him, and I was horrified to realize that he was copying games and giving them to his friends. To him, it wasn’t stealing; it was sharing. A generational shift in culture and technology meant that the game we’d poured our blood and treasure in would likely be pirated from its first days not just by the professional thieves, but also by everyday end users. At the time, no publisher was doing effective checking to ensure ownership of a PC game was legitimate. Valve would need to implement an authentication scheme.</p><p id="75c6">At around this time, we caught a lucky break. For about a year, we’d been pushing Sierra on plans for seeding Half-Life with OEMs, the computer manufacturers who produced the hardware on which Half-Life could be played. For a time, we were even talking seriously with Intel about a version of the game that would highlight the features in a new chip they had under development. All of this came from the collective Microsoft experience about working with OEMs and seeding product in mass quantities to spur user adoption. Ultimately, the Intel play didn’t happen but one of the programs that Sierra led involved providing a small portion of the game that they shipped as a trial disk called “Day One.”</p><p id="843c">When we first realized what had happened, that Sierra had shipped some of the Half-Life code as Day One, Mike and I were furious. We quickly realized that we were at the mercy of whatever happened next. Fortunately, Day One became a phenomenon. Game developers LOVED it and began buzzing. Half-Life was going viral. The product was still a month or two from completion and no one had played the final game, but the early buzz for the first portion of the game was hugely promising.</p><p id="a4aa">I continued to feed the Wall St. Journal reporter news about Half-Life, about its early reception, about what we were hearing from Day One users. It was all hugely positive.</p><p id="fa0e">Mike was still coding and he was exhausted. I was exhausted, and the team was antsy as we really didn’t know how Half-Life would do in the marketplace. Finally, in late 1998, a few weeks before Christmas, we were ready to ship. Valve had a party where Mike broke open a pinata filled with candies and small toys.</p><p id="94fa">While the game was in manufacturing, final disks went to the gaming press. We waited. At some point, when Mike was in the shower, I felt overwhelmed by anxiety and asked with a worried tone to my voice, “Is it really a good game?” and his honest response, “I don’t know.”</p><p id="a932">Gabe and Mike and I had all been through the shipping process at Microsoft, but being part of a large team where the company backed everything was completely different from being the funders of a company where we knew it was a one-shot deal. The game would either be a hit or it wouldn’t, and for a short while, when the disks were being duplicated and the packaging printed and shipped, we simply wouldn’t know. The first disks went straight to the key gaming press.</p><p id="f30f">About a year earlier, I had worked with Gabe to set the audacious goal of Half-Life winning at least three of the top industry Game of the Year awards. We very consciously thought through what it would take, including breakthrough technology, a compelling new angle, and broad industry support. It was going to be especially tough for a game that some insiders initially dismissed as “Microsoft developers building on id’s technology.”</p><p id="c7e1">Within a few weeks, Half-Life won the first of more than 50 Game of the Year awards. It had never happened before. Shortly before Christmas, the Wall St. Journal article came out with the headline, “Valve’s Storytelling Game is a Hit.” In the article, Sierra as our publisher was never mentioned. That was an explicit strategy of ours — since we were funding the development, we wanted Half-Life to be known as a Valve game, not a Sierra Studios game.</p><p id="d802">A few weeks before Christmas, we were all exhausted. Some members of the team took vacation, but most continued showing up, almost in a daze, as we moved from tight pre-production intensity to the purgatory of Wait and See. We hadn’t seen any sales figures. At one point, just after the game had shipped and we started seeing feedback online about how awful the authentication system was, Mike was yelling. It turned out that when someone complained about the authentication, Mike called the person back directly, challenging them and asking for sales verification. None of the people Mike contacted who complained early on had paid for the product. Mike was mad and tired but also vindicated.</p><p id="4502">The Game of the Year honors kept coming in, and we were optimistic that the great reception we were getting from industry influentials would ultimately translate into financial success for the company. I started working feverishly on post-launch marketing strategies, which mostly involved amplifying Half-Life’s success and making sure retailers were armed with sales data and point of sale materials so customers could find it. I had purchased the web site GameoftheYear.com and we launched it with all of the material about the Game of the Year honors Half-Life was winning, including from <em>PC Gamer</em> and <em>Computer Gaming World</em>, which then were the heavyweights in our industry.</p><p id="a06e">In the meantime, Gabe and Mike’s relationship had deteriorated. A huge part of this was the stress and the financial imbalance. Gabe had become the major funder, and Mike had essentially burned himself out doing the final critical coding work before Half-Life shipped. Where before we’d tried to shield the team from our collective anxiety, Mike’s yelling at customers had unnerved some people. I found myself in a situation where the two founders were essentially not talking to each other and people were tiptoeing around wondering what was going on.</p><p id="2f30">Then, in January at a time when I was getting ready to focus on Next Steps, Sierra asked for a meeting. Essentially their message to us was “thank you, the game’s done great, we’re moving on.” They were pulling all marketing from Valve to focus on one of their next titles. Basically, their marketing at the time amounted to Launch and Leave, whereas we were trying to market a franchise-worthy game that we could build on for years to come.</p><p id="174c">Gabe and I were stunned; I was also furious. I knew that the marketing of Half-Life was only getting started. We’d done all of this hard work to earn Game of the Year honors with the idea that that would help us break out from the pack and it was time to capitalize on that effort with sustained marketing. At Microsoft, winning awards was always the start of a much larger process where we leveraged positive reviews to win customers over time.</p><p id="ddf0">With steel in my voice, I told the Sierra team that they were not pulling marketing dollars from Half-Life. They were going to re-release it in a Game of the Year Box, and they were going to support it with huge marketing spend or we were going to walk away from our agreement and tell the industry that had fallen in love with Valve how screwed up Sierra really was. At the end of the meeting, I was shaking. We were vulnerable, the partners were barely speaking, and life at home and in the office was tense.</p><p id="66b7">Sierra relented and started working on the packaging for a new Game of the Year box. The icon on the front looked similar to an Academy Award. The basic idea was that for anyone walking cold into a game retailer, they would quickly see that Half-Life was the Game of the Year they couldn’t ignore.</p><p id="3b50">By early March, things at Valve were still very strained. In addition to ongoing communications and marketing work for Half-Life, I was working on developer strategies closely with Gabe and some of the other developers and Mike was spending less and less time at the office. At some point in the early Spring, Mike said to me, “I want to leave Valve.”</p><p id="e8fd">I was in a panic. We hadn’t made back the money we’d invested. The buzz was continuing to build, but we had a long way to go. I told Mike that we needed time to figure out a pathway out, as at that time, the value of our ownership stake was highly questionable. At the same time, I was also panicked because I’d read, for the first time, the contractual agreement between Sierra and Valve, and while I thought I’d understood the key terms Gabe and Mike had agreed to, there were several points I hadn’t been aware of. Chief among them was that Sierra owned all of the intellectual property for Half-Life and held the exclusive option to publish Valve’s next two games, all at a royalty rate for Valve of 15 percent. We would do all of the development work with an upfront royalty advance of $1 million for each of those games, and Sierra would get 85% of the revenue and all of the intellectual property. At the time, I knew development costs were approaching $5 million or more per game</p><p id="ca2b">Given the licensing deal we had with id for the game engine license, the lack of any ownership of our own IP, and the exclusive commitment for future game publishing rights, all I could see was Valve swimming in red ink for years to come.</p><p id="4188">We needed a different path forward.</p><p id="0a0f">Fortunately, one of the consequences of Valve’s work on an authentication system was that our customers were registering with Valve directly,</p><p id="b7f3">Early on, we started to understand the benefit of what we’d inadvertently done. Instead of a situation where we had no idea who our customers were, we actually knew exactly who our customers were. It was unprecedented. Every Half-Life registration meant a customer contact directly in Valve’s database.</p><p id="9047">Added to this, the previous year, Gabe had had the brilliant idea of recruiting a development team that had written one of the leading mods for id’s Doom and now they were part of Valve’s team. John Cook and Robin Walker were delightful Aussie additions to Valve. The game mod they’d written to run on top of id’s Doom was quickly adapted to run on top of Half-Life. Essentially the game mod enabled a player to play a completely different game on top of the technology of Half-Life. In the case of Team Fortress, it was a multiplayer game where you could team up with your friends over the Internet in a team-based game where each of you played a unique and fun character.</p><p id="f11a">While the Half-Life buzz was continuing to build through word-of-mouth and the new marketing push, the additional buzz and enthusiasm that came as a result of Team Fortress was layered on top of everything else. Soon there were hundreds of thousands of people playing Team Fortress on top of Half-Life and Valve knew who each and every one of them were. We had a direct pipeline, bypassing Sierra, to our own customers.</p><p id="083a">In late Spring, Team Fortress, the Cook/Walker mod for the Half-Life engine was presented at E3, where it won Best Action Game and Best Online Game on behalf of Valve.</p><p id="b0ac">Through all of this, I continued to noodle about the best ways I could position Valve for long-term success. With the bad publishing deal in hand, I knew I had to work on multiple fronts.</p><p id="2b83">If Mike and I were to leave, we needed to demonstrate value for Valve that wasn’t tied directly to the Half-Life IP. We needed to renegotiate our deal with Sierra. And we needed to figure out a way to cap our royalties to id, so that Valve wouldn’t be paying them a fee each time someone bought one of our future games.</p><p id="636c">Valve had already done a lot of work to customize the id engine code for our purposes, and we’d reached the point where for any next game, we’d either continue using and adapting that code base or we’d take the time to write new engine code ourselves from scratch.</p><p id="1aeb">With Gabe’s OK, I reached out to the team at id, and we came to quick agreement on a capped deal.</p><p id="da9a">The second step was to kick off steps to renegotiate our agreement with Sierra. I met with Valve’s attorney, and the basic approach we took was to make clear that if Sierra was going to insist on keeping the original deal, then the Valve team would pivot to a completely different category and never publish a second game. Since Gabe and Mike had a ton of experience in other facets of software development, the threat was not idle. The bottom line was we weren’t going to create games and take on all of the risk only to make other people rich.</p><p id="095d">One of my responsibilities in the mid-90s at Microsoft was overseeing the initial marketing for Expedia. I’d spent a lot of time with Josh, the person on my team who was tasked with laying out the market case. From that and other experience over the years, I knew a lot about how to scope and size the online opportunity for a completely new category. I got to work trying to figure out an online business opportunity that wasn’t dependent on the Half-Life IP.</p><p id="7991">Internally, we had rich experience to draw from. Gabe and Mike had a lot of experience with developers and with the developer marketplace, and at one time, I’d overseen Microsoft’s PR outreach related to software tools for the developer community. In addition, I knew the games business inside and out, and I’d been part of teams within Microsoft that were completely focused on the emerging online opportunity made possible by the Internet.</p><p id="0c44">By the summer of 1999, Mike was researching trawler yachts, I was immersed in figuring out Valve’s potential business opportunities, and Gabe was doing deep thinking, leading the team and communicating with customers. In the meantime, because of the way Gabe and Mike had structured the ownership, where employees could earn equity over time, Mike’s and my ownership stake was effectively shrinking.</p><p id="1fa7">Finally, at some point, I realized that the only way to prove the worth of some of the ideas I was focused on was to write them up and get an offer. Without that, I could make the case that Valve was worth a lot of money or nothing. And I knew Gabe could make that same case.</p><p id="d30e">With Gabe’s buy-in, I decided to pitch Amazon on a new business opportunity. I’d known about Amazon for a long time as one of my dear friends from Microsoft had been hired on as its original marketing lead. She had since left, but I’d followed the company closely. At the time, it was headquartered in South Seattle in an imposing building just South of the i-90 freeway exchange.</p><p id="c50e">In a nine-page document, I proposed that Valve and Amazon team up to create a new online entertainment platform. I scaled the business opportunity within four years at $500 million dollars. The gist of the idea was to create a made-for-the-medium platform that would bring users together in a sticky, compelling entertainment experience, with digital and offline content sales. I wanted Amazon’s financial backing as a way to gain first mover advantage against Microsoft and Electronic Arts, then the major PC games players. I didn’t see a role for Sierra. If pushed, we wouldn’t create any new games ourselves, and instead would team with outside developers so that they could distribute content not subject to an 85% publishing fee. At the time, I considered it an act of rebellion against the traditional publishing dynamic where independent developers took on huge risk, and the big publishing houses reaped the rewards.</p><p id="7bd1">With Gabe’s OK, I sent the document over to Amazon. Within a short period of time I had a response. “Let’s meet.”</p><p id="beb4">When I arrived, the Amazon Vice President I met with was super friendly. He was familiar but I couldn’t place him. Then he said, “You don’t remember me, but you interviewed me at Microsoft.”</p><p id="002c">I realized I must not have hired him, and for the first time, started getting uneasy about how things would go. He reminded me gently that when Amazon was pretty much in its infancy and a company I knew of mostly because of my friend, I had interviewed him for a position at Microsoft. He was interviewing at Microsoft because the company he worked for had just done a major layoff. Our division was pretty much in a temporary hiring freeze and he had a background in retail, so I suggested he look into a tiny company named Amazon. By the time we met again, he’d been there several years.</p><p id="258d">We had a great discussion, and a couple of weeks later, a champagne bottle appeared at Valve’s door.</p><p id="1583">It was exhilarating and scary at the same time. We had an offer from Amazon for a minority stake, but the dynamics within the company were tricky. Amazon could help propel Valve to the next level, but the partnership would not be without costs. Valve’s culture was still evolving. A partnership with a major outside player could help but it could also hurt what we’d all built.</p><p id="f98f">It was after the Amazon offer that Mike revealed to Gabe that he wanted to leave. With an offer in-hand, it didn’t take long for us to figure out the outline of a deal.</p><p id="0c91">Ultimately, Mike and I gave up ownership to start the next chapter of our lives. The structure of the deal meant that we would be vested in Valve’s success over the next five years. I knew the opportunity that lay ahead. I also saw huge risks. The Sierra deal might collapse publicly, employees might get spooked, we might not be able to actually get the IP back, and with any online endeavor, there would be huge new risks.</p><p id="c6a8">Within several months, Mike and I left Seattle for a new adventure on San Juan Island and Whistler B.C. I continued to make myself available to Valve. But for the most part, I shed my work identity and started on new projects, including a passion for protecting Washington’s resident orca whales.</p><p id="23f7">It was up to Gabe and the Valve team to move the business forward.</p><p id="e0cd">Several years after Valve, I went to work for the Bill &amp; Melinda Gates Foundation, and from there, took on another CMO role when Mike started a new partnership and business focused on photo editing. That company, Picnik, sold to Google in 2010.</p><p id="fec2">I continued to see the Valve team intermittently through the years, and even worked with the team on a <a href="https://www.nytimes.com/2012/09/09/technology/valve-a-video-game-maker-with-few-rules.html" rel="noopener ugc nofollow" target="_blank">story</a> that appeared in the New York Times in 2012 about Valve’s highly unusual flat culture. When the Half-Life IP was secured and owned by Valve, the team sent us a small trophy with the etching “Welcome back Gordon.” Gordon was the key protagonist in the Half-Life adventure.</p><p id="fd0e">In 2016, Mike and I separated and then divorced. In 2018, I joined Gabe and some friends from Valve on a cruise on Gabe’s yacht around the islands of Japan.</p><p id="2722">Some ten years after Half-Life’s release, PC Gamer named it the Best PC Game Ever. In a separate story, PC Gamer also named Half-life the Best Marketed Game Ever, with special credit to whoever came up with the Game of the Year box and retail push. Valve’s online platform Steam is now an industry phenomenon, with annual sales in the billions of dollars.</p><p id="7e3e">As I look back on the huge success Valve has become, I’m proud of what the team accomplished. I’m also proud of the work I did while recognizing that my biggest contributions to Valve’s business went largely unnoticed and unrecognized within the industry. Part of that was due to the bro culture of the software business, part of it was that I receded to support my husband in a partnership where he was effectively the lesser partner, and part of it was that women, especially in tech, often seem to disappear when the story gets told.</p><p id="6585">I was hugely disappointed when Valve released a video in 2023 about the creation of Half-Life where one of the people interviewed, Karen Laur, a wonderfully talented texture artist, talked about the isolating experience of being a woman at Valve and essentially said that the only other woman during her tenure there was an office manager. I understood why she felt as she did, but the senior Valve team knows better. Watching the <a href="https://www.youtube.com/watch?v=TbZ3HzvFEto" rel="noopener ugc nofollow" target="_blank">video</a>, I felt like my place in Valve’s history had been completely erased.</p><p id="0f3e">I know that Valve wouldn’t have been successful without Mike. It wouldn’t have been successful without Gabe. And it wouldn’t have been successful without me. A friend of mine who knows the full story once said to me, “you were a founding partner” and in hindsight, I agree. From the beginning, I invested time, treasure and industry expertise to make the company a huge success.</p><p id="b437">And it is.</p><figure><figcaption>The author celebrating the holidays along Route 66 with Scott.</figcaption></figure><p id="df2c"><em>Monica Harrington lives in Bend, Oregon, with Scott Walker and their spaniel Johnnie Walker Black and White.</em></p><p id="042b">Editorial note 8/21: The author updated the details regarding the authentication scheme and the name of the mod created by Robin Walker and John Cook, which ultimately shipped as Team Fortress Classic.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Clojure 1.12.0 is now available (339 pts)]]></title>
            <link>https://clojure.org/news/2024/09/05/clojure-1-12-0</link>
            <guid>41460037</guid>
            <pubDate>Thu, 05 Sep 2024 20:12:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0">https://clojure.org/news/2024/09/05/clojure-1-12-0</a>, See on <a href="https://news.ycombinator.com/item?id=41460037">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<div>
<h3 id="add_libs"><a href="#add_libs"></a>2.1 Add libraries for interactive use</h3>
<p>There are many development-time cases where it would be useful to add a library interactively without restarting the JVM - speculative evaluation, adding a known dependency to your project, or adding a library to accomplish a specific task.</p>

<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-lib">add-lib</a> takes a lib that is not available on the classpath, and makes it available by downloading (if necessary) and adding to the classloader. Libs already on the classpath are not updated. If the coordinate is not provided, the newest Maven or git (if the library has an inferred git repo name) version or tag are used.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/add-libs">add-libs</a> is like <code>add-lib</code>, but resolves a set of new libraries and versions together.</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.repl-api.html#clojure.repl.deps/sync-deps">sync-deps</a> calls <code>add-libs</code> with any libs present in deps.edn, but not yet present on the classpath.</p>
</li>
</ul>
</div>
<p>These new functions are intended only for development-time interactive use at the repl - using a deps.edn is still the proper way to build and maintain production code. To this end, these functions all check that <a href="https://clojure.github.io/clojure/branch-master/clojure.core-api.html#clojure.core/%2Arepl%2A">*repl*</a> is bound to true (that flag is bound automatically by <code>clojure.main/repl</code>). In a clojure.main REPL, these new functions are automatically referred in the <code>user</code> namespace. In other repls, you may need to <code>(require '[clojure.repl.deps :refer :all])</code> before use.</p>
<p>Library resolution and download are provided by <a href="https://github.com/clojure/tools.deps">tools.deps</a>. However, you do not want to add tools.deps and its many dependencies to your project classpath during development, and thus we have also added a new api for invoking functions out of process via the Clojure CLI.</p>
</div>
<div>
<h3 id="tool_functions"><a href="#tool_functions"></a>2.2 Invoke tool functions out of process</h3>
<p>There are many useful tools you can use at development time, but which are not part of your project’s actual dependencies. The Clojure CLI provides explicit support for <a href="https://clojure.org/reference/clojure_cli#tools">tools</a> with their own classpath, but there was not previously a way to invoke these interactively.</p>
<p>Clojure now includes <a href="https://clojure.github.io/clojure/branch-master/clojure.tools.deps.interop-api.html#clojure.tools.deps.interop/invoke-tool">clojure.tools.deps.interop/invoke-tool</a> to invoke a tool function out of process. The classpath for the tool is defined in deps.edn and you do not need to add the tool’s dependencies to your project classpath.</p>
<p><code>add-lib</code> functionality is built using <code>invoke-tool</code> but you can also use it to build or invoke your own tools for interactive use. Find more about the function execution protocol on the <a href="https://clojure.org/reference/clojure_cli#function_protocol">CLI reference</a>.</p>
</div>
<div>
<h3 id="_2_3_start_and_control_external_processes"><a href="#_2_3_start_and_control_external_processes"></a>2.3 Start and control external processes</h3>
<p>For a long time, we’ve had the <code>clojure.java.shell</code> namespace, but over time Java has provided new APIs for process info, process control, and I/O redirection. This release adds a new namespace <a href="https://clojure.github.io/clojure/branch-master/index.html#clojure.java.process">clojure.java.process</a> that takes advantage of these APIs and is easier to use. See:</p>
<div>
<ul>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/start">start</a> - full control over streams with access to the underlying Java objects for advanced usage</p>
</li>
<li>
<p><a href="https://clojure.github.io/clojure/branch-master/clojure.java.process-api.html#clojure.java.process/exec">exec</a> - covers the common case of executing an external process and returning its stdout on completion</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="method_values"><a href="#method_values"></a>2.4 Method values</h3>
<p>Clojure programmers often want to use Java methods in higher-order functions (e.g. passing a Java method to <code>map</code>). Until now, programmers have had to manually wrap methods in functions. This is verbose, and might require manual hinting for overload disambiguation, or incur incidental reflection or boxing.</p>
<p>Programmers can now use <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#qualified_methods">qualified methods</a> as ordinary functions in value contexts - the compiler will <a href="https://clojure.org/reference/java_interop#methodvalues">automatically generate the wrapping function</a>. The compiler will generate a reflective call when a qualified method does not resolve due to overloading. Developers can supply <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it.</p>
</div>
<div>
<h3 id="qualified_methods"><a href="#qualified_methods"></a>2.5 Qualified methods - <code>Class/method</code>, <code>Class/.method</code>, and <code>Class/new</code></h3>
<p>Java members inherently exist in a class.  For method values we need a way to explicitly specify the class of an instance method because there is no possibility for inference.</p>
<p>Qualified methods have value semantics when used in non-invocation positions:</p>
<div>
<ul>
<li>
<p><code>Classname/method</code> - value is a Clojure function that invokes a static method</p>
</li>
<li>
<p><code>Classname/.method</code> - value is a Clojure function that invokes an instance method</p>
</li>
<li>
<p><code>Classname/new</code> - value is a Clojure function that invokes a constructor</p>
</li>
</ul>
</div>
<p>Note: developers must use <code>Classname/method</code> and <code>Classname/.method</code> syntax to differentiate between static and instance methods.</p>
<p>Qualified method invocations with <a href="https://clojure.org/news/2024/09/05/clojure-1-12-0#param_tags"><code>:param-tags</code></a> use only the tags to resolve the method. Without param-tags they behave like the equivalent <a href="https://clojure.org/reference/java_interop#_the_dot_special_form">dot syntax</a>, except the qualifying class takes precedence over hints of the target object, and over its runtime type when invoked via reflection.</p>
<p>Note: Static fields are values and should be referenced without parens unless they are intended as function calls, e.g <code>(System/out)</code> should be <code>System/out</code>. Future Clojure releases will treat the field’s value as something invokable and invoke it.</p>
</div>
<div>
<h3 id="param-tags"><a href="#param-tags"></a>2.6 :param-tags metadata</h3>
<p>When used as values, qualified methods supply only the class and method name, and thus cannot resolve overloaded methods.</p>
<p>Developers can supply <a href="https://clojure.org/reference/java_interop#paramtags"><code>:param-tags</code></a> metadata on qualified methods to specify the signature of a single desired method, 'resolving' it. The <code>:param-tags</code> metadata is a vector of zero or more tags: <code>[tag …​]</code>. A tag is any existing valid <code>:tag</code> metadata value. Each tag corresponds to a parameter in the desired signature (arity should match the number of tags). Parameters with non-overloaded types can use the placeholder <code>_</code> in lieu of the tag. When you supply :param-tags metadata on a qualified method, the metadata must allow the compiler to resolve it to a single method at compile time.</p>
<p>A new metadata reader syntax <code>^[tag …​]</code> attaches <code>:param-tags</code> metadata to member symbols, just as <code>^tag</code> attaches <code>:tag</code> metadata to a symbol.</p>
</div>
<div>
<h3 id="_2_7_array_class_syntax"><a href="#_2_7_array_class_syntax"></a>2.7 Array class syntax</h3>
<p>Clojure supports symbols naming classes both as a value (for class object) and as a type hint, but has not provided syntax for array classes other than strings.</p>
<p>Developers can now refer to an <a href="https://clojure.org/reference/java_interop#_class_access">array class</a> using a symbol of the form <code>ComponentClass/#dimensions</code>, eg <code>String/2</code> refers to the class of a 2 dimensional array of Strings. Component classes can be fully-qualified classes, imported classes, or primitives. Array class syntax can be used as both type hints and values.</p>
<p>Examples: <code>String/1</code>, <code>java.lang.String/1</code>, <code>long/2</code>.</p>
</div>
<div>
<h3 id="_2_8_functional_interfaces"><a href="#_2_8_functional_interfaces"></a>2.8 Functional interfaces</h3>
<p>Java programs emulate functions with Java functional interfaces (marked with the <a href="https://docs.oracle.com/javase/8/docs/api/java/lang/FunctionalInterface.html">@FunctionalInterface</a> annotation), which have a single method.</p>
<p>Clojure developers can now invoke Java methods taking <a href="https://clojure.org/reference/java_interop#functional_interfaces">functional interfaces</a> by passing functions with matching arity. The Clojure compiler implicitly converts Clojure functions to the required functional interface by constructing a lambda adapter. You can explicitly coerce a Clojure function to a functional interface by hinting the binding name in a <code>let</code> binding, e.g. to avoid repeated adapter construction in a loop, e.g. <code>(let [^java.util.function.Predicate p even?] …​)</code>.</p>
</div>
<div>
<h3 id="_2_9_java_supplier_interop"><a href="#_2_9_java_supplier_interop"></a>2.9 Java Supplier interop</h3>
<p>Calling methods that take a <a href="https://docs.oracle.com/javase/8/docs/api/java/util/function/Supplier.html">Supplier</a> (a method that supplies a value) had required writing an adapter with reify. Clojure has a "value supplier" interface with semantic support already - <code>IDeref</code>. All <code>IDeref</code> impls (<code>delay</code>, <code>future</code>, <code>atom</code>, etc) now implement the <code>Supplier</code> interface directly.</p>
</div>
<div>
<h3 id="_2_10_streams_with_seq_into_reduce_and_transduce_support"><a href="#_2_10_streams_with_seq_into_reduce_and_transduce_support"></a>2.10 Streams with seq, into, reduce, and transduce support</h3>
<p>Java APIs increasingly return <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html">Stream</a>s and are hard to consume because they do not implement interfaces that Clojure already supports, and hard to interop with because Clojure doesn’t directly implement Java functional interfaces.</p>
<p>In addition to functional interface support, Clojure <a href="https://clojure.org/reference/java_interop#streams">now provides these functions</a> to interoperate with streams in an idiomatic manner, all functions behave analogously to their Clojure counterparts:</p>
<div>
<ul>
<li>
<p><code>(stream-seq! stream) ⇒ seq</code></p>
</li>
<li>
<p><code>(stream-reduce! f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-transduce! xf f [init-val] stream) ⇒ val</code></p>
</li>
<li>
<p><code>(stream-into! to-coll [xf] stream) ⇒ to-coll</code></p>
</li>
</ul>
</div>
<p>All of these operations are terminal stream operations (they consume the stream).</p>
</div>
<div>
<h3 id="_2_11_persistentvector_implements_spliterable"><a href="#_2_11_persistentvector_implements_spliterable"></a>2.11 PersistentVector implements Spliterable</h3>
<p>Java collections implement streams via <a href="https://docs.oracle.com/javase/8/docs/api/java/util/Spliterator.html">"spliterators"</a>, iterators that can be split for faster parallel traversal. <code>PersistentVector</code> now provides a custom spliterator that supports parallelism, with greatly improved performance.</p>
</div>
<div>
<h3 id="_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"><a href="#_2_12_efficient_drop_and_partition_for_persistent_or_algorithmic_collections"></a>2.12 Efficient drop and partition for persistent or algorithmic collections</h3>
<p>Partitioning of a collection uses a series of takes (to build a partition) and drops (to skip past that partition). <a href="https://clojure.atlassian.net/browse/CLJ-2713">CLJ-2713</a> adds a new internal interface (IDrop) indicating that a collection can drop more efficiently than sequential traversal, and implements that for persistent collections and algorithmic collections like <code>range</code> and <code>repeat</code>. These optimizations are used in <code>drop</code>, <code>nthrest</code>, and <code>nthnext</code>.</p>
<p>Additionally, there are new functions <code>partitionv</code>, <code>partitionv-all</code>, and <code>splitv-at</code> that are more efficient than their existing counterparts and produce vector partitions instead of realized seq partitions.</p>
</div>
<div>
<h3 id="_2_13_var_interning_policy"><a href="#_2_13_var_interning_policy"></a>2.13 Var interning policy</h3>
<p><a href="https://clojure.org/reference/vars#interning">Interning</a> a var in a namespace (vs aliasing) must create a stable reference that is never displaced, so that all references to an interned var get the same object. There were some cases where interned vars could get displaced and those have been tightened up in 1.12.0-alpha1. If you encounter this situation, you’ll see a warning like "REJECTED: attempt to replace interned var #'some-ns/foo with #'other-ns/foo in some-ns, you must ns-unmap first".</p>
<p>This addresses the root cause of an issue encountered with Clojure 1.11.0, which added new functions to clojure.core (particularly <code>abs</code>). Compiled code from an earlier version of Clojure with var names that matched the newly added functions in clojure.core would be unbound when loaded in a 1.11.0 runtime. In addition to <a href="https://clojure.atlassian.net/browse/CLJ-2711">CLJ-2711</a>, we rolled back a previous fix in this area (<a href="https://clojure.atlassian.net/browse/CLJ-1604">CLJ-1604</a>).</p>
</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Common food dye found to make skin and muscle temporarily transparent (295 pts)]]></title>
            <link>https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</link>
            <guid>41459865</guid>
            <pubDate>Thu, 05 Sep 2024 19:48:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent">https://www.theguardian.com/science/article/2024/sep/05/common-food-dye-found-to-make-skin-and-muscle-temporarily-transparent</a>, See on <a href="https://news.ycombinator.com/item?id=41459865">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Researchers have peered into the brains and bodies of living animals after discovering that a common food dye can make skin, muscle and connective tissues temporarily transparent.</p><p>Applying the dye to the belly of a mouse made its liver, intestines and bladder clearly visible through the abdominal skin, while smearing it on the rodent’s scalp allowed scientists to see blood vessels in the animal’s brain.</p><p>Treated skin regained its normal colour when the dye was washed off, according to researchers at Stanford University, who believe the procedure opens up a host of applications in humans, from locating injuries and finding veins for drawing blood to monitoring digestive disorders and spotting tumours.</p><p>“Instead of relying on invasive biopsies, doctors might be able to diagnose deep-seated tumours by simply examining a person’s tissue without the need for invasive surgical removal,” said Dr Guosong Hong, a senior researcher on the project. “This technique could potentially make blood draws less painful by helping phlebotomists easily locate veins under the skin.”</p><p>The trick has echoes of the approach taken by Griffin in HG Wells’s 1897 novel, The Invisible Man, in which the brilliant but doomed scientist discovers that the secret to invisibility lies in matching an object’s refractive index, or ability to bend light, to that of the surrounding air.</p><p>When light penetrates biological tissue, much of it is scattered because the structures inside, such as fatty membranes and cell nuclei, have different refractive indices. As light moves from one refractive index to another, it bends, making tissue opaque. The same effect makes a pencil look bent when dropped in a glass of water.</p><p>Dr Zihao Ou and his colleagues at Stanford theorised, counterintuitively, that particular dyes could make certain wavelengths of light pass more easily through skin and other tissues. Strongly absorbing dyes alter the refractive index of tissues that absorb them, allowing scientists to match the refractive indices of different tissues and suppress any scattering.</p><figure id="9530018f-bade-4b02-a276-7aa89115801e" data-spacefinder-role="inline" data-spacefinder-type="model.dotcomrendering.pageElements.ImageBlockElement"><div id="img-1"><picture><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=2&amp;s=none" media="(min-width: 660px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 660px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=620&amp;dpr=1&amp;s=none" media="(min-width: 660px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=2&amp;s=none" media="(min-width: 480px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 480px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=605&amp;dpr=1&amp;s=none" media="(min-width: 480px)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=2&amp;s=none" media="(min-width: 320px) and (-webkit-min-device-pixel-ratio: 1.25), (min-width: 320px) and (min-resolution: 120dpi)"><source srcset="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" media="(min-width: 320px)"><img alt="Before and after images of the use of the dye on a rodent. In the second image the internal images can be seen in red." src="https://i.guim.co.uk/img/media/09c9e23bd620561756b5770547e2c9e224a7f790/0_0_3306_2100/master/3306.png?width=445&amp;dpr=1&amp;s=none" width="445" height="282.6678765880218" loading="lazy"></picture></div><figcaption data-spacefinder-role="inline"><span><svg width="18" height="13" viewBox="0 0 18 13"><path d="M18 3.5v8l-1.5 1.5h-15l-1.5-1.5v-8l1.5-1.5h3.5l2-2h4l2 2h3.5l1.5 1.5zm-9 7.5c1.9 0 3.5-1.6 3.5-3.5s-1.6-3.5-3.5-3.5-3.5 1.6-3.5 3.5 1.6 3.5 3.5 3.5z"></path></svg></span><span>Before and after images of the use of the dye on a rodent.</span> Photograph: handout</figcaption></figure><p>In a series of experiments <a href="https://doi.org/10.1126/science.adm6869" data-link-name="in body link">described in Science</a>, the researchers show how a fresh chicken breast became transparent to red light minutes after being immersed in tartrazine solution, a yellow food dye used in US Doritos, SunnyD drink and other products. The dye reduced light scattering inside the tissue, allowing the rays to penetrate more deeply.</p><p>The team then smeared the yellow dye on a mouse’s underbelly, making the abdominal skin see-through and revealing the rodent’s intestines and organs. In another experiment, they applied dye to a mouse’s shaved head and, with a technique called laser speckle contrast imaging, saw blood vessels in the animal’s brain.</p><p>“The most surprising part of this study is that we usually expect dye molecules to make things less transparent. For example, if you mix blue pen ink in water, the more ink you add, the less light can pass through the water,” Hong said. “In our experiment, when we dissolve tartrazine in an opaque material like muscle or skin, which normally scatters light, the more tartrazine we add, the clearer the material becomes. But only in the red part of the light spectrum. This goes against what we typically expect with dyes.”</p><p>The researchers describe the process as “reversible and repeatable”, with skin reverting to its natural colour once the dye is washed away. At the moment, transparency is limited to the depth the dye penetrates, but Hong said microneedle patches or injections could deliver the dye more deeply.</p><p>The procedure has not yet been tested on humans and researchers will need to show it is safe to use, particularly if the dye is injected beneath the skin.</p><p>Others stand to benefit from the breakthrough. Many scientists study naturally transparent animals, such as zebrafish, to see how organs and features of disease, such as cancer, develop in living creatures. With transparency dyes, a much wider range of animals could be studied in this way.</p><p>In an <a href="https://doi.org/10.1126/science.adr7935" data-link-name="in body link">accompanying article</a>, Christopher Rowlands and Jon Gorecki, of Imperial College London, say there will be “extremely broad interest” in the procedure, which, when combined with modern imaging techniques, could allow scientists to image an entire mouse brain or spot tumours beneath centimetre-thick tissues. “HG Wells, who studied biology under TH Huxley, as a student would surely approve,” they write.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Reflection 70B, the top open-source model (195 pts)]]></title>
            <link>https://twitter.com/mattshumer_/status/1831767014341538166</link>
            <guid>41459781</guid>
            <pubDate>Thu, 05 Sep 2024 19:39:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/mattshumer_/status/1831767014341538166">https://twitter.com/mattshumer_/status/1831767014341538166</a>, See on <a href="https://news.ycombinator.com/item?id=41459781">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[My job is to watch dreams die (2011) (307 pts)]]></title>
            <link>https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</link>
            <guid>41459365</guid>
            <pubDate>Thu, 05 Sep 2024 18:43:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/">https://old.reddit.com/r/reddit.com/comments/k3zrz/by_request_from_the_jobs_thread_why_my_job_is_to/</a>, See on <a href="https://news.ycombinator.com/item?id=41459365">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p><a href="http://www.reddit.com/r/AskReddit/comments/k3ifx/whats_the_most_fucked_up_thing_youve_had_to_do_at/c2ha0il">Original post here</a>.</p>

<p>I work at a real estate office.  We primarily sell houses that were foreclosed on by lenders.  We aren't involved in the actual foreclosures or evictions - anonymous lawyers in the cloud somewhere is tasked with the paperwork - we are the boots on the ground that interacts with the actual walls, roofs and occasional bomb threat.</p>

<p>When the lender forecloses - or is thinking of foreclosing - on a property one of the first things that happens is they send somebody out to see if there is actually a house there and if there is anybody living there who needs to be evicted.  Lawyers are expensive so they send a real estate agent or a property preservation company out to check.  There is the occasional discovery of fraud where there was never a house on the parcel to begin with, but such instances are rare.  Sometimes this initial visit results in discovering a house that has burned down or demolished, is abandoned or occupied by somebody who has absolutely no connection with the homeowner.  Sometimes the houses are discovered to be crack dens or meth labs, sometimes the sites of cock or dog fighting operations, or you might even find a back yard filled with a pot cultivation that can't be traced back to anybody because it was planted in yet another vacant house in a blighted neighborhood.  The house could be worth less than zero - blighted to the point where you can't even give it away (this is a literal statement, I have tried to give away many houses or even vacant lots with no takers over the years) or it could be a waterfront mansion in a gated golf community worth well over seven figures that does not include the number "one".    Sometimes they are found to have been seized by the IRS, the local tax authority, the DEA or the US Marshal.  Variety is the rule.  The end results are the law.</p>

<p>If the house is occupied my job is to make contact and determine who they are: there are laws that establish what happens to a borrower as opposed to a tenant and the servicemember relief act adds an additional set of questions that must be answered. Some of the people have an idea of why I am there.  Some claim they never knew they were foreclosed on, or tell me that they have worked something out with their lender, some won't tell me a thing and some threaten me to never return in the name of the police, their lawyer, or the occasional "or else/if I were you".  During one initial visit the sight of 50-60 motorcycles parked on the lawn suggested that we try again the next day.  At a couple the police had cordoned off the area and at one they were in the process of dredging the lake searching for the body of a depressed former homeowner.</p>

<p>If nobody is home I have to determine if they are at work, on vacation, in the army, wintering/summering at their other home, in jail, in a nursing home, dead or if they moved away.  It isn't easy.  Utilities can be left on for months.  Neighbors can be staging the yard and house to appear occupied to prevent blight in their neighborhood.  By the same token people will stop cutting the lawn for months, let trash and old phone books pile up on their porch, lose gas and electric service and continue to live in properties that have not only physically unsafe to approach but are so filthy that when it comes time to clean them out the crews have to wear hazmat suits.  One house had a gallon pickle jar filled with dead roaches on the porch.  Somebody lived in that house and thought that was a logical thing to do.  People like me are tasked with first contact.</p>

<p>Evictions are expensive and time-consuming.  Ultimately once the process gets that far there isn't much that can be done to prevent it.  You didn't pay your mortgage, the lender gets the house back.  There are an infinite number of reasons why the mortgage couldn't be paid, some are more sympathetic than others, but in the end you will be leaving the property willingly or not.  The lawyers handle the evictions - they churn through the paperwork in the background, ten thousand properties at a time.  They have it down to rote function based on templates, personal experience with the various judges and intimate knowledge of the federal, state and municipal laws, along with dealing with the occasional sheriff who refuses to evict somebody, the informal policies established by the local judges and a myriad of other problems that can arise.  As a business decision many lenders have determined that it is cheaper to settle with the occupants - instead of going through the formal eviction they will offer cash.  In exchange for surrendering a property in reasonably clean condition with the furnace still hooked up, the kitchen not stripped and the basement not intentionally flooded the lender will cut the occupants a check.  It costs much less than an eviction, provides reasonable hope that the plumbing won't freeze and can take a fraction of the time to obtain possession.  This is where the personal element becomes real.</p>

<p>(Continued in comments)</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[UE5 Nanite in WebGPU (358 pts)]]></title>
            <link>https://github.com/Scthe/nanite-webgpu</link>
            <guid>41458987</guid>
            <pubDate>Thu, 05 Sep 2024 17:55:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Scthe/nanite-webgpu">https://github.com/Scthe/nanite-webgpu</a>, See on <a href="https://news.ycombinator.com/item?id=41458987">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Nanite WebGPU</h2><a id="user-content-nanite-webgpu" aria-label="Permalink: Nanite WebGPU" href="#nanite-webgpu"></a></p>
<blockquote>
<p dir="auto">TL;DR: <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Demo scene Jinx</a> (640m triangles). <a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene with many objects</a> (1.7b triangles). White triangles in the Jinx scene are software-rasterized. <strong>WebGPU is only available on Chrome!</strong></p>
</blockquote>
<p dir="auto">This project contains a <a href="https://youtu.be/qC5KtatMcUw?si=IOWaVk0sQNra_R6O&amp;t=97" rel="nofollow">Nanite</a> implementation in a web browser using WebGPU. This includes the meshlet LOD hierarchy, software rasterizer (at least as far as possible given the current state of WGSL), and billboard impostors. Culling on both per-instance and per-meshlet basis (frustum and occlusion culling in both cases). Supports textures and per-vertex normals. Possibly every statistic you can think of. There is a slider or a checkbox for every setting imaginable. Also works offline using Deno.</p>
<p dir="auto">First, we will see some screenshots, then there is (not even complete) list of features. Afterward, I will link you to a couple of <strong>demo scenes</strong> you can play with. In the FAQ section, you can read <strong>my thoughts about Nanite</strong>. Since this file got a bit long, I've moved usability-oriented stuff (stats/GUI explanation, build process, and unit test setup) into a separate <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>.</p>
<blockquote>
<p dir="auto">EDIT 16-08-2024: I've rewritten significant parts of this README once I had more time to look through it. And I've written <a href="https://github.com/Scthe/frostbitten-hair-webgpu">Frostbitten hair WebGPU</a> meantime #self-promo.</p>
</blockquote>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY"><img src="https://private-user-images.githubusercontent.com/9325337/348849467-ef4c8476-bf30-4241-96d0-a354efa0dea1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQ2Ny1lZjRjODQ3Ni1iZjMwLTQyNDEtOTZkMC1hMzU0ZWZhMGRlYTEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZWExMWU3NDVkN2JlZjA2MDU0YWFhMDEzNWY4MWFhMWRmNjhkNTBmZmY0ODE0MDlhMTI5ZjAwMDFmMDg3YjhhZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.G4mQjdsP12YTNSLhVg3WVUyvLgm6bQxd27pe0lwi1lY" alt="scene-multiobject"></a></p>
<p dir="auto"><em><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Sample scene</a> containing 1.7b triangles. Nearly 98% of the triangles are software rasterized, as it's much faster than hardware.</em></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo"><img src="https://private-user-images.githubusercontent.com/9325337/348849421-4eef5e85-03dd-43f3-9a99-afbce59407f0.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTQyMS00ZWVmNWU4NS0wM2RkLTQzZjMtOWE5OS1hZmJjZTU5NDA3ZjAucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MTRhY2I1NDQyZjNhMjBkYjAxMGFiNGQ1MmQ5MjcyMzY2ZmVlMmQxNjc3ZThhMGUyNzQ0ZjNjNzRiOTg3NDg0ZSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.xmvp4efV5JdJ_c1xPLWh7WbcIwRYrfRG5BNjHhOoCCo" alt="scene-jinx"></a></p>
<p dir="auto"><em>My <a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">primary test scene</a>. <a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Unfortunately, the best simplification we get is from 44k to 3k triangles. The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress the data to fit into 32 bits (u16 for depth, 2*u8 for octahedron-encoded normals). It's a painful limitation, but at least you can see the entire system is working.</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>Nanite
<ul dir="auto">
<li><strong>Meshlet LOD hierarchy.</strong>
<ul dir="auto">
<li>Mesh preprocessing executes in the browser, using WebAssembly for <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>. While it might raise eyebrows, this was one of the goals.</li>
<li>There is a file exporter too, if you don't like to wait between page refreshes.</li>
</ul>
</li>
<li><strong>Software rasterizer.</strong>
<ul dir="auto">
<li>WebGPU does not have the <code>atomic&lt;u64&gt;</code> needed to implement this feature efficiently. Currently, I'm packing depth (<code>u16</code>) and octahedron-encoded normals (<code>2 * u8</code>) into 32 bits. It's enough to show that the rasterizer works.</li>
<li>With only 32 bits, we are butchering the precision. My only concern here is to show that the rasterization works. If you see the software rasterized bunny model in the background it will be white and it will have <em>reasonable</em> shading. Reprojecting depth and "compressing" normals is enough to get something.. not offending.</li>
<li>This also affects the depth pyramid used for occlusion culling.</li>
<li>There are other algorithms to do this. PPLL, or something with tiles, or double rasterization (1st pass writes depth, 2nd does compareExchange). But the 32-bit limitation is only in WebGPU, so I choose to stick to UE5's solution instead.</li>
</ul>
</li>
<li><strong>Billboard impostors.</strong> 12 images around the UP-axis, blended (with dithering) based on the camera position. Does not handle up/down views. Contains both diffuse and normals, so we can do nice shading at a runtime. UE5 <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=97" rel="nofollow">uses</a> a more advanced version integrated with a visibility buffer.
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview demo scene</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
</li>
</ul>
</li>
<li>Culling:
<ul dir="auto">
<li><strong>Per-instance:</strong> frustum and occlusion culling.</li>
<li><strong>Per-meshlet:</strong> frustum and occlusion culling.</li>
<li><strong>Per-triangle:</strong> hardware backface culling and ofc. z-buffer. WebGPU does not have early-z.
<ul dir="auto">
<li>I have no idea how early-z works in WebGPU (it does not).</li>
</ul>
</li>
<li>I've also tried per-meshlet backface cone culling. It worked fine, but I cut it from the final release. See FAQ below for more details.</li>
<li>Occlusion culling is just a depth pyramid from the previous frame's depth buffer. No reprojection and no two-pass. The current implementation is enough to cull a lot of triangles (<strong>A LOT!</strong>) and to judge the performance impact (big improvement!). I expect someone will want to read the code, and they will be grateful this feature was not added.</li>
</ul>
</li>
<li>Switch between <strong>GPU-driven rendering</strong> and a <strong>naive CPU implementation</strong>. I have not spent much time optimizing the CPU version. It works, you can step through it with the debugger.</li>
<li>Supports <strong>textured models</strong> and <strong>many different objects</strong> at the same time.</li>
<li>Controls to <strong>change parameters at runtime</strong>. Debug views. "Freeze culling" allows the camera to move and inspect only what was drawn last frame.</li>
<li>A lot of <strong>stats</strong>. Memory, geometry. Total scene meshlets, triangles. Drawn meshlets, triangles (split between hardware and software rasterizer). Impostor count. Dedicated profiler button to get the timings.</li>
<li><strong>Custom file format</strong> so you don't have to preprocess the mesh every time. This is optional, you <strong>can also use an OBJ file</strong>.</li>
<li>Vertex <strong>position quantization</strong> (vec2u), <strong>octahedron encoded normals</strong> (vec2f).
<ul dir="auto">
<li>Position quantization is off by default. Toggle <code>CONFIG.useVertexQuantization</code> to enable. There are <em>funny</em> things happening to the numbers there, but everything <em>should</em> be handled correctly.</li>
</ul>
</li>
<li>Handles window resize. It's a web browser after all.</li>
<li>The whole app also <strong>runs offline in <a href="https://deno.com/" rel="nofollow">Deno</a></strong>. I've written shader unit tests this way.</li>
<li>Tons of WebGPU and WGSL code that you can copy to your own project. If you want to do something, I've either attempted to do it or discovered that it does not work.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Goals</h3><a id="user-content-goals" aria-label="Permalink: Goals" href="#goals"></a></p>
<p dir="auto">There were 2 primary goals for this project:</p>
<ol dir="auto">
<li><strong>Simplicity.</strong> We start with an OBJ file and everything is done in the app. No magic pre-processing steps, Blender exports, etc. You set the breakpoint at <code>loadObjFile()</code> and F10 your way till the first frame finishes.</li>
<li><strong>Experimentation.</strong> I could have built this with Vulkan and Rust. None would touch it. Instead, it's a webpage. You click the link, uncheck the checkbox and the FPS tanks 40%. And you think to yourself: "OK, that was an important checkbox. But what about this slider?". Or: "How does scene X affect memory allocation?". Right now I know that a lot of code can be optimized. Yet it would not matter till the simplification problem is solved.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Demo Scenes</h2><a id="user-content-demo-scenes" aria-label="Permalink: Demo Scenes" href="#demo-scenes"></a></p>
<ul dir="auto">
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1" rel="nofollow">Jinx</a> (120*120 instances, 640m triangles). A single Jinx is 44k triangles simplified to 3k at 59 root meshlets. Uses an OBJ file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=manyObjects2&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=1.0" rel="nofollow">Lucy and dragons</a> (both objects at 70*70 instances, 1.7b triangles). See below for per-object details.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=lucy1b&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Lucy</a> (110*110 instances, 1.2b triangles). A single Lucy statue is 100k triangles simplified to 86 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=dragonJson&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.5" rel="nofollow">Dragons</a> (70*70 instances, 1.2b triangles). A single dragon is 250k triangles simplified to 102 at a single root meshlet. Uses binary file.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=bunny1b&amp;impostors_threshold=1000&amp;softwarerasterizer_threshold=2400" rel="nofollow">Bunnies</a> (500*500 instances, 1.2b triangles). A single bunny is 5k triangles simplified to 96 at a single root meshlet. Uses an OBJ file. Bunnies are so small most are frustum culled.</li>
<li><a href="https://scthe.github.io/nanite-webgpu/?scene_file=jinxCombined&amp;impostors_threshold=4000&amp;softwarerasterizer_threshold=1360&amp;nanite_errorthreshold=0.1&amp;impostors_forceonlybillboards&amp;impostors_texturesize=512" rel="nofollow">Impostors preview</a>. For this demo, I've increased the impostor texture size. This way you can see more details.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">You can find details in <a href="https://github.com/Scthe/nanite-webgpu/blob/master/USAGE.md">USAGE.md</a>. Short version:</p>
<ul dir="auto">
<li>Use the <code>[W, S, A, D]</code> keys to move and <code>[Z, SPACEBAR]</code> to fly up or down. <code>[Shift]</code> to move faster.</li>
<li>If there is something weird, toggle culling options on/off. There are some minor bugs in the implementation.</li>
<li>The white triangles are software-rasterized triangles (between hardware-rasterized ones and the impostors in the far back). WebGPU does not support <code>atomic&lt;u64&gt;</code>, so I had to compress data to fit into 32 bits (u16 for depth, 2*u8 for octahedron encoded normals).
<ul dir="auto">
<li>16-bit depth is.. not a great idea. It produces <strong>tons</strong> of artifacts like z-fighting or leaks. Turn the software rasterizer off to easier inspect raw Nanite meshlets. Be prepared for a major performance hit!</li>
</ul>
</li>
<li>FPS might fluctuate due to the browser's enforced VSync. Use the "Profile" button instead.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">FAQ</h2><a id="user-content-faq" aria-label="Permalink: FAQ" href="#faq"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What are the major differences compared to UE5's Nanite?</h3><a id="user-content-what-are-the-major-differences-compared-to-ue5s-nanite" aria-label="Permalink: What are the major differences compared to UE5's Nanite?" href="#what-are-the-major-differences-compared-to-ue5s-nanite"></a></p>
<ul dir="auto">
<li>Error metric is just a simple projected simplification error (read below).</li>
<li>Meshlet simplification is.. simplistic.</li>
<li>No two-pass occlusion culling.
<ul dir="auto">
<li>This would not be complicated to add, just tedious to debug. Unfortunately, it also has some interactions with the GUI settings. ATM some parts of the code are riddled with <code>ifs</code> for certain user settings. For example, you could press "Freeze culling" to stop updating the list of drawn meshlets. This includes software rasterized meshlets. Move the camera in this mode and all 10+ million 1 px-sized software rasterized triangles might become fullscreen. Adding two-pass occlusion culling might expose more such interactions. It would also make the code harder to read, which goes against my goals.</li>
</ul>
</li>
<li>No work queue in shaders. For meshlet culling and LOD selection, I dispatch thread per-meshlet.</li>
<li>No VRAM eviction of unused LODs and streaming.
<ul dir="auto">
<li>Theoretically, to load new meshlet data, you would write requested <code>meshletIds</code> into a separate GPUBuffer. Download it to RAM and load the content. Keep LRU (timestamp per-meshlet, visible from CPU) to manage evictions. In practice, I suspect you might also want to add a priority system.</li>
</ul>
</li>
<li>No visibility buffer. It's not possible with the <code>atomic&lt;u64&gt;</code> limitation that I have.
<ul dir="auto">
<li>BTW if you render material data into a GBuffer, you get Nanite integration with your material system for free.</li>
</ul>
</li>
<li>No built-in shadows/multiview.</li>
<li>My implementation focuses on using a predictable amount of memory for demo cases. This means it's not scalable if you have many <strong>different</strong> objects (not instances). You would have to know the upper bound of the drawn meshlets to preallocate buffers that hold data between the stages. The naive solutions like <code>bottomLevelMeshletsCount * instanceCount</code> easily end up in GBs of VRAM!</li>
<li>No BVH for instances (or any other hierarchical implementation). I just take all instances and frustum + occlusion cull them.</li>
<li>I don't have a GPU profiler on the web/Deno. Or a debugger, or printf for that matter.
<ul dir="auto">
<li>ITWouldGenerate_DX_CODE_THATIWOULDHAVE_TO_READ_ANYWAY_SONOiGUESS.</li>
</ul>
</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Does xxx billions of triangles mean anything?</h3><a id="user-content-does-xxx-billions-of-triangles-mean-anything" aria-label="Permalink: Does xxx billions of triangles mean anything?" href="#does-xxx-billions-of-triangles-mean-anything"></a></p>
<p dir="auto">There was a video on YouTube showing how Nanite handles 120 billion triangles. Yet most of them were frustum culled? Performance depends on a lot of factors.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Dense meshes</h4><a id="user-content-dense-meshes" aria-label="Permalink: Dense meshes" href="#dense-meshes"></a></p>
<p dir="auto">Having a lot of dense meshes up close could have a negative performance impact. Unless you are so close to them that they cover 50% of the screen. Then, the occlusion culling kicks in. Dense geometry also means that meshlets are small. 128 triangles in a 20,000,000 triangle mesh? They do not take much space on the screen and are easily occlusion/cone culled.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Instance count</h4><a id="user-content-instance-count" aria-label="Permalink: Instance count" href="#instance-count"></a></p>
<p dir="auto">What about millions of instances? Each has its own mat4x3 transform matrix. This consumes VRAM. Obligatory link to <a href="https://pharr.org/matt/blog/2018/07/09/moana-island-pbrt-2" rel="nofollow">swallowing the elephant (part 2)</a>. During the frame, you also need to store a list of things to render. In the worst-case scenario, each instance will render its most dense meshlets. In my implementation, this allocates <code>instanceCount * bottomLevelMeshletsCount * sizeof(vec2u)</code> bytes. A 5k triangle bunny might have only 56 fine-level meshlets (out of 159 total), but what if I want to render 100,000 of them? This is not a scalable memory allocation. In Chrome, WebGPU has a 128MB limit for storage buffers (can be raised if needed). You might notice that the demo scenes above were tuned to reflect that.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Scene arrangement</h4><a id="user-content-scene-arrangement" aria-label="Permalink: Scene arrangement" href="#scene-arrangement"></a></p>
<p dir="auto">The scenes in my app have objects arranged in a square. For far objects, only a small part will be visible. But they will use coarse meshlet LOD, that contains more than just a visible part. The visible part passes occlusion culling and leads to a lot of overdraw for the rest of the meshlet. This is not an optimal scene arrangement. You would also think that a dense grid placement (objects close to each other) is bad. It certainly renders more triangles close to the camera. But it also means that there are no huge differences in depth between them. This is a dream for occlusion culling. You could build a wall from high-poly meshes and it's actually one of the most performant scenarios. Objects far from each other mean that a random distant pixel pollutes the depth pyramid (the <a href="https://www.youtube.com/@MentourPilot/videos" rel="nofollow">Swiss cheese theory</a>). Does your scene have a floor? Can you <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=96" rel="nofollow">merge</a> far objects into one?</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">In practice</h4><a id="user-content-in-practice" aria-label="Permalink: In practice" href="#in-practice"></a></p>
<p dir="auto">This leads to the <strong>Jinx test scene</strong>. The character is skinny. Looking down each row/column of the grid you can see the gaps. There is space between her arm and torso. This kills occlusion culling. The model does not simplify well. 3k triangles in the most coarse LOD (see below for more details). It's death by thousands of 1-pixel triangles. Software rasterizer helps a lot. Yet given the scene arrangement, most of the instances are rendered as impostors. Up close, the hardware rasterizer takes over. All 3 systems have different strengths.</p>
<blockquote>
<p dir="auto">With UE5's simplification algorithm, the balance is probably shifted. Much more software rasterizer, and less hardware one. And I wager a bet they don't have to rely on impostors as much. Their coarse LOD would be less than 3k tris (again, see below).</p>
</blockquote>
<p dir="auto">Basically, there are a lot of use cases. If you want a <strong>stable</strong> Nanite implementation, you have to test each one. But if you want a big triangle count, there are ways to cheat that too.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What surprised you about Nanite?</h3><a id="user-content-what-surprised-you-about-nanite" aria-label="Permalink: What surprised you about Nanite?" href="#what-surprised-you-about-nanite"></a></p>
<ol dir="auto">
<li>The goal of the DAG is not to "use fewer triangles for far objects". The goal is to have a consistent 1 pixel == 1 triangle across the entire screen. A triangle is our "unit of data". The artist imports a sculpture from ZBrush. We need to need a way (through an error metric) to display it no matter if it's 1m or 500m from the camera. This is not possible with discrete LOD meshes (each LOD level is a separate geometry). Sometimes you would want an LOD between 2 levels. You need continuous LODs. This is the reason for the meshlet hierarchy. It allows you to "sample" geometry at any detail level you choose.</li>
<li>You spend more time working on culling and meshlets instead of Nanite itself. You <strong>WILL</strong> reimplement both <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a>.</li>
<li>Meshlet LOD hierarchy is quite easy to get working. Praise <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a> and <a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>! But if you want to do it efficiently, <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=50" rel="nofollow">it will be a pain</a>. See next question for full story. I just went with the simplest option.</li>
<li>If your mesh does not simplify cleanly, you end up with e.g. ~3000 triangles that cover a single pixel (Jinx scene). The efficiency scales with your mesh simplification code. And if you want pixel-sized triangles (the main selling point for most people), you <strong>need</strong> a software rasterizer. The billboard impostors are also a good stability-oriented fallback. As mentioned above, the whole system should work cohesively.</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about mesh simplification?</h3><a id="user-content-what-about-mesh-simplification" aria-label="Permalink: What about mesh simplification?" href="#what-about-mesh-simplification"></a></p>
<blockquote>
<p dir="auto">Remember, we are not doing a simple "take a mesh and return something that has X% of the triangles". We are doing the simplification in the context of meshlets and METIS.</p>
</blockquote>
<p dir="auto">UE5 has its own mesh simplification code. It's the first thing that happens in the asset pipeline. Thus, everything saved here will have avalanche-like benefits for the rest of the system. It was also a problem with the Jinx model. On <a href="https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf#page=95" rel="nofollow">slide 95</a> Brian Karis states that <strong>all</strong> their LOD graphs end at a <strong>single</strong> root cluster. So no matter the model you provide, they can simplify it to 128 triangles. It makes you less reliant on the impostors. In my app, I could e.g. increase meshoptimizer's <code>target_error</code> parameter. But consider the following story:</p>
<ol dir="auto">
<li>My first test model was a bunny with 5k triangles. Easy to debug (check for holes, etc.). It simplified into a single 128 tris meshlet. Nice!</li>
<li>I've tried to load the Jinx model. At some point, the simplification stopped. You gave it X triangles and received the same X triangles. This crashed my app on an assertion.</li>
<li>OK, so if the model does not simplify beyond some level, I will allow the DAG to have many roots. If you failed to remove at least 6+% of the triangles, stop the algorithm for this part of the mesh.</li>
<li>The Jinx model now works correctly. It stops simplifying beyond 7-9 LOD levels, but this only means there are many hierarchy roots.</li>
<li>I load the bunny again and it no longer simplifies to a single root meshlet. Turns out, <strong>a lot of the meshlets did not reduce triangles that much</strong>. But with enough iterations, for such a simple model, we were able to reduce it to only 128 triangles. The whole time we were getting the &lt;6% simplification for some meshlets (so 94% of triangles were left untouched). We just did not know about it. And a lot of meshlets were also not "full". They contained less than 128 triangles.</li>
</ol>
<blockquote>
<p dir="auto">To reproduce, use <code>const SCENE_FILE: SceneName = 'singleBunny';</code> and set <code>CONFIG.nanite.preprocess.simplificationFactorRequirement: 0.94</code>. This option requires triangle reduction by at least 6%. We end up with 512 triangles. Then, set <code>simplificationFactorRequirement: 0.97</code> (require reducing triangle count by at least 3%, which is much more lenient). You end up with a single root that has 116 tris.</p>
</blockquote>
<p dir="auto">It was my first time using meshoptimizer, so you can probably tune it better. In the offline setting, it's possible to retry simplification with a bigger <code>target_error</code>. Or increase <code>target_error</code> for more coarse meshlet levels? From my experiments, both of these changes do not matter. You could also allow the hierarchy to have the bottom children on different levels (probably? there are some issues with this approach e.g. non-uniform mesh density). Maybe generate conservative (with a bigger triangle count than usual), discrete LOD levels in an old way and then use them if the algorithm gets stuck? This makes the error metric and the entire hierarchy pointless. Introduce new custom vertices? Merge more meshlets than 4? Smaller meshlets? Replace meshoptimizer? UE5 also has special weights for METIS partitioning. <strong>Most important, can your (METIS-enchanced) simplification, guarantee that splitting 256 triangles into 128 triangles, will ALWAYS result in 128 triangles?</strong> I think that once you have this guarantee, the simplification (while still not trivial), is significantly easier. With it, you no longer have to think about the concept of triangles in your meshlet hierarchy. You can start thinking only about DAG and nodes. This highlights the need for goor bottom-level meshlets.</p>
<p dir="auto">You may need someone to dedicate their time only to simplification. Personally, I just got it to work and moved on.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk"><img src="https://private-user-images.githubusercontent.com/9325337/348849351-157866eb-88e9-4896-895a-9400915478a4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDEsIm5iZiI6MTcyNTU2NDYwMSwicGF0aCI6Ii85MzI1MzM3LzM0ODg0OTM1MS0xNTc4NjZlYi04OGU5LTQ4OTYtODk1YS05NDAwOTE1NDc4YTQucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDkwNSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA5MDVUMTkzMDAxWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NmY5NjI3ZTVmNjQwMWNmMmVkNDQ5MGYzOWQxMzU5ZmFjN2RlODMwODg1ZDAxOTA5N2E3YWJjMzhmYWUwMjg2NSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.DRI0B0vfRRt63fIazOdKQ7-fYXvIZLESQLA9VzGgkNk" alt="simplification"></a></p>
<p dir="auto"><em>Trying to Nanite-simplify <a href="https://sketchfab.com/3d-models/modular-mecha-doll-neon-mask-1e0dcf3e016f4bc897d4b39819220732" rel="nofollow">Modular Mecha Doll Neon Mask</a> (910k tris) 3D model by Sketchfab user <a href="https://sketchfab.com/chambersu1996" rel="nofollow">Chambersu1996</a>. After the 5th hierarchy level, the simplification stops with 180k triangles left. This would be inefficient to render, but still manageable if we switched to impostors <strong>quickly</strong>. A better solution would be to actually spend X hours investigating the simplification process.</em></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What about error metric?</h3><a id="user-content-what-about-error-metric" aria-label="Permalink: What about error metric?" href="#what-about-error-metric"></a></p>
<p dir="auto">Assume you have a mesh that has 20,000,000 triangles. With meshlet hierarchy, you can render it at any triangle count you would have wanted (with a minimum of 128 triangles - 1 meshlet). How do you choose the right meshlets? What does the <em>right meshlet</em> mean? At the end of the day, <strong>THIS</strong> is exactly what Nanite is. Everything else (simplification, meshlet DAG, software rasterizer, etc.) is just a prerequisite to actually start working on this problem. I admit, as the author of this repo, it's a bit disheartening.</p>
<p dir="auto">A few days ago, SIGGRAPH 2024 presentations were published. In <a href="https://advances.realtimerendering.com/s2024/content/Cao-NanoMesh/AdavanceRealtimeRendering_NanoMesh0810.pdf" rel="nofollow">"Seamless rendering on mobile"</a>, Shun Cao from Tencent Games provided the following metric (slide 12):</p>
<div dir="auto" data-snippet-clipboard-copy-content="device_factor = device_power * device_level
// from the slightly blurred graph image it seems to be:
// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000
decay_factor = 1 / (1 + exp(distance_to_view / decay_distance))
threshold = projected_area * device_factor * decay_factor"><pre><span>device_factor</span> <span>=</span> <span>device_power</span> <span>*</span> <span>device_level</span>
<span>// from the slightly blurred graph image it seems to be:</span>
<span>// decay_factor(x) = 1 / (1 + exp(-(x-5000) / 1000)) from 0 to 9000</span>
<span>decay_factor</span> <span>=</span> <span>1</span> <span>/</span> <span>(</span><span>1</span> <span>+</span> <span>exp</span><span>(</span><span>distance_to_view</span> <span>/</span> <span>decay_distance</span><span>)</span><span>)</span>
<span>threshold</span> <span>=</span> <span>projected_area</span> <span>*</span> <span>device_factor</span> <span>*</span> <span>decay_factor</span></pre></div>
<p dir="auto"><em><a href="https://www.wolframalpha.com/input?i=f%28x%29+%3D+1+%2F+%281+%2B+exp%28-%28x-5000%29+%2F+1000%29%29+from+0+to+9000" rel="nofollow">Wolfram alpha for decay_factor</a> as far as I was able to decipher from the function image.</em></p>
<p dir="auto">I have used projected simplification error (as provided by meshoptimizer). It's not a great metric for Nanite. I think that other vertex attributes have to be part of this function too. You should be able to assign different weights on a per-attribute basis. Normals on Jinx's face were a huge problem. In my app, I could just move the LOD error threshold slider to the left. I can say that this approach has an educational value. You will have to find something better.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Should you write your own implementation of Nanite?</h3><a id="user-content-should-you-write-your-own-implementation-of-nanite" aria-label="Permalink: Should you write your own implementation of Nanite?" href="#should-you-write-your-own-implementation-of-nanite"></a></p>
<p dir="auto">Depends. The simplest answer is to just use UE5. You will not beat UE5 in its own game. Looking at Steam's front page, most of the games are simple enough to not need it. It's interesting that (at the time of the writing) the 2 most known Nanite titles are Fortnite and Senua's Saga: Hellblade II. Both have opposite objectives and tones. I recommend the Digital Foundry's <a href="https://www.youtube.com/watch?v=u-zmFVzUmPc" rel="nofollow">"Inside Senua's Saga: Hellblade 2 - An Unreal Engine 5 Masterpiece - The Ninja Theory Breakdown"</a>. E.g. they've mentioned a separate Houdini pipeline to extract transparency from static meshes. And while both games are different, both were developed by excellent engineering and visual teams.</p>
<p dir="auto">If you want to write your own implementation as a side project, then don't let me stop you. But unless you tackle simplification and error metric problems, you will end up with code similar to mine. You will still learn a lot.</p>
<p dir="auto">If you want to add this tech to the existing engine, I'm not a person you should be asking (I don't work in the industry). In my opinion, you should start by implementing <a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> and <a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> first. This is already quite a complex task. Multi-step culling is tricky. You have to handle scene and world chunk management. Animated meshes. And that's just the beginning. But with these incremental steps, you will have something that works and can be tested at every step of the transition. Once this is stable, you can try a software rasterizer. Even if you don't end up shipping it, there is a lot to learn. Depending on the codebase, it can be surprisingly easy to add. Only after you have done the above steps you should try adding Nanite-like tech. As the various sliders in my app can tell you, they are all required for Nanite to be performant. The basic meshlet hierarchy for a toy renderer is a weekend project. Real implementation will have to deal with simplification and error metric issues.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Is per-meshlet backface cone culling worth it?</h3><a id="user-content-is-per-meshlet-backface-cone-culling-worth-it" aria-label="Permalink: Is per-meshlet backface cone culling worth it?" href="#is-per-meshlet-backface-cone-culling-worth-it"></a></p>
<p dir="auto">I've implemented the basics, but the gains are limited. Check the comment in <a href="https://github.com/Scthe/nanite-webgpu/blob/8c15e85b32d8b890ef573f58f1fbb782544f972c/src/constants.ts#L160">constants.ts</a> for implementation details.</p>
<ol dir="auto">
<li>It works best if you have a dense mesh where all triangles in a cluster have similar normals. Dense meshes are something that Nanite was designed for. Yet coarse LOD levels will have normals pointing in different directions. Arseny Kapoulkine had <a href="https://zeux.io/2023/04/28/triangle-backface-culling/#estimating-culling-efficiency" rel="nofollow">similar observations</a>.</li>
<li>There is some duplication with occlusion culling. Backfaces are behind front faces in the z-buffer.</li>
<li>Computing the cone is done on a per-meshlet level. For me, this means a WebAssembly call every time. This took 30% of the whole preprocessing step. Preprocessing all models offline would solve this problem. Yet it goes against my goals for this project. I want you to take the simplest possible 3D object format and see that my program works. That's why this app is a webpage and not Rust+Vulkan. No one would have cloned the repo to run the code. But everyone has clicked the demo links above (right?).</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Honourable mentions</h2><a id="user-content-honourable-mentions" aria-label="Permalink: Honourable mentions" href="#honourable-mentions"></a></p>
<ul dir="auto">
<li>Arseny Kapoulkine. This app is only possible due to <a href="https://github.com/zeux/meshoptimizer">meshoptimizer</a>. I've also watched a few of niagara videos and read its source code. And read his blog.
<ul dir="auto">
<li><a href="https://github.com/zeux/meshoptimizer/pull/704" data-hovercard-type="pull_request" data-hovercard-url="/zeux/meshoptimizer/pull/704/hovercard">Newer meshoptimizer versions</a> have <code>meshopt_SimplifySparse</code> specifically for Nanite clones. I've not updated to this version as I am moving on from this app. I want to leave it in a state that I've tested during development.</li>
</ul>
</li>
<li>Folks from Epic Games. Not only for creating Nanite but also for being open on how it works under the hood.</li>
<li><a href="https://ubm-twvideo01.s3.amazonaws.com/o1/vault/gdc2016/Presentations/Wihlidal_Graham_OptimizingTheGraphics.pdf" rel="nofollow">"Optimizing the Graphics Pipeline with Compute"</a> by Graham Wihlidal.</li>
<li><a href="https://advances.realtimerendering.com/s2015/aaltonenhaar_siggraph2015_combined_final_footer_220dpi.pdf" rel="nofollow">"GPU-Driven Rendering Pipelines"</a> by Ulrich Haar and Sebastian Aaltonen.</li>
<li><a href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview" rel="nofollow">METIS</a>.</li>
<li><a href="https://emscripten.org/" rel="nofollow">Emscripten</a>. Used to run both meshoptimizer and METIS in the browser.
<ul dir="auto">
<li><a href="https://marcoselvatici.github.io/WASM_tutorial/" rel="nofollow">Webassembly Tutorial</a> by Marco Selvatici.</li>
</ul>
</li>
<li><a href="https://sketchfab.com/3d-models/arcane-jinx-b74f25a5ee6e43efbe9766b9fbebc705" rel="nofollow">Arcane - Jinx</a> 3D model by sketchfab user <a href="https://sketchfab.com/rizky08" rel="nofollow">Craft Tama</a>. Used under <a href="https://creativecommons.org/licenses/by/4.0/" rel="nofollow">CC Attribution</a> license.
<ul dir="auto">
<li>I've merged the textures, adjusted UVs, and removed the weapon.</li>
</ul>
</li>
</ul>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Deploying Rust in Existing Firmware Codebases (174 pts)]]></title>
            <link>https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</link>
            <guid>41458508</guid>
            <pubDate>Thu, 05 Sep 2024 17:02:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html">https://security.googleblog.com/2024/09/deploying-rust-in-existing-firmware.html</a>, See on <a href="https://news.ycombinator.com/item?id=41458508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-version="1" id="header">
<div>
<p><a href="https://security.googleblog.com/">
<img height="50" src="https://www.gstatic.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png">
</a></p><a href="https://security.googleblog.com/">
<h2>
            Security Blog
          </h2>
</a>
</div>
<p>
The latest news and insights from Google on security and safety on the Internet
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[serverless-registry: A Docker registry backed by Workers and R2 (157 pts)]]></title>
            <link>https://github.com/cloudflare/serverless-registry</link>
            <guid>41458240</guid>
            <pubDate>Thu, 05 Sep 2024 16:34:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/cloudflare/serverless-registry">https://github.com/cloudflare/serverless-registry</a>, See on <a href="https://news.ycombinator.com/item?id=41458240">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Container Registry in Workers</h2><a id="user-content-container-registry-in-workers" aria-label="Permalink: Container Registry in Workers" href="#container-registry-in-workers"></a></p>
<p dir="auto">This repository contains a container registry implementation in Workers that uses R2.</p>
<p dir="auto">It supports all pushing and pulling workflows. It also supports
Username/Password and public key JWT based authentication.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Deployment</h3><a id="user-content-deployment" aria-label="Permalink: Deployment" href="#deployment"></a></p>
<p dir="auto">You have to install all the dependencies with <a href="https://pnpm.io/installation" rel="nofollow">pnpm</a> (other package managers may work, but only pnpm is supported.)</p>

<p dir="auto">After installation, there is a few steps to actually deploy the registry into production:</p>
<ol dir="auto">
<li>Have your own <code>wrangler</code> file.</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ cp wrangler.toml.example wrangler.toml"><pre>$ cp wrangler.toml.example wrangler.toml</pre></div>
<ol start="2" dir="auto">
<li>Setup the R2 Bucket for this registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler --env production r2 bucket create r2-registry"><pre>$ npx wrangler --env production r2 bucket create r2-registry</pre></div>
<p dir="auto">Add this to your <code>wrangler.toml</code></p>
<div data-snippet-clipboard-copy-content="r2_buckets = [
    { binding = &quot;REGISTRY&quot;, bucket_name = &quot;r2-registry&quot;}
]"><pre><code>r2_buckets = [
    { binding = "REGISTRY", bucket_name = "r2-registry"}
]
</code></pre></div>
<ol start="3" dir="auto">
<li>Deploy your image registry</li>
</ol>
<div dir="auto" data-snippet-clipboard-copy-content="$ npx wrangler deploy --env production"><pre>$ npx wrangler deploy --env production</pre></div>
<p dir="auto">Your registry should be up and running. It will refuse any requests if you don't setup credentials.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding username password based authentication</h3><a id="user-content-adding-username-password-based-authentication" aria-label="Permalink: Adding username password based authentication" href="#adding-username-password-based-authentication"></a></p>
<p dir="auto">Set the USERNAME and PASSWORD as secrets with <code>npx wrangler secret put USERNAME --env production</code> and <code>npx wrangler secret put PASSWORD --env production</code>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding JWT authentication with public key</h3><a id="user-content-adding-jwt-authentication-with-public-key" aria-label="Permalink: Adding JWT authentication with public key" href="#adding-jwt-authentication-with-public-key"></a></p>
<p dir="auto">You can add a base64 encoded JWT public key to verify passwords (or token) that are signed by the private key.
<code>npx wrangler secret put JWT_REGISTRY_TOKENS_PUBLIC_KEY --env production</code></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Using with Docker</h3><a id="user-content-using-with-docker" aria-label="Permalink: Using with Docker" href="#using-with-docker"></a></p>
<p dir="auto">You can use this registry with Docker to push and pull images.</p>
<p dir="auto">Example using <code>docker push</code> and <code>docker pull</code>:</p>
<div dir="auto" data-snippet-clipboard-copy-content="export REGISTRY_URL=your-url-here

# Replace $PASSWORD and $USERNAME with the actual credentials
echo $PASSWORD | docker login --username $USERNAME --password-stdin $REGISTRY_URL
docker pull ubuntu:latest
docker tag ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker push $REGISTRY_URL/ubuntu:latest

# Check that pulls work
docker rmi ubuntu:latest $REGISTRY_URL/ubuntu:latest
docker pull $REGISTRY_URL/ubuntu:latest"><pre><span>export</span> REGISTRY_URL=your-url-here

<span><span>#</span> Replace $PASSWORD and $USERNAME with the actual credentials</span>
<span>echo</span> <span>$PASSWORD</span> <span>|</span> docker login --username <span>$USERNAME</span> --password-stdin <span>$REGISTRY_URL</span>
docker pull ubuntu:latest
docker tag ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker push <span>$REGISTRY_URL</span>/ubuntu:latest

<span><span>#</span> Check that pulls work</span>
docker rmi ubuntu:latest <span>$REGISTRY_URL</span>/ubuntu:latest
docker pull <span>$REGISTRY_URL</span>/ubuntu:latest</pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Configuring Pull fallback</h3><a id="user-content-configuring-pull-fallback" aria-label="Permalink: Configuring Pull fallback" href="#configuring-pull-fallback"></a></p>
<p dir="auto">You can configure the R2 regitry to fallback to another registry if
it doesn't exist in your R2 bucket. It will download from the registry
and copy it into the R2 bucket. In the next pull it will be able to pull it directly from R2.</p>
<p dir="auto">This is very useful for migrating from one registry to <code>serverless-registry</code>.</p>
<p dir="auto">It supports both Basic and Bearer authentications as explained in the
<a href="https://distribution.github.io/distribution/spec/auth/token/" rel="nofollow">registry spec</a>.</p>
<p dir="auto">In the wrangler.toml file:</p>
<div data-snippet-clipboard-copy-content="[env.production]
REGISTRIES_JSON = &quot;[{ \&quot;registry\&quot;: \&quot;https://url-to-other-registry\&quot;, \&quot;password_env\&quot;: \&quot;REGISTRY_TOKEN\&quot;, \&quot;username\&quot;: \&quot;username-to-use\&quot; }]&quot;"><pre><code>[env.production]
REGISTRIES_JSON = "[{ \"registry\": \"https://url-to-other-registry\", \"password_env\": \"REGISTRY_TOKEN\", \"username\": \"username-to-use\" }]"
</code></pre></div>
<p dir="auto">Set as a secret the registry token of the registry you want to setup
pull fallback in.</p>
<p dir="auto">For example <a href="https://cloud.google.com/artifact-registry/docs/reference/docker-api" rel="nofollow">gcr</a>:</p>
<div data-snippet-clipboard-copy-content="cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>cat ./registry-service-credentials.json | base64 | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto"><a href="https://github.com/settings/tokens">Github</a> for example uses a simple token that you can copy.</p>
<div data-snippet-clipboard-copy-content="echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN"><pre><code>echo $GITHUB_TOKEN | wrangler --env production secrets put REGISTRY_TOKEN
</code></pre></div>
<p dir="auto">The trick is always looking for how you would login in Docker for
the target registry and setup the credentials.</p>
<p dir="auto"><strong>Never put a registry password/token inside the wrangler.toml, please always use <code>wrangler secrets put</code></strong></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Known limitations</h3><a id="user-content-known-limitations" aria-label="Permalink: Known limitations" href="#known-limitations"></a></p>
<p dir="auto">Right now there is some limitations with this container registry.</p>
<ul dir="auto">
<li>Pushing with docker is limited to images that have layers of maximum size 500MB. Refer to maximum request body sizes in your Workers plan.</li>
<li>To circumvent that limitation, you can manually add the layer and the manifest into the R2 bucket or use a client that is able to chunk uploads in sizes less than 500MB (or the limit that you have in your Workers plan).</li>
<li>If you use <code>npx wrangler dev</code> and push to the R2 registry with docker, the R2 registry will have to buffer the request on the Worker.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">The project is licensed under the <a href="https://opensource.org/licenses/apache-2.0/" rel="nofollow">Apache License</a>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Contribution</h3><a id="user-content-contribution" aria-label="Permalink: Contribution" href="#contribution"></a></p>
<p dir="auto">See <code>CONTRIBUTING.md</code> for contributing to the project.</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: We built a FOSS documentation CMS with a pretty GUI (127 pts)]]></title>
            <link>https://kalmia.difuse.io/doc/</link>
            <guid>41458141</guid>
            <pubDate>Thu, 05 Sep 2024 16:26:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://kalmia.difuse.io/doc/">https://kalmia.difuse.io/doc/</a>, See on <a href="https://news.ycombinator.com/item?id=41458141">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="root"><div><p><a href="https://kalmia.difuse.io/doc/"><img src="https://downloads-bucket.difuse.io/kalmia-sideways-black.png" alt="logo" id="logo"><img src="https://downloads-bucket.difuse.io/kalmia-sideways-white-final.png" alt="logo" id="logo"></a></p><div><div><div><a target="" href="https://kalmia.difuse.io/doc/index.html"><p>Home</p></a><a target="" href="https://kalmia.difuse.io/doc/guides.html"><p>Documentation</p></a></div><div><div><a href="https://discord.gg/ahFzZApybH" target="_blank" rel="noopener noreferrer"><p><svg role="img" viewBox="0 0 24 24" width="24" height="24" xmlns="http://www.w3.org/2000/svg"><title>Discord</title><path d="M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z"></path></svg></p></a></div></div></div><div id="navScreen"><div><div><a target="" href="https://kalmia.difuse.io/doc/index.html"><p>Home</p></a></div><div><a target="" href="https://kalmia.difuse.io/doc/guides.html"><p>Documentation</p></a></div></div><div><a href="https://discord.gg/ahFzZApybH" target="_blank" rel="noopener noreferrer"><p><svg role="img" viewBox="0 0 24 24" width="24" height="24" xmlns="http://www.w3.org/2000/svg"><title>Discord</title><path d="M20.317 4.3698a19.7913 19.7913 0 00-4.8851-1.5152.0741.0741 0 00-.0785.0371c-.211.3753-.4447.8648-.6083 1.2495-1.8447-.2762-3.68-.2762-5.4868 0-.1636-.3933-.4058-.8742-.6177-1.2495a.077.077 0 00-.0785-.037 19.7363 19.7363 0 00-4.8852 1.515.0699.0699 0 00-.0321.0277C.5334 9.0458-.319 13.5799.0992 18.0578a.0824.0824 0 00.0312.0561c2.0528 1.5076 4.0413 2.4228 5.9929 3.0294a.0777.0777 0 00.0842-.0276c.4616-.6304.8731-1.2952 1.226-1.9942a.076.076 0 00-.0416-.1057c-.6528-.2476-1.2743-.5495-1.8722-.8923a.077.077 0 01-.0076-.1277c.1258-.0943.2517-.1923.3718-.2914a.0743.0743 0 01.0776-.0105c3.9278 1.7933 8.18 1.7933 12.0614 0a.0739.0739 0 01.0785.0095c.1202.099.246.1981.3728.2924a.077.077 0 01-.0066.1276 12.2986 12.2986 0 01-1.873.8914.0766.0766 0 00-.0407.1067c.3604.698.7719 1.3628 1.225 1.9932a.076.076 0 00.0842.0286c1.961-.6067 3.9495-1.5219 6.0023-3.0294a.077.077 0 00.0313-.0552c.5004-5.177-.8382-9.6739-3.5485-13.6604a.061.061 0 00-.0312-.0286zM8.02 15.3312c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9555-2.4189 2.157-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.9555 2.4189-2.1569 2.4189zm7.9748 0c-1.1825 0-2.1569-1.0857-2.1569-2.419 0-1.3332.9554-2.4189 2.1569-2.4189 1.2108 0 2.1757 1.0952 2.1568 2.419 0 1.3332-.946 2.4189-2.1568 2.4189Z"></path></svg></p></a></div></div></div></div><div><div><div><p>Pollinating Knowledge 🌼</p><div><div><p><a target="" href="https://kalmia.difuse.io/doc/guides/index.html">Get Started</a></p></div><div><p><a href="https://github.com/DifuseHQ/Kalmia" target="_blank" rel="noopener noreferrer">Github</a></p></div></div></div><p><img src="https://downloads-bucket.difuse.io/JfsN9iR.png" alt="Kalmia Logo" width="375" height="375"><img src="https://downloads-bucket.difuse.io/JfsN9iR.png" alt="Kalmia Logo" width="375" height="375"></p></div><div><div><article><div><p>👥</p></div><h2>Free &amp; OSS</h2><p>Kalmia is FOSS under the AGPL-3 license 🔓. Free to use, modify, and share.</p></article></div><div><article><div><p>⚡</p></div><h2>Blazingly Fast</h2><p>Written in Go and uses Rust-based RsPress for markdown processing, delivering exceptional speed in both execution and build times.</p></article></div><div><article><div><p>🌟</p></div><h2>Easy To Use</h2><p>Intuitive for all users, from beginners to experts. Simplifies documentation without sacrificing features</p></article></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Phind-405B and faster, high quality AI answers for everyone (310 pts)]]></title>
            <link>https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</link>
            <guid>41458083</guid>
            <pubDate>Thu, 05 Sep 2024 16:22:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches">https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches</a>, See on <a href="https://news.ycombinator.com/item?id=41458083">Hacker News</a></p>
Couldn't get https://www.phind.com/blog/introducing-phind-405b-and-better-faster-searches: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Boxed – Things I learned after lying in an MRI machine for 30 hours (121 pts)]]></title>
            <link>https://aethermug.com/posts/boxed</link>
            <guid>41457739</guid>
            <pubDate>Thu, 05 Sep 2024 15:50:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aethermug.com/posts/boxed">https://aethermug.com/posts/boxed</a>, See on <a href="https://news.ycombinator.com/item?id=41457739">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main components="[object Object]"><p>Last year a researcher from a Japanese lab asked me if he could borrow my <a href="https://en.wikipedia.org/wiki/Aphantasia" rel="nofollow noopener noreferrer" target="_blank">aphantasic</a> brain for an experiment. Neuroscientists have been studying the weird capacity most people have of "visualizing", or mentally conjuring pictures of things that aren't there, for a long time. The problem is that the brain is such a wild tangle of interconnections that it's hard to tell which of its parts are involved in visualization and which are unrelated, or downstream of it. Comparing people who visualize with aphantasics who don't is a very convenient way to partially work around that problem.</p>
<p>I agreed to participate in the experiment. That decision led to me spending an inordinate amount of time deep inside an MRI, looking at pictures or trying (and failing) to imagine pictures. The machine, worth half a dozen Lamborghinis, is hidden somewhere in a basement on the University of Tokyo campus.</p>
<hr><figure id="floating-1"><video src="https://aethermug.com/assets/posts/aphantasia/mri.mp4" controls="" autoplay="" loop="" muted=""></video><figcaption>Click to play the animation. Source: Fastfission, CC BY-SA 3.0.</figcaption></figure>
<p>Essentially, an MRI is a big pipe you climb into, which happens to be capable of seeing right through you. Wrapped around the pipe is a hidden network of metal coils cooled to 9 degrees Celsius above absolute zero, constantly switching very large currents to fill the hollow inside with a strong magnetic field and shooting (harmless) radio waves at some corner of your body—the brain, in this case.</p>
<p>In neuroscience, MRI is used to detect microscopic changes in blood flow inside the brain, allowing the researchers to obtain a 3D video map of which neural networks are active at each instant.</p>
<p>In my experiments, the researchers record what goes on in my head when I look at a picture of a fire hydrant, then a camel, then a piece of wood, and so on, covering thousands of pictures over the months. Other times they asked me to imagine various things, or to remember images that I had been shown earlier.</p>
<p>The scientific side of these experiments is very interesting, but there are many other places to read about it in depth. Here, instead, I want to leave some notes on my subjective observations of this experience. I've found that lying down in a cramped space for hours at a time, fully awake and with only very weird kinds of pastimes, leads to some unusual reflections.</p>
<p>These are a few of those reflections, in approximate order of decreasing banality.</p>
<h2>1 - It's noisy</h2>
<p>Anyone who has spent more than two seconds in an MRI knows this, but if you're thinking of trying for the first time, be warned: those things are LOUD.</p>
<p>The high-frequency current switching causes the metal coils around you to expand and contract and vibrate like a jackhammer. You feel like a rescue team is trying all they can to get you out of an unbreakable iron coffin. If you're sensitive to loud noises, you might want to avoid it.</p>
<p>Usually they give you foam earplugs before you go in, and those make the experience bearable for me. Still, there are risks. The other day I mis-inserted one of the earplugs and got a taste of the unabridged experience: I tried to continue like that for a while, but I was so worried for my eardrum that eventually I had to stop the experiment early. When I got out I felt like the left side of my head had been in the front row at a rock concert. Not healthy.</p>
<p>Even with properly-inserted earplugs, the racket echoing all around you is the most unpleasant part of the experiment. It's surprisingly draining. If it weren't for that, I think I could keep going just fine for much longer.</p>
<h2>2 - It's <em>very</em> good at putting you to sleep</h2>
<p>Remaining horizontal in a semi-dark space for a while doesn't help you stay awake. This shouldn't come as a surprise to most people, but it surprised me. I usually have the opposite problem, because I <em>never</em> nap or sleep during daytime (as in, maybe once a year), and not for lack of trying. There is something about the infinite possible activities that I could be doing instead of sleeping that excites my brain during the day, even when I'm very tired.</p>
<p>Yet, looking at long sequences of unrelated pictures or—much worse—the same set of pictures over and over has an almost magical power to induce slumber. I haven't succumbed to it yet, but I've come close a few times. I should try something similar for my next napping attempt.</p>
<hr><figure id="floating-2"><img src="https://aethermug.com/assets/posts/boxed/mri.webp" alt="A gray surface with a gray stripe running through it."><figcaption>The scenery as seen from inside the machine.</figcaption></figure>
<h2>3 - My brain craves novelty</h2>
<p>Aside from the problem of sleepiness, I was shocked at how the lack of new stimuli can strain my grasp on my mental faculties.</p>
<p>Some experiments consist of seeing the same images hundreds of times, or repeating the same task over and over with only minor variations. Since an MRI pipe is essentially an isolation chamber, those tasks and images (and the noise) are the only sensory input you're going to get. After a while, my brain started rejecting them.</p>
<p>At first, it wasn't a big deal, but after a few weeks of those repetitive tasks, even the simple act of paying attention to what was in front of my eyes began to take a tremendous effort of concentration. I came out in shambles every time, depleted of life force, and I even thought of the word <em>torture</em> once or twice. But hey, someone's gotta do this.</p>
<p>Luckily that repetitive series of experiments ended just before I reached my breaking point. I wonder if this is something that can be trained, but I'm not sure I'd want to do that either.</p>
<h2>4 - Novel, random images are great for creativity</h2>
<p>Most of the experiments involve looking at non-repeating images, meaning that I see each one only once and never again. My brain is apparently fine with this and, with a good infusion of caffeine, it's actually happy to go on the ride.</p>
<p>This is an experience you don't usually have in your daily life. Normally you know, more or less, what to expect to see next. Even when you can't predict what's coming—when you're watching a movie, for example—things have some kind of connection to each other, some theme or context that ties them together. With random images in a lab, none of that exists. Now you're seeing a picture of a man blowing smoke from his mouth in the Grand Canyon, next it could be a close-up on a smudged corner of a book, or a group of penguins near an ice cliff, or a pile of broken CRT monitors, or something else altogether.</p>
<p>Every four seconds or so, you see something new that you would never have guessed from the previous pictures. Each time it's a different cascade of activations in your brain, evoking random memories, creating unexpected connections, and stimulating thoughts that would never have occurred to you.</p>
<p>Something strange happens: even though it's all purely random, the brain tries to make sense of it all, tries to find patterns and associations. With no time to establish conventional <a href="https://aethermug.com/posts/a-framing-is-a-choice-of-boundaries">framings</a>, it has to improvise, take in the images in a partly-unconscious way, without thorough processing. This, I think, is a great way to stimulate creativity.</p>
<p>A few times during those experiments, I came up with so many ideas—things to write about, better ways to explain things, new intriguing questions about the world, etc.—that my biggest worry was trying to remember them all for the 20 or 30 minutes left until the end of the session. In the short pauses between bursts of images, I tried to rehearse the list of ideas with shortened mnemonics, but found that I could only keep around five in my head before I forgot some of them.</p>
<p>This is an amazing state to have my brain in, and I wish I could induce it at will. Social media feeds look similar on the surface, but they don't give you truly random stimuli. Their contents are highly edited to appeal to the viewer, and come with lots of cultural baggage and trend-following. They don't work to unhinge my creativity—rather, they trap it.</p>
<p>What I need is an app that does nothing but show you truly random pictures, with no curation and no memetic aspirations. If you know of one, please let me know.</p>
<hr><figure id="floating-3"><img src="https://aethermug.com/assets/posts/boxed/dylan-nolte-qxYDhV0rBPk-unsplash.webp" alt="An athlete's hand being wrapped in tape by someone."><figcaption>A random picture. I bet you didn't see that coming. Source: Dylan Nolte, Unsplash </figcaption></figure>
<h2>5 - Our sense of time is non-linear even at the shortest scales</h2>
<p>Everybody has experienced the subjective relativity of time. When you have fun, it flies. When you're waiting anxiously, it never budges. But, before these experiments, I had never realized just how warpy my perception of time can be even on the scale of a couple of seconds.</p>
<p>These tasks with pictures require me to stay focused on what's shown on the screen. To ensure that I'm not distracted, the screen will show certain cues at random intervals, to which I have to react by pressing a button. Usually, the cue is the repetition of the same image twice in a row: normally each image stays there only four seconds, but sometimes it will flash back to itself instead of being replaced by a different image. This sounds like an easy thing to spot, and most of the time it is. But when I'm not in an optimal shape, it can become fiendishly difficult.</p>
<p>I find myself asking, have I seen this same picture <em>two seconds ago</em> or not? Is this still the first four seconds?</p>
<p>This is so strange and almost disconcerting. In the highly controlled environment of the lab, I can easily notice these lapses in my perception of time, but what about all the other times? Does my sense of time ebb and flow like that every few seconds of my waking life?</p>
<h2>6 - Having your thoughts monitored feels... weird</h2>
<p>Sometimes, during the experiments, I wonder about things like:</p>
<ul>
<li>Is it enough to just watch, or should I think intensely about the subject?</li>
<li>I was asked to imagine the picture of a duck, but I imagined a moving duck, beating its wings quickly. Will this taint the results?</li>
</ul>
<p>In order to protect the objectivity of the experiments, the researchers don't tell me exactly how they're analyzing the fMRI data, or what their hypotheses are. Still, I know that they happen to have the closest thing ever to <em>mind-reading technology</em>, which has interesting implications.</p>
<p>In previous studies, they have successfully trained generative AI to read fMRI scans and replicate the images people were thinking about, or to add captions describing those mental images. Considering the amount of data they're taking of my brain, it would be possible for them to train an AI for my specific brain patterns.</p>
<p>I don't think many people have experienced this situation before. People have had tyrants and Big Brothers spying on their actions and words for millennia, but has anyone ever had their thoughts monitored? In a sense, I feel naked.</p>
<p>For now, this is not a big problem. At worst, I might worry that they would know it when I'm distracted, and I might try not to think about kinky stuff (usually a big mistake). Not the end of the world. But I can't help imagining about the dystopian societies that could emerge if the same technology was somehow scaled to portable sizes and affordable prices.</p>
<h2>7 - We are usually oblivious to what our brains are doing</h2>
<p>I've only ever tried short-ish sessions of mindfulness-like meditation, where the goal is to free your mind of thoughts, focus on your breathing, or something along those lines. I don't know about other types of meditation, but I would guess that most of them are about relaxing or at least not thinking very hard. All of these may help you, in one way or another, to feel better and more centered and even, in some cases, to know your body and mind better.</p>
<p>But I doubt there is a kind of meditation that prompts the level of introspection that long hours in an MRI machine doing simple but focused tasks can give you.</p>
<p>Inside the machine, I have to remain very still with no phone to check, nothing to read, no tossing and turning just for the sake of it, nothing to fiddle with, and—given that the tasks all require a moderate but constant level of attention, no opportunity to get really lost in thought. I did this for over 30 hours now, and counting.</p>
<p>In other words, I got to spend a lot of time in the peaceful, forced company of myself, not too cognitively busy but neither focusing on breathing or clearing my head. It's a sort of Goldilocks zone not only for creativity (point 4), but also for the observation of how my mind works.</p>
<p>In this process, I've learned more about myself than I had in the previous two decades. For example, it's how I uncovered the details of how my non-visual imagination works—something I had never noticed before. One day, right after a session in the MRI, I ran to a cafe and wrote a Twitter thread explaining exactly what that is like for me. The thread had a surprising success, attracting the keen interest of lots of other people (you can read a copy of the thread <a href="https://aethermug.com/posts/aphantasia">here</a>). Apparently it's a kind of description that is usually hard to come by, yet it came easily to me with all that time of confinement.</p>
<p>This unexpected success at discovering new things about myself has encouraged me to try this introspection outside the lab, too. That's how I first realized I have <a href="https://synesthesia-test.com/time-space-synesthesia" rel="nofollow noopener noreferrer" target="_blank">time-space synesthesia</a>, something that I hadn't even heard of before. It's also how I realized that wearing special earplugs in noisy places helps me understand what the people around me are saying, mitigating a mild auditory processing disorder that I had never thought much about. And so on, with a new quirk or peculiarity coming to my attention every now and then as I do other things.</p>
<p>Somehow the mere fact of staying still in a state halfway between emptying my mind and filling it to the brim has helped me become more attuned to myself. I feel a bit more centered in the moment, so to speak. Much more than before, I now consider the brain to be an organ that you can observe and study, a <a href="https://aethermug.com/posts/a-black-box-view-of-life">black box</a> you can tinker with (carefully) to better understand it. This kind of exploration can be very fruitful, showing you what works best for you, what to avoid, how to be kind to yourself, and generally how to "use" your brain more expertly. ●</p>
<div><p>Cover image:</p><p><em>Photo by Vladimir Kramer, Unsplash</em></p></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: AnythingLLM – Open-Source, All-in-One Desktop AI Assistant (279 pts)]]></title>
            <link>https://github.com/Mintplex-Labs/anything-llm</link>
            <guid>41457633</guid>
            <pubDate>Thu, 05 Sep 2024 15:40:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Mintplex-Labs/anything-llm">https://github.com/Mintplex-Labs/anything-llm</a>, See on <a href="https://news.ycombinator.com/item?id=41457633">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text">
<p dir="auto">
  <a href="https://anythingllm.com/" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/wordmark.png?raw=true" alt="AnythingLLM logo"></a>
</p>
<p><a href="https://trendshift.io/repositories/2415" rel="nofollow"><img src="https://camo.githubusercontent.com/13a0218d5bc383ce61ac2154a48e40d6ce0c079fe6c930524732238d033b7a72/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f32343135" alt="Mintplex-Labs%2Fanything-llm | Trendshift" width="250" height="55" data-canonical-src="https://trendshift.io/api/badge/repositories/2415"></a>
</p>
<p dir="auto">
    <b>AnythingLLM:</b> The all-in-one AI app you were looking for.<br>
    Chat with your docs, use AI Agents, hyper-configurable, multi-user, &amp; no frustrating set up required.
</p>
<p dir="auto">
  <a href="https://discord.gg/6UyHPeGZAC" rel="nofollow">
      <img src="https://camo.githubusercontent.com/3b816ca02eba9f9b0e63fe98bdfff52b93212792c74d87757fabdf67c42e9e36/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6d696e74706c65785f6c6162732d626c75652e7376673f7374796c653d666c6174266c6f676f3d646174613a696d6167652f706e673b6261736536342c6956424f5277304b47676f414141414e5355684555674141414341414141416743414d4141414245704972474141414149474e49556b304141486f6d41414341684141412b6741414149446f414142314d414141366d41414144715941414158634a79365554774141414831554578555251414141502f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f2f72362b75626e352b3775372f332b2f7633392f656e7136757271362f76372b393766333972623236656f715431425130704f54342b526b757a7337636e4b796b5a4b53304e4853486c3866647a6433656a6f365578505555424452647a63335277674968386a4a53416b4a6d3578637648783861616e714234694a46425456657a7437563568596c4a5656754c6a343370396669496d4b434d6e4b5a4b556c61616f7153456c4a32317763665430394f337537757672367a45304e72362f774355704b3571636e66372b2f6e68376645644b5448782b66307450554f546c3561697071696f754d477475627a3543524451344f7354477875666e35313568593761337548312f6758427964494f46686c5659577658323971616f7143516f4b7337507a2f507a38372f417755744f554e665932644852306d6872624f7672374535525579387a4e585232642f6633392b586c35555a4a53783068497a51334f647261322f7a382f476c736261476a7045524853657a73374c2f42775363724c5451344f646e61327a4d334f626d377533782f674b536d70396a5a3254314151752f7637317064586b56495372322b767967734c69496e4b54673750614f6c706973764d635847787a6b38506c646158504c793875377537726d36753753317473444277766a342b4d5045786265347565586d35732f51304b7966376577414141416f64464a4f5577414142436c73724e6a782f514d326c392f376c686d49366a54422f6b413147674b4a4e2b6e65613676792f4d4c5a515965564b4b3372564135744141414141574a4c523051422f774974336741414141643053553146422b634b4241416d4d5a42486a584941414149535355524256446a4c59324341416b596d5a685a574e6e594f446e593256685a6d4a6b5947564d44497963584e77367342426277386646796379456f5947666b4642445651674b414150794d6a516c35495745514444596749433846554d444b4b736d6c67415779694542574d6a474a5935594571784d4171474d57464e58414159584767416b594a5351326351464b436b59465253687133416d6b705267594a62676862553074624230547236756b626747684449313067795366427743774455574273596d706d447151744c4b327362545130624f3373485941384757594757576a34575473364f627534616d69344f546d3765786871654870352b344443564a5a42446d7164723775666e332b41726b5a676b4a2b6655334349526d67595746694f41525947766f354f5155486845554146546b462b6b564852734c42676b496579596d4c6a776f4f6334684d536b354a546e494e53303644433867776345455a3652715a476c704f6663335a4f626c352b675a2b5452324552574679425151464d4635656b6c6d7155705162352b52655536315a554f766b465656585851425341726169747132396f3147694b63664c7a63323975306d6a78427a71307451306b777735785a48744855476558686b5a6864784259675a3464304c49366334676a7764377369515172614f703141697651364375414b5a43444242525151514e516755622f4247663363714343695a4f636e4365335151494b484e5254706b36624467705a6a526b7a673370425154427264744363755a43676c7541443076506d4c316749647653697855755767714e7332594a2b4455686b4559787567676b476d4f515563636b72696f50544a434f58456e5a354a533559736c62476e75795645526c44444676474555504f5776777161483652566b484b6575444d4b36534b6e486c5668546778386a65546d71793645696a374b366e4c71694779507743687361314d55726e713177414141435630525668305a4746305a54706a636d5668644755414d6a41794d7930784d4330774e4651774d446f7a4f446f304f5373774d446f774d423956306138414141416c6445565964475268644755366257396b61575a35414449774d6a4d744d5441744d4452554d4441364d7a67364e446b724d4441364d44427543476b54414141414b4852465748526b5958526c4f6e52706257567a64474674634141794d44497a4c5445774c544130564441774f6a4d344f6a51354b7a41774f6a41774f5231497a4141414141424a52553545726b4a6767673d3d" alt="Discord" data-canonical-src="https://img.shields.io/badge/chat-mintplex_labs-blue.svg?style=flat&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAIGNIUk0AAHomAACAhAAA+gAAAIDoAAB1MAAA6mAAADqYAAAXcJy6UTwAAAH1UExURQAAAP////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////r6+ubn5+7u7/3+/v39/enq6urq6/v7+97f39rb26eoqT1BQ0pOT4+Rkuzs7cnKykZKS0NHSHl8fdzd3ejo6UxPUUBDRdzc3RwgIh8jJSAkJm5xcvHx8aanqB4iJFBTVezt7V5hYlJVVuLj43p9fiImKCMnKZKUlaaoqSElJ21wcfT09O3u7uvr6zE0Nr6/wCUpK5qcnf7+/nh7fEdKTHx+f0tPUOTl5aipqiouMGtubz5CRDQ4OsTGxufn515hY7a3uH1/gXBydIOFhlVYWvX29qaoqCQoKs7Pz/Pz87/AwUtOUNfY2dHR0mhrbOvr7E5RUy8zNXR2d/f39+Xl5UZJSx0hIzQ3Odra2/z8/GlsbaGjpERHSezs7L/BwScrLTQ4Odna2zM3Obm7u3x/gKSmp9jZ2T1AQu/v71pdXkVISr2+vygsLiInKTg7PaOlpisvMcXGxzk8PldaXPLy8u7u7rm6u7S1tsDBwvj4+MPExbe4ueXm5s/Q0Kyf7ewAAAAodFJOUwAABClsrNjx/QM2l9/7lhmI6jTB/kA1GgKJN+nea6vy/MLZQYeVKK3rVA5tAAAAAWJLR0QB/wIt3gAAAAd0SU1FB+cKBAAmMZBHjXIAAAISSURBVDjLY2CAAkYmZhZWNnYODnY2VhZmJkYGVMDIycXNw6sBBbw8fFycyEoYGfkFBDVQgKAAPyMjQl5IWEQDDYgIC8FUMDKKsmlgAWyiEBWMjGJY5YEqxMAqGMWFNXAAYXGgAkYJSQ2cQFKCkYFRShq3AmkpRgYJbghbU0tbB0Tr6ukbgGhDI10gySfBwCwDUWBsYmpmDqQtLK2sbTQ0bO3sHYA8GWYGWWj4WTs6Obu4ami4OTm7exhqeHp5+4DCVJZBDmqdr7ufn3+ArkZgkJ+fU3CIRmgYWFiOARYGvo5OQUHhEUAFTkF+kVHRsLBgkIeyYmLjwoOc4hMSk5JTnINS06DC8gwcEEZ6RqZGlpOfc3ZObl5+gZ+TR2ERWFyBQQFMF5eklmqUpQb5+ReU61ZUOvkFVVXXQBSAraitq29o1GiKcfLzc29u0mjxBzq0tQ0kww5xZHtHUGeXhkZhdxBYgZ4d0LI6c4gjwd7siQQraOp1AivQ6CuAKZCDBBRQQQNQgUb/BGf3cqCCiZOcnCe3QQIKHNRTpk6bDgpZjRkzg3pBQTBrdtCcuZCgluAD0vPmL1gIdvSixUuWgqNs2YJ+DUhkEYxuggkGmOQUcckrioPTJCOXEnZ5JS5YslbGnuyVERlDDFvGEUPOWvwqaH6RVkHKeuDMK6SKnHlVhTgx8jeTmqy6Eij7K6nLqiGyPwChsa1MUrnq1wAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMy0xMC0wNFQwMDozODo0OSswMDowMB9V0a8AAAAldEVYdGRhdGU6bW9kaWZ5ADIwMjMtMTAtMDRUMDA6Mzg6NDkrMDA6MDBuCGkTAAAAKHRFWHRkYXRlOnRpbWVzdGFtcAAyMDIzLTEwLTA0VDAwOjM4OjQ5KzAwOjAwOR1IzAAAAABJRU5ErkJggg==">
  </a> |
  <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">
      <img src="https://camo.githubusercontent.com/b4357f651d9e266e4b3854471ec55091f13a9f067f8b0144037ef6ab46239a66/68747470733a2f2f696d672e736869656c64732e696f2f7374617469632f76313f6c6162656c3d6c6963656e7365266d6573736167653d4d495426636f6c6f723d7768697465" alt="License" data-canonical-src="https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=white">
  </a> |
  <a href="https://docs.anythingllm.com/" rel="nofollow">
    Docs
  </a> |
   <a href="https://my.mintplexlabs.com/aio-checkout?product=anythingllm" rel="nofollow">
    Hosted Instance
  </a>
</p>
<p dir="auto">
  <b>English</b> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.zh-CN.md">简体中文</a> · <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/locales/README.ja-JP.md">日本語</a>
</p>
<p dir="auto">
👉 AnythingLLM for desktop (Mac, Windows, &amp; Linux)! <a href="https://anythingllm.com/download" rel="nofollow"> Download Now</a>
</p>
<p dir="auto">A full-stack application that enables you to turn any document, resource, or piece of content into context that any LLM can use as references during chatting. This application allows you to pick and choose which LLM or Vector Database you want to use as well as supporting multi-user management and permissions.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA"><img src="https://private-user-images.githubusercontent.com/16845892/294273127-cfc5f47c-bd91-4067-986c-f3f49621a859.gif?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1NjQ5MDIsIm5iZiI6MTcyNTU2NDYwMiwicGF0aCI6Ii8xNjg0NTg5Mi8yOTQyNzMxMjctY2ZjNWY0N2MtYmQ5MS00MDY3LTk4NmMtZjNmNDk2MjFhODU5LmdpZj9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDE5MzAwMlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTYyOTRlZDk0MGM3NThmMTFjMGQwZjY3NjFkYzQyZTQzZjY1ZjM5ODY2ZmRjMGRkMzk2ZTNlNjIwOTU5MTg4MjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.702OhOJH1eZoGn3wHkT3cpc8NgWnuhjAtLNyV5-8ztA" alt="Chatting" data-animated-image=""></a></p>
<details>
<summary><kbd>Watch the demo!</kbd></summary>
<p dir="auto"><a href="https://youtu.be/f95rGD9trL0" rel="nofollow"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/youtube.png" alt="Watch the video"></a></p>
</details>
<p dir="auto"><h3 tabindex="-1" dir="auto">Product Overview</h3><a id="user-content-product-overview" aria-label="Permalink: Product Overview" href="#product-overview"></a></p>
<p dir="auto">AnythingLLM is a full-stack application where you can use commercial off-the-shelf LLMs or popular open source LLMs and vectorDB solutions to build a private ChatGPT with no compromises that you can run locally as well as host remotely and be able to chat intelligently with any documents you provide it.</p>
<p dir="auto">AnythingLLM divides your documents into objects called <code>workspaces</code>. A Workspace functions a lot like a thread, but with the addition of containerization of your documents. Workspaces can share documents, but they do not talk to each other so you can keep your context for each workspace clean.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cool features of AnythingLLM</h2><a id="user-content-cool-features-of-anythingllm" aria-label="Permalink: Cool features of AnythingLLM" href="#cool-features-of-anythingllm"></a></p>
<ul dir="auto">
<li>🆕 <strong>Multi-modal support (both closed and open-source LLMs!)</strong></li>
<li>👤 Multi-user instance support and permissioning <em>Docker version only</em></li>
<li>🦾 Agents inside your workspace (browse the web, run code, etc)</li>
<li>💬 <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/embed/README.md">Custom Embeddable Chat widget for your website</a> <em>Docker version only</em></li>
<li>📖 Multiple document type support (PDF, TXT, DOCX, etc)</li>
<li>Simple chat UI with Drag-n-Drop funcitonality and clear citations.</li>
<li>100% Cloud deployment ready.</li>
<li>Works with all popular <a href="#supported-llms-embedder-models-speech-models-and-vector-databases">closed and open-source LLM providers</a>.</li>
<li>Built-in cost &amp; time-saving measures for managing very large documents compared to any other chat UI.</li>
<li>Full Developer API for custom integrations!</li>
<li>Much more...install and find out!</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Supported LLMs, Embedder Models, Speech models, and Vector Databases</h3><a id="user-content-supported-llms-embedder-models-speech-models-and-vector-databases" aria-label="Permalink: Supported LLMs, Embedder Models, Speech models, and Vector Databases" href="#supported-llms-embedder-models-speech-models-and-vector-databases"></a></p>
<p dir="auto"><strong>Large Language Models (LLMs):</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md#text-generation-llm-selection">Any open-source llama.cpp compatible model</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI (Generic)</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://aws.amazon.com/bedrock/" rel="nofollow">AWS Bedrock</a></li>
<li><a href="https://www.anthropic.com/" rel="nofollow">Anthropic</a></li>
<li><a href="https://ai.google.dev/" rel="nofollow">Google Gemini Pro</a></li>
<li><a href="https://huggingface.co/" rel="nofollow">Hugging Face (chat models)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (chat models)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all models)</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all models)</a></li>
<li><a href="https://www.together.ai/" rel="nofollow">Together AI (chat models)</a></li>
<li><a href="https://www.perplexity.ai/" rel="nofollow">Perplexity (chat models)</a></li>
<li><a href="https://openrouter.ai/" rel="nofollow">OpenRouter (chat models)</a></li>
<li><a href="https://mistral.ai/" rel="nofollow">Mistral</a></li>
<li><a href="https://groq.com/" rel="nofollow">Groq</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
<li><a href="https://github.com/LostRuins/koboldcpp">KoboldCPP</a></li>
<li><a href="https://github.com/BerriAI/litellm">LiteLLM</a></li>
<li><a href="https://github.com/oobabooga/text-generation-webui">Text Generation Web UI</a></li>
</ul>
<p dir="auto"><strong>Embedder models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/models/README.md">AnythingLLM Native Embedder</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
<li><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" rel="nofollow">Azure OpenAI</a></li>
<li><a href="https://localai.io/" rel="nofollow">LocalAi (all)</a></li>
<li><a href="https://ollama.ai/" rel="nofollow">Ollama (all)</a></li>
<li><a href="https://lmstudio.ai/" rel="nofollow">LM Studio (all)</a></li>
<li><a href="https://cohere.com/" rel="nofollow">Cohere</a></li>
</ul>
<p dir="auto"><strong>Audio Transcription models:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/Mintplex-Labs/anything-llm/tree/master/server/storage/models#audiovideo-transcription">AnythingLLM Built-in</a> (default)</li>
<li><a href="https://openai.com/" rel="nofollow">OpenAI</a></li>
</ul>
<p dir="auto"><strong>TTS (text-to-speech) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
<li><a href="https://github.com/rhasspy/piper">PiperTTSLocal - runs in browser</a></li>
<li><a href="https://platform.openai.com/docs/guides/text-to-speech/voice-options" rel="nofollow">OpenAI TTS</a></li>
<li><a href="https://elevenlabs.io/" rel="nofollow">ElevenLabs</a></li>
</ul>
<p dir="auto"><strong>STT (speech-to-text) support:</strong></p>
<ul dir="auto">
<li>Native Browser Built-in (default)</li>
</ul>
<p dir="auto"><strong>Vector Databases:</strong></p>
<ul dir="auto">
<li><a href="https://github.com/lancedb/lancedb">LanceDB</a> (default)</li>
<li><a href="https://www.datastax.com/products/datastax-astra" rel="nofollow">Astra DB</a></li>
<li><a href="https://pinecone.io/" rel="nofollow">Pinecone</a></li>
<li><a href="https://trychroma.com/" rel="nofollow">Chroma</a></li>
<li><a href="https://weaviate.io/" rel="nofollow">Weaviate</a></li>
<li><a href="https://qdrant.tech/" rel="nofollow">Qdrant</a></li>
<li><a href="https://milvus.io/" rel="nofollow">Milvus</a></li>
<li><a href="https://zilliz.com/" rel="nofollow">Zilliz</a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Technical Overview</h3><a id="user-content-technical-overview" aria-label="Permalink: Technical Overview" href="#technical-overview"></a></p>
<p dir="auto">This monorepo consists of three main sections:</p>
<ul dir="auto">
<li><code>frontend</code>: A viteJS + React frontend that you can run to easily create and manage all your content the LLM can use.</li>
<li><code>server</code>: A NodeJS express server to handle all the interactions and do all the vectorDB management and LLM interactions.</li>
<li><code>collector</code>: NodeJS express server that process and parses documents from the UI.</li>
<li><code>docker</code>: Docker instructions and build process + information for building from source.</li>
<li><code>embed</code>: Submodule for generation &amp; creation of the <a href="https://github.com/Mintplex-Labs/anythingllm-embed">web embed widget</a>.</li>
<li><code>browser-extension</code>: Submodule for the <a href="https://github.com/Mintplex-Labs/anythingllm-extension">chrome browser extension</a>.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🛳 Self Hosting</h2><a id="user-content--self-hosting" aria-label="Permalink: 🛳 Self Hosting" href="#-self-hosting"></a></p>
<p dir="auto">Mintplex Labs &amp; the community maintain a number of deployment methods, scripts, and templates that you can use to run AnythingLLM locally. Refer to the table below to read how to deploy on your preferred environment or to automatically deploy.</p>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Docker</th>
<th>AWS</th>
<th>GCP</th>
<th>Digital Ocean</th>
<th>Render.com</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/docker/HOW_TO_USE_DOCKER.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/docker.png" alt="Deploy on Docker"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/aws/cloudformation/DEPLOY.md"><img src="https://github.com/Mintplex-Labs/anything-llm/raw/master/images/deployBtns/aws.png" alt="Deploy on AWS"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/gcp/deployment/DEPLOY.md"><img src="https://camo.githubusercontent.com/8e9acf4df0af19c7eb27d48d2bd9966f7311d8ae1fe4900f504b8d4eabb8d769/68747470733a2f2f6465706c6f792e636c6f75642e72756e2f627574746f6e2e737667" alt="Deploy on GCP" data-canonical-src="https://deploy.cloud.run/button.svg"></a></td>
<td><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/cloud-deployments/digitalocean/terraform/DEPLOY.md"><img src="https://camo.githubusercontent.com/e093a0ed531124a715aad44362848ca2cff28c3182c2c0ca4a70b2564b681f59/68747470733a2f2f7777772e6465706c6f79746f646f2e636f6d2f646f2d62746e2d626c75652e737667" alt="Deploy on DigitalOcean" data-canonical-src="https://www.deploytodo.com/do-btn-blue.svg"></a></td>
<td><a href="https://render.com/deploy?repo=https://github.com/Mintplex-Labs/anything-llm&amp;branch=render" rel="nofollow"><img src="https://camo.githubusercontent.com/a103822afe1d58c7da6beafbc0c65bb7b8d622dd193dded1b45b3c0ad6466d82/68747470733a2f2f72656e6465722e636f6d2f696d616765732f6465706c6f792d746f2d72656e6465722d627574746f6e2e737667" alt="Deploy on Render.com" data-canonical-src="https://render.com/images/deploy-to-render-button.svg"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<markdown-accessiblity-table><table>
<thead>
<tr>
<th>Railway</th>
<th>RepoCloud</th>
<th>Elestio</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://railway.app/template/HNSCS1?referralCode=WFgJkn" rel="nofollow"><img src="https://camo.githubusercontent.com/e4002051668809c220b10ad92ddd6fb87f365d8cd4ff470e0aeca3bc5b05450e/68747470733a2f2f7261696c7761792e6170702f627574746f6e2e737667" alt="Deploy on Railway" data-canonical-src="https://railway.app/button.svg"></a></td>
<td><a href="https://repocloud.io/details/?app_id=276" rel="nofollow"><img src="https://camo.githubusercontent.com/ac294f4b769f6436dadb17205434b234b32e4d88831f182c522fd36b4534a7a3/68747470733a2f2f64313674307063343834367835322e636c6f756466726f6e742e6e65742f6465706c6f796c6f62652e737667" alt="Deploy on RepoCloud" data-canonical-src="https://d16t0pc4846x52.cloudfront.net/deploylobe.svg"></a></td>
<td><a href="https://elest.io/open-source/anythingllm" rel="nofollow"><img src="https://camo.githubusercontent.com/76131e33a65d9a0728265791dcf78030580a9932466d139e5ae38f87f926c5b5/68747470733a2f2f656c6573742e696f2f696d616765732f6c6f676f732f6465706c6f792d746f2d656c657374696f2d62746e2e706e67" alt="Deploy on Elestio" data-canonical-src="https://elest.io/images/logos/deploy-to-elestio-btn.png"></a></td>
</tr>
</tbody>
</table></markdown-accessiblity-table>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/BARE_METAL.md">or set up a production AnythingLLM instance without Docker →</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to setup for development</h2><a id="user-content-how-to-setup-for-development" aria-label="Permalink: How to setup for development" href="#how-to-setup-for-development"></a></p>
<ul dir="auto">
<li><code>yarn setup</code> To fill in the required <code>.env</code> files you'll need in each of the application sections (from root of repo).
<ul dir="auto">
<li>Go fill those out before proceeding. Ensure <code>server/.env.development</code> is filled or else things won't work right.</li>
</ul>
</li>
<li><code>yarn dev:server</code> To boot the server locally (from root of repo).</li>
<li><code>yarn dev:frontend</code> To boot the frontend locally (from root of repo).</li>
<li><code>yarn dev:collector</code> To then run the document collector (from root of repo).</li>
</ul>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/documents/DOCUMENTS.md">Learn about documents</a></p>
<p dir="auto"><a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/server/storage/vector-cache/VECTOR_CACHE.md">Learn about vector caching</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Telemetry &amp; Privacy</h2><a id="user-content-telemetry--privacy" aria-label="Permalink: Telemetry &amp; Privacy" href="#telemetry--privacy"></a></p>
<p dir="auto">AnythingLLM by Mintplex Labs Inc contains a telemetry feature that collects anonymous usage information.</p>
<details>
<summary><kbd>More about Telemetry &amp; Privacy for AnythingLLM</kbd></summary>
<p dir="auto"><h3 tabindex="-1" dir="auto">Why?</h3><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">We use this information to help us understand how AnythingLLM is used, to help us prioritize work on new features and bug fixes, and to help us improve AnythingLLM's performance and stability.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Opting out</h3><a id="user-content-opting-out" aria-label="Permalink: Opting out" href="#opting-out"></a></p>
<p dir="auto">Set <code>DISABLE_TELEMETRY</code> in your server or docker .env settings to "true" to opt out of telemetry. You can also do this in-app by going to the sidebar &gt; <code>Privacy</code> and disabling telemetry.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">What do you explicitly track?</h3><a id="user-content-what-do-you-explicitly-track" aria-label="Permalink: What do you explicitly track?" href="#what-do-you-explicitly-track"></a></p>
<p dir="auto">We will only track usage details that help us make product and roadmap decisions, specifically:</p>
<ul dir="auto">
<li>Typ of your installation (Docker or Desktop)</li>
<li>When a document is added or removed. No information <em>about</em> the document. Just that the event occurred. This gives us an idea of use.</li>
<li>Type of vector database in use. Let's us know which vector database provider is the most used to prioritize changes when updates arrive for that provider.</li>
<li>Type of LLM in use. Let's us know the most popular choice and prioritize changes when updates arrive for that provider.</li>
<li>Chat is sent. This is the most regular "event" and gives us an idea of the daily-activity of this project across all installations. Again, only the event is sent - we have no information on the nature or content of the chat itself.</li>
</ul>
<p dir="auto">You can verify these claims by finding all locations <code>Telemetry.sendTelemetry</code> is called. Additionally these events are written to the output log so you can also see the specific data which was sent - if enabled. No IP or other identifying information is collected. The Telemetry provider is <a href="https://posthog.com/" rel="nofollow">PostHog</a> - an open-source telemetry collection service.</p>
<p dir="auto"><a href="https://github.com/search?q=repo%3AMintplex-Labs%2Fanything-llm%20.sendTelemetry(&amp;type=code">View all telemetry events in source code</a></p>
</details>
<p dir="auto"><h2 tabindex="-1" dir="auto">👋 Contributing</h2><a id="user-content--contributing" aria-label="Permalink: 👋 Contributing" href="#-contributing"></a></p>
<ul dir="auto">
<li>create issue</li>
<li>create PR with branch name format of <code>&lt;issue number&gt;-&lt;short name&gt;</code></li>
<li>LGTM from core-team</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">🌟 Contributors</h2><a id="user-content--contributors" aria-label="Permalink: 🌟 Contributors" href="#-contributors"></a></p>
<p dir="auto"><a href="https://github.com/mintplex-labs/anything-llm/graphs/contributors"><img src="https://camo.githubusercontent.com/216243cb397375babf4a9b21f6a6968b7589f578ff1daa0db6ae56a33ca4997a/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d" alt="anythingllm contributors" data-canonical-src="https://contrib.rocks/image?repo=mintplex-labs/anything-llm"></a></p>
<p dir="auto"><a href="https://star-history.com/#mintplex-labs/anything-llm&amp;Date" rel="nofollow"><img src="https://camo.githubusercontent.com/455256132c080bc5bbf5419d1c14fa4d1a5727d75c41c4df0bc8e8fe7dbccb63/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d6d696e74706c65782d6c6162732f616e797468696e672d6c6c6d26747970653d54696d656c696e65" alt="Star History Chart" data-canonical-src="https://api.star-history.com/svg?repos=mintplex-labs/anything-llm&amp;type=Timeline"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">🔗 More Products</h2><a id="user-content--more-products" aria-label="Permalink: 🔗 More Products" href="#-more-products"></a></p>
<ul dir="auto">
<li><strong><a href="https://github.com/mintplex-labs/vector-admin">VectorAdmin</a>:</strong> An all-in-one GUI &amp; tool-suite for managing vector databases.</li>
<li><strong><a href="https://github.com/Mintplex-Labs/openai-assistant-swarm">OpenAI Assistant Swarm</a>:</strong> Turn your entire library of OpenAI assistants into one single army commanded from a single agent.</li>
</ul>
<p dir="auto"><a href="#readme-top"><img src="https://camo.githubusercontent.com/d658b6c3935e61bd4aab9a571190fc3c48cbafc89fe6b16250c0cba28ed73234/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2d4241434b5f544f5f544f502d3232323632383f7374796c653d666c61742d737175617265" alt="" data-canonical-src="https://img.shields.io/badge/-BACK_TO_TOP-222628?style=flat-square"></a></p>
<hr>
<p dir="auto">Copyright © 2024 <a href="https://github.com/mintplex-labs">Mintplex Labs</a>. <br>
This project is <a href="https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE">MIT</a> licensed.</p>

</article></div></div>]]></description>
        </item>
    </channel>
</rss>