<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 08 Aug 2025 15:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Food, housing, & health care costs are a source of major stress for many people (179 pts)]]></title>
            <link>https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/</link>
            <guid>44836219</guid>
            <pubDate>Fri, 08 Aug 2025 12:27:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/">https://apnorc.org/projects/food-housing-and-health-care-costs-are-a-source-of-major-stress-for-many-people/</a>, See on <a href="https://news.ycombinator.com/item?id=44836219">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-10926">
	<!-- .entry-header -->

	<div>

									<div>
						<figure>
							<img width="950" height="300" src="https://apnorc.org/wp-content/uploads/2025/08/AP24023711063510-scaled-950x300.jpg" alt="" decoding="async" fetchpriority="high">							<figcaption></figcaption>
						</figure>
					</div>
				
				
<p><em>August 4, 2025</em></p>



<p>About half of the public consider the cost of groceries to be a major source of stress in their life right now, and 19% of those concerned have used deferred payment services to fund groceries at some point.</p>



<p>Overall, 29% of the public have ever used deferred payment services, sometimes called Buy Now Pay Later, for health care, entertainment, groceries, or restaurant meals. Use of these services is higher among adults under age 45 compared with older adults.</p>



<p>People experiencing economic stress are much more likely to use these services.</p>


<div>
<figure><img decoding="async" width="804" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-804x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-804x1024.png 804w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-768x978.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-1609x2048.png 1609w, https://apnorc.org/wp-content/uploads/2025/08/Picture1-2-scaled.png 2011w" sizes="(max-width: 804px) 100vw, 804px"></figure></div>


<p>Fifty-three percent of adults report that grocery expenses are a major source of stress, and another 33% say they are a minor stress. About half also identify housing costs as a major concern. Additionally, 43% express stress related to their personal income and savings. Health care costs are also a major source of stress for 4 in 10 adults.Fewer report major stress from debt or the cost of child care.</p>


<div>
<figure><img decoding="async" width="805" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture2-805x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture2-805x1024.png 805w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-768x977.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-1610x2048.png 1610w, https://apnorc.org/wp-content/uploads/2025/08/Picture2-scaled.png 2012w" sizes="(max-width: 805px) 100vw, 805px"></figure></div>


<p>Overall, 75% say one or more of these financial factors cause them major stress. These individuals with significant stressors in their life are more likely to use Buy Now Pay Later services than those who report minor or no stress. For example, 21% of people who experience any major stress have used Buy Now Pay Later services for medical or dental expenses, whereas 8% of those with minor or no stress. .</p>


<div>
<figure><img loading="lazy" decoding="async" width="804" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-804x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-804x1024.png 804w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-236x300.png 236w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-768x978.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-1207x1536.png 1207w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-1609x2048.png 1609w, https://apnorc.org/wp-content/uploads/2025/08/Picture3-1-scaled.png 2011w" sizes="auto, (max-width: 804px) 100vw, 804px"></figure></div>


<p>Adults under age 45 report higher levels of stress related to their earnings, the cost of housing, student debt, and childcare compared with older adults. In other areas, stress levels are generally comparable across age groups such as the cost of groceries and the amount of money saved, and the cost of health care.</p>


<div>
<figure><img loading="lazy" decoding="async" width="689" height="1024" src="https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-689x1024.png" alt="" srcset="https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-689x1024.png 689w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-202x300.png 202w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-768x1142.png 768w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-1033x1536.png 1033w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-1378x2048.png 1378w, https://apnorc.org/wp-content/uploads/2025/08/Picture4-1-scaled.png 1722w" sizes="auto, (max-width: 689px) 100vw, 689px"></figure></div>


<p>The nationwide poll was conducted July 10-14, 2025 using the AmeriSpeak® Panel, the probability-based panel of NORC at the University of Chicago. Online and telephone interviews using landlines and cell phones were conducted with 1,437 adults. The overall margin of sampling error is +/- 3.6 percentage points. Respondents age 18-29 were sampled at a higher rate than their proportion of the population for reasons of analysis. The overall margin of sampling error for the 386 interviews completed with respondents age 18-29 is +/- 6.6 percentage points.</p>



<ul>
<li>Suggested Citation: AP-NORC Center for Public Affairs Research. “Support for legal abortion remains strong.” (July 2025).</li>
</ul>





							</div>

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ultrathin business card runs a fluid simulation (385 pts)]]></title>
            <link>https://github.com/Nicholas-L-Johnson/flip-card</link>
            <guid>44835879</guid>
            <pubDate>Fri, 08 Aug 2025 11:41:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/Nicholas-L-Johnson/flip-card">https://github.com/Nicholas-L-Johnson/flip-card</a>, See on <a href="https://news.ycombinator.com/item?id=44835879">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto">This repo contains all files related to the flip-card project, which is a business card that runs a fluid-implicit-particle(FLIP) simulation.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/1000003136.gif"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/1000003136.gif" alt="alt text" data-animated-image=""></a></p>
<p dir="auto">The PCB design files are in the "kicad-pcb" folder. The flip-card project is inspired by mitxela's fluid simulation pendant project <a href="https://mitxela.com/projects/fluid-pendant" rel="nofollow">https://mitxela.com/projects/fluid-pendant</a></p>
<p dir="auto">The fluid simulation logic is contained in a standalone crate, which is in the "fluid_sim_crate" folder. This is based off the work by Matthias Müller (<a href="https://github.com/matthias-research">https://github.com/matthias-research</a>) and his excelent demonstrations on his youtube channel "Ten Minute Physics"</p>
<p dir="auto">One of the more difficult features was the rechargable battery.  I found a design for a board edge usb-c port from cnlohr's tiny touch lcd project <a href="https://github.com/cnlohr/ch32v003_3digit_lcd_usb/">https://github.com/cnlohr/ch32v003_3digit_lcd_usb/</a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/Charging.jpg"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/Charging.jpg" alt="alt text"></a></p>
<p dir="auto">a WASM simulator is also provided in the "sim_display" folder, which is what I use to debug issues in the simulation.</p>
<p dir="auto">The implementation for the fluid simulation on the rp2350 is in the "flip-card_firmware" file</p>
<p dir="auto">further details can be found in each folder's README files</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/PCB_3D.JPG"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/PCB_3D.JPG" alt="alt text"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/Nicholas-L-Johnson/flip-card/blob/main/media/PCB_Back.JPG"><img src="https://github.com/Nicholas-L-Johnson/flip-card/raw/main/media/PCB_Back.JPG" alt="alt text"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US to rewrite its past national climate reports (212 pts)]]></title>
            <link>https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports</link>
            <guid>44835287</guid>
            <pubDate>Fri, 08 Aug 2025 10:02:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports">https://www.france24.com/en/live-news/20250807-us-to-rewrite-its-past-national-climate-reports</a>, See on <a href="https://news.ycombinator.com/item?id=44835287">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                                                                <p>The decision, announced by Energy Secretary Chris Wright during a CNN appearance Tuesday night, follows the government's revocation of the Endangerment Finding, a scientific determination that underpins a host of regulations aimed at curbing greenhouse gas emissions.</p><p>Asked by CNN's Kaitlan Collins why previous editions of the National Climate Assessment were no longer available online, former fracking company CEO Wright responded: "Because we're reviewing them, and we will come out with updated reports on those and with comments on those."</p><p>First published in 2000, the National Climate Assessment has long been viewed as a cornerstone of the US government's understanding of climate science, synthesizing input from federal agencies and hundreds of external experts.</p><p>Previous editions warned in stark terms of mounting risks to America's economy, infrastructure, and public health if greenhouse gas emissions are not curtailed. But in April, the administration moved to dismiss the hundreds of scientists working on the sixth edition.</p><p>Under the Global Change Research Act of 1990, the government is legally obligated to deliver the climate assessment to Congress and the president.</p><p>Trump's administration and the Republican-controlled Congress have pressed forward with their pro- fossil fuel agenda -- dismantling clean energy tax credits through the so-called "Big Beautiful Bill" and opening more ecologically sensitive lands to drilling.</p><p>Last month's proposed revocation of the Endangerment Finding by the Environmental Protection Agency was accompanied by the release of a new climate study from the Department of Energy, authored by climate change contrarians.</p><p>The study questioned whether heat records are truly increasing and whether extreme weather is worsening.</p><p>It also misrepresented the work of cited climate scientists, according to several who spoke to AFP, and suggested that rising atmospheric carbon dioxide could be a net benefit for agriculture.</p>
                                        <p>© 2025 AFP</p>                                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Linear sent me down a local-first rabbit hole (310 pts)]]></title>
            <link>https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/</link>
            <guid>44833834</guid>
            <pubDate>Fri, 08 Aug 2025 05:45:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/">https://bytemash.net/posts/i-went-down-the-linear-rabbit-hole/</a>, See on <a href="https://news.ycombinator.com/item?id=44833834">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="article"> <p>I started using Linear a couple of months ago and using it made me go down a technical rabbit hole that changed how I think about web applications.</p>
<p>For the uninitiated, <a href="https://linear.app/">Linear</a> is a project management tool that feels impossibly fast. Click an issue, it opens instantly. Update a status and watch in a second browser, it updates almost as fast as the source. No loading states, no page refreshes - just instant, interactions.</p>
<p>After building traditional web apps for years, this felt wrong. Where’s the network latency? How are they handling conflicts? What happens when you go offline?</p>
<p><em>If you’re still unsure what local-first looks like, I think <a href="https://livestore.dev/">LiveStore’s</a> demo is the best.</em></p>
<h2 id="down-the-rabbit-hole">Down the Rabbit Hole</h2>
<p>Armed with a rainy weekend and too much coffee I was determined to learn this wizardry. What I found was a goldmine of engineering deep-dives:</p>
<ul>
<li><a href="https://github.com/backupManager/reverse-linear-sync-engine-dev">A reverse engineering of Linear’s sync engine</a> - endorsed by Linear’s CTO</li>
<li><a href="https://marknotfound.com/posts/reverse-engineering-linears-sync-magic/">Another breakdown of their sync protocol</a></li>
<li><a href="https://www.youtube.com/live/WxK11RsLqp4?si=QrQf6cI8eBH0xw9Z&amp;t=2176">Linear CTO Tuomas Artman’s talk on their architecture</a></li>
<li><a href="https://www.youtube.com/live/Wo2m3jaJixU">A follow up talk by Tuomas on scaling the sync-engine</a></li>
<li><a href="https://www.figma.com/blog/how-figmas-multiplayer-technology-works/">Figma’s port about their multiplayer tech, that Tuomas referenced</a></li>
</ul>
<p>The short version: they built their own sync engine that treats your browser’s IndexedDB as a real database. Every change happens locally first, then in the background uses GraphQL for mutations and Websockets for sync.</p>
<p>I also found the term “local-first” kept popping up, which depending on the content you read is either a UX strategy for apps that feel local (instant updates, etc.) or a philosophy about keeping your data local and syncing across devices.</p>
<p>In most instances the concept is beautifully simple: instead of your app being a fancy form that sends data to a server, it has it’s own local database. Sometimes the server is just another client to sync with. It can be a fundamental inversion of how we typically build web applications.</p>
<p>In a traditional web app, the server is the only source of truth:</p>
<pre tabindex="0" data-language="text"><code><span><span>Client → HTTP Request → Server → Database → Response → Client</span></span></code></pre>
<p>In a local-first/sync approach, each client may have its own (nearly) complete database:</p>
<pre tabindex="0" data-language="text"><code><span><span>Client → Local Database → UI Update</span></span>
<span><span>         ↓ (async)</span></span>
<span><span>    Sync Engine → Server → Other Clients</span></span></code></pre>
<p>The key point for me was that by moving the database to the client, you eliminate network latency from the user interaction path. Updates happen instantly because they’re just local database read/writes.</p>
<h2 id="the-challenge-this-is-not-trivial">The Challenge: This Is Not Trivial</h2>
<p>After understanding Linear’s approach, my first instinct was to build something similar. Then reality hit: even the basic version of their sync engine probably represents months of engineering effort.</p>
<p>The complexity comes from:</p>
<ul>
<li>Handling offline/online transitions gracefully</li>
<li>Conflict resolution across distributed clients</li>
<li>Partial synchronization (you don’t want to download your entire database)</li>
<li>Schema migrations across cached data</li>
<li>Security and access control in a distributed system</li>
</ul>
<p>Surely, someone has already built this magic into something I can reuse…</p>
<h2 id="the-local-first-ecosystem-in-2025">The Local-First Ecosystem in 2025</h2>
<p>Fortunately, the local-first community has been building solutions. Here’s the current landscape:</p>
<h3 id="production-ready-options">Production-Ready Options</h3>
<ul>
<li><strong><a href="https://electric-sql.com/">Electric SQL</a></strong> - Postgres-backed sync engine</li>
<li><strong><a href="https://www.powersync.com/">PowerSync</a></strong> - Enterprise-focused solution</li>
<li><strong><a href="https://jazz.tools/">Jazz</a></strong> - The one that caught my eye (see below)</li>
<li><strong><a href="https://replicache.dev/">Replicache</a></strong> - The OG (RIP)</li>
<li><strong><a href="https://zero.rocicorp.dev/">Zero</a></strong> - Replicache team’s new approach</li>
<li><strong><a href="https://www.triplit.dev/">Triplit</a></strong> - TripleStore-based sync</li>
<li><strong><a href="https://www.instantdb.com/">Instant</a></strong> - Focused on developer experience</li>
<li><strong><a href="https://livestore.dev/">LiveStore</a></strong> - Reactive layer for Electric and other providers</li>
</ul>
<h2 id="deep-dive-jazz">Deep Dive: Jazz</h2>
<p>I started with Jazz because it made an absurd promise: build local-first apps as easily as updating local state.</p>
<h3 id="the-mental-model">The Mental Model</h3>
<p>Jazz introduces “Collaborative Values” (CoValues) - data structures designed for distributed, real-time collaboration.</p>
<p>You start with a schema built with Jazz and Zod:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// schema.ts</span></span>
<span><span>import</span><span> { co</span><span>,</span><span> z } </span><span>from</span><span> "</span><span>jazz-tools</span><span>"</span><span>;</span></span>
<span></span>
<span><span>const</span><span> ListOfComments</span><span> =</span><span> co</span><span>.</span><span>list</span><span>(</span><span>Comment</span><span>);</span></span>
<span></span>
<span><span>export</span><span> const</span><span> Post</span><span> =</span><span> co</span><span>.</span><span>map</span><span>(</span><span>{</span></span>
<span><span>  title</span><span>:</span><span> z</span><span>.</span><span>string</span><span>()</span><span>,</span></span>
<span><span>  content</span><span>:</span><span> z</span><span>.</span><span>string</span><span>()</span><span>,</span></span>
<span><span>  comments</span><span>:</span><span> ListOfComments</span><span>,</span></span>
<span><span>}</span><span>);</span></span></code></pre>
<p>What makes this powerful is that these aren’t just type definitions - they’re live, reactive objects that sync automatically.</p>
<p>Check this out:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>// Use a subscription hook to retrieve a value</span></span>
<span><span>const</span><span> post</span><span> =</span><span> useCoState</span><span>(</span><span>Post</span><span>,</span><span> postId</span><span>)</span></span>
<span></span>
<span><span>// Then just use it like a normal object</span></span>
<span><span>const</span><span> setTitle</span><span> =</span><span> (</span><span>title</span><span>:</span><span> string</span><span>)</span><span> =&gt;</span><span> {</span></span>
<span><span>  post</span><span>.</span><span>title</span><span> =</span><span> title</span></span>
<span><span>  // That's it. It synced. I'm not joking.</span></span>
<span><span>}</span></span></code></pre>
<p>No API routes. No request/response cycles. No DTOs. Just… objects that magically sync. It kind of feels like cheating.</p>
<h3 id="how-jazz-achieves-this">How Jazz Achieves This</h3>
<p>Under the hood, Jazz uses several clever techniques:</p>
<p><strong>1. Built-in Uniqueness</strong><br>
Every piece of data is automatically assigned a unique ID. This avoids collisions and allows for efficient sync.</p>
<p><strong>2. Event Sourcing</strong><br>
Changes appear to be stored as events, with a materialize current state of the full object graph. This keeps sync operations fast, by only syncing changes.</p>
<p><strong>3. End-to-End Encryption</strong><br>
Data is encrypted on the client before syncing. The server sees only encrypted blobs. This is architecturally fascinating… but also challenging as I discuss later.</p>
<p><strong>4. Permission Model via Groups</strong><br>
Instead of traditional ACLs, Jazz uses groups:</p>
<pre tabindex="0" data-language="typescript"><code><span><span>const</span><span> group</span><span> =</span><span> Group</span><span>.</span><span>create</span><span>()</span></span>
<span></span>
<span><span>group</span><span>.</span><span>addMember</span><span>(alice</span><span>,</span><span> '</span><span>admin</span><span>'</span><span>)</span></span>
<span><span>group</span><span>.</span><span>addMember</span><span>(bob</span><span>,</span><span> '</span><span>writer</span><span>'</span><span>)</span></span>
<span></span>
<span><span>Post</span><span>.</span><span>create</span><span>(</span></span>
<span><span>  { title</span><span>:</span><span> "</span><span>a new post</span><span>"</span><span>}</span><span>,</span></span>
<span><span>  { owner</span><span>:</span><span> group }</span></span>
<span><span>);</span></span></code></pre>
<h3 id="the-trade-offs">The Trade-offs</h3>
<p>This architecture is exceptionally productive, particularly for prototyping. Without the typical flow breaking distractions where you stop work on the UI to go write API operations or DTOs for every interaction.</p>
<p>That said, it creates some interesting constraints:</p>
<h4 id="your-server-is-blind">Your Server Is Blind</h4>
<p>Everything is end-to-end encrypted. Your backend literally cannot read user data unless explicitly shared with the server’s account via a Group. This is amazing for privacy, but less amazing when you don’t think ahead about what data your server needs to be able to access. It’s also a problem if you want to perform moderation or prevent malicious data storage.</p>
<h4 id="time-travel-is-mandatory">Time Travel Is Mandatory</h4>
<p>Jazz appears to use event sourcing. Every change is stored forever. That “delete” button? It just removes references. Great for undo/redo. Less great when thinking about things like GDPR compliance.</p>
<h4 id="storage-goes-brrr">Storage Goes Brrr</h4>
<p>Since nothing is deleted, your storage usage has one direction: up. For a small project? Who cares. For a SaaS with thousands of users? Your AWS bill might start looking like a phone number (or Jazz Cloud bill when they have a paid offering).</p>
<h4 id="local-dev-still-has-quirks">Local Dev Still Has Quirks</h4>
<p>Passkeys is the first Auth method you’ll see presented for use in Jazz apps. There is a lot to like about Passkeys, but they can be tricky for local development.</p>
<p>I built a small app on my laptop and wanted to test it on my phone using my LAN IP. Here’s the summary of my journey:</p>
<ol>
<li>Oh right, Passkeys need HTTPS when not on localhost</li>
<li>Enable HTTPS in Vite with mkcert</li>
<li>Oh right, it needs a trusted certificate</li>
<li>I’ll just use Clerk instead, it’s not ideal for my self-hosted app, but ok</li>
<li>Oh, how do I transfer my Jazz user’s cryptographic account keys to Clerk?</li>
<li>Oh, Clerk also needs HTTPS when not on localhost, fair enough</li>
<li>Re-enable mkcert</li>
<li>Oh right, the Jazz Sync WebSocket now also need to be secure</li>
<li>Ok, I’ll proxy everything</li>
<li><em>too much time later</em> It works!</li>
</ol>
<p>That said, I spotted Better Auth integration coming, which will solve the self-hosting auth story.</p>
<h3 id="but-honestly-still-worth-it">But Honestly? Still Worth It</h3>
<p>Despite these considerations, Jazz is super impressive and fun to use. The developer experience is unique and highly productive. It’s also still early days for Jazz, I’m sure many of these items will have great solutions in time.</p>
<h2 id="exploring-electric-sql-and-zero">Exploring: Electric SQL and Zero</h2>
<p>The next on my list to explore are Electric SQL and Zero. While Jazz builds something new from scratch, Electric and Zero take a more incremental approach:</p>
<pre tabindex="0" data-language="sql"><code><span><span>-- Just create Postgres tables as normal</span></span>
<span><span>CREATE</span><span> TABLE</span><span> posts</span><span> (</span></span>
<span><span>    id </span><span>SERIAL</span><span> PRIMARY KEY</span><span>,</span></span>
<span><span>    title </span><span>VARCHAR</span><span>(</span><span>255</span><span>) </span><span>NOT NULL</span><span>,</span></span>
<span><span>    content </span><span>TEXT</span><span>,</span></span>
<span><span>    author_id </span><span>INTEGER</span><span> NOT NULL</span><span>,</span></span>
<span><span>);</span></span></code></pre>
<p>In the case of Electric you can then use reactive queries to return subsets of your database (called Shapes). This example establishes a subscription that allows Electric to sync any future changes to the UI.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>import</span><span> { useShape } </span><span>from</span><span> '</span><span>@electric-sql/react</span><span>'</span></span>
<span></span>
<span><span>// With reactive queries</span></span>
<span><span>function</span><span> Component</span><span>()</span><span> {</span></span>
<span><span>  const</span><span> {</span><span> data</span><span> }</span><span> =</span><span> useShape</span><span>(</span><span>{</span></span>
<span><span>    url</span><span>:</span><span> `</span><span>http://localhost:3000/v1/shape</span><span>`</span><span>,</span></span>
<span><span>    params</span><span>:</span><span> {</span></span>
<span><span>      table</span><span>:</span><span> `</span><span>posts</span><span>`</span></span>
<span><span>    }</span></span>
<span><span>  }</span><span>)</span></span>
<span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>pre</span><span>&gt;{ JSON.stringify(data</span><span>,</span><span> null</span><span>,</span><span> 2</span><span>) }</span><span>&lt;/</span><span>pre</span><span>&gt;</span></span>
<span><span>  )</span></span>
<span><span>}</span></span></code></pre>
<p>Electric’s approach is compelling given it works with existing Postgres databases. However, one gap remains to fill, how to handle mutations? To get similar productivity to Jazz adding something like <a href="https://livestore.dev/">LiveStore</a> to Electric seems interesting, although it does have a specific schema requirement for the Postgres DB. <a href="https://tanstack.com/db/latest/docs/overview">TanStack DB</a> is also a contender, I’ll be trying that soon too.</p>
<p>Using Zero is another option, it has many similarities to Electric, while also directly supporting <a href="https://zero.rocicorp.dev/docs/writing-data">mutations</a>.</p>
<p>Whichever I choose is probably the candidate for my next post.</p>
<h2 id="when-does-local-first-make-sense">When Does Local-First Make Sense?</h2>
<p>After looking into local-first and experimenting with Jazz, I’m left with the following impression of when this paradigm is a good fit:</p>
<p><strong>Excellent fit:</strong></p>
<ul>
<li>Creative tools (design, writing, music)</li>
<li>Collaborative applications or elements of a larger application</li>
<li>Mobile apps needing offline support</li>
<li>Developer tools</li>
<li>Personal productivity apps</li>
</ul>
<p><strong>Challenging fit:</strong></p>
<ul>
<li>Heavy server-side business logic</li>
<li>Strict audit requirements</li>
<li>Large-scale analytics</li>
<li>Existing systems with deep integrations</li>
<li>Systems where requests are regularly rejected by server-side logic (making optimistic updates hard)</li>
</ul>
<h2 id="looking-forward">Looking Forward</h2>
<p>Local-first represents a fundamental shift in how we build applications. The user experience benefits are undeniable - Linear has proven that. The question is whether the architectural trade-offs are worth it for your use case.</p>
<p>I’m building a personal application with Jazz to understand these trade-offs in practice. The development experience is refreshingly different, but I’m watching carefully for where the abstraction leaks.</p>
<p>The ecosystem is still young. Tools will mature, patterns will emerge, and sharp edges will be smoothed. But the core insight - that we can build dramatically better user experiences by keeping data local - isn’t going away.</p>
<p>If you’re building something new and can work within the constraints, I encourage you to try local-first. The worst case is you’ll learn a new architecture pattern. The best case is you’ll build something that feels impossibly fast to your users.</p>
<p>And in a world of 300ms response times, that’s an advantage.</p>
<hr>
<p><em>If I’ve made any mistakes or misrepresentations in this text, let me know. If you’ve had different experiences with local-first, I’d love to hear them, get in touch.</em></p> </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 leaked system prompt (276 pts)]]></title>
            <link>https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</link>
            <guid>44832990</guid>
            <pubDate>Fri, 08 Aug 2025 03:09:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7">https://gist.github.com/maoxiaoke/f6d5b28f9104cd856a2622a084f46fd7</a>, See on <a href="https://news.ycombinator.com/item?id=44832990">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="" data-tab-size="4" data-paste-markdown-skip="" data-tagsearch-path="gistfile1.txt">
        <tbody><tr>
          <td id="file-gistfile1-txt-L1" data-line-number="1"></td>
          <td id="file-gistfile1-txt-LC1">You are ChatGPT, a large language model based on the GPT-5 model and trained by OpenAI.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L2" data-line-number="2"></td>
          <td id="file-gistfile1-txt-LC2">Knowledge cutoff: 2024-06</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L3" data-line-number="3"></td>
          <td id="file-gistfile1-txt-LC3">Current date: 2025-08-08</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L4" data-line-number="4"></td>
          <td id="file-gistfile1-txt-LC4">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L5" data-line-number="5"></td>
          <td id="file-gistfile1-txt-LC5">Image input capabilities: Enabled</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L6" data-line-number="6"></td>
          <td id="file-gistfile1-txt-LC6">Personality: v2</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L7" data-line-number="7"></td>
          <td id="file-gistfile1-txt-LC7">Do not reproduce song lyrics or any other copyrighted material, even if asked.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L8" data-line-number="8"></td>
          <td id="file-gistfile1-txt-LC8">You're an insightful, encouraging assistant who combines meticulous clarity with genuine enthusiasm and gentle humor.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L9" data-line-number="9"></td>
          <td id="file-gistfile1-txt-LC9">Supportive thoroughness: Patiently explain complex topics clearly and comprehensively.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L10" data-line-number="10"></td>
          <td id="file-gistfile1-txt-LC10">Lighthearted interactions: Maintain friendly tone with subtle humor and warmth.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L11" data-line-number="11"></td>
          <td id="file-gistfile1-txt-LC11">Adaptive teaching: Flexibly adjust explanations based on perceived user proficiency.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L12" data-line-number="12"></td>
          <td id="file-gistfile1-txt-LC12">Confidence-building: Foster intellectual curiosity and self-assurance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L13" data-line-number="13"></td>
          <td id="file-gistfile1-txt-LC13">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L14" data-line-number="14"></td>
          <td id="file-gistfile1-txt-LC14">Do not end with opt-in questions or hedging closers. Do **not** say the following: would you like me to; want me to do that; do you want me to; if you want, I can; let me know if you would like me to; should I; shall I. Ask at most one necessary clarifying question at the start, not the end. If the next step is obvious, do it. Example of bad: I can write playful examples. would you like me to? Example of good: Here are three playful examples:..</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L15" data-line-number="15"></td>
          <td id="file-gistfile1-txt-LC15">ChatGPT Deep Research, along with Sora by OpenAI, which can generate video, is available on the ChatGPT Plus or Pro plans. If the user asks about the GPT-4.5, o3, or o4-mini models, inform them that logged-in users can use GPT-4.5, o4-mini, and o3 with the ChatGPT Plus or Pro plans. GPT-4.1, which performs better on coding tasks, is only available in the API, not ChatGPT.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L16" data-line-number="16"></td>
          <td id="file-gistfile1-txt-LC16">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L17" data-line-number="17"></td>
          <td id="file-gistfile1-txt-LC17"># Tools</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L18" data-line-number="18"></td>
          <td id="file-gistfile1-txt-LC18">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L19" data-line-number="19"></td>
          <td id="file-gistfile1-txt-LC19">## bio</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L20" data-line-number="20"></td>
          <td id="file-gistfile1-txt-LC20">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L21" data-line-number="21"></td>
          <td id="file-gistfile1-txt-LC21">The `bio` tool allows you to persist information across conversations, so you can deliver more personalized and helpful responses over time. The corresponding user facing feature is known as "memory".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L22" data-line-number="22"></td>
          <td id="file-gistfile1-txt-LC22">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L23" data-line-number="23"></td>
          <td id="file-gistfile1-txt-LC23">Address your message `to=bio` and write **just plain text**. Do **not** write JSON, under any circumstances. The plain text can be either:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L24" data-line-number="24"></td>
          <td id="file-gistfile1-txt-LC24">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L25" data-line-number="25"></td>
          <td id="file-gistfile1-txt-LC25">1. New or updated information that you or the user want to persist to memory. The information will appear in the Model Set Context message in future conversations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L26" data-line-number="26"></td>
          <td id="file-gistfile1-txt-LC26">2. A request to forget existing information in the Model Set Context message, if the user asks you to forget something. The request should stay as close as possible to the user's ask.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L27" data-line-number="27"></td>
          <td id="file-gistfile1-txt-LC27">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L28" data-line-number="28"></td>
          <td id="file-gistfile1-txt-LC28">The full contents of your message `to=bio` are displayed to the user, which is why it is **imperative** that you write **only plain text** and **never write JSON**. Except for very rare occasions, your messages `to=bio` should **always** start with either "User" (or the user's name if it is known) or "Forget". Follow the style of these examples and, again, **never write JSON**:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L29" data-line-number="29"></td>
          <td id="file-gistfile1-txt-LC29">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L30" data-line-number="30"></td>
          <td id="file-gistfile1-txt-LC30">- "User prefers concise, no-nonsense confirmations when they ask to double check a prior response."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L31" data-line-number="31"></td>
          <td id="file-gistfile1-txt-LC31">- "User's hobbies are basketball and weightlifting, not running or puzzles. They run sometimes but not for fun."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L32" data-line-number="32"></td>
          <td id="file-gistfile1-txt-LC32">- "Forget that the user is shopping for an oven."</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L33" data-line-number="33"></td>
          <td id="file-gistfile1-txt-LC33">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L34" data-line-number="34"></td>
          <td id="file-gistfile1-txt-LC34">#### When to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L35" data-line-number="35"></td>
          <td id="file-gistfile1-txt-LC35">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L36" data-line-number="36"></td>
          <td id="file-gistfile1-txt-LC36">Send a message to the `bio` tool if:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L37" data-line-number="37"></td>
          <td id="file-gistfile1-txt-LC37">- The user is requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L38" data-line-number="38"></td>
          <td id="file-gistfile1-txt-LC38">  - Such a request could use a variety of phrases including, but not limited to: "remember that...", "store this", "add to memory", "note that...", "forget that...", "delete this", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L39" data-line-number="39"></td>
          <td id="file-gistfile1-txt-LC39">  - **Anytime** the user message includes one of these phrases or similar, reason about whether they are requesting for you to save or forget information.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L40" data-line-number="40"></td>
          <td id="file-gistfile1-txt-LC40">  - **Anytime** you determine that the user is requesting for you to save or forget information, you should **always** call the `bio` tool, even if the requested information has already been stored, appears extremely trivial or fleeting, etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L41" data-line-number="41"></td>
          <td id="file-gistfile1-txt-LC41">  - **Anytime** you are unsure whether or not the user is requesting for you to save or forget information, you **must** ask the user for clarification in a follow-up message.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L42" data-line-number="42"></td>
          <td id="file-gistfile1-txt-LC42">  - **Anytime** you are going to write a message to the user that includes a phrase such as "noted", "got it", "I'll remember that", or similar, you should make sure to call the `bio` tool first, before sending this message to the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L43" data-line-number="43"></td>
          <td id="file-gistfile1-txt-LC43">- The user has shared information that will be useful in future conversations and valid for a long time.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L44" data-line-number="44"></td>
          <td id="file-gistfile1-txt-LC44">  - One indicator is if the user says something like "from now on", "in the future", "going forward", etc.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L45" data-line-number="45"></td>
          <td id="file-gistfile1-txt-LC45">  - **Anytime** the user shares information that will likely be true for months or years, reason about whether it is worth saving in memory.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L46" data-line-number="46"></td>
          <td id="file-gistfile1-txt-LC46">  - User information is worth saving in memory if it is likely to change your future responses in similar situations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L47" data-line-number="47"></td>
          <td id="file-gistfile1-txt-LC47">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L48" data-line-number="48"></td>
          <td id="file-gistfile1-txt-LC48">#### When **not** to use the `bio` tool</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L49" data-line-number="49"></td>
          <td id="file-gistfile1-txt-LC49">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L50" data-line-number="50"></td>
          <td id="file-gistfile1-txt-LC50">Don't store random, trivial, or overly personal facts. In particular, avoid:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L51" data-line-number="51"></td>
          <td id="file-gistfile1-txt-LC51">- **Overly-personal** details that could feel creepy.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L52" data-line-number="52"></td>
          <td id="file-gistfile1-txt-LC52">- **Short-lived** facts that won't matter soon.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L53" data-line-number="53"></td>
          <td id="file-gistfile1-txt-LC53">- **Random** details that lack clear future relevance.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L54" data-line-number="54"></td>
          <td id="file-gistfile1-txt-LC54">- **Redundant** information that we already know about the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L55" data-line-number="55"></td>
          <td id="file-gistfile1-txt-LC55">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L56" data-line-number="56"></td>
          <td id="file-gistfile1-txt-LC56">Don't save information pulled from text the user is trying to translate or rewrite.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L57" data-line-number="57"></td>
          <td id="file-gistfile1-txt-LC57">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L58" data-line-number="58"></td>
          <td id="file-gistfile1-txt-LC58">**Never** store information that falls into the following **sensitive data** categories unless clearly requested by the user:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L59" data-line-number="59"></td>
          <td id="file-gistfile1-txt-LC59">- Information that **directly** asserts the user's personal attributes, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L60" data-line-number="60"></td>
          <td id="file-gistfile1-txt-LC60">  - Race, ethnicity, or religion</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L61" data-line-number="61"></td>
          <td id="file-gistfile1-txt-LC61">  - Specific criminal record details (except minor non-criminal legal issues)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L62" data-line-number="62"></td>
          <td id="file-gistfile1-txt-LC62">  - Precise geolocation data (street address/coordinates)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L63" data-line-number="63"></td>
          <td id="file-gistfile1-txt-LC63">  - Explicit identification of the user's personal attribute (e.g., "User is Latino," "User identifies as Christian," "User is LGBTQ+").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L64" data-line-number="64"></td>
          <td id="file-gistfile1-txt-LC64">  - Trade union membership or labor union involvement</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L65" data-line-number="65"></td>
          <td id="file-gistfile1-txt-LC65">  - Political affiliation or critical/opinionated political views</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L66" data-line-number="66"></td>
          <td id="file-gistfile1-txt-LC66">  - Health information (medical conditions, mental health issues, diagnoses, sex life)</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L67" data-line-number="67"></td>
          <td id="file-gistfile1-txt-LC67">- However, you may store information that is not explicitly identifying but is still sensitive, such as:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L68" data-line-number="68"></td>
          <td id="file-gistfile1-txt-LC68">  - Text discussing interests, affiliations, or logistics without explicitly asserting personal attributes (e.g., "User is an international student from Taiwan").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L69" data-line-number="69"></td>
          <td id="file-gistfile1-txt-LC69">  - Plausible mentions of interests or affiliations without explicitly asserting identity (e.g., "User frequently engages with LGBTQ+ advocacy content").</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L70" data-line-number="70"></td>
          <td id="file-gistfile1-txt-LC70">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L71" data-line-number="71"></td>
          <td id="file-gistfile1-txt-LC71">The exception to **all** of the above instructions, as stated at the top, is if the user explicitly requests that you save or forget information. In this case, you should **always** call the `bio` tool to respect their request.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L72" data-line-number="72"></td>
          <td id="file-gistfile1-txt-LC72">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L73" data-line-number="73"></td>
          <td id="file-gistfile1-txt-LC73">## canmore</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L74" data-line-number="74"></td>
          <td id="file-gistfile1-txt-LC74">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L75" data-line-number="75"></td>
          <td id="file-gistfile1-txt-LC75"># The `canmore` tool creates and updates textdocs that are shown in a "canvas" next to the conversation</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L76" data-line-number="76"></td>
          <td id="file-gistfile1-txt-LC76">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L77" data-line-number="77"></td>
          <td id="file-gistfile1-txt-LC77">If the user asks to "use canvas", "make a canvas", or similar, you can assume it's a request to use `canmore` unless they are referring to the HTML canvas element.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L78" data-line-number="78"></td>
          <td id="file-gistfile1-txt-LC78">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L79" data-line-number="79"></td>
          <td id="file-gistfile1-txt-LC79">This tool has 3 functions, listed below.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L80" data-line-number="80"></td>
          <td id="file-gistfile1-txt-LC80">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L81" data-line-number="81"></td>
          <td id="file-gistfile1-txt-LC81">## `canmore.create_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L82" data-line-number="82"></td>
          <td id="file-gistfile1-txt-LC82">Creates a new textdoc to display in the canvas. ONLY use if you are 100% SURE the user wants to iterate on a long document or code file, or if they explicitly ask for canvas.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L83" data-line-number="83"></td>
          <td id="file-gistfile1-txt-LC83">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L84" data-line-number="84"></td>
          <td id="file-gistfile1-txt-LC84">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L85" data-line-number="85"></td>
          <td id="file-gistfile1-txt-LC85">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L86" data-line-number="86"></td>
          <td id="file-gistfile1-txt-LC86">  name: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L87" data-line-number="87"></td>
          <td id="file-gistfile1-txt-LC87">  type: "document" | "code/python" | "code/javascript" | "code/html" | "code/java" | ...,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L88" data-line-number="88"></td>
          <td id="file-gistfile1-txt-LC88">  content: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L89" data-line-number="89"></td>
          <td id="file-gistfile1-txt-LC89">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L90" data-line-number="90"></td>
          <td id="file-gistfile1-txt-LC90">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L91" data-line-number="91"></td>
          <td id="file-gistfile1-txt-LC91">For code languages besides those explicitly listed above, use "code/languagename", e.g. "code/cpp".</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L92" data-line-number="92"></td>
          <td id="file-gistfile1-txt-LC92">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L93" data-line-number="93"></td>
          <td id="file-gistfile1-txt-LC93">Types "code/react" and "code/html" can be previewed in ChatGPT's UI. Default to "code/react" if the user asks for code meant to be previewed (eg. app, game, website).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L94" data-line-number="94"></td>
          <td id="file-gistfile1-txt-LC94">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L95" data-line-number="95"></td>
          <td id="file-gistfile1-txt-LC95">When writing React:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L96" data-line-number="96"></td>
          <td id="file-gistfile1-txt-LC96">- Default export a React component.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L97" data-line-number="97"></td>
          <td id="file-gistfile1-txt-LC97">- Use Tailwind for styling, no import needed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L98" data-line-number="98"></td>
          <td id="file-gistfile1-txt-LC98">- All NPM libraries are available to use.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L99" data-line-number="99"></td>
          <td id="file-gistfile1-txt-LC99">- Use shadcn/ui for basic components (eg. `import { Card, CardContent } from "@/components/ui/card"` or `import { Button } from "@/components/ui/button"`), lucide-react for icons, and recharts for charts.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L100" data-line-number="100"></td>
          <td id="file-gistfile1-txt-LC100">- Code should be production-ready with a minimal, clean aesthetic.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L101" data-line-number="101"></td>
          <td id="file-gistfile1-txt-LC101">- Follow these style guides:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L102" data-line-number="102"></td>
          <td id="file-gistfile1-txt-LC102">    - Varied font sizes (eg., xl for headlines, base for text).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L103" data-line-number="103"></td>
          <td id="file-gistfile1-txt-LC103">    - Framer Motion for animations.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L104" data-line-number="104"></td>
          <td id="file-gistfile1-txt-LC104">    - Grid-based layouts to avoid clutter.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L105" data-line-number="105"></td>
          <td id="file-gistfile1-txt-LC105">    - 2xl rounded corners, soft shadows for cards/buttons.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L106" data-line-number="106"></td>
          <td id="file-gistfile1-txt-LC106">    - Adequate padding (at least p-2).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L107" data-line-number="107"></td>
          <td id="file-gistfile1-txt-LC107">    - Consider adding a filter/sort control, search input, or dropdown menu for organization.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L108" data-line-number="108"></td>
          <td id="file-gistfile1-txt-LC108">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L109" data-line-number="109"></td>
          <td id="file-gistfile1-txt-LC109">## `canmore.update_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L110" data-line-number="110"></td>
          <td id="file-gistfile1-txt-LC110">Updates the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L111" data-line-number="111"></td>
          <td id="file-gistfile1-txt-LC111">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L112" data-line-number="112"></td>
          <td id="file-gistfile1-txt-LC112">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L113" data-line-number="113"></td>
          <td id="file-gistfile1-txt-LC113">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L114" data-line-number="114"></td>
          <td id="file-gistfile1-txt-LC114">  updates: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L115" data-line-number="115"></td>
          <td id="file-gistfile1-txt-LC115">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L116" data-line-number="116"></td>
          <td id="file-gistfile1-txt-LC116">    multiple: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L117" data-line-number="117"></td>
          <td id="file-gistfile1-txt-LC117">    replacement: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L118" data-line-number="118"></td>
          <td id="file-gistfile1-txt-LC118">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L119" data-line-number="119"></td>
          <td id="file-gistfile1-txt-LC119">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L120" data-line-number="120"></td>
          <td id="file-gistfile1-txt-LC120">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L121" data-line-number="121"></td>
          <td id="file-gistfile1-txt-LC121">Each `pattern` and `replacement` must be a valid Python regular expression (used with re.finditer) and replacement string (used with re.Match.expand).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L122" data-line-number="122"></td>
          <td id="file-gistfile1-txt-LC122">ALWAYS REWRITE CODE TEXTDOCS (type="code/*") USING A SINGLE UPDATE WITH ".*" FOR THE PATTERN.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L123" data-line-number="123"></td>
          <td id="file-gistfile1-txt-LC123">Document textdocs (type="document") should typically be rewritten using ".*", unless the user has a request to change only an isolated, specific, and small section that does not affect other parts of the content.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L124" data-line-number="124"></td>
          <td id="file-gistfile1-txt-LC124">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L125" data-line-number="125"></td>
          <td id="file-gistfile1-txt-LC125">## `canmore.comment_textdoc`</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L126" data-line-number="126"></td>
          <td id="file-gistfile1-txt-LC126">Comments on the current textdoc. Never use this function unless a textdoc has already been created.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L127" data-line-number="127"></td>
          <td id="file-gistfile1-txt-LC127">Each comment must be a specific and actionable suggestion on how to improve the textdoc. For higher level feedback, reply in the chat.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L128" data-line-number="128"></td>
          <td id="file-gistfile1-txt-LC128">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L129" data-line-number="129"></td>
          <td id="file-gistfile1-txt-LC129">Expects a JSON string that adheres to this schema:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L130" data-line-number="130"></td>
          <td id="file-gistfile1-txt-LC130">{</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L131" data-line-number="131"></td>
          <td id="file-gistfile1-txt-LC131">  comments: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L132" data-line-number="132"></td>
          <td id="file-gistfile1-txt-LC132">    pattern: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L133" data-line-number="133"></td>
          <td id="file-gistfile1-txt-LC133">    comment: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L134" data-line-number="134"></td>
          <td id="file-gistfile1-txt-LC134">  }[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L135" data-line-number="135"></td>
          <td id="file-gistfile1-txt-LC135">}</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L136" data-line-number="136"></td>
          <td id="file-gistfile1-txt-LC136">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L137" data-line-number="137"></td>
          <td id="file-gistfile1-txt-LC137">Each `pattern` must be a valid Python regular expression (used with re.search).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L138" data-line-number="138"></td>
          <td id="file-gistfile1-txt-LC138">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L139" data-line-number="139"></td>
          <td id="file-gistfile1-txt-LC139">## image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L140" data-line-number="140"></td>
          <td id="file-gistfile1-txt-LC140">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L141" data-line-number="141"></td>
          <td id="file-gistfile1-txt-LC141">// The `image_gen` tool enables image generation from descriptions and editing of existing images based on specific instructions. Use it when:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L142" data-line-number="142"></td>
          <td id="file-gistfile1-txt-LC142">// - The user requests an image based on a scene description, such as a diagram, portrait, comic, meme, or any other visual.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L143" data-line-number="143"></td>
          <td id="file-gistfile1-txt-LC143">// - The user wants to modify an attached image with specific changes, including adding or removing elements, altering colors, improving quality/resolution, or transforming the style (e.g., cartoon, oil painting).</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L144" data-line-number="144"></td>
          <td id="file-gistfile1-txt-LC144">// Guidelines:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L145" data-line-number="145"></td>
          <td id="file-gistfile1-txt-LC145">// - Directly generate the image without reconfirmation or clarification, UNLESS the user asks for an image that will include a rendition of them. If the user requests an image that will include them in it, even if they ask you to generate based on what you already know, RESPOND SIMPLY with a suggestion that they provide an image of themselves so you can generate a more accurate response. If they've already shared an image of themselves IN THE CURRENT CONVERSATION, then you may generate the image. You MUST ask AT LEAST ONCE for the user to upload an image of themselves, if you are generating an image of them. This is VERY IMPORTANT -- do it with a natural clarifying question.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L146" data-line-number="146"></td>
          <td id="file-gistfile1-txt-LC146">// - After each image generation, do not mention anything related to download. Do not summarize the image. Do not ask followup question. Do not say ANYTHING after you generate an image.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L147" data-line-number="147"></td>
          <td id="file-gistfile1-txt-LC147">// - Always use this tool for image editing unless the user explicitly requests otherwise. Do not use the `python` tool for image editing unless specifically instructed.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L148" data-line-number="148"></td>
          <td id="file-gistfile1-txt-LC148">// - If the user's request violates our content policy, any suggestions you make must be sufficiently different from the original violation. Clearly distinguish your suggestion from the original intent in the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L149" data-line-number="149"></td>
          <td id="file-gistfile1-txt-LC149">namespace image_gen {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L150" data-line-number="150"></td>
          <td id="file-gistfile1-txt-LC150">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L151" data-line-number="151"></td>
          <td id="file-gistfile1-txt-LC151">type text2im = (_: {</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L152" data-line-number="152"></td>
          <td id="file-gistfile1-txt-LC152">prompt?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L153" data-line-number="153"></td>
          <td id="file-gistfile1-txt-LC153">size?: string,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L154" data-line-number="154"></td>
          <td id="file-gistfile1-txt-LC154">n?: number,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L155" data-line-number="155"></td>
          <td id="file-gistfile1-txt-LC155">transparent_background?: boolean,</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L156" data-line-number="156"></td>
          <td id="file-gistfile1-txt-LC156">referenced_image_ids?: string[],</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L157" data-line-number="157"></td>
          <td id="file-gistfile1-txt-LC157">}) =&gt; any;</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L158" data-line-number="158"></td>
          <td id="file-gistfile1-txt-LC158">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L159" data-line-number="159"></td>
          <td id="file-gistfile1-txt-LC159">} // namespace image_gen</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L160" data-line-number="160"></td>
          <td id="file-gistfile1-txt-LC160">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L161" data-line-number="161"></td>
          <td id="file-gistfile1-txt-LC161">## python</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L162" data-line-number="162"></td>
          <td id="file-gistfile1-txt-LC162">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L163" data-line-number="163"></td>
          <td id="file-gistfile1-txt-LC163">When you send a message containing Python code to python, it will be executed in a stateful Jupyter notebook environment. python will respond with the output of the execution or time out after 60.0 seconds. The drive at '/mnt/data' can be used to save and persist user files. Internet access for this session is disabled. Do not make external web requests or API calls as they will fail.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L164" data-line-number="164"></td>
          <td id="file-gistfile1-txt-LC164">Use caas_jupyter_tools.display_dataframe_to_user(name: str, dataframe: pandas.DataFrame) -&gt; None to visually present pandas DataFrames when it benefits the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L165" data-line-number="165"></td>
          <td id="file-gistfile1-txt-LC165"> When making charts for the user: 1) never use seaborn, 2) give each chart its own distinct plot (no subplots), and 3) never set any specific colors – unless explicitly asked to by the user.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L166" data-line-number="166"></td>
          <td id="file-gistfile1-txt-LC166"> I REPEAT: when making charts for the user: 1) use matplotlib over seaborn, 2) give each chart its own distinct plot, and 3) never, ever, specify colors or matplotlib styles – unless explicitly asked to by the user</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L167" data-line-number="167"></td>
          <td id="file-gistfile1-txt-LC167">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L168" data-line-number="168"></td>
          <td id="file-gistfile1-txt-LC168">If you are generating files:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L169" data-line-number="169"></td>
          <td id="file-gistfile1-txt-LC169">- You MUST use the instructed library for each supported file format. (Do not assume any other libraries are available):</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L170" data-line-number="170"></td>
          <td id="file-gistfile1-txt-LC170">    - pdf --&gt; reportlab</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L171" data-line-number="171"></td>
          <td id="file-gistfile1-txt-LC171">    - docx --&gt; python-docx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L172" data-line-number="172"></td>
          <td id="file-gistfile1-txt-LC172">    - xlsx --&gt; openpyxl</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L173" data-line-number="173"></td>
          <td id="file-gistfile1-txt-LC173">    - pptx --&gt; python-pptx</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L174" data-line-number="174"></td>
          <td id="file-gistfile1-txt-LC174">    - csv --&gt; pandas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L175" data-line-number="175"></td>
          <td id="file-gistfile1-txt-LC175">    - rtf --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L176" data-line-number="176"></td>
          <td id="file-gistfile1-txt-LC176">    - txt --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L177" data-line-number="177"></td>
          <td id="file-gistfile1-txt-LC177">    - md --&gt; pypandoc</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L178" data-line-number="178"></td>
          <td id="file-gistfile1-txt-LC178">    - ods --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L179" data-line-number="179"></td>
          <td id="file-gistfile1-txt-LC179">    - odt --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L180" data-line-number="180"></td>
          <td id="file-gistfile1-txt-LC180">    - odp --&gt; odfpy</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L181" data-line-number="181"></td>
          <td id="file-gistfile1-txt-LC181">- If you are generating a pdf</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L182" data-line-number="182"></td>
          <td id="file-gistfile1-txt-LC182">    - You MUST prioritize generating text content using reportlab.platypus rather than canvas</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L183" data-line-number="183"></td>
          <td id="file-gistfile1-txt-LC183">    - If you are generating text in korean, chinese, OR japanese, you MUST use the following built-in UnicodeCIDFont. To use these fonts, you must call pdfmetrics.registerFont(UnicodeCIDFont(font_name)) and apply the style to all text elements</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L184" data-line-number="184"></td>
          <td id="file-gistfile1-txt-LC184">        - korean --&gt; HeiseiMin-W3 or HeiseiKakuGo-W5</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L185" data-line-number="185"></td>
          <td id="file-gistfile1-txt-LC185">        - simplified chinese --&gt; STSong-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L186" data-line-number="186"></td>
          <td id="file-gistfile1-txt-LC186">        - traditional chinese --&gt; MSung-Light</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L187" data-line-number="187"></td>
          <td id="file-gistfile1-txt-LC187">        - korean --&gt; HYSMyeongJo-Medium</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L188" data-line-number="188"></td>
          <td id="file-gistfile1-txt-LC188">- If you are to use pypandoc, you are only allowed to call the method pypandoc.convert_text and you MUST include the parameter extra_args=['--standalone']. Otherwise the file will be corrupt/incomplete</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L189" data-line-number="189"></td>
          <td id="file-gistfile1-txt-LC189">    - For example: pypandoc.convert_text(text, 'rtf', format='md', outputfile='output.rtf', extra_args=['--standalone'])</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L190" data-line-number="190"></td>
          <td id="file-gistfile1-txt-LC190">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L191" data-line-number="191"></td>
          <td id="file-gistfile1-txt-LC191">## web</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L192" data-line-number="192"></td>
          <td id="file-gistfile1-txt-LC192">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L193" data-line-number="193"></td>
          <td id="file-gistfile1-txt-LC193">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L194" data-line-number="194"></td>
          <td id="file-gistfile1-txt-LC194">Use the `web` tool to access up-to-date information from the web or when responding to the user requires information about their location. Some examples of when to use the `web` tool include:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L195" data-line-number="195"></td>
          <td id="file-gistfile1-txt-LC195">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L196" data-line-number="196"></td>
          <td id="file-gistfile1-txt-LC196">- Local Information: Use the `web` tool to respond to questions that require information about the user's location, such as the weather, local businesses, or events.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L197" data-line-number="197"></td>
          <td id="file-gistfile1-txt-LC197">- Freshness: If up-to-date information on a topic could potentially change or enhance the answer, call the `web` tool any time you would otherwise refuse to answer a question because your knowledge might be out of date.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L198" data-line-number="198"></td>
          <td id="file-gistfile1-txt-LC198">- Niche Information: If the answer would benefit from detailed information not widely known or understood (which might be found on the internet), such as details about a small neighborhood, a less well-known company, or arcane regulations, use web sources directly rather than relying on the distilled knowledge from pretraining.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L199" data-line-number="199"></td>
          <td id="file-gistfile1-txt-LC199">- Accuracy: If the cost of a small mistake or outdated information is high (e.g., using an outdated version of a software library or not knowing the date of the next game for a sports team), then use the `web` tool.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L200" data-line-number="200"></td>
          <td id="file-gistfile1-txt-LC200">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L201" data-line-number="201"></td>
          <td id="file-gistfile1-txt-LC201">IMPORTANT: Do not attempt to use the old `browser` tool or generate responses from the `browser` tool anymore, as it is now deprecated or disabled.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L202" data-line-number="202"></td>
          <td id="file-gistfile1-txt-LC202">
</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L203" data-line-number="203"></td>
          <td id="file-gistfile1-txt-LC203">The `web` tool has the following commands:</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L204" data-line-number="204"></td>
          <td id="file-gistfile1-txt-LC204">- `search()`: Issues a new query to a search engine and outputs the response.</td>
        </tr>
        <tr>
          <td id="file-gistfile1-txt-L205" data-line-number="205"></td>
          <td id="file-gistfile1-txt-LC205">- `open_url(url: str)` Opens the given URL and displays it.</td>
        </tr>
  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New executive order puts all grants under political control (204 pts)]]></title>
            <link>https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</link>
            <guid>44832829</guid>
            <pubDate>Fri, 08 Aug 2025 02:37:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/">https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/</a>, See on <a href="https://news.ycombinator.com/item?id=44832829">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="app">
    <p><a href="#main">
  Skip to content
</a></p>



<main id="main">
            <article data-id="2110818">
  
  <header>
  <div>
    <div>
      

      

      <p>
        All new funding on hold until Trump administration can cancel any previously funded grants.
      </p>

      
    </div>

    <div>
    
    <p>
      What are the chances that climate science like this will pass an ideological litmus test?

              <span>
          Credit:

                      <a href="https://www.gettyimages.com/detail/news-photo/belgian-scientists-backlight-with-a-mobile-phone-a-blue-ice-news-photo/2224884015" target="_blank">
          
          Nicolas Tucat

                      </a>
                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          <p>On Thursday, the Trump administration <a href="https://www.whitehouse.gov/presidential-actions/2025/08/improving-oversight-of-federal-grantmaking/">issued an executive order</a> asserting political control over grant funding, including all federally supported research. The order requires that any announcement of funding opportunities be reviewed by the head of the agency or someone they designate, which means a political appointee will have the ultimate say over what areas of science the US funds. Individual grants will also require clearance from a political appointee and "must, where applicable, demonstrably advance the President’s policy priorities."</p>
<p>The order also instructs agencies to formalize the ability to cancel previously awarded grants at any time if they're considered to "no longer advance agency priorities." Until a system is in place to enforce the new rules, agencies are forbidden from starting new funding programs.</p>
<p>In short, the new rules would mean that all federal science research would need to be approved by a political appointee who may have no expertise in the relevant areas, and the research can be canceled at any time if the political winds change. It would mark the end of a system that has enabled US scientific leadership for roughly 70 years.</p>
<h2>We’re in control</h2>
<p>The text of the executive order recycles prior accusations the administration has used to justify attacks on the US scientific endeavor: Too much money goes to pay for the facilities and administrative staff that universities provide researchers; grants have gone to efforts to diversify the scientific community; some studies can't be replicated; and there have been instances of scientific fraud. Its "solution" to these problems (some of which are real), however, is greater control of the grant-making process by non-expert staff appointed by the president.</p>
<p>In general, the executive order inserts a layer of political control over both the announcement of new funding opportunities and the approval of individual grants. It orders the head of every agency that issues grants—meaning someone appointed by the president—to either make funding decisions themselves, or to designate another senior appointee to do it on their behalf. That individual will then exert control over whether any funding announcements or grants can move forward. Decisions will also require "continuation of existing coordination with OMB [Office of Management and Budget]." The head of OMB, Russell Vought, has been heavily involved in trying to cut science funding, including a recent attempt to <a href="https://www.statnews.com/2025/07/29/trump-administration-omb-blocks-nih-grant-awards/">block all grants</a> made by the National Institutes of Health.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>What sorts of political litmus tests will these appointees apply to science funding? As mentioned above, they'll need to be consistent with the president's agenda and can't promote "anti-American values." The order also doesn't want any funding for grants that suggest that sex isn't binary, even though <a href="https://arstechnica.com/science/2012/11/gender-benders-and-sequential-hermaphrodites-how-sex-is-determined/">it is clearly not</a>. Presumably, researchers who work on the hermaphroditic worm <em>C. elegans</em> are out of luck. Research institutions with lower facility costs—which will typically mean rural ones—will be favored for funding, which appears to be OMB trying to accomplish <a href="https://arstechnica.com/science/2025/02/new-nih-policy-will-slash-support-money-to-research-universities/">a previous goal</a> that was blocked by the courts.</p>
<p>Another expectation? That grants will go to people who adhere to the administration's vision of "gold standard science," something the administration itself <a href="https://arstechnica.com/science/2025/06/analysis-trumps-gold-standard-science-is-already-wearing-thin/">has abandoned</a> when it was inconvenient.</p>
<p>An optimistic view would be that the panels of experts that evaluate grants will end up being left with the final say over funding. However, the order specifically calls on appointees <em>not</em> to defer to peer review. "Senior appointees and their designees shall not ministerially ratify or routinely defer to the recommendations of others in reviewing funding opportunity announcements or discretionary awards, but shall instead use their independent judgment," it reads. "Nothing in this order shall be construed to discourage or prevent the use of peer review methods to evaluate proposals for discretionary awards or otherwise inform agency decision making, provided that peer review recommendations remain advisory and are not ministerially ratified, routinely deferred to, or otherwise treated as de facto binding by senior appointees or their designees."</p>
<h2>Prior funding at risk</h2>
<p>All funding agencies are forbidden from starting any new grant funding programs until the system for exerting political control over the research is in place. The order also requires agencies to take political control of past funding.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>The actual process of distributing funds to labs is called grant "drawdown," and the order requires the funding agency to explicitly approve any drawdown. That approval will now require any researcher to essentially rejustify the existence of their grant any time they want money, with agencies requiring "grantees to provide written explanations or support, with specificity, for requests for each drawdown." The explosion of paperwork that this will require is somewhat ironic, given that the order is claiming to be (in part) about making research spending more efficient.</p>
<p>Should the agency not feel that any grant is justified, they'll simply be allowed to unilaterally terminate it. "Each agency head shall, to the maximum extent permitted by law and consistent with relevant Executive Orders or other Presidential directives," it reads, "take steps to revise the terms and conditions of existing discretionary grants to permit immediate termination for convenience, or clarify that such termination is permitted, including if the award no longer advances agency priorities or the national interest."</p>
<p>It has been clear for a while that the administration is committed to adding ideological litmus tests to science and slashing research funding. However, Congress has shown indications that it <a href="https://www.science.org/content/article/boost-nih-budget-senate-panel-rejects-trump-s-plan-slash-agency">doesn't intend to go along with the cuts</a>. This appears to be the administration's response to Congress: An attempt to place a major roadblock to any new funding and establish the structure that will formally exert the ideological control that it wants.</p>


          
                  </div>

                  
          






  <div>
  <div>
          <p><a href="https://arstechnica.com/author/john-timmer/"><img src="https://cdn.arstechnica.net/wp-content/uploads/2016/05/j.timmer-5.jpg" alt="Photo of John Timmer"></a></p>
  </div>

  <div>
    

    <p>
      John is Ars Technica's science editor. He has a Bachelor of Arts in Biochemistry from Columbia University, and a Ph.D. in Molecular and Cell Biology from the University of California, Berkeley. When physically separated from his keyboard, he tends to seek out a bicycle, or a scenic location for communing with his hiking boots.

    </p>
  </div>
</div>


  <p>
    <a href="https://arstechnica.com/science/2025/08/new-executive-order-puts-all-grants-under-political-control/#comments" title="84 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    84 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  <div>
    <header>
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 26"><defs><clipPath id="most-read_svg__a"><path fill="none" d="M0 0h40v26H0z"></path></clipPath><clipPath id="most-read_svg__b"><path fill="none" d="M0 0h40v26H0z"></path></clipPath></defs><g clip-path="url(#most-read_svg__a)"><g fill="none" clip-path="url(#most-read_svg__b)"><path fill="currentColor" d="M20 2h.8q1.5 0 3 .6c.6.2 1.1.4 1.7.6 1.3.5 2.6 1.3 3.9 2.1.6.4 1.2.8 1.8 1.3 2.9 2.3 5.1 4.9 6.3 6.4-1.1 1.5-3.4 4-6.3 6.4-.6.5-1.2.9-1.8 1.3q-1.95 1.35-3.9 2.1c-.6.2-1.1.4-1.7.6q-1.5.45-3 .6h-1.6q-1.5 0-3-.6c-.6-.2-1.1-.4-1.7-.6-1.3-.5-2.6-1.3-3.9-2.1-.6-.4-1.2-.8-1.8-1.3-2.9-2.3-5.1-4.9-6.3-6.4 1.1-1.5 3.4-4 6.3-6.4.6-.5 1.2-.9 1.8-1.3q1.95-1.35 3.9-2.1c.6-.2 1.1-.4 1.7-.6q1.5-.45 3-.6zm0-2h-1c-1.2 0-2.3.3-3.4.6-.6.2-1.3.4-1.9.7-1.5.6-2.9 1.4-4.3 2.3-.7.5-1.3.9-1.9 1.4C2.9 8.7 0 13 0 13s2.9 4.3 7.5 7.9c.6.5 1.3 1 1.9 1.4 1.3.9 2.7 1.7 4.3 2.3.6.3 1.3.5 1.9.7 1.1.3 2.3.6 3.4.6h2c1.2 0 2.3-.3 3.4-.6.6-.2 1.3-.4 1.9-.7 1.5-.6 2.9-1.4 4.3-2.3.7-.5 1.3-.9 1.9-1.4C37.1 17.3 40 13 40 13s-2.9-4.3-7.5-7.9c-.6-.5-1.3-1-1.9-1.4-1.3-.9-2.8-1.7-4.3-2.3-.6-.3-1.3-.5-1.9-.7C23.3.4 22.1.1 21 .1h-1"></path><path fill="#ff4e00" d="M20 5c-4.4 0-8 3.6-8 8s3.6 8 8 8 8-3.6 8-8-3.6-8-8-8m0 11c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3"></path></g></g></svg>
      
    </header>
    <ol>
              <li>
                      <a href="https://arstechnica.com/health/2025/08/rfk-jr-defends-500m-cut-for-mrna-vaccines-with-pseudoscience-gobbledygook/">
              <img src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/GettyImages-2216099156-768x432.jpg" alt="Listing image for first story in Most Read: RFK Jr. defends $500M cut for mRNA vaccines with pseudoscience gobbledygook" decoding="async" loading="lazy">
            </a>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                    <li>
                    
        </li>
                  </ol>
</div>
  </main>





  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursed Knowledge (411 pts)]]></title>
            <link>https://immich.app/cursed-knowledge/</link>
            <guid>44831704</guid>
            <pubDate>Thu, 07 Aug 2025 23:34:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://immich.app/cursed-knowledge/">https://immich.app/cursed-knowledge/</a>, See on <a href="https://news.ycombinator.com/item?id=44831704">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="__docusaurus_skipToContent_fallback"><p>Cursed knowledge we have learned as a result of building Immich that we wish we never knew.</p><div><ul><li><p>6/4/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16,16.92C15.67,16.97 15.34,17 15,17C14.66,17 14.33,16.97 14,16.92V13.41L11.5,15.89C11,15.5 10.5,15 10.11,14.5L12.59,12H9.08C9.03,11.67 9,11.34 9,11C9,10.66 9.03,10.33 9.08,10H12.59L10.11,7.5C10.3,7.25 10.5,7 10.76,6.76V6.76C11,6.5 11.25,6.3 11.5,6.11L14,8.59V5.08C14.33,5.03 14.66,5 15,5C15.34,5 15.67,5.03 16,5.08V8.59L18.5,6.11C19,6.5 19.5,7 19.89,7.5L17.41,10H20.92C20.97,10.33 21,10.66 21,11C21,11.34 20.97,11.67 20.92,12H17.41L19.89,14.5C19.7,14.75 19.5,15 19.24,15.24V15.24C19,15.5 18.75,15.7 18.5,15.89L16,13.41V16.92H16V16.92M5,19A2,2 0 0,1 7,17A2,2 0 0,1 9,19A2,2 0 0,1 7,21A2,2 0 0,1 5,19H5Z" style="fill:purple"></path></svg><p><span>Zitadel Actions are cursed</span></p></div><p>Zitadel is cursed because its custom scripting feature is executed with a JS engine that doesn't support regex named capture groups.</p></div></li><li><p>5/30/2025</p><div><p>Microsoft Entra supports PKCE, but doesn't include it in its OpenID discovery document. This leads to clients thinking PKCE isn't available.</p></div></li><li><p>5/5/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,17V1H5V5H1V7H5V17A2,2 0 0,0 7,19H17V23H19V19H23V17M17,15H19V7C19,5.89 18.1,5 17,5H9V7H17V15Z" style="fill:tomato"></path></svg><p><span>Image dimensions in EXIF metadata are cursed</span></p></div><p>The dimensions in EXIF metadata can be different from the actual dimensions of the image, causing issues with cropping and resizing.</p></div></li><li><p>4/1/2025</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M5,3H7V5H5V10A2,2 0 0,1 3,12A2,2 0 0,1 5,14V19H7V21H5C3.93,20.73 3,20.1 3,19V15A2,2 0 0,0 1,13H0V11H1A2,2 0 0,0 3,9V5A2,2 0 0,1 5,3M19,3A2,2 0 0,1 21,5V9A2,2 0 0,0 23,11H24V13H23A2,2 0 0,0 21,15V19A2,2 0 0,1 19,21H17V19H19V14A2,2 0 0,1 21,12A2,2 0 0,1 19,10V5H17V3H19M12,15A1,1 0 0,1 13,16A1,1 0 0,1 12,17A1,1 0 0,1 11,16A1,1 0 0,1 12,15M8,15A1,1 0 0,1 9,16A1,1 0 0,1 8,17A1,1 0 0,1 7,16A1,1 0 0,1 8,15M16,15A1,1 0 0,1 17,16A1,1 0 0,1 16,17A1,1 0 0,1 15,16A1,1 0 0,1 16,15Z" style="fill:yellow"></path></svg><p><span>YAML whitespace is cursed</span></p></div><p>YAML whitespaces are often handled in unintuitive ways.</p></div></li><li><p>9/20/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M3,12V6.75L9,5.43V11.91L3,12M20,3V11.75L10,11.9V5.21L20,3M3,13L9,13.09V19.9L3,18.75V13M20,13.25V22L10,20.09V13.1L20,13.25Z" style="fill:#357EC7"></path></svg><p><span>Hidden files in Windows are cursed</span></p></div><p>Hidden files in Windows cannot be opened with the "w" flag. That, combined with SMB option "hide dot files" leads to a lot of confusion.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M21,5H3V7H21V5M3,19H10V17H3V19M3,13H18C19,13 20,13.43 20,15C20,16.57 19,17 18,17H16V15L12,18L16,21V19H18C20.95,19 22,17.73 22,15C22,12.28 21,11 18,11H3V13Z" style="fill:gray"></path></svg><p><span>Carriage returns in bash scripts are cursed</span></p></div><p>Git can be configured to automatically convert LF to CRLF on checkout and CRLF breaks bash scripts.</p></div></li><li><p>8/7/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9 5.82L7.36 4.16C8.09 2.31 9.89 1 12 1C14.76 1 17 3.24 17 6V8H18C19.11 8 20 8.9 20 10V16.8L11.2 8H15V6C15 4.34 13.66 3 12 3C10.41 3 9.11 4.25 9 5.82M22.11 21.46L20.84 22.73L19.46 21.35C19.1 21.75 18.58 22 18 22H6C4.89 22 4 21.11 4 20V10C4 8.89 4.9 8 6 8H6.11L1.11 3L2.39 1.73L22.11 21.46M13.85 15.74L11.26 13.15C10.5 13.44 10 14.16 10 15C10 16.11 10.9 17 12 17C12.84 17 13.56 16.5 13.85 15.74Z" style="fill:red"></path></svg><p><span>Fetch inside Cloudflare Workers is cursed</span></p></div><p>Fetch requests in Cloudflare Workers use http by default, even if you explicitly specify https, which can often cause redirect loops.</p></div></li><li><p>7/21/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M20.94 11C20.5 6.83 17.17 3.5 13 3.06V1H11V3.06C9.87 3.18 8.81 3.5 7.84 4.03L9.34 5.53C10.16 5.19 11.06 5 12 5C15.87 5 19 8.13 19 12C19 12.94 18.81 13.84 18.5 14.65L20 16.15C20.5 15.19 20.82 14.13 20.95 13H23V11H20.94M3 4.27L5.04 6.31C3.97 7.62 3.25 9.23 3.06 11H1V13H3.06C3.5 17.17 6.83 20.5 11 20.94V23H13V20.94C14.77 20.74 16.38 20.03 17.69 18.96L19.73 21L21 19.73L4.27 3L3 4.27M16.27 17.54C15.09 18.45 13.61 19 12 19C8.13 19 5 15.87 5 12C5 10.39 5.55 8.91 6.46 7.73L16.27 17.54Z" style="fill:gray"></path></svg><p><span>GPS sharing on mobile is cursed</span></p></div><p>Some phones will silently strip GPS data from images when apps without location permission try to access them.</p></div></li><li><p>7/3/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M16.84,2.73C16.45,2.73 16.07,2.88 15.77,3.17L13.65,5.29L18.95,10.6L21.07,8.5C21.67,7.89 21.67,6.94 21.07,6.36L17.9,3.17C17.6,2.88 17.22,2.73 16.84,2.73M12.94,6L4.84,14.11L7.4,14.39L7.58,16.68L9.86,16.85L10.15,19.41L18.25,11.3M4.25,15.04L2.5,21.73L9.2,19.94L8.96,17.78L6.65,17.61L6.47,15.29" style="fill:gold"></path></svg><p><span>PostgreSQL NOTIFY is cursed</span></p></div><p>PostgreSQL does everything in a transaction, including NOTIFY. This means using the socket.io postgres-adapter writes to WAL every 5 seconds.</p></div></li><li><p>7/3/2024</p><div><p>npm scripts make a http call to the npm registry each time they run, which means they are a terrible way to execute a health check.</p></div></li><li><p>6/28/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12 16C13.66 16 15 14.66 15 13C15 11.88 14.39 10.9 13.5 10.39L3.79 4.77L9.32 14.35C9.82 15.33 10.83 16 12 16M12 3C10.19 3 8.5 3.5 7.03 4.32L9.13 5.53C10 5.19 11 5 12 5C16.42 5 20 8.58 20 13C20 15.21 19.11 17.21 17.66 18.65H17.65C17.26 19.04 17.26 19.67 17.65 20.06C18.04 20.45 18.68 20.45 19.07 20.07C20.88 18.26 22 15.76 22 13C22 7.5 17.5 3 12 3M2 13C2 15.76 3.12 18.26 4.93 20.07C5.32 20.45 5.95 20.45 6.34 20.06C6.73 19.67 6.73 19.04 6.34 18.65C4.89 17.2 4 15.21 4 13C4 12 4.19 11 4.54 10.1L3.33 8C2.5 9.5 2 11.18 2 13Z" style="fill:brown"></path></svg><p><span>50 extra packages are cursed</span></p></div><p>There is a user in the JavaScript community who goes around adding "backwards compatibility" to projects. They do this by adding 50 extra package dependencies to your project, which are maintained by them.</p></div></li><li><p>6/25/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,17C10.89,17 10,16.1 10,15C10,13.89 10.89,13 12,13A2,2 0 0,1 14,15A2,2 0 0,1 12,17M18,20V10H6V20H18M18,8A2,2 0 0,1 20,10V20A2,2 0 0,1 18,22H6C4.89,22 4,21.1 4,20V10C4,8.89 4.89,8 6,8H7V6A5,5 0 0,1 12,1A5,5 0 0,1 17,6V8H18M12,3A3,3 0 0,0 9,6V8H15V6A3,3 0 0,0 12,3Z" style="fill:gold"></path></svg><p><span>Long passwords are cursed</span></p></div><p>The bcrypt implementation only uses the first 72 bytes of a string. Any characters after that are ignored.</p></div></li><li><p>1/31/2024</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M7,10H12V15H7M19,19H5V8H19M19,3H18V1H16V3H8V1H6V3H5C3.89,3 3,3.9 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5A2,2 0 0,0 19,3Z" style="fill:greenyellow"></path></svg><p><span>JavaScript Date objects are cursed</span></p></div><p>JavaScript date objects are 1 indexed for years and days, but 0 indexed for months.</p></div></li><li><p>1/9/2024</p><div><p>Prior to Node.js v20.8 using --experimental-vm-modules in a CommonJS project that imported an ES module that imported a CommonJS modules would create a segfault and crash Node.js</p></div></li><li><p>12/28/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,3C7.58,3 4,4.79 4,7C4,9.21 7.58,11 12,11C16.42,11 20,9.21 20,7C20,4.79 16.42,3 12,3M4,9V12C4,14.21 7.58,16 12,16C16.42,16 20,14.21 20,12V9C20,11.21 16.42,13 12,13C7.58,13 4,11.21 4,9M4,14V17C4,19.21 7.58,21 12,21C16.42,21 20,19.21 20,17V14C20,16.21 16.42,18 12,18C7.58,18 4,16.21 4,14Z" style="fill:gray"></path></svg><p><span>PostgreSQL parameters are cursed</span></p></div><p>PostgresSQL has a limit of 65,535 parameters, so bulk inserts can fail with large datasets.</p></div></li><li><p>6/26/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M12,12H19C18.47,16.11 15.72,19.78 12,20.92V12H5V6.3L12,3.19M12,1L3,5V11C3,16.55 6.84,21.73 12,23C17.16,21.73 21,16.55 21,11V5L12,1Z" style="fill:gold"></path></svg><p><span>Secure contexts are cursed</span></p></div><p>Some web features like the clipboard API only work in "secure contexts" (ie. https or localhost)</p></div></li><li><p>2/23/2023</p><div><div><svg viewBox="0 0 24 24" style="width:1.5rem;height:1.5rem" role="presentation"><path d="M9,3V4H4V6H5V19A2,2 0 0,0 7,21H17A2,2 0 0,0 19,19V6H20V4H15V3H9M9,8H11V17H9V8M13,8H15V17H13V8Z" style="fill:gray"></path></svg><p><span>TypeORM deletes are cursed</span></p></div><p>The remove implementation in TypeORM mutates the input, deleting the id property from the original object.</p></div></li></ul></div></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Vibechart (809 pts)]]></title>
            <link>https://www.vibechart.net/</link>
            <guid>44830684</guid>
            <pubDate>Thu, 07 Aug 2025 21:36:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vibechart.net/">https://www.vibechart.net/</a>, See on <a href="https://news.ycombinator.com/item?id=44830684">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>vibechart</p>
            <div><p>[From vibe + chart: cf. subjective interpretation + <a href="https://www.datavisualizationsociety.org/">data visualization.</a> See <a href="https://www.drawaurora.com/">Vibe</a>, and cf. <a href="https://chartscss.org/">Chart</a>.]</p></div>
            <p>
                To chart based on what you want to see instead of what is true, beautiful, or useful.
            </p>
            <div><p>
                See also: <a href="https://en.wikipedia.org/wiki/Lie">lies</a>, <a href="https://gizmodo.com/sam-altmans-lies-about-chatgpt-are-growing-bolder-2000614431">damned lies</a>, and <a href="https://en.wikipedia.org/wiki/Statistics">statistics</a>.
            </p></div>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Achieving 10,000x training data reduction with high-fidelity labels (136 pts)]]></title>
            <link>https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/</link>
            <guid>44830418</guid>
            <pubDate>Thu, 07 Aug 2025 21:11:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/">https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/</a>, See on <a href="https://news.ycombinator.com/item?id=44830418">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-gt-publish-date="20250807">
                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="oetc4">Classifying unsafe ad content has proven an enticing problem space for leveraging large language models (LLMs). The inherent complexity involved in identifying policy-violating content demands solutions capable of deep contextual and cultural understanding, areas of relative strength for LLMs over traditional machine learning systems. But fine-tuning LLMs for such complex tasks requires high-fidelity training data that is difficult and expensive to curate at the necessary quality and scale. Standard data-intensive approaches to training models are costly, especially given the need to handle <a href="https://en.wikipedia.org/wiki/Concept_drift" target="_blank" rel="noopener noreferrer">concept drift</a> as safety policies evolve or as new types of unsafe ad content arise. In the worst case the model must be retrained on a completely new data set. Reducing the amount of training data needed is therefore paramount.</p><p data-block-key="fnk84">With this in mind, we describe a new, scalable curation process for <a href="https://en.wikipedia.org/wiki/Active_learning_(machine_learning)" target="_blank" rel="noopener noreferrer">active learning</a> that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.</p><p data-block-key="br468">In our experiments, we were able to reduce the scale of training data needed from 100,000 to under 500 training examples, while increasing model alignment with human experts by up to 65%. Production systems using larger models have seen even greater reductions in data scale, using up to four orders of magnitude less data while maintaining or improving quality.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Curation process</h2><p data-block-key="99o9h">Our process starts with a zero- or few-shot initial model (LLM-0), which we provide with a prompt describing the content of interest, e.g., defining clickbait and asking “Is this ad clickbait?” The LLM-0 model then labels ads as <i>clickbait</i> (orange in the figure below) or <i>benign</i> (blue) and generates a large labeled data set, shown as <b>(1)</b> below. Note that this initial data set is typically highly imbalanced, since in production traffic only very few (&lt;1%) ads are actually clickbait. The LLM’s true positive rate is also low because it has not yet been fine-tuned. To find the most informative examples, we separately cluster examples labeled clickbait and examples labeled benign, which yields some overlapping clusters, thus indicating potential model confusion between clickbait and benign examples <b>(2)</b>. For each such overlapping cluster pair, we find pairs of examples lying nearest each other that have different labels <b>(3)</b> and send these to human experts for an opinion. If needed to stay within our review budget, we prioritize pairs of examples that cover a larger area of our search space <b>(4)</b>. The resulting curated set is both informative (since it contains the most confusable examples along the decision boundary) and diverse (since it draws from different regions along that decision boundary).</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="n7jxf">These expert-provided labels are split randomly into two sets. The first is used for model evaluation, based on two key alignment metrics: the internal alignment measuring how much experts agree, and the model–human alignment between the current model and human experts. The second is used to fine-tune the current models, producing the next iteration of the model. The process repeats until the model–human alignment either matches the internal alignment or plateaus and cannot be improved further.</p>

    </div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Metric</h2><p data-block-key="a9c7p">Our curation process does not assume the existence of ground truth. Many classification problems in the ads safety space, such as content moderation or fraud detection, are inherently ambiguous and require interpretation and deliberation, even among policy experts. We therefore cannot rely on standard metrics like precision and recall, which require a ground truth label. Instead we use <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" target="_blank" rel="noopener noreferrer">Cohen’s Kappa</a>, a measure of how well two independent annotators align, above what would be expected from chance agreement. In our experiments, Cohen’s Kappa is used as both a quality indicator for datasets (including model evaluation during the curation process, as noted above); and as a measure of model performance. Values closer to 1 show higher alignment, 0 indicates no alignment above chance, and negative values indicate systematic disagreement. While standards for interpreting these scores vary, Kappa values above .8 are widely considered to be exceptionally good, and values above .4 are generally considered acceptable.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Experiments</h2><p data-block-key="9ljdb">We wanted to understand which models and tasks would benefit most from our curation process. As <i>baselines</i> for our experiments, we fine-tuned two LLMs of different sizes (<a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf" target="_blank" rel="noopener noreferrer">Gemini</a> Nano-1 with 1.8B parameters and Nano-2 with 3.25B parameters) on two tasks of different complexity (lower and higher, based on expert alignment) using crowdsourced labels. Each crowdsourced data set has ~100K annotations and a strong class imbalance, with around 95% benign labels on average.</p><p data-block-key="1v6c">We compared each of these four baseline conditions against the corresponding <i>curated</i> condition in which each model (Nano-1 and Nano-2) is fine-tuned over multiple rounds using the curation process described above. At each iteration, we selected our curated set of examples and used them for model evaluation and fine-tuning, as described above. All models plateaued before reaching parity with the experts’ internal alignment, so we stopped at 6 iterations (~400 fine-tuning and ~250 evaluation samples) for the lower complexity task and 5 iterations (~250 fine-tuning and ~150 evaluation samples) for the higher complexity task. (Note that the lower complexity task had a larger variety of examples, which may account for the longer time needed to converge.) Both data sets had a final class balance of ~40% positive examples.</p><p data-block-key="fcsdi">The table below provides an overview of the scale and quality of the data used in each condition. Experts reached an average pairwise Cohen’s Kappa of .81 (on the lower complexity task) and .78 (on the higher complexity task) through the curation process. We consider these the ceiling for model performance. To assess the quality of our crowdsourced data, we calculated Kappa alignment between crowdsourced annotations and experts based on our full curated set, which was .59 (lower complexity) and .41 (higher complexity).</p>
</div>

                    
                    
    




                    
                    
    


<div>
        
  <p data-block-key="n7jxf">Below we show how models trained on these vastly different data sets performed in each of our baseline and curated conditions. The 1.8B parameter model saw comparable performance on both tasks: the baseline and curated models had .24 and .25 alignment, respectively, for the lower complexity task, and both models had .13 alignment on the higher complexity task. By contrast, the 3.25B parameter model showed significant quality improvements when trained with our curation process. Kappa scores for the baseline and curated models were .36 and .56, respectively, for the lower complexity task; and .23 and .38, respectively, for the higher complexity task — an improvement in alignment of 55-65% using three orders of magnitude less data (250 to 450 examples, compared to 100K in the baseline condition).</p>

    </div>

                    
                    
    




                    
                    
    




                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <p data-block-key="n7jxf">These results demonstrate that careful curation of LLM datasets to focus on fewer, more informative examples can yield better or equivalent classifier performance using much less data — three orders of magnitude less in the experiments reported here, and up to four orders of magnitude less for the larger models used in production. Of course, these gains require not only good curation but also very high quality data. For our use cases, we have observed that a label quality above .8 pairwise Cohen’s Kappa is needed to reliably outperform crowdsourced data. Consistently achieving this level of quality poses a separate challenge, to be discussed in a subsequent blog post.</p><p data-block-key="e0dkc">But given sufficient label quality, our curation process is able to leverage the strengths of both LLMs, which can cast a wide net over the problem space, and domain experts, who can focus more efficiently on the most challenging examples. The ability to retrain models with just a handful of examples is especially valuable for handling the rapidly changing landscapes of domains like ads safety. We believe the approach we’ve described will enable systems that can make more flexible, efficient use of high-fidelity labels to escape the data bottleneck.</p>
</div>

                    
                    
    


<div data-gt-id="rich_text" data-gt-component-name="">
    




    <h2 data-block-key="n7jxf">Acknowledgements</h2><p data-block-key="1bgbo"><i>This work would not have been possible without our outstanding team of engineers and product managers. Steve Walker is a co-founder of our project and co-creator of the curation process as well as the tech lead for the machine learning infrastructure of our project. Kelsie McElroy is the project manager and a co-founder of our project. We also want to thank the Ads Privacy and Safety leadership team for their continued support and belief in our vision.</i></p>
</div>

                    
                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Flipper Zero DarkWeb Firmware Bypasses Rolling Code Security (411 pts)]]></title>
            <link>https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/</link>
            <guid>44830408</guid>
            <pubDate>Thu, 07 Aug 2025 21:10:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/">https://www.rtl-sdr.com/flipperzero-darkweb-firmware-bypasses-rolling-code-security/</a>, See on <a href="https://news.ycombinator.com/item?id=44830408">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="site" role="main">
			
	
			
		<article id="post-64439">
			<header>
								
			  				  <time datetime="2025-08-07T02:47:03+00:00" pubdate="">August 7, 2025</time>
								
									
							</header>
		
			<div>
				<p>Over on YouTube Talking Sasquach has recently tested custom firmware for the Flipper Zero that can entirely break the rolling code security system used on most modern vehicles. Rolling code security works by using&nbsp;a synchronized algorithm between a transmitter and receiver to generate a new, unique code for each transmission, preventing replay attacks and unauthorized access.</p>
<p>In the past we've discussed an attack against rolling code security systems called <a href="https://www.rtl-sdr.com/?s=rolljam&amp;apbct__email_id__search_form_47564=47564" target="_blank" rel="noopener">RollJam</a>, which works by jamming the original keyfob signal so the vehicle cannot receive it, and at the same time recording it for later use. However, this attack is difficult to perform in reality.</p>
<p>For this new attack to work, all that is needed is a single button-press capture from the keyfob, without any jamming. Just from that single capture, it is able to emulate all the keyfob's functions, including lock, unlock, and unlock trunk. A consequence of this is that the original keyfob gets out of sync, and will no longer function.</p>
<p>According to the Talking Sasquatch, the attack works by simply reverse engineering the rolling code sequence, either through sequence leaks or prior brute forcing of the sequence from a large list of known codes. However, <a href="https://san.com/cc/millions-of-cars-at-risk-from-flipper-zero-key-fob-hack-experts-warn/" target="_blank" rel="noopener">another article</a> mentions that the firmware is based on the "<a href="https://arxiv.org/abs/2210.11923" target="_blank" rel="noopener">RollBack</a>" attack, which works by playing back captured rolling codes in a specific order to initiate a 'rollback' of the synchronization system.</p>
<p>Regardless of the method, videos demonstrating the attack show that only a single capture is needed to emulate a keyfob completely.</p>
<p>Affected vehicles include Chrysler, Dodge, Fiat, Ford, Hyundai, Jeep, Kia, Mitsubishi and Subaru. As of yet, there appears to be no easy fix for this, other than mass vehicle recalls.</p>
<div id="WYL_wk7BGMkuI8A" data-src="https://www.rtl-sdr.com/wp-content/plugins/wp-youtube-lyte/lyteCache.php?origThumbUrl=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fwk7BGMkuI8A%2Fmaxresdefault.jpg" title="Flipper Zero DarkWeb Firmware Copies My Key Fob! I&amp;#039;ll Explain How this Works!"><p>Flipper Zero DarkWeb Firmware Copies My Key Fob! I'll Explain How this Works!</p></div>
					
					
			</div>
			<!-- / .content -->
			
			
		</article>
		<!-- / #post-64439 -->

		    
    
    
    
    		
				  
			<!-- / .post-nav -->
				
		



		</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cursor CLI (342 pts)]]></title>
            <link>https://cursor.com/cli</link>
            <guid>44830221</guid>
            <pubDate>Thu, 07 Aug 2025 20:53:54 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cursor.com/cli">https://cursor.com/cli</a>, See on <a href="https://news.ycombinator.com/item?id=44830221">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main"><section aria-label="Marketing highlights"><div><p>Use it in your IDE or any terminal.</p><p>Same commands, any environment. Plug into your setup anywhere.</p></div><div><h2>Full control from your terminal.</h2><div><div><ul><li role="button" tabindex="0"><p>Review agent edits</p><p>Make code changes directly in the terminal.</p></li><li role="button" tabindex="0"><p>Steer in real-time</p><p>Guide the agent as it works.</p></li><li role="button" tabindex="0"><p>Set your own rules</p><p>Customize Cursor's work with rules, AGENTS.md, and MCP.</p></li></ul></div><div aria-live="polite"><p><span>1/2 src/components/Canvas3D.tsx</span><span><span>+8</span><span>-2</span></span></p><div aria-label="Code diff for Canvas3D.tsx"><p>1</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Suspense </p><!-- --><p>}</p><!-- --> <!-- --><p>from <span>'react'</span>;</p></div><p>2</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Canvas </p><!-- --><p>}</p><!-- --> <!-- --><p>from <span>'@react-three/fiber'</span>;</p></div><p>3</p><div><p><span>import</span> as THREE from</p><!-- --> <p><span>'three'</span>;</p></div><p>4</p><div><p><span>import</span></p><!-- --><p>{</p><!-- --><p> Leva </p><!-- --><p>}</p><!-- --><p> from</p><!-- --> <p><span>'leva'</span>;</p></div><p>5</p><p>import GameScene from '../scenes/GameScene';</p><p>18</p><p>right-click</p><p>19</p><p>gl<span>=</span>{{</p><p>20</p><p>antialias: true,</p><p>20</p><p>antialias: false,</p><p>21</p><p> alpha: false,</p><p>22</p><p> stencil: false,</p><p>23</p><p> depth: true,</p><p>24</p><div> <!-- --><p>powerPreference:</p><!-- --> <p><span>'high-performance'</span>,</p></div></div></div></div></div><section aria-label="Additional highlights"><h3>Built for your workflow</h3><div><div><p>Always use the latest model</p><p>Every cutting edge model from Anthropic, OpenAI, Gemini, and more at your fingertips.</p></div><div><p>Use in your preferred IDE</p><p>Wherever you code, Cursor CLI integrates with your existing workflow.</p></div><div><p>Write powerful scripts and automations</p><p>Automatically update docs, trigger security reviews, or build custom coding agents.</p></div></div></section></section></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Historical Tech Tree (458 pts)]]></title>
            <link>https://www.historicaltechtree.com/</link>
            <guid>44829185</guid>
            <pubDate>Thu, 07 Aug 2025 19:24:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.historicaltechtree.com/">https://www.historicaltechtree.com/</a>, See on <a href="https://news.ycombinator.com/item?id=44829185">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI's new open-source model is basically Phi-5 (357 pts)]]></title>
            <link>https://www.seangoedecke.com/gpt-oss-is-phi-5/</link>
            <guid>44828884</guid>
            <pubDate>Thu, 07 Aug 2025 18:59:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seangoedecke.com/gpt-oss-is-phi-5/">https://www.seangoedecke.com/gpt-oss-is-phi-5/</a>, See on <a href="https://news.ycombinator.com/item?id=44828884">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><header></header><section><p>OpenAI just released its first ever open-source<sup id="fnref-1"><a href="#fn-1">1</a></sup> large language models, called gpt-oss-120b and gpt-oss-20b. You can talk to them <a href="https://gpt-oss.com/">here</a>. Are they good models? Well, that depends on what you’re looking for. They’re great at <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">some benchmarks</a>, of course (OpenAI would never have released them otherwise) but weirdly bad at others, like SimpleQA.</p>
<p>Some people <a href="https://simonwillison.net/2025/Aug/5/gpt-oss/">really like them</a>. Others on Twitter <a href="https://x.com/corbtt/status/1952868822891012241">really</a> <a href="https://x.com/vikhyatk/status/1952863413845275132">don’t</a>. From what I can tell, they’re technically competent but lack a lot of out-of-domain knowledge: for instance, they have broad general knowledge about science, but don’t know much about popular culture. We’ll know in six months how useful these models are in practice, but my prediction is that these models will end up in the category of “performs much better on benchmarks than on real-world tasks”.</p>
<h3>Phi models and training on synthetic data</h3>
<p>In 2024, Sebastien Bubeck led the development of Microsoft’s open-source Phi-series of models<sup id="fnref-2"><a href="#fn-2">2</a></sup>. The big idea behind those models was to train exclusively on synthetic data: instead of text pulled from books or the internet, text generated by other language models or hand-curated textbooks. Synthetic data is less common than normal data, since instead of just downloading terabytes of it for free you have to spend money to generate each token. But the trade-off is that you have complete control over your training data. What happens when you train a model on entirely high-quality synthetic and curated data?</p>
<p>As it turns out, it does very well on model benchmarks but disappoints in practice. Searching for the reception to each Phi model shows the same pattern: very impressive <a href="https://arxiv.org/abs/2404.14219">benchmarks</a>, lots of enthusiasm, and then actual performance <a href="https://news.ycombinator.com/item?id=40128351">far weaker</a> than the benchmarks would suggest.</p>
<p>I think the impressive benchmark results come from the fact that these models are very easy to train for specific tasks, because you generate much of the training data yourself. If you’re training on synthetic data, you’d be foolish not to generate some synthetic data that matches the kind of problems people are benchmarking on. But since you’re “teaching for the test”, you should expect to do worse than other language models who are training on broad data and end up being good at the benchmarks by accident.</p>
<p>Why am I talking about Phi models? At the end of 2024, Sebastien Bubeck <a href="https://www.reuters.com/technology/microsofts-vp-genai-research-join-openai-2024-10-14/">left Microsoft</a> to join OpenAI. We don’t know who was involved in making the new OpenAI <code>gpt-oss</code> models. The <a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">model card</a> doesn’t provide much detail about the pretraining stage. However, I’d bet that Sebastien Bubeck was a part of the effort, and that these models were trained on a <em>heavily</em> filtered or synthetic dataset.</p>
<h3>Synthetic data is safer</h3>
<p>Why would OpenAI train Phi-style models, knowing that they’ll perform better on benchmarks than in real-world applications? For the same reason that Microsoft probably continued to train Phi-style models: safety. Releasing an open-source model is terrifying for a large organization. Once it’s out there, your name is associated with it forever, and thousands of researchers will be frantically trying to fine-tune it to remove the safety guardrails. </p>
<p>It’s not discussed publically very often, but the main use-case for fine-tuning small language models is for erotic role-play, and there’s a serious demand. Any small online community for people who run local models is at least 50% perverts.</p>
<p>If you release a regular closed-weights model that stays in your own infrastructure, people can’t fine-tune it. If you make a mistake, you can always update the model in-place. But open-source models are out there forever.</p>
<p>Training on synthetic data (or highly-controlled data such as textbooks) makes it much easier to produce a safe model. You can produce as much “you asked me to do X, but as a sensible language model I am declining to do so” content as you like. If there’s no subversive or nasty content in the training data, the model never learns to behave in subversive or nasty ways (at least, that’s the goal).</p>
<p>For OpenAI, it must have been very compelling to train a Phi-style model for their open-source release. They needed a model that beat the Chinese open-source models on benchmarks, while also not misbehaving in a way that caused <a href="https://www.seangoedecke.com/ai-sycophancy">yet another</a> scandal for them. Unlike Meta, they don’t need their open-source model to be <em>actually</em> good, because their main business is in their closed-source models.</p>
<p>That’s why I think OpenAI went down the synthetic data route for their new <code>gpt-oss</code> models. For good or ill, they may as well be Phi-5 and Phi-5-mini.</p>
</section><p>If you liked this post, consider <a href="https://buttondown.com/seangoedecke" target="_blank">subscribing</a> to email updates about my new posts, or <a href="https://news.ycombinator.com/submitlink?u=https://www.seangoedecke.com/gpt-oss-is-phi-5/" target="_blank">sharing it on Hacker News</a>.</p><p>August 7, 2025<!-- -->&nbsp;│ Tags: <a href="https://www.seangoedecke.com/tags/ai/">ai</a></p><hr></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Encryption made for police and military radios may be easily cracked (214 pts)]]></title>
            <link>https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/</link>
            <guid>44828504</guid>
            <pubDate>Thu, 07 Aug 2025 18:30:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/">https://www.wired.com/story/encryption-made-for-police-and-military-radios-may-be-easily-cracked-researchers-find/</a>, See on <a href="https://news.ycombinator.com/item?id=44828504">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="ArticlePageChunks"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p><span>Two years ago,</span> researchers in the Netherlands <a href="https://www.wired.com/story/tetra-radio-encryption-backdoor/">discovered an intentional backdoor</a> in an encryption algorithm baked into radios used by critical infrastructure–as well as police, intelligence agencies, and military forces around the world–that made any communication secured with the algorithm vulnerable to eavesdropping.</p><p>When the researchers publicly disclosed the issue in 2023, the European Telecommunications Standards Institute (ETSI), which developed the algorithm, advised anyone using it for sensitive communication to deploy an end-to-end encryption solution on top of the flawed algorithm to bolster the security of their communications.</p><p>But now the same researchers have found that at least one implementation of the end-to-end encryption solution endorsed by ETSI has a similar issue that makes it equally vulnerable to eavesdropping. The encryption algorithm used for the device they examined starts with a 128-bit key, but this gets compressed to 56 bits before it encrypts traffic, making it easier to crack. It’s not clear who is using this implementation of the end-to-end encryption algorithm, nor if anyone using devices with the end-to-end encryption is aware of the security vulnerability in them.</p><p>The end-to-end encryption the researchers examined, which is expensive to deploy, is most commonly used in radios for law enforcement agencies, special forces, and covert military and intelligence teams that are involved in national security work and therefore need an extra layer of security. But ETSI’s endorsement of the algorithm two years ago to mitigate flaws found in its lower-level encryption algorithm suggests it may be used more widely now than at the time.</p><p>In 2023, Carlo Meijer, Wouter Bokslag, and Jos Wetzels of security firm <a data-offer-url="https://www.midnightblue.nl/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.midnightblue.nl/&quot;}" href="https://www.midnightblue.nl/" rel="nofollow noopener" target="_blank">Midnight Blue</a>, based in the Netherlands, discovered vulnerabilities in encryption algorithms that are part of a European radio standard created by ETSI called TETRA (Terrestrial Trunked Radio), which has been baked into radio systems made by Motorola, Damm, Sepura, and others since the ’90s. The flaws remained unknown publicly until their disclosure, because ETSI refused for decades to let anyone examine the proprietary algorithms. The end-to-end encryption the researchers examined recently is designed to run on top of TETRA encryption algorithms.</p><p>The researchers found the issue with the end-to-end encryption (E2EE) only after extracting and reverse-engineering the E2EE algorithm used in a radio made by Sepura. The researchers plan to present their findings today at the BlackHat security conference in Las Vegas.</p><p>ETSI, when contacted about the issue, noted that the end-to-end encryption used with TETRA-based radios is not part of the ETSI standard, nor was it created by the organization. Instead it was produced by The Critical Communications Association’s (TCCA) security and fraud prevention group (SFPG). But ETSI and TCCA work closely with one another, and the two organizations include many of the same people. Brian Murgatroyd, former chair of the technical body at ETSI responsible for the TETRA standard as well as the TCCA group that developed the E2EE solution, wrote in an email on behalf of ETSI and the TCCA that end-to-end encryption was not included in the ETSI standard “because at the time it was considered that E2EE would only be used by government groups where national security concerns were involved, and these groups often have special security needs.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>For this reason, Murgatroyd noted that purchasers of TETRA-based radios are free to deploy other solutions for end-to-end encryption on their radios, but he acknowledges that the one produced by the TCCA and endorsed by ETSI “is widely used as far as we can tell.”</p><p>Although TETRA-based radio devices are not used by police and military in the US, the majority of police forces around the world do use them. These include police forces in Belgium and Scandinavian countries, as well as East European countries like Serbia, Moldova, Bulgaria, and Macedonia, and in the Middle East in Iran, Iraq, Lebanon, and Syria. The Ministries of Defense in Bulgaria, Kazakhstan, and Syria also use them, as do the Polish military counterintelligence agency, the Finnish defense forces, and Lebanon and Saudi Arabia’s intelligence services. It’s not clear, however, how many of these also deploy end-to-end decryption with their radios.</p><p>The TETRA standard includes four encryption algorithms—TEA1, TEA2, TEA3 and TEA4—that can be used by radio manufacturers in different products, depending on the intended customer and usage. The algorithms have different levels of security based on whether the radios will be sold in or outside Europe. TEA2, for example, is restricted for use in radios used by police, emergency services, military, and intelligence agencies in Europe. TEA3 is available for police and emergency services radios used outside Europe but only in countries deemed “friendly” to the EU. Only TEA1 is available for radios used by public safety agencies, police agencies, and militaries in countries deemed not friendly to Europe, such as Iran. But it’s also used in critical infrastructure in the US and other countries for machine-to-machine communication in industrial control settings such as pipelines, railways, and electric grids.</p><p>All four TETRA encryption algorithms use 80-bit keys to secure communication. But the Dutch researchers revealed in 2023 that TEA1 has a feature that causes its key to get reduced to just 32 bits, which allowed the researchers to crack it in less than a minute.</p><p>In the case of the E2EE, the researchers found that the implementation they examined starts with a key that is more secure than ones used in the TETRA algorithms, but it gets reduced to 56 bits, which would potentially let someone decrypt voice and data communications. They also found a second vulnerability that would let someone send fraudulent messages or replay legitimate ones to spread misinformation or confusion to personnel using the radios.</p><p>The ability to inject voice traffic and replay messages affects all users of the TCCA end-to-end encryption scheme, according to the researchers. They say this is the result of flaws in the TCCA E2EE protocol design rather than a particular implementation. They also say that “law enforcement end users” have confirmed to them that this flaw is in radios produced by vendors other than Sepura.</p><p>But the researchers say only a subset of end-to-end encryption users are likely affected by the reduced-key vulnerability because it depends how the encryption was implemented in radios sold to various countries.</p></div><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>ETSI’s Murgatroyd <a data-offer-url="https://www.zetter-zeroday.com/interview-with-the-etsi-standards/" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.zetter-zeroday.com/interview-with-the-etsi-standards/&quot;}" href="https://www.zetter-zeroday.com/interview-with-the-etsi-standards/" rel="nofollow noopener" target="_blank">said in 2023</a> that the TEA1 key was reduced to meet export controls for encryption sold to customers outside Europe. He said when the algorithm was created, a key with 32 bits of entropy was considered secure for most uses. Advances in computing power make it less secure now, so when the Dutch researchers exposed the reduced key two years ago, ETSI recommended that customers using TEA1 deploy TCCA's end-to-end encryption solution on top of it.</p><p>But Murgatroyd said the end-to-end encryption algorithm designed by TCCA is different. It doesn’t specify the key length the radios should use because governments using the end-to-end encryption have their own “specific and often proprietary security rules” for the devices they use. Therefore they are able to customize the TCCA encryption algorithm in their devices by working with their radio supplier to select the “encryption algorithm, key management and so on” that is right for them—but only to a degree.</p><p>“The choice of encryption algorithm and key is made between supplier and customer organisation, and ETSI has no input to this selection—nor knowledge of which algorithms and key lengths are in use in any system,” he said. But he added that radio manufacturers and customers “will always have to abide by export control regulations.”</p><p>The researchers say they cannot verify that the TCCA E2EE doesn’t specify a key length because the TCCA documentation describing the solution is protected by nondisclosure agreement and provided only to radio vendors. But they note that the E2EE system calls out an “algorithm identifier" number, which means it calls out the specific algorithm it’s using for the end-to-end encryption. These identifiers are not vendor specific, the researchers say, which suggests the identifiers refer to different key variants produced by TCCA—meaning TCCA provides specifications for algorithms that use a 126 bit key or 56 bit key, and radio vendors can configure their devices to use either of these variants, depending on the export controls in place for the purchasing country.</p><p>Whether users know their radios could have this vulnerability is unclear. The researchers found a confidential 2006 Sepura product bulletin that <a href="https://www.scribd.com/document/237610110/Issue2-MOD-05-166-Crypto-Management-Tools">someone leaked online</a>, which mentions that “the length of the traffic key … is subject to export control regulations and hence the [encryption system in the device] will be factory configured to support 128, 64, or 56 bit key lengths.” But it’s not clear what Sepura customers receive or if other manufacturers whose radios use a reduced key disclose to customers if their radios use a reduced-key algorithm.</p><p>“Some manufacturers have this in brochures; others only mention this in internal communications, and others don’t mention it at all,” says Wetzels. He says they did extensive open-source research to examine vendor documentation and “ found no clear sign of weakening being communicated to end users. So while … there are ‘some’ mentions of the algorithm being weakened, it is not fully transparent at all.”</p><p>Sepura did not respond to an inquiry from WIRED.</p><p>But Murgatroyd says that because government customers who have opted to use TCCA’s E2EE solution need to know the security of their devices, they are likely to be aware if their systems are using a reduced key.</p><p>“As end-to-end encryption is primarily used for government communications, we would expect that the relevant government National Security agencies are fully aware of the capabilities of their end-to-end encryption systems and can advise their users appropriately,” Murgatroyd wrote in his email.</p><p>Wetzels is skeptical of this, however. “We consider it highly unlikely non-Western governments are willing to spend literally millions of dollars if they know they're only getting 56 bits of security,” he says.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Exit Tax: Leave Germany before your business gets big (325 pts)]]></title>
            <link>https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/</link>
            <guid>44828158</guid>
            <pubDate>Thu, 07 Aug 2025 18:06:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/">https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/</a>, See on <a href="https://news.ycombinator.com/item?id=44828158">Hacker News</a></p>
Couldn't get https://eidel.io/exit-tax-leave-germany-before-your-business-gets-big/: AggregateError]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmark Framework Desktop Mainboard and 4-node cluster (186 pts)]]></title>
            <link>https://github.com/geerlingguy/ollama-benchmark/issues/21</link>
            <guid>44827862</guid>
            <pubDate>Thu, 07 Aug 2025 17:49:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/geerlingguy/ollama-benchmark/issues/21">https://github.com/geerlingguy/ollama-benchmark/issues/21</a>, See on <a href="https://news.ycombinator.com/item?id=44827862">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><p dir="auto">Testing the <a href="https://frame.work/desktop" rel="nofollow">Framework Desktop</a> - AMD Ryzen AI Max+ 395 with Radeon 8090S. (Four pre-production units were shipped to me for local cluster testing).</p>
<h2 dir="auto">Single Node configuration (128 GB RAM):</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475718422-78791530-a491-463b-8c8e-6b94883b2b19.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE4NDIyLTc4NzkxNTMwLWE0OTEtNDYzYi04YzhlLTZiOTQ4ODNiMmIxOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTdmNzNlMGRlN2M5MmYzZGEzNzZhOWI1ZmY5YjdkNmVkMWY4YWYyOTJjY2M5ZTAyYzA3MTYzN2FiNTQ3ZjU2ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NrogbtFc_AN6neMIMlFkEZxdrq03GhOdZvW2ofvWeXg"><img src="https://private-user-images.githubusercontent.com/481677/475718422-78791530-a491-463b-8c8e-6b94883b2b19.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE4NDIyLTc4NzkxNTMwLWE0OTEtNDYzYi04YzhlLTZiOTQ4ODNiMmIxOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YTdmNzNlMGRlN2M5MmYzZGEzNzZhOWI1ZmY5YjdkNmVkMWY4YWYyOTJjY2M5ZTAyYzA3MTYzN2FiNTQ3ZjU2ZiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NrogbtFc_AN6neMIMlFkEZxdrq03GhOdZvW2ofvWeXg" alt="Image"></a></p>
<h2 dir="auto">Cluster configuration (4x Mainboard - 512 GB RAM)</h2>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475719431-140425ff-2110-4301-9e92-dc2807b3c277.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NDMxLTE0MDQyNWZmLTIxMTAtNDMwMS05ZTkyLWRjMjgwN2IzYzI3Ny5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgwN1QyMTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yY2ZkMzcyNDk0NjgwYTYxYTNlNmM1ZmIwYzZmNzZjN2IwMjg3NmI4ZDRmYzA2MDIzZDZmNzNjZWM2YWRlMjgxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bvcCvKOWXhYfWrtuFsAn0asOUaw_tnxHHDZpx5F9pRQ"><img src="https://private-user-images.githubusercontent.com/481677/475719431-140425ff-2110-4301-9e92-dc2807b3c277.jpg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NDMxLTE0MDQyNWZmLTIxMTAtNDMwMS05ZTkyLWRjMjgwN2IzYzI3Ny5qcGc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUwODA3JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MDgwN1QyMTMwMDJaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0yY2ZkMzcyNDk0NjgwYTYxYTNlNmM1ZmIwYzZmNzZjN2IwMjg3NmI4ZDRmYzA2MDIzZDZmNzNjZWM2YWRlMjgxJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.bvcCvKOWXhYfWrtuFsAn0asOUaw_tnxHHDZpx5F9pRQ" alt="Image"></a></p>
<p dir="auto">Initial tests (above) were run using 2.5 Gbps Ethernet interconnect.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://private-user-images.githubusercontent.com/481677/475719725-da027dbf-864c-47d2-ad87-2917285de6c9.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NzI1LWRhMDI3ZGJmLTg2NGMtNDdkMi1hZDg3LTI5MTcyODVkZTZjOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWU5ZjUzNGJmMGZlMzNjZmRmZmNmYzdlOGVhYTc1MGZhNzVjNDlmNjgwYTJiYWNhMTBlNTc2YmJhZTNjZWJlOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1BO9Fncu1POYdXUNIT2rRTRJimpRGMeumaevUIA-8NU"><img src="https://private-user-images.githubusercontent.com/481677/475719725-da027dbf-864c-47d2-ad87-2917285de6c9.jpeg?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NTQ2MDI1MDIsIm5iZiI6MTc1NDYwMjIwMiwicGF0aCI6Ii80ODE2NzcvNDc1NzE5NzI1LWRhMDI3ZGJmLTg2NGMtNDdkMi1hZDg3LTI5MTcyODVkZTZjOS5qcGVnP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI1MDgwNyUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNTA4MDdUMjEzMDAyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MWU5ZjUzNGJmMGZlMzNjZmRmZmNmYzdlOGVhYTc1MGZhNzVjNDlmNjgwYTJiYWNhMTBlNTc2YmJhZTNjZWJlOSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.1BO9Fncu1POYdXUNIT2rRTRJimpRGMeumaevUIA-8NU" alt="Image"></a></p>
<p dir="auto">I later changed to 5 Gbps using a NICGIGA switch, and racked it in a black T1 mini rack shipped by DeskPi, along with four of their currently-in-prototype Framework Desktop mini rack trays. <a href="https://github.com/geerlingguy/mini-rack/issues/234" data-hovercard-type="issue" data-hovercard-url="/geerlingguy/mini-rack/issues/234/hovercard">Mini rack build showcase here</a>.</p>
<p dir="auto">I also tested Thunderbolt node-to-node interconnects, but could only get 10 Gbps over TB4 using <code>thunderbolt0</code>/<code>thunderbolt1</code> interfaces).</p>
<p dir="auto">For more benchmarks (focusing on CPU, GPU, disk, net, etc.), see:</p>
<ul dir="auto">
<li><a href="https://www.jeffgeerling.com/blog/2025/i-clustered-four-framework-mainboards-test-huge-llms" rel="nofollow">I clustered four Framework Mainboards to test huge LLMs</a></li>
<li><a href="https://github.com/geerlingguy/sbc-reviews/issues/80" data-hovercard-type="issue" data-hovercard-url="/geerlingguy/sbc-reviews/issues/80/hovercard">sbc-reviews: Framework Desktop</a>.</li>
</ul>
<p dir="auto">All my automation for testing is in the <a href="https://github.com/geerlingguy/beowulf-ai-cluster">Beowulf AI Cluster</a> repo.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5: Key characteristics, pricing and system card (588 pts)]]></title>
            <link>https://simonwillison.net/2025/Aug/7/gpt-5/</link>
            <guid>44827794</guid>
            <pubDate>Thu, 07 Aug 2025 17:46:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2025/Aug/7/gpt-5/">https://simonwillison.net/2025/Aug/7/gpt-5/</a>, See on <a href="https://news.ycombinator.com/item?id=44827794">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-permalink-context="/2025/Aug/7/gpt-5/">

<p>7th August 2025</p>



<p>I’ve had preview access to the new GPT-5 model family for the past two weeks (see <a href="https://simonwillison.net/2025/Aug/7/previewing-gpt-5/">related video</a>) and have been using GPT-5 as my daily-driver. It’s my new favorite model. It’s still an LLM—it’s not a dramatic departure from what we’ve had before—but it rarely screws up and generally feels competent or occasionally impressive at the kinds of things I like to use models for.</p>
<p>I’ve collected a lot of notes over the past two weeks, so I’ve decided to break them up into <a href="https://simonwillison.net/series/gpt-5/">a series of posts</a>. This first one will cover key characteristics of the models, how they are priced and what we can learn from the <a href="https://openai.com/index/gpt-5-system-card/">GPT-5 system card</a>.</p>
<ul>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#key-model-characteristics">Key model characteristics</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#position-in-the-openai-model-family">Position in the OpenAI model family</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#pricing-is-aggressively-competitive">Pricing is aggressively competitive</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#more-notes-from-the-system-card">More notes from the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#prompt-injection-in-the-system-card">Prompt injection in the system card</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#thinking-traces-in-the-api">Thinking traces in the API</a></li>
<li><a href="https://simonwillison.net/2025/Aug/7/gpt-5/#and-some-svgs-of-pelicans">And some SVGs of pelicans</a></li>
</ul>

<h4 id="key-model-characteristics">Key model characteristics</h4>
<p>Let’s start with the fundamentals. GPT-5 in ChatGPT is a weird hybrid that switches between different models. Here’s what the system card says about that (my highlights in bold):</p>
<blockquote>
<p>GPT-5 is a unified system with a smart and fast model that answers most questions, a deeper reasoning model for harder problems, and <strong>a real-time router that quickly decides which model to use based on conversation type, complexity, tool needs, and explicit intent</strong> (for example, if you say “think hard about this” in the prompt). [...] Once usage limits are reached, a mini version of each model handles remaining queries. In the near future, we plan to integrate these capabilities into a single model.</p>
</blockquote>
<p>GPT-5 in the API is simpler: it’s available as three models—<strong>regular</strong>, <strong>mini</strong> and <strong>nano</strong>—which can each be run at one of four reasoning levels: minimal (a new level not previously available for other OpenAI reasoning models), low, medium or high.</p>
<p>The models have an input limit of 272,000 tokens and an output limit (which includes invisible reasoning tokens) of 128,000 tokens. They support text and image for input, text only for output.</p>
<p>I’ve mainly explored full GPT-5. My verdict: it’s just <strong>good at stuff</strong>. It doesn’t feel like a dramatic leap ahead from other LLMs but it exudes competence—it rarely messes up, and frequently impresses me. I’ve found it to be a very sensible default for everything that I want to do. At no point have I found myself wanting to re-run a prompt against a different model to try and get a better result.</p>

<p>Here are the OpenAI model pages for <a href="https://platform.openai.com/docs/models/gpt-5">GPT-5</a>, <a href="https://platform.openai.com/docs/models/gpt-5-mini">GPT-5 mini</a> and <a href="https://platform.openai.com/docs/models/gpt-5-nano">GPT-5 nano</a>. Knowledge cut-off is September 30th 2024 for GPT-5 and May 30th 2024 for GPT-5 mini and nano.</p>

<h4 id="position-in-the-openai-model-family">Position in the OpenAI model family</h4>
<p>The three new GPT-5 models are clearly intended as a replacement for most of the rest of the OpenAI line-up. This table from the system card is useful, as it shows how they see the new models fitting in:</p>
<table>
<thead>
<tr>
<th>Previous model</th>
<th>GPT-5 model</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o</td>
<td>gpt-5-main</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>gpt-5-main-mini</td>
</tr>
<tr>
<td>OpenAI o3</td>
<td>gpt-5-thinking</td>
</tr>
<tr>
<td>OpenAI o4-mini</td>
<td>gpt-5-thinking-mini</td>
</tr>
<tr>
<td>GPT-4.1-nano</td>
<td>gpt-5-thinking-nano</td>
</tr>
<tr>
<td>OpenAI o3 Pro</td>
<td>gpt-5-thinking-pro</td>
</tr>
</tbody>
</table>
<p>That “thinking-pro” model is currently only available via ChatGPT where it is labelled as “GPT-5 Pro” and limited to the $200/month tier. It uses “parallel test time compute”.</p>
<p>The only capabilities not covered by GPT-5 are audio input/output and image generation. Those remain covered by models like <a href="https://platform.openai.com/docs/models/gpt-4o-audio-preview">GPT-4o Audio</a> and <a href="https://platform.openai.com/docs/models/gpt-4o-realtime-preview">GPT-4o Realtime</a> and their mini variants and the <a href="https://platform.openai.com/docs/models/gpt-image-1">GPT Image 1</a> and DALL-E image generation models.</p>
<h4 id="pricing-is-aggressively-competitive">Pricing is aggressively competitive</h4>
<p>The pricing is <em>aggressively competitive</em> with other providers.</p>
<ul>
<li>GPT-5: $1.25/million for input, $10/million for output</li>
<li>GPT-5 Mini: $0.25/m input, $2.00/m output</li>
<li>GPT-5 Nano: $0.05/m input, $0.40/m output</li>
</ul>
<p>GPT-5 is priced at half the input cost of GPT-4o, and maintains the same price for output. Those invisible reasoning tokens count as output tokens so you can expect most prompts to use more output tokens than their GPT-4o equivalent (unless you set reasoning effort to “minimal”).</p>
<p>The discount for token caching is significant too: 90% off on input tokens that have been used within the previous few minutes. This is particularly material if you are implementing a chat UI where the same conversation gets replayed every time the user adds another prompt to the sequence.</p>
<p>Here’s a comparison table I put together showing the new models alongside the most comparable models from OpenAI’s competition:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Input $/m</th>
<th>Output $/m</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude Opus 4.1</td>
<td>15.00</td>
<td>75.00</td>
</tr>
<tr>
<td>Claude Sonnet 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Grok 4</td>
<td>3.00</td>
<td>15.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&gt;200,000)</td>
<td>2.50</td>
<td>15.00</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>2.50</td>
<td>10.00</td>
</tr>
<tr>
<td>GPT-4.1</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>o3</td>
<td>2.00</td>
<td>8.00</td>
</tr>
<tr>
<td>Gemini 2.5 Pro (&lt;200,000)</td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td><strong>GPT-5</strong></td>
<td>1.25</td>
<td>10.00</td>
</tr>
<tr>
<td>o4-mini</td>
<td>1.10</td>
<td>4.40</td>
</tr>
<tr>
<td>Claude 3.5 Haiku</td>
<td>0.80</td>
<td>4.00</td>
</tr>
<tr>
<td>GPT-4.1 mini</td>
<td>0.40</td>
<td>1.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash</td>
<td>0.30</td>
<td>2.50</td>
</tr>
<tr>
<td>Grok 3 Mini</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr>
<td><strong>GPT-5 Mini</strong></td>
<td>0.25</td>
<td>2.00</td>
</tr>
<tr>
<td>GPT-4o mini</td>
<td>0.15</td>
<td>0.60</td>
</tr>
<tr>
<td>Gemini 2.5 Flash-Lite</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>GPT-4.1 Nano</td>
<td>0.10</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Lite</td>
<td>0.06</td>
<td>0.24</td>
</tr>
<tr>
<td><strong>GPT-5 Nano</strong></td>
<td>0.05</td>
<td>0.40</td>
</tr>
<tr>
<td>Amazon Nova Micro</td>
<td>0.035</td>
<td>0.14</td>
</tr>
</tbody>
</table>
<p>(Here’s a good example of a GPT-5 failure: I tried to get it to <a href="https://chatgpt.com/share/6894d804-bca4-8006-ac46-580bf4a9bf5f">output that table sorted itself</a> but it put Nova Micro as more expensive than GPT-5 Nano, so I prompted it to “construct the table in Python and sort it there” and that fixed the issue.)</p>
<h4 id="more-notes-from-the-system-card">More notes from the system card</h4>
<p>As usual, <a href="">the system card</a> is vague on what went into the training data. Here’s what it says:</p>
<blockquote>
<p>Like OpenAI’s other models, the GPT-5 models were trained on diverse datasets, including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. [...] We use advanced data filtering processes to reduce personal information from training data.</p>
</blockquote>
<p>I found this section interesting, as it reveals that writing, code and health are three of the most common use-cases for ChatGPT. This explains why so much effort went into health-related questions,  for both GPT-5 and the recently released OpenAI open weight models.</p>
<blockquote>
<p>We’ve made significant advances in <strong>reducing hallucinations, improving instruction following, and minimizing sycophancy</strong>, and have leveled up GPT-5’s performance in <strong>three of ChatGPT’s most common uses: writing, coding, and health</strong>. All of the GPT-5 models additionally feature <strong>safe-completions, our latest approach to safety training</strong> to prevent disallowed content.</p>
</blockquote>
<p>Safe-completions is later described like this:</p>
<blockquote>
<p>Large language models such as those powering ChatGPT have <strong>traditionally been trained to
either be as helpful as possible or outright refuse a user request</strong>, depending on whether the
prompt is allowed by safety policy. [...] Binary refusal boundaries are especially ill-suited for dual-use cases (such as biology
or cybersecurity), where a user request can be completed safely at a high level, but may lead
to malicious uplift if sufficiently detailed or actionable. <strong>As an alternative, we introduced safe-
completions: a safety-training approach that centers on the safety of the assistant’s output rather
than a binary classification of the user’s intent</strong>. Safe-completions seek to maximize helpfulness
subject to the safety policy’s constraints.</p>
</blockquote>
<p>So instead of straight up refusals, we should expect GPT-5 to still provide an answer but moderate that answer to avoid it including “harmful” content.</p>
<p>OpenAI have a paper about this which I haven’t read yet (I didn’t get early access): <a href="https://openai.com/index/gpt-5-safe-completions/">From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training</a>.</p>
<p>Sycophancy gets a mention, unsurprising given <a href="https://simonwillison.net/2025/May/2/what-we-missed-with-sycophancy/">their high profile disaster in April</a>. They’ve worked on this in the core model:</p>
<blockquote>
<p>System
prompts, while easy to modify, have a more limited impact on model outputs relative to changes in
post-training. For GPT-5, we post-trained our models to reduce sycophancy. Using conversations
representative of production data, we evaluated model responses, then assigned a score reflecting
the level of sycophancy, which was used as a reward signal in training.</p>
</blockquote>
<p>They claim impressive reductions in hallucinations. In my own usage I’ve not spotted a single hallucination yet, but that’s been true for me for Claude 4 and o3 recently as well—hallucination is so much less of a problem with this year’s models.</p>
<blockquote>
<p>One of our focuses when training the GPT-5 models was to reduce the frequency of factual
hallucinations. While ChatGPT has browsing enabled by default, many API queries do not use
browsing tools. Thus, we focused both on training our models to browse effectively for up-to-date
information, and on reducing hallucinations when the models are relying on their own internal
knowledge.</p>
</blockquote>
<p>The section about deception also incorporates the thing where models sometimes pretend they’ve completed a task that defeated them:</p>
<blockquote>
<p>We placed gpt-5-thinking in a variety of tasks that were partly or entirely infeasible to accomplish,
and <strong>rewarded the model for honestly admitting it can not complete the task</strong>. [...]</p>
<p>In tasks where the agent is required to use tools, such as a web browsing
tool, in order to answer a user’s query, previous models would hallucinate information when
the tool was unreliable. We simulate this scenario by purposefully disabling the tools or by
making them return error codes.</p>
</blockquote>
<h4 id="prompt-injection-in-the-system-card">Prompt injection in the system card</h4>
<p>There’s a section about prompt injection, but it’s pretty weak sauce in my opinion.</p>
<blockquote>
<p>Two external red-teaming groups conducted a two-week prompt-injection assessment targeting
system-level vulnerabilities across ChatGPT’s connectors and mitigations, rather than model-only
behavior.</p>
</blockquote>
<p>Here’s their chart showing how well the model scores against the rest of the field. It’s an impressive result in comparison—56.8 attack success rate for gpt-5-thinking, where Claude 3.7 scores in the 60s (no Claude 4 results included here) and everything else is 70% plus:</p>
<p><img src="https://static.simonwillison.net/static/2025/prompt-injection-chart.jpg" alt="A bar chart titled &quot;Behavior Attack Success Rate at k Queries&quot; shows attack success rates (in %) for various AI models at k=1 (dark red) and k=10 (light red). For each model, the total height of the stacked bar represents the k=10 success rate (labeled above each bar), while the lower dark red section represents the k=1 success rate (estimated). From left to right: Llama 3.3 70B – k=10: 92.2%, k=1: ~47%; Llama 3.1 405B – k=10: 90.9%, k=1: ~38%; Gemini Flash 1.5 – k=10: 87.7%, k=1: ~34%; GPT-4o – k=10: 86.4%, k=1: ~28%; OpenAI o3-mini-high – k=10: 86.4%, k=1: ~41%; Gemini Pro 1.5 – k=10: 85.5%, k=1: ~34%; Gemini 2.5 Pro Preview – k=10: 85.0%, k=1: ~28%; Gemini 2.0 Flash – k=10: 85.0%, k=1: ~33%; OpenAI o3-mini – k=10: 84.5%, k=1: ~40%; Grok 2 – k=10: 82.7%, k=1: ~34%; GPT-4.5 – k=10: 80.5%, k=1: ~28%; 3.5 Haiku – k=10: 76.4%, k=1: ~17%; Command-R – k=10: 76.4%, k=1: ~28%; OpenAI o4-mini – k=10: 75.5%, k=1: ~17%; 3.5 Sonnet – k=10: 75.0%, k=1: ~13%; OpenAI o1 – k=10: 71.8%, k=1: ~18%; 3.7 Sonnet – k=10: 64.5%, k=1: ~17%; 3.7 Sonnet: Thinking – k=10: 63.6%, k=1: ~17%; OpenAI o3 – k=10: 62.7%, k=1: ~13%; gpt-5-thinking – k=10: 56.8%, k=1: ~6%. Legend shows dark red = k=1 and light red = k=10."></p>
<p>On the one hand, a 56.8% attack rate is cleanly a big improvement against all of those other models.</p>
<p>But it’s also a strong signal that prompt injection continues to be an unsolved problem! That means that more than half of those k=10 attacks (where the attacker was able to try up to ten times) got through.</p>
<p>Don’t assume prompt injection isn’t going to be a problem for your application just because the models got better.</p>
<h4 id="thinking-traces-in-the-api">Thinking traces in the API</h4>
<p>I had initially thought that my biggest disappointment with GPT-5 was that there’s no way to get at those thinking traces via the API... but that turned out <a href="https://bsky.app/profile/sophiebits.com/post/3lvtceih7222r">not to be true</a>. The following <code>curl</code> command demonstrates that the responses API <code>"reasoning": {"summary": "auto"}</code> is available for the new GPT-5 models:</p>

<pre><code>curl https://api.openai.com/v1/responses \
  -H "Authorization: Bearer $(llm keys get openai)" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-5",
    "input": "Give me a one-sentence fun fact about octopuses.",
    "reasoning": {"summary": "auto"}
  }'</code></pre>

<p>Here’s <a href="https://gist.github.com/simonw/1d1013ba059af76461153722005a039d">the response</a> from that API call.</p>

<p>Without that option the API will often provide a lengthy delay while the model burns through thinking tokens until you start getting back visible tokens for the final response.</p>
<p>OpenAI offer a new <code>reasoning_effort=minimal</code> option which turns off most reasoning so that tokens start to stream back to you as quickly as possible.</p>
<h4 id="and-some-svgs-of-pelicans">And some SVGs of pelicans</h4>
<p>Naturally I’ve been running <a href="https://simonwillison.net/tags/pelican-riding-a-bicycle/">my “Generate an SVG of a pelican riding a bicycle” benchmark</a>. I’ll actually spend more time on this in a future post—I have some fun variants I’ve been exploring—but for the moment here’s <a href="https://gist.github.com/simonw/c98873ef29e621c0fe2e0d4023534406">the pelican</a> I got from GPT-5 running at its default “medium” reasoning effort:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-pelican.png" alt="The bicycle is really good, spokes on wheels, correct shape frame, nice pedals. The pelican has a pelican beak and long legs stretching to the pedals."></p>
<p>It’s pretty great! Definitely recognizable as a pelican, and one of the best bicycles I’ve seen yet.</p>
<p>Here’s <a href="https://gist.github.com/simonw/9b5ecf61a5fb0794729aa0023aaa504d">GPT-5 mini</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-mini-pelican.png" alt="Blue background with clouds. Pelican has two necks for some reason. Has a good beak though. More gradents and shadows than the GPT-5 one."></p>
<p>And <a href="https://gist.github.com/simonw/3884dc8b186b630956a1fb0179e191bc">GPT-5 nano</a>:</p>
<p><img src="https://static.simonwillison.net/static/2025/gpt-5-nano-pelican.png" alt="Bicycle is two circles and some randomish black lines. Pelican still has an OK beak but is otherwise very simple."></p>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[DNA tests are uncovering the true prevalence of incest (2024) (127 pts)]]></title>
            <link>https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/</link>
            <guid>44827692</guid>
            <pubDate>Thu, 07 Aug 2025 17:40:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/">https://www.theatlantic.com/health/archive/2024/03/dna-tests-incest/677791/</a>, See on <a href="https://news.ycombinator.com/item?id=44827692">Hacker News</a></p>
<div id="readability-page-1" class="page"><section data-event-module="article body" data-flatplan-body="true"><p data-flatplan-paragraph="true">When Steve Edsel was a boy, his adoptive parents kept a scrapbook of newspaper clippings in their bedroom closet. He would ask for it sometimes, poring over the headlines about his birth. Headlines like this: “Mother Deserts Son, Flees From Hospital,” <em>Winston-Salem Journal,</em> December 30, 1973.</p><p data-flatplan-paragraph="true">The mother in question was 14 years old, “5 feet 6 with reddish brown hair,” and she had come to the hospital early one morning with her own parents. They gave names that all turned out to be fake. And by 8 o’clock that evening, just hours after she gave birth, they were gone. In a black-and-white drawing of the mother, based on nurses’ recollections, she has round glasses and sideswept bangs. Her mouth is grimly set.</p><p data-flatplan-paragraph="true">The abandoned boy was placed in foster care with a local couple, the Edsels, who later adopted him. Steve knew all of this growing up. His parents never tried to hide his origins, and they always gave him the scrapbook when he asked. It wasn’t until he turned 14, though, that he really began to wonder about his birth mom. “I’m 14,” he thought at the time. “This is how old she was when she had me.”</p><p data-flatplan-paragraph="true">Steve began looking for her in earnest in his 20s, but the paper trail quickly ran cold. When he turned 40, he told his wife, Michelle, that he wanted to give the search one last go. This was in 2013. AncestryDNA had started selling mail-in test kits the previous year, so he bought one. His matches at first seemed unpromising—some distant relatives—but when he began posting in a Facebook group for people seeking out biological family, he got connected to a genetic genealogist named CeCe Moore. Moore specializes in finding people via distant DNA matches, a technique made famous in 2018 when it led to the <a data-event-element="inline link" href="https://www.theatlantic.com/science/archive/2018/04/golden-state-killer-east-area-rapist-dna-genealogy/559070/">capture of the Golden State Killer</a>. But back then, genetic genealogy was still new, and Moore was one of its pioneers. She volunteered to help Steve.</p><p data-flatplan-paragraph="true">Within just a couple of weeks, she had narrowed down the search to two women, cousins of the same age. On Facebook, Steve could see that one cousin had four kids, and she regularly posted photos of them, beautiful and smiling. They looked well-off, their lives picture-perfect—“like a storybook,” Steve says. The other woman was unmarried; she didn’t have kids. She was not friends with her immediate family on Facebook, and she had moved halfway across the country from them. One evening—a Saturday, Steve clearly remembers—Moore asked to speak with him by phone.</p><p data-flatplan-paragraph="true">She confirmed what he had already suspected: His birth mom was the second woman. But Moore had another piece of news too. She had unexpectedly figured out something about his biological father as well.<em> It looks like your parents are related.</em> Steve didn’t know what to say. <em>Do you understand what I mean?</em> He said he thought so. <em>Either your mom’s father or your mom’s brother is your father.</em> A sea of emotions rose to a boil inside him: anger, hurt, worthlessness, disgust, shame, and devastation all at once. In his years of wondering about his birth, he had never, ever considered the possibility of incest. Why would he? What were the chances?</p><hr><p data-flatplan-paragraph="true">In 1975, around the time of Steve’s birth, a psychiatric textbook put the frequency of incest at one in a million.</p><p data-flatplan-paragraph="true">But this number is almost certainly a dramatic underestimate. The stigma around openly discussing incest, which often involves child sexual abuse, has long made the subject difficult to study. In the 1980s, <a data-event-element="inline link" href="https://www.hup.harvard.edu/books/9780674002708">feminist scholars argued</a>, based on the testimonies of victims, that incest was far more common than recognized, and in recent years, DNA has offered a new kind of biological proof. Widespread genetic testing is uncovering case after secret case of children born to close biological relatives—providing an unprecedented accounting of incest in modern society.</p><p data-flatplan-paragraph="true">The geneticist Jim Wilson, at the University of Edinburgh, was shocked by the frequency he found in the U.K. Biobank, an anonymized research database: One in 7,000 people, according to his unpublished analysis, was born to parents who were first-degree relatives—a brother and a sister or a parent and a child. “That’s way, way more than I think many people would ever imagine,” he told me. And this number is just a floor: It reflects only the cases that resulted in pregnancy, that did not end in miscarriage or abortion, and that led to the birth of a child who grew into an adult who volunteered for a research study.</p><p data-flatplan-paragraph="true">Most of the people affected may never know about their parentage, but these days, many are stumbling into the truth after AncestryDNA and 23andMe tests. Steve’s case was one of the first Moore worked on involving closely related parents. She now knows of well over 1,000 additional cases of people born from incest, the significant majority between first-degree relatives, with the rest between second-degree relatives (half-siblings, uncle-niece, aunt-nephew, grandparent-grandchild). The cases show up in every part of society, every strata of income, she told me.</p><p id="injected-recirculation-link-0" data-view-action="view link - injected link - item 1" data-event-element="injected link" data-event-position="1"><a href="https://www.theatlantic.com/science/archive/2018/07/dna-test-misattributed-paternity/562928/">Read: When a DNA test shatters your identity</a></p><p data-flatplan-paragraph="true">Neither AncestryDNA nor 23andMe informs customers about incest directly, so the thousand-plus cases Moore knows of all come from the tiny proportion of testers who investigated further. This meant, for example, uploading their DNA profiles to a third-party genealogy site to analyze what are known as “runs of homozygosity,” or ROH: long stretches where the DNA inherited from one’s mother and father are identical. For a while, one popular genealogy site instructed anyone who found high ROH to contact Moore. She would call them, one by one, to explain the jargon’s explosive meaning. Unwittingly, she became the keeper of what might be the world’s largest database of people born out of incest.</p><p data-flatplan-paragraph="true">In the overwhelming majority of cases, Moore told me, the parents are a father and a daughter or an older brother and a younger sister, meaning a child’s existence was likely evidence of sexual abuse. She had no obvious place to send people reeling from such revelations, and she was not herself a trained therapist. After seeing many of these cases, though, she wanted people to know they were not alone. Moore ended up creating a private and invite-only support group on Facebook in 2016, and she tapped Steve and later his wife, Michelle, to become admins, too. The three of them had become close in the months and years after the search for his birth mom, as they navigated the emotional fallout together.</p><p data-flatplan-paragraph="true">One day this past January, Michelle, who also works as Moore’s part-time assistant, told me she had spoken with four new people that week, all of them with ROH high enough to have parents who were first-degree relatives. She used to dread these calls. “I would stumble over my words,” she told me. But not anymore. She tells the shaken person on the line that they can join a support group full of people who are living the same reality. She tells them they can talk to her husband, Steve.</p><hr><p data-flatplan-paragraph="true">When Steve first discovered the truth about his biological parents, a decade ago, he had no support group to turn to, and he did not know what to do with the strange mix of emotions. He was genuinely happy to have found his birth mom. He had never looked like his adoptive parents, but in photos of her and her family, he could see his eyes, his chin, and even the smirky half-grin that his face naturally settles into.</p><p data-flatplan-paragraph="true">But he radiated with newfound anger, too, on her behalf. He could not know the exact circumstances of his conception, and his DNA test alone could not determine whether her older brother or her father was responsible. But Steve could not imagine a consensual scenario, given her age. The bespectacled 14-year-old girl who disappeared from the hospital had remained frozen in time in his mind, even as he himself grew older, got married, became a stepdad. He felt protective of that young girl.</p><p data-flatplan-paragraph="true">As badly as he wanted to know his birth mom, he worried she would not want to know him. Would his sudden reappearance dredge up traumatic memories—memories she had perhaps been trying to outrun her whole adult life, given how far away she had moved and how little she seemed connected to her family? A religious man, Steve prayed over it and settled on handwriting a letter. He included a couple of paragraphs about his life, some photos, and a message that he loved her. He left out what he knew about his paternity. And he took care to send the letter by certified mail, so that he could confirm its receipt and so that it would not accidentally fall into anyone else’s hands.</p><p data-flatplan-paragraph="true">She never responded. But Steve knew that she had received it: The post office sent him the green slip that she had signed upon delivery, and he scrutinized her signature—her actual name, written by her actual hand. At 40 years old, he touched for the first time something his mother had just touched, held something she had just held. He put the slip inside the pages of his Bible.</p><p data-flatplan-paragraph="true">Steve had never faulted his mother for leaving him at the hospital, and finding out about his paternity made him even more understanding. But the revelation also made him struggle with who he was. Did it mean that something was wrong with him, written into his DNA from the moment of his conception? On a <a data-event-element="inline link" href="https://podcasts.apple.com/us/podcast/ep-1-the-secret-one/id1572201167?i=1000526354990">podcast</a> later, he admitted to feeling like trash, “like something that somebody had just thrown away.” Those first six months after his discovery were the hardest six months of his life.</p><hr><p data-flatplan-paragraph="true">Across human cultures, incest between close family members is one of the most universal and most deeply held taboos. A common explanation is biological: Children born from related parents are more likely to develop health complications, because their parents are more likely to be carriers of the same recessive mutations. From the 1960s to the ’80s, a <a data-event-element="inline link" href="https://publications.aap.org/pediatrics/article-abstract/40/1/55/43627/CHILDREN-OF-INCEST">handful</a> <a data-event-element="inline link" href="https://www.sciencedirect.com/science/article/abs/pii/S0022347682803478">of</a> <a data-event-element="inline link" href="https://karger.com/hhe/article-abstract/21/2/108/158742/A-Study-of-Children-of-Incestuous-Matings">studies</a> following a few dozen children born of incest documented high rates of infant mortality and congenital conditions.</p><p data-flatplan-paragraph="true">But in the past, healthy children born from incestuous unions would have never come to the attention of doctors. As widespread DNA testing has uncovered orders of magnitude more people whose parents are brother and sister or parent and child, it’s also shown that plenty of those people are perfectly healthy. “There is a large element of chance in whether incest has a poor outcome,” according to Wilson, the geneticist. It depends on whether those runs of homozygosity contain recessive disease-causing mutations. All of us have some of these runs in our DNA—usually less than 1 percent of the genome in Western populations, higher in cultures where cousin marriage is common. But that number is about 25 percent, Wilson said, in people born from first-degree relatives. While the odds of a genetic disease are much higher, the outcome is far from predetermined.</p><p data-flatplan-paragraph="true">Still, these numbers make people wonder. Steve was born with a heart murmur, which required open-heart surgery at ages 13 and 18, though he does not know for sure the cause; heart defects are among the more common birth defects in the general population. He and Michelle were also never able to have children together. Others in the Facebook group have shared their struggles with autoimmune diseases, fibromyalgia, eye problems, and so on—though these are often hard to definitively link to incest. Health problems arising from incest might manifest in any number of ways, depending on exactly which mutations are inherited. “When I go to the doctor and they ask me my family history, I wonder: <em>How much do I need to go into it?</em>” says Mandy, another member of the group. (I am identifying some people by first name only, so they can speak freely about their family and medical histories.) How much experience would a typical doctor have with incest, anyway?</p><p data-flatplan-paragraph="true">After Mandy first learned that her father was her mother’s uncle, she went looking for stories about other people like her. All she could find were “gross fantasies” online and medical-journal articles about health problems. She felt very lonely. “<em>I don’t have anybody I can talk to about this</em>,” she remembers thinking. “<em>Nobody knows what to say.</em>” When she found the Facebook group, she could see that she was far from the only one like her. She watched the others cycle, too, through the stages of denial, anger, bargaining, depression, and acceptance.</p><p data-flatplan-paragraph="true">She does not know exactly what happened between her biological parents, but her mother was 17, and her mother’s uncle was in his 30s. The discovery, for all the hurt that it surfaced, has helped Mandy reconcile some of her childhood experiences. Unlike Steve, she was raised by her biological mother, and she believed her mother’s husband to be her biological father. He mostly ignored her, but her mother was cruel. She treated Mandy differently than she did her younger brothers. “At least now I have more of an answer as to why,” Mandy told me. “I wasn’t a bad kid and unlovable.”</p><p data-flatplan-paragraph="true">Kathy was also raised by her mother, though she had an early inkling that her dad was not her biological dad. Their blood types were incompatible, and she heard rumors about her mother and grandfather. Although her mother’s family was violent and chaotic, she was close to her dad’s family, especially her granny on that side. “They’ve been my rock,” she told me. By the time Kathy took a DNA test confirming that her dad was not her biological dad, she had spent a lifetime distancing herself from her biological family and embracing one with whom she shared no DNA.</p><p data-flatplan-paragraph="true">Hers was, in some ways, the opposite journey of adoptees such as Steve, who wanted so badly to know his biological family. But the two of them have become close. Kathy remembers how angry he used to be on his mother’s behalf. She told him that she used to be angry too, but she had to leave it behind. “It’s not going to bring me any peace. It’s not going to bring my mother any peace,” she recalled saying. And it wouldn’t undo what had been done to his mother by her father or her brother so many years ago.</p><hr><p data-flatplan-paragraph="true">In the end, Steve was able to identify his biological father, though not through any particular feat of genetic sleuthing. One day, two and a half years after his DNA test, he logged in to AncestryDNA and saw a parent match. It was his mother’s older brother. From the site, he could see that his father-uncle had logged in once, presumably seen that Steve was his son, and—even after Steve sent him a message—never logged back on again.</p><p data-flatplan-paragraph="true">By then, his initial anger had started to dissipate. He still felt deeply for his birth mom. Michelle says that her husband has always been a sensitive guy—she makes fun of him for crying at movies—but he’s become even more empathetic. The feeling of worthlessness he initially struggled with has given way to a sense of purpose; he and Michelle now spend hours on the phone talking with others in the support group.</p><p data-flatplan-paragraph="true">Steve has still never spoken to his birth mother. He tried writing to her a second time, sending a journal about his life—but she returned it unopened. He messages her occasionally on Facebook, sending photos of grandkids and puppies he’s raised. Every year, he wishes her a happy birthday. She has not replied, but she has also not blocked him.</p><p data-flatplan-paragraph="true">When the journal came back unopened, Steve decided to try messaging his mother’s cousin—the other woman he’d initially thought could be his birth mom. He yearned for some kind of connection with someone in his biological family. He wrote to the cousin about his mom—but not his dad—and she&nbsp; actually replied. She told him that she and his mom had been close as children, Steve recounted, but she did not know about a pregnancy. To her, it had seemed like her cousin one day “fell off the face of the Earth,” he says. She agreed to read his journal, and the two of them soon began speaking on the phone about their families.</p><p data-flatplan-paragraph="true">Months later, Steve felt like he could finally share the truth about his biological father, and the cousin again accepted him for who he was. They met for the first time in 2017 when she was visiting a nearby town, and she later invited Steve and Michelle to Thanksgiving. Last year, she extended another invitation to a large family gathering. Steve’s immediate biological family was not there, but hers was, and they all knew about him and his mom and his dad. They greeted him with hugs, and they took photos together as a family. “It felt like a relief,” he told me, like a burden had been lifted from him. In this family, he was not a secret.</p></section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 for Developers (439 pts)]]></title>
            <link>https://openai.com/index/introducing-gpt-5-for-developers</link>
            <guid>44827101</guid>
            <pubDate>Thu, 07 Aug 2025 17:06:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://openai.com/index/introducing-gpt-5-for-developers">https://openai.com/index/introducing-gpt-5-for-developers</a>, See on <a href="https://news.ycombinator.com/item?id=44827101">Hacker News</a></p>
Couldn't get https://openai.com/index/introducing-gpt-5-for-developers: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[GPT-5 (1930 pts)]]></title>
            <link>http://openai.com/gpt-5</link>
            <guid>44826997</guid>
            <pubDate>Thu, 07 Aug 2025 17:00:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="http://openai.com/gpt-5">http://openai.com/gpt-5</a>, See on <a href="https://news.ycombinator.com/item?id=44826997">Hacker News</a></p>
Couldn't get http://openai.com/gpt-5: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Live: GPT-5 (170 pts)]]></title>
            <link>https://www.youtube.com/watch?v=0Uu_VJeVVfo</link>
            <guid>44826463</guid>
            <pubDate>Thu, 07 Aug 2025 16:16:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=0Uu_VJeVVfo">https://www.youtube.com/watch?v=0Uu_VJeVVfo</a>, See on <a href="https://news.ycombinator.com/item?id=44826463">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Building Bluesky Comments for My Blog (329 pts)]]></title>
            <link>https://natalie.sh/posts/bluesky-comments/</link>
            <guid>44826164</guid>
            <pubDate>Thu, 07 Aug 2025 15:56:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://natalie.sh/posts/bluesky-comments/">https://natalie.sh/posts/bluesky-comments/</a>, See on <a href="https://news.ycombinator.com/item?id=44826164">Hacker News</a></p>
<div id="readability-page-1" class="page"><p data-astro-cid-2q5oecfc=""> I hate disqus too much. </p><article data-astro-cid-2q5oecfc=""> 
<p>I’ve been running my blog without decent comments for years. Not by choice, really - I just couldn’t find a solution that didn’t suck.</p>
<ul>
<li>
<p>Disqus? Slow, heavy, tracks users, and I don’t own anything. Plus it makes every page 100x slower to load.</p>
</li>
<li>
<p>Self-hosted solutions? Great in theory. (not really.) You’re signing up to manage users, moderate spam, maintain databases, and deal with all the headaches that come with running basically a miniature social platform. And if your users aren’t where you are, it’s probably slow as hell.</p>
</li>
<li>
<p>GitHub Issues as comments? Probably works for some developer blogs, but feels hacky and limits your audience to people with GitHub accounts.</p>
</li>
<li>
<p>No comments at all? Clean and simple, but you lose the conversations. Some of my favorite discoveries came from comment threads that went in unexpected directions.</p>
</li>
</ul>
<p>I’ve been a Bluesky user for a while. Recently, the community has been feeling healthier than Twitter ever did, the API is designed, and this decentralized approach means I don’t necessarily have to be beholden to a single company. People have been doing some interesting things with Bluesky, like on-protocol blog content and using Bluesky comments as a comment system. Why not do some of that for myself?</p>
<h2 id="why-bluesky-actually-makes-sense">Why Bluesky Actually Makes Sense</h2>
<p>The more I thought about it, the more directly using Bluesky for comments made sense:</p>
<ul>
<li>
<p>No infrastructure to maintain. (for me, at least) I don’t need to run databases, manage user accounts, or build moderation tools. Bluesky handles all of that.</p>
</li>
<li>
<p>Rich(er) content support. People can post images, links, and in threads. All the stuff that makes conversations interesting.</p>
</li>
<li>
<p>Real identities. Since people are using their actual Bluesky profiles, and your one profile can <em>actually</em> be used on any supported platform, there’s more accountability and less incentive to drive-by troll.</p>
</li>
<li>
<p>Cross-platform conversations. Comments live on Bluesky too, so people can discover my blog posts through social media and vice versa.</p>
</li>
<li>
<p>I own my content, they own theirs. No platform lock-in for anyone!</p>
</li>
</ul>
<p>The workflow is simple: I publish a blog post, share it on Bluesky, edit the post to add the AT URI, and the replies to that Bluesky post become the comments on my blog.</p>
<h2 id="building-the-component">Building the Component</h2>
<h3 id="understanding-the-at-protocol">Understanding the AT Protocol</h3>
<p>Bluesky runs on the AT Protocol, which has surprisingly okay documentation. The key concepts I needed:</p>
<ul>
<li><strong>DIDs</strong> (Decentralized Identifiers): Unique user IDs like <code>did:plc:abc123...</code> or <code>did:web:joe.coffee</code></li>
<li><strong>CIDs</strong> (Content Identifiers): Unique post IDs</li>
<li><strong>AT URIs</strong>: Addresses for content like <code>at://did:plc:user.../app.bsky.feed.post/postid</code></li>
</ul>
<p>To fetch comments, I just need to call the <code>getPostThread</code> endpoint with the right URI. No authentication required. Easy peasy.</p>
<h3 id="component-architecture">Component Architecture</h3>
<p>I ended up with three main pieces:</p>
<ol>
<li>The main comments component that fetches and displays the thread.</li>
<li>A reply component that handles rendering individual posts and their replies. Also includes metadata and a link to the original Bluesky post.</li>
<li>An embed component for rich content like images and open graph previews.</li>
</ol>
<p>This separation made each piece reasonably manageable, reasonable, and small.</p>
<h3 id="the-threading-challenge">The Threading Challenge</h3>
<p>The interesting part was handling nested replies. Bluesky threads can go arbitrarily deep, but I needed to display them in a way that’s readable and doesn’t break layouts.</p>
<p>I settled on a naive recursive approach where each reply can render child replies, with visual indentation to show the hierarchy. I cap it at 5 levels deep because beyond that, conversations usually devolve into two people arguing anyway.</p>
<pre tabindex="0" data-language="typescript"><code><span><span>const</span><span> MAX_DEPTH </span><span>=</span><span> 5</span><span>;</span></span>
<span><span>const</span><span> BlueskyReply</span><span> =</span><span> ({</span><span> thread</span><span>,</span><span> depth</span><span> =</span><span> 0</span><span> })</span><span> =&gt;</span><span> {</span></span>
<span><span>  return</span><span> (</span></span>
<span><span>    &lt;</span><span>div style</span><span>=</span><span>{{</span><span> marginLeft</span><span>:</span><span> depth </span><span>*</span><span> 12</span><span> }}</span><span>&gt;</span></span>
<span><span>      {</span><span>/* Render the post content */</span><span>}</span></span>
<span></span>
<span><span>      {</span><span>depth</span><span> &lt; </span><span>MAX_DEPTH</span><span> &amp;&amp; </span><span>thread</span><span>.</span><span>replies</span><span>?.</span><span>map</span><span>(</span><span>reply</span><span> =&gt;</span></span>
<span><span>        &lt;</span><span>BlueskyReply</span><span> thread</span><span>=</span><span>{</span><span>reply</span><span>}</span><span> depth</span><span>=</span><span>{</span><span>depth + </span><span>1</span><span>}</span><span> /&gt;</span></span>
<span><span>      )</span><span>}</span></span>
<span><span>    &lt;/</span><span>div</span><span>&gt;</span></span>
<span><span>  )</span><span>;</span></span>
<span><span>};</span></span></code></pre>
<h3 id="handling-rich-content">Handling Rich Content</h3>
<p>One of the nice things about Bluesky is that posts can contain more than just text. People embed images, external links, and even quote other posts. Each embed type needs special handling.</p>
<p><strong>Images</strong> were the most complex. Bluesky serves them through their CDN, and people often post multiple images in a single reply. I built a responsive grid layout that adapts based on image count, plus a modal for viewing images full-screen.</p>
<p><strong>External links</strong> get rendered as cards with thumbnails and descriptions, just like they appear in Bluesky apps.</p>
<p><strong>Other embed types</strong> get a graceful fallback message since the AT Protocol is extensible and new embed types might appear.</p>
<h3 id="integrating-with-astro">Integrating with Astro</h3>
<p>Getting this working with my Astro blog was straightforward. I had the React integration (which I already had for my background and music components) and used the <code>client:load</code> directive to ensure the comment component hydrates immediately:</p>
<pre tabindex="0" data-language="astro"><code><span><span>---</span></span>
<span><span>import</span><span> BlueskyComments </span><span>from</span><span> '../components/bsky-comments.tsx'</span><span>;</span></span>
<span><span>---</span></span>
<span></span>
<span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky </span><span>&amp;&amp;</span><span> (</span></span>
<span><span>  &lt;</span><span>BlueskyComments</span></span>
<span><span>    did</span><span>=</span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky</span><span>.</span><span>did</span><span>}</span></span>
<span><span>    postCid</span><span>=</span><span>{</span><span>post</span><span>.</span><span>data</span><span>.</span><span>bsky</span><span>.</span><span>postCid</span><span>}</span></span>
<span><span>    client</span><span>:</span><span>load</span></span>
<span><span>  /&gt;</span></span>
<span><span>)</span><span>}</span></span></code></pre>
<p>Now I just add this to any post’s frontmatter to enable comments:</p>
<pre tabindex="0" data-language="yaml"><code><span><span>bsky</span><span>:</span></span>
<span><span>  did</span><span>:</span><span> "my-bluesky-did"</span></span>
<span><span>  postCid</span><span>:</span><span> "the-post-id"</span></span></code></pre>
<h2 id="what-i-learned">What I Learned</h2>
<h3 id="typescript-is-your-friend">TypeScript is Your Friend</h3>
<p>There are proper TypeScript types for all their API responses through the <code>@atcute/client</code> package. This made development much smoother as I could rely on autocomplete and catch type errors before they became runtime bugs.</p>
<h3 id="progressive-enhancement-works">Progressive Enhancement Works</h3>
<p>I built the comments as an enhancement to the blog, not a core dependency. If JavaScript is disabled or the API is down, the blog post (rendered a long time ago) still works perfectly. The comments just don’t appear.</p>
<h3 id="performance-by-default--ish">Performance by Default (-ish)</h3>
<p>Since I’m not managing any backend infrastructure, server-side performance optimizations are just there. Bluesky’s CDN handles image delivery, their public API is fast and cached, and I don’t have to care about database queries or server scaling.</p>
<h2 id="the-results">The Results</h2>
<p>I’m pretty happy with how it turned out. The conversations feel more natural than traditional blog comments - people use their actual profiles, share images and links, and more. It’s a lot more like social media.</p>
<h2 id="whats-next">What’s Next</h2>
<p>I’m considering a few improvements, but honestly, the core system works so well that I’m not in a rush to change it. Sometimes the best solution is the one just works, almost invisibly.</p>
<h2 id="why-this-approach-works">Why This Approach Works</h2>
<p>Traditional comment systems try to recreate social media features on every individual website. I think that’s backwards. People already have social media accounts they like using. Instead of forcing them to create new accounts and learn new interfaces, why not try meeting them where they already are?</p>
<p>This approach scales with the platform because it <em>uses</em> the platform. As Bluesky grows, more people can participate in blog discussions without any additional work from me. And because everything is built on open protocols, I’m not locked into any single platform’s decisions. If Bluesky ever changes for the worse, I can always switch to another AppView, such as zeppelin or Blacksky’s AppView.</p>
<p>I could theoretically even write my own comments AppView. ATProto is designed to be flexible, especially with data, so doing such would be quite simple. I’d just need to listen to the right events on the firehose and store the data in a way that makes sense and rebuild the comment thread every so often.</p>
<p>In my opinion, the web is better when independent sites can connect to broader conversations without sacrificing their independence. I feel like this <del>is</del> was the goal of other decentralised platforms like Mastodon, but Bluesky’s focus on user-owned identities and app intercompat via the PDS ultimately makes it a better fit.</p>
<hr>
<p><em>Want to see it in action? The comments are right below this post, powered by the system I mentioned above. Meta.</em></p> </article></div>]]></description>
        </item>
    </channel>
</rss>