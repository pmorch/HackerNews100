<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Wed, 11 Jun 2025 14:30:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Show HN: S3mini – Tiny and fast S3-compatible client, no-deps, edge-ready (120 pts)]]></title>
            <link>https://github.com/good-lly/s3mini</link>
            <guid>44245577</guid>
            <pubDate>Wed, 11 Jun 2025 08:55:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/good-lly/s3mini">https://github.com/good-lly/s3mini</a>, See on <a href="https://news.ycombinator.com/item?id=44245577">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">s3mini | Tiny &amp; fast S3 client for node and edge platforms.</h2><a id="user-content-s3mini--tiny--fast-s3-client-for-node-and-edge-platforms" aria-label="Permalink: s3mini | Tiny &amp; fast S3 client for node and edge platforms." href="#s3mini--tiny--fast-s3-client-for-node-and-edge-platforms"></a></p>
<p dir="auto"><code>s3mini</code> is an ultra-lightweight Typescript client (~14 KB minified, ≈15 % more ops/s) for S3-compatible object storage. It runs on Node, Bun, Cloudflare Workers, and other edge platforms. It has been tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, and MinIO. (No Browser support!)</p>
<p dir="auto">[<a href="https://github.com/good-lly/s3mini">github</a>]
[<a href="https://github.com/good-lly/s3mini/issues">issues</a>]
[<a href="https://www.npmjs.com/package/s3mini" rel="nofollow">npm</a>]</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Features</h2><a id="user-content-features" aria-label="Permalink: Features" href="#features"></a></p>
<ul dir="auto">
<li>🚀 Light and fast: averages ≈15 % more ops/s and only ~14 KB (minified, not gzipped).</li>
<li>🔧 Zero dependencies; supports AWS SigV4 (no pre-signed requests).</li>
<li>🟠 Works on Cloudflare Workers; ideal for edge computing, Node, and Bun (no browser support).</li>
<li>🔑 Only the essential S3 APIs—improved list, put, get, delete, and a few more.</li>
<li>📦 <strong>BYOS3</strong> — <em>Bring your own S3-compatible bucket</em> (tested on Cloudflare R2, Backblaze B2, DigitalOcean Spaces, MinIO and Garage! Ceph and AWS are in the queue).</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Tested On</h4><a id="user-content-tested-on" aria-label="Permalink: Tested On" href="#tested-on"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/good-lly/s3mini/blob/dev/testedon.png"><img src="https://github.com/good-lly/s3mini/raw/dev/testedon.png" alt="Tested On"></a></p>
<p dir="auto">Dev:</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e"><img src="https://camo.githubusercontent.com/bf7b30c3cfdbd3ccc409731dfcf7309229b1fc1f2eaeece077ae0163720a2d7a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d69742d61637469766974792f6d2f676f6f642d6c6c792f73336d696e692f6465763f636f6c6f723d67726565656e" alt="GitHub commit activity (branch)" data-canonical-src="https://img.shields.io/github/commit-activity/m/good-lly/s3mini/dev?color=greeen"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/6a746b9ad4507bf242d021923444bddc75cc1a3d306fdc55134a85e6ccba8fa8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f676f6f642d6c6c792f73336d696e69" alt="GitHub Issues or Pull Requests" data-canonical-src="https://img.shields.io/github/issues/good-lly/s3mini"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/codeql.yml/badge.svg?branch=dev" alt="CodeQL Advanced"></a>
<a href="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml"><img src="https://github.com/good-lly/s3mini/actions/workflows/test-e2e.yml/badge.svg?branch=dev" alt="Test:e2e(all)"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c"><img src="https://camo.githubusercontent.com/822c31326e4a40a59e2a813db9dcbdb1bce1c5b687237c4aae892d8850a43e9b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f676f6f642d6c6c792f73336d696e693f7374796c653d736f6369616c" alt="GitHub Repo stars" data-canonical-src="https://img.shields.io/github/stars/good-lly/s3mini?style=social"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69"><img src="https://camo.githubusercontent.com/4de7585b00e2b338207229723bab399bde2b3fefa531015a529b52d874aa75fd/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f646d2f73336d696e69" alt="NPM Downloads" data-canonical-src="https://img.shields.io/npm/dm/s3mini"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/863f019439a06855c998b1a1a01302f70728781442f5236090683d1793445959/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f73336d696e693f636f6c6f723d677265656e" alt="NPM Version" data-canonical-src="https://img.shields.io/npm/v/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e"><img src="https://camo.githubusercontent.com/179914f6471af92eb18a918ccc151661f36231a77b5a8665f45e294eb52428af/68747470733a2f2f696d672e736869656c64732e696f2f62756e646c656a732f73697a652f73336d696e693f636f6c6f723d677265656e" alt="npm package minimized gzipped size" data-canonical-src="https://img.shields.io/bundlejs/size/s3mini?color=green"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69"><img src="https://camo.githubusercontent.com/130a065a22021d83141baa1284b5479f286d238097574885c18c4f1a78669ae4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f676f6f642d6c6c792f73336d696e69" alt="GitHub License" data-canonical-src="https://img.shields.io/github/license/good-lly/s3mini"></a></p>
<p dir="auto"><a href="https://github.com/good-lly/s3mini/issues/"> <img src="https://camo.githubusercontent.com/b20f68f697b27055937e1f8238eec47e2cb37f5fc74960da5dfbfe40e04af19a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e737667" alt="Contributions welcome" data-canonical-src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg"></a></p>
<p dir="auto">Performance tests was done on local Minio instance. Your results may vary depending on environment and network conditions, so take it with a grain of salt.
<a target="_blank" rel="noopener noreferrer nofollow" href="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png"><img src="https://raw.githubusercontent.com/good-lly/s3mini/dev/performance-screenshot.png" alt="performance-image"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Table of Contents</h2><a id="user-content-table-of-contents" aria-label="Permalink: Table of Contents" href="#table-of-contents"></a></p>
<ul dir="auto">
<li><a href="#supported-ops">Supported Ops</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#security-notes">Security Notes</a></li>
<li><a href="#contributions-welcomed">💙 Contributions welcomed!</a></li>
<li><a href="#license">License</a></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Supported Ops</h2><a id="user-content-supported-ops" aria-label="Permalink: Supported Ops" href="#supported-ops"></a></p>
<p dir="auto">The library supports a subset of S3 operations, focusing on essential features, making it suitable for environments with limited resources.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Bucket ops</h4><a id="user-content-bucket-ops" aria-label="Permalink: Bucket ops" href="#bucket-ops"></a></p>
<ul dir="auto">
<li>✅ HeadBucket (bucketExists)</li>
<li>✅ createBucket (createBucket)</li>
</ul>
<p dir="auto"><h4 tabindex="-1" dir="auto">Objects ops</h4><a id="user-content-objects-ops" aria-label="Permalink: Objects ops" href="#objects-ops"></a></p>
<ul dir="auto">
<li>✅ ListObjectsV2 (listObjects)</li>
<li>✅ GetObject (getObject, getObjectResponse, getObjectWithETag, getObjectRaw, getObjectArrayBuffer, getObjectJSON)</li>
<li>✅ PutObject (putObject)</li>
<li>✅ DeleteObject (deleteObject)</li>
<li>✅ HeadObject (objectExists, getEtag, getContentLength)</li>
<li>✅ listMultipartUploads</li>
<li>✅ CreateMultipartUpload (getMultipartUploadId)</li>
<li>✅ completeMultipartUpload</li>
<li>✅ abortMultipartUpload</li>
<li>✅ uploadPart</li>
<li>❌ CopyObject: Not implemented (tbd)</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation</h2><a id="user-content-installation" aria-label="Permalink: Installation" href="#installation"></a></p>



<blockquote>
<p dir="auto"><strong><g-emoji alias="warning">⚠️</g-emoji> Environment Support Notice</strong></p>
<p dir="auto">This library is designed to run in environments like <strong>Node.js</strong>, <strong>Bun</strong>, and <strong>Cloudflare Workers</strong>. It does <strong>not support browser environments</strong> due to the use of Node.js APIs and polyfills.</p>
<p dir="auto"><strong>Cloudflare Workers:</strong> To enable built-in Node.js Crypto API, add the <code>nodejs_compat</code> compatibility flag to your Wrangler configuration file. This also enables <code>nodejs_compat_v2</code> as long as your compatibility date is <code>2024-09-23</code> or later. <a href="https://developers.cloudflare.com/workers/configuration/compatibility-dates/#nodejs-compatibility-flag" rel="nofollow">Learn more about the Node.js compatibility flag and v2</a>.</p>
</blockquote>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="import { s3mini, sanitizeETag } from 's3mini';

const s3client = new s3mini({
  accessKeyId: config.accessKeyId,
  secretAccessKey: config.secretAccessKey,
  endpoint: config.endpoint,
  region: config.region,
});

// Basic bucket ops
let exists: boolean = false;
try {
  // Check if the bucket exists
  exists = await s3client.bucketExists();
} catch (err) {
  throw new Error(`Failed bucketExists() call, wrong credentials maybe: ${err.message}`);
}
if (!exists) {
  // Create the bucket based on the endpoint bucket name
  await s3client.createBucket();
}

// Basic object ops
// key is the name of the object in the bucket
const smallObjectKey: string = 'small-object.txt';
// content is the data you want to store in the object
// it can be a string or Buffer (recommended for large objects)
const smallObjectContent: string = 'Hello, world!';

// check if the object exists
const objectExists: boolean = await s3client.objectExists(smallObjectKey);
let etag: string | null = null;
if (!objectExists) {
  // put/upload the object, content can be a string or Buffer
  // to add object into &quot;folder&quot;, use &quot;folder/filename.txt&quot; as key
  const resp: Response = await s3client.putObject(smallObjectKey, smallObjectContent);
  // you can also get etag via getEtag method
  // const etag: string = await s3client.getEtag(smallObjectKey);
  etag = sanitizeETag(resp.headers.get('etag'));
}

// get the object, null if not found
const objectData: string | null = await s3client.getObject(smallObjectKey);
console.log('Object data:', objectData);

// get the object with ETag, null if not found
const response2: Response = await s3mini.getObject(smallObjectKey, { 'if-none-match': etag });
if (response2) {
  // ETag changed so we can get the object data and new ETag
  // Note: ETag is not guaranteed to be the same as the MD5 hash of the object
  // ETag is sanitized to remove quotes
  const etag2: string = sanitizeETag(response2.headers.get('etag'));
  console.log('Object data with ETag:', response2.body, 'ETag:', etag2);
} else {
  console.log('Object not found or ETag does match.');
}

// list objects in the bucket, null if bucket is empty
// Note: listObjects uses listObjectsV2 API and iterate over all pages
// so it will return all objects in the bucket which can take a while
// If you want to limit the number of objects returned, use the maxKeys option
// If you want to list objects in a specific &quot;folder&quot;, use &quot;folder/&quot; as prefix
// Example s3client.listObjects({&quot;/&quot; &quot;myfolder/&quot;})
const list: object[] | null = await s3client.listObjects();
if (list) {
  console.log('List of objects:', list);
} else {
  console.log('No objects found in the bucket.');
}

// delete the object
const wasDeleted: boolean = await s3client.deleteObject(smallObjectKey);

// Multipart upload
const multipartKey = 'multipart-object.txt';
const large_buffer = new Uint8Array(1024 * 1024 * 15); // 15 MB buffer
const partSize = 8 * 1024 * 1024; // 8 MB
const totalParts = Math.ceil(large_buffer.length / partSize);
// Beware! This will return always a new uploadId
// if you want to use the same uploadId, you need to store it somewhere
const uploadId = await s3client.getMultipartUploadId(multipartKey);
const uploadPromises = [];
for (let i = 0; i < totalParts; i++) {
  const partBuffer = large_buffer.subarray(i * partSize, (i + 1) * partSize);
  // upload each part
  // Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel
  // but be careful with the number of parallel uploads, it can cause throttling
  // or errors if you upload too many parts at once
  // You can also use generator functions to upload parts in batches
  uploadPromises.push(s3client.uploadPart(multipartKey, uploadId, partBuffer, i + 1));
}
const uploadResponses = await Promise.all(uploadPromises);
const parts = uploadResponses.map((response, index) => ({
  partNumber: index + 1,
  etag: response.etag,
}));
// Complete the multipart upload
const completeResponse = await s3client.completeMultipartUpload(multipartKey, uploadId, parts);
const completeEtag = completeResponse.etag;

// List multipart uploads
// returns object with uploadId and key
const multipartUploads: object = await s3client.listMultipartUploads();
// Abort the multipart upload
const abortResponse = await s3client.abortMultipartUpload(multipartUploads.key, multipartUploads.uploadId);

// Multipart download
// lets test getObjectRaw with range
const rangeStart = 2048 * 1024; // 2 MB
const rangeEnd = 8 * 1024 * 1024 * 2; // 16 MB
const rangeResponse = await s3client.getObjectRaw(multipartKey, false, rangeStart, rangeEnd);
const rangeData = await rangeResponse.arrayBuffer();"><pre><span>import</span> <span>{</span> <span>s3mini</span><span>,</span> <span>sanitizeETag</span> <span>}</span> <span>from</span> <span>'s3mini'</span><span>;</span>

<span>const</span> <span>s3client</span> <span>=</span> <span>new</span> <span>s3mini</span><span>(</span><span>{</span>
  <span>accessKeyId</span>: <span>config</span><span>.</span><span>accessKeyId</span><span>,</span>
  <span>secretAccessKey</span>: <span>config</span><span>.</span><span>secretAccessKey</span><span>,</span>
  <span>endpoint</span>: <span>config</span><span>.</span><span>endpoint</span><span>,</span>
  <span>region</span>: <span>config</span><span>.</span><span>region</span><span>,</span>
<span>}</span><span>)</span><span>;</span>

<span>// Basic bucket ops</span>
<span>let</span> <span>exists</span>: <span>boolean</span> <span>=</span> <span>false</span><span>;</span>
<span>try</span> <span>{</span>
  <span>// Check if the bucket exists</span>
  <span>exists</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>bucketExists</span><span>(</span><span>)</span><span>;</span>
<span>}</span> <span>catch</span> <span>(</span><span>err</span><span>)</span> <span>{</span>
  <span>throw</span> <span>new</span> <span>Error</span><span>(</span><span>`Failed bucketExists() call, wrong credentials maybe: <span><span>${</span><span>err</span><span>.</span><span>message</span><span>}</span></span>`</span><span>)</span><span>;</span>
<span>}</span>
<span>if</span> <span>(</span><span>!</span><span>exists</span><span>)</span> <span>{</span>
  <span>// Create the bucket based on the endpoint bucket name</span>
  <span>await</span> <span>s3client</span><span>.</span><span>createBucket</span><span>(</span><span>)</span><span>;</span>
<span>}</span>

<span>// Basic object ops</span>
<span>// key is the name of the object in the bucket</span>
<span>const</span> <span>smallObjectKey</span>: <span>string</span> <span>=</span> <span>'small-object.txt'</span><span>;</span>
<span>// content is the data you want to store in the object</span>
<span>// it can be a string or Buffer (recommended for large objects)</span>
<span>const</span> <span>smallObjectContent</span>: <span>string</span> <span>=</span> <span>'Hello, world!'</span><span>;</span>

<span>// check if the object exists</span>
<span>const</span> <span>objectExists</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>objectExists</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>let</span> <span>etag</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>null</span><span>;</span>
<span>if</span> <span>(</span><span>!</span><span>objectExists</span><span>)</span> <span>{</span>
  <span>// put/upload the object, content can be a string or Buffer</span>
  <span>// to add object into "folder", use "folder/filename.txt" as key</span>
  <span>const</span> <span>resp</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>putObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>smallObjectContent</span><span>)</span><span>;</span>
  <span>// you can also get etag via getEtag method</span>
  <span>// const etag: string = await s3client.getEtag(smallObjectKey);</span>
  <span>etag</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>resp</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
<span>}</span>

<span>// get the object, null if not found</span>
<span>const</span> <span>objectData</span>: <span>string</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data:'</span><span>,</span> <span>objectData</span><span>)</span><span>;</span>

<span>// get the object with ETag, null if not found</span>
<span>const</span> <span>response2</span>: <span>Response</span> <span>=</span> <span>await</span> <span>s3mini</span><span>.</span><span>getObject</span><span>(</span><span>smallObjectKey</span><span>,</span> <span>{</span> <span>'if-none-match'</span>: <span>etag</span> <span>}</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>response2</span><span>)</span> <span>{</span>
  <span>// ETag changed so we can get the object data and new ETag</span>
  <span>// Note: ETag is not guaranteed to be the same as the MD5 hash of the object</span>
  <span>// ETag is sanitized to remove quotes</span>
  <span>const</span> <span>etag2</span>: <span>string</span> <span>=</span> <span>sanitizeETag</span><span>(</span><span>response2</span><span>.</span><span>headers</span><span>.</span><span>get</span><span>(</span><span>'etag'</span><span>)</span><span>)</span><span>;</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object data with ETag:'</span><span>,</span> <span>response2</span><span>.</span><span>body</span><span>,</span> <span>'ETag:'</span><span>,</span> <span>etag2</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'Object not found or ETag does match.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// list objects in the bucket, null if bucket is empty</span>
<span>// Note: listObjects uses listObjectsV2 API and iterate over all pages</span>
<span>// so it will return all objects in the bucket which can take a while</span>
<span>// If you want to limit the number of objects returned, use the maxKeys option</span>
<span>// If you want to list objects in a specific "folder", use "folder/" as prefix</span>
<span>// Example s3client.listObjects({"/" "myfolder/"})</span>
<span>const</span> <span>list</span>: <span>object</span><span>[</span><span>]</span> <span>|</span> <span>null</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listObjects</span><span>(</span><span>)</span><span>;</span>
<span>if</span> <span>(</span><span>list</span><span>)</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'List of objects:'</span><span>,</span> <span>list</span><span>)</span><span>;</span>
<span>}</span> <span>else</span> <span>{</span>
  <span>console</span><span>.</span><span>log</span><span>(</span><span>'No objects found in the bucket.'</span><span>)</span><span>;</span>
<span>}</span>

<span>// delete the object</span>
<span>const</span> <span>wasDeleted</span>: <span>boolean</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>deleteObject</span><span>(</span><span>smallObjectKey</span><span>)</span><span>;</span>

<span>// Multipart upload</span>
<span>const</span> <span>multipartKey</span> <span>=</span> <span>'multipart-object.txt'</span><span>;</span>
<span>const</span> <span>large_buffer</span> <span>=</span> <span>new</span> <span>Uint8Array</span><span>(</span><span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>15</span><span>)</span><span>;</span> <span>// 15 MB buffer</span>
<span>const</span> <span>partSize</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span><span>;</span> <span>// 8 MB</span>
<span>const</span> <span>totalParts</span> <span>=</span> <span>Math</span><span>.</span><span>ceil</span><span>(</span><span>large_buffer</span><span>.</span><span>length</span> <span>/</span> <span>partSize</span><span>)</span><span>;</span>
<span>// Beware! This will return always a new uploadId</span>
<span>// if you want to use the same uploadId, you need to store it somewhere</span>
<span>const</span> <span>uploadId</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getMultipartUploadId</span><span>(</span><span>multipartKey</span><span>)</span><span>;</span>
<span>const</span> <span>uploadPromises</span> <span>=</span> <span>[</span><span>]</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>totalParts</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>const</span> <span>partBuffer</span> <span>=</span> <span>large_buffer</span><span>.</span><span>subarray</span><span>(</span><span>i</span> <span>*</span> <span>partSize</span><span>,</span> <span>(</span><span>i</span> <span>+</span> <span>1</span><span>)</span> <span>*</span> <span>partSize</span><span>)</span><span>;</span>
  <span>// upload each part</span>
  <span>// Note: uploadPart returns a promise, so you can use Promise.all to upload all parts in parallel</span>
  <span>// but be careful with the number of parallel uploads, it can cause throttling</span>
  <span>// or errors if you upload too many parts at once</span>
  <span>// You can also use generator functions to upload parts in batches</span>
  <span>uploadPromises</span><span>.</span><span>push</span><span>(</span><span>s3client</span><span>.</span><span>uploadPart</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>partBuffer</span><span>,</span> <span>i</span> <span>+</span> <span>1</span><span>)</span><span>)</span><span>;</span>
<span>}</span>
<span>const</span> <span>uploadResponses</span> <span>=</span> <span>await</span> <span>Promise</span><span>.</span><span>all</span><span>(</span><span>uploadPromises</span><span>)</span><span>;</span>
<span>const</span> <span>parts</span> <span>=</span> <span>uploadResponses</span><span>.</span><span>map</span><span>(</span><span>(</span><span>response</span><span>,</span> <span>index</span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
  <span>partNumber</span>: <span>index</span> <span>+</span> <span>1</span><span>,</span>
  <span>etag</span>: <span>response</span><span>.</span><span>etag</span><span>,</span>
<span>}</span><span>)</span><span>)</span><span>;</span>
<span>// Complete the multipart upload</span>
<span>const</span> <span>completeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>completeMultipartUpload</span><span>(</span><span>multipartKey</span><span>,</span> <span>uploadId</span><span>,</span> <span>parts</span><span>)</span><span>;</span>
<span>const</span> <span>completeEtag</span> <span>=</span> <span>completeResponse</span><span>.</span><span>etag</span><span>;</span>

<span>// List multipart uploads</span>
<span>// returns object with uploadId and key</span>
<span>const</span> <span>multipartUploads</span>: <span>object</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>listMultipartUploads</span><span>(</span><span>)</span><span>;</span>
<span>// Abort the multipart upload</span>
<span>const</span> <span>abortResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>abortMultipartUpload</span><span>(</span><span>multipartUploads</span><span>.</span><span>key</span><span>,</span> <span>multipartUploads</span><span>.</span><span>uploadId</span><span>)</span><span>;</span>

<span>// Multipart download</span>
<span>// lets test getObjectRaw with range</span>
<span>const</span> <span>rangeStart</span> <span>=</span> <span>2048</span> <span>*</span> <span>1024</span><span>;</span> <span>// 2 MB</span>
<span>const</span> <span>rangeEnd</span> <span>=</span> <span>8</span> <span>*</span> <span>1024</span> <span>*</span> <span>1024</span> <span>*</span> <span>2</span><span>;</span> <span>// 16 MB</span>
<span>const</span> <span>rangeResponse</span> <span>=</span> <span>await</span> <span>s3client</span><span>.</span><span>getObjectRaw</span><span>(</span><span>multipartKey</span><span>,</span> <span>false</span><span>,</span> <span>rangeStart</span><span>,</span> <span>rangeEnd</span><span>)</span><span>;</span>
<span>const</span> <span>rangeData</span> <span>=</span> <span>await</span> <span>rangeResponse</span><span>.</span><span>arrayBuffer</span><span>(</span><span>)</span><span>;</span></pre></div>
<p dir="auto">For more check <a href="https://github.com/good-lly/s3mini/blob/dev/USAGE.md">USAGE.md</a> file, examples and tests.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security Notes</h2><a id="user-content-security-notes" aria-label="Permalink: Security Notes" href="#security-notes"></a></p>
<ul dir="auto">
<li>The library masks sensitive information (access keys, session tokens, etc.) when logging.</li>
<li>Always protect your AWS credentials and avoid hard-coding them in your application (!!!). Use environment variables. Use environment variables or a secure vault for storing credentials.</li>
<li>Ensure you have the necessary permissions to access the S3 bucket and perform operations.</li>
<li>Be cautious when using multipart uploads, as they can incur additional costs if not managed properly.</li>
<li>Authors are not responsible for any data loss or security breaches resulting from improper usage of the library.</li>
<li>If you find a security vulnerability, please report it to us directly via email. For more details, please refer to the <a href="https://github.com/good-lly/s3mini/blob/dev/SECURITY.md">SECURITY.md</a> file.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributions welcomed!</h2><a id="user-content-contributions-welcomed" aria-label="Permalink: Contributions welcomed!" href="#contributions-welcomed"></a></p>
<p dir="auto">Contributions are greatly appreciated! If you have an idea for a new feature or have found a bug, we encourage you to get involved:</p>
<ul dir="auto">
<li>
<p dir="auto"><em>Report Issues</em>: If you encounter a problem or have a feature request, please open an issue on GitHub. Include as much detail as possible (environment, error messages, logs, steps to reproduce, etc.) so we can understand and address the issue.</p>
</li>
<li>
<p dir="auto"><em>Pull Requests</em>: We welcome PRs! If you want to implement a new feature or fix a bug, feel free to submit a pull request to the latest <code>dev branch</code>. For major changes, it's a good idea to discuss your plans in an issue first.</p>
</li>
<li>
<p dir="auto"><em>Lightweight Philosophy</em>: When contributing, keep in mind that s3mini aims to remain lightweight and dependency-free. Please avoid adding heavy dependencies. New features should provide significant value to justify any increase in size.</p>
</li>
<li>
<p dir="auto"><em>Community Conduct</em>: Be respectful and constructive in communications. We want a welcoming environment for all contributors. For more details, please refer to our <a href="https://github.com/good-lly/s3mini/blob/dev/CODE_OF_CONDUCT.md">CODE_OF_CONDUCT.md</a>. No one reads it, but it's there for a reason.</p>
</li>
</ul>
<p dir="auto">If you figure out a solution to your question or problem on your own, please consider posting the answer or closing the issue with an explanation. It could help the next person who runs into the same thing!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">This project is licensed under the MIT License - see the <a href="https://github.com/good-lly/s3mini/blob/dev/LICENSE.md">LICENSE.md</a> file for details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Sponsor This Project</h2><a id="user-content-sponsor-this-project" aria-label="Permalink: Sponsor This Project" href="#sponsor-this-project"></a></p>
<p dir="auto">Developing and maintaining s3mini (and other open-source projects) requires time and effort. If you find this library useful, please consider sponsoring its development. Your support helps ensure I can continue improving s3mini and other projects. Thank you!</p>
<p dir="auto"><a href="https://github.com/sponsors/good-lly"><img src="https://camo.githubusercontent.com/a12b8f68983edf77bb6d60e2b29a80bcfafd07082c88b0a5decd3c0f3b1dd672/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2546302539462539322542385f4769744875622d53706f6e736f722d6666363962343f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465" alt="Become a Sponsor" data-canonical-src="https://img.shields.io/badge/%F0%9F%92%B8_GitHub-Sponsor-ff69b4?logo=github&amp;logoColor=white"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Left-Pad (2024) (258 pts)]]></title>
            <link>https://azerkoculu.com/posts/left-pad</link>
            <guid>44245166</guid>
            <pubDate>Wed, 11 Jun 2025 07:44:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://azerkoculu.com/posts/left-pad">https://azerkoculu.com/posts/left-pad</a>, See on <a href="https://news.ycombinator.com/item?id=44245166">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>8 years have passed since the <a href="https://en.wikipedia.org/wiki/Npm_left-pad_incident">the left-pad incident</a> happened. It's been good for me to avoid this subject for focusing on actual work. Also, silence is gold. On the other hand, left-pad is seen as a notable event mentioned in books, so I'll share more info and thoughts about what exactly happened.</p>
<p>In most of 2016, I spent almost every weekend camping in remote areas without any signal. If you're curious how I felt when making the decision to unpublish; it was a choice made during self-reflection in nature.</p>
<p>Not driven by logic, anger, or greed.<br>
It was a decision guided by my heart.</p>
<p>And it came from a simple principle: if NPM breaks its own rules to remove one of my packages, they should remove all of them.</p>
<p>Not that I'm an inflexible "rules" person — quite the opposite. The spirit behind the rules matter more than the rules themselves to me. In a different context, we could be asking NPM to take down a package without owner's permission for a "good" reason. However, in this situation, a company like <a href="https://en.wikipedia.org/wiki/Kik_Messenger">Kik Messenger</a> was just posturing and exercising power over the open source community NPM was built on, sending threats such as "we'll bang on your door" and "take down your accounts". This is the context in which NPM went against the rules they wrote themselves, to serve something they deemed "higher" than the soul of their company and the community.</p>
<p>I was not afraid of Kik's threats, <br>
but NPM was afraid of losing Kik.</p>
<p>Many people oversimplify this event, framing it as "an angry man protested corporate interests". This narrative shows us three things; first, they didn't look at the dates of the emails. They don't understand the timeline. Second, they can't relate to standing your ground in a high pressure situation involving threats. And third, they haven't read Al-Ghazali yet, don't quite understand how (free) people make decisions.</p>
<p>There was nothing sudden or unexpected for NPM. I asked NPM to remove my modules, waited their response. I didn't set any deadline; NPM had the opportunity to adjust their APIs and tooling to make this transition smooth. Interestingly, they chose to provide me a script that removes all my packages at once. On the NPM side, I observed general condescending attitude towards developers, which led them make series of unreasonable decisions and ultimately blame me for all the cost.</p>
<p>Most of my open source work followed Unix philosophy, so the packages did one thing at a time. There was 350+ of them. In the surface, it looked like nobody used them. NPM didn't show usage stats, and there was almost no activity on Github. As a user, it was impossible to know the impact of unpublishing packages; but I still don't understand why NPM didn't take the time to find out if any of my modules were widely used and consider ways to handle the unpublishing without breaking anything.</p>
<h2>8 Years Later</h2>
<p>Several months after the left-pad incident, I quit my job and left US permanently, spent a year in Morocco, Jordan, Türkiye and Indonesia. I walked trails like Lycian Way, found new camping spots nobody knows about.</p>
<p>Left-pad was like a "death" and "re-birth" moment for me. The part of me passionate about open-source was dead, and something new took over. Now, I'm passionate about business, marketing, running companies / teams in different ways, as much as I'm about programming.</p>
<p>Life goes on.</p>
<p>Thanks for reading.</p>
<p><img src="https://live.staticflickr.com/65535/51992852851_0ba272a470_b.jpg" alt=""></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[It's the end of observability as we know it (and I feel fine) (242 pts)]]></title>
            <link>https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</link>
            <guid>44243050</guid>
            <pubDate>Wed, 11 Jun 2025 00:34:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine">https://www.honeycomb.io/blog/its-the-end-of-observability-as-we-know-it-and-i-feel-fine</a>, See on <a href="https://news.ycombinator.com/item?id=44243050">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
			<p>In a really broad sense, the history of observability tools over the past couple of decades have been about a pretty simple concept: how do we make terabytes of heterogeneous telemetry data comprehensible to human beings? New Relic did this for the Rails revolution, Datadog did it for the rise of AWS, and Honeycomb led the way for <a href="https://opentelemetry.io/" target="_blank" rel="noreferrer noopener">OpenTelemetry</a>.</p>
<p>The loop has been the same in each case. New abstractions and techniques for software development and deployment gain traction, those abstractions make software more accessible by hiding complexity, and that complexity requires new ways to monitor and measure what’s happening. We build tools like dashboards, adaptive alerting, and dynamic sampling. All of these help us compress the sheer amount of <em>stuff</em> happening into something that’s comprehensible to our human intelligence.</p>
<p>In AI, I see the death of this paradigm. It’s already real, it’s already here, and it’s going to fundamentally change the way we approach systems design and operation in the future.</p>
<div>
<hr>
<div>
<p>New to Honeycomb? Get your <strong>free</strong> account today.</p>

</div>

<hr>
</div>
<h2 id="useful">LLMs are just universal function approximators, but it turns out that those are really useful</h2>
<p>I’m going to tell you&nbsp;a story. It’s about this picture:</p>
<figure><img decoding="async" width="1024" height="808" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png" alt="LLMs are just universal function approximators, but it turns out that those are really useful" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1024x808.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-768x606.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-640x505.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-348x275.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image.png 1460w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>If you’ve ever seen a Honeycomb demo, you’ve probably seen this image. We love it, because it’s not only a great way to show a real-world problem—it’s something that plays well to our core strengths of enabling investigatory loops. Those little peaks you see in the heatmap represent slow requests in a <a href="https://www.honeycomb.io/frontend-observability">frontend</a> service that rise over time before suddenly resetting. They represent a small percentage of your users experiencing poor performance—and we all know what this means in the real world: lost sales, poor experience, and general malaise at the continued enshittification of software.</p>
<p>In a Honeycomb demo, we show you how easy it is to use our UI to understand what those spikes actually mean. You draw a box around them, and we run <a href="https://www.honeycomb.io/bubbleup">BubbleUp</a> to detect anomalies by analyzing the trace data that’s backing this visualization, showing you what’s similar and what’s different between the spikes and the baseline. Eventually, you can drill down to the specific service and even method call that’s causing the problem. It’s a great demo, and it really shows the power of our platform.</p>
<p>Last Friday, I showed a demo at our weekly internal Demo Day. It started with what I just showed you, and then I ran a single prompt through an AI agent that read as follows:</p>
<p><strong>Please investigate the odd latency spikes in the frontend service that happen every four hours or so, and tell me why they’re happening.</strong></p>
<figure><img decoding="async" width="1024" height="366" src="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png" alt="" srcset="https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1024x366.png 1024w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-768x275.png 768w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-1536x549.png 1536w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-640x229.png 640w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1-348x124.png 348w, https://www.honeycomb.io/wp-content/uploads/2025/06/image-1.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>
<p>The screenshot here elides the remainder of the response from the LLM (please find the entire text at the end of this post), but there’s a few things I want to call out. First, this wasn’t anything too special. The agent was something I wrote myself in a couple of days; it’s just an LLM calling tools in a loop. The model itself is off-the-shelf <a href="https://www.anthropic.com/claude/sonnet" target="_blank" rel="noreferrer noopener">Claude Sonnet 4</a>. The integration with Honeycomb is our new <a href="https://www.honeycomb.io/blog/mcp-easy-as-1-2-3">Model Context Protocol (MCP)</a> server. It took 80 seconds, made eight tool calls, and not only did it tell me why those spikes happened, it figured it out in a pretty similar manner to how we’d tell you to do it with BubbleUp.</p>
<p>This isn’t a contrived example. I basically asked the agent the same question we’d ask you in a demo, and the agent figured it out with no additional prompts, training, or guidance. It effectively zero-shot a real-world scenario.</p>
<p>And it did it for <strong>sixty cents.</strong></p>
<h2 id="youcandoit">If I can do this, so can you. So can anyone.</h2>
<p>I want to be clear, this was perhaps the <em>least </em>optimized version of this workflow. Inference costs are only going down, and we can certainly make our MCP server more efficient. There are ways to reduce the amount of input tokens even more. We can play around with more tailored aggregations and function calls that return LLM-optimized query results. It’s an exciting new era!</p>
<p>It also should serve as a wakeup call to the entire industry. This is a seismic shift in how we should conceptualize observability tooling. <strong>If your product’s value proposition is nice graphs and easy instrumentation, you are </strong><strong><em>le cooked</em></strong><strong>. </strong>An LLM commoditizes the analysis piece, OpenTelemetry commoditizes the instrumentation piece. <strong>The moats are emptying.</strong></p>
<p>I’m not gonna sit here and say this destroys the idea of <a href="https://www.honeycomb.io/blog/ai-where-in-the-loop-should-humans-go">humans being involved in the process</a>, though. I don’t think that’s true. The rise of the cloud didn’t destroy the idea of IT. The existence of Rails doesn’t mean we don’t need server programmers. Productivity increases <em>expand the map</em>. There’ll be more software, of all shapes and sizes. We’re going to need <em>more</em> of everything.</p>
<p>The question, then, is: what does this require from us? Where does observability sit in a world where code is cheap, refactors are cheap, and analysis is a constant factor?</p>
<h2 id="feedback">Fast feedback is the only feedback</h2>
<p>I’m gonna put a marker out there: the only thing that really matters is fast, tight feedback loops at every stage of development and operations. AI thrives on speed—it’ll outrun you every time. To succeed, you need tools that move at the speed of AI as well. The slower your analysis engine, the worse your results. LLMs will generate, test, and discard hypotheses faster than you ever can. They might be wrong a dozen times before they get it right, but again, <em>we’re talking about fractional pennies here</em>.&nbsp;</p>
<p>I don’t see this as a reason to despair, but as something to celebrate. The things that AI-assisted development and operations can lead us to are the things <em>we’ve always been about</em>. Honeycomb’s entire modus operandi is predicated on fast feedback loops, collaborative knowledge sharing, and treating everything as an experiment. We’re built for the future that’s here today, on a platform that allows us to be the best tool for tomorrow.</p>
<p>The exact contours of what this industry looks like in two, five, or ten years are out of my view. I don’t think there’s anyone that can credibly tell you what things will look like. I’m also not really in the business of making predictions. What I can say, though, is that I think we’re going to see a spectrum of AI assistance in development and operational tasks across the entire <a href="https://www.honeycomb.io/blog/honeycomb-fit-software-development-lifecycle">software development lifecycle</a>:</p>
<ul>
<li>AI agents can run passively as you write and deploy code, suggesting fixes to improve code quality based on real-world observations from production.</li>
<li>Emergent system behavior can be detected and addressed by AI, with unattended agents identifying impacts to users, investigating them, compiling reports, and presenting those to human operators for approval and tweaks before making changes.</li>
<li>Bleeding-edge organizations can adopt entirely unattended SWE/SRE roles, powered by tools like Honeycomb and friends, optimizing and writing software to meet business needs and goals.</li>
</ul>
<p>In all of these cases, what matters most is <em>speed</em>. Traditional monitoring and observability tools don’t have what it takes to compete in this world. Pretty dashboards, pre-built alerts, and magic integrations aren’t sufficient for the future we’re building today. Sub-second query performance, unified data storage, and collaborative workflows between humans and AI are the keys to success in the future of observability.</p>
<p>Let’s go do something cool.</p>
<p><em>Addendum:</em> Here’s the full output from the investigation screenshot earlier.</p>
<p><strong>Perfect! Now I have a clear picture of what’s happening. Based on my investigation, I can explain the latency spikes you’re seeing in your frontend service.</strong></p>
<hr>
<p><strong>🔍 Investigation Results: Frontend Latency Spikes</strong></p>
<hr>
<p><strong>Root Cause Identified: Checkout Service Performance Issues</strong></p>
<p>The latency spikes you’re experiencing every ~4 hours are <strong>NOT actually periodic</strong>—this appears to be a <strong>load testing or synthetic traffic pattern</strong> rather than a scheduled system issue. Here’s what I found:</p>
<hr>
<p><strong>📊 Key Findings:</strong></p>
<ol>
<li><strong>Primary Culprit: Checkout Operations</strong><strong><br></strong>
<ul>
<li>/cart/checkout endpoint: <strong>P95 = 3.7 seconds</strong> (vs normal ~500ms)</li>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li><strong>Maximum latency observed: 7.6 seconds</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Spike Pattern Analysis:</strong><strong><br></strong>
<ul>
<li>Spikes are concentrated in <strong>checkout operations only</strong><strong><br></strong></li>
<li>Other endpoints remain relatively stable</li>
<li><strong>485 slow checkout operations</strong> in the last 24 hours</li>
<li>Pattern suggests <strong>load testing or batch checkout processing</strong><strong><br></strong></li>
</ul>
</li>
<li><strong>Downstream Service Impact:</strong><strong><br></strong>
<ul>
<li>CheckoutService/PlaceOrder: <strong>P95 = 4.6 seconds</strong><strong><br></strong></li>
<li>getDiscounts operation: <strong>P95 = 4.2 seconds</strong><strong><br></strong></li>
<li>ShippingService/ShipOrder: <strong>P95 = 4.0 seconds</strong><strong><br></strong></li>
</ul>
</li>
</ol>
<hr>
<p><strong>🕵️ Evidence from Trace Analysis:</strong></p>
<p>Looking at the sample traces, I can see:</p>
<ul>
<li><strong>User Agent Pattern:</strong> python-requests/2.28.2 and python-requests/2.31.0</li>
<li><strong>Consistent Client IPs:</strong> 52.45.250.5 and internal IPs</li>
<li><strong>High Cart Values:</strong> $6,861, $7,036, $259 (suggesting test scenarios)</li>
<li><strong>Same User IDs:</strong> Multiple requests from user 20109 with identical cart totals</li>
</ul>
<hr>
<p><strong>💡 Why This is Happening:</strong></p>
<p><strong>Most Likely Cause: Load Testing or Synthetic Monitoring</strong></p>
<ul>
<li>The pattern suggests <strong>automated testing</strong> rather than real user traffic</li>
<li>Python requests library indicates <strong>scripted traffic</strong><strong><br></strong></li>
<li>High cart values and repeated scenarios point to <strong>test data</strong><strong><br></strong></li>
<li>The “every 4 hours” pattern is likely your <strong>load testing schedule</strong><strong><br></strong></li>
</ul>
<p><strong>Technical Root Cause:</strong></p>
<ul>
<li><strong>Checkout service bottleneck</strong> during high-volume operations</li>
<li><strong>Discount calculation delays</strong> (4.2s P95)</li>
<li><strong>Shipping service delays</strong> during order processing</li>
<li>Possible <strong>database contention</strong> or <strong>external API timeouts</strong><strong><br></strong></li>
</ul>
<hr>
<p><strong>🎯 Recommendations:</strong></p>
<ol>
<li><strong>Immediate Actions:</strong><strong><br></strong>
<ul>
<li><strong>Verify if this is load testing</strong>—check with your testing team</li>
<li><strong>Monitor checkout service resources</strong> during spike periods</li>
<li><strong>Check discount service performance</strong> and external dependencies</li>
</ul>
</li>
<li><strong>Performance Optimization:</strong><strong><br></strong>
<ul>
<li><strong>Optimize discount calculation logic</strong><strong><br></strong></li>
<li><strong>Add caching</strong> for shipping calculations</li>
<li><strong>Implement circuit breakers</strong> for external payment/shipping APIs</li>
<li><strong>Scale checkout service</strong> if this represents real user load</li>
</ul>
</li>
<li><strong>Monitoring Improvements:</strong><strong><br></strong>
<ul>
<li><strong>Separate alerts</strong> for synthetic vs. real user traffic</li>
<li><strong>Add SLI tracking</strong> specifically for checkout operations</li>
<li><strong>Monitor downstream dependencies</strong> (payment, shipping, discount services)</li>
</ul>
</li>
</ol>
<hr>
<p><strong>🚨 Action Items:</strong></p>
<ol>
<li>Confirm if the 4-hour pattern matches your load testing schedule</li>
<li>If it’s real traffic, investigate checkout service capacity and dependencies</li>
<li>Consider implementing checkout-specific performance monitoring and alerting</li>
</ol>
<hr>
<p><strong>The good news is that your other services are performing well—this appears to be an isolated issue with the checkout flow, likely exacerbated by load testing scenarios.</strong></p>
<hr>
		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Gentle Singularity (217 pts)]]></title>
            <link>https://blog.samaltman.com/the-gentle-singularity</link>
            <guid>44241549</guid>
            <pubDate>Tue, 10 Jun 2025 21:17:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.samaltman.com/the-gentle-singularity">https://blog.samaltman.com/the-gentle-singularity</a>, See on <a href="https://news.ycombinator.com/item?id=44241549">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post_body_2202875"><p>We are past the event horizon; the takeoff has started. Humanity is close to building digital superintelligence, and at least so far it’s much less weird than it seems like it should be.</p><p>Robots are not yet walking the streets, nor are most of us talking to AI all day. People still die of disease, we still can’t easily go to space, and there is a lot about the universe we don’t understand.</p><p>And yet, we have recently built systems that are smarter than people in many ways, and are able to significantly amplify the output of people using them. The least-likely part of the work is behind us; the scientific insights that got us to systems like GPT-4 and o3 were hard-won, but will take us very far.</p><p>AI will contribute to the world in many ways, but the gains to quality of life from AI driving faster scientific progress and increased productivity will be enormous; the future can be vastly better than the present. Scientific progress is the biggest driver of overall progress; it’s hugely exciting to think about how much more we could have.</p><p>In some big sense, ChatGPT is already more powerful than any human who has ever lived. Hundreds of millions of people rely on it every day and for increasingly important tasks; a small new capability can create a hugely positive impact; a small misalignment multiplied by hundreds of millions of people can cause a great deal of negative impact.</p><p>2025 has seen the arrival of agents that can do real cognitive work; writing computer code will never be the same. 2026 will likely see the arrival of systems that can figure out novel insights. 2027 may see the arrival of robots that can do tasks in the real world.</p><p>A lot more people will be able to create software, and art. But the world wants a lot more of both, and experts will probably still be much better than novices, as long as they embrace the new tools. Generally speaking, the ability for one person to get much more done in 2030 than they could in 2020 will be a striking change, and one many people will figure out how to benefit from.</p><p>In the most important ways, the 2030s may not be wildly different. People will still love their families, express their creativity, play games, and swim in lakes.<br></p><p>But in still-very-important-ways, the 2030s are likely going to be wildly different from any time that has come before. We do not know how far beyond human-level intelligence we can go, but we are about to find out.</p><p>In the 2030s, intelligence and energy—ideas, and the ability to make ideas happen—are going to become wildly abundant. These two have been the fundamental limiters on human progress for a long time; with abundant intelligence and energy (and good governance), we can theoretically have anything else.</p><p>Already we live with incredible digital intelligence, and after some initial shock, most of us are pretty used to it. Very quickly we go from being amazed that AI can generate a beautifully-written paragraph to wondering when it can generate a beautifully-written novel; or from being amazed that it can make live-saving medical diagnoses to wondering when it can develop the cures; or from being amazed it can create a small computer program to wondering when it can create an entire new company. This is how the singularity goes: wonders become routine, and then table stakes.</p><p>We already hear from scientists that they are two or three times more productive than they were before AI. Advanced AI is interesting for many reasons, but perhaps nothing is quite as significant as the fact that we can use it to do faster AI research. We may be able to discover new computing substrates, better algorithms, and who knows what else. If we can do a decade’s worth of research in a year, or a month, then the rate of progress will obviously be quite different.</p><p>From here on, the tools we have already built will help us find further scientific insights and aid us in creating better AI systems. Of course this isn’t the same thing as an AI system completely autonomously updating its own code, but nevertheless this is a larval version of recursive self-improvement.</p><p>There are other self-reinforcing loops at play. The economic value creation has started a flywheel of compounding infrastructure buildout to run these increasingly-powerful AI systems. And robots that can build other robots (and in some sense, datacenters that can build other datacenters) aren’t that far off.&nbsp;</p><p>If we have to make the first million humanoid robots the old-fashioned way, but then they can operate the entire supply chain—digging and refining minerals, driving trucks, running factories, etc.—to build more robots, which can build more chip fabrication facilities, data centers, etc, then the rate of progress will obviously be quite different.</p><p>As datacenter production gets automated, the cost of intelligence should eventually converge to near the cost of electricity. (People are often curious about how much energy a ChatGPT query uses; the average query uses about 0.34 watt-hours, about what an oven would use in a little over one second, or a high-efficiency lightbulb would use in a couple of minutes. It also uses about 0.000085 gallons of water; roughly one fifteenth of a teaspoon.)</p><p>The rate of technological progress will keep accelerating, and it will continue to be the case that people are capable of adapting to almost anything. There will be very hard parts like whole classes of jobs going away, but on the other hand the world will be getting so much richer so quickly that we’ll be able to seriously entertain new policy ideas we never could before. We probably won’t adopt a new social contract all at once, but when we look back in a few decades, the gradual changes will have amounted to something big.</p><p>If history is any guide, we will figure out new things to do and new things to want, and assimilate new tools quickly (job change after the industrial revolution is a good recent example). Expectations will go up, but capabilities will go up equally quickly, and we’ll all get better stuff. We will build ever-more-wonderful things for each other. People have a long-term important and curious advantage over AI: we are hard-wired to care about other people and what they think and do, and we don’t care very much about machines.</p><p>A subsistence farmer from a thousand years ago would look at what many of us do and say we have fake jobs, and think that we are just playing games to entertain ourselves since we have plenty of food and unimaginable luxuries. I hope we will look at the jobs a thousand years in the future and think they are very fake jobs, and I have no doubt they will feel incredibly important and satisfying to the people doing them.</p><p>The rate of new wonders being achieved will be immense. It’s hard to even imagine today what we will have discovered by 2035; maybe we will go from solving high-energy physics one year to beginning space colonization the next year; or from a major materials science breakthrough one year to true high-bandwidth brain-computer interfaces the next year. Many people will choose to live their lives in much the same way, but at least some people will probably decide to “plug in”.</p><p>Looking forward, this sounds hard to wrap our heads around. But probably living through it will feel impressive but manageable. From a relativistic perspective, the singularity happens bit by bit, and the merge happens slowly. We are climbing the long arc of exponential technological progress; it always looks vertical looking forward and flat going backwards, but it’s one smooth curve. (Think back to 2020, and what it would have sounded like to have something close to AGI by 2025, versus what the last 5 years have actually been like.)</p><p>There are serious challenges to confront along with the huge upsides. We do need to solve the safety issues, technically and societally, but then it’s critically important to widely distribute access to superintelligence given the economic implications. The best path forward might be something like:</p><ol>
<li><p>Solve the alignment problem, meaning that we can robustly guarantee that we get AI systems to learn and act towards what we collectively really want over the long-term (social media feeds are an example of misaligned AI; the algorithms that power those are incredible at getting you to keep scrolling and clearly understand your short-term preferences, but they do so by exploiting something in your brain that overrides your long-term preference).</p></li>
<li><p>Then focus on making superintelligence cheap, widely available, and not too concentrated with any person, company, or country. Society is resilient, creative, and adapts quickly. If we can harness the collective will and wisdom of people, then although we’ll make plenty of mistakes and some things will go really wrong, we will learn and adapt quickly and be able to use this technology to get maximum upside and minimal downside. Giving users a lot of freedom, within broad bounds society has to decide on, seems very important. The sooner the world can start a conversation about what these broad bounds are and how we define collective alignment, the better.</p></li>
</ol><p>We (the whole industry, not just OpenAI) are building a brain for the world. It will be extremely personalized and easy for everyone to use; we will be limited by good ideas. For a long time, technical people in the startup industry have made fun of “the idea guys”; people who had an idea and were looking for a team to build it. It now looks to me like they are about to have their day in the sun.</p><p>OpenAI is a lot of things now, but before anything else, we are a superintelligence research company. We have a lot of work in front of us, but most of the path in front of us is now lit, and the dark areas are receding fast. We feel extraordinarily grateful to get to do what we do.</p><p>Intelligence too cheap to meter is well within grasp. This may sound crazy to say, but if we told you back in 2020 we were going to be where we are today, it probably sounded more crazy than our current predictions about 2030.</p><p>May we scale smoothly, exponentially and uneventfully through superintelligence.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Chatbots are replacing Google's search, devastating traffic for some publishers (181 pts)]]></title>
            <link>https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</link>
            <guid>44241407</guid>
            <pubDate>Tue, 10 Jun 2025 21:03:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141">https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141</a>, See on <a href="https://news.ycombinator.com/item?id=44241407">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: I made a 3D printed VTOL drone (336 pts)]]></title>
            <link>https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</link>
            <guid>44241278</guid>
            <pubDate>Tue, 10 Jun 2025 20:47:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can">https://www.tsungxu.com/p/i-made-a-3d-printed-vtol-that-can</a>, See on <a href="https://news.ycombinator.com/item?id=44241278">Hacker News</a></p>
<div id="readability-page-1" class="page"><div dir="auto"><div id="youtube2-e7AIKGDrlgs" data-attrs="{&quot;videoId&quot;:&quot;e7AIKGDrlgs&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}" data-component-name="Youtube2ToDOM"><p><iframe src="https://www.youtube-nocookie.com/embed/e7AIKGDrlgs?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409"></iframe></p></div><p><br><span>I made this 130 mile capable VTOL drone in only 90 days. It can fly for 3 hours on a single charge. That would make it one of the longest range and endurance 3D printed VTOLs in the world.</span></p><p>This is the thing I'm most proud of building to date!</p><p><span>Before this project, I was a total CAD, 3D printing and aerodynamic modeling beginner. I had only </span><a href="https://www.tsungxu.com/p/first-vtol-design-build-and-flight" rel="">built and flown one VTOL before</a><span>. My CAD skills were restricted to extrusions of basic sketches. My Bambu A1 3D printer was only one month old, and I had never tried printing foaming PLA. I had never used any aerodynamic modeling tools to simulate lift-to-drag ratios or stability curves like Cm-alpha.</span></p><p>This video edit ended up shorter than I planned. Being my first Youtube video with significant post production effort, I underestimated the work required to make a longer in-depth video with voiceover, edited footage, etc.</p><p>Because of that, I did not discuss dozens of project challenges like how I chose the design parameters, learnt airframe CAD design, sourced components, improved foaming PLA print quality, in-depth power loss troubleshooting, and many more challenges this project faced.</p><p>If you want more details on my design and build process, let me know!</p><p><em><span>P.S. Reid Hoffman just </span><a href="https://x.com/reidhoffman/status/1932511491506262413" rel="">quote tweeted</a><span> my viral </span><a href="https://x.com/tsungxu/status/1932224925450043845" rel="">video post on X</a><span>:</span></em></p><blockquote><p><em>A century ago, you needed at least a brother and a bicycle shop to pioneer flight. Today, you just need the right toolchain…</em></p></blockquote><p>Incredibly humbling! </p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: A “Course” as an MCP Server (172 pts)]]></title>
            <link>https://mastra.ai/course</link>
            <guid>44241202</guid>
            <pubDate>Tue, 10 Jun 2025 20:36:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mastra.ai/course">https://mastra.ai/course</a>, See on <a href="https://news.ycombinator.com/item?id=44241202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div class="page"><nav><div><p><a href="https://mastra.ai/"><img alt="mastra" loading="lazy" width="24" height="23" decoding="async" data-nimg="1" src="https://mastra.ai/logo.svg"></a></p><ul data-open="false"><li><a href="https://mastra.ai/blog">Blog</a></li><li><a href="https://mastra.ai/university">University</a></li><li><a target="_blank" href="https://mastra.ai/docs">Docs</a></li></ul></div></nav><main><div><div><ul><li>Explained by Shane Thomas CPO of Mastra</li><li>Led by an MCP Agent</li><li>3 Lessons and more soon</li><li>12 Hours to compete</li><li>Interactive progress</li></ul><div><p>In this hands-on course, you'll learn to develop and deploy AI agents using Mastra, including how to equip them with tools, memory, and MCP. But here's the twist: you're not just learning about agents, you're learning from one.</p><p>This course is delivered entirely inside your agentic code editor, where your code agent guides you step-by-step, writes code with you, and helps you build real, working agents from the ground up.</p></div></div><div><div><p><span>Begin Mastra 101</span></p><div><p><span>1</span></p><p>Choose your editor and paste the command to install the mcp server automatically</p></div></div><div><p><span></span>Course Outline<span></span></p><div><div><p><span>Build your first agent</span><span><a href="https://youtu.be/buqexw-Xkvk" target="_blank">Watch lesson <!-- -->1<!-- --> intro</a></span></p><div><p>Build and deploy your first AI agent that can read data from external sources, use custom tools, and remember previous interactions. Set up your development environment, test your agent in a live playground, and ship your agent to production. This section covers the basics needed to create a working, memory and tool-enabled agent.</p><div><p>introduction to mastra</p><p>what is mastra</p><p>verifying installation</p></div></div></div><div><p><span>Adding Tools and MCP</span><span><a href="https://youtu.be/J5WVCPKTRxA" target="_blank">Watch lesson <!-- -->2<!-- --> intro</a></span></p><div><p>Add new abilities to your agent by connecting it to external services using MCP servers. Search across MCP registries to easily integrate tools for email, social media, GitHub, news, and local files, without writing any custom code.</p><div><p>introduction to mcp</p><p>installing mcp</p><p>setting up mcp configuration</p></div></div></div><div><p><span>Adding Memory</span><span><a href="https://youtu.be/Io2xtouOsMk" target="_blank">Watch lesson <!-- -->3<!-- --> intro</a></span></p><div><p>Add memory to your agent so it can remember previous conversations, user preferences, and important details. Learn how to configure conversation history, semantic recall, and working memory to create agents that provide more relevant and personalized responses.</p><div><p>understanding memory</p><p>why memory matters</p><p>installing memory</p></div></div></div></div><div><h3 id="faqs"><a href="#faqs">#</a>Faqs</h3></div></div></div></div></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI o3-pro (168 pts)]]></title>
            <link>https://help.openai.com/en/articles/9624314-model-release-notes</link>
            <guid>44240999</guid>
            <pubDate>Tue, 10 Jun 2025 20:15:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://help.openai.com/en/articles/9624314-model-release-notes">https://help.openai.com/en/articles/9624314-model-release-notes</a>, See on <a href="https://news.ycombinator.com/item?id=44240999">Hacker News</a></p>
Couldn't get https://help.openai.com/en/articles/9624314-model-release-notes: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Another Crack in the Chain of Trust: Uncovering (Yet Another) Secure Boot Bypass (116 pts)]]></title>
            <link>https://www.binarly.io/blog/another-crack-in-the-chain-of-trust</link>
            <guid>44240770</guid>
            <pubDate>Tue, 10 Jun 2025 19:55:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.binarly.io/blog/another-crack-in-the-chain-of-trust">https://www.binarly.io/blog/another-crack-in-the-chain-of-trust</a>, See on <a href="https://news.ycombinator.com/item?id=44240770">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h6><em>By Binarly REsearch</em></h6><p>In this blog post, the Binarly Research team documents a Secure Boot bypass that likely impacts most devices supporting UEFI. At the center of this discovery is <a href="https://www.binarly.io/advisories/brly-dva-2025-001">CVE-2025-3052 (BRLY-2025-001)</a>, a memory corruption vulnerability in a module signed with Microsoft’s third-party UEFI certificate. Attackers can exploit this vulnerability to run unsigned code during the boot process, effectively bypassing Secure Boot and compromising the system’s chain of trust. Because the attacker’s code executes before the operating system even loads, it opens the door for attackers to install bootkits and undermine OS-level security defenses. </p><p>The story behind this discovery begins with a UEFI application we found on VirusTotal, signed with Microsoft’s third-party UEFI certificate. This application has been publicly available since November 2024 when it was first submitted. However, the signature data indicates it was actually signed in October 2022, suggesting it may have been circulating for much longer. </p><p>The application is a BIOS flashing tool for devices sold by DT Research, a company specializing in rugged mobile devices. Although this module was originally developed for DT Research devices only, it can run on any device that trusts the certificate with subject "<strong>Microsoft Corporation UEFI CA 2011</strong>" certificate. This includes the vast majority of systems, as the same certificate is widely used to sign the Linux bootloader shim, significantly increasing the potential impact of this vulnerability.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/6843694d5c893ab5cff2eefb_image1.png" loading="lazy" alt=""></p><figcaption><em>Figure 1. The vulnerable module signed with the Microsoft UEFI CA 2011 certificate</em></figcaption></figure><p>The root cause of this bug, once again, lies in the unsafe handling of NVRAM variables. Issues involving NVRAM variables have been a persistent problem in the UEFI ecosystem, <a href="https://www.binarly.io/advisories">with Binarly alone responsibly disclosing hundreds of related vulnerabilities</a> over the past few years. Because these bugs have been so widespread and impactful, our Binarly Transparency Platform performs multiple analyses to detect these bug classes. One of these checks looks specifically for any unsafe usage of data read from NVRAM variables, and this analysis is what automatically discovered CVE-2025-3052. </p><p>As shown in the image below, the signed application reads the content of the <em>IhisiParamBuffer</em> variable and directly uses it as a pointer for multiple memory write operations, without performing any validation or sanity checks on its value. This allows an attacker to set the <em>IhisiParamBuffer</em> variable to any arbitrary address in memory, effectively granting them an arbitrary memory write primitive.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436b2c089cba16f0d50471_image2.png" loading="lazy" alt=""></p><figcaption><em>Figure 2. The signed module blindly trusting an NVRAM variable for memory writes &nbsp;(CVE-2025-3052)</em></figcaption></figure><p>We reported this finding to CERT/CC on February 26, 2025. During our disclosure with Microsoft, it became clear that this issue wasn’t limited to a single module, but it actually affected 14 different modules. For this reason, to address CVE-2025-3052, Microsoft added 14 new hashes to the Secure Boot dbx as a mitigation measure, which was delivered as part of Patch Tuesday on June 10, 2025.</p><p><strong>Secure Boot and Microsoft Certificates</strong></p><p>Before delving into the details of CVE-2025-3052, let’s recap what Secure Boot is and what security problems it solves. First of all, Secure Boot plays a crucial role in the chain of trust built by UEFI. It can be thought of as a bridge connecting the system’s firmware to the operating system, carrying trust from one bank to the other. At the end of the DXE phase, with the platform initialized, the firmware is ready to boot the OS. With Secure Boot, the firmware cryptographically verifies the integrity and authenticity of the OS loader before handing over control. This check stops attackers from replacing the legitimate OS loader with a malicious component, such as a bootkit. Without Secure Boot, it is trivial for attackers to gain early execution, compromising OS defenses before the OS even has a chance to install them.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436b65a6fcb3dcd3042f62_image3.png" loading="lazy" alt=""></p><figcaption><em>Figure 3. Summary of the UEFI Secure Boot signing and verification process</em></figcaption></figure><p>UEFI applications are signed with the private part of a signing certificate (leaf certificate on the image) that chains through one or more intermediate certificates to a trusted, root CA certificate from <em>db</em>. This is for example what happens with applications signed by Microsoft, where the private key of the certificate with subject “Microsoft Corporation UEFI CA 2011” signs the signing/leaf certificate, whose private key is in turn used to sign the Authentocode hash of the UEFI Application.</p><p>The verification process is instead based on two signature databases: <em>db</em> and <em>dbx</em>. The <em>db</em> contains a list of trusted Authenticode hashes and root certificates:</p><ul role="list"><li>if any Authenticode hash in the <em>db</em> matches the application’s Authenticode hash, the application is allowed to run</li><li>if any root certificate in the <em>db</em> successfully verifies the application’s certificate chain, the application is allowed to run</li></ul><p>The dbx instead contains Authenticode hashes and certificates that are untrusted or revoked.</p><p>Depending on whether the application is signed, the firmware allows execution based on either its Authenticode hash or CA certificate being present in the <em>db</em> and not present in the <em>dbx</em>. By default, most devices are shipped with at least these trusted certificates present in <em>db</em>:</p><ol role="list"><li><em>Microsoft Windows Production PCA 2011</em> — used to sign the Windows bootloader.</li><li><em>Microsoft Corporation UEFI CA 2011</em> — used to validate third-party bootloaders and components.</li><li><em>OEM-owned certificate(s)</em> — specific to the hardware manufacturer, typically used for hardware-related or servicing modules.</li></ol><p>The second certificate is the one used to sign the vulnerable application. As previously mentioned, this certificate is widely trusted and present in <em>db</em>, since it’s used to sign components like the Linux’s shim, the EFI application responsible for booting Linux systems.</p><p><strong>The Beginning: Module Discovery and Reconnaissance</strong></p><p>This REsearch began after the discovery of the vulnerable <a href="https://www.virustotal.com/gui/file/a11996aaf7f32aa09f78db608106e25a47970bfeeec76d49af743eee71b55f06/">UEFI module on VirusTotal</a>. The first submission on VT was done in November 2024 but the signing time dates back to October 2022, meaning that this module may have been around for a longer time. The original submission filename was Dtbios-efi64-71.22.efi.The filename, combined with details from the Authenticode certificate and strings found within the binary itself, led us to conclude that the module has been developed by “DT Research, Inc”. </p><p>Additionally, the name of the vulnerable NVRAM variable immediately pointed to Insyde as the Independent BIOS Vendor (IBV) involved. This same variable had already been at the center of two vulnerabilities we previously reported: <a href="https://www.binarly.io/advisories/brly-2022-023">BRLY-2022-023</a> and <a href="https://www.binarly.io/advisories/brly-2023-005">BRLY-2023-005</a>.</p><p>After some reversing, we also conclude that the module is a BIOS update utility, as it reads a BIOS image from a file and attempts to flash it onto the device’s ROM.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436bb53f9e1ed4d4bdc755_image7.png" loading="lazy" alt=""></p><figcaption><em>Figure 4. A screenshot from VirusTotal of the vulnerable module</em></figcaption></figure><p><strong>The Intermezzo: Finding and Exploiting CVE-2025-3052</strong></p><p>Our Binarly Transparency Platform can automatically detect a wide range of UEFI-related vulnerabilities, including issues involving NVRAM variable handling. As a result, <strong>identifying the vulnerability behind CVE-2025-3052 was as simple as uploading the affected module to our platform</strong>. </p><p>The generated report (shown in the image below) immediately revealed the impact of this finding: the root cause behind the vulnerability is the use of untrusted pointers originating from an NVRAM variable. Additionally, our <a href="https://www.binarly.io/news/binarly-patents-new-method-for-reachability-analysis-for-binary-executables">patented reachability metric</a> confirmed that the vulnerable function is directly accessible from the module's entry point, thus likely exploitable.</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436baad14dbc25aaa3c1f8_image9.png" loading="lazy" alt=""></p><figcaption><em>Figure 5. The report generated by the Binarly Transparency Platform.</em></figcaption></figure><p>The pseudocode shown in the next image, generated by our platform as evidence of the issue, enabled quick and effective triaging of the vulnerability. This pseudocode is generated using Binarly’s program analysis framework delivering comprehensive static analysis techniques. </p><p>The vulnerable function reads the value of the <em>IhisiParamBuffer</em> NVRAM variable and stores it in a global variable located at address <em>0xf7a0</em>. Later, the value of this global variable plus 0x18 is used as the target address for a memory write operation, setting it to zero. Subsequent writes are also attacker-controlled, since <em>var2</em> points to the same NVRAM variable (our platform generates independent findings for these additional write operations as well).</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/68436cdccdac3447a48504c5_image5.webp" loading="lazy" alt=""></p><figcaption><em>Figure 6. The pseudo-code generated as evidence of the finding.</em></figcaption></figure><p>While this write capability is constrained, as it only allows writing zeros or a few constants to an arbitrary address, it provides a powerful primitive that can be exploited in a variety of creative ways. For our proof of concept (PoC), we chose to overwrite the global variable <a href="https://github.com/tianocore/edk2/blob/62cd7d338b389aa21c00f0fc35f14a6fa9ba23b0/MdeModulePkg/Core/Dxe/DxeMain.h#L250" target="_blank">gSecurity2</a>. This variable holds a pointer to the Security2 Architectural Protocol, which the LoadImage function uses to enforce Secure Boot. By setting it to zero, we <a href="https://github.com/tianocore/edk2/blob/f75e6952ca29c9d496c9d6fadaddb4c71cd0c4ed/MdeModulePkg/Core/Dxe/Image/Image.c#L1272" target="_blank">effectively disable Secure Boot</a>, allowing the execution of any unsigned UEFI modules.</p><p>One prerequisite for this vulnerability is that the IhisiParamBuffer variable must be writable. On Insyde-based devices, this variable is often locked and read-only. Therefore, unless there is another vulnerability (such as <a href="https://www.binarly.io/advisories/brly-2023-005">BRLY-2023-005</a>, which allows setting arbitrary variables even if locked) Insyde-based devices are not vulnerable. </p><p>All other devices, however, remain vulnerable. Thinking about it, this situation is <em>quite unique</em> and it highlights, once again, the complexities surrounding the UEFI supply chain security, where a mistake by one vendor can affect the entire ecosystem, <em>except for the vendor itself!</em></p><p>The following figure provides a high-level overview of an attack leveraging CVE-2025-3052. A privileged attacker can set the <em>IhisiParamBuffer</em> variable from the operating system (Step 1) and register the vulnerable module in the UEFI Boot Manager, or replace an existing OS loader with the malicious module. </p><p>Additionally, the attacker registers a second unsigned module containing the actual payload they intend to execute (Step 2). After the victim's device reboots (Step 3) and the firmware enters the Boot Device Selection (BDS) phase, the vulnerable module is executed. Its arbitrary write capability allows the attacker to overwrite <em>gSecurity2</em>, effectively disabling Secure Boot. With Secure Boot disabled, the second unsigned module is then loaded and executed by the firmware, granting the attacker arbitrary code execution at the end of the DXE phase (Step 4).</p><figure><p><img src="https://cdn.prod.website-files.com/65dc9e6c7079139a2996b5d7/6843692488f7dc517d59d828_image4.png" loading="lazy" alt=""></p><figcaption><em>Figure 7. The high-level overview of a PoC exploiting CVE-2025-3052</em></figcaption></figure><p>Based on the previous description, we recorded a demo video demonstrating the different stages of this PoC. One important aspect is that the PoC is transparent to the operating system: as shown at the end of the video (at minute 1:07), Secure Boot still appears to be enabled from the OS, even though it has been effectively bypassed.</p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/bX6TfPcccXk" title="Proof of Concept for CVE-2025-3052"></iframe></p></figure><p><strong>The Conclusion: Disclosure and Remediation for CVE-2025-3052</strong></p><p>After discovering this vulnerability, we opened a case with CERT/CC on February 26, 2025 to notify the affected parties and help provide a fix and detection for impacted Binarly customers. </p><p>During the triage process, Microsoft determined that the issue did not affect just a single module as initially believed, but actually 14 different modules. For this reason, the updated dbx released during the Patch Tuesday on June 10, 2025 contains 14 new hashes. </p><div><table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Authenticode SHA256 hash</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>BiosFlashShell-efi64-80.02.efi</td>
      <td>C54A4060B3A76FA045B7B60EEAEBC8389780376BA3EF1F63D417BA1B5528E95</td>
    </tr>
    <tr>
      <td>BiosFlashShell-efi64-81.02.efi</td>
      <td>CBFAA286144EB2D165A6B17245BAD4F73058436C7292BE56DC6EBA29A369ADDF</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.17.efi</td>
      <td>9D7E7174C281C6526B44C632BAA8C3320ADDD0C77DC90778CC14893882D74618</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.18.efi</td>
      <td>9B1F35052CFC5FB06DABE5E8F7B747F081DA28D722DB59ADE253B9E38A7A3C76</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.19.efi</td>
      <td>E3CE55E584371D3F2FBCA2241EF0711FF80876EBF71BAB07D8ECEE645A40DCFC</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.20.efi</td>
      <td>EE093913ABBD34CB8B5EA31375179A8B55A298353C03AFE5055AA4E8E3F705EF</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.21.efi</td>
      <td>B4E1880425F7857B741B921D04FD9276130927CF90A427C454B970E7A2F442F9</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-70.22.efi</td>
      <td>CDA0B4A59390B36E1B654850428CBB5B4C7B5E4349E87ACDE97FB543736FF1D4</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.17.efi</td>
      <td>C87EFD057497F90321D62A69B311912B8EF8A045FE9C5E6BD5C8C1A4E6F39629</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.18.efi</td>
      <td>9E19DD645235341A555D6AC0665591453AE13918ECD37DF22DFBEE91EAA9A2DA</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.19.efi</td>
      <td>63F67824FDA998798964FF33B87441857DA92F3A8EE3E04166EEC3156E4E6B82</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.20.efi</td>
      <td>0BC4F078388D41AB039F87AE84CF8D39302CCBDD70C4ADEE02263EBF6A2DD328</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.21.efi</td>
      <td>E2AEC271B9596A461EB6D54DB81785E4E4C615CFDA5F4504BCC0A329248A4D36</td>
    </tr>
    <tr>
      <td>Dtbios-efi64-71.22.efi</td>
      <td>6B4328EBCBE46ED9118FF2D4472DE329D70BA83016DF7A6F50F8AF92342160A1</td>
    </tr>
  </tbody>
</table></div><p>In terms of remediation, we recommend that impacted users update the <em>dbx</em> as soon as possible. In our previous research blog “<a href="https://www.binarly.io/blog/from-trust-to-trouble-the-supply-chain-implications-of-a-broken-dbx">From Trust to Trouble: The Supply Chain Implications of a Broken DBX</a>,” we described in detail the potential impact of outdated and inconsistent <em>dbx.</em></p><figure><p><iframe allowfullscreen="true" frameborder="0" scrolling="no" src="https://www.youtube.com/embed/TnECRMf2CoQ" title="Combining a Secure Boot Bypass with a Bootkit on Windows 11"></iframe></p></figure><p><strong>Detecting CVE-2025-3052</strong></p><p>Secure Boot bypasses continue to be a persistent issue within the UEFI ecosystem, with new vulnerabilities surfacing a few times each year. For example, in January 2025, <a href="https://www.welivesecurity.com/en/eset-research/under-cloak-uefi-secure-boot-introducing-cve-2024-7344/">ESET researchers disclosed</a> the details of another Secure Boot bypass &nbsp;(CVE-2024-7344). In addition, following the <a href="https://www.binarly.io/blog/from-trust-to-trouble-the-supply-chain-implications-of-a-broken-dbx">discrepancies we uncovered in the dbx</a> and their supply-chain implications that we discussed earlier this year, we decided to implement a new feature in our product specifically focused on Secure Boot security. This addition ensures that the Binarly Transparency Platform users are promptly informed with precise findings about any issues affecting Secure Boot, so that they can quickly remediate them.</p><p>Here is a demonstration of the Binarly Transparency Platform detecting and reporting on this Secure Boot bypass issue.</p><p>Binarly Deep Vulnerability Analysis (DVA) technology detects automatically CVE-2025-3052 as a previously unknown vulnerability (0-day):</p><p>Binarly Mitigate technology proactively detects signed vulnerable modules in DB/DBX databases to highlight potential high-impact risks:</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Vassar Robotics (YC X25) – $219 robot arm that learns new skills (486 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44240302</link>
            <guid>44240302</guid>
            <pubDate>Tue, 10 Jun 2025 19:12:17 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44240302">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="44240853"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240853" href="https://news.ycombinator.com/vote?id=44240853&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Firstly, at the $219 price point you can have my money already.</p><p>Beyond that, things that appeal to me are basically anything which increase the likelihood I can accomplish high dexterous fine motor control skills, for things like tinkering and DIY assembly.  I think that would include extra wrist DOF and a longer-reach variant.</p><p>Integrated cameras are an interesting idea, but I'd like to be able to swap them out for my own.</p><p>My dream is to have some sort of multi-arm table at home.  I imagine holding a circuit board, small component, soldering iron, and wire with four robotic arms I control with shaky hands from my laptop. :D</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241222"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44241222" href="https://news.ycombinator.com/vote?id=44241222&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>So true. Every time I solder surface mount components, I always wish I could have a steady hand. Sadly, this arm doesn't have that kind of accuracy. The output shaft of the servos we use has about 1 degree of wiggle room and the mechanical structure adds more.</p><p>To get better accuracy, if sticking with this kind of RC servo, it's basically required to have two servos per joint to preload each other to kill that wiggle room. It's something I've been calculating, but I just can't figure out a way to offer it at a good price.</p><p>Interestingly, for arms that are popular in academia, even when the price goes to $10k (like ARX or Trossen), the wiggle room is still there (better, but still there).</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241397"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44241397" href="https://news.ycombinator.com/vote?id=44241397&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>even something twice the price ($438) would still be a great deal. Mind telling us something about your pricing strategy trade-off consideration matrix?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44241233"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241233" href="https://news.ycombinator.com/vote?id=44241233&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>You need some technical specs on the website. How many DOF does it have? Does it have joint angle sensing? If so, what's the resolution? What's the interface to the servos? What's the payload capacity? Does it have integrated motor controllers? How long is it, and what does the dexterous workspace look like?</p><p>As a roboticist, what I'd vote for, in order, is:</p><p>- more degrees of freedom</p><p>- interchangeable tools, either an actual tool changer (unlikely at the price point) or a fixed bolt pattern with electronic passthroughs</p><p>- better joint sensing, e.g. absolute encoders, joint torque sensing</p><p>- fingertip force sensing</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240932"><td></td></tr>
                <tr id="44241110"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44241110" href="https://news.ycombinator.com/vote?id=44241110&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>$219 is for the unassembled version
$299 is for the assembled version
$199 is for the first 20 units of the assembled version
$599 was the price before I spent hard hours dealing with supply chains</p><p>When I was just a hobbyist, I had to pay the price on the website. Now that I have some funding to order in large quantities, prices come down a lot. I do the dirty work of sourcing the components so hobbyists don't have to ;)</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44241060"><td></td></tr>
            <tr id="44241135"><td></td></tr>
                  <tr id="44241446"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241446" href="https://news.ycombinator.com/vote?id=44241446&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>You should put it on Amazon; we used a robotic arm in one of the classes I taught, and for logistics reasons it was basically the only way we could order stuff. Plus it helps with discovery.</p><p>I'm sure there's an extra fee but it's sometimes just impossible to order things if you're a big organization from small sites like this.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240818"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240818" href="https://news.ycombinator.com/vote?id=44240818&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Of course this arrives right after I order all the electronic parts and just kicked off the 24+ hour 3D print job to complete my SO-Arm101.</p><p>But I’m routing for you!</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241328"><td></td></tr>
                  <tr id="44241505"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241505" href="https://news.ycombinator.com/vote?id=44241505&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>As another robot hobbyist, I wish there were more detailed documentation on how things work. So many projects online just show a working demo—usually on YouTube—and it's impossible to decipher what’s actually happening, or if the robot is simply following some predefined movements.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44241571"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241571" href="https://news.ycombinator.com/vote?id=44241571&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>I'd be interested to hear about your experience working with suppliers. How did you go about finding suppliers and haggling with them?</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44241312"><td></td></tr>
            <tr id="44240475"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240475" href="https://news.ycombinator.com/vote?id=44240475&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Wow! Recently my son has been asking about doing a project with a robotic arm, and this looks <i>amazing</i>, especially at the hobbyist-friendly price point. And adding in AI is really cool - and just the thing to <i>really</i> grab the attention of an eight year old boy :)  Will these be available in the UK, perchance?</p><p>A bit of an aside, but how hard is it to get into building RC aeroplanes, compared to FPV copter drones?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241483"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44241483" href="https://news.ycombinator.com/vote?id=44241483&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Building RC planes is a little harder IMO, but not much.</p><p>The main difference in building planes is you have to pay attention to center-of-gravity much more; minute differences will make the difference between your plane flying amazingly, like a brick (nose heavy), or not at all (tail heavy). There's also more work to do in setting control linkages and surface throws. But, overall, it's not too tough with most models.</p><p>Takeoff with planes can be very stressful the first few times; you have to choose between ground/runway takeoff, which typically results in a very inefficient model due to landing gear drag and is prone to flipping over, throwing the plane by hand, which requires practice and can be quite hazardous with a "pusher" style plane with the prop at the back, and building some kind of bungee launcher, which you then have to set up and lug around.</p><p>Then you have to decide how to fly - line of sight or FPV. Line of sight flying is quite an acquired skill and has a very steep learning curve - you basically have to learn to "become the plane" and understand how your control stick inputs are affecting the attitude of the plane without being able to see it very well.</p><p>FPV plane flying, while less popular than LOS, is very easy and much more rewarding IMO. The reaction time in all but the most extreme plane stunt flying is much less dramatic than in FPV quads.</p><p>And, due to quirks of the general hobby flight control software scene, most hobby FPV planes have a working loiter-in-a-circle setting while most FPV quads have a barely-functional GPS rescue mode and little to no ability to actually hover (it's very rare for an FPV quad to "just stay put"; this is the realm of camera drones).</p><p>I fly FPV quads when I need a focus/adrenalin boost and FPV planes when I just want to relax and chill. You can fly planes in an adrenalin style, but they're much more conducive to just looking at the scenery and goofing around. Massive bonus points that most plane builds are almost silent compared to an FPV quad so you don't worry about bothering people so much.</p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240847"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44240847" href="https://news.ycombinator.com/vote?id=44240847&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>RC aeroplanes need some practice and a bigger field compared to FPV drones. I think I spent a week flying in simulators and another 2 weeks crashing several times to get a basic hold on it. It's kind of like training a robot foundation model to learn a new embodiment</p><p>That being said, I enjoyed every moment flying my planes. I built and flew quite a few quadcopters but they never felt that free because there's always that control algorithm between the pilot and the motors, while aeroplanes are basically just mapping the movement of the joystick to the servos. I believe the UK has a lot of great local clubs, and I believe that's the best place to get started.</p><p>Side note, when your son gets more experience in the field, he might wanna build his own gas turbine to power his planes. And this association based in UK is the best on this planet: <a href="https://www.gtba.co.uk/" rel="nofollow">https://www.gtba.co.uk</a></p><p>For UK delivery, let me look into how to set up international shipping. Will get back to you by end of the day.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241137"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44241137" href="https://news.ycombinator.com/vote?id=44241137&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>I hadn't thought about clubs, probably because I live in a small, rural Scottish town... but I just had a quick look, and incredibly there's an active club just a few miles from me, which I had no idea even existed!</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44240802"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44240802" href="https://news.ycombinator.com/vote?id=44240802&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Planes, like quadcopters, are as complicated or simple as you want them to be. They're available fully ready to fly, as kits with different levels of work needed, or you can build from scratch and choose your own parts and design.</p><p>Flying is pretty different, though. If you're used to a copter that will just stay put when you release the controls, flying planes will be an adjustment.</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44240917"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44240917" href="https://news.ycombinator.com/vote?id=44240917&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Yes yes! Flying an aeroplane has no pause button. You are on your own from taking off to landing. It's a great practice not to panic under stress (I never flew one but I guess racing FPV quadcopters probably has the same feeling)</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44240764"><td></td></tr>
            <tr id="44240729"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240729" href="https://news.ycombinator.com/vote?id=44240729&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Interesting - I was just thinking the other day that a well implemented MCP server driving a robot with access to a camera could be a really interesting project.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44240749"><td></td></tr>
                  <tr id="44241291"><td></td></tr>
                <tr id="44241303"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44241303" href="https://news.ycombinator.com/vote?id=44241303&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>if this can be extended to full size doors this could be a very good business, no more need to modify doors to have doggy doors</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241353"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_44241353" href="https://news.ycombinator.com/vote?id=44241353&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Opening a door is still a challenging problem for robots. Better to have electronic control of doors that can be opened programmatically.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                        <tr id="44241489"><td></td></tr>
            <tr id="44240617"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240617" href="https://news.ycombinator.com/vote?id=44240617&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Funny, I was just about to build an SO-101, but tariffs adding $100 to the price of the servos annoyed me.</p><p>How do I buy your kit, please?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44240661"><td></td></tr>
                  <tr id="44241510"><td></td></tr>
            <tr id="44240814"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240814" href="https://news.ycombinator.com/vote?id=44240814&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>I wonder if I can strap this to my Roborock from 2020 and train it to pick up socks.</p><p>Roborock sells a new model that does this [1] but it costs $3,000 and I refuse to pay that on principle when I know it's likely a straightforward model with some unsupervised training.</p><p>Also I can probably fix it easier once it (definitely) breaks at some point due to  collisions.</p><p>[1] <a href="https://www.youtube.com/shorts/vHVQxXVgBm4" rel="nofollow">https://www.youtube.com/shorts/vHVQxXVgBm4</a></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240723"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240723" href="https://news.ycombinator.com/vote?id=44240723&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>I've been looking for a cheap 7 DOF arm. The only reason I haven't bought SO100/101 yet is that it's 6 DOF (and that delivery to Europe is hard to find..)</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44241186"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241186" href="https://news.ycombinator.com/vote?id=44241186&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>So cool!</p><p>I would easily pay $1000-$1500 if you put two of these on a wheel base and made it all structurally sound. Extra points if the arms sit at least 1-2 feet of the ground and can reach the ground.</p></div></td></tr>
        </tbody></table></td></tr>
                      <tr id="44240904"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240904" href="https://news.ycombinator.com/vote?id=44240904&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>I love the idea of a trainable robot arm as a learning device at that price point.</p><p>However, seeing the chess demo instantly makes me think of that horrible tragedy with the robotic arm breaking a kid's finger. How strong is this to be used around kids?</p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241031"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44241031" href="https://news.ycombinator.com/vote?id=44241031&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div><p>Yeah, safety is an important aspect. The good news is that the servos are not that powerful. Peaking at 3 Nm, with a moment arm of 0.2 m, you get 15 N of force, which is basically equivalent to the weight of three 500mL water bottles. This force might cause some scratches but should not lead to serious injuries.</p><p>Initially, I was planning to launch a product using Piper Arms (much more powerful than the current product). But after testing them, I realized they could cause serious injuries if not used properly. So I canceled that version. I still have 8 of them sitting in my office.</p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44241074"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241074" href="https://news.ycombinator.com/vote?id=44241074&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Interesting project!  Sorry if I'm out of the loop, but how exactly does the MCP server hand off visual data to an external LLM service to formulate the robot control actions?  It's an interesting concept, but I'm having a hard time wrapping my head around how it works, because I thought MCP was text-oriented.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241153"><td></td></tr>
                  <tr id="44240971"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240971" href="https://news.ycombinator.com/vote?id=44240971&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>What a unique and fun build! So curious to hear about what ways it can be programmed and used for personal projects.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240726"><td></td></tr>
            <tr id="44240610"><td></td></tr>
                <tr id="44240672"><td></td></tr>
                      <tr id="44240643"><td></td></tr>
            <tr id="44240680"><td></td></tr>
                  <tr id="44241272"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241272" href="https://news.ycombinator.com/vote?id=44241272&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>As someone who's long dreamed of owning a robotic camera control arm, but who doesn't have a spare $50K kicking around to buy one, I've been following the development of these kinds of projects with great interest. While this particular arm doesn't look like it would have enough payload capacity or smooth enough motion for the use cases I have in mind, the fact its a couple hundred bucks means something that does what I need it to do for an actually affordable price isn't likely too far off.</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44241011"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44241011" href="https://news.ycombinator.com/vote?id=44241011&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>the website is pretty bad....could use a lot friendlier buttons, layout, more pictures, maybe some videos</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240515"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240515" href="https://news.ycombinator.com/vote?id=44240515&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Neat! Does this work with open source models like pi0 and OpenVLA? How does the inference-time teaching you outline work exactly?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44240649"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44240649" href="https://news.ycombinator.com/vote?id=44240649&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>The software stack is built around LeRobot. So anything you can run with LeRobot should be able to run with our software. Will do more testing before the official release. Personally, I feel GR00T N1 or ACT is much easier to train and do a fairly good job</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44241056"><td></td></tr>
                        <tr id="44240480"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240480" href="https://news.ycombinator.com/vote?id=44240480&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>Alright, watching the video - I'm sold, even at a sped up rate. How do I buy? I'll do in-town pickup if that's faster!</p>
              </div></td></tr>
        </tbody></table></td></tr>
            <tr id="44240779"><td></td></tr>
            <tr id="44240804"><td></td></tr>
            <tr id="44240634"><td></td></tr>
                <tr id="44240811"><td></td></tr>
                <tr id="44240873"><td></td></tr>
                        <tr id="44240584"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_44240584" href="https://news.ycombinator.com/vote?id=44240584&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>How backportable are the upgrades? If I have an SO-101, can I just replace a few parts to mount a camera and use your software?</p>
              </div></td></tr>
        </tbody></table></td></tr>
                <tr id="44240688"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_44240688" href="https://news.ycombinator.com/vote?id=44240688&amp;how=up&amp;goto=item%3Fid%3D44240302"></a></center>    </td><td><br><div>
                  <p>The software is fully compatible with LeRobot's repo. As long as your dataset matches your inference time setup, you are good to go.</p>
              </div></td></tr>
        </tbody></table></td></tr>
                  <tr id="44240507"><td></td></tr>
                <tr id="44240612"><td></td></tr>
                  <tr id="44240455"><td></td></tr>
                <tr id="44240608"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[You Can Drive but Not Hide: Detection of Hidden Cellular GPS Vehicle Trackers (166 pts)]]></title>
            <link>https://www.researchgate.net/publication/391704077_You_Can_Drive_But_You_Cannot_Hide_Detection_of_Hidden_Cellular_GPS_Vehicle_Trackers</link>
            <guid>44240286</guid>
            <pubDate>Tue, 10 Jun 2025 19:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.researchgate.net/publication/391704077_You_Can_Drive_But_You_Cannot_Hide_Detection_of_Hidden_Cellular_GPS_Vehicle_Trackers">https://www.researchgate.net/publication/391704077_You_Can_Drive_But_You_Cannot_Hide_Detection_of_Hidden_Cellular_GPS_Vehicle_Trackers</a>, See on <a href="https://news.ycombinator.com/item?id=44240286">Hacker News</a></p>
Couldn't get https://www.researchgate.net/publication/391704077_You_Can_Drive_But_You_Cannot_Hide_Detection_of_Hidden_Cellular_GPS_Vehicle_Trackers: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Xeneva Operating System (207 pts)]]></title>
            <link>https://github.com/manaskamal/XenevaOS</link>
            <guid>44240265</guid>
            <pubDate>Tue, 10 Jun 2025 19:08:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/manaskamal/XenevaOS">https://github.com/manaskamal/XenevaOS</a>, See on <a href="https://news.ycombinator.com/item?id=44240265">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">XenevaOS</h2><a id="user-content-xenevaos" aria-label="Permalink: XenevaOS" href="#xenevaos"></a></p>
<p dir="auto"><a href="https://discord.com/invite/YNsY7hhQ" rel="nofollow"><img src="https://camo.githubusercontent.com/29e1042d96fec01530b20f84d184c695e7daf5867cfe3d7ad4cc230e3fca46a9/68747470733a2f2f696d672e736869656c64732e696f2f646973636f72642f313235353332363537323631373932343631383f6c6162656c3d446973636f7264266c6f676f3d646973636f7264267374796c653d666c6174" alt="Discord" data-canonical-src="https://img.shields.io/discord/1255326572617924618?label=Discord&amp;logo=discord&amp;style=flat"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manaskamal/XenevaOS/blob/master/XeLogo.jpg?raw=true"><img src="https://github.com/manaskamal/XenevaOS/raw/master/XeLogo.jpg?raw=true" alt="alt text"></a></p>
<p dir="auto">Xeneva is an operating system for both x86_64 and ARM64  architectures, built from the ground up. The Kernel is known as <em>'Aurora'</em>
with hybrid kernel design and the entire operating system is known as <em>'Xeneva'</em>.<br> Please visit <a href="https://github.com/manaskamal/XenevaOS/blob/master/Docs/BuildInstructions.md">Build Instructions</a> to build the project</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Documentation</h2><a id="user-content-documentation" aria-label="Permalink: Documentation" href="#documentation"></a></p>
<p dir="auto">Refer to <a href="https://github.com/manaskamal/XenevaOS/blob/master/Docs/Introduction.md">Documentations of XenevaOS</a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contributing to the project</h2><a id="user-content-contributing-to-the-project" aria-label="Permalink: Contributing to the project" href="#contributing-to-the-project"></a></p>
<p dir="auto">XenevaOS is an open-source project that welcomes contributions from developers, researchers, and enthusiasts who are passionate about low level system development. Whether you're intereseted in Kernel development, driver development, low level graphics or application level features there are many ways to get involved. We encourage contributions in the form of code, documentation, bug reports, and feature suggestions. If you're interested in contibuting, check out out <a href="https://github.com/manaskamal/XenevaOS/blob/master/Docs/Contributing.md">Contribution Guideline</a>, explore open issues, and join the discussion to help shape the project.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto"><strong>Features</strong> :</h2><a id="user-content-features-" aria-label="Permalink: Features :" href="#features-"></a></p>
<ul dir="auto">
<li>ACPI (through acpica)</li>
<li>x64 and arm64 Kernel</li>
<li>Fully Higher Half Memory with MMIO mappings</li>
<li>Driver <em>loading and linking</em> through dll files</li>
<li>SSE/SSE3</li>
<li>USB3</li>
<li>Intel High Definition Audio</li>
<li>Multiprocessor support (muliprocessor scheduler not ready !!)</li>
<li>Application loader and dynamic library (shared library)</li>
<li>Freetype2 as font rendering engine</li>
<li>Compositing Window Manager called "Deodhai"</li>
<li>Graphics Library known as "Chitralekha"</li>
<li>Xeneva Terminal with basic ANSI/VT100 escape sequence support</li>
<li>Desktop environment called Namdapha Desktop</li>
<li>Storage supports : AHCI/SATA, NVMe</li>
<li>USB3 Class drivers (USB HID Mouse, USB Tablet, USB MSC driver) ..etc</li>
<li>Audio Server called "Deodhai-Audio" with
(44kHz/16bit audio format, stereo/mono panning, gain control)</li>
<li>Networking (IPv4, UDP/IP, TCP/IP, ICMP)</li>
</ul>
<p dir="auto"><em>And many more coming soon</em></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshot:</h2><a id="user-content-screenshot" aria-label="Permalink: Screenshot:" href="#screenshot"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manaskamal/XenevaOS/blob/master/XenevaV1_1.png?raw=true"><img src="https://github.com/manaskamal/XenevaOS/raw/master/XenevaV1_1.png?raw=true" alt="alt text"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">AppTray:</h2><a id="user-content-apptray" aria-label="Permalink: AppTray:" href="#apptray"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/manaskamal/XenevaOS/blob/master/apptray.png?raw=true"><img src="https://github.com/manaskamal/XenevaOS/raw/master/apptray.png?raw=true" alt="alt text"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installed Applications :</h2><a id="user-content-installed-applications-" aria-label="Permalink: Installed Applications :" href="#installed-applications-"></a></p>
<ul dir="auto">
<li>Audio player called Accent Player</li>
<li>File Browser</li>
<li>Calculator</li>
<li>Xeneva Terminal (ANSI/VT100 support)</li>
<li>XEShell</li>
<li>Audio Server called (DeodhaiAudio)</li>
<li>play (<em>CLI Application</em>)</li>
<li>piano (<em>CLI Application</em>)</li>
<li>Calender</li>
<li>Background Serivices like - Net Manager</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Building the project</h2><a id="user-content-building-the-project" aria-label="Permalink: Building the project" href="#building-the-project"></a></p>
<p dir="auto">The project is build purely on Windows Environment. Please Visit <a href="https://github.com/manaskamal/XenevaOS/blob/master/Docs/BuildInstructions.md">Build Instructions page </a> for more information</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Contact:</h2><a id="user-content-contact" aria-label="Permalink: Contact:" href="#contact"></a></p>
<p dir="auto">Email: <a href="mailto:manaskamal.kandupur@gmail.com">manaskamal.kandupur@gmail.com</a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Android 16 Is Here (300 pts)]]></title>
            <link>https://blog.google/products/android/android-16/</link>
            <guid>44239812</guid>
            <pubDate>Tue, 10 Jun 2025 18:29:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/products/android/android-16/">https://blog.google/products/android/android-16/</a>, See on <a href="https://news.ycombinator.com/item?id=44239812">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="jump-content" tabindex="-1">
            

    
    

    <article>

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Android 16 is here&quot;
  }">
      <div>
          
            <p>Jun 10, 2025</p>
          
          
            <p data-reading-time-render="">[[read-time]] min read</p>
          
        </div>
      
        <p>
          Check out more helpful and streamlined notifications, multitasking tools to keep you in the flow, enhanced support for hearing devices and more.
        </p>
      
    </div>

    

    
      










<div>
    <figure>
      <div>
        <p><img alt="Green sixteen numbers in the middle surrounded by blue, green and orange confetti. The sixteen is framed by visuals of a food order notification, a tablet screen with a multiple windows like a desktop, a notification with text Switch to hearing aid mic?" data-component="uni-progressive-image" fetchpriority="high" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Android_16_blog_header.width-200.format-webp.webp" width="360px" data-sizes="(max-width: 1023px) 100vw,(min-width: 1024px and max-width: 1259) 80vw, 1046px" data-srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Android_16_blog_header.width-800.format-webp.webp 800w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Android_16_blog_header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Android_16_blog_header.width-1600.format-webp.webp 1600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Android_16_blog_header.width-2200.format-webp.webp 2200w">
        </p>
      </div>
      
    </figure>
  </div>






    

    
    <div data-reading-time="true" data-component="uni-article-body">

            
              





<uni-article-speakable page-title="Android 16 is here" listen-to-article="Listen to article" data-date-modified="2025-06-10T18:00:03.192411+00:00" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-script-src="https://www.gstatic.com/readaloud/player/web/api/js/api.js"></uni-article-speakable>

            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }"><p data-block-key="eqnb7">Today, we’re bringing you Android 16, rolling out first to supported Pixel devices with more phone brands to come later this year. This is the <a href="https://android-developers.googleblog.com/2024/10/android-sdk-release-update.html">earliest</a> Android has launched a major release in the last few years, which ensures you get the latest updates as soon as possible on your devices.</p><p data-block-key="35du0">Android 16 lays the foundation for our new <a href="https://blog.google/products/android/material-3-expressive-android-wearos-launch/">Material 3 Expressive</a> design, with features that make Android more accessible and easy to use. Let’s take a look at what’s new:</p><h2 data-block-key="fq1jr">Streamlined and up-to-date notifications</h2><p data-block-key="1ueir">Waiting for your food delivery to get to your house? Instead of opening your delivery app every 5 minutes, live updates keep you informed in real-time. These live updates are starting with compatible ride-share and food delivery apps. We’re working together with these app partners to bring this capability to the Android ecosystem, including in Samsung’s Now Bar and OPPO and OnePlus’ Live Alerts.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Phone screen showing the action of unlocking the phone, tapping on the top left of the screen to open a notification of an Uber Eats update that shows status bar of the order." external-image="" or-mp4-video-title="Live updates" or-mp4-video-url="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Live_updates.mp4" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
    
  
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }">
        <p data-block-key="eqnb7">We’re also force-grouping notifications that come from a single app to help reduce information overload. Your notifications will automatically be grouped together to keep things looking tight and organized.</p>
      </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="A rounded corner rectangular notification box with a Google Home icon on the left and the text: Home, now, package seen front door, break, matt seen front door, break, animal seen outside back yard." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="A rounded corner rectangular notification box with a Google Home icon on the left and the text: Home, now, package seen front door, break, matt seen front door, break, animal seen outside back yard." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Notifications_grouping.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Notifications_grouping.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Notifications_grouping.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }"><h2 data-block-key="eqnb7">Clearer, simpler calling with hearing devices</h2><p data-block-key="6vl6c">Android 16 provides significant enhancements for people who use hearing aids
<a data-ga4-analytics-superscript-click="" data-target="inline text" href="#footnote-1" id="footnote-source-1"><sup>1</sup></a>
. Current LE audio hearing devices use their built-in front-facing mics for audio input. Those mics are designed to pick up the sound of the person you're speaking to — not the sound of your voice. Now you can switch to using your phone's microphone for clearer calls in noisy environments.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Phone screen with an active call with a person with black hair and name angana ghosh. Call duration is shown as 58 seconds. The icon for Anna’s Hearing Aids is lit up and there is a notification at the top of the screen prompting the user to switch to hearing aid mic for hands free calling on the go." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Phone screen with an active call with a person with black hair and name angana ghosh. Call duration is shown as 58 seconds. The icon for Anna’s Hearing Aids is lit up and there is a notification at the top of the screen prompting the user to switch to hearing aid mic for hands free calling on the go." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Change_mic_hearing_devices.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Change_mic_hearing_devices.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Change_mic_hearing_devices.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }">
        <p data-block-key="eqnb7">We’re also releasing native control for hearing devices in Android 16 for easier access and a more consistent experience. This means you can now control things like volume on your hearing devices right from your Android phone.</p>
      </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Phone screen showing the window for hearing aids control with the top lit up with Anna’s Hearing Aids Active and settings icon. Below shows different settings including a Preset for All-Around and default microphone for calls. The last section includes Surroundings and a control bar for left and right." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Phone screen showing the window for hearing aids control with the top lit up with Anna’s Hearing Aids Active and settings icon. Below shows different settings including a Preset for All-Around and default microphone for calls. The last section includes Surroundings and a control bar for left and right." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Mic_control_hearing_devices.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Mic_control_hearing_devices.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Mic_control_hearing_devices.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }"><h2 data-block-key="eqnb7">Stronger security in a single tap</h2><p data-block-key="ci737">With Android 16, you can now activate <a href="https://security.googleblog.com/2025/05/advanced-protection-mobile-devices.html">Advanced Protection</a>
<a data-ga4-analytics-superscript-click="" data-target="inline text" href="#footnote-2" id="footnote-source-2"><sup>2</sup></a>
, Google’s strongest mobile device protection. It enables an array of robust device security features that protect you from online attacks, harmful apps, unsafe websites, scam calls and more. Whether you’re a public figure or you just prioritize security, Advanced Protection gives you greater peace of mind that you’re protected against the most sophisticated attacks.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="A phone screen scrolling through the Advanced Protection page with a list of capabilities that are activated when Device Protection is turned on including Device Safety, Apps, Networks and Web." external-image="" or-mp4-video-title="Advanced Protection" or-mp4-video-url="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Advanced_protection.mp4" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }"><h2 data-block-key="eqnb7">Enhanced productivity for tablets</h2><p data-block-key="5ospg">Samsung <a href="https://www.samsung.com/us/apps/dex/">DeX</a> has helped maximize productivity on phones, foldables and tablets for years. In Android 16, we worked closely with Samsung to develop desktop windowing, a new way to interact with your apps and content on large-screen devices. In addition to the single app and split-screen modes, you can open, move and resize multiple app windows in a single screen, just like a desktop. This makes it easier to work across apps. Android 16’s desktop windowing will roll out later this year on compatible devices.</p></div>
  

  
    






<uni-image-full-width alignment="full" alt-text="A horizontal tablet screen showing a Chrome web browser window being tapped and pulled into a smaller window, with calendar and email opened as separate windows overlaid like on a desktop." external-image="" or-mp4-video-title="Desktop windowing" or-mp4-video-url="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_videos/Desktop_windowing.mp4" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }">
        <p data-block-key="eqnb7">Later this year, we’re also launching custom keyboard shortcuts, so you can create your own combination of hotkeys, and taskbar overflow, which provides a simple, visual way for you to find the app you need when the taskbar is full. Future updates will also bring even more productivity enhancements to Android, allowing you to connect tablets and phones to an external display for an expanded desktop experience, and <a href="https://android-developers.googleblog.com/2025/06/developer-preview-enhanced-android-desktop-experiences-connected-displays.html">developers can begin testing</a> those features starting today.</p>
      </div>
  

  
    






<uni-image-full-width alignment="full" alt-text="Two tablets side by side. The left tablet shows a window with the ability to customize shortcuts on your system and the right tablet shows multiple windows overlaid, with a bar ar the bottom showing all the different windows." external-image="" or-mp4-video-title="" or-mp4-video-url="" section-header="Android 16 is here" custom-class="image-full-width--constrained-width uni-component-spacing">
  
  
    <p><img alt="Two tablets side by side. The left tablet shows a window with the ability to customize shortcuts on your system and the right tablet shows multiple windows overlaid, with a bar ar the bottom showing all the different windows." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Taskbar_and_keyboard.width-100.format-webp.webp" loading="lazy" data-loading="{
            &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Taskbar_and_keyboard.width-500.format-webp.webp&quot;,
            &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Taskbar_and_keyboard.width-1000.format-webp.webp&quot;
          }">
    </p>
  
</uni-image-full-width>


  

  
    <div data-component="uni-article-paragraph" role="presentation" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Android 16 is here&quot;
         }"><p data-block-key="eqnb7">There are many more features to explore with Android 16 — like HDR screenshots, adaptive refresh rate, identity check and others — as well as additional updates coming to <a href="https://blog.google/products/android/new-android-features-june-2025/">Android</a> and <a href="https://blog.google/products/pixel/pixel-drop-june-2025/">Pixel</a> devices today. And later this year, more Material 3 Expressive design updates are coming to Android 16 and Wear OS 6 on Pixel devices.</p><p data-block-key="d8sqs">See all the Android 16 features at <a href="http://android.com/16">Android.com/16</a>.</p></div>
  


            
            

            
              




            
          </div>
  </article>
  



  



  



  

  


<div data-component="uni-related-articles" aria-roledescription="carousel" data-analytics-module="{
    &quot;module_name&quot;: &quot;Article Footer Related Stories&quot;,
    &quot;section_header&quot;: &quot;Related stories&quot;
  }">
        <h3>
          <p>
            Related stories
          </p>
        </h3>
      </div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Low-background Steel: content without AI contamination (377 pts)]]></title>
            <link>https://blog.jgc.org/2025/06/low-background-steel-content-without-ai.html</link>
            <guid>44239481</guid>
            <pubDate>Tue, 10 Jun 2025 17:55:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.jgc.org/2025/06/low-background-steel-content-without-ai.html">https://blog.jgc.org/2025/06/low-background-steel-content-without-ai.html</a>, See on <a href="https://news.ycombinator.com/item?id=44239481">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-body-6718543456445220885" itemprop="description articleBody">
<p>Somehow I forgot to blog my site: <a href="https://lowbackgroundsteel.ai/">https://lowbackgroundsteel.ai/</a>. I created it back in March 2023 as a clearinghouse for online resources that hadn't been contaminated with AI-generated content.&nbsp;</p><p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijWm8XoAHKZn2-fYIvZZXJFJLv4ASg4T0nmvExAee9kk32vI4V3gln9PpYKQBfJkMsae-fJvuo1A_cRI-ZX6E5i0f92RCHm5MXtZNHZr3ki3f3sX4n9HtlwwZCQVVGWigHYLhTJbz-pZQvRm0O5N6If_LqG6Cw2A9InEtGuBD4-mPqRtdc7NUBMQ/s2370/lbs.jpeg"><img data-original-height="2370" data-original-width="2066" height="640" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEijWm8XoAHKZn2-fYIvZZXJFJLv4ASg4T0nmvExAee9kk32vI4V3gln9PpYKQBfJkMsae-fJvuo1A_cRI-ZX6E5i0f92RCHm5MXtZNHZr3ki3f3sX4n9HtlwwZCQVVGWigHYLhTJbz-pZQvRm0O5N6If_LqG6Cw2A9InEtGuBD4-mPqRtdc7NUBMQ/w558-h640/lbs.jpeg" width="558"></a></p><p><a href="https://en.wikipedia.org/wiki/Low-background_steel">Low-background Steel </a>(and lead) is a type of metal uncontaminated by radioactive isotopes from nuclear testing. That steel and lead is usually recovered from ships that sunk before the Trinity Test in 1945. The site is about uncontaminated content that I'm terming "Low-background Steel". The idea is to point to sources of text, images and video that were created prior to the explosion of AI-generated content that occurred in 2022.</p><p>It currently contains pointers to a Wikipedia dump from prior to the release of ChatGPT, the Arctic Code Vault, Project Gutenberg, and more.</p><p>If you know of other sources of non-contaminated content plus <a href="https://lowbackgroundsteel.ai/submit">submit them</a>!</p>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI dropped the price of o3 by 80% (456 pts)]]></title>
            <link>https://twitter.com/sama/status/1932434606558462459</link>
            <guid>44239359</guid>
            <pubDate>Tue, 10 Jun 2025 17:41:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/sama/status/1932434606558462459">https://twitter.com/sama/status/1932434606558462459</a>, See on <a href="https://news.ycombinator.com/item?id=44239359">Hacker News</a></p>
Couldn't get https://twitter.com/sama/status/1932434606558462459: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[A Blacklisted American Magician Became a Hero in Brazil (116 pts)]]></title>
            <link>https://www.wsj.com/lifestyle/careers/magician-brazil-national-celebrity-d31f547a</link>
            <guid>44239107</guid>
            <pubDate>Tue, 10 Jun 2025 17:10:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/lifestyle/careers/magician-brazil-national-celebrity-d31f547a">https://www.wsj.com/lifestyle/careers/magician-brazil-national-celebrity-d31f547a</a>, See on <a href="https://news.ycombinator.com/item?id=44239107">Hacker News</a></p>
Couldn't get https://www.wsj.com/lifestyle/careers/magician-brazil-national-celebrity-d31f547a: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Show HN: Chili3d – A open-source, browser-based 3D CAD application (349 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=44238171</link>
            <guid>44238171</guid>
            <pubDate>Tue, 10 Jun 2025 15:39:57 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=44238171">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="44238171">
      <td><span></span></td>      <td><center><a id="up_44238171" href="https://news.ycombinator.com/vote?id=44238171&amp;how=up&amp;goto=item%3Fid%3D44238171"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=44238171">Show HN: Chili3d – A open-source, browser-based 3D CAD application</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_44238171">112 points</span> by <a href="https://news.ycombinator.com/user?id=xiange">xiange</a> <span title="2025-06-10T15:39:57 1749569997"><a href="https://news.ycombinator.com/item?id=44238171">2 hours ago</a></span> <span id="unv_44238171"></span> | <a href="https://news.ycombinator.com/hide?id=44238171&amp;goto=item%3Fid%3D44238171">hide</a> | <a href="https://hn.algolia.com/?query=Show%20HN%3A%20Chili3d%20%E2%80%93%20A%20open-source%2C%20browser-based%203D%20CAD%20application&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=44238171&amp;auth=0e8832f8414e75ac011f4abd7df1e72081b5cfa3">favorite</a> | <a href="https://news.ycombinator.com/item?id=44238171">24&nbsp;comments</a>        </span>
              </td></tr>
    <tr><td></td></tr><tr><td colspan="2"></td><td><div><p>I'm currently developing Chili3D, an open-source, browser-based 3D CAD application. By compiling OpenCascade to WebAssembly and integrating Three.js, Chili3D delivers near-native performance for powerful online modeling, editing, and rendering—all without local installation. Access it here:</p><p><a href="https://github.com/xiangechen/chili3d">https://github.com/xiangechen/chili3d</a></p><p>Features:</p><p>Modeling Tools: Create basic shapes (boxes, cylinders, cones, etc.), 2D sketches (lines, arcs, circles, etc.), and perform advanced operations (boolean operations, extrusion, revolution, etc.).</p><p>Snapping and Tracking: Precisely snap to geometric features, workplanes, and track axes for accurate alignment.</p><p>Editing Tools: Modify (chamfer, fillet, trim, etc.), transform (move, rotate, mirror), and perform advanced edits (feature removal, sub-shape manipulation).</p><p>Measurement Tools: Measure angles and lengths, and calculate sums of length, area, and volume.</p><p>Document Management: Create, open, and save documents, with full undo/redo history and support for importing/exporting STEP, IGES, BREP formats.</p><p>User Interface: Office-style interface with contextual command organization, hierarchical assembly management, dynamic workplanes, and 3D viewport controls.</p><p>Multi-Language Support: Built-in i18n support with current languages including Chinese and English.</p></div></td></tr>        <tr><td></td></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Malleable software: Restoring user agency in a world of locked-down apps (252 pts)]]></title>
            <link>https://www.inkandswitch.com/essay/malleable-software/</link>
            <guid>44237881</guid>
            <pubDate>Tue, 10 Jun 2025 15:16:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.inkandswitch.com/essay/malleable-software/">https://www.inkandswitch.com/essay/malleable-software/</a>, See on <a href="https://news.ycombinator.com/item?id=44237881">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>

<h2 id="motivation"><a href="#motivation">Motivation</a></h2>
<h3 id="we-want-to-adapt-our-environments"><a href="#we-want-to-adapt-our-environments">We want to adapt our environments</a></h3>
<p>Environments matter. To do our best work and live our best lives, we need spaces that let us each express our unique potential.</p>
<p>A guitar maker sets up their workshop with their saws, hammers, chisels and files arranged just so. They can also build new tools as needed to achieve the best result—a wooden block as a support, or a pair of pliers sanded down into the right shape.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/guitar-workshop.jpg" alt="guitar-workshop.jpg"></p>
</figure>
<p>Over the years, a home cook gradually assembles a combination of knives, cutting boards, and pots and pans, finding the ones that work best for them. They can install hooks on the ceiling and move shelves around to support their workflow—whether that’s cooking weekday dinners or hosting elaborate weekend cookouts.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/kitchen.jpg" alt="kitchen.jpg"></p>
</figure>
<p>These are everyday situations. <strong>In the physical world, the act of crafting our environments comes naturally, because physical reality is malleable.</strong></p>
<p>Many small tweaks—taping a post-it note to the wall, rearranging some drawers, moving a piece of furniture—can be done instantly without asking anyone’s permission. We can also take on larger changes that require more effort and skill, like building a workshop or renovating a kitchen. And should we lack those skills ourselves, we can recruit help from craftspeople in our local communities.</p>
<p>When we work and live in a physical space that we control, we tend to evolve it to suit our own needs. As Stewart Brand writes in his book <a href="https://en.wikipedia.org/wiki/How_Buildings_Learn">How Buildings Learn</a>: “Age plus adaptivity is what makes a building come to be loved. The building learns from its occupants, and they learn from it.”</p>
<h3 id="mass-produced-software-is-too-rigid"><a href="#mass-produced-software-is-too-rigid">Mass-produced software is too rigid</a></h3>
<p>These days, we spend more and more of our time in environments built from code, not atoms. We’ve gained many capabilities in this shift—we can collaborate instantly across continents and search thousands of files in an instant. But <strong>we’re also losing something important: the ability to adapt our environments and make them our own.</strong></p>
<p>Here’s an example. One of the authors worked on a software team that tracked its work with index cards taped to a wall. The team would constantly evolve the tracker—tape lines moved; checklists appeared; special zones of cards emerged around the main grid. The fluidity of the tool encouraged fluidity of process.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/wall-board.jpg" alt="wall-board.jpg"></p>
<figcaption>A wall of index cards is a malleable medium for managing software development.</figcaption>
</figure>
<p>Later, the team switched to a web-based issue tracker to support remote collaborators. Now there wasn’t a way to model or display a special zone of cards, so the team abandoned that part of their process.  Further process changes also ground to a halt. Before, new ideas took minutes to try; now they could take hours of wrangling configurations, if they were possible at all. <strong>Computerizing work led to a loss of agency.</strong></p>
<p>The rigidity of software isn’t just a minor inconvenience. It can seriously impede people doing important work. The doctor and writer Atul Gawande <a href="https://www.newyorker.com/magazine/2018/11/12/why-doctors-hate-their-computers">has written about</a> how computerization in the medical profession is leading to record levels of burnout. For instance, doctors would once skip irrelevant fields when filling out paper forms; now the software forces them to fill in those fields, and they have no power to edit those software rules. As Gawande says of one doctor: “Spending the extra time didn’t anger her. The pointlessness of it did.”</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/epic.jpg" alt="epic.jpg"></p>
<figcaption>Inflexible electronic medical records systems are driving doctors to burnout.</figcaption>
</figure>
<p>When you face a situation where software doesn’t meet your needs, you might try giving feedback to the developers—but that usually doesn’t result in immediate action. <strong>When different users have different needs, a centralized development team can’t possibly address everyone’s problems.</strong> And for that matter, when a developer does try to cram too many solutions into a single product, the result is a bloated mess. To avoid this trap, good product teams learn to decline most user requests, leaving a long tail of niche needs unserved.</p>
<p>It may seem inevitable that our specific requirements aren’t served by our software—but that’s only because we’ve taken for granted that software is controlled by centralized development teams. What if we were to shift more control into the hands of users who know their own needs?</p>
<p>Gawande tells a story of a neurosurgeon who worked with an IT analyst to adapt his department’s medical records system. “Before long, they had built a faster, more intuitive interface, designed specifically for neurosurgery office visits.” The requirements were driven by the needs of <em>their specific department</em>, not the needs of every doctor in the country. Beyond the direct productivity benefit, the physicians felt more in control of their tools—an antidote to burnout.</p>
<p>As inspiring as this story is, it’s more the exception than the rule, because <strong>the tools and infrastructure we use to deploy software treat users as passive recipients rather than active co-creators.</strong> Software is organized into monolithic applications rather than flexible remixable toolkits. Customization requires programming skills that most people don’t have—and besides, most software is closed source. Software doesn’t ship to users with the tools to edit the software. App stores are designed for companies distributing software to consumers, not amateurs sharing tools with their friends. This is a system of industrial mass production, not small-scale craft.</p>
<p>To be fair, mass-produced software has delivered many benefits. We can access a vast array of highly polished applications at reasonable prices. Software has made progress in reliability, accessibility, and security. Developers have created business models that can sustainably pay teams to deliver continuously improving software.</p>
<p>But as these stories and countless other examples show, inflexible mass-produced software also gets in the way. The more different you are from the average user, the more the benefits of customization outweigh the benefits of professional polish. Everyone is unique in some way—perhaps you have strong opinions about tools for writing, making music, having discussions, or planning projects. When you have specific needs, agency matters.</p>
<h3 id="our-goal-malleable-software"><a href="#our-goal-malleable-software">Our goal: malleable software</a></h3>
<p>We envision a new kind of computing ecosystem that gives users agency as co-creators. We call this idea malleable software—<strong>a software ecosystem where anyone can adapt their tools to their needs with minimal friction.</strong></p>

<ul>
<li>By “software ecosystem”, we mean the broad technical and cultural environment surrounding software and its users. Malleability isn’t a narrow technical problem.</li>
<li>By “anyone”, we mean that broad accessibility is the goal. And while individual self-sufficiency is useful to cultivate, cooperating with local community is also valuable.</li>
<li>When we say “adapting tools” we include a whole range of customizations, from making small tweaks to existing software, to deep renovations, to creating new tools that work well in coordination with existing ones. Adaptation doesn’t imply starting over from scratch.</li>
<li>Finally, “minimal friction” is key. Editing our tools should be fast. It should feel light. At best, it should be something we can do in the moment when a need arises, so we can get back to the task at hand.</li>
</ul>
<h3 id="existing-approaches"><a href="#existing-approaches">Existing approaches</a></h3>
<p>You may be wondering: what about settings or plugins? There are indeed many techniques for customizing software that deserve to be celebrated. However, they also have limits that prevent them from fully achieving the goals we’ve laid out above.</p>
<h4 id="settings"><a href="#settings">Settings</a></h4>
<p>Settings are a common way to change the way an application behaves. If the right setting exists, you can just toggle a checkbox and move on with your day.</p>
<p>But settings only offer controls that the application developers have thought to expose, leaving you stuck if there’s not a setting that does what you want. Settings also tend to become long lists of disjointed checkboxes without a coherent mental model tying them together.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/settings.png" alt="settings.png"></p>
<figcaption>
<p>Settings allow for certain customizations chosen in advance by an application developer.</p>
</figcaption>
</figure>
<h4 id="plugins"><a href="#plugins">Plugins</a></h4>
<p>One way to scale beyond the bandwidth of a central developer is to allow third-party plugins that extend the behavior of an application. A good plugin system makes it easy for users to get started customizing with a minimum of effort, because they can install plugins that other people have created. A plugin API also has the key benefit of stabilizing the contract between the underlying application and various extensions, helping with ongoing maintenance.</p>
<p>However, <strong>plugin systems still can only edit an app’s behavior in specific authorized ways.</strong> If there’s not a plugin surface available for a given customization, the user is out of luck. (In fact, most applications have no plugin API at all, because it’s hard work to design a good one!)</p>
<p>There are other problems too. Going from installing plugins to <em>making</em> one is a chasm that’s hard to cross. And each app has its own distinct plugin system, making it typically impossible to share plugins across different apps.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/obsidian-plugins.png" alt="obsidian-plugins.png"></p>
<figcaption>The Obsidian Markdown editor offers a rich ecosystem of community plugins for extending the editor's behavior.</figcaption>
</figure>
<h4 id="modding"><a href="#modding">Modding</a></h4>
<p>When settings and official extension APIs don’t go far enough (or aren’t provided in the first place), users can sometimes take control through permissionless <em>modding</em>. For instance, browser extensions can intervene in a website’s user interface and inject new client-side behavior without needing any hooks to be exposed by the original application developer.</p>

<p>Permissionless mods apply in a much broader range of scenarios than officially supported plugin APIs. In fact, they can even be used when the original developer is actively opposed to a given type of extension—ad blockers prioritize the interests of users over the interests of websites.</p>
<p>Mods face their own limitations. They can require tedious reverse-engineering to create. They are often difficult to maintain as the underlying application evolves. Different mods on the same app often don’t work together cleanly.</p>
<p>The limits of the underlying platform can also limit what they’re able to do—for example, <strong>browser extensions can’t modify server-side behavior</strong>, severely limiting the features they can provide. Also, much like other kinds of plugins, unsupported mods have a wide chasm between <em>installing</em> and <em>creating</em>. For instance, to make a browser extension, you need to leave the browser, write code, and distribute a packaged code artifact. (Later on we’ll discuss some strategies to avoid these steps.)</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/extension.png" alt="extension.png"></p>
<figcaption>
    An example of a browser extension serving a niche need: BoA Checklist adds a persistent checkbox next to each transaction on the Bank of America website, to track if you've already reviewed that transaction.
</figcaption>
</figure>
<h4 id="open-source"><a href="#open-source">Open source</a></h4>
<p>The open source software movement promotes the idea of distributing source code so that users of software can own their software, contribute back to it, and if needed create their own version that better meets their own needs. This has been a positive force for the world, and represents an important ingredient for malleability.</p>
<p>But having access to edit the code doesn’t mean minimal friction. <strong>Modifying a serious open source codebase usually requires significant expertise and effort</strong>. This applies even for making a tiny change, like changing the color of a button . Even for a skilled programmer, setting up a development environment and getting acquainted with a codebase represents enough of a hurdle that it’s not casually pursued in the moment.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/pull-requests.png" alt="pull-requests.png"></p>
<figcaption>Users can propose changes to an open source project on GitHub.</figcaption>
</figure>
<h4 id="ai-assisted-coding"><a href="#ai-assisted-coding">AI-assisted coding</a></h4>
<p>Finally, it’s worth addressing a hot topic of the moment. Will AI tools automatically make malleable software a reality?</p>
<p>Historically, one of the biggest puzzles in democratizing the creation of software was figuring out how to enable regular people to write computer programs. Many approaches have been tried: programming languages with friendlier syntax or simpler semantics; programming-by-demonstration macro recording systems; visual live programming environments, and more.</p>
<p>Today, large language models offer a new approach: to take fuzzy ideas expressed in natural language and automatically turn them into code. There is now momentum towards a world where anyone can generate a web application from a chat, without needing any programming experience. There are new examples every day, from a journalist making <a href="https://www.nytimes.com/2025/02/27/technology/personaltech/vibecoding-ai-software-programming.html">an app for generating school lunch ideas</a> to an environmental studies student making <a href="https://www.wsj.com/tech/ai/your-next-favorite-app-the-one-you-make-yourself-a6a84f5f">a website for tracking reforestation efforts</a>. We’ve personally found it useful to build our own software tools with AI, such as a specialized <a href="https://www.geoffreylitt.com/2023/07/25/building-personal-tools-on-the-fly-with-llms">Japanese translation app</a>, and a minimal workout timer with a custom workout plan:</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/claude-workout-artifact.png" alt="claude-workout-artifact.png"></p>
<figcaption>Building a workout timer tool using AI in Claude Artifacts</figcaption>
</figure>
<p>We think these developments hold exciting potential, and represent a good reason to pursue malleable software at this moment. But at the same time, <strong>AI code generation alone does not address all the barriers to malleability.</strong> Even if we presume that every computer user could perfectly write and edit code, that still leaves open some big questions.</p>
<p>How can users tweak the <em>existing</em> tools they’ve installed, rather than just making new siloed applications? How can AI-generated tools compose with one another to build up larger workflows over shared data? And how can we let users take more direct, precise control over tweaking their software, without needing to resort to AI coding for even the tiniest change? None of these questions are addressed by products that generate a cloud-hosted application from a prompt.</p>
<p><strong>Bringing AI coding tools into today’s software ecosystem is like bringing a talented sous chef to a food court.</strong> If you’re used purchasing meals from a menu, a skilled chef can’t do much to help you. Similarly, if you’re using closed-source software from an app store, an AI coding assistant can’t do very much to help you as a user. To fully take advantage of the capabilities of AI, we need to move past the food court to something more like a kitchen—a site of open-ended creation.</p>
<hr>
<p>Reshaping the foundations of computing around user agency is an ambitious goal. Luckily, since the dawn of personal computing, many people have proposed powerful ideas for achieving it.</p>
<p>In the next few sections, we’ll summarize three design patterns that we see as essential for achieving malleable software. These aren’t precise recipes for how to solve the problem. But we believe they are important ways of thinking that are underrepresented in today’s software world and will form part of the solution to achieving malleable software.</p>

<h2 id="a-gentle-slope-from-user-to-creator"><a href="#a-gentle-slope-from-user-to-creator">A gentle slope from user to creator</a></h2>
<p>Malleable software does not imply everybody creating all of their own tools from scratch. That would be a waste of time, since many off-the-shelf tools are already pretty close to serving our needs, with only minor tweaks needed.</p>
<p>A more sensible approach is to start out using existing software tools built by other people or companies, but to have the <em>option</em> of modifying these tools as you discover ways that they don’t meet your needs. You can start out as a passive user, and <em>gradually</em> become an editor and a creator.</p>
<p>In a 1990 paper <a href="https://dl.acm.org/doi/10.1145/97243.97271">User-Tailorable Systems: Pressing the Issues with Buttons</a>, Allan MacLean and his collaborators at EuroPARC propose a powerful mental model for designing software systems that invite users to gradually become creators.</p>
<p>Consider this chart where the x-axis represents the power or depth of a customization, and the y-axis represents the level of skill required to perform that customization. In this model, whenever someone needs to suddenly learn a lot more in order to achieve the next level of depth of customization, that appear as a vertical “cliff”.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/maclean-cliffs.png" alt="maclean-cliffs.png"></p>
<figcaption>Going from changing settings to full programming is a sharp "cliff" that's hard to scale. (figure from MacLean et al)</figcaption>
</figure>
<p>To flatten out the cliffs and encourage smooth progress, MacLean et al. proposed a design goal: <strong>each incremental increase in tailoring power should only require a small incremental investment of learning and skill.</strong> A system that follows this rule can be visualized as having a “gentle slope” free of cliffs.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/maclean-slope.png" alt="maclean-slope.png"></p>
<figcaption>A gentle slope of tailorability, made up of many different customization techniques. (figure from MacLean et al)</figcaption>
</figure>
<p>The researchers demonstrated this principle in an automation system called Buttons. To start out, users could move buttons around and change the text or color. Slightly further up the slope, users could edit variable values or use a toolkit to create simple UIs. Finally, at the top of the slope, users could create new behaviors by doing Lisp programming. The key point was that <strong>each customization could be done with the simplest technique possible, leaving full programming only as a last resort when absolutely needed.</strong></p>

<p>Many successful environments for end-user malleability employ the gentle slope pattern. Consider spreadsheets as an example. When someone sends you a complex spreadsheet, you can start by just viewing it, or editing a cell marked as an input. Next you might try changing some formatting or add some labels. As you get deeper into using the spreadsheet, you might start tweaking formulas or adding your own new cells. A spreadsheet can start out feeling like an “application”—an artifact that someone else put together, with a few knobs intended for you to control—but unlike an application, you can smoothly progress to deeper customizations.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/spreadsheet-slope.png" alt="spreadsheet-slope.png"></p>
<figcaption>Spreadsheets offer a gentle slope of customization, from use to deeper modification.</figcaption>
</figure>
<p>Spreadsheets show us a couple useful qualities for achieving a gentle slope. First, It has an <a href="https://www.inkandswitch.com/end-user-programming/#in-place-toolchain">in-place toolchain</a>: everyone using a spreadsheet is running the full editor. When you want to dive deeper into modifying a spreadsheet, there’s no need to install or open a separate development environment.</p>
<p>Also, spreadsheets often have a scrappy aesthetic. It can <em>feel</em> safer to change a spreadsheet, compared to the pixel-perfect polish of a professionally designed application.</p>

<p>Another way to achieve a gentle slope is through explicit modes. HyperCard, a Mac program from the 1980s in which users could create stacks of “cards”, offered “levels” which governed what actions were available to the user at any time.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/hypercard-levels.png" alt="hypercard-levels.png"></p>
<figcaption>In HyperCard, users could select a "level" to restrict their editing capabilities.</figcaption>
</figure>
<p>Level 1 was read-only; Levels 2 and 3 supported text and graphical editing with direct manipulation interactions; Level 4 added creating buttons and linking things together; Level 5 unlocked full programming using a scripting language called HyperTalk. These explicit modes served as guardrails, allowing users to safely explore without unintended consequences.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/hypercard-slope.png" alt="hypercard-slope.png"></p>
<figcaption>Each level in HyperCard represents a modest step up in power and skill.</figcaption>
</figure>
<p>Finally, let’s see what it looks like to smooth out a cliff. As we discussed earlier, there’s a big chasm from installing a browser extension to building one yourself.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/browser-extension-cliff.png" alt="browser-extension-cliff.png"></p>
<figcaption>Going from installing a browser extension to making one is a steep cliff. It requires leaving the browser and doing involved programming.</figcaption>
</figure>
<p>We can achieve a much smoother slope if we incorporate lighter weight techniques for customizing websites. Tools like <a href="https://userstyles.org/">Stylish</a> and <a href="https://resources.arc.net/hc/en-us/articles/19212718608151-Boosts-Customize-Any-Website">Arc Boosts</a> let users create and share custom styling themes from within the browser. Further up the curve, <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=7eb0bba253a1d494544a15c3df921dd5a8f2401f">macro systems</a> and let users record and replay actions taken on websites. And if the user knows a little bit of JavaScript, they can tweak user scripts written in <a href="https://www.tampermonkey.net/">Tampermonkey</a> directly in the browser.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/arc-styles.png" alt="arc-styles.png"></p>
<figcaption>Arc Boosts lets users restyle a website with direct manipulation interactions from directly within the browser.</figcaption>
</figure>
<p>Each of these techniques gives a different tradeoff between power and skill. If someone just wants to tweak a color on a website, they should be able to do it in a less expressive system focused on that task, rather than needing to build an entire browser extension. With these techniques, we can smooth out the slope:</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/browser-extension-steady-slope.png" alt="browser-extension-steady-slope.png"></p>
<figcaption>Web customization techniques provide a gentler slope before needing to create a browser extension.</figcaption>
</figure>
<p>There are two ways to make a slope gentler. One way is to start from the right side of the diagram and reduce the required skill. This strategy corresponds to a huge variety of work which starts with the full expressiveness of programming and makes it more approachable—with <a href="https://developer.apple.com/library/archive/documentation/AppleScript/Conceptual/AppleScriptLangGuide/reference/ASLR_cmds.html">friendlier syntax</a>, <a href="https://liveprog.org/">live programming environments</a>, <a href="https://support.microsoft.com/en-us/office/using-flash-fill-in-excel-3f9bcf1e-db93-4890-94a0-1578341f73f7">inferring programs from examples</a>, <a href="https://hedy.org/">gradual scaffolding</a> and more.</p>
<p>However, there’s another strategy that’s just as important, and sometimes overlooked: starting from the left side of the diagram and gradually offering more power. Many successful customizable systems such as spreadsheets, HyperCard, Flash, Notion, and Airtable follow a similar pattern: <strong>a media editor with optional programmability.</strong>  When an environment offers document editing with familiar direct manipulation interactions, users can get a lot done without needing to write any code.</p>
<p>One final note: <strong>not everyone needs to reach the top of the slope.</strong> At some point, many amateurs will enlist help from others in order to save time and energy, or to avoid having to learn the necessary skills. The key is that many customizations are accessible to amateurs, with reasonable incremental investments of learning and skill. A gentle slope also supports smoother collaboration between people of different skill levels. We’ll explore this point more later on when we discuss the importance of communal creation.</p>

<p>So far we’ve focused on the rigidity of individual applications. But there’s another reason that it’s hard to adapt software to meet our needs: the very idea of “applications”. Because we’re so accustomed to this idea, it can be hard to see the nature of the problem—so let’s consider an analogy from the kitchen.</p>
<h3 id="apps-are-avocado-slicers"><a href="#apps-are-avocado-slicers">Apps are avocado slicers</a></h3>
<p>One way to slice an avocado is to use an “avocado slicer”: a 3-in-1 gadget that combines a dull plastic knife for slicing the avocado in half, a circular grabber for extracting the pit, and a line of plastic rods that produce 7 slices at once.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/avocado-slicer.jpg" alt="avocado-slicer.jpg"></p>
<figcaption>An avocado slicer is specialized to one use case.</figcaption>
</figure>
<p>Anyone can use an avocado slicer with no practice, and it poses no safety risk. And yet, because the avocado slicer is narrowly focused on one task, it’s useless at anything else. If you used a specialized gadget for every single task, you’d end up with a mountain of plastic.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/kitchen-gadgets.jpg" alt="kitchen-gadgets.jpg"></p>
<figcaption>
There are specialized gadgets available for an enormous variety of tasks, from stripping corn to cutting lettuce.
</figcaption>
</figure>
<p>Another approach is to use a knife. A knife can handle all the steps of slicing an avocado, and much more: it can slice a chicken breast, dice an onion, or smash a garlic clove.  You do need to learn how to handle the knife safely and skillfully, but it’s worth the effort, because <strong>a knife is a general tool.</strong></p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/knife.jpg" alt="knife.jpg"></p>
<figcaption>A knife can cut an avocado, and much more.</figcaption>
</figure>
<p>How does this analogy apply to software? <strong>Many applications are avocado slicers.</strong> They’re a bundle of functionality targeted at some specific use case: planning a trip, tracking workouts, organizing recipes. Because an app needs to handle many tasks associated with a use case, it sometimes doesn’t handle any of them particularly well. You may have come across situations where an app is missing some functionality that’s important to you, while simultaneously including extra bits you don’t need.</p>

<p>On top of that, solving a larger task using multiple applications often requires manual coordination. We can put windows next to each other and copy-paste data, but not much more. If we want more knife-like software tools, we’ll need better ways for smaller software tools to work together.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/travel-apps.png" alt="travel-apps.png"></p>
<figcaption>Planning a vacation requires manually coordinating across many isolated applications.</figcaption>
</figure>
<p>This isn’t to say that the application model has no benefits. Users get a cohesive experience within the bounds of an application, since the developer can finetune it without worrying about interoperating with other tools. Siloing data by application is a convenient answer to some security and privacy concerns. Paying for an application (and support) is a reasonable billing model. These are important benefits which can’t be ignored.</p>
<p>But when you’re trying to solve a complex problem or construct a creative workflow spanning multiple applications, the isolation between applications can cause serious problems. <strong>How might we reorient software around more general, composable tools</strong>—that feels more like a knife and less like an avocado slicer? There are two sub-problems to address: sharing data between tools, and combining tools within the user interface.</p>

<h3 id="sharing-data-between-tools"><a href="#sharing-data-between-tools">Sharing data between tools</a></h3>
<p>If we’re going to use different software tools together in coordination to get a job done, it’s essential that those tools can operate on one shared reality.</p>
<p>In modern cloud and mobile platforms, each application manages its own data in a private silo. The plan for the trip lives across a notes app, a Google Maps list, a calendar. This fragmentation impedes malleability. <strong>When each application manages its own data, introducing a new tool into a workflow incurs more friction</strong>. It’s easier to stay within an existing application to complete the whole job, even if superior alternatives exist for completing some part of the task.</p>
<p>When data is instead shared among applications, it empowers end users to compose tools in more flexible ways. One well-known example is the desktop filesystem. When information is stored in files, <strong>you can edit the same file in different tools have that focus on different subtasks</strong>. You can edit the light levels on a photograph in Lightroom, and then use Apple Preview to scribble a note on the photo. <strong>Different collaborators can also use their preferred tools</strong>—one user can edit a code repository in VSCode while collaborating with a coworker who edits it in emacs.</p>

<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/files.png" alt="files.png"></p>
<figcaption>Files serve as a common ground between different tools.</figcaption>
</figure>
<p>Another example of tools operating on shared data is low-code database application builders—including desktop-based applications like Microsoft Access and Filemaker Pro, and cloud-based applications like Airtable and Notion. These environments support end-users in building multiple views of the same database. For example, Airtable comes with with general tools such as a grid table editor, a board of cards, and a calendar. It also offers tools for adding custom UIs, but those are only needed in specialized cases, since many tasks can be achieved in the general grid editor.</p>
<figure>

<figcaption>Editing the same data with multiple views in Airtable, with live reactivity</figcaption>
</figure>
<p><strong>The idea of shared data also extends to shared objects with behavior.</strong> In <a href="https://en.wikipedia.org/wiki/Smalltalk">Smalltalk</a>, a user works with an <em>image</em>: a repository of objects representing not only persisted state, but also code associated with that state. Because everything in the system is represented as objects, it’s possible to introduce new tools that operate on those existing objects—including calling methods to invoke behavior.</p>

<p>Finally, <strong>realtime collaboration over shared data enables different people to work live in different tools.</strong> One example of this is in <a href="https://webstrates.net/">Webstrates</a>, a malleable software platform for collaboration in the browser. In Webstrates, tools run by different users can collaboratively edit the contents of a shared, synchronized dataset stored in the browser’s DOM. As a result, two users can each use their preferred editor to work on the same research paper together, with one of them using a WYSIWYG editor and the other using a plain text interface.</p>
<figure>

<figcaption>The same paper being edited realtime in two different editors, in Webstrates</figcaption>
</figure>
<h3 id="composing-the-user-interface"><a href="#composing-the-user-interface">Composing the user interface</a></h3>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/pegboard.jpg" alt="pegboard.jpg"></p>
</figure>
<p>Tools need to share not only underlying data, but a workspace where they can be used together.</p>
<p>The app paradigm isn’t set up for this. An application takes control over a window or tab and builds a world inside of it. This has advantages – an application’s creator can carefully craft your experience in this world and make sure everything works together just right. But it means that <strong>using multiple applications together requires flipping between isolated user experiences.</strong> It’s as though you have to carry ingredients from one kitchen to another every time you want to use a new tool. Even if data interoperates, your experience is splintered.</p>
<p>How can we break up not just an application’s ownership of data, but its control of the interactive environment? Earlier, we discussed approaches like plugins and permissionless modding, which crack open seams to extend applications from within. These are great, but if we want a world where we work with “tools, not apps”, we need something a little different – environments where tools can be brought together and used in a shared space.</p>
<p>One set of ideas comes from <strong>compound-document systems</strong>, like <a href="https://archive.org/details/OpenDocProgrammersGuide">OpenDoc</a> and <a href="https://en.wikipedia.org/wiki/Object_Linking_and_Embedding">OLE</a>. These let authors compose documents out of multiple kinds of media embedded in each other. For instance, a text document might host an embedded diagram, which can in turn host a table of data. Selecting one of these parts opens up an editor in-place, with no flipping back and forth between windows or digging through the file system required. These systems are open-ended; developers can introduce new kinds of media, as well as new, alternative editors for existing media types.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/opendoc.png" alt=""></p>
<figcaption>OpenDoc documents are made of "parts" that are edited with different editors, but stored together in a unified document.</figcaption>
</figure>
<p>Splitting up the UI into smaller pieces raises new design challenges. How do users choose which editors to use for each part of a document? How can a user share a document and guarantee that others will have all the necessary sub-editors available? How do global interactions like selection and focus work across editors? Systems like OpenDoc developed answers to these questions which are worth studying, although the nature of modern collaborative and web-based applications have also changed the surrounding context.</p>
<p>Sometimes, embedding editors for parts of a document is not enough. Consider this example from Tchernavskij’s <a href="https://theses.hal.science/tel-02612943v1">Designing and programming malleable software</a>: <strong>how might a user reuse a color picker from one app inside of another app?</strong> Unlike a diagram editor, a color picker is not associated with just one part of a document; it’s a tool that interacts with objects <em>throughout</em> the document. In order to do its job, the color picker somehow needs to be aware of other objects and have the ability to edit their colors. Usually, user interfaces hardcode these kinds of connections, but to allow extension or reuse, we need a more flexible and open-ended approach</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/color-picker.png" alt="color-picker.png"></p>
<figcaption>
<p>A challenge problem from <a href="https://theses.hal.science/tel-02612943v1">Designing and programming malleable software</a>: reusing a color picker from one application in another.</p>
</figcaption>
</figure>
<p>To solve this, Tchernavskij proposes <em>entanglers</em>: a dedicated layer of the UI system that dynamically detects and connects related UI elements. Another research project, <a href="https://vis.csail.mit.edu/pubs/varv/">Varv</a>, solves the problem by specifying behavior with lists of event triggers which can be additively grown. While the details of these approaches differ, the common element is representing connections between parts of the UI in such a way that supports later extension.</p>
<p>A more radical approach to UI composition is to use the physical world as a foundation for composability, as seen in <a href="https://dynamicland.org/">Dynamicland</a>. Programs at Dynamicland are built as collections of physical objects (like pieces of paper) that can carry code and data and act as handles for interaction. At Dynamicland, you can compose user interfaces by bringing them near each other on a table and taping or gluing them together. Programs can also use relationships in space, like pointing and proximity, to create virtual connections between objects. While screen-based systems can’t directly emulate Dynamicland’s use of the physical world, they may still be able to take some lessons from the value of having a consistent materiality and physics.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/spatial-relationships.jpg" alt="spatial-relationships.jpg"></p>
<figcaption>
<p>Pieces of paper running programs at Dynamicland, assembled on a table, sending out green “whiskers” to define their connections.</p>
</figcaption>
</figure>
<p>Dynamicland also derives much of its composability from how it breaks up programs into small objects that work together. Communication between these objects happens through a global database. As a result, it’s possible to add new tools into the system that intercept messages between existing objects, modifing the behavior of existing tools. When a system doesn’t assume up front that certain connections exist between senders and receivers of information, it’s easier to additively extend the system with new uses of that information.</p>

<h2 id="communal-creation"><a href="#communal-creation">Communal creation</a></h2>
<p>While it’s exciting to imagine every person crafting their own unique computing experience, an individualistic view of malleable software will only get you so far.</p>
<p>Sure, individuals should be able to adapt their software to their needs in small ways in the moment, climbing up a “gentle slope from use to creation”. But as the changes they want to make get larger, software is going to take more time and skills to make. If everyone were forced to make these changes just for themselves, as one-offs, the benefits would probably not be worth the costs. Not everyone wants to be a software-making expert.</p>
<p>And there’s an even more fundamental reason to think of malleability through a communal lens: we use computers together! A product team needs a single system for tracking projects. A department at a hospital needs a single system for patient intake forms. These communities are certainly not well-served by one-size-fits-all applications they have no control over. But the solution can’t be every-user-for-themselves either. We should help communities build and maintain <em>shared</em> solutions to their problems.</p>

<p><strong>With the right infrastructure, we can work together to craft our software.</strong> People with similar needs around the world can exchange work and build collaboratively, as we have seen happen in free-software communities. And local communities, from companies to families to civic organizations, can build and maintain software suited to their local needs. When local needs get higher up the “slope” and call for special levels of skill and enthusiasm, you don’t need everyone in a community to attain these levels – just enough people to play that role and get the job done.</p>

<p><strong>Building software for “local” contexts is sometimes <em>easier</em> than building software for world-wide use.</strong> You don’t need to build airtight, industrial-grade software if you are in direct contact with your users and can be responsive to the situations they run into. You don’t need to anticipate everyone’s needs in your design, just your community’s. Clay Shirky termed this pattern <a href="https://gwern.net/doc/technology/2004-03-30-shirky-situatedsoftware.html">“situated software”</a>, describing how his students were able to rapidly build software for their communities by “taking advantage of social infrastructure or context-sensitive information”. On an even more intimate level, Robin Sloan memorably described how an app built for his family could be a <a href="https://www.robinsloan.com/notes/home-cooked-app/">“home-cooked meal”</a>.</p>
<p>For all these reasons, we believe that <em>technical</em> infrastructures for  malleable software will need to support <em>sociotechnical</em> systems of people working together, across many levels, to make software work for themselves and their communities.</p>
<p>The history of free software offers lessons on how sociotechnical systems like this can be constructed. We are especially inspired by situations where free-software communities don’t assume there should only be one centrally-controlled version of an application in the world. For instance, Mastodon instances run by small communities often <a href="https://runyourown.social/">edit Mastodon’s code</a> to implement community-specific features and policies. Of course, situations like this still require serious engineering work, and are still operating on apps. As we move towards a world with gentler slopes into software modification, and more tools rather than apps, we’ll need to figure out how smaller pieces of code can be shared and collaboratively developed, and how interoperability can be maintained in a world of pluralistic software.</p>
<h2 id="ink-switch-prototypes"><a href="#ink-switch-prototypes">Ink &amp; Switch prototypes</a></h2>
<p>At Ink &amp; Switch, we’ve spent several years building research prototypes that explore different aspects of malleable software. These projects aren’t commercial products. Rather, the goal of each prototype has been to develop our understanding of techniques for enabling malleability, and then to learn from deep usage of those prototypes. (In fact, we wrote this very essay in a homegrown malleable software environment, and will share some of our successes and failures from that experience.)</p>

<p>Our work has spanned the entire computing stack, and can be roughly grouped into two threads. One thread has explored <strong>foundational infrastructure</strong>—techniques for storing data, loading code, and defining user interfaces, in a way that supports malleable experiences being built on top. Higher up the stack, another thread of work has focused on a particular kind of user experience:  <strong>dynamic documents</strong> where static media can be gradually enriched with interactive behavior. Let’s cover each in turn.</p>
<h3 id="infrastructure-for-malleability"><a href="#infrastructure-for-malleability">Infrastructure for malleability</a></h3>
<p>We’ve discussed how most of the <em>infrastructure</em> used to run and develop software today is designed without malleability as a priority. We’ve also shown some inspirations for reorienting around malleability. The desktop filesystem enables shared data between tools. Smalltalk, Hypercard, and spreadsheets enable live modification by allowing code editing at runtime. Browser-based apps encoding their UIs using the DOM enables permissionless modding through extensions.</p>
<p>Inspired by these predecessors, <strong>we have been prototyping an infrastructure stack for building, running, and sharing software that prioritizes malleability.</strong> Our approach builds on another area of our research: <a href="https://www.inkandswitch.com/essay/local-first/">local-first software</a>, which is a philosophy that prioritizes giving users ownership and control over their data, while maintaining the ability to collaborate with others. As part of that work, we developed a collaboration library called <a href="https://automerge.org/">Automerge</a>, which persists and synchronizes JSON documents among users.</p>

<h4 id="pushpin-an-extensible-media-canvas"><a href="#pushpin-an-extensible-media-canvas">PushPin: an extensible media canvas</a></h4>
<p>To start figuring out how a local-first foundation could enable malleability, we prototyped <a href="https://www.inkandswitch.com/pushpin">PushPin</a>, a web-based collaborative media canvas. A key idea we developed in that project was “document functional reactive programming” (DFRP): representing a tool as a UI component authored in React, backed by a JSON document that was automatically persisted and synchronized through Automerge.</p>
<p>DFRP made it less work to extend the interface than in traditional applications, since we could just add a UI component without worrying about backend databases or REST APIs. Furthermore, the decoupling of data and UI made it straightforward to define new user interfaces for <em>existing documents</em>—since we could define a new component and then register it as an alternate editor.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/pushpin.jpg" alt="pushpin.jpg"></p>
<figcaption>A PushPin board with embedded cards, each implemented as a React component backed by its own synced Automerge document</figcaption>
</figure>
<p>PushPin also showcased several challenges. First: how do you choose which UI to use to show a document in a given context?  We tried a simple model based on hardcoded contexts—“show a compact view when rendering the document as a card on a board; show an expanded view when rendering as the main content”—but this approach wasn’t sufficient for all our needs. For one thing, it didn’t allow users to choose among views depending on their task.</p>
<p>We also found that an embedding model needs to balance isolation and coordination. We used a restrictive model in which components can embed one another but no further communication was allowed. While this strong isolation made it easy to reason about behavior within a given component, there was no easy way for UI components to share context like hover states, which prevented certain rich user experiences. And permissionless modding wasn’t well supported either, since there was no mechanism to reach into an existing component. These questions of dispatching views and coordinating across components remain open areas of exploration for us.</p>
<h4 id="cambria-mediating-schema-compatibility-across-tools"><a href="#cambria-mediating-schema-compatibility-across-tools">Cambria: mediating schema compatibility across tools</a></h4>
<p>Schema compatibility was a point of frustration using PushPin. Upgrading tools would frequently break compatibility with existing data. And it was difficult to make different tools interoperate if they couldn’t agree exactly on the format of their underlying JSON data. This is an instance of a broader class of challenges—how should tools cooperate if they can’t agree on the shape of their data? In a malleable environment, we can’t rely on a central authority to dictate the schema.</p>
<p>We prototyped a system called <a href="https://www.inkandswitch.com/cambria/">Cambria</a> which solves this challenge by decoupling write schemas from read schemas. Updates are written by each tool in its preferred schema, and then interpreted on demand in other schemas—which could include newer/older versions of the schema, as well as entirely different schemas used by other tools. These are not one-time data migrations; they are ongoing live translations, preserving the ability to write and read from any schema simultaneously.</p>
<p>Based on some limited testing of the Cambria prototype, we believe live data translations are a promising direction for supporting pluralistic schemas. However, we have not yet built a production-ready version of this system that integrates with Automerge; one challenge is that it may require deep integration with the underlying data engine.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/lens-graph.png" alt="lens-graph.png"></p>
<figcaption>
<p>Cambria supports translating data between tools live on demand, through a graph of transformations called <em>lenses</em>.</p>
</figcaption>
</figure>
<h4 id="farm-code-is-data"><a href="#farm-code-is-data">Farm: code is data</a></h4>
<p>Although PushPin lowered the bar to contributing new tools to the environment, the system wasn’t editable at runtime—the code was stored in a GitHub repository, and deployed through a standard web application pipeline. Thus, in our next project, we experimented with enabling fluid modifications by <strong>using the local-first data layer to host not just user data, but also the code for the system itself.</strong> We built a system called <a href="https://github.com/inkandswitch/farm">Farm</a> which allowed users to author tools in the Elm language, with the source code stored and synced in Automerge documents.</p>

<p>At a basic level, treating code as data streamlined the sharing of tools. However, many aspects of collaboration proved challenging. How could you work on a tool without breaking it for other people as you edited it? And how could multiple collaborators work on different aspects of the same tool in parallel? This set of challenges led us to our next project.</p>
<h4 id="patchwork-version-control-bootstrapping"><a href="#patchwork-version-control-bootstrapping">Patchwork: version control + bootstrapping</a></h4>
<p>Enabling exploration and divergence lies at the heart of malleability. Different users need the freedom to try changing a piece of software or make multiple variations, without fear of losing work or messing up their collaborators. In traditional software engineering projects, these problems are solved with version control concepts, like Git branches.</p>
<p>While version control systems are useful tools for software engineers, they’re not designed to support malleable software. They are difficult to learn, and targeted at formal collaboration workflows. We believe that malleable software demands new solutions which prioritize accessibility to a broad set of users, and which better support casual, informal exploration. We call this vision  <a href="https://www.inkandswitch.com/universal-version-control/">universal version control</a>.</p>
<p>To explore these version control ideas further, we created Patchwork—a web-based collaboration environment for malleable software. Patchwork builds on the core ideas of PushPin and Farm, storing both user data and software code in Automerge documents. On top of that, it adds version control utilities like history views and simple branching. These tools apply to any document in the system—whether a piece of writing, or code for a software tool.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/diff-sidebar-replace.png" alt="diff-sidebar-replace.png"></p>
<figcaption>
    Patchwork can display diffs on branches of text documents, among other media types.
</figcaption>
</figure>
<p>One process difference between Patchwork and our previous infrastructure projects is that we are now “bootstrapping”—<strong>most of our lab’s internal knowledge work, from writing to whiteboarding to project planning, happens inside of Patchwork.</strong> In fact, these very words are being typed into a collaboratively edited Markdown document hosted in Patchwork.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/patchwork-tools.png" alt="patchwork-tools.png"></p>
<figcaption>
<p>We’ve used a broad variety of tools hosted in Patchwork—from text editors and whiteboards to simulations, audio sequencers and games.</p>
</figcaption>
</figure>

<p>One thing we’ve observed from using Patchwork for our work is that <strong>opportunities for improving your software can emerge naturally within a malleable system</strong>. Here’s an example from the process of writing this essay. The main tool we use is a collaborative Markdown editor:</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/markdown.png" alt="markdown.png"></p>
<figcaption>Editing this essay in Patchwork</figcaption>
</figure>
<p>At some point, we realized the essay was too long, but it was hard to see the length of different sections just by looking in the Markdown editor. So we created a minimal Section Word Counter tool that measured the length of each section:</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/word-counter.png" alt="word-counter.png"></p>
</figure>
<p>In Patchwork, this new tool could be straightforwardly integrated into the existing workflow. It was coded in a desktop IDE and then synchronized into an Automerge document, with no need to rebuild or redeploy Patchwork itself. Once installed, the Section Word Counter could instantly be used to view the existing essay document, with live updates as the essay changed.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/two-tools.png" alt="two-tools.png"></p>
<figcaption>Two Patchwork tools can edit/view the same underlying Automerge document.</figcaption>
</figure>
<p>Treating code as data also simplifies sharing. The word counter tool was initially developed by one of the authors for his own personal use, and only loaded in his own Patchwork instance. But when another author asked to use the tool, it could be shared with a link, just like any other document. Each collaborator could choose which tools they wanted to install based on their own needs.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/share-tool.png" alt="share-tool.png"></p>
<figcaption>Sharing a tool with a collaborator via URL. (Chat is one use case we haven't managed to migrate into Patchwork yet)</figcaption>
</figure>
<p>We used a similar approach to develop several other tools as needs arose. A “Zoomout View” proved useful for talking about the overall structure of the essay:</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/zoomout.png" alt="zoomout.png"></p>
</figure>
<p>In some cases, new tools have proven less useful. We built a “Request Tracker” that would manage assigning feedback requests to coauthors, but this tool didn’t end up getting used very much. That wasn’t a huge loss, though—it didn’t take much work to create the tool, and it was easy to ignore and uninstall.</p>
<p>Another thing we’ve found while customizing Patchwork is that <strong>AI is a useful complement to a malleable environment.</strong> We argued earlier that AI-assisted coding alone does not guarantee malleability. But <em>when combined with a malleable environment</em>, AI-assisted development can make it much faster to edit your tools.</p>
<p>We’ve used AI assistance to rapidly build many tools in Patchwork. The Section Word Counter tool mentioned above was coded with AI assistance in just a few minutes—in the middle of a writing session, without needing to set aside dedicated time.</p>
<p>A malleable environment can also provide platform capabilities that make AI-generated software more useful. For example: we have an interface for making small software tools from an AI chat. While this UI superficially resembles existing products like <a href="https://support.anthropic.com/en/articles/9487310-what-are-artifacts-and-how-do-i-use-them">Claude Artifacts</a>, the generated tools gain capabilities from existing inside of Patchwork. They automatically support persistence and multi-user collaboration, and can compose with existing tools for editing existing data.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/chess-clock.png" alt="chess-clock.png"></p>
<figcaption>
<p>A presentation timer tool built with an AI chat in Patchwork has automatic persistence and multi-user synchronization.</p>
</figcaption>
</figure>
<p>While we’ve already experienced some glimpses of malleability in Patchwork, some big challenges remain.</p>
<p><strong>One open challenge is fully applying the ideas of lightweight universal version control to code.</strong> Although we have a basic mechanism for creating a branch of a software tool, we’ve yet to flesh out a full set of capabilities around branching—such as seeing and running different branches of a tool simultaneously. And more broadly, we’ve noticed there are tricky social challenges around collaborating on software in a less centralized way. How do people negotiate whether to share the same version of a tool or go their own way with separate forks? Who makes product decisions for a given tool—does the original creator stay in charge, or do they allow anyone to freely contribute? While these questions can’t be magically resolved, we think versioning tools can help.</p>
<p>A related challenge is managing expectations of quality. Most of the tools we’ve built aren’t anywhere close to the polish level of commercial products; they’re scrappy personal tools. When someone shares a tool, how can they communicate its level of quality? There’s a difference between sharing a tool “as-is” and committing to ongoing maintenance.</p>
<p>Finally, Patchwork currently has a limited embedding model that we’d like to improve on. Tools can embed other tools, but the UI of the embedded tool gets cramped, and users need better ways to choose tools at all levels of the hierarchy. We’ve also gradually discovered many reasons why UI components need to communicate in richer ways. For instance, when working on a <a href="https://www.inkandswitch.com/patchwork/notebook/11/">generic commenting utility</a> that could work with any tool, we realized that it’s useful for tools to report the current user selection as a semantic region of a document. We’ve added hardcoded workarounds to enable these kinds of tool composition, but we’d like to find a more general and less coupled approach, perhaps inspired by approaches like entanglers and Varv we mentioned earlier.</p>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/tldraw-embed.jpg" alt="tldraw-embed.jpg"></p>
<figcaption>Embedding a text document and a spreadsheet inside a whiteboard in Patchwork</figcaption>
</figure>
<p>Patchwork is still an active project. We’ve published some early findings from our version control work in a <a href="https://www.inkandswitch.com/patchwork/notebook/">lab notebook</a>, and we plan to share more about the broader system as we make progress. Eventually we also plan to release Patchwork as an open-source tool.</p>
<h3 id="dynamic-documents"><a href="#dynamic-documents">Dynamic documents</a></h3>
<p>In our infrastructure work, we’ve innovated on how software is hosted, but the behavior itself has been specified with traditional code. And yet, we know that writing code—even with AI assistance—still represents a steep “cliff” between usage and modification.</p>
<p>Thus, <strong>another strand of our work has focused on allowing the creation of custom tools without programming</strong>. Above, we showed some prior art like spreadsheets and Hypercard which embodied the approach of starting with some directly manipulable information and optionally layering on programming.</p>
<p>We’ve followed in this tradition in our work on <strong>dynamic documents</strong>. Our goal is to explore new techniques for letting users layer behavior onto their existing media documents. We’ve aimed to figure out: how should users represent information to make it possible to enrich with interaction? And how can tools cooperate and coexist within a document as a shared fabric?</p>
<h4 id="potluck-dynamic-plaintext-for-recipe-planning"><a href="#potluck-dynamic-plaintext-for-recipe-planning">Potluck: dynamic plaintext for recipe planning</a></h4>
<p>In <a href="https://www.inkandswitch.com/potluck/">Potluck</a>, we explored how users might enrich text notes with dynamic behavior like scaling ingredients and setting timers in a recipe. Users could represent information with any text syntax they found natural, and then write detectors that would parse meaningful structure out of the text.</p>
<p>We found that Potluck could support the creation of <a href="https://www.inkandswitch.com/potluck/#gallery">many useful tools</a>, and it felt nice to jot down information in any format that felt natural.</p>
<figure>
<figcaption>Scaling a recipe in Potluck document. The behavior of the scaling slider is specified within the Potluck editor using a formula language.</figcaption>
</figure>
<p>AI assistance also <a href="https://www.youtube.com/watch?v=bJ3i4K3hefI">integrates nicely</a> into Potluck. The AI can draft a set of detectors and computation rules, while preserving the user’s ability to see and edit the generated logic themselves in a live programming environment.</p>
<figure>

<figcaption>
<p>Doubling quantities in a recipe with AI support. The generated detectors and formulas can be directly viewed and edited.</p>
</figcaption>
</figure>
<p>However, one problem with Potluck was that parsing structure from arbitrary plaintext proved cumbersome. We struggled to represent connections between related information (like containment or sequence) without resorting to complex detector rules.</p>
<h4 id="embark-dynamic-outlines-for-travel-planning"><a href="#embark-dynamic-outlines-for-travel-planning">Embark: dynamic outlines for travel planning</a></h4>
<p>In our next project, we decided to try a more structured approach. We built an editor for travel planning documents called <a href="https://www.inkandswitch.com/embark">Embark</a>, which used a hierarchical outline as the base format. The outline provided some low-level structure that resolved some of the issues with detecting complex patterns in Potluck.</p>
<p>We also added a first-class concept of structured information. An object like a Google Maps location could be “mentioned” and stored within the outline. These objects could then be used as inputs to computations like routing and weather forecasting.</p>
<p>Finally, we allowed rich views like maps and calendars to be embedded in the outline, to visualize and interact with information about the trip.</p>
<figure>
<video controls="" src="https://www.inkandswitch.com/essay/malleable-software/embark-demo.mp4"></video>
    <figcaption>Computing a driving route in an Embark document. The route calculation draws on context from the surrounding document.</figcaption>
</figure>
<p>A key takeaway from Embark was that <strong>an embedding model with rich shared context enables powerful interactions</strong>. The embedded views in Embark are deeply aware of surrounding information—a map view can read and visualize the locations in the outline. The available information depends on <em>local</em> context—you can limit what information is shown on a map by moving it to a different part of the outline. Interactions are also synchronized—hovering on a place in the map highlights it in the outline, and vice versa. The cumulative result of these mechanisms is a cohesive user experience that’s not available in a more restricted embedding scheme, like the one we have in Patchwork currently, or one website embedding another one as an iFrame.</p>
<figure>

<figcaption>In Embark, a map can interact with structured data in the outline document.</figcaption>
</figure>
<p>A major challenge we faced in both Potluck and Embark is navigating the tension between more and less formal representations of information. It’s valuable to be able to express your thoughts in an unstructured way, but computational tools typically benefit from operating on more structured representations. Furthermore, data schemas aren’t limited to user input—Google Maps locations have their own structured format that needs to be usable across various tools.</p>

<p>Finally, it remains to be seen whether the ideas of Potluck and Embark can be generalized further to handle a broader variety of tasks. We grounded each of those projects in the specific use cases of recipes and travel to limit our scope and encourage authentic use, but we plan to try generalizing these ideas further and integrating them on top of a malleable infrastructure foundation such as Patchwork.</p>
<h2 id="towards-a-better-future"><a href="#towards-a-better-future">Towards a better future</a></h2>
<figure>
<p><img src="https://www.inkandswitch.com/essay/malleable-software/desktop-workspace.jpg" alt="desktop-workspace.jpg"></p>
</figure>
<p>Over the course of this essay, we’ve laid out a vision of a different kind of computing – one that could empower people to craft their own tools, shift power towards local communities, and help people do their best work. We’ve also shared many ideas for how exactly this different kind of computing might come about, from prior art and our own work.</p>
<p>At the same time, we also don’t want to understate the challenges that lie ahead. Enormous effort over many decades has gone into refining application-centric computing, and forging a new path will require grappling with many difficult questions. We’ve covered some of the questions we see as fundamental: How can we create gentle slopes from passive use to active creation? How can we break up applications into composable tools? How can we keep the needs of collaborating groups in focus?</p>
<p>That is still a partial list, and there are big challenges we’ve mostly left out of scope in this piece.</p>
<p><strong>Privacy and security</strong>: How do we reconcile the desire for extensible software and interoperability with the reality of bad actors? When untrusted strangers are sharing modifications to existing software that can access sensitive data, dangerous things can happen.</p>

<p><strong>Business models</strong>: How would developers make money from their software if they were shipping composable tools, not monolithic applications? How is support and maintenance paid for?</p>
<p><strong>Culture</strong>: How do we cultivate a movement towards personal agency where people <em>want</em> to modify their environments, both digital and otherwise?</p>
<p>These are daunting challenges. Technical capabilities can’t be a full solution; economic and cultural shifts will also be required. But change is possible—computing is still young, it has changed a lot in the past decades, and surely many structural shifts still lie ahead.</p>
<p>Many different kinds of people can play a role. If you are a computing researcher, it’s a ripe time to reimagine the foundational metaphors of computing to put more power in users’ hands. If you are a platform developer, consider tipping the balance towards seeing end-users as capable participants rather than passive users that need to be protected at all costs. Security and ease of initial use are virtues, but extensibility and power are as well. And if you make software products, consider how you might empower your users to take matters more into their own hands, rather than dictating every decision. While we believe radical change is needed to fully realize malleable software, incremental improvements are helpful too.</p>
<p>When the people living or working in a space gradually evolve their tools to meet their needs, the result is a special kind of quality. While malleable software may lack the design consistency of artifacts crafted behind closed doors in Palo Alto, we find that over time it develops the kind of charm of an old house. It bears witness to past uses and carries traces of its past decisions, even as it evolves to meet the needs of the day.</p>
<p>Everyone deserves the right to evolve their digital environments. It’s an important way to fulfill our creative potential and maintain a sense of agency in a world that is increasingly defined in code. We hope you’ll join us in making malleable software a reality.</p>
<hr>
<p><em>To follow our ongoing research, subscribe to our <a href="https://buttondown.com/inkandswitch">email newsletter</a>.</em></p>
<p><em>We are grateful to</em>:</p>
<ul>
<li><em>Roshan Choxi, Orion Henry, Ignatius Gilfedder, Mark McGranaghan, Alexander Obenauer, Jeff Peterson, Max Schoening, Paul Shen, Paul Sonnnentag, Matt Tognetti, and Adam Wiggins, for contributions to the research projects mentioned in this essay.</em></li>
<li><em>Mary Rose Cook, Marcel Goethals, Alex Komoroske, Clemens Klokmose, Steve Krouse, Jess Martin, Andy Matuschak, Paul Sonnentag, Alex Warth, and Adam Wiggins for valuable feedback on the essay.</em></li>
<li><em>Thomas Mitchell Clark, Guitar Builder; and Nathan Peterson, Food Practitioner; for allowing us to photograph their creative spaces.</em></li>
</ul>

  </article></div>]]></description>
        </item>
    </channel>
</rss>