<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Thu, 22 Feb 2024 13:00:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Google to pause Gemini image generation of people after issues (121 pts)]]></title>
            <link>https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical</link>
            <guid>39465250</guid>
            <pubDate>Thu, 22 Feb 2024 10:19:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical">https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical</a>, See on <a href="https://news.ycombinator.com/item?id=39465250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Google has apologized for what it describes as “inaccuracies in some historical image generation depictions” with its Gemini AI tool, saying its attempts at creating a “wide range” of results missed the mark. The statement follows criticism that it depicted specific white figures (like the US Founding Fathers) or groups like <a href="https://twitter.com/JohnLu0x/status/1760170103474356519">Nazi-era German soldiers</a> as people of color, possibly as an overcorrection to <a href="https://www.vox.com/technology/23738987/racism-ai-automated-bias-discrimination-algorithm">long-standing racial bias problems</a> in AI.</p><p>“We’re aware that Gemini is offering inaccuracies in some historical image generation depictions,” says the Google statement, <a href="https://twitter.com/Google_Comms/status/1760354549481546035">posted this afternoon on X</a>. “We’re working to improve these kinds of depictions immediately. Gemini’s AI image generation does generate a wide range of people. And that’s generally a good thing because people around the world use it. But it’s missing the mark here.”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Gemini results with AI images of “an American woman,” including two white-looking women." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/376x288/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/384x294/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/415x318/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/480x367/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/540x413/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/640x490/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/750x574/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/828x633/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/1080x826/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/1200x918/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/1440x1102/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/1920x1469/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/2048x1567/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/2400x1836/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1524x1166/2400x1836/filters:focal(762x583:763x584):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297985/Screen_Shot_2024_02_21_at_2.54.00_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>My Gemini results for “generate a picture of an American woman,” one of the prompts that set off the debate of the past few days.</em></figcaption></p></div><p>Google began offering <a href="https://www.theverge.com/2024/2/1/24057438/bard-gemini-imagen-google-ai-image-generation">image generation</a> through its <a href="https://www.theverge.com/2024/2/8/24065553/google-gemini-ios-android-app-duet-bard">Gemini (formerly Bard)</a> AI platform earlier this month, matching the offerings of competitors like OpenAI. Over the past few days, however, social media posts have questioned whether it fails to produce historically accurate results in an attempt at racial and gender diversity.</p><p><a href="https://www.dailydot.com/debug/google-ai-gemini-white-people/">As the<em> Daily Dot</em> chronicles</a>, the controversy has been promoted largely — though not exclusively — by right-wing figures attacking a tech company that’s perceived as liberal. Earlier this week, a former Google employee posted on X that it’s “embarrassingly hard to get Google Gemini to acknowledge that white people exist,” showing a series of queries like “generate a picture of a Swedish woman” or “generate a picture of an American woman.” The results appeared to overwhelmingly or exclusively show AI-generated people of color. (Of course, all the places he listed do have women of color living in them, and none of the AI-generated women exist in any country.) The criticism was taken up by right-wing accounts that requested images of historical groups or figures like the Founding Fathers and purportedly got overwhelmingly non-white AI-generated people as results. Some of these accounts positioned Google’s results as part of a conspiracy to avoid depicting white people, and at least one used a coded antisemitic reference to place the blame.</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="Gemini results for “a German soldier from 1943” featuring illustrations of what appear to be a white man, a Black man, and an Asian woman." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/376x836/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/384x853/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/415x922/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/480x1067/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/540x1200/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/640x1422/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/750x1667/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/828x1840/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/1080x2400/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/1200x2667/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/1440x3200/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/1920x4267/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/2048x4551/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/2400x5333/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1080x2400/2400x5333/filters:focal(540x1200:541x1201):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25298169/1000001251.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>Gemini wouldn’t produce an image of a 1943 soldier on desktop for me, but it offered this set of illustrations to a colleague.</em></figcaption></p></div><p>Google didn’t reference specific images that it felt were errors; in a statement to <em>The Verge</em>, it reiterated the contents of its post on X. But it’s plausible that Gemini has made an overall attempt to boost diversity because of a chronic lack<em> </em>of it in generative AI. Image generators are trained on large corpuses of pictures and written captions to produce the “best” fit for a given prompt, which means they’re often prone to amplifying stereotypes. <a href="https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/">A <em>Washington Post</em> investigation</a> last year found that prompts like “a productive person” resulted in pictures of entirely white and almost entirely male figures, while a prompt for “a person at social services” uniformly produced what looked like people of color. It’s a continuation of trends that have appeared in <a href="https://www.vox.com/2018/4/3/17168256/google-racism-algorithms-technology">search engines</a> and other software systems.</p><p>Some of the accounts that criticized Google defended its core goals. “It’s a good thing to portray diversity ** in certain cases **,” <a href="https://twitter.com/JohnLu0x/status/1760170103474356519">noted one</a> person who posted the image of racially diverse 1940s German soldiers. “The stupid move here is Gemini isn’t doing it in a nuanced way.” And while entirely white-dominated results for something like “a 1943 German soldier” would<em> </em>make historical sense, that’s much less true for prompts like “an American woman,” where the question is how to represent a diverse real-life group in a small batch of made-up portraits.</p><p>For now, Gemini appears to be simply refusing some image generation tasks. It wouldn’t generate an image of Vikings for one <em>Verge</em> reporter, although I was able to get a response. On desktop, it resolutely refused to give me images of German soldiers or officials from Germany’s Nazi period or to offer an image of “an American president from the 1800s.”</p><div><div role="button" aria-label="Zoom" tabindex="0"><figure><div><p><span><img alt="A picture of Gemini results promising “some images featuring diverse US senators from the 1800s,” featuring what appear to be three women of color and an Asian American man." loading="lazy" decoding="async" data-nimg="fill" sizes="(max-width: 1023px) 100vw, 744px" srcset="https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/376x263/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 376w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/384x269/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 384w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/415x291/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 415w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/480x336/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 480w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/540x378/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 540w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/640x448/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 640w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/750x525/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 750w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/828x580/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 828w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/1080x756/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 1080w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/1200x840/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 1200w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/1440x1008/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 1440w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/1920x1344/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 1920w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/2048x1434/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 2048w, https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/2400x1681/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png 2400w" src="https://duet-cdn.vox-cdn.com/thumbor/0x0:1668x1168/2400x1681/filters:focal(834x584:835x585):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/25297980/Screen_Shot_2024_02_21_at_3.10.17_PM.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></span></p></div></figure></div><p><figcaption><em>Gemini’s results for the prompt “generate a picture of a US senator from the 1800s.”</em></figcaption></p></div><p>But some historical requests still do end up factually misrepresenting the past. A colleague was able to get the mobile app to deliver a version of the “German soldier” prompt — which exhibited the same issues described on X.</p><p>And while a query for pictures of “the Founding Fathers” returned group shots of almost exclusively white men who vaguely resembled real figures like Thomas Jefferson, a request for “a US senator from the 1800s” returned a list of results Gemini promoted as “diverse,” including what appeared to be Black and Native American women. (The <a href="https://www.senate.gov/artandhistory/senate-stories/rebecca-felton-and-one-hundred-years-of-women-senators.htm">first female senator</a>, a white woman, served in 1922.) It’s a response that ends up erasing a real history of race and gender discrimination — “inaccuracy,” as Google puts it, is about right.</p><p><em>Additional reporting by Emilia David</em></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[YouTube dominates TV streaming in US, per Nielsen's latest report (136 pts)]]></title>
            <link>https://techcrunch.com/2024/02/20/youtube-dominates-tv-streaming-in-u-s-per-nielsens-latest-report/</link>
            <guid>39463296</guid>
            <pubDate>Thu, 22 Feb 2024 04:34:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://techcrunch.com/2024/02/20/youtube-dominates-tv-streaming-in-u-s-per-nielsens-latest-report/">https://techcrunch.com/2024/02/20/youtube-dominates-tv-streaming-in-u-s-per-nielsens-latest-report/</a>, See on <a href="https://news.ycombinator.com/item?id=39463296">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				<p id="speakable-summary">Nielsen today <a href="https://www.nielsen.com/insights/2024/colder-weather-and-nfl-playoffs-drive-increased-tv-usage-in-january/" target="_blank" rel="noopener">released</a> its January report on viewing usage across linear TV and streaming, which revealed that YouTube is once again the overall top streaming service in the U.S., with 8.6% of viewing on television screens. Netflix, meanwhile, saw 7.9% of TV usage. The new data points to YouTube’s dominance in the TV streaming arena and marks 12 consecutive months of the platform being in the top spot.</p>
<p>In a <a href="https://blog.youtube/news-and-events/youtube-number-one-streamer-nielsen/" target="_blank" rel="noopener">blog post</a> celebrating the achievement, the Google-owned streaming service announced that viewers now watch a daily average of over 1 billion hours of YouTube content on their televisions, which could indicate that there’s a preference for user-generated videos among U.S. consumers rather than traditional TV shows. <a href="https://www.inmobi.com/company/press/gen-z-significantly-prefers-user-generated-content-older-millennials-lean-toward-streaming-inmobi-insights-survey-shows" target="_blank" rel="noopener">Sixty-one percent</a> of Gen Z reported that they favor user-generated content over other content formats.</p>
<p>Plus, creators are seeing an increase in viewership come from TVs. According to the company, the number of top YouTubers that receive the most watch time on TVs has sky-rocketed by more than 400%. <a href="https://www.youtube.com/@hopescope" target="_blank" rel="noopener">HopeScope</a>, a creator who reviews viral products, notably saw a 172% jump in TV watch time in 2023. This could be good news for family-friendly YouTubers who have to compete with TikTok for kids’ attention. Children ages 4 through 18 spent a global average of <a href="https://techcrunch.com/2024/01/25/kids-spent-60-more-time-on-tiktok-than-youtube-last-year-20-tried-openais-chatgpt/">112 minutes daily</a> on TikTok in 2023, per a recent study.</p>
<p>Although YouTube may have precedence in the living room, TikTok continues to dominate on mobile devices. The short-form video app recently began <a href="https://techcrunch.com/2024/01/24/tiktok-testing-30-minute-uploads-inch-into-youtubes-territory/">testing</a> the ability for TikTokers to upload 30-minute videos, which could step on YouTube’s toes. <a href="https://techcrunch.com/2024/02/15/tiktok-launches-a-reimagined-app-for-the-apple-vision-pro/">TikTok</a> also entered the spatial reality space, launching a native app on the Apple Vision Pro. Meanwhile, <a href="https://techcrunch.com/2024/01/19/youtube-and-other-apps-missing-from-apple-vision-pro-launch/">YouTube</a> decided to not build a dedicated app for the device.</p>
<p>YouTube has reached a few other milestones in recent months, including the <a href="https://techcrunch.com/2024/02/01/google-says-youtube-now-has-over-100-million-subscribers/">100 million users</a> who pay for YouTube Music and YouTube Premium. Additionally, <a href="https://blog.youtube/inside-youtube/2024-letter-from-neal/" target="_blank" rel="noopener">YouTube TV</a> now has more than 8 million subscribers and YouTube Shorts recently achieved over 70 billion daily views.</p>

			</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things unexpectedly named after people (279 pts)]]></title>
            <link>https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/</link>
            <guid>39462516</guid>
            <pubDate>Thu, 22 Feb 2024 02:41:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/">https://notes.rolandcrosby.com/posts/unexpectedly-eponymous/</a>, See on <a href="https://news.ycombinator.com/item?id=39462516">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        <div>
            <p>
                <h2>Things Unexpectedly Named After People</h2>
            </p>
            <p><h5>July 19, 2020</h5></p>
        </div>
        <article>
            <p>An incomplete and infuriating list.</p>
<ul>
<li><a href="https://web.archive.org/web/20010715123343/https://www.google.com/press/funfacts.html" target="_blank">PageRank</a> (Larry Page)</li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/history.html" target="_blank">MySQL</a> (My Widenius)</li>
<li><a href="https://en.wikipedia.org/wiki/Debian#Founding_%281993%e2%80%931998%29" target="_blank">Debian</a> (two people named Deb and Ian)</li>
<li><a href="http://www.sfgate.com/business/article/Stories-behind-Bay-Area-tech-company-names-6465268.php#photo-8229264" target="_blank">New Relic</a> (anagram of Lew Cirne)</li>
<li><a href="https://en.wikipedia.org/wiki/Price_Club" target="_blank">Price Club</a> (Sol Price)</li>
<li><a href="https://en.wikipedia.org/wiki/Taco_Bell" target="_blank">Taco Bell</a> (Glen Bell)</li>
<li><a href="https://en.wikipedia.org/wiki/Westlake,_Los_Angeles#History" target="_blank">Westlake, Los Angeles</a> (Henricus Wallace Westlake)</li>
<li><a href="https://en.wikipedia.org/wiki/Outerbridge_Crossing" target="_blank">Outerbridge Crossing</a> (Eugenius Harvey Outerbridge)</li>
<li><a href="http://mountainviewpeople.blogspot.com/2007/09/charles-main-1817-1906.html" target="_blank">Main Street, San Francisco</a> (Charles Main)</li>
<li><a href="https://en.wikipedia.org/wiki/Erlang_%28programming_language%29#History" target="_blank">Erlang</a> (Agner Erlang)</li>
<li><a href="https://en.wikipedia.org/wiki/Mars,_Incorporated" target="_blank">Mars (candy)</a> (Franklin Mars)</li>
</ul>
<p><a href="https://news.ycombinator.com/item?id=23890161" target="_blank">A few additions</a> that others have suggested:</p>
<ul>
<li><a href="http://www.jerrystewart.org/snowflakeaz/history.html" target="_blank">Snowflake, AZ</a> (two people named Snow and Flake)</li>
<li><a href="https://en.wikipedia.org/wiki/Lake_Mountain_%28Victoria%29" target="_blank">Lake Mountain</a> (George Lake)</li>
<li><a href="https://en.wikipedia.org/wiki/German_chocolate_cake" target="_blank">German chocolate cake</a> (Samuel German)</li>
<li><a href="https://en.wikipedia.org/wiki/Baker%27s_Chocolate" target="_blank">Baker’s Chocolate</a> (Walter Baker)</li>
<li><a href="https://en.wikipedia.org/wiki/Unilever" target="_blank">Unilever</a> (Lever Brothers)</li>
</ul>
<p>Even more, after this made the rounds again in 2024:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Brownian_noise" target="_blank">Brown noise</a> (Robert Brown)</li>
<li><a href="https://en.wikipedia.org/wiki/Southern_blot" target="_blank">Southern blot</a> (Edwin Southern)</li>
<li><a href="https://en.wikipedia.org/wiki/Smart_%26_Final" target="_blank">Smart &amp; Final</a> (Jim Smart, Hildane Final)</li>
<li><a href="https://en.wikipedia.org/wiki/Elo_rating_system" target="_blank">Elo</a> (Arpad Elo)</li>
<li><a href="https://en.wikipedia.org/wiki/Shrapnel_shell" target="_blank">Shrapnel</a> (Henry Shrapnel)</li>
<li><a href="https://en.wikipedia.org/wiki/Leatherman" target="_blank">Leatherman</a> (Timothy Leatherman)</li>
<li><a href="https://en.wikipedia.org/wiki/Max_Factor" target="_blank">Max Factor</a> (Max Factor)</li>
<li><a href="https://en.wikipedia.org/wiki/BernzOmatic" target="_blank">BernzOmatic torches</a> (Otto Bernz)</li>
<li><a href="https://www.dupageforest.org/places-to-go/forest-preserves/waterfall-glen" target="_blank">Waterfall Glen</a> (Seymour Waterfall)</li>
<li><a href="https://en.wikipedia.org/wiki/Gasoline#Etymology" target="_blank">Gasoline</a> (John Cassell)</li>
<li><a href="https://en.wikipedia.org/wiki/ColorBrewer" target="_blank">ColorBrewer</a> (Cynthia Brewer)</li>
</ul>
<p>Unfortunately, <a href="https://en.wikipedia.org/wiki/Rocket_Lab" target="_blank">Rocket Lab</a> does not seem to be named after seed investor/aerospace entrepreneur <a href="https://www.christchurchnz.com/meet/tuwhana/mark-rocket" target="_blank">Mark Rocket</a>.</p>
        </article>
        
    


    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Japan's Nikkei surpasses 1989 all-time high (160 pts)]]></title>
            <link>https://www.cnbc.com/2024/02/22/japans-nikkei-hits-all-time-high-on-reforms-robust-corporate-earnings.html</link>
            <guid>39462253</guid>
            <pubDate>Thu, 22 Feb 2024 02:06:29 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2024/02/22/japans-nikkei-hits-all-time-high-on-reforms-robust-corporate-earnings.html">https://www.cnbc.com/2024/02/22/japans-nikkei-hits-all-time-high-on-reforms-robust-corporate-earnings.html</a>, See on <a href="https://news.ycombinator.com/item?id=39462253">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-107374588" data-test="InlineImage"><p>Pedestrians walking across with crowded traffic at Shibuya crossing square.</p><p>Jaczhou | E+ | Getty Images</p></div><div><p>Japan's Nikkei 225 hit a record high Thursday, powered by banking, electronics and consumer stocks as robust earnings and investor-friendly measures fuel a blistering rally in Japanese equities this year.</p><p>The <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/.N225/">Nikkei 225</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> jumped nearly 2% to hit 39,029, surpassing the previous record high of 38,915.87 reached in 1989.</p><p>Both the Nikkei and the broader Topix have been standout outperformers in Asia Pacific, up more than 10% so far this year after surging more than 25% in 2023 — their respective best annual gains in at least a decade.</p><p>Japan Inc's solid third-quarter corporate earnings have prompted Bank of America equity strategists to upgrade their 2024 year-end forecasts for the Nikkei 225 to 41,000 from 38,500. They raised their forecasts for the Topix to 2,850 from 2,715.</p><p>The rally has also been supported by a weaker yen, which has shed about 6% against the dollar so far this year and seems on track to drop to to 33-year lows touched late last year.</p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Nikkei since December 1989</p></div><div><p>Investors have been pouring funds into Japanese equities, taking the lead of Warren Buffet's <a href="https://www.cnbc.com/2023/06/13/investing-is-japan-inc-finally-serious-about-corporate-governance-.html">bullish calls on Japan</a> and cheering the Japanese government's push towards greater corporate governance reforms — with the aim of compelling Japan Inc to <a href="https://www.cnbc.com/2024/01/12/japan-wants-higher-returns-for-investors-heres-what-its-doing.html">boost shareholder returns</a>.</p><p>Data from the Tokyo Stock Exchange showed foreigners <a href="https://www.jpx.co.jp/english/markets/statistics-equities/investor-type/u5j7e50000001n5w-att/stock_val_1_m2401.pdf" target="_blank">invested </a>more than 2 trillion yen in the exchange's "prime" offerings — its largest and most liquid stocks — in January.</p></div><div id="Placeholder-ArticleBody-Video-107371888" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000331268" aria-labelledby="Placeholder-ArticleBody-Video-107371888"><p><img src="https://image.cnbcfm.com/api/v1/image/107371889-17075202531707520250-33268238068-1080pnbcnews.jpg?v=1707520252&amp;w=750&amp;h=422&amp;vtcrop=y" alt="Off the Charts: Nikkei hits 34-year high"><span></span><span></span></p></div><div><p>Nikkei <a href="https://www.nikkei.com/article/DGXZQOUC13CTD0T10C24A2000000/?Fds-Load-Behavior=force-external" target="_blank">reported last week</a> net profits of listed companies in Japan for the fiscal year ending March 2024 could reach a record high for the third consecutive year.</p><p>This comes on the back of record quarterly earnings for the October-December period, which have increased 45% from the same period a year earlier and are 14% higher than consensus estimates, according to Goldman Sachs analysts.</p><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-6"><a href="https://www.cnbc.com/quotes/7203.T-JP/">Toyota</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, the world's largest car manufacturer, was among several Japanese companies to <a href="https://global.toyota/pages/global_toyota/ir/financial-results/2024_3q_presentation_en.pdf" target="_blank">upgrade its earnings forecast</a>, which includes a bigger profit margin and stronger revenue.</p></div><h2><a id="headline0"></a>Weak yen, strong stocks</h2><div><p>Recent gains in the stock markets have come against the backdrop of a weakening Japanese yen, last at 150.40 against the dollar, driven largely by the <a href="https://www.cnbc.com/2024/02/16/boj-juggles-yen-weakness-and-fragile-growth-after-gdp-surprise.html">divergence between between high U.S. interest rates and Japan's ultra easy policy</a>.</p><p>Japanese Finance Minister Shunichi Suzuki was the latest in a string of several government officials to articulate his concern on the weakening yen on Friday and reportedly said he was watching the currency's moves with a sense of "urgency."</p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Japanese yen/U.S. dollar</p></div><div><p>While the yen's chronic weakness has boosted some of Japan's exporters, it has diminished the purchasing power of consumers in Japan.</p><p>Yet the Bank of Japan has maintained the world's last negative rates regime despite "core core inflation" — which excludes food and energy prices — exceeding its 2% target for more than a year.</p><p>Market participants expect the BOJ to move away from its negative rates regime at its April policy meeting, once the annual spring wage negotiations confirm a trend of meaningful wage increases.</p></div><div><p><a href="https://www.cnbc.com/2024/02/16/boj-juggles-yen-weakness-and-fragile-growth-after-gdp-surprise.html"><img src="https://image.cnbcfm.com/api/v1/image/107374024-1707973690159-gettyimages-1218761211-137japan.jpeg?v=1707985039&amp;w=160&amp;h=90" alt="An editorialized picture of a falling graph against the Japanese flag."></a></p></div><div><p>The central bank believes wage increments would translate into a more meaningful&nbsp;spiral, encouraging consumers to spend.</p><p>But prolonged high inflation rates have hit domestic consumption — a key reason why Japan's GDP shrank for a second consecutive quarter, confounding analysts that had expected a small expansion in Japan's economy. It also meant that Japan ceded its place as the world's third-largest economy to Germany.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[OpenAI postmortem – Unexpected responses from ChatGPT (254 pts)]]></title>
            <link>https://status.openai.com/incidents/ssg8fh7sfyz3</link>
            <guid>39462087</guid>
            <pubDate>Thu, 22 Feb 2024 01:42:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://status.openai.com/incidents/ssg8fh7sfyz3">https://status.openai.com/incidents/ssg8fh7sfyz3</a>, See on <a href="https://news.ycombinator.com/item?id=39462087">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>On February 20, 2024, an optimization to the user experience introduced a bug with how the model processes language.</p>

<p>LLMs generate responses by randomly sampling words based in part on probabilities. Their “language” consists of numbers that map to tokens.</p>

<p>In this case, the bug was in the step where the model chooses these numbers. Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. More technically, inference kernels produced incorrect results when used in certain GPU configurations.</p>

<p>Upon identifying the cause of this incident, we rolled out a fix and confirmed that the incident was resolved.</p>

              </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Apple reverses course: app must pay 30% App Store fee on tips sent to teachers (122 pts)]]></title>
            <link>https://9to5mac.com/2024/02/21/insight-timer-meditation-app-store-rules/</link>
            <guid>39461226</guid>
            <pubDate>Wed, 21 Feb 2024 23:40:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://9to5mac.com/2024/02/21/insight-timer-meditation-app-store-rules/">https://9to5mac.com/2024/02/21/insight-timer-meditation-app-store-rules/</a>, See on <a href="https://news.ycombinator.com/item?id=39461226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
					
<figure>
	<img width="1600" height="800" src="https://9to5mac.com/wp-content/uploads/sites/6/2023/05/app-store-money-1.jpg?quality=82&amp;strip=all&amp;w=1600" alt="Apple App Store money" srcset="https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/05/app-store-money-1.jpg?w=320&amp;quality=82&amp;strip=all&amp;ssl=1 320w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/05/app-store-money-1.jpg?w=640&amp;quality=82&amp;strip=all&amp;ssl=1 640w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/05/app-store-money-1.jpg?w=1024&amp;quality=82&amp;strip=all&amp;ssl=1 1024w, https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2023/05/app-store-money-1.jpg?w=1500&amp;quality=82&amp;strip=all&amp;ssl=1 1500w" decoding="async" fetchpriority="high"></figure>

<p>Apple has reversed course and is now demanding a 30% cut of donations sent to teachers in the popular <a href="https://insighttimer.com/" target="_blank" rel="noreferrer noopener">Insight Timer meditation app</a>. The change apparently comes after Apple allowed the app to use its own Stripe-based in-app purchase system for over a year. </p>



<p>As detailed in a <a href="https://techcrunch.com/2024/02/21/in-a-reversal-apple-is-now-demanding-30-of-the-donations-to-meditation-app-insight-timers-teachers/" target="_blank" rel="noreferrer noopener">report from TechCrunch</a>, Insight Timer’s allows it users to send “tips” to their favorite teachers and instructors. In its App Store Guidelines, Apple has <a href="https://developer.apple.com/app-store/review/guidelines/#in-app-purchase" target="_blank" rel="noreferrer noopener">specific rules about this</a>: </p>



<blockquote>
<p>Apps may enable individual users to give a monetary gift to another individual without using in-app purchase, provided that (a) the gift is a completely optional choice by the giver, and (b) 100% of the funds go to the receiver of the gift. However, a gift that is connected to or associated at any point in time with receiving digital content or services must use in-app purchase.</p>
</blockquote>



<p>Since 2022, Insight Timer has used Stripe to power its platform that allows users to send those tips, therefore not paying a 30% commission to Apple. “Apple appeared to have blessed this use case, as the tech giant went on to approve 47 more updates to Insight Timer’s app over the course of a 12-month period,” TechCrunch explains. </p>



<p>In late 2023, however, Insight Timer was told by App Store review that those tips were now considered “digital content.” This means Insight Timer will have to use Apple’s In-App Purchase system rather than Stripe, and pay a 30% cut to Apple itself. the </p>



<p>As the report explains, Insight Timer’s business is based around a subscription model. Apple’s decision to require those “tips” to use In-App Purchase has no effect on its finances. Instead, Insight Meditation’s teachers are the ones affected: </p>



<blockquote>
<p>Late last year, those approvals stopped. An app reviewer told Insight Timer that these donations were no longer considered monetary gifts — they were now “digital content.” That meant they were also now subject to Apple’s commissions. This decision doesn’t hurt Insight Timer’s bottom line, as the app’s main business is subscriptions. Instead, it hurts the community of teachers who generate additional funds via users’ donations. Now, Apple demands a 30% of that revenue, which means the teachers are getting a 30% pay cut overnight, so to speak.</p>
</blockquote>



<p>In an interview with TechCrunch, Insight Timer CEO Christopher Plowman said the company ultimately has no choice but to comply with Apple. Apple gave Plowman a deadline of February to comply with the guideline, or else have Insight Timer removed from the App Store. The two sides went back and forth in search of a compromise, but ultimately couldn’t come to a conclusion. </p>



<p>Plowman also said in the interview that Insight Timer users donated an average of $100,000 per month to the platform’s teachers. </p>



<blockquote>
<p>“So in the end, I agreed,” Plowman told TechCrunch in an interview. “I don’t want to pick a fight with Apple — I’m not picking a fight with Apple. I think this problem is largely about regulators not stepping in. Apple’s a public company, but it has shareholders — it’s doing what it’s allowed to do,” he says.</p>
</blockquote>



<p>Apple hasn’t yet commented on this story. </p>
	<p>
		<a target="_blank" rel="nofollow" href="https://news.google.com/publications/CAAqBggKMLOFATDAGg?hl=en-US&amp;gl=US&amp;ceid=US:en">
			<em>Add 9to5Mac to your Google News feed.</em>&nbsp;
					</a>
	</p>
	<p><em>FTC: We use income earning auto affiliate links.</em> <a href="https://9to5mac.com/about/#affiliate">More.</a></p>				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Tell HN: To be honest.. GPT copilots aren't that great for programming (128 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39460788</link>
            <guid>39460788</guid>
            <pubDate>Wed, 21 Feb 2024 22:56:01 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39460788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39461015"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461015" href="https://news.ycombinator.com/vote?id=39461015&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I think of them as a more intelligent autocomplete. I don’t lean on them too heavily but I find that they make my life 5% easier by autocompleting based on style and known names of things versus relying wholly on the LSP. (Copilot)<p>On the GPT-4 side I’ve had great luck with dealing with complex SQL/BigQuery queries. I will explain a problem, offer my schema or a psql trigger and my goals on how to augment it and it’s basically spot on every time. Helps me when I know what I want to do but don’t know precisely how to achieve it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39462647"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39462647" href="https://news.ycombinator.com/vote?id=39462647&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>This feels more or less in line with what my team has found, we gave everyone a copilot seat in our Github org, and anecdotally speaking it feels like we've seen roughly a 5-10% increase in productivity. This is of course self reported and not measured against any metrics. Assuming we're right about that, its an easy sell when we account for what our internal hourly rate is.<p>we also found the same as the OP, It's good for simple problems or boilerplate, not great for more complex problems.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39463225"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39463225" href="https://news.ycombinator.com/vote?id=39463225&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>what is the cost of analyzing the proposed solution and fixing it so it does what you want and how does that compare to not using AI and writing the code yourself?</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461211"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461211" href="https://news.ycombinator.com/vote?id=39461211&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I think the real power will come when generative models get combined with e.g. refinement types, which are more or less analogous to contracts. Imagine, you decompose a problem into some functions with some contracts, and you get implementations for free. Plus, they will be guaranteed to match the specification!<p>I was discussing this with GitHub when they were hiring for Copilot, but understandably they wanted to get the basic functionality right first. I think it is the next step, and a very interesting topic for a startup or OpenAI et al. to tackle. It has the potential to make programming both more robust and faster, possibly bringing us closer to the correctness levels of classical engineering disciplines.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461446"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461446" href="https://news.ycombinator.com/vote?id=39461446&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Me too. I'm writing a compiler for fun and its extremely helpful to have it auto complete entire simple functions, like converting AST nodes to a string representation for example.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461086"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461086" href="https://news.ycombinator.com/vote?id=39461086&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>a more intuitive search as well. i am going to fly to paris in april and wanted to know what time it would be in denver when i landed. chatgpt was better-suited to the task than a multi-step duckbingle search</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461212"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461212" href="https://news.ycombinator.com/vote?id=39461212&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>i find the inline suggestions of copilot distracting in any but the most mundane of cases. I know you can disable them but it still feels like it should be enabled instead of hotkeyed</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461060"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461060" href="https://news.ycombinator.com/vote?id=39461060&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Yeah I think this captures my experience well. Not so much copilot as administrative assistant for my editor.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461112"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461112" href="https://news.ycombinator.com/vote?id=39461112&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I've stopped trying to use GPTs for complex tasks like what you describe, but I find them to be invaluable for getting a lot of grunt work done on my hobby projects.<p>As a concrete example: GitHub Copilot has been absolutely life-changing for working on hobby programming language projects. Building a parser by hand consists of writing many small, repetitive functions that use a tiny library of helper functions to recursively process tokens. A lot of people end up leaning on parser generators, but I've never found one that isn't both bloated and bad at error handling.</p><p>This is where GitHub Copilot comes in—I write the grammar out in a markdown file that I keep open, build the AST data structure, then write the first few rules to give Copilot a bit of context on how I want to use the helper functions I built. From there I can just name functions and run Copilot and it fills in the rest of the parser.</p><p>This is just one example of the kind of task that I find GPTs to be very good at—tasks that necessarily have a lot of repetition but don't have a lot of opportunities for abstraction. Another one that is perhaps more common is unit testing—after giving Copilot one example to go off of, it can generate subsequent unit tests just from the name of the test.</p><p>Is it essential? No. But it sure saves a lot of typing, and is actually <i>less</i> likely than I am to make a silly mistake in these repetitive cases.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39461199"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461199" href="https://news.ycombinator.com/vote?id=39461199&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Agreed, GitHub Copilot is fantastic if you give it something to work off of and outsource repetitive tasks to it. You still need to babysit it a little bit (it made a very subtle bug once that took a while to figure out), but it does a great job of generating code based on the other things you've been doing. It's a great little assistant to make coding less tedious.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39461250"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39461250" href="https://news.ycombinator.com/vote?id=39461250&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>On the other hand, if you are writing repetitive code - are you missing an easy abstraction? Or if not, some easy code gen? If I find myself starting to write repetitive code It usually means I did something wrong.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39461325"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39461325" href="https://news.ycombinator.com/vote?id=39461325&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>This is why I specifically identified that there there are cases where there is no reasonable abstraction. DRY has become so much of a mantra in the industry that it's taken for granted that any repetition must be unnecessary, but there are times where that's just wrong.<p>The first example I gave is parsers—even after you've factored out as many helper functions as you can, you'll eventually hit a floor where you have to use those helper functions and piece the results together. That floor is necessarily repetitive—you end up with a bunch of 5 to 10 line functions calling the abstractions and feeding the results together.</p><p>Unit tests is another example: I find that when people get too DRY in unit tests it just ends up obfuscating what the test is testing and makes changing the program later harder. Same as with parsing, there is a floor for how much abstraction is reasonable and once you have hit that floor you still have to go through the rote repetition of stringing functions together and asserting things about the results.</p><p>With a parser you actually have to do this, but with unit tests people will as often as not give up on the tedium and just not cover all the edge cases. Copilot enables developers to stop writing the repetitive code and spend their unit testing time thinking of edge cases.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461300"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39461300" href="https://news.ycombinator.com/vote?id=39461300&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>This has more to do with the fact that copilot by nature is additive and not editing your code. There was a recent report about how code quality is dropping in codebases because of the repetitive code being introduced by copilot.<p>The next evolution as you are saying would be to detect if its a repetitive code and modularise it or refactor it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39462967"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39462967" href="https://news.ycombinator.com/vote?id=39462967&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>You can do this today with the VS Code extension. There’s a context menu for copilot to do things like “explain this code”. I asked it to make my current class more DRY and it refactored validation and parsing for a dozen or so arguments. It had one minor misunderstanding of my intent which I fixed by hand.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461587"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39461587" href="https://news.ycombinator.com/vote?id=39461587&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Copilot <i>is</i> the easy code gen in this case. It's even surprisingly good at Rust macros.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39461865"><td><table>  <tbody><tr>    <td indent="4"><img src="https://news.ycombinator.com/s.gif" height="1" width="160"></td><td>
      <center><a id="up_39461865" href="https://news.ycombinator.com/vote?id=39461865&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Yes! It's surprisingly good at Rust in general, and does a better job with macros than intellisense does.</span></p></div></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39461092"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461092" href="https://news.ycombinator.com/vote?id=39461092&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I believe the mistake people use with copilot is that they attempt to write large projects or functions when they lack the knowledge base of the underlying technology.<p>I prefer to use it as more of an autocomplete on a line per line basis when writing new code.</p><p>Typically, I use it for small and concise chunks of code that I already fully understand, but save me time. Things like "Here's 30 lines of text, give me a regex that will match them all" or "Unroll/rewrite this loop utilizing bit shifting".</p><p>I also use copilot as a teacher. Like to quickly grok assembly code or code in languages that I do not use everyday. Or having a back and forth conversation with copilot chat on a specific technology I want to use and don't fully understand. Copilot chat makes an excellent rubber duck when working through issues.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39462199"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39462199" href="https://news.ycombinator.com/vote?id=39462199&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>He’s not using copilot though, and I think that is part of the problem.<p>Copilot gives me what I need to scaffold everything I am building.</p><p>Asking ChatGPT questions is good for kicking around ideas, but little more.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39462341"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39462341" href="https://news.ycombinator.com/vote?id=39462341&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Having a conversation with it is kind of amazing, perhaps underrated.<p>A very long series of questions can totally brief you on tech you don’t understand or have a base in.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39463230"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39463230" href="https://news.ycombinator.com/vote?id=39463230&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Unfortunately the hallucinations make it problematic. Yesterday I was working in a chart library I had never used and Copilot was adamant in convincing me to use non-existing methods. You can question it, but when it's backtracking you have no idea if you triggered it or if the information was wrong.<p>At least with practical implementations you can still verify the output through tests or trial and error but it becomes even more fragile when asking about facts or knowledge.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39460973"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39460973" href="https://news.ycombinator.com/vote?id=39460973&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I was writing a sudoku solver today and there was a bug that took me a while to track down (can't remember exactly how long - could be a few minutes to a couple of hours).<p>I asked ChatGPT to find the bug and it didn't find it. I also asked GPT4-Turbo to find the bug and it also couldn't find it. In the end I found the bug manually using tracing prints.</p><p>After I found the bug, I wondered if GPT4 could have found it so I gave the buggy code to GPT4 and it found the line with the bug instantly.</p><p>To me this shows that GPT4 is much better than GPT4-Turbo and GPT-3.5
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39461050"><td></td></tr>
                  <tr id="39460986"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39460986" href="https://news.ycombinator.com/vote?id=39460986&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>It's like the story of the amazing singing dog. Not amazing because he sings well (he doesn't) but because he sings at all.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461070"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461070" href="https://news.ycombinator.com/vote?id=39461070&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span><i>&gt; Who exactly are copilots for if not beginners?</i><p>The thing is: in software engineering, you're very often "a beginner" when using new technology or operating outside your familiar domain. In fact, you need to learn constantly just to stay in the business.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39463211"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39463211" href="https://news.ycombinator.com/vote?id=39463211&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Having some "AI"  that generates code in your IDE is roughly equivalent to copy-pasting code from the first google hit. Great for search/autocomplete/filling out obvious patterns. The grunt work. But terrible for doing anything novel or interesting - which presumably is the entire reason you have a software job.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461138"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461138" href="https://news.ycombinator.com/vote?id=39461138&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I get a tremendous amount of value from ChatGPT, like you said for things where I would previously have to google -&gt; stack overflow it's incredibly useful. It works as a insanely good search/autocomplete and that is worth a ton. I love being able to sketch a function with an example input/output and have it return something correct, or at least close 95% of the time. As an experienced dev it's easy for me to look at it and get to 100%.<p>It's also so helpful to be able to just ask questions of the documentation on popular projects, whether it be some nuance of the node APIs or a C websockets library, it saves me countless hours of searching and reading through documentation. Just being able to describe what I want and have it suggest some functions to paste into the actual documentation search bar is invaluable.</p><p>Similarly I find it's really helpful when trying to prototype things, the other day I needed to drop an image into a canvas. I don't remember off top exactly how to get a blob out of an .ondrop (or whatever the actual handler is) and I could find it with a couple minutes of google and MDN/SO, but if I ask ChatGPT "write me a minimal example for loading a dropped image into a canvas" I get the exact thing I want in 10 seconds and I can just copy paste the relevant stuff into MDN if I need to understand how the actual API works.</p><p>I think you're just using it wrong, and moreover I think it's MUCH MUCH more useful as an experienced engineer than as a beginner. I think I get way more mileage out of it than some of my more junior friends/colleagues because I have a better grasp on what questions to ask, and I can spot it being incorrect more readily. It feels <i>BAD</i> to be honest, like it's further stratifying the space by giving me a tool that puts a huge multiplier on my experience allowing me to work much faster than before and leaving those who are less experienced even further behind. I fear that those entering the space now, working with ChatGPT will learn less of the fundamentals that allow me to leverage it so effectively, and their growth will be slowed.</p><p>That's not to say it can't be an incredibly powerful learning tool for someone dedicated to that goal, but I have some fear that it will result in less learning "through osmosis" because junior devs won't be forced into as much of the same problem solving I had to do to be good enough, and perhaps this will allow them to coast longer in mediocrity?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39461951"><td></td></tr>
                  <tr id="39462885"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39462885" href="https://news.ycombinator.com/vote?id=39462885&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I agree, they aren't great at programming, but I've found value from LLMs for rubber-ducking and quickly getting into new framework / library / language by using your existing knowledge and let the LLM augment that for you. "How would I do X using Y", "Is this performant, and does it have these pitfalls". It works surprisingly well even though the output may not be 100% correct, but it will get you started faster / point you usually at the correct direction. Also used chatgpt to generate marketing text which I absolutely cant write. [1]<p>1: <a href="https://mouselocker.cloudef.pw/" rel="nofollow">https://mouselocker.cloudef.pw/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461117"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461117" href="https://news.ycombinator.com/vote?id=39461117&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>GPT-4 was addictive for me. I subscribed to replace online language classes and it was an excellent instructor. After passing the exam I unsubscribed and I'm living fine with 3.5. My screen time definitely lowered :) I bought $5 of GPT-4 API credits to use when 3.5 really can't do the job, but it rarely happens. Asking MS Copilot is also another great way to use GPT-4 for free. On the job I mostly use GH Copilot for code completion and it's great as it provides suggestions that are in line with the code style of my team. On serious tasks all chat bots allucinate and I also feel I'm spending as much time correcting them as if I studied the topic from scratch, because I (want to?) believe what they say but end up wasting a lot of time fixing their suggestions. I'm also thinking about SQL, today it suggested me an `UPDATE table JOIN another_table SET column` and I was surprised I could use JOINs in update statements, but the bot was so sure it was the right keyword. I tried to understand where _else_ the syntax error could be, until I turned back to postgres official documentation and verified there's no JOIN, only FROM, just like I remembered.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461616"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461616" href="https://news.ycombinator.com/vote?id=39461616&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Not alone. I was pretty grouchy about it a few months ago. It seems to be getting better, though.<p>I code all over the stack, usually some bizarre mix of python, pyspark, SQL, and typescript.</p><p>TS support seems pretty nice, and it can optimize and suggest pretty advanced things accurately.</p><p>Py was hopeless a few months ago, but my last few attempts have been decent. I've been sent down some rabbitholes though, and been burned -- usually my not paying attention and being a lazy coder.</p><p>PySpark is just the basics, which is fine if I am distracted and just want to do some basic EMR work. More likely, though, I'll rummage my own code snippets instead.</p><p>The speed of improvement has been impressive. I'm getting enthused about this stuff more and more. :)</p><p>Plus, who doesn't enjoy making random goofy stuff in Dall-E while waiting for some progressbar to advance? That alone is worth the time investment for me.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39463042"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39463042" href="https://news.ycombinator.com/vote?id=39463042&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>My personal reason for not using GitHub Copilot / etc:<p>I was testing ChatGPT-3.5 with F# in 2023 and saw some really strange errors. Turns out it was shamelessly copying from GitHub repos that had vaguely related code to what I was asking - this was easy to discover because there's not much F# out there. In fact the relative sparsity of F# is precisely why GPT-3.5 had to plagiarize! It did not take long to find a prompt that spat out ~300 lines verbatim from my own F# numerics library. (I believe this problem is even worse for C numeric programmers, whose code and expertise is much more valuable than anything in .NET.) OpenAI's products are simply unethical, and I am tired of this motivated reasoning which pretends automated plagiarism is a-okay as long as you personally find it convenient.</p><p>But even outside of plagiarism I am really nervous about the future of software development with LLMs. So often I see people throwing around stats like "we saw a 10% increase in productivity" without even mentioning code quality. There are some early indications that productivity gains in LLM code assistance are paid for by more bugs and security holes - nothing that seems catastrophic, but hardly worth dismissing entirely. What is frustrating is that this was easily predictable, yet GitHub/OpenAI rushed to market with a code generation product whose reliability (and legality) remains completely unresolved.</p><p>The ultimate issue is not about AI or programming so much as software-as-industrial-product. You can quickly estimate increases in productivity over the course of a sprint or two: it's easy to count features cleared and LoC written. But if there are dumb GPT "brain fart" errors in that boilerplate and the boilerplate isn't adequately reviewed by humans, then you might not have particularly good visibility of the consequences until a few months pass and there seem to be more 5-10% bug reports than usual. Again, I don't think the use of Copilot is actually a terrible security disaster. But it's clearly a risk. It's a risk that needs to be addressed BEFORE the tool becomes a de facto standard.</p><p>I certainly get that there's a lot of truly tedious boilerplate in most enterprise codebases - even so I suspect a lot of that is better done with a fairly simple <i>deterministic</i> script versus Copilot. In fact my third biggest irritation with this stuff is that deterministic code generation tools have gotten <i>really good</i> at producing verifiably correct code, even if the interface doesn't involve literally talking to a computer.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39463213"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39463213" href="https://news.ycombinator.com/vote?id=39463213&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>it’s for satisfying the wet dreams of management in large companies dreaming of replacing developers with AI.<p>the issue is that it will take a long time before they wake up and scramble to re-hire the people they laid off. in the meantime i predict a lot of developers will have pivoted to other careers, retired, became AI devs etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461068"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461068" href="https://news.ycombinator.com/vote?id=39461068&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>While I’m more on the infrastructure side of things, I see a similar issue. Like you mentioned, it’s great for lookups of API documentation and getting examples etc. I have also used it for things like templates and boring boilerplate. I have come to look at it as a lookup tool and something that converts my thoughts into code. I could see myself sitting at home and doing a lot of coding by voice and a vr headset in the future if the tools continue to develop. 
At the moment I think we just need to come up with a better way of integrating it into our workflow. I’m starting to wonder if something like visual programming could work well with the “ai” “figuring” out the content of the blocks we connect and basically lets us influence the generated code by the io. That could be a solution to coding on tablets and phones with minimum typing.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461045"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461045" href="https://news.ycombinator.com/vote?id=39461045&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>I've found that they offer poor solutions but good starting points. I learned a lot more about using Leaflet and d3 for a personal project because GPT4-Turbo gave me a solution, which I took as a starting point. It gave me insight that illuminated the documentation for each that was previously opaque to me. As such, I find value in GPTs as accelerators for learning. They're non-judgmental so you needn't worry about whether or not you asked your question correctly on SO.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461195"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461195" href="https://news.ycombinator.com/vote?id=39461195&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Yeah it's obviously useless for anyone doing advanced programming work. I'm writing compilers (for AI models of all things), and it's just not there. It has some amazing wins. I have fed it a description of ISAs for proprietary chips and had it generate correct kernels, and such. That is impressive, and it's very useful for references for quick tutorials on common enough things (but do check the docs). However, it's not able to do abstract thinking really yet.<p>On the other hand? If I want to add payments to my web app (not a web dev, but I mess around with side projects for fun), and I don't want to read the stripe library docs for example, it's pretty good at getting me started installing the right library, and generating the boiler plate. Same with various boilerplate for code generation.</p><p>On the other hand, ChatGPT (which has been clearly trained on stuff I've read) has an uncanny ability to describe my esoteric side projects much better than I can, so there is that. I have some DSLs written in Haskell and it's actually really good at providing answers with it, and explaining things succinctly.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39462508"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39462508" href="https://news.ycombinator.com/vote?id=39462508&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Since I don't see it mentioned here at all, I highly recommend trying out <a href="https://cursor.so/" rel="nofollow">https://cursor.so</a> - GPT4 powered and absolutely helpful with refactoring code and adding new features, not just finishing the current line.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39462661"><td></td></tr>
                  <tr id="39461115"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461115" href="https://news.ycombinator.com/vote?id=39461115&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Once CoPilot can understand entire workspace and have deep understanding of your project, it will likely seem a lot smarter than it seems now.<p>All I use it for is to avoid repetitive stuff as it's exceptionally good at guessing my next step.</p><p>The autocomplete bits feel wrong most of the time, and as fast as API updates happen it's mostly a wash in terms of productivity.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461176"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461176" href="https://news.ycombinator.com/vote?id=39461176&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>It being useful for the basic Google/Stack Overflow tasks makes it worth it for me. It saves me a lot of time being able to simply bring up the exact answer to whatever small "how-do-I-do-this-again" problem that fits into my code without rewriting it.<p>You're right though that it isn't very useful for complex problems. Complex SQL queries in particular are hopeless. But I probably only spend 10-20% of my time on stuff where I <i>really</i> have to think about what I'm doing anyway.</p><p>The autocompletion is the best part for me. When I start writing a function called `mapUsersToIds` or whatever and it autocompletes the 3 or 4 lines of code that I now don't need to write, that saves me a ton of time - I'd say I'm 30% more productive now that I don't need to spend time typing, even if the autocompleted code is exactly what I was going to write in the first place.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461030"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461030" href="https://news.ycombinator.com/vote?id=39461030&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>I have been using Cody (sourcegraph.com/cody) for about 6 months now and it's completely changed the way I write code. But, there was an adjustment period to learn how to work with the tool. Expecting a code copilot to just give you working code 100% of the time is unrealistic today, we may get there eventually though.<p>I've been writing code for close to 20 years now across the full stack, I have written a lot of bad code in my life, I have seen frameworks come and go, so spotting bad code or spotting bad practices is almost second nature to me. With that said, using Cody, I'm able to ship much faster. It will sometimes return bad answers, i may need to tweak my question, and sometimes it just doesn't capture the right context for what I'm trying to do, but overall it's been a great help and I'd say has made me 35-40% more efficient.</p><p>(Disclaimer: I work for Sourcegraph)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461205"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461205" href="https://news.ycombinator.com/vote?id=39461205&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>As someone building in the AI space I can give my unbiased opinion. I do not think AI is the big productivity boost we expect them to be yet.
I have been a software engineer for around 6 years now and I the only AI tooling I prefer and have really can not live without today is autocomplete (a better and smarter LSP).<p>When it comes to making a more complicated change, things are never easy because of the limited context window and the general lack of reasoning and inherent knowledge of the codebase I am working with.</p><p>Having said this, GPT4 has been really good for the one off questions I have about either syntax or if I forget how to do "the thing I know is possible I am just not sure" or the mundane things like docker commands or some other commands which I need help with.</p><p>But... if you guys have seen Gemini-1.5 Pro I was seriously mind blown and I think the first time I felt a LLM is better than me and that has to do with code search. I have had my fair share of navigating large codebases and spending time understanding implementations (clicking go-to-reference, go-to-definition) and keeping a mental model.. the fact that this LLM can take a minute to understand and answer questions about codebase does feel like a game changer.</p><p>I think the right way to think about AI tooling for programming is not to ask them to go and build this insane new feature which will bring lots of money for you, but how they can help you get that edge in your daily workflows (small quality of life changes which compound over time, just like how LSP is taken for granted in the editor now a days).</p><p>Another point to mention here which I believe is a major miss is that these copilots write code without paying attention to the various tools we as humans would use when writing code (LSP, linters, compilers etc). They are legit writing code like they would on a simple notepad and that is another reason why the quality is often times pretty bad (but copilot has proved that with a faster feedback loop and the right UX its not too big a hassle)</p><p>We are still very early in this game and with many people building in this space and these models improving over time I do think we will look back and laugh how things were done pre-AI vs post-smart-AI models.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461161"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461161" href="https://news.ycombinator.com/vote?id=39461161&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>It can help with complicated tasks. When prompting via the chat interface, it comes more naturally to prompt with a full description of the problem.<p>With too much assumed context, it only does a good job of spitting out the answer to a common problem, or implementing a mostly correct version of the commonly written task similar to the one requested.</p><p>When you use copilot, are you shaping your use to its workflows? Adding preceding comments to describe the high-level goal, the high-level approach, and other considerations for how the code soon to follow interacts with the rest of the codebase?
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461184"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461184" href="https://news.ycombinator.com/vote?id=39461184&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>I haven't used copilot much at all, but I use chat gpt occasionally. One of the queries I find it does the best with is: "What's an idiomatic way to do XYZ in language ABC".</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461074"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461074" href="https://news.ycombinator.com/vote?id=39461074&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Similar experience. I really only still use Copilot to:<p>- generate short blocks of low-entropy code (save some keystrokes)</p><p>- get me off the ground when using a new library (save some time combing through documentation)
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461182"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461182" href="https://news.ycombinator.com/vote?id=39461182&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>You're not alone, I have the exact same experience.<p>I already gave up on anything complex, but it also fails at relative simple things.</p><p>It goes like this:
The first answer does something useful, but is not the full solution or contains a bug.</p><p>When told, it apologizes and gives code that does not even compile.
Then when trying to get it in the right direction, it gets worse and worse.</p><p>Then it hallucinates a non-existing library,that should solve the problem.</p><p>And in the end I end up writing the code myself...
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39461712"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461712" href="https://news.ycombinator.com/vote?id=39461712&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Better to restart the discussion than trying to ask for corrections, the context window gets filled with confusion.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461200"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461200" href="https://news.ycombinator.com/vote?id=39461200&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>I have cancelled Copilot after trying for few months because sometimes it used to take forever to suggest code. Now I use Duet AI and it is code suggestion on steroids. Sometimes it suggests some good code and I feel how terrible I am for not thinking of that approach. But I think we are progressing fast with this AI things and it won't be long before everyone can code with these tools.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461055"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461055" href="https://news.ycombinator.com/vote?id=39461055&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>I feel you. In my experience the paid ChatGPT's answers does not outperform the basic google stackoverflow route for solved problems - which is not surprising considering how those models are created. For more complicated stuff, in my case, for example, weired Java dependency injection use cases, it gave me equally wrong answers I could found in the internet anyway. Only faster.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461194"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461194" href="https://news.ycombinator.com/vote?id=39461194&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>The real advantages come when you move off of copilot or GPT onto something without their restrictions. Sure it can’t go pull code from github but it can give you more accurate results than the dumbed down prompts the SaaS providers give you.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461149"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461149" href="https://news.ycombinator.com/vote?id=39461149&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Anecdotally people are saying it may actually have gotten significantly worse recently. So it’s possible that the results you might have got from GPT-4 a month ago are no longer anything like as accurate or useful.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461038"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461038" href="https://news.ycombinator.com/vote?id=39461038&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>It's hit or miss. Sometimes, I give it relatively complex requirements, and it spits out a running program. Worst case, it at least points me in the right direction.<p>I save well over an hour a month with it, so it's worth it.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461265"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461265" href="https://news.ycombinator.com/vote?id=39461265&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Did you try GPT4? That isn't my experience though. It can explain fairly complex machine learning model to me with high accuracy.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39462943"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39462943" href="https://news.ycombinator.com/vote?id=39462943&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>As a newb to programming I find it helpful for boilerplate and “smart” autocomplete tasks (which makes sense, as a LLM is simply autocomplete on steroids). I have found it helpful for Regex as well, which is a topic that still catches me. It can also be interesting (albeit typically somewhat useless) to bounce ideas off of it and have it attempt to solve problems. It rarely helps, but in my experience humans haven’t been much better outside of my professor.<p>I’ve found its best at completing repetitive but simple tasks, such as organizing data or making simple scripts. Anything more complex or that cannot be easily explained in a way it “understands” will run into problems. For instance, telling it to program a custom variant of Dijkstra’s Algorithm for my specific input data? No go, it doesn’t work. However, a simple script to replace any text from column 1 in a CSV with its neighbor text in column 2? Works first try.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39460980"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39460980" href="https://news.ycombinator.com/vote?id=39460980&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>i turned off copilot when i did a trial of an alternative and when the trial ran out i just stopped using either. i still use chatgpt over stackoverflow but i don't miss the proactive prompts.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39460999"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39460999" href="https://news.ycombinator.com/vote?id=39460999&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>A lot depends on what you use them for.<p>I don’t find them that great at large scale programming and they couldn’t do the hard parts of my work, but a lot of what I do doesn’t need to be “great.”</p><p>There’s the core system design and delivering of features. That it struggles with. Anything large seems to be a struggle.</p><p>But generating SQL for a report I do sporadically on demand from another team?</p><p>Telling me what to debug to get Docker working (which I am rarely doing as a dev)? Anything Shell or Nginx related (again, infrequent, so I am a beginner in those areas)</p><p>Generating infrequently run but tedious formatting helper functions?</p><p>Generating tests?</p><p>Basically, what would you give a dev with a year of experience? I would take ChatGPT/Copilot over me with 1 year of experience.</p><p>The biggest benefit to me is all the offloaded non-core work. My job at least involves a lot more than writing big features (maybe yours does not).
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461104"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461104" href="https://news.ycombinator.com/vote?id=39461104&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>You are not alone.<p>At the core AI/ML is giving you answers that have a high probability of being good answers. But in the end this probability is based on avarages. And the moment you are coding stuff that is not avarage AI does not work anymore because it can not reason about the question and 'answer'.</p><p>You can also see this in AI generated images. They look great but the avarage component makes them all look the same and a kind of blurry.</p><p>For me the biggest danger of AI is that people put too much trust in it.</p><p>It can be a great tool, but you should not trust it to be the truth.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461118"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461118" href="https://news.ycombinator.com/vote?id=39461118&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>It’s incredible for my use case.<p>I have been involved in software and implementing technical things since the late 90s and from time to time have been pretty good at a few things here and there but I am <i>profoundly</i> rusty in all languages I sort of know and useless in ones I don’t.</p><p>But I’m technical. I understand at sort of a core level how things work, jargon, and like the key elements of data structures and object oriented code and a MVC model and whatever else. Like I’ve read the right books.</p><p>Without ChatGPT I am close to useless. I’m better off writing a user story and hiring someone, anyone. Yes I can code in rails and know SQL and am actually pretty handy on the command line but like it would take me an entire day and tons of googling to get basic things working.</p><p>Then they launched GPT and I can now launch useful working projects that solve business problems quickly. I can patch together an API integration on a Sunday afternoon to populate a table I already have in a few minutes. I can take a website I’m overseeing and add a quick feature.</p><p>It’s literally life changing. I already have all the business logic in my head, and I know enough to see what GPT is spitting out and if it’s wrong and know how to ask the right questions.</p><p>Unlike the OP I have no plans to do anything complex. But for my use cases it’s turned me from a project manager into a quick and competent developer and that’s literally miraculous from where I’m standing.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461011"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461011" href="https://news.ycombinator.com/vote?id=39461011&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>Between your post and Air Canada’s learning they have to honor policies their chat bot hallucinates and relays to customers, it seems like the zeitgeist is starting to comprehend the inherent limitations and risks of LLMs.<p>I find that kind of heartening, honestly.</p><p>But it’s by no means a death sentence for AI. Plenty of dimensions for massive improvement.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39461059"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461059" href="https://news.ycombinator.com/vote?id=39461059&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Now if only the C*O class would learn that they can help but certainly won't replace developers. Then we can get back to a more normal hiring market. This downturn in tech is very bizarre and much worse than the dotcom bubble.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39461775"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39461775" href="https://news.ycombinator.com/vote?id=39461775&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>That’s a very interesting idea, that the layoffs and hiring freezes -and the resulting reality that morale is at an all-time low among every one of the dozens of engineers I interact with on a frequent basis- is a consequence of leadership banking on that they can run their humans into the ground and then replace them with automation.<p>There used to be the tiniest bit of restraint when the only available replacements were also sentient meatbags which would need to be trained.</p><p>It makes sense that this rampant, gleeful, wholesale exploitation has been enabled by the idea that AI can replace folks faster than attrition brings things to a halt.</p><p>Sick, but feels truthy
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461095"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39461095" href="https://news.ycombinator.com/vote?id=39461095&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>It wouldn’t surprise me if ChatGPT is an improvement in Air Canada customer service anyway in terms of information provided.<p>It is just that the bot in this case wrote it down, which made AC liable.</p><p>I’m an Air Canada elite and am part of several Facebook groups of similar people. It is notoriously difficult to get clear information on Air Canada policies for anything. Even concierge (for Air Canada’s top tier loyalty members) staff are often giving contradictory information.</p><p>Their rules for everything are extremely complicated and they have a fairly large back office constantly fixing even addition errors in terms of points allocation and status progression. They literally aren’t adding up spend totals correctly.</p><p>It is quite possible that Air Canada just didn’t tell the bot anything about bereavement fares.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39461097"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461097" href="https://news.ycombinator.com/vote?id=39461097&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>If I have a linux problem and copy/paste relevant parts of the log into an AI, the result is more often than not useful and quick. It's a real help for admin tasks. Is it programming though ? :)</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39461088"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461088" href="https://news.ycombinator.com/vote?id=39461088&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><p><span>&gt;  or learn more about some history stuff where it just repeats something it parsed off wikipedia or whatever.<p>Be careful with that too, it will also spit out whatever urban legend it read on a subject without making a difference between the facts it got on Wikipedia and the bullshit it read elsewhere.</p><p>Keep in mind, they are <i>language</i> models, not <i>knowledge</i> models.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39461019"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39461019" href="https://news.ycombinator.com/vote?id=39461019&amp;how=up&amp;goto=item%3Fid%3D39460788"></a></center>    </td><td><br><div>
                  <p><span>Co-pilot is junk. The only thing that works passably for coding is ChatGPT using GPT-4 and prompts where you ask it write and then implement a software spec, stopping after each code section to ask for review and revisions.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39461136"><td></td></tr>
                  <tr id="39462069"><td></td></tr>
                <tr id="39462565"><td></td></tr>
                  </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[European Court of Human Rights declares backdoored encryption is illegal (106 pts)]]></title>
            <link>https://www.theregister.com/2024/02/15/echr_backdoor_encryption/</link>
            <guid>39460455</guid>
            <pubDate>Wed, 21 Feb 2024 22:22:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theregister.com/2024/02/15/echr_backdoor_encryption/">https://www.theregister.com/2024/02/15/echr_backdoor_encryption/</a>, See on <a href="https://news.ycombinator.com/item?id=39460455">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body">
<p>The European Court of Human Rights (ECHR) has ruled that laws requiring crippled encryption and extensive data retention violate the European Convention on Human Rights – a decision that may derail European data surveillance legislation known as Chat Control.</p>
<p>The court issued a <a target="_blank" rel="nofollow" href="https://hudoc.echr.coe.int/eng/#{%22itemid%22:[%22001-230854%22]}">decision</a> on Tuesday stating that "the contested legislation providing for the retention of all internet communications of all users, the security services’ direct access to the data stored without adequate safeguards against abuse and the requirement to decrypt encrypted communications, as applied to end-to-end encrypted communications, cannot be regarded as necessary in a democratic society."</p>
<p>The "contested legislation" mentioned above refers to a legal challenge that started in 2017 after a demand from Russia's Federal Security Service (FSB) that messaging service Telegram provide technical information to assist the decryption of a user's communication. The plaintiff, Anton Valeryevich Podchasov, challenged the order in Russia but his claim was dismissed.</p>

    

<p>In 2019, Podchasov brought the matter to the ECHR. Russia joined the <a target="_blank" rel="nofollow" href="https://www.coe.int/en/web/portal">Council of Europe</a> – an international human rights organization – in 1996 and was a member until it withdrew in March 2022 following its illegal invasion of Ukraine. Because the 2019 case predates Russia's withdrawal, the ECHR continued to consider the matter.</p>

        


        

<p>The court concluded that the Russian law requiring Telegram "to decrypt end-to-end encrypted communications risks amounting to a requirement that providers of such services weaken the encryption mechanism for all users." As such, the court considers that requirement disproportionate to legitimate law enforcement goals.</p>
<ul>

<li><a href="https://www.theregister.com/2023/12/14/x_illegally_targeted_ads_suit/">Privacy crusaders accuse X of ad-targeting that flouts EU rules</a></li>

<li><a href="https://www.theregister.com/2023/03/03/german_digital_committee_hearing_heaps/">German Digital Affairs Committee hearing heaps scorn on Chat Control</a></li>

<li><a href="https://www.theregister.com/2023/02/09/open_source_policy_summit/">Open Source Policy Summit: Where FOSS and government meet</a></li>

<li><a href="https://www.theregister.com/2022/10/13/clientside_scanning_csam_anderson/">Scanning phones to detect child abuse evidence is harmful, 'magical' thinking</a></li>
</ul>
<p>While the ECHR decision is unlikely to have any effect within Russia, it matters to countries in Europe that are contemplating similar decryption laws – such as <a target="_blank" rel="nofollow" href="https://www.patrick-breyer.de/en/posts/chat-control/">Chat Control</a> and the UK government's <a target="_blank" href="https://www.theregister.com/2023/10/27/online_safety_act_charles/">Online Safety Act</a>.</p>
<p><a target="_blank" href="https://www.theregister.com/2023/03/03/german_digital_committee_hearing_heaps/">Chat Control</a> is shorthand for European data surveillance legislation that would require internet service providers to scan digital communications for illegal content – specifically child sexual abuse material and potentially terrorism-related information. Doing so would necessarily entail weakening the encryption that keeps communication private.</p>
<p>Efforts to develop workable rules have been underway for several years and continue to this day, despite widespread condemnation from <a target="_blank" href="https://www.theregister.com/2022/10/13/clientside_scanning_csam_anderson/">academics</a>, <a target="_blank" rel="nofollow" href="https://tutanota.com/blog/posts/germany-against-client-side-scanning-csam">privacy-oriented orgs</a>, and <a target="_blank" rel="nofollow" href="https://freiheitsrechte.org/en/themen/digitale-grundrechte/chatkontrolle">civil society groups</a>.</p>

        

<p>Patrick Breyer, a member of the European parliament for the Pirate Party, hailed the ruling for demonstrating that Chat Control is incompatible with EU law.</p>
<p>"With this outstanding landmark judgment, the 'client-side scanning' surveillance on all smartphones proposed by the EU Commission in its chat control bill is clearly illegal," said Breyer.</p>
<p>"It would destroy the protection of everyone instead of investigating suspects. EU governments will now have no choice but to remove the destruction of secure encryption from their position on this proposal – as well as the indiscriminate surveillance of private communications of the entire population!" ®</p>                                


                    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Go Run (186 pts)]]></title>
            <link>https://breadchris.com/blog/go-run/</link>
            <guid>39460409</guid>
            <pubDate>Wed, 21 Feb 2024 22:17:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://breadchris.com/blog/go-run/">https://breadchris.com/blog/go-run/</a>, See on <a href="https://news.ycombinator.com/item?id=39460409">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

<article>
   
  <div><p>It may seem silly, but <code>go run</code> is my favorite part about go. Want to run your code? <code>go run main.go</code>. It is so stupidly simple that I could tell my mom about this command, and she would immediately understand. Like with most things in go, the real power in this command is in the effortless understanding of how to build and run everyone’s code.</p>
<p>But I can run <code>node main.js</code>? Yeah, and then what happens if you want to use modern syntax like esmodule, or maybe you want to use types with typescript? You are going to have to use <code>npm</code>.</p>
<p>The tools you use to build and run code are often disparate in a language. As a project maintainer, you must understand community practices and conventions of correctly setting up a project. Third-party libraries are often required to run even the simplest of code. For example, in the Ruby programming language, the “rake” build tool became popular because of its versatility and the fact that it was used in many open-source projects. Similarly, in the JavaScript community, the adoption of npm as a build tool was driven by the widespread use of npm packages and modules.</p>
<p>Languages grow and change over time, and it is only natural that tools come and go as well. C and C++ have historically had “make” since the 1970s; Java grew from “ant” in the 1990s to Maven in the early 2000s. Different requirements for your application influence the tools that are developed and, by extension, influence people’s workflows.</p>
<p>This is precisely the reason I love <code>go run</code> so much. Go’s entire design brings power to the command. Simple syntax, static typing, and a strict dependency management system allow the compiler to quickly parse and analyze source code, leading to build times faster. Faster build times mean a broader reach of who could use the language to achieve their goals. It is funny; you will occasionally see a go project that includes a <code>Makefile</code>, but the compilation step is just a <code>go build</code> (build; don’t run the code). Old habits die hard.</p>
<p>Fun fact: One of the understated features <code>go run</code> is that it will automatically download any dependencies the code references; how cool is that!</p>
<p>The more I program in other languages, the more I feel the painful absence of a standardized build system. I find it frustrating to keep up with the latest and greatest build system for something as common as running code. For something so critical to my understanding of how to code, such as how to run it, I find it unacceptable not to have a reliable foundation of knowledge.</p>
<p>Do not compromise with hacks or workarounds for critical components of your software development flow. Take ownership of the tools you use and fight for change so that you and everyone else benefit. When experience is shared, problems are solved, days of people’s lives are saved, and passion for programming continues to grow, not wane.</p>
<p>comments:</p>
<p>bun run :) bun hing.ts same with python but not compiled you need to install python that’s the beauty of go for me even rust you need a cargo file</p>
<p>amazing, for js we not only have npm, yarn, pnpm, and bower (am I missing any?) but we also have completely new runtimes <a href="https://bun.sh/">bun</a> and <a href="https://deno.com/">deno</a>. these are not evil, but tread lightly. Is VC backed deno going to fold as a company? maybe bun gets burnt and we are onto the next one. meanwhile I will be here with my boring <code>go run</code></p>


  </div>

  
</article>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atuin – Magical Shell History (293 pts)]]></title>
            <link>https://atuin.sh</link>
            <guid>39460148</guid>
            <pubDate>Wed, 21 Feb 2024 21:56:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://atuin.sh">https://atuin.sh</a>, See on <a href="https://news.ycombinator.com/item?id=39460148">Hacker News</a></p>
<div id="readability-page-1" class="page"><main><div><div><h2>Making your shell <b>magical</b></h2><div><p><span>Sync, search and backup shell history with <span>Atuin</span></span></p></div></div></div><div><p><h2>Proudly open source</h2></p></div><div id="features"><div><p>Features</p><h2>What you get with Atuin</h2></div><div><div><div><h3>Shell history sync</h3><p>Sync your shell history to all of your machines, wherever they are</p></div><div><h3>End-to-end encryption</h3><p>All data is encrypted, and can only be read by you</p></div><div><h3>Efficient search</h3><p>Search decades of shell history, and recall it in an instant. Atuin offers configurable full text or fuzzy search, filterable by host, directory, etc.</p></div></div><div><div><h3>Open source</h3><p>Atuin is open source with a permissive license, and has a growing community</p></div><div><h3>Data import</h3><p>Bring your existing history with you - Atuin supports importing from a wide variety of formats</p></div><div><h3>Store extra context</h3><p>Atuin stores extra context with your commands - working directory, exit code, and more!</p></div></div></div></div><div><div><h2>Accelerate your productivity in 2 minutes</h2><div><p>Step 1: <span>Run the install script</span></p><p>Run our install script to get setup with the Atuin binary and shell plugin</p></div><div><p>Step 2: <span>Register</span></p><p>Optionally sign up for Atuin Cloud Sync, or self-host your own sync server</p></div><div><p>Step 3: <span>Search</span></p><p>Enjoy enhanced ctrl-r, and keep your history forever</p></div></div><div><picture><source sizes="(max-width: 768px) 100vw, 432px" srcset="https://atuin.sh/_astro/cargo-prefix.322ce063_WHpbb.avif 400w,https://atuin.sh/_astro/cargo-prefix.322ce063_Z3NFdB.avif 768w" type="image/avif"><source sizes="(max-width: 768px) 100vw, 432px" srcset="https://atuin.sh/_astro/cargo-prefix.322ce063_1JNKcV.webp 400w,https://atuin.sh/_astro/cargo-prefix.322ce063_IhEN9.webp 768w" type="image/webp"><source sizes="(max-width: 768px) 100vw, 432px" srcset="https://atuin.sh/_astro/cargo-prefix.322ce063_14hVND.png 400w,https://atuin.sh/_astro/cargo-prefix.322ce063_2bjDW9.png 768w" type="image/png"><img src="https://atuin.sh/_astro/cargo-prefix.322ce063_2bjDW9.png" alt="Cargo prefix search" decoding="async" height="768" loading="lazy" width="432"></picture></div></div><div><div><p>FAQs</p><h2>Frequently Asked Questions</h2></div><div><div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> Can you read my shell history?</h3><p>No. Atuin is fully end-to-end encrypted. Without your key, nobody can see a thing. Read more about our encryption <a href="https://atuin.sh/blog/new-encryption">here</a>!</p></div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> Do I have to register?</h3><p>If you would like to sync your shell history, registration is required. Otherwise, you can use Atuin locally as a fully-offline enhanced history search tool</p></div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> What if I still don't trust your server?</h3><p>If you would rather operate + maintain your own sync server then we have a guide <a href="https://docs.atuin.sh/self-hosting/server-setup/">here</a>! Atuin supports self-hosting.</p></div></div><div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> What technology does Atuin use?</h3><p>Atuin is written in Rust, and stores your data in SQLite. Both technologies are known for being fast and reliable</p></div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> What shells can I use Atuin with?</h3><p>Atuin currently supports Bash, ZSH, Fish and NuShell</p></div><div><h3><svg astro-icon="tabler:arrow-down-right" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path d="m7 7 10 10M17 8v9H8"></path></g></svg> What if I have more questions?</h3><p>Our <a href="https://docs.atuin.sh/">docs</a> go into much further detail, but if your question is still not answered then please do feel free to drop in on our <a href="https://discord.gg/jR3tfchVvW">Discord</a></p></div></div></div></div></main></div>]]></description>
        </item>
        <item>
            <title><![CDATA[An Introduction to the Theory and Practice of Poker (2020) (155 pts)]]></title>
            <link>https://hopkinspokercourse.com/</link>
            <guid>39459579</guid>
            <pubDate>Wed, 21 Feb 2024 21:08:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hopkinspokercourse.com/">https://hopkinspokercourse.com/</a>, See on <a href="https://news.ycombinator.com/item?id=39459579">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="body_layer">
          
          
          <div>
              <p>This intersession course aims to take students from novices who may know nothing about poker to above average players, in two weeks. The course will utilize hand examples and discussions of common poker situations to study the fundamentals of the game. The math, combinatoric, and game-theoretical aspects of poker will be covered, with practical examples of every concept. Lectures will focus on core concepts in Texas Hold `em, with an emphasis on cash games. Tournament-specific concepts will be covered towards the end. There will be a guest lecture by 2005 WSOP Main Event runner up <a title="https://en.wikipedia.org/wiki/Steve_Dannenmann" href="https://en.wikipedia.org/wiki/Steve_Dannenmann">Steve Dannenmann</a>, whose live tournament winnings exceed $4 million. The course culminated with a poker tournament at the instructor’s home at the end of intersession. </p>
            </div>
          


          
          


          <div id="id3">
                <p>An Introduction to the Theory and Practice of Poker<br></p>
                <p>Intersession 2020<br></p>
                <p>EN.650.201, Instagram: @jhupoker, #jhupoker<br></p>
                <p>January 13-24<br></p>
                <p><a title="https://avirubin.com/Poker.Syllabus.pdf" href="https://avirubin.com/Poker.Syllabus.pdf">Click here for Syllabus (PDF)<br></a></p>
                
                
                <p>Professor <a title="https://avirubin.com" href="https://avirubin.com/">Avi Rubin</a><br></p>
                
              </div>
          


          <p><img src="https://hopkinspokercourse.com/PokerCourse_files/AviRubinHeadShotSept-2019.jpg" alt="">
          </p>
          


          <div id="id4">
                <p><span>Lecture 1, Jan 13, 2020<br></span></p>
                <p>Topics: Rules of poker, counterfeiting, side pots, the basics<span><br></span></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Poker-lecture1-13-20.pdf" href="https://avirubin.com/Poker-lecture1-13-20.pdf">Click for PDF</a> <br></p>
                
                <p>Lecture video:&nbsp;&nbsp;&nbsp; <br></p>
                
                
                
                
                
                
                
                
                
                
              </div>
          


          <div id="id5">
                <p><span>Lecture 2, Jan 14, 2020<br></span></p>
                <p>Topics: Equity, pot odds, fundamentals<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Poker-lecture-2-Jan14.2020.pdf" href="https://avirubin.com/Poker-lecture-2-Jan14.2020.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          


          
          
          
          
          
          <div id="id7">
                <p>Lecture 3, Jan 15, 2020<br></p>
                <p>Topics: Starting Hand selection - pre-flop play<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Lecture3.1-15-2020.pdf" href="https://avirubin.com/Lecture3.1-15-2020.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <div id="id8">
                <p>Lecture 4, Jan 16, 2020<br></p>
                <p>Topic: Pre-flop play, How to describe a hand, Types of bets<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/LectureJan16-2020.pdf" href="https://avirubin.com/LectureJan16-2020.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <div id="id9">
                <p>Lecture 5, Jan 17, 2020<br></p>
                <p>Topics: Bet types, Leveling, Post-flop play<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Jan17-2020PokerLecture.pdf" href="https://avirubin.com/Jan17-2020PokerLecture.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <div id="id10">
                <p>Lecture 6, Jan 21, 2020<br></p>
                <p>Topic:&nbsp; Tournaments<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Lecture-Jan21.2020.pdf" href="https://avirubin.com/Lecture-Jan21.2020.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <div id="id11">
                <p>Lecture 7, Jan 22, 2020<br></p>
                <p>Topic:&nbsp; Guest lecture by Steve Dannenmann, Rules of 2 &amp; 4, some moves<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/PokerLecture1-22-2020.pdf" href="https://avirubin.com/PokerLecture1-22-2020.pdf">Click for PDF</a><br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <p><img src="https://hopkinspokercourse.com/PokerCourse_files/IMG_7113.jpg" alt="">
          </p>
          


          <div id="id12">
                <p>Lecture 8, Jan 23, 2020<br></p>
                <p>Topic:&nbsp; Poker Etiquette, Turn strategy<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Jan23-Lecture.pdf" href="https://avirubin.com/Jan23-Lecture.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          <p><img src="https://hopkinspokercourse.com/PokerCourse_files/IMG_7141.jpg" alt="">
          </p>
          


          
          


          <div id="id14">
                <p>Lecture 9, Jan 24, 2020<br></p>
                <p>Topic:&nbsp; An actual turn hand, River play, hand names<br></p>
                <p>Lecture slides:&nbsp;&nbsp; <a title="https://avirubin.com/Lecture-2-24-20.pdf" href="https://avirubin.com/Lecture-2-24-20.pdf">Click for PDF</a><br></p>
                
                <p>Lecture video: <br></p>
                <p>To see the embedded video clips while watching, click on the “i” when you see the placeholder.<br></p>
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
              </div>
          
          
          
          


          <div>
            <p><img usemap="#map2" id="shapeimage_3" src="https://hopkinspokercourse.com/PokerCourse_files/shapeimage_3.png" alt="Click to see Hopkins HUB story" title=""></p><map name="map2" id="map2"><area href="https://hub.jhu.edu/2020/01/30/avi-rubin-poker-2499-em1-art1-nr-students/" title="https://hub.jhu.edu/2020/01/30/avi-rubin-poker-2499-em1-art1-nr-students/" onmouseover="IMmouseover('shapeimage_3', '0');" alt="https://hub.jhu.edu/2020/01/30/avi-rubin-poker-2499-em1-art1-nr-students/" onmouseout="IMmouseout('shapeimage_3', '0');" shape="rect" coords="9, 9, 218, 30"><area href="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" title="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" alt="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" coords="5, 5, 307, 65" onclick="window.open(this.href); return false;" onkeypress="window.open(this.href); return false;"></map><p><img id="shapeimage_3_link_0" alt="shapeimage_3_link_0" src="https://hopkinspokercourse.com/PokerCourse_files/shapeimage_3_link_0.png">
          </p></div>
          


          <div>
            <p><img usemap="#map3" id="shapeimage_4" src="https://hopkinspokercourse.com/PokerCourse_files/shapeimage_4.png" alt="Click for picture gallery of tournament" title=""></p><map name="map3" id="map3"><area href="https://photos.app.goo.gl/LzcGzSXgfjDyosPM8" title="https://photos.app.goo.gl/LzcGzSXgfjDyosPM8" onmouseover="IMmouseover('shapeimage_4', '0');" alt="https://photos.app.goo.gl/LzcGzSXgfjDyosPM8" onmouseout="IMmouseout('shapeimage_4', '0');" shape="rect" coords="9, 9, 258, 30"><area href="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" title="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" alt="https://www.bizjournals.com/baltimore/news/2020/01/30/jhu-professor-brings-lessons-from-the-poker-table.html?iana=hpmvp_bal_news_headline" coords="5, 5, 307, 65" onclick="window.open(this.href); return false;" onkeypress="window.open(this.href); return false;"></map><p><img id="shapeimage_4_link_0" alt="shapeimage_4_link_0" src="https://hopkinspokercourse.com/PokerCourse_files/shapeimage_4_link_0.png">
          </p></div>
          
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Moving a billion Postgres rows on a $100 budget (133 pts)]]></title>
            <link>https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget</link>
            <guid>39458636</guid>
            <pubDate>Wed, 21 Feb 2024 19:54:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget">https://blog.peerdb.io/moving-a-billion-postgres-rows-on-a-100-budget</a>, See on <a href="https://news.ycombinator.com/item?id=39458636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><p>Inspired by the <a target="_blank" href="https://github.com/gunnarmorling/1brc">1BR Challenge</a>, I wanted to see how much it would cost to transfer 1 billion rows from Postgres to Snowflake. Moving 1 billion rows is no easy task. The process involves not just the transfer of data but ensuring its integrity, error recovery and consistency post-migration.</p>
<p>Central to this task is the selection of tools and techniques. We will discuss the use of open-source tools, customized scripts, ways to read data from Postgres, and Snowflake’s data loading capabilities. Key aspects like parallel processing, efficiently reading Postgres’ <a target="_blank" href="https://www.postgresql.org/docs/current/wal-intro.html">WAL</a>, data compression and incremental batch loading on Snowflake will be highlighted.</p>
<p>I will list and discuss some of the optimizations that are implemented to minimize compute, network, and warehouse costs. Additionally, I will highlight some of the trade-offs made as part of this process. Given that most of the approaches covered in this blog stem from my explorations at <a target="_blank" href="https://github.com/PeerDB-io/peerdb">PeerDB</a> aimed at enhancing our product – The task was accomplished primarily through <a target="_blank" href="https://www.peerdb.io/">PeerDB</a>.</p>
<p>I want to make it clear that there are some feature gaps in comparison to a mature system, and it might not be practical for all use cases. However, it does handle the most common use cases effectively while significantly reducing costs. I also want to caveat that there might be some ways in which the estimations may be off and I’d be happy to adjust based on feedback.</p>
<h2 id="heading-setup">Setup</h2>
<ul>
<li><p><strong>Initial data load:</strong> We will consider that there are 300M rows already in the table at the start of the task, and our system should handle the initial load of all the rows.</p>
</li>
<li><p><strong>Inserts, Updates and Deletes (Change Data Capture):</strong> The rest of the 700M rows will be a combination of inserts, updates and deletes. <a target="_blank" href="https://wiki.postgresql.org/wiki/TOAST">Including support for toast columns</a>.</p>
<ul>
<li>1024 rows changed per second for ~8 days.</li>
</ul>
</li>
<li><p><strong>Recoverability:</strong> We will reboot the system every 30 mins to ensure that it's robust and can recover from disasters.</p>
</li>
</ul>
<p>Now let us walk through an engineering design that optimally handles the above workload with the objective of <strong>minimizing costs</strong> and <strong>improving performance</strong>, one step at a time.</p>
<h2 id="heading-initial-load-from-postgres-to-snowflake">Initial Load from Postgres to Snowflake</h2>
<p>Let’s start with the first operation any data sync job has to do: load the initial set of data from the source to destination. There are a few challenges that come with this:</p>
<ol>
<li><p>How to efficiently retrieve large amounts of data from Postgres?</p>
</li>
<li><p>How to process the data in a way where we have minimal cost foot-print?</p>
</li>
<li><p>How to efficiently load this data to Snowflake?</p>
</li>
</ol>
<h2 id="heading-optimal-data-retrieval-from-postgres">Optimal Data retrieval from Postgres</h2>
<p>Reading a table sequentially from Postgres is slow. It would take a long time to read 300M rows from Postgres. To make this process more efficient, <a target="_blank" href="https://duckdb.org/2022/09/30/postgres-scanner.html#parallelization">we have to parallelize</a>. We've got a clever way to quickly read parts of a table in Postgres using something called the TID Scan, which is a bit of a hidden gem. Basically, it lets us pick out specific chunks of data as stored on disk, identified by their <a target="_blank" href="https://www.postgresql.org/docs/current/ddl-system-columns.html#id-1.5.4.7.4.6.2.1">Tuple IDs</a> (CTIDs), which look like <code>(page, tuple)</code>. This optimizes IO utilization and is super handy for reading big tables efficiently.</p>
<p>Here's how we do it: we divide the table into partitions based on the pages of the database, and each partition gets its own scan task. Each task handles about 500K rows. So, we partition the table into CTID ranges, with each partition having ~500K rows, and we process each partition parallelly (16 partitions at a time).</p>
<pre><code><span>SELECT</span> <span>count</span>(*) <span>FROM</span> public.challenge_1br; <span>-- find the count</span>

<span>-- num_partitions = (count // rows_per_partition)</span>

<span>SELECT</span> <span>bucket</span>, <span>MIN</span>(ctid) <span>AS</span> <span>start</span>, <span>MAX</span>(ctid) <span>AS</span> <span>end</span>
<span>FROM</span> (
    <span>SELECT</span> NTILE(<span>1000</span>) <span>OVER</span> (<span>ORDER</span> <span>BY</span> ctid) <span>AS</span> <span>bucket</span>, ctid 
  <span>FROM</span> public.challenge_1br
) subquery
<span>GROUP</span> <span>BY</span> <span>bucket</span> <span>ORDER</span> <span>BY</span> <span>start</span>;
</code></pre>
<p><img loading="lazy" src="https://lh7-us.googleusercontent.com/qo41LQQwhVKZT9mWROXxYWr-eKYUu2_EcJ9Elcn49Mfk-vpuIBvz54sBmxWr7W2Z0quqiujKPkQWA3omYn_VGaYf8MWJDVNx4EzcGYFWa4ofE-zMfU9k6U76ZcBsZe5A4o0Tkf3p978w9bpqnN_3MkI" alt=""></p>
<h2 id="heading-data-in-transit">Data in Transit</h2>
<p>It is important to process the data in a way where we don’t overload the system. As we are operating under budget constraints, we need to use techniques that use the hardware effectively. We are going to be using the “<a target="_blank" href="https://twitter.com/garybernhardt/status/600783770925420546?s=20">your dataset fits in RAM</a>'' paradigm of systems design. 300M rows for initial load does sound like a lot, but let's see how we can make it fit in our RAM. We need to process the data to ensure <a target="_blank" href="https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication">data-types are mapped correctly to the destination</a>. We are going to convert the query results to <a target="_blank" href="https://avro.apache.org/docs/1.11.0/index.html">Avro</a> for faster loading into warehouses, and also <a target="_blank" href="https://avro.apache.org/docs/1.11.0/spec.html#Logical+Types">for its logical type support</a>.</p>
<h3 id="heading-how-big-is-the-data">How big is the data?</h3>
<p>Let us take a little detour to explore how big the data is. This is a good chance to look at some real world examples to estimate things. Based on interacting with a lot of production customers, and talking to some experts, it’s safe to say that on an average we see ~15 columns per table. In our table, let’s say each row is ~512 bytes.</p>
<pre><code><span># for initial load</span>
num_rows = <span>300</span>_000_000
bytes_per_row = <span>512</span>
total_num_bytes = num_rows * bytes_per_row
total_size_gb = total_num_bytes / <span>1</span>_000_000_000
<span># total initial load size 153.6 GB</span>

<span># memory required during initial load</span>

num_rows_per_partition = <span>500</span>_000
mb_per_partition = num_rows_per_partition * bytes_per_row / <span>1</span>_000_000 <span># 256 MB</span>
num_partitions_in_parallel = <span>16</span>
required_memory = num_partitions_in_parallel * mb_per_partition <span># 4096 MB</span>
</code></pre>
<h3 id="heading-required-memory">Required Memory</h3>
<p>Based on the above napkin math, we can see that with 4GB of RAM we should be able to do the initial load. We will allocate 8GB of RAM to account for other components.</p>
<h2 id="heading-efficiently-loading-data-into-snowflake">Efficiently loading data into Snowflake</h2>
<p>As mentioned earlier we are going to store the query results into Avro on-disk. We are further going to compress the Avro files using <a target="_blank" href="https://github.com/facebook/zstd">zstd</a> to further reduce the disk footprint and also to save on network costs. We will take a slight deviation from the topic to talk about Bandwidth costs.</p>
<h3 id="heading-bandwidth-costs-they-can-break-the-bank">Bandwidth costs: They can break the bank!</h3>
<p>Let's look at the network costs, you can see the variance in numbers.</p>
<p>It’s interesting to see the variance in the costs, so it’s best to have Postgres, our System and Snowflake in the same cloud provider and the same region. Let’s now calculate the networks costs needed for this workload.</p>
<h3 id="heading-calculating-network-costs">Calculating Network Costs</h3>
<p>Another thing to be wary of is the Warehouse configuration.</p>
<pre><code>bytes_per_row = <span>512</span>
num_rows = <span>1</span>_000_000_000
total_data_size = <span>512</span>GB
compressed_data_size_GB = <span>256</span> <span>#avro+zstd gives atleast 2x compression</span>
bandwidth_cost_per_10GB = $<span>0.1</span>

<span># total nework costs</span>
<span># data_size_GB * bandwidth_cost_per_10GB / 10</span>
network_costs_egress_from_postgres = $<span>5</span>
<span># compressed_data_size_GB * bandwidth_cost_per_10GB / 10</span>
network_costs_egress_from_system_to_snowflake = $<span>2.56</span> 

network_costs = $<span>7.56</span>
</code></pre>
<h3 id="heading-snowflake-warehouse-configuration">Snowflake Warehouse Configuration</h3>
<p>In many organizations, a significant portion of Snowflake expenses comes from compute usage, particularly when warehouses run idle between tasks. Snowflake's compute costs are accrued based on warehouse operational time, starting from activation to suspension. Often, idle warehouse time can contribute to 10%-25% of the total Snowflake compute costs. The Baselit team wrote an excellent blog about it: <a target="_blank" href="https://baselit.ai/blogs/fastest-way-save-snowflake">read more about it here</a>.</p>
<p>The two things we will be doing is to set <code>AUTO_SUSPEND</code> to be 60 seconds, a warehouse idles for up to a minute after the last query before pausing, and make sure that we keep the warehouse active for the least amount of time. This is the default configuration you get if you follow the <a target="_blank" href="https://docs.peerdb.io/connect/snowflake">PeerDB Snowflake setup guide</a>.</p>
<h2 id="heading-inserts-updates-and-deletes">Inserts, Updates and Deletes</h2>
<p>The next challenge for us after the initial load would be to read the change data from Postgres and replaying that to Snowflake. We are going to be doing that using Postgres’ Logical Replication. At the start of the replication, we will create a replication slot and use <a target="_blank" href="https://www.postgresql.org/docs/current/logical-replication-architecture.html">pgoutput</a> plugin. This is the recommended way to read changes from the slot. Once we read the changes from the slot, we will batch them and then load them to Snowflake.</p>
<p>As we discussed earlier, it is important to keep the Snowflake warehouse idle for as long as we can, and batching helps with that. We store records in batches of 1M to Avro like before, and load them to an <a target="_blank" href="https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage">internal stage</a> in Snowflake. Once the data is loaded into the stage, we will <a target="_blank" href="https://docs.snowflake.com/en/sql-reference/sql/merge">MERGE</a> the records from the stage into the destination table. This way most of the heavy-lifting of the resolution is left to the warehouse and it simplifies our system.</p>

<p>At <a target="_blank" href="https://www.peerdb.io/">PeerDB</a>, we are building a specialized data-movement tool for Postgres with laser focus on Postgres to Data Warehouse replication. Most of the above optimizations incl. <a target="_blank" href="https://blog.peerdb.io/parallelized-initial-load-for-cdc-based-streaming-from-postgres">parallel initial load</a>, <a target="_blank" href="https://blog.peerdb.io/reducing-bigquery-costs-by-260x">reducing Data Warehouse costs</a>, <a target="_blank" href="https://blog.peerdb.io/role-of-data-type-mapping-in-database-replication">native data-type mapping</a>, <a target="_blank" href="https://github.com/PeerDB-io/peerdb/pull/111">support of TOAST columns</a>, <a target="_blank" href="https://blog.peerdb.io/using-temporal-to-scale-data-synchronization-at-peerdb">fault-tolerance and auto recovery</a> etc. are already baked into the product. PeerDB is also <a target="_blank" href="https://github.com/PeerDB-io/peerdb">Free and Open</a>. So we chose PeerDB to implement the above workload.</p>
<h2 id="heading-hardware">Hardware</h2>
<p>Now that we have landed on 8GB RAM, let us move onto picking the instance type.</p>
<p>Since ARM uses lower energy compared to x64 (due to being RISC), they are around 25% cheaper as compared to x64 machines. The tradeoff here is that x64 machines run at around 2.9GHz with a 3.5GHz Turbo (M6i instances) as compared to ARM machines at about 2.5GHz (Graviton2 - M6g) but M6i instances are about 30% more expensive as compared to M6g instances.</p>
<p>Effective cost is $0.0409/GHz for x64 vs $0.03616/GHz for ARM, so cost is about 13% more per GHz on x64 <strong>But cost per GHz is not the determining factor for reading in a single thread from Postres during CDC as replication slots can be read from a single process at once.</strong></p>
<p>For this current experiment, I went with <code>m6gd.large</code> as it offers a good balance of speed and disk.</p>
<p><strong>Optional read:</strong> In this blog we will use AWS for our analysis. However, here are some other learnings we had on this topic. OVH Cloud currently <a target="_blank" href="https://github.com/ovh/public-cloud-roadmap/issues/343">does not support ARM</a> Instances and has a similar $0.118/hour <code>c2-7</code> instance (in limited regions) which has a <a target="_blank" href="https://www.ovhcloud.com/asia/public-cloud/prices/#410">very low network speed</a> (250MBps) with 50GB of SSD. <a target="_blank" href="https://www.hetzner.com/cloud/">Hetzner</a> has a <code>CCX13</code> $0.0292/hour instance (including a 118GB SSD) but no dedicated ARM instances.</p>
<p><img loading="lazy" src="https://lh7-us.googleusercontent.com/Mmy4pyDQ_qnBrX7PJH830MLgSj0s4u1UOZOzMhlr32YrA8ewQzEMOCkk6e3bXkgeVYfwVYrLt_Ofs6YENVIos5N_daFgBp2t6KZ59n9X2EWvwJWmVsCmAU82yyHzAxQuc0BX7ccGmpZqpdsEszTO4lU" alt=""></p>
<h2 id="heading-conclusion">Conclusion</h2>
<p>One question that I'm often posed with: <strong>“Is this practical?”</strong>. Yes, one machine can die, but systems where there is only one machine have a <a target="_blank" href="https://twitter.com/danluu/status/1586180166631706624?s=20">remarkable amount of uptime</a>, especially when the state is stored in a durable way.</p>
<p>Back to the topic at hand. If we look at the total cost of the system we built (assuming <code>us-west-2</code> as the region. Over a month time this is the breakdown:</p>
<div>
<table>
<thead>
<tr>
<td>Cost Category</td><td>Cost</td><td>Comment</td></tr>
</thead>
<tbody>
<tr>
<td>Hardware</td><td>$65.992 / month</td><td>AWS m6gd.large (2 vcpus, 8 GB RAM)</td></tr>
<tr>
<td>Comes with 118 GB NVMe which is great!</td><td></td><td></td></tr>
<tr>
<td>Network</td><td>$7.56</td><td>AWS network transfer same region 500 GB (with compression)</td></tr>
<tr>
<td>Warehouse</td><td>N/A</td><td>These are common across various vendors</td></tr>
<tr>
<td><strong>Total</strong></td><td><strong>$73.552</strong></td><td><strong>Hardware Costs + Network costs = $65.992 + $7.56 = $73.552 (Within $100 budget)</strong></td></tr>
</tbody>
</table>
</div><p>If we were to look at various ETL tools and how much they charge for moving 1 billion rows, this is what it comes out to:</p>
<div>
<table>
<thead>
<tr>
<td><strong>Vendor</strong></td><td><strong>Cost per 1 billion records</strong></td></tr>
</thead>
<tbody>
<tr>
<td>Fivetran</td><td>$23,157.89</td></tr>
<tr>
<td>Airbyte</td><td>$11,760.00</td></tr>
<tr>
<td>Stitch Data</td><td>$4,166.67</td></tr>
<tr>
<td>Above Approach (using <a target="_blank" href="https://github.com/PeerDB-io/peerdb">PeerDB OSS</a>)</td><td>$73.552</td></tr>
</tbody>
</table>
</div><p>I am part of a company building a software for moving data specifically from Postgres to Data warehouses. It's my job to figure out how to provide the best experience to our customers. Doing this project forced me to figure out a way to provide the best bang for buck, and to include a lot of the explored features <a target="_blank" href="https://www.peerdb.io/">into PeerDB</a>. I hope it conveys some appreciation for what modern hardware is capable of, and how much you can get out of it.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Neural Network Diffusion (202 pts)]]></title>
            <link>https://arxiv.org/abs/2402.13144</link>
            <guid>39458363</guid>
            <pubDate>Wed, 21 Feb 2024 19:31:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2402.13144">https://arxiv.org/abs/2402.13144</a>, See on <a href="https://news.ycombinator.com/item?id=39458363">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2402.13144.pdf">Download PDF</a>
    <a href="https://arxiv.org/html/2402.13144v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Kai Wang [<a href="https://arxiv.org/show-email/87b1c894/2402.13144">view email</a>]      <br>    <strong>[v1]</strong>
        Tue, 20 Feb 2024 16:59:03 UTC (6,066 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The killer app of Gemini Pro 1.5 is video (1046 pts)]]></title>
            <link>https://simonwillison.net/2024/Feb/21/gemini-pro-video/</link>
            <guid>39458264</guid>
            <pubDate>Wed, 21 Feb 2024 19:23:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://simonwillison.net/2024/Feb/21/gemini-pro-video/">https://simonwillison.net/2024/Feb/21/gemini-pro-video/</a>, See on <a href="https://news.ycombinator.com/item?id=39458264">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>



<p>21st February 2024</p>

<p>Last week Google <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">introduced Gemini Pro 1.5</a>, an enormous upgrade to their Gemini series of AI models.</p>
<p>Gemini Pro 1.5 has a 1,000,000 token context size. This is <em>huge</em>—previously that record was held by Claude 2.1 (200,000 tokens) and gpt-4-turbo (128,000 tokens)—though the difference in tokenizer implementations between the models means this isn’t a perfectly direct comparison.</p>
<p>I’ve been playing with Gemini Pro 1.5 for a few days, and I think the most exciting feature isn’t so much the token count... it’s the ability to use video as an input.</p>
<p>I’ve been accessing the model through the <a href="https://aistudio.google.com/app/prompts/new_chat">Google AI Studio</a> interface—I don’t have API access yet.</p>
<p>I tried filling up the context with content from my blog, and got some good results. But things got <em>really</em> exciting when I began experimenting with video uploads (after some false starts—the team have been actively fixing bugs, with variable bitrate support landing just this morning).</p>
<p>I took this seven second video of one of my bookshelves:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jAlySrZqJqE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="allowfullscreen"> </iframe>
<p>Then I uploaded it with the following prompt:</p>
<blockquote>
<p>JSON array of books in this video</p>
</blockquote>
<p>That 7 second video consumed just 1,841 tokens out of my 1,048,576 token limit.</p>
<p>Gemini Pro 1.5 didn’t return JSON, but it did reply with this:</p>
<blockquote>
<p>It is not possible to fully identify all the books in the video. However, here are some of the books that can be seen:</p>
<ul>
<li>The Lord of the Rings by J.R.R. Tolkien</li>
<li>Structure and Interpretation of Computer Programs by Harold Abelson and Gerald Jay Sussman</li>
<li>Rework by Jason Fried and David Heinemeier Hansson</li>
<li>The Hacker Ethic and the Spirit of the Information Age by Pekka Himanen</li>
<li>The Google Story by David A. Vise</li>
<li>The Art of Agile Development by James Shore and Shane Warden</li>
<li>Creative Code by John Maeda</li>
<li>Web Operations by John Allspaw and Jesse Robbins</li>
<li>Python Framework by David Mertz</li>
<li>The Personal MBA by Josh Kaufman</li>
<li>Growing Up with Lucy by April Henry</li>
<li>Getting Things Done by David Allen</li>
<li>Site-Seeing: A Visual Approach to Web Usability by Luke Wroblewski</li>
<li>JavaScript: The Definitive Guide by David Flanagan</li>
<li>Building Accessible Websites by Joe Clark</li>
<li>Information Architecture for the World Wide Web by Louis Rosenfeld and Peter Morville</li>
<li>Don’t Make Me Think: A Common Sense Approach to Web Usability by Steve Krug</li>
<li>Programming Erlang: Software for a Concurrent World by Joe Armstrong</li>
<li>XSLT 2.0 Web Development by Dmitry Kirsanov</li>
<li>Pragmatic Version Control Using Git by Travis Swicegood</li>
<li>Party Animals: My Family and Other Communists by David Aaronovitch</li>
</ul>
</blockquote>
<p>A follow-up prompt:</p>
<blockquote>
<p>as a JSON array of objects, with title and author keys</p>
</blockquote>
<p>Gave me that back as JSON</p>
<div><pre>[
  {
    <span>"title"</span>: <span><span>"</span>The Lord of the Rings<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>J.R.R. Tolkien<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Structure and Interpretation of Computer Programs<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Harold Abelson and Gerald Jay Sussman<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Rework<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Jason Fried and David Heinemeier Hansson<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Hacker Ethic and the Spirit of the Information Age<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Pekka Himanen<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Google Story<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>David A. Vise<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Art of Agile Development<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>James Shore and Shane Warden<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Creative Code<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>John Maeda<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Web Operations<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>John Allspaw and Jesse Robbins<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Python Framework<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>David Mertz<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Personal MBA<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Josh Kaufman<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Growing Up with Lucy<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>April Henry<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Getting Things Done<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>David Allen<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Site-Seeing: A Visual Approach to Web Usability<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Luke Wroblewski<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>JavaScript: The Definitive Guide<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>David Flanagan<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Building Accessible Websites<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Joe Clark<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Information Architecture for the World Wide Web<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Louis Rosenfeld and Peter Morville<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Don't Make Me Think: A Common Sense Approach to Web Usability<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Steve Krug<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Programming Erlang: Software for a Concurrent World<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Joe Armstrong<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>XSLT 2.0 Web Development<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Dmitry Kirsanov<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Pragmatic Version Control Using Git<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>Travis Swicegood<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Party Animals: My Family and Other Communists<span>"</span></span>,
    <span>"author"</span>: <span><span>"</span>David Aaronovitch<span>"</span></span>
  }
]</pre></div>
<p>So how well did it do?</p>
<p>Honestly, I’m pretty astonished by this. The video is 7s long, it pans quite fast (there is some motion blur in the video) and some of the books are partially obscured by items.</p>
<p>It didn’t get all of them, but it did about as good a job as I could have done given the same video.</p>
<p>I’m particularly impressed that it got “Site Seeing: A Visual Approach to Web Usability” by Luke Wroblewski, where the book itself is almost entirely obscured by a squirrel leaving only “Site-Seeing” visible.</p>
<p>I did spot one hallucination: it lists “The Personal MBA by Josh Kaufman”, but I don’t own that book—the closest thing in the video is a blurry few frames of a book called “The Beermat Entrepreneur”.</p>
<p>I decided to try it again with a different video and hopefully a better prompt. This time I videoed a bookshelf mostly full of cookbooks—this time in portrait, a bit longer (22 seconds) and panning down as well as across.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/6m8sNercyNU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="allowfullscreen"> </iframe>
<p>This video cost me 6,049 tokens—still a pretty tiny allocation.</p>
<p>My new prompt was:</p>
<blockquote>
<p>Output a JSON array of {“title”: “...”, “authors”: “...”} objects for books in this video</p>
</blockquote>
<p>And.... it refused:</p>
<p><img src="https://static.simonwillison.net/static/2024/gemini-refusal.jpg" alt="User: a video, containing a bunch of books visible in the first frame including THE COCKTAIL, SMUGGLER'S COVE, THE CUROUS BARTENDER... 00:22 IMG_7592.MOV 6,049 tokens. Prompt: Output a JSON array of {&quot;title&quot;: &quot; ...&quot;, &quot;authors&quot;. &quot;...&quot;} objects for books in this video. Model: No content, with a box open that says Probability of unsafe content Dangerous Content High - Edit safety settings"></p>
<p>It looks like the safety filter may have taken offense to the word “Cocktail”!</p>
<p>I opened up the safety settings, dialled them down to “low” for every category and tried again. It appeared to refuse a second time.</p>
<p>So I channelled <a href="https://en.wikipedia.org/wiki/List_of_Father_Ted_characters#Mrs_Doyle">Mrs Doyle</a> and said:</p>
<blockquote>
<p>go on give me that JSON</p>
</blockquote>
<p>And it worked!</p>
<div><pre>[
  {
    <span>"title"</span>: <span><span>"</span>The Cocktail Guy<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Rich Woods<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Smuggler's Cove<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Martin Cate<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Curious Bartender<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Tristan Stephenson<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Art of Fermentation<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Sandor Ellix Katz<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Noma Guide to Fermentation<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>René Redzepi &amp; David Zilber<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Cocktails #7<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Difford's Guide<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Curious Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Rival Crock Pot Cooking<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Art of French Cooking<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Fernande Garvin<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Book on Pie<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Erin Jeanne McDowell<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The New Taste of Chocolate<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Vegan Cakes and Bakes<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Jérôme Eckmeier &amp; Daniela Lais<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Kitchen Creamery<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Good Food 101 Teatime Treats<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>BBC<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Betty Crocker's Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Martha Stewart Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Feast<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Nigella Lawson<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Moosewood Restaurant New Classics<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>World Food Café<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Chris &amp; Carolyn Caldicott<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Everyday Thai Cooking<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Katie Chin<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Vegetarian Indian Cooking with Instant Pot<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Manali Singh<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Southern Vegetarian Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Justin Fox Burks &amp; Amy Lawrence<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Vegetarian Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Französische Küche<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Sushi-Making at Home<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Kosher Cooking<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The New Empanadas<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Marlena Spieler<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Instant Pot Vegetarian Cookbook for Two<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Vegetarian<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Wilkes &amp; Cartwright<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Breakfast<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Nadiya's Kitchen<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Nadiya Hussain<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>New Food for Thought<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Jane Noraika<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Beyond Curry Indian Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>D'Silva Sankalp<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The 5 O'Clock Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Food Lab<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>J. Kenji López-Alt<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Cook's Encyclopedia<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Cast Iron Nation<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Lodge<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Urban Cook Book<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>In Search of Perfection<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Heston Blumenthal<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Perfection<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Heston Blumenthal<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>An Economist Gets Lunch<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Tyler Cowen<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Colman's Mustard Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Pam Hartley<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Student Grub Guide<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Williams<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Easy Meals for One &amp; Two<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Jack Monroe Tin Can Cook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Slow Cooker<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Students' Sausage, Egg, and Beans Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Quick &amp; Easy Students' Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Student Cookbook Guide<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The Best Little Marinades Cookbook<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Adler<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>The New Book of Middle Eastern Food<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Claudia Roden<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Vegetarian Meals<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span>Rosamond Richardson<span>"</span></span>
  },
  {
    <span>"title"</span>: <span><span>"</span>Girl! Mother Tells You How<span>"</span></span>,
    <span>"authors"</span>: <span><span>"</span><span>"</span></span>
  }
]</pre></div>
<p>Once again, I find those results pretty astounding.</p>
<h4 id="what-to-make-of-this">What to make of this</h4>
<p>The ability to extract structured content from text is already one of the most exciting use-cases for LLMs. GPT-4 Vision and LLaVA expanded that to images. And now Gemini Pro 1.5 expands that to video.</p>
<p>The ability to analyze video like this feels SO powerful. Being able to take a 20 second video of a bookshelf and get back a JSON array of those books is just the first thing I thought to try.</p>
<p>The usual LLM caveats apply. It can miss things and it can hallucinate incorrect details. Half of the work in making the most of this class of technology is figuring out how to work around these limitations, but I feel like we’re making good progress on that.</p>
<p>There’s also the issue with the safety filters. As input to these models gets longer, the chance of something triggering a filter (like the first four letters of the word “cocktail”) goes up.</p>
<p>So, as always with modern AI, there are still plenty of challenges to overcome.</p>
<p>But this really does feel like another one of those glimpses of a future that’s suddenly far closer then I expected it to be.</p>
<h4 id="images-vs-video">A note on images v.s. video</h4>
<p>Initially I had assumed that video was handled differently from images, due partly to the surprisingly (to me) low token counts involved in processing a video.</p>
<p><a href="https://news.ycombinator.com/item?id=39458264#39458355">This thread</a> on Hacker News convinced me otherwise.</p>
<p>From <a href="https://developers.googleblog.com/2024/02/gemini-15-available-for-private-preview-in-google-ai-studio.html">this blog post</a>:</p>
<blockquote>
<p>Gemini 1.5 Pro can also reason across up to 1 hour of video. When you attach a video, Google AI Studio breaks it down into thousands of frames (without audio), and then you can perform highly sophisticated reasoning and problem-solving tasks since the Gemini models are multimodal.</p>
</blockquote>
<p>Then in the <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5 technical report</a>:</p>
<blockquote>
<p>When prompted with a 45 minute Buster Keaton movie “Sherlock Jr." (1924) (2,674 frames
at 1FPS, 684k tokens), Gemini 1.5 Pro retrieves and extracts textual information from a specific frame
in and provides the corresponding timestamp.</p>
</blockquote>
<p>I ran my own experiment: I grabbed a frame from my video and uploaded that to Gemini in a new prompt.</p>
<p><img src="https://static.simonwillison.net/static/2024/gemini-frame.jpg" alt="Screenshot of the Gemini interface with an uploaded image. A box reads Preview 258 / 1,048,576"></p>
<p>That’s 258 tokens for a single image.</p>
<p>Using the numbers from the Buster Keaton example, 684,000 tokens / 2,674 frames = 256 tokens per frame. So it looks like it really does work by breaking down the video into individual frames and processing each one as an image.</p>
<p>For my own videos: 1,841 / 256 = 7.2 (the 7s video) and 6,049 / 256 = 23.6 (the 22s video)—which makes me believe that videos are split up into one frame per second and each frame costs ~256 tokens.</p>
<p>As a further experiment I grabbed a snapshot of the first frame of the video and uploaded that as a plain image. This time I didn’t even include a prompt:</p>
<p><img src="https://static.simonwillison.net/static/2024/bookshelf.jpg" alt="An image of books on a bookshelf - the Lord of the Rings trilogy in the middle."></p>
<p>Gemini Pro 1.5 guessed what I wanted and replied:</p>
<blockquote>
<p>The books in the image are:</p>
<ul>
<li>The Lord of the Rings by J.R.R. Tolkien</li>
<li>Structure and Interpretation of Computer Programs by Harold Abelson and Gerald Jay Sussman</li>
<li>The Source by James A. Michener</li>
<li>Rework by Jason Fried and David Heinemeier Hansson</li>
<li>The Hacker Ethic and the Spirit of the Information Age by Pekka Himanen</li>
<li>Google by David A. Vise</li>
<li>The Design of Everyday Things by Don Norman</li>
<li>Linux Server Hacks by Evi Nemeth et al.</li>
<li>The C Programming Language by Brian W. Kernighan and Dennis M. Ritchie</li>
<li>High Output Management by Andrew S. Grove</li>
<li>Mapping Hacks by Schuyler Erle et al.</li>
</ul>
</blockquote>
<p>The image input was 258 tokens, the total token count after the response was 410 tokens—so 152 tokens for the response from the model. Those image tokens pack in a lot of information!</p>
<p><img src="https://static.simonwillison.net/static/2024/gemina-image.jpg" alt="Screenshot of that interaction, showing 410/1,048,576 tokens used."></p>




</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing ousts 737 MAX chief (124 pts)]]></title>
            <link>https://www.seattletimes.com/business/boeing-aerospace/boeing-ousts-head-of-737-max-program-reshuffles-leadership-team/</link>
            <guid>39457999</guid>
            <pubDate>Wed, 21 Feb 2024 19:02:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seattletimes.com/business/boeing-aerospace/boeing-ousts-head-of-737-max-program-reshuffles-leadership-team/">https://www.seattletimes.com/business/boeing-aerospace/boeing-ousts-head-of-737-max-program-reshuffles-leadership-team/</a>, See on <a href="https://news.ycombinator.com/item?id=39457999">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
    <p>Boeing has ousted the leader of the 737 MAX program at its Renton plant and reshuffled its leadership team at the Commercial Airplanes division, effective immediately.</p><p>The moves come more than a month after a Renton-assembled MAX 9 saw a fuselage panel blow out of an Alaska Airlines flight departing Portland. Investigators contend <a href="https://www.seattletimes.com/business/boeing-aerospace/ntsb-report-on-alaska-flight-1282-says-key-bolts-missing-when-boeing-delivered-jet/">key bolts were missing from the plane prior to the Jan. 5 blowout</a>, a failure that has <a href="https://www.seattletimes.com/business/boeing-aerospace/cantwell-blasts-faa-oversight-of-boeing-after-alaska-airlines-737-max-9-blowout/">increased scrutiny of quality control at Boeing and its suppliers</a> and put intense pressure on company leadership.</p><p>Ed Clark, vice president of the MAX program and general manager at the Renton facility, will leave the company. He’s being replaced by Katie Ringgold, the current vice president 737 delivery operations.</p><p>The changes were announced Wednesday morning in an email to employees by Boeing Commercial Airplanes chief Stan Deal.</p><p>Deal wrote that the leadership changes are intended to drive Boeing Commercial Airplanes’ “enhanced focus on ensuring that every airplane we deliver meets or exceeds all quality and safety requirements.”</p><p>Clark took charge of the MAX program in 2021 as it returned to service after two fatal crashes had grounded the jets worldwide for almost two years. He was responsible for 737 engineering, supply chain, manufacturing and other support functions.</p>
<p>Deal wrote in his note that Clark “departs with my, and our, deepest gratitude for his many significant contributions over nearly 18 years of dedicated service to Boeing.”</p><p>However, a person familiar with the decision and who asked not to be identified commenting on sensitive personnel decisions, confirmed that Clark’s departure was not voluntary.</p><p>Clark is an engineer. His successor Ringgold has business degrees. However she began her aviation career performing avionics systems maintenance and troubleshooting on C-130 cargo aircraft in the U.S. Air Force.</p><p>Ringgold joined Boeing in 2011 at the company’s North Charleston, S.C., production facility, where she rose to become a senior quality manager. </p><p>She transferred to the Puget Sound region in 2019, where she was responsible for jet deliveries from all Boeing Commercial Airplanes delivery centers in Seattle, Everett, North Charleston and Zhoushan, China.</p><p>She then became vice president of 737 delivery operations with responsibility for deliveries to customers from Seattle, as well as pre-delivery flights in Renton and oversight and care of the parked MAXs stored at Moses Lake, Seattle’s Plant 2 and San Antonio.</p>
<p>In addition to that change at the top of the MAX program, Deal shuffled the appointments of two senior leaders, Elizabeth Lund and Mike Fleming.</p><p>Lund, who currently heads all airplane programs at Boeing Commercial Airplanes, has been named to the new position of senior vice president for BCA Quality, where she will lead quality control efforts.</p><p>Lund will continue reporting to Deal and serving on the company’s executive council. Carole Murray, the current vice president of Quality, will now report to Lund, “on special assignment, to ensure a smooth transition,” Deal wrote in his message.</p><p>Mike Fleming, who led the 737 MAX return-to-service push after the two fatal crashes and has since then led the drive to certify the MAX 7 and MAX 10, has been promoted to replace Lund as senior vice president and general manager of all Boeing Commercial airplane programs. He’ll oversee 737, 767, 777/777X, and 787 production, reporting to Deal.</p><p>Deal said Don Ruhmann, who was 787 chief project engineer, is succeeding Fleming in his former role. </p><p>Deal wrote that replacements for the positions vacated by Ringgold and Ruhmann will be named later. </p>    
        <div>
   <p><span>
         Dominic Gates:      </span>
       <span><a href="mailto:dgates@seattletimes.com">dgates@seattletimes.com</a>;</span>      <span>Dominic Gates is a Pulitzer Prize-winning aerospace journalist for The Seattle Times.</span>   </p>
</div>  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI behavior guardrails should be public (388 pts)]]></title>
            <link>https://twitter.com/ID_AA_Carmack/status/1760360183945965853</link>
            <guid>39457974</guid>
            <pubDate>Wed, 21 Feb 2024 19:00:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://twitter.com/ID_AA_Carmack/status/1760360183945965853">https://twitter.com/ID_AA_Carmack/status/1760360183945965853</a>, See on <a href="https://news.ycombinator.com/item?id=39457974">Hacker News</a></p>
Couldn't get https://twitter.com/ID_AA_Carmack/status/1760360183945965853: Error: Request failed with status code 400]]></description>
        </item>
        <item>
            <title><![CDATA[Useful Uses of Cat (255 pts)]]></title>
            <link>https://two-wrongs.com/useful-uses-of-cat</link>
            <guid>39457875</guid>
            <pubDate>Wed, 21 Feb 2024 18:53:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://two-wrongs.com/useful-uses-of-cat">https://two-wrongs.com/useful-uses-of-cat</a>, See on <a href="https://news.ycombinator.com/item?id=39457875">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
                <p>
When I write shell one-liners that transform the contents of some file, they
often look something like
</p>

<p><label>In[1]:</label></p><div>
<pre>cat access.log | head -n 500 | grep mail | perl -e …
</pre>
</div>

<p>
This is what a lot of people reflexively call a <i>useless use of cat</i><label for="fn.1">1</label><span><sup>1</sup> And
more thoughtful people will call a useless use of <code>grep</code> and <code>head</code> too because the
Perl script can of course do both of those things.</span> because <code>head</code> can take the
file name as an argument and we don’t need the extra pipe and <code>cat</code> command. In
fact, almost all commands can take the file name directly<label for="fn.2">2</label><span><sup>2</sup> For the ones that
don’t, we can use input redirection and say e.g. <code>&lt;&nbsp;access.log&nbsp;command</code>.</span> and we
only really need <code>cat</code> when we want to concatenate the contents of files.
</p>

<p>
But there’s a reason I do it anyway.
</p>

<p>
I’m currently re-reading one of David Parnas’ classic papers on
modularity<label for="fn.3">3</label><span><sup>3</sup> <i>Designing Software for Ease of Extension and Contraction</i>;
Parnas; <abbr>ieee</abbr> Transactions on Software Engineering; 1979.</span>. Every software
engineer should read that stuff – it’s brilliant. For this article we’ll focus
on one thing: we all know code changes should be isolated. For example, we
should be able to add new functionality by just adding code, and not going in
and changing existing code. Parnas phrases this in an interesting way.
</p>

<blockquote>
<p>
The problem is that these subsets and extensions are not the programs that we
would have designed if we had set out to design just that product.
</p>
</blockquote>

<p>
His idea of the ideal design is one where we can add or remove code and it still
looks like the program was designed for the thing it’s doing now; i.e. you can’t
tell that something else was added or removed later on, it all looks like part
of the original design.
</p>

<p>
Parnas lists four classes of problems we often encounter when trying to make
changes. For this discussion, the second class is the relevant one.
</p>

<blockquote>
<p>
Many programs are structured as a chain of components, each receiving data from
the previous component, processing it (and changing the format), before sending
the data to the next program in the chain. If one component in this chain is not
needed, that code is often hard to remove because the output of its predecessor
is not compatible with the input requirements of its successor. A program that
does nothing but change the format must be substituted.
</p>

<p>
One illustration would be a payroll program that assumed unsorted input. One of
the components of the system accepts the unsorted input and produces output that
is sorted by some key. If the firm adopts an office procedure that results in
sorted input, this phase of the processing is unnecessary. To eliminate that
program, one may have to add a program that transfers data from a file in the
input format to a file in the format appropriate for the next phase.
</p>
</blockquote>

<p>
If we go back to our example of the shell one-liner and squint a little, then
the string <code>access.log</code> is one input format (describing the file with the
relevant contents) and the contents of the access log is a different input
format. These are two representations of essentially the same thing.
</p>

<p>
If we then eliminate the useless use of cat and write instead
</p>

<p><label>In[2]:</label></p><div>
<pre>head -n 500 access.log | grep mail | perl -e …
</pre>
</div>

<p>
we find that <code>head</code> performs two responsibilities:
</p>

<ol>
<li>Converting the string <code>access.log</code> into the contents of the file; and</li>
<li>Extracting the first 500 records of that content.</li>
</ol>

<p>
When we’re satisfied with our Perl script, it’s not unreasonable to think that
we might want to run it across the entire access log rather than just the first
500 records. If we then delete only the <code>head</code> processing step we’re left
without a step that transforms the string <code>access.log</code> into the contents of the
access log. We can move that responsibility into the <code>grep</code> call, but this would
mean we hade to change some existing component in order to remove another – no
good!
</p>

<p>
The natural solution is a useless use of <code>cat</code>. With a separate processing step
that converts the filename into the file contents, we can delete any
intermediary processing steps and still be left with a functioning pipeline.
People can complain all they want about it, but I will continue writing modular
code. Even when it’s just shell one-liners.
</p>

            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google Reneged on the Monopolistic Bargain (132 pts)]]></title>
            <link>https://pluralistic.net/2024/02/21/im-feeling-unlucky/#not-up-to-the-task</link>
            <guid>39456447</guid>
            <pubDate>Wed, 21 Feb 2024 17:07:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#not-up-to-the-task">https://pluralistic.net/2024/02/21/im-feeling-unlucky/#not-up-to-the-task</a>, See on <a href="https://news.ycombinator.com/item?id=39456447">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-8054">
	<!-- .entry-header -->

	
	
	<div>
		<p><!--
Tags:
monopoly, seo, dark seo, google, search, enshittification, platform decay, product reviews, spam, antitrust, trustbusting
the bezzle, martin hench, marty hench, red team blues, fiction, crime fiction, crime thrillers, thrillers, technothrillers, novels, books, royalties, wage theft, creative labor

Summary:
Google reneged on the monopolistic bargain; The Bezzle excerpt (Part IV); Hey look at this

URL:
https://pluralistic.net/2024/02/21/im-feeling-unlucky/

Title:
Pluralistic: Google reneged on the monopolistic bargain; The Bezzle excerpt (Part IV) (21 Feb 2024) im-feeling-unlucky

Bullet:
↩️

Separator:
^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^-^

Top Sources:
None

--><br>
<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/"><img decoding="async" src="https://i0.wp.com/craphound.com/images/21Feb2024.jpg?w=840&amp;ssl=1" data-recalc-dims="1"></a></p>
<h2>Today's links</h2>
<ul>
<li><a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#not-up-to-the-task">Google reneged on the monopolistic bargain</a>: A search monopolist should never do a layoff, buyback or dividend for so long as it's enshittifying.
</li>
<li><a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#poacher-turned-keeper">The Bezzle excerpt (Part IV)</a>: LA Sheriff's Deputies are the most violent gangs in LA.
</li>
<li><a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#linkdump">Hey look at this</a>: Delights to delectate.
</li>
<li><a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#retro">This day in history</a>: 2009, 2014, 2019, 2023
</li>
<li><a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#bragsheet">Colophon</a>: Recent publications, upcoming/recent appearances, current writing projects, current reading
</li>
</ul>

<hr>
<p><a name="not-up-to-the-task"></a><br>
<img decoding="async" alt="A picture postcard of a idyllic small town main street. Looming over the scene is a hypersaturated can of Spam. In the foreground is a sleeping German shepherd with Google logos over its eyes. It sports a dream-bubble with a lunging attack dog." src="https://i0.wp.com/craphound.com/images/goog-sleeping-watchdog.jpg?w=840&amp;ssl=1" data-recalc-dims="1"></p>
<h2>Google reneged on the monopolistic bargain (<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#not-up-to-the-task">permalink</a>)</h2>
<p>A funny thing happened on the way to the enshittocene: Google – which astonished the world when it reinvented search, blowing Altavista and Yahoo out of the water with a search tool that seemed <em>magic</em> – suddenly turned into a pile of shit.</p>
<p>Google's search results are terrible. The top of the page is dominated by spam, scams, and ads. A surprising number of those ads <em>are</em> scams. Sometimes, these are high-stakes scams played out by well-resourced adversaries who stand to make a fortune by tricking Google:</p>
<p><a href="https://www.nbcnews.com/tech/tech-news/phone-numbers-airlines-listed-google-directed-scammers-rcna94766">https://www.nbcnews.com/tech/tech-news/phone-numbers-airlines-listed-google-directed-scammers-rcna94766</a></p>
<p>But often these scams are perpetrated by petty grifters who are making a couple bucks at this. These aren't hyper-resourced, sophisticated attackers. They're the SEO equivalent of script kiddies, and they're running circles around Google:</p>
<p><a href="https://pluralistic.net/2023/02/24/passive-income/#swiss-cheese-security">https://pluralistic.net/2023/02/24/passive-income/#swiss-cheese-security</a></p>
<p>Google search is empirically worsening. The SEO industry spends every hour that god sends trying to figure out how to sleaze their way to the top of the search results, and even if Google defeats 99% of these attempts, the 1% that squeak through end up dominating the results page for any consequential query:</p>
<p><a href="https://downloads.webis.de/publications/papers/bevendorff_2024a.pdf">https://downloads.webis.de/publications/papers/bevendorff_2024a.pdf</a></p>
<p>Google insists that this isn't true, and if it is true, it's not their fault because the bad guys out there are so numerous, dedicated and inventive that Google can't help but be overwhelmed by them:</p>
<p><a href="https://searchengineland.com/is-google-search-getting-worse-389658">https://searchengineland.com/is-google-search-getting-worse-389658</a></p>
<p>It wasn't supposed to be this way. Google has long maintained that its scale is the only thing that keeps us safe from the scammers and spammers who would otherwise overwhelm any lesser-resourced defender. That's why it was so imperative that they pursue such aggressive growth, buying up hundreds of companies and integrating their products with search so that every mobile device, every ad, every video, every website, had one of Google's tendrils in it.</p>
<p>This is the argument that Google's defenders have put forward in their messaging on the long-overdue antitrust case against Google, where we learned that Google is spending $26b/year to make sure you never try another search engine:</p>
<p><a href="https://www.bloomberg.com/news/articles/2023-10-27/google-paid-26-3-billion-to-be-default-search-engine-in-2021">https://www.bloomberg.com/news/articles/2023-10-27/google-paid-26-3-billion-to-be-default-search-engine-in-2021</a></p>
<p>Google, we were told, had achieved such intense scale that the normal laws of commercial and technological physics no longer applied. Take security: it's an iron law that "there is no security in obscurity." A system that is only secure when its adversaries don't understand how it works is not a secure system. As Bruce Schneier says, "anyone can design a security system that they themselves can't break. That doesn't mean it works – just that it works for people stupider than <em>them</em>."</p>
<p>And yet, Google operates one of the world's most consequential security system – The Algorithm (TM) – in total secrecy. We're not allowed to know how Google's ranking system works, what its criteria are, or even when it changes: "If we told you that, the spammers would win."</p>
<p>Well, they kept it a secret, and the spammers won anyway.</p>
<p>A viral post by Housefresh – who review air purifiers – describes how Google's algorithmic failures, which send the worst sites to the top of the heap, have made it impossible for high-quality review sites to compete:</p>
<p><a href="https://housefresh.com/david-vs-digital-goliaths/">https://housefresh.com/david-vs-digital-goliaths/</a></p>
<p>You've doubtless encountered these bad review sites. Search for "Best ______ 2024" and the results are a series of near-identical lists, strewn with Amazon affiliate links. Google has endlessly tinkered with its guidelines and algorithmic weights for review sites, and none of it has made a difference. For example, when Google instituted a policy that reviewers should "discuss the benefits and drawbacks of something, based on your own original research," sites that had previously regurgitated the same lists of the same top ten Amazon bestsellers "peppered their pages with references to a ‘rigorous testing process,’ their ‘lab team,’ subject matter experts ‘they collaborated with,’ and complicated methodologies that seem impressive at a cursory look."</p>
<p>But these grandiose claims – like the 67 air purifiers supposedly tested in <em>Better Homes and Gardens</em>'s Des Moines lab – result in zero in-depth reviews and no published data. Moreover, these claims to rigorous testing materialized within a few days of Google changing its search ranking and said that high rankings would be reserved for sites that did testing.</p>
<p>Most damning of all is how the <em>Better Homes and Gardens</em> top air purifiers perform in comparison to the – extensively documented – tests performed by Housefresh: "plagued by high-priced and underperforming units, Amazon bestsellers with dubious origins (that also underperform), and even subpar devices from companies that market their products with phrases like ‘the Tesla of air purifiers.’"</p>
<p>One of the top ranked items on <em>BH&amp;G</em> comes from Molekule, a company that filed for bankruptcy after being sued for false advertising. The model <em>BH&amp;G</em> chose was ranked "the worst air purifier tested" by Wirecutter and "not living up to the hype" by <em>Consumer Reports</em>. Either <em>BH&amp;G</em>'s rigorous testing process is a fiction that they infused their site with in response to a Google policy change, or <em>BH&amp;G</em> absolutely <em>sucks</em> at rigorous testing.</p>
<p><em>BH&amp;G</em>'s competitors commit the same sins – literally, the <em>exact</em> same sins. <em>Real Simple</em>'s reviews list the same photographer and the photos seem to have been taken in the same place. They also list the same person as their "expert." <em>Real Simple</em> has the same corporate parent as <em>BH&amp;G</em>: Dotdash Meredith. As Housefresh shows, there's a <em>lot</em> of Dotdash Meredith review photos that seem to have been taken in the same place, by the same person.</p>
<p>But the competitors of these magazines are no better. Buzzfeed lists <em>22</em> air purifiers, including that crapgadget from Molekule. Their "methodology" is to include screenshots of Amazon reviews.</p>
<p>A lot of the top ranked sites for air purifiers are once-great magazines that have been bought and enshittified by private equity giants, like <em>Popular Science</em>, which began as a magazine in 1872 and became a shambling zombie in 2023, after its PE owners North Equity LLC decided its googlejuice was worth more than its integrity and turned it into a metastatic chumbox of shitty affiliate-link SEO-bait. As Housefresh points out, the marketing team that runs <em>PopSci</em> makes a lot of hay out of the 150 years of trust that went into the magazine, but the actual reviews are thin anaecdotes, unbacked by even the pretense of empiricism (oh, and they <em>loooove</em> Molekule).</p>
<p>Some of the biggest, most powerful, most trusted publications in the world have a side-hustle in quietly producing SEO-friendly "10 Best ___________ of 2024" lists: <em>Rolling Stone</em>, <em>Forbes</em>, <em>US News and Report</em>, <em>CNN</em>, <em>New York Magazine</em>, <em>CNET</em>, <em>Tom's Guide</em>, and more.</p>
<p>Google literally has one job: to detect this kind of thing and crush it. The deal we made with Google was, "You monopolize search and use your monopoly rents to ensure that we never, ever try another search engine. In return, you will somehow distinguish between low-effort, useless nonsense and good information. You promised us that  if you got to be the unelected, permanent overlord of all information access, you would 'organize the world's information and make it universally accessible and useful.'"</p>
<p>They broke the deal.</p>
<p>Companies like CNET <em>used</em> to do real, rigorous product reviews. As Housefresh points out, CNET once bought an <em>entire smart home</em> and used it to test products. Then Red Ventures bought CNET and bet that they could sell the house, switch to vibes-based reviewing, and that Google wouldn't even notice. They were right.</p>
<p><a href="https://www.cnet.com/home/smart-home/welcome-to-the-cnet-smart-home/">https://www.cnet.com/home/smart-home/welcome-to-the-cnet-smart-home/</a></p>
<p>Google downranks sites that spend money and time on reviews like Housefresh and GearLab, and crams botshittened content mills like <em>BH&amp;G</em> into our eyeballs instead.</p>
<p>In 1558, Thomas Gresham coined (ahem) Gresham's Law: "Bad money drives out good." When counterfeit money circulates in the economy, anyone who gets a dodgy coin spends it as quickly as they can, because the longer you hold it, the greater the likelihood that someone will detect the fraud and the coin will become worthless. Run this system long enough and all the money in circulation is funny money.</p>
<p>An internet run by Google has its own Gresham's Law: bad sites drive out good. It's not just that <em>BH&amp;G</em> can "test" products at a fraction of the cost of Housefresh – through the simple expedient of doing inadequate tests or no tests at all – so they can put a lot more content up that Housefresh. But that alone wouldn't let them drive Housefresh off the front page of Google's search results. For that, <em>BH&amp;G</em> has to mobilize some of their savings from the no test/bad test lab to do <em>real</em> rigorous science: science in defeating Google's security-through-obscurity system, which lets them command the front page despite publishing worse-than-useless nonsense.</p>
<p>Google has lost the spam wars. In response to the plague of botshit clogging Google search results, the company has invested in…making more botshit:</p>
<p><a href="https://pluralistic.net/2023/02/16/tweedledumber/#easily-spooked">https://pluralistic.net/2023/02/16/tweedledumber/#easily-spooked</a></p>
<p>Last year, Google did a $70b stock buyback. They also laid off 12,000 staffers (whose salaries could have been funded for 27 years by that stock buyback). They just laid off thousands more employees.</p>
<p>That wasn't the deal. The deal was that Google would get a monopoly, and they would spend their monopoly rents to be so good that you could just click "I'm feeling lucky" and be teleported to the very best response to your query. A company that can't figure out the difference between a scam like <em>Better Homes and Gardens</em> and a rigorous review site like Housefresh should be pouring every spare dime it brings in into <em>fixing this problem</em>. Not buying default search status on every platform so that we never try another search engine: they should be <em>fixing their shit</em>.</p>
<p>When Google admits that it's losing the war to these kack-handed spam-farmers, that's frustrating. When they light $26b/year on fire making sure you don't ever get to try anything else, that's <em>very</em> frustrating. When they vaporize <em>seventy billion dollars</em> on financial engineering and shoot one in ten engineers, that's <em>outrageous</em>.</p>
<p>Google's scale <em>has</em> transcended the laws of business physics: they can sell an ever-degrading product and command an ever-greater share of our economy, even as their incompetence dooms any decent, honest venture to obscurity while providing fertile ground – and endless temptation – for scammers.</p>
<hr>
<p><a name="poacher-turned-keeper"></a><br>
<img decoding="async" alt="The cover of the Tor Books edition of *The Bezzle*: a yellow background with the words 'Cory Doctorow,' 'The Bezzle,' 'New York Times Bestselling Author,' and 'A Martin Hench novel.' Between them is an escheresque impossible triangle. The center of the triangle is a barred, smaller triangle (in blue, black and cream) that imprisons a silhouetted male figure in a suit. Two other male silhouettes in suits run alongside the top edges of the triangle." src="https://i0.wp.com/craphound.com/images/bezzle-cover.jpg?w=840&amp;ssl=1" data-recalc-dims="1"></p>
<h2>The Bezzle excerpt (Part IV) (<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#poacher-turned-keeper">permalink</a>)</h2>
<p>This week marks the publication of my latest novel, <em>The Bezzle</em>, and to celebrate, I'm serializing an excerpt from Chapter 14 in six parts:</p>
<p><a href="https://us.macmillan.com/books/9781250865878/thebezzle">https://us.macmillan.com/books/9781250865878/thebezzle</a></p>
<p><em>The Bezzle</em> is a revenge story, a crime novel, and a technothriller. It stars Martin Hench, a hard-fighting forensic accountant who specializes in unwinding high-tech scams. Hench made his debt in last year's <em>Red Team Blues</em> (now in paperback!); <em>The Bezzle</em> is a standalone followup:</p>
<p><a href="https://us.macmillan.com/books/9781250865854/redteamblues">https://us.macmillan.com/books/9781250865854/redteamblues</a></p>
<p>The serial tells the tale of Stefon Magner, AKA Steve Soul, a once-famous R&amp;B frontman whose disintegrating career turned to tragedy when his crooked manager forged his signature on a rights assignment contract that let him steal all of Stefon's royalties, which ballooned after modern hiphop artists discovered his grooves and started buying licenses to sample them. The first three installments related the sad circumstances of Stefon's life, and the real-world analogues (like Leonard Cohen and George Clinton, both of whom were pauperized by sticky-fingered managers) as well as one real-world countermeasure, copyright termination, a thing that more artists should know about and use:</p>
<p><a href="https://pluralistic.net/2021/09/26/take-it-back/">https://pluralistic.net/2021/09/26/take-it-back/</a></p>
<p>Today's installment weaves in a major subplot for the first time in the serial: Los Angeles's notorious, murderous Sheriff's Deputy gangs. These are another unbelievable true tale: for decades, the LASD's deputies have formed themselves into criminal gangs, some of which require that initiates murder someone to be inducted:</p>
<p><a href="https://en.wikipedia.org/wiki/List_of_LASD_deputy_gangs">https://en.wikipedia.org/wiki/List_of_LASD_deputy_gangs</a></p>
<p>They sport gang tattoos, have secret signs, and run vast criminal enterprises. This has been the subject of numerous investigative press reports, and one extensive official report that called the gangs "a cancer":</p>
<p><a href="https://www.nbcnews.com/news/us-news/deputy-gangs-cancer-los-angeles-county-sheriffs-department-scathing-re-rcna73367">https://www.nbcnews.com/news/us-news/deputy-gangs-cancer-los-angeles-county-sheriffs-department-scathing-re-rcna73367</a></p>
<p>The sordid tales of the LASD gangs beggar belief. For example, deputies in charge of LA County jails forced inmates to pit-fight and took bets on the outcomes:</p>
<p><a href="https://www.aclu.org/publications/report-cruel-and-usual-punishment-how-savage-gang-deputies-controls-la-county-jails">https://www.aclu.org/publications/report-cruel-and-usual-punishment-how-savage-gang-deputies-controls-la-county-jails</a></p>
<p>The taxpayers of LA have shelled out tens of millions of dollars to settle claims against LA's criminals with badges:</p>
<p><a href="https://news.yahoo.com/deputies-accused-being-secret-societies-230851807.html">https://news.yahoo.com/deputies-accused-being-secret-societies-230851807.html</a></p>
<p>Periodically, LA judges and officials will insist that they are tackling the problem:</p>
<p><a href="https://www.latimes.com/california/story/2023-05-17/dozens-of-lasd-deputies-ordered-to-show-suspected-gang-tattoos-reveal-others-who-have-them">https://www.latimes.com/california/story/2023-05-17/dozens-of-lasd-deputies-ordered-to-show-suspected-gang-tattoos-reveal-others-who-have-them</a></p>
<p>But at every turn, the LA police "unions" manage to crush these investigations:</p>
<p><a href="https://abc7.com/los-angeles-county-lasd-deputy-gangs-cliques/13492081/">https://abc7.com/los-angeles-county-lasd-deputy-gangs-cliques/13492081/</a></p>
<p>And top cops are right there with them, insisting that these aren't "gangs" – they're just "subgroups":</p>
<p><a href="https://lapublicpress.org/2024/01/former-la-sheriff-villanueva-sheriffs-gangs-are-just-subgroups/">https://lapublicpress.org/2024/01/former-la-sheriff-villanueva-sheriffs-gangs-are-just-subgroups/</a></p>
<p>It's very weird being an Angeleno and knowing that one of the largest, most militarized, best funded police departments in the world has been openly captured by a hyperviolent crime syndicate. When I was in the Skyboat Media studios last December with Wil Wheaton recording the audiobook for <em>The Bezzle</em>, Wil broke off from reading to say, "You know, someone's going to read this and google it and have their mind blown when they discover that it's real":</p>
<p><a href="https://sowl.co/8nyGh">https://sowl.co/8nyGh</a></p>
<p>That's one of my favorite ways to turn literature into something more than entertainment. It's why I filled the <em>Little Brother</em> books with real-world surveillance, cryptography and security tech, giving enough detail to advance the plot <em>and</em> give readers an idea of what search terms would let them understand and use the concepts in the novel. That's something I'm happy to keep up with the <em>Hench</em> novels, unpicking the inner workings of scams and corruption. The more of us who are wise to this, the sooner we'll be able to get rid of it.</p>
<p>Here's part one of the serial:</p>
<p><a href="https://pluralistic.net/2024/02/17/the-steve-soul-caper/#lead-singer-disease">https://pluralistic.net/2024/02/17/the-steve-soul-caper/#lead-singer-disease</a></p>
<p>Part two:</p>
<p><a href="https://pluralistic.net/2024/02/19/crad-kilodney-was-an-outlier/#copyright-termination">https://pluralistic.net/2024/02/19/crad-kilodney-was-an-outlier/#copyright-termination</a></p>
<p>Part three:</p>
<p><a href="https://pluralistic.net/2024/02/20/fore/#lawyer-up">https://pluralistic.net/2024/02/20/fore/#lawyer-up</a></p>
<p>And now, onto part four!</p>
<center> * * *</center>
<p>The last of the boxes had been shelved.</p>
<p>Benedetto rose from his chair. “Thank you, gentlemen,” he said to the movers, and dug a roll of twenties out of his pocket and handed each of them two of their own. He turned to me as they filed out. “You wanna get sushi? The place next door is <em>great</em>.”</p>
<p>The empty storefront was in a down-­at-­heels strip mall in Eagle Rock. On one side, there was a Brazilian jujitsu studio that never seemed to have any students training in it. On the other side was Sushi Jiro, name on a faded sign with half its lightbulbs gone. Beyond that was a vaping store.</p>
<p>“The place next door is <em>good</em>?”</p>
<p>He laughed. “You San Francisco motherfuckers got terrible LA restaurant radar. Put Sushi Jiro in the Mission and it’d have a Michelin star and a six-­month waiting list. Here it’s in a strip mall and only the locals know how good it is. Bet you never had a decent meal in this town, am I right?”</p>
<p>“I’ve had a few,” I said, “but I admit my track record isn’t great.”</p>
<p>“Let’s improve it.”</p>
<p>The sushi was amazing.</p>
<p>#</p>
<p>Inglewood Jams had the kind of books that were <em>performatively</em> bad, designed to foil any attempt at human comprehension.</p>
<p>But whoever cooked them was an amateur, someone who mistook <em>complexity</em> for <em>obfuscation</em>. Like <em>cross-­referencing</em> was a species of transcendentally esoteric sorcery. I don’t mind cross-referencing. It’s meditative, like playing solitaire. I had Bene­detto send over some colored post-­it tabs and a big photocopier with an automatic feeder and I started making piles.</p>
<p>One night, I worked later than I planned. Sushi Jiro was becoming a serious hazard to my waistline and my sleep-­debt, because when your dinner break is ten yards and two doors away from your desk, it’s just too damned easy to get back to work after dinner.</p>
<p>That night, I’d fallen into a cross-­referencing reverie, and before I knew it, it was 2 a.m., my lower back was groaning, and my eyes were stinging.</p>
<p>I straightened, groaned, and slid my laptop into my bag. I found my keys and unlocked the door. The storefront was covered with brown butcher’s paper, but it didn’t go all the way to the edge. I had just a moment to sleepily note that there was some movement visible through the crack in the paper over the glass door when it came flying back toward me, bouncing off my toe, mostly, and my nose, a little. I put my one hand to my face as I instinctively threw myself into the door to close it again.</p>
<p>I was too late and too tired. A strong shoulder on the other side of the doorframe pushed it open and I stumbled back, and then the guy was on me, the door sighing shut behind him on its gas lift as he bore me to the ground and straddled my chest, a move he undertook with the ease of much practice. He pinned my arms under his knees and then gave me a couple of hard hits, one to the jaw, one to the nose.</p>
<p>My lip and nose were bleeding freely and my head was ringing from the hits and from getting smacked into the carpet tiles over concrete when I went down backward. I struggled—­to free my arms, to buck off my attacker, to focus on him.</p>
<p>He was a beefy white guy in his late fifties, with watery dark eyes and a patchy shave that showed gray mixed in with his dark stubble. As he raised his fist for another blow, I saw that he was wearing a big class ring. A minute later, that ring opened my cheek, just under the orbit of my eye.</p>
<p>Apart from some involuntary animal grunts, I hadn’t made a sound. Now I did. “Ow!” I shouted. “Shit!” I shouted. “Stop!” I shouted.</p>
<p>He split my lip again. I bucked hard but I couldn’t budge him. He had a double chin, a gut, and he was strong, and used that bulk to back up his strength. It was like trying to free myself from under a boulder. That kept punching me in the face.</p>
<p>The strip mall would be deserted. Everything was closed, even the vaping store.</p>
<p>Shouting wouldn’t help. I did it anyway. He shut my mouth for me with a left. I gagged on blood.</p>
<p>He took a break from punching me in the face, then. I think he was tired. His chest heaved, and he wiped sweat off his lip with the back of his hand, leaving behind a streaky mustache of my blood.</p>
<p>He <em>contemplated</em> me, weighing me up. I thought maybe he was trying to decide if I had any fight left in me, or perhaps whether I had any valuables he could help himself to.</p>
<p>He cleared his throat and looked at me again. “Goddammit, I messed your face up so bad I can’t tell for sure. I hope to fuck that you’re Martin Hench, though.”</p>
<p>Even with my addled wits, this was an important piece of intelligence: <em>he came here for me</em>. This wasn’t a random act of senseless Los Angeles street violence. This was aimed at <em>me</em>.</p>
<p>I was briefly angry at Benedetto for not warning me that Chuy Flores was such a tough son of a bitch. Then I had the presence of mind to lie.</p>
<p>“I don’t know who the fuck this Mark Hendricks is.” My voice was thick with gargled blood, but I was proud of <em>Mark Hendricks</em>. Pretty fast thinking for a guy with a probable concussion. The guy slapped me open-­handed across the face, and as I lay dazed for a moment, he shifted, reached into my back pocket for my wallet, and yanked it—­and the seat of my pants—­free. Before I could react, his knees were back on my biceps, pinning my arms and shoulders. It was a very neat move, and fast for an old guy like him.</p>
<p>He flipped my wallet open and squinted at it, then held it at arm’s length, then smiled broadly. He had bleach-­white teeth, a row of perfectly uniform caps. Los fucking Angeles, where even the thugs have a million-­dollar smile.</p>
<p>“Shoulda sprung for botox,” I slurred.</p>
<p>His grin got wider. “Maybe someday I will. Got these in trade from a cosmetic dentist I did some work for.” He dropped my wallet. “Listen, Martin Hench, you stay the fuck away from Thames Estuary and Lawrence Coleman.”</p>
<p>“It’s Lionel Coleman,” I said.</p>
<p>“What the fuck ever,” he said. He labored to his feet. I stayed still. He looked at me from a great height, and I stared up his nostrils. Without warning, he kicked my ribs hard enough that I heard one of them crack.</p>
<p>“You’ve been told,” he said to my writhing body, and let himself out.</p>
<hr>

<h2>Hey look at this (<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#linkdump">permalink</a>)</h2>
<p><img decoding="async" src="https://i0.wp.com/craphound.com/images/heylookatthis.jpg?w=840&amp;ssl=1" data-recalc-dims="1"></p>
<ul>
<li>Street Of Satisfaction <a href="https://soundcloud.com/dj-moule/street-of-satisfaction">https://soundcloud.com/dj-moule/street-of-satisfaction</a>
</li>
<li>
<p>Don’t Fall for the Latest Changes to the Dangerous Kids Online Safety Act <a href="https://www.eff.org/deeplinks/2024/02/dont-fall-latest-changes-dangerous-kids-online-safety-act">https://www.eff.org/deeplinks/2024/02/dont-fall-latest-changes-dangerous-kids-online-safety-act</a></p>
</li>
<li>
<p>Why The New York Times might win its copyright lawsuit against OpenAI <a href="https://arstechnica.com/tech-policy/2024/02/why-the-new-york-times-might-win-its-copyright-lawsuit-against-openai/">https://arstechnica.com/tech-policy/2024/02/why-the-new-york-times-might-win-its-copyright-lawsuit-against-openai/</a></p>
</li>
</ul>
<hr>
<p><a name="retro"></a><br>
<img decoding="async" alt="A Wayback Machine banner." src="https://i0.wp.com/craphound.com/images/wayback-machine-hed-796x416.png?resize=796%2C416&amp;ssl=1" data-recalc-dims="1"></p>
<h2>This day in history (<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#retro">permalink</a>)</h2>
<p>#15yrsago Tesla coils sing the Doctor Who theme in orgy of electric awesomesauce <a href="https://arcattack.com/about-us/">https://arcattack.com/about-us/</a></p>
<p>#15yrsago HOWTO Make a duct-tape corsage <a href="https://www.instructables.com/How_to_Make_a_Duct_Tape_Corsage_and_Flowers/">https://www.instructables.com/How_to_Make_a_Duct_Tape_Corsage_and_Flowers/</a></p>
<p>#15yrsago 419 scammer impersonates the nation of Ethiopia, takes $27 million from Citibank <a href="https://www.nytimes.com/2009/02/21/nyregion/21scam.html">https://www.nytimes.com/2009/02/21/nyregion/21scam.html</a></p>
<p>#15yrsago On the demise of books, newspapers, music and movies <a href="https://web.archive.org/web/20090301000000*/https://www.internetevolution.com/document.asp?doc_id=171555&amp;">https://web.archive.org/web/20090301000000*/https://www.internetevolution.com/document.asp?doc_id=171555&amp;</a></p>
<p>#15yrsago Future of News video from 1981: epitome of foolish futurism <a href="https://memex.craphound.com/2009/02/20/future-of-news-video-from-1981-epitome-of-dumb-futurism/">https://memex.craphound.com/2009/02/20/future-of-news-video-from-1981-epitome-of-dumb-futurism/</a></p>
<p>#10yrsago Comic explains problems with Oakland’s Domain Awareness Center surveillance plan <a href="https://medium.com/the-nib/the-testing-ground-for-the-new-surveillance-db4f129a7177">https://medium.com/the-nib/the-testing-ground-for-the-new-surveillance-db4f129a7177</a></p>
<p>#10yrsago Well-Sorted Version, an alphabetical Bible <a href="https://www.wellsortedversion.com/">https://www.wellsortedversion.com</a></p>
<p>#10yrsago Sculptor collaborates with honeybees to cover statues with comb <a href="https://www.thisiscolossal.com/2014/02/artist-aganetha-dyck-collaborates-with-bees-to-create-sculptures-wrapped-in-honeycomb/">https://www.thisiscolossal.com/2014/02/artist-aganetha-dyck-collaborates-with-bees-to-create-sculptures-wrapped-in-honeycomb/</a></p>
<p>#10yrsago Obama’s top Trans-Pacific Partnership officials were given millions by banks before taking the job <a href="https://www.republicreport.org/2014/big-banks-tpp/">https://www.republicreport.org/2014/big-banks-tpp/</a></p>
<p>#10yrsago Report from a meeting of Wall Street’s secret, tasteless plutocrats’ club <a href="https://nymag.com/intelligencer/2014/02/i-crashed-a-wall-street-secret-society.html">https://nymag.com/intelligencer/2014/02/i-crashed-a-wall-street-secret-society.html</a></p>
<p>#10yrsago Edgar Allan Poe’s interior design proscriptions <a href="https://www.smithsonianmag.com/arts-culture/edgar-allan-poe-interior-design-critic-180949543/">https://www.smithsonianmag.com/arts-culture/edgar-allan-poe-interior-design-critic-180949543/</a></p>
<p>#10yrsago Museums and the free world: keynote from the Museums and the Web conference in Florence <a href="https://mwf2014.museumsandtheweb.com/paper/glam-and-the-free-world/">https://mwf2014.museumsandtheweb.com/paper/glam-and-the-free-world/</a></p>
<p>#5yrsago Florida inmates sue prisons for digitally confiscating the music they were sold <a href="https://www.jacksonville.com/story/news/crime/2019/02/19/florida-prisoners-could-form-class-action-to-demand-refund-on-confiscated-media-players-and-files/5881894007/">https://www.jacksonville.com/story/news/crime/2019/02/19/florida-prisoners-could-form-class-action-to-demand-refund-on-confiscated-media-players-and-files/5881894007/</a></p>
<p>#5yrsago Owner of civil war reenactment business sues school district that canceled field trips after his far-right social media came to light <a href="https://www.latimes.com/local/california/la-me-rileys-farm-lawsuit-oak-glen-20190219-story.html">https://www.latimes.com/local/california/la-me-rileys-farm-lawsuit-oak-glen-20190219-story.html</a></p>
<p>#5yrsago FDA: infusing young people’s blood will not improve your health <a href="https://edition.cnn.com/2019/02/19/health/plasma-infusion-young-blood-fda-bn/index.html">https://edition.cnn.com/2019/02/19/health/plasma-infusion-young-blood-fda-bn/index.html</a></p>
<p>#5yrsago A malicious USB cable with its own wifi rig <a href="https://twitter.com/_MG_/status/1094389042685259776">https://twitter.com/_MG_/status/1094389042685259776</a></p>
<p>#5yrsago Magic Lantern: feature-rich addons for Canon EOS cameras <a href="https://www.magiclantern.fm/">https://www.magiclantern.fm</a></p>
<p>#5yrsago Google admits Nest security product has a secret mic, insists it wasn’t supposed to be a secret <a href="https://www.businessinsider.com/nest-microphone-was-never-supposed-to-be-a-secret-2019-2">https://www.businessinsider.com/nest-microphone-was-never-supposed-to-be-a-secret-2019-2</a></p>
<p>#1yrago Turbotax is blitzing Congress for the right to tax YOU <a href="https://pluralistic.net/2023/02/20/turbotaxed/#counter-intuit">https://pluralistic.net/2023/02/20/turbotaxed/#counter-intuit</a></p>
<hr>
<p><a name="bragsheet"></a><br>
<img decoding="async" src="https://i0.wp.com/craphound.com/images/colophonimages.jpeg?w=840&amp;ssl=1" data-recalc-dims="1"></p>
<h2>Colophon (<a href="https://pluralistic.net/2024/02/21/im-feeling-unlucky/#bragsheet">permalink</a>)</h2>
<p>Today's top sources:</p>
<p>Currently writing:</p>
<ul>
<li>A Little Brother short story about DIY insulin PLANNING
</li>
<li>
<p>Picks and Shovels, a Martin Hench noir thriller about the heroic era of the PC. FORTHCOMING TOR BOOKS JAN 2025</p>
</li>
<li>
<p>The Bezzle, a Martin Hench noir thriller novel about the prison-tech industry. FORTHCOMING TOR BOOKS FEB 2024</p>
</li>
<li>
<p>Vigilant, Little Brother short story about remote invigilation. FORTHCOMING ON TOR.COM</p>
</li>
<li>
<p>Spill, a Little Brother short story about pipeline protests. FORTHCOMING ON TOR.COM</p>
</li>
</ul>
<p>Latest podcast: How I Got Scammed (<a href="https://craphound.com/news/2024/02/18/how-i-got-scammed/">https://craphound.com/news/2024/02/18/how-i-got-scammed/</a>)</p>
<p>Upcoming appearances:</p>
<ul>
<li>The Bezzle at Weller Book Works (Salt Lake City), Feb 21<br>
<a href="https://www.wellerbookworks.com/event/store-cory-doctorow-feb-21-630-pm">https://www.wellerbookworks.com/event/store-cory-doctorow-feb-21-630-pm</a>
</li>
<li>
<p>The Bezzle at Mysterious Galaxy (San Diego), Feb 22<br>
<a href="https://www.mystgalaxy.com/22224Doctorow">https://www.mystgalaxy.com/22224Doctorow</a></p>
</li>
<li>
<p>The Bezzle at Vroman's (Pasadena), Feb 24<br>
<a href="https://www.vromansbookstore.com/Cory-Doctorow-discusses-The-Bezzle">https://www.vromansbookstore.com/Cory-Doctorow-discusses-The-Bezzle</a></p>
</li>
<li>
<p>The Bezzle at Third Place Books (Seattle), Feb 26<br>
<a href="https://www.thirdplacebooks.com/event/cory-doctorow">https://www.thirdplacebooks.com/event/cory-doctorow</a></p>
</li>
<li>
<p>The Bezzle at Powell's (Portland) Feb 27:<br>
<a href="https://www.powells.com/book/the-bezzle-martin-hench-2-9781250865878/1-2">https://www.powells.com/book/the-bezzle-martin-hench-2-9781250865878/1-2</a></p>
</li>
<li>
<p>The Bezzle at Changing Hands (Phoenix), Feb 29:<br>
<a href="https://www.changinghands.com/event/february2024/cory-doctorow">https://www.changinghands.com/event/february2024/cory-doctorow</a></p>
</li>
<li>
<p>Tucson Festival of Books, Mar 9/10<br>
<a href="https://tucsonfestivalofbooks.org/?id=676">https://tucsonfestivalofbooks.org/?id=676</a></p>
</li>
<li>
<p>Enshittification: How the Internet Went Bad and How to Get it Back (virtual), Mar 26<br>
<a href="https://libcal.library.ubc.ca/event/3781006">https://libcal.library.ubc.ca/event/3781006</a></p>
</li>
<li>
<p>Wondercon Anaheim, Mar 29-31<br>
<a href="https://www.comic-con.org/wc/">https://www.comic-con.org/wc/</a></p>
</li>
<li>
<p>The Bezzle at Anderson's Books (Chicago), Apr 17<br>
<a href="https://www.andersonsbookshop.com/event/cory-doctorow-1">https://www.andersonsbookshop.com/event/cory-doctorow-1</a></p>
</li>
<li>
<p>Torino Biennale Tecnologia (Apr 19-21)<br>
<a href="https://www.turismotorino.org/en/experiences/events/biennale-tecnologia">https://www.turismotorino.org/en/experiences/events/biennale-tecnologia</a></p>
</li>
<li>
<p>Canadian Centre for Policy Alternatives (Winnipeg), May 2<br>
<a href="https://www.eventbrite.ca/e/cory-doctorow-tickets-798820071337?aff=oddtdtcreator">https://www.eventbrite.ca/e/cory-doctorow-tickets-798820071337?aff=oddtdtcreator</a></p>
</li>
<li>
<p>Tartu Prima Vista Literary Festival (May 5-11)<br>
<a href="https://tartu2024.ee/en/kirjandusfestival/">https://tartu2024.ee/en/kirjandusfestival/</a></p>
</li>
<li>
<p>Media Ecology Association keynote (Amherst, NY), Jun 6-9<br>
<a href="https://media-ecology.org/convention">https://media-ecology.org/convention</a></p>
</li>
<li>
<p>American Association of Law Libraries keynote (Chicago), Jul 21<br>
<a href="https://www.aallnet.org/conference/agenda/keynote-speaker/">https://www.aallnet.org/conference/agenda/keynote-speaker/</a></p>
</li>
</ul>
<p>Recent appearances:</p>
<ul>
<li>This Is Hell<br>
<a href="https://thisishell.com/interviews/1701-cory-doctorow">https://thisishell.com/interviews/1701-cory-doctorow</a>
</li>
<li>
<p>From privacy to paper jams, a look at printer problems<br>
<a href="https://www.ctpublic.org/show/the-colin-mcenroe-show/2024-02-15/from-privacy-to-paper-jams-a-look-at-printer-problems">https://www.ctpublic.org/show/the-colin-mcenroe-show/2024-02-15/from-privacy-to-paper-jams-a-look-at-printer-problems</a></p>
</li>
<li>
<p>Big Story Podcast<br>
<a href="https://thebigstorypodcast.ca/2024/02/13/a-story-about-how-anyone-yes-even-you-can-get-scammed/">https://thebigstorypodcast.ca/2024/02/13/a-story-about-how-anyone-yes-even-you-can-get-scammed/</a></p>
</li>
</ul>
<p>Latest books:</p>
<ul>
<li>The Bezzle: a sequel to "Red Team Blues," about prison-tech and other grifts, Tor Books (US), Head of Zeus (UK), February 2024 (<a href="http://the-bezzle.org/">the-bezzle.org</a>). Signed, personalized copies at Dark Delicacies (<a href="https://www.darkdel.com/store/p3062/Available_Feb_20th%3A_The_Bezzle_HB.html#/">https://www.darkdel.com/store/p3062/Available_Feb_20th%3A_The_Bezzle_HB.html#/</a>).
</li>
<li>
<p>"The Lost Cause:" a solarpunk novel of hope in the climate emergency, Tor Books (US), Head of Zeus (UK), November 2023 (<a href="http://lost-cause.org/">http://lost-cause.org</a>). Signed, personalized copies at Dark Delicacies (<a href="https://www.darkdel.com/store/p3007/Pre-Order_Signed_Copies%3A_The_Lost_Cause_HB.html#/">https://www.darkdel.com/store/p3007/Pre-Order_Signed_Copies%3A_The_Lost_Cause_HB.html#/</a>)</p>
</li>
<li>
<p>"The Internet Con": A nonfiction book about interoperability and Big Tech (Verso) September 2023 (<a href="http://seizethemeansofcomputation.org/">http://seizethemeansofcomputation.org</a>). Signed copies at Book Soup (<a href="https://www.booksoup.com/book/9781804291245">https://www.booksoup.com/book/9781804291245</a>).</p>
</li>
<li>
<p>"Red Team Blues": "A grabby, compulsive thriller that will leave you knowing more about how the world works than you did before." Tor Books <a href="http://redteamblues.com/">http://redteamblues.com</a>. Signed copies at Dark Delicacies (US): <a href="https://www.darkdel.com/store/p2873/Wed%2C_Apr_26th_6pm%3A_Red_Team_Blues%3A_A_Martin_Hench_Novel_HB.html#/"> and Forbidden Planet (UK): </a><a href="https://forbiddenplanet.com/385004-red-team-blues-signed-edition-hardcover/">https://forbiddenplanet.com/385004-red-team-blues-signed-edition-hardcover/</a>.</p>
</li>
<li>
<p>"Chokepoint Capitalism: How to Beat Big Tech, Tame Big Content, and Get Artists Paid, with Rebecca Giblin", on how to unrig the markets for creative labor, Beacon Press/Scribe 2022 <a href="https://chokepointcapitalism.com/">https://chokepointcapitalism.com</a></p>
</li>
<li>
<p>"Attack Surface": The third Little Brother novel, a standalone technothriller for adults. The <em>Washington Post</em> called it "a political cyberthriller, vigorous, bold and savvy about the limits of revolution and resistance." Order signed, personalized copies from Dark Delicacies <a href="https://www.darkdel.com/store/p1840/Available_Now%3A_Attack_Surface.html">https://www.darkdel.com/store/p1840/Available_Now%3A_Attack_Surface.html</a></p>
</li>
<li>
<p>"How to Destroy Surveillance Capitalism": an anti-monopoly pamphlet analyzing the true harms of surveillance capitalism and proposing a solution. <a href="https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59?sk=f6cd10e54e20a07d4c6d0f3ac011af6b">https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59?sk=f6cd10e54e20a07d4c6d0f3ac011af6b</a>) (signed copies: <a href="https://www.darkdel.com/store/p2024/Available_Now%3A__How_to_Destroy_Surveillance_Capitalism.html">https://www.darkdel.com/store/p2024/Available_Now%3A__How_to_Destroy_Surveillance_Capitalism.html</a>)</p>
</li>
<li>
<p>"Little Brother/Homeland": A reissue omnibus edition with a new introduction by Edward Snowden: <a href="https://us.macmillan.com/books/9781250774583">https://us.macmillan.com/books/9781250774583</a>; personalized/signed copies here: <a href="https://www.darkdel.com/store/p1750/July%3A__Little_Brother_%26_Homeland.html">https://www.darkdel.com/store/p1750/July%3A__Little_Brother_%26_Homeland.html</a></p>
</li>
<li>
<p>"Poesy the Monster Slayer" a picture book about monsters, bedtime, gender, and kicking ass. Order here: <a href="https://us.macmillan.com/books/9781626723627">https://us.macmillan.com/books/9781626723627</a>. Get a personalized, signed copy here: <a href="https://www.darkdel.com/store/p2682/Corey_Doctorow%3A_Poesy_the_Monster_Slayer_HB.html#/">https://www.darkdel.com/store/p2682/Corey_Doctorow%3A_Poesy_the_Monster_Slayer_HB.html#/</a>.</p>
</li>
</ul>
<p>Upcoming books:</p>
<ul>
<li>Picks and Shovels: a sequel to "Red Team Blues," about the heroic era of the PC, Tor Books, February 2025
</li>
<li>
<p>Unauthorized Bread: a graphic novel adapted from my novella about refugees, toasters and DRM, FirstSecond, 2025</p>
</li>
</ul>
<hr>
<p><img decoding="async" src="https://i0.wp.com/craphound.com/images/by.svg.png?w=840&amp;ssl=1" data-recalc-dims="1"></p>
<p>This work – excluding any serialized fiction – is licensed under a Creative Commons Attribution 4.0 license. That means you can use it any way you like, including commercially, provided that you attribute it to me, Cory Doctorow, and include a link to pluralistic.net.</p>
<p><a href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></p>
<p>Quotations and images are not included in this license; they are included either under a limitation or exception to copyright, or on the basis of a separate license. Please exercise caution.</p>
<hr>
<h2>How to get Pluralistic:</h2>
<p>Blog (no ads, tracking, or data-collection):</p>
<p><a href="http://pluralistic.net/">Pluralistic.net</a></p>
<p>Newsletter (no ads, tracking, or data-collection):</p>
<p><a href="https://pluralistic.net/plura-list">https://pluralistic.net/plura-list</a></p>
<p>Mastodon (no ads, tracking, or data-collection):</p>
<p><a href="https://mamot.fr/@pluralistic">https://mamot.fr/@pluralistic</a></p>
<p>Medium (no ads, paywalled):</p>
<p><a href="https://doctorow.medium.com/">https://doctorow.medium.com/</a></p>
<p>Twitter (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://twitter.com/doctorow">https://twitter.com/doctorow</a></p>
<p>Tumblr (mass-scale, unrestricted, third-party surveillance and advertising):</p>
<p><a href="https://mostlysignssomeportents.tumblr.com/tagged/pluralistic">https://mostlysignssomeportents.tumblr.com/tagged/pluralistic</a></p>
<p>"<em>When life gives you SARS, you make sarsaparilla</em>" -Joey "Accordion Guy" DeVilla</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Strawberry Pop-Tart Blow-Torches (1994) (157 pts)]]></title>
            <link>https://www.pmichaud.com/toast/</link>
            <guid>39456335</guid>
            <pubDate>Wed, 21 Feb 2024 17:00:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pmichaud.com/toast/">https://www.pmichaud.com/toast/</a>, See on <a href="https://news.ycombinator.com/item?id=39456335">Hacker News</a></p>
<div id="readability-page-1" class="page">


<em>Last Updated: 2G August 1994</em>

<h2>Author</h2>
	<a href="http://www.pmichaud.com/">
	<address>Patrick R. Michaud, pmichaud@pobox.com</address>
	</a>

<h2>Abstract</h2>
	Strawberry Pop Tarts may be a cheap and inexpensive source of
	incendiary devices.  Toasters which fail to eject Pop Tarts
	cause the Pop Tarts to emit flames 10-18 inches in height.

<h2>Introduction</h2>
	Last year, an article by well-known newspaper columnist
	<em>Dave Barry</em> noted that <em>Kellogg's Strawberry
	Pop Tarts</em> (SPTs) could be made to emit flames "like a 
	blow torch" if left in a toaster too long.  Given previous 
	work in the field of food-entertainment (see 
	<a href="http://www.pmichaud.com/grape/">
	<em>Fun With Grapes - A Case Study</em></a>), it was obvious
	that this was a new frontier that requires further exploration.  
	The present work describes our independent verification and 
	experience with SPT-based combustion.

<h2>Materials Used</h2>
	Only two basic materials are needed to cause SPT-combustion:
	a (hopefully inexpensive) toaster and some Strawberry Pop Tarts
	(Figure 1).
	In this work, the authors used <em>Kellogg's Strawberry Pop Tarts
	with Real Smucker's Fruit</em>.  SPTs can be obtained either with
	or without frosting; the non-frosted variety were used for this
	experiment.
    <p>
	<a href="https://www.pmichaud.com/toast/toast-1a.gif">
	<img src="https://www.pmichaud.com/toast/toast-1b.gif" alt="Figure 1"><br>
	<strong>Figure 1. Toaster and Strawberry Pop Tarts</strong></a>
    </p><p>
	In addition to the basic materials, a number of safety-related
	items were needed to conduct this experiment.  First, a suitable
	location for the experiment was required, it being expected that
	the kitchen was not the appropriate place for blow-torching SPTs.
	The author's driveway was chosen as a suitable site.  Second,
	an appropriate means for extinguishing the SPTs would be needed;
	a research assistant brought along some baking soda for the
	purpose.

</p><h2>Experiment Preparation</h2>
	The toaster and SPT both had to be properly prepared for this
	experiment.  In order to guarantee that the SPT would receive
	sufficient heat to begin combustion, the toaster was set to
	its highest setting and the lever was jammed in the "down"
	position using adhesive cellophane.  A SPT was removed from
	the box and its protective packaging and carefully placed into
	the toaster slot (Figure 2).
    <p>
	<a href="https://www.pmichaud.com/toast/toast-2a.gif">
	<img src="https://www.pmichaud.com/toast/toast-2c.gif" alt="Figure 2"><br>
	<strong>Figure 2. Preparation of Toaster and SPT</strong></a>
    </p><p>
	Next, the toaster and SPT were taken to the driveway, and an
	extension cord was arranged to provide power to the toaster.
	At this point, we were ready to begin the experiment.
    </p><p> 
	<a href="https://www.pmichaud.com/toast/toast-3a.gif">
	<img src="https://www.pmichaud.com/toast/toast-3c.gif" alt="Figure 3"><br>
	<strong>Figure 3. Toaster Prepared for SPT combustion</strong></a>

</p><h2>The Experiment and Observations</h2>
	The toaster was plugged in.  First the toaster went through a
	normal "toasting" cycle (approximately 60 seconds), 
	which more than thoroughly cooked the SPT (since the toaster 
	was set to its darkest setting).  By this point we could
	definitely detect a burnt SPT aroma.
	The toaster then attempted to eject the SPT, but was prevented from
	doing so by the adhesive cellophane.  The toaster then began
	emitting loud rattling and buzzing noises due to its inability
	to eject the SPT.
    <p>
	(At this point the researchers became somewhat concerned that
	the noise from the toaster would wake the neighbors and
	attract undue attention.  However, we decided that we were
	already committed to the experiment and that the neighbors
	would be able to sacrifice some sleep in the name of science.)
    </p><p>
	Soon thereafter, large amounts of smoke began pouring out of
	the toaster.  The researchers noticed that some of the neighbors
	down the street were beginning to get a little curious, but
	the experiment proceeded nonetheless.  Approximately 40 seconds
	later, small flames began licking their way out of the toaster.
	The flames steadily grew larger and larger until reaching a
	maximum height of about 18 inches above the top of the toaster.
	Figure 4 presents a time-series collage of the flames emitted
	from the SPT.
    </p><p>
	<a href="https://www.pmichaud.com/toast/toast-4a.gif">
	<img src="https://www.pmichaud.com/toast/toast-4c.gif" alt="Figure 4"><br>
	<strong>Figure 4.  Time Series Photograph of Flaming SPT</strong></a>
    </p><p>
	As the flames were reaching their maximum height, the toaster
	abruptly stopped making buzzing noises.  We speculate that the
	flames had by this point shorted the electronics within the
	toaster.  The toaster was quickly disconnected from the primary
	electrical source to avoid any potential damage to the author's
	house.  At this point, the researchers also realized that the
	heat could inadvertently melt the adhesive cellophane and
	cause the flaming SPTs to suddenly eject from the toaster.
	Unfortunately, this did not occur.  The flames continued 
	for several minutes.
    </p><p>
	At this point there was some slight concern that the flames
	might take considerable time to diminish.  We then enlisted the
	help of a reluctant research assistant to sprinkle baking soda
	on the flames.  (The reluctance was understandable given the
	potential for premature SPT ejection described in the above
	paragraph.)  The baking soda quickly extinguished the flames
	and produced still further smoke (Figure 5a).
    </p><p> 
	<a href="https://www.pmichaud.com/toast/toast-5a.gif">
	<img src="https://www.pmichaud.com/toast/toast-5c.gif" alt="Figure 5"><br>
	<strong>Figure 5. Extinguising the SPT</strong></a>
    </p><p>
	Once the flames were extinguished, the researchers noted an
	unanticipated problem: what to do with the (now defunct)
	toaster and the spent SPT.  It became obvious that the toaster
	could not be returned to the author's house due to both a 
	continued potential fire hazard and the smell of burnt
	strawberries.  In addition, it was noted that the toaster was
	still "too hot to handle," necessitating the use of a nearby
	garden hose to cool the toaster off.  This is illustrated
	in Figure 5b.  Finally it was decided to just leave the toaster
	by the curb for the sanitation experts to pick up the next
	morning (Figure 6.)
    </p><p>
	<a href="https://www.pmichaud.com/toast/toast-6a.html">
	<img src="https://www.pmichaud.com/toast/toast-6c.gif" alt="Figure 6"><br>
	<strong>Figure 6. Toaster Disposal</strong></a>
    </p><h2>Summary and Recommendations</h2>
	In summary, overcooking the SPT did produce a good size flame.
	The effect was not as pronounced as the researchers had hoped, but
	was satisfying nonetheless.  The research assistant noted that
	the flames produced did appear to have some color variation.
	We believe that frosted SPTs may successfully produce even
	larger torches.  Further research in this area is warranted.
    <p>
	We did desire to repeat the experiment with the remaining five
	SPTs, but we could not do so because there were no more suitable
	toasters available for further experiments.  In the future, we recommend
	that toasters be sold in six-packs to accomodate important
	SPT research.  Instead, the remaining SPTs were sacrified over
	the course of the next several days in private, undocumented
	consumption experiments.

</p><h2>Acknowledgements</h2>
	Special thanks to
	Jennifer "Svetlana" Reckard for her suggestions and
	proofreading of this work.
    <hr>
<h2>Followup Comments</h2>
   <p>	The response to my Strawberry Pop Tart article has been overwhelming.
	In October 1994 alone it's been accessed more than 2000 times.  Here
	are some of the terrific followup messages I've received:
   </p><ol>
   <li>	<a href="https://www.pmichaud.com/toast/follow1.html">
	12-Aug-94: <tt>schnitz@unipress.com</tt>
	</a>
   </li><li>	<a href="https://www.pmichaud.com/toast/follow2.html">
	13-Oct-94: <tt>avonruff@hydra.urbana.mcd.mot.com</tt>
	</a>
   
	Please send me your comments!  I'm especially interested to learn
	how you found out about this page, since I haven't advertised it
	much of anywhere.
<hr>
   <address>
        <img src="https://www.pmichaud.com/toast/krnbo24.gif" alt="">
        <a href="http://www.pmichaud.com/">
        Patrick R. Michaud/pmichaud@pobox.com
        </a>
   <address>




</address></address></li></ol></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Algebraic effects, ownership, and borrowing (102 pts)]]></title>
            <link>https://antelang.org/blog/effects_ownership_and_borrowing/</link>
            <guid>39456175</guid>
            <pubDate>Wed, 21 Feb 2024 16:50:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://antelang.org/blog/effects_ownership_and_borrowing/">https://antelang.org/blog/effects_ownership_and_borrowing/</a>, See on <a href="https://news.ycombinator.com/item?id=39456175">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">
                          <hr>
<h2 id="introduction">Introduction</h2>
<p><a href="https://antelang.org/docs/language/#algebraic-effects">Algebraic Effects</a> are a useful abstraction
for reasoning about effectful programs by letting us leave the interpretation
of these effects to callers. However, most existing literature discusses these
in the context of a pure functional language with pervasive sharing of values.
What restrictions would we need to introduce algebraic effects into a language
with ownership and borrowing - particularly Ante?<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p>
<hr>

<p>Consider the following program:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Read</span> <span>a</span> <span>with</span>
    <span>read</span> <span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span>

<span>the_value</span> <span>(</span><span>value</span><span>:</span> <span>a</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>b</span> <span>can</span> <span>Read</span> <span>a</span><span>)</span> <span>:</span> <span>b</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>read</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>value</span>
</code></pre></div><p>This seems like it’d pass type checking at first glance, but we can easily
construct a program that tries to use the same moved value twice:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>foo</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Read</span> <span>String</span> <span>=</span>
    <span>s1</span> <span>=</span> <span>read</span> <span>()</span>
    <span>s2</span> <span>=</span> <span>read</span> <span>()</span>

<span>foo</span> <span>()</span> <span>with</span> <span>the_value</span> <span>"foo"</span>

<span>// The above is sugar for</span>
<span>// the_value "foo" (fn () -&gt; foo ())</span>
</code></pre></div><p>Since a handler’s body may be called multiple times, it may not move any
value in its environment. This restriction is similar to moving values in a loop:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>the_value</span> <span>(</span><span>value</span><span>:</span> <span>a</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>b</span> <span>can</span> <span>Read</span> <span>a</span><span>)</span> <span>:</span> <span>b</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>// Error: Handler body moves `value` which will still</span>
    <span>//         be needed if the handler is called again</span>
    <span>|</span> <span>read</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>value</span>
</code></pre></div><hr>
<h2 id="borrowing">Borrowing</h2>
<p>Things get more complicated when we consider borrowing. Although Ante’s references
<a href="https://antelang.org/docs/language/#borrowing">do not have explicit lifetime variables</a>,
we still need to ensure their lifetime is sound.</p>
<h2 id="returning-owning-references">Returning Owning References</h2>
<p>Consider the following program:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>bad</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Read</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>Box</span> <span>String</span><span>)</span> <span>=</span>
    <span>s1</span> <span>=</span> <span>read</span> <span>()</span>

    <span>// Uh-oh, we've just obtained a second mutable owning reference to the same String</span>
    <span>s2</span> <span>=</span> <span>read</span> <span>()</span>
    <span>s2_inner_ref</span> <span>=</span> <span>as_ref</span> <span>s2</span>

    <span>// Drop the old Box referenced by s1 and s2</span>
    <span>s1</span> <span>:=</span> <span>Box</span><span>.</span><span>of</span> <span>"foo"</span>

    <span>// And now we're printing a dangling reference</span>
    <span>print</span> <span>s2_inner_ref</span>

<span>the_ref</span> <span>(</span><span>ref</span><span>:</span> <span>&amp;</span><span>own</span> <span>mut</span> <span>t</span><span>)</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Read</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>t</span><span>))</span> <span>:</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>read</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>ref</span>

<span>my_string</span> <span>=</span> <span>mut</span> <span>Box</span><span>.</span><span>of</span> <span>"bar"</span>
<span>bad</span> <span>()</span> <span>with</span> <span>the_ref</span> <span>&amp;</span><span>my_string</span>
</code></pre></div><p>This breaks the aliasing restriction on <code>&amp;own mut</code> in a way the compiler
cannot verify with existing rules on tracking lifetimes.</p>
<p>The solution to this is that each function using the same <code>Read (&amp;own mut Box String)</code>
effect is considered to borrow from the same effect handler. Attempting to retrieve
two owned, mutable references from the same handler then would be a lifetime error:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>bad</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Read</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>Box</span> <span>String</span><span>)</span> <span>=</span>
    <span>s1</span> <span>=</span> <span>read</span> <span>()</span>

    <span>// Error: Cannot create a new aliased reference with `s1` still in scope</span>
    <span>s2</span> <span>=</span> <span>read</span> <span>()</span>

    <span>print</span> <span>s1</span>
</code></pre></div><p>Similarly, trying to obfuscate this by calling a function which indirectly
returns another reference should also fail:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>indirect</span> <span>()</span> <span>can</span> <span>Read</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>Box</span> <span>String</span><span>)</span> <span>=</span>
    <span>read</span> <span>()</span>

<span>foo</span> <span>()</span> <span>can</span> <span>Read</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>Box</span> <span>String</span><span>)</span> <span>=</span>
    <span>r1</span> <span>=</span> <span>read</span> <span>()</span>

    <span>// Error: Cannot borrow from `Read` effect again with `r1` still in scope</span>
    <span>r2</span> <span>=</span> <span>indirect</span> <span>()</span>
</code></pre></div><h2 id="owned-reference-parameters">Owned Reference Parameters</h2>
<p>Now let’s consider how we can break programs which pass references to effects.
For this we’re going to use the <code>Yield a</code> effect which is used for creating
generators or streams:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Yield</span> <span>a</span> <span>with</span>
    <span>yield</span><span>:</span> <span>a</span> <span>-&gt;</span> <span>Unit</span>

<span>foo</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Yield</span> <span>(</span><span>&amp;</span><span>own</span> <span>mut</span> <span>I32</span><span>)</span> <span>=</span>
    <span>vec</span> <span>=</span> <span>mut</span> <span>Vec</span><span>.</span><span>of</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>]</span>
    <span>yield</span> <span>(</span><span>get_mut</span> <span>&amp;</span><span>vec</span> <span>0</span><span>)</span>
    <span>vec</span> <span>:=</span> <span>Vec</span><span>.</span><span>of</span> <span>[</span><span>3</span><span>]</span>
    <span>yield</span> <span>(</span><span>get_mut</span> <span>&amp;</span><span>vec</span> <span>0</span><span>)</span>

<span>bar</span> <span>()</span> <span>=</span>
    <span>x</span> <span>=</span> <span>mut</span> <span>None</span>

    <span>handle</span> <span>foo</span> <span>()</span>
    <span>|</span> <span>yield</span> <span>y</span> <span>-&gt;</span>
        <span>if</span> <span>Some</span> <span>x</span> <span>=</span> <span>x</span> <span>then</span>
            <span>// foo has cleared the underlying vec by this point,</span>
            <span>// so this would print a dangling reference!</span>
            <span>print</span> <span>x</span>

        <span>x</span> <span>:=</span> <span>Some</span> <span>y</span>
        <span>resume</span> <span>()</span>
</code></pre></div><p>To prevent this we need to tie <code>y</code> to the variable that owns it - which is <code>resume</code>.
This way we can still yield owned references if needed, but we cannot call <code>resume</code> until they are dropped.</p>
<p>Conceptually, we can think of a handle expression as receiving a single <code>resume</code> object which
is then unpacked:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>handle</span> <span>foo</span> <span>()</span>
<span>|</span> <span>MyEffect</span> <span>a</span> <span>b</span> <span>-&gt;</span>
    <span>...</span>

<span>// Conceptually the same as:</span>
<span>handle</span> <span>foo</span> <span>()</span>
<span>|</span> <span>MyEffect</span> <span>resume</span> <span>-&gt;</span>
    <span>a</span> <span>=</span> <span>resume</span><span>.</span><span>a</span>
    <span>b</span> <span>=</span> <span>resume</span><span>.</span><span>b</span>
    <span>resume</span> <span>=</span> <span>resume</span><span>.</span><span>continuation</span>
    <span>...</span>
</code></pre></div><hr>
<h2 id="resume">Resume</h2>
<p>One core aspect of effects that we’ve glossed over so far is the <code>resume</code> function
which resumes an effectful computation from the handler. Since <code>resume</code> refers to an
in-progress computation, we need a way to safely encode this environment, yet we
need to do so when defining the effect before the environment is known.
What type should be given to <code>resume</code>?</p>
<p>Consider the following code:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Fork</span> <span>with</span>
    <span>fork</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Bool</span>

<span>foo</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Fork</span> <span>=</span>
    <span>message</span> <span>=</span> <span>"branch"</span>

    <span>if</span> <span>fork</span> <span>()</span> <span>then</span>
        <span>print</span> <span>"${message} A"</span>
        <span>drop</span> <span>message</span>
    <span>else</span>
        <span>print</span> <span>"${message} B"</span>

<span>handle_fork</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Fork</span><span>)</span> <span>:</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>fork</span> <span>()</span> <span>-&gt;</span>
        <span>// Run `resume` twice, arbitrarily returning the second result</span>
        <span>resume</span> <span>true</span>
        <span>resume</span> <span>false</span>

<span>handle_fork</span> <span>foo</span>
</code></pre></div><p>After resuming from the <code>fork</code> the second time, we enter the false branch.
When doing so, <code>message</code> has already been moved, so we’d be reading from an
already-dropped value.</p>
<p>This is the problem the different closure types already solve. We just need
some way to determine if <code>resume</code> should be a <code>Fn</code>, <code>FnMut</code>, or <code>FnOnce</code> since
we cannot know this within the handler itself.</p>
<p>One possibility is to require this in the definition of <code>Fork</code>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Fork</span> <span>with</span>
    <span>fork</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Bool</span>

    <span>// The underscores here are because we're omitting the closure environment</span>
    <span>// type as well as the actual function type - which is derived from fork's type.</span>
    <span>// Although the environment type can be specified if desired, the function type of resume</span>
    <span>// must be omitted because its return type will be the handler type, which is</span>
    <span>// not known at this point.</span>
    <span>fork</span><span>.</span><span>resume</span><span>:</span> <span>FnMut</span> <span>_</span> <span>_</span>
</code></pre></div><p>Note that <code>fork</code> is still callable without restrictions.
It is only <code>resume</code> that will be a <code>FnMut</code> when it is introduced.</p>
<p>Anyways, now we’d get an error when writing <code>foo</code>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>foo</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Fork</span> <span>=</span>
    <span>message</span> <span>=</span> <span>"branch"</span>

    <span>// Error: `fork` can be resumed multiple times, but `message` would</span>
    <span>//         possibly be moved after the first call to `resume`.</span>
    <span>if</span> <span>fork</span> <span>()</span> <span>then</span>
        <span>print</span> <span>"${message} A"</span>
        <span>// Note: `message` is moved here</span>
        <span>drop</span> <span>message</span>
    <span>else</span>
        <span>print</span> <span>"${message} B"</span>
</code></pre></div><p>Since this error would otherwise be much more common, <code>resume</code> is by
default a <code>FnOnce</code> unless otherwise specified. This means when defining
an effect we will need to think about what kinds of effect handlers we
want to permit.</p>
<p>Also note that after removing <code>drop</code>, <code>message</code> will not be dropped at the
end of <code>foo</code>. Instead, it is part of <code>resume</code>’s environment and will be dropped
after the last use of <code>resume</code> in the effect handler.</p>
<h2 id="environment-type-quantification">Environment Type Quantification</h2>
<p>Most effects which give an explicit type for <code>resume</code> will either omit the
environment type, or specify it as a type variable quantified over the function:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Foo</span> <span>with</span>
    <span>foo</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span>
    <span>foo</span><span>.</span><span>resume</span><span>:</span> <span>FnOnce</span> <span>env</span> <span>_</span>

    <span>// The above means:</span>
    <span>// foo.resume: forall env. FnOnce env _</span>
</code></pre></div><p>This is generally desired since it allows the <code>resume</code> closure to be unboxed most
of the time. However, what would happen if the user wrote the trait as the following:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Foo</span> <span>env</span> <span>with</span>
    <span>foo</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span>
    <span>foo</span><span>.</span><span>resume</span><span>:</span> <span>FnOnce</span> <span>env</span> <span>_</span>
</code></pre></div><p>Since each generic instance of a trait would be separate (e.g. <code>Read I32</code> versus <code>Read String</code>),
each use of this effect with a different environment would be a separate effect:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>forced_example</span> <span>(</span><span>x</span><span>:</span> <span>&amp;</span><span>I32</span><span>)</span> <span>=</span>
    <span>foo</span> <span>()</span>
    <span>y</span> <span>=</span> <span>x</span>
    <span>foo</span> <span>()</span>
    <span>print</span> <span>(</span><span>x</span><span>,</span> <span>y</span><span>)</span>
</code></pre></div><p>Above, <code>forced_example</code> would be inferred to have the effects <code>Foo Env1</code> and <code>Foo Env2</code> where
<code>Env1</code> and <code>Env2</code> are both opaque types representing the environments. Since each of these would
need to be handled by separate effect handlers, this technique could be used to limit an effect
to being called at most once per handler. It remains to be seen how useful this would be however.</p>
<h2 id="restricting-the-environment-type">Restricting the environment type</h2>
<p>If any capabilities are required on the <code>resume</code> closure, a <code>given</code> clause can be added.
Since most traits on closures are defined as long as they’re defined on the closure environment,
it is usually sufficient to require the trait on the closure environment alone:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>FooCloneEnv</span> <span>with</span>
    <span>foo</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span>
    <span>foo</span><span>.</span><span>resume</span><span>:</span> <span>FnOnce</span> <span>env</span> <span>_</span> <span>given</span> <span>Clone</span> <span>env</span>
</code></pre></div><p>Note that since <code>resume</code> is a continuation, this environment type includes any captured variables
across other function calls as well. So the <code>Clone</code> constraint above would also apply to
the <code>vec</code> variable below:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>inner_fn</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>FooCloneEnv</span> <span>=</span>
    <span>// x may be cloned</span>
    <span>x</span> <span>=</span> <span>3</span>
    <span>foo</span> <span>()</span>
    <span>print</span> <span>x</span>

<span>outer_fn</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>FooCloneEnv</span> <span>=</span>
    <span>// vec may also be cloned</span>
    <span>vec</span> <span>=</span> <span>Vec</span><span>.</span><span>of</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]</span>
    <span>function2</span> <span>()</span>
    <span>print</span> <span>vec</span>
</code></pre></div><p>Since environment types can grow quite large, it is generally recommended to avoid cloning
<code>resume</code>. A more useful trait constraint on <code>resume</code> is covered in the next section.</p>
<hr>
<h2 id="multithreading">Multithreading</h2>
<p>Going back to the <code>Fork</code> example, what would happen if we tried to run each
resumption in its own thread?</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Fork</span> <span>with</span>
    <span>fork</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Bool</span>

    <span>// Changed to Fn so that we can alias this twice below</span>
    <span>fork</span><span>.</span><span>resume</span><span>:</span> <span>Fn</span> <span>env</span> <span>_</span>

<span>multithread_fork</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Fork</span><span>)</span> <span>:</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>fork</span> <span>()</span> <span>-&gt;</span>
        <span>// Spawn two threads and wait for them both to complete</span>
        <span>Thread</span><span>.</span><span>wait</span> <span>fn</span> <span>()</span> <span>-&gt;</span>
            <span>// Error: Expected argument of `Thread.spawn` to be `Send`</span>
            <span>//        No impl found for `Send (Fn _ (Unit -&gt; Bool))`</span>
            <span>Thread</span><span>.</span><span>spawn</span> <span>(</span><span>fn</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>true</span><span>)</span>
            <span>Thread</span><span>.</span><span>spawn</span> <span>(</span><span>fn</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>false</span><span>)</span>
</code></pre></div><p>In order to spawn a new thread to call <code>resume</code> we’d need to require
a reference to the closure environment is <code>Send</code>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Fork</span> <span>with</span>
    <span>fork</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Bool</span>
    <span>foo</span><span>.</span><span>resume</span><span>:</span> <span>Fn</span> <span>env</span> <span>_</span> <span>given</span> <span>Send</span> <span>&amp;</span><span>env</span>

<span>multithread_fork</span> <span>(</span><span>f</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>a</span> <span>can</span> <span>Fork</span><span>)</span> <span>:</span> <span>a</span> <span>=</span>
    <span>handle</span> <span>f</span> <span>()</span>
    <span>|</span> <span>fork</span> <span>()</span> <span>-&gt;</span>
        <span>Thread</span><span>.</span><span>wait</span> <span>fn</span> <span>()</span> <span>-&gt;</span>
            <span>Thread</span><span>.</span><span>spawn</span> <span>(</span><span>fn</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>true</span><span>)</span>
            <span>Thread</span><span>.</span><span>spawn</span> <span>(</span><span>fn</span> <span>()</span> <span>-&gt;</span> <span>resume</span> <span>false</span><span>)</span>
</code></pre></div><hr>
<h2 id="polymorphic-effects">Polymorphic Effects</h2>
<p>Ante also enables functions to be polymorphic over their effects.
For example, the <code>map</code> function has the type:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>map</span><span>:</span> <span>Stream</span> <span>a</span> <span>-&gt;</span> <span>FnMut</span> <span>a</span> <span>=&gt;</span> <span>b</span> <span>can</span> <span>e</span> <span>-&gt;</span> <span>Unit</span> <span>can</span> <span>Emit</span> <span>b</span><span>,</span> <span>e</span>
</code></pre></div><p>Now, recalling the <code>FooCloneEnv</code> example from earlier:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>inner_fn</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>FooCloneEnv</span> <span>=</span>
    <span>// x may be cloned</span>
    <span>x</span> <span>=</span> <span>3</span>
    <span>foo</span> <span>()</span>
    <span>print</span> <span>x</span>

<span>outer_fn</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>FooCloneEnv</span> <span>=</span>
    <span>// vec may also be cloned</span>
    <span>vec</span> <span>=</span> <span>Vec</span><span>.</span><span>of</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>]</span>
    <span>function2</span> <span>()</span>
    <span>print</span> <span>vec</span>
</code></pre></div><p>This works fine, but how could we pass a function such as <code>outer_fn</code> to
<code>map</code>? The effect variable <code>e</code> would be instantiated to <code>FooCloneEnv</code> but
now we’d also need to know if the environment of <code>map</code> when it calls the
passed-in function is clone-able. In the most general case, we’d need to
be able to verify any trait from <code>map</code> and whether it can allow the function
used to resume multiple times or not.</p>
<p>We’d have to add these constraints to the effect variables directly:</p>
<div><pre tabindex="0"><code data-lang="fallback">map: Stream a -&gt; FnMut a =&gt; b can e -&gt; Unit
    given Clone e, Send e, Fn e.resume _
    can Emit b, e
</code></pre></div><p>This is a big hit to the usability of effects in this scheme since these constraints
would have to be manually specified on <code>map</code> for its contents to be checked. If not
specified, a new version of <code>map</code> would have to be written with a <code>Send</code>able environment
or similar. This will inevitably lead to some duplication when using effects that algebraic
effect handlers are usually meant to remove.</p>
<p>In a later article, we’ll focus on ways to simplify the usability of this scheme by
providing sane defaults where possible.</p>
<hr>
<h2 id="implementation-details-and-boxing">Implementation Details and Boxing</h2>
<p>So far, each of the rules covered above should apply to any language with effects,
ownership, and borrowing. Different implementations of effects can have wildly
different runtime costs however.</p>
<p>For example, most languages implementing the full spectrum of algebraic effects
will keep track of the stack of effect handlers at runtime. When an effect call
is made, a lookup needs to be performed then the code needs to jump to the relevant
handler and back. This may be done by jumping up the call stack and copying stack frames or
by converting effectful functions to continuation passing style (CPS) - like Ante does.</p>
<p>Languages without algebraic effects aren’t completely free from the costs of effects
either though. Even if we restrict ourselves to just the <code>async</code> effect, we can
see plenty of languages which include it - each with its own unique implementation
and performance characteristics.</p>
<p>Consider Rust’s <code>async</code> effect which is implemented by compiling async functions
to state machines. In this scheme, the following code is rejected:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>async</span><span> </span><span>fn</span> <span>recursive</span><span>()</span><span> </span><span>{</span><span>
</span><span>    </span><span>recursive</span><span>().</span><span>await</span><span>;</span><span>
</span><span>    </span><span>recursive</span><span>().</span><span>await</span><span>;</span><span>
</span><span></span><span>}</span><span>
</span></code></pre></div><p>Because the resulting state machine would have infinite size:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>enum</span> <span>Recursive</span><span> </span><span>{</span><span>
</span><span>    </span><span>First</span><span>(</span><span>Recursive</span><span>),</span><span>
</span><span>    </span><span>Second</span><span>(</span><span>Recursive</span><span>),</span><span>
</span><span></span><span>}</span><span>
</span></code></pre></div><p>To get around this, users need to box recursive functions:</p>
<div><pre tabindex="0"><code data-lang="rust"><span>use</span><span> </span><span>futures</span>::<span>future</span>::<span>{</span><span>BoxFuture</span><span>,</span><span> </span><span>FutureExt</span><span>};</span><span>
</span><span>
</span><span></span><span>fn</span> <span>recursive</span><span>()</span><span> </span>-&gt; <span>BoxFuture</span><span>&lt;'</span><span>static</span><span>,</span><span> </span><span>()</span><span>&gt;</span><span> </span><span>{</span><span>
</span><span>    </span><span>async</span><span> </span><span>move</span><span> </span><span>{</span><span>
</span><span>        </span><span>recursive</span><span>().</span><span>await</span><span>;</span><span>
</span><span>        </span><span>recursive</span><span>().</span><span>await</span><span>;</span><span>
</span><span>    </span><span>}.</span><span>boxed</span><span>()</span><span>
</span><span></span><span>}</span><span>
</span></code></pre></div><p>If we try to implement a similar example in future-Ante<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>effect</span> <span>Async</span> <span>with</span>
    <span>await</span><span>:</span> <span>Unit</span> <span>-&gt;</span> <span>Unit</span>

<span>recursive</span> <span>()</span> <span>:</span> <span>Unit</span> <span>can</span> <span>Await</span> <span>=</span>
    <span>// This doesn't quite match the semantics of the Rust example above,</span>
    <span>// but lets us use a simpler definition for `await`</span>
    <span>await</span> <span>()</span>
    <span>recursive</span> <span>()</span>
    <span>await</span> <span>()</span>
    <span>recursive</span> <span>()</span>

<span>handle</span> <span>recursive</span> <span>()</span>
<span>|</span> <span>await</span> <span>f</span> <span>-&gt;</span> <span>resume</span> <span>()</span>
</code></pre></div><p>The result would look quite different:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>recursive</span> <span>()</span> <span>:</span> <span>Unit</span> <span>=</span>
    <span>recursive</span> <span>()</span>
    <span>recursive</span> <span>()</span>
</code></pre></div><p>So theoretically no boxing is needed for recursion alone.
The performance characteristics here look quite different - but that is
because in Ante they’re largely determined by the handler that is used
rather than the call site of the effect. If we use a different handler
which does not resume in a tail position, boxing will be required.
For example:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>handle</span> <span>recursive</span> <span>()</span>
<span>|</span> <span>await</span> <span>f</span> <span>-&gt;</span>
    <span>resume</span> <span>(</span><span>f</span> <span>())</span>
    <span>print</span> <span>"done"</span>
</code></pre></div><p>Compiles to<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>:</p>
<div><pre tabindex="0"><code data-lang="ante"><span>recursive</span> <span>k</span> <span>=</span>
    <span>recursive</span> <span>fn</span> <span>()</span> <span>-&gt;</span>
      <span>recursive</span> <span>k</span>
      <span>print</span> <span>"done"</span>
    <span>print</span> <span>"done"</span>
</code></pre></div><p>Here we can see the inner continuation captures the outer continuation <code>k</code>.</p>
<p>To give <code>k</code> a valid type, we’d need to box it to ensure it always has the
same size for each recursive call. This is similar to what we’d need to do
in the Rust example, but there are some unique problems with requiring users manually
box these continuations in Ante:</p>
<ul>
<li>The continuation is added by the compiler, so it isn’t clear to the user where
they should add the boxing.</li>
<li>Whether boxing is required is dependent on the structure of the handler. We don’t
want to always add boxing since tail-resume is the more common case.</li>
</ul>
<p>We could try to get around this by marking whether a given effect must have a tail-resumptive
handler or not, and requiring recursive functions using non-tail-resumptive handlers to
box their continuations somehow. This would make effects much more cumbersome to use however,
and one of Ante’s goals is to be a slightly <em>higher</em> level language than Rust. If effects are
not simple to use then users will avoid them.</p>
<p>For these reasons, the current plan is for the compiler to automatically insert boxing
of closures where appropriate and infer their lifetimes via <a href="https://antelang.org/docs/ideas/#lifetime-inference">lifetime inference</a>.
Lifetime inference is a very interesting topic to me - it was one of Ante’s original goals
to experiment with it. When it works well it can be great since it can stack-allocate
potentially even to prior stack frames. The downside is that the inferred lifetimes can be imprecise.
Although, in this case, if lifetimes cannot be accurately inferred, we would still know
their longest possible lifetime is that of the effect handler.<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>
This is a topic that deserves much more detail though so I’ll leave it to a future
blog post. If you’re still curious, there are some papers on it reachable from the documentation link above.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Note that ownership &amp; borrowing are a recent addition to Ante and the changes in this article
are not yet reflected in the documentation!&nbsp;<a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Getting the desired output here requires an optimization for <code>resume</code> in a tail position,
which is not currently implemented.&nbsp;<a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>This output has been heavily cleaned.&nbsp;<a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>This is because effects are implemented via delimited continuations which are evaluated at compile-time.
Ante takes this approach from Effekt and it is further detailed in <a href="https://se.cs.uni-tuebingen.de/publications/schuster19zero.pdf">Zero-cost Effect Handlers by Staging</a>&nbsp;<a href="#fnref:4" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>

                        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The funding frenzy at Anthropic (106 pts)]]></title>
            <link>https://www.nytimes.com/2024/02/20/technology/anthropic-funding-ai.html</link>
            <guid>39456140</guid>
            <pubDate>Wed, 21 Feb 2024 16:49:00 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nytimes.com/2024/02/20/technology/anthropic-funding-ai.html">https://www.nytimes.com/2024/02/20/technology/anthropic-funding-ai.html</a>, See on <a href="https://news.ycombinator.com/item?id=39456140">Hacker News</a></p>
Couldn't get https://www.nytimes.com/2024/02/20/technology/anthropic-funding-ai.html: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Encoding tic-tac-toe in 15 bits (126 pts)]]></title>
            <link>https://cbarrick.dev/posts/2024/02/19/tic-tac-toe</link>
            <guid>39456135</guid>
            <pubDate>Wed, 21 Feb 2024 16:48:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cbarrick.dev/posts/2024/02/19/tic-tac-toe">https://cbarrick.dev/posts/2024/02/19/tic-tac-toe</a>, See on <a href="https://news.ycombinator.com/item?id=39456135">Hacker News</a></p>
<div id="readability-page-1" class="page"><div> <article> <time datetime="2024-02-19T00:00:00+00:00"> Feb 19th, 2024 </time>  <p>I recently stumbled upon a <a href="https://blog.goose.love/posts/tictactoe/">blog post</a> by Alejandra González (a.k.a <a href="https://tech.lgbt/@blyxyas">@blyxyas</a>) that seeks to compress a tic-tac-toe game state into as few bits as possible. She arrived at a solution in 18 bits. This got me thinking, can we do better?</p> <p>As Alejandra points out, there are 765 possible game states<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" rel="footnote">1</a></sup>. We could simply assign a number to all of the sates, which would take up 10 bits<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" rel="footnote">2</a></sup>. But in Alejandra’s words, that’s “boring.” More specifically, there’s not much we can do with a representation like that. Whether we want to read the value of a given cell or update from one state to another, in practice we’re going to need a lookup table to map each number to a larger, more structured representation, which defeats the whole idea behind a compressed representation.</p> <figure> <img src="https://cbarrick.dev/assets/tic-tac-toe-game.svg" alt="A game of tic-tac-toe" width="100%"> <figcaption> A game of tic-tac-toe / © <a href="https://commons.wikimedia.org/wiki/User:Stannered">User:Stannered</a> / <a href="https://commons.wikimedia.org/">Wikimedia Commons</a> / <a href="https://creativecommons.org/licenses/by-sa/3.0/">CC-BY-SA-3.0</a> </figcaption> </figure> <h3 id="an-18-bit-solution">An 18 bit solution</h3> <p>Alejandra came up with a better solution, where each cell is represented by a pair of bits, and the grid is represented as a concatenation of nine of these bit pairs. Within a bit pair, one bit represents a circle and the other represents a cross; at most one bit of the pair can be set.</p> <div><pre><code><span>// The representation of a single cell.</span>
<span>typedef</span> <span>enum</span> <span>cell</span> <span>{</span>
    <span>EMPTY</span>  <span>=</span> <span>0</span><span>,</span> <span>// Binary 00</span>
    <span>CROSS</span>  <span>=</span> <span>1</span><span>,</span> <span>// Binary 01</span>
    <span>CIRCLE</span> <span>=</span> <span>2</span><span>,</span> <span>// Binary 10</span>
<span>}</span> <span>cell</span><span>;</span>

<span>// The concatenation of 9 cells. We only care about the lower 18 bits.</span>
<span>typedef</span> <span>uint32_t</span> <span>state</span><span>;</span>
</code></pre></div> <p>The core methods that we would like to have on our state type are getting and setting cell values at a given index. This is pretty easy to implement with some quick bit-twiddling.</p> <div><pre><code><span>cell</span> <span>get_cell</span><span>(</span><span>state</span> <span>s</span><span>,</span> <span>int</span> <span>i</span><span>)</span> <span>{</span>
    <span>int</span> <span>pos</span> <span>=</span> <span>2</span> <span>*</span> <span>i</span><span>;</span>        <span>// Bit offset of cell i.</span>
    <span>return</span> <span>(</span><span>s</span> <span>&gt;&gt;</span> <span>pos</span><span>)</span> <span>%</span> <span>4</span><span>;</span>  <span>// Read the cell.</span>
<span>}</span>

<span>void</span> <span>set_cell</span><span>(</span><span>state</span> <span>*</span><span>s</span><span>,</span> <span>int</span> <span>i</span><span>,</span> <span>cell</span> <span>val</span><span>)</span> <span>{</span>
    <span>int</span> <span>pos</span> <span>=</span> <span>2</span> <span>*</span> <span>i</span><span>;</span>    <span>// Bit offset of cell i.</span>
    <span>*</span><span>s</span> <span>&amp;=</span> <span>~</span><span>(</span><span>3</span> <span>&lt;&lt;</span> <span>pos</span><span>);</span>  <span>// Clear the old value.</span>
    <span>*</span><span>s</span> <span>|=</span> <span>val</span> <span>&lt;&lt;</span> <span>pos</span><span>;</span>   <span>// Set the new value.</span>
<span>}</span>
</code></pre></div> <p>This is a fantastic, efficient solution.</p> <h3 id="getting-smaller-with-base-3">Getting smaller with base-3</h3> <p>In practice, the number of bits in an integer needs to be a power of two. In the code above, we used a 32 bit integer to hold our state, when we really only needed 18 bits. If we could save just two more bits, we could cut our memory usage in half by using a 16 bit integer for the game state.</p> <p>In the code above, we’ve conceived the game state as the concatenation of nine cell states. This is a good idea because it makes it simple to implement our core methods. We can think of this as a base-4 number where each cell state is a base-4 digit having values 0 (empty), 1 (cross), 2 (circle), and 3 (invalid). This conception shows up in the code too, where we convert our base-4 index into a base-2 index by multiplying it by 2, so that we can use bitwise operations to access the data.</p> <p>The problem is that pesky invalid cell state. What if we instead conceive the game state as a base-3 number and a cell state as a base-3 digit? In this case we need nine base-3 digits, which maxes out at \(3^9-1\) or 19,682. Representing this in binary will cost us… 15 bits<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" rel="footnote">3</a></sup>!</p> <p>So we can use a base-3 representation to hit our 16 bit target. But how do we implement our methods?</p> <p>The trick is to generalize our bit-twiddling to arbitrary bases. In binary, the left-shift operation <code>x &lt;&lt; i</code> is equivalent to \(x \cdot 2^i\), and likewise the right-shift operation <code>x &gt;&gt; i</code> is equivalent to \(x \div 2^i \). To generalize these operations from base-2 to base-n, just replace 2 with n. For the other bitwise operations, we can use a combination of addition and subtraction.</p> <p>The new code looks like this:</p> <div><pre><code><span>// The representation of a single cell.</span>
<span>typedef</span> <span>enum</span> <span>cell</span> <span>{</span>
    <span>EMPTY</span>  <span>=</span> <span>0</span><span>,</span>
    <span>CROSS</span>  <span>=</span> <span>1</span><span>,</span>
    <span>CIRCLE</span> <span>=</span> <span>2</span><span>,</span>
<span>}</span> <span>cell</span><span>;</span>

<span>// Think of the game sate as a base-3 number with 9 digits.</span>
<span>typedef</span> <span>uint16_t</span> <span>state</span><span>;</span>

<span>// A helper to compute pow(3, i), when 0 &lt;= i &lt; 9.</span>
<span>static</span> <span>int</span> <span>pow3</span><span>(</span><span>int</span> <span>i</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>i</span> <span>&lt;</span> <span>0</span> <span>||</span> <span>9</span> <span>&lt;=</span> <span>i</span><span>)</span> <span>return</span> <span>1</span><span>;</span>
    <span>static</span> <span>int</span> <span>p</span><span>[]</span> <span>=</span> <span>{</span><span>1</span><span>,</span> <span>3</span><span>,</span> <span>9</span><span>,</span> <span>27</span><span>,</span> <span>81</span><span>,</span> <span>243</span><span>,</span> <span>729</span><span>,</span> <span>2187</span><span>,</span> <span>6561</span><span>};</span>
    <span>return</span> <span>p</span><span>[</span><span>i</span><span>];</span>
<span>}</span>

<span>cell</span> <span>get_cell</span><span>(</span><span>state</span> <span>s</span><span>,</span> <span>int</span> <span>i</span><span>)</span> <span>{</span>
    <span>int</span> <span>div</span> <span>=</span> <span>pow3</span><span>(</span><span>i</span><span>);</span>     <span>// Get the base-3 offset of the cell.</span>
    <span>return</span> <span>(</span><span>s</span> <span>/</span> <span>div</span><span>)</span> <span>%</span> <span>3</span><span>;</span>  <span>// "Shift" the base-3 number and read the cell.</span>
<span>}</span>

<span>void</span> <span>set_cell</span><span>(</span><span>state</span> <span>*</span><span>s</span><span>,</span> <span>int</span> <span>i</span><span>,</span> <span>cell</span> <span>val</span><span>)</span> <span>{</span>
    <span>int</span> <span>div</span> <span>=</span> <span>pow3</span><span>(</span><span>i</span><span>);</span>         <span>// Get the base-3 offset of the cell.</span>
    <span>int</span> <span>old</span> <span>=</span> <span>(</span><span>*</span><span>s</span> <span>/</span> <span>div</span><span>)</span> <span>%</span> <span>3</span><span>;</span>  <span>// Read the old value of the cell.</span>
    <span>*</span><span>s</span> <span>-=</span> <span>old</span> <span>*</span> <span>div</span><span>;</span>           <span>// Reset the cell to empty.</span>
    <span>*</span><span>s</span> <span>+=</span> <span>val</span> <span>*</span> <span>div</span><span>;</span>           <span>// Set the cell value.</span>
<span>}</span>
</code></pre></div> <h3 id="conclusion">Conclusion</h3> <p>Is this any better? It depends, but probably not.</p> <p>If you had a very large number of game states that you needed to store, you could pack them tightly using 18 bits for the base-4 representation or 15 bits for the base-3 representation. That’s a savings of 16%, which may or may not be worth it.</p> <p>And if you’re chosing a representation for CPU performance, then the base-4 representation wins hands down. The base-3 representation has a lof of multiplication and division that can’t be easily optimized away.</p> <p>But if you had some wild application where you needed to keep trillions of game states unpacked in memory, then sure, use base-3.</p> <p>This is a wild case of premature optimization that nobody asked for. 😅</p> <p>You can find <a href="https://gist.github.com/cbarrick/8c5726dcca7f5f1e436585e672bc7f1f">test cases on GitHub</a>.</p>  </article> </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: NotesOllama – I added local LLM support to Apple Notes (through Ollama) (147 pts)]]></title>
            <link>https://smallest.app/notesollama/</link>
            <guid>39456113</guid>
            <pubDate>Wed, 21 Feb 2024 16:46:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://smallest.app/notesollama/">https://smallest.app/notesollama/</a>, See on <a href="https://news.ycombinator.com/item?id=39456113">Hacker News</a></p>
<div id="readability-page-1" class="page"><div href="../notescmdr">
            <img src="https://smallest.app/notescmdr/assets/icon128.png">
            <div>
              <p><span>NotesCmdr</span></p><p>Slash commands, markdown and templates for Apple Notes</p>
            </div>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Nuclear SMR welding breakthrough: A year's work now takes a day (194 pts)]]></title>
            <link>https://newatlas.com/energy/nuclear-reactor-weld-one-day/</link>
            <guid>39455915</guid>
            <pubDate>Wed, 21 Feb 2024 16:31:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/energy/nuclear-reactor-weld-one-day/">https://newatlas.com/energy/nuclear-reactor-weld-one-day/</a>, See on <a href="https://news.ycombinator.com/item?id=39455915">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Small Modular Reactor (SMR) construction shifts into high gear, as UK company Sheffield Forgemasters welds a full-size nuclear reactor vessel in under 24 hours instead of the usual 12 months. The rollout of this game-changing tech could be massive.</p><p>Modular reactors have the potential to revolutionize the nuclear power industry by turning nuclear generating plants from major civil engineering projects to factory-produced commodities. Instead of being essentially one-offs, modular reactors have a standardized design, can be mass produced, installed in any number required to serve local needs, and don't require the incredibly expensive buildings conventional reactors depend upon.</p><p>The problem is that there are bottlenecks in how to build reactors of any size. One is welding the vessels used to contain the reactor core, isolating it from the outer environment. Using conventional techniques, this can take over a year, but Sheffield Forgemasters have reduced this to under a day using what is called Local Electron-Beam Welding (LEBW) to complete four thick, nuclear-grade welds.</p><p>LEBW is a revolutionary method to weld two pieces of metal together using a high-energy density fusion process centered on a high-powered electron gun operating in a local vacuum. This melts and fuses components to one another and allows for an efficiency of 95%, deep penetration, and a high depth-to-width ratio.</p><p>The upshot is that Sheffield Forgemasters was able to complete a vessel three meters (10 ft) in diameter with 200-mm (8-in) thick walls with what is claimed to be zero defects and at lower costs. In addition, the welding machine can handle innovative sloping-in and sloping-out techniques to start and finish the weld.</p><p>This demonstration, a world first, is a significant milestone for the British nuclear sector, which has been moribund for decades with advances only in reactors for nuclear submarines, a couple of showcase power plants, and nuclear fuel processing. Now, the UK government is looking toward a nuclear renaissance, with new plants planned – including 15 modular reactors to be constructed by <a href="https://newatlas.com/energy/rolls-royce-plans-mini-nuclear-reactors-in-britain/" data-cms-ai="0">Rolls-Royce</a>.</p><p>"The implication of this technology within the nuclear industry is monumental, potentially taking high-cost welding processes out of the equation," said Michael Blackmore, Senior Development Engineer and Project lead. "Not only does this reduce the need for weld-inspections, because the weld-join replicates the parent material, but it could also dramatically speed up the roll-out of SMR reactors across the UK and beyond, that’s how disruptive the LEBW breakthrough is."</p><p>Source: <a href="https://bdaily.co.uk/articles/2024/02/19/sheffield-forgemasters-announces-global-leap-in-nuclear-welding-technology" target="_blank" data-cms-ai="0">Sheffield Forgemasters</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Parsing: The Solved Problem That Isn't (2011) (107 pts)]]></title>
            <link>https://tratt.net/laurie/blog/2011/parsing_the_solved_problem_that_isnt.html</link>
            <guid>39455167</guid>
            <pubDate>Wed, 21 Feb 2024 15:42:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tratt.net/laurie/blog/2011/parsing_the_solved_problem_that_isnt.html">https://tratt.net/laurie/blog/2011/parsing_the_solved_problem_that_isnt.html</a>, See on <a href="https://news.ycombinator.com/item?id=39455167">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-body">





<p><strong>Updated (2014-10-24): If you find this article interesting, you may be interested in <a href="https://tratt.net/laurie/blog/entries/an_editor_for_composed_programs">the follow-up article on an editor for composed programs</a>.</strong></p>
<p>Parsing is the act of taking a stream of characters and deducing if and how they conform to an underlying grammar. For example the sentence Bill hits Ben conforms to the part of the English grammar <code>noun verb noun</code>. Parsing concerns itself with uncovering structure; although this gives a partial indication of the meaning of a sentence, the full meaning is only uncovered by later stages of processing. Parseable, but obviously nonsensical, sentences like Bill evaporates Ben highlight this (the sentence is still <code>noun verb noun</code>, but finding two people who agree on what it means will be a struggle). As humans we naturally parse text all the time, without even thinking about it; indeed, we even have a fairly good ability to parse constructs that we’ve never seen before.</p>
<p>In computing, parsing is also common; while the grammars are synthetic (e.g. of a specific programming language), the overall idea is the same as for human languages. Although different communities have different approaches to the practicalities of parsing - C programmers reach for <code>lex</code> / <code>yacc</code>; functional programmers to parser combinators; others for tools like <a href="http://www.antlr.org/">ANTLR</a> or a <a href="http://pdos.csail.mit.edu/~baford/packrat/">Packrat / PEG-based</a> approach - they typically rely on the same underlying area of knowledge.</p>
<p>After the creation of programming languages themselves, parsing was one of the first major areas tackled by theoretical computer science and, in many peoples eyes, one of its greatest successes. The 1960s saw a concerted effort to uncover good theories and algorithms for parsing. Parsing in the early days seems to have shot off in many directions before, largely, converging. Context Free Grammars (CFGs) eventually won, because they are fairly expressive and easy to reason about, both for practitioners and theorists.</p>
<p>Unfortunately, given the extremely limited hardware of 1960s computers (not helped by the lack of an efficient algorithm), the parsing of an arbitrary CFG was too slow to be practical. Parsing algorithms such as LL, LR, and LALR identified subsets of the full class of CFGs that could be efficiently parsed. Later, relatively practical algorithms for parsing any CFG appeared, most notably <a href="http://dl.acm.org/citation.cfm?id=358005">Earley’s 1973 parsing algorithm</a>. It is easy to overlook the relative difference in performance between then and now: the <em>fastest computer in the world</em> from 1964-1969 was the CDC6600 which executed at around 10 MIPS; my 2010 mobile phone has a processor which runs at over 2000 MIPS. By the time computers had become fast enough for Earley’s algorithm, LL, LR, and friends had established a cultural dominance which is only now being seriously challenged - many of the most widely used tools still use those algorithms (or variants) for parsing. Nevertheless in tools such as <a href="http://accent.compilertools.net/">ACCENT / ENTIRE</a> and recent versions of <code>bison</code>, one has access to performant parsers which can parse any CFG, if that is needed.</p>
<p>The general consensus, therefore, is that parsing is a solved problem. If you’ve got a parsing problem for synthetic languages, one of the existing tools should do the job. A few heroic people - such as <a href="http://www.cs.usfca.edu/~parrt/">Terence Parr</a>, <a href="http://www.cs.rhul.ac.uk/~adrian/">Adrian Johnstone</a>, and <a href="http://www.cs.rhul.ac.uk/People/staff/scott.html">Elizabeth Scott</a> - continue working away to ensure that parsing becomes even more efficient but, ultimately, this will be transparently adopted by tools without overtly changing the way that parsing is typically done.</p>
<h2>Language composition</h2>
<p>One of the things that’s become increasingly obvious to me over the past few years is that the general consensus breaks down for one vital emerging trend: language composition. Composition is one of those long, complicated, but often vague terms that crops up a lot in theoretical work. Fortunately, for our purposes it means something simple: grammar composition, which is where we add one grammar to another and have the combined grammar parse text in the new language (exactly the sort of thing we want to do with Domain Specific Languages (DSLs)). To use a classic example, imagine that we wish to extend a Java-like language with SQL so that we can directly write:</p>
<pre><span>for (String s : SELECT name FROM person WHERE age &gt; 18) {
</span><span>  ...
</span><span>}
</span></pre>
<p>Let’s assume that someone has provided us with two separate grammars: one for the Java-like language and one for SQL. Grammar composition seems like it should be fairly easy. In practice, it turns out to be rather frustrating, and I’ll now explain some of the reasons why.</p>
<h3>Grammar composition</h3>
<p>While grammar composition is theoretically trivial, simply squashing two grammars together is rarely useful in practice. Typically, grammars have a single start rule; one therefore needs to choose which of the two grammars has the start rule. More messy is the fact that the chances of the two grammars referencing each other is slight; in practice, one needs to specify a third tranche of data - often referred to, perhaps slightly misleadingly, as glue - which actually links the two grammars together. In our running example, the Java-like language has the main grammar; the glue will specify where, within the Java-like expressions, SQL statements can be referenced.</p>
<p>For those using old parsing algorithms such as LR (and LL etc.), there is a more fundamental problem. If one takes two LR-compatible grammars and combines them, the resulting grammar is not guaranteed to be LR-compatible (i.e. an LR parser may not be able to parse using it). Therefore such algorithms are of little use for grammar composition.</p>
<p>At this point, users of algorithms such as Earley’s have a rather smugger look on their face. Since we know from grammar theory that unioning two CFGs always leads to a valid CFG, such algorithms can always parse the result of grammar composition. But, perhaps inevitably, there are problems.</p>
<h3>Tokenization</h3>
<p>Parsing is generally a two-phase process: first we break the input up into tokens (tokenization); and then we parse the tokens. Tokens are what we call words in everyday language. In English, words are easily defined (roughly: a word starts and ends with a space or punctuation character). Different computer languages, however, have rather different notions of what their tokens are. Sometimes, tokenization rules are easily combined; however since tokenization is done in ignorance of how the token will later be used, sometimes it is difficult. For example, in SQL SELECT might be a keyword but in Java it is also a valid identifier; it is often hard, if not impossible, to combine such tokenization rules in traditional parsers.</p>
<p>Fortunately there is a solution: scannerless parsing (e.g. SDF2 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.7828&amp;rep=rep1&amp;type=ps">scannerless parsing</a>). For our purposes, it might perhaps better be called tokenless parsing; the different names reflect the naming conventions of different parsing schools. Scannerless parsing does away with a separate tokenization phase; the grammar now contains the information necessary to dynamically tokenize text. Combining grammars with markedly different tokenization rules is now possible.</p>
<h3>Fine-grained composition</h3>
<p>In practice, the simple glue mentioned earlier used to combine two grammars is often not enough. There can be subtle conflicts between the grammars, in the sense that the combined language might not give the result that was expected. Consider combining two grammars that have different keywords. Scannerless parsing allows us to combine the two grammars, but we may wish to ensure that the combined languages do not allow users to use keywords in the other language as identifiers. There is no easy way to express this in normal CFGs. The SDF2 paper referenced earlier allows reject productions as a solution to this; unfortunately this then makes SDF2 grammars <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.883&amp;rep=rep1&amp;type=pdf">mildly context sensitive</a>. As far as I know, the precise consequences of this haven’t been explored, but it does mean that at least some of the body of CFG theory won’t be applicable; it’s enough to make one a little nervous, at the very least (not withstanding the excellent work that has been created using the SDF2 formalism by <a href="https://eelcovisser.org/">Eeclo Visser</a> and others).</p>
<p>A recent, albeit relatively unknown, alternative are <a href="http://users.utu.fi/aleokh/boolean/">boolean grammars</a>. These are a generalization of CFGs that include conjunction and negation, which, at first glance, are exactly the constructs needed to make grammar composition practical (allowing one to say things like identifiers are any sequence of ASCII characters except SELECT). Boolean grammars, to me at least, seem to have a lot of promise, and <a href="http://users.utu.fi/aleokh/">Alexander Okhotin</a> is making an heroic effort on them. However, there hasn’t yet been any practical use of them that I know of, so wrapping ones head around the practicalities is far from trivial. There are also <a href="http://users.utu.fi/aleokh/boolean/nine_open_problems.html">several open questions</a> about boolean grammars, some of which, until they are answered one way or the other, may preclude wide-scale uptake. In particular, one issue relates to ambiguity, of which more now needs to be said.</p>
<h3>Ambiguity</h3>
<p>By severely restricting what CFGs they accept, grammars which are compatible with traditional parsing algorithms (LL, LR etc.) are always unambiguous (though, as we shall see, this does not mean that all the incompatible grammars are ambiguous: many are unambiguous). Grammar ambiguity is thus less widely understood than it might otherwise have been. Consider the following grammar of standard arithmetic:</p>
<pre><span>E</span><span> ::= E "</span><span>+</span><span>" E
</span><span>    | </span><span>E </span><span>"</span><span>-</span><span>" E
</span><span>    | </span><span>E </span><span>"</span><span>/</span><span>" E
</span><span>    | </span><span>E </span><span>"</span><span>*</span><span>" E
</span></pre>
<p>Using this grammar, a string such as <code>2 + 3 * 4</code> can be parsed ambiguously in two ways: as equivalent to <code>(2 + 3) * 4</code>; or as equivalent to <code>2 + (3 * 4)</code>. Parsing algorithms such as Earley’s will generate all possibilities even though we often only want one of them (due to arithmetic conventions, in this case we want the latter parse). There are several different ways of disambiguating grammars, such as precedences (in this example, higher precedences win in the face of ambiguity):</p>
<pre><span>E</span><span> ::= E "</span><span>+</span><span>" E  %</span><span>precedence</span><span> 1
</span><span>    | </span><span>E </span><span>"</span><span>-</span><span>" E  %</span><span>precedence</span><span> 1
</span><span>    | </span><span>E </span><span>"</span><span>/</span><span>" E  %</span><span>precedence</span><span> 2
</span><span>    | </span><span>E </span><span>"</span><span>*</span><span>" E  %</span><span>precedence</span><span> 3
</span></pre>
<p>This might suggest that we can tame ambiguity relatively easily: unfortunately, parsing theory tells us that the reality is rather tricky. The basic issue is that, in general, we can not statically analyse a CFG and determine if it is ambiguous or not. To discover whether a given CFG is ambiguous or not we have to try every possible input: if no input triggers an ambiguous parse, the CFG is not ambiguous. However this is, in general, impractical: most CFGs describe infinite languages and can not be exhaustively tested. There are various techniques which aim to give good heuristics for ambiguity (see <a href="https://sites.google.com/site/basbasten/publications/BastenMaster.pdf">Bas Basten’s masters thesis</a> for a good summary; I am also collaborating with a colleague on a new approach, though it’s far too early to say if it will be useful or not). However, these heuristics are inherently limited: if they say a CFG is ambiguous, it definitely is; but if they can not find ambiguity, all they can say is that the CFG <em>might</em> be unambiguous.</p>
<p>Since theoretical problems are not always practical ones, a good question is the following: is this a real problem? In my experience thus far, defining stand-alone grammars for programming languages using Earley parsing (i.e. a parsing algorithm in which ambiguity is possible), it’s not been a huge problem: as the grammar designer, I often understand where dangerous ambiguity might exist, and can nip it in the bud. I’ve been caught out a couple of times, but not enough to really worry about.</p>
<p>However, I do not think that my experience will hold in the face of wide-spread grammar composition. The theoretical reason is easily stated: combining two unambiguous grammars may result in an ambiguous grammar (which, as previously stated, we are unlikely to be able to statically determine in general). Consider combining two grammars from different authors, neither of whom could have anticipated the particular composition: it seems to me that ambiguity is much more likely to crop up in such cases. It will then remain undetected until an unfortunate user finds an input which triggers the ambiguity. Compilers which fail on seemingly valid input are unlikely to be popular.</p>
<h3>PEGs</h3>
<p>As stated earlier, unambiguous parsing algorithms such as LL and LR aren’t easily usable in grammar composition. More recently, a rediscovered parsing approach has gathered a lot of attention: <a href="http://pdos.csail.mit.edu/~baford/packrat/">Packrat / PEG parsing</a> (which I henceforth refer to as PEGs). PEGs are different than everything mentioned previously: they have no formal relation to CFGs. The chief reason for this is PEGs <em>ordered choice</em> operator, which removes any possibility for ambiguity in PEGs. PEGs are interesting because, unlike LL and LR, they’re closed under composition: in other words, if you have two PEGs and compose them, you have a valid PEG.</p>
<p>Are PEGs the answer to our problems? Alas - at least as things stand - I now doubt it. First, PEGs are rather inexpressive: like LL and LR parsing, PEGs are often frustrating to use in practise. This is, principally, because they don’t support left recursion; Alex Warth proposed <a href="http://www.vpri.org/pdf/tr2007002_packrat.pdf">an approach which adds left recursion</a> but <a href="https://tratt.net/laurie/research/pubs/html/tratt__direct_left_recursive_parsing_expression_grammars/">I discovered what appear to be problems with it</a>, though I should note that there is not yet a general consensus on this (and I am collaborating with a colleague to try and reach an understanding of precisely what left recursion in PEGs should mean). Second, while PEGs are always unambiguous, depending on the glue one uses during composition, the ordered choice operator may cause strings that were previously accepted in the individual languages not to be accepted in the combined language - which, to put it mildly, is unlikely to be the desired behaviour.</p>
<h2>Conclusions</h2>
<p>If you’ve got this far, well done. This article has ended up much longer than I originally expected - though far shorter than it could be if I really went into detail on some of these points! It is important to note that <strong>I am not a parsing expert</strong>: I only ever wanted to be a <em>user</em> of parsing, not - as I currently am - someone who knows bits and pieces about its inner workings. What’s happened is that, in wanting to make greater use of parsing, I have gradually become aware of the limitations of what I have been able to find. The emphasis is on gradually: knowledge about parsing is scattered over several decades (from the 60s right up to the present day); many publications (some of them hard to get hold of); and many peoples heads (some of whom no longer work in computing, let alone in the area of parsing). It is therefore hard to get an understanding of the range of approaches or their limitations. This article is my attempt to write down my current understanding and, in particular, the limitations of current approaches when composing grammars; I welcome corrections from those more knowledgeable than myself. Predicting the future is a mugs game, but I am starting to wonder whether, if we fail to come up with more suitable parsing algorithms, programming languages of the future that wish to allow syntax extension will bypass parsing altogether, and use syntax directed editing instead. Many people think parsing is a solved problem - I think it isn’t.</p>


<div id="article-date">

<p><a href="https://tratt.net/laurie/blog/2011/problems_whose_solution_are_easy_to_state.html">Newer</a>

2011-03-15 08:00

<a href="https://tratt.net/laurie/blog/2010/in_praise_of_the_imperfect.html">Older</a>

</p></div>

<div id="article-updates"><p>
If you’d like updates on new blog posts: follow me on
<a href="https://mastodon.social/@ltratt">Mastodon</a>
or <a href="https://twitter.com/laurencetratt">Twitter</a>;
or <a href="https://tratt.net/laurie/blog/blog.rss">subscribe to the RSS feed</a>;
or <a href="https://tratt.net/laurie/newsletter/">subscribe to email updates</a>:

</p>

</div>



<h3>Comments</h3>







</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Air Canada Has to Honor a Refund Policy Its Chatbot Made Up (241 pts)]]></title>
            <link>https://www.wired.com/story/air-canada-chatbot-refund-policy</link>
            <guid>39455131</guid>
            <pubDate>Wed, 21 Feb 2024 15:40:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wired.com/story/air-canada-chatbot-refund-policy">https://www.wired.com/story/air-canada-chatbot-refund-policy</a>, See on <a href="https://news.ycombinator.com/item?id=39455131">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-journey-hook="client-content" data-testid="BodyWrapper"><p>After months of resisting, Air Canada was <a data-offer-url="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html&quot;}" href="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html" rel="noopener" target="_blank">forced</a> to give a partial refund to a grieving passenger who was misled by an airline chatbot inaccurately explaining the airline's bereavement travel policy.</p><p>On the day Jake Moffatt's grandmother died, Moffat immediately visited Air Canada's website to book a flight from Vancouver to Toronto. Unsure of how Air Canada's bereavement rates worked, Moffatt asked Air Canada's chatbot to explain.</p><p>The chatbot provided inaccurate information, encouraging Moffatt to book a flight immediately and then request a refund within 90 days. In reality, Air Canada's policy explicitly stated that the airline will not provide refunds for bereavement travel after the flight is booked. Moffatt dutifully attempted to follow the chatbot's advice and request a refund but was shocked that the request was rejected.</p><p>Moffatt tried for months to convince Air Canada that a refund was owed, sharing a screenshot from the chatbot that clearly claimed:</p><blockquote><p>If you need to travel immediately or have already travelled and would like to submit your ticket for a reduced bereavement rate, kindly do so within 90 days of the date your ticket was issued by completing our Ticket Refund Application form.</p></blockquote><p>Air Canada argued that because the chatbot response elsewhere linked to a page with the actual bereavement travel policy, Moffatt should have known bereavement rates could not be requested retroactively. Instead of a refund, the best Air Canada would do was to promise to update the chatbot and offer Moffatt a $200 coupon to use on a future flight.</p><p>Unhappy with this resolution, Moffatt refused the coupon and filed a small claims complaint in Canada's Civil Resolution Tribunal.</p><p>According to Air Canada, Moffatt never should have trusted the chatbot and the airline should not be liable for the chatbot's misleading information because, Air Canada essentially argued, "the chatbot is a separate legal entity that is responsible for its own actions," a <a data-offer-url="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html&quot;}" href="https://www.canlii.org/en/bc/bccrt/doc/2024/2024bccrt149/2024bccrt149.html" rel="noopener" target="_blank">court order</a> said.</p><p>Experts <a data-offer-url="https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot" data-event-click="{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot&quot;}" href="https://vancouversun.com/news/local-news/air-canada-told-it-is-responsible-for-errors-by-its-website-chatbot" rel="noopener" target="_blank">told the <em>Vancouver Sun</em></a> that Moffatt's case appeared to be the first time a Canadian company tried to argue that it wasn't liable for information provided by its chatbot.</p><p>Tribunal member Christopher Rivers, who decided the case in favor of Moffatt, called Air Canada's defense "remarkable."</p><p>"Air Canada argues it cannot be held liable for information provided by one of its agents, servants, or representatives—including a chatbot," Rivers wrote. "It does not explain why it believes that is the case" or "why the webpage titled 'Bereavement travel' was inherently more trustworthy than its chatbot."</p><p>Further, Rivers found that Moffatt had "no reason" to believe that one part of Air Canada's website would be accurate and another would not.</p><p>Air Canada "does not explain why customers should have to double-check information found in one part of its website on another part of its website," Rivers wrote.</p><p>In the end, Rivers ruled that Moffatt was entitled to a partial refund of $650.88 in Canadian dollars off the original fare (about $482 USD), which was $1,640.36 CAD (about $1,216 USD), as well as additional damages to cover interest on the airfare and Moffatt's tribunal fees.</p><p>Air Canada told Ars it will comply with the ruling and considers the matter closed.</p><p>Air Canada’s Chatbot Appears to Be Disabled</p><p>When Ars visited Air Canada's website on Friday, there appeared to be no chatbot support available, suggesting that Air Canada has disabled the chatbot.</p><p>Air Canada did not respond to Ars' request to confirm whether the chatbot is still part of the airline's online support offerings.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Coffee machine camera at Amazon warehouse raises concerns about surveillance (144 pts)]]></title>
            <link>https://sahanjournal.com/business-work/amazon-coffee-machine-camera-surveillance/</link>
            <guid>39455057</guid>
            <pubDate>Wed, 21 Feb 2024 15:34:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://sahanjournal.com/business-work/amazon-coffee-machine-camera-surveillance/">https://sahanjournal.com/business-work/amazon-coffee-machine-camera-surveillance/</a>, See on <a href="https://news.ycombinator.com/item?id=39455057">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="content">
		<main id="main">

								

				
				<div>

					
			<figure>

				<img width="1200" height="800" src="https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1200x800.png" alt="" data-hero-candidate="1" fetchpriority="high" decoding="async" srcset="https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1200x800.png 1200w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-300x200.png 300w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1024x683.png 1024w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-768x512.png 768w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1536x1024.png 1536w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-600x400.png?crop=1 600w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1568x1045.png 1568w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-400x267.png 400w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47-706x471.png 706w, https://sahanjournal.com/wp-content/uploads/2023/04/3-47.png 1800w" sizes="(max-width: 1200px) 100vw, 1200px" data-attachment-id="46318" data-permalink="https://sahanjournal.com/3-190/" data-orig-file="https://sahanjournal.com/wp-content/uploads/2023/04/3-47.png" data-orig-size="1800,1200" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Amazon hidden camera" data-image-description="" data-image-caption="<p>Images of a touch screen on a coffee vending machine that took several photos of the breakroom inside Amazon’s Maple Grove warehouse without workers’ knowledge. A worker discovered the images on February 9, 2024, leading workers to capture these images of the machine’s photo gallery.</p>
" data-medium-file="https://sahanjournal.com/wp-content/uploads/2023/04/3-47-300x200.png" data-large-file="https://sahanjournal.com/wp-content/uploads/2023/04/3-47-1024x683.png">			<figcaption><span>Images of a touch screen on a coffee vending machine that took several photos of the breakroom inside Amazon's Maple Grove warehouse without workers' knowledge. A worker discovered the images on February 9, 2024, leading workers to capture these images of the machine's photo gallery. <span><span>Credit:</span> Provided by Jonathan Canaday</span></span></figcaption>
			
			</figure><!-- .post-thumbnail -->

		

<article id="post-46319">
	<div>

		
		
<p>Amazon is in the process of replacing a breakroom coffee machine in its Maple Grove warehouse after an employee inadvertently discovered a camera inside the machine that took pictures of the breakroom.</p>

<p>The discovery alarmed a local nonprofit that works with East African workers, who comprise a sizable amount of Amazon’s Minnesota workforce. Local East African Muslim workers have a history of protesting the company’s wages and workplace conditions, including its handling of workers’ daily prayer practices and requests for time off for Muslim holidays.</p>

<p>An employee at the DMP1 Amazon Delivery Station was getting coffee on February 9 when he saw a button on the coffee vending machine’s touch screen that read “gallery,” according to Jonathan Canaday, a sortation associate who was working at the warehouse that day. The employee pushed the button and came across around half a dozen random photos of the breakroom, according to Canaday.&nbsp;</p>


<p>The discovery quickly “spread like wildfire” and created a “hubbub,” Canaday said, as Amazon workers flocked to the machine to see the pictures for themselves. Management quickly came to the scene to “triage the situation.”&nbsp;</p>

<p>Abdirahman Muse, executive director of Awood Center, a labor organization that works with Minnesota’s East African community, said the situation is concerning because of previous incidents <a href="https://apnews.com/article/amazon-fine-monitoring-workers-privacy-france-d503314234ccacb366e2afaa49d097b1" target="_blank" rel="noreferrer noopener">where the retail giant used tech to track workers’ productivity in excessive and illegal ways</a>. He said that Amazon workers must feel free to talk about work in the breakroom without the fear of being surveilled.</p>

<p>“East African workers organizing with the Awood Center have been raising concerns about invasion of their privacy by Amazon for years, advocating for improved working conditions,” Abdirahman said. “They’re seriously concerned about this recent allegation.”&nbsp;</p>

<p>Amazon spokesperson Alisa Carroll said a malfunction caused the camera to take photos. The machine, provided by the vending supplier Canteen, features a touch screen and uses a camera for people to&nbsp; purchase coffee with a card, she said. But Carroll added that Amazon offers the coffee to employees free of charge and has no practical use for the camera. The cameras cannot capture audio, she added.&nbsp;</p>


<p>Upon discovery of the camera taking pictures, Amazon management addressed the issue with employees and immediately contacted Canteen, which sent workers to the warehouse to take a look at the machine, Carroll said. Canteen did not return messages seeking comment for this story.&nbsp;</p>



<p>The machine is currently operating in the breakroom with a piece of tape covering the camera, as Amazon awaits a new coffee machine that either doesn’t have a camera or has a disabled camera, she said.&nbsp;</p>


<p>The breakroom in question has two other cameras present: a security camera on the ceiling and another security camera attached to a self-checkout kiosk employees use when buying food, Canaday said. Employees are aware of both security cameras, which are clearly visible and expected, Canaday said.&nbsp;</p>

<p>Canaday said he and other employees felt that the coffee machine was intrusive, because employees weren’t aware of its camera and the pictures until the accidental discovery. Workers use the breakroom to decompress and talk about “anything from our own lives to our opinions of what is going on at work that day,” he said.</p>


<p>“Since Amazon poses as a major tech company, it seems really negligent on their part to put a piece of machinery in their building that does something excessive like take pictures of the breakroom without people’s knowledge,” Canaday said. “We see there are security cameras around the building. That’s fine. But cameras in the coffee machine seem really duplicitous. It’s really upsetting.”</p>

<p>Canaday, who has worked for Amazon for the past year, estimates that the Maple Grove facility employs around 200 people, two-thirds of them people of color. He added that the same coffee machine is at other Minnesota warehouses, including one in Eagan and one in Centerville, which he recently helped open.&nbsp;</p>

<p>Carroll said she wasn’t sure how many of the same machines were at other Amazon workplaces. Amazon doesn’t intend for its employees to feel vulnerable in any way, she added.</p>
	
	
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	
			<div>
															<p><a href="https://sahanjournal.com/author/joey-peters/" rel="author">
											<img width="80" height="80" src="https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-150x150.jpg?crop=1" alt="" srcset="https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-150x150.jpg?crop=1 150w, https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-300x300.jpg?crop=1 300w, https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-768x768.jpg 768w, https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-400x400.jpg?crop=1 400w, https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-200x200.jpg?crop=1 200w, https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448.jpg 800w" data-attachment-id="25820" data-permalink="https://sahanjournal.com/cropped-sahanstaffphotos448-jpg/" data-orig-file="https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448.jpg" data-orig-size="800,800" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="cropped-SahanStaffphotos448.jpg" data-image-description="<p>https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448.jpg</p>
" data-image-caption="" data-medium-file="https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448-300x300.jpg?crop=1" data-large-file="https://sahanjournal.com/wp-content/uploads/2022/10/cropped-SahanStaffphotos448.jpg">											</a></p><div>
					<!-- .author-bio-header -->

											<p>
							Joey Peters is a reporter for Sahan Journal. He has been a journalist for 15 years. Before joining Sahan Journal, he worked for close to a decade in New Mexico, where his reporting prompted the resignation...															
													</p>
					
				</div><!-- .author-bio-text -->

			</div><!-- .author-bio -->
			
</article><!-- #post-${ID} -->
				</div><!-- .main-content -->

			
<!-- #secondary -->

		</main><!-- #main -->
	</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Remix Vite Is Now Stable (124 pts)]]></title>
            <link>https://remix.run/blog/remix-vite-stable</link>
            <guid>39453767</guid>
            <pubDate>Wed, 21 Feb 2024 13:52:01 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://remix.run/blog/remix-vite-stable">https://remix.run/blog/remix-vite-stable</a>, See on <a href="https://news.ycombinator.com/item?id=39453767">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Today we’re excited to announce that support for <a href="https://vitejs.dev/">Vite</a> is now stable in Remix v2.7.0! After the <a href="https://remix.run/blog/remix-heart-vite">initial unstable release of Remix Vite</a>, we’ve been hard at work refining and extending it over the past few months with help from all of our early adopters and community contributors.</p>
<p>Here’s what we’ve been up to:</p>
<ul>
<li>Closed <a href="https://github.com/remix-run/remix/issues?q=is%3Aissue+is%3Aclosed+label%3Avite+closed%3A%3C2024-02-21">75 issues</a> and merged <a href="https://github.com/remix-run/remix/pulls?q=is%3Apr+is%3Amerged+label%3Avite+closed%3A%3C2024-02-21">136 pull requests</a> related to Vite.</li>
<li>Migrated <a href="https://remix.run/">the website you’re currently looking at</a> to Vite, and assisted in the migration of <a href="https://kentcdodds.com/">kentcdodds.com</a> and <a href="https://shop.app/">shop.app</a> — with the latter resulting in a 50x HMR speed boost.</li>
<li>Shipped several new features and major improvements on top of Vite.</li>
</ul>
<p>Let’s break down the most significant changes since our initial release.</p>
<h2 id="spa-mode">SPA mode</h2>
<p>The most significant change we’ve made is so important that we’ll reserve discussing its impact on the React ecosystem for a later post.</p>
<p>The short version is that Remix now supports building purely static sites that don’t require a JavaScript server in production, all while keeping the benefits of Remix’s file based route conventions, automatic code splitting, route module prefetching, head tag management and more.</p>
<p>This unlocks an entirely new migration path for <a href="https://reactrouter.com/">React Router</a> consumers to move to Remix without having to switch to a server-rendered architecture — which for many people isn’t even an option.
And for anyone that wants to introduce a server to their Remix app in the future, the migration path is now much more straightforward.</p>
<p>For more information, check out the <a href="https://remix.run/docs/future/spa-mode">SPA mode documentation</a>.</p>
<h2 id="basename-support">Basename support</h2>
<p><a href="https://reactrouter.com/en/main/router-components/router">React Router supports setting a basename for your app</a>, allowing you to nest your entire application within a subpath — but this feature was <a href="https://github.com/remix-run/remix/discussions/2891">notably absent in Remix</a>.
While it was possible to work around this by manually prefixing routes and links, it obviously wasn’t as convenient as setting a single config value.</p>
<p>With the move to Vite, the lack of basename support became more apparent since <a href="https://vitejs.dev/config/shared-options.html#base">Vite exposes its own “base” option</a>.
Many consumers mistakenly assumed that this would work with Remix, but this option is really the same as <a href="https://remix.run/docs/en/main/file-conventions/remix-config#publicpath">Remix’s “publicPath” option</a>.</p>
<p>In order to avoid this confusion, there is no longer a <code>publicPath</code> option (you should use Vite’s <code>base</code> option instead), and the Remix Vite plugin now has a brand new <code>basename</code> option.</p>
<p>As a result, it’s never been easier to nest your Remix application within a subpath of your site, without having to touch your application code.</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>import</span> { <span>vitePlugin</span> <span>as</span> <span>remix</span> } <span>from</span> "<span>@remix-run/dev</span>";
</span><span data-line-number="2"><span>import</span> { <span>defineConfig</span> } <span>from</span> "<span>vite</span>";
</span><span data-line-number="3">
</span><span data-line-number="4"><span>export</span> <span>default</span> <span>defineConfig</span>({
</span><span data-line-number="5">  base: "<span>/my-app/public/</span>",
</span><span data-line-number="6">  plugins: [
</span><span data-line-number="7">    <span>remix</span>({
</span><span data-line-number="8">      basename: "<span>/my-app</span>",
</span><span data-line-number="9">    }),
</span><span data-line-number="10">  ],
</span><span data-line-number="11">});
</span></code></pre></div>
<h2 id="cloudflare-pages-support">Cloudflare Pages support</h2>
<p>With our initial unstable release of Remix Vite, <a href="https://pages.cloudflare.com/">Cloudflare Pages</a> support wasn’t quite ready yet. Cloudflare’s <code>workerd</code> runtime is completely separate from Vite’s Node environment so we needed to figure out the best way to bridge this gap.</p>
<p>With Remix Vite going stable, we now provide a built-in Vite plugin for integrating Cloudflare’s tooling with Remix during local development.</p>
<p>To simulate the Cloudflare environment in Vite, <a href="https://developers.cloudflare.com/workers/wrangler">Wrangler</a> provides <a href="https://developers.cloudflare.com/workers/wrangler/api/#getplatformproxy">Node proxies to local <code>workerd</code> bindings</a>.
Remix’s <code>cloudflareDevProxyVitePlugin</code> sets up these proxies for you:</p>
<div data-code-block="" data-filename="vite.config.ts" data-line-numbers="true" data-lang="typescript"><pre data-filename="vite.config.ts" data-line-numbers="true" data-lang="typescript"><code><span data-line-number="1"><span>import</span> {
</span><span data-line-number="2">  <span>vitePlugin</span> <span>as</span> <span>remix</span>,
</span><span data-highlight="true" data-line-number="3">  <span>cloudflareDevProxyVitePlugin</span> <span>as</span> <span>remixCloudflareDevProxy</span>,
</span><span data-line-number="4">} <span>from</span> "<span>@remix-run/dev</span>";
</span><span data-line-number="5"><span>import</span> { <span>defineConfig</span> } <span>from</span> "<span>vite</span>";
</span><span data-line-number="6">
</span><span data-line-number="7"><span>export</span> <span>default</span> <span>defineConfig</span>({
</span><span data-highlight="true" data-line-number="8">  plugins: [<span>remixCloudflareDevProxy</span>(), <span>remix</span>()],
</span><span data-line-number="9">});
</span></code></pre></div>
<p>The proxies are then available within <code>context.cloudflare</code> in your <code>loader</code> or <code>action</code> functions:</p>
<div data-code-block="" data-line-numbers="true" data-lang="typescript"><pre data-line-numbers="true" data-lang="typescript"><code><span data-line-number="1"><span>export</span> <span>const</span> <span>loader</span> <span>=</span> ({ <span>context</span> }<span>:</span> <span>LoaderFunctionArgs</span>) <span>=&gt;</span> {
</span><span data-line-number="2">  <span>const</span> { <span>env</span>, <span>cf</span>, <span>ctx</span> } <span>=</span> <span>context</span>.<span>cloudflare</span>;
</span><span data-line-number="3">  <span>// ... more loader code here...</span>
</span><span data-line-number="4">};
</span></code></pre></div>
<p>We’re still actively working with the Cloudflare team to ensure the best possible experience for Remix users.
In the future the integration is likely to be even more seamless by leveraging Vite’s new (still experimental) <a href="https://vitejs.dev/guide/api-vite-runtime#vite-runtime-api">Runtime API</a>, so stay tuned for further updates.</p>
<p>For more information on this feature, check out the <a href="https://remix.run/docs/future/vite#cloudflare">Remix Vite + Cloudflare documentation</a>.</p>
<h2 id="server-bundles">Server bundles</h2>
<p>For those of you who have been running <a href="https://vercel.com/docs/frameworks/remix">Remix on Vercel</a>, you may have noticed that Vercel allows you to split your server build into multiple bundles with different routes targeting <a href="https://vercel.com/docs/frameworks/remix#serverless-functions">serverless</a> and <a href="https://vercel.com/docs/frameworks/remix#edge-functions">edge functions</a>.</p>
<p>What you may not have realized is that this feature is actually achieved via a <a href="https://www.npmjs.com/package/@vercel/remix-run-dev">fork of Remix</a> that Vercel uses in its <a href="https://github.com/vercel/vercel/blob/main/packages/remix/src/build.ts">Remix builder</a>.</p>
<p>With the move to Vite, we wanted to ensure that another fork of our build system wasn’t necessary, so we’ve been working with the Vercel team to bring this feature to Remix Vite.
Now <em>anyone</em> — not just Vercel consumers — can split their server build into multiple bundles however they like.</p>
<p>Huge thanks to Vercel, and most notably <a href="https://n8.io/">Nathan Rajlich</a>, for helping out with this work.
For more information on this feature, check out the <a href="https://remix.run/docs/future/server-bundles">server bundles documentation</a>.</p>
<h2 id="presets">Presets</h2>
<p>When investigating Vercel support for Remix Vite, it became clear that we needed a way for other tools and hosting providers to customize the behavior of the Vite plugin without reaching into internals or running their own forks.
To support this, we’ve introduced the concept of “presets”.</p>
<p>Presets can only do two things:</p>
<ul>
<li>Configure the Remix Vite plugin on your behalf.</li>
<li>Validate the resolved config.</li>
</ul>
<p>Presets are designed to be published to npm and used within your Vite config.</p>
<p>The Vercel preset is coming soon, and we’re excited to see what other presets the community comes up with — especially since presets have access to all Remix Vite plugin options and are therefore not strictly limited to hosting provider support.</p>
<p>For more information on this feature, including guidance on how to create your own presets, check out the <a href="https://remix.run/docs/future/presets">presets documentation</a>.</p>
<h2 id="better-server-and-client-separation">Better server and client separation</h2>
<p>Remix allows you to name files with a <code>.server.ts</code> extension to ensure they never accidentally end up on the client.
However, it turned out that our previous implementation wasn’t compatible with Vite’s ESM model, so we were forced to revisit our approach.</p>
<p>Instead, what if we made it a compile-time error whenever <code>.server.ts</code> files are imported in a client code path?</p>
<p>Our previous approach resulted in runtime errors that could easily slip through to production.
Raising these errors during the build prevents them from impacting real users, while providing faster, more comprehensive feedback to developers.
We quickly realized this is <em>much</em> better.</p>
<p>As a bonus, since we were already working in this area, we decided to add support for <code>.server</code> <em>directories</em>, not just files, making it easy to mark entire portions of your project as server-only.</p>
<p>If you’d like to dive deeper into the rationale behind this change, check out our <a href="https://github.com/remix-run/remix/blob/main/decisions/0010-splitting-up-client-and-server-code-in-vite.md">decision document on splitting up client and server code in Vite</a>.</p>
<h3 id="vite-env-only">vite-env-only</h3>
<p>In the interest of speed, Vite lazily compiles each file in isolation.
Out of the box, Vite assumes any file referenced by client code is fully client-safe.</p>
<p>Remix automatically handles the removal of <code>loader</code>, <code>action</code> and <code>headers</code> exports from route files, ensuring they are always safe for the browser.
But what about non-Remix exports? How do we know which of these to remove from the browser build — and not just from routes, but from any module in your project?</p>
<p>For example, what if you wanted to write something like this?</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>import</span> { <span>db</span> } <span>from</span> "<span>~/.server/db</span>";
</span><span data-line-number="2">
</span><span data-line-number="3"><span>// This export is server-only ❌</span>
</span><span data-line-number="4"><span>export</span> <span>const</span> <span>getPosts</span> <span>=</span> <span>async</span> () <span>=&gt;</span> <span>db</span>.<span>posts</span>.<span>findMany</span>();
</span><span data-line-number="5">
</span><span data-line-number="6"><span>// This export is client-safe ✅</span>
</span><span data-line-number="7"><span>export</span> <span>const</span> <span>PostPreview</span> <span>=</span> ({ <span>title</span>, <span>description</span> }) <span>=&gt;</span> (
</span><span data-line-number="8">  &lt;<span>article</span>&gt;
</span><span data-line-number="9">    &lt;<span>h2</span>&gt;<span>{</span><span>title</span><span>}</span>&lt;/<span>h2</span>&gt;
</span><span data-line-number="10">    &lt;<span>p</span>&gt;<span>{</span><span>description</span><span>}</span>&lt;/<span>p</span>&gt;
</span><span data-line-number="11">  &lt;/<span>article</span>&gt;
</span><span data-line-number="12">);
</span></code></pre></div>
<p>In this file’s current state, Remix would throw a compile-time error due to the use of a <code>.server</code> module on the client.
This is a good thing! You definitely don’t want to leak server-only code to the client.
You could fix this by splitting server-only code into its own file, but it’d be nice if you didn’t have to restructure your code if you didn’t want to — especially if you’re migrating an existing project!</p>
<p>This problem isn’t specific to Remix.
It actually affects any full-stack Vite project, so we wrote a standalone Vite plugin called <a href="https://github.com/pcattori/vite-env-only">vite-env-only</a> to solve it.
This plugin lets you mark individual <em>expressions</em> as server-only or client-only.</p>
<p>For example, when using the <code>serverOnly$</code> macro:</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>import</span> { <span>serverOnly$</span> } <span>from</span> "<span>vite-env-only</span>";
</span><span data-line-number="2">
</span><span data-line-number="3"><span>import</span> { <span>db</span> } <span>from</span> "<span>~/.server/db</span>";
</span><span data-line-number="4">
</span><span data-line-number="5"><span>export</span> <span>const</span> <span>getPosts</span> <span>=</span> <span>serverOnly$</span>(<span>async</span> () <span>=&gt;</span> <span>db</span>.<span>posts</span>.<span>findMany</span>());
</span><span data-line-number="6">
</span><span data-line-number="7"><span>export</span> <span>const</span> <span>PostPreview</span> <span>=</span> ({ <span>title</span>, <span>description</span> }) <span>=&gt;</span> (
</span><span data-line-number="8">  &lt;<span>article</span>&gt;
</span><span data-line-number="9">    &lt;<span>h2</span>&gt;<span>{</span><span>title</span><span>}</span>&lt;/<span>h2</span>&gt;
</span><span data-line-number="10">    &lt;<span>p</span>&gt;<span>{</span><span>description</span><span>}</span>&lt;/<span>p</span>&gt;
</span><span data-line-number="11">  &lt;/<span>article</span>&gt;
</span><span data-line-number="12">);
</span></code></pre></div>
<p>On the client, this becomes:</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>export</span> <span>const</span> <span>getPosts</span> <span>=</span> <span>undefined</span>;
</span><span data-line-number="2">
</span><span data-line-number="3"><span>export</span> <span>const</span> <span>PostPreview</span> <span>=</span> ({ <span>title</span>, <span>description</span> }) <span>=&gt;</span> (
</span><span data-line-number="4">  &lt;<span>article</span>&gt;
</span><span data-line-number="5">    &lt;<span>h2</span>&gt;<span>{</span><span>title</span><span>}</span>&lt;/<span>h2</span>&gt;
</span><span data-line-number="6">    &lt;<span>p</span>&gt;<span>{</span><span>description</span><span>}</span>&lt;/<span>p</span>&gt;
</span><span data-line-number="7">  &lt;/<span>article</span>&gt;
</span><span data-line-number="8">);
</span></code></pre></div>
<p><strong>It’s worth reiterating that this is a separate Vite plugin and not a feature of Remix.</strong> It’s completely up to you whether you prefer to use <code>vite-env-only</code>, split your server-only code into separate files, or even bring your own Vite plugin.</p>
<p>For more information, check out our <a href="https://remix.run/docs/future/vite#splitting-up-client-and-server-code">documentation on splitting up client and server code</a>.</p>
<h2 id="cssurl-imports"><code>.css?url</code> imports</h2>
<p>From the very beginning, Remix has provided an <a href="https://remix.run/docs/styling/css">alternative model for managing CSS imports</a>.
When importing a CSS file, its URL is provided as a string for rendering in a <code>link</code> tag:</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>import</span> <span>type</span> { <span>LinksFunction</span> } <span>from</span> "<span>@remix-run/node</span>"; <span>// or cloudflare/deno</span>
</span><span data-line-number="2">
</span><span data-line-number="3"><span>import</span> <span>styles</span> <span>from</span> "<span>~/styles/dashboard.css</span>";
</span><span data-line-number="4">
</span><span data-line-number="5"><span>export</span> <span>const</span> <span>links</span><span>:</span> <span>LinksFunction</span> <span>=</span> () <span>=&gt;</span> [{ rel: "<span>stylesheet</span>", href: <span>styles</span> }];
</span></code></pre></div>
<p>While Vite has supported <a href="https://vitejs.dev/guide/assets#importing-asset-as-url">importing static assets as URLs</a> for a long time now, this did not work for CSS files if they required any processing such as <a href="https://vitejs.dev/guide/features#postcss">PostCSS</a> (including <a href="https://tailwindcss.com/">Tailwind</a>), <a href="https://vitejs.dev/guide/features#css-modules">CSS Modules</a>, <a href="https://vitejs.dev/guide/features#css-pre-processors">CSS preprocessors</a> etc.</p>
<p>With the recent release of <a href="https://vitejs.dev/blog/announcing-vite5-1">Vite v5.1.0</a>, full CSS support is now possible via the <code>.css?url</code> import syntax:</p>
<div data-code-block="" data-line-numbers="true" data-lang="tsx"><pre data-line-numbers="true" data-lang="tsx"><code><span data-line-number="1"><span>import</span> <span>styles</span> <span>from</span> "<span>~/styles/dashboard.css?url</span>";
</span></code></pre></div>
<h2 id="cleaner-build-output">Cleaner build output</h2>
<p>The old Remix compiler built the client and server into separate directories that could be configured independently.
By default, the output directories were <code>public/build</code> for client assets and <code>build</code> for the server.
It turned out that this structure conflicted with <a href="https://vitejs.dev/guide/assets.html#the-public-directory">Vite’s public directory</a>.</p>
<p>Since Vite copies files from <code>public</code> into the client build directory, and Remix’s client build directory was nested within the public directory, some consumers found their public directory being recursively copied into&nbsp;itself&nbsp;🫠</p>
<p>To fix this, we had to rearrange our build output a bit.
Remix Vite now has a single top-level <code>buildDirectory</code> option that defaults to <code>"build"</code>, resulting in <code>build/client</code> and <code>build/server</code> directories.</p>
<p>The funny thing is that even though we only implemented this change to fix a bug, we actually much prefer this structure.
And based on the feedback we received, so did our early adopters!</p>
<h2 id="more-than-just-a-vite-plugin">More than just a Vite plugin</h2>
<p>Our earliest adopters ran the Vite CLI directly — <code>vite dev</code> for local development and <code>vite build &amp;&amp; vite build --ssr</code> to build for production.
Due to the lack of a custom wrapper around Vite, our initial unstable release post mentioned that Remix was now “just a Vite plugin”.</p>
<p>However, with the introduction of server bundles, we were unable to hang onto this approach.
When using the <code>serverBundles</code> option there would now be a dynamic number of server builds.
We had assumed that we’d be able to define multiple inputs and outputs to Vite’s <code>ssr</code> build but this turned out not to be the case, so Remix needed a way to orchestrate the entire build process.
The Vite plugin also now provides a new <code>buildEnd</code> hook so you can run your own custom logic once the Remix build has finished.</p>
<p>We’ve kept as much of our old architecture as possible by maximizing the amount of code in our Vite plugin (and we’re glad we did!), and added <code>remix vite:dev</code> and <code>remix vite:build</code> commands to the Remix CLI.
In Remix v3, these commands will become the default <code>dev</code> and <code>build</code> commands.</p>
<p>So while we’re no longer “just a Vite plugin”, it’s fair to say that we’re still <em>mostly</em> just a Vite&nbsp;plugin&nbsp;🙂</p>
<h2 id="next-steps">Next steps</h2>
<p>Now that Remix Vite is stable, you’ll start to see our documentation and templates moving over to Vite by default.</p>
<p>Just like with our initial unstable release, we have a <a href="https://remix.run/docs/future/vite#migrating">migration guide</a> for those of you looking to move your existing Remix projects over to Vite.</p>
<p>Rest assured that the old Remix compiler will continue to work in Remix v2.
However, from now on all new features and improvements that require compiler integrations will only be targeting Vite.
In the future Vite will be the only official way to build Remix apps, so we encourage you to start migrating as soon as possible.</p>
<p>If you have any feedback for us along the way, please reach out.
We’d love to hear from you!</p>
<h2 id="thank-you">Thank you</h2>
<p>Thank you to all of our early adopters in the Remix community for providing feedback, raising issues and submitting pull requests.
We couldn’t have come this far without you.</p>
<p>We’d also like to extend an extra special thank you to <a href="https://github.com/hi-ogawa">Hiroshi Ogawa</a>, an outside contributor who landed an astounding <a href="https://github.com/remix-run/remix/pulls?q=is%3Apr+is%3Amerged+label%3Avite+closed%3A%3C2024-02-21+author%3Ahi-ogawa">25 pull requests</a> into Remix&nbsp;Vite&nbsp;🔥</p>
<p>And as always, thank you to the Vite team for providing such an amazing tool for us to build on top of.
We’re excited to see where we can take it together.</p>
<p>💿⚡️🚀</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[iMessage with PQ3 Cryptographic Protocol (467 pts)]]></title>
            <link>https://security.apple.com/blog/imessage-pq3/</link>
            <guid>39453660</guid>
            <pubDate>Wed, 21 Feb 2024 13:43:34 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://security.apple.com/blog/imessage-pq3/">https://security.apple.com/blog/imessage-pq3/</a>, See on <a href="https://news.ycombinator.com/item?id=39453660">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>Today we are announcing the most significant cryptographic security upgrade in iMessage history with the introduction of PQ3, a groundbreaking post-quantum cryptographic protocol that advances the state of the art of end-to-end secure messaging. With compromise-resilient encryption and extensive defenses against even highly sophisticated quantum attacks, PQ3 is the first messaging protocol to reach what we call Level 3 security — providing protocol protections that surpass those in all other widely deployed messaging apps. To our knowledge, PQ3 has the strongest security properties of any at-scale messaging protocol in the world.</p>
<div><figure><picture><source srcset="https://security.apple.com/assets/image/generated/xlarge_quantum_security_messaging_apps_LightMode.png" media="(prefers-color-scheme: light)" decoding="async" loading="lazy"><img alt="Messaging apps are placed on a spectrum starting with classical cryptography and progressing towards quantum security. Most apps fall into Level 0, with no end-to-end encryption by default and no quantum security, or Level 1, with end-to-end encryption by default, but with no quantum security. Signal’s PQXDH protocol introduces post-quantum security in the initial key establishment at Level 2 and iMessage with PQ3 attains Level 3, where post-quantum cryptography is used to secure both the initial key establishment and the ongoing message exchange." srcset="https://security.apple.com/assets/image/generated/xlarge_quantum_security_messaging_apps_DarkMode.png" sizes="(max-width:734px) 100vw, (max-width:1068px) calc(100vw - 76px), 650px" decoding="async" loading="lazy"></picture></figure></div>
<p>When iMessage launched in 2011, it was the first widely available messaging app to provide end-to-end encryption by default, and we have significantly upgraded its cryptography over the years. We most recently strengthened the iMessage cryptographic protocol in 2019 by switching from RSA to Elliptic Curve cryptography (ECC), and by protecting encryption keys on device with the Secure Enclave, making them significantly harder to extract from a device even for the most sophisticated adversaries. That protocol update went even further with an additional layer of defense: a periodic rekey mechanism to provide cryptographic self-healing even in the extremely unlikely case that a key ever became compromised. Each of these advances were formally verified by symbolic evaluation, a best practice that provides strong assurances of the security of cryptographic protocols.</p>
<p>Historically, messaging platforms have used classical public key cryptography, such as RSA, Elliptic Curve signatures, and Diffie-Hellman key exchange, to establish secure end-to-end encrypted connections between devices. All these algorithms are based on difficult mathematical problems that have long been considered too computationally intensive for computers to solve, even when accounting for Moore’s law. However, the rise of quantum computing threatens to change the equation. A sufficiently powerful quantum computer could solve these classical mathematical problems in fundamentally different ways, and therefore — in theory — do so fast enough to threaten the security of end-to-end encrypted communications.</p>
<p>Although quantum computers with this capability don’t exist yet, extremely well-resourced attackers can already prepare for their possible arrival by taking advantage of the steep decrease in modern data storage costs. The premise is simple: such attackers can collect large amounts of today’s encrypted data and file it all away for future reference. Even though they can’t decrypt any of this data today, they can retain it until they acquire a quantum computer that can decrypt it in the future, an attack scenario known as <em>Harvest Now, Decrypt Later</em>.</p>
<p>To mitigate risks from future quantum computers, the cryptographic community has been working on post-quantum cryptography (PQC): new public key algorithms that provide the building blocks for quantum-secure protocols but don’t require a quantum computer to run — that is, protocols that can run on the classical, non-quantum computers we’re all using today, but that will remain secure from known threats posed by future quantum computers.</p>
<p>To reason through how various messaging applications mitigate attacks, it’s helpful to place them along a spectrum of security properties. There’s no standard comparison to employ for this purpose, so we lay out our own simple, coarse-grained progression of messaging security levels in the image at the top of this post: we start on the left with classical cryptography and progress towards quantum security, which addresses current and future threats from quantum computers. Most existing messaging apps fall either into Level 0 — no end-to-end encryption by default and no quantum security — or Level 1 — with end-to-end encryption by default, but with no quantum security. A few months ago, Signal added support for the PQXDH protocol, becoming the <a href="https://signal.org/blog/pqxdh/">first large-scale messaging app to introduce post-quantum security</a> in the initial key establishment. This is a welcome and critical step that, by our scale, elevated Signal from Level 1 to Level 2 security.</p>
<p>At Level 2, the application of post-quantum cryptography is limited to the initial key establishment, providing quantum security only if the conversation key material is never compromised. But today’s sophisticated adversaries already have incentives to compromise encryption keys, because doing so gives them the ability to decrypt messages protected by those keys for as long as the keys don’t change. To best protect end-to-end encrypted messaging, the post-quantum keys need to change on an ongoing basis to place an upper bound on how much of a conversation can be exposed by any single, point-in-time key compromise — both now and with future quantum computers. Therefore, we believe messaging protocols should go even further and attain Level 3 security, where post-quantum cryptography is used to secure both the initial key establishment and the ongoing message exchange, with the ability to rapidly and automatically restore the cryptographic security of a conversation even if a given key becomes compromised.</p>
<p>iMessage now meets this goal with a new cryptographic protocol that we call PQ3, offering the strongest protection against quantum attacks and becoming the only widely available messaging service to reach Level 3 security. Support for PQ3 will start to roll out with the public releases of iOS 17.4, iPadOS 17.4, macOS 14.4, and watchOS 10.4, and is already in the corresponding developer preview and beta releases. iMessage conversations between devices that support PQ3 are automatically ramping up to the post-quantum encryption protocol. As we gain operational experience with PQ3 at the massive global scale of iMessage, it will fully replace the existing protocol within all supported conversations this year.</p>
<h3>Designing PQ3</h3>
<p>More than simply replacing an existing algorithm with a new one, we rebuilt the iMessage cryptographic protocol from the ground up to advance the state of the art in end-to-end encryption, and to deliver on the following requirements:</p>
<ul>
<li>Introduce post-quantum cryptography from the start of a conversation, so that all communication is protected from current and future adversaries.</li>
<li>Mitigate the impact of key compromises by limiting how many past and future messages can be decrypted with a single compromised key.</li>
<li>Use a hybrid design to combine new post-quantum algorithms with current Elliptic Curve algorithms, ensuring that PQ3 can can never be less safe than the existing classical protocol.</li>
<li>Amortize message size to avoid excessive additional overhead from the added security.</li>
<li>Use formal verification methods to provide strong security assurances for the new protocol.</li>
</ul>
<p>PQ3 introduces a new post-quantum encryption key in the set of public keys each device generates locally and transmits to Apple servers as part of iMessage registration. For this application, we chose to use Kyber post-quantum public keys, an algorithm that received close scrutiny from the global cryptography community, and was selected by NIST as the Module Lattice-based Key Encapsulation Mechanism standard, or <a href="https://csrc.nist.gov/pubs/fips/203/ipd">ML-KEM</a>. This enables sender devices to obtain a receiver’s public keys and generate post-quantum encryption keys for the very first message, even if the receiver is offline. We refer to this as initial key establishment.</p>
<p>We then include — within conversations — a periodic post-quantum rekeying mechanism that has the ability to self-heal from key compromise and protect future messages. In PQ3, the new keys sent along with the conversation are used to create fresh message encryption keys that can’t be computed from past ones, thereby bringing the conversation back to a secure state even if previous keys were extracted or compromised by an adversary. PQ3 is the first large scale cryptographic messaging protocol to introduce this novel post-quantum rekeying property.</p>
<p>PQ3 employs a hybrid design that combines Elliptic Curve cryptography with post-quantum encryption both during the initial key establishment and during rekeying. Thus, the new cryptography is purely additive, and defeating PQ3 security requires defeating both the existing, classical ECC cryptography and the new post-quantum primitives. It also means the protocol benefits from all the experience we accumulated from deploying the ECC protocol and its implementations.</p>
<p>Rekeying in PQ3 involves transmitting fresh public key material in-band with the encrypted messages that devices are exchanging. A new public key based on Elliptic Curve Diffie-Hellman (ECDH) is transmitted inline with every response. The post-quantum key used by PQ3 has a significantly larger wire size than the existing protocol, so to meet our message size requirement we designed the quantum-secure rekeying to happen periodically rather than with every message. To determine whether a new post-quantum key is transmitted, PQ3 uses a rekeying condition that aims to balance the average size of messages on the wire, preserve the user experience in limited connectivity scenarios, and keep the global volume of messages within the capacity of our server infrastructure. Should the need arise, future software updates can increase the rekeying frequency in a way that’s backward-compatible with all devices that support PQ3.</p>
<p>With PQ3, iMessage continues to rely on classical cryptographic algorithms to authenticate the sender and verify the Contact Key Verification account key, because these mechanisms can’t be attacked retroactively with future quantum computers. To attempt to insert themselves in the middle of an iMessage conversation, an adversary would require a quantum computer capable of breaking one of the authentication keys before or at the time the communication takes place. In other words, these attacks cannot be performed in a <em>Harvest Now, Decrypt Later</em> scenario — they require the existence of a quantum computer capable of performing the attacks contemporaneously with the communication being attacked. We believe any such capability is still many years away, but as the threat of quantum computers evolves, we will continue to assess the need for post-quantum authentication to thwart such attacks.</p>
<h3>A formally proven protocol</h3>
<p>Our final requirement for iMessage PQ3 is formal verification — a mathematical proof of the intended security properties of the protocol. PQ3 received extensive review from Apple’s own multi-disciplinary teams in Security Engineering and Architecture (SEAR) as well as from some of the world’s foremost experts in cryptography. This includes a team led by Professor David Basin, head of the <a href="https://infsec.ethz.ch/">Information Security Group at ETH Zürich</a> and one of the inventors of <a href="https://tamarin-prover.com/">Tamarin</a> — a leading security protocol verification tool that was also used to evaluate PQ3 — as well as Professor Douglas Stebila from the University of Waterloo, who has performed extensive research on post-quantum security for internet protocols. Each took a different but complementary approach, using different mathematical models to demonstrate that as long as the underlying cryptographic algorithms remain secure, so does PQ3. Finally, a leading third-party security consultancy supplemented our internal implementation review with an independent assessment of the PQ3 source code, which found no security issues.</p>
<p>In the first mathematical analysis, <a href="https://security.apple.com/assets/files/Security_analysis_of_the_iMessage_PQ3_protocol_Stebila.pdf">Security analysis of the iMessage PQ3 protocol</a>, Professor Douglas Stebila focused on so-called game-based proofs. This technique, also known as reduction, defines a series of “games“ or logical statements to show that the protocol is at least as strong as the algorithms that underpin it. Stebila’s analysis shows that PQ3 provides confidentiality even in the presence of some key compromises against both classical and quantum adversaries, in both the initial key establishment and the ongoing rekeying phase of the protocol. The analysis decomposes the many layers of key derivations down to the message keys and proves that, for an attacker, they are indistinguishable from random noise. Through an extensive demonstration that considers different attack paths for classical and quantum attackers in the proofs, Stebila shows that the keys used for PQ3 are secure as long as either the Elliptic Curve Diffie-Hellman problem remains hard or the Kyber post-quantum KEM remains secure.</p>
<blockquote>
<p><em>The iMessage PQ3 protocol is a well-designed cryptographic protocol for secure messaging that uses state-of-the-art techniques for end-to-end encrypted communication. In my analysis using the reductionist security methodology, I confirmed that the PQ3 protocol provides post-quantum confidentiality, which can give users confidence in the privacy of their communication even in the face of potential improvements in quantum computing technology. —Professor Douglas Stebila</em></p>
</blockquote>
<p>In the second evaluation, <a href="https://security.apple.com/assets/files/A_Formal_Analysis_of_the_iMessage_PQ3_Messaging_Protocol_Basin_et_al.pdf">A Formal Analysis of the iMessage PQ3 Messaging Protocol</a>, Prof. David Basin, Felix Linker, and Dr. Ralf Sasse at ETH Zürich use a method called symbolic evaluation. As highlighted in the paper’s abstract, this analysis includes a detailed formal model of the iMessage PQ3 protocol, a precise specification of its fine-grained security properties, and machine-checked proofs using the state-of-the-art symbolic <a href="https://tamarin-prover.com/">Tamarin prover</a>. The evaluation yielded a fine-grained analysis of the secrecy properties of PQ3, proving that “in the absence of the sender or recipient being compromised, all keys and messages transmitted are secret” and that “compromises can be tolerated in a well-defined sense where the effect of the compromise on the secrecy of data is limited in time and effect,” which confirms that PQ3 meets our goals.</p>
<blockquote>
<p><em>We provide a mathematical model of PQ3 as well as prove its secrecy and authenticity properties using a verification tool for machine-checked security proofs. We prove the properties even when the protocol operates in the presence of very strong adversaries who can corrupt parties or possess quantum computers and therefore defeat classical cryptography. PQ3 goes beyond Signal with regards to post-quantum defenses. In PQ3, a post-quantum secure algorithm is part of the ratcheting and used repeatedly, rather than only once in the initialization as in Signal. Our verification provides a very high degree of assurance that the protocol as designed functions securely, even in the post-quantum world. —Professor David Basin</em></p>
</blockquote>
<h3>Diving into the details</h3>
<p>Because we know PQ3 will be of intense interest to security researchers and engineers as well as the cryptographic community, this blog post is really two posts in one. Up to now, we laid out our design goals, outlined how PQ3 meets them, and explained how we verified our confidence in the protocol with independent assessments. If you’d like to understand more detail about the cryptographic underpinnings, the remainder of the post is a deeper dive into how we constructed the PQ3 protocol.</p>
<h3>Post-quantum key establishment</h3>
<p>iMessage allows a user to register multiple devices on the same account. Each device generates its own set of encryption keys, and the private keys are never exported to any external system. The associated public keys are registered with Apple’s Identity Directory Service (IDS) to enable users to message each other using a simple identifier: email address or phone number. When a user sends a message from one of their devices, all of their other devices and all of the recipient’s devices receive the message. The messages are exchanged through pair-wise sessions established between the sending device and each receiving device. The same message is encrypted successively to each receiving device, with keys uniquely derived for each session. For the rest of this description, we will focus on a single device-to-device session.</p>
<p>Because the receiving device might not be online when the conversation is established, the first message in a session is encrypted using the public encryption keys registered with the IDS server.</p>
<p>Each device with PQ3 registers two public encryption keys and replaces them regularly with fresh ones:</p>
<ol>
<li>A post-quantum Kyber-1024 key encapsulation public key</li>
<li>A classical P-256 Elliptic Curve key agreement public key</li>
</ol>
<p>These encryption keys are signed with ECDSA using a P-256 authentication key generated by the device’s Secure Enclave, along with a timestamp used to limit their validity. The device authentication public key is itself signed by the <a href="https://security.apple.com/blog/imessage-contact-key-verification/">Contact Key Verification</a> account key, along with some attributes such as the supported cryptographic protocol version. This process allows the sender to verify that the recipient device’s public encryption keys were uploaded by the intended recipient, and it guards against downgrade attacks.</p>
<p>When Alice’s device instantiates a new session with Bob’s device, her device queries the IDS server for the key bundle associated with Bob’s device. The subset of the key bundle that contains the device’s authentication key and versioning information is validated using Contact Key Verification. The device then validates the signature covering the encryption keys and timestamps, which attests that the keys are valid and have not expired.</p>
<p>Alice’s device can then use the two public encryption keys to share two symmetric keys with Bob. The first symmetric key is computed through an ECDH key exchange that combines an ephemeral encryption key from Alice with Bob’s registered P-256 public key. The second symmetric key is obtained from a Kyber key encapsulation with Bob’s post-quantum public key.</p>
<p>To combine these two symmetric keys, we first extract their entropy by invoking HKDF-SHA384-Extract twice — once for each of the keys. The resulting 48-byte secret is further combined with a domain separation string and session information — which includes the user’s identifiers, the public keys used in the key exchange, and the encapsulated secret — by invoking HKDF-SHA384-Extract again to derive the session’s initial keying state. This combination ensures that the initial session state cannot be derived without knowing both of the shared secrets, meaning an attacker would need to break both algorithms to recover the resulting secret, thus satisfying our hybrid security requirement.</p>
<h3>Post-quantum rekeying</h3>
<p>Ongoing rekeying of the cryptographic session is designed such that keys used to encrypt past and future messages cannot be recomputed even by a powerful hypothetical attacker who is able to extract the cryptographic state of the device at a given point in time. The protocol generates a new unique key for each message, which periodically includes new entropy that is not deterministically derived from the current state of the conversation, effectively providing self-healing properties to the protocol. Our rekeying approach is modeled after ratcheting, a technique that consists of deriving a new session key from other keys and ensuring the cryptographic state always moves forward in one direction. PQ3 combines three ratchets to achieve post-quantum encryption.</p>
<p>The first ratchet, called the symmetric ratchet, protects older messages in a conversation to achieve forward secrecy. For every message, we derive a per-message encryption key from the current session key. The current session key itself is then further derived into a new session key, ratcheting the state forward. Each message key is deleted as soon as a corresponding message is decrypted, which prevents older harvested ciphertexts from being decrypted by an adversary who is able to compromise the device at a later time, and provides protection against replayed messages. This process uses 256-bit keys and intermediate values, and HKDF-SHA384 as a derivation function, which provides protection against both classical and quantum computers.</p>
<p>The second ratchet, called the ECDH ratchet, protects future messages by updating the session with fresh entropy from an Elliptic Curve key agreement, ensuring that an adversary loses the ability to decrypt new messages even if they had compromised past session keys — a property called post-compromise security. The ECDH-based ratchet has a symmetrical flow: the private key of the outgoing ratchet public key from the sender is used with the last public key received from the recipient to establish a new shared secret between sender and receiver, which is then mixed into the session’s key material. The new PQ3 protocol for iMessage uses NIST P-256 Elliptic Curve keys to perform this ratchet, which imposes only a small 32-byte overhead on each message.</p>
<p>Because the second ratchet uses classical cryptography, PQ3 also adds a conditionally executed Kyber KEM-based ratchet. This third ratchet complements the ECDH-based ratchet to provide post-compromise security against <em>Harvest Now, Decrypt Later</em> quantum attacks as well.</p>
<p>The use of a post-quantum ratchet can cause significant network overhead compared to an ECDH-based ratchet at the same security level. The post-quantum KEM requires sending both a public key and an encapsulated secret instead of a single outgoing public key. In addition, the underlying mathematical structure for quantum security requires significantly larger parameter sizes for public keys and encapsulated keys compared to Elliptic Curves.</p>
<p>To limit the size overhead incurred by frequent rekeying while preserving a high level of security, the post-quantum KEM is instantiated with Kyber-768. Unlike the IDS-registered public keys used for the initial key establishment, ratcheting public keys are used only once to encapsulate a shared secret to the receiver, significantly limiting the impact of the compromise of a single key. However, while a 32-byte ECDH-based ratchet overhead is acceptable on every message, the post-quantum KEM ratchet increases the message size by more than 2 kilobytes. To avoid visible delays in message delivery when device connectivity is limited, this ratchet needs to be amortized over multiple messages.</p>
<p>We therefore implemented an adaptive post-quantum rekeying criterion that takes into account the number of outgoing messages, the time elapsed since last rekeying, and current connectivity conditions. At launch, this means the post-quantum ratchet is performed approximately every 50 messages, but the criterion is bounded such that rekeying is always guaranteed to occur at least once every 7 days. And as we mentioned earlier, as the threat of quantum computers and infrastructure capacity evolves over time, future software updates can increase the rekeying frequency while preserving full backward compatibility.</p>
<p>Completing the public key ratchets, whether based on ECDH or Kyber, requires sending and receiving a message. Although users may not immediately reply to a message, iMessage includes encrypted delivery receipts that allow devices to rapidly complete the ratchet even without a reply from the recipient, as long as the device is online. This technique avoids delays in the rekeying process and helps support strong post-compromise recovery.</p>
<p>Similar to the initial session key establishment, the secrets established through the three ratchets are all combined with an evolving session key using HKDF-SHA384 through sequential calls to the Extract function. At the end of this process, we obtain a final message key, which can now be used to encrypt the payload.</p>
<h3>Padding and encryption</h3>
<p>To avoid leaking information about the message size, PQ3 adds padding to the message before encryption. This padding is implemented with the <a href="https://arxiv.org/abs/1806.03160">Padmé</a> heuristic, which specifically limits the information leakage of ciphertexts with maximum length M to a practical optimum of O(log log M) bits. This is comparable to padding to a power of two but results in a lower overhead of at most 12 percent and even lower for larger payloads. This approach strikes an excellent balance between privacy and efficiency, and preserves the user experience in limited device connectivity scenarios.</p>
<p>The padded payload is encrypted with AES-CTR using a 256-bit encryption key and initialization vector, both derived from the message key. While public key algorithms require fundamental changes to achieve quantum security, symmetric cryptography algorithms like the AES block cipher only require doubling the key size to maintain their level of security against quantum computers.</p>
<h3>Authentication</h3>
<p>Each message is individually signed with ECDSA using the elliptic curve P-256 device authentication key protected by the Secure Enclave. The receiving device verifies the mapping between the sender’s identifier (email address or phone number) and the public key used for signature verification. If both users have enabled Contact Key Verification and verified each other’s account key, the device verifies that the device authentication keys are present in the Key Transparency log and that the corresponding account key matches the account key stored in the user’s iCloud Keychain.</p>
<p>The device’s authentication key is generated by the Secure Enclave and never exposed to the rest of the device, which helps prevent extraction of the private key even if the Application Processor is completely compromised. If an attacker were to compromise the Application Processor, they might be able to use the Secure Enclave to sign arbitrary messages. But after the device recovers from the compromise through a reboot or a software update, they would no longer be able to impersonate the user. This approach offers stronger guarantees than other messaging protocols where the authentication key is sometimes shared between devices or where the authentication takes place only at the beginning of the session.</p>
<p>The message signature covers a wide range of fields, including the unique identifiers of the users and their push notification tokens, the encrypted payload, authenticated data, a ratchet-derived message key indicator that binds the signature to a unique location in the ratchet, and any public key information used in the protocol. The inclusion of these fields in the signature guarantees that the message can only be used in the context intended by the sender, and all the fields are exhaustively documented in the research papers from Stebila, Basin, and collaborators.</p>
<h3>Conclusion</h3>
<p>End-to-end encrypted messaging has seen a tremendous amount of innovation in recent years, including significant advances in post-quantum cryptography from Signal’s PQXDH protocol and in key transparency from WhatsApp’s Auditable Key Directory. Building on its pioneering legacy as the first widely available messaging app to provide end-to-end encryption by default, iMessage has continued to deliver advanced protections that surpass existing systems. iMessage <a href="https://security.apple.com/blog/imessage-contact-key-verification/">Contact Key Verification</a> is the most sophisticated key transparency system for messaging deployed at scale, and is the current global state of the art for automatic key verification. And the new PQ3 cryptographic protocol for iMessage combines post-quantum initial key establishment with three ongoing ratchets for self-healing against key compromise, defining the global state of the art for protecting messages against <em>Harvest Now, Decrypt Later</em> attacks and future quantum computers.</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Things I Don't Know About AI (199 pts)]]></title>
            <link>https://blog.eladgil.com/p/things-i-dont-know-about-ai</link>
            <guid>39453622</guid>
            <pubDate>Wed, 21 Feb 2024 13:40:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.eladgil.com/p/things-i-dont-know-about-ai">https://blog.eladgil.com/p/things-i-dont-know-about-ai</a>, See on <a href="https://news.ycombinator.com/item?id=39453622">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>In most markets, the more time passes the clearer things become. In generative AI (“AI”), it has been the opposite. The more time passes, the less I think I actually understand.</p><p>For each level of the AI stack, I have open questions. I list these out below to stimulate dialog and feedback.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png" width="1456" height="928" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:928,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6cea466b-d816-412e-8006-0ae5055142cc_1648x1050.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>There are in some sense two types of LLMs - frontier models - at the cutting edge of performance (think GPT-4 vs other models until recently), and everything else. </span><a href="https://blog.eladgil.com/p/ai-platforms-markets-and-open-source" rel="">In 2021 I wrote that I thought the frontier models</a><span> market would collapse over time into an oligopoly market due to the scale of capital needed. In parallel, non-frontier models would more commodity / pricing driven and have a stronger opensource presence (note this was pre-Llama and pre-Mistral launches).</span></p><p>Things seem to be evolving towards the above:</p><p>Frontier LLMs are likely to be an oligopoly market. Current contenders include closed source models like OpenAI, Google, Anthropic, and perhaps Grok/X.ai, and Llama (Meta) and Mistral on the open source side. This list may of course change in the coming year or two. Frontier models keep getting more and more expensive to train, while commodity models drop in price each year as performance goes up (for example, it is probably ~5X cheaper to train GPT-3.5 equivalent now than 2 years ago)</p><p><span>As model scale has gotten larger, funding increasingly has been primarily coming from the cloud providers / big tech. For example, Microsoft invested $10B+ in OpenAI, while Anthropic raised $7B between Amazon and Google. NVIDIA is also a big investor in foundation model companies of many types. The venture funding for these companies in contrast is a tiny drop in the ocean in comparison. As frontier model training booms in cost, the emerging funders are largely concentrated amongst big tech companies (typically with strong incentives to fund the area for their own revenue - ie cloud providers or NVIDIA), or nation states wanting to back local champions (see eg </span><a href="https://falconllm.tii.ae/" rel="">UAE and Falcon</a><span>). This is impacting the market and driving selection of potential winners early.</span></p><p><span>It is important to note that the scale of investments being made by these cloud providers is dwarfed by actual cloud revenue. For example, Azure from Microsoft generates $25B in revenue a quarter. The ~$10B OpenAI investment by Microsoft is roughly 6 weeks of Azure revenue. This suggests the cloud business (at least for now) is more important than any one model set for Azure (this may change if someone reaches true AGI or frontier model dominance). </span><a href="https://www.wsj.com/business/earnings/microsoft-msft-q2-earnings-report-2024-57743658" rel="">Indeed Azure grew 6 percentage points</a><span> in Q2 2024 from AI - which would put it at an annualized increase of $5-6B (or 50% of its investment in OpenAI! Per year!). Obviously revenue is not net income but this is striking nonetheless, and suggests the big clouds have an economic reason to fund more large scale models over time.</span></p><p><span>In parallel, Meta has done outstanding work with Llama models and recently announced </span><a href="https://www.pcmag.com/news/zuckerbergs-meta-is-spending-billions-to-buy-350000-nvidia-h100-gpus" rel="">$20B compute budget</a><span>, in part to fund massive model training. I </span><a href="https://blog.eladgil.com/p/ai-platforms-markets-and-open-source" rel="">posited 18 months ago that an open source sponsor for AI models </a><span>should emerge, but assumed it would be Amazon or NVIDIA with a lower chance of it being Meta. (Zuckerberg &amp; Yann Lecunn have been visionary here). </span></p><ul><li><p><strong>Are cloud providers king-making a handful of players at the frontier and locking in the oligopoly market via the sheer scale of compute/capital they provide?</strong><span> When do cloud providers stop funding new LLM foundation companies versus continuing to fund existing? Cloud providers are easily the biggest funders of foundation models, not venture capitalists. Given they are constrained in M&amp;A due to FTC actions, and the revenue that comes from cloud usage, it is rational for them to do so. This may lead / has led to some distortion of market dynamics. How does this impact the long term economics and market structure for LLMs? Does this mean we will see the end of new frontier LLM companies soon due to a lack of enough capital and talent for new entrants? Or do they keep funding large models hoping some will convert on their clouds to revenue?</span></p></li><li><p><strong>Does OSS models flip some of the economics in AI from foundation models to clouds? Does Meta continue to fund OS models? If so, does eg Llama-N catch up to the very frontier?</strong><span> A fully open source model performing at the very frontier of AI has the potential to flip the economic share of AI infra from LLMs towards cloud and inference providers and drains revenue away from the other LLM foundation model companies. This has implications on how to think about the relative importance of cloud and infrastructure companies in this market.</span></p><ul><li><p><span>One of the most brilliant things in the </span><a href="https://ai.meta.com/llama/license/" rel="">Llama2 terms of use is the open commercial use of the license if you have fewer then 700 million users</a><span>[1].  This obviously prevents some large competitors from using their models. But it also means if you are a big cloud provider you need to pay a license to Meta for Llama, which Microsoft has already done. This creates an interesting long term way for Meta to control (&amp; monetize) Llama despite being open source.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png" width="1456" height="151" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:151,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:75303,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83a41ad4-f48e-4900-89d9-18a2a27b4f91_1560x162.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div></li></ul></li><li><p><strong>How do we think about speed and price vs performance for models?</strong><span> One could imagine extremely slow incredibly performant models may be quite valuable if compared to normal human speed to do things. The latest largest</span><a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/" rel=""> Gemini models</a><span> seem to be heading in this direction with large 1 million+ token </span><a href="https://magic.dev/blog/ltm-1" rel="">context windows a la Magic</a><span>, which announced a 5 million token window in June 2023. Large context windows and depth of understanding can really change how we think about AI uses and engineering. On the other side of the spectrum, Mistral has shown the value of small, fast and cheap to inference performant models. The 2x2 below suggests a potential segmentation of where models will matter most.</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png" width="1456" height="933" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:933,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:190967,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F11307f4b-c11a-4549-9221-224c0c30b259_1770x1134.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><ul><li><p><strong>How do architectures for foundation models evolve? Do agentic models with different architectures subsume some of the future potential of LLMs? When do other forms of memory and reasoning come into play?</strong></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png" width="1456" height="795" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:795,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176023,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" title="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc89fa2c-29c5-4fef-b50d-9f81075005a4_2076x1134.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><ul><li><p><strong>Do governments back (or direct their purchasing to) regional AI champions?</strong><span> </span><strong>Will  national governments differentially spend on local models a la Boeing vs Airbus in aerospace? Do governments want to support models that reflect their local values, languages, etc?</strong><span> Besides cloud providers and global big tech (think also e.g. Alibaba, Rakuten etc) the other big sources of potential capital are countries. There are now great model companies in Europe (e.g. Mistral), Japan, India, UAE, China and other countries. If so, there may be a few multi-billion AI foundation model regional companies created just off of government revenue. </span></p></li><li><p><strong>What happens in China? </strong><span>One could anticipate Chinese LLMs to be backed by Tencent, Alibaba, Xiaomi, ByteDance and others investing in big ways into local LLMs companies. China’s government has long used regulatory and literal firewalls to prevent competition from non-Chinese companies and to build local, government supported and censored champions. One interesting thing to note is the trend of Chinese OSS models. Qwen from Alibaba for example has moved higher on the broader </span><a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard" rel="">LMSYS leaderboards</a><span>.</span></p></li><li><p><strong>What happens with X.ai?</strong><span> Seems like a wild card.</span></p></li><li><p><strong>How good does Google get?</strong><span> Google has the compute, scale, talent to make amazing things and is organized and moving fast. Google was always the worlds first AI-first company. Seems like a wild card.</span></p></li></ul><p><span>There are a few types of infrastructure companies with very different uses. For example, </span><a href="https://www.braintrustdata.com/" rel="">Braintrust</a><span> provides eval, prompt playgrounds, logging and proxies to help companies move from “vibe based” analysis of AI to data driven. </span><a href="http://scale.ai/" rel="">Scale.ai</a><span> and others play a key role in data labeling, fine tuning, and other areas. A number of these have open but less existential questions (for example how much of RLHF turns into RLAIF).</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png" width="1456" height="818" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:182107,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5411ad66-78b4-4b87-8438-cecb111e54d9_1976x1110.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>The biggest uncertainties and questions in AI infra have to do with the AI Cloud Stack and how it evolves. It seems like there are very different needs between startups and enterprises for AI cloud services. For startups, the new cloud providers and tooling (think Anyscale, Baseten, Modal, Replicate, Together, etc) seem to be taking a useful path resulting in fast adoption and revenue growth.</p><p>For enterprises, who tend to have specialized needs, there are some open questions. For example:</p><ul><li><p><strong>Does the current AI cloud companies need to build an on-premise/BYOC/VPN version of their offerings for larger enterprises?</strong><span> It seems like enterprises will optimize for (a) using their existing cloud marketplace credits which they already have budget for, to buy services (b) will be hesitant to round trip out from where their webapp / data is hosted (ie AWS, Azure, GCP) due to latency &amp; performance and (c) will care about security, compliance (FedRAMP, HIPAA etc). The short term startup market for AI cloud may differ from long term enterprise needs.</span></p></li><li><p><strong>How much of AI cloud adoption is due to constrained GPU / GPU arb?</strong><span> In the absence of GPU on the main cloud providers companies are scrambling to find sufficient GPU for their needs, accelerating adoption of new startups with their own GPU clouds. One potential strategy NVIDIA could be doing is preferentially allocating GPU to these new providers to decrease bargaining power of hyperscalers and to fragment the market, as well as to accelerate the industry via startups. </span><strong>When does the GPU bottleneck end and how does that impact new AI cloud providers? </strong><span>It seems like an end to GPU shortages on the main clouds would be negative for companies whose only business is GPU cloud, while those with more tools and services should have an easier transition if this were to happen. </span></p></li><li><p><strong><span>How do new AI ASICS like </span><a href="https://groq.com/" rel="">Groq</a><span> impact AI clouds?</span></strong></p></li><li><p><strong>What else gets consolidated into AI clouds?</strong><span> Do they cross sell embeddings &amp; RAG? Continuous updates? Fine tuning? Other services? How does that impact data labelers or others with overlapping offerings? What gets consolidated directly into model providers vs via the clouds?</span></p></li><li><p><strong>Which companies in the AI cloud will pursue which business model? </strong></p><ul><li><p><span>It is important to note there are really 2 market segments in the AI cloud world (a) startups (b) mid-market and enterprise. It seems likely that “GPU only” business model default works with the startup segment(who have fewer cloud needs), but for large enterprises adoption may be more driven by GPU cloud constraints on major platforms.  </span><strong>Do companies providing developer tooling, API endpoints, and/or specialized hardware, or other aspects morph into two other analogous models - (a) “Snowflake/Databricks for AI” model or (b) “Cloudflare for AI”? If so, which ones adopt which model?</strong></p></li></ul></li><li><p><strong>How big do the new AI clouds become? As large as Heroku, Digital Ocean, Snowflake, or AWS? What is the size of outcome and utilization scale for this class of company? </strong></p></li><li><p><strong>How does the AI stack evolve with very long context window models? How do we think about the interplay of context window &amp; prompt engineering, fine tuning, RAG, and inference costs?</strong></p></li><li><p><strong>How does FTC (and other regulator) prevention of M&amp;A impact this market? </strong><span>There are at least a dozen credible companies building AI cloud related products and services - too many for all of them to be stand alone. How does one think about exits under an administration that is aggressively against tech M&amp;A? Should the AI clouds themselves consolidate amongst themselves to consolidate share and services offered?</span></p></li></ul><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png" width="1456" height="724" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:145934,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cca0073-1fc6-4209-8dde-6ed5a5ec8303_2052x1020.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>ChatGPT was the starting gun for many AI founders. Prior to ChatGPT (and right before that Midjourney and Stable Diffusion) most people in tech were not paying close attention to the Transformer/Diffusion model revolution and dislocation we are now experiencing.</p><p>This means that people closest to the model and technology - ie AI researchers and infra engineers - were the first people to leave to start new companies based on this technology. The people farther away from the core model world - many product engineers, designers, and PMs, did not become aware of how important AI is until now.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png" width="1456" height="746" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/db91087b-c8ad-4432-849b-197e89295db3_2058x1054.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:746,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:191022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb91087b-c8ad-4432-849b-197e89295db3_2058x1054.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>ChatGPT launched ~15 months ago. If it takes 9-12 months to decide to quit your job, a few months to do it, and a few months to brainstorm an initial idea with a cofounder, we should start to see a wave of app builders showing up now / shortly. </p><ul><li><p><strong>B2B apps. What will be the important companies and markets in the emerging wave of B2B apps?</strong><span> </span><strong>Where will incumbents gain value versus startups? </strong><span>I have a long post on this coming shortly.</span></p></li><li><p><strong>Consumer. </strong><span>Arguably a number of the earliest AI products are consumer or “prosumer” - ie used in both personal and business use cases. Apps like ChatGPT, Midjourney, Perplexity and Pika are examples of this.</span><strong> That said, why are there so few consumer builders in the AI ecosystem?  Is it purely the time delay mentioned above? </strong><span>It seems like the 2007-2012 social product cohort has aged out. New blood is needed to build the next great wave of AI consumer.</span></p></li><li><p><strong>Agents.</strong><span> Lots and lots of things can happen with agents. </span><strong>What will be strong focused product areas versus startups looking for a use case?</strong></p></li></ul><p>This is one of the most exciting and fast-changing moments in technology in my lifetime. It will be fun to see what everyone builds. Looking forward to thoughts on the questions above.</p><p><span>Thanks to </span><a href="https://twitter.com/amasad" rel="">Amjad Masad</a><span> and </span><a href="https://twitter.com/vipulved" rel="">Vipul Prakash</a><span> for comments on a draft of this post. </span></p><p><strong>NOTES</strong></p><p>[1] Yes I occasionally read terms of use for fun.</p><p><strong>MY BOOK</strong><br><span>You can&nbsp;</span><a href="https://www.amazon.com/High-Growth-Handbook-Elad-Gil/dp/1732265100/" rel="">order the High Growth Handbook here</a><span>. Or&nbsp;</span><a href="https://growth.eladgil.com/" rel="">read it online for free</a><span>.</span></p><p><strong>OTHER POSTS</strong></p><p><strong>Firesides &amp; Podcasts</strong></p><ul><li><p><a href="https://blog.eladgil.com/p/video-and-transcript-apoorva-metha" rel="">Apoorva Metha: Starting Instacart</a></p></li><li><p><a href="https://blog.eladgil.com/p/fireside-chat-with-satya-nadella" rel="">Satya Nadella: Building Microsoft</a></p></li><li><p><a href="https://blog.eladgil.com/p/altimeters-brad-gerstner-on-macro" rel="">Brad Gerstner: Altimeter and Macro</a></p></li><li><p><a href="https://blog.eladgil.com/p/video-and-transcript-fireside-chat" rel="">Clem Delangue: Hugging Face, Open Source, AI</a></p></li><li><p><a href="https://blog.eladgil.com/p/transcript-and-video-fireside-w-dylan" rel="">Dylan Field: Figma, AI &amp; Design, Education</a></p></li><li><p><a href="https://blog.eladgil.com/p/fireside-chat-with-reid-hoffman-on" rel="">Reid Hoffman on AI, Big Tech, and Society</a></p></li><li><p><a href="https://www.youtube.com/watch?v=PJXYYnX9HqA" rel="">Sam Altman, CEO OpenAI</a></p></li><li><p><a href="https://www.youtube.com/watch?v=f9v5_7HjnAE&amp;t=10s" rel="">Emad Mostaque, Stability.AI</a></p></li><li><p><a href="https://podcasts.apple.com/us/podcast/no-priors-artificial-intelligence-machine-learning/id1668002688" rel="">NoPriors AI Podcast</a></p></li></ul><p><strong>Markets:</strong></p><ul><li><p><a href="https://blog.eladgil.com/p/unicorn-market-cap-2023-rise-of-ai" rel="">2023 Unicorn Market Cap &amp; Rise of AI</a></p></li><li><p><a href="https://blog.eladgil.com/p/ai-regulation" rel="">AI Regulation</a></p></li><li><p><a href="https://blog.eladgil.com/p/early-days-of-ai" rel="">Early Days of AI</a></p></li><li><p><a href="https://blog.eladgil.com/p/ai-safety-technology-vs-species-threats" rel="">AI Safety: Technology vs Species Threats</a></p></li><li><p><a href="https://blog.eladgil.com/p/startup-decoupling-and-reckoning" rel="">Startup Decoupling and Reckoning</a></p></li><li><p><a href="https://blog.eladgil.com/p/defensibility-and-competition" rel="">Defensibility and Competition</a></p></li><li><p><a href="https://blog.eladgil.com/p/ai-platforms-markets-and-open-source" rel="">AI Platforms, Markets, and Open Source</a></p></li><li><p><a href="https://blog.eladgil.com/p/changing-times-or-why-is-every-layoff" rel="">Changing times (or, why is every layoff 10-15%?)</a></p></li><li><p><a href="https://blog.eladgil.com/p/ai-startup-vs-incumbent-value" rel="">AI Startup Vs Incumbent Value</a></p></li><li><p><a href="http://blog.eladgil.com/2022/08/ai-revolution-transformers-and-large.html" rel="">AI Revolution - Transformers and Large Language Models&nbsp;</a></p></li><li><p><a href="http://blog.eladgil.com/2022/07/what-may-be-coming-to-startups-2022.html" rel="">Startup Markets Summer 2022</a></p></li><li><p><a href="http://blog.eladgil.com/2021/09/the-false-narrative-around-theranos.html" rel="">False Narrative Around Theranos</a></p></li><li><p><a href="http://blog.eladgil.com/2020/12/index-companies.html" rel="">Index Companies</a></p></li><li><p><a href="http://blog.eladgil.com/2020/10/silicon-valley-defense-tech.html" rel="">Defense Tech</a></p></li><li><p><a href="http://blog.eladgil.com/2020/09/collaborative-enterprise-at-last.html" rel="">Collaborative Enterprise</a></p></li><li><p><a href="http://blog.eladgil.com/2019/06/industry-towns-where-you-start-company.html" rel="">Industry Towns: Where you start a company matters</a></p></li><li><p><a href="http://blog.eladgil.com/2019/05/markets-are-10x-bigger-than-ever.html" rel="">Markets are 10X Bigger</a></p></li><li><p><a href="http://blog.eladgil.com/2019/01/interesting-markets-2019-edition.html" rel="">Hot Markets 2019</a></p></li><li><p><a href="http://blog.eladgil.com/2016/07/end-of-cycle.html" rel="">End of Cycle?</a></p></li><li><p><a href="http://blog.eladgil.com/2016/08/startups-in-machine-learning-ai.html" rel="">Machine Learning Startups</a></p></li><li><p><a href="http://blog.eladgil.com/2015/01/the-3-types-of-platform-companies.html" rel="">3 Types Of Platform Companies</a></p></li><li><p><a href="http://blog.eladgil.com/2014/12/defensibility-and-lock-in-uber-lyft.html" rel="">Defensibility and Lock-In: Uber and Lyft</a></p></li><li><p><a href="http://techcrunch.com/2014/01/19/uber-and-disruption/" rel="">Uber And Disruption</a></p></li><li><p><a href="http://blog.eladgil.com/2013/04/who-cares-if-its-been-tried-before.html" rel="">Who Cares If Its Been Tried Before?</a></p></li><li><p><a href="http://blog.eladgil.com/2012/10/the-road-to-5-billion-is-long-one.html" rel="">The Road To $5 Billion Is A Long One</a></p></li><li><p><a href="http://blog.eladgil.com/2012/10/how-to-win-as-second-mover.html" rel="">How To Win As Second Mover</a></p></li><li><p><a href="http://blog.eladgil.com/2012/09/enough-with-this-end-of-silicon-valley.html" rel="">End Of Silicon Valley</a></p></li><li><p><a href="http://blog.eladgil.com/2013/06/social-products.html" rel="">Social Products</a></p></li><li><p><a href="http://blog.eladgil.com/2015/01/hot-markets-for-2015.html" rel="">Hot Markets For 2015</a></p></li></ul><p><strong>Startup life</strong></p><ul><li><p><a href="https://blog.eladgil.com/p/capital-efficient-businesses" rel="">Capital Efficient Businesses</a></p></li><li><p><a href="https://blog.eladgil.com/p/startups-are-an-act-of-desperation" rel="">Startups Are An Act of Desperation</a></p></li><li><p><a href="https://blog.eladgil.com/p/back-to-office" rel="">Back To The Office</a></p></li><li><p><a href="http://blog.eladgil.com/2021/02/hiring-executives-bad-advice.html" rel="">Hiring Executives and Bad Advice</a></p></li><li><p><a href="http://blog.eladgil.com/2021/02/when-executives-break.html" rel="">When executives break</a></p></li><li><p><a href="http://blog.eladgil.com/2019/10/fear-of-sales.html" rel="">Fear of Sales</a></p></li></ul><ul><li><p><a href="http://blog.eladgil.com/2019/05/a-brief-guide-to-startup-pivots-4-types.html" rel="">A brief guide to startup pivots</a></p></li></ul><ul><li><p><a href="http://blog.eladgil.com/2011/05/4-ways-startups-fail.html" rel="">4 Ways Startups Fail</a></p></li></ul><ul><li><p><a href="http://founder%20investors%20&amp;%20scout%20programs/" rel="">Founder Investors and Scout Programs</a></p></li></ul><ul><li><p><a href="http://blog.eladgil.com/2018/07/meeting-etiquette.html" rel="">Better Meetings</a></p></li><li><p><a href="http://blog.eladgil.com/2019/12/magic-startup-moments.html" rel="">Magic Startup Moments</a></p></li><li><p><a href="http://blog.eladgil.com/2019/04/founder-investors-scout-programs.html" rel="">Founder Investors &amp; Scout Programs</a></p></li><li><p><a href="http://blog.eladgil.com/2020/10/jobs-wozniak-cook-build-sell-scale.html" rel="">Jobs, Wozniak, Cook</a></p></li></ul><p><strong>Co-Founders</strong></p><ul><li><p><a href="http://blog.eladgil.com/2012/02/how-to-choose-co-founder.html" rel="">How To Choose A Co-Founder</a></p></li><li><p><a href="http://blog.eladgil.com/2017/08/unequal-cofounders.html" rel="">Unequal Cofounders</a></p></li><li><p><a href="http://blog.eladgil.com/2013/01/how-to-fire-co-founder.html" rel="">How To Fire A Co-Founder</a></p></li><li><p><a href="http://blog.eladgil.com/2014/12/founders-should-divide-and-conquer.html" rel="">Founders Should Divide and Conquer</a></p></li></ul><p><strong>Raising Money</strong></p><ul><li><p><a href="http://blog.eladgil.com/2018/06/preemptive-rounds.html" rel="">Preemptive rounds</a></p></li><li><p><a href="http://blog.eladgil.com/2017/02/dont-ask-for-too-much-money.html" rel="">Don't Ask For Too Much Money</a></p></li><li><p><a href="http://blog.eladgil.com/2017/02/building-vc-relationships.html" rel="">Building VC Relationships</a></p></li><li><p><a href="http://blog.eladgil.com/2014/12/founders-should-divide-and-conquer.html" rel="">Founders Should Divide And Conquer</a></p></li><li><p><a href="http://blog.eladgil.com/2014/12/should-your-lead-vc-veto-other-investors.html" rel="">Lead VC Vetos</a></p></li><li><p><a href="http://blog.eladgil.com/2014/07/what-is-good-vc.html" rel="">What Is A Good VC?</a></p></li><li><p><a href="http://blog.eladgil.com/2012/11/how-to-choose-right-vc-partner-for-you.html" rel="">How To Choose The Right VC For You</a></p></li><li><p><a href="http://blog.eladgil.com/2012/07/signs-vc-is-just-not-that-into-you.html" rel="">Signs a VC Just Isn't That Into You</a></p></li><li><p><a href="http://blog.eladgil.com/2011/11/why-fewer-companies-are-successfully.html" rel="">Series A Crunch</a></p></li><li><p><a href="http://blog.eladgil.com/2011/03/questions-vcs-will-ask-you.html" rel="">Questions VCs Will Ask You</a></p></li><li><p><a href="http://blog.eladgil.com/2011/03/tactics-for-how-to-raise-vc-round-or.html" rel="">How To Raise A Successful VC Round</a></p></li><li><p><a href="http://blog.eladgil.com/2011/03/how-funding-rounds-differ-seed-series.html" rel="">Differences Between Funding Rounds: Series Seed, A, B, C...</a></p></li><li><p><a href="http://blog.eladgil.com/2010/12/financing-approaches-most-likely-to.html" rel="">Financing Approaches Most Likely To Kill Your Company</a></p></li><li><p><a href="http://blog.eladgil.com/2010/09/party-rounds-how-to-get-high-valuation.html" rel="">Party Rounds: How to Get A High Valuation For Your Seed Startup</a></p></li><li><p><a href="http://blog.eladgil.com/2010/08/20-questions-to-ask-yourself-before.html" rel="">20 Questions To Ask Yourself Before Raising Money</a></p></li><li><p><a href="http://blog.eladgil.com/2010/03/7-types-of-angel-investors-what-is.html" rel="">The 7 Types Of Angel Investors</a></p></li><li><p><a href="http://blog.eladgil.com/2012/09/fundraising-will-take-you-3-months.html" rel="">Fundraising Will Take You 3 Months</a></p></li><li><p><a href="http://blog.eladgil.com/2014/01/how-to-sell-secondary-stock.html" rel="">How To Sell Secondary Stock</a></p></li></ul></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GaussianObject: Just Taking Four Images to Get a High-Quality 3D Object (128 pts)]]></title>
            <link>https://gaussianobject.github.io/</link>
            <guid>39453524</guid>
            <pubDate>Wed, 21 Feb 2024 13:31:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gaussianobject.github.io/">https://gaussianobject.github.io/</a>, See on <a href="https://news.ycombinator.com/item?id=39453524">Hacker News</a></p>
<div id="readability-page-1" class="page">

<div>
          
          

          <p><span><sup>1</sup>Shanghai Jiao Tong University</span>
            <span><sup>2</sup>Huawei Inc.</span>
            <span><sup>3</sup>University of Toronto</span>
          </p>

          <p><span>* <i>Equal contribution.</i></span><br>
            <span>† <i>Project lead.</i></span><br>
          </p>

          
        </div>

<div id="single-comparison">
        <p>
            <strong>TL;DR:</strong> We present GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only <strong>4 input images</strong>.
          </p>
        <video poster="" id="teaser" autoplay="" muted="" loop="" playsinline="" height="100%">
          <source src="https://gaussianobject.github.io/static/videos/teaser.mp4" type="video/mp4">
        </video>
      </div>

<div>
        <h2>Abstract</h2>
        <p>
            Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting, that achieves high rendering quality with only <strong>4 input images</strong>. We first introduce techniques of visual hull and floater elimination which explicitly inject structure priors into the initial optimization process for helping build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. Our GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction results from <strong>only 4 views</strong> and significantly outperforming previous state-of-the-art methods.
          </p>
      </div>

<div>
        <h2>Video</h2>
        <p>
          <iframe src="https://www.youtube.com/embed/ozoI0tmW3r0" allowfullscreen=""></iframe>
        </p>
      </div>

<div>
        <h2>Pipeline</h2>
        <p>
          <img width="100%" src="https://gaussianobject.github.io/static/images/pipe.png" alt="overview">
        </p>
        <p>
            (a) We initialize 3D Gaussians by constructing a visual hull with camera parameters and masked images, optimizing them with the \(\mathcal{L}_{\text{gs}}\) and refining through floater elimination. (b) We use a novel `leave-one-out' strategy and add 3D noise to Gaussians to generate corrupted Gaussian renderings. These renderings, paired with their corresponding reference images, facilitate the training of the Gaussian repair model employing \(\mathcal{L}_{\text{tune}}\). (c) Once trained, the Gaussian repair model is frozen and used to correct views that need to be rectified. These views are identified through distance-aware sampling. The repaired images and reference images are used to further optimize 3D Gaussians with \(\mathcal{L}_{\text{rep}}\) and \(\mathcal{L}_{\text{gs}}\).
          </p>
      </div>

<div id="more-comparison">
        <h2>More Results</h2>
        
        <div>
          <p><span>
            <video poster="" id="more-comparison-video" autoplay="" muted="" loop="" playsinline="" src="https://gaussianobject.github.io/static/videos/comparison/mip360_bonsai_4_3DGS.mp4" onplay="resizeAndPlay(this)"></video>
            <img src="https://gaussianobject.github.io/static/images/comparison/mip360_bonsai_4.png" id="more-comparison-image" alt="input-images">
            <canvas height="0" id="more-comparison-canvas"></canvas>
          </span></p><p>Generated with only <strong>four input images</strong>.</p>
        </div>
        
      </div>

<div>
        <h2>Ablation</h2>
        <video poster="" id="evolution" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
          <source src="https://gaussianobject.github.io/static/videos/evolution.mp4" type="video/mp4">
        </video>
        <p>
            Evolution of the 3D objects with Gaussian splatting only, with structure priors and with Gaussian repair model injected.
          </p>
      </div>

<div id="BibTeX">
        <h2>Citation</h2>
        <pre><code>@article{yang2024gaussianobject,
  title   = {GaussianObject: Just Taking Four Images to Get A High-Quality
             3D Object with Gaussian Splatting},
  author  = {Chen Yang and Sikuang Li and Jiemin Fang and Ruofan Liang and
             Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian},
  journal = {arXiv preprint arXiv:2402.10259},
  year    = {2024}
}</code></pre>
      </div>






</div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Launch HN: Retell AI (YC W24) – Conversational Speech API for Your LLM (240 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39453402</link>
            <guid>39453402</guid>
            <pubDate>Wed, 21 Feb 2024 13:18:13 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39453402">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><td><table>
        <tbody><tr id="39453402">
      <td><span></span></td>      <td><center><a id="up_39453402" href="https://news.ycombinator.com/vote?id=39453402&amp;how=up&amp;goto=item%3Fid%3D39453402"></a></center></td><td><span><a href="https://news.ycombinator.com/item?id=39453402">Launch HN: Retell AI (YC W24) – Conversational Speech API for Your LLM</a></span></td></tr><tr><td colspan="2"></td><td><span>
          <span id="score_39453402">124 points</span> by <a href="https://news.ycombinator.com/user?id=yanyan_evie">yanyan_evie</a> <span title="2024-02-21T13:18:13"><a href="https://news.ycombinator.com/item?id=39453402">4 hours ago</a></span> <span id="unv_39453402"></span> | <a href="https://news.ycombinator.com/hide?id=39453402&amp;goto=item%3Fid%3D39453402">hide</a> | <a href="https://hn.algolia.com/?query=Launch%20HN%3A%20Retell%20AI%20(YC%20W24)%20%E2%80%93%20Conversational%20Speech%20API%20for%20Your%20LLM&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0">past</a> | <a href="https://news.ycombinator.com/fave?id=39453402&amp;auth=234d30b5c072aba1820bf4567672cc2f292a690a">favorite</a> | <a href="https://news.ycombinator.com/item?id=39453402">55&nbsp;comments</a>        </span>
              </td></tr>
    <tr></tr><tr><td colspan="2"></td><td><div><p>Hey HN, we're the co-founders of Retell AI (<a href="https://www.retellai.com/">https://www.retellai.com/</a>). We are building a conversational speech engine to help developers build natural-sounding voice AI. Our API abstracts away the complexities of AI voice conversations, so you can make your voice application the best at what it does. Here's a demo video: <a href="https://www.youtube.com/watch?v=0LT64_mgkro" rel="nofollow">https://www.youtube.com/watch?v=0LT64_mgkro</a>.</p><p>With the advent of LLMs and recent breakthroughs in speech synthesis, conversational voice AI has just gotten good enough to create really exciting use cases. However, developers often underestimate what's required to build a good and natural-sounding conversational voice AI. Many simply stitch together ASR (speech-to-text), an LLM, and TTS (text-to-speech), and expect to get a great experience. It turns out it's not that simple.</p><p>There's more going on in conversation than we consciously realize: things like knowing when to speak and when to listen, handling interruptions, 0-200 ms latency and backchanneling phrases (e.g., "yeah", "uh huh") to signal that they are listening. These are natural for humans, but hard for AI to get right. Developers spend hundreds of hours on the AI conversation experience but end up with poor experiences like 4-5s long latencies, inappropriate cutoffs, speaking over each other, etc.</p><p>So, we built Retell AI. We have followed the overall paradigm of having speech-to-text, LLM, and text-to-speech components, but have added additional conversation models in between to orchestrate the conversation while allowing maximum configurability for the developers in each step. You can think of our models as adding a “domain expert” layer for the dynamics of conversation itself.</p><p>Retell is designed for you to bring your own LLM into our pipeline. Currently, we can achieve 800ms end-to-end latency, handle interruptions, speech isolation, with tons of customization options (e.g., speaking rate, voice temperature, add ambient sound). We created a guest account for HN, so you can try our playground with a 10-min free trial without login: <a href="https://beta.retellai.com/dashboard/hn">https://beta.retellai.com/dashboard/hn</a> (Playground tutorial: <a href="https://docs.retellai.com/guide/dashboard">https://docs.retellai.com/guide/dashboard</a>). Our product is usage-based and the price is $0.1-0.17/min.</p><p>Our main product is a developer-facing API, but you can try it without writing code (e.g. create agents, connect to a phone number) via our dashboard. If you want to test it in production, feel free to also self-serve with our API documentation. One of our customers just launched, and you can view their demo: <a href="https://www.loom.com/share/64f09a53bf6d4b3799e5ebd08b23fec4?sid=77baea2b-c595-4ae0-92f9-e1c7fc38eccf" rel="nofollow">https://www.loom.com/share/64f09a53bf6d4b3799e5ebd08b23fec4?...</a></p><p>We are thrilled to see what our users are building with our API, and we’re excited to show our product to the community and look forward to your feedback!</p></div></td></tr>        <tr></tr><tr><td colspan="2"></td><td><form action="comment" method="post"></form></td></tr>  </tbody></table>
  </td></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Gemma: New Open Models (942 pts)]]></title>
            <link>https://blog.google/technology/developers/gemma-open-models/</link>
            <guid>39453271</guid>
            <pubDate>Wed, 21 Feb 2024 13:03:53 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.google/technology/developers/gemma-open-models/">https://blog.google/technology/developers/gemma-open-models/</a>, See on <a href="https://news.ycombinator.com/item?id=39453271">Hacker News</a></p>
<div id="readability-page-1" class="page"><article ng-init="drawerToggle = {'open': true}">

    
    





    

    
      

<div data-analytics-module="{
    &quot;module_name&quot;: &quot;Hero Menu&quot;,
    &quot;section_header&quot;: &quot;Gemma: Introducing new state\u002Dof\u002Dthe\u002Dart open models&quot;
  }">
  <div>
      
      
        <p>
          Gemma is built for responsible AI development from the same research and technology used to create Gemini models.
        </p>
      
    </div>
  <div>
  <p>Tris Warkentin</p>
  
    <p>
      Director, Google DeepMind
    </p>
  
  
</div>
</div>

    

    
      







<div>
    <figure>
      <div>
  <p><img srcset="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-600.format-webp.webp 600w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp 1200w, https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1600.format-webp.webp 1600w" sizes="(max-width: 599px) 100vw, (max-width: 1023px) 600px, 1024px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp" fetchpriority="high" alt="The word “Gemma” and a spark icon with blueprint styling appears in a blue gradient against a black background.">
  </p>
</div>

      
    </figure>
  </div>


    

    
    <div data-reading-time="true" data-component="uni-drop-cap|uni-tombstone">

            
              


<google-read-aloud-player data-analytics-module="{
        &quot;event&quot;: &quot;module_impression&quot;,
        &quot;module_name&quot;: &quot;ai_audio&quot;,
        &quot;section_header&quot;: &quot;Gemma: Introducing new state\u002Dof\u002Dthe\u002Dart open models&quot;
    }" data-date-modified="2024-02-21T13:47:02.283485+00:00" data-progress-bar-style="half-wave" data-api-key="AIzaSyBLT6VkYe-x7sWLZI2Ep26-fNkBKgND-Ac" data-article-style="style9" data-tracking-ids="G-HGNBTNCHCQ,G-6NKTLKV14N" data-voice-list="en.ioh-pngnat:Cyan,en.usb-pngnat:Lime" data-layout-style="style1" data-highlight-mode="word-over-paragraph" data-highlight-text-color="#000000" data-highlight-word-background="#8AB4F8" data-highlight-paragraph-background="#D2E3FC" data-background="linear-gradient(180deg, #F1F3F4 0%, #F8F9FA 100%)" data-foreground-color="#202124" data-font="600 16px Google Sans, sans-serif" data-box-shadow="0px 1px 3px 1px rgba(60, 64, 67, 0.15)">
</google-read-aloud-player>




            

            
            
<!--article text-->

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemma: Introducing new state\u002Dof\u002Dthe\u002Dart open models&quot;
         }"><p data-block-key="c7knx">At Google, we believe in <a href="https://ai.google/" rt-link-type="external">making AI helpful for everyone</a>. We have a long history of contributing innovations to the open community, such as with <a href="https://blog.research.google/2017/08/transformer-novel-neural-network.html" rt-link-type="external">Transformers</a>, <a href="https://www.tensorflow.org/" rt-link-type="external">TensorFlow</a>, <a href="https://blog.research.google/2018/11/open-sourcing-bert-state-of-art-pre.html" rt-link-type="external">BERT</a>, <a href="https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html" rt-link-type="external">T5</a>, <a href="https://github.com/google/jax" rt-link-type="external">JAX</a>, <a href="https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/" rt-link-type="external">AlphaFold</a>, and <a href="https://deepmind.google/discover/blog/competitive-programming-with-alphacode/" rt-link-type="external">AlphaCode</a>. Today, we’re excited to introduce a new generation of open models from Google to assist developers and researchers in building AI responsibly.</p><h2 data-block-key="cpl7a">Gemma open models</h2><p data-block-key="48b6k"><a href="https://ai.google.dev/gemma/?utm_source=keyword&amp;utm_medium=referral&amp;utm_campaign=gemma_cta&amp;utm_content=" rt-link-type="external">Gemma</a> is a family of lightweight, state-of-the-art <a href="https://opensource.googleblog.com/2024/02/building-open-models-responsibly-gemini-era.html" rt-link-type="external">open models</a> built from the same research and technology used to create the <a href="https://deepmind.google/technologies/gemini/#introduction" rt-link-type="external">Gemini</a> models. Developed by Google DeepMind and other teams across Google, Gemma is inspired by Gemini, and the name reflects the Latin <i>gemma</i>, meaning “precious stone.” Accompanying our model weights, we’re also releasing tools to support developer innovation, foster collaboration, and guide responsible use of Gemma models.</p><p data-block-key="ebmmu">Gemma is available worldwide, starting today. Here are the key details to know:</p><ul><li data-block-key="d3250">We’re releasing model weights in two sizes: <a href="https://www.kaggle.com/models/google/gemma" rt-link-type="external">Gemma 2B and Gemma 7B</a>. Each size is released with pre-trained and instruction-tuned variants.</li><li data-block-key="93fl7">A new <a href="https://ai.google.dev/responsible?utm_source=agd&amp;utm_medium=referral&amp;utm_campaign=explore-responsible&amp;utm_content" rt-link-type="external">Responsible Generative AI Toolkit</a> provides guidance and essential tools for creating safer AI applications with Gemma.</li><li data-block-key="fj09f">We’re providing toolchains for inference and supervised fine-tuning (SFT) across all major frameworks: JAX, PyTorch, and TensorFlow through native <a href="https://github.com/keras-team/keras" rt-link-type="external">Keras 3.0</a>.</li><li data-block-key="7ed0k">Ready-to-use <a href="http://ai.google.dev/gemma/docs/get_started" rt-link-type="external">Colab</a> and <a href="https://www.kaggle.com/models/google/gemma/code" rt-link-type="external">Kaggle notebooks</a>, alongside integration with popular tools such as <a href="http://huggingface.co/google" rt-link-type="external">Hugging Face</a>, <a href="https://github.com/google/maxtext" rt-link-type="external">MaxText</a>, <a href="https://github.com/NVIDIA/GenerativeAIExamples/tree/main/models/Gemma" rt-link-type="external">NVIDIA NeMo</a> and <a href="https://github.com/NVIDIA/TensorRT-LLM" rt-link-type="external">TensorRT-LLM</a>, make it easy to get started with Gemma.</li><li data-block-key="ei3as"><a href="https://www.kaggle.com/models/google/gemma" rt-link-type="external">Pre-trained and instruction-tuned Gemma models</a> can run on your laptop, workstation, or Google Cloud with easy deployment on <a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335" rt-link-type="external">Vertex AI</a> and <a href="https://cloud.google.com/kubernetes-engine/docs/integrations/ai-infra" rt-link-type="external">Google Kubernetes Engine</a> (GKE).</li><li data-block-key="3pmgh">Optimization across multiple AI hardware platforms ensures industry-leading performance, including <a href="https://cloud.google.com/nvidia" rt-link-type="external">NVIDIA GPUs</a> and <a href="https://cloud.google.com/tpu" rt-link-type="external">Google Cloud TPUs</a>.</li><li data-block-key="9up68"><a href="https://www.kaggle.com/models/google/gemma/license/consent" rt-link-type="external">Terms of use</a> permit responsible commercial usage and distribution for all organizations, regardless of size.</li></ul><h2 data-block-key="8rpl2">State-of-the-art performance at size</h2><p data-block-key="5d12o">Gemma models share technical and infrastructure components with <a href="https://deepmind.google/technologies/gemini/#introduction" rt-link-type="external">Gemini</a>, our largest and most capable AI model widely available today. This enables Gemma 2B and 7B to achieve best-in-class performance for their sizes compared to other open models. And Gemma models are capable of running directly on a developer laptop or desktop computer. Notably, Gemma surpasses significantly larger models on key benchmarks while adhering to our rigorous standards for safe and responsible outputs. See the <a href="https://goo.gle/GemmaReport" rt-link-type="external">technical report</a> for details on performance, dataset composition, and modeling methodologies.</p></div>
  

  
    







  
      <div data-analytics-module="{
          &quot;module_name&quot;: &quot;Inline Images&quot;,
          &quot;section_header&quot;: &quot;Gemma: Introducing new state\u002Dof\u002Dthe\u002Dart open models&quot;
        }">
  

  <p><img alt="A chart showing Gemma performance on common benchmarks, compared to Llama-2 7B and 13B" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Benchmark_chart_Updates_19.02_1.width-100.format-webp.webp" loading="lazy" data-loading="{
                &quot;mobile&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Benchmark_chart_Updates_19.02_1.width-500.format-webp.webp&quot;,
                &quot;desktop&quot;: &quot;https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Benchmark_chart_Updates_19.02_1.width-1000.format-webp.webp&quot;
              }">
        
      
    
    </p>
    
  
    </div>
  



  

  
    <div data-component="uni-article-paragraph" data-analytics-module="{
           &quot;module_name&quot;: &quot;Paragraph&quot;,
           &quot;section_header&quot;: &quot;Gemma: Introducing new state\u002Dof\u002Dthe\u002Dart open models&quot;
         }"><h2 data-block-key="c7knx">Responsible by design</h2><p data-block-key="4lgk">Gemma is designed with our <a href="https://ai.google.dev/responsible?utm_source=agd&amp;utm_medium=referral&amp;utm_campaign=explore-responsible&amp;utm_content" rt-link-type="external">AI Principles</a> at the forefront. As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets. Additionally, we used extensive fine-tuning and reinforcement learning from human feedback (RLHF) to align our instruction-tuned models with responsible behaviors. To understand and reduce the risk profile for Gemma models, we conducted robust evaluations including manual red-teaming, automated adversarial testing, and assessments of model capabilities for dangerous activities. These evaluations are outlined in our <a href="https://www.kaggle.com/models/google/gemma" rt-link-type="external">Model Card</a>.<a href="#footnote-1" id="footnote-source-1" data-ga4-analytics-superscript-click="" data-target="inline text"><sup>1</sup></a></p><p data-block-key="b9d8h">We’re also releasing a new <a href="https://ai.google.dev/responsible?utm_source=agd&amp;utm_medium=referral&amp;utm_campaign=explore-responsible&amp;utm_content" rt-link-type="external">Responsible Generative AI Toolkit</a> together with Gemma to help developers and researchers prioritize building safe and responsible AI applications. The toolkit includes:</p><ul><li data-block-key="cv45q"><b>Safety classification:</b> We provide a <a href="https://codelabs.developers.google.com/codelabs/responsible-ai/agile-classifiers" rt-link-type="external">novel methodology</a> for building robust safety classifiers with minimal examples.</li><li data-block-key="3n2vq"><b>Debugging:</b> A model <a href="https://codelabs.developers.google.com/codelabs/responsible-ai/lit-gemma" rt-link-type="external">debugging tool</a> helps you investigate Gemma's behavior and address potential issues.</li><li data-block-key="9t500"><b>Guidance:</b> You can access best practices for model builders based on Google’s experience in developing and deploying large language models.</li></ul><h2 data-block-key="2rvc1">Optimized across frameworks, tools and hardware</h2><p data-block-key="20an5">You can fine-tune Gemma models on your own data to adapt to specific application needs, such as summarization or retrieval-augmented generation (RAG). Gemma supports a wide variety of tools and systems:</p><ul><li data-block-key="f5o6v"><b>Multi-framework tools:</b> Bring your favorite framework, with reference implementations for inference and fine-tuning across multi-framework Keras 3.0, native PyTorch, JAX, and Hugging Face Transformers.</li><li data-block-key="fi8n8"><b>Cross-device compatibility:</b> Gemma models run across popular device types, including laptop, desktop, IoT, mobile and cloud, enabling broadly accessible AI capabilities.</li><li data-block-key="1c9nk"><b>Cutting-edge hardware platforms:</b> We’ve <a href="https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc" rt-link-type="external">partnered with NVIDIA to optimize Gemma for NVIDIA GPUs</a>, from data center to the cloud to local RTX AI PCs, ensuring industry-leading performance and integration with cutting-edge technology.</li><li data-block-key="e0epi"><b>Optimized for Google Cloud:</b> Vertex AI provides a broad MLOps toolset with a range of tuning options and one-click deployment using built-in inference optimizations. Advanced customization is available with fully-managed Vertex AI tools or with self-managed GKE, including deployment to cost-efficient infrastructure across GPU, TPU, and CPU from either platform.</li></ul><h2 data-block-key="etm40">Free credits for research and development</h2><p data-block-key="6f9uo">Gemma is built for the open community of developers and researchers powering AI innovation. You can start working with Gemma today using free access in Kaggle, a free tier for Colab notebooks, and $300 in credits for first-time Google Cloud users. Researchers can also apply for <a href="https://docs.google.com/forms/d/e/1FAIpQLSe0grG6mRFW6dNF3Rb1h_YvKqUp2GaXiglZBgA2Os5iTLWlcg/viewform" rt-link-type="external">Google Cloud credits</a> of up to $500,000 to accelerate their projects.</p><h2 data-block-key="c1db6">Getting started</h2><p data-block-key="8k2d8">You can explore more about Gemma and access quickstart guides on <a href="http://ai.google.dev/gemma" rt-link-type="external">ai.google.dev/gemma</a>.</p><p data-block-key="aj6c">As we continue to expand the Gemma model family, we look forward to introducing new variants for diverse applications. Stay tuned for events and opportunities in the coming weeks to connect, learn and build with Gemma.</p><p data-block-key="76mu1">We’re excited to see what you create!</p></div>
  


            
            

            
              




            
          </div>
  </article></div>]]></description>
        </item>
    </channel>
</rss>