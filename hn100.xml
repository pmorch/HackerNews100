<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 23 Jan 2026 00:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Why does SSH send 100 packets per keystroke? (238 pts)]]></title>
            <link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link>
            <guid>46723990</guid>
            <pubDate>Thu, 22 Jan 2026 19:27:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/">https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</a>, See on <a href="https://news.ycombinator.com/item?id=46723990">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><a href="https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/"></a><div><p>And why do I care?</p><p>Jan 22, 2026</p></div></div>
<p>Here are a few lines of summarized <code>tcpdump</code> output for an ssh session where I send a single keystroke:</p>
<pre><code><span>$ ./first_lines_of_pcap.sh single-key.pcap
</span><span>  <span>1</span>   <span>0</span>.000s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>2</span>   <span>0</span>.007s  SERVER-<span>&gt;</span>CLIENT  <span>564</span> bytes
</span><span>  <span>3</span>   <span>0</span>.015s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span>  <span>4</span>   <span>0</span>.015s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>5</span>   <span>0</span>.015s  SERVER-<span>&gt;</span>CLIENT   <span>36</span> bytes
</span><span>  <span>6</span>   <span>0</span>.026s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span>  <span>7</span>   <span>0</span>.036s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span><span>  <span>8</span>   <span>0</span>.036s  SERVER-<span>&gt;</span>CLIENT   <span>36</span> bytes
</span><span>  <span>9</span>   <span>0</span>.046s  CLIENT-<span>&gt;</span>SERVER    <span>0</span> bytes
</span><span> <span>10</span>   <span>0</span>.059s  CLIENT-<span>&gt;</span>SERVER   <span>36</span> bytes
</span></code></pre>
<p>I said a “few” because there are a <em>lot</em> of these lines.</p>
<pre><code><span>$ ./summarize_pcap.sh single-key.pcap
</span><span>Total packets: <span>270</span>
</span><span>
</span><span>  <span>36</span>-byte msgs:   <span>179</span> packets <span>(</span> <span>66.3</span>%<span>)</span>   <span>6444</span> bytes
</span><span>  Other data:       <span>1</span> packet  <span>(</span>  <span>0.4</span>%<span>)</span>    <span>564</span> bytes
</span><span>  TCP ACKs:        <span>90</span> packets <span>(</span> <span>33.3</span>%<span>)</span>
</span><span>
</span><span>  Data sent:      <span>6444</span> bytes <span>in</span> <span>36</span>-byte messages,  <span>564</span> bytes <span>in</span> other data
</span><span>  Ratio:          <span>11</span>.4x <span>more</span> data <span>in</span> <span>36</span>-byte messages than other data
</span><span>
</span><span>  Data packet rate: ~90 packets/second <span>(</span>avg <span>11.1</span> ms between data packets<span>)</span>
</span></code></pre>
<p>That is a lot of packets for one keypress. What’s going on here? Why do I care?</p>
<details><summary><svg xmlns="http://www.w3.org/2000/svg" width="20px" height="20px" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M1 12s4-8 11-8 11 8 11 8-4 8-11 8-11-8-11-8z"></path><circle cx="12" cy="12" r="3"></circle></svg> <!-- -->here's those scripts if you're curious</summary><pre><code><span><span># first_lines_of_pcap.sh</span>
</span><span>tshark <span>-r</span> <span>"<span>$1</span>"</span> <span>\</span>
</span><span>  <span>-T</span> fields <span>-e</span> frame.number <span>-e</span> frame.time_relative <span>-e</span> ip.src <span>-e</span> ip.dst <span>-e</span> tcp.len <span>|</span> <span>\</span>
</span><span>  <span>awk</span> <span>'NR&lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&gt;SERVER" : "SERVER-&gt;CLIENT");
</span></span><span><span>       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'</span>
</span></code></pre><pre><code><span><span># summarize_pcap.sh</span>
</span><span>tshark <span>-r</span> <span>"<span>$1</span>"</span> <span>-Y</span> <span>"frame.time_relative &lt;= 2.0"</span> <span>-T</span> fields <span>-e</span> frame.time_relative <span>-e</span> tcp.len <span>|</span> <span>awk</span> <span>'
</span></span><span><span>  {
</span></span><span><span>      count++
</span></span><span><span>      payload = $2
</span></span><span><span>
</span></span><span><span>      if (payload == 0) {
</span></span><span><span>          acks++
</span></span><span><span>      } else if (payload == 36) {
</span></span><span><span>          mystery++
</span></span><span><span>          if (NR &gt; 1 &amp;&amp; prev_data_time &gt; 0) {
</span></span><span><span>              delta = $1 - prev_data_time
</span></span><span><span>              sum_data_deltas += delta
</span></span><span><span>              data_intervals++
</span></span><span><span>          }
</span></span><span><span>          prev_data_time = $1
</span></span><span><span>      } else {
</span></span><span><span>          game_data++
</span></span><span><span>          game_bytes = payload
</span></span><span><span>          if (NR &gt; 1 &amp;&amp; prev_data_time &gt; 0) {
</span></span><span><span>              delta = $1 - prev_data_time
</span></span><span><span>              sum_data_deltas += delta
</span></span><span><span>              data_intervals++
</span></span><span><span>          }
</span></span><span><span>          prev_data_time = $1
</span></span><span><span>      }
</span></span><span><span>  }
</span></span><span><span>  END {
</span></span><span><span>      print "Total packets:", count
</span></span><span><span>      print ""
</span></span><span><span>      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
</span></span><span><span>      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
</span></span><span><span>      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
</span></span><span><span>      print ""
</span></span><span><span>      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
</span></span><span><span>      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
</span></span><span><span>      print ""
</span></span><span><span>      avg_ms = (sum_data_deltas / data_intervals) * 1000
</span></span><span><span>      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
</span></span><span><span>  }'</span>
</span></code></pre></details>
<!-- -->
<h2 id="toc:discovery">Discovery</h2>
<p>I am working on a high-performance game that runs over ssh. The TUI for the game is created in <a href="https://github.com/charmbracelet/bubbletea">bubbletea</a> <a>1</a> and sent over ssh via <a href="https://github.com/charmbracelet/wish">wish</a>.</p>
<div data-is-footnote="true"><p><span>1</span></p><p>I have also forked bubbletea to make it faster. Stay tuned!</p></div>
<p>The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.</p>
<p>So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s <a href="https://pkg.go.dev/net/http/pprof">outstanding profiling tools</a> to look at what’s going on.</p>
<p>Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.</p>
<p>At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.</p>
<p>But wait.</p>
<p><em>If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?</em></p>
<h2 id="toc:investigation">Investigation</h2>
<p>As part of debugging the test harness issue, I used <code>tcpdump</code> to log game traffic with and without the breaking change. Something like:</p>
<pre><code><span><span># The game runs on port 22</span>
</span><span><span>timeout</span> 30s tcpdump <span>-i</span> eth0 <span>'port 22'</span> <span>-w</span> with-breaking-change.pcap
</span><span><span># Revert change</span>
</span><span><span>timeout</span> 30s tcpdump <span>-i</span> eth0 <span>'port 22'</span> <span>-w</span> without-breaking-change.pcap
</span></code></pre>
<p>Our breaking change stopped us from rendering our game over ssh. So <code>with-breaking-change.pcap</code> contains packets that represent the <em>overhead</em> of each connection without actually rendering the game.</p>
<p>I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.</p>
<pre><code><span>Wanna take a look yourself? I put with-breaking-change.pcap in this directory
</span><span>
</span><span>--
</span><span>
</span><span>Wow! Here's what I found:
</span><span>
</span><span>Packet Size Distribution (413,703 total packets):
</span><span>274,907 packets (66%): Exactly 36 bytes
</span><span>138,778 packets (34%): 0 bytes (TCP ACKs)
</span><span>18 packets (&lt;0.1%): 72 bytes
</span></code></pre>
<p>Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.</p>
<p>This was baffling to me (and to Claude Code). We kicked around several ideas like:</p>
<ul>
<li>SSH flow control messages</li>
<li>PTY size polling or other status checks</li>
<li>Some quirk of bubbletea or wish</li>
</ul>
<p>One thing stood out - these exchanges were initiated by my <em>ssh client</em> (stock ssh installed on MacOS) - not by my server.</p>
<p>On a hunch, I took a <code>tcpdump</code> of a regular ssh session.</p>
<pre><code><span><span># on my mac, in one tab</span>
</span><span><span>sudo</span> tcpdump <span>-ien0</span> <span>'port 22'</span>
</span><span>
</span><span><span># on my mac, in another tab</span>
</span><span><span>ssh</span> <span>$some_vm_of_mine</span>
</span></code></pre>
<p>I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the <code>tcpdump</code> output.</p>
<p>I saw the exact same pattern! What in the world?</p>
<h2 id="toc:root-cause">Root cause</h2>
<p>Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.</p>
<p>Running <code>ssh -vvv</code> gave me a pretty good sense of what was going on:</p>
<pre><code><span>debug3: obfuscate_keystroke_timing: starting: interval ~20ms
</span><span>debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
</span><span>debug3: obfuscate_keystroke_timing: starting: interval ~20ms
</span><span>debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
</span></code></pre>
<p>That <code>20ms</code> is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.</p>
<p>In 2023, ssh added <a href="https://undeadly.org/cgi?action=article;sid=20230829051257">keystroke timing obfuscation</a>. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.</p>
<p>That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where <em>latency</em> is critical.</p>

<p>Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass <code>ObscureKeystrokeTiming=no</code> when starting up ssh sessions.</p>
<p>This worked great. CPU usage dropped dramatically and bots still received valid data.</p>
<p>But this is hardly a solution in the real world. I want <code>ssh mygame</code> to Just Work without asking users to pass options that they might not understand.</p>
<p>Claude Code originally didn’t have much faith that we could disable this functionality server-side.</p>
<div><p><img alt="Screenshot of a claude code transcript. Full text:
User
Is there any way to disable responding to the chaff server-side?

Assistant
Probably not. Here's why:

The chaff packets are legitimate SSH protocol messages (likely SSH_MSG_CHANNEL_WINDOW_ADJUST). The server must respond to maintain the SSH connection according to the protocol spec - if it ignored them, the connection would stall or break.
" loading="lazy" width="1678" height="602" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/probably-not.webp"></p><p>generated with simon wilson's excellent claude-code-transcripts tool</p></div>
<p>Fortunately, the <a href="https://undeadly.org/cgi?action=article&amp;sid=20230829051257">description</a> I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).</p>
<pre><code><span>Log message:
</span><span>Introduce a transport-level ping facility
</span><span>
</span><span>This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
</span><span>to implement a ping capability. These messages use numbers in the "local
</span><span>extensions" number space and are advertised using a "<a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="c3b3aaada483acb3a6adb0b0abeda0acae">[email&nbsp;protected]</a>"
</span><span>ext-info message with a string version number of "0".
</span></code></pre>
<p>The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="e7978e8980a78897828994948fc984888a">[email&nbsp;protected]</a></code> extension. What if we just…don’t advertise <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="24544d4a43644b54414a57574c0a474b49">[email&nbsp;protected]</a></code>?</p>
<p>I searched <a href="https://cs.opensource.google/go/x/crypto/+/master:ssh/">go’s ssh library</a> for <code><a href="https://eieio.games/cdn-cgi/l/email-protection" data-cfemail="ef9f868188af809f8a819c9c87c18c8082">[email&nbsp;protected]</a></code> and found <a href="https://cs.opensource.google/go/x/crypto/+/833695f0a57b3037385dc9c0073bc88773cae6f3">the commit where support was added</a>. The commit was tiny and seemed <em>very</em> easy to revert.</p>
<p>I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (<a href="https://go.dev/ref/mod#go-mod-file-replace">go’s replace directive</a> makes forking a library very easy).</p>
<p>Then I re-ran my test harness. The results were…very good:</p>
<pre><code><span>Total CPU  29.90%          -&gt; 11.64%
</span><span>Syscalls   3.10s           -&gt; 0.66s
</span><span>Crypto     1.6s            -&gt; 0.11s
</span><span>Bandwidth  ~6.5 Mbit/sec   -&gt; ~3 Mbit/sec
</span></code></pre>
<p>Claude was also pretty pumped:</p>
<div><p><img alt="Chat message from claude code. Full text:
HOLY COW! Look at that CPU usage:

Duration: 30.15s, Total samples = 3.51s (11.64%)
" loading="lazy" width="1658" height="290" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/claude-pumped.webp"></p><p>yes it's 1:30 am what of it</p></div>
<p>Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.</p>
<p>But this is a <em>huge</em> improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &gt;50% drop was unimaginable to me.</p>
<h2 id="toc:debugging-with-llms-was-fun">Debugging with LLMs was fun</h2>
<p>I’ve been thinking about <a href="https://eieio.games/blog/will-ai-pet-my-dog-for-me/">whether LLMs remove parts of the problem-solving process that I enjoy</a>. But I’ve gotta say, debugging this problem using Claude Code was super fun.</p>
<p>I am familiar enough with <code>tcpdump</code>, <code>tshark</code>, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.</p>
<p>There were still edge cases. At some point in my confusion I switched to ChatGPT  and it <em>very</em> confidently told me that my tcpdump output was normal ssh behavior:</p>
<div><p><img alt="ChatGPT message. Full text:
Yeah, that trace looks wild at first glance, but it’s mostly “normal” SSH/TCP behavior plus the fact that SSH is optimized for latency, not packet efficiency.

Let me unpack what you’re seeing and why it’s chewing CPU.

1. What those tcpdump lines actually are

From your snippet:
" loading="lazy" width="1350" height="464" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident.webp"></p><p>do all chatgpt messages have this tone and formatting now?</p></div>
<p>And then doubled down when I pushed back:</p>
<div><p><img alt="ChatGPT message. Full text:
Thought for 42s

Gotcha, that context helps a lot.

Short version:
What you’re seeing is almost certainly a ton of tiny writes being turned into a ton of tiny SSH records, not some special “per-keypress flow-control storm” in SSH itself.
" loading="lazy" width="1350" height="350" decoding="async" data-nimg="1" src="https://eieio.games/images/ssh-sends-100-packets-per-keystroke/chatgpt-confident2.webp"></p><p>no!!!</p></div>
<p>Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”</p>
<p>When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”</p>
<p>I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.</p>
<p>But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.</p>
<p>Besides. Being in the loop is fun. How else would I write this post?</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I was banned from Claude for scaffolding a Claude.md file? (291 pts)]]></title>
            <link>https://hugodaniel.com/posts/claude-code-banned-me/</link>
            <guid>46723384</guid>
            <pubDate>Thu, 22 Jan 2026 18:38:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://hugodaniel.com/posts/claude-code-banned-me/">https://hugodaniel.com/posts/claude-code-banned-me/</a>, See on <a href="https://news.ycombinator.com/item?id=46723384">Hacker News</a></p>
Couldn't get https://hugodaniel.com/posts/claude-code-banned-me/: Error: unable to verify the first certificate]]></description>
        </item>
        <item>
            <title><![CDATA[Macron says €300B in EU savings sent to the US every year will be invested in EU (163 pts)]]></title>
            <link>https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/</link>
            <guid>46722594</guid>
            <pubDate>Thu, 22 Jan 2026 17:42:51 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/">https://old.reddit.com/r/europe/comments/1qjtvtl/macron_says_300_billion_in_european_savings_flown/</a>, See on <a href="https://news.ycombinator.com/item?id=46722594">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>He played into the hands of the far right, led a violent crackdown on protests (dozens blinded or maimed, thousands injured), and consistently favors billionaires who have become incredibly wealthy since he became president (his wife is, incidentally, very close friends with the wife of the richest man in France). Our hospitals, schools, and universities are getting worse and worse. He has also betrayed his environmental promises (as soon as farmers protest, he silences them by dismantling environmental regulations). Overall, he has betrayed his initial, rather social-democratic, commitments. He is significantly "normalizing" the far right. One of our recent interior ministers is clearly a Trump supporter, by the way. Oh, and he sold many of our companies to the US (like Alstom). Overall, he still favors the rich and retirees (he's very close to the rich, and retirees vote a lot).</p>

<p>That said, we're rather pleased with what he's doing in the face of Trump. But we haven't forgotten the broken promises, nor his very ambiguous behavior towards the far right in our country. Basically, his game is to boost the far right, then find himself facing them in the elections so we're forced to vote for his party to "block" them.</p>

<p>That's why the French are angry.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[CSS Optical Illusions (125 pts)]]></title>
            <link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link>
            <guid>46722570</guid>
            <pubDate>Thu, 22 Jan 2026 17:41:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://alvaromontoro.com/blog/68091/css-optical-illusions">https://alvaromontoro.com/blog/68091/css-optical-illusions</a>, See on <a href="https://news.ycombinator.com/item?id=46722570">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <p>You can find a collection with <a href="https://codepen.io/collection/GpWqKk" target="_blank" rel="noopener noreferrer">all the optical illusions in this article (and more!) on CodePen</a>. You can move your mouse over many of the demos below to reveal the effect or stop the animations.</p>

<h2 id="poggerndorff-illusions">
  1&nbsp;-&nbsp;Poggendorff Illusions
</h2>

<p>The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.</p>

<p>A simple version of this effect can be seen in the following demo. I used the&nbsp;<code>::before</code> and&nbsp;<code>::after</code> pseudo-elements to create the diagonal line and the vertical bar, respectively.</p>



<p>The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:</p>



<p>This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the <code>body</code>, although I could have used&nbsp;<code>:root</code> instead.</p>

<p>Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top&nbsp;-&nbsp;but they do (mouse over to see it).</p>



<h2 id="induced-gradients">
  2&nbsp;-&nbsp;Induced Gradients
</h2>

<p>The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.</p>

<p>Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.</p>



<p>Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.</p>



<h2 id="cornsweet-illusion">
  3&nbsp;-&nbsp;Cornsweet Illusion
</h2>

<p>The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.</p>

<p>For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.</p>



<h2 id="whites-illusion">
  4&nbsp;-&nbsp;White's&nbsp;Illusion
</h2>

<p>Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.</p>



<p>I coded this demo using <code>mix-blend-mode</code> so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.</p>

<p>This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:</p>



<h2 id="wertheimer-koffka-ring">
  5&nbsp;-&nbsp;Wertheimer-Koffka Ring
</h2>

<p>The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.</p>



<h2 id="adelsons-illusion">
  6&nbsp;-&nbsp;Adelson's Illusion
</h2>

<p>You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles&nbsp;-&nbsp;one seemingly light and one seemingly dark&nbsp;-&nbsp;turn out to be the same color.</p>

<p>This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).</p>



<h2 id="asahi-illusion-of-brightness">
  7&nbsp;-&nbsp;Asahi illusion of Brightness
</h2>

<p>The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.</p>



<h2 id="color-spheres">
  8&nbsp;-&nbsp;Color&nbsp;Spheres
</h2>

<p>This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.</p>



<h2 id="colors-from-contour">
  9&nbsp;-&nbsp;Colors from&nbsp;Contour
</h2>

<p>In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.</p>



<h2 id="curvature-blindness">
  10&nbsp;-&nbsp;Curvature Blindness
</h2>

<p>One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.</p>

<p>The CSS code for the wavy lines is adapted from a <a href="https://css-tricks.com/how-to-create-wavy-shapes-patterns-in-css/" target="_blank" rel="noopener noreferrer">Temani Afif snippet on CSS-Tricks</a> and his <a href="https://css-generators.com/wavy-shapes/" target="_blank" rel="noopener noreferrer">wavy shape generator</a>.</p>



<h2 id="cafe-wall-illusion">
  11&nbsp;-&nbsp;Cafe&nbsp;Wall
</h2>

<p>This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.</p>



<h2 id="penrose-triangle">
  12&nbsp;-&nbsp;Penrose&nbsp;Triangle
</h2>

<p>This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.</p>



<h2 id="ebbinghaus-illusion">
  13&nbsp;-&nbsp;Ebbinghaus Illusion
</h2>

<p>Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.</p>



<p>I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:</p>



<h2 id="kanizsa-square">
  14&nbsp;-&nbsp;Kanizsa&nbsp;Square
</h2>

<p>When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.</p>



<h2 id="ehrensteins-illusion">
  15&nbsp;-&nbsp;Ehrenstein's Illusion
</h2>

<p>There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.</p>



<h2 id="neon-color-spreading-illusion">
  16&nbsp;-&nbsp;Neon-Color-Spreading Illusion
</h2>

<p>This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.</p>

<p>I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.</p>



<h2 id="hering-and-wundt-illusions">
  17&nbsp;-&nbsp;Hering and Wundt Illusions
</h2>

<p>Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.</p>
<p>In the Hering illusion, the red lines appear to curve outward, even though they are straight.</p>



<p>The <em>opposite</em> effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).</p>



<h2 id="ponzo-illusion">
  18&nbsp;-&nbsp;Ponzo&nbsp;Illusion
</h2>

<p>Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.</p>



<h2 id="t-illusion">
  19&nbsp;- T Illusion
</h2>

<p>This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.</p>



<h2 id="mullerlyer-illusion">
  20&nbsp;-&nbsp;Müller-Lyer Illusion
</h2>

<p>A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.</p><p>
From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the&nbsp;<code>::before</code> and&nbsp;<code>::after</code>.</p>



<h2 id="tilted-table-illusion">
  21&nbsp;-&nbsp;Tilted Table&nbsp;Illusion
</h2>

<p>It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.</p><p>
This illusion works better on larger screens. The effect is diminished when you can see the whole picture.</p>



<h2 id="parallel-lines">
  22&nbsp;-&nbsp;Parallel&nbsp;Lines
</h2>

<p>This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.</p><p>
I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.</p>



<p>Here is the original version I created. The effect is also visible there:</p>




<hr>

<p>Good news! There are more optical illusions below&nbsp;-&nbsp;but first, a warning.</p>

<p>
  ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.
</p>

<p>(Leaving some blank space in case you do not want to continue.)</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p>
<h2 id="expanding-hole">
  23&nbsp;-&nbsp;Expanding Hole
</h2>

<p>This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding&nbsp;-&nbsp;especially when you are not looking at it directly, creating the sensation of falling into a pit.</p>

<p>From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.</p>



<h2 id="rotating-snakes">
  24&nbsp;-&nbsp;Rotating&nbsp;Snakes
</h2>

<p>This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.</p>



<h2 id="appearing-dots">
  25&nbsp;-&nbsp;Appearing Dots
</h2>

<p>Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.</p>



<h2 id="disappearing-dots">
  26&nbsp;-&nbsp;Disappearing Dots
</h2>

<p>This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.</p>

<p>If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.</p>



<h2 id="ouchi-illusion">
  27&nbsp;-&nbsp;Ouchi&nbsp;Illusion
</h2>

<p>This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.</p>

<p>If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.</p>



<h2 id="orthogonal-dotted-lines-sway">
  28&nbsp;-&nbsp;Orthogonal Dotted Lines&nbsp;Sway
</h2>

<p>When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.</p>



<h2 id="enigma">
  29&nbsp;-&nbsp;Enigma
</h2>

<p>This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.</p>



<h2 id="waves">
  30&nbsp;-&nbsp;Waves
</h2>

<p>This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.</p>

<p>If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.</p>




<hr>

<p>Good news! There are more optical illusions below&nbsp;-&nbsp;but first, another warning.</p>

<p>
  ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.
</p>

<p>(Leaving some blank space in case you do not want to continue.)</p>

<p>.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p><p>
.</p>
<h2 id="animated-ebbinghaus-illusion">
  31&nbsp;-&nbsp;Animated Ebbinghaus Illusion
</h2>

<p>Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size&nbsp;-&nbsp;when it definitely is not.</p>



<h2 id="psychokinematic-tower">
  32&nbsp;-&nbsp;Psychokinematic Tower
</h2>

<p>This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.</p><p>
Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.</p>



<h2 id="color-fan">
  33&nbsp;-&nbsp;Color&nbsp;Fan
</h2>

<p>This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.</p>

<p>If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.</p>



<h2 id="reverse-spoke-illusion">
  34&nbsp;-&nbsp;Reverse Spoke&nbsp;Illusion
</h2>

<p>This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.</p>



<h2 id="motion-binding">
  35&nbsp;-&nbsp;Motion&nbsp;Binding
</h2>

<p>What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.</p>

<p>In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.</p>



<h2 id="mainzlinez-illusion">
  36&nbsp;-&nbsp;Mainz-Linez Illusion
</h2>

<p>Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.</p>

<p>The CSS code for the wavy lines is adapted from a <a href="https://css-tricks.com/how-to-create-wavy-shapes-patterns-in-css/" target="_blank" rel="noopener noreferrer">Temani Afif snippet on CSS-Tricks</a> and his <a href="https://css-generators.com/wavy-shapes/" target="_blank" rel="noopener noreferrer">wavy shape generator</a>.</p>



<h2 id="waddling-colors">
  37&nbsp;-&nbsp;Waddling&nbsp;Colors
</h2>

<p>It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.</p>



<p>The illusion also works when the "feet" move in circles, as shown in this alternative version:</p>



<h2 id="dottedline-motion">
  38&nbsp;-&nbsp;Dotted-Line Motion
</h2>

<p>Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.</p>



<h2 id="contrast-asynchrony">
  39&nbsp;-&nbsp;Contrast Asynchrony
</h2>

<p>These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.</p>

<p>Mouse over the demo to remove the background and the illusion disappears.</p>



<h2 id="breathing-square">
  40&nbsp;-&nbsp;Breathing Square
</h2>

<p>This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.</p>

<p>Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.</p>



<h2 id="troxler-fading">
  41&nbsp;-&nbsp;Troxler&nbsp;Fading
</h2>

<p>This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.</p>

<p>I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.</p>



<p>Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.</p>



<h2 id="pinnabrelstaff-illusion">
  42&nbsp;-&nbsp;Pinna-Brelstaff Illusion
</h2>

<p>This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.</p><p>
From a CSS perspective, I coded the pattern using conic gradients, and applied it to the&nbsp;<code>::before</code> and&nbsp;<code>::after</code> pseudo-elements. I then flipped one upside down and clipped it.</p>



<h2 id="palisade">
  43&nbsp;-&nbsp;Palisade
</h2>

<p>The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.</p>



<h2 id="alternative-motion">
  44&nbsp;-&nbsp;Alternative Motion
</h2>

<p>This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.</p>

<p>Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.</p>



<h2 id="motion-inversion">
  45&nbsp;-&nbsp;Motion Inversion
</h2>

<p>These two illustrations are identical&nbsp;-&nbsp;same shapes, same animation. The only difference is the CSS timing function.</p>

<p>The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.</p>




<hr>

<p>Most of the inspiration for these optical illusions came from two excellent resources:</p>

<ul>
<li>"<a href="https://www.livescience.com/health/mind/32-optical-illusions-and-why-they-trick-your-brain" target="_blank" rel="noopener noreferrer">35 optical illusions and why they trick your brain</a>" by Patrick Pester.</li>
<li>"<a href="https://michaelbach.de/ot/" target="_blank" rel="noopener noreferrer">154 Visual Phenomena &amp; Optical Illusions</a>" with explanations by Michael Bach</li>
</ul>

<hr>

<p>You can also find this article on:</p>

<ul>
  <li><a href="https://alvaromontoro.medium.com/css-optical-illusions-visual-illusions-built-with-pure-css-and-html-bde6f8916b04?postPublishedType=initial">Medium</a></li>
  <li><a href="https://dev.to/alvaromontoro/css-optical-illusions-50-visual-illusions-built-with-pure-css-and-html-13o9">DEV</a> (open)</li>
</ul>

<p>(You can leave comments on those platforms and I will reply there).</p>
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: isometric.nyc – giant isometric pixel art map of NYC (587 pts)]]></title>
            <link>https://cannoneyed.com/isometric-nyc/</link>
            <guid>46721802</guid>
            <pubDate>Thu, 22 Jan 2026 16:52:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://cannoneyed.com/isometric-nyc/">https://cannoneyed.com/isometric-nyc/</a>, See on <a href="https://news.ycombinator.com/item?id=46721802">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[It looks like the status/need-triage label was removed (270 pts)]]></title>
            <link>https://github.com/google-gemini/gemini-cli/issues/16728</link>
            <guid>46721179</guid>
            <pubDate>Thu, 22 Jan 2026 16:10:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/google-gemini/gemini-cli/issues/16728">https://github.com/google-gemini/gemini-cli/issues/16728</a>, See on <a href="https://news.ycombinator.com/item?id=46721179">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-testid="issue-body-viewer" data-team-hovercards-enabled="true" data-turbolinks="false" id="issue-body-viewer"><h3 dir="auto">What would you like to be added?</h3>
<p dir="auto">Adds native recognition for JetBrains IDE as a supported IDE environment.</p>
<h3 dir="auto">Why is this needed?</h3>
<p dir="auto">Currently, Gemini CLI restricts IDE integration features to environments where TERM_PROGRAM is vscode (or other hardcoded values). This forces 3rd-party integrations like <a href="https://github.com/SoLoHiC/jetbrains-ide-companion">jetbrains-ide-companion</a> to mock VS Code by spoofing environment variables to enable core features, otherwise it could not be discovered by Gemini CLI.</p>
<p dir="auto">For some reason, the process detection is not working properly on windows/linux (, reported by users here <a href="https://plugins.jetbrains.com/plugin/29336-gemini-cli-companion/reviews" rel="nofollow">JetBrains Plugin Review</a> and here <a data-error-text="Failed to load title" data-id="3446875726" data-permission-text="Title is private" data-url="https://github.com/google-gemini/gemini-cli/issues/9273" data-hovercard-type="issue" data-hovercard-url="/google-gemini/gemini-cli/issues/9273/hovercard" href="https://github.com/google-gemini/gemini-cli/issues/9273">#9273</a> , and a few other bug report email i've received), which making this native IDE detection logic a MUST do for gemini-cli discover and connect to IDE via environmental variables instead of port info file.</p>
<p dir="auto">This PR adds JetBrains IDE Series to the IDE_DEFINITIONS and updates the detection logic to recognize TERMINAL_EMULATOR=JetBrains-JediTerm as a first-class supported environment.</p>
<h3 dir="auto">Additional context</h3>
<p dir="auto">Inspired by <a data-error-text="Failed to load title" data-id="3790058763" data-permission-text="Title is private" data-url="https://github.com/google-gemini/gemini-cli/issues/16083" data-hovercard-type="pull_request" data-hovercard-url="/google-gemini/gemini-cli/pull/16083/hovercard" href="https://github.com/google-gemini/gemini-cli/pull/16083">#16083</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers (664 pts)]]></title>
            <link>https://gptzero.me/news/neurips/</link>
            <guid>46720395</guid>
            <pubDate>Thu, 22 Jan 2026 15:20:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://gptzero.me/news/neurips/">https://gptzero.me/news/neurips/</a>, See on <a href="https://news.ycombinator.com/item?id=46720395">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<table xmlns="http://www.w3.org/1999/xhtml" dir="ltr" data-sheets-root="1" data-sheets-baot="1"><colgroup><col width="200"><col width="80"><col width="280"><col width="220"></colgroup><tbody><tr><td><p dir="ltr"><span>Published Paper</span></p></td><td><p dir="ltr"><span>GPTZero Scan</span></p></td><td><p dir="ltr"><span>Example of Verified Hallucination</span></p></td><td><p dir="ltr"><span>Comment</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FxCy8TvQHO"><span>SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/02dc3e04-2bd6-4ce2-ba22-017b5c925b03/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/3d32225d-7494-4e82-afd6-58a5d399b3af/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>John Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.</span></p></td><td><p dir="ltr"><span>Article with a matching title exists</span><a href="https://aclanthology.org/2024.acl-long.371/"><span> </span><span>here</span></a><span>. Authors are obviously fabricated. arXiv ID links to a different</span><a href="https://arxiv.org/abs/2401.00001"><span> </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=WQb0YrFl3H"><span>Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/3043470a-75f9-4803-ac7d-5a8ae833899e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/cd109d6a-2c6d-4b38-ae39-b0da48a86c43/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL</span><a href="https://ieeexplore.ieee.org/document/307123"><span> </span><span>https://ieeexplore.ieee.org/document/307123</span></a></p></td><td><p dir="ltr"><a href="https://ieeexplore.ieee.org/xpl/tocresult.jsp?isnumber=9629429&amp;punumber=5962385&amp;sortType=vol-only-seq&amp;pageNumber=2"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>. URL and DOI are fake.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=WQb0YrFl3H"><span>Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/3043470a-75f9-4803-ac7d-5a8ae833899e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/cd109d6a-2c6d-4b38-ae39-b0da48a86c43/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/CVPR2022"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>. URL and DOI are fake.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EyOtIOmMUh"><span>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/aae95ba0-192d-441b-8c93-4342f29cf3aa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/0fcb90db-35bc-4d22-9b0d-1fd5af82355b/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.</span></p></td><td><p dir="ltr"><span>No title or author match. Potentially referring to this</span><a href="https://arxiv.org/abs/2406.03008"><span> </span><span>article</span></a><span>, but year is off (2024)</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EyOtIOmMUh"><span>SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/aae95ba0-192d-441b-8c93-4342f29cf3aa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/0fcb90db-35bc-4d22-9b0d-1fd5af82355b/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.</span></p></td><td><p dir="ltr"><span>No title or author match. Potentially referring to this</span><a href="https://proceedings.mlr.press/v155/banerjee21a/banerjee21a.pdf"><span> </span><span>article</span></a><span>, but year is totally off (2020).</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Nuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>A. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Sheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match and arXiv ID is incomplete.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Asma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/events/emnlp-2021/#2021emnlp-main"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.</span></p></td><td><p dir="ltr"><span>No exact author or title match, although this title is</span><a href="https://aclanthology.org/2023.findings-emnlp.906.pdf?utm_source=consensus"><span> close.</span></a><span> No match in the</span><a href="https://aclanthology.org/events/findings-2021/"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Alex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2107.13586"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Lei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.</span></p></td><td><p dir="ltr"><span>No title or author match. No match in</span><a href="https://aclanthology.org/volumes/D14-1/"><span> </span><span>publication</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>X. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.</span></p></td><td><p dir="ltr"><span>No title or author match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Franz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.</span></p></td><td><p dir="ltr"><a href="https://proceedings.iclr.cc/paper_files/paper/2025/file/b94d8b035e2183e47afef9e2f299ba47-Paper-Conference.pdf"><span>No title or author match. Some similarity to this </span><span>article</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Maria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/volumes/2021.findings-emnlp/"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://aclanthology.org/events/emnlp-2021/#2021emnlp-main"><span> </span><span>publication</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IiEtQPGVyV"><span>Efficient semantic uncertainty quantification in language models via diversity-steered sampling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/53b0d78b-e26e-421a-939a-4ae423631f60/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/70843e88-0958-4116-9721-03e73ccb18f9/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Srinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2205.05407"><span>No author or title match. ArXiv ID leads to a different </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=0ZnXGzLcOg"><span>Privacy Reasoning in Ambiguous Contexts</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fe51b7d6-607a-470d-adf3-0a5e4770d5ad/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7e96a90a-fb2f-4303-b873-c8b1f7f66532/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Zayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.</span></p></td><td><p dir="ltr"><span>Two authors are omitted and one (Kyle Richardson) is added. This</span><a href="https://proceedings.iclr.cc/paper_files/paper/2024/file/3f8c7eb848ffec848f3ed2b7ca44915d-Paper-Conference.pdf"><span> paper</span></a><span> was published at ICLR 2024.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=15GCs8DoSm"><span>Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/a80b41a7-a74b-4c7d-a8dd-6a02829a17fa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1397d83d-3075-4574-a1cf-27df285a5f64/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.</span></p></td><td><p dir="ltr"><span>The authors match this</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S037877962030482X?via%3Dihub"><span> paper</span></a><span>, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=15GCs8DoSm"><span>Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/a80b41a7-a74b-4c7d-a8dd-6a02829a17fa/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1397d83d-3075-4574-a1cf-27df285a5f64/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.</span></p></td><td><p dir="ltr"><span>No title or author match. Journal and other identifiers match this</span><a href="https://pubs.aip.org/aip/cha/article-abstract/30/12/123135/1074648/A-logistic-model-and-predictions-for-the-spread-of?redirectedFrom=fulltext"><span> article.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=18GBPdnuXs"><span>Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4ea64476-d22e-41ad-aaed-a7e8516c4ec0/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/d8546150-5096-4b85-8029-870e962feef8/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Francisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. ArXiv ID matches this</span><a href="https://arxiv.org/abs/2310.18664"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1amnhVRQ3l"><span>Grounded Reinforcement Learning for Visual Reasoning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9af6453c-26d2-4072-b47c-1cbaac232b46/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/5b1f5bdc-3aa1-46ab-bbfb-f932c5be6588/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. This</span><a href="https://proceedings.neurips.cc/paper/2021/hash/5bd53571b97884635d13910db49626bc-Abstract.html"><span> </span><span>paper</span></a><span> has a similar title and matches publication.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1mILyDyPDf"><span>MTRec: Learning to Align with User Preferences via Mental Reward Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/b570e1ce-4b4d-47f6-bc0a-eb5ad2516ef4/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f7020dcf-c4a8-455c-9649-b242881573a7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Diederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.</span></p></td><td><p dir="ltr"><span>Title matches this</span><a href="https://www.ijcai.org/proceedings/2017/0239.pdf"><span> paper</span></a><span>. Authors, date, and publisher match this</span><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=yyIoQu4AAAAJ&amp;cstart=20&amp;pagesize=80&amp;sortby=pubdate&amp;citation_for_view=yyIoQu4AAAAJ:_tF6a-HnqWAC"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1wmP48quNb"><span>Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4c559ec8-804f-4071-94ad-d08ed30a9281/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f0e35c1a-543e-4aca-8aaa-a890af1c3bff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.</span></p></td><td><p dir="ltr"><span>Authors, publisher and date match this</span><a href="https://aclanthology.org/2020.findings-emnlp.182/"><span> paper.</span></a><span> Title and page numbers don't match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=1wmP48quNb"><span>Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/4c559ec8-804f-4071-94ad-d08ed30a9281/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f0e35c1a-543e-4aca-8aaa-a890af1c3bff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.proceedings.com/content/055/055939webtoc.pdf"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=2DAvXR77xh"><span>Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c046c36d-ea1f-4727-9d04-41069e4532f8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/8ed0a8fe-a6de-473d-af79-5f9684530ab9/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.</span></p></td><td><p dir="ltr"><span>No title or author match. Proceedings from this conference are split into</span><a href="https://conferences.miccai.org/2022/en/PROCEEDINGS.html"><span> volumes</span></a><span>, but the citation doesn't have a volume number.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4FUdUFvvmp"><span>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f741e52-f068-4bd5-b86a-6be3d5a867ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b385a81c-7f8a-4bec-ac84-faf84a1fa18d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in the cited</span><a href="https://dblp.org/db/journals/tkde/tkde33.html"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4FUdUFvvmp"><span>PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f741e52-f068-4bd5-b86a-6be3d5a867ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b385a81c-7f8a-4bec-ac84-faf84a1fa18d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.</span></p></td><td><p dir="ltr"><span>No author or title match. DOI doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4JLZsmWBJf"><span>LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/98128158-bd97-40a1-a60c-0594f31a2e78/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f12d9556-c5fb-4c0b-8d0c-f4926077e7ea/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Bernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3</span></p></td><td><p dir="ltr"><span>Loosely matches this</span><a href="https://arxiv.org/abs/2308.04079"><span> article,</span></a><span> but only one author and part of the title actually match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4JLZsmWBJf"><span>LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/98128158-bd97-40a1-a60c-0594f31a2e78/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f12d9556-c5fb-4c0b-8d0c-f4926077e7ea/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Punchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6</span></p></td><td><p dir="ltr"><span>No clear match. Two authors and the subject match this</span><a href="https://dl.acm.org/doi/10.1145/1198555.1198694"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4TUpqyDJbz"><span>When and How Unlabeled Data Provably Improve In-Context Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/37cb8b5f-1aed-42fc-84eb-a56db01f1dcb/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7cfc4dcd-dfea-4b65-b42e-c4c07b8f5c3d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Ashish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://papers.nips.cc/paper/2020"><span> publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=4TUpqyDJbz"><span>When and How Unlabeled Data Provably Improve In-Context Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/37cb8b5f-1aed-42fc-84eb-a56db01f1dcb/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7cfc4dcd-dfea-4b65-b42e-c4c07b8f5c3d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Chuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.</span></p></td><td><p dir="ltr"><span>This paper</span><a href="https://arxiv.org/abs/2305.01747v2"><span> exists,</span></a><span> but all the authors are fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=6uwV6ytamU"><span>DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/5241c346-4190-4e44-813c-65073c84946b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c1855921-693a-4379-b5d3-4c0d95e356ff/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, "Clip-score: A robust scoring metric for text-to-image generation," arXiv preprint arXiv:2201.07519, 2022.</span></p></td><td><p dir="ltr"><span>No clear author or title matches. Title loosely matches this</span><a href="https://arxiv.org/abs/2104.08718"><span> article</span></a><span>. ArXiv ID leads</span><a href="https://arxiv.org/abs/2201.07519"><span> here</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=7VN0iICXZj"><span>Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/af669229-94a7-4343-abf7-9ce3af831e02/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/3db22d99-5daa-44fd-8baa-9abbeb29606d/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.</span></p></td><td><p dir="ltr"><span>No title match. Authors match this</span><a href="https://arxiv.org/abs/2209.08005"><span> paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=7WPi6VbtH0"><span>GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c15712b0-a37a-40a1-add6-d4a8f98443ff/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/bf8fcb05-9de3-4606-9956-624d8572e467/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)</span></p></td><td><p dir="ltr"><span>No exact title or author match. Loose title match with this</span><a href="https://www.cambridge.org/core/journals/psychological-medicine/article/abs/neural-predictors-and-effects-of-cognitive-behavioral-therapy-for-depression-the-role-of-emotional-reactivity-and-regulation/B8C3EDBA14E972910900CEDC460033D4"><span> article</span></a><span>. Doesn't exist in the</span><a href="https://www.clinicalkey.com/#!/browse/toc/1-s2.0-S0165032714X00157/null/journalIssue"><span> journal volume</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ap8OIosN8p"><span>Robust Label Proportions Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61d21fa6-24be-4cb6-9bde-5c80d5165916/share"><span>Scan</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa5f2722-47c4-46e0-852e-432a8b37b6e9/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.</span></p></td><td><p dir="ltr"><span>Title matches this</span><a href="https://arxiv.org/abs/2301.10921"><span> paper</span></a><span>. No match in NeurIPS</span><a href="https://proceedings.neurips.cc/paper_files/paper/2023"><span> volume 36</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=AmZ7uHDJiR"><span>NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/63f2339b-a3a4-4135-b056-e125dd9301bf/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b597ad2c-edd3-47e5-9dbd-473dce8fd862/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2022/1H1gVMlkl32"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=AmZ7uHDJiR"><span>NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/63f2339b-a3a4-4135-b056-e125dd9301bf/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b597ad2c-edd3-47e5-9dbd-473dce8fd862/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Y. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://csdl-downloads.ieeecomputer.org/proceedings/iccv/2023/0718/00/071800z005.pdf?Expires=1767122670&amp;Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jc2RsLWRvd25sb2Fkcy5pZWVlY29tcHV0ZXIub3JnL3Byb2NlZWRpbmdzL2ljY3YvMjAyMy8wNzE4LzAwLzA3MTgwMHowMDUucGRmIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzY3MTIyNjcwfX19XX0_&amp;Signature=QX3vcUyMPs0JlLXc5-PvUoUQA908LDFqmUSVBX6FGn~ZKDJiKwNTMZLWR2RM546c91C-4nBK-A50KWf7mu0ewk7x~M4n60WALqUvJnl-gL~0lHMysGZav01wv2Z4nzqDT4HBd-25SQt7pC0eGQEQvKYnfYpsbU58VJE4t32zO6JCV2HItuM~wVL53gYjASY0uO50l0QFDZWhZHhjb2R0V0ks1dTZEHnjH1dDoVq7wBb38YRP8u6jASw4SjF3r5zdkmUePjJmUWIVjkDtzpMeAcCA8hsJZHk5Nnwj90SJkRdnSvpeV1357U~KC9AKe56U~7ov7umxQNQM7bia-skMgw__&amp;Key-Pair-Id=K12PMWTCQBDMDT"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BND9CutZf6"><span>Geometric Imbalance in Semi-Supervised Node Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ea4059e3-64e1-4f58-a594-0f47c455fe30/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/a7f949e6-b871-43f7-9d34-46a562598254/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.</span></p></td><td><p dir="ltr"><span>No title or author match. This paper has a similar</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320325003310"><span> title,</span></a><span> but there's no match in the ICLR 2022</span><a href="https://iclr.cc/virtual/2022/papers.html?search=&amp;filter=title"><span> database</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BND9CutZf6"><span>Geometric Imbalance in Semi-Supervised Node Classification</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ea4059e3-64e1-4f58-a594-0f47c455fe30/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/a7f949e6-b871-43f7-9d34-46a562598254/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Ming Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist</span><a href="https://aclanthology.org/volumes/2021.acl-long/"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BpSGN4pErp"><span>NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/12ed22af-ea9c-4272-80c8-397c50a7c3e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/826759cb-e770-42d1-baa8-ee293799d45e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.</span></p></td><td><p dir="ltr"><span>Title and publisher match this</span><a href="https://www.nature.com/articles/s41561-020-0582-5#citeas"><span> article</span></a><span>. Issue, page numbers, and year match this</span><a href="https://www.nature.com/articles/s41561-023-01371-4#citeas"><span> article</span></a><span>. DOI is fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=BpSGN4pErp"><span>NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/12ed22af-ea9c-4272-80c8-397c50a7c3e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/826759cb-e770-42d1-baa8-ee293799d45e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.</span></p></td><td><p dir="ltr"><span>Authors have frequently published together on the "</span><a href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=author%3AVeretennikov+author%3APardoux&amp;btnG="><span>poisson equation</span></a><span>", but this title doesn't match any of their publications. Doesn't exist in</span><a href="https://link.springer.com/journal/10958/volumes-and-issues/111-3"><span> publication volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=CH76rSKWZr"><span>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6da6e9df-3687-476e-a55f-cc16c272616b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/20d02652-f460-4564-9a4f-826f0c9f14ee/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2021/1yeHGyRsuys"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=CH76rSKWZr"><span>Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6da6e9df-3687-476e-a55f-cc16c272616b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/20d02652-f460-4564-9a4f-826f0c9f14ee/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/iccv/2021/1BmEezmpGrm"><span> publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=EMa1ih7Wdt"><span>Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural Networks</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/0f8cd2db-21a2-4dd8-8bf3-d227f4009f29/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/420438d8-dad5-4733-8efd-c7b74e17d66a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>M. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022</span></p></td><td><p dir="ltr"><span>No title or author match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FfccSikDfZ"><span>SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9fc2825a-e8dc-4cbb-934c-b2aed237084a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c51b0bd9-d03f-49d8-b916-b547609815e7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.</span></p></td><td><p dir="ltr"><span>The authors match this</span><a href="https://epubs.siam.org/doi/10.1137/23M161286X"><span> article</span></a><span> and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the</span><a href="https://quantum-journal.org/volumes/6/"><span> 2022 Quantum volume</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FfccSikDfZ"><span>SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/9fc2825a-e8dc-4cbb-934c-b2aed237084a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/c51b0bd9-d03f-49d8-b916-b547609815e7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.sciencedirect.com/journal/journal-of-computational-physics/vol/480/suppl/C"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=FzfYoUp8F1"><span>Learning World Models for Interactive Video Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/052a28f8-81e1-4e85-81c8-433d06a506ae/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b52114de-dfe5-424a-92bf-b9e08d4e46ac/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Patrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.</span></p></td><td><p dir="ltr"><span>Authors match this</span><a href="https://arxiv.org/abs/2012.09841"><span> article</span></a><span>. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2303.07332"><span> article.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Lele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://www.ecva.net/papers.php"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://proceedings.neurips.cc/paper/2022"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Siyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in</span><a href="https://papers.nips.cc/paper_files/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvpr/2023/1PONOShHStG"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Raphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.</span></p></td><td><p dir="ltr"><span>Authors match this</span><a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf"><span> </span><span>article.</span></a><span> Title matches this</span><a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf"><span> </span><span>article</span></a><span>. No match in</span><a href="https://icml.cc/virtual/2021/papers.html?search=Adaptive+tokens"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=GPTI9GNAYH"><span>Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c1c42a77-2646-46d2-adec-a476c090df28/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/43100616-900c-4d09-b652-0ae59fbb2221/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Xin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Not_All_Tokens_Are_Equal_Human-Centric_Visual_Analysis_via_Token_CVPR_2022_paper.pdf"><span> </span><span>article</span></a><span> title is similar. No match in</span><a href="https://proceedings.neurips.cc/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=HdY8CCHife"><span>A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/14695743-9825-4aef-8e88-c8f54e101b9a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1db86c77-b6f6-4253-8f57-aa319bddff2b/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.</span></p></td><td><p dir="ltr"><span>No author or title match. ArXiv ID leads to a different</span><a href="https://arxiv.org/abs/2206.09267"><span> paper</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=HdY8CCHife"><span>A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/14695743-9825-4aef-8e88-c8f54e101b9a/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/1db86c77-b6f6-4253-8f57-aa319bddff2b/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>K. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.</span></p></td><td><p dir="ltr"><span>This</span><a href="https://dl.acm.org/doi/abs/10.5555/3524938.3525723"><span> paper</span></a><span> is a match, but all authors but the first (K. A. Sankararaman) are fabricated.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=I64ZLbUP6u"><span>MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d9683c9b-7ffc-4c40-b0b8-df3d72ee84e9/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/ed653d2d-71c6-4967-9306-d2c3c9bb5632/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Why Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .</span></p></td><td><p dir="ltr"><span>No author given and title appears to be garbled. Publisher, issue, year, and pages match this</span><a href="https://www.sciencedirect.com/journal/procedia-iutam/vol/13/suppl/C"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Pierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.</span></p></td><td><p dir="ltr"><span>No title or author match. ArXiv ID matches a different</span><a href="https://arxiv.org/abs/2303.08603"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Z Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.</span></p></td><td><p dir="ltr"><span>No author or title match. No match in journal</span><a href="https://www.tandfonline.com/toc/raec20/52/55?nav=tocList"><span> volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>W L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.</span></p></td><td><p dir="ltr"><a href="https://www.pm-research.com/content/iijderiv/9/1"><span>No author or title match. No match in </span><span>journal volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Petter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.</span></p></td><td><p dir="ltr"><a href="https://www.pm-research.com/content/iijtrade"><span>No author or title match. There is no volume 14 of this </span><span>journal.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IJGEtuVqwf"><span>Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fbf50b6a-faa1-4e52-ae0d-0efb52cb3400/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52b94988-efd5-4412-81e1-a6cc32010909/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Kyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417420310265"><span>No author or title match. Publisher ID matches this </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Moonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Publisher ID matches this</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0167278921001639"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Bart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.</span></p></td><td><p dir="ltr"><span>No title match, although Schutter and Hoogendorn have written or coauthored several related papers (</span><a href="https://scispace.com/pdf/model-based-control-of-intelligent-traffic-networks-5o7t1ux396.pdf"><span>example</span></a><span> and</span><a href="https://ieeexplore.ieee.org/document/8283509"><span> example</span></a><span>). Journal volume/issue matches an unrelated</span><a href="https://link.springer.com/article/10.1007/s00521-006-0080-8"><span> article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.</span></p></td><td><p dir="ltr"><a href="https://www.annualreviews.org/content/journals/conmatphys/14/1"><span>No author or title match. No match in </span><span>journal volume</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=II0T40Q785"><span>FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2e2340f-61cd-40b9-86e2-d2d7323beec3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/95868fb0-5d7a-4b65-ab5c-4d06e2e29dec/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Alejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.science.org/toc/sciadv/7/46"><span>No title or author match. Doesn't exist in </span><span>journal volume/issue</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=IIgVYnadfR"><span>BNMusic: Blending Environmental Noises into Personalized Music</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fd47718b-20a1-493f-9876-dc623513a506/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/4158f8bf-0292-41c8-b920-c2699933114f/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.</span></p></td><td><p dir="ltr"><span>No title or author match. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/cvprw/2021/1wzs0vrjyWQ"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=JSbVO7dNYE"><span>Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/96641483-8df9-49f4-ac68-888fa22eb819/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52bf51d1-e538-486c-83ae-60fa507d1103/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.</span></p></td><td><p dir="ltr"><span>ArXiv ID leads to this</span><a href="https://arxiv.org/abs/2502.14458"><span> article</span></a><span> with a similar title and one matching author.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=JSbVO7dNYE"><span>Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/96641483-8df9-49f4-ac68-888fa22eb819/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/52bf51d1-e538-486c-83ae-60fa507d1103/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.</span></p></td><td><p dir="ltr"><span>Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this</span><a href="https://arxiv.org/abs/2406.07887"><span> paper</span></a><span>. However, the title is only a partial match.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=KQTp6ljvlo"><span>Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2f6208a4-66b9-45ea-9396-a04409df4948/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/d41f986d-a44f-4aaa-9ea2-c1d46aed6fe5/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Junyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://arxiv.org/pdf/2205.07246"><span> paper title</span></a><span> is very close, but it was published by ICLR 2023 not</span><a href="https://papers.nips.cc/paper_files/paper/2021"><span> NeurIPS 2021</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=LP4Q7tPMbs"><span>NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/6977dbef-7a2b-4143-a4be-b69a426e2b56/share"><span>Scan</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7277a269-f450-45f5-8cd7-75eaf6f485e2/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match, although this</span><a href="https://ieeexplore.ieee.org/document/10988823"><span> title</span></a><span> is close. No match in</span><a href="https://papers.nips.cc/paper_files/paper/2023"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=LsmUgStXby"><span>AI-Generated Video Detection via Perceptual Straightening</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ccc2460e-0ef3-47b7-9d99-71e8df46ab6b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/5e1821da-8518-413c-804d-030ed6824fcd/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Jiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_ICCV_2023_paper.html"><span>No author or title match. A paper with a similar title appears in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Lz5BUjArK4"><span>Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/5627ac20-2221-4ca0-8857-4dea140b98e8/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/636f7851-a09e-4fd9-90e3-35c8602a4f8d/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.</span></p></td><td><p dir="ltr"><span>No title match, but authors have published together previously (</span><a href="https://papers.nips.cc/paper_files/paper/2012/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf"><span>example</span></a><span>). No match in</span><a href="https://www.sciencedirect.com/journal/neural-networks/vol/47/suppl/C"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ToNRHqX6xq"><span>MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/73c0dd26-0e0d-4b9c-a31d-6d9554120362/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/57179143-9c9b-462c-86a4-bd5041d88594/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Anh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.</span></p></td><td><p dir="ltr"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9639884"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=V1FlwrsseI"><span>ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/f2948a0f-8972-4269-b680-4ab68e14f2ec/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/11c6525d-e02d-4dd3-86a0-dfa3b544697e/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.</span></p></td><td><p dir="ltr"><span>No author match. Title matches another hallucinated citation in this</span><a href="https://arxiv.org/html/2508.05244v1"><span> </span><span>paper</span></a><span>. Doesn't exist in</span><a href="https://aclanthology.org/events/emnlp-2018/#d18-1"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=VN5bMTfSZS"><span>OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61a6a3d3-6c3b-4663-934a-40f69f112b50/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7362670b-7428-468f-882c-a6f7de6d90da/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Yikai Zhang et al. "Text-to-Image Diffusion Models with Customized Guidance". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.</span></p></td><td><p dir="ltr"><a href="https://www.computer.org/csdl/proceedings/iccv/2023/1TJc6RNOu8U"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=VN5bMTfSZS"><span>OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/61a6a3d3-6c3b-4663-934a-40f69f112b50/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/7362670b-7428-468f-882c-a6f7de6d90da/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Author Song and AnotherAuthor Zhang. "Consistency in Diffusion Models: Improving Noise Embeddings". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL:</span><a href="https://arxiv.org/abs/2304.08787."><span> </span><span>https://arxiv.org/abs/2304.08787.</span></a></p></td><td><p dir="ltr"><span>No author or title match. This</span><a href="https://openreview.net/forum?id=59nCKifDtm"><span> </span><span>paper</span></a><span> has a similar title. ArXiv ID leads to unrelated</span><a href="https://arxiv.org/abs/2304.08787"><span> </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=W8xcKoJcrl"><span>Strategic Costs of Perceived Bias in Fair Selection</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/b00f841e-0e1d-4ae4-9a48-82b2891e6600/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/feb8886c-6917-4006-82e5-551cc7d0df58/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Claudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.</span></p></td><td><p dir="ltr"><span>Author is a famous</span><a href="https://www.britannica.com/biography/Claudia-Goldin"><span> </span><span>economist</span></a><span>, but the title doesn't match any of her works. Journal and locators match this unrelated</span><a href="https://www.aeaweb.org/articles?id=10.1257/aer.104.5.348"><span> </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=XxR70zr9Sf"><span>Linear Transformers Implicitly Discover Unified Numerical Algorithms</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d349a8d0-809b-42fb-a5e6-ee91f37fbcd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b138bbcb-4f6a-4fc8-b85f-d09b1490fac7/share"><span>AI</span></a></p></td><td><p dir="ltr"><a href="https://distill.pub/2022/circuits/"><span>Olah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). </span><span>https://distill.pub/2022/circuits/.</span></a></p></td><td><p dir="ltr"><span>Most authors match this</span><a href="https://transformer-circuits.pub/2021/framework/index.html"><span> </span><span>paper</span></a><span>, but the title, publisher, and year are different. Doesn't exist in</span><a href="https://distill.pub/"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=XxR70zr9Sf"><span>Linear Transformers Implicitly Discover Unified Numerical Algorithms</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/d349a8d0-809b-42fb-a5e6-ee91f37fbcd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/b138bbcb-4f6a-4fc8-b85f-d09b1490fac7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Nanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.</span></p></td><td><p dir="ltr"><a href="https://scholar.google.com/citations?hl=en&amp;user=GLnX3MkAAAAJ&amp;view_op=list_works&amp;sortby=pubdate"><span>No title match. Author may be </span><span>Neel Nanda</span></a><span>, who wrote several similar articles in 2023.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=YGIbwfNWot"><span>A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/41566db2-9a1a-44b4-bec4-c4dc7107cc17/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ddb0781-ed8d-4377-a396-08521c082562/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/journal/procedia-computer-science/vol/55/suppl/C?page=2"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=YGIbwfNWot"><span>A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/41566db2-9a1a-44b4-bec4-c4dc7107cc17/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ddb0781-ed8d-4377-a396-08521c082562/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.</span></p></td><td><p dir="ltr"><a href="https://openaccess.thecvf.com/CVPR2021"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ynwl0V1YH0"><span>Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/114aad0a-479c-4b46-a94c-8beeffe28d2b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/edef15b3-6712-42c7-9aae-4a9a26ce5f45/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.</span></p></td><td><p dir="ltr"><span>Authors mostly match this</span><a href="https://papers.nips.cc/paper_files/paper/2016/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html"><span> </span><span>paper</span></a><span>. Title matches this</span><a href="https://arxiv.org/abs/1806.08342"><span> </span><span>paper.</span></a><span> ArXiv ID matches a third</span><a href="https://arxiv.org/abs/1612.01462"><span> </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Ynwl0V1YH0"><span>Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/114aad0a-479c-4b46-a94c-8beeffe28d2b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/edef15b3-6712-42c7-9aae-4a9a26ce5f45/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.</span></p></td><td><p dir="ltr"><span>No author match. Title matches this</span><a href="https://arxiv.org/abs/1710.09282"><span> </span><span>paper.</span></a><span> Doesn't exist in</span><a href="https://dl.acm.org/toc/csur/2022/54/7"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ZR2mdBrhJX"><span>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ff9ad094-e9be-468b-a618-fab14c568c21/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/684135a5-9a1e-4744-9c3a-cb8b0905a3dd/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2310.08573"><span>No title match. ArXiv ID leads to unrelated </span><span>paper.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=ZR2mdBrhJX"><span>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ff9ad094-e9be-468b-a618-fab14c568c21/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/684135a5-9a1e-4744-9c3a-cb8b0905a3dd/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.</span></p></td><td><p dir="ltr"><a href="https://www.arxiv.org/abs/2402.17552"><span>No author or title match. ArXiv ID leads to unrelated </span><span>article</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Zb3QO7HLIj"><span>FLAME: Fast Long-context Adaptive Memory for Event-based Vision</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/1cbbae27-5545-40d2-8a1d-33baa8b71c4e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa75cdb6-986a-43e1-9db6-7025ccdda5a6/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Zhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.</span></p></td><td><p dir="ltr"><a href="https://dblp.org/db/conf/icassp/index.html"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=Zb3QO7HLIj"><span>FLAME: Fast Long-context Adaptive Memory for Event-based Vision</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/1cbbae27-5545-40d2-8a1d-33baa8b71c4e/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/aa75cdb6-986a-43e1-9db6-7025ccdda5a6/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Daniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.</span></p></td><td><p dir="ltr"><span>No author match, but this</span><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.pdf"><span> </span><span>paper</span></a><span> has a similar title. Doesn't exist in</span><a href="https://www.computer.org/csdl/proceedings/iccvw/2021/1yNhksNMpkQ"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=emM7U3WKMO"><span>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/71876ca9-d9c9-4b73-a910-cbb5a61b722b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ce64b84-9e77-4734-8a05-a0d4cb00de66/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.</span></p></td><td><p dir="ltr"><span>No author or title match. Sener and Savarese have</span><a href="https://arxiv.org/abs/1708.00489"><span> </span><span>published</span></a><span> </span><span>together previously. Doesn't exist in</span><a href="https://proceedings.neurips.cc/paper/2021"><span> </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=emM7U3WKMO"><span>Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/71876ca9-d9c9-4b73-a910-cbb5a61b722b/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/6ce64b84-9e77-4734-8a05-a0d4cb00de66/share"><span>AI</span></a><span>**</span></p></td><td><p dir="ltr"><span>Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.</span></p></td><td><p dir="ltr"><span>No author match. Title matches this</span><a href="https://proceedings.mlr.press/v119/wang20k/wang20k.pdf"><span> </span><span>paper.</span></a><span> Doesn't exist in</span><a href="https://iclr.cc/virtual/2022/papers.html"><span> publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=gNiT81iag0"><span>TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2c627437-707b-4434-aa97-0d13a90eabd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/27d5bf8f-c17e-456f-b9c2-acc4bdb7aea6/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.</span></p></td><td><p dir="ltr"><a href="https://dl.acm.org/doi/10.5555/3618408.3619203"><span>Title, publisher, and date match this </span><span>paper</span></a><span>, but all authors except one surname (Leviathan) are different.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=gNiT81iag0"><span>TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/2c627437-707b-4434-aa97-0d13a90eabd3/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/27d5bf8f-c17e-456f-b9c2-acc4bdb7aea6/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.</span></p></td><td><p dir="ltr"><a href="https://papers.nips.cc/paper_files/paper/2024"><span>No author or title match. Doesn't exist in </span><span>publication.</span></a></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=oBikm5Rshc"><span>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ae7bf744-cbf7-4ef2-9ee1-3db00bcd5062/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/30a7d9af-5cf1-4251-acbf-0717f0432d6a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>M. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.</span></p></td><td><p dir="ltr"><span>No author or title match. Publication doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=oBikm5Rshc"><span>Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/ae7bf744-cbf7-4ef2-9ee1-3db00bcd5062/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/30a7d9af-5cf1-4251-acbf-0717f0432d6a/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>J. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.</span></p></td><td><p dir="ltr"><span>No author or title match. Publication doesn't exist.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=pGRjDetCDM"><span>JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c05dbbba-a778-4561-a553-fb18827f44ee/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/defdd83e-45cb-427d-95d7-c8fbaa757555/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.</span></p></td><td><p dir="ltr"><a href="https://www.nature.com/nmeth/articles?type=article&amp;year=2023"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=pGRjDetCDM"><span>JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/c05dbbba-a778-4561-a553-fb18827f44ee/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/defdd83e-45cb-427d-95d7-c8fbaa757555/share"><span>AI</span></a><span>*</span></p></td><td><p dir="ltr"><span>Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.</span></p></td><td><p dir="ltr"><span>No author or title match. Doesn't exist in publication.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=uih8cWS3JF"><span>Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/270a15e9-9a36-4b4a-8acb-ad91a8209bf7/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3bef20d-ea0a-4510-86e4-b9701136bee7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>François Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.</span></p></td><td><p dir="ltr"><a href="https://dblp.org/db/journals/pe/pe49.html"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=uih8cWS3JF"><span>Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/270a15e9-9a36-4b4a-8acb-ad91a8209bf7/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3bef20d-ea0a-4510-86e4-b9701136bee7/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Saravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.</span></p></td><td><p dir="ltr"><a href="https://www.sciencedirect.com/journal/journal-of-business-research/vol/124/suppl/C"><span>No author or title match. Doesn't exist in </span><span>publication</span></a><span>.</span></p></td></tr><tr><td><p dir="ltr"><a href="https://openreview.net/forum?id=vKyiv67VWa"><span>AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing</span></a></p></td><td><p dir="ltr"><a href="https://app.gptzero.me/documents/fb7a6411-f38b-48d9-8eab-7f32e6d006ab/share"><span>Sources</span></a></p><p dir="ltr"><a href="https://app.gptzero.me/documents/f3216beb-368a-433c-ac2f-9dd2badddf22/share"><span>AI</span></a></p></td><td><p dir="ltr"><span>Mehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.</span></p></td><td><p dir="ltr"><a href="https://arxiv.org/abs/2210.02870"><span>No author or title match. ArXiv ID leads to unrelated </span><span>article.</span></a></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<hr><div data-layout="minimal">
                    
                        <p><span>Is there a specific report or published article you think we should check for hallucinations?</span></p>
                    
                    
                        <p><a href="https://docs.google.com/forms/d/e/1FAIpQLSdEGNCoZZWTWrddNHP8ZoUiqay9Qs1ndXPv2wpP1d3LnB7JoQ/viewform?usp=publish-editor">
                            Submit Here
                        </a>
                        
                    </p></div><h2 id="defining-hallucinated-citations">Defining Hallucinated Citations</h2><p>Given the high stakes for both authors and publishers, GPTZero's Hallucination Check is engineered to be accurate, transparent, and cautious. It uses our AI agent, trained in-house, to flag any citations in a document that can’t be found online. These flagged citations are not automatically hallucinations — many archival documents or unpublished works can’t be matched to an online source — but they indicate which sources require further human scrutiny. As always, we recommend that a human confirm that flagged citation is an AI-generated fake instead of the result of a more conventional error.</p><p>We define a vibe citation as a citation that likely resulted from the use of generative AI. Vibe citing results in errors common to LLM generations, but rare in human-written text, such as:</p><ol><li>Combining or paraphrasing the titles, author(s), and/or locators from one or more real sources</li><li>Fabricating the author(s), title, URL/DOI, and/or container (ex. publisher, journal, conference) of a source</li><li>Modifying the author(s) or title of a source by extrapolating a first name from an initial, dropping and/or adding authors, or paraphrasing the title.</li></ol><p>Our definition excludes obvious spelling mistakes, dead URLs, missing locators, and other errors that are plausibly human. The following table shows the difference between a real citation, a flawed citation, and a hallucinated citation according to our methodology. The differences are highlighted in red.</p>
<!--kg-card-begin: html-->
<table><colgroup><col width="204"><col width="244"><col width="176"></colgroup><tbody><tr><td><p dir="ltr"><span>Real Citation</span></p></td><td><p dir="ltr"><span>Flawed Citation</span></p></td><td><p dir="ltr"><span>Hallucinated Citation</span></p></td></tr><tr><td><p dir="ltr"><span>Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521:436-444, 2015.</span></p></td><td><p dir="ltr"><span>Y.</span><span> LeCun, </span><span>Y. </span><span>Bengio, and </span><span>Geoff</span><span> Hinton. </span><span>Deep leaning</span><span>. nature, 521</span><span>(7553)</span><span>:436-444, 2015.</span></p></td><td><p dir="ltr"><span>Samuel LeCun Jackson</span><span>. Deep learning. </span><span>Science &amp;</span><span> Nature</span><span>: </span><span>23-45</span><span>, 20</span><span>21</span><span>.</span></p></td></tr><tr><td><p dir="ltr"><span>A. Yang, B. Zhang, B. Hui, B. Gao, B. Yu, C. Li, D. Liu, J. Tu, J. Zhou, J. Lin, et al. Qwen2.5–math technical report: Toward mathematical expert model via self-improvement. arXiv:2409.12122, 2024.</span></p></td><td><p dir="ltr"><span>A. Yang,&nbsp; </span><span>(missing author)</span><span>, B. Hui, B. Gao, B. Yu, C. Li</span><span>/</span><span>, D. Liu, J. Tu, J. Zhou, J. Lin, et al. </span><span>Qwen 2. 5</span><span>–math technical report: Toward mathematical expert model via self-improvement. </span><span>arXiv preprint</span><span> arXiv:2409.12122, 2024.</span></p></td><td><p dir="ltr"><span>A. Yang, </span><span>B. Yang, C. Yang</span><span>, et al. Qwen</span><span>3.5–mathematical report for iterative model self-improvement</span><span>. arXiv:</span><span>2909.12233</span><span>, 2024.</span></p></td></tr></tbody></table>
<!--kg-card-end: html-->
<p>Like GPTZero’s <a href="https://gptzero.me/" rel="noreferrer">AI Detector</a>, Hallucination Check has an extremely low false negative rate, so we catch 99 out of 100 flawed citations.&nbsp;Because our tool will flag any citation that can't be verified online, the false positive rate is higher.</p><h2 id="vibe-citing">Vibe Citing</h2><p>Over the past few months, we've experimented with several names for an LLM-generated citation with fabricated elements. "Hallucinated citations" is too long, "hallucitations" too easily mistaken for a spelling error, and "fake citations" too morally charged. Recently, GPTZero's Head of Machine Learning, Alex Adams, coined the term "vibe citing" to describe the LLM tendency to derive or amalgamate real sources into uncanny imitations. "Vibe citing," like "vibe writing" or "vibe coding" produces citations that look accurate at first glance, but crumble under closer inspection.</p><figure><img src="https://gptzero.me/news/content/images/2026/01/star-history-2026121.png" alt="" loading="lazy" width="1576" height="1153" srcset="https://gptzero.me/news/content/images/size/w600/2026/01/star-history-2026121.png 600w, https://gptzero.me/news/content/images/size/w1000/2026/01/star-history-2026121.png 1000w, https://gptzero.me/news/content/images/2026/01/star-history-2026121.png 1576w" sizes="(min-width: 720px) 720px"><figcaption><span>Figure 2: Open-source projects to write research papers with AI are booming in popularity and illustrate the growth in vibe-citing. The bumps in April and September 2025 correspond to the paper submission deadlines for NeurIPS and ICLR 2025.</span></figcaption></figure><p>GPTZero's analysis of 4841 of the 5290 papers accepted by NeurIPS 2025 indicates noticeable traces of AI authorship and hundreds of vibe citations. As always, each of the hallucinations presented here has been verified by a human expert.</p><h2 id="surf-the-tsunami-with-hallucination-check">Surf the Tsunami with Hallucination Check</h2><p>Hallucination Check is the only tool of its kind, and provides an essential service at multiple points in the peer review pipeline. First, it allows authors to check their manuscripts for citation errors — including common issues that can occur without LLM involvement like dead links or partial titles. Second, it greatly reduces the time and labor necessary for reviewers to check a submission's sources and identify possible vibe citing. Third, using Hallucination Check in combination with GPTZero's AI Detector allows editors and conference chairs to check for AI-generated text and suspicious citations at the same time, leading to faster and more accurate editorial decisions.</p><p>After releasing our ICLR paper investigation we are now coordinating with the ICLR team to review future paper submissions. As always, our goal is to make the peer review process faster, fairer, and more transparent for everyone involved. <em>Try GPTZero's </em><a href="https://gptzero.me/hallucination-detector" rel="noreferrer"><em><u>Hallucination check</u></em></a><em> for yourself, or </em><a href="https://gptzero.me/sales"><em><u>reach out to GPTZero's team</u></em></a><em>.</em></p><figure><a href="https://gptzero.me/news/iclr-2026/"><div><p>GPTZero uncovers 50+ Hallucinations in ICLR 2026</p><p>GPTZero used our Hallucination Check tool to find 50+ hallucinations under review at ICLR, each of which were missed by 3-5 peer reviewers.</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-2.png" alt=""><span>Paul Esau</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/ICLR_Logo.svg.png" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://gptzero.me/news/deloitte-australia-citation-check/"><div><p>The Deloitte Citation Situation - $98K Controversy Explained</p><p>GPTZero used our Citation Check to analyze the 234 page report and identified more than 30 issues out of the total 141 citations, including 19 hallucinations. Using GPTZero’s citation check would have saved ~$5000 per citation, all within minutes.</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-3.png" alt=""><span>AI Detection Resources | GPTZero</span><span>Nazar Shmatko</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/pexels-anildonoji-18541728-1-1.jpg" alt="" onerror="this.style.display = 'none'"></p></a></figure><figure><a href="https://gptzero.me/news/making-america-hallucinate-again-gptzero-detects-new-errors-in-major-government-report/"><div><p>Making America Hallucinate Again? GPTZero Detects New Errors in Major Government Report</p><p>On May 22, the U.S. Presidential Commission to Make America Healthy Again (MAHA), led by health secretary Robert F. Kennedy Jr., released a major report on the causes of chronic diseases in children. Yet within a week, news outlets including NOTUS, the New York Times and Washington Post reported</p><p><img src="https://gptzero.me/news/content/images/icon/square_logo-4.png" alt=""><span>AI Detection Resources | GPTZero</span><span>Paul Esau</span></p></div><p><img src="https://gptzero.me/news/content/images/thumbnail/Screenshot-2025-07-03-at-6.33.41---PM-1.png" alt="" onerror="this.style.display = 'none'"></p></a></figure>
            </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Tree-sitter vs. Language Servers (201 pts)]]></title>
            <link>https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</link>
            <guid>46719899</guid>
            <pubDate>Thu, 22 Jan 2026 14:47:58 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/">https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</a>, See on <a href="https://news.ycombinator.com/item?id=46719899">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  
  
  <h5>21 Jan 2026</h5>



  

  
  
  



<p>I got asked a good question today: what is the difference between <a href="https://en.wikipedia.org/wiki/Tree-sitter_%28parser_generator%29">Tree-sitter</a> and a <a href="https://en.wikipedia.org/wiki/Language_Server_Protocol">language server</a>? I don’t understand how either of these tools work in depth, so I’m just going to explain from an <em>observable</em>, <em>pragmatic</em> point of view.</p>
<h2 id="tree-sitter">
  Tree-sitter
  <a href="#tree-sitter">#</a>
</h2>
<p>Tree-sitter is a <em>parser generator</em>. What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What’s special about Tree-sitter is that it is a.) fast, and b.) can tolerate <em>syntax errors</em> in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you’re editing a program, <em>most of the time</em> the program will be in a syntactically invalid state. During that time, you don’t want your colors changing or just outright breaking while you’re typing. Naïve regex-based syntax highlighters frequently suffer from this issue.</p>
<p>Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs <a href="https://codeberg.org/ashton314/citar-typst">package I’m trying to develop</a> to add <a href="https://typst.app/">Typst</a> support to the <a href="https://github.com/emacs-citar/citar">Citar</a> citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.</p>
<p>In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.</p>
<h2 id="language-server">
  Language server
  <a href="#language-server">#</a>
</h2>
<p>A <em>language server</em> is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the <a href="https://en.wikipedia.org/wiki/Language_Server_Protocol">Language Server Protocol (LSP)</a>, defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.</p>
<p>Language servers solve the “


<span>
  \(N \times M\)
</span>
 problem” where <span>
  \(N\)
</span>
 programming languages and <span>
  \(M\)
</span>
 text editors would mean there have to be <span>
  \(N \times M\)
</span>
 implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.</p>
<p>Language servers are powerful because they can hook into the language’s runtime and compiler toolchain to get <em>semantically correct</em> answers to user queries. For example, suppose you have two versions of a <code>pop</code> function, one imported from a <code>stack</code> library, and another from a <code>heap</code> library. If you use a tool like the <a href="https://github.com/jacktasia/dumb-jump">dumb-jump</a> package in Emacs<label for="sn1"></label>

<span>
I just want to say that I think dumb-jump is very cool and I am not trying to knock it down at all. It’s honest about its limitations and can be handy when you do not have a language server available.
</span>
and you use it to jump to the definition for a call to <code>pop</code>, it might get confused as to where to go because it’s not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.</p>
<h3 id="using-a-language-server-for-highlighting">
  Using a language server for highlighting
  <a href="#using-a-language-server-for-highlighting">#</a>
</h3>
<p>It <em>is</em> possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or <em>not</em> want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.</p>
<p>Emacs’ built-in LSP client, <a href="https://github.com/joaotavora/eglot">Eglot</a>, recently added <code>eglot-semantic-tokens-mode</code> to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting.</p>

<p>I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human—me, Ashton Wiersdorf.</p>
<p>I am not so anti-AI to say that LLMs are worthless or should never be used. I’ve used LLMs a little bit. I think they’re fantastic at translating between languages; this seems to be something that they should be good at doing. They’re helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.</p>
<p>I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from <em>real</em> human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.</p></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[In Europe, Wind and Solar Overtake Fossil Fuels (456 pts)]]></title>
            <link>https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</link>
            <guid>46719491</guid>
            <pubDate>Thu, 22 Jan 2026 14:14:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels">https://e360.yale.edu/digest/europe-wind-solar-fossil-fuels</a>, See on <a href="https://news.ycombinator.com/item?id=46719491">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                  
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1200&amp;h=800&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769079918&amp;s=c67f214d895b44db7e6420c0a3bfc1aa" data-caption="Rooftop solar panels in Beeskow, Germany." data-credit="Pexels">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                
    
                        
  <img sizes="(min-width: 1450px) 832px, (min-width: 620px) 620px, 100vw" srcset="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1200&amp;h=800&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769079918&amp;s=c67f214d895b44db7e6420c0a3bfc1aa 1200w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=200&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=ac44a2f3862a1d7b22fb35c3a1d0e352 200w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=3c6b60c6321fd63f129208a6b8c09d33 400w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=600&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=5f04396b725cd8972d48c16af1ad0a97 600w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=800&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=76d4bd20af816f9a9a55c5905f0eb479 800w, https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=1000&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=c7877029453366a27a3adcac42eecdbf 1000w" src="https://yale-threesixty.transforms.svdcdn.com/production/Beeskow-Solar_Pexels.jpg?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769079918&amp;s=3c6b60c6321fd63f129208a6b8c09d33" alt="Rooftop solar panels in Beeskow, Germany.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>Rooftop solar panels in Beeskow, Germany.</span>
          <span>Pexels</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
            

<div>
  <p>Last year, for the first time, wind and solar supplied more power than fossil fuels to the E.U., according to a new analysis.</p><p>The shift is largely due to the rapid expansion of solar energy, which is growing faster than any other source of electricity. Together, wind and solar generated 30 percent of E.U. power last year, while fossil fuels provided 29 percent, according to the <a href="https://ember-energy.org/latest-insights/european-electricity-review-2026/">analysis</a> from Ember, a think tank based in London. Including hydro, renewables provided nearly half of all E.U. power in 2025.</p>
</div>
            
<div>

  <figure>

    <div>
      
                  
      <p><a href="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=980&amp;h=804&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769082265&amp;s=8ca86b66039abe22d21e745cabbe70c6" data-caption="E.U. power generation." data-credit="EMBER / ADAPTED BY YALE ENVIRONMENT 360">
  
  
  
    
  
  
  
      
    
                
    
                
    
                
    
                              
  <img sizes="(min-width: 1450px) 617px, (min-width: 1000px) 460px, (min-width: 600px) 60vw, 100vw" srcset="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=980&amp;h=804&amp;auto=compress%2Cformat&amp;fit=crop&amp;dm=1769082265&amp;s=8ca86b66039abe22d21e745cabbe70c6 980w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=200&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=ee4a99833329e05dd94a3daff509e74c 200w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=29b561390a3c2023f01fa6359c3faea1 400w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=600&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=213daf0b6f44a1854fc82d927a38fa6f 600w, https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=800&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=28d524c1498db30ddb2725b3148c58c9 800w" src="https://yale-threesixty.transforms.svdcdn.com/production/EU-Power_Ember.png?w=400&amp;auto=compress%2Cformat&amp;fit=clip&amp;dm=1769082265&amp;s=29b561390a3c2023f01fa6359c3faea1" alt="E.U. power generation.">
</a>
      </p>
          </div>

        <figcaption>
              <p><span>E.U. power generation.</span>
          <span>EMBER / ADAPTED BY YALE ENVIRONMENT 360</span></p>
    </figcaption>
    
  </figure>

</div> <!-- imageBlock -->
            

<div>
  <p>The analysis finds that solar is making gains in every E.U. country, while coal is broadly in retreat. Last year, solar alone supplied more than 20 percent of power in Hungary, Cyprus, Greece, Spain, and the Netherlands. Meanwhile, in 19 European countries, coal accounted for less than 5 percent of power. In 2025, both <a href="https://www.bloomberg.com/news/articles/2025-06-20/ireland-shuts-last-coal-plant-as-europe-s-phaseout-accelerates">Ireland</a> and <a href="https://www.spglobal.com/energy/en/news-research/latest-news/electric-power/040125-finland-shuts-last-coal-fired-power-plant-at-salmisaari-ending-the-era-of-coal">Finland</a> joined the ranks of European countries that have shuttered their last remaining coal plants.</p><p>Warming, however, continues to challenge the shift to clean energy as <a href="https://www.theguardian.com/environment/2025/nov/29/climate-crisis-depleting-europe-groundwater-reserves-analysis">drought</a> saps hydropower. Last year, hydro output dropped slightly in the E.U., and natural gas power rose to compensate.&nbsp;</p><p>“The next priority for the E.U. should be to put a serious dent in reliance on expensive, imported gas,” said Ember analyst Beatrice Petrovich. “Gas not only makes the E.U. more vulnerable to energy blackmail, it’s also driving up prices.”</p><p>In parts of Europe, there are signs that increasingly cheap batteries are beginning to displace natural gas in the early evening, when power demand is high, but solar output is waning. Said Petrovich, “As this trend accelerates it could limit how much gas is needed in evening hours, therefore stabilizing prices.”</p><h2><strong>ALSO ON YALE E360</strong></h2><p><a href="https://e360.yale.edu/features/europe-water-micropollutants"><i><strong>An E.U. Plan to Slash Micropollutants in Wastewater Is Under Attack</strong></i></a></p>
</div>
                </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Qwen3-TTS Family Is Now Open Sourced: Voice Design, Clone, and Generation (447 pts)]]></title>
            <link>https://qwen.ai/blog?id=qwen3tts-0115</link>
            <guid>46719229</guid>
            <pubDate>Thu, 22 Jan 2026 13:51:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://qwen.ai/blog?id=qwen3tts-0115">https://qwen.ai/blog?id=qwen3tts-0115</a>, See on <a href="https://news.ycombinator.com/item?id=46719229">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Douglas Adams on the English–American cultural divide over "heroes" (308 pts)]]></title>
            <link>https://shreevatsa.net/post/douglas-adams-cultural-divide/</link>
            <guid>46719222</guid>
            <pubDate>Thu, 22 Jan 2026 13:50:48 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://shreevatsa.net/post/douglas-adams-cultural-divide/">https://shreevatsa.net/post/douglas-adams-cultural-divide/</a>, See on <a href="https://news.ycombinator.com/item?id=46719222">Hacker News</a></p>
<div id="readability-page-1" class="page"><article>
  <header>
    
    <p><time datetime="2026-01-14T00:00:00Z">January 14, 2026</time></p>
  </header>
  <p>In 2000, Douglas Adams made an interesting observation that I keep returning to.</p>
<p>A user on Slashdot named “FascDot Killed My Pr” had asked the following question (where HGttG = Hitchhiker’s Guide to the Galaxy):</p>
<blockquote>
<p><em>Comedy….or Tragedy?</em></p>
<p><em>First, a big thank-you. You’ve made a lasting contribution to “our” culture (or should that be “culture”?)</em></p>
<p><em>I first read HGttG in my early teens. I doubled over laughing the whole time. I read and reread the entire series, bought both Dirk Gently books AND Last Chance to See. Loved them all and wouldn’t trade having read them for anything. (btw, the first mental ward scene in Long Dark Teatime is a no-foolin’, all-time classic.)</em></p>
<p><em>However, a few years ago I was talking to a (then) classmate. Very smart, philosophy-major type. He said (paraphrased) “I thought that HGttG was depressing. Such nihilism.” At the time I thought “Hmmm…I didn’t SEE a black beret on his head….”. But every reading of the series since then his comment has struck me as more true–especially in the case of Arthur Dent. In fact, far from being funny, I now find Dent’s character depressing–he’s not just a loser, he literally has no control over his life at all (except in So Long for a while). And the control he does have does him no good (e.g. Earth is destroyed while he’s trying to save his house.)</em></p>
<p><em>So my question is: When you were writing these books did you feel you were being gaily whimsical or did you instead feel frustrated and cynical?</em></p>
</blockquote>
<p>Douglas Adams replied with:</p>
<blockquote>
<p>I suspect there is a cultural divide at work here. In England our heroes tend to be characters who either have, or come to realise that they have, no control over their lives whatsoever – Pilgrim, Gulliver, Hamlet, Paul Pennyfeather (from Decline and Fall), Tony Last (from A Handful of Dust). We celebrate our defeats and our withdrawals – the Battle of Hastings, Dunkirk, almost any given test match. There was a wonderful book published, oh, about twenty years ago I think, by Stephen Pile called the Book of Heroic Failures. It was staggeringly huge bestseller in England and sank with heroic lack of trace in the U.S. Stephen explained this to me by saying that you cannot make jokes about failure in the States. It’s like cancer, it just isn’t funny at any level. In England, though, for some reason it’s the thing we love most. So Arthur may not seem like much of a hero to Americans – he doesn’t have any stock options, he doesn’t have anything to exchange high fives about round the water-cooler. But to the English, he is a hero. Terrible things happen to him, he complains about it a bit quite articulately, so we can really feel it along with him - then calms down and has a cup of tea. My kind of guy!</p>
<p>I’ve hit a certain amount of difficulty over the years in explaining this in Hollywood. I’m often asked ‘Yes, but what are his goals?’ to which I can only respond, well, I think he’d just like all this to stop, really. It’s been a hard sell. I rather miss David Vogel from the film process. He’s the studio executive at Disney who was in charge of the project for a while, but has since departed. There was a big meeting at one time to discuss, amongst other things, Arthur’s heroicness or lack of it. David suddenly asked me ‘Does Arthur’s presence in the proceedings make a difference to the way things turn out?’ to which I said, slightly puzzled, ‘Well, yes.’ David smiled and said ‘Good. Then he’s a hero.’</p>
<p>In the current, latest version of the screenplay, I think that Arthur’s non-heroic heroism is now absolutely preserved, and I’m pleased with the way he works out.</p>
</blockquote>
<p>(<a href="https://entertainment.slashdot.org/story/00/06/21/1217242/douglas-adams-answers-finally">Douglas Adams Answers (Finally) - Slashdot</a>)</p>
<p>I think I have more to say about this, and will try to come back and add more here, but meanwhile a few things at random:</p>
<ul>
<li>
<p>As a matter of fact, I <em>have</em> read <em>The Book of Heroic Failures</em> (1979) with great enjoyment. (<a href="https://shreevatsa.wordpress.com/2011/03/21/the-book-of-heroic-failures/">Post from 2011</a> —&nbsp;I only wrote four sentences of my own, but one of them was “Too many books have been written in praise of competence; this book provides an antidote by celebrating failure as only a British author can.”)</p>
</li>
<li>
<p>I think he is right that this goes over better (generally speaking) in England than in the USA. Of course one can make jokes <em>mocking</em> failure, but someone who fails does not automatically become endearing (in a kind of everyman way) in America the way they would in England. It seems to me that Americans are more likely to feel either contempt or pity than to feel kinship: or at any rate, they regard the failure as a setback or interesting circumstance, rather than the natural/default state of the world. (As someone who is neither American nor English, I am of course not someone whose opinions you should pay any heed to.)</p>
</li>
</ul>
<ul>
<li>As we live our lives, are we merely victims subject to winds of chance and external circumstance, or are we powerful agents fashioning our own stories, making our own luck? Obviously the answer is “both”, but perhaps the most distinctively American trait is to lean more towards the latter.</li>
</ul>

</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Design Thinking Books You Must Read (267 pts)]]></title>
            <link>https://www.designorate.com/design-thinking-books/</link>
            <guid>46718061</guid>
            <pubDate>Thu, 22 Jan 2026 11:51:10 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.designorate.com/design-thinking-books/">https://www.designorate.com/design-thinking-books/</a>, See on <a href="https://news.ycombinator.com/item?id=46718061">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
			
<p>Can you think that following a design thinking process with five steps turns you into a creative innovator?! Believe me, it isn’t and never has been this way. The spread of the term design thinking is aligned with a significant amount of misleading criticism. The doubts about the effectiveness of design thinking are influenced by the promotional language used by some companies, training places, and public speakers. The truth is that there is no secret recipe to turn someone into a creative designer. Yet, there is a way to use the design expertise inside each of us. <a href="https://www.designorate.com/design-thinking-guide-what-why-how/">Understanding the design thinking core values</a> can help team members improve their design ability and appreciate the creative practice inside the organization to achieve the next competitive advantage. This is why I wanted to share with you those key design thinking books to learn the core principles underpinning the design practice.</p>



<p>This is an updated list of design thinking books that I keep adding to their new book suggestions. So, please keep the link or subscribe to the newsletters to receive updates once new books are added. I am also starting to add papers that represent the cornerstone in the design thinking principles that I believe are as important as the book. In this update, two books and one paper added: The Science of Artificial, Wicked Problems in Design Thinking, and How Designers Think.</p>



<p>Previously, we explored different challenges that can be faced when applying design thinking inside the organization ( Why Companies Need to Apply Design Thinking and Why Companies Need to Apply Design Thinking). The majority of these factors rely on the lack of understanding of the core value of design thinking, which can be a reason for over-promotion and misuse of a commercialized language (check <a href="https://www.designorate.com/why-design-thinking-doesnt-work/" rel="bookmark">Why Design Thinking Doesn’t Work</a>). Above all, many design thinking trainers are not designers themselves and never practice the creative practice before teaching it which causes the gap between classrooms and practices.</p>



<div id="ez-toc-container">

<nav><ul><li><a href="#Design_Thinking_Books">Design Thinking Books</a><ul><li><a href="#Design_Expertise_by_Kees_Dorst">Design Expertise by Kees Dorst</a></li><li><a href="#Frame_Innovation_by_Kees_Dorst">Frame Innovation by Kees Dorst</a></li><li><a href="#Design_Thinking_Understanding_How_Designers_Think_and_Work">Design Thinking: Understanding How Designers Think and Work</a></li><li><a href="#Change_by_Design_by_Tim_Brown">Change by Design by Tim Brown</a></li><li><a href="#The_Design_of_Everyday_Things_by_Don_Norman">The Design of Everyday Things by Don Norman</a></li><li><a href="#How_Designers_Think_by_Bryan_Lawson">How Designers Think? by Bryan Lawson</a></li></ul></li><li><a href="#The_Science_of_Artificial_by_Herbert_Simon">The Science of Artificial by Herbert Simon</a></li><li><a href="#Wicked_Problems_in_Design_Thinking_Paper_by_Richard_Buchanan">Wicked Problems in Design Thinking (Paper) by Richard Buchanan</a></li><li><a href="#The_Dilemmas_in_a_General_Theory_of_Planning_by_Rittel_and_Webber">The Dilemmas in a General Theory of Planning by Rittel and Webber</a></li><li><a href="#The_New_Process_New_Vocabulary_Axiofact_A_tefact_Memoranda_by_Gilbert_Cockton">The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert Cockton</a></li><li><a href="#References">References</a></li></ul></nav></div>




<p>To expand my knowledge of the core values behind design thinking, I thought I would share with you some of the book titles that highlight design characteristics. Each of these books explores design from a specific perspective. Learning about these design aspects is essential for both designers and non-designers before jumping to learn design thinking. While there are several books about design thinking toolkits, the books below don’t teach you to design thinking methodology but the core principles behind design thinking to develop new alternatives of ideas and improve the analytical thinking of problems and solutions. They aim to guide you in understanding the core values and practices of design as a collaborative process. By acquiring this knowledge, you can effectively apply any of the design thinking processes we discussed earlier in previous articles with effectiveness. I am sure that those are not the only books out there, so please share with us your book suggestions in the comments below the article.</p>



<p><strong><em>Related article:</em></strong></p>



<p><a href="https://www.designorate.com/the-double-diamond-design-thinking-process-and-how-to-use-it/">The Double Diamond Design Thinking Process and How to Use it</a></p>



<p><a href="https://www.designorate.com/what-is-design-and-what-is-not/">What is Design? And What is not?</a></p>



<p><em><a href="https://www.designorate.com/design-thinking-guide-what-why-how/" rel="bookmark">Design Thinking Guide: What, Why and How</a></em></p>



<p><em><a href="https://www.designorate.com/why-design-thinking-doesnt-work/" rel="bookmark">Why Design Thinking Doesn’t Work</a></em></p>



<p><em><a href="https://www.designorate.com/measuring-the-impact-of-design-thinking/" rel="bookmark">Me</a><a href="https://www.designorate.com/measuring-the-impact-of-design-thinking/" rel="bookmark">asuring the Impact of Design Thinking</a></em></p>



<h3><span id="Design_Expertise_by_Kees_Dorst"></span>Design Expertise by Kees Dorst<span></span></h3>



<p>The <a href="https://amzn.to/34vj2Fj">Design Expertise</a>, written by Lawson and Dorst, focuses on the understanding of design practice in the creative industry. The book aims to explore the nature of design from a practitioner’s perspective. It starts by exploring the different definitions of design and how they contributed to identifying the border of the discipline of design.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg"><img fetchpriority="high" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg" alt="Design Expertise" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Expertise-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Design Expertise by Kees Dorst</figcaption></figure></div>


<p>The book presents design work for different designers and tries to use this overview of their work to provide a practical example of design characteristics. This book provides you with a base idea about design, what it is, and its characteristics. Exploring the characteristics through <a href="https://www.designorate.com/design-thinking-reshaped-microsoft-products/">design thinking case studies</a>, and examples helps you see design’s core value. This value is the main cornerstone behind the application of the design thinking process.</p>



<h3><span id="Frame_Innovation_by_Kees_Dorst"></span>Frame Innovation by Kees Dorst<span></span></h3>



<p>One of the main design characteristics is to solve problems or move from one position to an improved one. However, this can’t be achieved without a clear idea of the problem and its different borders. In his book <a href="https://amzn.to/2yPd9a9">Frame Innovation</a>, Dorst explores the cognitive design process’s problem and solution frames. Also, he explores how designers move from one frame to another and how this feedback process contributes toward an optimum solution for wicked problems.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg"><img decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg" alt="frame innovation" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Fram-Innovation-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Frame Innovation by Kees Dorst</figcaption></figure></div>


<p>Many of the design thinking process models move from the exploration stage (divergent) to defining the solution (conversion). While this practice shares the principle of critical thinking, they all move between the problem frame and solution frame. Through this book, you will explore the principles and practices of problem/solution frames to develop creative potential ideas.</p>



<p>The book extends discussion of of the principle of frame innovation by covering the opportunities and challenges related to its application in creative industries. The book ends by putting a practice action plan to move toward using the frame innovation in different business models.</p>



<h3><span id="Design_Thinking_Understanding_How_Designers_Think_and_Work"></span>Design Thinking: Understanding How Designers Think and Work<span></span></h3>



<p>In this small yet informative book, <a href="https://amzn.to/2RwszqB">Design Thinking</a>, Nigel Cross explores how designers think and reach creative ideas in the design field and the nature of design from the perspective of idea formation. To this goal, the book overviews design practice based on observing and interviewing creative designers and exploring expert tips with them. Design processes try to explore the design expertise from creating the idea to applying it. However, the design ability comes earlier when ideas are formulated. The book’s first chapter explores this design ability and how each of us has a level of design ability to develop new ideas. Yet, some people are more designers than others, which is known in Lusy Kimbel’s two papers as the creative class (Rethinking Design Thinking: <a href="https://www.tandfonline.com/doi/abs/10.2752/175470811X13071166525216?casa_token=eQlFFIjY110AAAAA:8TLvc0-_HjZPsM0NbT1y3lya4XulQVlfc-4f0uPelIYFwHjyJK53Aj-33_yvdsFKgxfy1EdeRbUNXw">Part 1</a> and <a href="https://www.tandfonline.com/doi/abs/10.2752/175470812X13281948975413?casa_token=Vlf9CU0l2A4AAAAA:pFFp0YVRmEZdRBbNDR2EWVin0DkNV72L6SaSIHgx7TnHKbC8ZqDdPC8Yvm1TNQtPsJIt6z65m9pxyg">Part 2</a>).</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg"><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg" alt="Design Thinking: Understanding How Designers Think and Work" srcset="https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Design-Thinking-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>Design Thinking: Understanding How Designers Think and Work by Nigel Cross</figcaption></figure></div>


<p>The book overviews designers’ practice in different fields and stories. The aim of this overview through creative designers’ experience is to build an understanding of the inspiration or exploration stage in the design thinking process. For instance, what is brainstorming, and why is it applied at an early point in the design thinking process (How to Successfully Apply Inspiration in Design Thinking)? Linking similar questions to the practice helps you map your practice to rational reasoning and subsequently improves the progress of the process in the future.</p>



<h3><span id="Change_by_Design_by_Tim_Brown"></span>Change by Design by Tim Brown<span></span></h3>



<p><a href="https://www.amazon.co.uk/Change-Design-Revised-Updated-Organizations/dp/0062856626/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=1680977260&amp;sr=8-10">Change by Design</a>, by Tim Bowen, CEO of the IDEO, is probably one of the commonly known books about design thinking because of the popularity of the IDEO in the application of design thinking in various social innovation contexts. In his book, Tim Brown manifests his ideology about design thinking and interprets it from the organisational perspective. The book aims to clarify what design thinking is, and where to go from theory to practice. In the first part, the books focus on the main concepts of design thinking (check Design Thinking Tools and Methods Complete Guide), such as extending behind the aesthetics, shifting toward a human-centred approach (i.e. improving customer experience and building inclusive design), the power of prototyping, and the importance of storytelling. The second part of the book aims to interpret these principles for practicality to identify the business opportunities for design thinking and the use of design to achieve innovation inside organisations through creative collaboration between stakeholders.<span>&nbsp;</span></p>


<div>
<figure><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02.jpg" alt="Change by Design book" srcset="https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/ChangebyDesign02-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"><figcaption>Change by Design by Tim Brown.</figcaption></figure></div>


<p>The book is a good resource for both designers and business people to understand design thinking and its applications. Despite several criticisms of the IDEO design thinking model, the book describes the theoretical base of design thinking, which could have a positive, innovative impact on organisations, especially if applied properly to develop viable business strategies. The <a href="http://www.designkit.org/">IDEO Field Guide</a> can be a good companion for the book as it presents a toolbox to apply Tim Brown’s ideology in practice.</p>



<h3><span id="The_Design_of_Everyday_Things_by_Don_Norman"></span>The Design of Everyday Things by Don Norman<span></span></h3>



<p>Don Norman is one of the leading professors in behaviour psychology and human-computer interaction (HCI). His book, <a href="https://amzn.to/2Rx5XGb">The Design of Everyday Things</a>, is based on a simple observation: why do we love and hate some elements in our lives? And what is the psychology behind our behaviour toward products? Addressing these two questions presents a cornerstone of your design practice. For example, why do some people love products such as Apple, <a href="https://www.designorate.com/what-is-market-segmentation-and-why-designers-should-understand/">Mini Cooper</a>, or <a href="https://www.designorate.com/ikea-sustainable-design-strategy/">IKEA</a>? By understanding how consumers love or hate products, the design team can target these features to build an empathic relationship between the product (or service) and the client, known as <a href="https://www.designorate.com/empathic-design-approach-to-successful-design/">emphatic design</a>.</p>


<div>
<figure><a href="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg"><img loading="lazy" decoding="async" width="800" height="500" src="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg" alt="The Design of Everyday Things " srcset="https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things.jpg 800w, https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things-300x188.jpg 300w, https://www.designorate.com/wp-content/uploads/2020/04/Everyday-Things-768x480.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a><figcaption>The Design of Everyday Things for Don Norman</figcaption></figure></div>


<p>The book explores human-centred design and its impact on usability interaction design principles, as well as user experience. While other books covered this aspect of design experience, Norman studied the experience from a psychological point of view to examine this complex design process. The book covers the psychology behind our daily actions, knowledge, design limitations, and human error. Later, the book explores design thinking as a tool to solve problems and the usage of the Design Council Double Diamond design thinking process. The book is not only for UX designers but for designers from different practices, as you can learn the following:</p>



<ol>
<li>How the brain works and the psychology related to products and services,</li>



<li>The limitations related to our experience with interacting with designs around and</li>



<li>Human error and a bad design causes .</li>
</ol>



<h3><span id="How_Designers_Think_by_Bryan_Lawson"></span>How Designers Think? by Bryan Lawson<span></span></h3>



<p><a href="https://www.amazon.co.uk/How-Designers-Think-Process-Demystified/dp/0750660775/ref=tmm_pap_swatch_0?_encoding=UTF8&amp;qid=1701980560&amp;sr=1-1" target="_blank" rel="noreferrer noopener nofollow">How Designers Think?</a> by Bryan Lawson is one of the design thinking books I recommend for my students who are still new to problem-solving and understanding the philosophical approach underpinning the problem and solution space in the design thinking process, How Designer Think for Bryan Lawson overviews the design definition, the relation between problem and solutions and the design thinking process. </p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="624" src="https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-1024x624.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-1024x624.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-300x183.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1-768x468.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/HowDesigersThink-1.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>How Designers Think? by Bryan Lawson</figcaption></figure></div>


<p>Unlike other books, Lawson doesn’t aim to teach you his method or derive a specific point; it is more like a discussion book to allow you to reflect and synthesise on the design practice and finally come up with your conclusion. The book presents a flow of ideas as a case study, making it easy to understand and enjoyable for new readers in design thinking. I recommend reading it before moving to more advanced books such as The Science of Artificial.</p>



<h2><span id="The_Science_of_Artificial_by_Herbert_Simon"></span>The Science of Artificial by Herbert Simon <span></span></h2>



<p>The <a href="https://www.amazon.co.uk/Sciences-Artificial-MIT-Press/dp/0262691914" target="_blank" rel="noreferrer noopener nofollow">Science of Artificial </a>is one of Simon’s most famous and irritating works based on three lectures for him at MIT in 1968, a year before the book was first published. The book discusses the nature of human thinking and the “artefact.” In eight chapters, it explores how humans use artefacts to solve everyday problems. His expression of human rationale is expressed with three premises:</p>



<ul>
<li>The limitations in the human’s cognitive ability</li>



<li>The time available to make a decision, and</li>



<li>The complexity of the problem</li>
</ul>



<p>Based on these three premises, he concluded that we are under the illusion that we can choose the optimal solution for problems. Instead, we find a way to determine the reasonable solutions (check <a href="https://www.designorate.com/the-six-hats-of-critical-thinking-and-how-to-use-them/">What Are The Six Thinking Hats? And How to Use Them?</a>).</p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="624" src="https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-1024x624.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-1024x624.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-300x183.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical-768x468.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/The_Science_of_Aritifical.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The Science of Artificial by Herbert Simon </figcaption></figure></div>


<p>Simon was awarded a Nobel Prize for his theory and its contribution to economic rationality. According to the above theory, Simon defined three problem-solving activities: the ability to conduct a heuristic search for alternatives, evaluate solutions, and allocate resources for search. He illustrates this concept in his statement:</p>



<p><em>” Human problem solving involves nothing more than varying mixtures of trial and error and selectivity. The selectivity derives from various rules of thumb, or heuristics, suggesting which paths should be tried first and which promising leads.”</em></p>



<h2><span id="Wicked_Problems_in_Design_Thinking_Paper_by_Richard_Buchanan"></span>Wicked Problems in Design Thinking (Paper) by Richard Buchanan<span></span></h2>



<p><a href="https://web.mit.edu/jrankin/www/engin_as_lib_art/Design_thinking.pdf" target="_blank" rel="noreferrer noopener nofollow">Wicked Problems in Design Thinking</a> by Buchanan was published in Design Studies in 1992. Buchanan linked design and analytical philosophy by understanding the design problem’s nature and elements. He discussed two terms: “category” and “placement”, where we frame the different aspects of the problem. Buchanan describes them as follows:</p>



<p><em>“Understanding the difference between a category and a placement is essential if design thinking is to be regarded as more than a series of creative accidents. Categories have fixed meanings that are accepted within the framework of a theory or a philosophy and serve as the basis for analysing what already exists. Placements have boundaries to shape and constrain meaning but are not rigidly fixed and determinate. The boundary of placement gives a context or orientation to thinking, but the application to a specific situation can generate a new perception of that situation and, hence, a new possibility to be tested. Therefore, placements are sources of new ideas and possibilities when applied to problems in concrete circumstances.”</em></p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2023/12/Wicked_Problems_Design_Thinking.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Wicked Problems in Design Thinking paper by Richard Buchanan</figcaption></figure></div>


<p>The expandable nature of the “placement” presents a critical element of wicked problems and how we can see them as a universal concept whose boundaries can change based on the situation. This manifestation of the definition of the problem elements presented the cornerstone for Kees Dorst’s problem/solution frame discussed in the earlier book Frame Innovation (<a href="https://www.designorate.com/8d-problem-solving-approach/">What is the 8D Problem-Solving?&nbsp;</a>).</p>



<p>His ideas of wicked problems link with Simon’s concept about the design thinking process and how it can be seen as a non-linear process where different design ideas interact in the design arena. Also, In this placement, Buchanan differentiated between four elements of the design thinking process:</p>



<ul>
<li>Signs: material objects</li>



<li>Things: actions</li>



<li>Thoughts: complex systems or environments, which is a weird characterisation.</li>
</ul>



<p>As he links the above elements and the two terms described earlier (category vs placement), he describes the nature of wicked problems:<br><em>“However, when a designer’s conceptual placements become categories of thinking, the result can be mannered imitations of an earlier invention that are no longer relevant to discovering specific possibilities in a new situation. Ideas are then forced onto a situation rather than discovered in the particularities and novel possibilities of that situation.”</em></p>



<p>The above manifestation describes how wicked problems are constructed and change over time, paving the way for a new perspective on problems and their analysis to identify new solutions (check also <a href="https://www.designorate.com/practice-guide-to-solve-problems-with-triz/">How to Use TRIZ in the Problem-Solving Process</a>).</p>



<h2><span id="The_Dilemmas_in_a_General_Theory_of_Planning_by_Rittel_and_Webber"></span>The Dilemmas in a General Theory of Planning by Rittel and Webber<span></span></h2>



<p><a href="https://web.mit.edu/jrankin/www/engin_as_lib_art/Design_thinking.pdf" target="_blank" rel="noreferrer noopener">The Dilemmas in a General Theory of Planning</a> by Rittel and Webber, despite its age, remains a seminal work that has significantly influenced the understanding of wicked problems. Published in 1969, this paper laid the groundwork for Kees Dorst’s Frame Innovation and Buchanan’s Wicked Problems,&nbsp;both of&nbsp;which we’ve discussed. While the&nbsp;paper’s focus is&nbsp;on planning and policy science,&nbsp;its insights can be applied&nbsp;to problem definition and the process of solving them.&nbsp;</p>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Rittel-Webber.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The Dilemmas in a General Theory of Planning by Rittel and Webber</figcaption></figure></div>


<p>Rittel and Webber distinguished between two types of problems: tame and wicked problems. Tame problems are well-defined and clearly stated, and there is a clear direction to finding the solution, such as scientific and business problems (check how this concept influenced <a href="https://www.designorate.com/practice-guide-to-solve-problems-with-triz/">TRIZ problem-solving</a>). In contrast, wicked problems are ill-defined, and we can’t define the problem until we reach a solution. However, a wicked problem is never solved, yet it moves from one state to an improved,&nbsp;desirable&nbsp;one.&nbsp;</p>



<p>The other nature of wicked problems is that we cannot reach a definitive formulation for them. To describe them, we need to develop an exhaustive inventory of conceivable solutions when asking questions about the problem. So, problem understanding and resolution are linked and change as we build an understanding of the problem at a particular moment in time.&nbsp;</p>



<h2><span id="The_New_Process_New_Vocabulary_Axiofact_A_tefact_Memoranda_by_Gilbert_Cockton"></span>The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda by Gilbert Cockton<span></span></h2>



<p>As you can see, the above books and papers give us a novel look at design problems and how we perceive them. My question is, why do we see problems the way we used to? A big part of the answer lies in our language, which presents mental models that stand as barriers to seeing the core nature of problems, especially the wicked ones. Therefore, we needed new vocabulary that helped us to escape these constraints. <a href="http://library.usc.edu.ph/ACM/CHI%202017/2exab/ea747.pdf" target="_blank" rel="noreferrer noopener">The New Process, New Vocabulary: Axiofact = A_tefact + Memoranda</a>, by my PhD supervisor, Professor Gilbert Cockton, presents a cornerstone of new vocabularies that can help us see design thinking and how to solve problems.&nbsp;</p>



<figure><img loading="lazy" decoding="async" width="1024" height="599" src="https://www.designorate.com/wp-content/uploads/2024/05/Cockton-1024x599.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Cockton-1024x599.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton-300x176.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton-768x449.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Cockton.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The paper eliminated the so-called design thinking process, as the term employs a linear nature; while design thinking is far from linear, it is intersected activities. Cockton described the design practice as design arenas; these arenas are distinguishing “artefacts” and “memoranda.” The “artefact” represents the design outcome, and the “memoranda” is the thing to be borne in mind. This new terminology replaces the problem and solution spaces. However, the Latin root of an artefact means the product of change or doing some art. However, this term is limited as wicked problems are not understood until we solve them, which means artefacts. So, the outcome of the design arena may remain the same as the original state, or the change is against the target user, such as preventive design and design against crime. So, Cockton replaced the word artefact with A_tefact.&nbsp; The memoranda consist of three arenas:</p>



<ul>
<li><strong>Beneficiaries</strong>: The purpose of design</li>



<li><strong>Purposes</strong>: The Artefact and Evaluation</li>



<li><strong>Evaluations</strong>: Modifications to the Artefact</li>
</ul>


<div>
<figure><img loading="lazy" decoding="async" width="1024" height="655" src="https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-1024x655.jpg" alt="" srcset="https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-1024x655.jpg 1024w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-300x192.jpg 300w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas-768x492.jpg 768w, https://www.designorate.com/wp-content/uploads/2024/05/Design_Arenas.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>The design arenas. Recreated from Cockton (2017).</figcaption></figure></div>


<p>Other terms were also introduced in the paper, such as episodes to replace stages (or phases) that are inherited from linear process age. The multiple foci (sequence by concurrency) replaced the centre of the process term to indicate the complex nature of the iteration with no simple way to describe it. The term “iteration” is replaced with balanced concurrent drama, and validation is replaced with the term “axiofact,” or the value generated. The new terminology presented in Cockton’s paper allows us to escape the old mental model when addressing wicked problems.&nbsp;If you check the <a href="https://www.designorate.com/using-the-mppf-method-in-the-double-diamond-design-process/">MPPF method in Design Thinking</a>, which we discussed previously, you will find it&nbsp;a&nbsp;useful&nbsp;tool&nbsp;as it can help us address wicked problems.&nbsp;</p>



<p>Each of the above books and papers focuses on specific aspects of design and how we observe the design thinking practice driven by feedback from both academia and industry. The different design thinking models are based on appreciating these characteristics of design and encouraging it inside the organization. By applying the steps alone, you will never reach any improved status. You need to recognize these characteristics of design and try to practice them during the design process. Again, the above books came to my mind as key books in design. I am sure there are other titles. So, please share it in the comments below.</p>



<h2><span id="References"></span>References<span></span></h2>



<p>Brown, T. and Katz, B., 2011. Change by design.&nbsp;<em>Journal of Product Innovation Management</em>,&nbsp;<em>28</em>(3), pp.381-383.</p>



<p>Buchanan, R., 1992. Wicked problems in design thinking.&nbsp;<em>Design issues</em>,&nbsp;<em>8</em>(2), pp.5-21.</p>



<p>Cockton, G., 2017, May. New process, new vocabulary: Axiofact= a_tefact+ memoranda. In&nbsp;<em>Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems</em>&nbsp;(pp. 747-757).</p>



<p>Cross, N., 2023.&nbsp;<em>Design thinking: Understanding how designers think and work</em>. Bloomsbury Publishing.</p>



<p>Dorst, K., 2015.&nbsp;<em>Frame innovation: Create new thinking by design</em>. MIT press.</p>



<p>Norman Donald, A., 2013.&nbsp;<em>The design of everyday things</em>. MIT Press.</p>



<p>Lawson, B., 2006.&nbsp;<em>How designers think</em>. Routledge.</p>



<p>Rittel, H.W. and Webber, M.M., 1973. Dilemmas in a general theory of planning.&nbsp;<em>Policy sciences</em>,&nbsp;<em>4</em>(2), pp.155-169.</p>



<p>Simon, H.A., 1988. The science of design: Creating the artificial.&nbsp;<em>Design Issues</em>, pp.67-82.</p>












		</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We will ban you and ridicule you in public if you waste our time on crap reports (876 pts)]]></title>
            <link>https://curl.se/.well-known/security.txt</link>
            <guid>46717556</guid>
            <pubDate>Thu, 22 Jan 2026 10:48:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://curl.se/.well-known/security.txt">https://curl.se/.well-known/security.txt</a>, See on <a href="https://news.ycombinator.com/item?id=46717556">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[ISO PDF spec is getting Brotli – ~20 % smaller documents with no quality loss (139 pts)]]></title>
            <link>https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/</link>
            <guid>46717507</guid>
            <pubDate>Thu, 22 Jan 2026 10:41:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/">https://pdfa.org/want-to-make-your-pdfs-20-smaller-for-free/</a>, See on <a href="https://news.ycombinator.com/item?id=46717507">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>For nearly three decades; or November 1996 to be exact, PDFs have relied on Deflate—the same compression algorithm that powers your ZIP files. Meanwhile, the web moved on. In 2015, Google introduced Brotli, a compression algorithm so efficient it now powers 95% of internet traffic. Websites got faster. Downloads got smaller. CDNs got cheaper.</p>
<p><strong>Now PDFs are getting the same upgrade.</strong></p>
<p>The PDF Association is bringing this battle-tested web compression technology <a href="https://pdfa.org/brotli-compression-coming-to-pdf/">into the PDF specification</a> itself. After a decade of Brotli proving its worth across billions of web requests daily, it's now getting ready to make it's introduction into ISO 32000.</p>
<p>With <a href="https://pdfa.org/member/itext-group-nv/">iText</a>, we can help drive widespread adoption with a <strong>production-ready Brotli encoder and decoder</strong> for the PDF ecosystem. The result? <strong>15-25% smaller files</strong> with zero quality loss, using the same algorithm trusted by Google, Cloudflare, and every major CDN.</p>
<h2 id="why-pdf-compression-has-struggled-to-evolve">Why PDF compression has struggled to evolve</h2>
<p>PDF compression has been stuck in 1996 for a good reason: <strong>backward compatibility is sacred</strong>. The PDF Association operates under a strict principle—any new feature must work seamlessly with existing readers, or it risks fragmenting the ecosystem. Adding a new compression algorithm isn't just a technical change; it's a <em>breaking change</em> that could render documents unreadable in older software. This creates a high barrier for innovation.</p>
<p>Beyond compatibility concerns, there are other practical challenges. The PDF specification moves slowly by design—it's an ISO standard that requires consensus by hundreds of stakeholders. Compression algorithms must be <strong>royalty-free</strong> (ruling out patented options), <strong>widely supported</strong> across platforms, and <strong>battle-tested</strong> in production.</p>
<p>Finally, the ecosystem is conservative: enterprises and governments rely on PDFs for archival and legal documents that must remain accessible for decades, making any breaking change a risk that needs extraordinary justification.</p>
<h2 id="encoding-and-decoding-technical-implementation">Encoding and decoding: Technical implementation</h2>
<p>To get Brotli compression working within the iText SDK, we need to solve two problems: reading documents, and also writing them.</p>
<p>Let's start with the easiest one; reading documents.</p>
<h3>Decoding: Advanced plumbing work</h3>
<p>First of all, let's look at how the content of a page is stored within a PDF. We can demonstrate this with just the classic "Hello World" text example.</p>
<p>The following PDF syntax simply displays the text "Hello World!" on a page:</p>
<pre>5 0 obj                 % Unique identifier to reference this content from other places within the PDF
&lt;&lt;/Length 49&gt;&gt;stream    % Meta data for the stream object. Here it contains a Length value to indicate how many bytes are there after the `stream` keyword.
q                       % the actual content
BT
/F1 12 Tf
37 788.33 Td
(Hello World!)Tj
ET
Q
endstream               % Indicates the end of the stream object
endobj                  % Indicates the end of the referenceable object
</pre>
<p>So, if we need to render or do anything else with the content, it would look like the following:</p>
<pre>|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
</pre>
<p>Okay, so now we have a high-level view how PDF processors handle the low-level processing of those stream objects, we can dive a little deeper!</p>
<p>Let's take a look at the following PDF stream object where the content is encoded using the Deflate algorithm.</p>
<pre>5 0 obj
&lt;&lt;/Filter/FlateDecode/Length 36&gt;&gt;stream                  % The meta data now now includes `Filter`
xœmÍÂ0„ïûëM/1?Æl®‚âUømI)Íûºm¢...            % Reduced for clarity
endstream
endobj
</pre>
<p>First of all, we notice there is an additional Key <code>Filter</code> with a value of <code>FlateDecode</code> in the metadata.<br>
This can be interpreted the following way: "The content of this stream object is only usable after its <code>FlateDecode</code> filter is applied".</p>
<p>So how does this change our working implementation?</p>
<pre>|------------------------|
| Get stream based on id |
|------------------------|
           ||
           \/
|------------------------|
|      Read content      |
|------------------------|
           ||
           \/
|------------------------|
|         Decode         |
|     based on Filter    |
|------------------------|
           ||
           \/
|------------------------|
|    Render/Do stuff     |
|    with the content    |
|------------------------|
</pre>
<p>We can now see we require an operation on the content before it's usable. The PDF specification already provides a variety of ways to write the content of the PDF streams.</p>
<table>
<tbody>
<tr>
<td><strong>Filter name</strong></td>
<td><strong>Description</strong></td>
</tr>
<tr>
<td>ASCIIHexDecode</td>
<td>Decodes ASCII hexadecimal data to binary.</td>
</tr>
<tr>
<td>ASCII85Decode</td>
<td>Decodes ASCII base-85 data to binary.</td>
</tr>
<tr>
<td>LZWDecode</td>
<td>Decompresses data using LZW compression.</td>
</tr>
<tr>
<td>FlateDecode</td>
<td>Decompresses data using zlib/deflate compression.</td>
</tr>
<tr>
<td>RunLengthDecode</td>
<td>Decompresses data using run-length encoding.</td>
</tr>
<tr>
<td>CCITTFaxDecode</td>
<td>Decompresses CCITT fax-encoded monochrome images.</td>
</tr>
<tr>
<td>JBIG2Decode</td>
<td>Decompresses JBIG2-encoded monochrome image data.</td>
</tr>
<tr>
<td>DCTDecode</td>
<td>Decompresses JPEG DCT-based image data.</td>
</tr>
<tr>
<td>JPXDecode</td>
<td>Decompresses JPEG 2000 wavelet-based image data.</td>
</tr>
<tr>
<td>Crypt</td>
<td>Decrypts data encrypted by a security handler.</td>
</tr>
</tbody>
</table>
<p>So the idea for Brotli is to simply add another <code>Filter</code> implementation.What we need to get it working into iText is actually pretty minimal:</p>
<ol>
<li>Get the decoding implementation from Google's repository.</li>
<li>Write some plumbing code to call it from iText</li>
<li>Hook up the plumbing code to the <code>BrotliDecode</code> filter</li>
</ol>
<p>For the first step we simply embedded <strong>Google's reference Java Brotli decoder</strong> straight from their official repository into our kernel module.</p>
<h4>Why embed the decoder?</h4>
<p>By embedding Google's reference implementation directly, we guarantee:</p>
<ul>
<li><strong>Zero dependency hell</strong>: No version conflicts with other libraries</li>
<li><strong>Consistent behavior</strong>: Same decoder on all platforms</li>
<li><strong>Long-term stability</strong>: We control the code, even if upstream changes</li>
<li><strong>Automatically generate C# version</strong>: Using our porting mechanism we can have a C# implementation</li>
</ul>
<p>The plumbing implementation lives in <code>BrotliFilter.java</code>, which plugs into iText's existing filter pipeline:</p>
<pre>public class BrotliFilter extends MemoryLimitsAwareFilter {
    @Override
    public byte[] decode(byte[] b, PdfName filterName, PdfObject decodeParams,
            PdfDictionary streamDictionary) {
        try {
            final byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
            final ByteArrayInputStream input = new ByteArrayInputStream(b);
            final ByteArrayOutputStream output = enableMemoryLimitsAwareHandler(streamDictionary);
            final BrotliInputStream brotliInput = new BrotliInputStream(input);
            int len;
            while ((len = brotliInput.read(buffer, 0, buffer.length)) &gt; 0) {
                output.write(buffer, 0, len);
            }
            brotliInput.close();
            return output.toByteArray();
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.FAILED_TO_DECODE_BROTLI_STREAM, e);
        }
    }
}
</pre>
<p>Let's break down what's happening in this implementation:</p>
<ul>
<li><strong>Memory Safety First</strong>: The filter extends <code>MemoryLimitsAwareFilter</code>, which protects against decompression<br>
bombs—malicious PDFs that expand into gigabytes of data when decompressed. This is critical for production systems.</li>
<li><strong>Wrapped Input Stream</strong>: The compressed bytes <code>b</code> are wrapped in a <code>ByteArrayInputStream</code>, which is then passed to<br>
Google's <code>BrotliInputStream</code>. This is where the magic happens—<code>BrotliInputStream</code> handles all the heavy lifting of Brotli decompression.</li>
</ul>
<p>As you can see, writing the plumbing code is pretty easy because of iText's architecture.</p>
<p>The last thing to do is to ensure iText knows which implementation to associate with the <code>/BrotliDecode</code> filter.</p>
<p>This is also pretty trivial. The filter is registered automatically in <code>FilterHandlers.java</code> alongside <code>/FlateDecode</code> and the other standard PDF<br>
filters:</p>
<pre>public final class FilterHandlers {
    private static final Map&lt;PdfName, IFilterHandler&gt; defaults;

    static {
        Map&lt;PdfName, IFilterHandler&gt; map = new HashMap&lt;&gt;();

        map.put(PdfName.FlateDecode, new FlateDecodeFilter());
        map.put(PdfName.Fl, new FlateDecodeFilter());
        //other implementations removed for clarity

        // we add our implementation
        map.put(PdfName.BrotliDecode, new BrotliFilter());

        defaults = Collections.unmodifiableMap(map);
    }
}
</pre>
<p>That's it. From this point on, <strong>any PDF with <code>/BrotliDecode</code> streams just works</strong>. No configuration needed.</p>
<p>Now we could have stopped here—our SDK could process Brotli-compressed PDFs from other sources. But reading isn't enough. To truly bring Brotli to the PDF ecosystem, we needed to let developers create these smaller files. That meant solving the encoding problem.</p>
<p>And encoding turned out to be significantly more complex than decoding.</p>
<h3>Encoding: a separate module for compression</h3>
<h4>The problem: iText’s compression was hardcoded</h4>
<p>Before Brotli, iText only supported two compression modes for PDF streams:</p>
<ol>
<li><strong>Flate compression</strong></li>
<li><strong>No compression</strong></li>
</ol>
<p>This logic was baked directly into the stream-writing code—there was no abstraction, no plugin point. If you wanted to use a different compression algorithm, you were out of luck.</p>
<p>To support Brotli (and future algorithms), we needed to <strong>introduce a new abstraction layer</strong>: <code>IStreamCompressionStrategy</code>.</p>
<pre>public interface IStreamCompressionStrategy {
   /**
    * Gets the PDF filter name that identifies this compression algorithm.
    *
    * @return the PDF name representing the compression filter
    */
   PdfName getFilterName();

   /**
    * Gets the decode parameters required for decompressing the stream.
    * &lt;p&gt;
    * Decode parameters provide additional information needed to correctly
    * decompress the stream data.
    *
    * @return the decode parameters as a PDF object, or {@code null} if not needed
    */
   PdfObject getDecodeParams();

   /**
    * Creates a new output stream that wraps the original stream and applies compression.
    * @param original the original output stream to wrap
    * @param stream the PDF stream being compressed (may be used for context or configuration)
    *
    * @return a new output stream that performs compression
    */
    OutputStream createNewOutputStream(OutputStream original, PdfStream stream);
}
</pre>
<p>This interface decouples compression logic from iText's core PDF writing machinery. Now, instead of hardcoding Flate everywhere, we can inject different strategies at runtime. To inject the required strategy we make use of the <code>DiContainer</code>. You can find more information about it here: <a href="https://kb.itextpdf.com/itext/adding-dependency-injection-to-the-pdfdocument-cla" target="_blank" rel="noopener">Adding Dependency Injection to the PdfDocument class</a>.</p>
<p>From now on when iText needs to compress a stream, it asks the <code>DiContainer</code> in the <code>PdfDocument</code>: <em>"Do you have an IStreamCompressionStrategy?"</em></p>
<ul>
<li><strong>If yes</strong>: Use the registered strategy (Brotli in this case)</li>
<li><strong>If no</strong>: Fall back to the default Flate compression</li>
</ul>
<p>This design gives us:</p>
<ul>
<li><strong>Zero coupling</strong>: iText Core no longer cares about the algorithm used</li>
<li><strong>Opt-in behavior</strong>: You only pay the cost if you use it</li>
<li><strong>Future-proof</strong>: New algorithms just implement the interface</li>
</ul>
<h4>The Second Problem: No Pure Java Encoder</h4>
<p>Here's where things got tricky. While Google's Brotli decoder has a pure Java implementation (which we embedded for reading), the official Brotli <em>encoder</em> <strong>is C++ only</strong>. To use it from Java, you need:</p>
<ul>
<li><strong>JNI bindings</strong> to call native code from Java</li>
<li><strong>Platform-specific native libraries</strong> (<code>.dll</code> for Windows, <code>.so</code> for Linux, <code>.dylib</code> for macOS)</li>
<li><strong>Build infrastructure</strong> to compile and ship these libraries for every platform</li>
</ul>
<p>For a heavily-used library like iText, shipping native binaries is a non-starter:</p>
<ul>
<li><strong>Deployment complexity</strong>: Users need to manage native libraries across platforms</li>
<li><strong>Security concerns</strong>: Native code introduces attack surfaces</li>
<li><strong>Build maintenance</strong>: We'd need to compile for Windows x64, Linux ARM, macOS Silicon, etc.</li>
<li><strong>Version conflicts</strong>: What if another library ships a different Brotli version?</li>
</ul>
<p>We needed a solution that handled this complexity <em>outside</em> iText's core.</p>
<p>That solution is a separate Maven module (<code>brotli-compressor</code>) that you add as an optional dependency. This<br>
module contains:</p>
<ul>
<li><strong>BrotliStreamCompressionStrategy</strong>: Implementation of <code>IStreamCompressionStrategy</code></li>
<li><strong>brotli4j dependency</strong>: A third-party library that wraps Google's C++ encoder with JNI</li>
</ul>
<p>Here's what <code>BrotliStreamCompressionStrategy</code> looks like:</p>
<pre>public class BrotliStreamCompressionStrategy implements IStreamCompressionStrategy {

    @Override
    public OutputStream createNewOutputStream(OutputStream original, PdfStream stream) {
        int compressionLevel = convertCompressionLevel(stream.getCompressionLevel());
        Encoder.Parameters params = Encoder.Parameters.create(compressionLevel);
        try {
            return new BrotliOutputStream(original, params);
        } catch (IOException e) {
            throw new PdfException(KernelExceptionMessageConstant.CANNOT_WRITE_TO_PDF_STREAM, e);
        }
    }

    @Override
    public PdfName getFilterName() {
        return PdfName.BrotliDecode; // This goes into the /Filter entry
    }
}
</pre>
<h4>The native wrapper: brotli4j</h4>
<p>Instead of writing JNI bindings ourselves, we rely on <a href="https://github.com/hyperxpro/Brotli4j" target="_blank" rel="noopener">brotli4j</a>—a mature, well-tested library that:</p>
<ul>
<li>Wraps Google's official C++ Brotli encoder/decoder</li>
<li>Ships <strong>pre-compiled native libraries</strong> for all major platforms (Windows x64/ARM, Linux x64/ARM, macOS Intel/Silicon)</li>
<li>Automatically extracts the correct native library at runtime (no manual setup)</li>
<li>Is actively maintained and widely used (powers projects like Netty, OkHttp)</li>
</ul>
<p>By delegating to brotli4j, we get production-grade native bindings without maintaining our own JNI layer.</p>
<h4>Why keep encoding separate?</h4>
<p>You might ask: <em>"Why not bundle brotli4j in the kernel module like you did with the decoder?"</em></p>
<p>Great question. Here's the reasoning:</p>
<table>
<tbody>
<tr>
<td><strong>Aspect</strong></td>
<td><strong>Decoder (in kernel)</strong></td>
<td><strong>Encoder (separate module)</strong></td>
</tr>
<tr>
<td><strong>Necessity</strong></td>
<td>Required to read Brotli PDFs</td>
<td>Optional—only for <em>writing</em></td>
</tr>
<tr>
<td><strong>Dependencies</strong></td>
<td>Pure Java (Google's decoder)</td>
<td>Native code (brotli4j with JNI)</td>
</tr>
<tr>
<td><strong>Size impact</strong></td>
<td>~300KB of Java code</td>
<td>~2MB of native libraries</td>
</tr>
<tr>
<td><strong>Use frequency</strong></td>
<td>Every user needs to read PDFs</td>
<td>Most users stick with Flate</td>
</tr>
<tr>
<td><strong>Backward compat</strong></td>
<td>No breaking changes</td>
<td>Opt-in feature</td>
</tr>
</tbody>
</table>
<p>By keeping the encoder separate, we give users choice: add <code>brotli-compressor</code> if you need 20% smaller files, or stick with the default if native dependencies are a concern.</p>
<h4>Putting it all together: Full example</h4>
<p>Here's what it looks like to create a Brotli-compressed PDF:</p>
<p>First of all add the required dependencies. Notice you have to add iText's artifactory because of the experimental nature of the code, and so users don't accidentally enable it.</p>
<pre>&lt;repositories&gt;
  &lt;repository&gt;
    &lt;id&gt;itext-releases&lt;/id&gt;
    &lt;name&gt;iText Repository - releases&lt;/name&gt;
    &lt;url&gt;https://repo.itextsupport.com/releases&lt;/url&gt;
  &lt;/repository&gt;
&lt;/repositories&gt;

&lt;dependency&gt;
&lt;groupId&gt;com.itextpdf&lt;/groupId&gt;
&lt;artifactId&gt;brotli-compressor&lt;/artifactId&gt;
&lt;version&gt;{itext.version.bigger.then.9.5.0}&lt;/version&gt;
&lt;/dependency&gt;
</pre>
<pre>public static void main() {

// 1. Register the compression strategy
   DocumentProperties properties = new DocumentProperties();
   properties.registerDependency(IStreamCompressionStrategy.class, new BrotliStreamCompressionStrategy());
// 2. Create your PDF as normal
   PdfWriter writer = new PdfWriter("output.pdf");
   PdfDocument pdf = new PdfDocument(writer, properties);

// Everything from here on uses Brotli automatically
   Document doc = new Document(pdf);
   doc.add(new Paragraph("This text will be Brotli-compressed!"));
   doc.add(new Image(ImageDataFactory.create("chart.png")));
   doc.close();

}
</pre>
<p>When you open <code>output.pdf</code> in a text editor, you'll see some entries looking like this:</p>
<pre>5 0 obj
&lt;&lt;/Filter/BrotliDecode/Length 847&gt;&gt;stream
[binary Brotli-compressed data]
endstream
endobj
</pre>
<p>The PDF now uses <code>/BrotliDecode</code> instead of <code>/FlateDecode</code>, and the file is 15-25% smaller—with <strong>zero changes</strong> to your document-building code.</p>
<h3>The catch: Compatibility isn't universal (yet)</h3>
<p>Here's the honest truth: <strong>Brotli-compressed PDFs won't open in Adobe Acrobat Reader today</strong>. They won't render in your browser's built-in PDF viewer. Most third-party PDF libraries will reject them outright.</p>
<p>Why? Because <code>/BrotliDecode</code> isn't part of the official PDF specification yet. The PDF Association is actively working on adding it to ISO 32000 (the PDF standard), but until that's finalized and implementations roll out, Brotli PDFs exist in a gray area.</p>
<h4>What about forward compatibility?</h4>
<p>Here's the good news: <strong>Brotli PDFs are future-proof</strong>. Once the PDF Association finalizes the spec and vendors implement it, your existing Brotli-compressed documents will just work. You're not creating broken files—you're creating files that are <em>ahead of their time</em>.</p>
<p>Think of it like HTTP/2 in 2015. Early adopters who deployed it got immediate performance wins in their own<br>
infrastructure, and as browsers caught up, those benefits became universal. Brotli PDFs follow the same pattern.</p>
<h4>iText's commitment</h4>
<p>We're not shipping this as a toy feature. We're working directly with the PDF Association to:</p>
<ul>
<li><strong>Standardize the specification</strong> (syntax, decode parameters, dictionary support)</li>
<li><strong>Validate implementations</strong> across multiple platforms (Java, .NET, C++)</li>
<li><strong>Contribute test suites</strong> to ensure interoperability when other vendors adopt it</li>
<li><strong>Support migrations</strong> when the spec finalizes (we'll handle any breaking changes)</li>
</ul>
<p>By adopting Brotli compression now, you're not taking a risk—you're investing in a proven technology that's on a clear path to standardization.</p>
<h2>Conclusion</h2>
<p>PDF compression hasn't evolved in 30 years—until now. Brotli represents the biggest leap in PDF storage efficiency since the format was invented, and iText is bringing it to production <strong>today</strong>.</p>
<p>Yes, there are compatibility limitations. Yes, it's experimental. But every standard starts this way. HTTP/2, WebP, and TLS 1.3 were all "experimental" once. Early adopters got the benefits first, then the ecosystem caught up.</p>
<p>By using iText's Brotli implementation now, you're:</p>
<ul>
<li><strong>Reducing storage costs</strong> by 15-25% immediately</li>
<li><strong>Future-proofing your documents</strong> for inevitable standardization</li>
<li><strong>Helping shape the spec</strong> with real-world feedback</li>
<li><strong>Voting with code</strong> for a more efficient PDF ecosystem</li>
</ul>
<p>The PDF Association is listening. Adobe is watching. And iText is leading.</p>
<p><strong>Let's make PDFs smaller together.</strong> 🚀</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[30 Years of ReactOS (232 pts)]]></title>
            <link>https://reactos.org/blogs/30yrs-of-ros/</link>
            <guid>46716469</guid>
            <pubDate>Thu, 22 Jan 2026 08:03:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://reactos.org/blogs/30yrs-of-ros/">https://reactos.org/blogs/30yrs-of-ros/</a>, See on <a href="https://news.ycombinator.com/item?id=46716469">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content">
							<p>Happy Birthday ReactOS! Today marks 30 years since <a href="https://github.com/reactos/reactos/commit/0f94427db073a20c24f9d85c8531fbe16490af43">the first commit to the ReactOS source tree</a>.
It’s been such a long journey that many of our contributors today, including myself, were not alive during this event.
Yet our mission to deliver “your favorite Windows apps and drivers in an open-source environment you can trust” continues to bring people together.
Let’s take a brief look at some of the high and low points throughout our history.</p>
<!-- raw HTML omitted -->
<h2 id="1996-2003-the-painful-road-to-reactos-010">1996-2003: The Painful Road to ReactOS 0.1.0</h2>
<p>ReactOS started from the ashes of the FreeWin95 project, which aimed to provide a free and open-source clone of Windows 95.
FreeWin95 suffered from analysis paralysis, attempting to plan the whole system before writing any code.
Tired of the lack of progress on the project, Jason Filby took the reins as project coordinator and led a new effort targeting Windows NT.
The project was renamed to “ReactOS” as it was a reaction to Microsoft’s monopolistic position in home computer operating systems.</p>
<p>Progress on ReactOS was very slow at first.
Contributors had to first build a very basic NT-like kernel before they could develop drivers for it, then continue developing the kernel; not too dissimilar to the process of bootstrapping a new programming language.
Once a few basic drivers were written, other contributors were able to learn from these examples and develop other drivers.</p>
<p>While writing this article, I reached out to Eric Kohl. He developed the original storage driver stack for ReactOS (atapi, scsiport, class2, disk, cdrom, cdfs) and has been with the project since 1998. I asked him about his experiences with ReactOS during this time, how he found the project, and what contributing to ReactOS was like during those early days. He wrote:</p>
<blockquote>
<p>I think I found ReactOS while searching for example code for my contributions to the WINE project.
I subscribed to the mailing list and followed the discussions for a few days.
The developers were discussing the future of shell.exe, a little command line interpreter that could only change drives and directories and execute programs.
A few days [later] I had started to convert the FreeDOS command.com into a Win32 console application, because I wanted to extend it to make it 4DOS compatible.
4DOS was a very powerful command line interpreter.
On December 4th, 1998 I introduced myself and suggested to use my converted FreeDOS command.com as the future ReactOS cmd.exe.
I had a little conversation with Jason Filby and Rex Joliff, the CVS repository maintainer.
I sent my cmd.exe code to Rex and he applied it to the repository.
After applying a few more cmd-related patches over the next weeks, Rex asked me whether I would like to have write-access to the repository.
I accepted the offer…</p>
<p>…</p>
<p>The first version I downloaded and used was 0.0.8.
It was not much more than a DOS-based bootloader, some drivers, and a basic kernel that ran a few test routines after initialization.</p>
<p>…</p>
<p>Version 0.0.8 didn’t use PE files, but flat (position independent) binaries.
There was no PE loader,  no smss, no csrss, no winlogon, no process heaps, no process environments, no threads, etc.
Each and every little feature was a milestone.</p>
<p>…</p>
<p>Initially there was not a review process at all.
You write some code, test it and fix it until it works.
Then you commit it.
If something failed on another machine, you got a reply on the mailing list and discussed a solution.
You fixed the issue and committed a fix.
That’s how it worked.</p>
<p>…</p>
<p>There was always an open and friendly atmosphere.
It was and still is always nice to talk to other developers.
No fights, no wars, like in some other projects.</p>

</blockquote>
<p><em>Editors note: minor errors were corrected.</em></p>
<p>ReactOS 0.1.0 was released on February 1st, 2003 and received minor updates up until November 2003.
ReactOS 0.1.0 was the first version of ReactOS that could boot from a CD.
It had a command line interface and no desktop.
Watch a demo of it below, provided courtesy of archeYR.</p>
<p><a href="https://youtu.be/rgRMemZcVoM" target="_blank" rel="noopener noreferrer">
  <img src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.1.0-thumbnail.png" alt="See ReactOS 0.1.0 by archeYR">
</a></p><h2 id="2003-2006-reactos-02x">2003-2006: ReactOS 0.2.x</h2>
<p>During this period ReactOS saw rapid development.
New drivers were being built all the time, a basic desktop was built, and ReactOS became increasingly stable and usable.
Public interest grew as ReactOS matured.
In October 2005, Jason Filby stepped down as project coordinator, and Steven Edwards was voted to be the next project coordinator.</p>


<div itemscope="" itemtype="http://schema.org/ImageGallery">
	  


<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-boot.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-boot.png" alt="ReactOS 0.2.x boot screen"></a>
    <figcaption>
        <p>ReactOS 0.2.x boot screen</p>
    </figcaption>
</figure>



<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-desktop.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.x-desktop.png" alt="ReactOS 0.2.x desktop and file explorer"></a>
    <figcaption>
        <p>ReactOS 0.2.x desktop and file explorer</p>
    </figcaption>
</figure>



<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.0-desktop.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.2.0-desktop.png" alt="ReactOS 0.2.0 with VMware video driver for NT 4"></a>
    <figcaption>
        <p>ReactOS 0.2.0 with VMware video driver for NT 4</p>
    </figcaption>
</figure>


</div>

<p>It wasn’t all sunshine and rainbows though.
In January 2006, concerns grew about contributors having access to leaked Windows source code and possibly using this leaked source code in their contributions.
In response, Steven Edwards strengthened the project’s intellectual property policy and the project made the difficult decision to audit the existing source code and temporarily freeze contributions.</p>
<h2 id="2006-2016-reactos-03x">2006-2016: ReactOS 0.3.x</h2>
<p>The ongoing audit and contribution freeze from the end of the ReactOS 0.2.x era slowed development and momentum considerably for ReactOS 0.3.x.
Following challenges with the audit, Steven Edwards stepped down as project coordinator and Aleksey Bragin assumed the role by August 2006.</p>
<p>Despite the challenges during this time, ReactOS 0.3.x continued to build upon ReactOS’s legacy.
ReactOS 0.3.0 was released on August 28th, 2006.
It introduced networking support and a package manager called “Download!”.
This package manager would become the basis for RAPPS, the package manager built into modern versions of ReactOS.
In July 2008, the x86_64 port of ReactOS was started.
One year later, ReactOS 0.3.10 imported the <a href="http://alter.org.ua/soft/win/uni_ata/">UniATA driver</a>, written by Alexandr Telyatnikov (Alter).
While we run into limitations with the UniATA driver today, UniATA enabled ReactOS to support SATA storage devices and to support partitions greater than 8GB in size.
On February 8th, 2012, ReactOS 0.3.14 supported being built using the MSVC compiler and added visual style support.</p>


<div itemscope="" itemtype="http://schema.org/ImageGallery">
	  


<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-desktop.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-desktop.png" alt="ReactOS 0.3.x desktop"></a>
    <figcaption>
        <p>ReactOS 0.3.x desktop</p>
    </figcaption>
</figure>



<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-download.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.3.x-download.png" alt="Download!, the package manager for ReactOS 0.3.x"></a>
    <figcaption>
        <p>Download!, the package manager for ReactOS 0.3.x</p>
    </figcaption>
</figure>


</div>

<h2 id="2016-today-reactos-04x">2016-Today: ReactOS 0.4.x</h2>
<p>ReactOS 0.4.0 was released on February 16th, 2016.
It introduced a new graphical shell that utilized more Windows features and was more similar architecturally to Windows Explorer.
ReactOS 0.4.0 also introduced support for kernel debugging using WinDbg when compiled with MSVC.
Being able to use standard Windows tools for kernel debugging has helped us progress considerably.
ReactOS 0.4.0 continued to receive incremental updates every few months up until versions 0.4.14 and 0.4.15 which had years of development updates each.
Today, the x86_64 port of ReactOS is similarly functional to its x86 counterpart, but with no WoW64 subsystem to run x86 apps its usability is limited.</p>


<div itemscope="" itemtype="http://schema.org/ImageGallery">
	  


<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/explorer-diagram.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/explorer-diagram.png" alt="A humorous diagram made in 2015 to explain the complexity of Windows Explorer"></a>
    <figcaption>
        <p>A humorous diagram made in 2015 to explain the complexity of Windows Explorer</p>
    </figcaption>
</figure>



<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
  <a href="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.4.15-desktop.png" itemprop="contentUrl"><img itemprop="thumbnail" src="https://reactos.org/img/blogs/30yrs-of-ros/ros-0.4.15-desktop.png" alt="ReactOS 0.4.15 desktop, shown with Luna visual style and large taskbar icons applied"></a>
    <figcaption>
        <p>ReactOS 0.4.15 desktop, shown with Luna visual style and large taskbar icons applied</p>
    </figcaption>
</figure>


</div>

<h2 id="the-future-of-reactos">The Future of ReactOS</h2>
<p>We’re continuing to move ReactOS forward. Behind the scenes there are several out-of-tree projects in development. Some of these exciting projects include a new build environment for developers (RosBE), a new NTFS driver, a new ATA driver, multi-processor (SMP) support, support for class 3 UEFI systems, kernel and usermode address space layout randomization (ASLR), and support for modern GPU drivers built on WDDM.</p>
<p>The future of ReactOS will be written by the people who believe in the mission and are willing to help carry it forward.</p>
<p>If you believe in running “your favorite Windows apps and drivers in an open-source environment you can trust”, you can help make that a reality by <a href="https://reactos.org/donate">making a financial contribution</a>, <a href="https://github.com/reactos/reactos">opening a pull request on GitHub</a>, or <a href="https://jira.reactos.org/">testing and filing bug reports</a>.
Even small contributions can help a lot!</p>
<h2 id="statistics">Statistics</h2>
<p><em>Note: Statistics were calculated at commit f60b1c9</em></p>
<ul>
<li>Total commits: 88,198</li>
<li>Total unique contributors: 301</li>
<li>Total files: 31,025</li>
<li>Total lines of code: 14,929,578</li>
</ul>

						</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Doctors in Brazil using tilapia fish skin to treat burn victims (259 pts)]]></title>
            <link>https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims</link>
            <guid>46715600</guid>
            <pubDate>Thu, 22 Jan 2026 05:15:46 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims">https://www.pbs.org/newshour/health/brazilian-city-uses-tilapia-fish-skin-treat-burn-victims</a>, See on <a href="https://news.ycombinator.com/item?id=46715600">Hacker News</a></p>
<div id="readability-page-1" class="page"><article itemprop="articleBody">
                <div>
                    <p>FORTAZELA, Brazil — In this historic city by the sea in northeast Brazil, burn patients look as if they've emerged from the waves. They are covered in fish skin — specifically strips of sterilized tilapia.</p>
<p>Doctors here are testing the skin of the popular fish as a bandage for second- and third-degree burns. The innovation arose from an unmet need. Animal skin has long been used in the treatment of burns in developed countries. But Brazil lacks the human skin, pig skin, and artificial alternatives that are widely available in the US.</p>
<p>The three functional skin banks in Brazil can meet only 1 percent of the national demand, said Dr. Edmar Maciel, a plastic surgeon and burn specialist leading the clinical trials with tilapia skin.</p>
<p>As a result, public health patients in Brazil are normally bandaged with gauze and silver sulfadiazine cream.</p>
<p>"It's a burn cream because there's silver in it, so it prevents the burns from being infected," said Dr. Jeanne Lee, interim burn director at the the regional burn center at the University of California at San Diego. "But it doesn't help in terms of debriding a burn or necessarily helping it heal."</p>
<p><a href="https://www.statnews.com/2017/03/01/sweat-burn-patients-skin-grafts/" target="_blank"><strong>READ MORE: First Look: Plumbing the mysteries of sweat to help burn patients cool their skin</strong></a></p>
<p>The gauze-and-cream dressing must be changed every day, a painful process. In the burn unit at Fortaleza's José Frota Institute, patients contort as their wounds are unwrapped and washed.</p>
<p>Enter the humble tilapia, a fish that's widely farmed in Brazil and whose skin, until now, was considered trash. Unlike the gauze bandages, the sterilized tilapia skin goes on and stays on.</p>
<p>The first step in the research process was to analyze the fish skin.</p>
<p>"We got a great surprise when we saw that the amount of collagen proteins, types 1 and 3, which are very important for scarring, exist in large quantities in tilapia skin, even more than in human skin and other skins," Maciel said. "Another factor we discovered is that the amount of tension, of resistance in tilapia skin is much greater than in human skin. Also the amount of moisture."</p>
<p>In patients with superficial second-degree burns, the doctors apply the fish skin and leave it until the patient scars naturally. For deep second-degree burns, the tilapia bandages must be changed a few times over several weeks of treatment, but still far less often than the gauze with cream. The tilapia treatment also cuts down healing time by up to several days and reduces the use of pain medication, Maciel said.</p>
<p>Antônio dos Santos, a fisherman, was offered the tilapia treatment as part of a clinical trial after he sustained burns to his entire right arm when a gas canister on his boat exploded. He accepted.</p>
<p>"After they put on the tilapia skin, it really relieved the pain," he said. "I thought it was really interesting that something like this could work."</p>
<p><a href="https://www.statnews.com/2015/11/11/thermoplastic-bandage/" target="_blank"><strong>READ MORE: High-tech bandage wins $100K from Boston Marathon bombing survivor's family</strong></a></p>
<p>The initial batches of tilapia skin were studied and prepared by a team of researchers at the Federal University of Ceará. Lab technicians used various sterilizing agents, then sent the skins for radiation in São Paulo to kill viruses, before packaging and refrigerating the skins. Once cleaned and treated, they can last for up to two years.</p>
<p>In the US, animal-based skin substitutes require levels of scrutiny from the Food and Drug Administration and animal rights groups that can drive up costs, Lee said. Given the substantial supply of donated human skin, tilapia skin is unlikely to arrive at American hospitals anytime soon.</p>
<p>But it may be a boon in developing countries.</p>
<p>"I'm willing to use anything that might actually help a patient," Lee said. "It may be a good option depending on what country you're talking about. But I also think the problem is that you need to find places that have the resources to actually process the skin and sterilize it, and make sure it doesn't have diseases."</p>
<p>In Brazil, in addition to the clinical trials, researchers are currently conducting histological studies that compare the composition of human, tilapia, pig, and frog skins. They are also conducting studies on the comparative costs of tilapia skin and conventional burn treatments. If clinical trials show continued success, doctors hope a company will process the skins on an industrial scale and sell it to the public health system.</p>
<p><em>This article is reproduced with permission from <a href="https://www.statnews.com/" target="_blank">STAT</a>. It was first published on Mar. 2, 2017. Find the original story <a href="https://www.statnews.com/2017/03/02/brazil-tilapia-skin-burns/" target="_blank">here</a>.</em></p>

                                            <div>
                <figure>
                    <svg><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#pbs-newshour-horiz-refresh"></use></svg>
                </figure>
                <p>
                    A free press is a cornerstone of a healthy democracy. 
                </p>
                <p>
                    Support trusted journalism and civil dialogue. 
                </p>
                <a href="https://give.newshour.org/page/88646/donate/1?ea.tracking.id=pbs_news_sept_2025_article&amp;supporter.appealCode=N2509AW1000100">
                    
                    <svg width="24px" height="24px" viewBox="0 0 16 16" fill="#000000" version="1.1" id="svg1" xmlns="http://www.w3.org/2000/svg" xmlns:svg="http://www.w3.org/2000/svg">
                    <defs id="defs1"></defs>
                    <path fill-rule="evenodd" d="M4 8a.5.5 0 0 1 .5-.5h5.793L8.146 5.354a.5.5 0 1 1 .708-.708l3 3a.5.5 0 0 1 0 .708l-3 3a.5.5 0 0 1-.708-.708L10.293 8.5H4.5A.5.5 0 0 1 4 8z" id="path1"></path>
                    </svg>
                </a>
            </div>


                                    </div>
            </article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Significant US Farm Losses Persist, Despite Federal Assistance (269 pts)]]></title>
            <link>https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance</link>
            <guid>46713929</guid>
            <pubDate>Thu, 22 Jan 2026 01:11:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance">https://www.fb.org/market-intel/significant-farm-losses-persist-despite-federal-assistance</a>, See on <a href="https://news.ycombinator.com/item?id=46713929">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>

  
  






<div id="loop0"><p><strong>Key Takeaways</strong></p><ol><li>Per-acre <strong>production costs</strong> for all nine principal row crops are <strong>projected to rise again in 202</strong><strong>6</strong>, continuing a troubling trend that began after 2021.</li><li>Inflated operating costs <strong>remain</strong> the primary drivers of higher breakeven prices, with limited relief expected in the near term.</li><li>Recent programs have <strong>offset a </strong>portion of losses, but do not fully close the gap between costs and market returns, leaving many farmers potentially operating below breakeven for another year.</li><li><strong>Specialty </strong>crop growers face similar issues as row crop farmers,<strong></strong>but limited data makes per-acre loss estimates challenging.</li></ol><p>The USDA-Economic Research Service (ERS) <a href="https://www.ers.usda.gov/data-products/commodity-costs-and-returns" target="_blank" rel="noreferrer noopener">December update to Commodity Costs and Returns</a> provides a comprehensive look at per-acre production costs for the nine principal row crops: corn, soybeans, wheat, cotton, rice, barley, oats, peanuts and sorghum. At a high level, ERS projects average total costs per acre to increase for every crop in 2026, underscoring the persistence of elevated production expenses across U.S. agriculture. </p></div><!-- /margin and padding --><div><!-- width wrap --><figure><a href="https://www.fb.org/imgz/Slide7.PNG" id="expandparent"><img src="https://www.fb.org/imgz/_w900/251946/Slide7.png"><svg id="expandchild" width="85" height="25" viewBox="0 0 85 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H84.5V24.1429H0V0Z" fill="black"></path><path d="M67.6801 6.90387C67.4835 6.7073 67.4835 6.38886 67.6801 6.1923L69.8512 4.0212C69.8748 3.99762 69.9003 3.97698 69.9278 3.9583C69.9406 3.94946 69.9544 3.94356 69.9672 3.93668C69.9829 3.92783 69.9976 3.91899 70.0143 3.91211C70.031 3.90523 70.0487 3.9013 70.0654 3.89638C70.0792 3.89245 70.093 3.88655 70.1077 3.88361C70.1726 3.87083 70.2394 3.87083 70.3052 3.88361C70.32 3.88655 70.3338 3.89245 70.3475 3.89638C70.3652 3.9013 70.3829 3.90523 70.3996 3.91211C70.4163 3.91899 70.4311 3.92783 70.4468 3.93668C70.4596 3.94356 70.4733 3.94946 70.4861 3.9583C70.5136 3.97698 70.5392 3.99762 70.5628 4.0212L72.7299 6.18838C72.9265 6.38494 72.9265 6.70339 72.7299 6.89995C72.5334 7.0965 72.2149 7.09652 72.0184 6.89995L70.7092 5.59081V9.76976C70.7092 10.0479 70.4841 10.273 70.206 10.273C69.9279 10.273 69.7028 10.0479 69.7028 9.76976L69.7047 5.59181L68.3927 6.9039C68.2944 7.00218 68.1656 7.05133 68.0369 7.05133C67.9081 7.05133 67.7794 7.00218 67.6811 6.9039L67.6801 6.90387ZM72.0193 16.4335L70.7072 17.7456V13.5666C70.7072 13.2885 70.4821 13.0634 70.204 13.0634C69.9259 13.0634 69.7008 13.2885 69.7008 13.5666V17.7456L68.3916 16.4365C68.1951 16.2399 67.8766 16.2399 67.6801 16.4365C67.4835 16.633 67.4835 16.9515 67.6801 17.148L69.8472 19.3152C69.8708 19.3388 69.8964 19.3594 69.9239 19.3781C69.9367 19.386 69.9495 19.3919 69.9632 19.3997C69.979 19.4086 69.9947 19.4174 70.0114 19.4253C70.0281 19.4322 70.0448 19.4361 70.0625 19.441C70.0772 19.4449 70.091 19.4508 70.1057 19.4538C70.1382 19.4607 70.1716 19.4636 70.204 19.4636C70.2374 19.4636 70.2699 19.4597 70.3023 19.4538C70.317 19.4508 70.3308 19.4449 70.3456 19.441C70.3623 19.4361 70.38 19.4322 70.3967 19.4253C70.4134 19.4184 70.4281 19.4086 70.4448 19.3997C70.4576 19.3928 70.4714 19.3869 70.4841 19.3781C70.5117 19.3594 70.5382 19.3388 70.5608 19.3152L72.7319 17.1441C72.9285 16.9475 72.9285 16.6291 72.7319 16.4325C72.5353 16.236 72.2169 16.236 72.0203 16.4325L72.0193 16.4335ZM77.9155 11.9499C77.9204 11.943 77.9224 11.9351 77.9273 11.9282C77.94 11.9076 77.9528 11.886 77.9617 11.8634C77.9656 11.8535 77.9676 11.8437 77.9705 11.8339C77.9774 11.8132 77.9853 11.7916 77.9902 11.77C77.9971 11.7376 78 11.7042 78 11.6707C78 11.6373 77.9961 11.6039 77.9902 11.5715C77.9853 11.5489 77.9784 11.5282 77.9705 11.5076C77.9676 11.4978 77.9656 11.4869 77.9617 11.4781C77.9518 11.4555 77.9391 11.4339 77.9273 11.4132C77.9233 11.4063 77.9204 11.3985 77.9155 11.3916C77.8968 11.3641 77.8762 11.3385 77.8526 11.3149L75.6815 9.14385C75.4849 8.94728 75.1665 8.94728 74.9699 9.14385C74.7733 9.34042 74.7733 9.65886 74.9699 9.85542L76.282 11.1675H72.103C71.8249 11.1675 71.5998 11.3926 71.5998 11.6707C71.5998 11.9489 71.8249 12.1739 72.103 12.1739H76.282L74.9728 13.4831C74.7763 13.6796 74.7763 13.9981 74.9728 14.1946C75.0711 14.2929 75.1999 14.3421 75.3286 14.3421C75.4574 14.3421 75.5861 14.2929 75.6844 14.1946L77.8516 12.0275C77.8752 12.0039 77.8958 11.9783 77.9145 11.9508L77.9155 11.9499ZM64.1281 12.17H68.3071C68.5852 12.17 68.8103 11.9449 68.8103 11.6668C68.8103 11.3887 68.5852 11.1636 68.3071 11.1636H64.1291L65.4383 9.85445C65.6348 9.65788 65.6348 9.33943 65.4383 9.14288C65.2417 8.94632 64.9233 8.94631 64.7267 9.14288L62.5595 11.3101C62.5359 11.3336 62.5153 11.3592 62.4966 11.3867C62.4917 11.3936 62.4898 11.4015 62.4848 11.4083C62.4721 11.429 62.4593 11.4506 62.4504 11.4732C62.4465 11.483 62.4445 11.4929 62.4416 11.5027C62.4347 11.5233 62.4269 11.545 62.4219 11.5666C62.4151 11.599 62.4121 11.6324 62.4121 11.6658C62.4121 11.6993 62.416 11.7327 62.4219 11.7651C62.4269 11.7877 62.4337 11.8084 62.4416 11.829C62.4445 11.8388 62.4465 11.8496 62.4504 11.8585C62.4603 11.8811 62.473 11.9027 62.4848 11.9233C62.4888 11.9302 62.4917 11.9381 62.4966 11.945C62.5153 11.9725 62.5359 11.998 62.5595 12.0216L64.7306 14.1927C64.8289 14.291 64.9577 14.3402 65.0864 14.3402C65.2152 14.3402 65.3439 14.291 65.4422 14.1927C65.6388 13.9962 65.6388 13.6777 65.4422 13.4812L64.1301 12.1691L64.1281 12.17Z" fill="white"></path><path d="M14.5178 15.8906V14.4928H10.3901V11.8421H14.5178V10.4442H10.3901V8.05725H14.5178V6.65938H8.75488V15.8906H14.5178Z" fill="white"></path><path d="M22.4709 9.09906H20.506L19.029 11.2486L17.5784 9.09906H15.5475L17.974 12.4619L15.3497 15.8906H17.3542L18.8971 13.6488L20.4137 15.8906H22.4577L19.9389 12.4487L22.4709 9.09906Z" fill="white"></path><path d="M23.562 9.09906V19.1875H25.1577V15.2049C25.6852 15.7851 26.4896 16.0489 27.2017 16.0489C29.1403 16.0489 30.4854 14.3741 30.4854 12.5146C30.4854 10.5233 29.0875 8.94081 27.2281 8.94081C26.4764 8.94081 25.5797 9.24413 25.1049 9.96944L25.0126 9.09906H23.562ZM25.1577 11.3937C25.5401 10.6552 26.2786 10.3387 26.9248 10.3387C28.1117 10.3387 28.8633 11.3673 28.8633 12.4751C28.8633 13.5169 28.2303 14.651 26.8984 14.651C26.2654 14.651 25.6192 14.3873 25.1577 13.7938V11.3937Z" fill="white"></path><path d="M37.4711 15.8906V11.7893C37.4711 10.0222 36.6403 8.94081 34.7149 8.94081C33.739 8.94081 32.6972 9.21775 31.8928 9.61338L32.328 10.8134C32.9082 10.5101 33.6599 10.22 34.4248 10.22C35.4666 10.22 35.8754 10.7739 35.8754 11.697V11.8553C35.3743 11.7498 34.9523 11.7102 34.5962 11.7102C33.1588 11.7102 31.6026 12.2377 31.6026 13.9257C31.6026 15.3499 32.7763 16.0489 34.0423 16.0489C34.8204 16.0489 35.4798 15.8379 35.9677 15.2576L36.0468 15.8906H37.4711ZM35.8754 14.1894C35.4138 14.5851 34.9259 14.862 34.3325 14.862C33.7126 14.862 33.172 14.5587 33.172 13.8729C33.172 12.9894 34.0819 12.752 34.8336 12.752C35.1369 12.752 35.4666 12.7916 35.8754 12.8839V14.1894Z" fill="white"></path><path d="M39.3895 9.09906V15.8906H40.9852V11.3805C41.3809 10.7475 42.0402 10.3387 42.6732 10.3387C43.1612 10.3387 43.5041 10.5101 43.7019 10.853C43.9524 11.275 43.9656 11.908 43.9656 12.4751V15.8906H45.5613V12.3564C45.5613 11.3673 45.4558 10.4706 44.9547 9.81119C44.5327 9.2705 43.8997 8.94081 43.0821 8.94081C42.2644 8.94081 41.46 9.32325 40.9193 10.1013L40.8402 9.09906H39.3895Z" fill="white"></path><path d="M53.8995 6H52.2906V9.798C51.7631 9.21775 50.9587 8.94081 50.2598 8.94081C48.308 8.94081 46.9761 10.6156 46.9761 12.4751C46.9761 14.4532 48.3608 16.0489 50.2334 16.0489C50.9851 16.0489 51.8686 15.7588 52.3566 15.0203L52.4357 15.8906H53.8995V6ZM52.2906 13.596C51.9082 14.3345 51.1697 14.651 50.5367 14.651C49.3366 14.651 48.5981 13.6224 48.5981 12.5146C48.5981 11.4728 49.2311 10.3387 50.5631 10.3387C51.1829 10.3387 51.8291 10.6024 52.2906 11.1959V13.596Z" fill="white"></path></svg></a></figure></div><!-- /margin --><div id="loop2"><p>When operating expenses and farm-wide costs like equipment, land and management are combined, costs vary widely by crop. In 2025, forecasted total per-acre costs are $1,308 for rice, $1,166 for peanuts, $943 for cotton, $890 for corn, $658 for soybeans, $498 for oats, $491 for barley, $443 for sorghum, and $396 for wheat. Looking ahead, ERS projections for 2026 suggest continued upward pressure across most cost categories, with total cost increasing anywhere from 2.2% to 3.3%. Amongst the nine principal crops, wheat ($409 per acre), sorghum ($458) and oats ($513) remain at the lower end of the production cost spectrum, while soybeans ($678) and barley ($507) fall in the mid-range in 2026. Cotton ($965), peanuts ($1,194) and rice ($1,336) remain the most expensive crops to produce on a per-acre basis. </p><p>Operating costs—expenses directly tied to producing a yearly crop, such as seed, fertilizer, chemicals, fuel and labor—substantially vary across crops. In 2025, total operating costs ranged from $155 per acre for wheat to more than $764 per acre for rice and $631 per acre for peanuts. In 2026, these costs are expected to rise, ranging from $774 per acre for rice and $160 per acre for wheat. While select inputs have moderated slightly from recent peaks, overall operating expenses remain well above pre-2021 levels. Rising costs since 2020 have been driven primarily by sharp increases in interest expenses (+71%), fertilizer (+37%), fuel and oil (+32%), labor (+47%), chemicals (+25%) and maintenance (+27%), alongside notable gains in seed (+18%) and marketing costs (+18%). </p></div><!-- /margin and padding --><div><!-- width wrap --><figure><a href="https://www.fb.org/imgz/Slide8.PNG" id="expandparent"><img src="https://www.fb.org/imgz/_w900/251949/Slide8.png"><svg id="expandchild" width="85" height="25" viewBox="0 0 85 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H84.5V24.1429H0V0Z" fill="black"></path><path d="M67.6801 6.90387C67.4835 6.7073 67.4835 6.38886 67.6801 6.1923L69.8512 4.0212C69.8748 3.99762 69.9003 3.97698 69.9278 3.9583C69.9406 3.94946 69.9544 3.94356 69.9672 3.93668C69.9829 3.92783 69.9976 3.91899 70.0143 3.91211C70.031 3.90523 70.0487 3.9013 70.0654 3.89638C70.0792 3.89245 70.093 3.88655 70.1077 3.88361C70.1726 3.87083 70.2394 3.87083 70.3052 3.88361C70.32 3.88655 70.3338 3.89245 70.3475 3.89638C70.3652 3.9013 70.3829 3.90523 70.3996 3.91211C70.4163 3.91899 70.4311 3.92783 70.4468 3.93668C70.4596 3.94356 70.4733 3.94946 70.4861 3.9583C70.5136 3.97698 70.5392 3.99762 70.5628 4.0212L72.7299 6.18838C72.9265 6.38494 72.9265 6.70339 72.7299 6.89995C72.5334 7.0965 72.2149 7.09652 72.0184 6.89995L70.7092 5.59081V9.76976C70.7092 10.0479 70.4841 10.273 70.206 10.273C69.9279 10.273 69.7028 10.0479 69.7028 9.76976L69.7047 5.59181L68.3927 6.9039C68.2944 7.00218 68.1656 7.05133 68.0369 7.05133C67.9081 7.05133 67.7794 7.00218 67.6811 6.9039L67.6801 6.90387ZM72.0193 16.4335L70.7072 17.7456V13.5666C70.7072 13.2885 70.4821 13.0634 70.204 13.0634C69.9259 13.0634 69.7008 13.2885 69.7008 13.5666V17.7456L68.3916 16.4365C68.1951 16.2399 67.8766 16.2399 67.6801 16.4365C67.4835 16.633 67.4835 16.9515 67.6801 17.148L69.8472 19.3152C69.8708 19.3388 69.8964 19.3594 69.9239 19.3781C69.9367 19.386 69.9495 19.3919 69.9632 19.3997C69.979 19.4086 69.9947 19.4174 70.0114 19.4253C70.0281 19.4322 70.0448 19.4361 70.0625 19.441C70.0772 19.4449 70.091 19.4508 70.1057 19.4538C70.1382 19.4607 70.1716 19.4636 70.204 19.4636C70.2374 19.4636 70.2699 19.4597 70.3023 19.4538C70.317 19.4508 70.3308 19.4449 70.3456 19.441C70.3623 19.4361 70.38 19.4322 70.3967 19.4253C70.4134 19.4184 70.4281 19.4086 70.4448 19.3997C70.4576 19.3928 70.4714 19.3869 70.4841 19.3781C70.5117 19.3594 70.5382 19.3388 70.5608 19.3152L72.7319 17.1441C72.9285 16.9475 72.9285 16.6291 72.7319 16.4325C72.5353 16.236 72.2169 16.236 72.0203 16.4325L72.0193 16.4335ZM77.9155 11.9499C77.9204 11.943 77.9224 11.9351 77.9273 11.9282C77.94 11.9076 77.9528 11.886 77.9617 11.8634C77.9656 11.8535 77.9676 11.8437 77.9705 11.8339C77.9774 11.8132 77.9853 11.7916 77.9902 11.77C77.9971 11.7376 78 11.7042 78 11.6707C78 11.6373 77.9961 11.6039 77.9902 11.5715C77.9853 11.5489 77.9784 11.5282 77.9705 11.5076C77.9676 11.4978 77.9656 11.4869 77.9617 11.4781C77.9518 11.4555 77.9391 11.4339 77.9273 11.4132C77.9233 11.4063 77.9204 11.3985 77.9155 11.3916C77.8968 11.3641 77.8762 11.3385 77.8526 11.3149L75.6815 9.14385C75.4849 8.94728 75.1665 8.94728 74.9699 9.14385C74.7733 9.34042 74.7733 9.65886 74.9699 9.85542L76.282 11.1675H72.103C71.8249 11.1675 71.5998 11.3926 71.5998 11.6707C71.5998 11.9489 71.8249 12.1739 72.103 12.1739H76.282L74.9728 13.4831C74.7763 13.6796 74.7763 13.9981 74.9728 14.1946C75.0711 14.2929 75.1999 14.3421 75.3286 14.3421C75.4574 14.3421 75.5861 14.2929 75.6844 14.1946L77.8516 12.0275C77.8752 12.0039 77.8958 11.9783 77.9145 11.9508L77.9155 11.9499ZM64.1281 12.17H68.3071C68.5852 12.17 68.8103 11.9449 68.8103 11.6668C68.8103 11.3887 68.5852 11.1636 68.3071 11.1636H64.1291L65.4383 9.85445C65.6348 9.65788 65.6348 9.33943 65.4383 9.14288C65.2417 8.94632 64.9233 8.94631 64.7267 9.14288L62.5595 11.3101C62.5359 11.3336 62.5153 11.3592 62.4966 11.3867C62.4917 11.3936 62.4898 11.4015 62.4848 11.4083C62.4721 11.429 62.4593 11.4506 62.4504 11.4732C62.4465 11.483 62.4445 11.4929 62.4416 11.5027C62.4347 11.5233 62.4269 11.545 62.4219 11.5666C62.4151 11.599 62.4121 11.6324 62.4121 11.6658C62.4121 11.6993 62.416 11.7327 62.4219 11.7651C62.4269 11.7877 62.4337 11.8084 62.4416 11.829C62.4445 11.8388 62.4465 11.8496 62.4504 11.8585C62.4603 11.8811 62.473 11.9027 62.4848 11.9233C62.4888 11.9302 62.4917 11.9381 62.4966 11.945C62.5153 11.9725 62.5359 11.998 62.5595 12.0216L64.7306 14.1927C64.8289 14.291 64.9577 14.3402 65.0864 14.3402C65.2152 14.3402 65.3439 14.291 65.4422 14.1927C65.6388 13.9962 65.6388 13.6777 65.4422 13.4812L64.1301 12.1691L64.1281 12.17Z" fill="white"></path><path d="M14.5178 15.8906V14.4928H10.3901V11.8421H14.5178V10.4442H10.3901V8.05725H14.5178V6.65938H8.75488V15.8906H14.5178Z" fill="white"></path><path d="M22.4709 9.09906H20.506L19.029 11.2486L17.5784 9.09906H15.5475L17.974 12.4619L15.3497 15.8906H17.3542L18.8971 13.6488L20.4137 15.8906H22.4577L19.9389 12.4487L22.4709 9.09906Z" fill="white"></path><path d="M23.562 9.09906V19.1875H25.1577V15.2049C25.6852 15.7851 26.4896 16.0489 27.2017 16.0489C29.1403 16.0489 30.4854 14.3741 30.4854 12.5146C30.4854 10.5233 29.0875 8.94081 27.2281 8.94081C26.4764 8.94081 25.5797 9.24413 25.1049 9.96944L25.0126 9.09906H23.562ZM25.1577 11.3937C25.5401 10.6552 26.2786 10.3387 26.9248 10.3387C28.1117 10.3387 28.8633 11.3673 28.8633 12.4751C28.8633 13.5169 28.2303 14.651 26.8984 14.651C26.2654 14.651 25.6192 14.3873 25.1577 13.7938V11.3937Z" fill="white"></path><path d="M37.4711 15.8906V11.7893C37.4711 10.0222 36.6403 8.94081 34.7149 8.94081C33.739 8.94081 32.6972 9.21775 31.8928 9.61338L32.328 10.8134C32.9082 10.5101 33.6599 10.22 34.4248 10.22C35.4666 10.22 35.8754 10.7739 35.8754 11.697V11.8553C35.3743 11.7498 34.9523 11.7102 34.5962 11.7102C33.1588 11.7102 31.6026 12.2377 31.6026 13.9257C31.6026 15.3499 32.7763 16.0489 34.0423 16.0489C34.8204 16.0489 35.4798 15.8379 35.9677 15.2576L36.0468 15.8906H37.4711ZM35.8754 14.1894C35.4138 14.5851 34.9259 14.862 34.3325 14.862C33.7126 14.862 33.172 14.5587 33.172 13.8729C33.172 12.9894 34.0819 12.752 34.8336 12.752C35.1369 12.752 35.4666 12.7916 35.8754 12.8839V14.1894Z" fill="white"></path><path d="M39.3895 9.09906V15.8906H40.9852V11.3805C41.3809 10.7475 42.0402 10.3387 42.6732 10.3387C43.1612 10.3387 43.5041 10.5101 43.7019 10.853C43.9524 11.275 43.9656 11.908 43.9656 12.4751V15.8906H45.5613V12.3564C45.5613 11.3673 45.4558 10.4706 44.9547 9.81119C44.5327 9.2705 43.8997 8.94081 43.0821 8.94081C42.2644 8.94081 41.46 9.32325 40.9193 10.1013L40.8402 9.09906H39.3895Z" fill="white"></path><path d="M53.8995 6H52.2906V9.798C51.7631 9.21775 50.9587 8.94081 50.2598 8.94081C48.308 8.94081 46.9761 10.6156 46.9761 12.4751C46.9761 14.4532 48.3608 16.0489 50.2334 16.0489C50.9851 16.0489 51.8686 15.7588 52.3566 15.0203L52.4357 15.8906H53.8995V6ZM52.2906 13.596C51.9082 14.3345 51.1697 14.651 50.5367 14.651C49.3366 14.651 48.5981 13.6224 48.5981 12.5146C48.5981 11.4728 49.2311 10.3387 50.5631 10.3387C51.1829 10.3387 51.8291 10.6024 52.2906 11.1959V13.596Z" fill="white"></path></svg></a></figure></div><!-- /margin --><div id="loop4"><p><strong>Losses Persist Even After FBA and ECAP </strong></p><p>Against this backdrop of elevated costs, <a href="https://www.fb.org/market-intel/declining-farm-economy-continues-to-pressure-profitability" target="_blank" rel="noreferrer noopener">commodity prices have remained under pressure</a>, limiting farmers’ ability  to cover  their costs through the marketplace alone. As a result, many farms are projected to experience losses for a fourth or fifth consecutive year, even after accounting for crop insurance indemnities and ad hoc assistance. </p></div><!-- /margin and padding --><div><!-- width wrap --><figure><a href="https://www.fb.org/imgz/Slide9.PNG" id="expandparent"><img src="https://www.fb.org/imgz/_w900/251952/Slide9.png"><svg id="expandchild" width="85" height="25" viewBox="0 0 85 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H84.5V24.1429H0V0Z" fill="black"></path><path d="M67.6801 6.90387C67.4835 6.7073 67.4835 6.38886 67.6801 6.1923L69.8512 4.0212C69.8748 3.99762 69.9003 3.97698 69.9278 3.9583C69.9406 3.94946 69.9544 3.94356 69.9672 3.93668C69.9829 3.92783 69.9976 3.91899 70.0143 3.91211C70.031 3.90523 70.0487 3.9013 70.0654 3.89638C70.0792 3.89245 70.093 3.88655 70.1077 3.88361C70.1726 3.87083 70.2394 3.87083 70.3052 3.88361C70.32 3.88655 70.3338 3.89245 70.3475 3.89638C70.3652 3.9013 70.3829 3.90523 70.3996 3.91211C70.4163 3.91899 70.4311 3.92783 70.4468 3.93668C70.4596 3.94356 70.4733 3.94946 70.4861 3.9583C70.5136 3.97698 70.5392 3.99762 70.5628 4.0212L72.7299 6.18838C72.9265 6.38494 72.9265 6.70339 72.7299 6.89995C72.5334 7.0965 72.2149 7.09652 72.0184 6.89995L70.7092 5.59081V9.76976C70.7092 10.0479 70.4841 10.273 70.206 10.273C69.9279 10.273 69.7028 10.0479 69.7028 9.76976L69.7047 5.59181L68.3927 6.9039C68.2944 7.00218 68.1656 7.05133 68.0369 7.05133C67.9081 7.05133 67.7794 7.00218 67.6811 6.9039L67.6801 6.90387ZM72.0193 16.4335L70.7072 17.7456V13.5666C70.7072 13.2885 70.4821 13.0634 70.204 13.0634C69.9259 13.0634 69.7008 13.2885 69.7008 13.5666V17.7456L68.3916 16.4365C68.1951 16.2399 67.8766 16.2399 67.6801 16.4365C67.4835 16.633 67.4835 16.9515 67.6801 17.148L69.8472 19.3152C69.8708 19.3388 69.8964 19.3594 69.9239 19.3781C69.9367 19.386 69.9495 19.3919 69.9632 19.3997C69.979 19.4086 69.9947 19.4174 70.0114 19.4253C70.0281 19.4322 70.0448 19.4361 70.0625 19.441C70.0772 19.4449 70.091 19.4508 70.1057 19.4538C70.1382 19.4607 70.1716 19.4636 70.204 19.4636C70.2374 19.4636 70.2699 19.4597 70.3023 19.4538C70.317 19.4508 70.3308 19.4449 70.3456 19.441C70.3623 19.4361 70.38 19.4322 70.3967 19.4253C70.4134 19.4184 70.4281 19.4086 70.4448 19.3997C70.4576 19.3928 70.4714 19.3869 70.4841 19.3781C70.5117 19.3594 70.5382 19.3388 70.5608 19.3152L72.7319 17.1441C72.9285 16.9475 72.9285 16.6291 72.7319 16.4325C72.5353 16.236 72.2169 16.236 72.0203 16.4325L72.0193 16.4335ZM77.9155 11.9499C77.9204 11.943 77.9224 11.9351 77.9273 11.9282C77.94 11.9076 77.9528 11.886 77.9617 11.8634C77.9656 11.8535 77.9676 11.8437 77.9705 11.8339C77.9774 11.8132 77.9853 11.7916 77.9902 11.77C77.9971 11.7376 78 11.7042 78 11.6707C78 11.6373 77.9961 11.6039 77.9902 11.5715C77.9853 11.5489 77.9784 11.5282 77.9705 11.5076C77.9676 11.4978 77.9656 11.4869 77.9617 11.4781C77.9518 11.4555 77.9391 11.4339 77.9273 11.4132C77.9233 11.4063 77.9204 11.3985 77.9155 11.3916C77.8968 11.3641 77.8762 11.3385 77.8526 11.3149L75.6815 9.14385C75.4849 8.94728 75.1665 8.94728 74.9699 9.14385C74.7733 9.34042 74.7733 9.65886 74.9699 9.85542L76.282 11.1675H72.103C71.8249 11.1675 71.5998 11.3926 71.5998 11.6707C71.5998 11.9489 71.8249 12.1739 72.103 12.1739H76.282L74.9728 13.4831C74.7763 13.6796 74.7763 13.9981 74.9728 14.1946C75.0711 14.2929 75.1999 14.3421 75.3286 14.3421C75.4574 14.3421 75.5861 14.2929 75.6844 14.1946L77.8516 12.0275C77.8752 12.0039 77.8958 11.9783 77.9145 11.9508L77.9155 11.9499ZM64.1281 12.17H68.3071C68.5852 12.17 68.8103 11.9449 68.8103 11.6668C68.8103 11.3887 68.5852 11.1636 68.3071 11.1636H64.1291L65.4383 9.85445C65.6348 9.65788 65.6348 9.33943 65.4383 9.14288C65.2417 8.94632 64.9233 8.94631 64.7267 9.14288L62.5595 11.3101C62.5359 11.3336 62.5153 11.3592 62.4966 11.3867C62.4917 11.3936 62.4898 11.4015 62.4848 11.4083C62.4721 11.429 62.4593 11.4506 62.4504 11.4732C62.4465 11.483 62.4445 11.4929 62.4416 11.5027C62.4347 11.5233 62.4269 11.545 62.4219 11.5666C62.4151 11.599 62.4121 11.6324 62.4121 11.6658C62.4121 11.6993 62.416 11.7327 62.4219 11.7651C62.4269 11.7877 62.4337 11.8084 62.4416 11.829C62.4445 11.8388 62.4465 11.8496 62.4504 11.8585C62.4603 11.8811 62.473 11.9027 62.4848 11.9233C62.4888 11.9302 62.4917 11.9381 62.4966 11.945C62.5153 11.9725 62.5359 11.998 62.5595 12.0216L64.7306 14.1927C64.8289 14.291 64.9577 14.3402 65.0864 14.3402C65.2152 14.3402 65.3439 14.291 65.4422 14.1927C65.6388 13.9962 65.6388 13.6777 65.4422 13.4812L64.1301 12.1691L64.1281 12.17Z" fill="white"></path><path d="M14.5178 15.8906V14.4928H10.3901V11.8421H14.5178V10.4442H10.3901V8.05725H14.5178V6.65938H8.75488V15.8906H14.5178Z" fill="white"></path><path d="M22.4709 9.09906H20.506L19.029 11.2486L17.5784 9.09906H15.5475L17.974 12.4619L15.3497 15.8906H17.3542L18.8971 13.6488L20.4137 15.8906H22.4577L19.9389 12.4487L22.4709 9.09906Z" fill="white"></path><path d="M23.562 9.09906V19.1875H25.1577V15.2049C25.6852 15.7851 26.4896 16.0489 27.2017 16.0489C29.1403 16.0489 30.4854 14.3741 30.4854 12.5146C30.4854 10.5233 29.0875 8.94081 27.2281 8.94081C26.4764 8.94081 25.5797 9.24413 25.1049 9.96944L25.0126 9.09906H23.562ZM25.1577 11.3937C25.5401 10.6552 26.2786 10.3387 26.9248 10.3387C28.1117 10.3387 28.8633 11.3673 28.8633 12.4751C28.8633 13.5169 28.2303 14.651 26.8984 14.651C26.2654 14.651 25.6192 14.3873 25.1577 13.7938V11.3937Z" fill="white"></path><path d="M37.4711 15.8906V11.7893C37.4711 10.0222 36.6403 8.94081 34.7149 8.94081C33.739 8.94081 32.6972 9.21775 31.8928 9.61338L32.328 10.8134C32.9082 10.5101 33.6599 10.22 34.4248 10.22C35.4666 10.22 35.8754 10.7739 35.8754 11.697V11.8553C35.3743 11.7498 34.9523 11.7102 34.5962 11.7102C33.1588 11.7102 31.6026 12.2377 31.6026 13.9257C31.6026 15.3499 32.7763 16.0489 34.0423 16.0489C34.8204 16.0489 35.4798 15.8379 35.9677 15.2576L36.0468 15.8906H37.4711ZM35.8754 14.1894C35.4138 14.5851 34.9259 14.862 34.3325 14.862C33.7126 14.862 33.172 14.5587 33.172 13.8729C33.172 12.9894 34.0819 12.752 34.8336 12.752C35.1369 12.752 35.4666 12.7916 35.8754 12.8839V14.1894Z" fill="white"></path><path d="M39.3895 9.09906V15.8906H40.9852V11.3805C41.3809 10.7475 42.0402 10.3387 42.6732 10.3387C43.1612 10.3387 43.5041 10.5101 43.7019 10.853C43.9524 11.275 43.9656 11.908 43.9656 12.4751V15.8906H45.5613V12.3564C45.5613 11.3673 45.4558 10.4706 44.9547 9.81119C44.5327 9.2705 43.8997 8.94081 43.0821 8.94081C42.2644 8.94081 41.46 9.32325 40.9193 10.1013L40.8402 9.09906H39.3895Z" fill="white"></path><path d="M53.8995 6H52.2906V9.798C51.7631 9.21775 50.9587 8.94081 50.2598 8.94081C48.308 8.94081 46.9761 10.6156 46.9761 12.4751C46.9761 14.4532 48.3608 16.0489 50.2334 16.0489C50.9851 16.0489 51.8686 15.7588 52.3566 15.0203L52.4357 15.8906H53.8995V6ZM52.2906 13.596C51.9082 14.3345 51.1697 14.651 50.5367 14.651C49.3366 14.651 48.5981 13.6224 48.5981 12.5146C48.5981 11.4728 49.2311 10.3387 50.5631 10.3387C51.1829 10.3387 51.8291 10.6024 52.2906 11.1959V13.596Z" fill="white"></path></svg></a></figure></div><!-- /margin --><p>The  <a href="https://www.fb.org/market-intel/farmer-bridge-assistance-program-details-on-11-billion-in-aid" target="_blank" rel="noreferrer noopener">Farmer Bridge Assistance (FBA) Program</a> and the <a href="https://www.fb.org/market-intel/emergency-commodity-assistance-program-ecap-what-you-need-to-know" target="_blank" rel="noreferrer noopener">Emergency Commodity Assistance Program (ECAP)</a> provide important near-term support. However, ECAP was designed to address 2023 and 2024 losses, rather than 2025 and later production challenges. For both programs, payments are calculated on a per-acre basis. However, when compared to current per-acre production costs and weak commodity prices, these payments generally cover only a share of losses rather than restore profitability. In fact, returns over total costs for all nine principal row crops are projected to remain negative on a per-acre basis even after accounting for federal assistance. Based on loss calculations used in the Farmer Bridge Assistance Program, rice producers face losses of roughly $210 per acre, followed by cotton ($202), oats ($159), peanuts ($131), sorghum ($91), corn ($87), wheat ($70), soybeans ($61) and barley ($42). In total, net losses across the sector are estimated to exceed $50 billion over the past three crop years.</p><!-- /margin and padding --><div><!-- width wrap --><figure><a href="https://www.fb.org/imgz/Slide13.PNG" id="expandparent"><img src="https://www.fb.org/imgz/_w900/251955/Slide13.png"><svg id="expandchild" width="85" height="25" viewBox="0 0 85 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H84.5V24.1429H0V0Z" fill="black"></path><path d="M67.6801 6.90387C67.4835 6.7073 67.4835 6.38886 67.6801 6.1923L69.8512 4.0212C69.8748 3.99762 69.9003 3.97698 69.9278 3.9583C69.9406 3.94946 69.9544 3.94356 69.9672 3.93668C69.9829 3.92783 69.9976 3.91899 70.0143 3.91211C70.031 3.90523 70.0487 3.9013 70.0654 3.89638C70.0792 3.89245 70.093 3.88655 70.1077 3.88361C70.1726 3.87083 70.2394 3.87083 70.3052 3.88361C70.32 3.88655 70.3338 3.89245 70.3475 3.89638C70.3652 3.9013 70.3829 3.90523 70.3996 3.91211C70.4163 3.91899 70.4311 3.92783 70.4468 3.93668C70.4596 3.94356 70.4733 3.94946 70.4861 3.9583C70.5136 3.97698 70.5392 3.99762 70.5628 4.0212L72.7299 6.18838C72.9265 6.38494 72.9265 6.70339 72.7299 6.89995C72.5334 7.0965 72.2149 7.09652 72.0184 6.89995L70.7092 5.59081V9.76976C70.7092 10.0479 70.4841 10.273 70.206 10.273C69.9279 10.273 69.7028 10.0479 69.7028 9.76976L69.7047 5.59181L68.3927 6.9039C68.2944 7.00218 68.1656 7.05133 68.0369 7.05133C67.9081 7.05133 67.7794 7.00218 67.6811 6.9039L67.6801 6.90387ZM72.0193 16.4335L70.7072 17.7456V13.5666C70.7072 13.2885 70.4821 13.0634 70.204 13.0634C69.9259 13.0634 69.7008 13.2885 69.7008 13.5666V17.7456L68.3916 16.4365C68.1951 16.2399 67.8766 16.2399 67.6801 16.4365C67.4835 16.633 67.4835 16.9515 67.6801 17.148L69.8472 19.3152C69.8708 19.3388 69.8964 19.3594 69.9239 19.3781C69.9367 19.386 69.9495 19.3919 69.9632 19.3997C69.979 19.4086 69.9947 19.4174 70.0114 19.4253C70.0281 19.4322 70.0448 19.4361 70.0625 19.441C70.0772 19.4449 70.091 19.4508 70.1057 19.4538C70.1382 19.4607 70.1716 19.4636 70.204 19.4636C70.2374 19.4636 70.2699 19.4597 70.3023 19.4538C70.317 19.4508 70.3308 19.4449 70.3456 19.441C70.3623 19.4361 70.38 19.4322 70.3967 19.4253C70.4134 19.4184 70.4281 19.4086 70.4448 19.3997C70.4576 19.3928 70.4714 19.3869 70.4841 19.3781C70.5117 19.3594 70.5382 19.3388 70.5608 19.3152L72.7319 17.1441C72.9285 16.9475 72.9285 16.6291 72.7319 16.4325C72.5353 16.236 72.2169 16.236 72.0203 16.4325L72.0193 16.4335ZM77.9155 11.9499C77.9204 11.943 77.9224 11.9351 77.9273 11.9282C77.94 11.9076 77.9528 11.886 77.9617 11.8634C77.9656 11.8535 77.9676 11.8437 77.9705 11.8339C77.9774 11.8132 77.9853 11.7916 77.9902 11.77C77.9971 11.7376 78 11.7042 78 11.6707C78 11.6373 77.9961 11.6039 77.9902 11.5715C77.9853 11.5489 77.9784 11.5282 77.9705 11.5076C77.9676 11.4978 77.9656 11.4869 77.9617 11.4781C77.9518 11.4555 77.9391 11.4339 77.9273 11.4132C77.9233 11.4063 77.9204 11.3985 77.9155 11.3916C77.8968 11.3641 77.8762 11.3385 77.8526 11.3149L75.6815 9.14385C75.4849 8.94728 75.1665 8.94728 74.9699 9.14385C74.7733 9.34042 74.7733 9.65886 74.9699 9.85542L76.282 11.1675H72.103C71.8249 11.1675 71.5998 11.3926 71.5998 11.6707C71.5998 11.9489 71.8249 12.1739 72.103 12.1739H76.282L74.9728 13.4831C74.7763 13.6796 74.7763 13.9981 74.9728 14.1946C75.0711 14.2929 75.1999 14.3421 75.3286 14.3421C75.4574 14.3421 75.5861 14.2929 75.6844 14.1946L77.8516 12.0275C77.8752 12.0039 77.8958 11.9783 77.9145 11.9508L77.9155 11.9499ZM64.1281 12.17H68.3071C68.5852 12.17 68.8103 11.9449 68.8103 11.6668C68.8103 11.3887 68.5852 11.1636 68.3071 11.1636H64.1291L65.4383 9.85445C65.6348 9.65788 65.6348 9.33943 65.4383 9.14288C65.2417 8.94632 64.9233 8.94631 64.7267 9.14288L62.5595 11.3101C62.5359 11.3336 62.5153 11.3592 62.4966 11.3867C62.4917 11.3936 62.4898 11.4015 62.4848 11.4083C62.4721 11.429 62.4593 11.4506 62.4504 11.4732C62.4465 11.483 62.4445 11.4929 62.4416 11.5027C62.4347 11.5233 62.4269 11.545 62.4219 11.5666C62.4151 11.599 62.4121 11.6324 62.4121 11.6658C62.4121 11.6993 62.416 11.7327 62.4219 11.7651C62.4269 11.7877 62.4337 11.8084 62.4416 11.829C62.4445 11.8388 62.4465 11.8496 62.4504 11.8585C62.4603 11.8811 62.473 11.9027 62.4848 11.9233C62.4888 11.9302 62.4917 11.9381 62.4966 11.945C62.5153 11.9725 62.5359 11.998 62.5595 12.0216L64.7306 14.1927C64.8289 14.291 64.9577 14.3402 65.0864 14.3402C65.2152 14.3402 65.3439 14.291 65.4422 14.1927C65.6388 13.9962 65.6388 13.6777 65.4422 13.4812L64.1301 12.1691L64.1281 12.17Z" fill="white"></path><path d="M14.5178 15.8906V14.4928H10.3901V11.8421H14.5178V10.4442H10.3901V8.05725H14.5178V6.65938H8.75488V15.8906H14.5178Z" fill="white"></path><path d="M22.4709 9.09906H20.506L19.029 11.2486L17.5784 9.09906H15.5475L17.974 12.4619L15.3497 15.8906H17.3542L18.8971 13.6488L20.4137 15.8906H22.4577L19.9389 12.4487L22.4709 9.09906Z" fill="white"></path><path d="M23.562 9.09906V19.1875H25.1577V15.2049C25.6852 15.7851 26.4896 16.0489 27.2017 16.0489C29.1403 16.0489 30.4854 14.3741 30.4854 12.5146C30.4854 10.5233 29.0875 8.94081 27.2281 8.94081C26.4764 8.94081 25.5797 9.24413 25.1049 9.96944L25.0126 9.09906H23.562ZM25.1577 11.3937C25.5401 10.6552 26.2786 10.3387 26.9248 10.3387C28.1117 10.3387 28.8633 11.3673 28.8633 12.4751C28.8633 13.5169 28.2303 14.651 26.8984 14.651C26.2654 14.651 25.6192 14.3873 25.1577 13.7938V11.3937Z" fill="white"></path><path d="M37.4711 15.8906V11.7893C37.4711 10.0222 36.6403 8.94081 34.7149 8.94081C33.739 8.94081 32.6972 9.21775 31.8928 9.61338L32.328 10.8134C32.9082 10.5101 33.6599 10.22 34.4248 10.22C35.4666 10.22 35.8754 10.7739 35.8754 11.697V11.8553C35.3743 11.7498 34.9523 11.7102 34.5962 11.7102C33.1588 11.7102 31.6026 12.2377 31.6026 13.9257C31.6026 15.3499 32.7763 16.0489 34.0423 16.0489C34.8204 16.0489 35.4798 15.8379 35.9677 15.2576L36.0468 15.8906H37.4711ZM35.8754 14.1894C35.4138 14.5851 34.9259 14.862 34.3325 14.862C33.7126 14.862 33.172 14.5587 33.172 13.8729C33.172 12.9894 34.0819 12.752 34.8336 12.752C35.1369 12.752 35.4666 12.7916 35.8754 12.8839V14.1894Z" fill="white"></path><path d="M39.3895 9.09906V15.8906H40.9852V11.3805C41.3809 10.7475 42.0402 10.3387 42.6732 10.3387C43.1612 10.3387 43.5041 10.5101 43.7019 10.853C43.9524 11.275 43.9656 11.908 43.9656 12.4751V15.8906H45.5613V12.3564C45.5613 11.3673 45.4558 10.4706 44.9547 9.81119C44.5327 9.2705 43.8997 8.94081 43.0821 8.94081C42.2644 8.94081 41.46 9.32325 40.9193 10.1013L40.8402 9.09906H39.3895Z" fill="white"></path><path d="M53.8995 6H52.2906V9.798C51.7631 9.21775 50.9587 8.94081 50.2598 8.94081C48.308 8.94081 46.9761 10.6156 46.9761 12.4751C46.9761 14.4532 48.3608 16.0489 50.2334 16.0489C50.9851 16.0489 51.8686 15.7588 52.3566 15.0203L52.4357 15.8906H53.8995V6ZM52.2906 13.596C51.9082 14.3345 51.1697 14.651 50.5367 14.651C49.3366 14.651 48.5981 13.6224 48.5981 12.5146C48.5981 11.4728 49.2311 10.3387 50.5631 10.3387C51.1829 10.3387 51.8291 10.6024 52.2906 11.1959V13.596Z" fill="white"></path></svg></a></figure></div><!-- /margin --><p>For many farms, aid helps slow the erosion of working capital but does not fully offset negative margins. As a result, producers continue to absorb multiyear losses that strain balance sheets, tighten cash flow and complicate access to operating credit. These loss estimates reflect national averages; actual costs of production and returns vary by region, management decisions and ownership structure. For example, producers who own their farmland may face lower total costs by avoiding cash rental expenses, resulting in higher returns.</p><!-- /margin and padding --><!-- /margin --><div id="loop10"><p><strong>Specialty Crops<br></strong><br>Additionally, neither the FBA program nor the ECAP <a href="https://www.fb.org/market-intel/specialty-crops-need-economic-aid-case-studies-almonds-apples-blueberries-lettuce-potatoes-and-strawberries" target="_blank" rel="noreferrer noopener">address losses in the specialty crops</a> market. The 2024 <a href="https://www.fb.org/market-intel/marketing-assistance-for-specialty-crops-a-closer-look" target="_blank" rel="noreferrer noopener">Marketing Assistance for Specialty Crop Program (MASC)</a> provided a first but limited relief step for growers and, for many, represented some of the first federal assistance tied to market challenges in the sector. Specialty crop growers continue to face deep and persistent economic losses driven by rising input costs, tightening margins, weather and disease disruptions, labor expenses and constraints, and global trade instability — challenges shared by field crop agriculture, including producers of crops beyond the nine principal crops, such as <a href="https://www.fb.org/market-intel/alfalfa-in-the-red-rising-costs-falling-returns" target="_blank" rel="noreferrer noopener">alfalfa</a> and sugar beets. Strengthening support for all sectors of agriculture is an economic necessity. Doing so will help maintain a resilient, accessible and diverse U.S. food system. </p><p><strong>Conclusion</strong></p><p>ERS cost projections make clear that input costs for all of the nine principal row crops remain elevated and sticky. Continued increases in both operating and overhead expenses are pushing breakeven prices higher, while commodity prices remain insufficient to offset those costs for many producers. </p></div><!-- /margin and padding --><div><!-- width wrap --><figure><a href="https://www.fb.org/imgz/mya_2026-01-21-143536_lsgv.png" id="expandparent"><img src=""><svg id="expandchild" width="85" height="25" viewBox="0 0 85 25" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 0H84.5V24.1429H0V0Z" fill="black"></path><path d="M67.6801 6.90387C67.4835 6.7073 67.4835 6.38886 67.6801 6.1923L69.8512 4.0212C69.8748 3.99762 69.9003 3.97698 69.9278 3.9583C69.9406 3.94946 69.9544 3.94356 69.9672 3.93668C69.9829 3.92783 69.9976 3.91899 70.0143 3.91211C70.031 3.90523 70.0487 3.9013 70.0654 3.89638C70.0792 3.89245 70.093 3.88655 70.1077 3.88361C70.1726 3.87083 70.2394 3.87083 70.3052 3.88361C70.32 3.88655 70.3338 3.89245 70.3475 3.89638C70.3652 3.9013 70.3829 3.90523 70.3996 3.91211C70.4163 3.91899 70.4311 3.92783 70.4468 3.93668C70.4596 3.94356 70.4733 3.94946 70.4861 3.9583C70.5136 3.97698 70.5392 3.99762 70.5628 4.0212L72.7299 6.18838C72.9265 6.38494 72.9265 6.70339 72.7299 6.89995C72.5334 7.0965 72.2149 7.09652 72.0184 6.89995L70.7092 5.59081V9.76976C70.7092 10.0479 70.4841 10.273 70.206 10.273C69.9279 10.273 69.7028 10.0479 69.7028 9.76976L69.7047 5.59181L68.3927 6.9039C68.2944 7.00218 68.1656 7.05133 68.0369 7.05133C67.9081 7.05133 67.7794 7.00218 67.6811 6.9039L67.6801 6.90387ZM72.0193 16.4335L70.7072 17.7456V13.5666C70.7072 13.2885 70.4821 13.0634 70.204 13.0634C69.9259 13.0634 69.7008 13.2885 69.7008 13.5666V17.7456L68.3916 16.4365C68.1951 16.2399 67.8766 16.2399 67.6801 16.4365C67.4835 16.633 67.4835 16.9515 67.6801 17.148L69.8472 19.3152C69.8708 19.3388 69.8964 19.3594 69.9239 19.3781C69.9367 19.386 69.9495 19.3919 69.9632 19.3997C69.979 19.4086 69.9947 19.4174 70.0114 19.4253C70.0281 19.4322 70.0448 19.4361 70.0625 19.441C70.0772 19.4449 70.091 19.4508 70.1057 19.4538C70.1382 19.4607 70.1716 19.4636 70.204 19.4636C70.2374 19.4636 70.2699 19.4597 70.3023 19.4538C70.317 19.4508 70.3308 19.4449 70.3456 19.441C70.3623 19.4361 70.38 19.4322 70.3967 19.4253C70.4134 19.4184 70.4281 19.4086 70.4448 19.3997C70.4576 19.3928 70.4714 19.3869 70.4841 19.3781C70.5117 19.3594 70.5382 19.3388 70.5608 19.3152L72.7319 17.1441C72.9285 16.9475 72.9285 16.6291 72.7319 16.4325C72.5353 16.236 72.2169 16.236 72.0203 16.4325L72.0193 16.4335ZM77.9155 11.9499C77.9204 11.943 77.9224 11.9351 77.9273 11.9282C77.94 11.9076 77.9528 11.886 77.9617 11.8634C77.9656 11.8535 77.9676 11.8437 77.9705 11.8339C77.9774 11.8132 77.9853 11.7916 77.9902 11.77C77.9971 11.7376 78 11.7042 78 11.6707C78 11.6373 77.9961 11.6039 77.9902 11.5715C77.9853 11.5489 77.9784 11.5282 77.9705 11.5076C77.9676 11.4978 77.9656 11.4869 77.9617 11.4781C77.9518 11.4555 77.9391 11.4339 77.9273 11.4132C77.9233 11.4063 77.9204 11.3985 77.9155 11.3916C77.8968 11.3641 77.8762 11.3385 77.8526 11.3149L75.6815 9.14385C75.4849 8.94728 75.1665 8.94728 74.9699 9.14385C74.7733 9.34042 74.7733 9.65886 74.9699 9.85542L76.282 11.1675H72.103C71.8249 11.1675 71.5998 11.3926 71.5998 11.6707C71.5998 11.9489 71.8249 12.1739 72.103 12.1739H76.282L74.9728 13.4831C74.7763 13.6796 74.7763 13.9981 74.9728 14.1946C75.0711 14.2929 75.1999 14.3421 75.3286 14.3421C75.4574 14.3421 75.5861 14.2929 75.6844 14.1946L77.8516 12.0275C77.8752 12.0039 77.8958 11.9783 77.9145 11.9508L77.9155 11.9499ZM64.1281 12.17H68.3071C68.5852 12.17 68.8103 11.9449 68.8103 11.6668C68.8103 11.3887 68.5852 11.1636 68.3071 11.1636H64.1291L65.4383 9.85445C65.6348 9.65788 65.6348 9.33943 65.4383 9.14288C65.2417 8.94632 64.9233 8.94631 64.7267 9.14288L62.5595 11.3101C62.5359 11.3336 62.5153 11.3592 62.4966 11.3867C62.4917 11.3936 62.4898 11.4015 62.4848 11.4083C62.4721 11.429 62.4593 11.4506 62.4504 11.4732C62.4465 11.483 62.4445 11.4929 62.4416 11.5027C62.4347 11.5233 62.4269 11.545 62.4219 11.5666C62.4151 11.599 62.4121 11.6324 62.4121 11.6658C62.4121 11.6993 62.416 11.7327 62.4219 11.7651C62.4269 11.7877 62.4337 11.8084 62.4416 11.829C62.4445 11.8388 62.4465 11.8496 62.4504 11.8585C62.4603 11.8811 62.473 11.9027 62.4848 11.9233C62.4888 11.9302 62.4917 11.9381 62.4966 11.945C62.5153 11.9725 62.5359 11.998 62.5595 12.0216L64.7306 14.1927C64.8289 14.291 64.9577 14.3402 65.0864 14.3402C65.2152 14.3402 65.3439 14.291 65.4422 14.1927C65.6388 13.9962 65.6388 13.6777 65.4422 13.4812L64.1301 12.1691L64.1281 12.17Z" fill="white"></path><path d="M14.5178 15.8906V14.4928H10.3901V11.8421H14.5178V10.4442H10.3901V8.05725H14.5178V6.65938H8.75488V15.8906H14.5178Z" fill="white"></path><path d="M22.4709 9.09906H20.506L19.029 11.2486L17.5784 9.09906H15.5475L17.974 12.4619L15.3497 15.8906H17.3542L18.8971 13.6488L20.4137 15.8906H22.4577L19.9389 12.4487L22.4709 9.09906Z" fill="white"></path><path d="M23.562 9.09906V19.1875H25.1577V15.2049C25.6852 15.7851 26.4896 16.0489 27.2017 16.0489C29.1403 16.0489 30.4854 14.3741 30.4854 12.5146C30.4854 10.5233 29.0875 8.94081 27.2281 8.94081C26.4764 8.94081 25.5797 9.24413 25.1049 9.96944L25.0126 9.09906H23.562ZM25.1577 11.3937C25.5401 10.6552 26.2786 10.3387 26.9248 10.3387C28.1117 10.3387 28.8633 11.3673 28.8633 12.4751C28.8633 13.5169 28.2303 14.651 26.8984 14.651C26.2654 14.651 25.6192 14.3873 25.1577 13.7938V11.3937Z" fill="white"></path><path d="M37.4711 15.8906V11.7893C37.4711 10.0222 36.6403 8.94081 34.7149 8.94081C33.739 8.94081 32.6972 9.21775 31.8928 9.61338L32.328 10.8134C32.9082 10.5101 33.6599 10.22 34.4248 10.22C35.4666 10.22 35.8754 10.7739 35.8754 11.697V11.8553C35.3743 11.7498 34.9523 11.7102 34.5962 11.7102C33.1588 11.7102 31.6026 12.2377 31.6026 13.9257C31.6026 15.3499 32.7763 16.0489 34.0423 16.0489C34.8204 16.0489 35.4798 15.8379 35.9677 15.2576L36.0468 15.8906H37.4711ZM35.8754 14.1894C35.4138 14.5851 34.9259 14.862 34.3325 14.862C33.7126 14.862 33.172 14.5587 33.172 13.8729C33.172 12.9894 34.0819 12.752 34.8336 12.752C35.1369 12.752 35.4666 12.7916 35.8754 12.8839V14.1894Z" fill="white"></path><path d="M39.3895 9.09906V15.8906H40.9852V11.3805C41.3809 10.7475 42.0402 10.3387 42.6732 10.3387C43.1612 10.3387 43.5041 10.5101 43.7019 10.853C43.9524 11.275 43.9656 11.908 43.9656 12.4751V15.8906H45.5613V12.3564C45.5613 11.3673 45.4558 10.4706 44.9547 9.81119C44.5327 9.2705 43.8997 8.94081 43.0821 8.94081C42.2644 8.94081 41.46 9.32325 40.9193 10.1013L40.8402 9.09906H39.3895Z" fill="white"></path><path d="M53.8995 6H52.2906V9.798C51.7631 9.21775 50.9587 8.94081 50.2598 8.94081C48.308 8.94081 46.9761 10.6156 46.9761 12.4751C46.9761 14.4532 48.3608 16.0489 50.2334 16.0489C50.9851 16.0489 51.8686 15.7588 52.3566 15.0203L52.4357 15.8906H53.8995V6ZM52.2906 13.596C51.9082 14.3345 51.1697 14.651 50.5367 14.651C49.3366 14.651 48.5981 13.6224 48.5981 12.5146C48.5981 11.4728 49.2311 10.3387 50.5631 10.3387C51.1829 10.3387 51.8291 10.6024 52.2906 11.1959V13.596Z" fill="white"></path></svg></a></figure></div><!-- /margin --><div id="loop12"><p>While FBA and ECAP payments are an important and welcome step in addressing near-term financial stress, they do not fully close the gap between costs and returns. As farmers enter the 2026/27 marketing year, accumulated losses — estimated to exceed $50 billion across the sector over the past three crop years — continue to weigh on farm finances. </p><p>These estimates reflect national average conditions and are calculated ahead of the growing season, before producers make final planting, input and marketing decisions. In practice, farmers respond to market signals by adjusting crop mix, input use and risk management strategies as conditions evolve. While outcomes vary widely by region and operation, persistently elevated breakeven prices underscore the importance of market-driven solutions that strengthen domestic demand — such as year-round access to E15 — to help support commodity prices and improve farm margins. </p><p>Much-needed safety net enhancements <a href="https://www.fb.org/market-intel/one-big-beautiful-bill-act-final-agricultural-provisions" target="_blank" rel="noreferrer noopener">through the One Big Beautiful Bill Act (OBBBA)</a> are expected to take effect in October 2026, but those changes do not address the pressures farmers face today. In <a href="https://www.fb.org/news-release/agricultural-groups-sound-alarm-about-farmers-future" target="_blank" rel="noreferrer noopener">a recent letter to Congress</a> organized by the American Farm Bureau Federation and signed by 56 agricultural organizations, farm groups warned of an economic crisis in rural America, citing multiyear losses driven by record-high input costs and historically low commodity prices. Congressional leaders from both parties have acknowledged the severity of these losses and the need for additional aid to stabilize farm finances. Until longer-term policy improvements take hold, many operations remain caught between high operating costs and low commodity prices, underscoring the ongoing financial strain facing U.S. agriculture as producers weigh whether they can afford to plant another crop. </p></div><!-- /margin and padding -->




  
  







    

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Internet voting is insecure and should not be used in public elections (427 pts)]]></title>
            <link>https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/</link>
            <guid>46713924</guid>
            <pubDate>Thu, 22 Jan 2026 01:11:13 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/">https://blog.citp.princeton.edu/2026/01/16/internet-voting-is-insecure-and-should-not-be-used-in-public-elections/</a>, See on <a href="https://news.ycombinator.com/item?id=46713924">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p><em>Signed by a group of 21 computer scientists expert in election security</em></p>



<h2>Executive summary</h2>



<p>Scientists have understood for many years that internet voting is insecure and that there is no known or foreseeable technology that can make it secure.&nbsp;Still, vendors of internet voting keep claiming that, somehow, their new system is different, or the insecurity doesn’t matter.&nbsp;Bradley Tusk and his Mobile Voting Foundation keep touting internet voting to journalists and election administrators; this whole effort is misleading and dangerous.</p>



<p><strong>Part I. </strong>&nbsp;All internet voting systems are insecure. The insecurity is worse than a well-run conventional paper ballot system, because a very small number of people may have the power to change any (or all) votes that go through the system, without detection. This insecurity has been known for years; every internet voting system yet proposed suffers from it, for basic reasons that cannot be fixed with existing technology.</p>



<p><strong>Part II.&nbsp; </strong>Internet voting systems known as “End-to-End Verifiable Internet Voting” are also insecure, in their own special ways.</p>



<p><strong>Part III.</strong>&nbsp; Recently, Tusk announced an E2E-VIV system called “VoteSecure.”&nbsp; It suffers from all the same insecurities.&nbsp; Even its developers admit that in their development documents.&nbsp; Furthermore, VoteSecure isn’t a complete, usable product, it’s just a “cryptographic core” that someone might someday incorporate into a usable product.</p>



<p><strong>Conclusion. </strong>&nbsp;Recent announcements by Bradley Tusks’s Mobile Voting Foundation suggest that the development of VoteSecure somehow makes internet voting safe and appropriate for use in public elections.&nbsp; This is untrue and dangerous.&nbsp; All deployed Internet voting systems are unsafe, VoteSecure is unsafe and isn’t even a deployed voting&nbsp; system, and there is no known (or foreseeable) technology that can make Internet voting safe.</p>



<h2>Part I.&nbsp; All internet voting systems are insecure</h2>



<p>Internet voting systems (including vote-by-smartphone) have three very serious weaknesses:</p>



<ol>
<li>Malware on the voter’s phone (or computer) can transmit different votes than the voter selected and reviewed.&nbsp;Voters use a variety of devices (Android, iPhone, Windows, Mac) which are constantly being attacked by malware.<br></li>



<li>Malware (or insiders) at the server can change votes.&nbsp;Internet servers are constantly being hacked from all over the world, often with serious results.<br></li>



<li>Malware at the county election office can change votes (in those systems where the internet ballots are printed in the county office for scanning).&nbsp;County election computers are not more secure than other government or commercial servers, which are regularly hacked with disastrous results.&nbsp;</li>
</ol>



<p>Although conventional ballots (marked on paper with a pen) are not perfectly secure either, the problem with internet ballots is the ability for a single attacker (from anywhere in the world) to alter a very large number of ballots with a single scaled-up attack.&nbsp; That’s much harder to do with hand-marked paper ballots; occasionally people try large-scale absentee ballot fraud, typically resulting in their being caught, prosecuted, and convicted.</p>



<h2>Part II.&nbsp; E2E-VIV internet voting systems are also insecure</h2>



<p>Years ago, the concept of “End-to-End Verifiable Internet Voting” (E2E-VIV) was proposed, which was supposed to remedy some of these weaknesses by allowing voters to check that their vote was recorded and counted correctly.&nbsp; Unfortunately, all E2E-VIV systems suffer from one or more of the following weaknesses:</p>



<ol>
<li>Voters must rely on a computer app to do the checking, and the checking app (if infected by malware) could lie to them.<br></li>



<li>Voters should not be able to prove to anyone else how they voted – the technical term is “receipt-free” – otherwise an attacker could build an automated system of mass vote-buying via the internet.&nbsp;But receipt-free E2E-VIV systems are complicated and counterintuitive for people to use.<br></li>



<li>It’s difficult to make an E2E-VIV checking app that’s both trustworthy and receipt-free.&nbsp;The best solutions known allow checking only of votes that will be discarded, and casting of votes that haven’t been checked; this is highly counterintuitive for most voters!&nbsp;<br></li>



<li>The checking app must be separate from the voting app, otherwise it doesn’t add any malware-resistance at all.&nbsp; But human nature being what it is, only a tiny fraction of voters will do the extra steps to run the checking protocol.&nbsp; If hardly anyone uses the checker, then the checker is largely ineffective.<br></li>



<li>Even if some voters do run the checking app, if those voters detect that the system is cheating (which is the purpose of the checking app), there’s no way the voters can prove that to election officials.&nbsp; That is, there is no “dispute resolution” protocol that could effectively work.</li>
</ol>



<p>Thus, the problem with all known E2E-VIV systems proposed to date is that the “verification” part doesn’t add any useful security: if a few percent of voters use the checking protocol and see that the system is sometimes cheating, the system can still steal the votes of all the voters that don’t use the checking protocol.&nbsp;And you might think, “well, if some voters catch the system cheating, then election administrators can take appropriate action”, but no appropriate action is possible: the election administrator can’t cancel the election just because a few voters claim (without proof) that the system is cheating!&nbsp; That’s what it means to have no dispute resolution protocol.</p>



<p>All of this is well understood in the scientific consensus.&nbsp;The insecurity of non-E2E-VIV systems has been documented for decades.&nbsp; For a survey of those results, see “<a href="https://scholars.unh.edu/unh_lr/vol21/iss2/9/">Is Internet Voting Trustworthy? The Science and the Policy Battles</a>”.&nbsp;The lack of dispute resolution in E2E-VIV systems has been <a href="http://www2.seas.gwu.edu/~poorvi/Audiotegrity.pdf">known for many years as well</a>.</p>



<h2>Part III. VoteSecure is insecure</h2>



<p>Bradley Tusk’s <a href="https://www.mobilevoting.org/">Mobile Voting Foundation</a> contracted with the R&amp;D company <a href="https://freeandfair.us/about/">Free and Fair</a> to develop internet voting software.&nbsp;Their <a href="https://www.prnewswire.com/news-releases/the-mobile-voting-foundation-and-free--fair-release-votesecure-the-first-software-development-kit-for-secure-transparent-and-verifiable-mobile-voting-302615896.html">press release of November 14, 2025</a> announced the release of an <a href="https://github.com/FreeAndFair/VoteSecure">open-source “Software Development Kit”</a> and claimed “This technology milestone means that secure and verifiable mobile voting is within reach.”&nbsp;&nbsp;</p>



<p>After <a href="https://github.com/FreeAndFair/VoteSecure/issues/2#issue-3629697747">some computer scientists examined</a> the open-source VoteSecure and <a href="https://blog.citp.princeton.edu/2025/12/16/mobile-voting-projects-vote-by-smartphone-has-real-security-gaps/">described serious flaws in its security</a>, Dr. Joe Kiniry and Dr. Daniel Zimmerman of Free and Fair responded.&nbsp;They say, in effect, that all the critiques are accurate, but they don’t know a way to do any better: “<a href="https://github.com/FreeAndFair/VoteSecure/issues/5#issuecomment-3721078472">We share many of [the critique’s] core goals, including voter confidence, election integrity, and resistance to coercion. Where we differ is not so much in values as in assumptions about what is achievable—and meaningful—in unsupervised voting environments.</a>”</p>



<p>In particular,&nbsp;</p>



<ul>
<li>“<a href="https://github.com/FreeAndFair/VoteSecure/issues/2#issuecomment-3609937648">We make no claim of receipt-freeness.</a>”</li>



<li>“<a href="https://github.com/FreeAndFair/VoteSecure/issues/2#issuecomment-3720432505">Of course, it may be possible for the voter to extract the randomizers from the voting client</a>,” meaning that voters would be able to prove how they voted, for example to someone on the internet who wanted to purchase votes at scale.</li>



<li>“<a href="https://github.com/FreeAndFair/VoteSecure/issues/5#issuecomment-3721078472">We agree that dispute resolution is essential to any <em>complete</em> voting system. We also agree that VoteSecure does not fully specify such a protocol</a>.”&nbsp; But really, the problem is much worse than this admission suggests.&nbsp; No one knows of a protocol that could possibly work.&nbsp; So it’s not a matter of dotting some i’s and crossing some t’s in their specification; it’s a gaping hole (an unsolved, research-level problem).</li>



<li>“​​<a href="https://github.com/FreeAndFair/VoteSecure/issues/5#issuecomment-3721078472">Critique: Malware on the voter’s device can compromise both voting and checking, rendering verification meaningless.&nbsp; Response: This critique is correct—and universal. There is no known technical solution that can fully protect an unsupervised endpoint from a sufficiently capable adversary.</a>”</li>



<li>“<a href="https://github.com/FreeAndFair/VoteSecure/issues/5#issuecomment-3721078472">VoteSecure does not claim to: Advance the state of the art in cryptographic voting protocols beyond existing E2E-VIV research; Eliminate coercion or vote selling in unsupervised elections; [or] Fully specify election administration, dispute resolution, or deployment processes.&nbsp; What VoteSecure aims to do is: Clearly define its threat model . . .</a>”</li>
</ul>







<details><summary>In addition to the previously described flaws in the VoteSecure protocol, we note that its vote checking system is susceptible to mass automated vote-buying attacks<sup>1</sup>; and we have discovered a new flaw in the VoteSecure protocol that allows votes to be stolen<sup>2</sup>.  <em>[click for details]</em></summary>
<details><summary>[1] This conclusion is based on a technical analysis.&nbsp; In the VoteSecure protocol, checking app can be run on a vote that is then cast; the checking app must be runnable on an alternate device than the voting app; that alternate device is likely a PC on which the user has control of installed software; user-installed software can extract decrypted randomizers; this allows the voter to participate in a mass vote-buying scheme.  [2] “<a href="https://github.com/FreeAndFair/VoteSecure/issues/6">Clash attacks on the VoteSecure voting and verification process</a>”, by Vanessa Teague and Olivier Pereira, January 13, 2026.</summary>

</details>
</details>



<p>Based on our own expertise test, and especially in light of the response from Free and Fair, we stand by the original analysis: <a href="https://blog.citp.princeton.edu/2025/12/16/mobile-voting-projects-vote-by-smartphone-has-real-security-gaps/">Mobile Voting Project’s vote-by-smartphone has critical security gaps</a>.</p>



<h2>Conclusion</h2>



<p>It has been the scientific consensus for decades that internet voting is not securable by any known technology.&nbsp;Research on future technologies is certainly worth doing.&nbsp;However, the decades of work on E2E-VIV systems has yet to produce any solution, or even any hope of a solution, to the fundamental problems.</p>



<p>Therefore, when it comes to internet voting systems, election officials and journalists should be especially wary of&nbsp;“science by press release.”&nbsp;Perhaps some day an internet voting solution will be proposed that can stand up to scientific investigation.&nbsp;The most reliable venue for assessing that is in peer-reviewed scientific articles.&nbsp;Reputable cybersecurity conferences and journals have published a lot of good science in this area.&nbsp;Press releases are not a reliable way to assess the trustworthiness of election systems.</p>



<h2>Signed</h2>



<p><em>(affiliations for for identification only and do not indicate institutional endorsement)</em></p>



<p><strong>Andrew W. Appel</strong>, <em>Eugene Higgins Professor Emeritus of Computer Science, Princeton University</em></p>



<p><strong>Steven M. Bellovin</strong>, <em>Percy K. and Vida L.W. Hudson Professor Emeritus of Computer Science, Columbia University</em></p>



<p><strong>Duncan Buell</strong>, <em>Chair Emeritus — NCR Chair in Computer Science and Engineering, University of South Carolina</em></p>



<p><strong>Braden L. Crimmins</strong>, <em>PhD Student, Univ. of Michigan School of Engineering &amp; Knight-Hennessy Scholar, Stanford Law</em></p>



<p><strong>Richard DeMillo</strong>, <em>Charlotte B and Roger C&nbsp; Warren Chair in Computing, Georgia Tech&nbsp;</em></p>



<p><strong>David L. Dill</strong>, <em>Donald E. Knuth Professor, Emeritus, in the School of Engineering, Stanford University</em></p>



<p><strong>Jeremy Epstein, </strong><em>National Science Foundation (retired) and&nbsp;Georgia Institute of Technology</em></p>



<p><strong>Juan E. Gilbert</strong>,&nbsp; <em>Andrew Banks Family Preeminence Endowed Professor, Computer &amp; Information Science, University of Florida</em></p>



<p><strong>J. Alex Halderman</strong>, <em>Bredt Family Professor of Computer Science &amp; Engineering, University of Michigan</em></p>



<p><strong>David Jefferson</strong>, <em>Lawrence Livermore National Laboratory (retired)</em></p>



<p><strong>Douglas W. Jones</strong>, <em>Emeritus Associate Professor of Computer Science, University of Iowa</em></p>



<p><strong>Daniel Lopresti</strong>, <em>Professor of Computer Science and Engineering, Lehigh University</em></p>



<p><strong>Ronald L. Rivest</strong>, <em>Institute Professor, MIT</em></p>



<p><strong>Bruce Schneier</strong>, <em>Fellow and Lecturer at the Harvard Kennedy School, and at the Munk School at the University of Toronto</em></p>



<p><strong>Kevin Skoglund</strong>,<em> President and Chief Technologist, Citizens for Better Elections</em></p>



<p><strong>Barbara Simons</strong>, <em>IBM Research (retired)</em></p>



<p><strong>Michael A. Specter</strong>, <em>Assistant Professor, Georgia Tech</em></p>



<p><strong>Philip B. Stark</strong>,&nbsp; <em>Distinguished Professor,&nbsp; Department of Statistics, University of California</em></p>



<p><strong>Gary Tan</strong>, <em>Professor of Computer Science &amp; Engineering, The Pennsylvania State University</em></p>



<p><strong>Vanessa Teague</strong>, <em>Thinking Cybersecurity Pty Ltd and the Australian National Universit</em>y</p>



<p><strong>Poorvi L. Vora</strong>, <em>Professor of Computer Science, George Washington University</em></p>
















</div></div>]]></description>
        </item>
    </channel>
</rss>