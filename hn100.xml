<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 05 Sep 2025 06:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Fil's Unbelievable Garbage Collector (255 pts)]]></title>
            <link>https://fil-c.org/fugc</link>
            <guid>45133938</guid>
            <pubDate>Fri, 05 Sep 2025 00:55:52 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://fil-c.org/fugc">https://fil-c.org/fugc</a>, See on <a href="https://news.ycombinator.com/item?id=45133938">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        
        <main>
<h2>Fil's Unbelievable Garbage Collector</h2>

<p>Fil-C uses a <em>parallel concurrent on-the-fly grey-stack Dijkstra accurate non-moving</em> garbage collector called FUGC (Fil's Unbelievable Garbage Collector). You can find the source code for the collector itself in <a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/fugc.c">fugc.c</a>, though be warned, that code cannot possibly work without lots of support logic in the rest of the runtime and in the compiler.</p>

<p>Let's break down FUGC's features:</p>

<ul>
<li><p>Parallel: marking and sweeping happen in multiple threads, in parallel. The more cores you have, the
faster the collector finishes.</p></li>
<li><p>Concurrent: marking and sweeping happen on some threads other than the <em>mutator</em> threads (i.e. your
program's threads). Mutator threads don't have to stop and wait for the collector. The interaction
between the collector thread and mutator threads is mostly non-blocking (locking is only used on
allocation slow paths).</p></li>
<li><p>On-the-fly: there is no global stop-the-world, but instead we use
"soft handshakes" (aka "ragged safepoints"). This means that the GC may ask threads to do some work (like scan stack), but threads do this
asynchronously, on their own time, without waiting for the collector or other threads. The only "pause"
threads experience is the callback executed in response to the soft handshake, which does work bounded
by that thread's stack height. That "pause" is usually shorter than the slowest path you might take
through a typical <code>malloc</code> implementation.</p></li>
<li><p>Grey-stack: the collector assumes it must rescan thread stacks to fixpoint. That is, GC starts with
a soft handshake to scan stack, and then marks in a loop. If this
loop runs out of work, then FUGC does another soft handshake. If that reveals more objects, then
concurrent marking resumes. This prevents us from having a <em>load barrier</em> (no instrumentation runs
when loading a pointer from the heap into a local variable). Only a <em>store barrier</em> is
necessary, and that barrier is very simple. This fixpoint converges super quickly because all newly
allocated objects during GC are pre-marked.</p></li>
<li><p>Dijkstra: storing a pointer field in an object that's in the heap or in a global variable while FUGC
is in its marking phase causes the newly pointed-to object to get marked. This is called a <em>Dijkstra
barrier</em> and it is a kind of <em>store barrier</em>. Due to the grey stack, there is no load barrier like
in the <a href="https://lamport.azurewebsites.net/pubs/garbage.pdf">classic Dijkstra collector</a>. The FUGC store
barrier uses a compare-and-swap with relaxed memory ordering on the slowest path (if the GC is running
and the object being stored was not already marked).</p></li>
<li><p>Accurate: the GC accurately (aka precisely, aka exactly) finds all pointers to objects, nothing more,
nothing less. <code>llvm::FilPizlonator</code> ensures that the runtime always knows where the root pointers are
on the stack and in globals. The Fil-C runtime has a clever API and Ruby code generator for tracking
pointers in low-level code that interacts with pizlonated code. All objects know where their outgoing
pointers are - they can only be in the <a href="https://fil-c.org/invisicaps.html">InvisiCap</a> auxiliary allocation.</p></li>
<li><p>Non-moving: the GC doesn't move objects. This makes concurrency easy to implement and avoids
a lot of synchronization between mutator and collector. However, FUGC will "move" pointers to free
objects (it will repoint the <a href="https://fil-c.org/invisicaps.html">capability</a> pointer to the free singleton so it doesn't have to mark the
freed allocation).</p></li>
</ul>

<p>This makes FUGC an <em>advancing wavefront</em> garbage collector. Advancing wavefront means that the
mutator cannot create new work for the collector by modifying the heap. Once an
object is marked, it'll stay marked for that GC cycle. It's also an <em>incremental update</em> collector, since
some objects that would have been live at the start of GC might get freed if they become free during the
collection cycle.</p>

<p>FUGC relies on <em>safepoints</em>, which comprise:</p>

<ul>
<li><p><em>Pollchecks</em> emitted by the compiler. The <code>llvm::FilPizlonator</code> compiler pass emits pollchecks often enough that only a
bounded amount of progress is possible before a pollcheck happens. The fast path of a pollcheck is
just a load-and-branch. The slow path runs a <em>pollcheck callback</em>, which does work for FUGC.</p></li>
<li><p>Soft handshakes, which request that a pollcheck callback is run on all threads and then waits for
this to happen.</p></li>
<li><p><em>Enter</em>/<em>exit</em> functionality. This is for allowing threads to block in syscalls or long-running
runtime functions without executing pollchecks. Threads that are in the <em>exited</em> state will have
pollcheck callbacks executed by the collector itself (when it does the soft handshake). The only
way for a Fil-C program to block is either by looping while entered (which means executing a
pollcheck at least once per loop iteration, often more) or by calling into the runtime and then
exiting.</p></li>
</ul>

<p>Safepointing is essential for supporting threading (Fil-C supports pthreads just fine) while avoiding
a large class of race conditions. For example, safepointing means that it's safe to load a pointer from
the heap and then use it; the GC cannot possibly delete that memory until the next pollcheck or exit.
So, the compiler and runtime just have to ensure that the pointer becomes tracked for stack scanning at
some point between when it's loaded and when the next pollcheck/exit happens, and only if the pointer is
still live at that point.</p>

<p>The safepointing functionality also supports <em>stop-the-world</em>, which is currently used to implement
<code>fork(2)</code> and for debugging FUGC (if you set the <code>FUGC_STW</code> environment variable to <code>1</code> then the
collector will stop the world and this is useful for triaging GC bugs; if the bug reproduces in STW
then it means it's not due to issues with the store barrier). The safepoint infrastructure also allows
safe signal delivery; Fil-C makes it possible to use signal handling in a practical way. Safepointing is
a common feature of virtual machines that support multiple threads and accurate garbage collection,
though usually, they are only used to stop the world rather than to request asynchronous activity from all
threads. See <a href="https://foojay.io/today/the-inner-workings-of-safepoints/">here</a> for a write-up about
how OpenJDK does it. The Fil-C implementation is in <a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/filc_runtime.c"><code>filc_runtime.c</code></a>.</p>

<p>Here's the basic flow of the FUGC collector loop:</p>

<ol>
<li>Wait for the GC trigger.</li>
<li>Turn on the store barrier, then soft handshake with a no-op callback.</li>
<li>Turn on black allocation (new objects get allocated marked), then soft handshake with a callback
that resets thread-local caches.</li>
<li>Mark global roots.</li>
<li>Soft handshake with a callback that requests stack scan and another reset of thread-local caches.
If all collector mark stacks are empty after this, go to step 7.</li>
<li>Tracing: for each object in the mark stack, mark its outgoing references (which may grow the mark
stack). Do this until the mark stack is empty. Then go to step 5.</li>
<li>Turn off the store barrier and prepare for sweeping, then soft handshake to reset thread-local
caches again.</li>
<li>Perform the sweep. During the sweep, objects are allocated black if they happen to be allocated out
of not-yet-swept pages, or white if they are allocated out of alraedy-swept pages.</li>
<li>Victory! Go back to step 1.</li>
</ol>

<p>If you're familiar with the literature, FUGC is sort of like the DLG (Doligez-Leroy-Gonthier) collector
(published in <a href="https://xavierleroy.org/publi/concurrent-gc.pdf">two</a>
<a href="http://moscova.inria.fr/~doligez/publications/doligez-gonthier-popl-1994.pdf">papers</a> because they
had a serious bug in the first one), except it uses the Dijkstra barrier and a grey stack, which
simplifies everything but isn't as academically pure (FUGC fixpoints, theirs doesn't). I first came
up with the grey-stack Dijkstra approach when working on
<a href="http://www.filpizlo.com/papers/pizlo-eurosys2010-fijivm.pdf">Fiji VM</a>'s CMR and
<a href="http://www.filpizlo.com/papers/pizlo-pldi2010-schism.pdf">Schism</a> garbage collectors. The main
advantage of FUGC over DLG is that it has a simpler (cheaper) store barrier and it's a slightly more
intuitive algorithm. While the fixpoint seems like a disadvantage, in practice it converges after a few
iterations.</p>

<p>Additionally, FUGC relies on a sweeping algorithm based on bitvector SIMD. This makes sweeping insanely
fast compared to marking. This is made thanks to the
<a href="https://github.com/pizlonator/fil-c/blob/deluge/libpas/src/libpas/verse_heap.h">Verse heap config</a>
that I added to
<a href="https://github.com/WebKit/WebKit/blob/main/Source/bmalloc/libpas/Documentation.md">libpas</a>. FUGC
typically spends &lt;5% of its time sweeping.</p>

<h2>Bonus Features</h2>

<p>FUGC supports a most of C-style, Java-style, and JavaScript-style memory management. Let's break down what that means.</p>

<h3>Freeing Objects</h3>

<p>If you call <code>free</code>, the runtime will flag the object as free and all subsequent accesses to the object will trap. Additionally, FUGC will not scan outgoing references from the object (since they cannot be accessed anymore).</p>

<p>Also, FUGC will redirect all capability pointers (<em>lower</em>s in <a href="https://fil-c.org/invisicaps.html">InvisiCaps</a> jargon) to free objects to point at the free singleton object instead. This allows freed object memory to really be reclaimed.</p>

<p>This means that freeing objects can be used to prevent <em>GC-induced leaks</em>. Surprisingly, a program that works fine with <code>malloc</code>/<code>free</code> (no leaks, no crashes) that gets converted to GC the naive way (<code>malloc</code> allocates from the GC and <code>free</code> is a no-op) may end up leaking due to dangling pointers that the program never accesses. Those dangling pointers will be treated as live by the GC. In FUGC, if you freed those pointers, then FUGC will really kill them.</p>

<h3>Finalizers</h3>

<p>FUGC supports finalizer queues using the <code>zgc_finq</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. This feature allows you to implement finalizers in the style of Java, except that you get to set up your own finalizer queues and choose which thread processes them.</p>

<h3>Weak References</h3>

<p>FUGC supports weak references using the <code>zweak</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. Weak references work just like the weak references in Java, except there are no reference queues. Fil-C does not support phantom or soft references.</p>

<h3>Weak Maps</h3>

<p>FUGC supports weak maps using the <code>zweak_map</code> API in <a href="https://github.com/pizlonator/fil-c/blob/deluge/filc/include/stdfil.h">stdfil.h</a>. This API works almost exactly like the JavaScript <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WeakMap">WeakMap</a>, except that Fil-C's weak maps allow you to iterate all of their elements and get a count of elements.</p>

<h2>Conclusion</h2>

<p>FUGC allows Fil-C to give the strongest possible guarantees on misuse of <code>free</code>:</p>

<ul>
<li><p>Freeing an object and then accessing it is guaranteed to result in a trap. Unlike tag-based approaches, which will trap on use after free until until memory reclamation is forced, FUGC means you will trap even after memory is reclaimed (due to <em>lower</em> repointing to the free singleton).</p></li>
<li><p>Freeing an object twice is guaranteed to result in a trap.</p></li>
<li><p>Failing to free an object means the object gets reclaimed for you.</p></li>
</ul>
        </main>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I ditched Spotify and set up my own music stack (178 pts)]]></title>
            <link>https://leshicodes.github.io/blog/spotify-migration/</link>
            <guid>45133109</guid>
            <pubDate>Thu, 04 Sep 2025 22:47:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://leshicodes.github.io/blog/spotify-migration/">https://leshicodes.github.io/blog/spotify-migration/</a>, See on <a href="https://news.ycombinator.com/item?id=45133109">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="__blog-post-container">
<!-- -->
<h2 id="why-i-ditched-spotify-and-how-i-set-up-my-own-music-stack">Why I Ditched Spotify, and How I Set Up My Own Music Stack<a href="#why-i-ditched-spotify-and-how-i-set-up-my-own-music-stack" aria-label="Direct link to Why I Ditched Spotify, and How I Set Up My Own Music Stack" title="Direct link to Why I Ditched Spotify, and How I Set Up My Own Music Stack">​</a></h2>
<p>For years, I relied on Spotify like millions of others. The convenience was undeniable stream anything, anywhere, discover new music through algorithms, and share playlists with friends. But over time, several issues became impossible to ignore: <a href="https://en.wikipedia.org/wiki/Criticism_of_Spotify#Allegations_of_unfair_artist_compensation" target="_blank" rel="noopener noreferrer">artists getting paid fractions of pennies per stream</a>, <a href="https://en.wikipedia.org/wiki/Controversy_over_fake_artists_on_Spotify" target="_blank" rel="noopener noreferrer">fake Artists and ghost Tracks</a>, <a href="https://www.musicradar.com/music-tech/recording/ill-never-be-able-to-sing-that-perfectly-in-tune-but-i-dont-want-to-im-human-meet-the-real-artists-that-are-victims-of-ai-fake-tracks" target="_blank" rel="noopener noreferrer">AI music and impersonation</a>, <a href="https://www.techradar.com/audio/spotify/spotify-introduces-face-scanning-age-checks-for-uk-uses-as-some-furious-fans-threaten-to-return-to-piracy" target="_blank" rel="noopener noreferrer">creepy age verification complicity</a> and the fact that despite paying monthly, I never actually owned anything.
So I decided to take back control of my music experience. Here's how I built my own self-hosted music streaming setup that gives me everything Spotify offered and more.</p>
<div><p><span><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</p><p>There are components of this post which may be improved if you zoom in with your device. The mermaid diagram and code blocks in particular may be hard to read on smaller screens.</p></div>
<h2 id="high-level-overview">High Level Overview<a href="#high-level-overview" aria-label="Direct link to High Level Overview" title="Direct link to High Level Overview">​</a></h2>
<!-- -->
<h2 id="the-components">The Components<a href="#the-components" aria-label="Direct link to The Components" title="Direct link to The Components">​</a></h2>
<h3 id="music-player-navidrome">Music Player: Navidrome<a href="#music-player-navidrome" aria-label="Direct link to Music Player: Navidrome" title="Direct link to Music Player: Navidrome">​</a></h3>
<p>At the core of my setup is <a href="https://www.navidrome.org/docs/installation/docker/" target="_blank" rel="noopener noreferrer">Navidrome</a>, an open-source music server that handles streaming your personal music collection.</p>
<p><img src="https://leshicodes.github.io/assets/images/navidrome-62470ba14db4ff4a00eebfc17a06b528.png" alt="navidrome"></p><p>To access my music from anywhere, I expose Navidrome via a <a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/" target="_blank" rel="noopener noreferrer">CloudFlare Tunnel</a>, which provides secure access without exposing my home IP address or dealing with port forwarding.</p>
<p>For client apps, I use:</p>
<ul>
<li>Browser: Navidrome's built-in web player works perfectly</li>
<li>iOS: <a href="https://apps.apple.com/us/app/play-sub-music-streamer/id955329386" target="_blank" rel="noopener noreferrer">Play<!-- -->:Sub</a> connects seamlessly</li>
<li>Android: <a href="https://symfonium.app/" target="_blank" rel="noopener noreferrer">Symfonium</a> offers excellent playback quality and features</li>
<li>Desktop: <a href="https://github.com/jeffvli/feishin" target="_blank" rel="noopener noreferrer">Feishin</a> provides a native app experience with synced lyrics</li>
</ul>
<p><img src="https://leshicodes.github.io/assets/images/feishin-847514abddff0b9ca914f06eaf00ad1e.png" alt="feishin"></p><div><p><span><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Definition</p><p><strong>Scrobbling</strong>: (Internet slang) To publish one's music-listening habits to the Internet via software, in order to track when and how often certain songs are played.</p></div>
<p>Every track I play through this setup automatically scrobbles to my <a href="https://www.last.fm/user/dudelaaaa" target="_blank" rel="noopener noreferrer">Last.fm</a> account, which becomes important for music discovery later.</p>
<h3 id="music-collection-management-lidarr">Music Collection Management: Lidarr<a href="#music-collection-management-lidarr" aria-label="Direct link to Music Collection Management: Lidarr" title="Direct link to Music Collection Management: Lidarr">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lidarr-0e2bac8a263d84420f5dc5dc2f3e765e.png" alt="lidarr"></p><p><a href="https://github.com/Lidarr/Lidarr" target="_blank" rel="noopener noreferrer">Lidarr</a> helps manage my music collection by tracking artists and albums I own or purchase. It can monitor for new releases from favorite artists and helps organize my library.</p>
<div><p><span><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>warning</p><p>Lidarr is just a tool. Like any tool, it can be misused.
Yes, people <em>could</em> point it at less-than-legal sources. No, I'm not telling you to do that.
If you want to support artists, buy their work. If you don't, don't pretend Spotify streams are "support."</p></div>
<p><strong>Important Note</strong>: Always ensure you're obtaining music through legal channels such as:</p>
<ul>
<li>Digital purchases (Bandcamp, iTunes, Amazon, etc.)</li>
<li>Ripping CDs you've purchased</li>
<li>Free legal downloads offered by artists</li>
<li>Music available under Creative Commons licenses</li>
</ul>
<p>My setup uses <a href="https://hub.docker.com/r/linuxserver/sabnzbd" target="_blank" rel="noopener noreferrer">sabnzbd</a> integrated with Lidarr for handling downloads of content I've purchased. Both services run in Docker containers and are NOT exposed to the internet for security.</p>
<h3 id="synced-lyrics-lrcget-kasm">Synced Lyrics: lrcget-kasm<a href="#synced-lyrics-lrcget-kasm" aria-label="Direct link to Synced Lyrics: lrcget-kasm" title="Direct link to Synced Lyrics: lrcget-kasm">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lrcget-9e4a5896f560d23b5e5dfb8028346ce0.png" alt="lrcget"></p><p>A feature I missed from Spotify was synced lyrics. <a href="https://github.com/ShadowsDieThrice/lrcget-kasm" target="_blank" rel="noopener noreferrer">lrcget-kasm</a> fills this gap by mass-downloading LRC synced lyrics files for my music library.</p>
<p>Since lrcget is GUI-only (no CLI version yet), I'm using a containerized version of it via <a href="https://www.kasmweb.com/" target="_blank" rel="noopener noreferrer">Kasm</a> and access it through my browser. It's a bit resource-intensive, so I only run it when adding new music or updating lyrics.</p>
<p>I've opened a <a href="https://github.com/tranxuanthang/lrcget/discussions/245" target="_blank" rel="noopener noreferrer">feature request</a> for a CLI version, which would make this process more automation-friendly.</p>
<h3 id="music-discovery-lidify">Music Discovery: Lidify<a href="#music-discovery-lidify" aria-label="Direct link to Music Discovery: Lidify" title="Direct link to Music Discovery: Lidify">​</a></h3>
<p><img src="https://leshicodes.github.io/assets/images/lidify-7c9fe9c04989d7b44fe3f586bcd83968.png" alt="lidify"></p><p>One of Spotify's strongest features was music discovery. For this, I use <a href="https://github.com/TheWicklowWolf/Lidify" target="_blank" rel="noopener noreferrer">Lidify</a>, which connects to my Lidarr library and Last.fm account to generate recommendations.</p>
<p>I've also connected my Last.fm scrobbles to <a href="https://listenbrainz.org/user/leshicodes/" target="_blank" rel="noopener noreferrer">ListenBrainz</a>, which promises to build weekly discovery playlists similar to Spotify's in the future.</p>
<h2 id="the-results">The Results<a href="#the-results" aria-label="Direct link to The Results" title="Direct link to The Results">​</a></h2>
<p>After several months with this setup, I'm extremely satisfied with the results:</p>
<h3 id="how-my-solution-compares-to-spotify">How My Solution Compares to Spotify<a href="#how-my-solution-compares-to-spotify" aria-label="Direct link to How My Solution Compares to Spotify" title="Direct link to How My Solution Compares to Spotify">​</a></h3>
<table><thead><tr><th>Feature</th><th>Spotify</th><th>My Self-Hosted Stack</th></tr></thead><tbody><tr><td>Music Quality</td><td>Up to 320kbps</td><td>Unlimited (FLAC/lossless)</td></tr><tr><td>Monthly Cost</td><td>$9.99-$14.99</td><td>One-time server setup + storage</td></tr><tr><td>Artist Payment</td><td>~$0.003-0.005 per stream</td><td>Direct support via purchases</td></tr><tr><td>Music Ownership</td><td>Rental only</td><td>Full ownership forever</td></tr><tr><td>Offline Access</td><td>Limited downloads</td><td>Complete library available</td></tr><tr><td>Privacy</td><td>Data collection &amp; tracking</td><td>Complete privacy</td></tr><tr><td>Content Permanence</td><td>Can disappear anytime</td><td>Never removed unless I choose</td></tr></tbody></table>
<p>The initial setup took a weekend, but maintenance is minimal. When I want new music, Lidarr handles it automatically. If I need to manually add something, I just drop the files in the right folder and Navidrome indexes them immediately.</p>
<p>Is it for everyone? No. But if you care about music, value ownership, and have basic technical skills, building your own music streaming solution is both achievable and rewarding. The freedom from corporate streaming platforms is worth the effort.</p>
<h2 id="supporting-artists">Supporting Artists<a href="#supporting-artists" aria-label="Direct link to Supporting Artists" title="Direct link to Supporting Artists">​</a></h2>
<p>Moving away from Spotify doesn't mean abandoning artists. In fact, I now support musicians more directly by:</p>
<ul>
<li>Purchasing music directly from platforms like Bandcamp where artists receive 82-90% of sales</li>
<li>Buying physical media from official stores</li>
<li>Supporting Patreon/subscription services for favorite artists</li>
<li>Attending concerts and buying merchandise</li>
</ul>
<p>Buying a $10 album on Bandcamp puts about $8.20-$9.00 in the artist's pocket. To match that on Spotify, you're talking roughly 1.6k-3k streams of that album <strong>per listener</strong>. If the artist has a label taking a cut on Spotify, the stream counts needed go up further.</p>
<p>My self-hosted setup is about controlling my listening experience and owning what I pay for, not avoiding fair compensation to artists.</p>
<h2 id="whats-next">What's Next?<a href="#whats-next" aria-label="Direct link to What's Next?" title="Direct link to What's Next?">​</a></h2>
<p>I'm continually refining this setup. Future improvements include automating the lyrics process and exploring more discovery tools. The beauty of a self-hosted solution is that it can evolve with my needs, rather than changing at the whim of a company's business model.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What Is the Fourier Transform? (239 pts)]]></title>
            <link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link>
            <guid>45132810</guid>
            <pubDate>Thu, 04 Sep 2025 22:11:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/">https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</a>, See on <a href="https://news.ycombinator.com/item?id=45132810">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-role="selectable">
    <p>As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.</p>
<p>It took mathematicians until the 19th century to master this same calculation.</p>
<p>In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and you’ll get your original function. The technique, today called the Fourier transform, allowed the mathematician — previously an ardent proponent of the French revolution — to spur a mathematical revolution as well.</p>
<p>Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.</p>
<p>“It’s hard to overestimate the influence of Fourier analysis in math,” said <a href="https://math.nyu.edu/~greengar/">Leslie Greengard</a> of New York University and the Flatiron Institute. “It touches almost every field of math and physics and chemistry and everything else.”</p>
<h2><strong>Flames of Passion </strong></h2>
<p>Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.</p>

<p>Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 — shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British — Fourier could think of nothing else.</p>
<p>If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly — meaning they have more energy — decay first, followed eventually by the lower frequencies. It’s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.</p>
<p>The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work “nothing short of impossible.”</p>
<p>What troubled his peers most were strange cases where the heat distribution might be sharply irregular — like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.</p>
<p>Today, we know that Fourier was broadly right.</p>
<p>“You can represent anything as a sum of these very, very simple oscillations,” said <a href="https://www.math.princeton.edu/people/charles-fefferman">Charles Fefferman</a>, a mathematician at Princeton University. “It’s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethoven’s Ninth Symphony.” The process only fails for <a href="https://www.quantamagazine.org/the-jagged-monstrous-function-that-broke-calculus-20250123/">the most bizarre functions</a>, like those that oscillate wildly no matter how much you zoom in on them.</p>
<p>So how does the Fourier transform work?</p>
<h2><strong>A Well-Trained Ear</strong></h2>
<p>Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.</p>
<p>Mathematically, the Fourier transform is a function. It takes a given function — which can look complicated — as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, you’ll get the original function.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-1-1.svg" alt="" decoding="async">    </p>
            <figcaption>
            <p>Samuel Velasco/<em>Quanta Magazine</em></p>
        </figcaption>
    </figure>

<p>To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Let’s look at a simple example.</p>
<p>Consider the following function:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-2-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-2-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Here’s what happens if we multiply the original by a sine wave with a frequency of 3:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-3-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-3-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.</p>
<p>Now let’s test if the frequency 5 is present. Here’s what you get when you multiply the original function by a sine wave with the frequency 5:</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-4-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-4-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.</p>
<p>The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)</p>
<p>In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.</p>
<p>If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and — despite mathematicians’ early hesitation to accept such a thing — it is now an essential tool in the analysis of functions.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-5-1.svg" alt="" decoding="async"><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-5-Mobile-1.svg" alt="" decoding="async">    </p>
    </figure>

<h2><strong>Encore</strong></h2>
<p>The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns — and simple combinations of them that resemble checkerboards — can be added together to re-create any image.</p>
<p>Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.</p>
<figure>
    <p><img src="https://www.quantamagazine.org/wp-content/uploads/2025/09/Fourier-figure-6-1.svg" alt="" decoding="async">    </p>
    </figure>

<p>In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly —&nbsp;aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. “It’s now a part of everyday life,” Greengard said.</p>
<p>It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics — the physics of the very small — it even provides the mathematical foundation for the uncertainty principle, which says that it’s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particle’s possible positions; the Fourier transform of that function will describe the particle’s possible momenta. When your function can tell you where a particle will be located with high probability — represented by a sharp peak in the graph of the function — the Fourier transform will be very spread out. It will be impossible to determine what the particle’s momentum should be. The opposite is also true.</p>
        
        
<p>The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis — which studies the Fourier transform, as well as how to reverse it to rebuild the original function — is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. They’ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.</p>
<p>“If people didn’t know about the Fourier transform, I don’t know what percent of math would then disappear,” Fefferman said. “But it would be a big percent.”</p>
<p><em>Editor’s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between </em>Quanta Magazine<em> and the Simons Foundation is available </em><a href="https://www.quantamagazine.org/about/"><em>here</em></a><em>. </em></p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[io_uring is faster than mmap (149 pts)]]></title>
            <link>https://www.bitflux.ai/blog/memory-is-slow-part2/</link>
            <guid>45132710</guid>
            <pubDate>Thu, 04 Sep 2025 22:01:11 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bitflux.ai/blog/memory-is-slow-part2/">https://www.bitflux.ai/blog/memory-is-slow-part2/</a>, See on <a href="https://news.ycombinator.com/item?id=45132710">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
                <h2 id="tl-dr">TL;DR</h2>
<p>Sourcing data directly from disk <em>IS</em> faster than caching in memory.  I brought receipts.
Because hardware got wider but not faster, the old methods don't get you there.  You need new tools to use what is scaling and avoid what isn't.</p>
<h2 id="introduction">Introduction</h2>
<p>In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades.  I then asserted, without proof, that sourcing data from disk can be faster than from memory.  What follows is the proof.</p>
<p>Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast.  Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.</p>
<h2 id="experimental-set-up">Experimental set up</h2>
<p>We need data and something straight forward to do with the data.  I used my free will or the illusion thereof to create a benchmark I cleverly call "counting 10s".  I write some pseudo random integers between 0 and 20 to a buffer and then count how many of the integers are 10.  I want to make sure we are doing all the counting in a single thread to simulate an Amdahl's Law situation.</p>
<p>So how fast can we expect this to run?  The upper limit would be the memory bandwidth.</p>
<p>My testing rig is a server with an old AMD EPYC 7551P 32-Core Processor on a Supermicro H11SSL-i and 96GB of DDR4 2133 MHz and a couple of 1.92TB Samsung PM983a PCIe 3.0 SSDs I pieced together from EBay parts.  Given the way this server is configured, the upper limit for memory bandwidth can be calculated as 3 channels * 2133MT/s * 8B/T / 4 numa domains = ~13GB/s for a single thread.  It's kind of an odd system but that just makes it more fun to optimize for!</p>
<p>The disks are rated at 3.1GB/s read BW each for an upper limit of 6.2GB/s.  I made a raid0 volume with 4KB stripe size, formatted the the raid as ext4 with no journaling, and made sure it fully finished initializing the metadata before running the tests.</p>
<pre data-lang="bash"><code data-lang="bash"><span>sudo</span><span> mdadm</span><span> --create</span><span> /dev/md0</span><span> --level</span><span>=0</span><span> --raid-devices</span><span>=2</span><span> --chunk</span><span>=4K /dev/nvme1n1 /dev/nvme2n1
</span><span>sudo</span><span> mkfs.ext4</span><span> -F -L</span><span> data</span><span> -O</span><span> ^has_journal</span><span> -E</span><span> lazy_itable_init=0 /dev/md0
</span><span>sudo</span><span> mount</span><span> -o</span><span> noatime /dev/md0 mnt
</span></code></pre>
<p>We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.</p>
<h2 id="simple-loop">Simple Loop</h2>
<p>The simple and cleanest way to do this in C would look like this.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>
</span><span>// count_10_loop
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>int</span><span>* data = (</span><span>int</span><span>*)</span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>Found </span><span>%ld</span><span> 10s</span><span>\n</span><span>", count);
</span><span>}
</span></code></pre>
<p>Just mmap() the file which will give us a buffer that we can read from.  Then we just loop and count the 10s.</p>
<p>Because the point is to benchmark we will integrate some timing mechanisms before we move on.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>// count_10_loop
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>int</span><span>* data = (</span><span>int</span><span>*)</span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>simple loop found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>For the first run we're going to be reading from the disk. The disk/filesystem read is going to limit the performance before the memory bandwidth can.</p>
<pre><code><span>❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
</span><span>simple loop found 167802249 10s processed at 0.61 GB/s
</span></code></pre>
<p>As expected, it's not anywhere near memory speeds because as everyone knows, disk is slow.  We can look at the system and confirm that the first run cached the data to memory.</p>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/cached.png" alt="cached"></p>
<p>Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.</p>
<pre><code><span>❯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
</span><span>simple loop found 167802249 10s processed at 3.71 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance1.png" alt="performance1"></p>
<p>It is faster, but clearly that’s slower than the memory can feed it to the processor.  What bottleneck might we be hitting?  This speed does look possibly correlated to the instructions per second limit for this generation of CPU (between 2GHz * 1.5 IPC = 3G and 3GHz boost * 1.5 IPC = 4.5G instructions per second).</p>
<p>We can use perf to see if the CPU is using vector instructions, if not then the actual compute is the bottleneck.</p>
<pre><code><span>Percent│      test     %rbp,%rbp
</span><span>       │    ↓ je       84
</span><span>       │      lea      (%rbx,%rbp,4),%rcx
</span><span>       │      mov      %rbx,%rax
</span><span>       │      xor      %ebp,%ebp
</span><span>       │      nop
</span><span>       │70:   xor      %edx,%edx
</span><span>  1.31 │      cmpl     $0xa,(%rax)
</span><span> 42.38 │      sete     %dl
</span><span> 45.72 │      add      $0x4,%rax
</span><span>  0.01 │      add      %rdx,%rbp
</span><span> 10.42 │      cmp      %rax,%rcx
</span><span>  0.16 │    ↑ jne      70
</span><span>       │84:   xor      %eax,%eax
</span><span>       │      shr      $0x14,%r12
</span><span>       │    → call     get_time_us
</span><span>       │      pxor     %xmm0,%xmm0
</span><span>       │      pxor     %xmm1,%xmm1
</span></code></pre>
<p>Confirmed. We're running non-vectorized instructions, with a single thread counting that's as fast as it can go with a 2GHz CPU.  Well crap.  We’ve hit our first non-exponential limit.  Even a brand new CPU running this machine code would probably struggle to do much better than a 50% improvement, still well below the memory bandwidth limit.</p>
<h2 id="unrolling-the-loop">Unrolling the loop</h2>
<p>Good news is this code can definitely be vectorized if we help the compiler.  Unroll the loop!</p>
<p>We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>// count_10_unrolled
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY);
</span><span>    </span><span>void</span><span>* buffer = </span><span>mmap</span><span>(</span><span>NULL</span><span>, size_bytes, PROT_READ, MAP_SHARED, fd, </span><span>0</span><span>);
</span><span> 
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> data = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>(buffer, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(data[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(data[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(data[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(data[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(data[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(data[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(data[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(data[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(data[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(data[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(data[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(data[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(data[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(data[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(data[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(data[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    count = (t0 + t1) + (t2 + t3);
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>unrolled loop found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>Check if we now have vectorized instructions with <code>perf</code>.</p>
<pre><code><span>Percent│       movq      %xmm0,%rcx
</span><span>       │       movdqa    %xmm7,%xmm14
</span><span>       │       pxor      %xmm0,%xmm0
</span><span>       │       nop
</span><span>       │ e8:   movdqa    %xmm6,%xmm4
</span><span>  0.30 │       movdqa    %xmm6,%xmm3
</span><span>  0.12 │       movdqa    %xmm6,%xmm2
</span><span>  0.35 │       add       $0x1,%rdx
</span><span>  1.54 │       pcmpeqd   (%rax),%xmm4
</span><span> 54.64 │       pcmpeqd   0x10(%rax),%xmm3
</span><span>  1.62 │       movdqa    %xmm6,%xmm1
</span><span>  0.99 │       add       $0x40,%rax
</span><span>  0.12 │       pcmpeqd   -0x20(%rax),%xmm2
</span><span>  3.03 │       pcmpeqd   -0x10(%rax),%xmm1
</span><span>  1.32 │       pand      %xmm5,%xmm4
</span><span>  1.25 │       pand      %xmm5,%xmm3
</span><span>  1.55 │       movdqa    %xmm4,%xmm15
</span><span>  0.24 │       punpckhdq %xmm0,%xmm4
</span><span>
</span></code></pre>
<p>Confirmed. We're using 128bit vector instructions, this should be up to 4x faster than the original.</p>
<blockquote>
<p>NOTE: These are 128-bit vector instructions, but I expected 256-bit.  I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions.  I forced the compiler to use 256-bit instructions and found it was actually slower.  Looks like the compiler was smart enough to know that here.</p>
</blockquote>
<p>Let's benchmark this unrolled version with the data as page cache in memory.</p>
<pre><code><span>❯ sudo  ./count_10_unrolled ./mnt/datafile.bin 53687091200
</span><span>unrolled loop found 167802249 10s processed at 5.51 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance2.png" alt="performance2"></p>
<p>We're still nowhere close to hitting the memory bus speed limit of 13GB/s but 50% faster than the original is a win.  There must be some other bottleneck.</p>
<h2 id="can-the-ssds-beat-that">Can the SSDs beat that?</h2>
<p>5.51GB/s?  On paper the SSDs can read at 6.2GB/s, but the first run from disk only did 0.61GB/s.  How can I meet or beat this performance sourcing the data directly from disk?</p>
<p>Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk.  When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk.  Unfortunately these legacy mechanisms just aren't set up for serious high performance IO.  Note that at 610MB/s it's faster than what a disk SATA can do.  On the other hand, it only managed 10% of our disk's potential.  Clearly we're going to have to do something else.</p>
<p>SSDs don't just automatically read data at multigigabyte speeds.  You need to put some real effort into an IO pipeline to get serious performance.</p>
<p>I made a io_uring based IO engine, a kind of userspace driver, that can hit these speeds.  The main thread will request data, the IO engine will handle the IO, then the main thread will do the counting when the data is in a buffer.  We will use a set of queues to manage the IO requests, responses, and buffers.  The IO engine will start 6 workers, target a queue depth of 8192, and have a buffer size of 16KB.</p>
<p>I wish I had tighter code here, but A) I didn’t have time to clean it up B) some of the complexity is intractable.  The IO engine code was a lot to scroll through so I moved it to github <a href="https://github.com/bitflux-ai/blog_notes/tree/main/memory_is_slow_part2/diskbased">link</a></p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>"</span><span>io_engine.h</span><span>"
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>getopt.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/mman.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>
</span><span>#define </span><span>DEFAULT_WORKERS </span><span>6
</span><span>#define </span><span>DEFAULT_BLOCK_SIZE </span><span>16384
</span><span>#define </span><span>DEFAULT_QUEUE_DEPTH </span><span>8192
</span><span>
</span><span>// Count the number of "10" (int format) in the buffer
</span><span>static inline </span><span>size_t </span><span>count_tens_unrolled</span><span>(</span><span>void</span><span>* </span><span>data</span><span>, size_t </span><span>size_bytes</span><span>) {
</span><span>    </span><span>const </span><span>size_t total = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> p = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>(data, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(p[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(p[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(p[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(p[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(p[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(p[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(p[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(p[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(p[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(p[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(p[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(p[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(p[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(p[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(p[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(p[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    </span><span>return </span><span>(t0 + t1) + (t2 + t3);
</span><span>}
</span><span>
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>
</span><span>    </span><span>// Set up the io engine
</span><span>    ioengine_t* na = </span><span>ioengine_alloc</span><span>(filename, size_bytes, DEFAULT_QUEUE_DEPTH, DEFAULT_BLOCK_SIZE, DEFAULT_WORKERS);
</span><span>
</span><span>    </span><span>sleep</span><span>(</span><span>1</span><span>);
</span><span>
</span><span>    </span><span>// Use the background workers to read file directly
</span><span>    size_t total_blocks = na-&gt;file_size / na-&gt;block_size;
</span><span>    uint64_t uid = </span><span>1</span><span>;
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>
</span><span>    </span><span>// Read all blocks
</span><span>    size_t blocks_queued = </span><span>0</span><span>;
</span><span>    size_t blocks_read = </span><span>0</span><span>;
</span><span>    </span><span>int</span><span> buffer_queued = </span><span>0</span><span>;
</span><span>    </span><span>while </span><span>(blocks_read &lt; total_blocks) {
</span><span>        </span><span>//// Queue IO phase //////
</span><span>        </span><span>//     Do we have more blocks to queue up?
</span><span>        </span><span>if </span><span>(buffer_queued &lt; na-&gt;num_io_buffers/</span><span>2 </span><span>&amp;&amp; blocks_queued &lt;= total_blocks) {
</span><span>            </span><span>// Calculate how many blocks on average we want our workers to queue up
</span><span>            size_t free_buffers = (size_t)(na-&gt;num_io_buffers - buffer_queued - </span><span>4</span><span>); </span><span>// hold back a few buffers
</span><span>            size_t blocks_remaining = total_blocks - blocks_queued;  </span><span>// how many blocks have we not queued
</span><span>            size_t blocks_to_queue = free_buffers &gt; blocks_remaining ? blocks_remaining : free_buffers;
</span><span>            </span><span>int</span><span> blocks_to_queue_per_worker = (</span><span>int</span><span>) (blocks_to_queue + na-&gt;num_workers - </span><span>1</span><span>) / na-&gt;num_workers;
</span><span>            </span><span>// Iterate through workers and assign work
</span><span>            </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; na-&gt;num_workers; i++) {
</span><span>                worker_thread_data_t* worker = &amp;na-&gt;workers[i];
</span><span>                </span><span>// Try to queue N blocks to this worker
</span><span>                </span><span>for </span><span>(</span><span>int</span><span> j = </span><span>0</span><span>; j &lt; blocks_to_queue_per_worker; j++) {
</span><span>                    </span><span>if </span><span>(blocks_queued == total_blocks) </span><span>break</span><span>;
</span><span>                    </span><span>int</span><span> bgio_tail = worker-&gt;bgio_tail;
</span><span>                    </span><span>int</span><span> bgio_head = worker-&gt;bgio_head;
</span><span>                    </span><span>int</span><span> bgio_next = (bgio_tail + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    </span><span>int</span><span> next_bhead = (worker-&gt;buffer_head + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    </span><span>if </span><span>(bgio_next == bgio_head) </span><span>break</span><span>;  </span><span>// queue for send requests is full
</span><span>                    </span><span>if </span><span>(next_bhead == worker-&gt;buffer_tail) </span><span>break</span><span>; </span><span>// queue for recieving completed IO is full
</span><span>                    </span><span>// Queue this block with the worker.  We have to track which buffer it's going to.
</span><span>                    </span><span>int</span><span> buffer_idx = worker-&gt;buffer_start_idx + worker-&gt;buffer_head;
</span><span>                    na-&gt;buffer_state[buffer_idx] = BUFFER_PREFETCHING;
</span><span>                    worker-&gt;bgio_uids[bgio_tail] = (uid++)&lt;&lt;</span><span>16</span><span>; </span><span>// unique id helps track IOs in io_uring, we encode 4 bytes later
</span><span>                    worker-&gt;bgio_buffer_idx[bgio_tail] = buffer_idx;
</span><span>                    worker-&gt;bgio_block_idx[bgio_tail] = blocks_queued++;  </span><span>// block sized index into file
</span><span>                    worker-&gt;bgio_queued[bgio_tail] = -</span><span>1</span><span>;  </span><span>// Requested but not yet queued
</span><span>                    </span><span>int</span><span> next_tail = (bgio_tail + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>                    worker-&gt;bgio_tail = next_tail;
</span><span>                    </span><span>// Log the buffer in an ordered queue for us to read
</span><span>                    worker-&gt;complete_ring[worker-&gt;buffer_head] = buffer_idx;
</span><span>                    worker-&gt;buffer_head = next_bhead;
</span><span>                    buffer_queued++;
</span><span>                }
</span><span>                </span><span>// Tell the worker to submit IOs as a group
</span><span>                worker-&gt;bgio_submit++;
</span><span>            }
</span><span>        }
</span><span>
</span><span>        </span><span>//// Completion Phase //////
</span><span>        </span><span>//     Iterate through worker and check if they have complete IOs
</span><span>        </span><span>for </span><span>(</span><span>int</span><span> i = </span><span>0</span><span>; i &lt; na-&gt;num_workers; i++) {
</span><span>            worker_thread_data_t* worker = &amp;na-&gt;workers[i];
</span><span>            </span><span>int</span><span> current = worker-&gt;buffer_tail;
</span><span>            </span><span>// We know what IO's we're waiting on, but we have to poll
</span><span>            </span><span>//  to see if they are done.
</span><span>            </span><span>for </span><span>(</span><span>int</span><span> scan = </span><span>0</span><span>; scan &lt; worker-&gt;num_max_bgio; scan++) {
</span><span>                </span><span>// Scan until we get to the end of the list
</span><span>                </span><span>if </span><span>(current == worker-&gt;buffer_head) </span><span>break</span><span>;
</span><span>                </span><span>int</span><span> buffer_idx = worker-&gt;complete_ring[current];
</span><span>                </span><span>int</span><span> state = na-&gt;buffer_state[buffer_idx];
</span><span>                </span><span>if </span><span>(state == BUFFER_PREFETCHED) {
</span><span>                    </span><span>// This buffer is completed - Process this buffer.
</span><span>                    count += </span><span>count_tens_unrolled</span><span>(na-&gt;io_buffers[buffer_idx], na-&gt;block_size);
</span><span>                    na-&gt;buffer_state[buffer_idx] = BUFFER_UNUSED;
</span><span>                    blocks_read++;
</span><span>                    buffer_queued--;
</span><span>                }
</span><span>                current = (current + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>            }
</span><span>            </span><span>// IO's might have been completed out of order, advance the tail when we can
</span><span>            current = worker-&gt;buffer_tail;
</span><span>            </span><span>while </span><span>(current != worker-&gt;buffer_head) {
</span><span>                </span><span>int</span><span> buffer_idx = worker-&gt;complete_ring[current];
</span><span>                </span><span>int</span><span> state = na-&gt;buffer_state[buffer_idx];
</span><span>                </span><span>if </span><span>(state != BUFFER_UNUSED) </span><span>break</span><span>;
</span><span>                current = (current + </span><span>1</span><span>) % worker-&gt;num_max_bgio;
</span><span>            }
</span><span>            worker-&gt;buffer_tail = current;
</span><span>            worker-&gt;bgio_submit++;  </span><span>// probably unnecessary
</span><span>        }
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>    </span><span>printf</span><span>("</span><span>diskbased found </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>", count, (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>
</span><span>    </span><span>// Cleanup I/O system
</span><span>    </span><span>ioengine_free</span><span>(na);
</span><span>
</span><span>    </span><span>return </span><span>0</span><span>;
</span><span>}
</span></code></pre>
<p>I hope all this extra code makes it faster.</p>
<pre><code><span>❯ sudo ./diskbased/benchmark ./mnt/datafile.bin 53687091200
</span><span>diskbased found 167802249 10s processed at 5.81 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance3.png" alt="performance3"></p>
<p>Boom!  Disk is faster than memory!  It takes several hundred lines of code but now we can source the data from my SSDs faster than the copy from the page cache in memory.</p>
<h2 id="so-what-s-going-on-here">So what's going on here?</h2>
<p>Of course my 6GB/s disk stripe isn’t actually faster than the memory bus, even on this weird hack of a system.  So what is happening?  Where is the bottleneck?  It's got to be the way the data is being read from the page cache in memory.</p>
<p>What if we replace the mmap() with a read() from disk into a preallocated buffer.  That way we can measure the counting with the data in-memory without any page cache related overhead mmap() can introduce.</p>
<pre data-lang="c"><code data-lang="c"><span>#include </span><span>&lt;</span><span>stdio.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdlib.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/time.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>sys/stat.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>fcntl.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>unistd.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>stdint.h</span><span>&gt;
</span><span>#include </span><span>&lt;</span><span>string.h</span><span>&gt;
</span><span>
</span><span>long </span><span>get_time_us</span><span>() {
</span><span>    </span><span>struct</span><span> timeval tv;
</span><span>    </span><span>gettimeofday</span><span>(&amp;tv, </span><span>NULL</span><span>);
</span><span>    </span><span>return</span><span> tv.</span><span>tv_sec </span><span>* </span><span>1000000</span><span>L </span><span>+ tv.</span><span>tv_usec</span><span>;
</span><span>}
</span><span>
</span><span>int </span><span>main</span><span>(</span><span>int </span><span>argc</span><span>, </span><span>char </span><span>*</span><span>argv</span><span>[]) {
</span><span>    </span><span>char</span><span>* filename = argv[</span><span>1</span><span>];
</span><span>    size_t size_bytes = </span><span>strtoull</span><span>(argv[</span><span>2</span><span>], </span><span>NULL</span><span>, </span><span>10</span><span>);
</span><span>    size_t total_ints = size_bytes / sizeof(</span><span>int</span><span>);
</span><span>    size_t count = </span><span>0</span><span>;
</span><span>
</span><span>    </span><span>int</span><span> fd = </span><span>open</span><span>(filename, O_RDONLY|O_DIRECT);
</span><span>    </span><span>void </span><span>*buf;
</span><span>    </span><span>posix_memalign</span><span>(&amp;buf, </span><span>4096</span><span>, size_bytes);
</span><span>    </span><span>int </span><span>*data = buf;
</span><span>
</span><span>    size_t off = </span><span>0</span><span>;
</span><span>    </span><span>while </span><span>(off &lt; size_bytes) {
</span><span>        ssize_t n = </span><span>read</span><span>(fd, (</span><span>char</span><span>*)data + off, size_bytes - off);
</span><span>        off += (size_t)n;   </span><span>// YOLO: assume n &gt; 0 until done
</span><span>    }
</span><span>
</span><span>    </span><span>long</span><span> start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; ++i) {
</span><span>        </span><span>if </span><span>(data[i] == </span><span>10</span><span>) count++;
</span><span>    }
</span><span>    </span><span>long</span><span> elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>simple loop </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>",
</span><span>           count,
</span><span>           (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>
</span><span>
</span><span>    </span><span>// Get the compiler to align the buffer
</span><span>    </span><span>const int </span><span>* </span><span>__restrict</span><span> p = (</span><span>const int </span><span>* </span><span>__restrict</span><span>)</span><span>__builtin_assume_aligned</span><span>((</span><span>void</span><span>*)data, </span><span>4096</span><span>);
</span><span>    uint64_t c0=</span><span>0</span><span>, c1=</span><span>0</span><span>, c2=</span><span>0</span><span>, c3=</span><span>0</span><span>,
</span><span>            c4=</span><span>0</span><span>, c5=</span><span>0</span><span>, c6=</span><span>0</span><span>, c7=</span><span>0</span><span>,
</span><span>            c8=</span><span>0</span><span>, c9=</span><span>0</span><span>, c10=</span><span>0</span><span>, c11=</span><span>0</span><span>,
</span><span>            c12=</span><span>0</span><span>, c13=</span><span>0</span><span>, c14=</span><span>0</span><span>, c15=</span><span>0</span><span>;
</span><span>
</span><span>    start = </span><span>get_time_us</span><span>();
</span><span>    </span><span>// Unrolling the compiler knows it can use a vector unit like AVX2 to process
</span><span>    </span><span>for </span><span>(size_t i = </span><span>0</span><span>; i &lt; total_ints; i += </span><span>16</span><span>) {
</span><span>        </span><span>// removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
</span><span>        c0  += (</span><span>unsigned</span><span>)(p[i+ </span><span>0</span><span>] == </span><span>10</span><span>);
</span><span>        c1  += (</span><span>unsigned</span><span>)(p[i+ </span><span>1</span><span>] == </span><span>10</span><span>);
</span><span>        c2  += (</span><span>unsigned</span><span>)(p[i+ </span><span>2</span><span>] == </span><span>10</span><span>);
</span><span>        c3  += (</span><span>unsigned</span><span>)(p[i+ </span><span>3</span><span>] == </span><span>10</span><span>);
</span><span>        c4  += (</span><span>unsigned</span><span>)(p[i+ </span><span>4</span><span>] == </span><span>10</span><span>);
</span><span>        c5  += (</span><span>unsigned</span><span>)(p[i+ </span><span>5</span><span>] == </span><span>10</span><span>);
</span><span>        c6  += (</span><span>unsigned</span><span>)(p[i+ </span><span>6</span><span>] == </span><span>10</span><span>);
</span><span>        c7  += (</span><span>unsigned</span><span>)(p[i+ </span><span>7</span><span>] == </span><span>10</span><span>);
</span><span>        c8  += (</span><span>unsigned</span><span>)(p[i+ </span><span>8</span><span>] == </span><span>10</span><span>);
</span><span>        c9  += (</span><span>unsigned</span><span>)(p[i+ </span><span>9</span><span>] == </span><span>10</span><span>);
</span><span>        c10 += (</span><span>unsigned</span><span>)(p[i+</span><span>10</span><span>] == </span><span>10</span><span>);
</span><span>        c11 += (</span><span>unsigned</span><span>)(p[i+</span><span>11</span><span>] == </span><span>10</span><span>);
</span><span>        c12 += (</span><span>unsigned</span><span>)(p[i+</span><span>12</span><span>] == </span><span>10</span><span>);
</span><span>        c13 += (</span><span>unsigned</span><span>)(p[i+</span><span>13</span><span>] == </span><span>10</span><span>);
</span><span>        c14 += (</span><span>unsigned</span><span>)(p[i+</span><span>14</span><span>] == </span><span>10</span><span>);
</span><span>        c15 += (</span><span>unsigned</span><span>)(p[i+</span><span>15</span><span>] == </span><span>10</span><span>);
</span><span>    }
</span><span>
</span><span>    </span><span>// pairwise reduce to help some compilers schedule better
</span><span>    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
</span><span>    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
</span><span>    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;
</span><span>
</span><span>    count = (t0 + t1) + (t2 + t3);
</span><span>    elapsed = </span><span>get_time_us</span><span>() - start;
</span><span>
</span><span>    </span><span>printf</span><span>("</span><span>unrolled loop </span><span>%ld</span><span> 10s processed at </span><span>%0.2f</span><span> GB/s</span><span>\n</span><span>",
</span><span>           count,
</span><span>           (</span><span>double</span><span>)(size_bytes/</span><span>1073741824</span><span>)/((</span><span>double</span><span>)elapsed/</span><span>1.0e6</span><span>));
</span><span>}
</span></code></pre>
<p>If we keep the dataset smaller than a numa domain and we bind this to a single numa node to prevent numa overheads we see that the theoretical memory bandwidth we projected seems to be the primary bottleneck for the unrolled loop as we hoped to see at the outset.</p>
<pre><code><span>❯  sudo numactl --cpunodebind=0   ./in_ram mnt/datafile.bin 2147483648
</span><span>simple loop 6709835 10s processed at 4.76 GB/s
</span><span>unrolled loop 6709835 10s processed at 13.04 GB/s
</span></code></pre>
<p>But this isn't useful to compare the with the other runs with the 50GB dataset.  However if we do the full 50GB dataset the performance suffers.  We have to get much of the data across numa domains which is going to be higher cost.</p>
<pre><code><span>❯ sudo ./in_ram ./mnt/datafile.bin 53687091200
</span><span>simple loop 167802249 10s processed at 3.76 GB/s
</span><span>unrolled loop 167802249 10s processed at 7.90 GB/s
</span></code></pre>
<p><img src="https://www.bitflux.ai/pics/memory_is_slow_part2/performance4.png" alt="performance4"></p>
<p>Comparing the results of "fully in-memory (50GB)" which is pre-loaded in memory before measuring against the "unrolled loop" that is only cached in memory we see 40% overhead.  That's 2.75 seconds out of 9 seconds that was spent waiting on the caching system instead of counting.  Why so much?</p>
<p><strong>mmap()</strong></p>
<p>The mmap() call presents the process with a buffer that is a blank slate even when the data is already in memory.  The buffer is populated page by page as it's accessed from the page cache.  This isn't a copy, it's just the operating system mapping the cached memory into the process.  This costs more than it might seem.  The worst case with mmap() the counting has to pause at every 4KB page boundary while the kernel processes a fault, tracks down the page of data in the page cache, then updates the page table of the process to insert the memory into the process.  Fundamentally this is a process that is limited by the memory latency, not the CPU speed or memory bandwidth.  With the potential for TLB walks and searching lists that track the page cache, we’re taking potentially dozens of CPU cache misses and several microseconds of waiting on memory for every 4KB page.</p>
<p><strong>direct IO</strong></p>
<p>Using our direct from disk approach uses pipelines and streams which avoids the kind of memory latency dominated bottleneck that mmap() has.  In our case we're limited by the bandwidth of our disks yet because of the pipelining, the larger latency of the IOs doesn't get in the critical path of the counting very much.  Allowing for higher throughput.</p>
<h2 id="scaling">Scaling</h2>
<p>Consider the implications of these experiments as we scale.  The well vetted solution to get data from memory to a process is slower than using the disk directly.  This isn't because the memory is slower than the disk.  The memory has higher bandwidth than the disk, not by an order of magnitude, but a decent margin.  But the latency of the memory is orders of magnitude lower than the disk.  Nevertheless the <em>way</em> the data in memory is accessed is the culprit.  Its a synchronous approach that assumes memory operations are cheap and low latency.  These accesses add up and it ends up waiting on memory latencies.  The disk method on the other hand is as a streaming approach built to leverage bandwidth and hide latencies.</p>
<p><strong>extending the existing rig</strong></p>
<p>If I got a few more of these disks I could push the IO bandwidth to be greater than the 13GB/s per thread memory bandwidth limit.  IO is DMA'ed to buffers that are pretty small compared to the total dataset. These buffers scale with the throughput capabilities of the CPU and the disks, not the dataset size. The buffers can be located in a single numa domain allowing us to avoid the overhead of accessing the buffers between NUMA domains.  Add more disks to this system I might be able to create a disk based solution to count at the full 13GB/s rather than be limited to the 7.90GB/s we see with the in memory example at the full 50GB dataset.  With such a system our throughput would not be affected by the dataset size, unlike the in-memory case, which has numa overhead and eventually runs out of memory to scale.</p>
<p><strong>faster than memory is possible</strong></p>
<p>On a proper modern server the CPUs will let you do IO directly to the L3 cache, bypassing memory altogether.  Because PCIe bandwidth is higher than memory bandwidth, on paper we could even get more max bandwidth than we can get from memory if we carefully pin the buffers into the CPU cache.  I haven't confirm this works in practice, however, it could be made to work and is the sort of thing that CPU designs will be forced to lean into to push performance forward.</p>
<p><strong>memory is changing too</strong></p>
<p>This isn't just about disks vs memory.  Similar techniques and principles apply to memory.  Memory bandwidth is still scaling even if the latency is not.  This means to take advantage of memory performance you have to actually treat it more like a disk and less like Random Access Memory.  To scale performance with generational updates you have to make sure to stream data from memory into the CPU caches in blocks, similar to how data is streamed from disk to memory.  If not you end up with 90s level memory throughput.  A custom mechanism to cache data in memory could easily avoid the memory latency problems seen with the default mmap() solution with much less code than the io_uring solution.</p>
<h2 id="is-this-worth-it">Is this worth it?</h2>
<p>I'm not going to say that going to the effort of implementing something like this is always worth it.  The mmap() method is sure elegant from a coding perspective, especially when compared to all the code I had to write to get the io_uring setup working.  Sometimes the simple way is the way to go.</p>
<p>Is using 6 cores of IO for 1 core of compute is always the right answer?  Probably not.  This was an extreme situation to prove a point.  In realworld situations you'll need to look at the tradeoffs and decide what's best for your use case.  Correctly understanding the strengths and weaknesses of the hardware can open up a number of possibilities where you can get a lot more performance for a lot less money.</p>
<p>The kind of overhead demonstrated with mmap() isn’t going to go away, new hardware isn't going to fix it.  At the same time disk bandwidth and the number of cores are scaling each generation.  But doing things that scale performance with new technology is going to take extra code and effort.</p>
<p>But don't just blow this stuff off.  Sure you <em>can</em> dedicate a server with 3TB of memory to serve 10K client connections. Memory in the cloud is like ~$5/GB/month, if you can afford it, then you do you.  However it is worth considering that humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app.  I figure either the karmic debt to the planet, or a vengeful AI demigod hungry for silicon and electricity will come for those that don't heed this warning, eventually.  Either way my conscience is clear.</p>
<h2 id="recap">Recap</h2>
<ul>
<li>Memory is slow - when you use it oldschool.</li>
<li>Disk is fast - when you are clever with it.</li>
<li>Test the dogma - compounded exponentials are flipping somethings from true to false.</li>
</ul>
<p><strong>Bad news</strong> is that this cleverness requires extra code and effort.</p>
<p><strong>Good news</strong> is we now have AI to write and test the extra code this cleverness requires.</p>
<p><strong>Better news</strong> is that, for those that are willing to learn, AI's don't do this unless you know how to ask them.</p>
<p>Lean into things that scale, avoid things that don’t.</p>
<h2 id="next-time">Next Time</h2>
<p>What will be revealed in the next episode?</p>
<ul>
<li>Is O(√n) actually faster than O(log n)?  Will the foundations of Computer Science survive this unveiling?</li>
<li>Will traditional code be consumed into the latent space of our AI overlords?</li>
<li>Is AI hiding these performance gains from me?  Is AI even capable of writing optimized code?</li>
</ul>
<hr>
<p><em>Jared Hulbert</em></p>
<blockquote>
<p>A few notes for the "um actually" haters commenting on Hacker News:</p>
<ul>
<li>This is not and does not claim to be an academic paper.</li>
<li>I do not intend to prove that NAND is a drop in replacement for DRAM.</li>
<li>Tis but a humble and hopefully fun exercise in exploring the limits and trends of modern hardware and the tradeoffs needed to maximize performance.</li>
<li>As I stated before I have no problem with your choice to ignore this and write lazy code that will perform just as fast on new hardware in 15 years as it does on todays hardware.  In fact I applaud your choice.  Jeff Bezos has an orbital yacht to build, someone has to pay for it, why not you?</li>
<li>I am not an AI.  I am a human with a computer that don't write perfect.</li>
</ul>
</blockquote>
<blockquote>
<p>source code can be found <a href="https://github.com/bitflux-ai/blog_notes">here</a>.</p>
</blockquote>

            </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[What If OpenDocument Used SQLite? (146 pts)]]></title>
            <link>https://www.sqlite.org/affcase1.html</link>
            <guid>45132498</guid>
            <pubDate>Thu, 04 Sep 2025 21:36:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.sqlite.org/affcase1.html">https://www.sqlite.org/affcase1.html</a>, See on <a href="https://news.ycombinator.com/item?id=45132498">Hacker News</a></p>
<div id="readability-page-1" class="page">
<div>
<p><a href="https://www.sqlite.org/index.html">
<img src="https://www.sqlite.org/images/sqlite370_banner.svg" alt="SQLite">
</a></p>
<p>
Small. Fast. Reliable.<br>Choose any three.
</p>



</div>






<h2>Introduction</h2>

<p>Suppose the
<a href="http://en.wikipedia.org/wiki/OpenDocument">OpenDocument</a> file format,
and specifically the "ODP" OpenDocument Presentation format, were
built around SQLite.  Benefits would include:
</p><ul>
<li>Smaller documents
</li><li>Faster File/Save times
</li><li>Faster startup times
</li><li>Less memory used
</li><li>Document versioning
</li><li>A better user experience
</li></ul>

<p>
Note that this is only a thought experiment.
We are not suggesting that OpenDocument be changed.
Nor is this article a criticism of the current OpenDocument
design.  The point of this essay is to suggest ways to improve
future file format designs.

</p><h2>About OpenDocument And OpenDocument Presentation</h2>

<p>
The OpenDocument file format is used for office applications:
word processors, spreadsheets, and presentations.  It was originally
designed for the OpenOffice suite but has since been incorporated into
other desktop application suites.  The OpenOffice application has been
forked and renamed a few times.  This author's primary use for OpenDocument is 
building slide presentations with either 
<a href="https://www.neooffice.org/neojava/en/index.php">NeoOffice</a> on Mac, or
<a href="http://www.libreoffice.org/">LibreOffice</a> on Linux and Windows.

</p><p>
An OpenDocument Presentation or "ODP" file is a
<a href="http://en.wikipedia.org/wiki/Zip_%28file_format%29">ZIP archive</a> containing
XML files describing presentation slides and separate image files for the
various images that are included as part of the presentation.
(OpenDocument word processor and spreadsheet files are similarly
structured but are not considered by this article.) The reader can
easily see the content of an ODP file by using the "zip -l" command.
For example, the following is the "zip -l" output from a 49-slide presentation
about SQLite from the 2014
<a href="http://southeastlinuxfest.org/">SouthEast LinuxFest</a>
conference:

</p><blockquote><pre>Archive:  self2014.odp
  Length      Date    Time    Name
---------  ---------- -----   ----
       47  2014-06-21 12:34   mimetype
        0  2014-06-21 12:34   Configurations2/statusbar/
        0  2014-06-21 12:34   Configurations2/accelerator/current.xml
        0  2014-06-21 12:34   Configurations2/floater/
        0  2014-06-21 12:34   Configurations2/popupmenu/
        0  2014-06-21 12:34   Configurations2/progressbar/
        0  2014-06-21 12:34   Configurations2/menubar/
        0  2014-06-21 12:34   Configurations2/toolbar/
        0  2014-06-21 12:34   Configurations2/images/Bitmaps/
    54702  2014-06-21 12:34   Pictures/10000000000001F40000018C595A5A3D.png
    46269  2014-06-21 12:34   Pictures/100000000000012C000000A8ED96BFD9.png
<i>... 58 other pictures omitted...</i>
    13013  2014-06-21 12:34   Pictures/10000000000000EE0000004765E03BA8.png
  1005059  2014-06-21 12:34   Pictures/10000000000004760000034223EACEFD.png
   211831  2014-06-21 12:34   content.xml
    46169  2014-06-21 12:34   styles.xml
     1001  2014-06-21 12:34   meta.xml
     9291  2014-06-21 12:34   Thumbnails/thumbnail.png
    38705  2014-06-21 12:34   Thumbnails/thumbnail.pdf
     9664  2014-06-21 12:34   settings.xml
     9704  2014-06-21 12:34   META-INF/manifest.xml
---------                     -------
 10961006                     78 files
</pre></blockquote>

<p>
The ODP ZIP archive contains four different XML files:
content.xml, styles.xml, meta.xml, and settings.xml.  Those four files
define the slide layout, text content, and styling.  This particular
presentation contains 62 images, ranging from full-screen pictures to
tiny icons, each stored as a separate file in the Pictures
folder.  The "mimetype" file contains a single line of text that says:

</p><blockquote><pre>application/vnd.oasis.opendocument.presentation
</pre></blockquote>

<p>The purpose of the other files and folders is presently 
unknown to the author but is probably not difficult to figure out.

</p><h2>Limitations Of The OpenDocument Presentation Format</h2>

<p>
The use of a ZIP archive to encapsulate XML files plus resources is an
elegant approach to an application file format.
It is clearly superior to a custom binary file format.
But using an SQLite database as the
container, instead of ZIP, would be more elegant still.

</p><p>A ZIP archive is basically a key/value database, optimized for
the case of write-once/read-many and for a relatively small number
of distinct keys (a few hundred to a few thousand) each with a large BLOB
as its value.  A ZIP archive can be viewed as a "pile-of-files"
database.  This works, but it has some shortcomings relative to an
SQLite database, as follows:

</p><ol>
<li><p><b>Incremental update is hard.</b>
</p><p>
It is difficult to update individual entries in a ZIP archive.
It is especially difficult to update individual entries in a ZIP
archive in a way that does not destroy
the entire document if the computer loses power and/or crashes
in the middle of the update.  It is not impossible to do this, but
it is sufficiently difficult that nobody actually does it.  Instead, whenever
the user selects "File/Save", the entire ZIP archive is rewritten.  
Hence, "File/Save" takes longer than it ought, especially on
older hardware.  Newer machines are faster, but it is still bothersome
that changing a single character in a 50 megabyte presentation causes one
to burn through 50 megabytes of the finite write life on the SSD.

</p></li><li><p><b>Startup is slow.</b>
</p><p>
In keeping with the pile-of-files theme, OpenDocument stores all slide 
content in a single big XML file named "content.xml".  
LibreOffice reads and parses this entire file just to display
the first slide.
LibreOffice also seems to
read all images into memory as well, which makes sense seeing as when
the user does "File/Save" it is going to have to write them all back out
again, even though none of them changed.  The net effect is that
start-up is slow.  Double-clicking an OpenDocument file brings up a
progress bar rather than the first slide.
This results in a bad user experience.
The situation grows ever more annoying as
the document size increases.

</p></li><li><p><b>More memory is required.</b>
</p><p>
Because ZIP archives are optimized for storing big chunks of content, they
encourage a style of programming where the entire document is read into
memory at startup, all editing occurs in memory, then the entire document
is written to disk during "File/Save".  OpenOffice and its descendants
embrace that pattern.

</p><p>
One might argue that it is ok, in this era of multi-gigabyte desktops, to
read the entire document into memory.
But it is not ok.
For one, the amount of memory used far exceeds the (compressed) file size
on disk.  So a 50MB presentation might take 200MB or more RAM.  
That still is not a problem if one only edits a single document at a time.  
But when working on a talk, this author will typically have 10 or 15 different 
presentations up all at the same
time (to facilitate copy/paste of slides from past presentations) and so
gigabytes of memory are required.
Add in an open web browser or two and a few other 
desktop apps, and suddenly the disk is whirling and the machine is swapping.
And even having just a single document is a problem when working
on an inexpensive Chromebook retrofitted with Ubuntu.
Using less memory is always better.
</p>

</li><li><p><b>Crash recovery is difficult.</b>
</p><p>
The descendants of OpenOffice tend to segfault more often than commercial
competitors.  Perhaps for this reason, the OpenOffice forks make
periodic backups of their in-memory documents so that users do not lose
all pending edits when the inevitable application crash does occur.
This causes frustrating pauses in the application for the few seconds
while each backup is being made.
After restarting from a crash, the user is presented with a dialog box
that walks them through the recovery process.  Managing the crash
recovery this way involves lots of extra application logic and is
generally an annoyance to the user.

</p></li><li><p><b>Content is inaccessible.</b>
</p><p>
One cannot easily view, change, or extract the content of an 
OpenDocument presentation using generic tools.
The only reasonable way to view or edit an OpenDocument document is to open
it up using an application that is specifically designed to read or write
OpenDocument (read: LibreOffice or one of its cousins).  The situation
could be worse.  One can extract and view individual images (say) from
a presentation using just the "zip" archiver tool.  But it is not reasonable
try to extract the text from a slide.  Remember that all content is stored
in a single "context.xml" file.  That file is XML, so it is a text file.
But it is not a text file that can be managed with an ordinary text
editor.  For the example presentation above, the content.xml file
consist of exactly two lines. The first line of the file is just:

</p><blockquote><pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
</pre></blockquote>

<p>The second line of the file contains 211792 characters of
impenetrable XML.  Yes, 211792 characters all on one line.
This file is a good stress-test for a text editor.
Thankfully, the file is not some obscure
binary format, but in terms of accessibility, it might as well be
written in Klingon.
</p></li></ol>

<h2>First Improvement:  Replace ZIP with SQLite</h2>

<p>
Let us suppose that instead of using a ZIP archive to store its files,
OpenDocument used a very simple SQLite database with the following
single-table schema:

</p><blockquote><pre>CREATE TABLE OpenDocTree(
  filename TEXT PRIMARY KEY,  -- Name of file
  filesize BIGINT,            -- Size of file after decompression
  content BLOB                -- Compressed file content
);
</pre></blockquote>

<p>
For this first experiment, nothing else about the file format is changed.
The OpenDocument is still a pile-of-files, only now each file is a row
in an SQLite database rather than an entry in a ZIP archive.
This simple change does not use the power of a relational
database.  Even so, this simple change shows some improvements.

<a name="smaller"></a>

</p><p>
Surprisingly, using SQLite in place of ZIP makes the presentation
file smaller.  Really.  One would think that a relational database file
would be larger than a ZIP archive, but at least in the case of NeoOffice
that is not so.  The following is an actual screen-scrape showing
the sizes of the same NeoOffice presentation, both in its original 
ZIP archive format as generated by NeoOffice (self2014.odp), and 
as repacked as an SQLite database using the 
<a href="https://sqlite.org/sqlar/doc/trunk/README.md">SQLAR</a> utility:

</p><blockquote><pre>-rw-r--r--  1 drh  staff  10514994 Jun  8 14:32 self2014.odp
-rw-r--r--  1 drh  staff  10464256 Jun  8 14:37 self2014.sqlar
-rw-r--r--  1 drh  staff  10416644 Jun  8 14:40 zip.odp
</pre></blockquote>

<p>
The SQLite database file ("self2014.sqlar") is about a
half percent smaller than the equivalent ODP file!  How can this be?
Apparently the ZIP archive generator logic in NeoOffice
is not as efficient as it could be, because when the same pile-of-files
is recompressed using the command-line "zip" utility, one gets a file
("zip.odp") that is smaller still, by another half percent, as seen
in the third line above.  So, a well-written ZIP archive
can be slightly smaller than the equivalent SQLite database, as one would
expect.  But the difference is slight.  The key take-away is that an
SQLite database is size-competitive with a ZIP archive.

</p><p>
The other advantage to using SQLite in place of
ZIP is that the document can now be updated incrementally, without risk
of corrupting the document if a power loss or other crash occurs in the
middle of the update.  (Remember that writes to 
<a href="https://www.sqlite.org/atomiccommit.html">SQLite databases are atomic</a>.)   True, all the
content is still kept in a single big XML file ("content.xml") which must
be completely rewritten if so much as a single character changes.  But
with SQLite, only that one file needs to change.  The other 77 files in the
repository can remain unaltered.  They do not all have to be rewritten,
which in turn makes "File/Save" run much faster and saves wear on SSDs.

</p><h2>Second Improvement:  Split content into smaller pieces</h2>

<p>
A pile-of-files encourages content to be stored in a few large chunks.
In the case of ODP, there are just four XML files that define the layout
of all slides in a presentation.  An SQLite database allows storing
information in a few large chunks, but SQLite is also adept and efficient
at storing information in numerous smaller pieces.

</p><p>
So then, instead of storing all content for all slides in a single
oversized XML file ("content.xml"), suppose there was a separate table
for storing the content of each slide separately.  The table schema
might look something like this:

</p><blockquote><pre>CREATE TABLE slide(
  pageNumber INTEGER,   -- The slide page number
  slideContent TEXT     -- Slide content as XML or JSON
);
CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional
</pre></blockquote>

<p>The content of each slide could still be stored as compressed XML.
But now each page is stored separately.  So when opening a new document,
the application could simply run:

</p><blockquote><pre>SELECT slideContent FROM slide WHERE pageNumber=1;
</pre></blockquote>

<p>This query will quickly and efficiently return the content of the first
slide, which could then be speedily parsed and displayed to the user.
Only one page needs to be read and parsed in order to render the first screen,
which means that the first screen appears much faster and
there is no longer a need for an annoying progress bar.

</p><p>If the application wanted
to keep all content in memory, it could continue reading and parsing the
other pages using a background thread after drawing the first page.  Or,
since reading from SQLite is so efficient, the application might 
instead choose to reduce its memory footprint and only keep a single
slide in memory at a time.  Or maybe it keeps the current slide and the
next slide in memory, to facilitate rapid transitions to the next slide.

</p><p>
Notice that dividing up the content into smaller pieces using an SQLite
table gives flexibility to the implementation.  The application can choose
to read all content into memory at startup.  Or it can read just a
few pages into memory and keep the rest on disk.  Or it can read just a
single page into memory at a time.  And different versions of the application
can make different choices without having to make any changes to the
file format.  Such options are not available when all content is in
a single big XML file in a ZIP archive.

</p><p>
Splitting content into smaller pieces also helps File/Save operations
to go faster.  Instead of having to write back the content of all pages
when doing a File/Save, the application only has to write back those
pages that have actually changed.

</p><p>
One minor downside of splitting content into smaller pieces is that
compression does not work as well on shorter texts and so the size of
the document might increase.  But as the bulk of the document space 
is used to store images, a small reduction in the compression efficiency 
of the text content will hardly be noticeable, and is a small price 
to pay for an improved user experience.

</p><h2>Third Improvement:  Versioning</h2>

<p>
Once one is comfortable with the concept of storing each slide separately,
it is a small step to support versioning of the presentation.  Consider
the following schema:

</p><blockquote><pre>CREATE TABLE slide(
  slideId INTEGER PRIMARY KEY,
  derivedFrom INTEGER REFERENCES slide,
  content TEXT     -- XML or JSON or whatever
);
CREATE TABLE version(
  versionId INTEGER PRIMARY KEY,
  priorVersion INTEGER REFERENCES version,
  checkinTime DATETIME,   -- When this version was saved
  comment TEXT,           -- Description of this version
  manifest TEXT           -- List of integer slideIds
);
</pre></blockquote>

<p>
In this schema, instead of each slide having a page number that determines
its order within the presentation, each slide has a unique
integer identifier that is unrelated to where it occurs in sequence.
The order of slides in the presentation is determined by a list of
slideIds, stored as a text string in the MANIFEST column of the VERSION
table.
Since multiple entries are allowed in the VERSION table, that means that
multiple presentations can be stored in the same document.

</p><p>
On startup, the application first decides which version it
wants to display.  Since the versionId will naturally increase in time
and one would normally want to see the latest version, an appropriate
query might be:

</p><blockquote><pre>SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;
</pre></blockquote>

<p>
Or perhaps the application would rather use the
most recent checkinTime:

</p><blockquote><pre>SELECT manifest, versionId, max(checkinTime) FROM version;
</pre></blockquote>

<p>
Using a single query such as the above, the application obtains a list
of the slideIds for all slides in the presentation.  The application then
queries for the content of the first slide, and parses and displays that
content, as before.

</p><p>(Aside:  Yes, that second query above that uses "max(checkinTime)"
really does work and really does return a well-defined answer in SQLite.
Such a query either returns an undefined answer or generates an error
in many other SQL database engines, but in SQLite it does what you would 
expect: it returns the manifest and versionId of the entry that has the
maximum checkinTime.)

</p><p>When the user does a "File/Save", instead of overwriting the modified
slides, the application can now make new entries in the SLIDE table for
just those slides that have been added or altered.  Then it creates a
new entry in the VERSION table containing the revised manifest.

</p><p>The VERSION table shown above has columns to record a check-in comment
(presumably supplied by the user) and the time and date at which the File/Save
action occurred.  It also records the parent version to record the history
of changes.  Perhaps the manifest could be stored as a delta from the
parent version, though typically the manifest will be small enough that
storing a delta might be more trouble than it is worth.  The SLIDE table
also contains a derivedFrom column which could be used for delta encoding
if it is determined that saving the slide content as a delta from its
previous version is a worthwhile optimization.

</p><p>So with this simple change, the ODP file now stores not just the most
recent edit to the presentation, but a history of all historic edits.  The
user would normally want to see just the most recent edition of the
presentation, but if desired, the user can now go backwards in time to 
see historical versions of the same presentation.

</p><p>Or, multiple presentations could be stored within the same document.

</p><p>With such a schema, the application would no longer need to make
periodic backups of the unsaved changes to a separate file to avoid lost
work in the event of a crash.  Instead, a special "pending" version could
be allocated and unsaved changes could be written into the pending version.
Because only changes would need to be written, not the entire document,
saving the pending changes would only involve writing a few kilobytes of
content, not multiple megabytes, and would take milliseconds instead of
seconds, and so it could be done frequently and silently in the background.
Then when a crash occurs and the user reboots, all (or almost all)
of their work is retained.  If the user decides to discard unsaved changes, 
they simply go back to the previous version.

</p><p>
There are details to fill in here.
Perhaps a screen can be provided that displays all historical changes
(perhaps with a graph) allowing the user to select which version they
want to view or edit.  Perhaps some facility can be provided to merge
forks that might occur in the version history.  And perhaps the
application should provide a means to purge old and unwanted versions.
The key point is that using an SQLite database to store the content,
rather than a ZIP archive, makes all of these features much, much easier
to implement, which increases the possibility that they will eventually
get implemented.

</p><h2>And So Forth...</h2>

<p>
In the previous sections, we have seen how moving from a key/value
store implemented as a ZIP archive to a simple SQLite database
with just three tables can add significant capabilities to an application
file format.
We could continue to enhance the schema with new tables, with indexes
added for performance, with triggers and views for programming convenience,
and constraints to enforce consistency of content even in the face of
programming errors.  Further enhancement ideas include:
</p><ul>
<li> Store an <a href="https://www.sqlite.org/undoredo.html">automated undo/redo stack</a> in a database table so that
     Undo could go back into prior edit sessions.
</li><li> Add <a href="https://www.sqlite.org/fts3.html#fts4">full text search</a> capabilities to the slide deck, or across
     multiple slide decks.
</li><li> Decompose the "settings.xml" file into an SQL table that
     is more easily viewed and edited by separate applications.
</li><li> Break out the "Presenter Notes" from each slide into a separate
     table, for easier access from third-party applications and/or scripts.
</li><li> Enhance the presentation concept beyond the simple linear sequence of
     slides to allow for side-tracks and excursions to be taken depending on
     how the audience is responding.
</li></ul>

<p>
An SQLite database has a lot of capability, which
this essay has only begun to touch upon.  But hopefully this quick glimpse
has convinced some readers that using an SQL database as an application
file format is worth a second look.

</p><p>
Some readers might resist using SQLite as an application
file format due to prior exposure to enterprise SQL databases and
the caveats and limitations of those other systems.  
For example, many enterprise database
engines advise against storing large strings or BLOBs in the database
and instead suggest that large strings and BLOBs be stored as separate
files and the filename stored in the database.  But SQLite 
is not like that.  Any column of an SQLite database can hold
a string or BLOB up to about a gigabyte in size.  And for strings and
BLOBs of 100 kilobytes or less, 
<a href="https://www.sqlite.org/intern-v-extern-blob.html">I/O performance is better</a> than using separate
files.

</p><p>
Some readers might be reluctant to consider SQLite as an application
file format because they have been inculcated with the idea that all
SQL database schemas must be factored into
<a href="https://en.wikipedia.org/wiki/Third_normal_form">Third Normal Form (3NF)</a>
and store only small primitive data types such as strings and integers.  Certainly
relational theory is important and designers should strive to understand
it.  But, as demonstrated above, it is often quite acceptable to store
complex information as XML or JSON in text fields of a database.
Do what works, not what your database professor said you ought to do.

</p><h2>Review Of The Benefits Of Using SQLite</h2>

<p>
In summary,
the claim of this essay is that using SQLite as a container for an application
file format like OpenDocument
and storing lots of smaller objects in that container
works out much better than using a ZIP archive holding a few larger objects.
To wit:

</p><ol>
<li><p>
An SQLite database file is approximately the same size, and in some cases
smaller, than a ZIP archive holding the same information.

</p></li><li><p>
The <a href="https://www.sqlite.org/atomiccommit.html">atomic update capabilities</a>
of SQLite allow small incremental changes
to be safely written into the document.  This reduces total disk I/O
and improves File/Save performance, enhancing the user experience.

</p></li><li><p>
Startup time is reduced by allowing the application to read in only the
content shown for the initial screen.  This largely eliminates the
need to show a progress bar when opening a new document.  The document
just pops up immediately, further enhancing the user experience.

</p></li><li><p>
The memory footprint of the application can be dramatically reduced by
only loading content that is relevant to the current display and keeping
the bulk of the content on disk.  The fast query capability of SQLite
make this a viable alternative to keeping all content in memory at all times.
And when applications use less memory, it makes the entire computer more
responsive, further enhancing the user experience.

</p></li><li><p>
The schema of an SQL database is able to represent information more directly
and succinctly than a key/value database such as a ZIP archive.  This makes
the document content more accessible to third-party applications and scripts
and facilitates advanced features such as built-in document versioning, and
incremental saving of work in progress for recovery after a crash.
</p></li></ol>

<p>
These are just a few of the benefits of using SQLite as an application file
format — the benefits that seem most likely to improve the user
experience for applications like OpenOffice.  Other applications might
benefit from SQLite in different ways. See the <a href="https://www.sqlite.org/appfileformat.html">Application File Format</a>
document for additional ideas.

</p><p>
Finally, let us reiterate that this essay is a thought experiment.
The OpenDocument format is well-established and already well-designed.
Nobody really believes that OpenDocument should be changed to use SQLite
as its container instead of ZIP.  Nor is this article a criticism of
OpenDocument for not choosing SQLite as its container since OpenDocument
predates SQLite.  Rather, the point of this article is to use OpenDocument
as a concrete example of how SQLite can be used to build better 
application file formats for future projects.
</p><p><small><i>This page last modified on  <a href="https://sqlite.org/docsrc/honeypot" id="mtimelink" data-href="https://sqlite.org/docsrc/finfo/pages/affcase1.in?m=26b0aa442a">2025-05-12 11:56:41</a> UTC </i></small></p>

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[Amazon RTO policy is costing it top tech talent, according to internal document (122 pts)]]></title>
            <link>https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9</link>
            <guid>45132093</guid>
            <pubDate>Thu, 04 Sep 2025 20:56:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9">https://www.businessinsider.com/amazon-rto-policy-costing-it-top-tech-talent-ai-recruiters-2025-9</a>, See on <a href="https://news.ycombinator.com/item?id=45132093">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="post-body" data-component-type="post-body" data-load-strategy="exclude" data-lock-content="">
            
            
            
            <div data-component-type="post-hero" data-load-strategy="exclude">
                
                <figure data-type="img" data-e2e-name="image-figure-image" data-media-container="image" itemscope="" itemtype="https://schema.org/ImageObject">
                    <div>
                      <meta itemprop="contentUrl" content="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700">
                      <p><img src="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700" srcset="https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=400&amp;format=jpeg&amp;auto=webp 400w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=500&amp;format=jpeg&amp;auto=webp 500w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=700&amp;format=jpeg&amp;auto=webp 700w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=1000&amp;format=jpeg&amp;auto=webp 1000w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=1300&amp;format=jpeg&amp;auto=webp 1300w, https://i.insider.com/6520574a9f7ca8b2bbdc599b?width=2000&amp;format=jpeg&amp;auto=webp 2000w" sizes="(min-width: 1280px) 900px" alt="A man walks on the street near the Amazon headquarters in Seattle, featuring large glass domes." decoding="sync">
                    </p></div>
                
                  <span>
                        <span>
                          
                          <label for="caption-drawer-btn">
                            <svg role="img" xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 24 24">
                              <path fill="currentColor" fill-rule="evenodd" d="m4.56 18.5 7.486-7.72 7.394 7.626 2.56-2.64L12.046 5.5 2 15.86l2.56 2.64Z"></path>
                            </svg>        </label>
                  
                          <figcaption data-e2e-name="image-caption">
                            <span>Amazon's Seattle headquarters.</span>
                            <span>
                              <span data-e2e-name="image-source" itemprop="creditText">
                                David Ryder/Getty Images
                              </span>          </span>
                          </figcaption>
                        </span>
                  </span></figure>
            </div>
    
    
    
              
      
            
      
              
              
              
              <div data-component-type="post-summary-bullets" data-load-strategy="exclude" data-track-marfeel="post-summary-bullets">
                <ul>
                    <li>Amazon's strict return-to-office policy is limiting its recruitment efforts.</li>
                    <li>The policy requires employees to work in an office 5 days a week and relocate to hubs.</li>
                    <li>Amazon's AI reputation and pay structure also challenge its ability to attract talent.</li>
                </ul>
              </div>
      
            
            
            
            
            <section data-component-type="post-body-content" data-load-strategy="exclude" data-track-content="" data-post-type="story" data-track-marfeel="post-body-content">
            
                <p><a target="_self" href="https://www.businessinsider.com/amazon-stricter-performance-review-process-leadership-principles-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">Amazon's</a> 5-day return-to-office policy may be restoring <a target="_self" href="https://www.businessinsider.com/inside-amazons-hardcore-culture-reset-day-1-roots-2025-9" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">discipline</a>, but it's costing the company in the war for tech talent.</p><p>An internal document and accounts from Amazon insiders show that the company's aggressive in-office work policy and requirement to move near designated <a target="_self" href="https://www.businessinsider.com/amazon-voluntary-resignation-employees-relocate-rto-2023-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">"hub"</a> offices are hampering recruiting efforts.</p><p>The hub strategy is listed as one of the "hotly debated topics" for Amazon's recruiters, as it is limiting the ability to hire "high-demand talent, like those with GenAI skills," according to the internal document from late last year, obtained by Business Insider.</p><p>Some Amazon recruiters told Business Insider that, starting last year, they saw an increase in candidates declining job offers specifically because of RTO. Those people were open to lower pay from other companies in exchange for the flexibility to work remotely.</p><p>One of the recruiters said the company is losing out on tech talent due to this. The people who spoke with Business Insider asked not to be identified discussing sensitive topics.</p>
                  
                <p>In addition to the strict RTO rules, Amazon has flagged its unusual pay structure and lagging AI reputation as obstacles to recruitment, BI previously <a target="_self" href="https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">reported</a>.</p><p>The stakes are high. Amazon is racing to stay ahead in the highly competitive generative AI space, but without attracting top talent, the company risks falling behind.</p><p>Oracle, for example, has hired away more than 600 Amazon employees in the past 2 years because Amazon's strict RTO policy has made poaching easier, Bloomberg reported recently.</p>
              
              
              
            <p>In an email to BI, Amazon's spokesperson said that the premise of this story was wrong, adding that the company continues to attract and retain some of the best people in the world."</p><p>"We are always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations," the spokesperson said. </p><p>Many firms are tightening RTO, but Amazon stands out. It demands 5 days in-office and ties compliance to <a target="_self" href="https://www.businessinsider.com/amazon-suspends-promotions-employees-not-meeting-rto-mandate-2023-11" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">promotions</a> and performance reviews. Those who refuse to relocate to "hubs" are considered by Amazon to have <a target="_self" href="https://www.businessinsider.com/amazon-voluntary-resignation-employees-relocate-rto-2023-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">voluntarily resigned</a>.</p><p>"We continue to believe that teams produce the best results when they're collaborating and inventing in person, and we've observed that to be true now that we've had most people back in the office each day for some time," the Amazon spokesperson said. </p><p>Wall Street is already <a target="_self" href="https://www.businessinsider.com/amazon-tumbles-ceo-andy-jassy-aws-cloud-ai-growth-concerns-2025-7" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}" rel="">noticing</a>. A recent report by venture capital firm SignalFire found Amazon on the lower end of engineer retention, behind Meta, OpenAI, and Anthropic.</p><p><em>Have a tip? Contact this reporter via email at </em><a target="_blank" href="mailto:ekim@businessinsider.com" data-track-click="{&quot;click_type&quot;:&quot;other&quot;,&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;outbound_click&quot;}" rel=" nofollow"><em><u>ekim@businessinsider.com</u></em></a><em> or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; </em><a target="_self" rel="" href="https://www.businessinsider.com/insider-guide-to-securely-sharing-whistleblower-information-about-powerful-institutions-2021-10" data-track-click="{&quot;element_name&quot;:&quot;body_link&quot;,&quot;event&quot;:&quot;tout_click&quot;,&quot;index&quot;:&quot;bi_value_unassigned&quot;,&quot;product_field&quot;:&quot;bi_value_unassigned&quot;}"><em><u>here's our guide to sharing information securely</u></em></a><em>.</em></p>
            
            
            </section>
            
            
            
            
              
            
    
    
    
    
      </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Classic 8×8-pixel B&W Mac patterns (131 pts)]]></title>
            <link>https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</link>
            <guid>45131538</guid>
            <pubDate>Thu, 04 Sep 2025 19:53:42 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html">https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</a>, See on <a href="https://news.ycombinator.com/item?id=45131538">Hacker News</a></p>
<div id="readability-page-1" class="page"><div itemprop="articleBody">
        <p><a href="https://paulsmith.github.io/classic-mac-patterns/"><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_preview.png" alt="all 38 patterns in a 2x19 grid"></a></p>
<p><strong><a href="https://paulsmith.github.io/classic-mac-patterns/">TL;DR: I made a website for the original classic Mac patterns</a></strong></p>
<p>I was working on something and thought it would be fun to use one of the
classic Mac black-and-white patterns in the project. I'm talking about the
original 8×8-pixel ones that were in the original Control Panel for setting the
desktop background and in MacPaint as fill patterns.</p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/controlpanel2.png" alt="pic of Control Panel in System 1"></p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/macpaint2.png" alt="pic of MacPaint"></p>
<p>Screenshots via to <a href="https://aresluna.org/frame-of-preference/">Marcin's awesome interactive
history</a></p>
<p>I figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere
on the web. And perhaps there are, but after poking around a bit, I ran out of
energy for that, but by then had a head of steam for extracting the patterns en
masse from the original source, somehow. Then I could produce whatever format I
needed for them.</p>
<p>There are 38 patterns, introduced in the original System 1.0 in the 1984 debut
of the Macintosh. They were unchanged in later versions, so I decided to get
them from a System 6 disk, since that's a little easier with access to utility
programs.</p>
<h2><span>Preparation</span></h2>
<ul>
<li>Download <a href="https://www.gryphel.com/c/minivmac/">Mini vMac</a>.</li>
<li>Acquire "<a href="https://en.wikipedia.org/wiki/Old_World_ROM">old world</a>" Mac ROMs.</li>
<li>Download a <a href="https://www.macintoshrepository.org/16994-mac-system-6-0-8-for-minivmac">System 6</a> startup disk image.</li>
<li>Download <a href="https://www.gryphel.com/c/minivmac/extras/exportfl/index.html">ExportFl</a> disk image.</li>
<li>Download <a href="https://www.gryphel.com/c/minivmac/extras/sitpack/index.html">sitPack</a> disk image.</li>
<li>Install "<a href="https://theunarchiver.com/">The Unarchiver</a>" (<code>brew install --cask the-unarchiver</code>)</li>
<li>Install the <a href="https://kagi.com/search?q=xcode+command+line+tools">Xcode command-line tools</a>.</li>
</ul>
<h2><span>Extraction process</span></h2>
<p>Start System 6 (drag the ROM onto the Mini vMac icon, then drag the System 6
disk onto the window when you see the flashing floppy disk). Mount the ExportFl
and sitPack disks by dragging their files and dropping on the classic Mac
desktop.</p>
<h3><span>In emulation</span></h3>
<p>Double-click sitPack to launch the program. Command-O to open, then navigate to
the startup disk by clicking "Drive". Scroll to find "System Folder" and
double-click on it. Scroll to the bottom, select "System" and click "Open". Save
the output file as "System.sit" in the top-level of the startup disk. Quit
sitPack back to the Finder.</p>
<p>Start the ExportFl program. Command-O or pick "Open" from the "File" menu. Find
the "System.sit" created in the last step and click "Open". A regular file save
dialog will appear on the modern Mac, pick a location and save the file.</p>
<h3><span>On the modern Mac</span></h3>
<p>Drag the "System.sit" file onto The Unarchiver, or open the file from within it.
This will produce a file called "System" (with no extension).</p>
<p>Run DeRez (part of the Xcode developer command-line tools) on the System file.
I first added <code>/Library/Developer/CommandLineTools/usr/bin</code> to my <code>$PATH</code>, then
ran:</p>
<div><pre><span></span>$<span> </span>DeRez<span> </span>-only<span> </span>PAT<span>\#</span><span> </span>System<span> </span>&gt;<span> </span>patterns.r
</pre></div>

<p>This produces a text representation of the <code>PAT#</code> resource in the System file.
It's a series of bytes that comprise 38 8×8 patterns meant for QuickDraw
commands. There's a leading big-endian unsigned 16-bit number (<code>0026</code>) to indicate the number of 8-byte patterns to follow.</p>
<pre><code>data 'PAT#' (0, purgeable) {
	$"0026 FFFF FFFF FFFF FFFF DDFF 77FF DDFF"
	$"77FF DD77 DD77 DD77 DD77 AA55 AA55 AA55"
	$"AA55 55FF 55FF 55FF 55FF AAAA AAAA AAAA"
	$"AAAA EEDD BB77 EEDD BB77 8888 8888 8888"
	$"8888 B130 031B D8C0 0C8D 8010 0220 0108"
	$"4004 FF88 8888 FF88 8888 FF80 8080 FF08"
	$"0808 8000 0000 0000 0000 8040 2000 0204"
	$"0800 8244 3944 8201 0101 F874 2247 8F17"
	$"2271 55A0 4040 550A 0404 2050 8888 8888"
	$"0502 BF00 BFBF B0B0 B0B0 0000 0000 0000"
	$"0000 8000 0800 8000 0800 8800 2200 8800"
	$"2200 8822 8822 8822 8822 AA00 AA00 AA00"
	$"AA00 FF00 FF00 FF00 FF00 1122 4488 1122"
	$"4488 FF00 0000 FF00 0000 0102 0408 1020"
	$"4080 AA00 8000 8800 8000 FF80 8080 8080"
	$"8080 081C 22C1 8001 0204 8814 2241 8800"
	$"AA00 40A0 0000 040A 0000 0384 4830 0C02"
	$"0101 8080 413E 0808 14E3 1020 54AA FF02"
	$"0408 7789 8F8F 7798 F8F8 0008 142A 552A"
	$"1408"
};
</code></pre>
<p>It would have been simple enough to
parse this text, but I had Claude quickly make a <a href="https://github.com/paulsmith/mac-desktop-patterns/blob/3e033b4f0f3aedd9de089c440bcec3387478bcef/extract.py">Python
program</a>
to do so and output them in .pbm format, which is part of the Netpbm image
format class. This is a simple image format that is text-based, a '1' or a
'0' indicating a black or white pixel in a row and column.</p>
<p>For example, this subway tile pattern <img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_011-64x64.png" alt="example pattern11"> is represented like
this in .pbm:</p>
<pre><code>P1
8 8
1 1 1 1 1 1 1 1
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
</code></pre>
<p>From here, I can generate image files for the patterns in any format and
resolution I want, using ImageMagick or similar. It's important when scaling the
patterns to use <code>-filter point</code>, so that ImageMagick doesn't try to interpolate
the pixels it needs to fill in, which would lead to blurry results.</p>
<p><a href="https://paulsmith.github.io/classic-mac-patterns/"><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_preview.png" alt="all 38 patterns in a 2x19 grid"></a></p>
<h2><span>Why do all this?</span></h2>
<p>It's nostalgic, I have a fondness for these old patterns and the original B&amp;W
Mac aesthetic, it reminds me of playing games like Dark Castle and Glider,
messing around with HyperCard, and using Tex-Edit and hoarding early shareware
programs.</p>
<p>The whole point of the above is to get a copy of the System file out with the
<a href="https://en.wikipedia.org/wiki/Resource_fork">resource fork</a> intact, that's
where the desktop patterns live.</p>
<p>According to old classic Mac
<a href="https://developer.apple.com/library/archive/documentation/mac/pdf/ImagingWithQuickDraw.pdf">manuals</a>,
the patterns were QuickDraw bit-pattern resources, a simple bitmap of 8 bits
per row packed into 8 bytes (columns). It was fast for QuickDraw to copy them
over an area of the screen. For example the following pattern was used for the
default gray desktop pattern on black-and-white Mac screens.</p>
<p><img src="https://www.pauladamsmith.com/images/classic-mac-patterns/pattern_03_64x64.png" alt=""></p>
<p>I could have extracted all 38 patterns other ways: I could have screenshotted
each one, I could have looked at each one and hand-written .pbm files, both of
which would have been tedious and error-prone.</p>
<p>Ultimately, I wanted to extract the exact original data from the source (or
close enough copy thereof) and have the patterns in a format I considered
archival for this limited purpose (.pbm files are trivial to parse and
manipulate).</p>
<p>Head over to my <a href="https://paulsmith.github.io/classic-mac-patterns/">pattern site</a> to get the patterns for yourself.</p>
<p>(Credit for replica <a href="https://fontstruct.com/fontstructions/show/1744455/geneva-9-2">Geneva 9pt</a> and <a href="https://fontstruct.com/fontstructions/show/2230457/chicago-12-13-11">Chicago 12pt</a> fonts)</p>


        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[LLM Visualization (308 pts)]]></title>
            <link>https://bbycroft.net/llm</link>
            <guid>45130260</guid>
            <pubDate>Thu, 04 Sep 2025 18:06:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://bbycroft.net/llm">https://bbycroft.net/llm</a>, See on <a href="https://news.ycombinator.com/item?id=45130260">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>LLM Visualization</p><div><p><a href="https://bbycroft.net/">Home</a></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI not affecting job market much so far, New York Fed says (103 pts)]]></title>
            <link>https://money.usnews.com/investing/news/articles/2025-09-04/ai-not-affecting-job-market-much-so-far-new-york-fed-says</link>
            <guid>45129267</guid>
            <pubDate>Thu, 04 Sep 2025 16:47:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://money.usnews.com/investing/news/articles/2025-09-04/ai-not-affecting-job-market-much-so-far-new-york-fed-says">https://money.usnews.com/investing/news/articles/2025-09-04/ai-not-affecting-job-market-much-so-far-new-york-fed-says</a>, See on <a href="https://news.ycombinator.com/item?id=45129267">Hacker News</a></p>
Couldn't get https://money.usnews.com/investing/news/articles/2025-09-04/ai-not-affecting-job-market-much-so-far-new-york-fed-says: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[A PM's Guide to AI Agent Architecture (137 pts)]]></title>
            <link>https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture</link>
            <guid>45129237</guid>
            <pubDate>Thu, 04 Sep 2025 16:45:39 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture">https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture</a>, See on <a href="https://news.ycombinator.com/item?id=45129237">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p>Last week, I was talking to a PM who'd in the recent months shipped their AI agent. The metrics looked great: 89% accuracy, sub-second respond times, positive user feedback in surveys. But users were abandoning the agent after their first real problem, like a user with both a billing dispute and a locked account.</p><blockquote><p>"Our agent could handle routine requests perfectly, but when faced with complex issues, users would try once, get frustrated, and immediately ask for a human."</p></blockquote><p>This pattern is observed across every product team that focuses on making their agents "smarter" when the real challenge is making architectural decisions that shape how users experience and begin to trust the agent. </p><p>In this post, I'm going to walk you through the different layers of AI agent architecture. How your product decisions determine whether users trust your agent or abandon it. By the end of this, you'll understand why some agents feel "magical" while others feel "frustrating" and more importantly, how PMs should architect for the magical experience.</p><p>We'll use a concrete customer support agent example throughout, so you can see exactly how each architectural choice plays out in practice. We’ll also see why the counterintuitive approach to trust (hint: it's not about being right more often) actually works better for user adoption.</p><p>You're the PM building an agent that helps users with account issues - password resets, billing questions, plan changes. Seems straightforward, right?</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!lsF4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!lsF4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 424w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 848w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 1272w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!lsF4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp" width="422" height="294.0531914893617" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:786,&quot;width&quot;:1128,&quot;resizeWidth&quot;:422,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;ConversationRelay | Twilio&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="ConversationRelay | Twilio" title="ConversationRelay | Twilio" srcset="https://substackcdn.com/image/fetch/$s_!lsF4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 424w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 848w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 1272w, https://substackcdn.com/image/fetch/$s_!lsF4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4bee9fce-5586-4d0c-a98b-d54f816e1a85_1128x786.webp 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p><span>But when a user says </span><em><strong>"I can't access my account and my subscription seems wrong"</strong></em><span> what should happen?</span></p><div><p><strong>Scenario A:</strong><span> Your agent immediately starts checking systems. It looks up the account, identifies that the password was reset yesterday but the email never arrived, discovers a billing issue that downgraded the plan, explains exactly what happened, and offers to fix both issues with one click.</span></p><p><strong>Scenario B: </strong><span>Your agent asks clarifying questions. "When did you last successfully log in? What error message do you see? Can you tell me more about the subscription issue?" After gathering info, it says "Let me escalate you to a human who can check your account and billing."</span></p><p><strong>Same user request. Same underlying systems. Completely different products.</strong></p></div><p>Think of agent architecture like a stack where each layer represents a product decision you have to make.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!JMHA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!JMHA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 424w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 848w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 1272w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!JMHA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png" width="406" height="456.49751243781094" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:904,&quot;width&quot;:804,&quot;resizeWidth&quot;:406,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!JMHA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 424w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 848w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 1272w, https://substackcdn.com/image/fetch/$s_!JMHA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d1c3e46-44f3-4986-b1b1-b50c7e9ffb4e_804x904.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>The Decision: </strong><span>How much should your agent remember, and for how long?</span></p><p><em>This isn't just technical storage - it's about creating the illusion of understanding. Your agent's memory determines whether it feels like talking to a robot or a knowledgeable colleague.</em></p><p><strong>For our support agent:</strong><span> Do you store just the current conversation, or the customer's entire support history? Their product usage patterns? Previous complaints? </span></p><p><strong>Types of memory to consider:</strong></p><ul><li><p><strong>Session memory:</strong><span> Current conversation ("You mentioned billing issues earlier...")</span></p></li><li><p><strong>Customer memory: </strong><span>Past interactions across sessions ("Last month you had a similar issue with...")</span></p></li><li><p><strong>Behavioral memory:</strong><span> Usage patterns ("I notice you typically use our mobile app...")</span></p></li><li><p><strong>Contextual memory: </strong><span>Current account state, active subscriptions, recent activity</span></p></li></ul><p>The more your agent remembers, the more it can anticipate needs rather than just react to questions. Each layer of memory makes responses more intelligent but increases complexity and cost.</p><p><strong>The Decision:</strong><span> Which systems should your agent connect to, and what level of access should it have?</span></p><p><em>The deeper your agent connects to user workflows and existing systems, the harder it becomes for users to switch. This layer determines whether you're a tool or a platform.</em></p><p><strong>For our support agent:</strong><span> Should it integrate with just your Stripe’s billing system, or also your Salesforce CRM, ZenDesk ticketing system , user database, and audit logs? Each integration makes the agent more useful but also creates more potential failure points - </span><em>think API rate limits, authentication challenges, and system downtime</em><span>.</span></p><p>Here's what's interesting - Most of us get stuck trying to integrate with everything at once. But the most successful agents started with just 2-3 key integrations and added more based on what users actually asked for.</p><p><strong>The Decision:</strong><span> Which specific capabilities should your agent have, and how deep should they go?</span></p><p><em>Your skills layer is where you win or lose against competitors. It's not about having the most features - it's about having the right capabilities that create user dependency.</em></p><p><strong>For our support agent:</strong><span> Should it only read account information, or should it also modify billing, reset passwords, and change plan settings? Each additional skill increases user value but also increases complexity and risk.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!AfC4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!AfC4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 424w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 848w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 1272w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!AfC4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png" width="442" height="359.07678883071554" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:931,&quot;width&quot;:1146,&quot;resizeWidth&quot;:442,&quot;bytes&quot;:147006,&quot;alt&quot;:&quot;How to Use Model Context Protocol for Scalable AI Integrations?&quot;,&quot;title&quot;:&quot;How to Use Model Context Protocol for Scalable AI Integrations?&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="How to Use Model Context Protocol for Scalable AI Integrations?" title="How to Use Model Context Protocol for Scalable AI Integrations?" srcset="https://substackcdn.com/image/fetch/$s_!AfC4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 424w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 848w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 1272w, https://substackcdn.com/image/fetch/$s_!AfC4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ace5e82-816b-426e-abc7-b9c8733a14d5_1146x931.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Implementation note:</strong><span> </span><a href="https://www.productcurious.com/p/the-new-distribution-game-building" rel="">Tools like MCP</a><span> (Model Context Protocol) are making it much easier to build and share skills across different agents, rather than rebuilding capabilities from scratch. </span></p><p><strong>The Decision:</strong><span> How do you measure success and communicate agent limitations to users?</span></p><p><em>This layer determines whether users develop confidence in your agent or abandon it after the first mistake. It's not just about being accurate - it's about being trustworthy.</em></p><p><strong>For our support agent:</strong><span> Do you show confidence scores ("I'm 85% confident this will fix your issue")? Do you explain your reasoning ("I checked three systems and found...")? Do you always confirm before taking actions ("Should I reset your password now?")? Each choice affects how users perceive reliability.</span></p><p><strong>Trust strategies to consider:</strong></p><ul><li><p><strong>Confidence indicators:</strong><span> "I'm confident about your account status, but let me double-check the billing details"</span></p></li><li><p><strong>Reasoning transparency:</strong><span> "I found two failed login attempts and an expired payment method"</span></p></li><li><p><strong>Graceful boundaries:</strong><span> "This looks like a complex billing issue - let me connect you with our billing specialist who has access to more tools"</span></p></li><li><p><strong>Confirmation patterns:</strong><span> When to ask permission vs. when to act and explain</span></p></li></ul><blockquote><p>The counterintuitive insight: users trust agents more when they admit uncertainty than when they confidently make mistakes.</p></blockquote><p>Okay, so you understand the layers. Now comes the practical question that every PM asks: "How do I actually implement this? How does the agent talk to the skills? How do skills access data? How does evaluation happen while users are waiting?"</p><p><em>Your orchestration choice determines everything about your development experience, your debugging process, and your ability to iterate quickly.</em></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!B42p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!B42p!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 424w, https://substackcdn.com/image/fetch/$s_!B42p!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 848w, https://substackcdn.com/image/fetch/$s_!B42p!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 1272w, https://substackcdn.com/image/fetch/$s_!B42p!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!B42p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png" width="1456" height="693" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:693,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:246009,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://www.productcurious.com/i/172725542?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!B42p!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 424w, https://substackcdn.com/image/fetch/$s_!B42p!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 848w, https://substackcdn.com/image/fetch/$s_!B42p!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 1272w, https://substackcdn.com/image/fetch/$s_!B42p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ad3a5c4-6a5b-4553-9b3e-0567b150f3da_1610x766.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Lets walk through the main approaches, and I'll be honest about when each one works and when it becomes a nightmare.</p><p>Everything happens in one agent's context. </p><p><strong>For our support agent:</strong><span> </span><em>When the user says "I can't access my account," one agent handles it all - checking account status, identifying billing issues, explaining what happened, offering solutions. </em></p><p><strong>Why this works:</strong><span> Simple to build, easy to debug, predictable costs. You know exactly what your agent can and can't do.</span></p><p><strong>Why it doesn't:</strong><span> Can get expensive with complex requests since you're loading full context every time. Hard to optimize specific parts.</span></p><blockquote><p>Most teams start here, and honestly, many never need to move beyond it. If you're debating between this and something more complex, start here.</p></blockquote><p>You have a router that figures out what the user needs, then hands off to specialized skills.</p><p><strong>For our support agent:</strong><span> </span><em>Router realizes this is an account access issue and routes to the `LoginSkill`. If the LoginSkill discovers it's actually a billing problem, it hands off to `BillingSkill`.</em></p><p><strong>Real example flow:</strong></p><ol><li><p><em>User: "I can't log in"</em></p></li><li><p><em>Router → LoginSkill</em></p></li><li><p><em>LoginSkill checks: Account exists ✓, Password correct ✗, Billing status... wait, subscription expired</em></p></li><li><p><em>LoginSkill → BillingSkill: "Handle expired subscription for user123"</em></p></li><li><p><em>BillingSkill handles renewal process</em></p></li></ol><p><strong>Why this works:</strong><span> More efficient - you can use cheaper models for simple skills, expensive models for complex reasoning. Each skill can be optimized independently.</span></p><p><strong>Why it doesn't:</strong><span> Coordination between skills gets tricky fast. Who decides when to hand off? How do skills share context?</span></p><blockquote><p>Here's where MCP really helps - it standardizes how skills expose their capabilities, so your router knows what each skill can do without manually maintaining that mapping.</p></blockquote><p>You predefine step-by-step processes for common scenarios. Think LangGraph, CrewAI, AutoGen, N8N, etc.</p><p><strong>For our support agent:</strong><span> </span><em>"Account access problem" triggers a workflow:</em></p><ol><li><p><em>Check account status</em></p></li><li><p><em>If locked, check failed login attempts  </em></p></li><li><p><em>If too many failures, check billing status</em></p></li><li><p><em>If billing issue, route to payment recovery</em></p></li><li><p><em>If not billing, route to password reset</em></p></li></ol><p><strong>Why this works:</strong><span> Everything is predictable and auditable. Perfect for compliance-heavy industries. Easy to optimize each step.</span></p><p><strong>Why it doesn't:</strong><span> When users have weird edge cases that don't fit your predefined workflows, you're stuck. Feels rigid to users.</span></p><p>Multiple specialized agents work together using A2A (agent-to-agent) protocols. </p><p><strong>The vision: </strong><span>Your agent discovers that another company's agent can help with issues, automatically establishes a secure connection, and collaborates to solve the customer's problem. </span><em>Think a booking.com agent interacting with an American Airlines agent!</em><span> </span></p><p><strong>For our support agent:</strong><span> </span><em>`AuthenticationAgent` handles login issues, `BillingAgent` handles payment problems, `CommunicationAgent` manages user interaction. They coordinate through standardized protocols to solve complex problems.</em></p><p><strong>Reality check:</strong><span> This sounds amazing but introduces complexity around security, billing, trust, and reliability that most companies aren't ready for. We're still figuring out the standards.</span></p><p>This can produce amazing results for sophisticated scenarios, but debugging multi-agent conversations is genuinely hard. When something goes wrong, figuring out which agent made the mistake and why is like detective work.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!KdJQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!KdJQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 424w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 848w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 1272w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!KdJQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png" width="1456" height="1196" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1196,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Overview&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Overview" title="Overview" srcset="https://substackcdn.com/image/fetch/$s_!KdJQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 424w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 848w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 1272w, https://substackcdn.com/image/fetch/$s_!KdJQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c4507e2-2478-4d44-aaee-82024692d318_1492x1226.png 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p><strong>Here's the thing:</strong><span> </span><strong>start simple</strong><span>. Single-agent architecture handles way more use cases than you think. Add complexity only when you hit real limitations, not imaginary ones.</span></p><p>But here's what's interesting - even with the perfect architecture, your agent can still fail if users don't trust it. That brings us to the most counterintuitive lesson about building agents.</p><p>Here's something counterintuitive: Users don't trust agents that are right all the time. They trust agents that are honest about when they might be wrong.</p><p>Think about it from the user's perspective. Your support agent confidently says "I've reset your password and updated your billing address." User thinks "great!" Then they try to log in and... it doesn't work. Now they don't just have a technical problem - they have a trust problem.</p><p>Compare that to an agent that says "I think I found the issue with your account. I'm 80% confident this will fix it. I'm going to reset your password and update your billing address. If this doesn't work, I'll immediately escalate to a human who can dive deeper."</p><p><strong>Same technical capability. Completely different user experience.</strong></p><p>Building trusted agents requires focus on three things:</p><ol><li><p><strong>Confidence calibration:</strong><span> When your agent says it's 60% confident, it should be right about 60% of the time. Not 90%, not 30%. Actual 60%.</span></p></li><li><p><strong>Reasoning transparency: </strong><span>Users want to see the agent's work. "I checked your account status (active), billing history (payment failed yesterday), and login attempts (locked after 3 failed attempts). The issue seems to be..."</span></p></li><li><p><strong>Graceful escalation: </strong><span>When your agent hits its limits, how does it hand off? A smooth transition to a human with full context is much better than "I can't help with that."</span></p></li></ol><p>A lot of times we obsess over making agents more accurate, when what users actually want was more transparency about the agent's limitations.</p><p>In Part 2, I'll dive deeper into the autonomy decisions that keep most PMs up at night. How much independence should you give your agent? When should it ask for permission vs forgiveness? How do you balance automation with user control?</p><p>We'll also walk through the governance concerns that actually matter in practice - not just theoretical security issues, but the real implementation challenges that can make or break your launch timeline.</p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Age Simulation Suit (168 pts)]]></title>
            <link>https://www.age-simulation-suit.com/</link>
            <guid>45129190</guid>
            <pubDate>Thu, 04 Sep 2025 16:41:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.age-simulation-suit.com/">https://www.age-simulation-suit.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45129190">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content">

<div id="inhalt01">

<h2>GERonTologic simulator GERT</h2>
<div><p>The age simulation suit <strong>GERT</strong> offers the opportunity to experience the impairments of older persons even for younger people.
</p><p>
The age-related impairments are:</p></div>
<p>■&nbsp;&nbsp;opacity of the eye lens</p>
<p>■&nbsp;&nbsp;narrowing of the visual field</p>
<p>■&nbsp;&nbsp;high-frequency hearing loss</p>
<p>■&nbsp;&nbsp;head mobility restrictions</p>
<p>■&nbsp;&nbsp;joint stiffness</p>
<p>■&nbsp;&nbsp;loss of strength</p>
<p>■&nbsp;&nbsp;reduced grip ability</p>
<p>■&nbsp;&nbsp;reduced coordination skills</p>
<h4>GERT for only  � 1390,‑ / � 1250,-</h4>
<p>complete as pictured, plus shipping and VAT if applicable<br>
New: now with 2 pairs of glasses instead of the model shown</p>

<p><br><a href="mailto:info@age-simulation-suit.com?subject=Request%20for%20quotation&amp;body=Please%20send%20me%20a%20quotation%20for%3A%0A%0A(%E2%86%93%20Please%20specify%20the%20quantity)%0A__%20Age%20simulation%20suit%20GERT%0A__%20Additional%20cervical%20collar%20and%2020%20changing%20covers%0A__%20Suitcase%0A__%20Overshoes%0A__%20Knee%20wraps%0A__%20Knee%20pain%20simulator%0A__%20Kyphosis%20simulator%0A__%20COPD%20simulator%0A__%20Tremor%20simulator%0A__%20Tinnitus%20simulator%0A__%20Simulation%20glasses%0A__%20Hemiparesis%20simulator%0A__%20Back%20pain%20simulator%0A%0ALegal%20entity%20and%20address%20for%20the%20quotation%3A%0A%0A%0A%0A%0A%0AVAT%20no.%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(for%20a%20VAT%20free%20delivery%20within%20the%20EU)%0AEmail%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(to%20send%20the%20quotation)%0ATel.%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(for%20any%20queries%20we%20may%20have)%0A%0ARemarks%3A%0A%0A%0A%0A%0A" title="request a quotation by email"><img src="https://www.age-simulation-suit.com/images/quotation.png" alt="Request a quotation" onmouseover="this.src='./images/quotation_2.png';" onmouseout="this.src='./images/quotation.png';"></a></p>
<p><a href="mailto:info@age-simulation-suit.com?subject=Order&amp;body=I%20hereby%20order%20according%20to%20the%20price%20list%20from%201st%20September%202024%20(www.age-simulation-suit.com%2Fdownload%2FPrice_list.pdf)%0A%0A(%E2%86%93%20Please%20specify%20the%20quantity)%0A__%20Age%20simulation%20suit%20GERT%0A__%20Additional%20cervical%20collar%20and%2020%20changing%20covers%0A__%20Suitcase%0A__%20Overshoes%0A__%20Knee%20wraps%0A__%20Knee%20pain%20simulator%0A__%20Kyphosis%20simulator%0A__%20COPD%20simulator%0A__%20Tremor%20simulator%0A__%20Tinnitus%20simulator%0A__%20Simulation%20glasses%0A__%20Hemiparesis%20simulator%0A__%20Back%20pain%20simulator%0A%0ALegal%20entity%20and%20address%20for%20the%20invoice%3A%0A%0A%0A%0A%0A%0AVAT%20no.%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(for%20a%20VAT%20free%20delivery%20within%20the%20EU)%0AEmail%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(to%20send%20an%20order%20confirmation)%0ATel.%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(for%20any%20queries%20we%20may%20have)%0A%0ADelivery%20address%3A%0A%0A%0A%0A%0A%0ADelivery%20date%0Anot%20before%3A%0Anot%20later%20than%3A%0A%0AConsignee%0AName%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(to%20be%20indicated%20to%20our%20shipping%20partner)%0AEmail%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(to%20send%20a%20shipping%20notification)%0ATel.%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20(for%20any%20queries%20we%20may%20have)%0A%0ARemarks%3A%0A%0A%0A%0A%0A" title="place an order by email"><img src="https://www.age-simulation-suit.com/images/order.png" alt="Place an order now" onmouseover="this.src='./images/order_2.png';" onmouseout="this.src='./images/order.png';"></a></p>



<p><br>Due to the significant increase in the time and effort required to process orders, in particular as a result of incomplete or incorrect information provided with orders, and the fact that we increasingly have to send reminders for invoices for smaller amounts, we can only accept orders with a value of at least 300 euros or pounds.</p>

<p><br><strong>Customer reviews:</strong></p>
<p><strong>The quality is great and it works how it is supposed to.</strong> I�m happy with my purchase.</p>
<p><strong>Great way to teach about elderly behavior.</strong> I�ve been using this suit for a while now and it�s very durable and easy to use. Thanks!!</p>
</div>

<p><a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_K.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT - complete without accessories"><img src="https://www.age-simulation-suit.com/images/age_simulation_suit.jpg" title="Image gallery - click to open" alt="Age simulation suit GERT"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_1.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + overshoes"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_2.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + overshoes"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_3.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + overshoes and 2 of 6 simulation glasses"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_4.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + reinforced knee wraps (right)"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_5.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + reinforced knee wraps and overshoes"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_8.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT - complete without accessories"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_9.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT - complete without accessories"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_10.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + reinforced knee wraps and overshoes"></a>
<a href="https://www.age-simulation-suit.com/lightbox/images/age-simulation-suit_11.jpg" rel="lightbox[GERT-2]" title="Age simulation suit GERT + reinforced knee wraps and overshoes"></a>
</p>

</div><p>For many years our<br>age simulation suit<br><strong>GERT</strong> has been by<br>far the most popular<br>product worldwide.</p><p>The European<br>Competence Centre<br>for Accessibility has<br>certified our age<br>simulation suit <strong>GERT</strong>.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Stripe Launches L1 Blockchain: Tempo (616 pts)]]></title>
            <link>https://tempo.xyz</link>
            <guid>45129085</guid>
            <pubDate>Thu, 04 Sep 2025 16:32:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://tempo.xyz">https://tempo.xyz</a>, See on <a href="https://news.ycombinator.com/item?id=45129085">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div aria-labelledby="why-blockchain-title"><h2 id="why-blockchain-title">Why create a<br>new blockchain?</h2><p>Stablecoins enable instant, borderless, programmable transactions, but current blockchain infrastructure isn’t designed for them: existing systems are either fully general or trading-focused. Tempo is a blockchain designed and built for real-world payments.</p></div><div><h2>Optimized for<br>real-world flows</h2><div><p>Tempo was started by Stripe and Paradigm, with design input from Anthropic, Coupang, Deutsche Bank, DoorDash, Lead Bank, Mercury, Nubank, OpenAI, Revolut, Shopify, Standard Chartered, Visa, and more.</p><p>If you’re a company with large, real-world economic flows and would like to help shape the future of Tempo, get in touch.</p></div><a href="mailto:partners@tempo.xyz?subject=Partnering%20with%20Tempo" target="_blank" title="Copy"><p>Partner with us</p></a></div><div><h2>Transform how <br>your business <br> moves money</h2><div><h2><span>01 ::</span> <!-- -->Purpose-built payments capabilities</h2><p>Optimize your financial flows with embedded payment features, including memo fields and batch&nbsp;transfers.</p></div><div><h2><span>02 ::</span> Speed and reliability</h2><p>Process over 100,000 transactions per second (TPS) with sub-second finality, enabling real-time payments at a global scale.</p></div><div><h2><span>03 ::</span> Predictable low fees</h2><p>Transform your cost structure with near-zero transaction fees that are highly predictable and can be paid in any stablecoin.</p></div><div><h2><span>04 ::</span> Built-in privacy measures</h2><p>Protect your users by keeping important transaction details private while maintaining compliance standards.</p></div></div><div><h2>Performant and <br>scalable for any <br>payments <br>use&nbsp;case</h2><div><h2><span>01 ::</span> Remittances</h2><p>Send money across borders instantly, securely, and at a fraction of traditional costs.</p></div><div><h2><span>02 ::</span> Global payouts</h2><p>Pay anyone, anywhere, in any currency—without banking delays or fees.</p></div><div><h2><span>03 ::</span> Embedded finance</h2><p>Build compliant, programmable payments—in any stablecoin—directly into your products.</p></div><div><h2><span>04 ::</span> <!-- -->Microtransactions</h2><p>Enable sub-cent payments for digital goods and on-demand services.</p></div><div><h2><span>05 ::</span> Agentic commerce</h2><p>Facilitate low-cost, instant payments for agents to autonomously execute transactions.</p></div><div><h2><span>06 ::</span> Tokenized deposits</h2><p>Move customer funds onchain for instant settlement and efficient interbank transfers.</p></div></div><div><h2>Technical<br>features</h2><div><div><h2><span>01 ::</span> Fee flexibility</h2><p>Pay transaction fees in any stablecoin.</p></div><div><h2><span>02 ::</span> Dedicated payments lane</h2><p>Transfer funds cheaply and reliably in blockspace that’s isolated from other activity.</p></div><div><h2><span>03 ::</span> <!-- -->Stablecoin interoperability</h2><p>Swap stablecoins, including custom-issued ones, natively with low fees.</p></div><div><h2><span>04 ::</span> Batch transfers</h2><p>Send multiple transactions onchain at once with native account abstraction.</p></div><div><h2><span>05 ::</span> <!-- -->Blocklists <span>/</span> <!-- -->allowlists</h2><p>Meet compliance standards by setting user-level permissions for transactions.</p></div><div><h2><span>06 ::</span> Memo fields</h2><p>Speed up reconciliation with offchain transactions by adding context that’s compatible with ISO 20022 standards.</p></div></div></div><div><h2>Frequently<br>asked questions</h2><div><div><h2><span>01 ::</span> How is Tempo different from other blockchains?</h2><p>Tempo is an EVM-compatible L1 blockchain, purpose-built for payments. It doesn’t displace other general-purpose blockchains; rather, it incorporates design choices that meet the needs of high-volume payment use cases. These include predictable low fees in a dedicated payments lane, stablecoin neutrality, a built-in stablecoin exchange, high throughput, low latency, private transactions, payment memos compatible with standards like ISO 20022, compliance hooks, and more.</p></div><div><h2><span>02 ::</span> Who can build on Tempo?</h2><p>Tempo is a neutral, permissionless blockchain open for anyone to build on. We’re currently collaborating with global partners to test various use cases, including cross-border payouts, B2B payments, remittances, and ecommerce. Interested in working with Tempo? Request access to our private testnet<!-- --> <a href="mailto:partners@tempo.xyz?subject=Requesting%20access%20to%20Tempo" target="_blank" title="Copy">here</a>.</p></div><div><h2><span>03 ::</span> When will Tempo launch?</h2><p>We’re providing select partners with priority access to our testnet now. Contact us<!-- --> <a href="mailto:partners@tempo.xyz?subject=Partnering%20with%20Tempo" target="_blank" title="Copy">here</a> <!-- -->if you’re interested.</p></div><div><h2><span>04 ::</span> Who will run validator nodes?</h2><p>A diverse group of independent entities, including some of Tempo’s design partners, will run validator nodes initially before we transition to a permissionless model.</p></div></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google deletes net-zero pledge from sustainability website (333 pts)]]></title>
            <link>https://www.nationalobserver.com/2025/09/04/investigations/google-net-zero-sustainability</link>
            <guid>45128640</guid>
            <pubDate>Thu, 04 Sep 2025 15:51:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.nationalobserver.com/2025/09/04/investigations/google-net-zero-sustainability">https://www.nationalobserver.com/2025/09/04/investigations/google-net-zero-sustainability</a>, See on <a href="https://news.ycombinator.com/item?id=45128640">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p dir="ltr"><span>Google’s CEO Sundar Pichai stood smiling in a leafy-green California garden in September 2020 and declared that the IT behemoth was entering the “most ambitious decade yet” in its climate action.</span></p><p dir="ltr"><span>“Today, I’m proud to announce that we intend to be the first major company to operate carbon free — 24 hours a day, seven days a week, 365 days a year,” he said, in a&nbsp;</span><a href="https://www.youtube.com/watch?v=oPz-6eCXpCo"><span>video announcement</span></a><span>.</span></p><p dir="ltr"><span>Pichai added that he knew the “road ahead would not be easy,” but Google “aimed to prove that a carbon-free future is both possible and achievable fast enough to prevent the most dangerous impacts of climate change.”</span></p></div><div><p dir="ltr"><span>Five years on, just how hard Google’s “energy journey” would become is clear. In June, Google’s Sustainability website proudly boasted a&nbsp;</span><a href="https://web.archive.org/web/20250626204437/https://sustainability.google/operating-sustainably/"><span>headline pledge</span></a><span> to achieve net-zero emissions by 2030. By July, that had all changed.&nbsp;</span></p><p dir="ltr"><span>An investigation by&nbsp;</span><em>Canada’s National Observer</em><span> has found that Google’s net-zero pledge has&nbsp;quietly been scrubbed, demoted from having its own section on the site to an entry in the appendices of the company's sustainability report.</span></p><p dir="ltr"><span>Genna Schnurbach, an external spokesperson for Google,&nbsp;</span><a href="https://urldefense.com/v3/__https:/sustainability.google/reports/google-2025-environmental-report/__;!!IfJP2Nwhk5Z0yJ43lA!M4oP5acacqxrtEdl52f1SQT4d-pYzAmOrRLnHVshoF7VBBrgaFOK-szSZlvqRfnKtN9xGtYBCsKbszzmD718fuLYmg$"><span>referring to the report</span></a><span>, told us: “As you can see from the document, Google is still committed to their ambition of net zero by 2030.”&nbsp;</span></p></div><div><p dir="ltr"><span>By tracing back through the history of Google’s Sustainability website, we found that the company edited it in late June, removing almost all mention of its lauded net-zero goals. (A separate website referring to data centres specifically&nbsp;</span><a href="https://datacenters.google/operating-sustainably/"><span>has maintained its existing language</span></a><span> around net-zero commitments.)&nbsp;</span></p><div>  <blockquote>Five years ago, Google’s climate action ambitions were the gold standard for Big Tech. Then, with power demand spikes from AI data centres, in July it scrubbed its sustainability website of its 2030 net zero pledge.</blockquote>  </div><p dir="ltr"><span>The page on Operating Sustainably has been rebranded to Operations, and the section on net-zero carbon was deleted. In its place is a new priority area: Energy.&nbsp;</span></p><p dir="ltr"><span>“Running the global infrastructure behind our products and services, including AI, takes considerable energy,” said Google in its Environment 2025 report, which explained that it will be almost impossible to meet its erstwhile net-zero ambitions, partly due to its expansion in AI.</span></p><p dir="ltr"><span>These significant removals come as Big Tech is racing to build new, power-devouring, hyperscale data centres to capitalize on the global boom in artificial intelligence. They are also coming at a time when the Trump administration has targeted institutions that have environmental ambitions.</span></p><p dir="ltr"><span>“While we remain committed to our climate moonshots, it’s become clear that achieving them is now more complex and challenging across every level — from local to global,” the Google report authors state. In the same report last year, Net Zero Carbon was a key priority.&nbsp;</span></p><h2><strong>First in Big Tech to make net-zero pledge</strong></h2><p dir="ltr"><span>Google, which has data centres in Toronto, Ont., and Montreal, Que., was one of the first tech giants to set sweeping sustainability goals — and it appears to be one of the first to be considering sweeping them out of sight. The latest sustainability reports from Big Tech peers, Microsoft and Amazon, by contrast, still present net-zero emissions as a headline priority area.&nbsp;</span></p><figure role="group"><article>            <picture>                  <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/drop.png?itok=EF620-6s 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_2x/public/img/2025/09/03/drop.png?itok=VWBS_Ifa 2x" media="all and (min-width: 1024px)" type="image/png" width="1290" height="668">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_1x/public/img/2025/09/03/drop.png?itok=v1AjomkU 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_2x/public/img/2025/09/03/drop.png?itok=YzxPXgKQ 2x" media="all and (min-width: 768px)" type="image/png" width="800" height="414">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_1x/public/img/2025/09/03/drop.png?itok=ORex2_dz 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_2x/public/img/2025/09/03/drop.png?itok=VJSfhfNw 2x" media="all and (min-width: 430px)" type="image/png" width="475" height="246">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_1x/public/img/2025/09/03/drop.png?itok=iQBwMtIq 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_2x/public/img/2025/09/03/drop.png?itok=KZUrml9C 2x" media="all and (max-width: 429px)" type="image/png" width="400" height="207">                  <img loading="lazy" width="1290" height="668" src="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/drop.png?itok=EF620-6s" alt="">  </picture>    </article><figcaption>Canada's National Observer used the online tool Wayback Archive to examine the history of Google's operating sustainability page, discovering that it no longer lists net-zero as a headline priority area.</figcaption></figure><p dir="ltr"><span>Google’s yearly electricity consumption increased by 26 per cent in 2024 to 32.2 terrawatt-hours, which is almost as much as the consumption of Ireland. Last month, it released a technical report which revealed that a single chat message to its Gemini AI model consumes 0.24 watt-hours of energy, equivalent to 2.4 minutes of running a small LED bulb.&nbsp;</span></p><p dir="ltr"><span>A&nbsp;</span><a href="https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-cost-of-compute-a-7-trillion-dollar-race-to-scale-data-centers"><span>recent report from McKinsey &amp; Co</span></a><span> calculates that by 2030, it will cost $6.7 trillion worldwide in new investment to keep pace with the exploding demand for computing power. Data centres equipped to handle AI processing loads will need $5.2 trillion in investment, the consultancy predicted.&nbsp;</span></p><p dir="ltr"><span>The stratospheric rise in forecasted AI workloads could drive about 70 per cent of new electricity demand, McKinsey found, with the estimated global data centre capacity demand in its “mid-range” scenario expected to rise 3.5-fold to close to 250 gigawatts by 2030. In the US, this means the sector&nbsp;</span><a href="https://www.mckinsey.com/quarterly/the-five-fifty/five-fifty-more-data-more-demand"><span>would account for</span></a><span> almost 12 per cent of total national energy demand, up from just over five per cent today.</span></p><figure role="group"><article>            <picture>                  <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=QQ_I954C 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_2x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=WcuiiI8J 2x" media="all and (min-width: 1024px)" type="image/png" width="744" height="507">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_1x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=s3MDYAKV 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_2x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=vh9TsXq0 2x" media="all and (min-width: 768px)" type="image/png" width="744" height="507">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_1x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=Dw92MUbe 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_2x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=ESm9r3uH 2x" media="all and (min-width: 430px)" type="image/png" width="475" height="324">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_1x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=MMRShAjq 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_2x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=cru76Ucs 2x" media="all and (max-width: 429px)" type="image/png" width="400" height="273">                  <img loading="lazy" width="744" height="507" src="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/59931565-1756801309907648_origin.png?itok=QQ_I954C" alt="">  </picture>    </article><figcaption>McKinsey &amp; Co</figcaption></figure><p dir="ltr"><span>“The pressure to get any possible source of electricity is rather overwhelming right now, especially in the developed world, which hasn't touched its grid for 40 years,” Michael Barnard,&nbsp;a prominent clean energy technology analyst and self-styled climate futurist, told&nbsp;</span><em>Canada’s National Observer.&nbsp;</em></p><p dir="ltr"><span>This pressure was explored in a&nbsp;</span><a href="https://newclimate.org/resources/publications/corporate-climate-responsibility-monitor-2025-tech-sector"><span>recent report</span></a><span> from the New Climate Institute, a think-tank, which found that the tech sector is facing a "climate strategy crisis," where emissions targets have “lost their meaning amid soaring energy demand.”</span></p><h2><strong>AI power demand growth slows climate action</strong></h2><p dir="ltr"><span>For some in the sector, the power demand growth linked to AI expansion has become incompatible with their existing environmental commitments. And this inconvenient truth has coincided with the reelection of Donald Trump, whose administration has signalled it will roll back climate policies, and whose allies have disparaged corporate sustainability efforts as part of a “woke agenda.”&nbsp;</span></p><p dir="ltr"><span>Barnard noted “there's a lot of pandering to Trump going on," referring to Google chief investment officer Ruth Porat championing&nbsp;</span><a href="https://www.desmog.com/2025/08/19/google-president-praised-maga-speech-slamming-climate-extremist-agenda/"><span>expansion of the use of “incredibly clean” coal&nbsp;</span></a><span>plants and other fossil fuels for its future power data centres.&nbsp;Pichai himself attended Trump’s inauguration, to which Google donated $1 million.&nbsp;</span></p><p dir="ltr"><span>But Barnard pointed to the fact that Google continues to sign corporate power purchase deals for various renewable energy sources, including&nbsp;</span><a href="https://www.reuters.com/sustainability/boards-policy-regulation/google-inks-3-billion-us-hydropower-deal-largest-clean-energy-agreement-its-kind-2025-07-15/"><span>hydropower</span></a><span>,&nbsp;</span><a href="https://blog.google/feed/google-offshore-wind-power-purchase-agreement-taiwan/"><span>offshore wind</span></a><span> and even advanced&nbsp;</span><a href="https://blog.google/around-the-globe/google-asia/geothermal-taiwan/"><span>geothermal</span></a><span>.</span></p><p dir="ltr"><span>“Bit of a tight rope for Google,” he said, adding: “Watch what everyone actually does as opposed to what they say when dealing with Trump.”&nbsp;</span></p><figure role="group"><article>            <picture>                  <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=bcDc-D-_ 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_2x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=alB0KJtV 2x" media="all and (min-width: 1024px)" type="image/png" width="1290" height="657">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_1x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=K95XRSli 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_md_2x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=XgreJhs9 2x" media="all and (min-width: 768px)" type="image/png" width="800" height="407">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_1x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=A4F5NyR7 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xs_2x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=hQfImIk7 2x" media="all and (min-width: 430px)" type="image/png" width="475" height="242">              <source srcset="https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_1x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=9bR_jvO8 1x, https://www.nationalobserver.com/sites/default/files/styles/scale_width_xxs_2x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=XtBejlLX 2x" media="all and (max-width: 429px)" type="image/png" width="400" height="204">                  <img loading="lazy" width="1290" height="657" src="https://www.nationalobserver.com/sites/default/files/styles/scale_width_lg_1x/public/img/2025/09/03/screenshot_2025-09-03_at_7.43.47_pm.png?itok=bcDc-D-_" alt="">  </picture>    </article><figcaption>Still image from Google's original video where Sundar Pichai presents the tech giant's 2030 net-zero pledge</figcaption></figure><p dir="ltr"><span>John Lang, co-founder of the Net Zero Tracker, a data analysis firm that focuses on fact-checking&nbsp;net zero in every nation and the largest cities and companies around the globe, told&nbsp;</span><em>Canada's National Observer</em><span> that he believes the world is temporarily in what he dubs a "net-zero recession"– but it hasn’t caused companies to abandon climate action.</span></p><p dir="ltr"><span>“What we're seeing is we are seeing some backtracking, but it's highly concentrated in two sectors: finance and fossil fuels,” Lang said.</span></p><p dir="ltr"><span>In other sectors, Lang said corporations are now recalibrating their early sustainability goals to be more realistic and reduce reliance on carbon credits. This, he added, is “a really, really good thing.”</span></p><p dir="ltr"><span>Google, whose parent company Alphabet has a market cap of US$2.79 trillion, has taken a more ambiguous approach. Despite removing its net-zero headline from its Sustainability website, the company insists that it remains committed to its 2030 goal — which relies heavily on carbon offsetting. An external PR representative for Google declined to reply to&nbsp;</span><em>Canada’s National Observer’s</em><span> challenge of her claim that it was “still committed” to net zero by 2030, despite the pledge being demoted to an appendix.</span></p><p dir="ltr"><span>The UN’s High Level Expert Group on the Net Zero Emissions Commitments of Non-State Entities released a report in 2022, which found that unrealistic sustainability pledges “erode confidence in net zero pledges overall” and “undermine sovereign state commitments.”</span></p><p dir="ltr"><span>Lang is more sympathetic to “stretch goals,” like Google’s climate moonshots, as long as the deadlines are set close in the future, as they can motivate urgency.&nbsp;</span></p><p dir="ltr"><span>“It still needs to be realistic. You still need to be able to deliver it,” he added. He praised Google’s decision to invest $200 million in durable carbon removals as setting a positive precedent for other companies.</span></p><p dir="ltr"><span>It is unclear whether Google’s decision to delete its net-zero pledges from its Sustainability website sets a more worrying precedent.</span></p><p dir="ltr"><span>Lang is convinced the setbacks are only temporary and rapid emissions reductions will soon be reprioritized. “It's our one and only solution to climate change. It's as simple as that. There's no other option.”</span></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pump the Brakes on Your Police Department's Use of Flock Safety (139 pts)]]></title>
            <link>https://www.aclu.org/news/privacy-technology/how-to-pump-the-brakes-on-your-police-departments-use-of-flocks-mass-surveillance-license-plate-readers</link>
            <guid>45128605</guid>
            <pubDate>Thu, 04 Sep 2025 15:48:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.aclu.org/news/privacy-technology/how-to-pump-the-brakes-on-your-police-departments-use-of-flocks-mass-surveillance-license-plate-readers">https://www.aclu.org/news/privacy-technology/how-to-pump-the-brakes-on-your-police-departments-use-of-flocks-mass-surveillance-license-plate-readers</a>, See on <a href="https://news.ycombinator.com/item?id=45128605">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
	  <p>From <a href="https://southpasadenareview.com/2022/11/26/city-council-oks-contract-for-flock-safety-cameras/">Pasadena, California</a> to <a href="https://spectrumnews1.com/ky/louisville/news/2022/11/28/lexington-s-officials-wanting-to-provide-more-flock-safety-cameras">Lexington, Kentucky</a> to <a href="https://www.bizjournals.com/milwaukee/inno/stories/news/2022/03/22/wisconsin-cities-recover-stolen-cars-flock-safety.html">Menasha, Wisconsin</a>, to <a href="https://www.newarkadvocate.com/story/news/2022/11/28/newark-city-council-approves-negotiating-deal-for-flock-safety-cameras/69672495007/">Newark, New Jersey,</a> the surveillance company Flock Safety is blanketing American cities with dangerously powerful and unregulated automatic license plate recognition (ALPR) cameras. While license plate readers have been around for some time, Flock is the first to create a nationwide mass-surveillance system out of its customers’ cameras.</p>
<p>Working with police departments, neighborhood watches, and other private customers, Flock not only allows private camera owners to create their own “hot lists” that will generate alarms when listed plates are spotted, but also runs all plates against state police watchlists and the FBI’s primary criminal database, the National Crime Information Center (NCIC). Flock’s goal is to expand to “every city in the United States,” and its cameras are already <a href="https://www.flocksafety.com/about/meet-flock-safety">in use</a> in over 2,000 cities in at least 42 states.</p>
<p>Unlike a targeted ALPR camera system that is designed to take pictures of license plates, check the plates against local hot lists, and then flush the data if there’s no hit, Flock is building a giant camera network that records people’s comings and goings across the nation, and then makes that data available for search by any of its law enforcement customers. Such a system provides even small-town sheriffs access to a sweeping and powerful mass-surveillance tool, and allows big actors like federal agencies and large urban police departments to access the comings and goings of vehicles in even the smallest of towns. And every new customer that buys and installs the company’s cameras extends Flock’s network, contributing to the creation of a centralized mass surveillance system of Orwellian scope. Motorola Solutions, a competitor to Flock, is pursuing a similar business model.</p>
<p>If the police or government leaders are pushing for Flock or another centralized mass-surveillance ALPR system in your community, we urge you to oppose it, full stop. You can do this by urging your local councilperson or other elected representatives to adopt our recommendations into law, attending public meetings and hearings, and raising the profile of the issue by writing letters to the editor and op-eds. You can also use social media to highlight the issues — be sure to tag your elected officials — including by sharing this blog post. If you’re an elected official or community leader, you may also be able to engage directly with your police department — we have found that some departments are willing to do so.</p>
<p>In a few places, residents concerned about privacy and over-policing have successfully blocked their police departments’ acquisition of Flock or other ALPR systems. But, in many other cities, those efforts have been thwarted. In communities where such systems can’t be stopped entirely, we can still help protect our and our neighbors’ civil liberties by working with our local police department and elected officials to ensure that local ALPR cameras do not feed into a mass surveillance system that lets potentially every law enforcement department in the world spy on the residents and visitors of any city in America.</p>
<p>We don’t find every use of ALPRs objectionable. For example, we do not generally object to using them to check license plates against lists of stolen cars, for AMBER Alerts, or for toll collection, provided they are deployed and used fairly and subject to proper checks and balances, such as ensuring devices are not disproportionately deployed in low-income communities and communities of color, and that the “hot lists” they are run against are legitimate and up to date. But there’s no reason the technology should be used to create comprehensive records of everybody’s comings and goings — and that is precisely what ALPR databases like Flock’s are doing. In our country, the government should not be tracking us unless it has individualized suspicion that we’re engaged in wrongdoing. We more fully lay out our concerns with this technology in a March 2022 <a href="https://www.aclu.org/sites/default/files/field_document/flock_1.pdf">white paper</a> on Flock, and in a 2013 <a href="https://www.aclu.org/issues/privacy-technology/location-tracking/you-are-being-tracked">report</a> on law enforcement use.</p>
  </div><div>
	  <p>Many police departments neither understand nor endorse Flock’s nationwide, mass surveillance-driven approach to ALPR use, but are adopting the company’s cameras simply because other police departments in their region are doing so. As such, they may be amenable to compromise. That might even include using another vendor that does not tie its cameras into a mass-surveillance system. In other cases, you may be able to get your police department or local legislators to add addendums to Flock’s standard contract that limit its ALPR system’s mass surveillance capabilities and highly permissive data sharing.</p>
<p>In those situations, the three most important areas for regulation and negotiation are how long the data is retained, who the data is shared with, and how that data is used by law enforcement. We obtained samples of Flock’s Government Agency Customer Agreements with the Greensboro, North Carolina Police Department and other Flock contracts with local police. Below is suggested contract language across these three areas, based on these agreements, that you can use in your local advocacy efforts.</p>
  </div><div>
	  <p>Whether ALPRs are being used for Amber Alerts, toll collection, or to identify stolen vehicles, a license plate can be run against a watchlist in seconds. The police do not need records of every person’s coming and goings, including trips to doctor’s offices, religious institutions, and political gatherings.</p>
<p>New Hampshire <a href="https://www.gencourt.state.nh.us/rsa/html/XXI/261/261-75-b.htm">state law</a>, which requires law enforcement to delete non-hit license plate capture data within three minutes, is a good model. But you should get the shortest retention period you can in your community. From worst to best, here are three approaches that can be taken to the retention of ALPR data:</p>
  </div><div>
	  <p>One of the most important privacy-protective steps you can take is to restrict your community’s ALPR system to local use, meaning local ALPR scans are only checked against locally developed watch lists. Allowing local ALPR data to be used by outside law enforcement creates significant risks. Your local ALPR data could be used to enforce anti-abortion or anti-immigrant laws from other jurisdictions, or even to assist foreign, authoritarian regimes in hunting down political opponents and refugees living in America (Flock’s default provisions give the company a “worldwide” license to use its customers’ APLR data).</p>
<p>These risks are simply not worth taking, especially since there are many other companies that sell locally focused systems. From worst to best, here are three data sharing and use approaches:</p>
  </div><p>As much as we might hope that all police watchlists were 100 percent reliable, we know they are not. In fact, the largest and most commonly used national watch list — the National Crime Information Center (NCIC) database — does not even comply with the 1974 United States Privacy Act’s basic accuracy, reliability, and completeness requirements. That means allowing your ALPR data to be run against such databases will subject anyone living in or visiting your town to unjustified arrest and detention, which is an especially dangerous proposition for members of vulnerable, already overpoliced communities. Again, from worst to best, here are three database use approaches:</p><p>In the end, neither local police departments, nor government officials, nor residents should blindly accept Flock’s model simply because it advances Flock’s bottom line, or because other jurisdictions have unwisely chosen to do so. We continue to believe that using Flock cameras should be opposed outright. But where that battle can’t be won, then any system should at least be confined to the community itself and not made part of a national and international mass-surveillance system.</p></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cache (140 pts)]]></title>
            <link>https://developer.mozilla.org/en-US/docs/Web/API/Cache</link>
            <guid>45128578</guid>
            <pubDate>Thu, 04 Sep 2025 15:46:23 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache">https://developer.mozilla.org/en-US/docs/Web/API/Cache</a>, See on <a href="https://news.ycombinator.com/item?id=45128578">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <mdn-survey></mdn-survey>
            <!--lit-part--><!--lit-part yt6QwQDVfnA=--><!--lit-node 0--><section aria-labelledby="instance_methods">
    <!--lit-part UnXTIHS8z7o=--><!--lit-node 0--><h2 id="instance_methods"><!--lit-node 1--><a href="#instance_methods">Instance methods</a></h2><!--/lit-part-->
    <!--lit-part eT0DL1LSJe0=--><dl>
<dt id="cache.match"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/match"><code>Cache.match()</code></a></dt>
<dd>
<p>Returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"><code>Promise</code></a> that resolves to the response associated with the first matching request in the <code>Cache</code> object.</p>
</dd>
<dt id="cache.matchall"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/matchAll"><code>Cache.matchAll()</code></a></dt>
<dd>
<p>Returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"><code>Promise</code></a> that resolves to an array of all matching responses in the <code>Cache</code> object.</p>
</dd>
<dt id="cache.add"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/add"><code>Cache.add()</code></a></dt>
<dd>
<p>Takes a URL, retrieves it and adds the resulting response object to the given cache. This is functionally equivalent to calling <code>fetch()</code>, then using <code>put()</code> to add the results to the cache.</p>
</dd>
<dt id="cache.addall"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/addAll"><code>Cache.addAll()</code></a></dt>
<dd>
<p>Takes an array of URLs, retrieves them, and adds the resulting response objects to the given cache.</p>
</dd>
<dt id="cache.put"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/put"><code>Cache.put()</code></a></dt>
<dd>
<p>Takes both a request and its response and adds it to the given cache.</p>
</dd>
<dt id="cache.delete"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/delete"><code>Cache.delete()</code></a></dt>
<dd>
<p>Finds the <code>Cache</code> entry whose key is the request, returning a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"><code>Promise</code></a> that resolves to <code>true</code> if a matching <code>Cache</code> entry is found and deleted. If no <code>Cache</code> entry is found, the promise resolves to <code>false</code>.</p>
</dd>
<dt id="cache.keys"><a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/keys"><code>Cache.keys()</code></a></dt>
<dd>
<p>Returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"><code>Promise</code></a> that resolves to an array of <code>Cache</code> keys.</p>
</dd>
</dl><!--/lit-part-->
  </section><!--/lit-part--><!--lit-part yt6QwQDVfnA=--><!--lit-node 0--><section aria-labelledby="examples">
    <!--lit-part cjTOnCoGGWc=--><!--lit-node 0--><h2 id="examples"><!--lit-node 1--><a href="#examples">Examples</a></h2><!--/lit-part-->
    <!--lit-part Q5cGFqZt+Y4=--><p>This code snippet is from the <a href="https://github.com/GoogleChrome/samples/blob/gh-pages/service-worker/selective-caching/service-worker.js" target="_blank">service worker selective caching sample</a>. (see <a href="https://googlechrome.github.io/samples/service-worker/selective-caching/" target="_blank">selective caching live</a>) The code uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/CacheStorage/open"><code>CacheStorage.open()</code></a> to open any <code>Cache</code> objects with a <code>Content-Type</code> header that starts with <code>font/</code>.</p>
<p>The code then uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/match"><code>Cache.match()</code></a> to see if there's already a matching font in the cache, and if so, returns it. If there isn't a matching font, the code fetches the font from the network and uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/Cache/put"><code>Cache.put()</code></a> to cache the fetched resource.</p>
<p>The code handles exceptions thrown from the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/fetch" title="fetch()"><code>fetch()</code></a> operation. Note that an HTTP error response (e.g., 404) will not trigger an exception. It will return a normal response object that has the appropriate error code.</p>
<p>The code snippet also shows a best practice for versioning caches used by the service worker. Though there's only one cache in this example, the same approach can be used for multiple caches. It maps a shorthand identifier for a cache to a specific, versioned cache name. The code also deletes all caches that aren't named in <code>CURRENT_CACHES</code>.</p>
<p>In the code example, <code>caches</code> is a property of the <a href="https://developer.mozilla.org/en-US/docs/Web/API/ServiceWorkerGlobalScope"><code>ServiceWorkerGlobalScope</code></a>. It holds the <code>CacheStorage</code> object, by which it can access the <a href="https://developer.mozilla.org/en-US/docs/Web/API/CacheStorage"><code>CacheStorage</code></a> interface.</p>
<p><strong>Note:</strong>
In Chrome, visit <code>chrome://inspect/#service-workers</code> and click on the "inspect" link below the registered service worker to view logging statements for the various actions the <a href="https://github.com/GoogleChrome/samples/blob/gh-pages/service-worker/selective-caching/service-worker.js" target="_blank"><code>service-worker.js</code></a> script is performing.</p>
<div><pre><code>const CACHE_VERSION = 1;
const CURRENT_CACHES = {
  font: `font-cache-v${CACHE_VERSION}`,
};

self.addEventListener("activate", (event) =&gt; {
  // Delete all caches that aren't named in CURRENT_CACHES.
  // While there is only one cache in this example, the same logic
  // will handle the case where there are multiple versioned caches.
  const expectedCacheNamesSet = new Set(Object.values(CURRENT_CACHES));
  event.waitUntil(
    caches.keys().then((cacheNames) =&gt;
      Promise.all(
        cacheNames.map((cacheName) =&gt; {
          if (!expectedCacheNamesSet.has(cacheName)) {
            // If this cache name isn't present in the set of
            // "expected" cache names, then delete it.
            console.log("Deleting out of date cache:", cacheName);
            return caches.delete(cacheName);
          }
          return undefined;
        }),
      ),
    ),
  );
});

self.addEventListener("fetch", (event) =&gt; {
  console.log("Handling fetch event for", event.request.url);

  event.respondWith(
    caches
      .open(CURRENT_CACHES.font)
      .then((cache) =&gt; cache.match(event.request))
      .then((response) =&gt; {
        if (response) {
          // If there is an entry in the cache for event.request,
          // then response will be defined and we can just return it.
          // Note that in this example, only font resources are cached.
          console.log(" Found response in cache:", response);

          return response;
        }

        // Otherwise, if there is no entry in the cache for event.request,
        // response will be undefined, and we need to fetch() the resource.
        console.log(
          " No response for %s found in cache. About to fetch " +
            "from network…",
          event.request.url,
        );

        // We call .clone() on the request since we might use it
        // in a call to cache.put() later on.
        // Both fetch() and cache.put() "consume" the request,
        // so we need to make a copy.
        // (see https://developer.mozilla.org/en-US/docs/Web/API/Request/clone)
        return fetch(event.request.clone()).then((response) =&gt; {
          console.log(
            "  Response for %s from network is: %O",
            event.request.url,
            response,
          );

          if (
            response.status &lt; 400 &amp;&amp;
            response.headers.has("content-type") &amp;&amp;
            response.headers.get("content-type").match(/^font\//i)
          ) {
            // This avoids caching responses that we know are errors
            // (i.e. HTTP status code of 4xx or 5xx).
            // We also only want to cache responses that correspond
            // to fonts, i.e. have a Content-Type response header that
            // starts with "font/".
            // Note that for opaque filtered responses
            // https://fetch.spec.whatwg.org/#concept-filtered-response-opaque
            // we can't access to the response headers, so this check will
            // always fail and the font won't be cached.
            // All of the Google Web Fonts are served from a domain that
            // supports CORS, so that isn't an issue here.
            // It is something to keep in mind if you're attempting
            // to cache other resources from a cross-origin
            // domain that doesn't support CORS, though!
            console.log("  Caching the response to", event.request.url);
            // We call .clone() on the response to save a copy of it
            // to the cache. By doing so, we get to keep the original
            // response object which we will return back to the controlled
            // page.
            // https://developer.mozilla.org/en-US/docs/Web/API/Request/clone
            cache.put(event.request, response.clone());
          } else {
            console.log("  Not caching the response to", event.request.url);
          }

          // Return the original response object, which will be used to
          // fulfill the resource request.
          return response;
        });
      })
      .catch((error) =&gt; {
        // This catch() will handle exceptions that arise from the match()
        // or fetch() operations.
        // Note that a HTTP error response (e.g. 404) will NOT trigger
        // an exception.
        // It will return a normal response object that has the appropriate
        // error code set.
        console.error("  Error in fetch handler:", error);

        throw error;
      }),
  );
});
</code></pre></div><!--/lit-part-->
  </section><!--/lit-part--><!--lit-part yt6QwQDVfnA=--><!--lit-node 0--><section aria-labelledby="cookies_and_cache_objects">
    <!--lit-part O2q4Rg4z6CI=--><!--lit-node 0--><h3 id="cookies_and_cache_objects"><!--lit-node 1--><a href="#cookies_and_cache_objects">Cookies and Cache objects</a></h3><!--/lit-part-->
    <!--lit-part 5iKUtyBzBIs=--><p>The <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">Fetch API</a> requires <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Set-Cookie"><code>Set-Cookie</code></a> headers to be stripped before returning a <a href="https://developer.mozilla.org/en-US/docs/Web/API/Response"><code>Response</code></a> object from <a href="https://developer.mozilla.org/en-US/docs/Web/API/Window/fetch" title="fetch()"><code>fetch()</code></a>. So a <code>Response</code> stored in a <code>Cache</code> won't contain <code>Set-Cookie</code> headers, and therefore won't cause any cookies to be stored.</p><!--/lit-part-->
  </section><!--/lit-part--><!--lit-part yt6QwQDVfnA=--><!--lit-node 0--><section aria-labelledby="specifications">
    <!--lit-part s915xX5HiM4=--><!--lit-node 0--><h2 id="specifications"><!--lit-node 1--><a href="#specifications">Specifications</a></h2><!--/lit-part-->
    <!--lit-part lzPdEGUoFG8=--><table>
    <thead>
      <tr>
        <th scope="col"><!--lit-part-->Specification<!--/lit-part--></th>
      </tr>
    </thead>
    <tbody>
      <!--lit-part--><!--lit-part--><!--lit-part a/3Tq0vckuA=--><tr>
              <td><!--lit-part 7GI82/RZJBE=--><!--lit-node 0--><a href="https://w3c.github.io/ServiceWorker/#cache-interface" rel="noopener" target="_blank"><!--lit-part--><!--lit-part BRUAAAUVAAA=--><!--lit-part-->Service Workers<!--/lit-part--><!--?--><!--/lit-part--><!--lit-part--><!--/lit-part--><!--lit-part BIKGC1k/hws=--><br><!--/lit-part--><!--lit-part--><!--/lit-part--><!--lit-part hrUCAIW1AgA=--># <!--lit-part-->cache-interface<!--/lit-part--><!--?--><!--/lit-part--><!--/lit-part--></a><!--/lit-part--></td>
            </tr><!--/lit-part--><!--/lit-part--><!--/lit-part-->
    </tbody>
  </table><!--/lit-part-->
  </section><!--/lit-part--><!--lit-part qYvjIQSKJAY=--><!--lit-node 0--><section aria-labelledby="browser_compatibility">
    <!--lit-part y0+5SMr/T3s=--><!--lit-node 0--><h2 id="browser_compatibility"><!--lit-node 1--><a href="#browser_compatibility">Browser compatibility</a></h2><!--/lit-part-->
    <!--lit-node 2--><mdn-compat-table-lazy locale="en-US" query="api.Cache"><template shadowroot="open" shadowrootmode="open"><!--lit-part y6RVfKR7hws=--><p><!--lit-part-->Loading…<!--/lit-part--></p><!--/lit-part--></template></mdn-compat-table-lazy>
  </section><!--/lit-part--><!--lit-part yt6QwQDVfnA=--><!--lit-node 0--><section aria-labelledby="see_also">
    <!--lit-part p88DAUaBJzA=--><!--lit-node 0--><h2 id="see_also"><!--lit-node 1--><a href="#see_also">See also</a></h2><!--/lit-part-->
    <!--lit-part 3gaWdBlIqxs=--><ul>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/Service_Worker_API/Using_Service_Workers">Using Service Workers</a></li>
<li><a href="https://github.com/mdn/dom-examples/tree/main/service-worker/simple-service-worker" target="_blank">Service workers basic code example</a></li>
<li><a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers">Using web workers</a></li>
</ul><!--/lit-part-->
  </section><!--/lit-part--><!--/lit-part--> <!--lit-part OetAS4CxZLI=-->
      
    <!--/lit-part-->
          </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Wikipedia survives while the rest of the internet breaks (298 pts)]]></title>
            <link>https://www.theverge.com/cs/features/717322/wikipedia-attacks-neutrality-history-jimmy-wales</link>
            <guid>45128391</guid>
            <pubDate>Thu, 04 Sep 2025 15:30:20 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theverge.com/cs/features/717322/wikipedia-attacks-neutrality-history-jimmy-wales">https://www.theverge.com/cs/features/717322/wikipedia-attacks-neutrality-history-jimmy-wales</a>, See on <a href="https://news.ycombinator.com/item?id=45128391">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="zephr-anchor"><p><span>When</span><span> armies invade, hurricanes form, or governments fall, a Wikipedia editor will typically update the relevant articles seconds after the news breaks. So quick are editors to change “is” to “was” in cases of notable deaths that they are said to have the fastest past tense in the West. So it was unusual, according to one longtime editor who was watching the page, that on the afternoon of January 20th, 2025, hours after Elon Musk made a gesture resembling a Nazi salute at a rally following President Donald Trump’s inauguration and well into the ensuing public outcry, no one had added the incident to the encyclopedia.</span></p><p><span>Then, just before 4PM, an editor by the name of PickleG13 <a href="https://en.wikipedia.org/w/index.php?title=Elon_Musk&amp;diff=prev&amp;oldid=1270703286#mw-diffpage-visualdiff-cite_note-597">added a single sentence</a> to Musk’s 8,600-word biography: “Musk appeared to perform a Nazi salute,” citing an article in <em>The</em> <em>Jerusalem Post</em>. In a note explaining the change, the editor wrote, “This controversy will be debated, but it does appear and is being reported that Musk may have performed a Hitler salute.” Two minutes later, another editor deleted the line for violating Wikipedia’s stricter standards for unflattering information in biographies of living people.</span></p><p><span>But PickleG13 was correct. That evening, as the controversy over the gesture became a vortex of global attention, another editor called for an official discussion about whether it deserved to be recorded in Wikipedia. At first, the debate on the article’s “talk page,” where editors discuss changes, was much the same as the one playing out across social media and press: it was obviously a Nazi salute vs. it was an awkward wave vs. it couldn’t have been a wave, just look at the touch to his shoulder, the angle of his palm vs. he’s autistic vs. no, he’s antisemitic vs. I don’t see the biased media calling out Obama for doing a Nazi salute in this photo I found on Twitter vs. that’s just a still photo, stop gaslighting people about what they obviously saw. But slowly, through the barbs and rebuttals and corrections, the trajectory shifted.</span></p><p><span>Wikipedia is the largest compendium of human knowledge ever assembled, with more than 7 million articles in its English version, the largest and most developed of 343 language projects. Started nearly 25 years ago, the site was long mocked as a byword for the unreliability of information on the internet, yet today it is, without exaggeration, the digital world’s factual foundation. It’s what Google puts at the top of search results otherwise awash in ads and spam, what social platforms cite when they deign to correct conspiracy theories, and what AI companies scrape in their ongoing quest to get their models to stop regurgitating info-slurry — and consult with such frequency that they are <a href="https://diff.wikimedia.org/2025/04/01/how-crawlers-impact-the-operations-of-the-wikimedia-projects/">straining the encyclopedia’s servers</a>. Each day, it’s where <a href="https://stats.wikimedia.org/#/en.wikipedia.org/reading/unique-devices/normal%7Cline%7C2-year%7C(access-site)~mobile-site*desktop-site%7Cdaily">approximately 70 million</a> people turn for reliable information on everything from <a href="https://en.wikipedia.org/wiki/Particle_physics">particle physics</a> to <a href="https://en.wikipedia.org/wiki/North_Ronaldsay_sheep">rare Scottish sheep</a> to the <a href="https://en.wikipedia.org/wiki/Erfurt_latrine_disaster">Erfurt latrine disaster of 1184</a>, a testament both to Wikipedia’s success and to the total degradation of the rest of the internet as an information resource. </span></p><div><p><span><span>“It’s basically the only place on the internet that doesn’t function as a confirmation bias machine.”</span></span></p></div><p><span>But as impressive as this archive is, it is the byproduct of something that today looks almost equally remarkable: strangers on the internet disagreeing on matters of existential gravity and breathtaking pettiness and, through deliberation and debate, building a common ground of consensus reality.</span></p><p><span>“One of the things I really love about Wikipedia is it forces you to have measured, emotionless conversations with people you disagree with in the name of trying to construct the accurate narrative,” said DF Lovett, a Minnesota-based writer and marketer who mostly edits articles about local landmarks and favorite authors but later joined the salute debate to argue that “Elon Musk straight-arm gesture controversy” was a needlessly awkward description. “It’s basically the only place on the internet that doesn’t function as a confirmation bias machine,” he said, which is also why he thinks people sometimes get mad at it. Wikipedia is one of the few platforms online where tremendous computing power isn’t being deployed in the service of telling you exactly what you want to hear.</span></p><p><span>Whether Musk had made a Nazi salute or was merely awkward, the editors decided, was not for them to say, even if they had their opinions. What was a fact, they agreed, was that on January 20th, Musk had “twice extended his right arm toward the crowd in an upward angle,” that many observers compared the gesture to a Nazi salute, and that Musk denied any meaning behind the motion. Consensus was reached. The lines were added back. Approximately 7,000 words of deliberation to settle, for a time, three sentences. This was Wikipedia’s process working as intended.</span></p><p><span>It was at this point that Musk himself cannonballed into the discourse, tweeting that the encyclopedia was “legacy media propaganda!”</span></p><p><span>This was not Musk’s first time attacking the site — that appears to have been in 2019, when he complained that it <a href="https://www.citationneeded.news/elon-musk-and-the-rights-war-on-wikipedia/">accurately described</a> him as an early investor in Tesla rather than its founder. But recently he has taken to accusing the encyclopedia of a liberal bias, mocking it as “wokepedia,” and calling for it to be defunded. In so doing, he has joined a growing number of powerful people, groups, and governments that have made the site a target. In August, Republicans on the US House Oversight Committee <a href="https://oversight.house.gov/wp-content/uploads/2025/08/082725-letter-to-Wikimedia.pdf">sent a letter</a> to the Wikimedia Foundation requesting information on attempts to “inject bias” into the encyclopedia and data about editors suspected of doing so.</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-full.gif"></p><p><em>Musk repeating the salute before saying: “My heart goes out to you. It is thanks to you that the future of civilization is assured.”</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/2.png"></p><p>Hannah Arendt was a German and American historian and philosopher. She was one of the most influential political theorists of the twentieth century.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/5.png"></p><p>Members of the Hitler Youth in Berlin performing the Nazi salute at a rally in 1933.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/4.png"></p><p>Pyramidology refers to various religious or pseudoscientific speculations regarding pyramids, most often the Giza pyramid complex and the Great Pyramid of Giza in Egypt. </p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/3.png"></p><p>The human understanding when it has once adopted an opinion ... draws all things else to support and agree with it. And though there be a greater number and weight of instances to be found on the other side, yet these it either neglects or despises, or else by some distinction sets aside or rejects[.] - Francis Bacon</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/6.png"></p><p>An MRI scanner allowed researchers to examine how the human brain deals with dissonant information.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/7.png"></p><p>Mock trials allow researchers to examine confirmation biases in a realistic setting.</p></div></div><p><span>When governments have cowed the press and flooded social platforms with viral propaganda, Wikipedia has become the <a href="https://techpolicy.press/what-attacks-on-wikipedia-reveal-about-free-expression">next target</a>, and a more stubborn one. Because it is edited by thousands of mostly pseudonymous volunteers around the world — and in theory, by anyone who feels like it — its contributors are difficult for any particular state to persecute. Since it’s supported by donations, there is no government funding to cut off or advertisers to boycott. And it is so popular and useful that even highly repressive governments have been hesitant to block it.</span></p><p><span>Instead, they have developed an array of more sophisticated strategies. In Hong Kong, Russia, India, and elsewhere, government officials and state-aligned media have accused the site of ideological bias while online vigilantes harass editors. In several cases, editors have been sued, arrested, or threatened with violence.</span></p><p><span>When several dozen editors gathered in San Francisco this February, many were concerned that the US could be next. The US, with its strong protections for online speech, has historically been a refuge when the encyclopedia has faced attacks elsewhere in the world. It is where the Wikimedia Foundation, the nonprofit that supports the project, is based. But the site has become a popular target for conservative media and influencers, some of whom now have positions in the Trump administration. In January, <a href="https://forward.com/news/686797/heritage-foundation-wikipedia-antisemitism/">the <em>Forward</em></a> published slides from the Heritage Foundation, the think tank responsible for Project 2025, <a href="https://forward.com/news/686797/heritage-foundation-wikipedia-antisemitism/">outlining a plan</a> to reveal the identities of editors deemed antisemitic for adding information critical of Israel, a cudgel that the administration <a href="https://www.ed.gov/about/news/press-release/us-department-of-educations-office-civil-rights-sends-letters-60-universities-under-investigation-antisemitic-discrimination-and-harassment">has wielded against academia</a>. </span></p><p><span>“It’s about creating doubt, confusion, attacking sources of trust,” an editor told the assembled group. “It came for the media and now it’s coming for Wikipedia and we need to be ready.”</span></p><hr><p><span>In</span><span> 1967, <a href="https://archives.newyorker.com/newyorker/1967-02-25/flipbook/048/">Hannah Arendt published an essay in <em>The New Yorker</em></a> about what she saw as an inherent conflict between politics and facts. As varieties of truth go, she wrote, facts are fragile. Unlike axioms and mathematical proofs that can be derived by anyone at any time, there is nothing necessary about the fact, to use Arendt’s example, that German troops crossed the border with Belgium on the night of August 4th, 1914, and not some other border at some other time. Like all facts, this one is established through witnesses, testimony, documents, and collective agreement about what counts as evidence — it is political, and as the propaganda machines of the 20th century showed, political power is perfectly capable of destroying it. Furthermore, they will always be tempted to, because facts represent a sort of rival power, a constraint and limit “hated by tyrants who rightly fear the competition of a coercive force they cannot monopolize,” and at risk in democracies, where they are suspiciously impervious to public opinion. Facts, in other words, don’t care about your feelings. “Unwelcome facts possess an infuriating stubbornness,” Arendt wrote.</span></p><p><span>This infuriating stubbornness turns out to be important, though. A lie might be more plausible or useful than a fact, but it lacks a fact’s dumb arbitrary quality of being the case for no particular reason and no matter your opinion or influence. History once rewritten can be rewritten again and becomes insubstantial. Rather than believe the lie, people stop believing anything at all, and even those in power lose their bearings. This gives facts “great resiliency” that is “oddly combined” with their fragility. Having a stubborn common ground of shared reality turns out to be a basic precondition of collective human life — of politics. Even political power seems to recognize this, Arendt wrote, when it establishes ideally impartial institutions insulated from its own influence, like the judiciary, the press, and academia, charged with producing facts according to methods other than the pure exercise of power.</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-left.jpg"></p><p><em>Leonardo DiCaprio is an American actor and film producer.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/3_0d06e1.png"></p><p><em>Outside Wikipedia, original research is a key part of scholarly work. However, Wikipedia editors must base their contributions on reliable, published sources, not their own original research.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/2_66bb4f.png"></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/24973c3ff665e0510d3d241bc6e2f8a3fa7253f7-min.png"></p><p><em>On the floor of the US Senate, Republican Sen. Jim Inhofe displayed a snowball — on February 26th, 2015, in winter — as evidence the globe was not warming, in a year that was found to be Earth’s warmest on record at the time.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/08/5_0930cd.png"></p><p>Grouvellinus leonardodicaprioi<em> is a species of riffle beetle in the superfamily </em>Byrrhoidea<em>.The species was named after actor and environmentalist Leonardo DiCaprio to acknowledge his work in “promoting environmental awareness and bringing the problems of climate change and biodiversity loss into the spotlight.”</em></p></div></div><p><span>Wikipedia has come to play a similar role of factual ballast to an increasingly unmoored internet, but without the same institutional authority and with its own methods developed piecemeal over the last two decades for arriving at consensus fact. How to defend it from political attacks is not straightforward. At the conference, many editors felt both that attacks from the Trump administration were a genuine threat and that being cast as “the resistance” risked jeopardizing the encyclopedia’s position of trusted neutrality.</span></p><p><span>“I would really argue not to take the attack approach, to really take the passive approach,” said one editor when someone broached the idea of actively debunking some of the false information swamping the rest of the internet. “People see us as credible because we don’t attack, because we are just providing information to everyone all the time in a boring way. Sometimes boring is good. Boring is <em>credible</em>.”</span></p><p><span>Even the editor at the summit who had been most directly affected by the Trump administration urged against a direct response. Jamie Flood had been a librarian and outreach specialist at the National Agricultural Library, where among other duties she led group trainings and uploaded research on topics like germplasm and childhood nutrition to Wikipedia. Museums and libraries around the world employ such “Wikipedians in residence” to act as liaisons with the encyclopedia’s community for the same reason that <a href="https://www.who.int/news/item/22-10-2020-the-world-health-organization-and-wikimedia-foundation-expand-access-to-trusted-information-about-covid-19-on-wikipedia">the World Health Organization partnered</a> with Wikipedia during the covid-19 pandemic to make the latest information available: if you want research to reach the public, there is no better place.</span></p><p><span>Along with several other Wikipedians employed by the federal government, Flood had just been laid off by DOGE, collateral damage in a general dismantling of research and archival institutions. “I’m a casualty of this administration’s war on information,” Flood said.</span></p><div><p><span><span>“‘Imagine a world where all knowledge is freely available to everyone.’”</span></span></p></div><p><span>Still, Wikipedia absolutely should not counterattack, Flood said. “Wikipedia is always in the background. They’re not making a big statement, and I don’t think they should. I’ve been training people for a long time and I still go back to this early quote of Jimmy Wales, one of the founders: ‘Imagine a world where all knowledge is freely available to everyone.’ That’s enough. That’s a statement in and of itself. In a time of misinformation, in a time of suppression, having this place where people can come and bring knowledge and share knowledge, that is a statement.”</span></p><p><span>Wikipedia should be, in other words, as stubborn as a fact. But then, facts are fragile things. </span></p><hr><p><span>A</span><span> common refrain among Wikipedians is that the site works in practice but not in theory. It seems to flout everything we’ve learned about human behavior online: anonymous strangers discussing divisive topics and somehow, instead of dissolving into factions and acrimony, working together to build something of value.</span></p><p><span>The project’s origins go back to 1999. Wales, a former options trader who had founded a laddish web portal called Bomis, wanted to start a free online encyclopedia. He hired an acquaintance from an Ayn Rand listserv that Wales previously ran, a philosophy PhD student named Larry Sanger. Their first attempt, called Nupedia, was not so different from encyclopedias as they have existed since <a href="https://en.wikipedia.org/wiki/Encyclop%C3%A9die">Diderot’s <em>Encyclopédie</em></a> in 1751. Experts would write articles that went through seven stages of editorial review. It was slow going. After a year, Nupedia had just over 20 articles.</span></p><p><span>In an attempt to speed things along, they decided to experiment with wikis, a web format gaining popularity among open-source software developers that allowed multiple people to collaboratively edit a project. (Wiki is the Hawaiian word for “quick.”) The wiki was intended to be a forum where the general public could contribute draft articles that would then be fed into Nupedia’s peer-review pipeline, but the experts objected and the crowdsourced site was given its own domain, Wikipedia.com. It went live on January 15th, 2001. Within days, it had more articles than all of Nupedia, albeit of varying quality. After a year, Wikipedia had more than 20,000 articles.</span></p><div><p><span><span>“...write about <em>what people believe</em>, rather than <em>what is so</em>”</span></span></p></div><p><span>There were few rules at first, but one that Wales <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/E3MK6PICICOIS3M7UGRO472OUABHLQE3/#E3MK6PICICOIS3M7UGRO472OUABHLQE3">said</a> was “non-negotiable” was that Wikipedia should be written from a “neutral point of view.” The policy, abbreviated as NPOV, was imported from the “nonbias policy” Sanger had written for Nupedia. But on Wikipedia, Wales considered it as much a “social concept of cooperation” as an editorial standard. If this site was going to be open to anyone to edit, the only way to avoid endless flame wars over who is right was, provocatively speaking, to set questions of truth aside. “We could talk about that and get nowhere,” Wales wrote to the Wikipedia <a href="https://lists.wikimedia.org/pipermail/wikien-l/2003-September/006715.html">email list</a>. “Perhaps the easiest way to make your writing more encyclopedic is to write about <em>what people believe</em>, rather than <em>what is so</em>,” he <a href="https://meta.wikimedia.org/wiki/Neutral_point_of_view">explained</a>.</span></p><p><span>Ideally, the neutrality principle would allow people of different views to agree, if not on the matter at hand, then at least on what it was they were disagreeing about. “If you’ve got a kind and thoughtful Catholic priest and a kind and thoughtful Planned Parenthood activist, they’re never going to agree about abortion, but they can probably work together on an article,” Wales would later say.</span></p><p><span>This view faced an immediate challenge, which is that people believe all sorts of things: that the Earth <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/OWRA4PUJFRG5AGD4R3LEQZSRRQL5NKX7/#ZIPOJJHIXN6WSVX2UJFMAOOGBBYNFOVZ">is 6,000 years old</a>, that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/C3ESAN47MOASFAFYPN26QE6PPWOXJBSC/#C3ESAN47MOASFAFYPN26QE6PPWOXJBSC">climate change</a> is a scam, that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/message/4GIYS3ZSO77FLF36NYZZACFPL7BKEXZD/">the Holocaust was a hoax</a>, that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/PBZYJFC2OCAFL6HDUDQUUREW3P563CFR/#YK3YM7BOPNQJRELE2N4OUJDUU7TVBDPG">the Irish potato famine was overblown</a>, that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/OZBZBM7ZM2L5H52H5TVCMVRFNKM2OD2G/#773R74U5RYSS5VUQ3GN5OLZMGXBAM7CC">chiropractors are all charlatans</a>, that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/DZKIXUNU4JYIMKX3Y5XNNN3QGX3CODZS/#DZKIXUNU4JYIMKX3Y5XNNN3QGX3CODZS">they have discovered a new geometry</a>, and that <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/JOXMMREF5DIHM66QHDUASCHUD4WEDRIL/#67DW2Y5JGJQK3O67HGXVMOHJKDMPDBUK">Mother Teresa was a jerk</a>.</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-right.png"></p><p><em>Lawrence Mark Sanger is an American Internet project developer and philosopher who cofounded Wikipedia along with Jimmy Wales.</em> </p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2.png"></p><p><em>Anti-denialist banner at the 2017 Climate March in Washington, DC.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-4.jpg"></p><p><em>Mary Teresa Bojaxhiu was an Albanian Indian Catholic nun, founded the Missionaries of Charity, and is a Catholic saint.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3.png"></p><p><em>Young Earth creationism (YEC) is a form of creationism that holds as a central tenet that the Earth and its lifeforms were created by supernatural acts of the Abrahamic God between about 10,000 and 6,000 years ago, contradicting established scientific data that puts the age of Earth around 4.54 billion years.</em></p></div></div><p><span>In response, the early volunteers added another rule. You can’t just say things; any factual claim needs a <a href="https://en.wikipedia.org/wiki/Wikipedia:Verifiability">citation that readers can check for themselves</a>. When people started emailing Wales their proofs that Einstein was wrong about relativity, he clarified that the cited source could not be your own <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">“original research.”</a> Sorry, Wales <a href="https://lists.wikimedia.org/pipermail/wikien-l/2003-September/006718.html">wrote to an Einstein debunker</a>, it doesn’t matter whether your theory is true. When it is published in a physics journal, you can cite that.</span></p><p><span>Instead of trying to ascertain the truth, editors assessed the credibility of sources, looking to signals like whether a publication had a fact-checking department, got cited by other reputable sources, and issued corrections when it got things wrong. </span></p><p><span>At their best, these ground rules ensured debates followed a productive dialectic. An editor might write that human-caused climate change was a fact; another might change the line to say there was ongoing debate; a third editor would add the line back, backed up by surveys of climate scientists, and demand peer-reviewed studies supporting alternate theories. The outcome was a more accurate description of the state of knowledge than many journalists were promoting at the time by giving “both sides” equal weight, and also a lot of work to arrive at. <a href="https://www.nature.com/articles/s41562-019-0541-6">A 2019 study published in <em>Nature</em></a> found that Wikipedia’s most polarizing articles — eugenics, global warming, Leonardo DiCaprio — are the highest quality, because each side keeps adding citations in support of their views. Wikipedia: a machine for turning conflict into bibliographies. </span></p><p><span>Coupled with some technical features of wikis, like the ability for anyone to edit anyone else’s writing, and some early administrative rules, like not being allowed to undo someone else’s edit more than three times per day, users were practically forced to talk through disagreements and arrive at “consensus.” This became Wikipedia’s governing principle.</span></p><p><span>This may make the process sound more peaceful than it is. Disputes were constant. Early on, Sanger, who had remained partial to a more hierarchical, expert-driven model, clashed repeatedly with editors he decried as “anarchists” and demanded greater authority for himself, which the editors rejected. When revenue from Bomis dried up after the dot-com crash, Wales laid Sanger off and took over management of the project.</span></p><p><span>Wales governed from a greater remove, appearing only occasionally to broker peace between warring editors, resolve an impasse, or <a href="https://lists.wikimedia.org/hyperkitty/list/wikien-l@lists.wikimedia.org/thread/XVPJYPIZEOMRQ5L5R4RHP5ITZARJYQLJ/#GHKUV6C26BGYLMEBPKWAQXV77G43YWAC">reassure</a> people that they didn’t need to spend time devising procedures to screen out a sudden influx of neo-Nazis that were planning to overwhelm discussion, because if they showed up, “I will personally ban them all if necessary, and that’s that.” Editors sometimes ironically referred to him as their “<a href="https://en.wikipedia.org/wiki/Wikipedia_talk:Neutral_point_of_view/Archive_001">God King</a>” or “benevolent dictator,” but he described his role as a sort of constitutional monarch safeguarding the community as it developed the processes to fully govern itself. Because Wikipedia was under a Creative Commons license, anyone who didn’t like the way the project was run could copy it and start their own, as a group of Spanish users did when the possibility of running ads was raised in 2002. The next year Wales established a nonprofit, the Wikimedia Foundation, to raise funds and handle the technical and legal work required to keep the project running. The encyclopedia itself, however, would be entirely edited and managed by volunteers.</span></p><p><span>In early 2004, Wales delegated his moderating powers to a group of elected editors, called the Arbitration Committee. From that point onward, he was essentially another editor, <a href="https://en.wikipedia.org/wiki/User:Jimbo_Wales">screenname Jimbo Wales</a>, liable to have his edits undone like anyone else. He attempted several times to update <a href="https://en.wikipedia.org/w/index.php?title=Jimmy_Wales&amp;diff=prev&amp;oldid=5987088">his own</a> birthdate to reflect the fact that his mother says he was born slightly before midnight on August 7th, 1966, not on August 8th, as his birth certificate read, only to be reprimanded for editing his own page and trying to cite his own “original research.” (After <a href="https://en.wikipedia.org/wiki/Talk:Jimmy_Wales/Birthdate">several years</a> of debates and citable coverage from reliable sources, August 7th eventually won, with a note explaining the discrepancy.)</span></p><div><p><span><span><a href="https://en.wikipedia.org/wiki/Wikipedia:Assume_good_faith"><img src="https://s3.us-east-1.amazonaws.com/assets.sbnation.com/csk/uploads/verge-features/wikipedia/pqs/AGF.svg" alt="AGF"></a></span></span></p></div><p><span>Over the ensuing two decades, editors amended policies to cope with conspiracy theorists, revisionist historians, militant fandoms, and other perennial goblins of the open web. There were the three core content guidelines of Neutral Point of View, Verifiability, and No Original Research; <a href="https://en.wikipedia.org/wiki/Wikipedia:Five_pillars">the five pillars of Wikipedia</a>; and a host of rules around editor conduct, like the injunction to avoid ad hominem attacks and assume good faith of others, defined and refined in interlinked articles and essays. There are specialized forums and noticeboards where editors can turn for help making an article more neutral, figuring out whether a source was reliable, or deciding whether a certain view was fringe or mainstream. By 2005, the pages where editors stipulated policy and debated articles were found to be <a href="https://users.ece.utexas.edu/~perry/education/382v-s08/papers/viegas07.pdf">growing faster</a> than the articles themselves. Today, this administrative backend is at least five times the size of the encyclopedia it supports.</span></p><p><span>The most important thing to know about this system is that, like the neutrality principle from which it arose, it largely ignores content to focus on process. If editors disagree about, for example, whether the article for the uninhabited islands claimed by both Japan and China should be titled “Senkaku Islands,” “Diaoyu Islands,” or “Pinnacle Islands,” they first try to reach an agreement on the article’s Talk page, not by arguing who is correct, but by arguing which side’s position better accords with specific Wikipedia policies. If they can’t agree, they can summon an uninvolved editor to weigh in, or file a “request for comment” and open the issue to wider debate for 30 days.</span></p><p><span>If this fails and editors begin to quarrel, they might get called before the Arbitration Committee, but this elected panel of editors will also not decide who is right. Instead, they will examine the reams of material generated by the debate and rule only on who has violated Wikipedia process. They might ban an editor for 30 days for conspiring off-Wiki to sway debate, or forbid another editor from working on articles about Pacific islands over repeated ad hominem attacks, or in extreme cases ban someone for life. Everyone else can go back to debating, following the process this time.</span></p><p><span>As a result, explosive political controversies and ethnic conflicts are reduced to questions of formatting consistency. But because process decides all, process itself can be a source of intense strife. The topics of “gun control” and “the Balkans” are officially designated as “<a href="https://en.wikipedia.org/wiki/Template:Contentious_topics/list">contentious</a>” due to recurring edit wars, where people keep reverting each other’s edits without attempting to build consensus; so, too, are the Wikipedia manual of style and the question of what information belongs in sidebars. In one infamous battle, debate over whether to capitalize “into” in the film title <em>Star Trek Into Darkness</em> raged for more than 40,000 words.</span></p><div><p><span><span>Because disputes on Wikipedia are won or lost based on who has better followed Wikipedia process, every dispute becomes an opportunity to reiterate the project’s rules and principles</span></span></p></div><p><span>In 2009, law professors David A. Hoffman and Salil K. Mehra <a href="https://scholarlycommons.law.emory.edu/cgi/viewcontent.cgi?article=1360&amp;context=elj">published a paper</a> analyzing conflicts like these on Wikipedia and noted something unusual. Wikipedia’s dispute resolution system does not actually resolve disputes. In fact, it seems to facilitate them continuing forever.</span></p><p><span>These disputes may be crucial to Wikipedia’s success, the researchers wrote. Online communities are in perpetual danger of dissolving into anarchy. But because disputes on Wikipedia are won or lost based on who has better followed Wikipedia process, every dispute becomes an opportunity to reiterate the project’s rules and principles.</span></p><p><span>Trolls who repeatedly refuse to follow the process eventually get banned, but initial infractions are often met with explanations of how Wikipedia works. Several of the editors I spoke with began as vandals only to be won over by someone explaining to them how they could contribute productively. Editors will often restrict who can work on controversial topics to people who have logged a certain number of edits, ensuring that only those bought into the ethos of the project can participate.</span></p><p><span>In 2016, <a href="https://www.nber.org/papers/w22744">researchers published a study</a> of 10 years of Wikipedia edits about US politics. They found that articles became more neutral over time — and so, too, did the editors themselves. When editors arrived, they often proposed extreme edits, received pushback, and either left the project or made increasingly moderate contributions.</span></p><p><span>This is obviously not the reigning dynamic of the rest of the internet. The social platforms where culture and politics increasingly play out are governed by algorithms that have the opposite effect of Wikipedia’s bureaucracy in nearly every respect. Optimized to capture attention, they boost the novel, extreme, and sensational rather than subjecting them to increased scrutiny, and by sending content to users most likely to engage with it, they sort people into clusters of mutual agreement. This phenomenon has many names. Filter bubbles, <a href="https://medium.com/datasociety-points/agnotology-and-epistemological-fragmentation-56aa3c509c6b">epistemological fragmentation</a>, <a href="https://www.vox.com/technology/353958/online-lies-invisible-rulers-book-successful-misinformation">bespoke realities</a>, the sense that everyone has lost their minds. On Wikipedia, it’s called a “point of view split,” and editors banned it early. You are simply not allowed to make a new article on the same topic. Instead, you must make the case for a given perspective’s place amid all the others while staying, literally, on the same page.</span></p><hr><p><span>In</span><span> February, the conservative organization Media Research Center released a report claiming that “Wikipedia Effectively Blacklists ALL Right-Leaning Media.” It was essentially a summary of a publicly available policy page on Wikipedia that lists discussions about the reliability of sources and color codes them according to the latest consensus — green for generally reliable, yellow for lack of clear consensus, and red for generally unreliable. <em>ProPublica</em> is green because it has an “excellent reputation for fact-checking and accuracy, is widely cited by reliable sources, and has received multiple Pulitzer Prizes.” <em>Newsweek</em> is yellow after a decline in editorial standards following its 2013 acquisition and recent use of AI to write articles. Newsmax, the One America News Network, and several other popular right-leaning sources are red due to repeatedly publishing <a href="https://www.cnbc.com/2021/08/10/dominion-sues-pro-trump-oan-newsmax-overstock-founder-over-election-conspiracies.html">stories that were proven wrong</a>. (As are some left-leaning sources, like Occupy Democrats.) The <em>New York Post</em> (generally unreliable, but marginally reliable on entertainment) used the report as the basis for an editorial titled “<a href="https://nypost.com/2025/02/05/opinion/big-tech-must-block-wikipedia-until-it-stops-censoring-and-pushing-disinformation/">Big Tech must block Wikipedia until it stops censoring and pushing disinformation.”</a></span></p><p><span>The page is called <a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Perennial_sources">Reliable sources/Perennial sources</a>, as in sources that are perennially discussed. Editors made the page in 2018 as a repository for past discussions that they could refer to instead of having to repeatedly debate the reliability of the <em>Daily Mail</em> — the first publication to be deprecated, the year before — every time someone tried to cite it. It is not a list of preapproved or banned sources, the page reads. Context matters, and consensus can change.</span></p><p><span>But to Wikipedia’s critics, the page has become a symbol of the encyclopedia’s biases. Sanger, the briefly tenured cofounder, has found a receptive audience in right-wing activist <a href="https://christopherrufo.com/p/larry-sanger-speaks-out">Christopher Rufo</a> and other conservatives by <a href="https://larrysanger.org/2021/06/wikipedia-is-more-one-sided-than-ever/">claiming Wikipedia has strayed from its neutrality principle</a> by making judgments about the reliability of sources. Instead, he argues, it should present all views equally, including things “many Republicans believe,” like the existence of widespread fraud in the 2020 election and the FBI playing a role in the January 6th Capitol attack.</span></p><p><span>Last spring, the reliable source page collided with one of the most intense political flashpoints on Wikipedia, the Israel-Palestine conflict. In April, <a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard/Archive_434#Is_it_time_to_re-evaluate_the_ADL%3F">an editor asked</a> whether it was time to reevaluate the reliability of the Anti-Defamation League in light of changes to the way it categorizes antisemitic incidents to include protests of Israel, among other recent controversies. About 120 editors debated the topic for two months, producing text equal to 1.9 <em>The Old Man and the Sea</em>s, or “<a href="https://en.wikipedia.org/wiki/Wikipedia:Tomats">tomats</a>,” a standard unit of Wikipedia discourse. The consensus was that the ADL was reliable on antisemitism generally <a href="https://en.wikipedia.org/wiki/Wikipedia:Reliable_sources/Noticeboard/Archive_439#RFC:_The_Anti-Defamation_League">but not when the Israel-Palestine conflict was involved</a>.</span></p><p><span>Unusually for a Wikipedia administrative process, the decision received enormous attention. <em>The Times of Israel</em> <a href="https://www.timesofisrael.com/wikipedia-rebuffs-jewish-groups-call-to-override-editors-move-against-adl/">called it a “staggering blow” for the ADL</a>, which mustered Jewish groups to petition the foundation to overrule the editors. <a href="https://wikimediafoundation.org/news/2024/06/26/wikimedia-foundation-statement-volunteer-processes-reliable-sources/">The foundation responded</a> with a fairly technical explanation of how Wikipedia’s self-governing reliability determinations work.</span></p><p><span>In the year since, conservative and pro-Israel organizations have published a series of reports examining the edit histories of articles to make a case that Wikipedia is biased against Israel. In March, the ADL itself issued one such report, called “<a href="https://www.adl.org/resources/report/editing-hate-how-anti-israel-and-anti-jewish-bias-undermines-wikipedias-neutrality">Editing for Hate</a>,” claiming that a group of 30 “malicious editors” slanted articles to be critical of Israel and favorable to Palestine. As evidence, the report highlights examples like the removal of the phrase “Palestinian terrorism” from the introduction of the article on Palestinian political violence.</span></p><p><span>Yet the edit histories show that these examples are often plucked from long editing exchanges, the outcome of which goes unmentioned. The “terrorism” line that the ADL cited was indeed removed — it had also only just been added, was added back shortly after being cut, then was removed again, added back, and revised repeatedly before editors brokered a compromise on the talk page.</span></p><p><span><em>Breitbart</em>, <em>Pirate Wires</em>, and other right-leaning publications now regularly mine Wikipedia’s lengthy debates for headlines like “How Wikipedia Launders Regime Propaganda,” accusing the site of being a mouthpiece for the Democratic Party, or “Cover Up: Wikipedia Editors Propose Deleting Page on Iran Advocating for Israel’s Destruction,” despite the article having just been created and the outcome being to merge the contents into the article on Iran-Israel relations. These reports are a dependable source of viral outrage on X. The strategy also appears effective at convincing lawmakers. In May, Rep. Debbie Wasserman Schultz (D-FL) and 22 other members <a href="https://wassermanschultz.house.gov/news/documentsingle.aspx?DocumentID=3330">wrote to the Wikimedia Foundation</a> citing the ADL report and demanding Wikimedia “rein in antisemitism, uphold neutrality.”</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-left-2.png"></p><p>The term filter bubble was coined by internet activist Eli Pariser, circa 2010.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3-1.png"></p><p><em>Alice O’Connor, better known by her pen name Ayn Rand, was a Russian-born American writer and philosopher. She is known for her fiction and for developing a philosophical system that she named Objectivism.</em> </p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2-1.png"></p><p><em>Here are two black swans, but even with no black swans to possibly falsify it, “All swans are white” would still be shown falsifiable by “Here is a black swan” — it would still be a valid observation statement in the empirical language, even if empirically false.</em></p></div></div><p><span>The August <a href="https://oversight.house.gov/wp-content/uploads/2025/08/082725-letter-to-Wikimedia.pdf">letter</a> from House Republicans requesting information on attempts to influence the encyclopedia, data on editors who had been disciplined by Arbcom, and other records also cited the ADL report. </span></p><p><span>While some search for bias in the minutiae of edit histories, others try to encompass all of Wikipedia. Last year, a researcher at the conservative <a href="https://manhattan.institute/article/is-wikipedia-politically-biased">Manhattan Institute</a> scraped Wikipedia for mentions of political terms and public officials and used a GPT language model to analyze them for bias. The report found “a mild to moderate” tendency to associate figures on the political right with more negative sentiment than those on the left. The study, which was not peer reviewed, has become a regular fixture in claims of liberal bias on Wikipedia.</span></p><p><span>The report still illustrates the challenges of evaluating the neutrality of a text as vast and stripped of subjective opinion as Wikipedia. An examination of the <a href="https://zenodo.org/records/10775984">datasets</a> shows that the passages GPT classified as non-neutral are often anodyne factual statements: that a lawmaker won or lost an election, represented a certain district, or died. It also conflated unrelated people of the same name, so, for example, most of the non-neutral statements about Mike Johnson concern not Mike Johnson the current Republican House Majority Speaker but a robber in a 1923 silent film, a prog-rock guitarist, multiple football players, and a famous yodeler. </span></p><p><span>But the more fundamental question is whether balanced sentiment or balanced anything across the contemporary political spectrum is the correct expectation for a project that operates by a different standard, one based on measures of reliability. Supposing the sentiment readings do reflect a real imbalance, is that due to the biases of editors, biases in their sources, or some other external imbalance, like a tendency by right-leaning politicians to express negative sentiments of fear or anger (a possibility the report raises, then dismisses).</span></p><p><span>Wikipedia has a long history of attempting to disentangle and correct its various biases. The site’s editor community has been overwhelmingly white, male, and based in the United States and Europe since the site began. In 2018, <a href="https://meta.wikimedia.org/wiki/Community_Insights/2018_Report/Contributors#:~:text=The%20majority%20of%20contributors%20who,and%209.3%25%20identified%20as%20female.">90 percent of editors were men</a>, and only <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Women_in_Red">18 percent</a> of biographies in the encyclopedia were of women. That year, the Canadian physicist <a href="https://en.wikipedia.org/wiki/Donna_Strickland">Donna Strickland</a> won a Nobel Prize, and people turning to Wikipedia to learn about her discovered she lacked an article.</span></p><div><p><span><span>Women have been historically excluded from the sciences, underrepresented in coverage of the sciences, and therefore underrepresented in the sources Wikipedia editors can cite</span></span></p></div><p><span>But the causal connection between these facts was not straightforward. Women have been historically excluded from the sciences, underrepresented in coverage of the sciences, and therefore underrepresented in the sources Wikipedia editors can cite. An editor had tried to make an article on Strickland several months before the Nobel but was overruled due to a lack of coverage in reliable sources. “Wikipedia is a mirror of the world’s biases, not the source of them. We can’t write articles about what you don’t cover,” <a href="https://x.com/krmaher/status/1047453672790093824">tweeted then-executive director Katherine Maher</a>.</span></p><p><span>Wikipedia’s sourcing guidelines are conservative in their deference to traditional institutions of knowledge production, like established newsrooms and academic peer review, and this means that it is sometimes late to ideas in the process of moving from fringe to mainstream. The possibility that covid-19 emerged from a lab was relegated to a section on conspiracy theories and is only now, after reporting by reliable sources, gaining a toehold on the covid pandemic article. Similarly, as awareness grew of the ways Western academic and journalistic institutions have excluded the perspectives of colonized people, critics argued that Wikipedia’s reliance on these same institutions made it impossible for the encyclopedia to be truly comprehensive.</span></p><p><span>Not all the bias comes from the project’s sources, though. <a href="https://journals.sagepub.com/doi/10.1177/2378023118823946">A study that attempted to control for offline inequalities</a> by examining only contemporary sociologists of similar achievement found that male academics were still more likely to have articles. As volunteers, editors work on topics they think are important, and the encyclopedia’s emphases and omissions reflect their demographics. Minor skirmishes in World War II and every episode of <em>The Simpsons</em> have an article, some of which are longer than the articles on the <a href="https://en.wikipedia.org/wiki/Ethiopian_Civil_War">Ethiopian civil war</a> or <a href="https://en.wikipedia.org/wiki/Climate_change_in_the_Maldives">climate change in the Maldives</a>. In an effort to fill in these gaps, the foundation has for several years funded editor recruitment and training initiatives under the banner of “knowledge equity.”</span></p><p><span>“Most editors on Wikipedia are English-speaking men, and our coverage is of things that are of interest to English-speaking men,” said a retired market analyst in Cincinnati who has been editing for over 20 years. “Our sports coverage is second to none. Video games, we got it covered. Wars, the history of warfare, my god. Trains, radio stations... But our coverage of foods from other countries is very low, and there is an absolute systemic bias against coverage of women and people of color.” For her part, she tries to fill gaps around food, creating new articles whenever she encounters a Peruvian chili sauce or African <a href="https://en.wikipedia.org/wiki/Fufu">fufu</a> that lacks one.</span></p><p><span>Yet these initiatives have come under attack as “DEI” by conservative influencers and Musk, who <a href="https://x.com/elonmusk/status/1871443771424116954?lang=en">called</a> for Wikipedia to be defunded until “they restore balance.”</span></p><div><p><span><span>If you think something is wrong on Wikipedia, you can fix it yourself</span></span></p></div><p><span>These accusations of bias, familiar from attacks on the media and social platforms, encounter some unique challenges when leveled against Wikipedia. Crucially, if you think something is wrong on Wikipedia, you can fix it yourself, though it will require making a case based on verifiability rather than ideological “balance.”</span></p><p><span>Over the years, Wikipedia has developed an immune response to outside grievances. When people on X start complaining about Wikipedia’s suppression of UFO sightings or refusal to change the name of the Gulf of Mexico to Gulf of America, an editor often restricts the page to people who are logged in and puts up a notice directing newcomers to read the latest debate. If anything important was missed, they are welcome to suggest it, the notice reads, provided their suggestion meets Wikipedia’s rules, which can be read about on the following pages. That is, Wikipedia’s first and best line of defense is to explain how Wikipedia works. </span></p><p><span>Occasionally, people stick around and learn to edit. More often, they get bored and leave.</span></p><hr><p><span>It</span><span> was not unusual for skirmishes to break out over the Wikipedia page for <a href="https://en.wikipedia.org/wiki/Asian_News_International">Asian News International</a>, or ANI. It is the largest newswire service in India, and as its Wikipedia article explains, it has a history of promoting false anti-Muslim and pro-government propaganda. It was these facts that various anonymous editors — not logged into Wikipedia accounts, so appearing only as IP addresses — attempted to remove last spring. </span></p><p><span>As typically happens, an experienced editor quickly reinstated the deleted sentences, noting that they had been removed without explanation. Then came another drive-by edit: actually, ANI is not propaganda and very credible, someone wrote, citing a YouTube video. Reverted: YouTube commentary is not a reliable source. Then another IP address, deleting a sentence about ANI promoting a false viral story about necrophilia in Pakistan. Reverted again. Another IP address, deleting the mention of propaganda with the explanation that the sources were “leftist dogs and swine.”</span></p><p><span>As the edit battle escalated, an editor locked the page so that only people who were logged in and had made a certain number of edits could make changes, ending the barrage of IP addresses.</span></p><p><span>Two months later, ANI sued. </span></p><p><span>The lawsuit revealed that several of the IP addresses had belonged to representatives of ANI attempting to remove unflattering information about the company. Blocked from doing so, ANI sued for defamation under a recent amendment to India’s equivalent of <a href="https://www.theverge.com/21273768/section-230-explained-internet-speech-law-definition-guide-free-moderation">Section 230</a> that places stricter requirements on platforms to moderate content. When the Wikimedia Foundation declined to reveal the identities of three editors who had defended the page, the presiding judge said he would ask the government to block the site, threatening to cut off the country with the highest number of English Wikipedia readers after the US and the UK. “If you don’t like India,” <a href="https://www.barandbench.com/news/will-ask-government-to-block-wikipedia-delhi-high-court-contempt-court-notice">the judge said</a>, “please don’t work in India.”</span></p><p><span>During the appeal, Wikimedia’s lawyer argued that disclosing the identities of editors would destroy the encyclopedia’s self-regulating system and expose contributors to reprisals. Also, he noted, the sentences in question, like every assertion on Wikipedia, were only summarizing other sources, and those sources — the publications <em>The Caravan</em> and <em>The Ken — </em>had not been sued for defamation. (As with editors, the foundation’s first response to external threats is often to explain how Wikipedia works.) The judge dismissed the argument, <a href="https://www.barandbench.com/news/ani-versus-wikipedia-what-is-at-stake">saying that</a> journalism might be “read by a hundred people, you don’t bother about it… it does not have the gravitas.” Wikipedia, however, is read by millions.</span></p><p><span>By this point the case had garnered enough coverage to warrant <a href="https://en.wikipedia.org/wiki/Asian_News_International_vs._Wikimedia_Foundation">its own Wikipedia page</a>. This seemed to enrage the judge, particularly the line noting that the judge’s demand to reveal the identities of editors had been described as “censorship and a threat to the flow of information.” This “borders on contempt,” the judge said, demanding that the foundation take the page down within 36 hours. In a rare move, the foundation complied.</span></p><p><span>The case alarmed editors around the world. <a href="https://en.wikipedia.org/wiki/Wikipedia:2024_open_letter_to_the_Wikimedia_Foundation">An open letter</a> calling on the Wikimedia Foundation to protect the anonymity of the editors garnered more than 1,300 signatures, the most of any letter directed at the foundation. Nevertheless, last December, the foundation disclosed the editors’ identities to the judge under seal. Responding to outrage on Wikipedia’s editor forum, Wales <a href="https://en.wikipedia.org/wiki/Wikipedia_talk:2024_open_letter_to_the_Wikimedia_Foundation/Archive_2">asked for calm</a> and urged people not to jump to conclusions.</span></p><p><span>The Wikimedia Foundation has historically taken a hard line against attempts to influence the project. In 2017, when the Turkish government demanded several articles be deleted, Wikipedia refused and was blocked for nearly three years as it fought to the country’s Constitutional Court and won. For the second half of 2024, <a href="https://wikimediafoundation.org/who-we-are/transparency/2024-2/user/">the most recent data available</a>, the foundation complied with about 8 percent of requests for user data, compared to Google’s 82 percent and Meta’s 77 percent. And the data provided was sparse, because Wikipedia retains <a href="https://foundation.wikimedia.org/wiki/Legal:Wikimedia_Foundation_Data_Retention_Guidelines">almost none</a>.</span></p><div><p><span><span>Instead of brute censorship, what has emerged is a sort of gray-zone information warfare</span></span></p></div><p><span>But attempts to influence the site have grown more sophisticated. The change is likely due to multiple factors: a global rise of political movements that wish to control independent media, the increased centrality of Wikipedia, and a technical change to the website itself. In 2015, Wikipedia switched to the encrypted HTTPS extension by default, making it impossible to see what pages users visited, only that they were visiting the Wikipedia domain. This meant that governments that had previously been censoring specific articles on opposition figures or historic protests had to choose between blocking all of Wikipedia or none of it. Almost every country save China (<a href="https://www.theverge.com/2015/8/27/9210475/russia-wikipedia-ban-censorship">and Russia, for several hours</a>) chose to not to block it. This was a victory for open knowledge, but it also meant governments had a greater interest in controlling what was written in the encyclopedia.</span></p><p><span>Instead of brute censorship, what has emerged is a sort of gray-zone information warfare. After mainland China quashed protests against the Hong Kong national security law in 2019, a battle began <a href="https://hongkongfp.com/2021/07/11/wikipedia-wars-how-hongkongers-and-mainland-chinese-are-battling-to-set-the-narrative/">over how the protests would be remembered</a>. Editors in mainland China — which can edit using VPNs — argued for the inclusion of <a href="https://globalvoices.org/2021/09/28/a-veteran-hong-kong-wikipedia-editor-wikipedias-policies-are-vulnerable-to-authoritarian-abuse/">state-friendly media</a> that described the protests as “riots” or “terrorist attacks” while removing citations to independent media for unreliability and bias. In one case, an editor attempted to strip all citations to one of Hong Kong’s premier papers, <em>Apple Daily</em>, hours before it was shut down by the government. By conspiring offline and using fake accounts, they won elections to admin positions and with them the power to see other editors’ IP addresses, which they discussed using to reveal their opponents’ identities to the police. Shortly afterward, the <a href="https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/6ANVSSZWOGH27OXAIN2XMJ2X7NWRVURF/">Wikimedia Foundation banned or restricted</a> more than a dozen editors operating from mainland China, saying that the project had been “infiltrated” and that “some users have been physically harmed as a result.”</span></p><p><span>Russia employed similar tactics after its invasion of Ukraine in 2022. State media and government officials attacked Wikipedia in the press with accusations of anti-Russian bias, promulgation of fake news, and foreign manipulation. The site remained accessible, but Russian search engines put a banner above it saying it was in violation of the law. Meanwhile, the government harassed the foundation with a series of fines for publishing “false” information about the military, which the foundation has refused to pay. Finally, on the encyclopedia, state-aligned editors pushed the government’s view while vigilantes <a href="https://restofworld.org/2022/russias-6-month-war-on-wikipedia/">doxxed and threatened</a> their opposition. Last year, the head of Wikimedia Russia was <a href="https://www.bloomberg.com/news/articles/2024-01-24/russia-s-wikipedia-shuts-down-under-pressure-from-putin">declared a “foreign agent”</a> and forced to resign from his job as a professor at Moscow State University.</span></p><p><span>In neighboring Belarus, editor Mark Bernstein was doxxed by a pro-Russian group in 2022, arrested, and sentenced to three years of home confinement. As many as five other editors have been detained by Belarusian authorities in recent months, according to media reports and editors.</span></p><p><span>As these battles continued, the Russian government supported the creation of a more compliant alternative, called Ruwiki, which launched early last year with the copying of 1.9 million articles from the originals, edited to reflect the government view. On Ruwiki, edits must comply with Russian laws and are subject to approval from outside experts. There, the map of Ukraine does not include Donetsk or Kherson, the war is a “special operation” in response to NATO aggression, and accounts of torture in Bucha are fake news.</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-partial-2.png"></p><p><em>The first large-scale anti-Zionist demonstrations in Palestine, March 1920, during the Occupied Enemy Territory Administration. The crowd of Muslim and Christian Palestinians are shown outside Damascus Gate, Old City of Jerusalem.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2-2.png"></p><p><em>Palestinian political violence refers to acts of violence or terrorism committed by Palestinians with the intent to accomplish political goals in the context of the Israeli–Palestinian conflict.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3-2.png"></p><p><em>On January 6th, 2021, the United States Capitol in Washington, DC, was attacked by a mob of supporters of President Donald Trump in an attempted self-coup, two months after his defeat in the 2020 presidential election. </em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-4-1.png"></p><p><em>Pareidolia is the tendency for perception to impose a meaningful interpretation on a nebulous stimulus, usually visual, so that one detects an object, pattern, or meaning where there is none.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-5.png"></p><p>The Old Man and the Sea<em> is a 1952 novella by the American author Ernest Hemingway. </em></p></div></div><p><span>Wikipedia remains online in Russia, but with Ruwiki, the government may now feel emboldened to block it. In May, at a hearing on media safety for children, the head of the Russian Duma Committee on the Protection of the Family said that the encyclopedia’s “interpretation of our historical events feels so hostile that we need to raise the issue of blocking this information resource,” and that the encyclopedia’s depiction of history is opposed to Russian “<a href="https://www.moscowtimes.ru/2025/05/15/v-gosdume-prizvali-zablokirovat-v-rossii-vikipediyu-iz-za-vrazhdebnih-traktovok-istorii-a163473">traditional, spiritual values</a>.”</span></p><p><span>The goal of these campaigns is what the Wikimedia Foundation calls “project capture.” The term originates in an independent report the foundation commissioned in response to the takeover of the Croatian-language Wikipedia by a cabal of far-right editors.</span></p><p><span>In 2010, a group of editors won election to admin positions and began citing far-right alternative media to rewrite history. On Croatian Wikipedia, the Nazis invaded Poland to stop a genocide against the German people, Croatia’s role in the Holocaust is foreign propaganda, and <a href="https://en.wikipedia.org/wiki/Ratko_Mladi%C4%87">Ratko Mladić</a> was a decorated military leader whose conviction by the UN for genocide (briefly noted quite far down) was the result of an international conspiracy. When other editors attempted to correct the articles, the admins banned them for violating rules against hate speech or harassment.</span></p><p><span>The encyclopedia became so warped that it began receiving press coverage. The Croatian Minister of Education warned students not to use it. In an interview with a Croatian paper, Wales confirmed the foundation was aware of the problem and looking into it. Yet the foundation has a policy of allowing Wikipedia projects to self-govern, and interfering with Croatian Wikipedia risked opening a door to the many governments and companies that want things on Wikipedia changed.</span></p><p><span>Editors mounted a resistance and attempted to vote the admins out, but the admins defeated the attempt using votes from what were later revealed to be dozens of fake accounts. But because the admins were the only ones with the technical ability to trace IP addresses, the opposition had no way to prove this. The cabal now controlled all the levers of power. By 2019, nearly all of the editors who opposed them had been banned or harassed off the project.</span></p><p><span>In 2020, one of the few remaining dissident editors compiled a comprehensive textual and statistical analysis of editing patterns of dozens of accounts and filed a request for an admin to run IP traces to see if they were sock puppets. The admin stalled, then attempted to fudge the traces, but did so in such a transparent way that it was clear the accounts were indeed fakes.</span></p><p><span>This was the evidence required to procedurally break the cabal. High-ranking admins called “stewards” from other-language Wikipedias administered a new vote on banning the Croatian admins. This time, the admins lost. Their ringleader, username Kubura, was banned from all Wikipedia projects forever, a punishment that had been leveled against less than a dozen others in Wikipedia history. <a href="https://www.jutarnji.hr/vijesti/hrvatska/kuburin-pad-dobio-je-globalnu-blokadu-a-njegovi-se-sljedbenici-sada-bez-vode-povlace-15046719">A local daily covered</a> the incident with the headline “Kubura’s Downfall: Banned Globally, His Followers Retreat, Leaderless.”</span></p><div><p><span><span>Wikipedia’s processes are only effective if they are administered by people who believe in the spirit of the project</span></span></p></div><p><span><a href="https://meta.wikimedia.org/wiki/Croatian_Wikipedia_Disinformation_Assessment-2021">The foundation’s postmortem analysis</a> compared the takeover to “state capture, one of the most pressing issues of today’s worldwide democratic backsliding.” The clique still cited the reliability of sources and invoked rules of debate, but it bent these processes to serve their nationalist purpose. As many governments have discovered, it is extremely difficult to insert propaganda into Wikipedia without running afoul of some rule or another. But what the Croatia capture showed is that Wikipedia’s processes are only effective if they are administered by people who believe in the spirit of the project. If they can be silenced or replaced, it becomes possible to steer the encyclopedia in a different direction. </span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-right-1.png"></p><p><em>Donna Theo Strickland (born May 27th, 1959) is a Canadian optical physicist and pioneer in the field of pulsed lasers.</em> </p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3-3.png"></p><p><em>A telescope in the Very Large Telescope system producing four orange laser guide stars.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2-3.png"></p><p><em>Oral tradition, or oral lore, is a form of human communication in which knowledge, art, ideas, and culture are received, preserved, and transmitted orally from one generation to another.</em></p></div></div><p><span>One editor I spoke with, who asked to remain anonymous for reasons that will be obvious, had been editing Wikipedia for several years while living in a Middle Eastern country where much other media is tightly controlled. One day he received a call from a member of the intelligence service inviting him to lunch. He cried for hours — everyone knew what this meant. </span></p><p><span>The meeting was cordial but clear. They didn’t want him to stop editing Wikipedia. They wanted his help. They knew the encyclopedia has rules and you can’t just insert flagrant propaganda, but as a respected member of the community, maybe he could edit in ways that were a little friendlier to the government, maybe decide in its favor when certain topics came up for debate. In exchange, maybe the service could help him if he ever got in trouble with the police, for example, over his sexuality; he was gay in a country where that was illegal. </span></p><p><span>He fled the country weeks later. He now edits from abroad, but he knows of five to 10 others who have faced arrest or intimidation over their editing. They must do constant battle with editors he believes to be government agents who push the state’s perspective, debating tirelessly for hours because it is literally their job. </span></p><p><span>It’s a rare person who is able to uproot their life in the service of a volunteer side project. Understandably, many others faced with such threats become more cautious in their editing or stop altogether. Multiple editors based in India said that they now avoid editing topics related to their country. The ANI case had a chilling effect, as have recurring harassment campaigns. The far-right online publication <em>OpIndia</em> regularly accuses Wikipedia of “anti-Hindu and anti-India bias,” in ways that parallel attacks from the US right, down to citations of Manhattan Institute research and quotes from the disgruntled cofounder, Sanger. The organization has published the real names and employers of editors it accuses of being “leftists” or “Islamists,” leading at least one veteran editor to delete their account.</span></p><p><span>Even ancient history can be cause for reprisals. In February, after the release of a Bollywood action film about <a href="https://en.wikipedia.org/wiki/Sambhaji">Chhatrapati Sambhaji Maharaj</a>, a 17th-century king who fought the Mughals, accounts on X began whipping up outrage over several facts on Sambhaji’s Wikipedia page that they deemed to be anti-Hindu. When editors reversed attempts to delete the offending lines, another X user posted their usernames and called on government officials to investigate them. Days later, local press reported that the Maharashtra cyber police opened cases against at least four editors.</span></p><div><p><span><span>“If you issue cases and file complaints against editors, they tend not to edit those pages anymore”</span></span></p></div><p><span>“Various editors have left Wikipedia over this persecution, fearing their own safety,” said an Indian Wikipedia editor who asked to remain anonymous out of fear of retaliation. “I believe this is completely useful for the right wing, if you issue cases and file complaints against editors, they tend not to edit those pages anymore, fearing for their safety in real life.”</span></p><p><span>He still edits, but mostly sticks to the safer ground of the Roman Empire.</span></p><hr><p><span>In</span><span> April, the Trump administration’s interim US attorney for DC, Edward Martin Jr., <a href="https://www.theverge.com/news/656720/ed-martin-dc-attorney-wikipedia-nonprofit-threat">sent a letter to the Wikimedia Foundation</a> accusing the organization of disseminating “propaganda” and intimating that it had violated its duties as a tax-exempt nonprofit.</span></p><p><span>From a legal perspective, it was an odd document. The tax status of nonprofits is not generally the jurisdiction of the US attorney for DC, and many of the supposed violations, like having foreign nationals on its board or permitting “the rewriting of key, historical events and biographical information of current and previous American leaders,” are not against the law. Sanger is quoted, criticizing editor anonymity. In several cases, the rules Martin accuses Wikipedia of violating are Wikipedia’s own, like a commitment to neutrality. But the implied threat was clear.</span></p><p><span>“We’ve been anticipating something like this letter happening for some time,” a longtime editor, Lane Rasberry, said. It fits the pattern seen in India and elsewhere. He has been hearing more reports of threats against editors who work on pages related to trans issues and has been conducting security trainings to prevent their identities being revealed. Several US-based editors told me they now avoid politically contentious topics out of fear that they could be doxxed and face professional or legal retaliation. “There are more Wikipedia editors getting threats, more people getting scared,” Rasberry said.</span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-partial.png"></p><p><em>The “little green men” were Russian soldiers who were masked and wore unmarked uniforms upon the outbreak of the Russo–Ukrainian War in 201</em>4.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2-4.png"></p><p><em>The 2019–2020 Hong Kong protests (also known by other names) were a series of demonstrations against the Hong Kong government’s introduction of a bill to amend the Fugitive Offenders Ordinance in regard to extradition. </em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-4-2.png"></p><p><em>May 2015 satellite image of the Crimean Peninsula</em>.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-6.png"></p><p><em>The owl of Athena, a symbol of knowledge in the Western world.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3-4.png"></p><p><em>Sambhaji, also known as Shambhuraje, ruled from 1681 to 1689 as the second king (Chhatrapati) of the Maratha Empire, a prominent state in early modern India.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-5-1.png"></p><p><em>Stanislav Alexandrovich Kozlovsky is a Russian scientist-psychologist and specialist in the field of cognitive neuroscience of memory and perception.</em></p></div></div><p><span>Talking to editors, I encountered a confounding spread of opinions about the seriousness of the threat to Wikipedia, often in the same conversation. The site has sloughed off more than two decades of attacks, and so far the latest round is no different. The Heritage Foundation plan to dox editors has yet to materialize. Musk’s calls for his followers to stop donating have resulted in surges in donations, according to publicly available data.</span></p><p><span>In India, the High Court struck down the order to take down the article about ANI’s defamation case, though the case itself is ongoing. Wikipedia’s critics on the right and in the Silicon Valley elite often propose generative AI as the solution to Wikipedia’s perceived biases, for each user a bespoke source of ideologically agreeable information. Yet all these projects remain wholly reliant on Wikipedia, and so far the most aggressive such initiative, Musk’s Grok, has spent much of its existence flailing between fact-checking Musk’s own conspiracy theories and <a href="https://www.theguardian.com/technology/2025/jul/09/grok-ai-praised-hitler-antisemitism-x-ntwnfb">proclaiming itself MechaHitler</a>.</span></p><p><span>But new threats continue to appear. In August, the foundation lost its case arguing for an exemption from the UK’s Online Safety Act, which would force Wikipedia to verify the identities of its editors, though it is continuing to appeal. In Portugal the foundation received a court order arising from a defamation case brought by Portuguese American businessman Cesar DePaço, who objected to information on his page about past criminal allegations and links to the far-right Portuguese party Chega. Complying with the ruling, the foundation struck several facts from his biography and disclosed “a small amount of user data” about eight editors. The foundation is now bringing the case before the European Court of Human Rights. And in the US, there is the recent House Oversight letter.</span></p><p><span>No matter the outcome, these cases contribute to a general increase in pressure on the project’s already strained editors. English Wikipedia has fewer than 40,000 active editors, defined as users who have made five or more edits in the last month. The number of active administrators, crucial to maintaining the site and enforcing policy, peaked in 2008 and now stands at around 450. AI threatens to squeeze the editor pipeline further. The more people who get information from AI summaries of Wikipedia rather than the site itself, the fewer people who will wander down a rabbit hole, encounter an error that needs correcting, and become editors themselves. </span></p><div><p><span><span>“Wikipedia should not be taken for granted.”</span></span></p></div><p><span>At the same time, people are <a href="https://aclanthology.org/2024.wikinlp-1.12/">using AI</a> to add plausible-looking but false or biased information to the encyclopedia, increasing the workload for editors. Harassment, ideological editing campaigns, government investigations, targeted lawsuits — even if they lead nowhere, they will make the prospect of editing more daunting and increase the odds that current editors burn out. “Wikipedia should not be taken for granted,” Rasberry said. “This is an existential threat.” </span></p><p><span>The first reactions to the Martin letter on the Wikipedia editor forums were radical: the foundation should leave the US, maybe for France, or Iceland, or Germany. This would not be unprecedented, an editor pointed out. The Encyclopédistes fled to <a href="https://library.csun.edu/sca/peek-stacks/encyclopedie">Switzerland</a> when the ancien régime attempted to censor them. Maybe the site should go dark in protest. </span></p><p><span>But moderation soon prevailed. “The community needs to chill on the blackout talk,” wrote an editor by the name of Tazerdadog. “We’re not there yet.” Right now, the best response to these threats is to double down on Wikipedia’s policies, particularly the refusal to be censored and its dedication to neutral point of view, they wrote. </span></p><div><p><span><span><a href="https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view"><img src="https://s3.us-east-1.amazonaws.com/assets.sbnation.com/csk/uploads/verge-features/wikipedia/pqs/NPOV.svg" alt="NPOV"></a></span></span></p></div><p><span>“I 100% agree with you, Tazerdadog,” replied “Jimbo Wales.” “Emphasizing to the WMF that NPOV is non-negotiable is not really the issue.” In fact, Wales wrote, he is chairing a working group on strengthening the policy. The initiative was announced in March, framed as a response to the <a href="https://diff.wikimedia.org/2025/03/04/global-trends-2025/">global rise</a> in threats to sources of neutral information, and to a fragmentation of the public’s understanding of the very concepts of neutrality and facts. Wikipedia’s response, it seemed, would be to neutral harder. </span></p><p><span>In May, I met Wales for coffee at a members club in Chelsea where he had been granted an honorary membership after giving a talk. (Wikipedia, as journalists have noted for years, <a href="https://qz.com/98600/wikipedia-founder-jimmy-wales-is-only-worth-1-million">did not make Wales a tech billionaire</a>.) Extravagant bouquets of pastel flowers were arranged in an arch above the doorway and festooned the tables of the interior. Wales, dressed to meet his wife at the Chelsea Flower Show, matched the decor in a green linen suit and floral shirt. He does not, he said, normally dress like a leprechaun. </span></p><p><span>He was not particularly concerned about the attacks on Wikipedia, he said, though he warned that he is “pathologically optimistic.” Wikipedia has been attacked since it began. It fought Turkey’s ban to the Constitutional Court and won. Even Russian Wikipedia has proven resilient. In the US, the government lacks much of the leverage it has deployed against other institutions. Wikipedia doesn’t rely on government funding, and protections for online speech are strong. <a href="https://meta.wikimedia.org/wiki/Fundraising/2023-24_Report#Average_Amount_by_Fiscal_Year">In the last fiscal year</a>, the foundation took in $170 million in donations, with an average size of about $10.</span></p><p><span>As for the accusations of bias, why not investigate? Whether the attacks are in good faith or bad, it doesn’t really matter, Wales said. The foundation had already decided that it was a good time, given the fragmented and polarizing world, to examine and bolster Wikipedia’s neutrality processes. Wales, leaning over the coffee table, seemed excited at the prospect. </span></p><p><span>“If somebody turns up on a talk page and says, ‘Hey, this article is a mess, it’s wrong. It’s really biased,’ the right answer is to not scream at them and run and hide. The right answer is go, ‘Oh, tell me more. Let’s dig in. Where is it biased? How do we think about how do we fix that?’”</span></p><p><span>Let’s figure out the best methodologies for studying neutrality, Wales said. Let’s look at how editors evaluate the reliability of sources. Maybe Wikipedia does use the label “far-right” more than “far-left,” Wales said, <a href="https://davidrozado.substack.com/p/mentions-of-political-extremism-in-wikipedia">a criticism that has been leveled at the site</a>. Is that because the media uses the term more, and does Wikipedia use the term more or less than the media does, and does the media use the term more because there are more far-right movements in the world today? </span></p><p><span>“You have to chew on these things. There’s no simple answers.”</span></p><p><span>But there are answers. If the social platforms and language models that increasingly shape our understanding of the world are inscrutable black boxes, Wikipedia is the opposite, maybe the most legible, endlessly explainable information management system ever made. For any sentence, there is a source, and a reason that that source was used, and a reason for that reason. </span></p><p><span>“Let’s dig in,” Wales repeated. “Let’s assess the evidence. Let’s talk to a lot of different people. Let’s really try and understand.” Come, be part of the process. His working group is starting to discuss the best approach. The meetings, Wales acknowledged, have been very tedious so far.</span></p><p><span>As for the letter from the interim DC attorney, Trump withdrew Martin’s nomination in May, though he still has a position leading the Justice Department’s retribution-oriented “<a href="https://www.nytimes.com/2025/07/01/us/politics/justice-department-rioter-weaponization.html">task force on weaponization</a>.” In any case, the Wikimedia Foundation responded promptly. </span></p><p><span>“The foundation staff spent a lot of passion writing it,” Wales said of the reply. “Then they ran it by me for review, and I was ready to jump in, but I was like, actually, it’s perfect.” </span></p><p><span>“It’s very calm,” Wales said. “Here are the answers to your questions, here is what we do.” It explains how Wikipedia </span><span>works.<span></span></span></p><div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-1-partial-1.png"></p><p><em>An edit-a-thon is an event where some editors of online communities such as Wikipedia, OpenStreetMap (also known as a “mapathon”), and LocalWiki edit and improve a specific topic or type of content. </em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-2-5.png"></p><p><em>The Quaker business method or Quaker decision-making is a form of group decision-making and discernment, as well as of direct democracy, used by Quakers, or members of the Religious Society of Friends, to organise their religious affairs.</em> </p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-4-4.png"></p><p><em>Wikipedia’s goal is to create a well-written, reliable encyclopedia like the</em> Encyclopædia Britannica<em>, except Wikipedia is much, much bigger:</em> Britannica <em>has about 120,000 articles, while the English Wikipedia has over 7 million articles.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/WIKI-GLOBE-5.gif"></p><p>The Wikipedia globe.</p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-7.png"></p><p><em>The Wikipedia Monument, located in Słubice, Poland, is a statue designed by Armenian sculptor Mihran Hakobyan honoring Wikipedia contributors.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-5-2.png"></p><p><em>Socrates was known to steadfastly assume others around him were acting in good faith.</em></p></div><div tabindex="0"><p><img alt="" loading="lazy" decoding="async" data-nimg="fill" src="https://platform.theverge.com/wp-content/uploads/sites/2/2025/09/image-3.jpg"></p><p><em>Jimmy Donal Wales (born August 7th, 1966), also known as Jimbo Wales, is an American internet entrepreneur and former financial trader. </em></p></div></div><div><h4>Credits</h4><p><span>Editor: Kevin Nguyen</span><span>Creative director: Kristen Radtke</span><span>Art director/designer: Cath Viginia</span><span>Developer: Graham MacAree</span><span>Copyeditor: Kallie Plagge</span><span>Factchecker: Tiên Nguyễn</span><span>Engagement editors: Esther Cohen &amp; Tristan Cooper</span><span>Managing editor: Kara Verlaney</span><span>Editor-in-chief: Nilay Patel</span><span>Publisher: Helen Havlak</span></p><p>Photos by A. Ghizzi Panizza, Anton Holoborodko, Arkady Zakharov, Barbara Niggl Radloff, Bengt Nyman, C-SPAN, Clister V. Pangantihon, Daniele Venturelli, David Gadd, Edward Kimmel, Eric Chan, Eric Gaba,German Federal Archive, Getty Images, Gnom, Hadi Mohammad, Hendrik Freitag, Iva Njunjić, Jan Ainali, Knight Foundation, Larry Sanger, Marie-Lan Nguyen, Mario Tama, NASA, Nina Aldin Thune, Nostrix, PBS News Hour, Rodhullandemu, Sannse, SiGarb, Studio Incendo, Tyler Merbler, Zack McCune</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[WiFi signals can measure heart rate (333 pts)]]></title>
            <link>https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/</link>
            <guid>45127983</guid>
            <pubDate>Thu, 04 Sep 2025 14:53:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/">https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/</a>, See on <a href="https://news.ycombinator.com/item?id=45127983">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>


<main><div>



<div id="press-inquiries-block-block_5f5cf9b7242cb54cc87d9c25bbc726c0" inert="">
						<h2>Press Contact</h2>
						
					</div>



<div>
<h2 id="h-key-takeaways">Key takeaways</h2>



<ul>
<li>The Pulse-Fi system is highly accurate, achieving clinical-level heart rate monitoring with ultra low-cost WiFi devices, making it useful for low resource settings. </li>



<li>The system works with the person in a variety of different positions and from up to 10 feet away.</li>
</ul>
</div>



<p>Heart rate is one of the most basic and important indicators of health, providing a snapshot into a person’s physical activity, stress and anxiety, hydration level, and more.</p>



<p>Traditionally, measuring heart rate requires some sort of wearable device, whether that be a smart watch or hospital-grade machinery. But new research from engineers at the University of California, Santa Cruz, shows how the signal from a household WiFi device can be used for this crucial health monitoring with state-of-the-art accuracy—without the need for a wearable.</p>



<p>Their proof of concept work demonstrates that one day, anyone could take advantage of this non-intrusive WiFi-based health monitoring technology in their homes. The team proved their technique works with low-cost WiFi devices, demonstrating its usefulness for low resource settings.</p>



<p><a href="https://ieeexplore.ieee.org/abstract/document/11096342">A study</a> demonstrating the technology, which the researchers have coined “Pulse-Fi,” was published in the proceedings of the 2025 IEEE International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT).</p>



<h4 id="h-measuring-with-wifi"><strong>Measuring with WiFi</strong></h4>



<figure><img decoding="async" width="2560" height="1707" src="https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-scaled.jpg" alt="Katia Obraczka and Nayan Bhatia with a computer in the lab." srcset="https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-scaled.jpg 2560w, https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-300x200.jpg 300w, https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-1024x683.jpg 1024w, https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-768x512.jpg 768w, https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-1536x1024.jpg 1536w, https://news.ucsc.edu/wp-content/uploads/2025/08/20250814-Pulse-Fi-Research-EC-07-2048x1365.jpg 2048w" sizes="(max-width: 2560px) 100vw, 2560px"><figcaption>Professor of Computer Science and Engineering Katia Obraczka and Ph.D. student Nayan Bhatia in the lab.</figcaption></figure>



<p>A team of researchers at UC Santa Cruz’s Baskin School of Engineering that included Professor of Computer Science and Engineering Katia Obraczka, Ph.D. student Nayan Bhatia, and high school student and visiting researcher Pranay Kocheta designed a system for accurately measuring heart rate that combines low-cost WiFi devices with a machine learning algorithm.</p>



<p>WiFi devices push out radio frequency waves into physical space around them and toward a receiving device, typically a computer or phone. As the waves pass through objects in space, some of the wave is absorbed into those objects, causing mathematically detectable changes in the wave.</p>



<p>Pulse-Fi uses a WiFi transmitter and receiver, which runs Pulse-Fi’s signal processing and machine learning algorithm. They trained the algorithm to distinguish even the faintest variations in signal caused by a human heart beat by filtering out all other changes to the signal in the environment or caused by activity like movement.</p>



<p>“The signal is very sensitive to the environment, so we have to select the right filters to remove all the unnecessary noise,” Bhatia said.</p>



<div>
<figure><img decoding="async" width="605" height="814" src="https://news.ucsc.edu/wp-content/uploads/2025/09/Pranay_headshot.jpg" alt="Portrait of Pranay Kocheta" srcset="https://news.ucsc.edu/wp-content/uploads/2025/09/Pranay_headshot.jpg 605w, https://news.ucsc.edu/wp-content/uploads/2025/09/Pranay_headshot-223x300.jpg 223w" sizes="(max-width: 605px) 100vw, 605px"><figcaption>High school student Pranay Kocheta joined the Pulse-Fi project as a researcher through UC Santa Cruz’s Science Internship Program.</figcaption></figure>
</div>



<h4 id="h-dynamic-results"> Dynamic results</h4>



<p>The team ran experiments with 118 participants and found that after only five seconds of signal processing, they could measure heart rate with clinical-level accuracy. At five seconds of monitoring, they saw only half a beat-per-minute of error, with longer periods of monitoring time increasing the accuracy.</p>



<p>The team found that the Pulse-Fi system worked regardless of the position of the equipment in the room or the person whose heart rate was being measured—no matter if they were sitting, standing, lying down, or walking, the system still performed. For each of the 118 participants, they tested 17 different body positions with accurate results</p>



<p>These results were found using ultra-low-cost ESP32 chips, which retail between $5 and $10 and Raspberry Pi chips, which cost closer to $30. Results from the Raspberry Pi experiments show even better performance. More expensive WiFi devices like those found in commercial routers would likely further improve the accuracy of their system.</p>



<p>They also found that their system had accurate performance with a person three meters, or nearly 10 feet, away from the hardware. Further testing beyond what is published in the current study shows promising results for longer distances.</p>



<p>“What we found was that because of the machine learning model, that distance apart basically had no effect on performance, which was a very big struggle for past models,” Kocheta said. “The other thing was position—all the different things you encounter in day to day life, we wanted to make sure we were robust to however a person is living.”</p>



<h4 id="h-creating-the-dataset">Creating the dataset</h4>



<div>
<figure><img loading="lazy" decoding="async" width="895" height="1024" src="https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955-895x1024.jpg" alt="Nayan holds up an ESP32 chip." srcset="https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955-895x1024.jpg 895w, https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955-262x300.jpg 262w, https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955-768x879.jpg 768w, https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955-1342x1536.jpg 1342w, https://news.ucsc.edu/wp-content/uploads/2025/09/20250814-Pulse-Fi-Research-EC-12-scaled-e1756417722955.jpg 1707w" sizes="auto, (max-width: 895px) 100vw, 895px"><figcaption>The researchers proved their heart rate monitoring technique works with ultra-low-cost, WiFi-emitting ESP32 chips, which retail between $5 and $10.</figcaption></figure>
</div>



<p>To make their heart rate detection system work, the researchers needed to train their machine learning algorithm to distinguish the faint detections in WiFi signals caused by a human heartbeat. They found that there was no existing data for these patterns using an ESP32 device, so they set out to create their own dataset.</p>



<p>In the UC Santa Cruz Science and Engineering library, they set up their ESP32 system along with a standard oximeter to gather “ground truth” data. By combining the data from the Pulse-Fi setup with the ground truth data, they could teach a neural network which changes in signals corresponded with heart rate.</p>



<p>In addition to the ESP32 dataset they collected, they also tested Pulse-Fi using a dataset produced by a team of researchers in Brazil using a Raspberry Pi device, which created the most extensive existing dataset on WiFi for heart monitoring, as far as the researchers are aware.</p>



<h4 id="h-beyond-heart-rate">Beyond heart rate</h4>



<p>Now, the researchers are working on further research to extend their technique to detect breathing rate in addition to heart rate, which can be useful for the detection of conditions like sleep apnea. Unpublished results show high promise for accurate breathing rate and apnea detection.</p>



<p>Those interested in commercial use of this technology can contact Assistant Director of Innovation Transfer Marc Oettinger: <a href="mailto:marc.oettinger@ucsc.edu">marc.oettinger@ucsc.edu</a>.</p>


<div><h2>Related Topics</h2></div>




</div></main>


</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Hollow Knight: Silksong Causes Server Chaos on Xbox, Steam, and Nintendo (204 pts)]]></title>
            <link>https://www.eurogamer.net/silksong-causes-server-chaos-on-xbox-steam-and-nintendo-as-platforms-grind-to-a-halt</link>
            <guid>45127816</guid>
            <pubDate>Thu, 04 Sep 2025 14:39:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eurogamer.net/silksong-causes-server-chaos-on-xbox-steam-and-nintendo-as-platforms-grind-to-a-halt">https://www.eurogamer.net/silksong-causes-server-chaos-on-xbox-steam-and-nintendo-as-platforms-grind-to-a-halt</a>, See on <a href="https://news.ycombinator.com/item?id=45127816">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
      <article data-ads="true" data-article-type="news" data-article-group="news" data-paywalled="false" data-premium="false" data-sponsored="false" data-type="article">


<header data-component="article-header">

    


  <div>
  <figure>
  <img src="https://assetsio.gnwcdn.com/silk-down.jpg?width=570&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/silk-down.jpg?width=570&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/silk-down.jpg?width=570&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" loading="eager" alt="" data-src-full="https://assetsio.gnwcdn.com/silk-down.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" width="570" height="321" fetchpriority="high">

        <figcaption>
          <span>Image credit: <cite>Eurogamer</cite></span>
        </figcaption>
  </figure>
  </div>

    

</header>
    <div data-component="article-content">



            <p>A little game by the name of <a href="https://www.eurogamer.net/games/hollow-knight-silksong">Hollow Knight: Silksong</a> just released, and it has thrown platforms into chaos.</p>
<p>As you can see from images captured by the Eurogamer team, the likes of Steam was brought to a grinding halt as many flocked to get their hands on the highly-anticipated sequel.</p>
<p>Meanwhile, several of us have been unable to add the game to our carts across Xbox, PlayStation and Switch. The PS store, for example, is stuck on Wishlisted at the time of writing.</p>
<p>
In the words of our Connor: "Steam it looks like every step has issues, trying to pay with Paypal is leading to error messages."</p>
<p>Are you having more luck than us?</p>
<figure>
<img alt="PS Store" data-src-full="https://assetsio.gnwcdn.com/20250904150859.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="388" loading="lazy" src="https://assetsio.gnwcdn.com/20250904150859.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/20250904150859.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/20250904150859.jpg?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="690">
<figcaption>Silksong is stuck on Wishlist on PlayStation.  | <span>Image credit: <cite>Eurogamer</cite></span></figcaption>
</figure>
<figure>
<img alt="Dom's TV as he tries to get Silksong on Xbox" data-src-full="https://assetsio.gnwcdn.com/PXL_20250904_140557914.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="520" loading="lazy" src="https://assetsio.gnwcdn.com/PXL_20250904_140557914.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/PXL_20250904_140557914.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/PXL_20250904_140557914.jpg?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="690">
<figcaption>Trying to get Silksong on Xbox, but only getting this blank screen.  | <span>Image credit: <cite>Eurogamer</cite></span></figcaption>
</figure>
<figure>
<img alt="" data-src-full="https://assetsio.gnwcdn.com/PXL_20250904_142226809.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="916" loading="lazy" src="https://assetsio.gnwcdn.com/PXL_20250904_142226809.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/PXL_20250904_142226809.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/PXL_20250904_142226809.jpg?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="690">
<figcaption>Dom also got this 'Silksong unavailable' screen on Xbox. </figcaption>
</figure>
<figure>
<img alt="Image from Steam showing an error when trying add Silksong to the cart" data-src-full="https://assetsio.gnwcdn.com/image-(89).png?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="218" loading="lazy" src="https://assetsio.gnwcdn.com/image-(89).png?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/image-(89).png?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/image-(89).png?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="315">
<figcaption>Unable to add Silksong to cart on Steam.  | <span>Image credit: <cite>Eurogamer</cite></span></figcaption>
</figure>
<figure>
<img alt="" data-src-full="https://assetsio.gnwcdn.com/PXL_20250904_140926248.jpg?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="520" loading="lazy" src="https://assetsio.gnwcdn.com/PXL_20250904_140926248.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/PXL_20250904_140926248.jpg?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/PXL_20250904_140926248.jpg?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="690">
<figcaption>Switch 2 is also having some Silksong-related issues. </figcaption>
</figure>
<figure>
<img alt="" data-src-full="https://assetsio.gnwcdn.com/image-(88)_i6qCFQg.png?width=1920&amp;height=1920&amp;fit=bounds&amp;quality=70&amp;format=jpg&amp;auto=webp" height="341" loading="lazy" src="https://assetsio.gnwcdn.com/image-(88)_i6qCFQg.png?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp" srcset="https://assetsio.gnwcdn.com/image-(88)_i6qCFQg.png?width=690&amp;quality=70&amp;format=jpg&amp;auto=webp 1x, https://assetsio.gnwcdn.com/image-(88)_i6qCFQg.png?width=690&amp;quality=70&amp;format=jpg&amp;dpr=2&amp;auto=webp 2x" width="690">
<figcaption>Steam screenshot showing that "something went wrong" as we tried to purchase Silksong.  | <span>Image credit: <cite>Eurogamer</cite></span></figcaption>
</figure>

        </div>


      </article>



  
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Atlassian is acquiring the Browser Company (144 pts)]]></title>
            <link>https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html</link>
            <guid>45127636</guid>
            <pubDate>Thu, 04 Sep 2025 14:25:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html">https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html</a>, See on <a href="https://news.ycombinator.com/item?id=45127636">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108193265" data-test="InlineImage"><p>Mike Cannon-Brookes, co-founder and CEO of Atlassian, speaks at the National Electrical Vehicle Summit in Canberra, Australia, on Aug. 19, 2022. Cannon-Brookes is urging Australia to show more ambition on climate action, even as the new government legislates plans to strengthen the country's carbon emissions cuts.</p><p>Hilary Wardhaugh | Bloomberg | Getty Images</p></div><div><p><span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-1"><a href="https://www.cnbc.com/quotes/TEAM/">Atlassian</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> said it has agreed to acquire The Browser Co., a startup that offers a web browser with <a href="https://www.cnbc.com/ai-effect/">artificial intelligence</a> features, for $610 million in cash.</p><p>The companies aim to close the deal in Atlassian's fiscal second quarter, which ends in December.</p><p>Established in 2019, The Browser Co. has gone up against some of the world's largest companies, including <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-3"><a href="https://www.cnbc.com/quotes/GOOGL/">Google</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, with Chrome, and <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-4"><a href="https://www.cnbc.com/quotes/AAPL/">Apple</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span>, which includes Safari on its computers running MacOS.</p><p>The startup debuted Arc, a customizable browser with a built-in whiteboard and the ability to share groups of tabs, in 2022. The Dia browser, a simpler option that allows people to chat with an AI assistant about multiple browser tabs at once, became available in beta in June.</p><p>Atlassian co-founder and CEO Mike Cannon-Brookes said he sees shortcomings in the most popular browsers for those who do much of their work on computers.</p><p>"Whatever it is that you're actually doing in your browser is not particularly well served by a browser that was built in the name to browse," he said in an interview. "It's not built to work, it's not built to act, it's not built to do."</p><p>Cannon-Brookes said Arc has helped him feel like he can manage his work, with its ability to organize tabs and automatically archive old ones.</p><p>But only a small percentage of people who used The Browser Co.'s Arc adopted the program's special features.</p><p>"Our metrics were more like a highly specialized professional tool (like a video editor) than a mass-market consumer product, which we aspired to be closer to," Josh Miller, The Browser Co.'s co-founder and CEO, said in a <a href="https://browsercompany.substack.com/p/letter-to-arc-members-2025" target="_blank">newsletter update</a>. The startup stopped building new features for Arc, leading to questions of whether it would release the browser under an open-source license.</p></div><div id="RegularArticle-RelatedContent-1"><h2>Read more CNBC tech news</h2><div><ul><li><a href="https://www.cnbc.com/2025/09/04/huawei-mate-xts-trifold-launch-specs-price-features.html">Huawei launches second trifold smartphone at $2,500 as it looks to cement comeback</a></li><li><a href="https://www.cnbc.com/2025/09/03/c3-ai-earnings-ceo-siebel.html">C3 AI reports declining revenue, announces new CEO to replace Siebel</a></li><li><a href="https://www.cnbc.com/2025/09/03/openai-boosts-size-of-secondary-share-sale-to-10point3-billion.html">OpenAI boosts size of secondary share sale to $10.3 billion</a></li><li><a href="https://www.cnbc.com/2025/09/03/apple-might-raise-iphone-prices-despite-its-handling-of-tariffs-so-far.html">Apple has survived Trump's tariffs so far. It might raise iPhone prices anyway</a></li></ul></div></div><div><p>AI search startup Perplexity, which <a href="https://www.cnbc.com/2025/08/12/perplexity-google-chrome-ai.html">offered Google</a> $34.5 billion for Chrome, talked with The Browser Co. about a possible acquisition in December, <a href="https://www.theinformation.com/articles/wild-chrome-bid-perplexity-hunting-browsers" target="_blank">The Information</a> reported. OpenAI also held deal talks with The Browser Co., according to the report.</p><p>Cannon-Brookes wouldn't specify whether Atlassian considered buying Google's browser. Last year, the U.S. Justice Department <a href="https://www.cnbc.com/2024/11/20/doj-pushes-for-google-to-break-off-chrome-browser-after-antitrust-case.html">proposed</a> a divestiture after a federal judge ruled that the company enjoyed an internet search monopoly.</p><p>"I'm not even sure if there is a bidding competition for Chrome," Cannon-Brookes said. "I didn't see Google putting up an auction just yet. Look, I think we focus on actually getting acquisitions done and actually making those products a part of a coherent whole and delivering value for our customers. I'm not sure that stunt PR acquisition offers are really our thing, but we'll leave that for them to do."</p><p>Perplexity has been providing early access to its own AI browser, which is named Comet.</p><p>The Browser Co. was valued at $550 million last year. Investors include Atlassian Ventures, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-9"><a href="https://www.cnbc.com/quotes/CRM/">Salesforce</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> Ventures, <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-10"><a href="https://www.cnbc.com/quotes/FIG/">Figma</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> co-founder Dylan Field and LinkedIn co-founder Reid Hoffman.</p><p>The browser is central for those using Atlassian products, such as the Jira project management software, which shows existing support requests on the web. But the plan isn't simply to make it nicer to work with Atlassian products online.</p><p>"It's really about taking Arc's SaaS application experience and power user features, and Dia's AI and elegance and speed and sort of svelte nature, and Atlassian's enterprise know-how, and working out how to put all that together into Dia, or into the AI part of the browser," Cannon-Brookes said.</p></div><div id="Placeholder-ArticleBody-Video-108170380" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000381960" aria-labelledby="Placeholder-ArticleBody-Video-108170380"><p><img src="https://image.cnbcfm.com/api/v1/image/108170381-17521703481752170344-40657684475-1080pnbcnews.jpg?v=1752170347&amp;w=750&amp;h=422&amp;vtcrop=y" alt="AI browsers eye Google's turf"><span></span><span></span></p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Calling your boss a dickhead is not a sackable offence, UK tribunal rules (243 pts)]]></title>
            <link>https://www.theguardian.com/money/2025/sep/04/calling-your-boss-a-dickhead-is-not-a-sackable-offence-tribunal-rules</link>
            <guid>45127542</guid>
            <pubDate>Thu, 04 Sep 2025 14:16:59 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.theguardian.com/money/2025/sep/04/calling-your-boss-a-dickhead-is-not-a-sackable-offence-tribunal-rules">https://www.theguardian.com/money/2025/sep/04/calling-your-boss-a-dickhead-is-not-a-sackable-offence-tribunal-rules</a>, See on <a href="https://news.ycombinator.com/item?id=45127542">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="maincontent"><p>Managers and supervisors brace yourselves: calling the boss a dickhead is not necessarily a sackable offence, a tribunal has ruled.</p><p>The ruling came in the case of an office manager who was sacked on the spot when – during a row – she called her manager and another director dickheads.</p><p>Kerrie Herbert has been awarded almost £30,000 in compensation and legal costs after an employment tribunal found she had been unfairly dismissed.</p><p>The employment judge Sonia Boyes ruled that the scaffolding and brickwork company she worked for had not “acted reasonably in all the circumstances in treating [her] conduct as a sufficient reason to dismiss her”.</p><p>“She made a one-off comment to her line manager about him and a director of the business,” Boyes said. “The comment was made during a heated meeting.</p><p>“Whilst her comment was not acceptable, there is no suggestion that she had made such comments previously. Further … this one-off comment did not amount to gross misconduct or misconduct so serious to justify summary dismissal.”</p><p>The hearing in Cambridge was told Herbert started her £40,000-a-year role at the <a href="https://www.theguardian.com/uk-news/northampton" data-link-name="in body link" data-component="auto-linked-tag">Northampton</a> firm Main Group Services in October 2018. The business was run by Thomas Swannell and his wife, Anna.</p><p>The tribunal heard that in May 2022 the office manager had found documents in her boss’s desk about the costs of employing her, and became upset as she believed he was going to let her go.</p><p>When Swannell then raised issues about her performance, she began crying, the hearing was told.</p><p>She told the tribunal that she said: “If it was anyone else in this position they would have walked years ago due to the goings-on in the office, but it is only because of you two dickheads that I stayed.”</p><p>She said Swannell retorted: “Don’t call me a fucking dickhead or my wife. That’s it, you’re sacked. Pack your kit and fuck off.”</p><p>Herbert said she asked if he was really firing her and he answered: “Yes I have, now fuck off.”</p><p>The office manager then sued the firm for unfair dismissal.</p><p>The hearing was told that under the terms of her contract, she could be fired for “the provocative use of insulting or abusive language”.</p><p>However, this required she be given a prior warning. Only more serious breaches such as “threatening and intimidating language” would be gross misconduct and warrant summary dismissal.</p><p>Boyes found that Herbert was summarily fired because of her use of the word “dickheads” and ruled that the company had failed to follow proper disciplinary procedures.</p><p>She concluded that calling her bosses dickheads was not sufficient to fire Herbert and ordered the firm to pay £15,042.81 in compensation.</p><p>In her latest judgment she also ruled it had to pay £14,087 towards her legal fees.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[We Found the Hidden Cost of Data Centers. It's in Your Electric Bill [video] (151 pts)]]></title>
            <link>https://www.youtube.com/watch?v=YN6BEUA4jNU</link>
            <guid>45126531</guid>
            <pubDate>Thu, 04 Sep 2025 12:33:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.youtube.com/watch?v=YN6BEUA4jNU">https://www.youtube.com/watch?v=YN6BEUA4jNU</a>, See on <a href="https://news.ycombinator.com/item?id=45126531">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Almost anything you give sustained attention to will begin to loop on itself (621 pts)]]></title>
            <link>https://www.henrikkarlsson.xyz/p/attention</link>
            <guid>45126503</guid>
            <pubDate>Thu, 04 Sep 2025 12:29:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.henrikkarlsson.xyz/p/attention">https://www.henrikkarlsson.xyz/p/attention</a>, See on <a href="https://news.ycombinator.com/item?id=45126503">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!jygJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!jygJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 424w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 848w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 1272w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!jygJ!,w_2400,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png" width="1200" height="841" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:false,&quot;imageSize&quot;:&quot;large&quot;,&quot;height&quot;:841,&quot;width&quot;:1200,&quot;resizeWidth&quot;:1200,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:&quot;center&quot;,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!jygJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 424w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 848w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 1272w, https://substackcdn.com/image/fetch/$s_!jygJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7fb0ff-d0da-4f32-91f3-f20fb768fc58_1200x841.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><h6><strong>Brioches and Knife, Eliot Hodgkin, 08/1961</strong></h6><p><span>When people talk about the value of paying attention and slowing down, they often make it sound prudish and monk-like. Attention is something we “have to protect.” And we have to “pay”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-1-169544261" target="_self" rel="">1</a></span><span> attention—like a tribute.</span></p><p>But we shouldn’t forget how interesting and overpoweringly pleasurable sustained attention can be. Slowing down makes reality vivid, strange, and hot.</p><p>Let me start with the most obvious example.</p><p>As anyone who has had good sex knows, sustained attention and delayed satisfaction are a big part of it. When you resist the urge to go ahead and get what you want and instead stay in the moment, you open up a space for seduction and fantasy. Desire begins to loop on itself and intensify.</p><p>I’m not sure what is going on here, but my rough understanding is that the expectation of pleasure activates the dopaminergic system in the brain. Dopamine is often portrayed as a pleasure chemical, but it isn’t really about pleasure so much as the expectation that pleasure will occur soon. So when we are being seduced and sense that something pleasurable is coming—but it keeps being delayed, and delayed skillfully—the phasic bursts of dopamine ramp up the levels higher and higher, pulling more receptors to the surface of the cells, making us more and more sensitized to the surely-soon-to-come pleasure. We become hyperattuned to the sensations in our genitals, lips, and skin.</p><p>And it is not only dopamine ramping up that makes seduction warp our attentional field, infusing reality with intensity and strangeness. There are a myriad of systems that come together to shape our feeling of the present: there are glands and hormones and multiple areas of the brain involved. These are complex physical processes: hormones need to be secreted and absorbed; working memory needs to be cleared and reloaded, and so on. The reason deep attention can’t happen the moment you notice something is that these things take time.</p><p><span>What’s more, each of these subsystems update what they are reacting to at a different rate. Your visual cortex can cohere in less than half a second. A stress hormone like cortisol, on the other hand, has a half-life of 60–90 minutes and so can take up to 6 hours to fully clear out after the onset of an acute stressor. This means that if we switch what we pay attention to more often than, say, every 30 minutes, our system will be more or less decohered—different parts will be “attending to” different aspects of reality.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-2-169544261" target="_self" rel="">2</a></span><span> There will be “attention residue” floating around in our system—leftovers from earlier things we paid attention to (thoughts looping, feelings circling below consciousness, etc.), which crowd out the thing we have in front of us right now, making it less vivid.</span></p><p>Inversely, the longer we are able to sustain the attention without resolving it and without losing interest, the more time the different systems of the body have to synchronize with each other, and the deeper the experience gets.</p><p>Locked in on the same thing, the subsystems begin to reinforce each other: the dopamine makes us aware of our skin, and sensations on the skin ramp up dopamine release, making us even more aware of our skin. A finger touches our belly, and we start to fantasize about where that finger might be going; and so now our fantasies are locked in, too, releasing even more dopamine and making us even more aware of our skin. The more the subsystems lock in, the more intense the feedback loops get. After twenty minutes, our sense of self has evaporated, and we’re in a realm where we do, feel, and think things that would seem surreal in other contexts.</p><p>Similar things happen when we are able to sustain our attention to things other than sex, too. The exact mechanics differ, I presume, but the basic pattern is that when we let our attention linger on something, our bodily systems synchronize and feed each other stimuli in an escalatory loop that restructures our attentional field.</p><p>Almost anything that we are able to direct sustained attention at will begin to loop on itself and bloom.</p><p>To take a dark example, if you focus on your anxiety, the anxiety can begin to loop on itself until you hyperventilate and get tunnel vision and become filled with nightmarish thoughts and feelings—a panic attack.</p><p><span>And you do the same thing with joy. If you learn to pay sustained attention to your happiness, the pleasant sensation will loop on itself until it explodes and pulls you into a series of almost hallucinogenic states, ending in cessation, where your consciousness lets go and you disappear for a while. This takes practice. The practice is called jhanas, and it is sometimes described as the inverse of a panic attack. I have only ever entered the first jhana, once while spending an hour putting our four-year-old to sleep and meditating on how wonderful it is to lie there next to her. It was really weird and beautiful. If you want to know more about these sorts of mental states, I recommend José Luis Ricón Fernández de la Puente’s recent write-up of </span><a href="https://nintil.com/jhanas" rel="">his experiences</a><span>, Nadia Asparouhova on </span><a href="https://asteriskmag.com/issues/06/manufacturing-bliss" rel="">her experiences</a><span>, and her </span><a href="https://nadia.xyz/jhanas#jhanas-are-learned-by-doing-not-reading" rel="">how-to guide</a><span>.</span></p><p>Here is José, whose blog is normally detailed reflections on cell biology and longevity and metascience, describing the second evening of a jhana retreat:</p><blockquote><p>So I went down to the beach. “Kinda nice”, I thought. The sky had a particularly vibrant blue color, the waves had ‘the right size’, their roar was pleasant. I started to walk around trying to continue meditating. I focused my awareness on an arising sensation of open heartedness and then I noticed my eyes tearing up (“Huh? I thought”). I looked again at the ocean and then I saw it. It was fucking amazing. So much color and detail: waves within waves, the fractal structure of the foamy crests as they disintegrate back into the ocean. The feeling of the sun on my skin. I felt overwhelmed. As tears ran down my face and lowkey insane grin settled on my face I found myself mumbling “It’s... always been like this!!!!” “What the fuck??!” followed by “This is too much!! Too much!!!”. The experience seemed to be demanding from me to feel more joy and awe than I was born to feel or something like that. In that precise moment I felt what “painfully beautiful” means for the first time in my life.</p></blockquote><p><span>The fact that we can enter fundamentally different, and often exhilarating, states of mind by learning how to sustain our attention is fascinating. It makes you wonder what other states are waiting out there. What will happen if you properly pay attention to an octopus?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-3-169544261" target="_self" rel="">3</a></span><span> What about your sense of loneliness?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-4-169544261" target="_self" rel="">4</a></span><span> A mathematical idea?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-5-169544261" target="_self" rel="">5</a></span><span> The weights of a neural net?</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-169544261" href="https://www.henrikkarlsson.xyz/p/attention#footnote-6-169544261" target="_self" rel="">6</a></span><span> The footnotes here take you to examples of people who have done that. There are so many things to pay attention to and experience.</span></p><p>One of my favorite things to sustain attention toward is art.</p><p>There was a period in my twenties when I didn’t get art. I thought artists were trying to say something, but I felt superior because I thought there had to be better ways of getting their ideas across (and also, better ideas). But then I realized that good art—at least the art I am spontaneously drawn to—has little to do with communication. Instead, it is about crafting patterns of information that, if you feed them sustained attention, will begin to structure your attentional field in interesting ways. Art is guided meditation. The point isn’t the words, but what happens to your mind when you attend to those words (or images, or sounds). There is nothing there to understand; it is just something to experience, like sex. But the experiences can be very deep and, sometimes, transformative.</p><p>In 2019, for example, I saw a performance of Jean Sibelius’s 5th Symphony at the University Hall in Uppsala.</p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!xK2X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!xK2X!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!xK2X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg" width="1456" height="996" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:996,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Vault detail&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="Vault detail" title="Vault detail" srcset="https://substackcdn.com/image/fetch/$s_!xK2X!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xK2X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa9dd1a48-4d99-46bd-af59-943842790142_2560x1751.jpeg 1456w" sizes="100vw" loading="lazy"></picture></div></a></figure></div><p>Before the concert began, I spent a few minutes with my eyes closed, doing a body scan, to be fully present when the music began. As the horns at the opening of the piece called out, I decided to keep my eyes closed, so I wouldn’t be distracted by looking at the hands of the musicians. Then… a sort of daydream started up. The mood suggested to me the image of a cottage overlooking a sloping meadow and a thick wood of pines, a few hours from Helsinki. It was a pretty obvious image, since I knew that Sibelius wrote the piece at Aniola, which is 38 km north of Helsinki. But then I saw an old man walking up the meadow and into the house. The camera cut. Through an open door, I saw the man, alone, working at a desk. I saw it as clearly as if it had been projected on a screen before me: the camera moved slowly toward the back of the man.</p><p>Through the window above his desk, I could see a light in the distance. Perhaps it was Helsinki? No, it felt alive, like a being—something alive and growing, something that was headed here. But then again, if you were to see a city from space, watching it sped up by 100,000x, it would look like a being moving through the landscape, spreading, getting closer. The old man sat there for a hundred years, watching the light. There was a sinking feeling in my body.</p><p>One spring, birds fell dead from the sky. They littered the fields, whole droves of them filled the ditches—blue birds, red birds, and black. The man carried them into his woodshed and placed them in waist-high piles.</p><p>The film kept going, and the emotional intensity and complexity gradually ramped up. For the thirty minutes that it took the orchestra to play the three movements of the symphony, I experienced what felt like two or three feature films, all interconnected by some strange emotional logic. In the third movement, a group of hunter-gatherers was living in a cave that reminded me of the entrance to a nuclear waste facility. A girl hiding behind a tree saw men with cars arrive…</p><p>The structure of the music was such that it gave me enough predictability and enough surprise to allow my attention to deeply cohere. The melody lines and harmonies dredged up memories and images from my subconscious, weaving them into a rich cinematic web of stories. Guided by the music, my mind could tunnel into an attentional state where I was able to see things I had never seen before and where I could work through some deep emotional pain that seemed to resolve itself through the images.</p><p>When the music stopped, I barely knew where I was.</p><p>I opened my eyes and remembered that my brother was sitting next to me.</p><p>“What did you think?” I said.</p><p>“I don’t know,” he said. “I felt kind of restless.”</p><p><em>Like always, the research for this essay was funded by the contribution of paying subscribers. Thank you! We wouldn’t have been able to do this without you. If you enjoy the essays and want to support Escaping Flatland, we are not yet fully funded:</em></p><p><em>A special thanks to Johanna Karlsson, Nadia Asparouhova, Packy McCormick, and Esha Rana, who all read and commented on drafts of this essay. The image of the University Hall is by Ann-Sofi Cullhed.</em></p><p>If you liked this essay, you might also like:</p><div data-component-name="DigestPostEmbed"><a href="https://www.henrikkarlsson.xyz/p/perceptive" rel="noopener" target="_blank"><h2>Becoming perceptive</h2></a><div><a href="https://www.henrikkarlsson.xyz/p/perceptive" rel="noopener" target="_blank"><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!Ea2j!,w_424,h_212,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_848,h_424,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_1272,h_636,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_1300,h_650,c_fill,f_webp,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 1300w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!Ea2j!,w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg" sizes="100vw" alt="Becoming perceptive" srcset="https://substackcdn.com/image/fetch/$s_!Ea2j!,w_424,h_212,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_848,h_424,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_1272,h_636,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Ea2j!,w_1300,h_650,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F77a4ecad-b0d2-4ade-b586-15d17bfa05c9_1000x801.jpeg 1300w" width="1300" height="650"></picture></a></div><p>This is the second part of an essay series that began with “Everything that turned out well in my life followed the same design process.” There is also a third part. It can be read on its own.</p></div></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The Browser Company (Arc, Dia) Has Been Acquired by Atlassian (413 pts)]]></title>
            <link>https://www.atlassian.com/blog/announcements/atlassian-acquires-the-browser-company</link>
            <guid>45126358</guid>
            <pubDate>Thu, 04 Sep 2025 12:12:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlassian.com/blog/announcements/atlassian-acquires-the-browser-company">https://www.atlassian.com/blog/announcements/atlassian-acquires-the-browser-company</a>, See on <a href="https://news.ycombinator.com/item?id=45126358">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
				
					<p>Building the AI browser for knowledge workers</p>
					</div><div>
					
<p>Today, I’m excited to share an exciting step forward for Atlassian. We’ve entered into an agreement to acquire The Browser Company of New York, the team behind the incredible Dia and Arc browsers.</p>



<p>By combining The Browser Company’s passion for building browsers people love with Atlassian’s deep expertise on how the world’s best teams operate, we have the opportunity to transform how work gets done in the AI era.</p>



<figure><p>
<iframe title="Atlassian + The Browser Company: Building the AI Browser for Knowledge Workers" width="640" height="360" src="https://www.youtube.com/embed/qtNEmBUW5fs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
</p></figure>



<h2>A Browser for Doing, Not Just Browsing</h2>



<p>Today’s browsers weren’t built for work. They were built for browsing – reading the news, watching videos, looking up recipes. And sure, you may do some of those things in your browser during the workday, but most of those tabs represent a task that needs to get done. A meeting to schedule. A design to review. A work item to update in Jira. A memo to write. Before you know it, it’s hard to see through the forest of tabs.</p>



<figure><img decoding="async" width="2043" height="1237" src="https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload.png" alt="" srcset="https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload.png 2043w, https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload-300x182.png 300w, https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload-600x363.png 600w, https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload-768x465.png 768w, https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/tab-overload-1536x930.png 1536w" sizes="(max-width: 2043px) 100vw, 2043px"></figure>



<p>Here’s the rub: your current browser isn’t designed to help you move any of that work forward. It was designed before the explosion of SaaS apps, and well before the current AI revolution. It’s a bystander in your workflow, treating every tab the same, with no awareness of your work context, no understanding of your priorities, and no help connecting the dots between your tools.</p>



<p>It’s time for a browser that’s actually built for work – a browser that helps you do, not just browse.</p>



<h2>Making Dia the Knowledge Worker’s Browser</h2>



<p>Knowledge workers need a browser designed for their specific needs, not one that’s been built for everyone on the planet. That’s what we will build with The Browser Company. Our vision is to make Dia the browser:</p>



<ul>
<li><strong>Optimized for the SaaS apps where you spend your day.</strong> Whether you’re working in email or a project management tool or a design app, your tabs will be enriched with context that helps move your work forward.</li>
</ul>



<figure><img loading="lazy" decoding="async" width="573" height="450" src="https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/projectdoer_productdemos.gif" alt=""></figure>



<ul>
<li><strong>Packed with AI skills</strong> and your personal work memory to connect the dots between your apps, tabs, and tasks.</li>
</ul>



<figure><img loading="lazy" decoding="async" width="1152" height="648" src="https://atlassianblog.wpengine.com/wp-content/uploads/2025/09/dia-search.gif" alt=""></figure>



<ul>
<li><strong>Built with trust and security in mind</strong>, so you can bring it to the office. While <a href="https://www.inforisktoday.in/whitepapers/gartner-report-future-enterprise-browsers-w-15316">85% of enterprise workflows</a> occur within web browsers, <a href="https://www.gartner.com/en/newsroom/press-releases/2025-04-29-gartner-predicts-25-percent-of-organizations-will-use-secure-enterprise-browsers-to-enhance-remote-access-and-endpoint-security-by-2028">less than 10%</a> of organizations have adopted a secure browser. Security, compliance, and admin controls will be baked into every aspect of Dia.</li>
</ul>



<h2>Looking Ahead</h2>



<p>We are set to create a browser knowledge workers will love.</p>



<p>Together, we’re sprinting toward this opportunity, leveraging each other’s strengths. I am stoked for the road ahead and can’t wait to see how we will extend Atlassian’s mission – to unleash the potential of every team – to the browser.</p>



<h6><strong>Forward-Looking Statements</strong></h6>



<h6><strong>This blog contains forward-looking statements that are subject to risks and uncertainties that could cause actual events to differ materially from those referred to in these forward-looking statements, including but not limited to: Atlassian’s ability to successfully integrate the business, technology, product, personnel and operations of The Browser Company, and to achieve the expected benefits of the acquisition; the ability of Atlassian to extend its leadership in the team collaboration and productivity software space, or to develop and commercialize browser software; the potential benefits of the transaction to Atlassian and The Browser Company customers; anticipated new features and solutions that will become available; the ability of Atlassian and The Browser Company to close the announced transaction and the expected timing of the closing of the transaction; the ability to integrate Atlassian’s and The Browser Company’s technology, including in AI and security investments; the financial statement impact of the transaction on Atlassian, including any impact on its share repurchase strategy; risks related to any statements of expectation or belief; and risks related to any statements of assumptions underlying any of the foregoing. Information on risks that could affect the expected results of the transaction is included in Atlassian’s filings made with the Securities and Exchange Commission from time to time, including the section titled “Risk Factors” in its most recently filed Forms 10-K and 10-Q, as well as those that may be updated in its future filings with the SEC.</strong></h6>




				</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Le Chat. Custom MCP Connectors. Memories (370 pts)]]></title>
            <link>https://mistral.ai/news/le-chat-mcp-connectors-memories</link>
            <guid>45125859</guid>
            <pubDate>Thu, 04 Sep 2025 11:04:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://mistral.ai/news/le-chat-mcp-connectors-memories">https://mistral.ai/news/le-chat-mcp-connectors-memories</a>, See on <a href="https://news.ycombinator.com/item?id=45125859">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2 dir="ltr">Today, we’re giving you more reasons to switch to Le Chat.</h2>
<ol>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><strong>The widest enterprise-ready <a href="https://chat.mistral.ai/connections" target="_blank" rel="noopener">connector directory (beta)</a>, with custom extensibility, making it easy to bring workflows into your AI assistant.</strong></p>
<ul>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Directory of 20+ secure connectors—spanning data, productivity, development, automation, commerce, and custom integrations. Search, summarize, and act in tools like Databricks, Snowflake, GitHub, Atlassian, Asana, Outlook, Box, Stripe, Zapier, and more.</p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Custom extensibility: Add your own MCP connectors to broaden coverage and drive more precise actions and insights.</p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Flexible deployment: run on mobile, in your browser, or deploy on-premises or in your cloud.</p>
</li>
</ul>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><strong>Context that carries: introducing <a href="https://chat.mistral.ai/memories" target="_blank" rel="noopener">Memories (beta).</a></strong></p>
<ul>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Highly-personalized responses based on your preferences and facts.</p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Careful and reliable memory handling: saves what matters, slips sensitive or fleeting info.</p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">Complete control over what to store, edit, or delete.</p>
</li>
<li dir="ltr" aria-level="2">
<p dir="ltr" role="presentation">And… fast import of your memories from ChatGPT.</p>
</li>
</ul>
</li>
<li dir="ltr" aria-level="1">
<p dir="ltr" role="presentation"><strong>Everything available on the Free plan.</strong></p>
</li>
</ol>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/tKUnOrl-OTw?si=oieNA4c9qiHmw2M4" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h2 dir="ltr">Plug it right in.</h2>
<p dir="ltr">Today, we’re releasing 20+ secure, MCP-powered connectors in Le Chat, enabling you to search, summarize, and take actions with your business-critical tools. Le Chat’s connector directory spans essential categories, simplifying how you integrate your workflows in chats.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/92e9c189-28ba-4d83-b8c9-058850e1ad30.png?width=1920&amp;height=1920" alt="Logo Cloud 1 1"></p>
<p dir="ltr">The new-look <a href="https://chat.mistral.ai/connections" target="_blank" rel="noopener">Connectors directory</a> opens direct pipelines into enterprise tools, turning Le Chat into a single surface for data, documents, and actions.&nbsp;</p>
<ul>
<li dir="ltr"><strong>Data</strong>: Search and analyze datasets in Databricks (coming soon), Snowflake (coming soon), Pinecone, Prisma Postgres, and DeepWiki.</li>
<li dir="ltr"><strong>Productivity</strong>: Collaborate on team docs in Box and Notion, spin up project boards in Asana or Monday.com, and triage across Atlassian tools like Jira and Confluence.</li>
<li dir="ltr"><strong>Development</strong>: Manage issues, pull requests, repositories, and code analysis in GitHub; create tasks in Linear, monitor errors in Sentry, and integrate with Cloudflare Development Platform.</li>
<li dir="ltr"><strong>Automation</strong>: Extend workflows through Zapier and campaigns in Brevo.</li>
<li dir="ltr"><strong>Commerce</strong>: Access and act on merchant and payment data from PayPal, Plaid, Square, and Stripe.</li>
<li dir="ltr"><strong>Custom</strong>: Add your own MCP connectors to extend coverage, so you can query, get summaries, and act on the systems and workflows unique to your business.</li>
<li dir="ltr"><strong>Deployment</strong>: Run on-prem, in your cloud, or on Mistral Cloud, giving you full control over where your data and workflows live.</li>
</ul>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/2Xd3-ZX8Bsk?si=SsvHnmNaEH9zDcK-" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<h3 dir="ltr">Connectors in action.</h3>
<div>
<div>
<p><img src="https://cms.mistral.ai/assets/91cf7ae4-1e23-4557-ba81-11a9f1153f45.svg" alt="Databrick Asana"></p>
</div>
<h4>Databricks and Asana</h4>
<p>Summarizing customer reviews in Databricks, then raising a ticket in Asana to address the top issues.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/NAAwv-D-4cA?si=PijEwVINQU0FdQMf" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div>
<div>
<div>
<p><img src="https://cms.mistral.ai/assets/231fb518-99d4-4f85-84bd-0fba406c78f9.svg?width=null&amp;height=null" alt="GitHub Notion"></p>
</div>
<h4>GitHub and Notion</h4>
<p>Reviewing open pull requests in GitHub, then creating Jira issues for follow-up and documenting the changes in Notion.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/vAEqEYIp23s?si=kjh0A3WHCP1NDlRB" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div>
<div>
<p><img src="https://cms.mistral.ai/assets/4a059f07-6386-4859-9cc1-7f7dcc9788d5.svg?width=null&amp;height=null" alt="Box"></p>
<h4>Box</h4>
<p>Comparing financial obligations across legal documents in Box, then uploading a concise summary back into Box.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/MIM6Uk7A1sw?si=zPEYtDCWm67FPork" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div>
<div>
<p><img src="https://cms.mistral.ai/assets/a61455d1-664c-4ef8-9e89-9552df4191b2.svg?width=null&amp;height=null" alt="Atlassian"></p>
<h4>Confluence and Jira</h4>
<p>Summarizing active issues from Jira, then drafting a Confluence sprint overview page for team planning.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/6M-t0OiujnM?si=daMTg8P_KwrbmuKq" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div>
<div>
<p><img src="https://cms.mistral.ai/assets/03bc7129-9871-432a-9bed-02209a47af32.svg?width=null&amp;height=null" alt="Stripe Linear"></p>
<h4>Stripe and Linear</h4>
<p>Retrieving business payment insights from Stripe, then logging anomalies as a development project and task in Linear.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/qJ381CswnKw?si=BU-xbsOACKMFaOcy" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
</div>
<p>Learn more about Connectors in our <a href="https://help.mistral.ai/en/collections/911943-connectors" target="_blank" rel="noopener">Help Center</a>.</p>
<h3 dir="ltr">Connect any MCP server.</h3>
<p dir="ltr">For everything else, you can now <a href="https://help.mistral.ai/en/articles/393572-configuring-a-custom-connector" target="_blank" rel="noopener">connect to any remote MCP server</a> of choice—even if it’s not listed in the <a href="https://chat.mistral.ai/connections" target="_blank" rel="noopener">Connectors directory</a>—to query, cross-reference, and perform actions on any tool in your stack.</p>
<h3 dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/bZR2ctE2LyQ?si=vMFuFgS8LqKguwH5" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></h3>
<h3 dir="ltr">Your rules. Your control.</h3>
<p dir="ltr">Admin users can confidently control which connectors are available to whom in their organization, with on-behalf authentication, ensuring users only access data they’re permitted to.</p>
<p dir="ltr">Deploy Le Chat your way—self-hosted, in your private or public cloud, or as a fully managed service in the Mistral Cloud.&nbsp;<a href="https://mistral.ai/contact">Talk to our team</a> about enterprise deployments.</p>
<h2 dir="ltr">Hold that thought.</h2>
<p dir="ltr">Memories in Le Chat carry your context across conversations, retrieving insights, decisions, and references from the past when needed. They power more relevant responses, adaptive recommendations tailored for you, and richer answers infused with the specifics of your work—delivering a faster, more relevant, and fully personalized experience.</p>
<p dir="ltr"><iframe title="YouTube video player" src="https://www.youtube.com/embed/mNfre6VhHj8?si=FSQptHhZehGe90NC" width="560" height="315" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p>
<p dir="ltr">Memories score high in our evaluations for accuracy and reliability: saving what’s important, avoiding forbidden or sensitive inferences, ignoring ephemeral content, and retrieving the right information without hallucinations.</p>
<p dir="ltr"><img src="https://cms.mistral.ai/assets/a0e581b9-125b-4ec7-b03a-9fea8eacbd63.png?width=1600&amp;height=1200" alt="Grid Lechat"></p>
<p dir="ltr">Most importantly, you stay in full control—add, edit, update, or remove any entry at any time, with <a href="https://help.mistral.ai/en/articles/396497-how-do-you-handle-my-data-when-using-the-memories-feature" target="_blank" rel="noopener">clear privacy settings</a> and selective memory handling you can trust.</p>
<h2 dir="ltr">Get started in Le Chat.</h2>
<p dir="ltr">Both Connectors and Memories are available to all Le Chat users.</p>
<p dir="ltr">Try out the new features at&nbsp;<a href="http://chat.mistral.ai/">chat.mistral.ai</a>, or by downloading the Le Chat mobile by Mistral AI app from the <a href="https://apps.apple.com/fr/app/le-chat-by-mistral-ai/id6740410176" target="_blank" rel="noopener">App Store</a> or <a href="https://play.google.com/store/apps/details?id=ai.mistral.chat&amp;hl=fr" target="_blank" rel="noopener">Google Play Store</a>, for free; no credit card needed.</p>
<p dir="ltr"><a href="https://mistral.ai/contact">Reach out to us</a> to learn how Le Chat Enterprise can transform your mission-critical work.&nbsp;</p>
<h2 dir="ltr">See you at our MCP webinar and hackathon?</h2>
<div><p><span> Webinar</span></p><h3 dir="ltr">Getting Started with MCP in Le Chat, September 9, Online.</h3>
<p dir="ltr">Join our webinar on September 9 to dive into Le Chat’s new MCP capabilities with the Mistral team. Learn key insights, ask your questions, and prepare to build cutting-edge projects—all before the hackathon begins.</p>
<p><a dir="ltr" href="https://www.linkedin.com/events/7368644665111158785/" target="_blank" rel="noopener"> Sign up now. </a></p></div>
<div><p><span>Hackathon</span></p><h3 dir="ltr">Mistral AI MCP Hackathon, September 13-14, Paris.</h3>
<p dir="ltr">Gather with the best AI engineers for a 2-day overnight hackathon (Sep. 13-14) and turn ideas into reality using your custom MCPs in Le Chat. Network with peers, get hands-on guidance from Mistral experts, and push the boundaries of what’s possible.</p>
</div>
<div>
<h2 dir="ltr">We’re hiring!</h2>
<p dir="ltr">If you’re interested in joining us on our mission to build world-class AI products, we welcome your application to <a href="https://mistral.ai/careers" target="_blank" rel="noopener">join our team</a>!</p>
</div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Google was down in eastern EU and Turkey (111 pts)]]></title>
            <link>https://www.novinite.com/articles/234225/Google+Down+in+Eastern+Europe+%28UPDATED%29</link>
            <guid>45124955</guid>
            <pubDate>Thu, 04 Sep 2025 08:23:35 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.novinite.com/articles/234225/Google+Down+in+Eastern+Europe+%28UPDATED%29">https://www.novinite.com/articles/234225/Google+Down+in+Eastern+Europe+%28UPDATED%29</a>, See on <a href="https://news.ycombinator.com/item?id=45124955">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="textsize">
            <p>Users across multiple Eastern European countries&nbsp;reported a significant and ongoing outage affecting a suite of <b>Google</b> services, causing widespread disruption to both work and daily life.</p>
<blockquote>
<p dir="ltr" lang="en"><a href="https://twitter.com/hashtag/BREAKING?src=hash&amp;ref_src=twsrc%5Etfw">#BREAKING</a> <b>Google</b> services <b>down</b> in some countries, primarily felt across Southeastern <b>Europe</b> <a href="https://t.co/cMYRYPHFi8">pic.twitter.com/cMYRYPHFi8</a></p>
— Anadolu English (@anadoluagency) <a href="https://twitter.com/anadoluagency/status/1963505253518581854?ref_src=twsrc%5Etfw">September 4, 2025</a></blockquote>

<p>Reports began flooding into downdetector.com and social media platforms around from users in Bulgaria, Turkey, Greece and other Eastern European countries. The issues appear to be widespread and are affecting core <b>Google</b> products.</p>
<blockquote>
<p dir="ltr" lang="en"><b>Google</b> <b>down</b> in the Caucasus, Turkey, and the Balkans <a href="https://t.co/E2SHOuKOA9">pic.twitter.com/E2SHOuKOA9</a></p>
— Hov Nazaretyan (@HovhanNaz) <a href="https://twitter.com/HovhanNaz/status/1963505155363512687?ref_src=twsrc%5Etfw">September 4, 2025</a></blockquote>
<p><strong>Which Services Were Impacted?</strong></p>
<p>The outage did not seem to be universal for all <b>Google</b> services, but the affected apps were critical to many:</p>
<ul>
<li>YouTube: Users experienced an inability to load videos, with many seeing error messages or an endless loading loop. Both the website and mobile app were affected.</li>
<li><b>Google</b> Maps: The service was failing to load map data, search for locations, or calculate routes, leaving travellers and commuters without navigation assistance.</li>
<li><b>Google</b> Search: In a particularly impactful failure, the core <b>Google</b> Search engine was returning error messages or failing to complete searches for a significant number of users.</li>
<li>Gmail: Some users were reporting issues with sending and receiving emails, though this appeared to be less consistent than the other outages.</li>
<li><b>Google</b> Drive: Access to cloud-stored documents and files was also disrupted for many.</li>
</ul>
<p>The common thread among error messages was a "<em>5xx server error</em>" – a type of error that indicates a problem on <b>Google</b>'s end, not with the user's individual internet connection.</p>
<p><img src="https://www.novinite.com/media/photos_more/202509/path_5624.jpg" alt=""></p>
<p>Alternatively, users can use other search engines such as&nbsp;<span><a href="https://www.bing.com/hp" target="_blank">Bing</a>, <a href="https://search.yahoo.com/" target="_blank">Yahoo</a>, <a href="https://duckduckgo.com/" target="_blank">DuckDuckGo</a>, and <a href="https://search.brave.com/" target="_blank">Brave Search</a>.</span></p>
<p><em><strong>This is a developing story. We will update this article with more information as it becomes available, including an official response from Google.</strong></em></p>
            
        </div><div id="more_from_category">
			<p><a href="https://www.novinite.com/articles/234234/New-Generation+Tanks%2C+Fighting+Vehicles+Amaze+at+China%27s+V-Day+Parade" title="Bulgaria: New-Generation Tanks, Fighting Vehicles Amaze at China's V-Day Parade"><img src="https://www.novinite.com/media/images/2025-09/photo_small_234234.jpg"></a></p><div>
				<h2><a href="https://www.novinite.com/articles/234234/New-Generation+Tanks%2C+Fighting+Vehicles+Amaze+at+China%27s+V-Day+Parade" title="Bulgaria: New-Generation Tanks, Fighting Vehicles Amaze at China's V-Day Parade">New-Generation Tanks, Fighting Vehicles Amaze at China's V-Day Parade</a></h2>
				<p>China's newly developed tanks and military vehicles were among the first displays of armaments to march past Tian'anmen Square for inspection on Wednesday, showcasing major advancements over previous-generation vehicles in terms of informatization and bat</p>
				<p><a href="https://www.novinite.com/articles/category/30/World" title="World News">World</a> <span>|</span>   September 4, 2025, Thursday // 15:06</p>
			</div>
				
		</div></div>]]></description>
        </item>
    </channel>
</rss>