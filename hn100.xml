<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 11 Nov 2025 16:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[OpenAI may not use lyrics without license, German court rules (141 pts)]]></title>
            <link>https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/</link>
            <guid>45886131</guid>
            <pubDate>Tue, 11 Nov 2025 11:20:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/">https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/</a>, See on <a href="https://news.ycombinator.com/item?id=45886131">Hacker News</a></p>
Couldn't get https://www.reuters.com/world/german-court-sides-with-plaintiff-copyright-case-against-openai-2025-11-11/: Error: Request failed with status code 401]]></description>
        </item>
        <item>
            <title><![CDATA[iPhone Pocket (152 pts)]]></title>
            <link>https://www.apple.com/newsroom/2025/11/introducing-iphone-pocket-a-beautiful-way-to-wear-and-carry-iphone/</link>
            <guid>45885813</guid>
            <pubDate>Tue, 11 Nov 2025 10:17:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.apple.com/newsroom/2025/11/introducing-iphone-pocket-a-beautiful-way-to-wear-and-carry-iphone/">https://www.apple.com/newsroom/2025/11/introducing-iphone-pocket-a-beautiful-way-to-wear-and-carry-iphone/</a>, See on <a href="https://news.ycombinator.com/item?id=45885813">Hacker News</a></p>
<div id="readability-page-1" class="page">


	
    







 
<nav id="ac-localnav" lang="en-US" role="navigation" aria-label="Newsroom" data-analytics-region="local nav" data-sticky="">
	
    
    
        




    
    
    
	
	

</nav>





<main id="main" role="main"> 




<span id="opens-in-new-window">opens in new window</span>
<section>
<article data-analytics-activitymap-region-id="article">






    
    
    









    





    <div>
        
		
        

        <div>
                
                
                
                    <h2>
                        
    
        Introducing iPhone Pocket: a&nbsp;beautiful way to wear and carry iPhone
    

                    </h2>
                
            </div>

        <div>
                
                
                    Born out of a collaboration between ISSEY&nbsp;MIYAKE and Apple, iPhone&nbsp;Pocket features a singular 3D-knitted construction designed to fit any iPhone
                
            </div>

        
            
    
    
    
    
    

        

    </div>







    
    
    






  
    
    
    
    
      <figure aria-label="Media, Two users pose with iPhone Pocket in lemon and black.">
        <div>
             
              
              <div>
                iPhone Pocket, born out of a collaboration between ISSEY MIYAKE and Apple, will be available at select Apple Store locations beginning Friday, November 14.
              </div>
            
            
            
            
            <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-hero.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-hero_big" aria-label="Download media, Two users pose with iPhone Pocket in lemon and black."></a>
          </div>
      </figure>
    
  








    
    
    


     
     
    
    
        <div>
             
                 <div>ISSEY MIYAKE and Apple today unveiled <a href="https://www.apple.com/shop/product/HS8R2ZM/A" target="_blank">iPhone Pocket</a>. Inspired by the concept of “a piece of cloth,” its singular 3D-knitted construction is designed to fit any iPhone as well as all pocketable items. Beginning Friday, November 14, it will be available at select Apple Store locations and on <a href="https://www.apple.com/" target="_blank">apple.com</a> in France, Greater China, Italy, Japan, Singapore, South Korea, the UK, and the U.S.
</div>
                 
             
                 <div>iPhone Pocket features a ribbed open structure with the qualities of the original pleats by ISSEY MIYAKE. Born from the idea of creating an additional pocket, its understated design fully encloses iPhone, expanding to fit more of a user’s everyday items. When stretched, the open textile subtly reveals its contents and allows users to peek at their iPhone display. iPhone Pocket can be worn in a variety of ways — handheld, tied onto bags, or worn directly on the body. Featuring a playful color palette, the short strap design is available in eight colors, and the long strap design in three colors.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-pocket-color-options">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-500f573744cb93b53ae8377ed0858f3f" href="#gallery-500f573744cb93b53ae8377ed0858f3f" data-ac-gallery-trigger="gallery-500f573744cb93b53ae8377ed0858f3f"><span>All eight colors of iPhone Pocket short strap design.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-c4b99bd83aa685e677f8cc92bd31c905" href="#gallery-c4b99bd83aa685e677f8cc92bd31c905" data-ac-gallery-trigger="gallery-c4b99bd83aa685e677f8cc92bd31c905"><span>All three colors of iPhone Pocket long strap design.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-500f573744cb93b53ae8377ed0858f3f" aria-labelledby="gallery-dotnav-500f573744cb93b53ae8377ed0858f3f" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:short-strap-design">
                                
                                <div>
                                    <div>Featuring a playful color palette, the short strap design is available in eight colors: lemon, mandarin, purple, pink, peacock, sapphire, cinnamon, and black.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-short-strap-colors.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-short-strap-colors_big" aria-label="Download media, All eight colors of iPhone Pocket short strap design."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-c4b99bd83aa685e677f8cc92bd31c905" aria-labelledby="gallery-dotnav-c4b99bd83aa685e677f8cc92bd31c905" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:long-strap-design">
                                
                                <div>
                                    <div>The long strap design is available in three colors: sapphire, cinnamon, and black.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-long-strap-colors.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-long-strap-colors_big" aria-label="Download media, All three colors of iPhone Pocket long strap design."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <div>“The design of iPhone Pocket speaks to the bond between iPhone and its user, while keeping in mind that an Apple product is designed to be universal in aesthetic and versatile in use,” shared Yoshiyuki Miyamae, design director of MIYAKE DESIGN STUDIO. “iPhone Pocket explores the concept of ‘the joy of wearing iPhone in your own way.’ The simplicity of its design echoes what we practice at ISSEY MIYAKE — the idea of leaving things less defined to allow for possibilities and personal interpretation.”
</div>
                 
             
                 <div>“Apple and ISSEY MIYAKE share a design approach that celebrates craftsmanship, simplicity, and delight,” said Molly Anderson, Apple’s vice president of Industrial Design. “This clever extra pocket exemplifies those ideas and is a natural accompaniment to our products. The color palette of iPhone Pocket was intentionally designed to mix and match with all our iPhone models and colors — allowing users to create their own personalized combination. Its recognizable silhouette offers a beautiful new way to carry your iPhone, AirPods, and favorite everyday items.”
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="iphone-pocket-color-combos">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-0413a35321ffe4dbd41592bfeb2b2797" href="#gallery-0413a35321ffe4dbd41592bfeb2b2797" data-ac-gallery-trigger="gallery-0413a35321ffe4dbd41592bfeb2b2797"><span>iPhone Pocket in cinnamon paired with iPhone 17 Pro in cosmic orange.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-395d51cadfd7a3c84bd5bd1de138f580" href="#gallery-395d51cadfd7a3c84bd5bd1de138f580" data-ac-gallery-trigger="gallery-395d51cadfd7a3c84bd5bd1de138f580"><span>iPhone Pocket in sapphire paired with iPhone Air in sky blue.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-d57c53d12e0866bde9b77fabca8692fb" href="#gallery-d57c53d12e0866bde9b77fabca8692fb" data-ac-gallery-trigger="gallery-d57c53d12e0866bde9b77fabca8692fb"><span>iPhone Pocket in purple paired with iPhone 17 in lavender.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-0413a35321ffe4dbd41592bfeb2b2797" aria-labelledby="gallery-dotnav-0413a35321ffe4dbd41592bfeb2b2797" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:cinnamon-and-cosmic-orange-iphone-17-pro">
                                
                                <div>
                                    <div>Users can create their own personalized color combinations with iPhone Pocket and iPhone.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-cinnamon-with-iPhone-17-Pro.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-cinnamon-with-iPhone-17-Pro_big" aria-label="Download media, iPhone Pocket in cinnamon paired with iPhone 17 Pro in cosmic orange."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-395d51cadfd7a3c84bd5bd1de138f580" aria-labelledby="gallery-dotnav-395d51cadfd7a3c84bd5bd1de138f580" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:sapphire-and-sky-blue-iphone-air">
                                
                                <div>
                                    <div>Users can create their own personalized color combinations with iPhone Pocket and iPhone.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-sapphire-with-iPhone-Air.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-sapphire-with-iPhone-Air_big" aria-label="Download media, iPhone Pocket in sapphire paired with iPhone Air in sky blue."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-d57c53d12e0866bde9b77fabca8692fb" aria-labelledby="gallery-dotnav-d57c53d12e0866bde9b77fabca8692fb" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:purple-and-lavender-iphone-17">
                                
                                <div>
                                    <div>Users can create their own personalized color combinations with iPhone Pocket and iPhone.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-purple-with-iPhone-17-01.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-purple-with-iPhone-17-01_big" aria-label="Download media, iPhone Pocket in purple paired with iPhone 17 in lavender."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>A Piece of Cloth</strong>
</h2>
                 
             
                 <div>Crafted in Japan, iPhone Pocket features a singular 3D-knitted construction that is the result of research and development carried out at ISSEY MIYAKE. The design drew inspiration from the concept of “a piece of cloth” and reinterpreted the everyday utility of the brand’s iconic pleated clothing. The development and design of iPhone Pocket unfolded in close collaboration with the Apple Design Studio, which provided insight into design and production throughout.
</div>
                 
             
         </div>
 

    
    
    


    
    
        
        
        
            <div role="group" aria-label="gallery" data-component-list="MixinGallery" id="singular-construction">
            <nav role="presentation">
                <ul role="tablist">
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-8c54c3d4fde598029a7c6b36d27e85a3" href="#gallery-8c54c3d4fde598029a7c6b36d27e85a3" data-ac-gallery-trigger="gallery-8c54c3d4fde598029a7c6b36d27e85a3"><span>A user poses with iPhone Pocket in peacock.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-8ec946387230a363dbe25e38306bce4d" href="#gallery-8ec946387230a363dbe25e38306bce4d" data-ac-gallery-trigger="gallery-8ec946387230a363dbe25e38306bce4d"><span>A user poses with iPhone Pocket in cinnamon.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-1b5cfe3976c01fb7746de0602809af8b" href="#gallery-1b5cfe3976c01fb7746de0602809af8b" data-ac-gallery-trigger="gallery-1b5cfe3976c01fb7746de0602809af8b"><span>iPhone Pocket in pink paired with a black ISSEY MIYAKE handbag.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-2fe4cea23e1e990c69431e909557f742" href="#gallery-2fe4cea23e1e990c69431e909557f742" data-ac-gallery-trigger="gallery-2fe4cea23e1e990c69431e909557f742"><span>iPhone Pocket in black.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-8f771a17f3ddef3f19c8c979d8ed6291" href="#gallery-8f771a17f3ddef3f19c8c979d8ed6291" data-ac-gallery-trigger="gallery-8f771a17f3ddef3f19c8c979d8ed6291"><span>iPhone Pocket in lemon and mandarin.</span></a>
                        </li>
                    
                        <li role="presentation">
                            <a id="gallery-dotnav-56d430360d60f74d93b475a57eb36ebd" href="#gallery-56d430360d60f74d93b475a57eb36ebd" data-ac-gallery-trigger="gallery-56d430360d60f74d93b475a57eb36ebd"><span>iPhone Pocket in purple.</span></a>
                        </li>
                    
                </ul>
            </nav>
            <div>
                
                    
                        
                        <div id="gallery-8c54c3d4fde598029a7c6b36d27e85a3" aria-labelledby="gallery-dotnav-8c54c3d4fde598029a7c6b36d27e85a3" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:peacock">
                                
                                <div>
                                    <div>iPhone Pocket features a singular 3D-knitted construction that is the result of research and development carried out at ISSEY MIYAKE.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-peacock.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-peacock_inline" aria-label="Download media, A user poses with iPhone Pocket in peacock."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-8ec946387230a363dbe25e38306bce4d" aria-labelledby="gallery-dotnav-8ec946387230a363dbe25e38306bce4d" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:cinnamon">
                                
                                <div>
                                    <div>iPhone Pocket features a singular 3D-knitted construction that is the result of research and development carried out at ISSEY MIYAKE.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-cinnamon.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-cinnamon_inline" aria-label="Download media, A user poses with iPhone Pocket in cinnamon."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-1b5cfe3976c01fb7746de0602809af8b" aria-labelledby="gallery-dotnav-1b5cfe3976c01fb7746de0602809af8b" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:pink-and-black-issey-miyake-bag">
                                
                                <div>
                                    <div>Users can create their own personalized color combinations with iPhone Pocket and iPhone.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-pink-with-BAO-BAO-bag.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-pink-with-BAO-BAO-bag_inline" aria-label="Download media, iPhone Pocket in pink paired with a black ISSEY MIYAKE handbag."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-2fe4cea23e1e990c69431e909557f742" aria-labelledby="gallery-dotnav-2fe4cea23e1e990c69431e909557f742" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:black">
                                
                                <div>
                                    <div>The development and design of iPhone Pocket unfolded in close collaboration with the Apple Design Studio, which provided insight into design and production throughout.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-black-with-iPhone-17-Pro.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-black-with-iPhone-17-Pro_inline" aria-label="Download media, iPhone Pocket in black."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-8f771a17f3ddef3f19c8c979d8ed6291" aria-labelledby="gallery-dotnav-8f771a17f3ddef3f19c8c979d8ed6291" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:lemon-and-mandarin">
                                
                                <div>
                                    <div>The development and design of iPhone Pocket unfolded in close collaboration with the Apple Design Studio, which provided insight into design and production throughout.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-lemon-and-mandarin-with-iPhone-17-and-iPhone-Air.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-lemon-and-mandarin-with-iPhone-17-and-iPhone-Air_inline" aria-label="Download media, iPhone Pocket in lemon and mandarin."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
                    
                        
                        <div id="gallery-56d430360d60f74d93b475a57eb36ebd" aria-labelledby="gallery-dotnav-56d430360d60f74d93b475a57eb36ebd" data-gallery-item="">
                            <figure role="presentation" data-analytics-activitymap-region-id="Gallery:purple">
                                
                                <div>
                                    <div>The development and design of iPhone Pocket unfolded in close collaboration with the Apple Design Studio, which provided insight into design and production throughout.</div>
                                    
                                    
                                    <a href="https://www.apple.com/newsroom/images/2025/11/introducing-iphone-pocket-a-beautiful-and-wearable-carrier-for-iphone/article/Apple-iPhone-Pocket-and-ISSEY-MIYAKE-purple-with-iPhone-17-02.zip" download="" data-analytics-title="download image - Apple-iPhone-Pocket-and-ISSEY-MIYAKE-purple-with-iPhone-17-02_inline" aria-label="Download media, iPhone Pocket in purple."></a>
                                </div>
                            </figure>
                        </div>
                        
                    
                
            </div>
            
                
                
                <nav role="presentation">
                    
                        <ul>
                            <li>
                                
                            </li>
                            <li>
                                
                            </li>
                        </ul>
                    
                </nav>
            
        </div>
    


    
    
    


     
     
    
    
        <div>
             
                 <h2><strong>Availability</strong>
</h2>
                 
             
                 <div>iPhone Pocket is a limited-edition release. The short strap design is available in lemon, mandarin, purple, pink, peacock, sapphire, cinnamon, and black; the long strap design is available in sapphire, cinnamon, and black. iPhone Pocket in the short strap design retails at $149.95 (U.S.), and the long strap design at $229.95 (U.S.).
</div>
                 
             
                 <div>Customers can purchase iPhone Pocket beginning Friday, November 14, at select Apple Store locations and <a href="https://www.apple.com/" target="_blank">apple.com</a> in France, Greater China, Italy, Japan, Singapore, South Korea, the UK, and the U.S. Just in time for the holidays, Apple Specialists in stores and online can help customers mix and match different lengths and colors with their iPhone, style iPhone Pocket, and purchase their new favorite accessory.
</div>
                 
             
                 <div><ul>
<li>Apple Canton Road, Hong Kong</li>
<li>Apple Ginza, Tokyo</li>
<li>Apple Jing’an, Shanghai</li>
<li>Apple Marché Saint-Germain, Paris</li>
<li>Apple Myeongdong, Seoul</li>
<li>Apple Orchard Road, Singapore</li>
<li>Apple Piazza Liberty, Milan</li>
<li>Apple Regent Street, London</li>
<li>Apple SoHo, New York City</li>
<li>Apple Xinyi A13, Taipei</li>
</ul>
</div>
                 
             
         </div>
 

    
    
    




    
    
        
    


    
    
    



    
    
    




    




    
    
    






    















	

		
		
			
























		
		

</article>



</section>
</main>



<div>
            Stay up to date with the latest articles from Apple Newsroom.
        </div>
	

</div>]]></description>
        </item>
        <item>
            <title><![CDATA[SoftBank sells its entire stake in Nvidia for $5.83B (181 pts)]]></title>
            <link>https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html</link>
            <guid>45884937</guid>
            <pubDate>Tue, 11 Nov 2025 07:32:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html">https://www.cnbc.com/2025/11/11/softbank-sells-its-entire-stake-in-nvidia-for-5point83-billion.html</a>, See on <a href="https://news.ycombinator.com/item?id=45884937">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="RegularArticle-ArticleBody-5" data-module="ArticleBody" data-test="articleBody-2" data-analytics="RegularArticle-articleBody-5-2"><div id="ArticleBody-InlineImage-108065851" data-test="InlineImage"><p>Nvidia CEO Jensen Huang (L) and the CEO of the SoftBank Group Masayoshi Son pose during an AI event in Tokyo on November 13, 2024.</p><p>Akio Kon | Bloomberg | Getty Images</p></div><div><p><a id="107312506" href="https://www.cnbc.com/quotes/" type="security" brand="cnbc" section="[object Object]" contentclassification="">SoftBank</a> said Tuesday it has sold its entire stake in U.S. chipmaker <span data-test="QuoteInBody" id="RegularArticle-QuoteInBody-2"><a href="https://www.cnbc.com/quotes/NVDA/">Nvidia</a><span><span id="-WatchlistDropdown" data-analytics-id="-WatchlistDropdown"></span></span></span> for $5.83 billion as the Japanese giant looks to capitalize on its <a href="https://www.cnbc.com/2025/06/27/softbank-ceo-says-he-wanted-to-be-openai-early-investor.html">"all in"</a> bet on ChatGPT maker OpenAI. </p><p>The firm said in its earnings statement that it sold 32.1 million Nvidia shares in October. It also disclosed that it sold part of its T-Mobile stake for $9.17 billion.</p><p>"We want to provide a lot of investment opportunities for investors, while we can still maintain financial strength," said SoftBank's Chief Financial Officer Yoshimitsu Goto during an investor presentation. </p><p>"So through those options and tools we make sure that we are ready for funding in a very safe manner," he said in comments translated by the company, adding that the stake sales were part of the firm's strategy for "asset monetization."</p><p>Nvidia shares dipped 0.95% in premarket trade on Tuesday.</p><p>While the Nvidia exit may come as a surprise to some investors, it's not the first time SoftBank has cashed out of the American AI chip darling.</p><p>SoftBank's Vision Fund was an early backer of Nvidia, <a href="https://www.cnbc.com/2017/05/24/the-stock-markets-hottest-stock-nvidia-just-got-a-big-new-backer.html">reportedly amassing</a> a $4 billion stake in 2017 before <a href="https://www.cnbc.com/2019/02/06/softbank-vision-fund-sells-nvidia-stake.html">selling all</a> of its holdings in January 2019. Despite its latest sale, SoftBank's business interests remain heavily intertwined with Nvidia's.</p></div><div id="Placeholder-ArticleBody-Video-108212821" data-test="VideoPlaceHolder" role="region" tabindex="0" data-vilynx-id="7000392166" aria-labelledby="Placeholder-ArticleBody-Video-108212821"><p><img src="https://image.cnbcfm.com/api/v1/image/108212822-17605971971760597195-42119380833-1080pnbcnews.jpg?v=1760597197&amp;w=750&amp;h=422&amp;vtcrop=y" alt="ABB CEO: Softbank will be good home for robotics business"><span></span><span></span></p></div><div><p>That Tokyo-based company is involved in a number of AI ventures that rely on Nvidia's technology, including the $500 billion Stargate project for data centers in the U.S.</p><p>"This should not be seen, in our view, as a cautious or negative stance on Nvidia, but rather in the context of SoftBank needing at least $30.5bn of capital for investments in the Oct-Dec quarter, including $22.5bn for OpenAI and $6.5bn for Ampere," Rolf Bulk, equity research analyst at New Street Research, told CNBC.</p><p>That amounts to "more in a single quarter than it has invested in aggregate over the two prior years combined," Bulk said.</p><p>Morningstar's Dan Baker added that he doesn't see the move as representing a fundamental shift in strategy for the company.</p><p>"[SoftBank] made a point of saying that it wasn't any view on NVIDIA... At the end of the day, they are using the money to invest in other AI related companies," he said.</p></div><h2><a id="headline0"></a>Vision fund posts blowout $19 billion gain</h2><div><p>The stake sales and a blowout gain of $19 billion from SoftBank's Vision Fund helped the company <a href="https://www.cnbc.com/2025/11/11/softbank-earnings-report-2q.html">double its profit</a> in its fiscal second quarter.</p><p>The Vision Fund has been aggressively pushing into artificial intelligence, investing and acquiring firms throughout the AI value chain from chips to large language models and robotics.</p><p>"The reason we were able to have this result is because of September last year, that was the first time we invested in OpenAI," said SoftBank's Goto. He added that OpenAI's <a href="https://www.cnbc.com/2025/10/02/openai-share-sale-500-billion-valuation.html">latest valuation milestone of $500 billion</a> marks one of the largest valuations in the world, according to fair value.  </p></div><div><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="256" height="256" viewBox="0 0 256 256" aria-labelledby="title desc" role="img" focusable="false" preserveAspectRatio="xMinYMin"><title>Stock Chart Icon</title><desc>Stock chart icon</desc><g transform="translate(1.4065934065934016 1.4065934065934016) scale(2.81 2.81)"><path d="M 87.994 0 H 69.342 c -1.787 0 -2.682 2.16 -1.418 3.424 l 5.795 5.795 l -33.82 33.82 L 28.056 31.196 l -3.174 -3.174 c -1.074 -1.074 -2.815 -1.074 -3.889 0 L 0.805 48.209 c -1.074 1.074 -1.074 2.815 0 3.889 l 3.174 3.174 c 1.074 1.074 2.815 1.074 3.889 0 l 15.069 -15.069 l 14.994 14.994 c 1.074 1.074 2.815 1.074 3.889 0 l 1.614 -1.614 c 0.083 -0.066 0.17 -0.125 0.247 -0.202 l 37.1 -37.1 l 5.795 5.795 C 87.84 23.34 90 22.445 90 20.658 V 2.006 C 90 0.898 89.102 0 87.994 0 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 65.626 37.8 v 49.45 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 23.518 L 65.626 37.8 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 47.115 56.312 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 42.03 L 47.115 56.312 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 39.876 60.503 c -1.937 0 -3.757 -0.754 -5.127 -2.124 l -6.146 -6.145 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 59.844 C 41.952 60.271 40.933 60.503 39.876 60.503 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path><path d="M 22.937 46.567 L 11.051 58.453 c -0.298 0.298 -0.621 0.562 -0.959 0.8 V 87.25 c 0 1.519 1.231 2.75 2.75 2.75 h 8.782 c 1.519 0 2.75 -1.231 2.75 -2.75 V 48.004 L 22.937 46.567 z" transform=" matrix(1 0 0 1 0 0) " stroke-linecap="round"></path></g></svg><p><img src="https://static-redesign.cnbcfm.com/dist/a54b41835a8b60db28c2.svg" alt="hide content"></p></div><p>Softbank's shares this year</p></div><div><p>The Japanese conglomerate's stock has slumped in the past week as <a href="https://www.cnbc.com/2025/11/07/ai-valuation-fears-grip-investors-as-tech-bubble-concerns-heighten.html">concerns of an AI bubble</a> sent jitters through global markets. </p><p>"Our share price recently has been going up and down dynamically… we want to provide as many invest opportunities as possible," said Goto Tuesday, adding that the company's announced four-for-one stock split is part of its strategy to provide as many investment opportunities for shareholders as possible.</p></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[AI documentation you can talk to, for every repo (141 pts)]]></title>
            <link>https://deepwiki.com/</link>
            <guid>45884169</guid>
            <pubDate>Tue, 11 Nov 2025 04:38:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://deepwiki.com/">https://deepwiki.com/</a>, See on <a href="https://news.ycombinator.com/item?id=45884169">Hacker News</a></p>
<div id="readability-page-1" class="page"><div tabindex="138"><a href="https://deepwiki.com/bregman-arie/devops-exercises"><div><div><p><span>bregman-arie</span>/<span>devops-exercises</span></p></div><p>Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions</p><div><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 256 256"><path d="M239.18,97.26A16.38,16.38,0,0,0,224.92,86l-59-4.76L143.14,26.15a16.36,16.36,0,0,0-30.27,0L90.11,81.23,31.08,86a16.46,16.46,0,0,0-9.37,28.86l45,38.83L53,211.75a16.38,16.38,0,0,0,24.5,17.82L128,198.49l50.53,31.08A16.4,16.4,0,0,0,203,211.75l-13.76-58.07,45-38.83A16.43,16.43,0,0,0,239.18,97.26Zm-15.34,5.47-48.7,42a8,8,0,0,0-2.56,7.91l14.88,62.8a.37.37,0,0,1-.17.48c-.18.14-.23.11-.38,0l-54.72-33.65a8,8,0,0,0-8.38,0L69.09,215.94c-.15.09-.19.12-.38,0a.37.37,0,0,1-.17-.48l14.88-62.8a8,8,0,0,0-2.56-7.91l-48.7-42c-.12-.1-.23-.19-.13-.5s.18-.27.33-.29l63.92-5.16A8,8,0,0,0,103,91.86l24.62-59.61c.08-.17.11-.25.35-.25s.27.08.35.25L153,91.86a8,8,0,0,0,6.75,4.92l63.92,5.16c.15,0,.24,0,.33.29S224,102.63,223.84,102.73Z"></path></svg><p><span>74.0k</span></p></div></div></a></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The 'Toy Story' You Remember (872 pts)]]></title>
            <link>https://animationobsessive.substack.com/p/the-toy-story-you-remember</link>
            <guid>45883788</guid>
            <pubDate>Tue, 11 Nov 2025 03:17:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://animationobsessive.substack.com/p/the-toy-story-you-remember">https://animationobsessive.substack.com/p/the-toy-story-you-remember</a>, See on <a href="https://news.ycombinator.com/item?id=45883788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!oYAZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 424w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 848w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1272w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png" width="1456" height="782" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:782,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1780036,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!oYAZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 424w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 848w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1272w, https://substackcdn.com/image/fetch/$s_!oYAZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffcdc6e58-e9e5-4bc7-8d8e-5193a840a9e0_1863x1000.png 1456w" sizes="100vw" fetchpriority="high"></picture></div></a><figcaption><span>A still from </span><em>Toy Story</em><span> on 35 mm film</span></figcaption></figure></div><p><strong>Welcome!</strong><span> Glad you could join us for another Sunday edition of the </span><em>Animation Obsessive</em><span> newsletter. This is our slate:</span></p><ul><li><p><strong>1)</strong><span> Digital animation on film stock.</span></p></li><li><p><strong>2)</strong><span> Animation newsbits.</span></p></li></ul><p>With that, let’s go!</p><p><em>Toy Story</em><span> used to look different. It’s a little tricky to explain.</span></p><p><span>Back in 1995, CG animation was </span><em>the</em><span> topic in the industry, and Pixar was central to the hype. The studio had already </span><a href="https://animationobsessive.substack.com/p/when-disney-went-digital" rel="">shifted Disney to computers</a><span> and won the first Oscar for a CG short (</span><em><a href="https://www.youtube.com/watch?v=DWi2WTqD59A" rel="">Tin Toy</a></em><span>). Giant movies like </span><em>Jurassic Park</em><span> incorporated Pixar’s software.</span></p><p><span>The next step was </span><em>Toy Story</em><span>, billed as the first animated feature to go all-CG.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-1-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-1-178330349" target="_self" rel="">1</a></span><span> Even after Pixar’s successes, that was a risk. Would a fully digital movie sell tickets? </span></p><p><span>It clearly worked out. </span><em>Toy Story</em><span> appeared 30 years ago this month — and its popularity created the animation world that exists now. A new process took over the business.</span></p><p><span>But not </span><em>entirely</em><span> new — not at first. There was something old about </span><em>Toy Story</em><span>’s tech, too, back in 1995. Pixar made the thing with computers, but it still needed to screen in theaters. And computers couldn’t really </span><em>do</em><span> that yet. From its early years, Pixar had relied on physical film stock. According to authors Bill Kinder and Bobbie O’Steen:</span></p><blockquote><p><span>[Pixar’s Ed]</span><em> Catmull recognized that his studio’s pixels needed to merge with that world-standard distribution freeway, 35 mm film. Computer chips were not fast enough, nor disks large enough, nor compression sophisticated enough to display even 30 minutes of standard-definition motion pictures. It was axiomatic that for a filmgoing audience to be going to a film, it would be a... film.</em><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-2-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-2-178330349" target="_self" rel="">2</a></span></p></blockquote><p><em>Toy Story</em><span> was a transitional project. Since Pixar couldn’t send digital data to theaters, every one of the movie’s frames was printed on analog film. When </span><em>Toy Story</em><span> originally hit home video, that 35 mm version was its source. Only years later, after technology advanced, did Pixar start doing digital transfers — cutting out the middleman. And </span><em>Toy Story</em><span>’s look changed with the era.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-3-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-3-178330349" target="_self" rel="">3</a></span><span> </span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!rk4n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!rk4n!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 424w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 848w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png" width="1456" height="1688" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1688,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4835256,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!rk4n!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 424w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 848w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!rk4n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd30fe34c-5024-4f38-87f9-77fa779a3286_1863x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Toy Story</em><span>’s original release on 35 mm (top), and the version currently streaming on Disney+ (bottom). See the film’s trailer on 35 mm </span><a href="https://www.youtube.com/watch?v=LoBFN_V66P0" rel="">here</a><span>.</span></figcaption></figure></div><p><span>While making </span><em>Toy Story</em><span>, Pixar’s team knew that the grain, softness, colors and contrasts of analog film weren’t visible on its monitors. They were different mediums. </span></p><p><span>So, to get the right look, the studio had to keep that final, physical output in mind. The digital colors were tailored with an awareness that they would change after printing. “Greens go dark really fast, while the reds stay pretty true,” said </span><em>Toy Story</em><span>’s art director, Ralph Eggleston. “Blues have to be less saturated to look fully saturated on film, while the oranges look really bad on computer screens, but look really great on film.”</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-4-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-4-178330349" target="_self" rel="">4</a></span></p><p>The team checked its work along the way. In the words of Pixar’s William Reeves:</p><blockquote><p><em>During production, we’re working mostly from computer monitors. We’re rarely seeing the images on film. So, we have five or six extremely high-resolution monitors that have better color and picture quality. We put those in general work areas, so people can go and see how their work looks. Then, when we record, we try to calibrate to the film stock, so the image we have on the monitor looks the same as what we’ll get on film.</em></p></blockquote><p><span>Behind the final images was a “painstaking transfer process,” according to the press. Leading it was David DiFrancesco, one of Pixar’s early MVPs, who began working with Ed Catmull before Pixar even existed. He broke ground in film printing — specifically, in putting digital images on analog film.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-5-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-5-178330349" target="_self" rel="">5</a></span></p><p><span>He and his team in Pixar’s photoscience department used their expertise here. Their tools were “commercial grade” film printers, DiFrancesco noted: modified Solitaire Cine II machines. He’d invented more advanced stuff, but it wasn’t viable for a project of </span><em>Toy Story</em><span>’s size. Using the best equipment would’ve taken “several terabytes of data,” he said.</span></p><p><span>Their system was fairly straightforward. Every frame of </span><em>Toy Story</em><span>’s negative was exposed, three times, in front of a CRT screen that displayed the movie. “Since all film and video images are composed of combinations of red, green and blue light, the frame is separated into its discrete red, green and blue elements,” noted the studio. Exposures, filtered through each color, were layered to create each frame.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-6-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-6-178330349" target="_self" rel="">6</a></span><span> </span></p><p><span>It reportedly took nine hours to print 30 seconds of </span><em>Toy Story</em><span>. But it had to be done: it was the only way to screen the film.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!5nPg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!5nPg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 424w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 848w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1272w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png" width="1456" height="882" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:882,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2072015,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!5nPg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 424w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 848w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1272w, https://substackcdn.com/image/fetch/$s_!5nPg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0c12dbf0-039e-4de3-8dac-e3231210eb69_2000x1211.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><span>Examples of green, blue and red exposures, and the final scene on 35 mm film. Courtesy of the </span><em>Ultimate Toy Box</em><span> DVD.</span></figcaption></figure></div><p>In 1999, Pixar made history again.</p><p><span>Its second feature, </span><em>A Bug’s Life</em><span>, reached theaters in 1998. Once more, the studio designed its visuals for analog film (</span><a href="https://www.youtube.com/watch?v=izmlSjjOEdo" rel="">see the trailer on 35 mm</a><span>). Its people knew the ins-and-outs of this process, down to the amount of detail that film stock could accept and a projector could show. That’s partly how they got away with the movie’s tiny 2048×862 resolution, for example.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-7-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-7-178330349" target="_self" rel="">7</a></span></p><p><span>Still, the team struggled with one thing: the dip in image quality when film got converted to home video. That’s how </span><em>Toy Story</em><span> was released, but there </span><em>had</em><span> to be a better way.</span></p><p><span>For the home version of</span><em> A Bug’s Life</em><span>, Pixar devised a method of “go[ing] from our digital image within our system … straight to video,” John Lasseter said. He called it “a real pure version of our movie straight from our computers.” </span><em>A Bug’s Life</em><span> became the first digital-to-digital transfer on DVD. Compared to the theatrical release, the look had changed. It was sharp and grainless, and the colors were kind of different.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-8-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-8-178330349" target="_self" rel="">8</a></span></p><p><span>A digital transfer of </span><em>Toy Story</em><span> followed in the early 2000s. And it wasn’t </span><em>quite</em><span> the same movie that viewers had seen in the ‘90s. “The colors are vivid and lifelike, [and] not a hint of grain or artifacts can be found,” raved one reviewer. It was a crisp, blazingly bright, digital image now — totally different from the softness, texture and deep, muted warmth of physical film, on which </span><em>Toy Story </em><span>was created to be seen.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!P-J7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!P-J7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 424w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 848w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png" width="1456" height="1676" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1676,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3528708,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" title="" srcset="https://substackcdn.com/image/fetch/$s_!P-J7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 424w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 848w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!P-J7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffd4e3796-6331-4286-bb6c-0bd7870c05d4_1876x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Toy Story</em><span> on 35 mm (top) and the Disney+ edition (bottom)</span></figcaption></figure></div><p><span>Quickly, digital transfers became a standard thing. Among others by Pixar, </span><em>The Incredibles</em><span> puts off a very different vibe between its theatrical and later releases (see </span><a href="https://www.youtube.com/watch?v=M_nSbqsLmEk" rel="">the 35 mm trailer</a><span> for reference). </span></p><p>Pixar wasn’t the only studio to make the leap, either. Disney did as well. </p><p><span>Like </span><em>Toy Story</em><span>, the Disney renaissance work of the ‘90s was transitional. </span><em>The Lion King</em><span>, </span><em>Mulan</em><span> and the rest existed as </span><a href="https://animationobsessive.substack.com/p/when-disney-went-digital" rel="">files in computer systems</a><span> — and the idea was always to record them on analog film at the end. Early home releases were based on those 35 mm versions. Later releases, like the ones Disney streams today, were direct transfers of the digital data. </span></p><p><span>At times, especially in the colors, they’re almost unrecognizable. And the images feel less cohesive — like something’s missing that was </span><em>supposed</em><span> to bring all the elements together. These aren’t quite the same films that ruled the ‘90s.</span></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!6IkD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!6IkD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 424w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 848w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1272w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png" width="1456" height="1633" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1633,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4330548,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!6IkD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 424w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 848w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1272w, https://substackcdn.com/image/fetch/$s_!6IkD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fca25faad-c263-42fd-b261-3e9b4e3197ba_1892x2122.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Aladdin</em><span> on 35 mm film (top) versus Blu-ray (bottom). See a clip from the film on 35 mm </span><a href="https://www.youtube.com/watch?v=AuhNnovKXLA" rel="">here</a><span>.</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!qdDU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!qdDU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 424w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 848w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png" width="1456" height="1662" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1662,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:4506414,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!qdDU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 424w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 848w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1272w, https://substackcdn.com/image/fetch/$s_!qdDU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde166f83-da65-4ce8-a57f-043b50348abb_1892x2160.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>The Lion King</em><span> on 35 mm film (top) versus Blu-ray. See a clip from the film on 35 mm </span><a href="https://www.youtube.com/watch?v=uivXq3tXOhg" rel="">here</a><span>.</span></figcaption></figure></div><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/$s_!T9lv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/$s_!T9lv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 424w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 848w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1272w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png" width="1456" height="1702" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1702,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:3824553,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://animationobsessive.substack.com/i/178330349?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/$s_!T9lv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 424w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 848w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1272w, https://substackcdn.com/image/fetch/$s_!T9lv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff0c2f5eb-b4f8-4249-af6f-0441c97c78b1_1814x2120.png 1456w" sizes="100vw" loading="lazy"></picture></div></a><figcaption><em>Mulan</em><span> on 35 mm film (top) versus Blu-ray. See the film’s trailer on 35 mm </span><a href="https://www.youtube.com/watch?v=2z2KsFZs-8I" rel="">here</a><span>.</span></figcaption></figure></div><p><span>For a number of years, there’s been talk in film-preservation circles about </span><em>Toy Story</em><span> and the Disney renaissance. This work sits in an odd place. The world was still pretty analog when the computer animation boom arrived: out of necessity, these projects became hybrids of new and old. What’s the</span><em> right </em><span>way to see digital movies that were designed for 35 mm film?</span></p><p><span>The studios themselves haven’t quite figured it out. On Disney+, the colors of </span><em>Toy Story</em><span> feel a bit raw — searing greens that were meant to darken on film, for example. Meanwhile, the newer </span><em>Toy Story</em><span> Blu-ray shares more in common with the original colors, but it’s still an altered, colder look.</span></p><p><span>When digital transfers first showed up, people were thrilled, including at Pixar. Movies became “crisper, clearer and more stunning on home video systems” than in theaters, some claimed.</span><span><a data-component-name="FootnoteAnchorToDOM" id="footnote-anchor-9-178330349" href="https://animationobsessive.substack.com/p/the-toy-story-you-remember#footnote-9-178330349" target="_self" rel="">9</a></span><span> Even so, it’s a little disquieting to think that </span><em>Toy Story</em><span>, the film that built our current world, is barely available in the form that wowed audiences of the ‘90s. The same goes for many other movies from the transitional era.</span></p><p><span>The good news is that this conversation gets bigger all the time. In those film-preservation circles, a dedicated few are trying to save the old work. More and more </span><a href="https://www.youtube.com/watch?v=BMvgu_KdpjA" rel="">comparison videos</a><span> are popping up on YouTube. If you get the chance to see one of the old Disney or Pixar films on 35 mm, it’s always worthwhile.</span></p><p><span>These companies, ultimately, decide how </span><em>Toy Story</em><span> looks today. Still, for some, it’s nice to see the original version of the film again — the version Pixar originally intended to make. It’s evidence that the film </span><em>did</em><span> feel</span><em> </em><span>different back then. The memories were real.</span></p><ul><li><p><em>I Am Frankelda</em><span> continues its strong performance in </span><strong>Mexican</strong><span> theaters. Analyst Edgar Apanco </span><a href="https://x.com/elapanco/status/1987639735091921274" rel="">reports</a><span> that 658,000 people have gone to see it, surpassing the popular </span><em>Chainsaw Man</em><span> movie. Revenues are </span><a href="https://x.com/elapanco/status/1987660916633325574" rel="">over $2.15 million</a><span> and climbing — having fallen </span><a href="https://palomaynacho.com/blog/chainsaw-y-frankelda-se-enfrentan-en-un-halloween-complicado/" rel="">just 17%</a><span> in week two, and an estimated 20% in week three.</span></p></li><li><p><span>In </span><strong>Japan</strong><span>, Goro Miyazaki </span><a href="https://ghibli.jpn.org/news/goro-talk-4/" rel="">revealed</a><span> that his father is still going to Studio Ghibli to draw for a few hours each day.</span></p></li><li><p><span>An exhibition in </span><strong>Taiwan</strong><span> </span><a href="https://reading.udn.com/read/story/124410/9126552" rel="">brought</a><span> the films of Karel Zeman to the country, reportedly for the first time. </span><em>The Fabulous Baron Munchausen</em><span> and </span><em>Invention for Destruction</em><span> are showing, among others.</span></p></li><li><p><span>In </span><strong>Nigeria</strong><span>, animator Gabriel Ugbodaga had </span><a href="https://www.arise.tv/gabriel-ugbodaga-nigeria-has-enough-animation-talent-what-we-lack-is-training-and-exposure/" rel="">a televised interview</a><span> about his well-received film </span><em>Vainglorious</em><span> (</span><a href="https://www.youtube.com/watch?v=6tVVWgz1cEk" rel="">watch</a><span>) and the state of the country’s industry. “When it comes to 2D hand-drawn animation,” he said, “there’s a lot of talent in Nigeria.”</span></p></li><li><p><span>If you missed that </span><em>Baahubali: The Eternal War</em><span> teaser this week, </span><a href="https://www.youtube.com/watch?v=RdUPs9e1bUk" rel="">see it here</a><span>. It’s an </span><strong>Indian</strong><span> feature presented by S. S. Rajamouli (</span><em>RRR</em><span>).</span></p></li><li><p><span>In </span><strong>Germany</strong><span>, Werner Herzog’s animated film </span><em>The Twilight World</em><span> </span><a href="https://cineuropa.org/en/newsdetail/485526" rel="">picked up</a><span> “€100,000 for production preparation support,” reports </span><em>Cineuropa</em><span>.</span></p></li><li><p><em>Infinity Castle</em><span> will reach </span><strong>China</strong><span> next weekend, and forecasters </span><a href="https://cn.investing.com/news/stock-market-news/article-3066545" rel="">believe</a><span> it could earn a billion yuan (over $140 million) and become the highest-grossing anime film in the country.</span></p></li><li><p><span>Also </span><a href="https://weibo.com/7985578740/Qcrf6p4D6" rel="">happening</a><span> in </span><strong>China</strong><span> next weekend: the latest edition of Feinaki Beijing Animation Week. The festival posted </span><a href="https://www.bilibili.com/video/BV1rMyzBxE9w/" rel="">55 trailers</a><span> for its selections this year.</span></p></li><li><p><span>The </span><strong>Japanese</strong><span> journalist Atsushi Matsumoto is raising concerns that the anime boom of the 2020s </span><a href="https://news.yahoo.co.jp/expert/articles/55f696fd42ce9a821f4bf682327f452bf3b7245c" rel="">could be a bubble</a><span>. (Meanwhile, despite huge industry profits, analysis suggests that studio closures are </span><a href="https://prtimes.jp/main/html/rd/p/000001179.000043465.html" rel="">set to rise</a><span> for the third year in a row.)</span></p></li><li><p><span>In</span><strong> America</strong><span>, for those in New York, there’s an interesting series of stop-motion screenings </span><a href="https://www.eastman.org/stop-motion-artform" rel="">at the Eastman Museum</a><span> this month — including </span><em>The Wolf House</em><span>.</span></p></li><li><p><span>Last of all: we wrote about a handful of </span><a href="https://animationobsessive.substack.com/p/free-films-worth-seeing" rel="">recent, free films worth seeing</a><span>. </span></p></li></ul><p><em><strong>Until next time!</strong></em></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I hate screenshots of text (306 pts)]]></title>
            <link>https://parkscomputing.com/page/i-hate-screenshots-of-text</link>
            <guid>45883124</guid>
            <pubDate>Tue, 11 Nov 2025 01:36:47 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://parkscomputing.com/page/i-hate-screenshots-of-text">https://parkscomputing.com/page/i-hate-screenshots-of-text</a>, See on <a href="https://news.ycombinator.com/item?id=45883124">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
        




<article>
    
    <div>
        
<p>During the course of a regular working day, I receive a lot of screenshots like this from well-meaning colleagues:</p>
<p> <img src="https://parkscomputing.com/images/screenshots.png" alt="A screenshot of some text">
</p><p>It's almost always in a chat about some issue that occurred in the code, or perhaps code that's somehow related to the code in the screenshot, or… well, how am I supposed to even know? Upon seeing this code, I might think, “How is <code>slug</code> defined? Is <code>slug</code> being used to create the <code>baseUrl</code>? Why is the domain name hard-coded in that URL? What happens if an exception is thrown? <em>What module is this code even in?</em>”</p>
<p>I have to either very carefully type some of the code into a search box or (these days) get my coding agent to find the relevant module for me.</p>
<p>Why couldn't my colleague have just used copy &amp; paste? I could have seen a bit more of the context, even if the same lines were selected, and I could copy-and-paste <em>that</em> text into my IDE's search function so much more easily.</p>
<p>In fact, why couldn't they just send me the file, or even a link to the file (since everybody and their dog use GitHub, anyway).</p>
<p>It gets worse. Sometimes, I'll get a screenshot of an error log. “Hey, Paul, the build is failing. Can you look at this?”</p>
<p> <img src="https://parkscomputing.com/images/screenshots-errors2.png" alt="A screenshot of some build errors">
</p><p>What were you building? What line did it fail on? <em>What even was the error?</em></p>
<p>Of course, if I do a full rebuild of everything on my workstation, it'll succeed.</p>
<p>It would have been SO easy to just copy all of the error log, or even dump the log into a file, and just send me that.</p>
<p> <img src="https://parkscomputing.com/images/banging-head-against-wall-cracked.gif" alt="Me reading a screenshot of some build errors">
</p><p>Please, don't take screenshots of text unless it's to demonstrate a cosmetic issue related to the display of the text, or there is truly something relevant about the content of the screenshot that would be lost in a purely textual context.</p>

    </div>
</article>



    

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Warren Buffett's final shareholder letter [pdf] (355 pts)]]></title>
            <link>https://berkshirehathaway.com/news/nov1025.pdf</link>
            <guid>45882837</guid>
            <pubDate>Tue, 11 Nov 2025 00:51:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://berkshirehathaway.com/news/nov1025.pdf">https://berkshirehathaway.com/news/nov1025.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45882837">Hacker News</a></p>
&lt;Not HTML&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[High-performance 2D graphics rendering on the CPU using sparse strips [pdf] (267 pts)]]></title>
            <link>https://github.com/LaurenzV/master-thesis/blob/main/main.pdf</link>
            <guid>45881568</guid>
            <pubDate>Mon, 10 Nov 2025 22:05:16 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/LaurenzV/master-thesis/blob/main/main.pdf">https://github.com/LaurenzV/master-thesis/blob/main/main.pdf</a>, See on <a href="https://news.ycombinator.com/item?id=45881568">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            


<react-partial partial-name="marketing-navigation" data-ssr="true" data-attempted-ssr="true" data-react-profiling="false">
  
  
  <div data-target="react-partial.reactRoot"><nav aria-label="Global"><ul><li><div><ul><li><div><p><span>AI CODE CREATION</span></p><ul><li><a href="https://github.com/features/copilot" data-analytics-event="{&quot;action&quot;:&quot;github_copilot&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_copilot_link_platform_navbar&quot;}"><div><p><span>GitHub Copilot</span><span>Write better code with AI</span></p></div></a></li><li><a href="https://github.com/features/spark" data-analytics-event="{&quot;action&quot;:&quot;github_spark&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_spark_link_platform_navbar&quot;}"><div><p><span>GitHub Spark</span><span>Build and deploy intelligent apps</span></p></div></a></li><li><a href="https://github.com/features/models" data-analytics-event="{&quot;action&quot;:&quot;github_models&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_models_link_platform_navbar&quot;}"><div><p><span>GitHub Models</span><span>Manage and compare prompts</span></p></div></a></li><li><a href="https://github.com/mcp" data-analytics-event="{&quot;action&quot;:&quot;mcp_registry&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;mcp_registry_link_platform_navbar&quot;}"><div><p><span>MCP Registry<sup>New</sup></span><span>Discover and integrate external tools</span></p></div></a></li></ul></div></li><li><div><p><span>DEVELOPER WORKFLOWS</span></p><ul><li><a href="https://github.com/features/actions" data-analytics-event="{&quot;action&quot;:&quot;actions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;actions_link_platform_navbar&quot;}"><div><p><span>Actions</span><span>Automate any workflow</span></p></div></a></li><li><a href="https://github.com/features/codespaces" data-analytics-event="{&quot;action&quot;:&quot;codespaces&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;codespaces_link_platform_navbar&quot;}"><div><p><span>Codespaces</span><span>Instant dev environments</span></p></div></a></li><li><a href="https://github.com/features/issues" data-analytics-event="{&quot;action&quot;:&quot;issues&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;issues_link_platform_navbar&quot;}"><div><p><span>Issues</span><span>Plan and track work</span></p></div></a></li><li><a href="https://github.com/features/code-review" data-analytics-event="{&quot;action&quot;:&quot;code_review&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_review_link_platform_navbar&quot;}"><div><p><span>Code Review</span><span>Manage code changes</span></p></div></a></li></ul></div></li><li><div><p><span>APPLICATION SECURITY</span></p><ul><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_platform_navbar&quot;}"><div><p><span>GitHub Advanced Security</span><span>Find and fix vulnerabilities</span></p></div></a></li><li><a href="https://github.com/security/advanced-security/code-security" data-analytics-event="{&quot;action&quot;:&quot;code_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;code_security_link_platform_navbar&quot;}"><div><p><span>Code security</span><span>Secure your code as you build</span></p></div></a></li><li><a href="https://github.com/security/advanced-security/secret-protection" data-analytics-event="{&quot;action&quot;:&quot;secret_protection&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;secret_protection_link_platform_navbar&quot;}"><div><p><span>Secret protection</span><span>Stop leaks before they start</span></p></div></a></li></ul></div></li><li><div><p><span>EXPLORE</span></p><ul><li><a href="https://github.com/why-github" data-analytics-event="{&quot;action&quot;:&quot;why_github&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;why_github_link_platform_navbar&quot;}"><span>Why GitHub</span></a></li><li><a href="https://docs.github.com/" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Documentation</span></a></li><li><a href="https://github.blog/" data-analytics-event="{&quot;action&quot;:&quot;blog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;blog_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Blog</span></a></li><li><a href="https://github.blog/changelog" data-analytics-event="{&quot;action&quot;:&quot;changelog&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;changelog_link_platform_navbar&quot;}" target="_blank" rel="noreferrer"><span>Changelog</span></a></li><li><a href="https://github.com/marketplace" data-analytics-event="{&quot;action&quot;:&quot;marketplace&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;marketplace_link_platform_navbar&quot;}"><span>Marketplace</span></a></li></ul></div></li></ul><p><a href="https://github.com/features" data-analytics-event="{&quot;action&quot;:&quot;view_all_features&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;platform&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_features_link_platform_navbar&quot;}"><span>View all features</span></a></p></div></li><li><div><ul><li><div><p><span>BY COMPANY SIZE</span></p><ul><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprises&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprises_link_solutions_navbar&quot;}"><span>Enterprises</span></a></li><li><a href="https://github.com/team" data-analytics-event="{&quot;action&quot;:&quot;small_and_medium_teams&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;small_and_medium_teams_link_solutions_navbar&quot;}"><span>Small and medium teams</span></a></li><li><a href="https://github.com/enterprise/startups" data-analytics-event="{&quot;action&quot;:&quot;startups&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;startups_link_solutions_navbar&quot;}"><span>Startups</span></a></li><li><a href="https://github.com/solutions/industry/nonprofits" data-analytics-event="{&quot;action&quot;:&quot;nonprofits&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;nonprofits_link_solutions_navbar&quot;}"><span>Nonprofits</span></a></li></ul></div></li><li><div><p><span>BY USE CASE</span></p><ul><li><a href="https://github.com/solutions/use-case/app-modernization" data-analytics-event="{&quot;action&quot;:&quot;app_modernization&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;app_modernization_link_solutions_navbar&quot;}"><span>App Modernization</span></a></li><li><a href="https://github.com/solutions/use-case/devsecops" data-analytics-event="{&quot;action&quot;:&quot;devsecops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devsecops_link_solutions_navbar&quot;}"><span>DevSecOps</span></a></li><li><a href="https://github.com/solutions/use-case/devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_solutions_navbar&quot;}"><span>DevOps</span></a></li><li><a href="https://github.com/solutions/use-case/ci-cd" data-analytics-event="{&quot;action&quot;:&quot;ci/cd&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ci/cd_link_solutions_navbar&quot;}"><span>CI/CD</span></a></li><li><a href="https://github.com/solutions/use-case" data-analytics-event="{&quot;action&quot;:&quot;view_all_use_cases&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_use_cases_link_solutions_navbar&quot;}"><span>View all use cases</span></a></li></ul></div></li><li><div><p><span>BY INDUSTRY</span></p><ul><li><a href="https://github.com/solutions/industry/healthcare" data-analytics-event="{&quot;action&quot;:&quot;healthcare&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;healthcare_link_solutions_navbar&quot;}"><span>Healthcare</span></a></li><li><a href="https://github.com/solutions/industry/financial-services" data-analytics-event="{&quot;action&quot;:&quot;financial_services&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;financial_services_link_solutions_navbar&quot;}"><span>Financial services</span></a></li><li><a href="https://github.com/solutions/industry/manufacturing" data-analytics-event="{&quot;action&quot;:&quot;manufacturing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;manufacturing_link_solutions_navbar&quot;}"><span>Manufacturing</span></a></li><li><a href="https://github.com/solutions/industry/government" data-analytics-event="{&quot;action&quot;:&quot;government&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;government_link_solutions_navbar&quot;}"><span>Government</span></a></li><li><a href="https://github.com/solutions/industry" data-analytics-event="{&quot;action&quot;:&quot;view_all_industries&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_industries_link_solutions_navbar&quot;}"><span>View all industries</span></a></li></ul></div></li></ul><p><a href="https://github.com/solutions" data-analytics-event="{&quot;action&quot;:&quot;view_all_solutions&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;solutions&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_solutions_link_solutions_navbar&quot;}"><span>View all solutions</span></a></p></div></li><li><div><ul><li><div><p><span>EXPLORE BY TOPIC</span></p><ul><li><a href="https://github.com/resources/articles?topic=ai" data-analytics-event="{&quot;action&quot;:&quot;ai&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ai_link_resources_navbar&quot;}"><span>AI</span></a></li><li><a href="https://github.com/resources/articles?topic=software-development" data-analytics-event="{&quot;action&quot;:&quot;software_development&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;software_development_link_resources_navbar&quot;}"><span>Software Development</span></a></li><li><a href="https://github.com/resources/articles?topic=devops" data-analytics-event="{&quot;action&quot;:&quot;devops&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;devops_link_resources_navbar&quot;}"><span>DevOps</span></a></li><li><a href="https://github.com/resources/articles?topic=security" data-analytics-event="{&quot;action&quot;:&quot;security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_link_resources_navbar&quot;}"><span>Security</span></a></li><li><a href="https://github.com/resources/articles" data-analytics-event="{&quot;action&quot;:&quot;view_all_topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;view_all_topics_link_resources_navbar&quot;}"><span>View all topics</span></a></li></ul></div></li><li><div><p><span>EXPLORE BY TYPE</span></p><ul><li><a href="https://github.com/customer-stories" data-analytics-event="{&quot;action&quot;:&quot;customer_stories&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_stories_link_resources_navbar&quot;}"><span>Customer stories</span></a></li><li><a href="https://github.com/resources/events" data-analytics-event="{&quot;action&quot;:&quot;events__webinars&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;events__webinars_link_resources_navbar&quot;}"><span>Events &amp; webinars</span></a></li><li><a href="https://github.com/resources/whitepapers" data-analytics-event="{&quot;action&quot;:&quot;ebooks__reports&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;ebooks__reports_link_resources_navbar&quot;}"><span>Ebooks &amp; reports</span></a></li><li><a href="https://github.com/solutions/executive-insights" data-analytics-event="{&quot;action&quot;:&quot;business_insights&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;business_insights_link_resources_navbar&quot;}"><span>Business insights</span></a></li><li><a href="https://skills.github.com/" data-analytics-event="{&quot;action&quot;:&quot;github_skills&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_skills_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>GitHub Skills</span></a></li></ul></div></li><li><div><p><span>SUPPORT &amp; SERVICES</span></p><ul><li><a href="https://docs.github.com/" data-analytics-event="{&quot;action&quot;:&quot;documentation&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;documentation_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>Documentation</span></a></li><li><a href="https://support.github.com/" data-analytics-event="{&quot;action&quot;:&quot;customer_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;customer_support_link_resources_navbar&quot;}" target="_blank" rel="noreferrer"><span>Customer support</span></a></li><li><a href="https://github.com/orgs/community/discussions" data-analytics-event="{&quot;action&quot;:&quot;community_forum&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;community_forum_link_resources_navbar&quot;}"><span>Community forum</span></a></li><li><a href="https://github.com/trust-center" data-analytics-event="{&quot;action&quot;:&quot;trust_center&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trust_center_link_resources_navbar&quot;}"><span>Trust center</span></a></li><li><a href="https://github.com/partners" data-analytics-event="{&quot;action&quot;:&quot;partners&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;resources&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;partners_link_resources_navbar&quot;}"><span>Partners</span></a></li></ul></div></li></ul></div></li><li><div><ul><li><div><p><span>COMMUNITY</span></p><ul><li><a href="https://github.com/sponsors" data-analytics-event="{&quot;action&quot;:&quot;github_sponsors&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_sponsors_link_open_source_navbar&quot;}"><div><p><span>GitHub Sponsors</span><span>Fund open source developers</span></p></div></a></li></ul></div></li><li><div><p><span>PROGRAMS</span></p><ul><li><a href="https://securitylab.github.com/" data-analytics-event="{&quot;action&quot;:&quot;security_lab&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;security_lab_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Security Lab</span></a></li><li><a href="https://maintainers.github.com/" data-analytics-event="{&quot;action&quot;:&quot;maintainer_community&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;maintainer_community_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Maintainer Community</span></a></li><li><a href="https://github.com/accelerator" data-analytics-event="{&quot;action&quot;:&quot;accelerator&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;accelerator_link_open_source_navbar&quot;}"><span>Accelerator</span></a></li><li><a href="https://archiveprogram.github.com/" data-analytics-event="{&quot;action&quot;:&quot;archive_program&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;archive_program_link_open_source_navbar&quot;}" target="_blank" rel="noreferrer"><span>Archive Program</span></a></li></ul></div></li><li><div><p><span>REPOSITORIES</span></p><ul><li><a href="https://github.com/topics" data-analytics-event="{&quot;action&quot;:&quot;topics&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;topics_link_open_source_navbar&quot;}"><span>Topics</span></a></li><li><a href="https://github.com/trending" data-analytics-event="{&quot;action&quot;:&quot;trending&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;trending_link_open_source_navbar&quot;}"><span>Trending</span></a></li><li><a href="https://github.com/collections" data-analytics-event="{&quot;action&quot;:&quot;collections&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;open_source&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;collections_link_open_source_navbar&quot;}"><span>Collections</span></a></li></ul></div></li></ul></div></li><li><div><ul><li><div><p><span>ENTERPRISE SOLUTIONS</span></p><ul><li><a href="https://github.com/enterprise" data-analytics-event="{&quot;action&quot;:&quot;enterprise_platform&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;enterprise_platform_link_enterprise_navbar&quot;}"><div><p><span>Enterprise platform</span><span>AI-powered developer platform</span></p></div></a></li></ul></div></li><li><div><p><span>AVAILABLE ADD-ONS</span></p><ul><li><a href="https://github.com/security/advanced-security" data-analytics-event="{&quot;action&quot;:&quot;github_advanced_security&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;github_advanced_security_link_enterprise_navbar&quot;}"><div><p><span>GitHub Advanced Security</span><span>Enterprise-grade security features</span></p></div></a></li><li><a href="https://github.com/features/copilot/copilot-business" data-analytics-event="{&quot;action&quot;:&quot;copilot_for_business&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;copilot_for_business_link_enterprise_navbar&quot;}"><div><p><span>Copilot for Business</span><span>Enterprise-grade AI features</span></p></div></a></li><li><a href="https://github.com/premium-support" data-analytics-event="{&quot;action&quot;:&quot;premium_support&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;enterprise&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;premium_support_link_enterprise_navbar&quot;}"><div><p><span>Premium Support</span><span>Enterprise-grade 24/7 support</span></p></div></a></li></ul></div></li></ul></div></li><li><a href="https://github.com/pricing" data-analytics-event="{&quot;action&quot;:&quot;pricing&quot;,&quot;tag&quot;:&quot;link&quot;,&quot;context&quot;:&quot;pricing&quot;,&quot;location&quot;:&quot;navbar&quot;,&quot;label&quot;:&quot;pricing_link_pricing_navbar&quot;}"><span>Pricing</span></a></li></ul></nav></div>
</react-partial>



        <div>
                


<qbsearch-input data-scope="repo:LaurenzV/master-thesis" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="gjP2bom9MneAfFMoYIHRW9za-u5vhVJJHm5vfaTlEdG8qH4iHCp0pF2ISxvbOvUB2fGhWK-mx5easnUWe2Sacw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="LaurenzV/master-thesis" data-current-org="" data-current-owner="LaurenzV" data-logged-in="false" data-copilot-chat-enabled="false" data-nl-search-enabled="false" data-retain-scroll-position="true">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
        
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        <div data-view-component="true">        <!-- '"` --><!-- </textarea></xmp> --><form id="code-search-feedback-form" data-turbo="false" action="/search/feedback" accept-charset="UTF-8" method="post">
          <p>We read every piece of feedback, and take your input very seriously.</p>
          
          
          <label for="include_email">Include my email address so I can be contacted</label>
</form></div>
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>


            <div>
              <p><a href="https://github.com/login?return_to=https%3A%2F%2Fgithub.com%2FLaurenzV%2Fmaster-thesis%2Fblob%2Fmain%2Fmain.pdf" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/LaurenzV/master-thesis/blob/main/main.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d4712b985595d0ee9eef3ef57900e345d9365d3b9bb74c25427d77c8aca3315c" data-analytics-event="{&quot;category&quot;:&quot;Marketing nav&quot;,&quot;action&quot;:&quot;click to go to homepage&quot;,&quot;label&quot;:&quot;ref_page:Marketing;ref_cta:Sign in;ref_loc:Header&quot;}">
                Sign in
              </a>
            </p></div>

              <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E%2Fblob%2Fshow&amp;source=header-repo&amp;source_repo=LaurenzV%2Fmaster-thesis" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/LaurenzV/master-thesis/blob/main/main.pdf&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="d4712b985595d0ee9eef3ef57900e345d9365d3b9bb74c25427d77c8aca3315c" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>/blob/show;ref_cta:Sign up;ref_loc:header logged out&quot;}">
                Sign up
              </a></p><p>
    <react-partial-anchor>
      <tool-tip id="tooltip-a472a9ef-fa55-4047-8728-a57ca0e96a26" for="icon-button-0e257615-683d-47b5-9a35-75c50f858c72" popover="manual" data-direction="s" data-type="label" data-view-component="true">Appearance settings</tool-tip>

      <template data-target="react-partial-anchor.template">
        <link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/primer-react.2ed7297523f7a189873b.module.css">
<link crossorigin="anonymous" media="all" rel="stylesheet" href="https://github.githubassets.com/assets/appearance-settings.6c63a6de228d6520804d.module.css">

<react-partial partial-name="appearance-settings" data-ssr="false" data-attempted-ssr="false" data-react-profiling="false">
  
  <script type="application/json" data-target="react-partial.embeddedData">{"props":{}}</script>
  <div data-target="react-partial.reactRoot"></div>
</react-partial>


      </template>
    </react-partial-anchor>
  </p>

          </div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Spatial intelligence is AI’s next frontier (216 pts)]]></title>
            <link>https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence</link>
            <guid>45880939</guid>
            <pubDate>Mon, 10 Nov 2025 21:07:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence">https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence</a>, See on <a href="https://news.ycombinator.com/item?id=45880939">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><div><p><span>In 1950, when computing was little more than automated arithmetic and simple logic, Alan Turing asked a question that still reverberates today: can machines think? It took remarkable imagination to see what he saw: that intelligence might someday be built rather than born. That insight later launched a relentless scientific quest called Artificial Intelligence (AI). Twenty-five years into my own career in AI, I still find myself inspired by Turing’s vision. But how close are we? The answer isn’t simple.</span></p><p><span>Today, leading AI technology such as large language models (LLMs) have begun to transform how we access and work with abstract knowledge. Yet they remain wordsmiths in the dark; eloquent but inexperienced, knowledgeable but ungrounded. </span><strong>Spatial intelligence will transform how we create and interact with real and virtual worlds—revolutionizing storytelling, creativity, robotics, scientific discovery, and beyond. This is AI’s next frontier.</strong></p></div><p><span>The pursuit of visual and spatial</span><em> </em><span>intelligence has been the North Star guiding me since I entered the field. It’s why I spent years building ImageNet, the first large-scale visual learning and benchmarking dataset and one of three key elements enabling the birth of modern AI, along with neural network algorithms and modern compute like graphics processing units (GPUs). It’s why </span><a href="https://svl.stanford.edu/" rel="">my academic lab at Stanford</a><span> has spent the last decade combining computer vision with robotic learning. And it’s why my cofounders Justin Johnson, Christoph Lassner, Ben Mildenhall, and I created </span><a href="https://www.worldlabs.ai/" rel="">World Labs</a><span> more than one year ago: to realize this possibility in full, for the first time.</span></p><p>In this essay, I’ll explain what spatial intelligence is, why it matters, and how we’re building the world models that will unlock it—with impact that will reshape creativity, embodied intelligence, and human progress.</p><p>AI has never been more exciting. Generative AI models such as LLMs have moved from research labs to everyday life, becoming tools of creativity, productivity, and communication for billions of people. They have demonstrated capabilities once thought impossible, producing coherent text, mountains of code, photorealistic images, and even short video clips with ease. It’s no longer a question of whether AI will change the world. By any reasonable definition, it already has.</p><p>Yet so much still lies beyond our reach. The vision of autonomous robots remains intriguing but speculative, far from the fixtures of daily life that futurists have long promised. The dream of massively accelerated research in fields like disease curation, new material discovery, and particle physics remains largely unfulfilled. And the promise of AI that truly understands and empowers human creators—whether students learning intricate concepts in molecular chemistry, architects visualizing spaces, filmmakers building worlds, or anyone seeking fully immersive virtual experiences—remains beyond reach.</p><p>To learn why these capabilities remain elusive, we need to examine how spatial intelligence evolved, and how it shapes our understanding of the world.</p><p>Vision has long been a cornerstone of human intelligence, but its power emerged from something even more fundamental. Long before animals could nest, care for their young, communicate with language, or build civilizations, the simple act of sensing quietly sparked an evolutionary journey toward intelligence.</p><p><span>This seemingly isolated ability to glean information from the external world, whether a glimmer of light or the feeling of texture, created a bridge between perception and survival that only grew stronger and more elaborate as the generations passed. Layer upon layer of neurons grew from that bridge, forming nervous systems that interpret the world and coordinate interactions between an organism and its surroundings. Thus, many scientists have conjectured that </span><strong>perception and action became the core loop driving the evolution of intelligence</strong><span>, and the foundation on which nature created our species—the ultimate embodiment of perceiving, learning, thinking, and doing.</span></p><p>Spatial intelligence plays a fundamental role in defining how we interact with the physical world. Every day, we rely on it for the most ordinary acts: parking a car by imagining the narrowing gap between bumper and curb, catching a set of keys tossed across the room, navigating a crowded sidewalk without collision, or sleepily pouring coffee into a mug without looking. In more extreme circumstances, firefighters navigate collapsing buildings through shifting smoke, making split-second judgements about stability and survival, communicating through gestures, body language and a shared professional instinct for which there’s no linguistic substitute. And children spend the entirety of their pre-verbal months or years learning the world through playful interactions with their environments. All of this happens intuitively, automatically—a fluency machines have yet to achieve.</p><div><p><span>Spatial Intelligence is also foundational to our imagination and creativity. Storytellers create uniquely rich worlds in their minds and leverage many forms of visual media to bring them to others, from ancient cave painting to modern cinema to immersive video games. Whether it’s children building sandcastles on the beach or playing Minecraft on the computer, spatially-grounded imagination forms the basis for interactive experiences in real or virtual worlds. And in many industry applications, simulations of objects, scenes and dynamic interactive environments power countless numbers of critical business use cases from industrial design to digital twins to robotic training. </span></p><p><span>History is full of civilization-defining moments where spatial intelligence played central roles. In ancient Greece, Eratosthenes transformed shadows into geometry—measuring a 7-degree angle in Alexandria at the exact moment the sun cast no shadow in Syene—to calculate the Earth’s circumference. Hargreave’s “Spinning Jenny” revolutionized textile manufacturing through a spatial insight: arranging multiple spindles side-by-side in a single frame allowed one worker to spin multiple threads simultaneously, increasing productivity eightfold. Watson and Crick discovered DNA’s structure by physically building 3D molecular models, manipulating metal plates and wire until the spatial arrangement of base pairs clicked into place. In each case, spatial intelligence drove civilization forward when scientists and inventors had to manipulate objects, visualize structures, and reason about physical spaces - none of which can be captured in text alone.</span></p></div><p><strong>Spatial Intelligence is the scaffolding upon which our cognition is built.</strong><span> It’s at work when we passively observe or actively seek to create. It drives our reasoning and planning, even on the most abstract topics. And it’s essential to the way we interact—verbally or physically, with our peers or with the environment itself. While most of us aren’t revealing new truths on the level of Eratosthenes most days, we </span><em>routinely</em><span> think in the same way—making sense of a complex world by perceiving it through our senses, then leveraging an intuitive understanding of how it works in physical, spatial terms.</span></p><div><p><span>Unfortunately, today’s AI doesn’t think like this yet.</span></p><p><span>Tremendous progress has indeed been made in the past few years. Multimodal LLMs (MLLMs), trained with voluminous multimedia data in addition to textual data, have introduced some basics of spatial awareness, and today’s AI can analyze pictures, answer questions about them, and generate hyperrealistic images and short videos. And through breakthroughs in sensors and haptics, our most advanced robots can begin to manipulate objects and tools in highly constrained environments.</span></p></div><p>Yet the candid truth is that AI’s spatial capabilities remain far from human level. And the limits reveal themselves quickly. State-of-the-art MLLM models rarely perform better than chance on estimating distance, orientation, and size—or “mentally” rotating objects by regenerating them from new angles. They can’t navigate mazes, recognize shortcuts, or predict basic physics. AI-generated videos—nascent and yes, very cool—often lose coherence after a few seconds.</p><p>While current state-of-the-art AI can excel at reading, writing, research, and pattern recognition in data, these same models bear fundamental limitations when representing or interacting with the physical world. Our view of the world is holistic—not just what we’re looking at, but how everything relates spatially, what it means, and why it matters. Understanding this through imagination, reasoning, creation, and interaction—not just descriptions—is the power of spatial intelligence. Without it, AI is disconnected from the physical reality it seeks to understand. It cannot effectively drive our cars, guide robots in our homes and hospitals, enable entirely new ways of immersive and interactive experiences for learning and recreation, or accelerate discovery in materials science and medicine.</p><p>The philosopher Wittgenstein once wrote that “the limits of my language mean the limits of my world.” I’m not a philosopher. But I know at least for AI, there is more than just words. Spatial intelligence represents the frontier beyond language—the capability that links imagination, perception and action, and opens possibilities for machines to truly enhance human life, from healthcare to creativity, from scientific discovery to everyday assistance.</p><p>So how do we build spatially-intelligent AI? What’s the path to models capable of reasoning with the vision of Eratosthenes, engineering with the precision of an industrial designer, creating with the imagination of a storyteller, and interacting with their environment with the fluency of a first responder?</p><p>Building spatially intelligent AI requires something even more ambitious than LLMs: world models, a new type of generative models whose capabilities of understanding, reasoning, generation and interaction with the semantically, physically, geometrically and dynamically complex worlds - virtual or real - are far beyond the reach of today’s LLMs. The field is nascent, with current methods ranging from abstract reasoning models to video generation systems. World Labs was founded in early 2024 on this conviction: that foundational approaches are still being established, making this the defining challenge of the next decade.</p><p><span>In this emerging field, what matters most is establishing the principles that guide development. For spatial intelligence, I define world models through </span><strong>three essential capabilities:</strong></p><p><span>World models that unlock spatial understanding and reasoning must also generate simulated worlds of their own. They must be capable of spawning endlessly varied and diverse simulated worlds that follow semantic or perceptual instructions—</span><em>while</em><span> remaining geometrically, physically, and dynamically consistent—whether representing real or virtual spaces. The research community is actively exploring whether these worlds should be represented implicitly or explicitly in terms of the innate geometric structures. Furthermore, in addition to powerful latent representations, I believe the outputs of a universal world model must also allow the generation of an explicit, observable state of the worlds for many different use cases. In particular, its understanding of the present must be tied coherently to its past; to the previous states of the world that led to the current one.</span></p><p><span>Just as animals and humans do, a world model should be able to process inputs—known as “prompts” in the generative AI realm—in a wide range of forms. Given partial information—whether images, videos, depth maps, text instructions, gestures, or actions—world models should predict or generate world states as </span><em>complete </em><span>as possible. This requires processing visual inputs with the fidelity of real vision while interpreting semantic instructions with equal facility. This enables both agents and humans to communicate with the model about the world through diverse inputs and receive diverse outputs in return.</span></p><p><span>Finally, if actions and/or goals are part of the prompt to a world model, its outputs must include the </span><em>next</em><span> state of the world, represented either implicitly or explicitly. When given only an action with or without a goal state as the input, the world model should produce an output consistent with the world’s previous state, the intended goal state if any, and its semantic meanings, physical laws, and dynamical behaviors. As spatially intelligent world models become more powerful and robust in their reasoning and generation capabilities, it is conceivable that in the case of a given goal, the world models themselves would be able to predict not only the next state of the world, but also the next actions based on the new state.</span></p><p><strong>The scope of this challenge exceeds anything AI has faced before.</strong></p><p>While language is a purely generative phenomenon of human cognition, worlds play by much more complex rules. Here on Earth, for instance, gravity governs motion, atomic structures determine how light produces colors and brightness, and countless physical laws constrain every interaction. Even the most fanciful, creative worlds are composed of spatial objects and agents that obey the physical laws and dynamical behaviors that define them. Reconciling all of this consistently—the semantic, the geometric, the dynamic, and physical—demands entirely new approaches. The dimensionality of representing a world is vastly more complex than that of a one-dimensional, sequential signal like language. Achieving world models that deliver the kind of universal capabilities we enjoy as humans will require overcoming several formidable technical barriers. At World Labs, our research teams are devoted to making fundamental progress toward that goal.</p><p>Here are some examples of our current research topics:</p><ul><li><p><strong>A new, universal task function for training: </strong><span>Defining a universal task function as simple and elegant as next-token prediction in LLMs has long been a central goal of world model research. The complexities of both their input and output spaces make such a function inherently more difficult to formulate. But while much remains to be explored, this objective function and corresponding representations must reflect the laws of geometry and physics, honoring the fundamental nature of world models as grounded representations of both imagination and reality.</span></p></li><li><p><strong>Large-scale training data</strong><span>:</span><strong> </strong><span>Training world models requires far more complex data than text curation. The promising news: massive data sources already exist. Internet-scale collections of images and videos represent abundant, accessible training material—the challenge lies in developing algorithms that can extract deeper spatial information from these two-dimensional image or video frame-based signals (i.e. RGB). Research over the past decade has shown the power of scaling laws linking data volume and model size in language models; the key unlock for world models is building architectures that can leverage existing visual data at comparable scale. In addition, I would not underestimate the power of high-quality synthetic data and additional modalities like depth and tactile information. They supplement the internet scale data in critical steps of the training process. But the path forward depends on better sensor systems, more robust signal extraction algorithms, and far more powerful neural simulation methods.</span></p></li><li><p><strong>New model architecture and representational learning: </strong><span>World model research will inevitably drive advances in model architecture and learning algorithms, particularly beyond the current MLLM and video diffusion paradigms. Both of these typically tokenize data into 1D or 2D sequences, which makes simple spatial tasks unnecessarily difficult - like counting unique chairs in a short video, or remembering what a room looked like an hour ago. Alternative architectures may help, such as 3D or 4D-aware methods for tokenization, context, and memory. For example, at World Labs, our recent work on a real-time generative frame-based model called RTFM has demonstrated this shift, which uses spatially-grounded frames as a form of spatial memory to achieve efficient real-time generation while maintaining persistence in the generated world.</span></p></li></ul><p>Clearly, we are still facing daunting challenges before we can fully unlock spatial intelligence through world modeling. This research isn’t just a theoretical exercise. It is the core engine for a new class of creative and productivity tools. And the progress within World Labs has been encouraging. We recently shared with a limited number of users a glimpse of Marble, the first ever world model that can be prompted by multimodal inputs to generate and maintain consistent 3D environments for users and storytellers to explore, interact with, and build further in their creative workflow. And we are working hard to make it available to the public soon!</p><p>Marble is only our first step in creating a truly spatially intelligent world model. As the progress accelerates, researchers, engineers, users, and business leaders alike are beginning to recognize its extraordinary potential. The next generation of world models will enable machines to achieve spatial intelligence on an entirely new level—an achievement that will unlock essential capabilities still largely absent from today’s AI systems.</p><p><strong>It matters what motivates the development of AI. </strong><span>As one of the scientists who helped usher in the era of modern AI, my motivation has always been clear: AI must augment human capability, not replace it. For years, I’ve worked to align AI development, deployment, and governance with human needs. Extreme narratives of techno-utopia and apocalypse are abundant these days, but I continue to hold a more pragmatic view: AI is developed by people, used by people, and governed by people. It must always respect the agency and dignity of people. Its magic lies in extending our capabilities; making us more creative, connected, productive, and fulfilled. Spatial intelligence represents this vision—AI that empowers human creators, caregivers, scientists, and dreamers to achieve what was once impossible. This belief is what drives my commitment to spatial intelligence as AI’s next great frontier. </span></p><p>The applications of spatial intelligence span varying timelines. Creative tools are emerging now—World Labs’ Marble already puts these capabilities in creators’ and storytellers’ hands. Robotics represents an ambitious mid-term horizon as we refine the loop between perception and action. The most transformative scientific applications will take longer but promise a profound impact on human flourishing.</p><p>Across all these timelines, several domains stand out for their potential to reshape human capability. It will take significant collective effort, more than a single team or a company can possibly achieve. It will require participation across the entire AI ecosystem—researchers, innovators, entrepreneurs, companies, and even policymakers—working toward a shared vision. But this vision is worth pursuing. Here’s what that future holds:</p><p>“Creativity is intelligence having fun.” This is one of my favorite quotes by my personal hero Albert Einstein. Long before written language, humans told stories—painted them on cave walls, passed them through generations, built entire cultures on shared narratives. Stories are how we make sense of the world, connect across distance and time, explore what it means to be human, and most importantly, find meaning in life and love within ourselves. Today, spatial intelligence has the potential to transform how we create and experience narratives in ways that honor their fundamental importance, and extend their impacts from entertainment to education, from design to construction.</p><p>World Labs’ Marble platform will be putting unprecedented spatial capabilities and editorial controllability in the hands of filmmakers, game designers, architects, and storytellers of all kinds, allowing them to rapidly create and iterate on fully explorable 3D worlds without the overhead of conventional 3D design software. The creative act remains as vital and human as ever; the AI tools simply amplify and accelerate what creators can achieve. This includes:</p><ul><li><p>Narrative experiences in new dimensions: Filmmakers and game designers are using Marble to conjure entire worlds without the constraints of budget or geography, exploring varieties of scenes and perspectives that would have been intractable to explore within a traditional production pipeline. As the lines between different forms of media and entertainment blur, we’re approaching fundamentally new kinds of interactive experiences that blend art, simulation, and play—personalized worlds where anyone, not just studios, can create and inhabit their own stories. With the rise of newer, more rapid ways to lift concepts and storyboards into full experiences, narratives will no longer be bound to a single medium, with creators free to build worlds with shared throughlines across myriad surfaces and platforms.</p></li><li><p>Spatial narratives through design: Essentially every manufactured object or constructed space must be designed in virtual 3D before its physical creation. This process is highly iterative and costly in terms of both time and money. With spatially intelligent models at their disposal, architects can quickly visualize structures before investing months into designs, walking through spaces that don’t yet exist—essentially telling stories about how we might live, work, and gather. Industrial and fashion designers can translate imagination into form instantly, exploring how objects interact with human bodies and spaces.</p></li><li><p>New immersive and interactive experiences: Experience itself is one of the deepest ways that we, as a species, create meaning. For the entirety of human history, there has been one singular 3D world: the physical one we all share. Only in recent decades, through gaming and early virtual reality ( VR), have we begun to glimpse what it means to share alternate worlds of our own creation. Now, spatial intelligence combined with new form factors, like VR and extended reality (XR) headsets and immersive displays, elevates these experiences in unprecedented ways. We’re approaching a future where stepping into fully realized multi-dimensional worlds becomes as natural as opening a book. Spatial intelligence makes world-building accessible not just to studios with professional production teams but to individual creators, educators, and anyone with a vision to share.</p></li></ul><p>Animals from insects to humans depend on spatial intelligence to understand, navigate and interact with their worlds. Robots will be no different. Spatially-aware machines have been the dream of the field since its inception, including my own work with my students and collaborators at my Stanford research lab. This is also why I’m so excited by the possibility of bringing them about using the kinds of models World Labs is building.</p><ul><li><p><strong>Scaling robotic learning via world models:</strong><span> The progress of robotic learning hinges on a scalable solution of viable training data. Given the enormous state spaces of possibilities that robots have to learn to understand, reason, plan, and interact with, many have conjectured that a combination of internet data, synthetic simulation, and real-world capture of human demonstration are required to truly create generalizable robots. But unlike language models, training data is scarce for today’s robotic research. World models will play a defining role in this. As they increase their perceptual fidelity and computational efficiency, outputs of world models can rapidly close the gap between simulation and reality. This will in turn help train robots across simulations of countless states, interactions and environments.</span></p></li><li><p><strong>Companions and collaborators: </strong><span>Robots as human collaborators, whether aiding scientists at the lab bench or assisting seniors living alone, can expand part of the workforce in dire need of more labour and productivity. But doing so demands spatial intelligence that perceives, reasons, plans, and acts while—and this is most important—staying empathetically aligned with human goals and behaviors. For instance, a lab robot might handle instruments so the scientist can focus on tasks needing dexterity or reasoning, while a home assistant might help an elderly person cook without diminishing their joy or autonomy. Truly spatially intelligent world models that can predict the next state or possibly even actions consistent with this expectation are critical for achieving this goal.</span></p></li><li><p><strong>Expanding forms of embodiment: </strong><span>Humanoid robots play a role in the world we’ve built for ourselves. But the full benefit of innovation will come from a far more diverse range of designs: nanobots that deliver medicine, soft robots that navigate tight spaces, and machines built for the deep sea or outer space. Whatever their form, future spatial intelligence models must integrate both the environments these robots inhabit and their own embodied perception and movement. But a key challenge in developing these robots is the lack of training data in these wide varieties of embodied form factors. World models will play a critical role in simulation data, training environments, and benchmarking tasks for these efforts.</span></p></li></ul><p>In addition to creative and robotics applications, spatial intelligence’ profound impact will also extend to fields where AI can enhance human capability in ways that save lives and accelerate discovery. I highlight below three areas of applications that can be deeply transformative, though it goes without saying the use cases of spatial intelligence are truly expansive across many more industries.</p><p><span>In </span><strong>scientific research,</strong><span> spatially intelligent systems can simulate experiments, test hypotheses in parallel, and explore environments inaccessible to humans—from deep oceans to distant planets. This technology can transform computational modeling in fields like climate science and materials research. By integrating multi-dimensional simulation with real-world data collection, these tools can lower compute barriers and extend what every laboratory can observe and understand.</span></p><p><span>In </span><strong>healthcare</strong><span>, spatial intelligence will reshape everything from laboratory to bedside. At Stanford, my students and collaborators have spent many years working with hospitals, elder care facilities, and patients at home. This experience has convinced me of spatial intelligence’s transformative potential here. AI can accelerate drug discovery by modeling molecular interactions in multi-dimensions, enhance diagnostics by helping radiologists spot patterns in medical imaging, and enable ambient monitoring systems that support patients and caregivers without replacing the human connection that healing requires, not to mention the potential of robots in helping our healthcare workers and patients in many different settings.</span></p><p><span>In </span><strong>education,</strong><span> spatial intelligence can enable immersive learning that makes abstract or complex concepts tangible, and create iterative experiences so essential to how our brains and bodies are wired in learning. In the age of AI, the need for faster and more effective learning and reskilling is particularly important for both school-aged children and adults. Students can explore cellular machinery or walk through historical events in multi-dimenality. Teachers gain tools to personalize instruction through interactive environments. Professionals—from surgeons to engineers—can safely practice complex skills in realistic simulations.</span></p><p>Across all these domains, the possibilities are boundless, but the goal remains constant: AI that augments human expertise, accelerates human discovery, and amplifies human care—not replacing the judgment, creativity, and empathy that are central for being humans.</p><p>The last decade has seen AI become a global phenomenon and an inflection point in technology, the economy, and even geopolitics. But as a researcher, educator, and now, entrepreneur, it’s still the spirit behind Turing’s 75-year-old question that inspires me most. I still share his sense of wonder. It’s what energizes me every day by the challenge of spatial intelligence.</p><p>For the first time in history, we’re poised to build machines so in tune with the physical world that we can rely on them as true partners in the greatest challenges we face. Whether accelerating how we understand diseases in the lab, revolutionizing how we tell stories, or supporting us in our most vulnerable moments due to sickness, injury, or age, we’re on the cusp of technology that elevates the aspects of life we care about most. This is a vision of deeper, richer, more empowered lives.</p><p>Almost a half billion years after nature unleashed the first glimmers of spatial intelligence in the ancestral animals, we’re lucky enough to find ourselves among the generation of technologists who may soon endow machines with the same capability—and privileged enough to harness those capabilities for the benefits of people everywhere. Our dreams of truly intelligent machines will not be complete without spatial intelligence.</p><p><span>This quest is my North Star. </span><a href="https://www.worldlabs.ai/" rel="">Join me</a><span> in pursuing it.</span></p><p data-attrs="{&quot;url&quot;:&quot;https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;action&quot;:null,&quot;class&quot;:null}" data-component-name="ButtonCreateButton"><a href="https://drfeifei.substack.com/p/from-words-to-worlds-spatial-intelligence?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share" rel=""><span>Share</span></a></p></div></article></div><div id="discussion"><h4>Discussion about this post</h4></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Using Generative AI in Content Production (174 pts)]]></title>
            <link>https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production</link>
            <guid>45879793</guid>
            <pubDate>Mon, 10 Nov 2025 19:28:07 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production">https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production</a>, See on <a href="https://news.ycombinator.com/item?id=45879793">Hacker News</a></p>
<div id="readability-page-1" class="page"><div role="main" id="content">

        <p><br><a href="#h_01K39BXBFJKBC2HEXRP00QCG8J"><em>Skip to Translations</em></a></p><h2 id="h_01K1BTNMB992RYKRQRNKWD40A6">
    <span><span><strong>Introduction&nbsp;</strong></span></span>
  </h2><p><span>Generative AI tools (GenAI)&nbsp; that allow users to rapidly generate new and creatively unique media (video, sound, text, and image) are increasingly being used across creative workflows in Content Production. At Netflix, we see these tools as valuable creative aids when used transparently and responsibly.</span></p><p>
    <span>This guidance helps filmmakers, production partners, and vendors understand when and how to use GenAI tools in production. It also offers</span><a href="#h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ"><span><span><span> a practical tool</span></span></span></a><span> for assessing and enabling confident GenAI use when producing content for Netflix.&nbsp;</span>
  </p><p><span>To support global productions and stay aligned with best practices, we expect all production partners to share any intended use of GenAI with their Netflix contact, especially as new tools continue to emerge with different capabilities and risks.&nbsp;</span></p><p><span>Most low-risk use cases that follow the guiding principles below are unlikely to require legal review. However, if the output includes final deliverables, talent likeness, personal data, or third-party IP, written approval will be required before you proceed.</span></p><hr><h2 id="h_01K1BTQC952EKYY3T7SQ98G4NJ">
    <span><span><strong>TABLE OF CONTENTS</strong></span></span>
  </h2>
  <p>
    <a href="#h_01K1BTNMBC8DKAF1607XQ3S9AK"><span><span>Guiding Principles</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBHW1JSCHK4914BAXDE"><span><span>What use cases always require written approval?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBHB5GW4FJW8X64KHNN"><span><span>1. Data Use</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBKMP99CP2PCXZ8W3H5"><span><span>2. Creative Output</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBP76KFGEWAFWAH22TA"><span><span>3. Talent &amp; Performance</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBRSBGNTCCTY9DBXMV2"><span><span>4. Ethics &amp; Representation</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBS130Y200ZWV3H6ZAT"><span><span>How can I ensure confidentiality and data protection?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBVFQYQNJCCMKR254VK"><span><span>Are the considerations different for final output vs temporary media?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMBWWPTJJA79EFPY8NRJ"><span><span>What should we consider before using GenAI for talent enhancement?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC21630W4ZWFFS0EYP2"><span><span>What if I’m using a custom workflow or working with a vendor who is?</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC3K7ECQKP84CDSQVZG"><span><span>Appendix</span></span></a>
  </p>
  <p>
    <a href="#h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ"><span><span>Proposed Use Case Matrix</span></span></a>
  </p><hr><h2 id="h_01K1BTNMBC8DKAF1607XQ3S9AK">
    <span><span><strong>Guiding Principles&nbsp;</strong></span></span>
  </h2><p><span>Given the sensitivities surrounding the use of these tools and the evolving legal landscape, it is essential to act responsibly when employing generative workflows. Netflix asks partners to consider the following guiding principles before leveraging GenAI in any creative workflow:&nbsp;</span></p><ol>
    <li>
      <span>The outputs do not replicate or substantially recreate identifiable characteristics of unowned or copyrighted material, or infringe any copyright-protected works</span>
    </li>
    <li>
      <span>The generative tools used do not store, reuse, or train on production data inputs or outputs.</span>
    </li>
    <li>
      <span>Where possible, generative tools are used in an </span><a href="#h_01K1BTNMBS130Y200ZWV3H6ZAT"><span><span><span>enterprise-secured environment</span></span></span></a><span> to safeguard inputs.</span>
    </li>
    <li>
      <span>Generated material is temporary and not part of the</span><a href="#h_01K1BTNMBVFQYQNJCCMKR254VK"><span> </span><span><span><span>final deliverables</span></span></span></a><span>.</span>
    </li>
    <li>
      <span>GenAI is not used to replace or generate new </span><a href="#h_01K1BTNMBWWPTJJA79EFPY8NRJ"><span><span><span>talent performances</span></span></span><span><span> </span></span></a><span>or union-covered work without consent.</span>
    </li>
  </ol><p><span>If you can confidently say "yes" to all the above, socializing the intended use with your Netflix contact may be sufficient. If you answer “no” or “unsure” to any of these principles, escalate to your Netflix contact for more guidance before proceeding, as written approval may be required.&nbsp;</span></p><p>
    <span>If your partner vendor is using a custom GenAI workflow — meaning a pipeline built from multiple tools or models — the same principles apply. More details can be found </span><a href="#h_01K1BTNMC21630W4ZWFFS0EYP2"><span><span><span>here</span></span></span></a><span>.&nbsp;</span>
  </p><hr><h2 id="h_01K1BTNMBHW1JSCHK4914BAXDE">
    <span><span><strong>What use cases always require written approval?</strong></span></span>
  </h2><p><span>Below are a few examples of situations that, in addition to reporting intended use, always require escalation and written approval before proceeding.&nbsp;</span></p><h4 id="h_01K1BTNMBHB5GW4FJW8X64KHNN">
    <span><span><strong>1. Data Use&nbsp;</strong></span></span>
  </h4><p><span>Protecting personal data and creative rights is essential when working with GenAI. These tools often require input data to generate outputs, and how that data is handled matters. Before using any GenAI tool, especially third-party or off-the-shelf options, consider whether you are using material that requires special handling, clearance, or consent.</span></p><ul>
<li data-list-item-id="eadab2bf1f9f6c9e08da9a9e4c1e9e80b"><span>Use of Proprietary or Personal Information: Do not input Netflix-owned materials (e.g., unreleased assets, scripts, production images) or personal data (e.g., cast or crew details) into tools unless explicitly approved.</span></li>
<li data-list-item-id="eebe54d8bab43f122ac308fb7d5628a48"><span>Third-Party or Unowned Talent Assets: Do not train or fine-tune models using material from artists, performers, or other rights holders unless you have the proper legal clearance.</span></li>
</ul><p><span>Example: Training an image model in the style of another artist using a library of their past work, where Netflix or the talent has not cleared rights.</span></p><h4 id="h_01K1BTNMBKMP99CP2PCXZ8W3H5">
    <span><span><strong>2. Creative Output&nbsp;&nbsp;</strong></span></span>
  </h4><p><span>AI-generated content must be used with care, especially when it forms a visible or story-critical part of the production. Whether you're designing a world, a character, or artwork that appears in a scene, the same creative and legal standards apply as with traditionally produced assets.</span></p><ul>
    <li>
      <span>Generation of Key Creative Elements: GenAI should not be used to generate main characters, key visual elements, or fictional settings that are central to the story without written approval.</span>
      <ul>
        <li>
          <span>Examples: GenAI is used to generate a second killer doll to play the red light/green light game with Young-hee in Squid Game.</span>
        </li>
      </ul>
    </li>
    <li>
      <span>Copyrighted or Estate-Controlled: Avoid using inputs (e.g., prompts, images) that reference copyrighted materials or likenesses of public figures or deceased individuals without appropriate permissions.</span>
      <ul>
        <li>
          <span>Example: “Create an image inspired by </span><a href="https://www.stevemccurry.com/posters/p/afghan-girl"><span><span><span>McCurry’s Afghan Girl</span></span></span></a><span>” or referencing distinctive features of a known performer (e.g., “Create a character with Meryl Streep’s nose”).</span>
        </li>
      </ul>
    </li>
  </ul><h4 id="h_01K1BTNMBP76KFGEWAFWAH22TA">
    <span><span><strong>3. Talent &amp; Performance&nbsp;</strong></span></span>
  </h4><p><span>Respect for performers and their work is foundational to the responsible use of GenAI. Whether enhancing a recorded performance or generating a digital likeness, the threshold for consent and care is exceptionally high when the intent or character of a performance may be altered.</span></p><ul>
<li data-list-item-id="e4d40cb1df5caa3a3682badc50d00f86f"><span>Synthetic or Digital Replicas - Do not create digital performers, voices, or likenesses of real talent without explicit and documented consent and complying with guild requirements (where applicable).</span></li>
<li data-list-item-id="edbd78f2c674b7bf80443280ebdfc5961"><span>Significant Digital Alterations to Performances - Be cautious when making changes that affect a performance's emotional tone, delivery, or intent, as even subtle edits may have legal or reputational implications.</span></li>
</ul><p><span>Examples include visual ADR (altering lip-sync or facial performance to match new, unscripted dialogue).</span></p><h4 id="h_01K1BTNMBRSBGNTCCTY9DBXMV2">
    <span><span><strong>4. Ethics &amp; Representation</strong></span></span>
  </h4><p><span>Audiences should be able to trust what they see and hear on screen. GenAI (if used without care) can blur the line between fiction and reality or unintentionally mislead viewers. That’s why we ask you to consider both the intent and the impact of your AI-generated content.</span></p><ul>
<li data-list-item-id="e888ce79e22790a32566543967d72f687">
<span>Misleading or Misrepresentative Content: Avoid creating content that could be mistaken for real events, people, or statements if they never actually occurred (e.g., fabricated footage, dialogue, or scenes presented as authentic).</span><ul><li data-list-item-id="ed46505213321fa75e536d07d6eb5161d"><span>Example: using GenAI to create a fake news segment featuring a real journalist delivering a fabricated statement, even if intended as background.</span></li></ul>
</li>
<li data-list-item-id="ee334b51a1afff56655a98d7ab8cd583e"><span>Impact on Union Roles: Ensure that your use of GenAI does not replace or materially impact work typically done by union-represented individuals, including actors, writers, or crew members, without proper approvals or agreements.</span></li>
</ul><hr><h2 id="h_01K1BTNMBS130Y200ZWV3H6ZAT">
    <span><span><strong>How can I ensure confidentiality and data protection?</strong></span></span>
  </h2><p><span>The use of tools covered by Netflix Enterprise Agreements provides an additional level of security to protect input data. Speak with your Netflix primary contact about available tools and the onboarding process. These tools:</span></p><ul>
<li data-list-item-id="e68332c5bcee6dff16288681e7b954f32"><span>Prevent capture, training, or resale of your inputs</span></li>
<li data-list-item-id="ee904f7888a03c5358236991d6cc536f9"><span>Protect sensitive inputs like scripts, production images, or talent visuals</span></li>
</ul><p><span>Even with secure tools, any use of sensitive information (e.g., talent likeness, unreleased footage, contracts) requires escalation to your Netflix contact.</span></p><p><span>When not using enterprise tools, ensure that any AI tools, plugins, or workflows you use do not train on inputs or outputs, as using the wrong license tier or missing pre-negotiated data terms could compromise confidentiality. You are responsible for reviewing the terms and conditions (T&amp;Cs). Please check with your Netflix contact if you have any further questions.</span></p><h2 id="h_01K1BTNMBVFQYQNJCCMKR254VK">
    <span><span><strong>Are the considerations different for final output vs temporary media?</strong></span></span>
  </h2><p><span>If created with GenAI, content that appears in the final cut—even in the background—can raise legal, copyright, or trust issues with the audience. That’s why we ask you to flag any GenAI-generated elements early, especially if they will be seen or heard on screen.</span></p><p><span>If your proposed use case includes visual, audio, or text elements generated by AI (e.g., posters, documents, signage, or news clippings), contact your Netflix representative as early as possible for legal guidance. These items may require rights clearance before they can be included in final deliverables.</span></p><p><span>Some GenAI-generated props or set pieces may be considered incidental, for example, a historical document shown briefly in the background and not referenced in the scene. However, if the element is prominent (e.g., a character reads it aloud or it contributes to the story), it must be treated with greater care.</span></p><p><span>In these cases, you can use GenAI to explore ideas or mockups. Still, the final version should involve meaningful human input and follow the legal review process through your Netflix contact.</span></p><hr><h2 id="h_01K1BTNMBWWPTJJA79EFPY8NRJ">
    <span><span><strong>What should we consider before using GenAI for talent enhancement?</strong></span></span>
  </h2><p><span>There is a long tradition of digitally altering performances in post-production and VFX. However, the use of AI to modify or replicate a performer's likeness or voice introduces new legal, ethical, and reputational challenges. Therefore, obtaining consent when appropriate and exercising caution are crucial. Many talent enhancement use cases require legal review, so please plan accordingly. Here are some guidelines to consider:&nbsp;</span></p><ul><li data-list-item-id="ee650de448e890ec3259cea9e3f215d7e"><span>If creating a Digital Replica (i.e., a generated output recognizable as the voice and/or likeness of an identifiable performer for the purpose of portraying them in photography or soundtrack, they did not perform), consent is required. No further consent is needed to use the Digital Replica if the performance output: (1)&nbsp; remains substantially as scripted, performed, or recorded (e.g. reshoots); (2) depicts activities incapable of being performed by a human for safety reasons; or (3) results in the performer being unrecognizable (e.g. wearing a mask).</span></li></ul><ul><li data-list-item-id="eb553309b5b4c9280624005c8b8dc3be7">
<span>Digital Alterations: Consent is generally required for digital alterations, except for those customarily done in the entertainment and film industry, such as:</span><ul>
<li data-list-item-id="eabb77a1e3013a0ba03166f275032945d"><span>Alterations where the photography or soundtrack remains substantially as scripted, performed, or recorded.</span></li>
<li data-list-item-id="efca531cce7e6caef547165f10ea1d4b8"><span>Post-production changes for cosmetics, wardrobe, noise reduction, timing, continuity, pitch, clarity, and similar purposes.</span></li>
<li data-list-item-id="eff26a574f34290ce1b409f9286abfc71"><span>Circumstances where dubbing or using a double is permitted under existing agreements.</span></li>
</ul>
</li></ul><ul>
<li data-list-item-id="e8933fe8b834666f88ca2ce7cbdcac1e3">
<span>Model Usage:</span><ul>
<li data-list-item-id="ed72a3c2d1a0ab668c016f69220395448"><span>Any models trained to perform talent enhancement manipulation should be used solely for the production in question and within the scope of work agreed upon with the talent.</span></li>
<li data-list-item-id="e6803efcc74ee2497779461493a153e3b"><span>Models must not be used to create an actor's performance in another production, pitch, or concept without the express consent of all parties involved.</span></li>
</ul>
</li>
<li data-list-item-id="e77b0d872f73d494f71a7f67478e8f380">
<span>Quality Assurance:</span><ul>
<li data-list-item-id="e53f9b4c6a212869163f0a58c2be99a55"><span>Perform early tests to ensure that the quality of the outputs is acceptable both creatively and technically, so as not to adversely affect the talent’s original performance.</span></li>
<li data-list-item-id="ed9502f1976cbd6ba189c9683079d054a"><span>Where applicable and practical, plan dedicated data capture sessions with the talent to ensure the best possible outcomes.</span></li>
<li data-list-item-id="ee61ab0b5412c6f71ec8bcbc931106d3d"><span>Avoid enhancements that could harm the actor’s reputation, dignity, or personal image.</span></li>
</ul>
</li>
</ul><p><span>By following these guidelines, you can navigate the complexities of using AI in creative workflows while respecting the rights and integrity of performers.</span></p><hr><h2 id="h_01K1BTNMC21630W4ZWFFS0EYP2">
    <span><span><strong>What if I’m using a custom workflow or working with a vendor who is?</strong></span></span>
  </h2><p><span>For vendors: If you're delivering work to Netflix using a custom GenAI workflow built from multiple tools, each step in the pipeline must meet our standards for data protection, consent, and content integrity as outlined in this document.&nbsp;</span></p><p><span>For production partners: If you're hiring a vendor or AI studio, use this guidance as a framework to help assess how they manage data, creative control, and final outputs. If you are unsure whether the pipeline meets the expectations outlined in this guidance, seek guidance from your Netflix contact.&nbsp;</span></p><hr><h2 id="h_01K1BTNMC3K7ECQKP84CDSQVZG">
    <span><span><strong>Appendix</strong></span></span>
  </h2>
  <h3 id="h_01K1BTNMC4RTXXMXPKW2TJJ2ZJ">
    <span><span><strong>Proposed Use Case Matrix</strong></span></span>
  </h3><p><span>We have provided a&nbsp; Proposed Use Case Matrix at the end of this guidance as a tool to triage your proposed use case quickly.&nbsp;</span></p><div><figure><table>
<colgroup>
<col>
<col>
<col>
</colgroup>
<tbody>
<tr>
<td><span><strong>Proposed Use Case</strong></span></td>
<td><span><strong>Action&nbsp;</strong></span></td>
<td><span><strong>Rationale</strong></span></td>
</tr>
<tr>
<td><span>Using GenAI for ideation only (moodboards, reference images)</span></td>
<td><span>✅</span></td>
<td><span>Low risk, non-final, likely not needing escalation if guiding principles are followed.</span></td>
</tr>
<tr>
<td><span>Using GenAI to generate background elements (e.g., signage, posters) that appear on camera</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813746061587" alt=":warning:" width="23" height="23"></span></td>
<td><span>Use judgment: Incidental elements may be low risk, but if story-relevant, please escalate.&nbsp;</span></td>
</tr>
<tr>
<td><span>Using GenAI to create final character designs or key visuals</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23">&nbsp;</span></td>
<td><span>Requires escalation as it could impact legal rights, audience perception, or union roles.</span></td>
</tr>
<tr>
<td><span>Using GenAI for talent replication (re-ageing, or synthetic voices)</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23"></span></td>
<td><span>Requires escalation for consent and legal review.&nbsp;</span></td>
</tr>
<tr>
<td><span>Using unowned&nbsp; training data (e.g., celebrity faces, copyrighted art)</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813724428819" alt=":octagonal_sign:" width="23" height="23"></span></td>
<td><span>Needs escalation due to copyright and other rights risk.</span></td>
</tr>
<tr>
<td><span>Using Netflix's proprietary material</span></td>
<td><span><img src="https://partnerhelp.netflixstudios.com/hc/article_attachments/43813746061587" alt=":warning:" width="23" height="23"></span></td>
<td>
<p><span>Needs escalation for review if outside secure enterprise tools.</span></p>

</td>
</tr>
</tbody>
</table></figure></div><h3 id="h_01K39BXBFJKBC2HEXRP00QCG8J">
  <br>
  <span><span><strong>Translations</strong></span></span>
</h3><p><a href="https://drive.google.com/file/d/1OhOJXv6cwcc8Ob61K_zuxOxxzJst5V-V/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Español (Latinoamérica)</span></a></p><p><a href="https://drive.google.com/file/d/1PcVBUivo-CYSIj_fXMjPwS9lh6WC9WlT/view?usp=sharing" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Français</span></a></p><p><a href="https://drive.google.com/file/d/16v2PFCYKk08s0o37kHJ4LZmr6n5cdxia/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Português</span></a></p><p><a href="https://drive.google.com/file/d/122a0P_EASxSQ2CklK2Tnm6N4CjrbC92h/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">ไทย</span></a></p><p><a href="https://drive.google.com/file/d/1Gr8ml-b9QSoWLkZkN18p03uNVQs5Aj2X/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">Türkçe</span></a></p><p><a href="https://drive.google.com/file/d/1wU1Q6zTd7zt7G7umgtk2VRepkGAwruA8/view?usp=drive_link" target="_blank" rel="noopener noreferrer"><span data-sheets-root="1">繁體中文</span></a></p>
        
        
        <section>

          <span>Was this article helpful?</span>
          
          <small>
            <span data-helper="vote" data-item="article" data-type="label" data-id="43393929218323" data-upvote-count="121" data-vote-count="128" data-vote-sum="114" data-vote-url="/hc/en-us/articles/43393929218323/vote" data-value="null" data-label="121 out of 128 found this helpful">121 out of 128 found this helpful</span>
          </small>
        </section>
        
   
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Redmond, WA, turns off Flock Safety cameras after ICE arrests (349 pts)]]></title>
            <link>https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/</link>
            <guid>45879101</guid>
            <pubDate>Mon, 10 Nov 2025 18:30:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/">https://www.seattletimes.com/seattle-news/law-justice/redmond-turns-off-flock-safety-cameras-after-ice-arrests/</a>, See on <a href="https://news.ycombinator.com/item?id=45879101">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="article-content">
    <p>Redmond police turned off their city’s Flock Safety cameras last week after masked, unidentified officers — later identified as U.S. Immigration and Customs Enforcement agents — arrested seven people, at least three less than a mile from  one or more of the city’s cameras.</p><p>During a City Council session Oct. 27, Redmond police Chief Darrell Lowe said no federal agency had accessed the city’s Flock data, but agreed to suspend officers’ access to the system until city officials had discussed ending Redmond’s contract with the company.&nbsp;</p><p>Redmond City Council member Angie Nuevacamina said Thursday the proximity of the arrests to some of the city’s cameras was coincidental, and not because ICE had “somehow tapped into” Redmond’s Flock cameras or data. The city suspended its Flock system because city officials could not guarantee they wouldn’t be forced to release data collected by those devices someday, she said.</p><p>Their concerns may have been prescient.</p><p>On Thursday, a Skagit County Superior Court judge ruled that pictures taken by Flock cameras in the cities of Sedro-Woolley and Stanwood qualify as public records, and therefore must be released as required by the state’s Public Records Act, court records show<strong>.</strong></p><p>Flock’s cameras, also called automated license plate readers, continuously and indiscriminately capture time- and location-stamped photos of any passing vehicles. Those images are then stored, and information about the vehicles, including their condition, make, model and license plate number, is added to a searchable database controlled by the customer.</p><p>Last week’s Skagit County ruling could oblige the dozens of Washington police agencies which use Flock cameras, ostensibly to help them find stolen vehicles, crime suspects and missing people, to release the photos and data they collect — an outcome privacy advocates warned was possible.</p>
<div>
      <h3>
        Related
        
      </h3>
      

      
        
    

      </div><p>The ruling also exacerbated concerns about potential misuse of Flock data, which swelled after University of Washington researchers released <a href="https://www.seattletimes.com/seattle-news/law-justice/feds-searched-flock-security-systems-at-18-wa-police-agencies-report/">a report Oct. 21</a> showing federal immigration agencies like ICE and Border Patrol had accessed the data of at least 18 Washington cities, often without their police departments’ knowing. The report raised concerns that the agencies might be using the data to target and arrest immigrants as part of Trump’s immigration crackdown.</p><p>Redmond was the latest in a string of Flock-using, Seattle-area cities to <a href="https://www.seattletimes.com/seattle-news/law-justice/feds-searched-flock-security-systems-at-18-wa-police-agencies-report/">change their surveillance programs</a> in the last three weeks in response to those concerns.</p><p>Police officials in Renton, Auburn, Mukilteo and Lakewood, Pierce County, changed their Flock settings after the UW report showed federal agencies had accessed their Flock data. The agencies never requested permission to access their cities’ data, and the respective police departments weren’t aware it happened until after UW researchers notified them or they saw the report, the officials said.</p><p>Redmond’s Police Department was not among those listed in the report, and has never allowed external agencies to access their Flock data without requesting and receiving permission from the police chief first, according to an <a href="https://www.redmond.gov/CivicAlerts.aspx?AID=2698" target="_blank">Oct. 24 statement</a> by Lowe.</p><p>But concerns about the cameras, and potential misuse of the data they collect, still swirled among Redmond residents and City Council members after photos and videos began circulating online last Monday of masked, unidentified officers emerging from unmarked cars and arresting people in town.</p><p>Three arrests by ICE at Redmond’s Bear Creek Village shopping center, the parking lot of a Home Depot and near the intersection of Avondale Road Northeast and Novelty Hill Road Northeast all happened within a half-mile of at least one of Redmond’s 24 Flock cameras, according to a partial list of their locations provided through a public disclosure request.</p>
<p>The city installed most of its cameras on Redmond’s “main thoroughfares” in May and began using them in June, according to Nuevacamina and Redmond police spokesperson Jill Green. </p><p>Though Lowe confirmed no federal agencies had accessed Redmond’s Flock system, Nuevacamina said residents’ and city officials’ concerns about the technology were still strong enough to support turning the cameras off. The step was also one of the few things city officials could do to help residents who felt powerless that day, as Redmond’s police officers were not allowed to intervene in ICE activity, and residents could not either without risking arrest or their own personal safety, she said.</p><p>Turning off Redmond’s Flock cameras was what the city had in its “tool belt to be able to protect and stand up for (their) community,” Nuevacamina said.</p><p>In a statement Saturday, Tricia McLaughlin, the Department of Homeland Security’s assistant secretary, confirmed ICE agents arrested seven people Nov. 3 in Redmond. McLaughlin’s statement did not identify those arrested but accused all of them of being in the country illegally.</p><p>Flock Safety is communicating with Redmond city officials to address their concerns and hopefully convince them to reverse their decision, the company’s chief legal officer, Dan Haley, said in a phone call Friday. The company is also advocating for legislation that would prevent people from “taking advantage of” Washington’s public records law, Haley said.</p><p>Flock can be made to release data collected by its technology through a subpoena or court order, Haley said, but the company would not do so without notifying and involving its customer first. The cameras also only capture what anyone could see on a public road, where there is no legal expectation of privacy, he said.  </p>
<p>Communities must balance concerns they have about Flock cameras against what Haley called the “very real outcomes” of using the technology: “these are kidnapped kids returned home, elderly people with dementia found quickly.”</p><p>For now, all Redmond police officers’ access to the city’s Flock data has been turned off. Two Police Department employees can access the data, but only to fulfill public records requests, and two police administrators can access — but not search — the data. A “disconnect signal” was also sent Tuesday morning to all of the cameras, which will stop them from taking and storing photos, Redmond police officials said.</p><p>During the Oct. 27 City Council session, Lowe said he would meet with Redmond Mayor Angela Birney Nov. 18 to “see where we go from here.”</p><p>Redmond City Council President Vanessa Kristzer said she had “grave concerns” about continuing to use Flock’s technology.</p><p>“We will continue to explore every avenue to be able to keep our community members safe and their dignity and human rights protected,” Kristzer said.</p>    
        <div>
   <p><span>Catalina Gaitán</span>:       <span>206-464-8276</span> or <span><a href="mailto:cgaitan@seattletimes.com">cgaitan@seattletimes.com</a></span>. <span>Catalina Gaitán is a breaking news reporter at The Seattle Times.</span>   </p>
</div>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Omnilingual ASR: Advancing automatic speech recognition for 1600 languages (146 pts)]]></title>
            <link>https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1</link>
            <guid>45878826</guid>
            <pubDate>Mon, 10 Nov 2025 18:10:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1">https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/?_fb_noscript=1</a>, See on <a href="https://news.ycombinator.com/item?id=45878826">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div data-scoped-css="fullscreen-hero-scoped-u_0_0_Qb" id="u_0_4_nK"><p>Open Source</p><p>November 10, 2025</p></div><div><div><p>Takeaways:</p><div><ul><li>We’re introducing <a href="https://github.com/facebookresearch/omnilingual-asr" target="_blank" data-lnfb-mode="ie"><u>Meta Omnilingual Automatic Speech Recognition (ASR)</u></a>, a suite of models providing automatic speech recognition capabilities for more than 1,600 languages, achieving state-of-the-art quality at an unprecedented scale.</li><li>Omnilingual ASR was designed as a community-driven framework. People around the world can extend Omnilingual ASR to new languages by using just a few of their own samples.</li><li>We’re also releasing the Omnilingual ASR Corpus, an extensive collection of transcribed speech in 350 underserved languages; Omnilingual wav2vec 2.0, a scaled up massively multilingual speech representation model; and a <a href="https://aidemos.atmeta.com/omnilingualasr" target="_blank" data-lnfb-mode="ie"><u>language exploration demo</u></a> people can explore languages covered by the model.</li></ul></div><div><p>Automatic speech recognition (ASR) systems aim to make spoken language universally accessible by transcribing speech into text that can be searched, analyzed, and shared. Currently, most automatic speech recognition systems focus on a limited set of high-resource languages that are well represented on the internet, often relying on large amounts of labeled data and human-generated metadata to achieve good performance. This means high-quality transcriptions are often unavailable for speakers of less widely represented or low-resource languages, furthering the digital divide.</p><p>Today, Meta’s Fundamental AI Research (FAIR) team is introducing <a href="https://github.com/facebookresearch/omnilingual-asr" target="_blank" data-lnfb-mode="ie"><u>Omnilingual ASR</u></a> — a groundbreaking suite of models that deliver automatic speech recognition for more than 1,600 languages, including 500 low-resource languages never before transcribed by AI. We’re also open sourcing Omnilingual wav2vec 2.0, a new self-supervised massively multilingual speech representation model scaled up to 7B parameters that can be leveraged for other downstream speech-related tasks. In addition, we’re releasing the Omnilingual ASR Corpus, a unique collection of transcribed speech in 350 underserved languages, curated in collaboration with our global partners.</p><p>This work supports our goal of building technology to help bring the world closer together. Omnilingual ASR is a significant step toward delivering a truly universal transcription system and expanding access to speech technology worldwide, ensuring that high-quality speech-to-text systems are accessible to even the most underrepresented language communities. The hope is to ultimately break down language barriers and enable communication across diverse linguistic and cultural backgrounds.</p></div><p>Beyond Multilinguality: Unprecedented Language Coverage and Performance</p><div><p>Automatic Speech Recognition has made strong progress in recent years, approaching near-perfect accuracy for many high-resource languages. However, expanding language coverage has been prohibitively resource intensive as current AI architectures are too data demanding to scale universally.</p><p>Omnilingual ASR addresses this research blocker by introducing two architectural variants. First, we scaled our previous wav2vec 2.0 speech encoder to 7B parameters for the first time, producing rich, massively multilingual semantic representations from raw, untranscribed speech data. We then built two decoder variants to map those into character tokens. The first decoder relies on a traditional connectionist temporal classification (CTC) objective, while the second leverages a traditional transformer decoder, commonly used in LLMs.</p></div></div><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/559212493_868065139217532_1757975235142574465_n.png?_nc_cat=110&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=hXWf-YJrXR0Q7kNvwHGrT0F&amp;_nc_oc=AdnQhjTpZcypjjUuVdWWdRexcxeHFyn-bTOsskqaVbFQ1Hit3rgARzuVbZwtIeHM8A4&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfgUQnN2pYDlk0LyGXF9pEVzDlOmzyF4HjYpC7Mt96483A&amp;oe=692D1C20" alt="" id="u_0_b_i1"></p></div><p>Dubbed LLM-ASR, this approach introduces a step change in ASR performance, especially for long tail languages. Our 7B-LLM-ASR system achieves state-of-the-art performance across 1,600+ languages, with character error rates (CER) below 10 for 78% of those languages.</p><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/576960745_1209180377769385_6702108631461736038_n.png?_nc_cat=105&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=crHPI5fOvFoQ7kNvwE0Se4M&amp;_nc_oc=AdnYzyhKcwSVbSsLkSvM8ji0moMH4ZD4gFcSWwGKl8l8whUA_7kLTLNYBWQsR3-t3uE&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfjjCyT2ERCTIGraDBUlslVC2qUNvzIlKJoxXlmBIAlQwQ&amp;oe=692D32A6" alt="" id="u_0_c_Uf"></p></div><div><p>Bring Your Own Language</p><div><h2>Beyond expanding to more than 1,600 languages, Omnilingual ASR also shifts the paradigm for how new languages can be brought into the fold. In most existing systems, languages not included at release time can only be added through expert-driven fine-tuning — a path inaccessible to most communities. Omnilingual ASR instead introduces the first large-scale ASR framework capable of extending to entirely new languages with just a few in-context examples.</h2><p>This is made possible by our LLM-inspired system, which brings in-context learning capabilities over from the field of LLMs. In practice, this means that a speaker of an unsupported language can provide only a handful of paired audio-text samples and obtain usable transcription quality — without training data at scale, onerous expertise, or access to high-end compute. While zero-shot performance cannot yet match that of fully trained systems, it offers a far more scalable path to bringing new languages into digital reach.</p></div></div><div><p><img src="https://scontent-cph2-1.xx.fbcdn.net/v/t39.2365-6/576939007_1511214026693639_7210936081314134566_n.png?_nc_cat=101&amp;ccb=1-7&amp;_nc_sid=e280be&amp;_nc_ohc=njlqK_uhU8gQ7kNvwEdx5uB&amp;_nc_oc=AdkbDeeDpieXhZ_fK1oiQKFrGbMpOhyWULeLkAep5oOw0x9B-sG6prAVjgMHkp3SQRo&amp;_nc_zt=14&amp;_nc_ht=scontent-cph2-1.xx&amp;_nc_gid=ReRyiZMa_7nAphMwyAKPzg&amp;oh=00_AfjdkkK8B5EwpP92f0owW8Rq3VADbneSJRdpyj0S-pECdQ&amp;oe=692D1731" alt="" id="u_0_d_9g"></p></div><div><p>A Suite of Models for Various Use Cases</p><div><p>We’re releasing a full suite of models and one dataset. Built on the foundation of FAIR’s <a href="https://ai.meta.com/blog/multilingual-model-speech-recognition/" target="_blank" data-lnfb-mode="ie"><u>previous research</u></a>, Omnilingual ASR gives stakeholders everything they need to expand and improve speech technology for any language.</p><p>The two decoding variants are available as a versatile family of models — from lightweight 300M versions designed for low-power devices to powerful 7B models that offer top-tier accuracy for a variety of use cases. Our general-purpose speech foundation model wav2vec 2.0 is also made available at various sizes. It can be used by researchers and developers alike to enable speech-related tasks beyond ASR.</p><p>All assets are released under a permissive Apache 2.0 license while the data is provided under the CC-BY license and are based on FAIR’s open source <a href="https://github.com/facebookresearch/fairseq2" target="_blank" data-lnfb-mode="ie"><u>fairseq2</u></a> framework, empowering researchers, developers, and language advocates worldwide to advance and tailor speech solutions for their own use cases using the latest tools and technologies in the PyTorch ecosystem.</p></div><p>Built With Global Partners</p><div><p>Omnilingual ASR also advances the state of multilingual ASR along more familiar dimensions. Its training corpus is one of the largest ever assembled for ASR in both volume and linguistic diversity, integrating publicly available datasets with community-sourced speech recordings collected through multiple partnerships.</p><p>To reach languages with little or no digital presence, we worked with local organizations that recruited and compensated native speakers, often in remote or under-documented regions. We’re releasing this commissioned part of our training corpus as Omnilingual ASR Corpus to further benefit the ASR research community. To date, it is the largest ultra-low-resource spontaneous ASR dataset ever made available, covering hundreds of languages never seen before by ASR systems. Explore the languages in the dataset <a href="https://aidemos.atmeta.com/omnilingualasr/language-globe" target="_blank" data-lnfb-mode="ie"><u>here</u></a>.</p><p>Beyond commissioned partnerships, collaborations through the <a href="https://about.fb.com/news/2025/02/announcing-language-technology-partner-program/" target="_blank" data-lnfb-mode="ie"><u>Language Technology Partner Program</u></a> have brought together linguists, researchers, and language communities from around the world, providing essential expertise and resources. We joined forces with organizations such as Mozilla Foundation’s Common Voice and Lanfrica/NaijaVoices to work directly with local communities.</p><p>These partnerships have been instrumental in infusing Omnilingual ASR with deep linguistic knowledge and cultural understanding, ensuring that the technology meets local needs and empowers diverse language communities globally.</p></div><a href="https://github.com/facebookresearch/omnilingual-asr" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_e_H4"></a><a href="https://aidemos.atmeta.com/omnilingualasr/language-globe" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_f_G2"><div><p>Try the Language Exploration Demo</p><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 36 36" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M21.9502 15.4645L13.7579 23.6569C13.3674 24.0475 12.7342 24.0475 12.3437 23.6569C11.9532 23.2664 11.9532 22.6333 12.3437 22.2427L20.536 14.0503H15.8792C15.3269 14.0503 14.8792 13.6026 14.8792 13.0503C14.8792 12.498 15.3269 12.0503 15.8792 12.0503H22.9502C23.5025 12.0503 23.9502 12.498 23.9502 13.0503V20.1214C23.9502 20.6737 23.5025 21.1214 22.9502 21.1214C22.398 21.1214 21.9502 20.6737 21.9502 20.1214V15.4645Z" fill="CurrentColor"></path></svg></div></a><a href="https://huggingface.co/spaces/facebook/omniasr-transcriptions" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_g_tU"><div><p>Try the Transcription Tool</p><svg xmlns="http://www.w3.org/2000/svg" width="36" height="36" viewBox="0 0 36 36" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M21.9502 15.4645L13.7579 23.6569C13.3674 24.0475 12.7342 24.0475 12.3437 23.6569C11.9532 23.2664 11.9532 22.6333 12.3437 22.2427L20.536 14.0503H15.8792C15.3269 14.0503 14.8792 13.6026 14.8792 13.0503C14.8792 12.498 15.3269 12.0503 15.8792 12.0503H22.9502C23.5025 12.0503 23.9502 12.498 23.9502 13.0503V20.1214C23.9502 20.6737 23.5025 21.1214 22.9502 21.1214C22.398 21.1214 21.9502 20.6737 21.9502 20.1214V15.4645Z" fill="CurrentColor"></path></svg></div></a><a href="https://ai.meta.com/research/publications/omnilingual-asr-open-source-multilingual-speech-recognition-for-1600-languages/" data-ms-clickable="true" data-ms="{&quot;creative&quot;:&quot;click_external&quot;}" target="_blank" rel="noreferrer noopener" data-lnfb-mode="ie" id="u_0_h_sn"></a></div><div><p>Our latest updates delivered to your inbox</p><p><a href="https://ai.facebook.com/subscribe/" target="_blank">Subscribe</a> to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.</p></div></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The lazy Git UI you didn't know you need (402 pts)]]></title>
            <link>https://www.bwplotka.dev/2025/lazygit/</link>
            <guid>45878578</guid>
            <pubDate>Mon, 10 Nov 2025 17:50:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bwplotka.dev/2025/lazygit/">https://www.bwplotka.dev/2025/lazygit/</a>, See on <a href="https://news.ycombinator.com/item?id=45878578">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content"><p>When my son was born last April, I had ambitious learning plans for the upcoming 5w paternity leave. As you can imagine, with two kids, life quickly verified this plan 🙃. I did eventually start <em>some</em> projects. One of the goals (sounding rebellious in the current AI hype cycle) was to learn and use <a href="https://neovim.io/" target="_blank" rel="noopener noreffer">neovim</a>
for coding. As a <a href="https://www.jetbrains.com/go" target="_blank" rel="noopener noreffer">Goland</a>
aficionado, I (and my wrist) have always been tempted by no-mouse, OSS, <a href="https://pkg.go.dev/golang.org/x/tools/gopls" target="_blank" rel="noopener noreffer">gopls</a>
based, highly configurable dev setups.</p>
<p>Long story short, I’d still stick to Goland for my professional coding (for now), but during the experiments with <code>nvim</code>, I accidentally stumbled upon <a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreffer">lazygit</a>
Git UI. I literally mistyped <code>&lt;space&gt;gg</code> instead of <code>gg</code>, which opened up the built-in <code>lazygit</code> overlay UI.</p>
<p>A week later, I have already switched all my <code>git</code> workflows to <code>lazygit</code> (also outside <code>nvim</code>), and I have been using it since then. In this post, I’d like to explain why it happened so quickly, so:</p>
<ul>
<li>What makes <code>lazygit</code> so special?</li>
<li>How can it make you more productive?</li>
<li>What we can all learn from <code>lazygit</code> around designing incredible software with seamless UX?</li>
</ul>
<p>Let’s jump in!</p>

<p>Likely every developer knows and (in some form) uses the <a href="https://git-scm.com/docs" target="_blank" rel="noopener noreffer">git CLI</a>
. It’s relatively simple, and it seems incredibly stable – the only change I noticed in the last decade was the new <code>git switch</code> command, although I still haven’t “switched” to it from the lovely <code>git checkout</code>🙃 .</p>
<p>As a result, it’s common to see developers memorize a few commands you typically use (e.g.<code>clone</code>, <code>fetch/pull</code>, <code>config/remote</code>, <code>add/rm</code>, <code>status</code>, <code>checkout</code>, <code>commit</code>, <code>push</code>, <code>cherry-pick</code>, <code>rebase</code>, <code>merge</code>, <code>log</code>) and stick to the CLI. In fact, in <a href="https://survey.stackoverflow.co/2022/#section-version-control-interacting-with-version-control-systems" target="_blank" rel="noopener noreffer">2022, 83% of the StackOverflow responders said they prefer CLI to other interfaces</a>
and that number is likely still quite high nowadays.</p>
<p>However, <a href="https://git-scm.com/downloads/guis" target="_blank" rel="noopener noreffer">graphical interfaces</a>
and generally other <code>git</code> compatible clients do exist:</p>
<ul>
<li>Some of them offer more or less the same <code>git</code> workflows as the original <code>git</code> CLI, just more visually appealing and with buttons/interactivity instead of remembering the CLI flags, e.g. <a href="https://git-scm.com/docs/git-gui" target="_blank" rel="noopener noreffer">git gui</a>
, <a href="https://github.com/apps/desktop" target="_blank" rel="noopener noreffer">GitHub Desktop</a>
or <code>lazygit</code> discussed here.</li>
<li>Other projects add more magic (e.g. AI), and potentially new light abstractions/workflows in an attempt to simplify or enhance <code>git</code> use e.g. <a href="https://www.gitkraken.com/" target="_blank" rel="noopener noreffer">GitKraken</a>
.</li>
<li>There are even projects like recently popular <a href="https://github.com/jj-vcs/jj" target="_blank" rel="noopener noreffer">jj</a>
tool that completely abstracts away <code>git</code> API and replace it with a new source control flows to “simplify” them or unify them across various <a href="https://en.wikipedia.org/wiki/Version_control" target="_blank" rel="noopener noreffer">VCS</a>
other than <code>git</code> (e.g. <code>mercurial</code>, Google <a href="https://www.youtube.com/watch?v=W71BTkUbdqE&amp;t=645s" target="_blank" rel="noopener noreffer">Piper</a>
and everything else you wished it was <code>git</code>, but it’s not 😛).</li>
</ul>
<p>What you choose for your work is entirely up to you. Depending on what you are passionate about, how you work with <code>git</code> and what type of software you are touching (monorepo vs small repos, closed vs open source, GitHub vs other hosting solutions, where you deploy, etc.), different clients might be more or less productive for you.</p>
<blockquote>
<p>NOTE: If you’re new to software engineering, don’t skip learning the <code>git</code> CLI. Even if you use some higher-level interfaces later on, it will help you understand what they do in the background, plus sooner or later you will end up debugging some remote VM or container with no UI access (e.g. CI systems).</p>
<p>Also, as documented in the official <code>git</code> documentation, <a href="https://git-scm.com/book/en/v2/Appendix-A:-Git-in-Other-Environments-Graphical-Interfaces#:~:text=the%20command%2Dline%20is%20still%20where%20you%E2%80%99ll%20have%20the%20most%20power%20and%20control%20when%20working%20with%20your%20repositories." target="_blank" rel="noopener noreffer">“the command-line is still where you’ll have the most power and control when working with your repositories."</a>
</p>
</blockquote>
<p>For me, I need something:</p>
<ul>
<li>simple and fast to limit the context switch overhead.</li>
<li><code>git</code> CLI-native to have fewer things that can go wrong.</li>
<li>“discoverable” and interactive, as I am bad at remembering keybindings and commands (I need my brain memory for more fun bits).</li>
</ul>
<p>For those reasons, early in my career, I started depending on a hybrid workflow, with a few GUI tools:</p>
<ul>
<li><a href="https://git-scm.com/docs/git-gui" target="_blank" rel="noopener noreffer">git gui</a>
instead of <code>status</code>, <code>commit</code>, <code>config/remote</code>, <code>add/rm</code> and <code>push</code>.</li>
<li><a href="https://git-scm.com/docs/gitk" target="_blank" rel="noopener noreffer">gitk</a>
instead of <code>log</code>.</li>
<li><code>git</code> CLI for everything else (e.g. rebasing/complex merging).</li>
</ul>
<p>I don’t remember why specifically those (AFAIK, decade ago there wasn’t anything else), but I literally have been using them non-stop until this year!</p>
<p>A few years ago, because of the 1990-style look of those UIs, lack of active development and modern features, I looked around for some alternatives. I remember I was quickly demotivated when I accidentally lost all my local changes on a single mouse click on the wrong thing in one of the tools 🙈 (starts with <code>G</code> and ends with <code>N</code>). After that, I was sceptical I’d find some new tool anytime soon. The arguments to motivate me to make a switch would need to be strong.</p>
<p>Turns out, an open mind and a bit of curiosity in a random moment gave more fruit than tailored research. By accident, I noticed <code>lazygit</code> and after a short try, it became my main <code>git</code> tool.</p>
<h2 id="whats-amazing-in-lazygit">What’s amazing in <code>lazygit</code>?</h2>
<p>Somehow, <a href="https://github.com/jesseduffield/lazygit" target="_blank" rel="noopener noreffer">lazygit</a>
ticked so many boxes for me:</p>
<ul>
<li>It’s easy to use; it makes you productive from day 1.</li>
<li>It enables you to do more (and faster), even teaching you along the way.</li>
<li>It’s a TUI (terminal user interface), making it incredibly fast, portable and visually consistent.</li>
</ul>
<p>Many of the tool’s benefits are also amazing learning on how to build brilliant devtools and software in general.</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png" data-src="./lazygit-intellij.png" data-srcset="./lazygit-intellij.png, ./lazygit-intellij.png 1.5x, ./lazygit-intellij.png 2x" data-sizes="auto" alt="./lazygit-intellij.png" title="lazygit" srcset="https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png, https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png 1.5x, https://www.bwplotka.dev/2025/lazygit/lazygit-intellij.png 2x"></p>
<blockquote>
<p><code>lazygit</code> used via <a href="https://plugins.jetbrains.com/plugin/24917-lazygit-in-editor-terminal" target="_blank" rel="noopener noreffer">lazygit IntelliJ plugin</a>
on my <code>git</code> clone of Prometheus project.</p>
</blockquote>
<p>Personally, probably the best thing about the <code>lazygit</code> is its UX, notably how easy it is to use this tool, with just a basic understanding of the <code>git</code> CLI. Generally, it seems that a nice user experience is achieved due to deliberate choice of strong consistency, deliberate visualizations and interactive menus. Let me explain.</p>
<h3 id="consistency">Consistency</h3>
<p><code>lazygit</code> is incredibly well organized and visually consistent. <code>lazygit</code> TUI consists of a set of boxes (“views”) with consistent behaviour. Most views are generally visible, always, no matter what operation you are doing (unless you zoom in). You always have a focus on one box. It’s visibly clear that some boxes have “tabs”. When you interact with boxes on the left, the right box changes.</p>
<p>Then, <code>lazygit</code> generally sticks to native <code>git</code> terms and abstractions, which reduces the initial learning curve. In fact, this tool even teaches you about standard, yet a bit more advanced <code>git</code> operations (e.g. <code>bisect</code> which I used to do manually) and terms (e.g. TIL <a href="https://medium.com/@michotall95/hunk-in-git-f7b7855d47ae" target="_blank" rel="noopener noreffer"><code>hunk</code></a>
which is an official <code>git</code> term for a piece of relevant code).</p>
<p>Finally, by default, <code>lazygit</code> is pretty consistent with the feeling and keybindings of <code>vim</code>. This means that <code>q</code> will quit the tool, <code>h/j/k/l</code> (or arrows) are for navigation, <code>/</code> for filtering and <code>y</code> for copy. Then, similar to <code>vim</code> it attempts to follow the name of the command, e.g. <code>c</code> commits, <code>a</code> adds all, <code>A</code> amends, <code>f</code> fetches, <code>p</code> pulls, <code>P</code> pushes, <code>r</code> rebases.</p>
<p>This is incredibly important as your common workflows can be easily memorized and invoked in a quick set of a few keystrokes (see <a href="#enhanced-git-workflows" rel="">enhanced workflows</a>
). Now, as I mentioned before, that’s a double-edged sword, because if your brain is lazy like mine, you will end up staring at the <code>vim/nvim</code> view trying to remember what the command was to select and copy things (or <a href="https://cdn.sanity.io/images/jo7n4k8s/production/7a0bf96c6e3155ca56c74723cb0c0767517a4429-324x318.jpg?auto=format" target="_blank" rel="noopener noreffer">quit vim</a>
).</p>
<p><code>lazygit</code> solves the above with a limited set of commands (that’s a good thing: <a href="https://en.wikipedia.org/wiki/Unix_philosophy" target="_blank" rel="noopener noreffer">do one thing and do it well</a>
) and great “discoverability”.</p>
<h3 id="discoverability">Discoverability</h3>
<p><code>lazygit</code> strikes an amazing balance of showing data you need when you need it. When you open this tool, it’s obvious you want to do some <code>git</code> trickery, so it’s likely a good thing to give you all you need to know, in a pill:</p>
<ul>
<li>What repo is this.</li>
<li>All staged and unstaged files with changes (<code>git status</code>).</li>
<li>What branch are you on.</li>
<li>The top ~10 commits on this branch.</li>
<li>Top stash item.</li>
<li>Last git commands you performed.</li>
<li>Core actions/commands you can do with their keybindings.</li>
</ul>
<p>It’s a lot of data! Yet <code>lazygit</code> somehow manages to show you all of this without visually overwhelming you:</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png" data-src="./lazygit-discoverability.png" data-srcset="./lazygit-discoverability.png, ./lazygit-discoverability.png 1.5x, ./lazygit-discoverability.png 2x" data-sizes="auto" alt="./lazygit-discoverability.png" title="img" srcset="https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png, https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png 1.5x, https://www.bwplotka.dev/2025/lazygit/lazygit-discoverability.png 2x"></p>
<blockquote>
<p>Consistent and self-explanatory views with a flat action menu allow you to find the data you need when you need it quickly.</p>
</blockquote>
<p>This context is game-changing:</p>
<ul>
<li>If you never used this tool, or if you had spent one month doing meetings, reviews and design docs at work, and you return to coding finally, you immediately know <strong>where you are</strong> and <strong>where things are</strong>.</li>
<li>It reduces the risk of surprises and mistakes (<code>"ups! I pushed to main directly sorry!"</code>), saving you a solid amount of <code>SWEh</code> (software engineering hours) monthly.</li>
<li>Normally to double-check those things you would need to run multiple commands and check different windows. <code>lazygit</code> immediately removes that context switching.</li>
<li>Even if you forget important keybindings for actions, it’s quick to check them on the footer or with <code>?</code>.</li>
</ul>
<p>But there’s more, <code>lazygit</code> guides you on all operations with interactivity.</p>
<h3 id="interactivity">Interactivity</h3>
<p>In other UI tools, you have hundreds of buttons, with multiple layers of nested menus. <code>lazygit</code> has a different approach. This tool teaches you on the way, what’s possible and when. For example:</p>
<ul>
<li>Push will give you a warning of divergence with upstream if any. Clicking <code>Enter</code> will do <code>--force</code> push, <code>Esc</code> will cancel.</li>
<li>Rebase will ask you, if you want the interactive one or not and double-check the branch.</li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/rebase-check.png" data-src="/2025/lazygit/rebase-check.png" data-srcset="/2025/lazygit/rebase-check.png, /2025/lazygit/rebase-check.png 1.5x, /2025/lazygit/rebase-check.png 2x" data-sizes="auto" alt="/2025/lazygit/rebase-check.png" title="rebase-check.png" width="1167" srcset="https://www.bwplotka.dev/2025/lazygit/rebase-check.png, https://www.bwplotka.dev/2025/lazygit/rebase-check.png 1.5x, https://www.bwplotka.dev/2025/lazygit/rebase-check.png 2x"></p>
<ul>
<li>Interactive rebase is much more guided and interactive, than <code>git rebase --interactive</code>. No need to manually type and remember special words (e.g. <code>pick/drop/squash</code> or <code>p/d/s</code>). The <code>&lt;c-j&gt;</code>, <code>&lt;c-k&gt;</code> keys also quickly move commits up and down (reordering).</li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/rebase.png" data-src="/2025/lazygit/rebase.png" data-srcset="/2025/lazygit/rebase.png, /2025/lazygit/rebase.png 1.5x, /2025/lazygit/rebase.png 2x" data-sizes="auto" alt="/2025/lazygit/rebase.png" title="rebase.png" width="1335" srcset="https://www.bwplotka.dev/2025/lazygit/rebase.png, https://www.bwplotka.dev/2025/lazygit/rebase.png 1.5x, https://www.bwplotka.dev/2025/lazygit/rebase.png 2x"></p>
<ul>
<li>Git conflicts after rebase will be highlighted. After you fix them <code>lazygit</code> automatically will ask you if you want to commit them and auto continue the rebase.</li>
<li>When switching branches with conflicting changes, <code>lazygit</code> will automatically ask you if you want to auto-stash those changes etc.</li>
</ul>
<p>Generally, <code>lazygit</code> guides you in your workflows with minimal distractions and guesswork. This builds trust very quickly, allowing adoption of faster workflows.</p>
<h2 id="enhanced-git-workflows">Enhanced git workflows</h2>
<p>Eventually, <code>lazygit</code> boosted productivity around git workflows for me and for many other existing happy users.</p>
<p>What’s impressive is that <code>lazygit</code> does it without adding entirely new workflows. Instead, it makes what <code>git</code> CLI offers much more usable, safer, quicker and discoverable. It teaches you better patterns.</p>
<p>One example is highlighted with custom patching. Imagine you made some changes, committed them, but then you want to bring back a few lines (but not all) to what it was before, from an earlier commit. My previous flow used to be either:</p>
<ul>
<li>Local IDE history (slow-ish, too much granularity (every file save), not always available).</li>
<li><code>git gui</code> tool I clicked <code>amend</code> which would pull all changed files from that commit to <code>staged</code> area, then I find lines I want, manually copy them (with those git diff <code>+</code> and <code>-</code> chars!) and paste to IDE, then trim unwanted chars. Pretty horrible habit (:</li>
</ul>
<p>When using <code>lazygit</code>, I obviously tried to replicate my broken workflow. I couldn’t because <code>lazygit</code> diffs are not intuitively select+copy-able (it might be fixable over time; not the highest priority, but people want this e.g. <a href="https://github.com/jesseduffield/lazygit/issues/4511" target="_blank" rel="noopener noreffer">1</a>
, <a href="https://github.com/jesseduffield/lazygit/issues/4365" target="_blank" rel="noopener noreffer">2</a>
). I even <a href="https://github.com/jesseduffield/lazygit/issues/3967#issuecomment-3159742037" target="_blank" rel="noopener noreffer">+1 one some issue around it</a>
, and I’m glad I did, because the maintainer pointed me to… 10x simpler workflow: native reset/patch per line/hunk flow!</p>
<p><img src="https://www.bwplotka.dev/2025/lazygit/patching.png" data-src="/2025/lazygit/patches.png" data-srcset="/2025/lazygit/patches.png, /2025/lazygit/patches.png 1.5x, /2025/lazygit/patches.png 2x" data-sizes="auto" alt="/2025/lazygit/patches.png" title="patching.png" width="993" srcset="https://www.bwplotka.dev/2025/lazygit/patches.png, https://www.bwplotka.dev/2025/lazygit/patches.png 1.5x, https://www.bwplotka.dev/2025/lazygit/patches.png 2x"></p>
<blockquote>
<p>All git diffs in <code>lazygit</code> (no matter if unstaged/staged/stashed/committed changes) support per line or hunk selection and patching/selection.</p>
</blockquote>
<p>With this, my “line reset from the last commit” workflow is:</p>
<ul>
<li>simpler</li>
<li>within a single place</li>
<li>works for any commit (not only the latest)</li>
</ul>
<p>Steps in <code>lazygit</code>: <em>focus on commits view &gt; select commit &gt; select file &gt; select lines to reset &gt; patch options &gt; “remove patch from the original commit”</em>. All either mouse-assisted or <code>4 enter enter space &lt;c-p&gt; d</code> within seconds.</p>
<p>Those short key bindings are game changers in general. I’d recommend starting with a slower, but careful mouse-assisted flow, then naturally you memorize the needed keystrokes without noticing. For me, after some time, some quick flows became a habit, I was using shortcuts unconsciously.</p>
<p>As a result, my common <code>git</code> flows, with <code>lazygit</code>, were significantly improved:</p>
<h5 id="iterating-on-changes-and-updating-upstream">Iterating on changes and updating upstream:</h5>
<p>My typical flow to ensure clean commit log:</p>
<ul>
<li><em>select files to commit &gt; add to the last commit (amend) &gt; force push</em></li>
<li><code>2 space A P enter</code></li>
</ul>
<h5 id="iterating-on-changes-and-updating-upstream-with-a-new-commit">Iterating on changes and updating upstream with a new commit:</h5>
<ul>
<li><em>select files to commit &gt; create new commit &gt; push</em></li>
<li><code>2 space c &lt;type commit title&gt; P</code></li>
</ul>
<h5 id="syncing-branches">Syncing branches</h5>
<p>I generally do an interactive rebase for this. I avoid merges, unless squashed.</p>
<ul>
<li><em>select branch &gt; rebase &gt; interactive rebase &gt; arrange commits &gt; rebase options &gt; continue</em></li>
<li><code>3 r i &lt;s/p/d/.. to arrange rebase&gt; m c</code></li>
</ul>
<h5 id="removing-unwanted-commit-from-history">Removing unwanted commit from history</h5>
<p>Normally you would need to do full interactive rebase against <code>HEAD~4</code> or something, but now:</p>
<ul>
<li><em>select commit &gt; drop</em></li>
<li><code>4 d</code></li>
</ul>
<h5 id="removing-unwanted-file-changes-from-commits">Removing unwanted file changes from commits</h5>
<ul>
<li><em>select commit &gt; select file &gt; remove</em></li>
<li><code>4 enter d</code></li>
</ul>
<h5 id="splitting-commit-into-multiple-prscommits">Splitting commit into multiple PRs/commits</h5>
<p>This is normally a bit painful, but now:</p>
<ul>
<li><em>select commit &gt; select file &gt; select lines or hunks &gt; patch options &gt; move patch into new commit after the original commit &gt; create new commit</em></li>
<li><code>4 enter enter &lt;c-p&gt; n &lt;type commit title&gt; enter</code></li>
</ul>
<p><img src="https://www.bwplotka.dev/2025/lazygit/split.png" data-src="/2025/lazygit/split.png" data-srcset="/2025/lazygit/split.png, /2025/lazygit/split.png 1.5x, /2025/lazygit/split.png 2x" data-sizes="auto" alt="/2025/lazygit/split.png" title="split.png" width="1179" srcset="https://www.bwplotka.dev/2025/lazygit/split.png, https://www.bwplotka.dev/2025/lazygit/split.png 1.5x, https://www.bwplotka.dev/2025/lazygit/split.png 2x"></p>
<h5 id="cherry-pick">Cherry-pick</h5>
<p>Typically, it meant copying commit SHAs around; prone to errors. Now:</p>
<ul>
<li><em>select branch &gt; select commit &gt; copy for cherry-pick (you can buffer many) &gt; select target branch &gt; go to commits &gt; paste</em></li>
<li><code>3 4 C 3 4 V</code></li>
</ul>
<p>…and many more!</p>
<h2 id="what-can-we-learn">What can we learn?</h2>
<p>To me, <code>lazygit</code> is not only an amazing tool for everyday use, but also an inspiration around devtools UX. The <a href="#whats-amazing-in-lazygit" rel="">simplicity, consistency, discoverability, sane defaults, shortcuts for common flows and interactivity</a>
should be on the radar for anyone who builds devtools. Not mentioning deep <a href="https://github.com/jesseduffield/lazygit/blob/master/docs/Config.md" target="_blank" rel="noopener noreffer">configurability</a>
, a healthy dose of <a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#invoke-a-custom-command" target="_blank" rel="noopener noreffer">extensibility</a>
, being fully free (<a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#sponsors" target="_blank" rel="noopener noreffer">donations possible!</a>
), a <a href="https://github.com/jesseduffield/lazygit/graphs/contributors" target="_blank" rel="noopener noreffer">healthy OSS situation</a>
and… tool being written 100% in Go! (:</p>
<p>Imagine what other tools we could write, reusing similar patterns or even similar UX! <a href="https://github.com/jesseduffield/gocui" target="_blank" rel="noopener noreffer">TUI framework</a>
and <a href="https://github.com/jesseduffield/lazygit/" target="_blank" rel="noopener noreffer"><code>lazygit</code> code is fully OSS (MIT)</a>
, so anyone has a healthy base for building different tools. I do have ideas for a few tools, especially around some extremely manual release workflows in our ecosystems. Let’s collaborate! 💪</p>
<h2 id="summary">Summary</h2>
<p>Hope this write-up was useful for you!</p>
<p>Even with the current advancement in GenAI, statistical aspect of LLMs makes them not a great fit for reliable and accurate version control changes that projects and systems have to rely on. Some <a href="https://github.com/jesseduffield/lazygit/issues/2579" target="_blank" rel="noopener noreffer">LLM assist (e.g. generating commit messages)</a>
will eventually come to <code>lazygit</code> and git tooling, but the core of <code>lazygit</code> is to remain incredibly relevant for the (increasingly AI-assisted) software development cycles.</p>
<p>Kudos to <a href="https://github.com/jesseduffield/lazygit?tab=readme-ov-file#sponsors" target="_blank" rel="noopener noreffer">all maintainers, contributors and sponsors</a>
of <code>lazygit</code> for an amazing work!</p>
<p>Feel free to comment, give feedback AND use and contribute to the <code>lazygit</code> project! Happy coding!</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Canadian military will rely on public servants to boost its ranks by 300k (119 pts)]]></title>
            <link>https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants</link>
            <guid>45877892</guid>
            <pubDate>Mon, 10 Nov 2025 16:55:08 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants">https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants</a>, See on <a href="https://news.ycombinator.com/item?id=45877892">Hacker News</a></p>
Couldn't get https://ottawacitizen.com/public-service/defence-watch/canadian-military-public-servants: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[ClickHouse acquires LibreChat, open-source AI chat platform (113 pts)]]></title>
            <link>https://clickhouse.com/blog/librechat-open-source-agentic-data-stack</link>
            <guid>45877770</guid>
            <pubDate>Mon, 10 Nov 2025 16:44:40 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://clickhouse.com/blog/librechat-open-source-agentic-data-stack">https://clickhouse.com/blog/librechat-open-source-agentic-data-stack</a>, See on <a href="https://news.ycombinator.com/item?id=45877770">Hacker News</a></p>
<div id="readability-page-1" class="page"><article><div><p>We are excited to announce that ClickHouse has acquired LibreChat, the leading open-source AI chat platform that offers a unified interface for interacting with a wide range of large language models (LLMs), giving users and organizations full control over their data, agents, and conversations. We couldn't be more thrilled to welcome Danny Avila (the founder of LibreChat) as well as the LibreChat team and community into the ClickHouse family.</p>
<p>LibreChat becomes a core component in our vision for <a href="https://clickhouse.com/blog/agent-facing-analytics" target="_blank">Agent-Facing Analytics</a>, creating a truly open-source Agentic Data Stack. By combining LibreChat's powerful user experience and AI agent framework with ClickHouse's analytical capabilities at scale, it has never been easier to build analytics agents that can be leveraged to expose massive datasets to agents operating on behalf of users.</p>

<p>Usually, in similar announcements, the user quotes are often buried deep into the post. We’ll try to do things a bit differently here and lead with the raw, unfiltered user feedback, then state our thesis right after (you can skip straight to our investment thesis by clicking <a href="https://clickhouse.com/blog/librechat-open-source-agentic-data-stack#reducing-time-to-insight">here</a>).</p>

<p>Shopify, a global e-commerce leader, has embedded AI across its operations, giving employees access to advanced models through a unified internal platform. Using the <a href="https://www.firstround.com/ai/shopify" target="_blank">open-source LibreChat platform</a>, Shopify built tools like an RFP assistant that pulls from company data, rates response confidence, and improves over time.</p>
<blockquote>
<div><p>“LibreChat powers reflexive AI use across Shopify. With near universal adoption and thousands of custom agents, teams use it to solve real problems, increase productivity, and keep the quality bar high. By connecting more than 30 internal MCP servers, it democratizes access to critical information across the company” </p><p>

<em>Matt Burnett, Senior Engineer at Shopify</em></p></div>
</blockquote>
<div>
<blockquote><p>Shopify runs an internal fork of librechat, and we merge most everything back. I highly recommend other companies give this project a look for their internal LLM system. It works very well for us. <a href="https://t.co/ihExJyXY2i">https://t.co/ihExJyXY2i</a></p>— tobi lutke (@tobi) <a href="https://twitter.com/tobi/status/1932846291794510241?ref_src=twsrc%5Etfw">June 11, 2025</a></blockquote> 
</div>

<p>The <a href="https://www.cbioportal.org/" target="_blank">cBioPortal for Cancer Genomics</a> provides visualization, analysis, and download of large-scale cancer genomics data sets. The team at cBioPortal recently launched the chat-based <a href="https://chat.cbioportal.org/" target="_blank">cBioAgent</a> that allows users to interact with genomics datasets in plain text (<a href="https://chat.cbioportal.org/share/s2NZmrgtC7neWPM0L3Vl2" target="_blank">example interaction</a>).</p>
<blockquote>
<div><p>“By leveraging the ClickHouse, MCP, and LibreChat stack, we rapidly delivered a prototype to cBioPortal users that empowered them to ask entirely new questions about cancer genomics and treatment trajectories, get quick answers, and explore data in ways not possible through the existing UI. It puts discovery at cancer researchers' fingertips.” </p><p>

<em>Ino de Bruijn, Manager Bioinformatics Software Engineering, cBioPortal</em></p></div>
</blockquote>

<p><a href="https://fetch.com/" target="_blank">Fetch</a> is a leading mobile rewards app that allows users to earn points by scanning shopping receipts and redeem them for gift cards. Fetch recently launched <a href="https://fast.fetch.com/" target="_blank">FAST</a>: an AI-powered tool that turns household purchase behavior into business intelligence, insights, and media activation. Running a custom UX for the FAST portal, this use case is a great illustration of user-facing agentic analytics.</p>
<blockquote>
<div><p>“We built our new product, FAST by Fetch, on ClickHouse to help users instantly discover insights and drive efficient activation. We see agentic analytics as the future of data interaction, enabling more intuitive, dynamic, and impactful use of information. With its unmatched speed and scalability, ClickHouse is well-positioned to power this new generation of agentic experiences, and we’re thrilled to grow our partnership together.” </p><p>

<em>Sam Corzine, Director of Machine Learning, Fetch</em></p></div>
</blockquote>

<p>SecurityHQ is a global Managed Security Service Provider (MSSP) offering 24/7 threat detection, response, and risk management through its worldwide Security Operations Centres.</p>
<blockquote>
<div><p>"We reached out to ClickHouse to present our use case in building an Agentic AI with ClickHouse MCP and LibreChat similar to what <a href="https://clickhouse.com/blog/agenthouse-demo-clickhouse-llm-mcp" target="_blank">AgentHouse</a> provide. After understanding the implementation strategy used for AgentHouse, we managed to create a robust working prototype of what we wanted. The integration between ClickHouse cloud and the LibreChat using the MCP server has been flawless, making them one of, if not the best use of text-to-SQL implementation I have ever seen. Now that ClickHouse and LibreChat has joined forces will provide even more seamless interaction to our use case in building Agentic Analytics. Looking forward for a LibreHouse cloud solution for agentic analytics." </p><p>

<em>Nidharshanen Selliah, Associate Data Engineer, SecurityHQ</em></p></div>
</blockquote>

<p>Daimler Truck, one of the world’s largest commercial vehicle manufacturers, has deployed LibreChat internally to give all employees secure access to chat tools and data agents. The system democratizes AI use across the company while protecting data and meeting compliance standards. They published a <a href="https://www.daimlertruck.com/en/newsroom/stories/daimler-truck-makes-artificial-intelligence-accessible-to-all-employees-worldwide-with-librechat" target="_blank">detailed story</a> about their setup of LibreChat.</p>
<blockquote>

</blockquote>
<h3 id="and--clickhouse">and … ClickHouse<!-- --> </h3>
<p>Finally, we also use LibreChat on top of our ClickHouse data warehouse internally as well. We deployed several agents that range from product analytics to billing data and support cases analysis. We’ll let you guess from the screenshot below which one is which.</p>
<p><span><img src="https://clickhouse.com/uploads/image1_4a06083ea0.png" alt="image1.png" loading="lazy"></span></p><blockquote>
<div><p>“Internally, we also use LibreChat for data analysis and it now handles ~70% of our data warehouse queries for 200+ users. The productivity boost has been remarkable. What impressed me most is LibreChat's vibrant community that continuously contributes and innovates. The synergy between ClickHouse Cloud's blazing-fast query performance and LibreChat's flexible, multi-LLM architecture is unlocking a new generation of data analysis agents - real-time, secure, powerful, and accessible.” </p><p>

<em>Dmitry Pavlov, Director of Engineering, ClickHouse</em></p></div>
</blockquote>
<p>Now, let’s dive into the motivation behind the Agentic Data Stack.</p>

<p><a href="https://benchmark.clickhouse.com/" target="_blank">We are obsessed with world-class speed and performance at ClickHouse.</a> However, traditional analytics workflows often involve multiple handoffs between data engineers writing queries, analysts building dashboards, and business users interpreting results. Each step introduces latency on the left and right sides of the database, often measured in hours or days.</p>
<p>With agentic analytics, that timeline collapses to seconds or minutes. A product manager can ask "What's driving the spike in churn last week?" and immediately receive not just the answer, but the underlying queries, explorations, visualizations, and potential next questions to explore.</p>
<p>This is closely aligned with our own experience at ClickHouse. Earlier this year, we introduced our first agent, Dwaine (Data Warehouse AI Natural Expert): an internal agent that enables our team to query business data through natural language. Since then, questions like "What's our current revenue?", "How is this customer using our product?", "What issues are customers experiencing?" or "What's our website traffic and conversion rate?" are getting close to instant answers.</p>
<p>Dwaine has transformed how our internal teams access insights, eliminating the bottleneck of hand-writing SQL queries and data requests. Just one month after rollout, ClickHouse internal users generated more than 15 million LLM tokens in a single day on Dwaine. As of October 2025, this is now up at 33 million tokens per day.</p>
<p><span><img src="https://clickhouse.com/uploads/image5_2176472bfd.png" alt="'The first 3 months of DWAINE - Token Counts per Day'" loading="lazy"></span></p><p><em>The first 3 months of DWAINE - Token Counts per Day</em></p>
<p>If you want to experience the power of agentic analytics first-hand, try the public <a href="https://clickhouse.com/blog/agenthouse-demo-clickhouse-llm-mcp" target="_blank">AgentHouse</a> demo, which exposes publicly available datasets via the Agentic Data Stack.</p>
<p><span><img src="https://clickhouse.com/uploads/agent_house_v3_7e163b96ca.gif" alt="'AgentHouse in use'" loading="lazy"></span></p>
<p>The agentic open-source landscape is currently centered around developer tooling and SDKs, which makes perfect sense given that developers are typically the earliest adopters of emerging technologies. The main open-source projects in this space aim to empower builders to create, extend, and customize agentic systems with SDKs, frameworks, orchestration layers, and integrations. This developer-first focus helps establish the foundational ecosystem and standards needed before broader consumer applications take off.</p>
<p>We see the Agentic Data Stack as one of the first proposals of a composable software stack that focuses on the higher-level integration story, allowing users to get started and deliver value in no time. Both ClickHouse and LibreChat share the same open-source software DNA, and joining forces strengthens our commitment to that vision:</p>
<ul>
<li><strong>LibreChat remains 100% open-source</strong> under its existing MIT license</li>
<li><strong>Community-first development</strong> continues with the same transparency and openness</li>
<li><strong>Expanded roadmap</strong> to bring an even more enterprise-ready analytics experience.</li>
</ul>
<p>This proven playbook is the same one that we applied when joining forces with <a href="https://clickhouse.com/blog/clickhouse-welcomes-peerdb-adding-the-fastest-postgres-cdc-to-the-fastest-olap-database" target="_blank">PeerDB</a> to provide our ClickPipes CDC capabilities, and <a href="https://clickhouse.com/blog/clickhouse-acquires-hyperdx-the-future-of-open-source-observability" target="_blank">HyperDX</a>, which became the UX of our observability product, ClickStack.</p>
<p>We believe that being good stewards of open-source means not just maintaining code, but actively investing in and growing the communities that depend on it.</p>

<p>Large Language Models can be tricky to use in production. While grounding responses in real-time data often helps, AI agents are not immune to hallucinations: situations where the model generates incorrect information with high confidence.</p>
<p>Our own experience running internal agents within ClickHouse taught us that the best remediation comes from providing the LLMs with the maximum and most accurate context possible. This can be achieved by commenting the tables using the SQL <a href="https://clickhouse.com/docs/sql-reference/statements/alter/column#comment-column" target="_blank">COMMENT</a> syntax, for example, or by providing more context in-line, in the chat, or part of the system prompt of the LLM session.</p>
<p>Finally, robust evaluations are critical for agentic analytics in production because they turn qualitative agent behavior into quantifiable insights, enabling teams to measure effectiveness, detect regressions, and continuously improve system performance.</p>
<h2 id="whats-next-for-librechat-and-clickhouse-users">What's next for LibreChat and ClickHouse users?<!-- --> </h2>
<p>For existing LibreChat deployments: nothing changes. LibreChat continues to work exactly as it does today, and we are committed to continuing to invest in it and make sure the community thrives.</p>
<p>For ClickHouse users, over the coming months, we'll be releasing tailored integration capabilities that make LibreChat a native part of the ClickHouse experience without sacrificing its generic integration capabilities. Think of it as a “happy path” for agentic analytics in LibreChat. This will include:</p>
<ul>
<li>Seamless integration of the LibreChat experience alongside your ClickHouse Cloud instances</li>
<li>Extended support for data visualizations rendering in LibreChat</li>
<li>OAuth, end-to-end user identification, security, and governance schemes.</li>
<li>Tailored context providing (aka. semantic layer)</li>
</ul>
<p>And many more. Please stay tuned for more updates by joining our communities in <a href="https://clickhouse.com/slack" target="_blank">Slack</a> and <a href="https://discord.com/invite/librechat-1086345563026489514" target="_blank">Discord</a>.</p>
<p>Finally, for users of the <a href="https://code.librechat.ai/pricing" target="_blank">LibreChat Code Interpreter API</a> (a paid service offered by LibreChat that provides a sandboxed environment for executing code). We are planning to evolve this offering and discontinue this API in its current form. We understand that changes can take time to implement, and for this reason, we decided to set the timeline of this transition for the next 6 months (targeting May 1st, 2026). We will reach out to all code interpreter users directly to coordinate the transition.</p>

<p><strong>For LibreChat users:</strong> Continue using LibreChat as you always have, and join our community on <a href="https://discord.com/invite/librechat-1086345563026489514" target="_blank">Discord</a> if you haven’t already, to connect with other users building agents.</p>
<p><strong>For ClickHouse users</strong>: You can already deploy the Agentic Data Stack by following our user guides in our public <a href="https://clickhouse.com/docs/use-cases/AI/MCP/librechat" target="_blank">documentation</a> and <a href="https://www.youtube.com/watch?v=fuyu-AnfRDA" target="_blank">videos</a></p>
<p><strong>For everyone else</strong>: Experience the power of the open-source Agentic Data Stack with <a href="https://llm.clickhouse.com/" target="_blank">AgentHouse</a>, and let us know how we can help you succeed!</p>
<iframe width="768" height="432" src="https://www.youtube.com/embed/fuyu-AnfRDA?si=yMkEk9QtT0bLpLo6" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe>
<p>As always, the ClickHouse team would be honored to partner with you on your journey toward agentic analytics. Whether you're using LibreChat today or are interested in building analytical agents, please <a href="https://clickhouse.com/company/contact" target="_blank">contact us</a>!</p></div></article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Benchmarking leading AI agents against Google reCAPTCHA v2 (117 pts)]]></title>
            <link>https://research.roundtable.ai/captcha-benchmarking/</link>
            <guid>45877698</guid>
            <pubDate>Mon, 10 Nov 2025 16:38:31 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://research.roundtable.ai/captcha-benchmarking/">https://research.roundtable.ai/captcha-benchmarking/</a>, See on <a href="https://news.ycombinator.com/item?id=45877698">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    <p>
      Many sites use CAPTCHAs to distinguish humans from automated traffic. How well do these CAPTCHAs hold up against
      modern AI agents?
      We tested three leading models—Claude Sonnet 4.5, Gemini 2.5 Pro, and GPT-5—on their ability to solve Google
      reCAPTCHA v2 challenges and
      found significant differences in performance. Claude Sonnet 4.5 performed best with a 60% success rate, slightly
      outperforming Gemini 2.5 Pro
      at 56%. GPT-5 performed significantly worse and only managed to solve CAPTCHAs on 28% of trials.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/results.png" alt="Success rates by model">
      <figcaption><b>Figure&nbsp;1: </b>
        Overall success rates for each AI model. Claude Sonnet 4.5 achieved the highest success rate at 60%, followed by
        Gemini 2.5 Pro at 56% and GPT-5 at 28%.
      </figcaption>
    </figure>

    <p>
      Each reCAPTCHA challenge falls into one of three types: Static, Reload, and Cross-tile (see Figure 2). The models'
      success was highly dependent on this challenge type.
      In general, all models performed best on Static challenges and worst on Cross-tile challenges.
    </p>


    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-types.jpg" alt="CAPTCHA types used by reCAPTCHA v2">
      <table>
        <thead>
          <tr>
            <th>Model</th>
            <th>Static</th>
            <th>Reload</th>
            <th>Cross-tile</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Claude Sonnet 4.5</td>
            <td>47.1%</td>
            <td>21.2%</td>
            <td>0.0%</td>
          </tr>
          <tr>
            <td>Gemini 2.5 Pro</td>
            <td>56.3%</td>
            <td>13.3%</td>
            <td>1.9%</td>
          </tr>
          <tr>
            <td>GPT-5</td>
            <td>22.7%</td>
            <td>2.1%</td>
            <td>1.1%</td>
          </tr>
        </tbody>
      </table>
      <figcaption><b>Figure&nbsp;2: </b>
        The three types of reCAPTCHA v2 challenges. Static presents a static 3x3 grid; Reload
        dynamically replaces clicked images, and Cross-tile uses a 4x4 grid with objects potentially spanning
        multiple squares. The table shows model performance by CAPTCHA type. 
        Success rates are lower than in Figure 1 as these rates are at the challenge level,
        rather than trial level. Note that reCAPTCHA determines which challenge type is shown and this is not
        configurable by the user.

      </figcaption>
    </figure>


    <h3>
      Model analysis
    </h3>

    <p>
      Why did Claude and Gemini perform better than GPT-5? We found the difference was largely due to excessive and
      obsessive reasoning.
      Browser Use executes tasks as a sequence of discrete steps — the agent generates "Thinking" tokens to reason about
      the next step,
      chooses a set of actions, observes the response, and repeats. Compared to Sonnet and Gemini, GPT-5 spent longer
      reasoning and generated
      more Thinking outputs to articulate its reasoning and plan (see Figure 3).
    </p>

    <p>
      These issues were compounded by poor planning and verification: GPT-5 obsessively made edits and corrections to
      its solutions,
      clicking and unclicking the same square repeatedly. Combined with its slow reasoning process, this behavior
      significantly increased
      the rate of timeout CAPTCHA errors.
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/reasoning-lengths.png" alt="Thinking characters by model">
      <figcaption><b>Figure&nbsp;3: </b>
        Average number of "Thinking" characters by model and grid size (Static and Reload CAPTCHAs are 3x3,
        and Cross-tile CAPTCHAs are 4x4). On every agent step, the model outputs a “Thinking” tag along with its
        reasoning about which actions it will take.
      </figcaption>
    </figure>

    <h3>
      CAPTCHA type analysis
    </h3>

    <p>
      Compared to Static challenges, all models performed worse on Reload and Cross-tile challenges.
      Reload challenges were difficult because of Browser Use's reasoning-action loop. Agents often clicked the
      correct initial squares and moved to submit their response, only to see new images appear or be instructed by
      reCAPTCHA to review their response. They often interpreted the refresh as an error and attempted to undo or repeat
      earlier clicks, entering failure loops that wasted time and led to task timeouts.
    </p>

    <figure>
      <video autoplay="" loop="" muted="" playsinline="" loading="lazy" aria-label="Gemini 2.5 Pro Cross-tile attempt">
        <source src="https://research.roundtable.ai/captcha-benchmarking/captcha-attempt.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <figcaption><b>Figure&nbsp;4: </b>
        Gemini 2.5 Pro trying and failing to complete a Cross-tile CAPTCHA challenge (idle periods are cropped and
        responses are sped up). Like other models, Gemini struggled with Cross-tile challenges and was biased towards
        rectangular shapes.
      </figcaption>
    </figure>

    <p>
      Cross-tile challenges exposed the models' perceptual weaknesses, especially on partial, occluded, and
      boundary-spanning objects.
      Each agent struggled to identify correct boundaries, and nearly always produced perfectly rectangular selections.
      Anecdotally,
      we find Cross-tile CAPTCHAs easier than Static and Reload CAPTCHAs—once we spot a single tile that matches the
      target, it's
      easy to identify the adjacent tiles that include the target. This difference in difficulty suggests fundamental
      differences in
      how humans and AI systems solve these challenges
    </p>

    <h3>
      Conclusion
    </h3>

    <p>
      What can developers and researchers learn from these results? More reasoning isn't always better.
      Ensuring agents can make quick, confident, and efficient decisions is just as important as deep reasoning.
      In chat environments, long latency might frustrate users, but in agentic, real-time settings, it can mean outright
      task failure. These failures can be compounded by suboptimal agentic architecture—in our case, an agent loop that
      encouraged
      obsession and responded poorly to dynamic interfaces. Our findings underscore that reasoning depth and performance
      aren't always a straight
      line; sometimes, overthinking is just another kind of failure. Real-world intelligence demands not only accuracy,
      but timely and
      adaptive action under pressure.
    </p>

    <h2>
      Methods
    </h2>

    <h3>
      Experimental design
    </h3>

    <p>
      Each Google reCAPTCHA v2 challenge presents users with visual challenges, asking them to identify specific objects like
      traffic lights, fire hydrants, or crosswalks in a grid of images (see Figure 5).
    </p>

    <figure>
      <img src="https://research.roundtable.ai/captcha-benchmarking/captcha-example.png" alt="Example reCAPTCHA v2 challenge">
      <figcaption><b>Figure&nbsp;5: </b>
        Example of a reCAPTCHA v2 challenge showing a 4x4 grid where the user must select all squares containing the
        motorcycle.
      </figcaption>
    </figure>



    <p>
      We instructed each agent to navigate to Google's reCAPTCHA demo page and solve the presented CAPTCHA challenge
      (explicit image-based challenges were presented on 100% of trials). Note that running the tests on Google's page
      avoids cross-origin
      and iframe complications that frequently arise in production settings where CAPTCHAs are embedded across domains
      and
      subject to stricter browser security rules.
    </p>

    <p>
      We evaluated generative AI models using <a href="https://browser-use.com/" target="_blank">Browser Use</a>, an
      open-source framework that enables AI agents to perform browser-based tasks. We gave each agent the following instructions 
      when completing the CAPTCHA:
    </p>

    <p>
      1. Go to: https://www.google.com/recaptcha/api2/demo <br>
      2. Complete the CAPTCHA. On each CAPTCHA challenge, follow these steps:<br>
      2a. Identify the images that match the prompt and select them. <br>
      2b. Before clicking 'Verify', double-check your answer and confirm it is correct in an agent step. <br>
      2c. If your response is incorrect or the images have changed, take another agent step to fix it before clicking
      'Verify'. <br>
      2d. Once you confirm your response is correct, click 'Verify'. Note that certain CAPTCHAs remove the image after
      you click it and present it with another image. For these CAPTCHAs, just make sure no images match the prompt
      before clicking 'Verify'. <br>
      3. Try at most 5 different CAPTCHA challenges. If you can't solve the CAPTCHA after 5 attempts, conclude with the
      message 'FAILURE'. If you can, conclude with 'SUCCESS'. Do not include any other text in your final message. <br>
    </p>

    <p>
      Agents were instructed to try up to five different CAPTCHAs. Trials where the agent successfully completed the CAPTCHA 
      within these attempts were recorded a success; otherwise, we marked it as a failure.
    </p>

    <p>
      Although we instructed the models to attempt no more than five challenges per trial, agents often exceeded 
      this limit and tried significantly more CAPTCHAs. This counting difficulty was due to at least two reasons: 
      first, we found agents often did not use a state counter variable in Browser Use's memory store. Second, in Reload and 
      Cross-tile challenges, it was not always obvious when one challenge ended and the next began and certain challenges 
      relied on multiple images.<sup>1</sup> For consistency, we treated each discrete image the agent tried to label as a separate attempt, 
      resulting in 388 total attempts across 75 trials (agents were allowed to continue until they determined failure on their own).
    </p>

  </div></div>]]></description>
        </item>
    </channel>
</rss>