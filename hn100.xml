<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Fri, 07 Feb 2025 08:30:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Mad at Meta? Don't Let Them Collect and Monetize Your Personal Data (125 pts)]]></title>
            <link>https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data</link>
            <guid>42969272</guid>
            <pubDate>Fri, 07 Feb 2025 04:09:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data">https://www.eff.org/deeplinks/2025/01/mad-meta-dont-let-them-collect-and-monetize-your-personal-data</a>, See on <a href="https://news.ycombinator.com/item?id=42969272">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <article role="article">
  
  
  <div><p><span>If you‚Äôre fed up with Meta right now, you‚Äôre not alone. Google searches for deleting Facebook and Instagram </span><a href="https://techcrunch.com/2025/01/09/google-searches-for-deleting-facebook-instagram-explode-after-meta-ends-fact-checking/"><span>spiked last week</span></a><span> after Meta announced its latest policy changes. These changes, seemingly </span><a href="https://www.eff.org/deeplinks/2025/01/metas-new-content-policy-will-harm-vulnerable-users-if-it-really-valued-free"><span>designed to appease</span></a><span> the incoming Trump administration, included </span><a href="https://www.washingtonpost.com/business/2025/01/08/meta-facebook-hate-speech-trump-immigrant-transgender/034e3410-ce0f-11ef-be73-ad8966084721_story.html"><span>loosening Meta‚Äôs hate speech policy</span></a><span> to allow for the targeting of LGBTQ+ people and immigrants.&nbsp;</span></p>
<p><span>If these changes‚Äî</span><a href="https://www.eff.org/deeplinks/2018/12/facebooks-sexual-solicitation-policy-honeypot-trolls"><span>or</span></a> <a href="https://www.eff.org/deeplinks/2019/03/privacy-focused-facebook-well-believe-it-when-we-see-it"><span>Meta‚Äôs</span></a> <a href="https://www.eff.org/deeplinks/2011/10/facebook%E2%80%99s-hotel-california-cross-site-tracking-and-potential-impact-digital-privacy"><span>long</span></a> <a href="https://www.eff.org/deeplinks/2021/08/facebooks-secret-war-switching-costs"><span>history</span></a> <a href="https://www.404media.co/meta-is-blocking-links-to-decentralized-instagram-competitor-pixelfed/"><span>of</span></a> <a href="https://www.eff.org/deeplinks/2020/11/once-again-facebook-using-privacy-sword-kill-independent-innovation"><span>anti-competitive</span></a><span>, </span><a href="https://www.eff.org/deeplinks/2021/05/amid-systemic-censorship-palestinian-voices-facebook-owes-users-transparency"><span>censorial</span></a><span>, </span><a href="https://www.eff.org/deeplinks/2015/06/facebook-reforms-inmate-account-takedown-process"><span>and</span></a> <a href="https://www.eff.org/deeplinks/2010/04/facebook-timeline"><span>invasive</span></a> <a href="https://www.eff.org/deeplinks/2021/08/facebooks-attack-research-everyones-problem"><span>practices</span></a><span>‚Äîmake you want to cut ties with the company, it‚Äôs sadly not as simple as deleting your Facebook account or spending less time on Instagram. Meta tracks your activity across millions of websites and apps, regardless of whether you use its platforms, and it profits from that data through targeted ads. If you want to limit Meta‚Äôs ability to collect and profit from your personal data, here‚Äôs what you need to know.</span></p>
<h3><span>Meta‚Äôs Business Model Relies on Your Personal Data</span></h3>
<p><span>You might think of Meta as a social media company, but its primary business is </span><a href="https://www.eff.org/deeplinks/2023/05/save-news-we-must-ban-surveillance-advertising"><span>surveillance advertising</span></a><span>. Meta‚Äôs </span><a href="https://www.eff.org/deeplinks/2021/11/after-facebook-leaks-here-what-should-come-next"><span>business model</span></a><span> relies on collecting as much information as possible about people in order to sell highly-targeted ads. That‚Äôs why Meta is </span><a href="https://duckduckgo.github.io/tracker-radar-wiki/"><span>one of the main companies</span></a><span> tracking you across the internet‚Äîmonitoring your activity far beyond its own platforms. When Apple introduced changes to make tracking harder on iPhones, </span><a href="https://www.forbes.com/sites/kateoflahertyuk/2021/11/06/apples-new-iphone-privacy-features-cost-facebook-10-billion/"><span>Meta lost billions in revenue</span></a><span>, demonstrating just how valuable your personal data is to its business.&nbsp;</span></p>
<h3><span>How Meta Harvests Your Personal Data</span></h3>
<p><span>Meta‚Äôs tracking tools are embedded in </span><a href="https://trends.builtwith.com/analytics/Facebook-Pixel"><span>millions</span></a><span> of </span><a href="https://themarkup.org/newsletter/hello-world/facebooks-pervasive-pixel"><span>websites</span></a><span> and </span><a href="https://privacyinternational.org/report/2647/how-apps-android-share-data-facebook-report"><span>apps</span></a><span>, so you can‚Äôt escape the company‚Äôs surveillance just by avoiding or deleting Facebook and Instagram. Meta‚Äôs tracking pixel, found on </span><a href="https://themarkup.org/newsletter/hello-world/facebooks-pervasive-pixel"><span>30% of the world‚Äôs most popular websites</span></a><span>, monitors people‚Äôs behavior across the web and can expose sensitive information, including </span><a href="https://themarkup.org/pixel-hunt/2022/04/28/applied-for-student-aid-online-facebook-saw-you"><span>financial</span></a><span> and </span><a href="https://www.ftc.gov/news-events/news/press-releases/2023/03/ftc-ban-betterhelp-revealing-consumers-data-including-sensitive-mental-health-information-facebook"><span>mental</span></a> <a href="https://www.ftc.gov/news-events/news/press-releases/2024/04/proposed-ftc-order-will-prohibit-telehealth-firm-cerebral-using-or-disclosing-sensitive-data"><span>health</span></a><span> data. </span><a href="https://themarkup.org/newsletter/hello-world/facebooks-pervasive-pixel"><span>A 2022 investigation by </span><i><span>The Markup</span></i></a><span> found that a third of the top U.S. hospitals had sent sensitive patient information to Meta through its tracking pixel.&nbsp;</span></p>
<p><span>Meta‚Äôs surveillance isn‚Äôt limited to your online activity. The company also </span><a href="https://www.facebook.com/business/help/1142103235885551?id=565900110447546"><span>encourages businesses</span></a><span> to send them data about your offline purchases and interactions. Even deleting your Facebook and Instagram accounts won‚Äôt stop Meta from harvesting your personal data. Meta in 2018 admitted to </span><a href="https://www.vox.com/2018/4/20/17254312/facebook-shadow-profiles-data-collection-non-users-mark-zuckerberg"><span>collecting information about non-users</span></a><span>, including their contact details and browsing history.</span></p>
<h3><span>Take These Steps to Limit How Meta Profits From Your Personal Data</span></h3>
<p><span>Although Meta‚Äôs surveillance systems are pervasive, there are ways to limit how Meta collects and uses your personal data.&nbsp;</span></p>
<h4><span>Update Your Meta Account Settings</span></h4>
<p><span>Open your Instagram or Facebook app and navigate to the </span><b><i>Accounts Center</i></b><span> page.&nbsp;</span></p>
<ul>
<li><span>You‚Äôll find a link to </span><b><i>Accounts Center</i></b><span> on the </span><b><i>Settings</i></b><span> pages of both apps. If you have trouble finding </span><b><i>Accounts Center</i></b><span>, check Meta‚Äôs help pages for </span><a href="https://www.facebook.com/help/247395082112892?cms_platform=www&amp;helpref=platform_switcher"><span>Facebook</span></a><span> and </span><a href="https://www.facebook.com/help/instagram/245100253430454"><span>Instagram</span></a><span>.&nbsp;</span></li>
<li><span>If you use a web browser instead of Meta‚Äôs apps, visit </span><a href="http://accountscenter.facebook.com/"><span>accountscenter.facebook.com</span></a><span> or </span><a href="http://accountscenter.instagram.com/"><span>accountscenter.instagram.com</span></a><span>.</span></li>
</ul>
<p><span><img src="https://www.eff.org/files/2025/01/17/metaaccountscenteroutlined.png" width="2154" height="1120" alt="A screenshot of the Meta Accounts Center page. " title="A screenshot of the Meta Accounts Center page. "></span></p>
<p><span>If your Facebook and Instagram accounts are linked on your </span><b><i>Accounts Center</i></b><span> page, you only have to update&nbsp;the following&nbsp;settings once. If not, you‚Äôll have to update them separately for Facebook and Instagram. Once you find your way to the </span><b><i>Accounts Center</i></b><span>, the directions below are the same for both platforms.</span></p>
<p><span>Meta makes it harder than it should be to find and update these settings. The following steps are accurate at the time of publication, but Meta often changes their settings and adds additional steps. The exact language below may not match what Meta displays in your region, but you should have a setting controlling each of the following permissions.</span></p>
<p>Once you‚Äôre on the ‚ÄúAccounts Center‚Äù page, make the following changes:</p>
<p><strong>1) Stop Meta from targeting ads based on data it collects about you on other apps and websites:&nbsp;</strong></p>
<blockquote><p>Click the <b><i>Ad preferences </i></b><span>option under Accounts Center, then select the </span><b><i>Manage Info</i></b><span> tab (this tab may be called </span><b><i>Ad settings</i></b><span> depending on your location). Click the </span><b><i>Activity information from ad partners</i></b><span> option, then </span><b><i>Review Setting</i></b><span>. Select the option for </span><b><i>No, don‚Äôt make my ads more relevant by using this information</i></b><span> and click the ‚ÄúConfirm‚Äù button when prompted.</span></p>
</blockquote>
<p><span><img src="https://www.eff.org/files/2025/01/17/adpartnerinfo.png" width="2474" height="1538" alt="A screenshot of the &quot;Activity information from ad partners&quot; setting with the &quot;No&quot; option selected" title="A screenshot of the &quot;Activity information from ad partners&quot; setting with the &quot;No&quot; option selected"></span></p>
<p><strong>2) Stop Meta from using your data (from Facebook and Instagram) to help advertisers target you on other apps.</strong>&nbsp;<a href="https://www.facebook.com/help/119468292028768">Meta‚Äôs ad network</a> connects advertisers with other apps through <a href="https://www.eff.org/deeplinks/2025/01/online-behavioral-ads-fuel-surveillance-industry-heres-how">privacy-invasive ad auctions</a>‚Äîgenerating more money and data for Meta in the process.</p>
<blockquote><p><span>Back on the </span><b><i>Ad preferences</i></b><span> page, click the </span><b><i>Manage info</i></b><span> tab again (called </span><b><i>Ad settings</i></b><span> depending on your location), </span><span>then select the </span><b><i>Ads shown outside of Meta </i></b><span>setting, select </span><b><i>Not allowed</i></b><span> and then click the ‚ÄúX‚Äù button to close the pop-up.</span></p>
<p>Depending on your location, this setting will be called <b><i>Ads from ad partners </i></b>on the <b><i>Manage info </i></b>tab.</p>
</blockquote>
<p><img src="https://www.eff.org/files/2025/01/17/adsoutsidemeta.png" width="2490" height="1236" alt="A screenshot of the &quot;Ads outside Meta&quot; setting with the &quot;Not allowed&quot; option selected" title="A screenshot of the &quot;Ads outside Meta&quot; setting with the &quot;Not allowed&quot; option selected"></p>
<p><strong>3) Disconnect the data that other companies share with Meta about you from your account:</strong></p>
<blockquote><p><span>From the </span><b><i>Accounts Center</i></b><span> screen, click the </span><b><i>Your information and permissions</i></b><span> option, followed by </span><b><i>Your activity off Meta technologies</i></b><span>, then </span><b><i>Manage future activity</i></b><span>. On this screen, choose the option to </span><b><i>Disconnect future activity</i></b><span>, followed by the </span><b><i>Continue</i></b><span> button, then confirm one more time by clicking the </span><b><i>Disconnect future activity</i></b><span> button. </span><b>Note</b><span>: This may take up to 48 hours to take effect.</span></p>
<p><span><b>Note: </b>This will also clear previous activity, which might log you out of apps and websites you‚Äôve signed into through Facebook.</span></p>
</blockquote>
<p><span><img src="https://www.eff.org/files/2025/01/17/disconnectactivity.png" width="2448" height="1292" alt="A screenshot of the &quot;Manage future activity&quot; setting with the &quot;Disconnect future activity&quot; option selected" title="A screenshot of the &quot;Manage future activity&quot; setting with the &quot;Disconnect future activity&quot; option selected"></span></p>
<p><span>While these settings </span><i><span>limit</span></i><span> how Meta uses your data, they won‚Äôt necessarily stop the company from collecting it and potentially using it for other purposes.&nbsp;</span></p>
<h4><span>Install Privacy Badger to Block Meta‚Äôs Trackers</span></h4>
<p><a href="https://privacybadger.org/"><span>Privacy Badger</span></a><span> is a free browser extension by EFF that blocks trackers‚Äîlike Meta‚Äôs pixel‚Äîfrom loading on websites you visit. It also replaces embedded Facebook posts, Like buttons, and Share buttons with </span><a href="https://www.eff.org/deeplinks/2024/01/privacy-badger-puts-you-control-widgets"><span>click-to-activate placeholders</span></a><span>, blocking another way that Meta tracks you. The next version of Privacy Badger (coming next week) will extend this protection to embedded Instagram and Threads posts, which also send your data to Meta.</span></p>
<p><span>Visit </span><a href="https://privacybadger.org/"><span>privacybadger.org</span></a><span> to install Privacy Badger on your web browser. Currently, Firefox on Android is the only mobile browser that supports Privacy Badger.&nbsp;</span></p>
<h4><span>Limit Meta‚Äôs Tracking on Your Phone</span></h4>
<p><span>Take these additional steps on your mobile device:</span></p>
<ul>
<li><b>Disable your phone‚Äôs advertising ID</b><span> to make it harder for Meta to track what you do across apps. Follow EFF‚Äôs instructions for doing this on your </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#disable-ad-tracking"><span>iPhone</span></a><span> or </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#disable-ad-tracking"><span>Android</span></a><span> device.</span></li>
<li><b>Turn off location access for Meta‚Äôs apps</b><span>. Meta doesn‚Äôt need to know where you are all the time to function, and you can safely disable location access without affecting how the Facebook and Instagram apps work. Review this setting using EFF‚Äôs guides for your </span><a href="https://ssd.eff.org/module/how-to-get-to-know-iphone-privacy-and-security-settings#audit-your-privacy-permissions"><span>iPhone</span></a><span> or </span><a href="https://ssd.eff.org/module/how-to-get-to-know-android-privacy-and-security-settings#audit-your-privacy-permissions"><span>Android</span></a><span> device.</span></li>
</ul>
<h3><span>The Real Solution: Strong Privacy Legislation</span></h3>
<p><span>Stopping a company you distrust from profiting off your personal data shouldn‚Äôt require tinkering with hidden settings and installing browser extensions. Instead, your data should be private by default. That‚Äôs why we need </span><a href="https://www.eff.org/wp/privacy-first-better-way-address-online-harms#Legislation"><span>strong federal privacy legislation</span></a><span> that puts you‚Äînot Meta‚Äîin control of your information.&nbsp;</span></p>
<p><span>Without strong privacy legislation, Meta will <a href="https://mashable.com/article/iphone-apps-push-notifications-data-collection-report">keep</a> <a href="https://petsymposium.org/popets/2024/popets-2024-0086.pdf">finding</a> <a href="https://techcrunch.com/2024/04/15/consent-or-pay-open-letter-edpb/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAF-zi8O-D6A1dZPGbzlMH8M_d2Zl2b4EXKjC3wbwlYJ9BuGp_0wNdFm57oh3xyaEyTq8SVGrJv2QVO1uylN0roSaotvmBvW9gBLtFJK_kAcJnCQm7nouxWpLqSl6wEmuAp8uDYT3eaD8c71sXwPC9p-ZZPISupt7P3t7GdRmejbw">ways</a> to bypass your privacy protections and monetize your personal data. Privacy is about more than safeguarding your sensitive information‚Äîit‚Äôs about having the power to prevent companies like Meta from exploiting your personal data for profit.</span></p>

</div>

          </article>
    </div><div>
          <h2>Join EFF Lists</h2>
        
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Transformer ‚Äì Spreadsheet (147 pts)]]></title>
            <link>https://www.byhand.ai/p/transformer-spreadsheet</link>
            <guid>42968547</guid>
            <pubDate>Fri, 07 Feb 2025 02:13:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.byhand.ai/p/transformer-spreadsheet">https://www.byhand.ai/p/transformer-spreadsheet</a>, See on <a href="https://news.ycombinator.com/item?id=42968547">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><div dir="auto"><p><span>üîó Google Sheet: </span><a href="https://by-hand.ai/sp/tfmr" rel="">https://by-hand.ai/sp/tfmr</a></p><div><figure><a target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif" data-component-name="Image2ToDOM" rel=""><div><picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 1456w" sizes="100vw"><img src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif" width="1080" height="1080" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1080,&quot;width&quot;:1080,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1128954,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/gif&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false}" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe37c91-51c4-423a-8c79-8a14982484c5_1080x1080.gif 1456w" sizes="100vw" fetchpriority="high"></picture></div></a></figure></div><p>Over the past few months, I've collaborated with several AI educators to customize my AI by Hand exercises. I am glad that my materials are being used and appreciated in many classrooms around the world! </p><p>However, because the customization process is entirely by hand, sometimes my solution contains errors, only to be caught by students, which actually made me happy that students are paying attention. üòÖ</p><p> Lately, I have been thinking about developing a tool to let people create their own AI by Hand exercises, with custom numbers and solutions. </p><p>After considering a range of technologies, I decided to use Google Sheets. My goal is to maximize reach and broaden access. </p><p>Because this tool is still in the early stage, I would really appreciate your feedback!</p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Elon Musk's Demolition Crew (158 pts)]]></title>
            <link>https://projects.propublica.org/elon-musk-doge-tracker/</link>
            <guid>42968430</guid>
            <pubDate>Fri, 07 Feb 2025 01:55:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://projects.propublica.org/elon-musk-doge-tracker/">https://projects.propublica.org/elon-musk-doge-tracker/</a>, See on <a href="https://news.ycombinator.com/item?id=42968430">Hacker News</a></p>
Couldn't get https://projects.propublica.org/elon-musk-doge-tracker/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[California bill would require bots to disclose that they are bots (264 pts)]]></title>
            <link>https://www.veeto.app/bill/1955756</link>
            <guid>42968347</guid>
            <pubDate>Fri, 07 Feb 2025 01:41:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.veeto.app/bill/1955756">https://www.veeto.app/bill/1955756</a>, See on <a href="https://news.ycombinator.com/item?id=42968347">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Frank Lloyd Wright's mile high skyscraper proposal (2021) (126 pts)]]></title>
            <link>https://www.onverticality.com/blog/frank-lloyd-wright-mile-high-skyscraper</link>
            <guid>42967226</guid>
            <pubDate>Thu, 06 Feb 2025 22:39:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.onverticality.com/blog/frank-lloyd-wright-mile-high-skyscraper">https://www.onverticality.com/blog/frank-lloyd-wright-mile-high-skyscraper</a>, See on <a href="https://news.ycombinator.com/item?id=42967226">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page" role="main">
        
          
<article id="sections" data-page-sections="5e1e630093fea070e53da8f3">
  
  
    
    


  
  


<div data-content-field="main-content" data-item-id="" data-test="page-section" data-section-theme="" data-section-id="5e1e630093fea070e53da8f5" data-controller="SectionWrapperController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;customSectionHeight&quot;: 10,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;customContentWidth&quot;: 50,
&quot;sectionTheme&quot;: &quot;&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-current-context="{
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0,
&quot;videoSourceProvider&quot;: &quot;none&quot;
},
&quot;backgroundImageId&quot;: null,
&quot;backgroundMediaEffect&quot;: null,
&quot;divider&quot;: null,
&quot;typeName&quot;: &quot;blog-masonry&quot;
}" data-animation="none">
  <article id="article-">
  
    
    
    
    <div data-layout-label="Post Body" data-type="item" id="item-5ff089870493bd28271993a7"><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1609599376423_3875">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg" data-image-dimensions="1200x2043" data-image-focal-point="0.5,0.5" alt="Main perspective rendering and project description for Frank Lloyd Wright‚Äôs Illinois skyscraper proposal." data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg" width="1200" height="2043" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599505893-FRV21KJMRBY89P12FCA6/960_epiteszforum-lakotornyok-chicago-varosepiteszete-02--benko-melinda-sorozata-3-230-2020-07-07-212257.jpeg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Main perspective rendering and project description for Frank Lloyd Wright‚Äôs </em>Illinois<em> skyscraper proposal.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-ae7320ebb21463d04d96"><p>In the debate over density, architects and planners are split into two camps. The first is pro-density, which believes in dense, centralized cities that function through complex mass-transit systems and clusters of skyscrapers. This is the pro-city crowd. The second is the anti-density camp, which believes in de-centralized, spread-out networks of neighborhoods that rely on automobiles and low buildings. This is the pro-suburb crowd. Modern architect Frank Lloyd Wright was an outspoken member of the latter camp, and his visionary <em>Broadacre City</em> project is a model suburb, with low buildings and green space prioritized over tall buildings and density.</p><p>In addition to his anti-city stance, Wright is famous for his <em>organic</em> approach to architecture, which is based on a building‚Äôs intimate connection to its surroundings. Organic buildings respond to their context and should be unique to their site. The antithesis of this is a design that can be built anywhere and is out-of-context. This includes skyscrapers and tall buildings in general. With this in mind, it‚Äôs hard to believe the tower design pictured above came from Wright. It‚Äôs called <em>The Illinois</em>, and it was planned to be a mile (1,609 meters, or 5,280 feet) in height. That‚Äôs more than four times the height of the Empire State Building, and nearly twice the height of the Burj Dubai.</p></div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1609599376423_7697">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg" data-image-dimensions="1200x1572" data-image-focal-point="0.5,0.5" alt="Concept sketch of Frank Lloyd Wright‚Äôs Illinois skyscraper proposal. Wright shows the main silhouette of the tower, along with height comparisons of other famous tall buildings, around the world." data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg" width="1200" height="1572" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599710678-7QTS6WYY6D8BFGQMX3M9/1957-FrankLloydWright-Illinois-2.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Concept sketch of Frank Lloyd Wright‚Äôs </em>Illinois<em> skyscraper proposal. Wright shows the main silhouette of the tower, along with height comparisons of other famous tall buildings throughout history.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1609599376423_8962">

<p>Wright‚Äôs justification for the project was quite clever. He argued that the sheer size of The Illinois would encapsulate an entire city within a single building, which would kill two birds with one stone. It would provide the density that people crave, while freeing up the surrounding landscape for his Broadacre City plan, with its parks and low buildings. Essentially, Wright was using the <em>if you can‚Äôt beat ‚Äòem, join ‚Äòem</em> mentality when it came to the city. Give ‚Äòem their density so they can come together, but concentrate it into one massive structure and surround it with your larger vision for the world-as-one-big-suburb. He envisioned skyscrapers like this built here and there throughout the countryside, taking the place of cities. That way, everyone could live in a suburban paradise, surrounded by green space and ample daylight.</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1609599376423_9422">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg" data-image-dimensions="1200x928" data-image-focal-point="0.5,0.5" alt="Floor plan illustration of Frank Lloyd Wright‚Äôs Illinois skyscraper." data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg" width="1200" height="928" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599728611-KKSSZGVNUDKJD9UC37SO/1957-FrankLloydWright-Illinois-3.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Floor plan illustration of Frank Lloyd Wright‚Äôs </em>Illinois<em> skyscraper.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1609599376423_10803">

<p>The tower itself had a tripod-like design, with three wings of structure and a central core. This tripod, combined with a tapered form that came to a point at the tower‚Äôs summit, were used by Wright to shed wind forces. At the base, Wright used his <em>taproot</em> foundation design that took inspiration from a tree. He had previously used it for another of his tower designs, the S.C. Johnson Research Tower. The foundation, much like the tower, burrows into the earth and progresses down to a point at its base.</p>




















  
  



</div><div data-test="image-block-inline-outer-wrapper" data-block-type="5" id="block-yui_3_17_2_1_1609599376423_11260">

      

      
        <figure>
          
        
        

        
          
            
          <div data-animation-role="image">
            <p><img data-stretch="false" data-src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg" data-image-dimensions="1000x1144" data-image-focal-point="0.5,0.5" alt="Section illustration showing the base of Frank Lloyd Wright‚Äôs Illinois skyscraper. Pictured is Wright‚Äôs taproot foundation, which is inspired by tree structures." data-load="false" elementtiming="system-image-block" src="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg" width="1000" height="1144" sizes="(max-width: 640px) 100vw, (max-width: 767px) 100vw, 100vw" onload="this.classList.add(&quot;loaded&quot;)" srcset="https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=100w 100w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=300w 300w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=500w 500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=750w 750w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=1000w 1000w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=1500w 1500w, https://images.squarespace-cdn.com/content/v1/5e1e5ef4c84b953ac52564ba/1609599746475-4PU6MYEOFMLGK3OXHJEF/1957-FrankLloydWright-Illinois-4.jpg?format=2500w 2500w" loading="lazy" decoding="async" data-loader="sqs">

            </p>
          </div>
        
          
        

        
          
          <figcaption>
            <p><em>Section illustration showing the base of Frank Lloyd Wright‚Äôs </em>Illinois<em> skyscraper. Pictured is Wright‚Äôs taproot foundation, which is inspired by tree structures.</em></p>
          </figcaption>
        
      
        </figure>
      

    </div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1609599376423_12524">
  <p>It‚Äôs telling that an architect as outwardly anti-density as Wright would put in a design for the world‚Äôs tallest building. It‚Äôs also telling that he went a step further and designed a <em>mile-high</em> building, which is far and away taller than any other building ever built. This suggests two theories about his motivations. First, he exaggerated the height of The Illinois so much because he was trying to point out the absurdity of the tall building race and skyscrapers in general. This wink-wink approach seems a bit far-fetched, but still believable considering Wright‚Äôs massive ego. Second, and much more believable, is that Wright was silently acknowledging the need for humanity to achieve verticality, even if it ran contrary to his beliefs about the city. After all, he was no stranger to the building type; his previous tower designs include the S.C. Johnson Research Tower, Saint Mark‚Äôs Tower, and Price Tower. In addition, he‚Äôs quoted as saying that <em>towers have always been erected by humankind - it seems to gratify humanity‚Äôs ambition somehow and they are beautiful and picturesque.</em>[1] This is Wright acknowledging the human need for verticality, and his design for The Illinois is evidence that even those of us who are anti-density still have this need within themselves.</p>
</div></div>
  
</article>

</div>

  
</article>


          

          
            
              

            
          
        
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Understanding Reasoning LLMs (270 pts)]]></title>
            <link>https://magazine.sebastianraschka.com/p/understanding-reasoning-llms</link>
            <guid>42966720</guid>
            <pubDate>Thu, 06 Feb 2025 21:34:12 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://magazine.sebastianraschka.com/p/understanding-reasoning-llms">https://magazine.sebastianraschka.com/p/understanding-reasoning-llms</a>, See on <a href="https://news.ycombinator.com/item?id=42966720">Hacker News</a></p>
Couldn't get https://magazine.sebastianraschka.com/p/understanding-reasoning-llms: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[DOGE staffer resigns over racist posts (130 pts)]]></title>
            <link>https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93</link>
            <guid>42966412</guid>
            <pubDate>Thu, 06 Feb 2025 20:55:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93">https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93</a>, See on <a href="https://news.ycombinator.com/item?id=42966412">Hacker News</a></p>
Couldn't get https://www.wsj.com/tech/doge-staffer-resigns-over-racist-posts-d9f11a93: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[R1 Computer Use (104 pts)]]></title>
            <link>https://github.com/agentsea/r1-computer-use</link>
            <guid>42965954</guid>
            <pubDate>Thu, 06 Feb 2025 20:02:03 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/agentsea/r1-computer-use">https://github.com/agentsea/r1-computer-use</a>, See on <a href="https://news.ycombinator.com/item?id=42965954">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">R1 Computer Use</h2><a id="user-content-r1-computer-use" aria-label="Permalink: R1 Computer Use" href="#r1-computer-use"></a></p>
<p dir="auto">Applying the ideas of <a href="https://github.com/deepseek-ai/DeepSeek-R1">Deepseek R1</a> and <a href="https://github.com/huggingface/open-r1">Open R1</a> to computer use.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Overview</h2><a id="user-content-overview" aria-label="Permalink: Overview" href="#overview"></a></p>
<p dir="auto">r1-computer-use is an experimental project that applies large-scale Reinforcement Learning techniques similar to DeepSeek-R1 to computer usage scenarios. The primary goal is to train an agent to interact with a computer environment (e.g., file system, web browser, command line) while utilizing a neural reward model to validate the correctness of the agent‚Äôs actions and reason about intermediate steps.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Architecture</h2><a id="user-content-architecture" aria-label="Permalink: Architecture" href="#architecture"></a></p>
<p dir="auto">DeepSeek-R1 has shown that large language models can develop powerful reasoning skills through iterative reward optimization. Traditionally, such projects rely on hard verifiers or rule-based scripts to determine correctness in tasks like math or coding. However, these methods are too difficult to reproduce at scale for general computer usage.</p>
<p dir="auto">We aim to replace hard-coded verifiers with a neural reward model that itself reasons about whether or not the agent‚Äôs actions are correct or helpful.</p>
<p dir="auto">Both the actor and reward models follow a three-step cycle which can be seen as an extention of <a href="https://react-lm.github.io/" rel="nofollow">ReACT</a> into reinforcement learning.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/agentsea/r1-computer-use/blob/main/static/rac.svg"><img src="https://github.com/agentsea/r1-computer-use/raw/main/static/rac.svg" alt="diagram" width="500"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Agent</h2><a id="user-content-agent" aria-label="Permalink: Agent" href="#agent"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="observation = &quot;Current directory contains: setup.py requirements.txt&quot;
reasoning = &quot;&quot;&quot;
1. Project appears to be a Python package
2. No virtual environment detected
3. Should create venv before proceeding
&quot;&quot;&quot;
action = &quot;python -m venv .venv&quot;"><pre><span>observation</span> <span>=</span> <span>"Current directory contains: setup.py requirements.txt"</span>
<span>reasoning</span> <span>=</span> <span>"""</span>
<span>1. Project appears to be a Python package</span>
<span>2. No virtual environment detected</span>
<span>3. Should create venv before proceeding</span>
<span>"""</span>
<span>action</span> <span>=</span> <span>"python -m venv .venv"</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reward Model</h2><a id="user-content-reward-model" aria-label="Permalink: Reward Model" href="#reward-model"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="analysis = &quot;&quot;&quot;
1. Correctly identified project type
2. Appropriate prerequisite check
3. Standard venv location chosen
&quot;&quot;&quot;
reward = 0.85"><pre><span>analysis</span> <span>=</span> <span>"""</span>
<span>1. Correctly identified project type</span>
<span>2. Appropriate prerequisite check</span>
<span>3. Standard venv location chosen</span>
<span>"""</span>
<span>reward</span> <span>=</span> <span>0.85</span></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage (in progress)</h2><a id="user-content-usage-in-progress" aria-label="Permalink: Usage (in progress)" href="#usage-in-progress"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="from r1_computer_use import Agent, RewardModel

agent = Agent()
reward_model = RewardModel()

result = agent.run(
    task=&quot;Set up Python development environment&quot;,
    observe_reasoning=True
)

feedback = reward_model.evaluate(
    actions=result.actions,
    reasoning=result.reasoning
)"><pre><span>from</span> <span>r1_computer_use</span> <span>import</span> <span>Agent</span>, <span>RewardModel</span>

<span>agent</span> <span>=</span> <span>Agent</span>()
<span>reward_model</span> <span>=</span> <span>RewardModel</span>()

<span>result</span> <span>=</span> <span>agent</span>.<span>run</span>(
    <span>task</span><span>=</span><span>"Set up Python development environment"</span>,
    <span>observe_reasoning</span><span>=</span><span>True</span>
)

<span>feedback</span> <span>=</span> <span>reward_model</span>.<span>evaluate</span>(
    <span>actions</span><span>=</span><span>result</span>.<span>actions</span>,
    <span>reasoning</span><span>=</span><span>result</span>.<span>reasoning</span>
)</pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Training Pipeline</h2><a id="user-content-training-pipeline" aria-label="Permalink: Training Pipeline" href="#training-pipeline"></a></p>
<p dir="auto">The training pipeline consists of multiple stages:</p>
<ol dir="auto">
<li>
<p dir="auto"><strong>Cold Start</strong></p>
<ul dir="auto">
<li>Expert demonstrations with reasoning traces</li>
<li>Initial reward model training</li>
<li>Base model fine-tuning</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Reasoning-Focused GRPO</strong></p>
<ul dir="auto">
<li>Group-based sampling from current policy</li>
<li>Reward model evaluates each group</li>
<li>Compute advantages within groups</li>
<li>Policy updates with clipped probability ratios</li>
<li>KL divergence constraint with reference policy</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Rejection Sampling Stage</strong></p>
<ul dir="auto">
<li>Filter top-k solutions based on reward model</li>
<li>Create new training dataset from best examples</li>
<li>Fine-tune base model on filtered data</li>
</ul>
</li>
<li>
<p dir="auto"><strong>General Preference Alignment</strong></p>
<ul dir="auto">
<li>Apply RL to full task distribution</li>
<li>Use reward models for general preferences</li>
<li>Focus on helpfulness and safety</li>
<li>Evaluate complete responses</li>
</ul>
</li>
<li>
<p dir="auto"><strong>Evaluation</strong></p>
<ul dir="auto">
<li>Task completion metrics</li>
<li>Reasoning quality assessment</li>
<li>Safety verification</li>
<li>Distribution shift analysis</li>
</ul>
</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Roadmap</h2><a id="user-content-roadmap" aria-label="Permalink: Roadmap" href="#roadmap"></a></p>
<ul>
<li> Collect cold startand neural reward model data (in progress)</li>
<li> SFT train base model</li>
<li> GRPO RL training</li>
<li> Rejection sampling</li>
<li> General preference alignment</li>
<li> Evaluation</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Research</h2><a id="user-content-research" aria-label="Permalink: Research" href="#research"></a></p>
<p dir="auto">Current areas of investigation:</p>
<ul dir="auto">
<li>Reward model architectures</li>
<li>Base model evaluations</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">MIT</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Citation</h2><a id="user-content-citation" aria-label="Permalink: Citation" href="#citation"></a></p>
<div dir="auto" data-snippet-clipboard-copy-content="@software{r1_computer_use,
  title     = {R1-Computer-Use: Reasoning-First Computer Interaction},
  author    = {Barker, Patrick},
  year      = {2025},
  url       = {https://github.com/agentsea/r1-computer-use},
}"><pre><span>@software</span>{<span>r1_computer_use</span>,
  <span>title</span>     = <span><span>{</span>R1-Computer-Use: Reasoning-First Computer Interaction<span>}</span></span>,
  <span>author</span>    = <span><span>{</span>Barker, Patrick<span>}</span></span>,
  <span>year</span>      = <span><span>{</span>2025<span>}</span></span>,
  <span>url</span>       = <span><span>{</span>https://github.com/agentsea/r1-computer-use<span>}</span></span>,
}</pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An API that takes a URL and returns a file with browser screenshots (160 pts)]]></title>
            <link>https://github.com/US-Artificial-Intelligence/scraper</link>
            <guid>42965267</guid>
            <pubDate>Thu, 06 Feb 2025 18:48:05 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/US-Artificial-Intelligence/scraper">https://github.com/US-Artificial-Intelligence/scraper</a>, See on <a href="https://news.ycombinator.com/item?id=42965267">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">Simple URL üåê to Screenshots üì∑ API</h2><a id="user-content-simple-url--to-screenshots--api" aria-label="Permalink: Simple URL üåê to Screenshots üì∑ API" href="#simple-url--to-screenshots--api"></a></p>
<p dir="auto">You run the API on your machine, you send it a URL, and you get back the website data as a file plus screenshots of the site. Simple as.</p>
<p dir="auto">This project was made to support <a href="https://github.com/US-Artificial-Intelligence/abbey">Abbey</a>, an AI platform. Its author is <a href="https://x.com/gkamer8" rel="nofollow">Gordon Kamer</a>.</p>
<p dir="auto">Some highlights:</p>
<ul dir="auto">
<li>Scrolls through the page and takes screenshots of different sections</li>
<li>Runs in a docker container</li>
<li>Browser-based (will run websites' Javascript)</li>
<li>Gives you the HTTP status code and headers from the first request</li>
<li>Automatically handles 302 redirects</li>
<li>Handles download links properly</li>
<li>Tasks are processed in a queue with configurable memory allocation</li>
<li>Blocking API</li>
<li>Zero state or other complexity</li>
</ul>
<p dir="auto">This web scraper is resource intensive but higher quality than many alternatives. Websites are scraped using Playwright, which launches a Firefox browser context for each job.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Setup</h2><a id="user-content-setup" aria-label="Permalink: Setup" href="#setup"></a></p>
<p dir="auto">You should have Docker and <code>docker compose</code> installed.</p>
<ol dir="auto">
<li>Clone this repo</li>
<li>Run <code>docker compose up</code> (a <code>docker-compose.yml</code> file is provided for your use)</li>
</ol>
<p dir="auto">...and the service will be available at <code>http://localhost:5006</code>. See the Usage section below for details on how to interact with it.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">API Keys</h3><a id="user-content-api-keys" aria-label="Permalink: API Keys" href="#api-keys"></a></p>
<p dir="auto">You may set an API key using a <code>.env</code> file inside the <code>/scraper</code> folder (same level as <code>app.py</code>).</p>
<p dir="auto">You can set as many API keys as you'd like; allowed API keys are those that start with <code>SCRAPER_API_KEY</code>. For example, here is a <code>.env</code> file that has three available keys:</p>
<div data-snippet-clipboard-copy-content="SCRAPER_API_KEY=should-be-secret
SCRAPER_API_KEY_OTHER=can-also-be-used
SCRAPER_API_KEY_3=works-too"><pre><code>SCRAPER_API_KEY=should-be-secret
SCRAPER_API_KEY_OTHER=can-also-be-used
SCRAPER_API_KEY_3=works-too
</code></pre></div>
<p dir="auto">API keys are sent to the service using the Authorization Bearer scheme.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Usage</h2><a id="user-content-usage" aria-label="Permalink: Usage" href="#usage"></a></p>
<p dir="auto">The root path <code>/</code> returns status 200 if online, plus some Gilbert and Sullivan lyrics (you can go there in your browser to see if it's online).</p>
<p dir="auto">The only other path is <code>/scrape</code>, to which you send a JSON formatted POST request and (if all things go well) receive a <code>multipart/mixed</code> type response.</p>
<p dir="auto">The response will be either:</p>
<ul dir="auto">
<li>Status 200: <code>multipart/mixed</code> response where the first part is type <code>application/json</code> with information about the request; the second part is the website data (usually <code>text/html</code>); and the remaining parts are up to 5 screenshots.</li>
<li>Not status 200: <code>application/json</code> response with an error message under the "error" key.</li>
</ul>
<p dir="auto">Here's a sample cURL request:</p>
<div data-snippet-clipboard-copy-content="curl -X POST &quot;http://localhost:5006/scrape&quot;
    -H &quot;Content-Type: application/json&quot;
    -d '{&quot;url&quot;: &quot;https://us.ai&quot;}'"><pre><code>curl -X POST "http://localhost:5006/scrape"
    -H "Content-Type: application/json"
    -d '{"url": "https://us.ai"}'
</code></pre></div>
<p dir="auto">Here is a code example using Python and the requests_toolbelt library to let you interact with the API properly:</p>
<div data-snippet-clipboard-copy-content="import requests
from requests_toolbelt.multipart.decoder import MultipartDecoder
import sys
import json

data = {
    'url': &quot;https://us.ai&quot;
}
# Optional if you're using an API key
headers = {
    'Authorization': f'Bearer Your-API-Key'
}

response = requests.post('http://localhost:5006/scrape', json=data, headers=headers, timeout=30)
if response.status_code != 200:
    my_json = response.json()
    message = my_json['error']
    print(f&quot;Error scraping: {message}&quot;, file=sys.stderr)
else:
    decoder = MultipartDecoder.from_response(response)
    resp = None
    for i, part in enumerate(decoder.parts):
        if i == 0:  # First is some JSON
            json_part = json.loads(part.content)
            req_status = json_part['status']  # An integer
            req_headers = json_part['headers']  # Headers from the request made to your URL
            metadata = json_part['metadata']  # Information like the number of screenshots and their compressed / uncompressed sizes
            # ...
        elif i == 1:  # Next is the actual content of the page
            content = part.content
            headers = part.headers  # Will contain info about the content (text/html, application/pdf, etc.)
            # ...
        else:  # Other parts are screenshots, if they exist
            img = part.content
            headers = part.headers  # Will tell you the image format
            # ..."><pre><code>import requests
from requests_toolbelt.multipart.decoder import MultipartDecoder
import sys
import json

data = {
    'url': "https://us.ai"
}
# Optional if you're using an API key
headers = {
    'Authorization': f'Bearer Your-API-Key'
}

response = requests.post('http://localhost:5006/scrape', json=data, headers=headers, timeout=30)
if response.status_code != 200:
    my_json = response.json()
    message = my_json['error']
    print(f"Error scraping: {message}", file=sys.stderr)
else:
    decoder = MultipartDecoder.from_response(response)
    resp = None
    for i, part in enumerate(decoder.parts):
        if i == 0:  # First is some JSON
            json_part = json.loads(part.content)
            req_status = json_part['status']  # An integer
            req_headers = json_part['headers']  # Headers from the request made to your URL
            metadata = json_part['metadata']  # Information like the number of screenshots and their compressed / uncompressed sizes
            # ...
        elif i == 1:  # Next is the actual content of the page
            content = part.content
            headers = part.headers  # Will contain info about the content (text/html, application/pdf, etc.)
            # ...
        else:  # Other parts are screenshots, if they exist
            img = part.content
            headers = part.headers  # Will tell you the image format
            # ...
</code></pre></div>
<p dir="auto"><h2 tabindex="-1" dir="auto">Security Considerations</h2><a id="user-content-security-considerations" aria-label="Permalink: Security Considerations" href="#security-considerations"></a></p>
<p dir="auto">Navigating to untrusted websites is a serious security issue. Risks are somewhat mitigated in the following ways:</p>
<ul dir="auto">
<li>Runs as isolated container (container isolation)</li>
<li>Each website is scraped in a new browser context (process isolation)</li>
<li>Strict memory limits and timeouts for each task</li>
<li>Checks the URL to make sure that it's not too weird (loopback, non http, etc.)</li>
</ul>
<p dir="auto">You may take additional precautions depending on your needs, like:</p>
<ul dir="auto">
<li>Only giving the API trusted URLs (or otherwise screening URLs)</li>
<li>Running this API on isolated VMs (hardware isolation)</li>
<li>Using one API instance per user</li>
<li>Not making any secret files or keys available inside the container (besides the API key for the scraper itself)</li>
</ul>
<p dir="auto"><strong>If you'd like to make sure that this API is up to your security standards, please examine the code and open issues! It's not a big repo.</strong></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Other Configuration</h2><a id="user-content-other-configuration" aria-label="Permalink: Other Configuration" href="#other-configuration"></a></p>
<p dir="auto">You can control memory limits and other variables at the top of <code>scraper/worker.py</code>. Here are the defaults:</p>
<div data-snippet-clipboard-copy-content="MEM_LIMIT_MB = 4_000  # 4 GB memory threshold for child scraping process
MAX_SCREENSHOTS = 5
SCREENSHOT_JPEG_QUALITY = 85
BROWSER_HEIGHT = 2000
BROWSER_WIDTH = 1280
USER_AGENT = &quot;Mozilla/5.0 (compatible; Abbey/1.0; +https://github.com/US-Artificial-Intelligence/scraper)&quot;"><pre><code>MEM_LIMIT_MB = 4_000  # 4 GB memory threshold for child scraping process
MAX_SCREENSHOTS = 5
SCREENSHOT_JPEG_QUALITY = 85
BROWSER_HEIGHT = 2000
BROWSER_WIDTH = 1280
USER_AGENT = "Mozilla/5.0 (compatible; Abbey/1.0; +https://github.com/US-Artificial-Intelligence/scraper)"
</code></pre></div>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SQLite Disk Page Explorer (217 pts)]]></title>
            <link>https://github.com/QuadrupleA/sqlite-page-explorer</link>
            <guid>42965198</guid>
            <pubDate>Thu, 06 Feb 2025 18:40:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/QuadrupleA/sqlite-page-explorer">https://github.com/QuadrupleA/sqlite-page-explorer</a>, See on <a href="https://news.ycombinator.com/item?id=42965198">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">SQLite Page Explorer</h2><a id="user-content-sqlite-page-explorer" aria-label="Permalink: SQLite Page Explorer" href="#sqlite-page-explorer"></a></p>
<p dir="auto">A small GUI application built in <a href="https://redbean.dev/" rel="nofollow">redbean</a> that lets you explore your <a href="https://sqlite.com/" rel="nofollow">SQLite</a> databases "page by page" the way SQLite sees them.</p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer" href="https://github.com/QuadrupleA/sqlite-page-explorer/blob/github_media/github_media/top_view.png"><img src="https://github.com/QuadrupleA/sqlite-page-explorer/raw/github_media/github_media/top_view.png" alt="Top-level view"></a></p>
<hr>
<p dir="auto"><h2 tabindex="-1" dir="auto">Why?</h2><a id="user-content-why" aria-label="Permalink: Why?" href="#why"></a></p>
<p dir="auto">SQLite (and most databases) store data in disk-block-sized pages, usually 4KB, which helps make reads and writes as fast as possible.</p>
<p dir="auto">Normally developers interact with databases on the "schema level" - tables, rows, and SQL. But taking a peek at the "page level" can give you some interesting insights:</p>
<ul dir="auto">
<li>What your indexes actually look like on disk (they're basically separate little tables).</li>
<li>How to store things more compactly (and thus make your queries and applications faster).</li>
<li>Spot problems and inefficiencies you might not see on the schema level.</li>
<li>Gain an intuition for B-Trees, one of computing's most important data structures, the foundation of most filesystems and databases.</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Run it anywhere</h2><a id="user-content-run-it-anywhere" aria-label="Permalink: Run it anywhere" href="#run-it-anywhere"></a></p>
<p dir="auto">Thanks to the magic of redbean, <a href="https://github.com/jart/cosmopolitan">cosmopolitan</a> and <a href="https://justine.lol/ape.html" rel="nofollow">Œ±cœÑ¬µŒ±lly pŒ¥rœÑŒ±blŒµ ŒµxŒµc¬µœÑŒ±blŒµ</a>, it's just a single 6.5 MB executable that runs natively on Windows, Linux, MacOS, various BSDs, on both ARM64 and x64.</p>
<p dir="auto">It's also a zip file that contains the Lua code that runs the app, parses the database binary format, etc.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to install</h2><a id="user-content-how-to-install" aria-label="Permalink: How to install" href="#how-to-install"></a></p>
<ul dir="auto">
<li>Download <code>sqlite-page-explorer.com</code> from the <a href="https://github.com/QuadrupleA/sqlite-page-explorer/releases/">releases</a>.</li>
<li>On Unix-likes, <code>chmod +x</code>.</li>
<li>Drag a database file to it, or run it on the console: <code>sqlite-page-explorer.com mySqliteDatabase.db</code>. The app should open in a browser tab.</li>
<li>When you're done, hit Ctrl-C twice in the console.</li>
</ul>
<p dir="auto">You might get virus warnings - Œ±cœÑ¬µŒ±lly pŒ¥rœÑŒ±blŒµ ŒµxŒµc¬µœÑŒ±blŒµs seem to freak out browsers, operating system virus detection, etc. and generate false positives. I trust <a href="https://github.com/jart/">jart</a> is not propagating malware here, and some notable projects like <a href="https://github.com/Mozilla-Ocho/llamafile">llamafile</a> are using these same polyglot binary techniques, but take your usual precautions with anything you download off the internet.</p>
<p dir="auto">Also if you throw a large database at it (500 MB or more) it will likely be slow to load the top-level view, which reads every page.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">How to build</h2><a id="user-content-how-to-build" aria-label="Permalink: How to build" href="#how-to-build"></a></p>
<p dir="auto">To build, you just need to <code>zip</code> the contents of <code>files/</code> into the stock <code>redbean-3.0.0-cosmos.com</code> which I downloaded from <a href="https://cosmo.zip/pub/cosmos/bin/" rel="nofollow">https://cosmo.zip/pub/cosmos/bin/</a> (click "redbean" on the list). You might need <code>zip</code> from there too if your system doesn't have it.</p>
<p dir="auto">Or just run the <code>zipitup.py</code> python (3.6+) script that's included.</p>
<p dir="auto">If you want to hack on it, you can run <code>redbean-3.0.0-cosmos.com -D files</code> to serve the app from the <code>files</code> subdirectory, so you don't have to rebuild the zip on every change.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Not a masterpiece</h2><a id="user-content-not-a-masterpiece" aria-label="Permalink: Not a masterpiece" href="#not-a-masterpiece"></a></p>
<p dir="auto">This was partly an experiment to try out redbean, and also my first time using Lua, so the code is probably klunkier than it could be. It might benefit from a templating system, ala Jinja or bottle.py's native templates, rather than so many string concatenations and Write() statements. Would be nice to auto-close the console when the last tab closes, and maybe stop at page 10,000 or so for huge databases, unless the user confirms. PR's welcome!</p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Scala 3 Migration: Report from the Field (147 pts)]]></title>
            <link>https://blog.pierre-ricadat.com/scala-3-migration-report-from-the-field</link>
            <guid>42964773</guid>
            <pubDate>Thu, 06 Feb 2025 17:54:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://blog.pierre-ricadat.com/scala-3-migration-report-from-the-field">https://blog.pierre-ricadat.com/scala-3-migration-report-from-the-field</a>, See on <a href="https://news.ycombinator.com/item?id=42964773">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="post-content-parent"><p>April 30, 2024. I decided to dedicate a week to migrate our main project at work (a multiplayer mobile game server in production for over 4 years) from Scala 2.13 to Scala 3.</p>
<p>May 7, 2024. I gave up. The removal of several features from Scala 3 (macro annotations, type projections, etc.), combined with the large number of changes necessary for the migration, was overwhelming. I was barely able to migrate a single module, had to modify thousands of lines of code (while my colleagues were adding new features to the main branch, a large number of merge conflicts were already appearing), and the IDE was completely unresponsive due to hundreds of compile errors. At that point, I thought the project might be stuck on Scala 2 forever.</p>
<p>Flash forward to January 2025. I had a little free time, so I decided to give it another try. And (spoiler!) this time I made it to the end. Let‚Äôs see what the various problems I encountered were, the changes I had to make, and the workarounds I implemented.</p>
<h2 id="heading-preamble">Preamble</h2>
<p>The main place to look when starting a migration is the official <a target="_blank" href="https://docs.scala-lang.org/scala3/guides/migration/compatibility-intro.html">Scala 3 Migration Guide</a>. It contains a lot of information about the changes in the language and details on how to proceed.</p>
<p>As I mentioned, the large number of changes required was an issue because it caused a lot of merge conflicts with the main branch. It was not possible to stop all other developments during the migration, so I decided to apply as many changes as possible in the Scala 2 main branch to avoid these conflicts.</p>
<p>The main thing you can do while still on Scala 2.13 is to compile with the <code>-Xsource:3</code> compiler flag, which enables the Scala 3 syntax for imports (<code>*</code> instead of <code>_</code>, <code>as</code> instead of <code>=&gt;</code>), intersection types (<code>&amp;</code> instead of <code>with</code>), and more, and also turns on a number of warnings for things no longer supported in Scala 3 (e.g., <code>.map(CaseClass)</code> should become <code>.map(CaseClass.apply)</code>).</p>
<p>Most of those changes were easy to apply, but there were a lot of them, which was challenging. Scala 3 offers a ‚Äúmigration mode‚Äù and is able to rewrite the code with the new syntax, but this is not applicable if you want to apply these changes in a Scala 2 codebase. My salvation actually came from IntelliJ, which has an inspection for code compiled with <code>-Xsource:3</code> and a quick fix action to replace all the code at once. Incredibly useful!</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1737941342711/38246c7a-77a4-455b-aaf8-fbd28d76eb11.png?auto=compress,format&amp;format=webp" alt="IntelliJ inspection for -Xsource:3"></p>
<p>IntelliJ even lets you select which of these changes you want to apply, so I excluded the ‚Äú<code>case</code> in pattern bindings of for-comprehensions‚Äù because it transformed the code in a weird, unnecessary way.</p>
<p>After this was done, I was able to apply a large number of changes directly to our main branch, avoiding many more conflicts!</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1737941447355/dd5a5858-c5de-42ff-a28e-0975572b8294.png?auto=compress,format&amp;format=webp" alt=""></p>
<h2 id="heading-dropped-features">Dropped Features</h2>
<p>While it brought a number of new and interesting features such as enums or opaque types, Scala 3 dropped a few features altogether, and this proved to be particularly challenging for us. The dropped features are listed <a target="_blank" href="https://docs.scala-lang.org/scala3/reference/dropped-features/">on this page</a>, and there were two of them that we relied on heavily: macro annotations and type projections.</p>
<h3 id="heading-macro-annotations">Macro annotations</h3>
<p>Macro annotations let you annotate Scala 2 types to generate code at compile-time, most typically by adding code to the companion object of annotated classes.</p>
<p>For example, using the <a target="_blank" href="https://github.com/circe/circe">Circe JSON library</a>, you could write this:</p>
<pre><code><span>@JsonCodec</span>
<span>case</span> <span><span>class</span> <span>Bar</span>(<span>i: <span>Int</span>, s: <span>String</span></span>)</span>
</code></pre>
<p>This will automatically generate an implicit <code>Codec[Bar]</code> in the companion object of <code>Bar</code>. Very concise, very convenient. In the case of Circe, there was an "easy" workaround, which is to use the <code>derives</code> keyword available in Scala 3. I put quotes around "easy" because, for some reason, it is not mentioned at all in the <a target="_blank" href="https://circe.github.io/circe/codecs/semiauto-derivation.html#jsoncodec">Circe documentation</a>.</p>
<p>The code above can be changed to the following for the same result:</p>
<pre><code><span>case</span> <span><span>class</span> <span>Bar</span>(<span>i: <span>Int</span>, s: <span>String</span></span>) <span>derives</span> <span>Codec</span>.<span>AsObject</span></span>
</code></pre>
<p>Case closed? Not exactly, because our main use of macro annotations was not with Circe, but with <a target="_blank" href="https://github.com/optics-dev/Monocle">Monocle</a> and its <code>@Lenses</code> annotation.</p>
<pre><code><span>@Lenses</span>
<span>case</span> <span><span>class</span> <span>Bar</span>(<span>i: <span>Int</span>, s: <span>String</span></span>)</span>
</code></pre>
<p>This will generate the following in the companion object of <code>Bar</code>:</p>
<pre><code><span><span>object</span> <span>Bar</span> </span>{
  <span>val</span> i: <span>Lens</span>[<span>Bar</span>, <span>Int</span>] = ??? <span>// implementation omitted for clarity</span>
  <span>val</span> s: <span>Lens</span>[<span>Bar</span>, <span>String</span>] = ??? 
}
</code></pre>
<p>Our project, being a complex game, has a huge user state object, lots of business logic, and domain entities. Lenses allow us to modify parts of the user state in a concise and elegant manner without having to use a chain of nested <code>copy</code>.</p>
<p>The removal of that macro annotation left us with no clear path or alternative for the migration. Unlike the Circe case, this is not a typeclass instance, so we can‚Äôt use the <code>derives</code> keyword: we need a <code>val</code> generated for each field of the case class. There is an <a target="_blank" href="https://github.com/optics-dev/Monocle/issues/1337">open issue</a> in the Monocle repository that discusses various options, but nothing tangible (Kit Langton has an <a target="_blank" href="https://contributors.scala-lang.org/t/scala-3-macro-annotations-and-code-generation/6035/69">interesting approach</a> using <code>Selectable</code>, but this is not supported by IntelliJ).</p>
<p>One obvious alternative was to write those lenses ourselves. That was definitely doable; however, it would have required considerable effort to write thousands of these, and it would have added an enormous amount of boilerplate to the project, making Scala 3 quite unpopular within our team. This alone stopped the migration effort I started in 2024.</p>
<p>We are always trying to reduce boilerplate in our project, so we‚Äôve used a few techniques over the years to address it. Sometimes it‚Äôs doable with macros or mirrors, but one way is to use sbt‚Äôs source generators, which allow you to run some custom code before compilation to generate additional source code files. Combined with <a target="_blank" href="https://scalameta.org/">Scalameta</a>, you can parse and analyze your own code to generate more code. It is ultimately this technique that we used to generate the lenses.</p>
<p>The code generation works like this:</p>
<ul>
<li><p>Look for all case classes in a specific module that contain the <code>@lenses</code> annotation</p>
</li>
<li><p>For each of those case classes, create an object</p>
<ul>
<li>For each field of the case class, create a lens with the appropriate types</li>
</ul>
</li>
</ul>
<p>Using Scalameta is a little bit involved, so I‚Äôve shared a snippet of our code <a target="_blank" href="https://gist.github.com/ghostdogpr/3b5bd33dd3356e16434db42595924bf4">in this gist</a> so that it may be used by others. One downside of this approach is that the generated lenses are no longer in the companion objects of the case classes (we can generate new source files but not modify the existing ones), which required us to change all the lenses usage to use different object names. But it was worth it since it unlocked the migration path.</p>
<p>Note that a ‚Äúmacro annotation‚Äù feature was added to Scala 3, but it is much more limited than what was possible in Scala 2 and does not allow implementing the Monocle <code>@Lenses</code> annotation (the generated code is not visible to the user).</p>
<h3 id="heading-type-projections">Type projections</h3>
<p>Imagine you have a type <code>Request</code> that has an abstract <code>type Result</code> defined inside it.</p>
<pre><code><span><span>trait</span> <span>Request</span> </span>{
  <span><span>type</span> <span>Result</span></span>
}

<span>case</span> <span><span>class</span> <span>IntRequest</span>(<span></span>) <span>extends</span> <span>Request</span> </span>{
  <span><span>type</span> <span>Result</span> </span>= <span>Int</span>
}
</code></pre>
<p>In Scala 2, you can write a function that, for a given <code>Request</code>, returns <code>Request#Result</code>, meaning it returns the <code>Result</code> that matches the subtype of <code>Request</code> that was used. So if <code>Request</code> is <code>IntRequest</code>, we will get an <code>Int</code> back.</p>
<pre><code><span><span>def</span> <span>foo</span></span>[<span>R</span> &lt;: <span>Request</span>](req: <span>Request</span>): <span>R</span>#<span>Result</span> = ???
</code></pre>
<p>This is no longer possible in Scala 3 if <code>R</code> is abstract! You get a compile error saying <code>R is not a legal path since it is not a concrete type</code>. There is an easy workaround if you have a value of type <code>Request</code>, which is to use a function dependent type and return <code>req.Result</code>.</p>
<pre><code><span><span>def</span> <span>foo</span></span>[<span>R</span> &lt;: <span>Request</span>](req: <span>Request</span>): req.<span>Result</span> = ???
</code></pre>
<p>However, our code had various uses of this pattern, and not all of them could be changed to a function dependent type. We ended up using a combination of different techniques depending on each case: function dependent types in some places, typeclasses in others, and we had to give up on making the code generic in a few places. Overall, this felt like a regression from the old code, but at least we were able to make it compile without changing too much code.</p>
<p>EDIT: After publishing this article, Voytek Pitu≈Ça <a target="_blank" href="https://www.reddit.com/r/scala/comments/1ihf75z/comment/mawmkn0/">suggested a different workaround on Reddit</a> using match types, and I was able to apply it successfully in the places where I had no alternatives. It made the code much nicer! I had heard of match types as an alternative before, but I thought I would have to construct a giant pattern matching with the list of all requests and their matching results. I had no idea it could be used in a generic way. Here‚Äôs his approach applied to our example:</p>
<pre><code><span><span>trait</span> <span>Request</span> </span>{
  <span><span>type</span> <span>Result</span> </span>
}

<span><span>object</span> <span>Request</span> </span>{
  <span><span>type</span> <span>Aux</span>[<span>T</span>] </span>= <span>Request</span> { <span><span>type</span> <span>Result</span> </span>= <span>T</span> }
  <span><span>type</span> <span>Result</span>[<span>T</span> &lt;: <span>Request</span>] </span>= <span>T</span> <span>match</span> {
    <span>case</span> <span>Aux</span>[s] =&gt; s
  }
}

<span><span>def</span> <span>foo</span></span>[<span>T</span> &lt;: <span>Request</span>]: <span>Request</span>.<span>Result</span>[<span>T</span>]
</code></pre>
<h2 id="heading-unsupportedbroken-libraries">Unsupported/broken libraries</h2>
<p>Most libraries we were using were available on Scala 3, and for a few missing ones (mostly related to Spark or Kryo), we used <code>cross(CrossVersion.for3Use2_13)</code>, which allows depending on a library built for Scala 2.13.</p>
<p>However, a few of them were not available or didn‚Äôt work as expected, so they required a complete change.</p>
<h3 id="heading-newtypes-and-refined-types">Newtypes and refined types</h3>
<p>In Scala 2, we were using a combination of <a target="_blank" href="https://github.com/estatico/scala-newtype">scala-newtype</a> and <a target="_blank" href="https://github.com/fthomas/refined">refined</a> to define custom types used all over our business logic (IDs, bounded values, etc.). There is no Scala 3 version of scala-newtype, which makes sense because it can be entirely replaced by opaque types. Refined is sneakier: it has a Scala 3 version, but if you try to use it, you will notice that it is only partially implemented; the macros are missing, so the library is not usable (the first example in their README doesn‚Äôt compile).</p>
<p>In another project using Scala 3, we were already using the <a target="_blank" href="https://github.com/kitlangton/neotype">neotype</a> library, which lets you define both newtypes and refined types and is built on top of opaque types, therefore having no runtime cost. We switched to using this library instead. It might sound simple on paper, but we rely on these types so much that it was quite an invasive change impacting a lot of files. At least the migrated code felt better than the old one since writing refined type validation is nicer and slightly less boilerplate-y, and the runtime impact was reduced.</p>
<h3 id="heading-magnolia-typeclass-derivation">Magnolia typeclass derivation</h3>
<p>Another issue we had was with typeclass derivation using <a target="_blank" href="https://github.com/softwaremill/magnolia">Magnolia</a>. While the library supports Scala 3, our existing derivation code caused a compile error for reaching <code>-Xmax-inlines</code> (too much inlined code). I tried to increase it up to 10,000 (!) and it finally failed with a stack overflow in the compiler.</p>
<p>The failing derivation occurred while deriving a sealed trait with a LOT of subtypes (~1,000), but there was already a typeclass instance for each of the subtypes. After looking at the internals of Magnolia, I noticed that a recursive method was used to fold over the list of subtypes, and that method was not tail-recursive, explaining why the number of inlines (and the stack depth) was increasing proportionally to the number of subtypes. To make matters worse, that recursive method also called <code>distinctBy</code> and <code>sortBy</code> on the list of subtypes at <em>every</em> iteration, which is pretty bad when you have lots of them. I opened <a target="_blank" href="https://github.com/softwaremill/magnolia/issues/565">an issue</a> to report this behavior and changed the code locally, but then I ran into a <code>Method too large</code> error because the generated code was longer than what the JVM allows.</p>
<p>After doing a little research, I came across a great feature of Scala 3 that is poorly documented: <code>Tuple.Map</code>. Mirrors give you access to two tuples: for a sum type, <code>MirroredElemLabels</code> is a tuple with the names of the subtypes, while <code>MirroredElemTypes</code> is a tuple with the actual subtypes. You can use <code>summonAll</code> and <code>Tuple.Map</code> to materialize the list of names of those types or even to summon a typeclass instance for each of them.</p>
<pre><code><span><span>trait</span> <span>TC</span>[<span>A</span>]</span>

inline <span><span>def</span> <span>gen</span></span>[<span>A</span>](using m: <span>Mirror</span>.<span>SumOf</span>[<span>A</span>]): <span>TC</span>[<span>A</span>] = {
  <span>// get TC instances of all subtypes</span>
  <span>val</span> subTypes = compiletime.summonAll[<span>Tuple</span>.<span>Map</span>[m.<span>MirroredElemTypes</span>, <span>TC</span>]]
  <span>new</span> <span>TC</span>[<span>A</span>] {
    ??? <span>// given (a: A), we can then use subTypes(m.ordinal(a)).asInstanceOf[TC[A]]</span>
  }
}
</code></pre>
<p>I posted a <a target="_blank" href="https://gist.github.com/ghostdogpr/6f2ca0939c67765a0657a255ed653765">full example on Gist</a> that shows how to derive a typeclass for a sealed trait without even needing Magnolia. This solution is very concise and does not run into inline or <code>Method too large</code> issues. I just wish there were more learning materials about these <code>Tuple</code> utilities because I think they are very powerful.</p>
<h2 id="heading-macros">Macros</h2>
<p>We had a few macros developed in-house, mostly to reduce boilerplate code. They proved relatively easy to port, except for one of them. The reason it was difficult is that Scala 3 macros are much more strict than Scala 2 macros, which let you generate any kind of code. On the other hand, Scala 3 macros require that the code you generate is valid in the context where the macro is defined (which might be different from where the macro is used, making things trickier). I am not a macro expert, so apologies if this is a little imprecise; my colleague <a target="_blank" href="https://x.com/nox737">@nox737</a> is the one who made the magic happen.</p>
<p>It took us quite a long time to make the macro compile with these restrictions (note: AI agents were not helpful at all for this kind of task!), and in the end, the code still failed to compile because of a <code>Method too large</code> error. Compile time felt a bit slower too. We ended up removing the macro entirely and replacing it with another source generator written with Scalameta. It made the code easier to inspect and to split into smaller chunks.</p>
<h2 id="heading-dependency-issues">Dependency issues</h2>
<p>As mentioned earlier, we used <code>CrossVersion.for3Use2_13</code> for a few libraries not available in Scala 3, but one tough problem arose. One of those libraries was <a target="_blank" href="https://github.com/scalapb/sparksql-scalapb">sparksql-scalapb</a>, which lets us use protobuf with Spark. This library depends on Spark, so it is only available for 2.13. It also depends on <code>scalapb-runtime</code>, so depending on it brings <code>scalapb_runtime_2.13</code> into dependencies. The problem is that the rest of our code already depended on <code>scalapb_runtime_3</code>. In that case, sbt failed to resolve the build with this error:</p>
<pre><code>Modules were resolved with conflicting cross-version suffixes in ProjectRef(uri("..."), "spark"):
org.scala-lang.modules:scala-collection-compat _3, _2.13
com.thesamet.scalapb:lenses _3, _2.13
com.thesamet.scalapb:scalapb-runtime _3, _2.13
</code></pre>
<p>In other words, you can‚Äôt depend on the same library in both 2.13 and 3 versions.</p>
<p>I initially tried to solve that issue by shading dependencies, but it didn‚Äôt work because one function we use from <code>sparksql-scalapb</code> expects a specific input extending a type from ScalaPB, which means the rest of our code needs to extend that type. If that type is shaded only in the spark module, it doesn‚Äôt match the type from our other modules.</p>
<p>The solution was actually relatively simple: I forked <code>sparksql-scalapb</code> and made it compile with Scala 3, depending on <code>scalapb_runtime_3</code> and using <code>CrossVersion.for3Use2_13</code> for its other dependencies. The code was very straightforward to port, with just some minor things to fix. Then I embedded the produced JAR in our project instead of depending on the 2.13 library. I had to add the transitive dependencies of that library explicitly in our project, and that was it.</p>
<h2 id="heading-slow-compile-time">Slow compile time</h2>
<p>Once all the code was migrated and I was able to compile successfully for the first time, I noticed that it was taking longer than usual. I also noticed that IntelliJ was constantly compiling to show syntax highlighting. There was definitely something wrong. I had already debugged slow compile times with Scala 2 and was accustomed to using the <code>-Vstatistics</code> compiler flag to see which phases were taking time, and even using <a target="_blank" href="https://github.com/scalacenter/scalac-profiling">scalac-profiling</a> to profile the compilation. Unfortunately, a little research made me realize that such tools did not exist for Scala 3. After asking around on <a target="_blank" href="https://x.com/ghostdogpr/status/1881657774817591559">Twitter</a>, I heard that the new version of Scala (3.6.3) released a day earlier was <a target="_blank" href="https://www.scala-lang.org/news/3.6.3/">bringing a compiler flag to generate compiler traces</a>. What a nice timing, I really got lucky with this one.</p>
<p>I immediately upgraded from 3.6.2 to 3.6.3 and enabled the traces. Within minutes, I was able to generate the following flamegraph:</p>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1737952440000/f9cded6c-8ccc-4c73-b884-886469c80831.jpeg?auto=compress,format&amp;format=webp" alt=""></p>
<p>This was extremely useful: as you can see, it breaks down the compilation time by phases, but also by files and even methods! This helped me pinpoint which code was slow to compile. Even though I did not really understand why it was slow (I tried to reproduce it in an isolated example but failed), I was able to refactor the code in a way that made it fast. The issue was about using an extremely large intersection type (with over 100+ types) as a ZIO environment. Reorganizing the environment into fewer types completely solved this issue, made the compile time on par with 2.13, and made IntelliJ very reactive.</p>
<p>This tool is so useful that I plan to spend more time on it in the future because I am pretty sure that it will allow me to find other slow points, considering how detailed the output is. But my goal for the migration was only to be as fast as with 2.13.</p>
<h2 id="heading-intellij-support">IntelliJ support</h2>
<p>Speaking of IntelliJ, I did run into a couple of issues, which I reported to JetBrains:</p>
<ul>
<li><p><a target="_blank" href="https://youtrack.jetbrains.com/issue/SCL-23387/Monocles-focus-macro-is-not-supported-in-Scala-3-works-with-Scala-2-scala-3-context-functions">Context functions are not well supported when combined with an actual function</a> (e.g., <code>Context ?=&gt; From =&gt; To</code>), which comes up when using the Monocle <code>focus</code> macro.</p>
</li>
<li><p><a target="_blank" href="https://youtrack.jetbrains.com/issue/SCL-21142/scala3-cant-resolve-definitions-from-intersection-types-from-self-type">Using a self type in combination with intersection types is broken</a> (e.g., <code>trait A { self: B &amp; C =&gt;</code>), fortunately, it works when using <code>with</code> instead of <code>&amp;</code>, so the workaround was easy.</p>
</li>
</ul>
<p>I hope these bugs get fixed in the near future since they have very simple and easy reproducers (the first one was fixed as I was writing this post, though not released yet). I briefly looked into it, but the Scala plugin for IntelliJ is not really approachable, and I didn‚Äôt even know where to start looking.</p>
<p>Other than that, IntelliJ support was pretty good. One thing I recommend is to select <code>Use separate compiler output paths</code> in the sbt configuration menu because the sbt shell and IntelliJ‚Äôs own compiler tend to conflict with each other otherwise.</p>
<h2 id="heading-compiler-flags">Compiler flags</h2>
<p>Here are a few notable compiler flags I ended up using:</p>
<ul>
<li><p><code>-language:experimental.betterFors</code> (available under <code>-experimental</code>): this allows using <code>=</code> on the first line of for-comprehensions, and it also optimizes the generated bytecode by avoiding the extra <code>map</code> call at the end of the <code>flatMap</code> calls.</p>
</li>
<li><p><code>-no-indent</code>: I am strongly against significant indentation in Scala, wish it never happened, but at least I am glad it is easy to disable. This is coupled with <code>runner.dialectOverride.allowSignificantIndentation = false</code> in Scalafmt.</p>
</li>
<li><p><code>-Wunused:all</code>: I had a bunch of <code>@nowarn</code> I had to add with Scala 2 because of false positives, and I was able to remove them. It also found some extra unused code that Scala 2 didn‚Äôt detect, so it seemed to work better.</p>
</li>
</ul>
<h2 id="heading-conclusion">Conclusion</h2>
<p><img data-zoomable="true" loading="lazy" src="https://cdn.hashnode.com/res/hashnode/image/upload/v1738645933609/3ecd0d93-c28e-45ac-b3a0-bc2b723ea770.png?auto=compress,format&amp;format=webp" alt=""></p>
<p>Finally, on February 4, the CI turned green on this PR. It has been a long journey with a lot of hurdles, but the situation felt much better in 2025 than a year before. Overall, our code did not change heavily, and most of the changes are for the best. The two things that I really regret are the lack of macro annotations (fortunately, sbt source generators and Scalameta are powerful enough to emulate it) and the removal of general type projections that made our code uglier in some places.</p>
<p>To wrap things up, I am glad our main project did not become a painful legacy stuck in the past, and I am now excited to be able to play with some of the powerful tools that Scala 3 has to offer, particularly around metaprogramming. I hope this read will be helpful to others, whether you have a similar migration to perform or are involved directly with the development of the language and its tooling.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[GitHub Copilot: The Agent Awakens (196 pts)]]></title>
            <link>https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/</link>
            <guid>42964327</guid>
            <pubDate>Thu, 06 Feb 2025 17:07:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/">https://github.blog/news-insights/product-news/github-copilot-the-agent-awakens/</a>, See on <a href="https://news.ycombinator.com/item?id=42964327">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
	
<p>When we introduced GitHub Copilot back in 2021, we had a clear goal: to make developers‚Äô lives easier with an AI pair programmer that helps them write better code. The name reflects our belief that artificial intelligence (AI) isn‚Äôt replacing the developer. Instead, it‚Äôs always on their side. And like any good first officer, Copilot can also fly by itself: for example, when providing pull request feedback, autofixing security vulnerabilities, or brainstorming on how to implement an issue.</p>
<p>Today, we are upgrading GitHub Copilot with the force of even more agentic AI ‚Äì introducing agent mode and announcing the General Availability of Copilot Edits, both in VS Code. We are adding Gemini 2.0 Flash to the model picker for all Copilot users. And we unveil a first look at Copilot‚Äôs new autonomous agent, codenamed Project Padawan. From code completions, chat, and multi-file edits to workspace and agents, Copilot puts the human at the center of the creative work that is software development. AI helps with the things you don‚Äôt want to do, so you have more time for the things you do.</p>
<h2 id="agent-mode-available-in-preview-%f0%9f%a4%96">Agent mode available in preview ü§ñ<a href="#agent-mode-available-in-preview-%f0%9f%a4%96" aria-label="Agent mode available in preview ü§ñ"></a></h2>
<p>GitHub Copilot‚Äôs new agent mode is capable of iterating on its own code, recognizing errors, and fixing them automatically. It can suggest terminal commands and ask you to execute them. It also analyzes run-time errors with self-healing capabilities.</p>
<p>In agent mode, Copilot will iterate on not just its own output, but the result of that output. And it will iterate until it has completed all the subtasks required to complete your prompt. Instead of performing just the task you requested, Copilot now has the ability to infer additional tasks that were not specified, but are also necessary for the primary request to work. Even better, it can catch its own errors, freeing you up from having to copy/paste from the terminal back into chat.</p>
<p>Here‚Äôs an example where GitHub Copilot builds a web app to track marathon training:</p>
<p>
			<iframe loading="lazy" src="https://www.youtube.com/embed/of--3Fq1M3w?feature=oembed" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0"></iframe>
		</p>
<p>To get started, you‚Äôll need to download VS Code Insiders and then enable the agent mode setting for GitHub Copilot Chat:</p>
<p><img data-recalc-dims="1" decoding="async" src="https://github.blog/wp-content/uploads/2025/02/Settings.png?w=1024&amp;resize=1024%2C561" alt="Settings screen for Visual Studio Code showing the words 'Copilot Agent' in the settings search box, and the option for Chat Agent: Enabled activated" width="1024" height="561" loading="lazy" srcset="https://github.blog/wp-content/uploads/2025/02/Settings.png?w=3850 3850w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=300 300w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=768 768w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=1024 1024w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=1536 1536w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=2048 2048w, https://github.blog/wp-content/uploads/2025/02/Settings.png?w=3000 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></p>
<p>Then, when in the Copilot Edits panel, switch from Edit to Agent right next to the model picker:</p>
<p><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video id="video-82456-1" width="1920" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://github.blog/wp-content/uploads/2025/02/Editor_PickerDemo.mp4#t=0.001?_=1"><a href="https://github.blog/wp-content/uploads/2025/02/Editor_PickerDemo.mp4#t=0.001">https://github.blog/wp-content/uploads/2025/02/Editor_PickerDemo.mp4#t=0.001</a></video></p>
<p>Agent mode will change the way developers work in their editor; and as such, we will bring it to all IDEs that Copilot supports. We also know that today‚Äôs Insiders build isn‚Äôt perfect, and welcome your feedback as we improve both VS Code and the underlying agentic technology in the coming months.</p>
<h2 id="copilot-edits-now-ga-in-vs-code-%f0%9f%8e%89">Copilot Edits, now GA in VS Code üéâ<a href="#copilot-edits-now-ga-in-vs-code-%f0%9f%8e%89" aria-label="Copilot Edits, now GA in VS Code üéâ"></a></h2>
<p>Announced at GitHub Universe in October last year, Copilot Edits combines the best of Chat and Inline Chat with a conversational flow and the ability to make inline changes across a set of files that you manage. <a href="https://github.com/microsoft/vscode-copilot-release/issues/95">The feedback</a> <a href="https://github.com/microsoft/vscode-copilot-release/issues/1098">you provided in the past</a> was instrumental in shipping this feature as GA in VS Code today. Thank you!</p>
<p>In Copilot Edits you specify a set of files to be edited, and then use natural language to ask GitHub Copilot for what you need. Copilot Edits makes inline changes in your workspace, across multiple files, using a UI designed for fast iteration. You stay in the flow of your code while reviewing the suggested changes, accepting what works, and iterating with follow-up asks.</p>
<p><img data-recalc-dims="1" decoding="async" src="https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=1024&amp;resize=1024%2C579" alt="Visual Studio Code showing multiple files added to Copilot Edit" width="1024" height="579" loading="lazy" srcset="https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=3850 3850w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=300 300w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=768 768w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=1024 1024w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=1536 1536w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=2048 2048w, https://github.blog/wp-content/uploads/2025/02/Multifile_Edit.png?w=3000 3000w" sizes="auto, (max-width: 1000px) 100vw, 1000px"></p>
<p>Behind the scenes, Copilot Edits leverages a dual-model architecture to enhance editing efficiency and accuracy. First, a foundation language model considers a full context of the Edits session to generate initial edit suggestions. You can choose the foundation language model that you prefer between: OpenAI‚Äôs GPT-4o, o1, o3-mini, Anthropic‚Äôs Claude 3.5 Sonnet, and now, Google‚Äôs Gemini 2.0 Flash. For the optimal experience, we developed a speculative decoding endpoint, optimized for fast application of changes in files. The proposed edits from the foundation model are sent to the speculative decoding endpoint that will then propose those changes inline in the editor.</p>
<p>Copilot Edits works because it puts you in control, from setting the right context to accepting changes. The experience is iterative: when the model gets it wrong, you can review changes across multiple files, accept good ones and iterate until, together with Copilot, you arrive at the right solution. After accepting changes, you can run the code to verify the changes and, when needed, undo in Copilot Edits to get back to a previous working state. Copilot Edits is in the Secondary Side Bar (default on the right) so that you can interact with views in the Primary Side Bar, such as the Explorer, Debug, or Source Control view, while you‚Äôre reviewing proposed changes. For example, you can have unit tests running in the <a href="https://code.visualstudio.com/docs/editor/testing">Testing view</a> on the left, while using the Copilot Edits view on the right, so that in every iteration you can verify if the changes Copilot Edits proposed are passing your unit tests.</p>
<p><a href="https://code.visualstudio.com/docs/editor/voice">Using your voice</a> is a natural experience while using Copilot Edits. Just talking to Copilot makes the back and forth smooth and conversational. It almost feels like interacting with a colleague with area expertise, using the same kind of iterative flow that you would use in real-life pair programming.</p>
<p>Next on our roadmap is to improve the performance of the apply changes speculative decoding endpoint, support transitions into Copilot Edits from Copilot Chat by preserving context, suggest files to the working set, and allow you to undo suggested chunks. If you want to be among the first to get your hands on these improvements, make sure to use <a href="https://code.visualstudio.com/insiders/">VS Code Insiders</a> and the pre-release version of the <a href="https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat">GitHub Copilot Chat</a> extension. To help improve the feature, <a href="https://github.com/microsoft/vscode-copilot-release?utm_source=agent-awakens-announcement&amp;utm_medium=blog&amp;utm_campaign=agentic-ai">please file issues in our repo</a>.</p>
<p>Beyond the GA in VS Code, Copilot Edits is now in preview for Visual Studio 2022.</p>
<h2 id="project-padawan-swe-agents-on-github">Project Padawan: SWE agents on GitHub<a href="#project-padawan-swe-agents-on-github" aria-label="Project Padawan: SWE agents on GitHub"></a></h2>

<p>We‚Äôre excited to share a first look at our autonomous SWE agent and how we envision these types of agents will fit into the GitHub user experience. When the product we are building under the codename Project Padawan ships later this year, it will allow you to directly assign issues to GitHub Copilot, using any of the GitHub clients, and have it produce fully tested pull requests. Once a task is finished, Copilot will assign human reviewers to the PR, and work to resolve feedback they add. In a sense, it will be like onboarding Copilot as a contributor to every repository on GitHub. ‚ú®</p>
<p>
			<iframe loading="lazy" src="https://www.youtube.com/embed/VWvV2-XwBMM?feature=oembed" title="YouTube video player" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0"></iframe>
		</p>
<p>Behind the scenes, Copilot automatically spins up a secure cloud sandbox for every task it‚Äôs assigned. It then asynchronously clones the repository, sets up the environment, analyzes the codebase, edits the necessary files, and builds, tests, and lints the code. Additionally, Copilot takes into account any discussion within the issue or PR, and any custom instruction within the repository, so it understands the full intent of its task, as well as the guidelines and conventions of the project.</p>
<p>And just as we did with Copilot Extensions and the model picker in Copilot, we will also provide opportunities to integrate into this AI-native workflow and work closely with partners and customers in a tight feedback loop. We believe the end-state of Project Padawan will result in transforming how teams manage critical-yet-mundane tasks, such as fixing bugs or creating and maintaining automated tests. Because ultimately, it‚Äôs all about empowering developers by allowing them to focus on what matters, and letting copilots do the rest. And don‚Äôt worry. We will have patience, so the agent won‚Äôt turn to the dark side. üòâ</p>


	
<section>
	<hr>
	<div>
		<h2>Tags:</h2>
		<ul>
							<li>
					<a href="https://github.blog/tag/agent-mode/" rel="tag">
						agent mode					</a>
				</li>
							<li>
					<a href="https://github.blog/tag/agentic-ai/" rel="tag">
						agentic AI					</a>
				</li>
							<li>
					<a href="https://github.blog/tag/github-copilot/" rel="tag">
						GitHub Copilot					</a>
				</li>
							<li>
					<a href="https://github.blog/tag/github-copilot-chat/" rel="tag">
						GitHub Copilot Chat					</a>
				</li>
							<li>
					<a href="https://github.blog/tag/vs-code/" rel="tag">
						VS Code					</a>
				</li>
					</ul>
	</div>
</section>
	<div>
	<h2>
		Written by	</h2>
	
			<article>
	<div>
					<div>
				<picture>
					<source srcset="https://secure.gravatar.com/avatar/46326e2a5a2cc69564a29dbaf1d13a63?s=200&amp;d=mm&amp;r=g" width="120" height="120" media="(min-width: 768px)">
					<img src="https://secure.gravatar.com/avatar/46326e2a5a2cc69564a29dbaf1d13a63?s=200&amp;d=mm&amp;r=g" alt="Thomas Dohmke" width="80" height="80" loading="lazy" decoding="async">
				</picture>
			</div>
				
					<p>Fascinated by software development since his childhood in Germany, Thomas Dohmke has built a career building tools to accelerate developer happiness. Currently, Thomas is the Chief Executive Officer of GitHub, where he has overseen the rise of the world‚Äôs most widely adopted AI developer tools ‚Äì including the launches of GitHub Copilot, Copilot Workspace, and GitHub Models. Thomas is a celebrated TED speaker and holds a PhD in mechanical engineering from University of Glasgow, UK.</p>
			</div>
</article>
	</div>
</section><div>
	<h2>
		Explore more from GitHub	</h2>
	<div>
		<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon-Circle.svg" width="44" height="44" alt="Docs"></p><h3>
			Docs		</h3>
		<p>Everything you need to master GitHub, all in one place.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Go to Docs; ref_location:bottom recirculation;" href="https://docs.github.com/" target="_blank" aria-label="Go to Docs">
					Go to Docs											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_95220f.svg" width="44" height="44" alt="GitHub"></p><h3>
			GitHub		</h3>
		<p>Build what‚Äôs next on GitHub, the place for anyone from anywhere to build anything.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Start building; ref_location:bottom recirculation;" href="https://github.blog/developer-skills/github/" target="_blank" aria-label="Start building">
					Start building											<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16" fill="none"><path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path><path stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2024/07/Icon_da43dc.svg" width="44" height="44" alt="Customer stories"></p><h3>
			Customer stories		</h3>
		<p>Meet the companies and engineering teams that build with GitHub.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Learn more; ref_location:bottom recirculation;" href="https://github.com/customer-stories" target="_blank" aria-label="Learn more">
					Learn more											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
<div>
		<p><img src="https://github.blog/wp-content/uploads/2022/05/careers.svg" width="44" height="44" alt="Work at GitHub!"></p><h3>
			Work at GitHub!		</h3>
		<p>Check out our current job openings.</p>
					<p>
				<a data-analytics-click="Blog, click on module, text: Apply now; ref_location:bottom recirculation;" href="https://www.github.careers/careers-home" target="_blank" aria-label="Apply now">
					Apply now											<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M10.604 1h4.146a.25.25 0 01.25.25v4.146a.25.25 0 01-.427.177L13.03 4.03 9.28 7.78a.75.75 0 01-1.06-1.06l3.75-3.75-1.543-1.543A.25.25 0 0110.604 1zM3.75 2A1.75 1.75 0 002 3.75v8.5c0 .966.784 1.75 1.75 1.75h8.5A1.75 1.75 0 0014 12.25v-3.5a.75.75 0 00-1.5 0v3.5a.25.25 0 01-.25.25h-8.5a.25.25 0 01-.25-.25v-8.5a.25.25 0 01.25-.25h3.5a.75.75 0 000-1.5h-3.5z"></path></svg>
									</a>
			</p>
			</div>
	</div>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Show HN: An homage to Tom Dowdy's 1991 screensaver, "Kaos" (130 pts)]]></title>
            <link>https://thestrikeagency.com/kaos/</link>
            <guid>42963346</guid>
            <pubDate>Thu, 06 Feb 2025 15:35:32 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://thestrikeagency.com/kaos/">https://thestrikeagency.com/kaos/</a>, See on <a href="https://news.ycombinator.com/item?id=42963346">Hacker News</a></p>
Couldn't get https://thestrikeagency.com/kaos/: Error: timeout of 10000ms exceeded]]></description>
        </item>
        <item>
            <title><![CDATA[U.S. Government Disclosed 39 Zero-Day Vulnerabilities in 2023, First-Ever Report (208 pts)]]></title>
            <link>https://www.zetter-zeroday.com/u-s-government-disclosed-39-zero-day-vulnerabilities-in-2023-per-first-ever-report/</link>
            <guid>42962702</guid>
            <pubDate>Thu, 06 Feb 2025 14:35:04 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.zetter-zeroday.com/u-s-government-disclosed-39-zero-day-vulnerabilities-in-2023-per-first-ever-report/">https://www.zetter-zeroday.com/u-s-government-disclosed-39-zero-day-vulnerabilities-in-2023-per-first-ever-report/</a>, See on <a href="https://news.ycombinator.com/item?id=42962702">Hacker News</a></p>
<div id="readability-page-1" class="page"><section>
        <p><em>What the government didn't reveal is how many zero days it discovered in 2023 that it kept to exploit rather than disclose. Whatever that number, it likely will increase under the Trump administration, which has vowed to ramp up government hacking operations. </em></p><p>In a first-of-its-kind report, the US government has revealed that it disclosed 39 zero-day software vulnerabilities to vendors or the public in 2023 for the purpose of getting the vulnerabilities patched or mitigated, as opposed to retaining them to use in hacking operations.</p><p><br>It‚Äôs the first time the government has revealed specific numbers about its controversial Vulnerabilities Equities Process (VEP) ‚Äî the process it uses to adjudicate decisions about whether zero-day vulnerabilities it discovers should be kept secret so law enforcement, intelligence agencies, and the military can exploit them in hacking operations or be disclosed to vendors to fix them. Zero-day vulnerabilities are security holes in software that are unknown to the software maker and are therefore unpatched at the time of discovery, making systems that use the software at risk of being hacked by anyone who discovers the flaw.</p><p><br>In the past, the government has said that it discloses more than 90 percent of the vulnerabilities that go through its VEP review, but without providing specific numbers for context. This has made it difficult for the public to assess the size of the government‚Äôs zero-day stockpile and whether the equities process favors disclosure over exploitation, as the government claims. It‚Äôs not clear that the single-page unclassified document, released quietly last month by the Office of the Director of National Intelligence, helps with this assessment.</p><p><br>The document doesn‚Äôt say how many vulnerabilities in total went through VEP adjudication in 2023, or how many the government kept secret that year. It only says that of the 39 vulnerabilities disclosed, ten of these had been through the adjudication process before ‚Äî meaning that members of the VEP review board had voted to keep them secret in a previous year or years, before deciding in 2023 to disclose them. Under the VEP policy, once the board makes a decision about a zero day, the decision stands until the board revisits it the following year or the government learns that criminal hackers or nation-state adversaries are exploiting the flaw.</p><figure><img src="https://www.zetter-zeroday.com/content/images/2025/01/Screen-Shot-2025-01-29-at-1.43.47-PM-1-1-1-1.png" alt="" loading="lazy" width="500" height="299"><figcaption><span>One-page unclassified document released by the Office of the Director of National Intelligence about zero days the government disclosed in 2023.</span></figcaption></figure><p>Katie Moussouris, founder and CEO of Luta Security and former advisor to the government‚Äôs now-disbanded Cyber Safety Review Board, says that since one of the factors guiding VEP decisions is whether the vulnerability poses a risk to U.S. critical infrastructure or the general public, this means that every other time they had been resubmitted to the VEP ‚Äúthe answer must have been that the risk [hadn‚Äôt] increased enough for us to stop using‚Äù the vulnerabilities.</p><p><br>What changed the calculus in 2023 isn‚Äôt clear. But if the government discovered that other parties were exploiting the vulnerabilities, this would be good information for the public to have, since it could help gauge the ‚Äúcollision rate‚Äù of government zero days ‚Äî collision rate refers to the likelihood that a zero day discovered by one entity will be discovered by others in the same timeframe. A low or high collision rate could impact the risk assessment for whether government zero days should be disclosed or not.</p><p><br>The document doesn‚Äôt say how many years the government withheld the ten vulnerabilities before disclosing them in 2023. But a <a href="https://theintercept.com/2017/03/10/government-zero-days-7-years/?ref=zetter-zeroday.com">2017 RAND study</a> found that in the case of one set of vulnerabilities made available to the U.S. government by a third-party seller, the vulnerabilities generally lasted seven or more years before someone disclosed the vulnerability to the software maker to be patched, or the software maker unwittingly eliminated the vulnerability when it released a new version of the program. A similar timeframe may be true for the government‚Äôs zero days, suggesting that agencies may be using some of them for years before they‚Äôre no longer useful.</p><blockquote><strong>This lack of transparency could become a greater issue under the Trump administration, which has vowed to ramp up the government's cyber offensive operations, suggesting that the government demand for zero-day vulnerabilities may increase over the next four years.</strong></blockquote><p>The unclassified ODNI report got little notice when it was published last month by Senator Ron Wyden (D - Oregon) after receiving it from the intelligence office as the Biden administration was coming to a close. Under the Intelligence Authorization Act, the ODNI is required to submit an annual classified report to the House and Senate intelligence committees identifying the number of vulnerabilities submitted that year for review under the VEP, the number discovered but excluded from review (the process has loopholes that allow the government to withhold some vulnerabilities from review), and the number disclosed to vendors or the public.</p><p><br>The ODNI is also required to include an unclassified appendix to the annual report that reveals the number of vulnerabilities disclosed for patching and the number that subsequently got patched. The unclassified appendix is supposed to be made public, and the ODNI is supposed to have produced ones going back to 2018. But so far it has only made the one for 2023 available. And even that one does not meet the full requirement because it does not indicate how many of the 39 vulnerabilities that got disclosed were subsequently patched. The ODNI says the intelligence community doesn‚Äôt collect that information and therefore can‚Äôt include it in the appendix.</p><figure><img src="https://www.zetter-zeroday.com/content/images/2025/02/Screen-Shot-2025-02-05-at-1.55.30-PM-1.png" alt="" loading="lazy" width="500" height="184"><figcaption><span>2020 Amendment showing that the requirement to make the unclassified report available to the public applies retroactively to 2018 and 2019.</span></figcaption></figure><p><br>It‚Äôs not clear how much more information gets submitted to the intelligence committees in their classified versions of the VEP reports. But when asked if these versions provide sufficient transparency about the vulnerabilities process for the committees to provide proper oversight of the process, Wyden‚Äôs office says no.</p><p><br>‚ÄúThe public remains in the dark about how many VEP decisions are unanimous among the agencies that participate, how many involve offensive agencies outvoting defensive agencies, and how many final decisions are the result of an appeal,‚Äù a Wyden aide says. ‚ÄúSenator Wyden believes Americans need far more visibility into how the government decides which exploitable software vulnerabilities it discloses to companies to fix, and which it keeps secret, leaving Americans vulnerable to foreign hacks.‚Äù</p><p><br>This lack of transparency could become a greater issue under the Trump administration, which has vowed to <a href="https://cyberscoop.com/aggressive-cyber-offense-trump-administration-us-strategy-debate/?ref=zetter-zeroday.com">ramp up the government's cyber offensive operations</a>, suggesting that the government demand for zero-day vulnerabilities may increase over the next four years. If this occurs, the government‚Äôs previous statements that the VEP favors disclosure and defense over withholding and offense may no longer be true.</p><blockquote><strong>‚Äú[The Trump administration] could say we‚Äôre disclosing too [many vulnerabilities]. If the default [in the past] was to disclose unless there is a reason to keep, I could easily imagine the default is going to be to keep unless there is a reason to disclose.‚Äù</strong></blockquote><p><br>‚ÄúThe VEP and that number of 90 percent was one of the few places where the president and the White House could set the dial on how much they liked defense vs offense,‚Äù says Jason Healey, senior research scholar at Columbia University‚Äôs School of International and Public Affairs and former senior cybersecurity strategist for CISA. ‚Äú[The Trump administration] could say we‚Äôre disclosing too [many vulnerabilities]. If the default [in the past] was to disclose unless there is a reason to keep, I could easily imagine the default is going to be to keep unless there is a reason to disclose.‚Äù</p><h3 id="vep-creation">VEP Creation</h3><p><br>The government created the VEP in 2010, producing a charter with details about how it should operate. But the VEP‚Äôs existence remained classified until 2014, when the so-called Heartbleed vulnerability was discovered. Heartbleed was a significant flaw in an OpenSSL cryptography library that web sites use to encrypt traffic between a user‚Äôs computer and an internet domain. The flaw had been in the code since 2012 but remained unknown to its developers and the public until security researchers discovered it in 2014.</p><p><br>Bloomberg News reported at the time that the NSA had <a href="https://www.bloomberg.com/news/articles/2014-04-11/nsa-said-to-have-used-heartbleed-bug-exposing-consumers?ref=zetter-zeroday.com">already known</a> about the vulnerability for at least two years and kept it secret to exploit it for intelligence collection. But the ODNI released a rare response disputing this, saying the intelligence community learned about the vulnerability only when the public did. Had the government known about it before, the ODNI insisted, it would have disclosed it to be fixed, due to the flaw‚Äôs severity. To underscore this, the ODNI revealed that the government had a formal process for deciding when to disclose or withhold vulnerabilities, and the process was weighted in favor of disclosure: ‚Äú[U]nless there is a clear national security or law enforcement need, this process is biased toward responsibly disclosing such vulnerabilities,‚Äù the ODNI said.<br></p><p>The ODNI didn‚Äôt mention, however, that between 2010 and 2013, agencies required to follow the process failed to do so. Michael Daniel, former cybersecurity advisor to President Obama, says only the NSA was doing a review of vulnerabilities, but not in the way the VEP charter dictated.</p><p><br>‚ÄúNSA was doing ‚Ä¶ its own review of vulnerabilities that it found,‚Ä¶ following the policy internally, but‚Ä¶it wasn‚Äôt being surfaced all the way up to the National Security Council level,‚Äù he says. ‚Äú[And] there wasn‚Äôt a robust process at the NSC level to really get the whole process working and all of the agencies involved.‚Äù<br></p><p>According to the <a href="https://www.eff.org/document/vulnerabilities-equities-process-january-2016?ref=zetter-zeroday.com">2010 VEP charter</a>, the review board is composed of representatives from government entities with offensive or defensive interest in software vulnerabilities. This includes the CIA, DoJ, DHS, NSA, U.S. CyberCommand, Department of Energy, State Department and others. The board reviews vulnerabilities discovered by the U.S. government, by contractors working for the government, or by foreign partner governments that share vulnerabilities with the U.S.</p><p><br>When an agency discovers a zero-day, it submits a description of it to the VEP secretariat along with a recommendation about whether to disclose or withhold. The secretariat has one business day to notify all participating agencies, who have five days to say whether disclosing or withholding the vulnerability will impact their offensive or defensive operations. The agency that found the vulnerability can call on experts to argue in favor of its position, and if the agencies don't reach consensus, the members of the board vote.</p><p><br>If the decision is to disclose, the agency that discovered the zero day has seven business days to do this. If the decision is to withhold, the board will revisit the decision annually until it eventually decides to disclose the vulnerability, or the vulnerability otherwise becomes publicly known or patched. If the government becomes aware in the meantime that the zero day is being exploited by criminal hackers or foreign adversaries, they must notify the executive secretariat, and members have one business day to determine if the vulnerability should be disclosed for patching.</p><p><br>The board can also decide to disseminate information about a vulnerability just to select entities, limit how the government can exploit the vulnerability, or use ‚Äúindirect means‚Äù to let a vendor know about the vulnerability. The document does not elaborate on what the latter means.</p><p><br>Not every vulnerability undergoes a VEP review. Vulnerabilities that government agencies purchase from a seller under a non-disclosure agreement are exempt from VEP review. A seller would require an NDA if the sale isn‚Äôt exclusive and it wants to sell the zero day to other customers. Zero days obtained from a foreign government agency under a memo of understanding are also exempt from VEP review. Zero days that are excluded from review still need to be reported to the chair of the board, and the number of vulnerabilities each agency excludes from review has to be disclosed to all members.</p><p><br>When the charter was made public in 2016 in response to a lawsuit, civil liberties groups expressed concern that only government agencies were allowed to participate in decision making and that there didn‚Äôt appear to be anyone representing public interests. There also didn‚Äôt appear to be independent oversight of the process outside of the member agencies. The board was supposed to produce an annual report about all the zero days that underwent review, but there was no requirement to provide it to Congress or the public.</p><p><br>In 2017, the government released a revised charter, making the National Security Council overseer of the process. And whereas the previous charter emphasized U.S. government interests, the revised charter says the process should prioritize the public's interest and the security of critical information and infrastructure systems as long as there is no ‚Äúdemonstrable, overriding interest in the use of the vulnerability for lawful intelligence, law enforcement, or national security purposes.‚Äù In the ‚Äúvast majority of cases,‚Äù it says disclosure is ‚Äúin the national interest.‚Äù The requirement to provide reports to the intelligence communities for oversight came later and is the only part of the VEP that is codified. This means that as long as the process remains just policy and not law, the sitting administration can change it, though they are required to notify the intelligence communities if they do.</p><p><br>One important piece of information about the process that is still unknown and that troubles Moussouris is how the review board makes its risk assessments about whether a zero day should be disclosed. Moussouris, who previously was lead senior security strategist at Microsoft, says there‚Äôs no reliable formula for doing this, and even Microsoft finds it difficult to accurately gauge the risk posed by many vulnerabilities. The company might know how many of its direct customers are using a piece of vulnerable code, but it‚Äôs very difficult to gauge how many resellers and others have embedded the vulnerable code in critical infrastructure components, medical devices, banking machines and other systems.</p><p><br>‚ÄúIf even the vendors themselves would have a hard time gauging relative risk, how is the federal government supposed to asses that risk?‚Äù she says.</p><p>Updated 12pm: To clarify when Moussouris was part of the government's CSRB.</p><p><em>Thank you for reading. If you like this content and want to receive more like it in your inbox, become a subscriber. Zero Day is a reader-supported publication. You can support my work by becoming a paid subscriber and receiving content that is only available to paid subscribers or you can subscribe for free.</em></p>
    </section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Simulating Water over Terrain (300 pts)]]></title>
            <link>https://lisyarus.github.io/blog/posts/simulating-water-over-terrain.html</link>
            <guid>42962508</guid>
            <pubDate>Thu, 06 Feb 2025 14:15:28 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://lisyarus.github.io/blog/posts/simulating-water-over-terrain.html">https://lisyarus.github.io/blog/posts/simulating-water-over-terrain.html</a>, See on <a href="https://news.ycombinator.com/item?id=42962508">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="page-content">
            
            <br><center><p>Simulating water over terrain</p></center><br>
            <center><span>2025 Feb 4</span></center>

            <p><i>If you don't want a long an boring introduction, skip directly to the <a href="#section-virtual-pipes-method">virtual pipes method description</a>.<br>I will get a little bit sad if you do this, though x)</i></p>

            <p>I'm somewhat obsessed with terrain generation, grid-based games, simulations, and stuff like that. And this stuff often involves <i>water</i>, ‚Äî or at least it seems natural for water to be there.</p>

            <p>Say, you're generating a map for a strategy game, and you don't want the map borders to just be filled with inpenetrable void (like in old-school RTS games). Wouldn't it be nice for the border to be filled with water, like in this map from one of my abandoned projects:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/island-map.png"></center>

            <p>This provides a nice natural border, and maybe allows you to introduce some more water-related mechanics like sailing, fishing, trading, and sea warfare. Here's a 3D view of a similar island from the same project, for no particular purpose:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/island-sunset-view.jpeg"></center>

            <p>Or, say, you're making a peaceful village/town simulation game and you want your town to have a river for drinking, fishing, transportation, or even pure aesthetics. Or maybe you want a river just as a divider between areas, like in <a href="https://store.steampowered.com/app/2403100/Costa_Verde_Transport_Department">my first released game</a>:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/costa-verde-river.jpeg"></center>

            <p>I hope I've convinved you: having water <i>is really nice</i>. (Not that it wasn't obvious anyway).</p>

            <p>But water has problems.</p>

            <h2>Contents</h2>
            <ul id="contents"></ul>

            <h2 id="section-problems-with-water">Problems with water</h2>

            <p>Most games don't allow terrain modification, which is reasonable ‚Äî not every game needs it. Even those that do can often get away with some simplistic approach: I can easily imagine being able to fill up a hill in a game like Civilization (though it would probably break a lot of stuff gameplay-wise), and it doesn't interact with water in any way.</p>

            <p><a href="https://www.youtube.com/playlist?list=PLSGI94QoFYJwGaieAkqw5_qfoupdppxHN">My game</a>, however, <i>does</i> need terrain modification:</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/digging.mp4" type="video/mp4"></video></center>

            <p>Why? Well, uhm, because my design docs say so! I promise, it was somewhere on that page... or that one...</p>

            <p>Seriously, though, there are indeed a couple of reasons in my case:</p>

            <ul>
                <li>Some resourses in my game are literally taken from the ground, like dirt, sand, and clay, and it just makes sense that when you dig them up, the ground gets removed (otherwise we'd get an infinite source, which is not ideal)</li>
                <li>Similarly, for stuff like stones &amp; metal ores I'd prefer for them to get excavated from the ground when mining (instead of having a shiny "gold ore" boulder lying on the ground, as many games do), and it makes sense that the mined ground gets removed as well</li>
                <li>My game doesn't allow constructing buildings on slopes, so it's important to be able to level the terrain before building something on it, like in The Sims</li>
                <li>I want to give the player a ton of tools for creative expression, and terrain modification is one of them</li>
                <li>Let's be honest, <a href="https://www.windowscentral.com/gaming/minecraft/minecraft-crosses-300-million-copies-sold-as-it-prepares-to-celebrate-its-15th-anniversary">everybody loves</a> terrain modification</li>
            </ul>

            <p>So, OK, that does this have to do with water? Say, you have a lake, or even a puddle, and you've dug up its border and opened a free passage for the water to flow out. What does the water do?</p>

            <p>There are a number of simple options:</p>

            <ul>
                <li>Water doesn't go anywhere, and stays where it was initially generated (of course, this just feels boring and dumb)</li>
                <li>Water doesn't flow; instead, everything below a certain height level is considered water, like in Sapiens</li>
                <li>Water also exists below some level, but you can't even dig that far, or maybe you can only dig one level down to remove hills/mountains, like in RimWorld</li>
                <li>Water flows using some extremely simplistic model, like in Minecraft</li>
                <li>Water flows using some nice but still simplistic cellular-automata-ish model, like in Dwarf Fortress</li>
            </ul>

            <p>Among these solutions, not one feels satisfactory to me. They are good fallbacks to consider if I fail to find a better option, but as a main water model they are just too...boring. Dwarf Fortress comes closer than others, but still, their model is too blocky yet also designed for 3D, which I don't really need (see the next section).</p>

            <p>For years (yep, literally) I was passively searching for a model that would work for my needs. I researched a ton of literature on fluid simulations, particularly in climate &amp; oceans/rivers simulations, and got really scared because most of the models used there are insanely complicated, while their applicability to my case was always blurry since the scientific community usually doesn't cover game design. One time I even managed to compute something like the expected currents flowing around an island, by solving a mass conservation equation for the liquid flow, i.e. something like \(\nabla (\text{mass}\cdot\text{velocity})=0\):</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/island-currents.jpeg"></center>

            <p>It's already something, but I need a dynamic model, not a static assignment of currents. <i>(It can still be useful for some funny climate map generation, I guess.)</i> By the way, it is theoretically easy to turn a dynamic model into a static one: just add equations like \(\frac{dX}{dt}=0\) for all your state variables \(X\), saying that you have a <i>steady state</i> (i.e. a solution that's perfectly stable and doesn't change with time, like a slowly flowing river).</p>

            <p>I could just give up, of course. After all, it's important to remember that, in the end, I'm not making some reality simulator, ‚Äî I'm <i>making a game</i>, and I'm free to bend the rules of it any way I see fit. However, I also couldn't shake off the feeling that some clever combination of simple formulas should work for my case. And I was right!</p>

            <p><i>By the way, if reading this section made you scream "TIMBERBOOOORN" seven times, guess what: they <a href="https://www.gamedeveloper.com/design/deep-dive-timberborn-s-water-mechanics">use exactly the same model</a> as I'm going to describe.</i></p>

            <p><i>Also, if you know other models that you think would suit my case ‚Äî tell me! I'd love to hear about them!</i></p>

            <h2 id="section-setup">The setup</h2>

            <p>I've said the phrase "my case" four times already, but what exactly do I mean? Here's the list of features I want my water simulation to have:</p>

            <ul>
                <li>The simulation should probably work on a grid, preferably on the same grid I'm using for the terrain</li>
                <li>The average scale of the simulation should be around 1 meter or so, ‚Äî I don't care about tiny splashes of water, while a kilometer-wide simulation is too coarse for something like a city builder game (I can easily add fake higher-frequency visual details on the rendering side without having to simulate them)</li>
                <li>I'm fine with assuming that water is a height field over the terrain, i.e. it doesn't flow vertically and it doesn't have gaps in vertical cross-sections, because my game's terrain is itself a height field, and because this basically reduces the problem to 2D</li>
                <li>Water should be able to flow (duh)</li>
                <li>Water shouldn't magically disappear due to simulation errors (this happens with some models I tried before)</li>
                <li>The simulation should be controllably stable</li>
                <li>The simulation should be fast enough, ideally the cost of a single simulation step should be linear in the simulation size (i.e. a few for-loops over the simulation domain)</li>
            </ul>

            <p>As you can guess, I've found such a model, and this is what this article is about. But first,</p>

            <h2 id="section-non-solutions">Non-solutions</h2>

            <p>Let me describe a couple of popular options that <i>do not</i> suit my scenario.</p>

            <p>Smoothed Particle Hydrodynamics is an <a href="https://www.youtube.com/watch?v=rSKMYc1CQHE">insanely popular</a> way to do fluid simulations which produces impressive results. However, this method also solves <i>a completely different problem</i>! It gives you realistic, highly detailed and beautiful simulations, which is not what I want. Remember the "average scale" thing I said earlier? Having 1 meter-sized water particles doesn't really work, ‚Äî they'll look like water-filled balloons, ‚Äî but having smaller particles hits performance too much. I don't need a high-detailed simulation, I want a fast and reasonable one!</p>

            <p>Jos Stam's <a href="https://pages.cs.wisc.edu/~chaol/data/cs777/stam-stable_fluids.pdf">Stable Fluids</a> is probably the most well-known fluid simulation work known in computer graphics, which <i>also solves a different problem</i>. It works with a full volume of fluid, rather than with a free surface like I need. Think of a closed tank filled with water, as opposed to water over terrain. It is also far from fast: some steps of the simulation require iteratively solving a sparse linear system, which is doable but still expensive.</p>

            <p>In fact, I've implemented Stable Fluids once, and it's a really fun and impressive model, it's just a model for a different thing <i>(it solves the full Navier-Stokes instead of the shallow water equations that we actually need)</i>:</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/stable-fluids.mp4" type="video/mp4"></video></center>

            <h2 id="section-shallow-water-equations">Shallow water equations</h2>

            <p><i>Beware: I'm not a physicist, and this section might be full of complete nonsense.</i></p>

            <p>Now, whenever we talk about seeking a <i>mathematical model</i> of something, it usually means we need some <i>equations</i> to solve. Generally, fluid motion is described by the <a href="https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations">Navier-Stokes equations</a>, or by the simpler <a href="https://en.wikipedia.org/wiki/Euler_equations_(fluid_dynamics)">Euler equations</a>. However, these equations also don't talk about a free water surface, but instead about a volume completely filled with fluid.</p>

            <p>If we look at the variables these equations work with, we see fluid velocity, pressure, density, thermodynamic work, stress tensor, and some other things, and you'd need a good full course in fluid dynamics to figure out which of these are known, which are unknowns, and which can be derived from others (after lazily researching this for years I still don't know the answer, btw). Notice that there's nothing like "amount of water" here. We do have mass density, but all equations <i>divide</i> by it, so we can't really have zero density or a boundary between fluid and air. <i>(This can actually be done using stuff like the <a href="https://en.wikipedia.org/wiki/Particle-in-cell">particle-in-cell</a> and <a href="http://plaza.ufl.edu/ebrackear/">marker-and-cell</a> methods, if I'm not mistaken.)</i></p>

            <p>Even in the simplest form, these equations involve pressure \(p\), which is unknown and changes with time, but there's no equation for <i>how exactly</i> it evolves! I.e. there's no \(\frac{dp}{dt}=\dots\) equation. Instead, the time evolution of the pressure field is implicitly built into the other equtaions. This is closely related to the <i>projection step</i> in the Stable Fluids solver.</p>

            <p>What we need is to take something like Navier-Stokes, assume that we have a layer of water on top of some terrain (commonly called the <i>bed</i> in this case), and do some sort of averaging out along the vertical direction, so that we're left with purely 2D equations describing how water moves. That's exactly what the <a href="https://en.wikipedia.org/wiki/Shallow_water_equations">shallow water equations</a> are about!</p>

            <p>The "shallow" part means that we assume the typical vertical size of a water column to be much smaller than our typical horizontal scales. This is usually more or less true in things like modelling climate or river floods: the water heights (say, meters or tens of meters for a river) is much smaller than horizontal distances we're interested in (kilometers).</p>

            <p>These equations are what is typically used in a lot of "water over terrain" situations in science, and they are what we're going to solve as well. I won't put the equations themselves here ‚Äî you can find them <a href="https://en.wikipedia.org/wiki/Shallow_water_equations">on wikipedia</a>, and that's not what this post is about. Instead, I'll describe the solution method directly, and try to make some sense of it.</p>

            <h2 id="section-staggered-grids">Staggered grids</h2>

            <p>One thing we need to talk about before describing the full solution is the grid. Usually when numerically solving differential equations, we discretize the simulation area into a grid (say, a grid of squares), and store the values of our field (velocity, pressure, stuff like that) per grid cell or per grid vertex.</p>

            <p>For example, when solving the <a href="https://en.wikipedia.org/wiki/Wave_equation">wave equation</a> (which works better for sound waves and EM waves rather than water waves), we store the wave height <code>u(i,j)</code>  and it's time derivative <code>du(i,j)</code> per each cell, and then the update code is</p>

            <pre><code>du(i,j) += (u(i+1,j)+u(i,j+1)+u(i-1,j)+u(i,j-1)-4*u(i,j)) * dt/dx/dx;
u(i,j) += du(i,j) * dt;</code></pre>

            <p>It may be a bit cryptic, but the main idea is that both quantities are stored on the same grid. This leads to nice finite-difference equations, is simple to code, and generally makes sense.</p>

            <p>Such grids are called <i>collocated</i>, which I always read as co-located: both quantities are <i>located</i> on the same grid. And such grids suck for fluid dynamics!</p>

            <p>One reason is that they make finite difference methods confusing. In the wave equation we have the second derivative, which can be computed in a nice and symmetric way as</p>

            <pre><code>(f(x+1) + f(x-1) - 2*f(x)) / (dx * dx)</code></pre>

            <p>But fluid dynamics feature first derivatives, in stuff like "the acceleration of fluid is proportional to the difference between how much water is on the left and the right". So, how do you compute this with finite differencies? <code>(f(x+1) - f(x)) / dx</code> makes the simulation biased to the right, may ignore some directional effects, and generally leads to instabilities. <code>(f(x) - f(x-1)) / dx</code> has the same problems. <code>(f(x+1) - f(x-1)) / (2*dx)</code> is nicely symmetric but ignores the value at the current cell <code>f(x)</code>, which also leads to instabilities. In general, naive discretization of fluid dynamics equations is typically extremely unstable, and everything just oscillates in a stupid way instead of solving the equation.</p>

            <p>Another way to see that this grid is not ideal is to consider this situation: assume that we store the water height and the water velocity vector in each cell. Now consider a cell that has incoming water flows both from left and right, and the water flows out to top and bottom cells. Here's an illustration:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/collocated-zero-velocity.png"></center>
            <center><i>Image based on illustrations on <a href="https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html">this site</a>.</i></center>

            <p>The total velocity in this cell is zero! This is nonsense, since the water is clearly flowing quite a lot here.</p>

            <p>Things like these led people to reconsider their grid methods, and invent something called <a href="https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html"><i>staggered grids</i></a>. There are many variants of these, but the typical one used for fluid simulation works like this: we store, say, water height/density/etc in square cells, but we store the velocity in <i>edges between cells</i>. Vertical edges (i.e. edges between horizontal neighbours) store the horizontal velocity, and vice versa. Here's an image:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/staggered-grid.png"></center>
            <center><i>Image from <a href="https://tum-pbs.github.io/PhiFlow/Staggered_Grids.html">this site</a>.</i></center>

            <p>Blue arrows indicate the stored values, one per each edge between cells. Notice that the boundary edges are also here: these correspond to the boundary conditions of your simulation (we'll get to these a bit later).</p>

            <p>So, if we want an \(N\times N\) square grid as our simulation area, we'd have to store</p>

            <ul>
                <li>\(N\times N\) array for, say, water height</li>
                <li>\((N+1)\times N\) array for X velocity</li>
                <li>\(N\times (N+1)\) array for Y velocity</li>
            </ul>

            <p>This might feel unusual, but the sooner you accept it, the better your fluid simulations will get :)</p>

            <h2 id="section-virtual-pipes-method">Virtual pipes method</h2>

            <p>At last, we've arrived to the actual method I'm using for simulating water over terrain. This method is called <i>virtual pipes</i>, because it is derived by assuming that the water cells are connected by imaginary pipes of some radius. I used <a href="https://diglib.eg.org/server/api/core/bitstreams/47f5228c-6f1c-4afb-ab80-b98c44575bc8/content">these</a> <a href="https://inria.hal.science/inria-00402079/document">two</a> papers as a reference for this method. Notice that the first paper also consideres multi-level water columns and vertical connections, while the second paper is primarily about hydraulic erosion; I didn't really do any of these since that's not my goal.</p>

            <p>First, for simplicity sake, let's ignore the terrain part, it will be trivial to add later. We'll store, just like in the previous section, three values on a staggered grid:</p>

            <ul>
                <li>\(N\times N\) <code>water</code> array for the height of water surface in this cell</li>
                <li>\((N+1)\times N\) <code>flowX</code> array for the total water <i>flow</i> between horizontally adjacent cells</li>
                <li>\(N\times (N+1)\) <code>flowY</code> array for the total water <i>flow</i> between vertically adjacent cells</li>
            </ul>

            <p><code>flowX(i,j)</code> is the horizontal flow (also calles <i>flux</i>) between cells <code>water(i-1,j)</code> and <code>water(i,j)</code>, unless it is a boundary edge (<code>i == 0</code> or <code>i == N</code>), and similarly for <code>flowY</code>.</p>

            <p><i>By the way, in what follows I'll use the notation <code>array(i,j)</code> to mean the <code>i,j</code>-th element of an array. In my actual C++ code the 2D arrays have an overloaded <code>operator()</code> to provide read-write element access, since multi-dimensional <code>operator[]</code> is only available since C++23.</i></p>

            <p>Notice how we'll store the <i>flow</i>, not the <i>velocity</i>. To make sense of a flow, imagine you've put a magic curtain between two adjacent cells which can measure how much water goes through it. The water stream going through this curtain has a certain cross-section area, and it moves with some velocity. If you multiply this two values, you get water volume per unit of time, which is exactly what we'll store.</p>

            <p>Flow tends to behave better than velocity when you have no water. For two adjacent empty cells, the flow between them is obviously zero, since there's no water to move. However, velocity is something like flow divided by water cross-section, i.e. zero divided by zero, which is always a problematic thing. You could say that in this case the velocity is obviously zero, but it's more subtle than that: what if both the water and the flow are very small, but non-zero? We'd quickly get into the territory of floating-point problems and discontinuities, and we'd have to figure out some thresholds such that below this water level threshold velocity is considered zero. All this is really messy, and working with flows instead of velocities just solves all these issues elegantly.</p>

            <p>Enough talking, here are the three steps involved in the method:</p>

            <ol>
                <li><a href="#section-flow-acceleration">Flow acceleration</a></li>
                <li><a href="#section-outflow-scaling">Outflow scaling</a></li>
                <li><a href="#section-water-column-updating">Water column updating</a></li>
            </ol>

            <p>Let's work them through one by one.</p>

            <h2 id="section-flow-acceleration">Flow acceleration</h2>

            <p>If you have two neighbouring water cells, and their water heights is different, basic reasoning tells us that water will flow from the larger to the smaller column. That's exactly what flow acceleration does: we take all <i>interior</i> (i.e. non-boundary) edges, and accelerate the flow in them based on the difference in water levels in corresponding water cells. We do this for the X flows:</p>

            <pre><code>for (int y = 0; y &lt; N; ++y)
    for (int x = 1; x &lt; N; ++x)
        flowX(x,y) += (water(x-1,y) - water(x,y)) * g * dt * A / dx;</code></pre>

            <p>and similarly for Y:</p>

            <pre><code>for (int y = 1; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        flowY(x,y) += (water(x,y-1) - water(x,y)) * g * dt * A / dy;</code></pre>

            <p>Here, <code>dx</code> and <code>dy</code> are the horizontal and vertical sizes of our sells (the lengths of the corresponding "pipes"), <code>dt</code> is the simulation time step (I'll talk about it later), <code>g</code> is the gravity, and <code>A</code> is the cross-section area of the virtual pipe. The <a href="https://diglib.eg.org/server/api/core/bitstreams/47f5228c-6f1c-4afb-ab80-b98c44575bc8/content">paper</a> takes <code>A=dx*dx</code>, while I took <code>A=1</code>. In general, <code>A</code> and <code>g</code> are only used in this step, and always as a product, so for our simple needs we can just ignore <code>A</code> and pretend that it is merged with <code>g</code>.</p>

            <p>Usually, we also add <i>friction</i> to this step. Friction simply scales down the flow on each iteration, typically making the simulation converge to a static stable state. The paper recommends using a factor of <code>pow(friction,dt)</code> to make it <code>dt</code>-independent (which is closely related to <a href="https://lisyarus.github.io/blog/posts/exponential-smoothing.html">exponential smoothing</a> I've explained in another article). Thus, the simulation code becomes</p>

            <pre><code>for (int y = 0; y &lt; N; ++y)
    for (int x = 1; x &lt; N; ++x)
        flowX(x,y) = flowX(x,y) * pow(friction,dt)
                   + (water(x-1,y) - water(x,y)) * g * dt / dx;</code></pre>

            <p>and similarly for Y:</p>

            <pre><code>for (int y = 1; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        flowY(x,y) = flowY(x,y) * pow(friction,dt)
                   + (water(x,y-1) - water(x,y)) * g * dt / dy;</code></pre>

            <p><code>friction</code> is typically between 0 and 1. <code>friction=0</code> means maximal friction, i.e. the flow from the previous simulation step is completely annihilated. <code>friction=1</code> means no friction at all. Because of this I'm actually using the formula <code>pow(1-friction,dt)</code> to make the <code>friction</code> value more intuitive. Of course, we should precompute the friction factor <code>pow(1-friction, dt)</code> at the start of simulation step, instead of computing it for each cell.</p>

            <p>The time step <code>dt</code> is a bit tricky. Obviously, the larger it is, the faster the simulation goes. However, large values of <code>dt</code> also lead to instabilities. In general, there is a famous <a href="https://en.wikipedia.org/wiki/Courant%E2%80%93Friedrichs%E2%80%93Lewy_condition">Courant-Friedrichs-Lewy condition (CFL)</a> common to all fluid simulations. It states that, roughly speaking, the quotient <code>dx/dt</code> must not be less than the maximum speed of the fluid. In practice that means that we have to decrease <code>dt</code> (thus increasing the value of <code>dx/dt</code>) until the simulation becomes stable. I've used values around <code>0.001</code> and <code>0.01</code>.</p>

            <p>So, this step essentially acelerates the water flow, kinda like \(\frac{dv}{dt} = a = \frac{F}{m}\) from Newton's laws.</p>

            <h2 id="section-water-column-updating">Water column updating</h2>

            <p>Yep, we'll first look at the third step of the simulation, because it will be important for understanding the second step.</p>

            <p>This step is probably the simplest. For each water cell, we simply add or remove water according to the horizontal and vertical flows adjacent to this cell:</p>

            <pre><code>for (int y = 0; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        water(x,y) += (
                  flowX(x,  y) + flowY(x,y  )
                - flowX(x+1,y) - flowY(x,y+1)
            ) * dt/dx/dy;</code></pre>

            <p><code>flowX(x,y)</code> and <code>flowY(x,y)</code> flow <i>towards</i> our <code>water(x,y)</code> cell, so we <i>add</i> them. The flows <code>flowX(x+1,y)</code> and <code>flowY(x,y+1)</code> flow <i>out of</i> our cell, thus we <i>subtract</i> them.</p>

            <p>This step just moves the water between the cells, according to the computed flows. The \(\frac{dx}{dt} = v\) part of Newton's laws, if you like.</p>

            <h2 id="section-outflow-scaling">Outflow scaling</h2>

            <p>This second step is probably the trickiest. See, we might have a problem on step 3: if the outgoing flow is large enough, the amount of water in a cell can <i>become negative</i>! This is bad, since there's no such thing as a negative amount of water (as opposed to e.g. EM waves).</p>

            <p>Fortunately, there is a simple solution called <i>outflow scaling</i>. We look at the flows adjacent to some water cell, and only consider the <i>outgoing</i> flows, i.e. flows that <i>remove</i> water from this cell, not <i>add</i> water. We take the total outgoing flow by summing these flows, and compare it to the actual amount of water in this cell. If we figure out that on the 3rd step we'll try to remove more water than the cell actually has, we simply <i>scale</i> the outgoing flows down, so that the water amount stays positive (or zero). Here's the code:</p>

            <pre><code>for (int y = 0; y &lt; N; ++y) {
    for (int x = 0; x &lt; N; ++x) {
        float total_outflow = 0.f;
        total_outflow += max(0.f, -flowX(x,y));
        total_outflow += max(0.f, -flowY(x,y));
        total_outflow += max(0.f, flowX(x+1,y));
        total_outflow += max(0.f, flowY(x,y+1));

        float max_outflow = water(x, y) * dx*dy/dt;

        if (total_outflow &gt; 0.f) {
            float scale = min(1.f, max_outflow / total_outflow);

            if (flowX(x,y) &lt; 0.f) flowX(x,y) *= scale;
            if (flowY(x,y) &lt; 0.f) flowX(x,y) *= scale;
            if (flowX(x+1,y) &gt; 0.f) flowX(x+1,y) *= scale;
            if (flowX(x,y+1) &gt; 0.f) flowX(x,y+1) *= scale;
        }
    }
}</code></pre>

            <p>It's a bit messy due to having to filter only outgoing flows, but it gets the job done.</p>

            <h2 id="section-terrain-elevation">Terrain elevation</h2>

            <p>Now, we want water moving over <i>terrain</i>, but where's the terrain in these equations? Adding terrain is actually extremely easy. Imagine two neighbouring water cells with equal water column heights: they don't try to move into each other, because the water level is the same. Now let's imagine that the left cell has some non-zero terrain elevation below it. This moves the water <i>surface</i> height up, and now this cell's <i>surface</i> is higher than the right cell, and the water starts to move.</p>

            <p>That is to say, when accelerating the flows, we simply need to replace the water <i>column</i> height by the water <i>surface</i> height, which is just terrain height plus column height.</p>

            <p>If <code>terrain(x,y)</code> is the terrain elevation at a cell, then all we need is to update acceleration computations:</p>

            <pre><code>for (int y = 0; y &lt; N; ++y)
    for (int x = 1; x &lt; N; ++x)
        flowX(x,y) = flowX(x,y) * pow(friction,dt)
                   + (
                      water(x-1,y) + terrain(x-1,y)
                    - water(x,y) - terrain(x,y)
                   ) * g * dt / dx;</code></pre>

            <p>and similarly for Y:</p>

            <pre><code>for (int y = 1; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        flowY(x,y) = flowY(x,y) * pow(friction,dt)
                   + (
                      water(x,y-1) + terrain(x,y-1)
                    - water(x,y) - terrain(x,y)
                   ) * g * dt / dy;</code></pre>

            <h2 id="section-boundary-conditions">Boundary conditions</h2>

            <p>When solving partial differential equations (which is what we're secretely doing here!), it's always important to consider what happens <i>at the boundary</i> of our simulation. For the virtual pipes method, the boundary conditions are implicitly defined by the boundary flow values, i.e.</p>

            <ul>
                <li>Left boundary: <code>flowX(0,y)</code></li>
                <li>Right boundary: <code>flowX(N,y)</code></li>
                <li>Bottom boundary: <code>flowY(x,0)</code></li>
                <li>Top boundary: <code>flowY(N,0)</code></li>
            </ul>

            <p>We can set these to whatever we want:</p>

            <ul>
                <li>Setting them to 0 makes them act like walls: water waves will simply collide with them</li>
                <li>Setting them to inflow (positive for left/bottom, negative for right/top) will make them add water to the simulation</li>
                <li>Setting them to outflow (negative for left/bottom, positive for right/top) will make them remove water from the simulation</li>
            </ul>

            <p>For stuff like water over terrain, outflow boundary conditions seem reasonable (water on the edge of the map will simply disappear through the edge). If some parts of the boundary cross a river, we'd probably want this parts to be an inflow instead, so that the river actually, y'know, has flowing water in it.</p>

            <p>It is important to set these values at the start of each simulation step, because outflow scaling can change them (and your outflow boundary can turn into a wall boundary).</p>

            <h2 id="section-viscosity">Viscosity</h2>

            <p>The <a href="https://diglib.eg.org/server/api/core/bitstreams/47f5228c-6f1c-4afb-ab80-b98c44575bc8/content">paper</a> also adds viscosity to the simulation, simply scaling the flows by some factor that depends on the water height (thus, it is different from friction). The idea is that smaller water layers have trouble moving around due to various internal forces, but larger water layers can move freely. This turns into simply multiplying the flows by \(\frac{H^2}{H^2+3\cdot \Delta t\cdot \nu}\), where \(H\) is the water level of the cell where the flow originates (e.g. left cell for positive X-flow, and right cell for a negative X-flow), and \(\nu\) is the viscosity constant.</p>

            <p>Here's the code:</p>

            <pre><code>for (int y = 0; y &lt; N; ++y) {
    for (int x = 1; x &lt; N; ++x) {
        float H = (flowX(x,y) &gt; 0.f) ? water(x-1,y) : water(x,y);
        H *= H;

        if (H &gt; 0.f)
            flowX(x,y) *= H/(H + 3*dt*viscosity);
    }
}</code></pre>

            <p>And similarly for Y:</p>

            <pre><code>for (int y = 1; y &lt; N; ++y) {
    for (int x = 0; x &lt; N; ++x) {
        float H = (flowY(x,y) &gt; 0.f) ? water(x,y-1) : water(x,y);
        H *= H;

        if (H &gt; 0.f)
            flowY(x,y) *= H/(H + 3*dt*viscosity);
    }
}</code></pre>

            <p>I imagine this could be useful for something like magma flow, but I didn't use it for water. The effects of viscosity are important on small scales (the paper applies it to blood flow inside organs), but on large terrain scales viscosity has almost no effect.</p>

            <h2 id="section-full-simulation-code">Full simulation code</h2>

            <p>So, here's the full simulation code that I used:</p>

            <pre><code>// Init boundary flows
for (int i = 0; i &lt; N; ++i) {
    flowX(0,i) = ...; // left boundary
    flowX(N,i) = ...; // right boundary
    flowY(i,0) = ...; // bottom boundary
    flowY(i,N) = ...; // top boundary
}

// Precompute the friction factor
float frictionFactor = pow(1-friction,dt);

// Accelerate X-flows
for (int y = 0; y &lt; N; ++y)
    for (int x = 1; x &lt; N; ++x)
        flowX(x,y) = flowX(x,y) * frictionFactor
                   + (
                      water(x-1,y) + terrain(x-1,y)
                    - water(x,y) - terrain(x,y)
                   ) * g * dt / dx;

// Accelerate Y-flows
for (int y = 1; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        flowY(x,y) = flowY(x,y) * frictionFactor
                   + (
                      water(x,y-1) + terrain(x,y-1)
                    - water(x,y) - terrain(x,y)
                   ) * g * dt / dy;

// Scale outflows to prevent negative water amounts
for (int y = 0; y &lt; N; ++y) {
    for (int x = 0; x &lt; N; ++x) {
        float total_outflow = 0.f;
        total_outflow += max(0.f, -flowX(x,y));
        total_outflow += max(0.f, -flowY(x,y));
        total_outflow += max(0.f, flowX(x+1,y));
        total_outflow += max(0.f, flowY(x,y+1));

        float max_outflow = water(x, y) * dx*dy/dt;

        if (total_outflow &gt; 0.f) {
            float scale = min(1.f, max_outflow / total_outflow);

            if (flowX(x,y) &lt; 0.f) flowX(x,y) *= scale;
            if (flowY(x,y) &lt; 0.f) flowX(x,y) *= scale;
            if (flowX(x+1,y) &gt; 0.f) flowX(x+1,y) *= scale;
            if (flowX(x,y+1) &gt; 0.f) flowX(x,y+1) *= scale;
        }
    }
}

// Update water columns
for (int y = 0; y &lt; N; ++y)
    for (int x = 0; x &lt; N; ++x)
        water(x,y) += (
                  flowX(x,  y) + flowY(x,y  )
                - flowX(x+1,y) - flowY(x,y+1)
            ) * dt/dx/dy;</code></pre>

            <p>And that's it! It might look intimidating at first, but as I've tried to explain earlier each step is actually pretty reasonable and intuitive. In the end, the bulk of the simulation is just 4 for-loops over a few 2D arrays with some fairly simple formulas inside.</p>

            <p>You can have a look at full C++ update code <a href="https://bitbucket.org/lisyarus/psemek/src/3655fc9c6f424559179570c640dbdae39352f089/examples/water_2d.cpp#lines-155">here</a>, though there's quite a bit of other stuff going on.</p>

            <p>Here's what it looks like:</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/water.mp4" type="video/mp4"></video></center>
            <center><i>Footage from my <a href="https://github.com/lisyarus/webgpu-shallow-water">WebGPU water simulator</a> that I released several days ago.</i></center>

            <p><i>(The particles in the video are for visualization only, they don't take part in the simulation itself.)</i></p>

            <p>Once you've found good parameter values (for <code>dt</code> and <code>g</code>), this thing seems to be stable as heck, while still satisfying all my requirements and generally looking more or less like water. Hooray!</p>

            <h2 id="section-model-shortcomings">Model shortcomings</h2>

            <p>Of course, this model isn't perfect. One of the most obvious problems is that it doesn't have <i>inertia</i> and <i>velocity diffusion</i>. A fast water stream entering a lake won't propagate further inside the lake, but will instead spread out in all directions, ignoring all accumulated inertia. Two parallel water streams going in opposite directions can exist and not interact with each other (provided the water levels are equal).</p>

            <p>It also tends to create these waves when the water first enters some area, which looks a bit weird, but I guess this is acceptable:</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/spill-waves.mp4" type="video/mp4"></video></center>

            <h2 id="section-triangular-grids">Bonus: hex/triangular grids</h2>

            <p>My game actually uses a regular triangular grid, not a square one, because...reasons. In fact, Boris The Brave has <a href="https://www.boristhebrave.com/2021/05/23/triangle-grids/">neatly summarized</a> all the advantages of such grids, and I'll refer you to his article.</p>

            <p>Triangle grids can be seen as duals to hexagonal grids: just connect the centers of adjacent hexagons with lines, and they'll form a regular triangular grid. If we look at <a href="https://www.redblobgames.com/">Red Blob Games</a>' <a href="https://www.redblobgames.com/grids/hexagons/">amazing article</a> on hexagonal grids, what I'm using is basically the axial coordinate system on a dual to a pointy-top hex orientation.</p>

            <p>What's cool is that we can store such grid in <i>usual 2D arrays</i> (just skewed a bit), and refer to vertices of such grid using <code>X,Y</code> coordinates:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/triangular-grid.png"></center>
            <center><i>Image credit to <a href="https://tex.stackexchange.com/a/562366">user cis</a> on tex.stackexchange</i></center>

            <p>So, to simulate water on such a grid, I'll store water column height at grid vertices, which will make it easy to render the water surface (it's just a triangulation, much like the terrain itself).</p>

            <p>The flows are a bit more complicated. There are flows in the X direction, flows in the Y direction, and flows in the...Z direction? Let's call it like that. Here's an illustration:</p>

            <center><img src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/hex-grid-flows.jpg"></center>
            <center><i>You have to excuse my drawing skills, I didn't have the time for a proper vector drawing</i></center>

            <p>So, for each vertex, there are:</p>

            <ul>
                <li>X-flow coming from the left</li>
                <li>X-flow going to the right</li>
                <li>Y-flow coming from bottom-left</li>
                <li>Y-flow going to top-right</li>
                <li>Z-flow coming from bottom-right</li>
                <li>Z-flow going to top-left</li>
            </ul>

            <p>For an \(N\times N\) grid of vertices, we have:</p>

            <ul>
                <li>\((N+1)\times N\) array of X-flows</li>
                <li>\(N\times (N+1)\) array of Y-flows</li>
                <li>\((N+1)\times (N+1)\) array of Z-flows, with the bottom-left and top-right values being unused</li>
            </ul>

            <p>This way,</p>

            <ul>
                <li><code>flowX(x,y)</code> is the flow from <code>(x-1,y)</code> to <code>(x,y)</code></li>
                <li><code>flowY(x,y)</code> is the flow from <code>(x,y-1)</code> to <code>(x,y)</code></li>
                <li><code>flowZ(x,y)</code> is the flow from <code>(x,y-1)</code> to <code>(x-1,y)</code></li>
            </ul>

            <p>Not much changes from the square grid case, we just need to incorporate the Z-flow into 1) boundary conditions, 2) acceleration, 3) outflow scaling, and 4) water updating. The hardest part here is not to mess up the indexing :)</p>

            <p>You can find the C++ code for such a simulation <a href="https://bitbucket.org/lisyarus/psemek/src/3655fc9c6f424559179570c640dbdae39352f089/examples/water_2d_hex.cpp#lines-115">here</a>. It works pretty well, and is hopefully a bit more isotropic than the square grid case :)</p>

            <center><video muted="" loop="" controls=""><source src="https://lisyarus.github.io/blog/media/simulating-water-over-terrain/hex-grid-water.mp4" type="video/mp4"></video></center>

            <p>I've yet to implement all this in my game, though, ‚Äî there are many, many more important things to do. But I'm quite determined that my game will have at least some form of basic water simulation now, which sounds super exciting. Imagine digging trenches to automatically water large farms, or diverting a river so that it floods a nearby village? The possibilities are endless. And for now, thanks for reading.</p>

            
            
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Pre-Trained Large Language Models Use Fourier Features for Addition (2024) (128 pts)]]></title>
            <link>https://arxiv.org/abs/2406.03445</link>
            <guid>42960989</guid>
            <pubDate>Thu, 06 Feb 2025 10:31:06 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arxiv.org/abs/2406.03445">https://arxiv.org/abs/2406.03445</a>, See on <a href="https://news.ycombinator.com/item?id=42960989">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="content-inner">
    
    
                
    <p><a href="https://arxiv.org/pdf/2406.03445">View PDF</a>
    <a href="https://arxiv.org/html/2406.03445v1">HTML (experimental)</a></p><blockquote>
            <span>Abstract:</span>Pre-trained large language models (LLMs) exhibit impressive mathematical reasoning capabilities, yet how they compute basic arithmetic, such as addition, remains unclear. This paper shows that pre-trained LLMs add numbers using Fourier features -- dimensions in the hidden state that represent numbers via a set of features sparse in the frequency domain. Within the model, MLP and attention layers use Fourier features in complementary ways: MLP layers primarily approximate the magnitude of the answer using low-frequency features, while attention layers primarily perform modular addition (e.g., computing whether the answer is even or odd) using high-frequency features. Pre-training is crucial for this mechanism: models trained from scratch to add numbers only exploit low-frequency features, leading to lower accuracy. Introducing pre-trained token embeddings to a randomly initialized model rescues its performance. Overall, our analysis demonstrates that appropriate pre-trained representations (e.g., Fourier features) can unlock the ability of Transformers to learn precise mechanisms for algorithmic tasks.
    </blockquote>

    <!--CONTEXT-->
    
  </div><div>
      <h2>Submission history</h2><p> From: Deqing Fu [<a href="https://arxiv.org/show-email/20287840/2406.03445" rel="nofollow">view email</a>]      <br>    <strong>[v1]</strong>
        Wed, 5 Jun 2024 16:40:53 UTC (14,504 KB)<br>
</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Aluminum Batteries Outlive Lithium-Ion with a Pinch of Salt (188 pts)]]></title>
            <link>https://spectrum.ieee.org/aluminum-battery</link>
            <guid>42960907</guid>
            <pubDate>Thu, 06 Feb 2025 10:13:26 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://spectrum.ieee.org/aluminum-battery">https://spectrum.ieee.org/aluminum-battery</a>, See on <a href="https://news.ycombinator.com/item?id=42960907">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-headline="A Pinch of Salt Boosts Aluminum Batteries"><p><a href="https://spectrum.ieee.org/tag/electric-vehicles">Electric vehicles</a>( <a href="https://spectrum.ieee.org/tag/evs">EVs</a>) and <a href="https://spectrum.ieee.org/tag/green-energy" target="_blank">green energy</a> sources rely heavily on <a href="https://spectrum.ieee.org/tag/batteries">batteries</a> to store electricity. Currently, more than 75 percent of the world‚Äôs <a href="https://spectrum.ieee.org/tag/energy-storage">energy storage</a>&nbsp;<a href="https://www.prnewswire.com/news-releases/battery-for-energy-storage-systems-market-to-grow-by-usd-22-18-billion-2025-2029-transition-from-fossil-fuels-to-renewable-energy-drives-growth-report-on-ai-driven-market-evolution---technavio-302359010.html" rel="noopener noreferrer" target="_blank">depends on batteries that contain lithium</a>, an expensive mineral that‚Äôs subject to volatile pricing. <a href="https://spectrum.ieee.org/tag/lithium-ion">Lithium-ion</a> (Li-ion) batteries themselves can be volatile, too, because they use a flammable electrolyte that can catch fire when overcharged. </p><p>Now, a group of scientists based in <a href="https://spectrum.ieee.org/tag/beijing">Beijing</a> believes that aluminum offers a better solution. Aluminum is the <a href="https://www.batterytechonline.com/materials/earth-abundant-elements-like-aluminum-could-replace-lithium-in-batteries" rel="noopener noreferrer" target="_blank">third-most abundant mineral in the Earth‚Äôs crust</a> and costs about one-quarter as much as lithium. And if built right, aluminum-based batteries may offer longer life expectancy and a safer, more sustainable design than their volatile counterparts. Led by scientists from the Beijing Institute of Technology and the University of Science and Technology Beijing, the group has found a way to <a href="https://pubs.acs.org/doi/10.1021/acscentsci.4c01615" rel="noopener noreferrer" target="_blank">stabilize aluminum batteries</a> that can last far longer. </p><p><a href="https://spectrum.ieee.org/tag/aluminum-ion">Aluminum-ion</a> (Al-ion) batteries have been the subject of research for years. But previous attempts have generally used <a href="https://spectrum.ieee.org/tag/ionic-liquid">ionic liquid</a>&nbsp;<a href="https://spectrum.ieee.org/tag/electrolytes">electrolytes</a>, which can lead to anode <a href="https://spectrum.ieee.org/tag/corrosion">corrosion</a>, especially in humid conditions. Other researchers have used gel polymer electrolytes, halfway between liquid and solid-state alternatives, but these tend to have low conductivity. <span>This team of researchers took a different approach and added a pinch of salt</span><span>‚Äînamely, </span><span>an inert aluminum fluoride salt</span><span>‚Äî</span><span>to a liquid electrolyte containing aluminum <a href="https://spectrum.ieee.org/tag/ions">ions</a></span><span>, creating</span><span> a solid-state electrolyte. </span></p><p><span><strong><span></span></strong>Well, more than a pinch of salt, really. The salt</span><span> has a porous 3D structure, which allows it to act like a rigid sponge that absorbs and stabilizes the liquid, yet still allows the ions to move more freely. This increases conductivity of the material, and the result is a solid composite material that cannot leak. The researchers also coated the electrodes with a thin layer of material that helps prevent crystals of aluminum from forming, which would degrade battery performance over time. </span></p><p><span>‚ÄúOur research shows that a stable, recyclable solid-state electrolyte can improve aluminum-ion batteries by solving issues like corrosion, safety, and long-cycle life, making them a potential alternative to lithium-based batteries,‚Äù says Shuqiang Jiao, a professor of electrochemical engineering at the University of Science and Technology Beijing.</span></p><h2>Aluminum‚Äôs Advantages</h2><p>The researcher‚Äôs tests demonstrated that the resulting battery design can have an extremely long life, with the battery retaining 99 percent of its original capacity after 10,000 charge/discharge cycles. In contrast, a typical <a href="https://spectrum.ieee.org/tag/li-ion-battery">Li-ion battery</a> retains only 80 percent of its charge capacity after 300 to 500 cycles, depending on conditions.</p><p>The solid-state electrolyte is also safer than typical Li-ion designs, which use liquid electrolytes. It won‚Äôt leak the way conventional batteries with a liquid electrolyte can, so the researcher‚Äôs Al-ion batteries <strong></strong>continued to function normally when damaged by <u></u>repeated punctures, even when penetrated all the way through. The batteries were also tested at temperatures as high as 200 degrees Celsius; the output voltage was nearly the same in spite of the heat, and the battery didn‚Äôt expand or deform excessively. <strong></strong></p><p>In addition to these performance advantages, the Al-ion battery boasts better recyclability, compared to conventional Li-ion designs. The researchers found that they were able to easily recover as much as 80 percent of the aluminum fluoride salt from the lab experiments, and they predict much higher levels of recovery at industrial scales. The solid aluminum foil can also be reused after a simple surface cleaning. As a result, the entire design lends itself to efficient <a href="https://spectrum.ieee.org/tag/recycling">recycling</a> of the components.</p><p>A better battery could have a major impact on many markets. ‚ÄúInnovations that elongate battery life and bring down costs have the potential to transform multiple industries, from automotive to <a href="https://spectrum.ieee.org/topic/consumer-electronics/">consumer electronics</a> and home security,‚Äù says <a href="https://www.parksassociates.com/team-detail/jennifer-kent" target="_blank">Jennifer Kent</a>, vice president of research at market research firm Parks Associates. ‚ÄúBattery range is a top barrier for EV purchases, for instance, with 39 percent of those not owning or intending to buy an EV saying that EVs need to be charged too frequently or have a limited driving range.‚Äù Less expensive batteries could also play an important role in advancing the use of <a href="https://spectrum.ieee.org/tag/sustainable-energy">sustainable energy</a> sources, such as wind and solar, by providing a cost-effective way to store excess energy until it is needed.</p><p>The new battery structure should be easy to <a href="https://spectrum.ieee.org/solid-state-battery-production-challenges" target="_blank">manufacture at commercial scale</a>. But before the Al-ion battery is ready for commercial applications, its <a href="https://spectrum.ieee.org/tag/energy-density">energy density</a> will need to be improved, the researchers say. Still, this new approach offers the potential for batteries that are safer, less expensive, longer lasting, and easier to recycle efficiently.</p><p><em>This story was updated on 5 February 2025 to include a quote from Shuqjiang Jiao.</em><br></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[US Cloud soon illegal in EU? US punches first hole in EU-US Data Deal (144 pts)]]></title>
            <link>https://noyb.eu/en/us-cloud-soon-illegal-trump-punches-first-hole-eu-us-data-deal</link>
            <guid>42960788</guid>
            <pubDate>Thu, 06 Feb 2025 09:52:27 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://noyb.eu/en/us-cloud-soon-illegal-trump-punches-first-hole-eu-us-data-deal">https://noyb.eu/en/us-cloud-soon-illegal-trump-punches-first-hole-eu-us-data-deal</a>, See on <a href="https://news.ycombinator.com/item?id=42960788">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><ul><li><a href="https://noyb.eu/en/eu-us-data-transfers-0">Background on the EU-US data transfer saga</a></li><li><a href="https://ec.europa.eu/commission/presscorner/api/files/attachment/872132/Trans-Atlantic%20Data%20Privacy%20Framework.pdf">TADPF information PDF by the EU</a></li><li><a href="https://www.dataprivacyframework.gov/">TADPF program page by the US Government</a></li><li><a href="https://eur-lex.europa.eu/eli/dec_impl/2023/1795/oj/eng">TADPF Decision (EU) 2023/1795</a></li><li>CJEU: <a href="https://curia.europa.eu/juris/liste.jsf?nat=or&amp;mat=or&amp;pcs=Oor&amp;jur=C%2CT%2CF&amp;num=C-362%252F14&amp;for=&amp;jge=&amp;dates=&amp;language=en&amp;pro=&amp;cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&amp;oqp=&amp;td=%3BALL&amp;avg=&amp;lgrec=de&amp;lg=&amp;page=1&amp;cid=24419979">Schrems I</a> and <a href="https://curia.europa.eu/juris/liste.jsf?nat=or&amp;mat=or&amp;pcs=Oor&amp;jur=C%2CT%2CF&amp;num=C-311%252F18&amp;for=&amp;jge=&amp;dates=&amp;language=en&amp;pro=&amp;cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&amp;oqp=&amp;td=%3BALL&amp;avg=&amp;lgrec=de&amp;lg=&amp;page=1&amp;cid=24420097">Schrems II</a></li><li><a href="https://news.bloomberglaw.com/privacy-and-data-security/trump-terminates-trio-of-democrats-from-privacy-oversight-board">Report on the removal of PCLOB members</a></li></ul><p><strong>The EU-US Data Transfer System - a mix of EU and US law. </strong>Generally, EU law prohibits exporting personal data to countries outside of the EU since 1995, unless there is an absolute need (e.g. when sending an email to any non-EU country). Data can be sent abroad when the non-EU country provides "essentially equivalent" protection of Europeans' personal data. The US, on the other hand, has <a href="https://noyb.eu/en/eu-us-data-transfers-0">very strong mass surveillance laws</a> (e.g. FISA702 or EO 12.333), that allow the US government to access any data stored with Amazon, Meta, Microsoft, Google and any other US Big Tech firm without probable cause or individual judicial approval. Therefore, the European Court of Justice has held twice (<a href="https://curia.europa.eu/juris/liste.jsf?nat=or&amp;mat=or&amp;pcs=Oor&amp;jur=C%2CT%2CF&amp;num=C-362%252F14&amp;for=&amp;jge=&amp;dates=&amp;language=en&amp;pro=&amp;cit=none%252CC%252CCJ%252CR%252C2008E%252C%252C%252C%252C%252C%252C%252C%252C%252C%252Ctrue%252Cfalse%252Cfalse&amp;oqp=&amp;td=%3BALL&amp;avg=&amp;lgrec=de&amp;lg=&amp;page=1&amp;cid=24419979">Schrems I</a> and <a href="https://noyb.eu/en/cjeu">Schrems II</a>) that US law is not "essentially equivalent". However, Ursula von der Leyen has insisted to pass a third EU-US deal, called <a href="https://noyb.eu/en/european-commission-gives-eu-us-data-transfers-third-round-cjeu">"Transatlantic Data Privacy Framework"</a> (TADPF).</p><p><strong>TADPF was built on sand. </strong>On 10.7.2023 the European Commission issued <a href="https://eur-lex.europa.eu/eli/dec_impl/2023/1795/oj/eng">Implementing Decision (EU) 2023/1795</a>, formally passing the TADPF. This allowed any EU business to freely transfer data to US providers, despite US surveillance laws. The European Commission relied on (very questionable) executive orders or letters by the US government, including the PCLOB, to find that the US is "essentially equivalent". However, these elements are not reflected in US statutes and codified law, because there was no majority in the US Congress to pass such laws<span>. </span>It was long criticised that the next US president could kill these protections with the strike of a pen. This scenario is now on the horizon. In its decision, the European Commission mentioned the PCLOB a whopping 31 times to explain why the US has "essentially equivalent" protections. The PCLOB is the only general "oversight" body that monitors if US services actually compy with laws, orders and other promises. Other elements of US law, like various redress mechanisms, require a plaintiff to become active. The US has traditionally blocked access to these bodies via various "standing" rules, leading to basically no lawsuits ever beeing admitted. This means that the PCLOB is the only relevant oversight mechanism that the TADPF relied upon.</p><p>Max Schrems: "<em>This deal was always built on sand, but the EU business lobby and the European Commission wanted it anyways. Instead of stable legal limitations, the EU agreed to executive promises that can be overturned in seconds. Now that the first Trump waves hit this deal, it quickly throws many EU businesses into a legal limbo. The PCLOB itself is only one puzzle piece, and as long as it is only temporarily not functioning, there is an argument that the deal is not worse then before. However, the direction this is taking in the first week of the Trump Presidency is not looking good. We are closely monitoring, if this is a temporary problem or if the PCLOB is being killed for good."</em></p><p><strong>Independence of executive bodies called into question.</strong> Different to data protection authorities in the EU, most US oversight bodies are creatures of the executive branch and hence not independent. Independence is often only granted by the President, but can be revoked or overruled at any time. Many of these strange legal concepts are a reults of the structural inability to pass actual legislation in the US. Instead, entire legal areas are merely regulated by Presidential orders. The fact that the US president is now attempting to simply remove people, calls into question if the idea of (allegedly) "independent" executive bodies was even factually arguable from the get go. Many other elements of the TADPF, like the Data Protection Review Court have even weaker legal protections than the PCLOB.</p><p>Max Schrems: "<em>There were many questions on the independence of these oversight mechanisms. Unfortunately, it seems that they may not even stand the test of just the first days of a Trump Presidency. This is the difference between solid legal protections in law and wishful thinking. The European Commission has solely relied on the latter.</em>"</p><p><strong>45 days for next crunch point. </strong>In <a href="https://www.whitehouse.gov/presidential-actions/2025/01/initial-rescissions-of-harmful-executive-orders-and-actions/">one of the first Executive Orders Trump has signed</a> on Monday, he determined that all Biden national security decisions (including the relevant decisions that the EU-US transfers rely upon) shall be reviewed and potentially scrapped within 45 days. This means that further elements the TADPF relied upon could collapse within days. As the entire deal is based on Biden executive decisions, Trump could scrap all key elements of the deal with a single signature <span lang="de">‚Äì</span> leading to instantly illegal data transfers between the EU and the US.</p><p>Max Schrems: "<em>I can hardly imagine that a Biden Executive Order that was forced on the US by the EU and that regulates US espionage abroad could survive Trump's 'America First' logic. The problem is, that not just US Big Tech, but especially normal EU businesses all rely on this system of instable executive orders to argue that using US cloud systems is legal in the EU."</em></p><p><strong>Commission manoeuvred EU businesses towards a cliff.</strong> Despite all facts and criticism by the European Parliament and EU data protection authorities, the European Commission has consistently argued that the TADPF is solid and sound. The EU business lobby pushed for a(ny) deal <span lang="de">‚Äì</span> no matter how unstable or wacky. Equally, US Big Tech wanted to stay on the EU market without any technical limitations in relation to US government access. Now, everyone from large banks, entire national school systems to many small businesses may wake up to a legal situation, where the use of US cloud products is soon illegal.</p><p><strong>EU-US data transfers legal for now </strong><span lang="de"><strong>‚Äì</strong></span><strong> but get prepared.</strong> A decision by the US administration will not instantly make US transfers illegal. The European Commission's decision is generally legal as long as it is on the books and not annulled by the Commission itself or the Court of Justice. So even if the material finding becomes wrong, the decision still formally exists until it is overturned. However, if key elements that the EU has relied upon are not functioning, the EU will have to annul the deal.</p><p>Max Schrems: <em>"While the arguments for the EU-US deal seem to fall apart, companies can rely on the deal as long as it is not formally annulled. However, given the developments in the US, it is more crucial than ever for businesses and other organisation to have a 'host in Europe' contingency plan."</em></p><p><strong>European Comission in a tough spot. </strong>The European Commission has manoeuvred itself in a tough spot not only from a credibility perspective, but also from a diplomatic perspective. If it now reacts quickly and annuls the TADPF, <a href="https://www.politico.eu/article/zuckerberg-urges-trump-to-stop-eu-from-screwing-with-fining-us-tech-companies/">the US Tech Oligarchy will cry that the EU would be "<em>screwing with</em>" US Big Tech</a>. The Trump administration may take this as a reason to start a first major fight with the EU. However, not taking action and failing to officially warn EU businesses, public bodies and other organisations that send data to the US also seems problematic. The future of the TADPF may be very short-lived.</p><p><strong>EU version of the US TikTok debate?</strong> While the US has long belittled European fears about personal data flowing to the US and being used in mass surveillance, the US has suddenly turned around once its own data was aggregated by TikTok. On one hand, a prohibition or a compulsory acquisition of US Big Tech in Europe would be legally impossible. US businesses would be protected from the EU passing an equivalent to a "TikTok ban". At the same time, a duty to keep EU data outside of the hands of the US government is the default under EU law since 1995. It would also be the law, once the European Commission annuls the EU-US deal. US Big Tech would then have to shield their EU data centers from access by their US parent companies..</p></div></div>]]></description>
        </item>
    </channel>
</rss>