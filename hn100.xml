<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
    <channel>
        <title>HN100 - Readable Contents</title>
        <link>https://hn.algolia.com/api/v1/search_by_date?tags=%28story,poll%29&amp;numericFilters=points%3E100</link>
        <description>Uses Readability to add bodies to the RSS feed</description>
        <lastBuildDate>Tue, 27 Feb 2024 19:00:24 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Dad died in 2022. Since 2023, things he selfhosted have slowly begun breaking (106 pts)]]></title>
            <link>https://old.reddit.com/r/selfhosted/comments/1b0rdft/dad_died_in_2022_since_2023_things_he_selfhosted/</link>
            <guid>39526863</guid>
            <pubDate>Tue, 27 Feb 2024 17:34:38 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/selfhosted/comments/1b0rdft/dad_died_in_2022_since_2023_things_he_selfhosted/">https://old.reddit.com/r/selfhosted/comments/1b0rdft/dad_died_in_2022_since_2023_things_he_selfhosted/</a>, See on <a href="https://news.ycombinator.com/item?id=39526863">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Throwaway for privacy reasons. As I said, things he maintained have been going down since at least mid-2023. His death was very sudden, so we didn't exactly have things set up for others to take over. Before anyone asks, I don't think he posted here. I don't see any of his usual handles, and he'd been running these things longer than Reddit has existed.</p>

<p>The main thing I'm hoping to recover is the webmail (I think) service most of my family used. That went down in September, and we've lost access to a number of other accounts because of that. There's also a Plex server and a handful of websites he hosted that would be good to get up and running. </p>

<p>I'm computer literate, but not tech-savvy enough to even really know where to start. I'm sorry if this is basic or not much to go off of, but I'm kind of overwhelmed. I just need some starting steps. Here's what little I do know:  </p>

<ul>
<li>Dad had two physical servers, Ranma and Akane. One is a mail server and the other a web server. We know the mail server (Ranma, I think) went down in September, but rebooting it wasn't enough.</li>
<li>Last we checked, the domain names for the websites are still safe. I'm guessing that just means the hosting server went down, which is probably the mentioned web server?</li>
<li>We do have a list of passwords. It's not very well organized, and I seem to remember we had trouble finding the admin password last time we tried to check on the servers, but in theory we're able to get in eventually.</li>
<li>The mail service seems to be IMAP.</li>
</ul>

<p>What I don't know:</p>

<ul>
<li><p>The operating system of either server. They're <em>probably</em> Macs, but more than that I don't know. They've been running for as long as I can remember, and I have no idea how much their hardware or software were updated over the years.
EDIT: Based on comments, probably Linux. Kinda just a command line terminal.</p></li>
<li><p>Assuming things don't fix themselves once we're into the servers, how do I get things back online? I don't know anything about hosting websites or mail services.</p></li>
<li><p>Any details about how anything was run. He didn't talk much about it, but then again, we never asked much about it.</p></li>
</ul>

<p>It's entirely possible that successfully logging in will be enough to get things running, but I haven't been able to test that far. The servers are physically located across town, and getting there without a car is kind of a multi-hour process. Dad was able to use remote logins for most issues, but I don't know how to do that.</p>

<p>Even if that did work, though, I'll need to know some basic troubleshooting steps if I want to keep them running long term, even if it's only in maintenance mode. Again, I'm sorry if this seems basic or unhelpful, I am just really out of my element here.</p>

<p>Edit: Hello! Thank you all for the comments and messages, I am slowly picking through everything. I figured I should add some more info based on what’s been mentioned.</p>

<p>I haven’t been able to check the hardware as of the evening after posting the thread due to a small miscommunication with the building owner, but we know the storage for the Plex server, at least, is on a Synology. Everything else, who knows. I’ll hopefully know more tomorrow afternoon.</p>

<p>Several people have urged me to clone or backup everything before I mess around. I agree! That’s a very good idea. How do I do that? I think we do have actually have a drive cloner, but it’s for one drive at a time and I think exclusively SSDs, which I’m not 100% sure the setup is using. Plus, I’m not sure I want to be removing drives before knowing the state of everything.</p>

<p>Lastly, it genuinely didn’t occur to me that people would offer to help locally. I don’t want to publicly give the specific city, but if any of you are or know someone in the Colorado Front Range, feel free to message me.</p>

<p>Thank you all for the comments. I’ll try and keep this post updated.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[How Netflix really uses Java (113 pts)]]></title>
            <link>https://www.infoq.com/presentations/netflix-java/</link>
            <guid>39526220</guid>
            <pubDate>Tue, 27 Feb 2024 16:44:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.infoq.com/presentations/netflix-java/">https://www.infoq.com/presentations/netflix-java/</a>, See on <a href="https://news.ycombinator.com/item?id=39526220">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="presentationNotes">
                                    <h2>Transcript</h2>

<p>Bakker: I'm going to talk about how Netflix is really using Java. You probably know that Netflix is really just about RxJava microservices, with Hystrix and Spring Cloud. Really, Chaos Monkeys are just running the show. I'm only half getting here because a few years ago, this was actually mostly true, maybe except the Chaos Monkeys. This stack was something that we were building on in the last several years. Things have changed. Quite often, I have conversations with people at conferences like this one, where they're like, yes, we were using the Netflix stack. Like, which stack exactly are you talking about? It's almost never the stack that we're actually using. These are just things that people associate with Netflix, because we've been talking about our technology for so many years, but things might have changed a little bit. We're going to bust some myths. We're going to take a look at what we're actually doing with Java. Things are ever-evolving. Things are literally just changing all the time.</p>

<h2>Background</h2>

<p>My name is Paul. I'm in the Java Platform at Netflix. Java Platform is responsible for the libraries, frameworks, and tooling that we built around Java, so that all our Java developers have a good time developing Java applications. I'm also a Java champion. I have been in the Java space for quite a long time. In the past, I wrote two books about Java modularity. I'm also one of the first authors of the DGS framework, that's the GraphQL framework we use for Java. We'll talk quite a bit about DGS, and how that all fits in the architecture.</p>

<h2>Evolving Architecture</h2>

<p>Before we start diving into JVMs and how we use Java, and the framework that we're using, we have to understand a little bit better how our architecture has been evolving. That explains why we did things in a certain way with Java several years ago, and we're doing things quite differently today. What you should understand about Java at Netflix is that we have a lot of Java. We are basically a Java shop, and every backend at Netflix is basically a Java app. We have many applications. At the size of Netflix, there's lots of internal applications to just keep track of things. We're also one of the largest film studios in the world. There's a lot of software being developed just to produce films, basically, again, all Java. Then of course, we have what we call the streaming app, which is basically the Netflix app, as you probably know it. That is what we're looking at here. This screen here is what we call the LOLOMO, the list of list of movies. That is just one example of an application that is backed by Java. You have to understand that pretty much everything that I'm talking about, that is true for basically every backend in Java. We use the same architecture now for pretty much all our different systems, both internal and consumer facing, and we use the same tech stack everywhere. Although I'm giving that example, because it's just a large example to play with, it's much more universal than that.</p>

<h2>The Groovy Era</h2>

<p>When I joined Netflix almost seven years ago, we were in what I call the Groovy era. What you probably know about Netflix, and this is still true, is that Netflix has a microservices ecosystem. Basically, every piece of functionality and every piece of data is owned by a specific microservice. There's many of them, literally thousands of them. On the slide here, I just made it up, because it makes sense in my head. It's a much-simplified version of what we actually have in production. Think about this LOLOMO screen, this list of list of movies that we just looked at, at a previous slide, you're probably familiar with that screen, that to render that screen, we would have to fetch data from many different microservices. Maybe there's like a top 10 service that we need, because we need a top 10 list of movies. That's backed by a specific service. Then there's an artwork service that gives us the images as we show in the LOLOMO, and these are all personalized as well. There's probably a movie metadata service, which gives us movie titles and actors and descriptions of movies. There's probably a LOLOMO service which is actually giving us what lists to actually render, which again is personalized. I say that we have maybe 10 services to call out to. It will be usually inefficient if your device, let's say, your TV, or your iOS device will just do 10 network calls to these different microservices. It will just not scale at all. You would have a very bad customer experience. It would feel like using the Disney app. It's just not ideal. Instead, we need a single front door for the API where your device is calling out to. From there, we do a fanout to all the different microservices, because now we are in our network, we are on a very fast network. Now we can do that fanout without performance implications. We have another problem to solve, because all these different devices, in subtle ways, they're all a little bit different. We try to make the UI look and behave similar on every different device. All these different devices, like a TV versus an iOS device have very different limitations when it comes to memory, network bandwidth. They actually load data in subtly different ways.</p>

<p>Think about, how would you create an API that would work for all these different devices? Let's say you create a REST API. We're probably going to get either too little or too much data. If we create one REST API to rule them all, it's going to be a bad experience for all these different devices, because we always waste some data, or we have to do multiple network calls, which is also bad. To fix that problem, what we did is we used what we call a backend for frontend pattern. Basically, every frontend, every UI gets its own mini backend. That mini backend is then responsible for doing the fanout and get the data that that UI exactly needs at that specific point. They used to be backed by a Groovy script. That mini backend was basically a Groovy script for a specific screen on a specific device, or actually a version of a specific device. These scripts would be written by UI developers, because they are the only ones who actually know what data exactly they need to render a specific screen. This Groovy script would just live in an API server, which is a giant Java app, basically. It would do a fanout to all these different microservices by just calling Java client libraries. These client libraries are just basically wrappers for either a gRPC service, or a REST client.</p>

<p>Now, here we started seeing an interesting problem, because, how do you take care of such a fanout in Java? That's actually really not trivial. Because if you will do this the traditional way, you create a bunch of threads, and you start to manage that fanout with just minimal thread management, that gets very hairy very quickly, because it's not just managing a bunch of threads, it is also taking care of fault tolerance. What if one of those services are not responding quickly enough? What if it is just failing? Now we have to clean up threads and make sure that everything comes together nicely again. Again, not trivial at all. This is where RxJava and reactive programming really came in. Because reactive programming gives you a much better way to do such fanouts. It will take care of all the thread management and stuff like that you need to do. Exactly because of this fanout behavior, that is why we went so deep into the reactive programming space, and we were partly responsible for making RxJava a big thing many years ago. On top of RxJava, we created Hystrix, which is a fault tolerant library, which takes care of failover and bulkheading, and all these things. This made a lot of sense seven years ago when I joined. This was the big architecture that was serving most traffic. Actually, it is still a big part of our architecture, because depending on what device you're using, if it's a slightly older device, you probably still get served by this API, because we don't have just the one architecture we have many architectures, because it is nicer that way.</p>

<h2>Limitations</h2>

<p>There are some limitations, although this obviously works really well, because we have been able to grow our member base based on this architecture primarily. One downside is that there's a script for each endpoint. Because, again, we need an API for each of these different UIs. There are just a lot of scripts to maintain and manage. Another problem is that because the UI developers have to create all the mini backends because they are the ones who know what data they need, they have to write those. Now they are in the Groovy Java space and using RxJava. Although they're very capable of doing so, it's probably not a primary language that they are using on a daily basis. The main problem is really that reactive is just really hard. Speaking for myself, I've been doing reactive programming for at least 10 years. I used to be extremely excited about it, and tell everyone about how great it all is. It is actually hard, because even if with that experience, look at a non-trivial piece of reactive code, I have no clue what's going on. It takes me quite a bit of time to actually wrap my head around, ok, this is actually what's happening. These are the operations that are supposed to happen. This is the fallback behavior. It's hard.</p>

<h2>GraphQL Federation</h2>

<p>Slowly, we have been migrating to a completely new architecture, and that is, we're putting things to a different perspective. That's all based on GraphQL Federation. Comparing GraphQL to REST, one very important aspect of GraphQL is that with GraphQL, you always have a schema. In your schema, you put all your operations, so your queries and your mutations, and you define them, and you tell it exactly which fields are available from the types that you're returning from your queries. Here we have a shows query, which returns a show type, and a show as a title, and it has reviews. Reviews again is another type that we define. Then we can send a query to our API, which is on the right-hand side of the slide. What we have to do there, and this is, again, really important, we have to be explicit about our field selection. We can't just ask for shows and get old data from shows. Now we have to say specifically that you want to get a title and the star score on reviews on a show. If we're not asking for a field, we're not getting a field. It is super important because again, compared with REST, very basically, you get whatever the REST service decides to send you. You're just getting the data that you're explicitly asking for. It's more work if you specify your query, but it solves the whole problem of over-fetching, where you get much more data than you actually need. This makes it much easier to create one API that serves all the different UIs. Typically, when you send a GraphQL query, you will just get the result back encoded as JSON.</p>

<p>We're not just doing GraphQL, we're actually doing GraphQL Federation to fit it back into our microservices architecture. In this picture, we still have our microservices, but now we call them DGSs. They're just a term that we at Netflix came up with. It's a domain graph service. Basically, it's just a GraphQL service. There's really nothing special about it, but we call them DGSs. A DGS is just a Java microservice, but it has a GraphQL endpoint. It has a GraphQL API. That also means it has a schema, because we said that for GraphQL, you always have a schema. The interesting thing is that we have, of course, many different DGSs, many different microservices. From the perspective of a device, so from the perspective of your TV, for example, there's just one big GraphQL schema. The GraphQL schema contains all the possible data that we have to render, let's say a LOLOMO. Your device doesn't care that there might be a whole bunch of different microservices in the backend, and that these different microservices might provide part of that schema. On the other side of the story on the microservices sides, in this example, our LOLOMO DGS is defining a type show, with just a title. The images DGS can extend that type show and add an artwork URL to it. These two different DGSs don't know anything about each other than the fact that there is a show type. It can both contribute parts of that schema, even on the same types. All they need to do is publish their schema to the federated gateway. Now the federated gateway knows how to talk to a DGS because they all have a /GraphQL endpoint. That's it. It knows these different parts of the schema, so if a query comes in where we ask for both title and artwork URL, it knows that it has to call out to these different DGSs, and fetch the data that it needs. On a very high level, not that different from what you previously had, but there's a lot of differences in the details.</p>

<p>I'll also change our story here. First of all, we don't have any API duplication anymore. We don't need a backend for frontend anymore because GraphQL as an API is flexible enough, because of field selection that we don't really need to create those device specific APIs anymore. It also means we don't have server-side development for UI engineers anymore. That's great. We do get a schema to collaborate on. That's a big deal, because now we have closed the gap between UI developers and backend engineers, because now they can collaborate on a schema and figure out, ok, what data do we need in what format? Very importantly, we don't have any client libraries in Java anymore, because the federated gateway just knows how to talk to a generic GraphQL service. It doesn't need specific code to call out to the specific API. It's all just GraphQL. All it needs to know, how to talk to a GraphQL service. That's all. It's all based on the GraphQL specification. We don't need specific code to call to a specific microservice anymore.</p>

<h2>What Does that Mean for Our Java Stack?</h2>

<p>Now we get into, how does that change our Java stack? There's really no place anymore where we need Rx, or Hystrix, or such things, because previously, we needed this because we needed that specific code to call out, ok, I want to call this microservice and then this microservice, and at the same time, this other microservice. We needed an API for that. We don't need it anymore, because that's now taken care of by the GraphQL Federation specification. That's not completely true, because the federated gateway itself is actually still using a web client to call the different DGSs, and that is still reactive. However, it is not using any specific code for this microservice anymore. It's actually a very straightforward piece of web client code where it knows, ok, I have to call these three services, just go do it. It's all GraphQL, so it's very simple. All the DGSs and the other microservices in the backend, they're all just normal Java apps. There's not really anything specific about them. They don't need to do any reactive style of programming pretty much anywhere.</p>

<h2>The Micro in Microservices</h2>

<p>Before we dive deep into the rest of our Java stack, I want to speak a little bit about the micro in microservices, because it's another thing that people seem to be confused about how it actually works in practice. It is true that a microservice owns a specific functionality or dataset. More importantly that such microservices are owned by a single team. That is a really important part about microservices. It is all even more true with this GraphQL federated architecture, because it's now even easier to just split things out in different microservices and make it all work very nicely. However, don't be fooled by the size of those microservices, because a lot of those so-called microservices at Netflix are a lot larger, just looking at the code base, than the big monoliths that I've worked at, at many other companies. Some of these systems are really big. There's a lot of code there. Of course, when they get deployed, they might be deployed on clusters of thousands of AWS instances. There's really nothing small about them. That also answers the question, should I do microservices? It depends on your team size. Do you have like the one team that takes care of everything, and it's just a small team? If you would add microservices there, you're just adding complexity at that point for no good reason. If you want to split your team into smaller teams, basically, and just because of team size, then it also makes sense to split up your larger system into smaller pieces so that each team can own and operate one or more of those services.</p>

<h2>Java at Netflix</h2>

<p>Time to actually really get into the Java side of things. We now know, on a higher level, how and where we're using Java. Now we talk about how it actually looks like. We are now mostly on Java 17. It is about time. We are already also actively testing and rolling out with Java 21. Java 21 just came out officially. We're just using a regular Azul Zulu JVM. It's just an OpenJDK build. We are not building our own JVM, we don't have any plans to build our own JVM. Although there was a very interesting Reddit thread claiming that we do. We really don't, and have no interest in doing so. OpenJDK is really great. We have about 2800 Java applications. These are mostly all microservices of a variety of sizes. Then about 1500 internal libraries. Some of them are actual libraries, and many of them are just client libraries, which is basically just sitting in front of a gRPC or REST service. For our build system, we use Gradle, and on top of Gradle we have Nebula, that's a set of open sourced Gradle plugins. The most important aspect of Nebula, and I highly recommend looking into this, is, first in resolution of libraries. As you know, Java has a flat classpath. You can only have the one version of the library at a given time, if you have more than one version, interesting things happen. To prevent these interesting things from happening, you really want to just pick one, basically, and Nebula takes care of that. The next thing that Nebula does is version locking. Basically, you will get reproducible builds that you always build with the same set of versions of libraries until you explicitly upgrade. That makes it all very reproducible. We're pretty much exclusively using IntelliJ as our IDE. In the last few years, we have also invested a lot of effort in actually developing IntelliJ plugins, to help developers doing the right thing.</p>

<h2>The Java 17 Upgrade</h2>

<p>We are mostly on Java 17. That is actually a big deal, because this is embarrassing, but at the beginning of the year, we were mostly on Java 8. Java 8 is old. Why were we still on Java 8? Because we had Java 11, and then Java 17 available for a very long time already. Somehow, we just didn't move. One of the reasons is that until about a year ago, about half of our microservices, especially the bigger ones, were still on our old application stack. It was not Spring. It was a homegrown thing based on Guice, and a lot of old Java EE APIs, lots of old libraries that were no longer maintained. At the very beginning when we started upgrading to Java 11 initially, a lot of these older libraries were just not compatible. Then developers just got the impression that this upgrade is hard, and it breaks things, and I should probably just not do it. On the other hand, there was also very limited perceived benefits for developers, because if you compare Java 8 to Java 17, there's definitely some nice language features. Text blocks alone are enough reason for me to upgrade, but it's not that big of a deal. The differences between 8 and 17 is nice, but it's not like changing your life that much. There was more excitement about moving to Kotlin than we did in just upgrading to JDK.</p>

<p>When we finally did start pushing on updating to Java 17, we saw something really interesting. We saw about 20% better CPU usage on 17 versus Java 8, without any code changes. It was all just because of improvements in G1, the garbage collector that we are mostly using. Twenty-percent better CPU is a big deal at the scale that we're running. That's a lot of money, potentially. Speaking about G1, G1 is the garbage collector that we use for most of our workloads, at the moment. We've tested with all the different garbage collectors available. G1 is generally where we got the best balance of tradeoffs. There are some exceptions, for example, Zuul, which is our proxy. It runs on Shenandoah, that's the low pause time garbage collector. For most workloads, Shenandoah doesn't work as well as G1 does. Although G1 isn't that exciting anymore, it is still just really good.</p>

<h2>Java 21+</h2>

<p>Now that we have finally made a big push to Java 17, and we've got most services just upgraded, we also have Java 21 available. We've been testing with that for quite a few months already. Now things really get exciting. The first exciting thing is that if you're on Java 17, upgrading to Java 21 is almost a no-op. It's just super easy. You don't have the problems that we had from Java 8 to newer versions. There's also just a lot more interesting features. The first obvious one that I'm super excited about is virtual threads. This is just copy-paste, it's from the JEP, the specification from Java 21 of virtual threads. It's supposed to enable server applications written in a simple thread-per-request style to scale at near optimal hardware utilization. It sounds pretty good. This thread-per-request style, if you're using something that's based on servlets, so Spring Web MVC, or any other framework based on servlets, thread-per-request is basically what you get. A request comes in, Tomcat or whatever server you're using gives it a thread. That thread is basically where all the work happens, or starts happening for the specific request, and stays through that request until the request is done. That is a very simple style and easy to understand style of programming, and all the frameworks are based on that. It has some scalability limitations, because you can only have so many threads effectively running in a system. If you have a lot of requests coming in, which we obviously have, then the number of threads is just a limiting factor in how you can scale your systems. Changing that model is really important. The alternative to that is, of course, doing reactive again, so do something like WebFlux. That also gets you in reactive programming, again, with all the complexities that we already talked about.</p>

<p>Now, I think that virtual threads is probably the most exciting Java feature since probably lambdas. I think that down the line, it is really going to change the way we write and scale our Java code. I think that, in the end, it is probably going to further reduce reactive code, because there's just not really any need for it anymore. It just takes away that complexity. We have already been running virtual threads in production for the last month or so, experimenting with it a little bit. I'll get back to that in more detail. Then the other interesting feature in Java 21 is the new garbage collector or the updated garbage collector, because ZGZ is not new. That was already available in previous versions. They now made it generational, and that makes it give more benefits over G1 as a garbage collector has. That will make ZGC a better fit for a broader variety of workloads. It's still focused on low pause times, but it will just work in a broader variety of use cases. It's a little bit early to tell because we haven't done enough testing with this yet, but we are expecting that ZGC is now going to be a really good performance upgrade, basically, for a lot of our workloads and a lot of our services. Again, these things are a really big deal, where we could save a lot of money on resources. Shenandoah is also now generational, but that is still in preview. Again, we're going to just run with that and see what happens. Garbage collection is really just too complex of a topic to just know that, drop in this garbage collector with this flex, and it's all going to be magic and super-fast. Just doesn't work that way. It's a business where you just try things out and then you tweak it a bit, and you try it again, and then you find the optimal state. We're not quite there yet. We are expecting to see some very interesting things there. Then, finally, in Java 21, you just also have a lot of nice language features. We get this concept of data-oriented programming now in the Java language. It is really nice. It's the combination of records and pattern matching and things like that. Java is pretty nice right now.</p>

<h2>Virtual Threads</h2>

<p>Back to virtual threads. Although I said that this is a big deal, and is probably going to change the way we write our code and scale our code, it is also not a free lunch. It's not just that you enable Java 21 on your instances, and now by the magic of virtual threads, everything runs faster. It doesn't work that way. First of all, we have to change our framework library, and to some extent application code to actually start leveraging virtual threads, so step one. There are a few obvious places where we can do that and already started experimenting, so the Tomcat connection pool. Again, these are the pool of threads where it gives threads-per-request. That seems a fairly obvious place where we can just use virtual threads instead. Instead of using a thread pool, you use virtual threads. Before you enable that, you are already running some big services in production with virtual threads enabled. It doesn't automatically make things a lot faster, because you need to do other things as well to really leverage it. It also doesn't make things worse. If you can just safely enable this basically, sometimes get some benefits out of it, sometimes it doesn't really change it because it wasn't a limiting factor. That's something that you should probably start with. Async task execution in Spring that is, again, just a thread pool, and very often you get blocking code for other network calls there anyway. It seems to be a good candidate for virtual threads, so we enabled it there. Then a really big one that we haven't really gotten into yet, but I expect that will be game changing is how we do GraphQL query execution. Potentially with GraphQL, every field can be fetched in parallel. It makes a lot of sense that we would actually do that on virtual threads because, again, this is often work in code where you do more network calls and things like that. Virtual threads just make a lot of sense there, but we have to implement this and test it out, and it'll probably take a little bit of time before we get the optimal model there.</p>

<p>Then we have some other places that seemed obvious. For example, we have a thread worker pool for gRPC clients where the gRPC calls to outgoing services happen. It seemed like such an obvious place like, let's drop in virtual threads there. Then we saw that we actually decreased performance by a few percent. It turns out that these gRPC client worker pools are very CPU intensive. If you then drop in virtual threads, you actually make things worse. That's not a bad thing, necessarily. This is just something that we had to learn. It does show that this is not a free lunch. We actually have to figure out, where does it make sense, where does it not make sense, and implement virtual threads at the right points, basically. The good news is this is mostly all framework work at this point. We can do it as a platform team, and we can do it in open source libraries that we're using. Then our developers will just get faster apps, basically. It's good. In Spring 6.1, or Spring Boot 3.2, there's a lot of work being done to leverage virtual threads out of the box, that will come out next month. We will probably adopt that somewhere early next year. Then there's a really interesting discussion going on on GitHub, in GraphQL Java, about changing the GraphQL query execution, or potentially even rewriting it to fully leverage virtual threads. That is not figured out yet. It's a discussion going on. If you're in that space, that's definitely something to contribute to, I think. Then for the user code, because all this other stuff is mostly framework code, for user code, I think structured concurrency is the other place that we're going to see a lot of replacement of reactive code. Because structured concurrency is finally giving us the API to deal with things like fanouts, and then bringing everything together again. Structured concurrency is still in preview in Java 21. It seems very close to final, so I think it's at least safe to start experimenting with this and try things out. Then a little bit further down the line, we also get scoped values, which is another new specification coming out related to virtual threads. That is going to give us a way to basically get rid of ThreadLocal. This is again mostly framework related work. It's just a much nicer and more efficient way of something similar to ThreadLocal.</p>

<h2>Spring Boot Netflix</h2>

<p>I've already mentioned a little bit that we use Spring Boot. Since about a year or so we have completely standardized on Spring Boot. Up until a year ago, about 50% of our applications were still on our own homegrown, not maintained at all, Java stack based on Guice, and a bunch of very outdated Java EE libraries. We didn't really make a good push in getting everything on Spring Boot. All the new applications were based on Spring Boot already. That became very messy, especially because that old homegrown framework just wasn't maintained very well. We made a really big effort to just get all the services migrated to Spring Boot. That migration was mostly just a lot of blood, sweat, and tears of a lot of teams. It's just not easy to go from one programming model to another one. As platform teams, we did provide a lot of tooling, for example, IntelliJ plugins to take care of, where possible, the code migrations and configuration migrations and things like that. Still, it was just a lot of work. Pretty painful. Now that we are on Spring Boot, though, we have like the one framework that everyone is using that makes things a lot nicer for everyone. We are trying to mostly just use the latest version of OSS Spring Boot. We're going to be using 3.1, and try to stay as close as possible to the open source community because that's where we get the most benefit. On top of that, we need a lot of integration with our Netflix ecosystem and the infrastructure that we have. That is what we call Spring Boot Netflix, and is basically just a whole set of modules which we build on top of Spring Boot. That's basically just developed in the same way as Spring Boot itself is built, so lots of auto-configurations. That's where we add things like gRPC client and server support that's very integrated with our SSO stack, for AuthZ and AuthN. You get observability, so tracing, metrics, and distributed logging. We have a whole bunch of HTTP clients that take care of mTLS and again observability and integration with the security stack. We deploy all these applications with embedded Tomcat, which is pretty standard for a Spring Boot application.</p>

<p>To give an idea of the features, how that looks like. We have, for example, a gRPC Spring client. This looks very Spring-like, but it is something that we added. Basically, this is referencing a property file, which describes the gRPC service, it tells where the service lives. It configures failover behavior. That way, you can just use a Java API with an extra annotation to call another gRPC service. With that, you also get things like observability completely for free. For any request, either gRPC or HTTP, you get observability for free with tracing, and metrics, and all these things available. Another example is maybe integrate with Spring security, so we can get our SSO color. You get the user basically, that's called your service, even if there were many services in between in a cold chain. As I said, we integrated with Spring Security to also do role-based authentication based on our own authentication models.</p>

<h2>Why Spring Boot?</h2>

<p>You might be wondering, why are we using Spring Boot, why not some other more fancy framework? Because, of course, there's been a lot of innovation in the Java space in the last few years with other frameworks available. Spring Boot is really the most popular Java framework, that doesn't necessarily make it better, but it does give a lot of leverage when it comes to using the open source community, which is really big, of course, for Spring Boot, and accessing documentation, training, and all these things. More importantly, I think, is just looking at the Spring framework, it has been just so well maintained over the years. I think I started using the Spring framework 15 years ago. It is quite amazing, actually, that that framework has been so stable and so well-evolved, basically, over time, because it's not the same thing as it was 15 years ago, but a lot of the concepts are still there. It gives us a lot of trust, basically in the Spring team that also in the future, this will be a very good place to be basically.</p>

<h2>The Road to Spring Boot 3</h2>

<p>Almost a year ago, Spring Boot 3 came out, and that was a big deal, because Spring Boot 3 really just involves the Java ecosystem, I think, because the Java ecosystem was a little bit stuck in two different ways. The first reason is that if you look at the open source ecosystem in Java, it was stuck on Java 8, because a lot of companies were stuck on Java 8, and no one wanted to be the first one who would break that basically. Companies didn't upgrade because everything just worked fine on Java 8 anyway. Now, finally, the Spring team has said, we are done with Java 8, Java 17 is your new baseline. Now we force the whole community basically, to say, ok, fine, we'll do Java 17, and everything can start moving again. Now we can start leveraging those new language features. It also makes it possible that although it's just baseline on Java 17, we can actually also start using Java 21 with virtual threads under the hood. That's exactly what they're doing. The second part is the whole mess around Javax to Jakarta, thanks to Oracle. This is just a simple namespace change, but it is extremely complex for a library ecosystem, because a library can either use Javax or Jakarta, and that makes it either compatible with one but not the other. That's super painful now, because the Spring team is now saying, ok, if you're just doing Jakarta, now the whole ecosystem can start moving because it had such a big impact. We finally get past that point that they were stuck on. It is a big change to get on these new things still, so moving to Spring Boot 3 isn't fulfilled, and we've done a lot of tooling work to make that happen. Probably the most interesting one there is we open sourced a Gradle plugin that does bytecode transformation at artifact resolution time. When you download an artifact, a JAR file, it will do bytecode translation if you're on Spring Boot 3 from Javax to Jakarta, so it basically just fixes that whole namespace problem on the fly, and you don't have to change your library. That gets us unstuck.</p>

<h2>DGS Framework</h2>

<p>Then I talked quite a bit about DGS. DGS is not some concept, GraphQL Federation is the concept. The DGS framework is just a framework that that we use to build our GraphQL services in Java. About three or four years ago, when we started the journey on to GraphQL and GraphQL Federation, there really wasn't any good Java framework out there, that was mature enough for us to use it at our scale. There was GraphQL Java, which is a lower level GraphQL library. That library is great, and we are building on top of it. This is completely crucial for us, but it's too low level to use directly in an application, at least in my opinion. With v1 that is a GraphQL framework for Spring Boot, and basically giving a programming model based on annotation as you are used to in Spring Boot. We needed things like code generation for schema types, and support for federation and all these things. That's exactly what you're getting with the DGS framework. About, I think it's almost three years ago, we decided to open source the DGS framework. It's on GitHub. There's a really large community. There's lots of companies using it now. It's also exactly the version that we were using at Netflix, so we're not using a fork or anything like that. It's really evolved really nicely over the last few years.</p>

<p>You might be wondering if you are actually in the GraphQL and Spring space, you probably have seen that in Spring Boot 3, the Spring team also added GraphQL support, which they called Spring GraphQL. That was not ideal for the larger community, because now the community would have to choose between, ok, do I bet on the DGS framework, or do I go with Spring GraphQL? Both seem interesting, both seem great. Both have an interesting feature set, but a different feature set. What do I bet on? I could go and sell you the DGS framework, how that's better and better evolves, and faster, and all these things which are right now probably true, because we've been around for a little bit longer. That's really not the point, the point is that you shouldn't have to choose. In the last few months, we have been working with the Spring team to get full integration between those two frameworks. What you basically get with that is that you can combine the DGS and Spring GraphQL programming models and its features in the same app, and it will just happily live together. That's possible because we're both using GraphQL Java as the low-level library. That's how it all fits together. We just integrated the framework really deeply. We're still finishing that, and that is probably going to be released early 2024. At least that gives you that idea. It doesn't really matter if you would pick the DGS framework today. It doesn't get you stuck in there and not be able to leverage features coming from Spring team, because very soon you will just be able to combine both very nicely.</p>

<h2>Questions and Answers</h2>

<p>Participant 1: Are you guys still using Zuul?</p>

<p>Bakker: We are, yes. Zuul is sitting in front of literally every request. Zuul is just a proxy. It's doing a lot of traffic control, basically. It's not the API server that we talked about earlier. Zuul sits in front of either the DGS federated architecture or like the old architecture.</p>

<p>Participant 2: You talked about the upgrade for Java having a limited perceived value there. I think that's interesting. I think a lot of enterprises tend to have this mindset of if it isn't broke, don't fix it, [inaudible 00:44:02]. What did you do to change that perception, or was it just the Spring upgrade that kicked your guys about to do the upgrade?</p>

<p>Bakker: No, actually, the main story was the performance benefit. The fact that we could say that, you get 20% better performance. It depends a little bit on the service, how that number actually looks like and what it actually means. The number is real. The fact that you could say that, that made a lot of service owners more interested in it, but it also gave leadership higher up just to push like, this is going to save money, go do it. That was actually the most helpful thing. The Spring Boot upgrade came later, and also forces the issue, but it was after the fact.</p>

<p>Participant 3: A lot of advancements to OpenJDK, so from 8 to 17, did it directly go from 8 to 17?</p>

<p>Bakker: We had services running on Java 11 because the plan was 8, 11, 17. Java 11, we had services running there, it never really took off because there just wasn't enough benefit. We mostly went from 8 to 17.</p>

<p>Participant 3: Then that's one of the things depending on the collectors as he was talking about, there was some impact with respect to stop-the-world pauses and some background collections that's happening with Shenandoah and ZGC. There's a tradeoff, but a lot of improvements went into reducing the memory sets and everything like that.</p>

<p>Participant 4: You mentioned that 20% was what you needed, but how did you even secure the time to actually experiment with that? How did you convince stakeholders to say, we're going to spend some time doing an upgrade on some services, and then we'll demonstrate the values with that?</p>

<p>Bakker: There is the benefit of having a platform team as we have. If I look at my own time, I could do whatever I want. If I think there is some interesting failure to be had in experimenting with garbage collection, I'm actually not mostly doing performance work, there's actually other folks who are much better at that. It's just an example. If there is potential failure in there, if you can get a time to just experiment with it and play with it, basically, because our time of like one or two people is like drops in the water.</p>

<p>Participant 5: Did you see any difference in the memory footprint between virtual threads versus a traditional one for the same number of request-responses. The second is regarding the GraphQL versus traditional SOAP, because SOAP was superseded by REST back in the days when I was thinking that was very precious, and your network was very important if you don't have a large number of data going through easily. Now that data is cheap, so it has the disadvantage of the schema going between the client and the server. I see that GraphQL also had the same problem now that we have the other query and the schema, going between the client and the server. How do you see the REST, SOAP, and GraphQL in that conjecture?</p>

<p>Bakker: I think SOAP had, conceptually, a few things. For example, the fact that there is a schema, that was a good thing. It was so incredibly hard to use and complex, that the overhead of doing the right things was just too much. Then REST, at least the way everyone is using REST, went the other extreme like no schema, no nothing at all, nothing is defined. You just throw in some data and we're all good. I think GraphQL sits in the middle there. It doesn't have a lot of overhead for developers to implement the schema. It's very easy. It's much easier than SOAP was, just from using it. You do get a schema and that takes away a lot of the downsides of just having REST in the schema. It feels like it has found the sweet spot for APIs. Probably if I'm back here 10 years from now, I will be like, "GraphQL, a terrible idea. How did we ever get to that?" You know how that goes. Right now, it feels like a sweet spot.</p>

<p>There is a difference, that is why we have to be very careful about ending virtual threads where we replace traditional thread pools. Depending on if these thread pools are very CPU intensive or not, it does or does not make a lot of sense. The memory footprint doesn't seem to be a big factor. We haven't seen any significant bumps there at all. Again, it's all very early days, and we're just experimenting with everything. We haven't quite figured it out yet. It seems to be very straightforward from memory.</p>

<p>Participant 6: Then I was just wondering about your Kotlin usage percentage, and what that is looking like?</p>

<p>Bakker: It is fairly low. For a while we had a bunch of teams, including my own team, very excited about Kotlin. The DGS framework itself is written in Kotlin, although it's targeting mostly Java apps. That's my choice. We have microservices written in Kotlin, as well. The only downside that we see with Kotlin is we invest more in developer tooling, so IntelliJ plugins and automated tooling based on Gradle to help with these version upgrades with Spring, and all these things. That story is much harder for a platform team if you have to deal with multiple languages. Because either for an IntelliJ plugin, even if it's both from JetBrains, you need to write your inspections in IntelliJ twice if you want to use both Java and Kotlin. It's just a lot more work. It's just a lot easier for platform teams if everyone is just happily using Java. That doesn't make Kotlin bad, though. We have only seen good things about Kotlin and it works just pretty well. It's a great language.</p>




<p><big><strong>See more <a href="https://www.infoq.com/transcripts/presentations/">presentations with transcripts</a></strong></big></p>



                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Airfoil (730 pts)]]></title>
            <link>https://ciechanow.ski/airfoil/</link>
            <guid>39526057</guid>
            <pubDate>Tue, 27 Feb 2024 16:32:49 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://ciechanow.ski/airfoil/">https://ciechanow.ski/airfoil/</a>, See on <a href="https://news.ycombinator.com/item?id=39526057">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>The dream of soaring in the sky like a bird has captivated the human mind for <a href="https://en.wikipedia.org/wiki/Icarus">ages</a>. Although many failed, some eventually <a href="https://en.wikipedia.org/wiki/Wright_Flyer">succeeded</a> in achieving that goal. These days we take air transportation for granted, but the physics of flight can still be puzzling.</p>
<p>In this article we’ll investigate what makes airplanes fly by looking at the forces generated by the flow of air around the aircraft’s wings. More specifically, we’ll focus on the cross section of those wings to reveal the shape of an <a href="https://en.wikipedia.org/wiki/Airfoil"><em>airfoil</em></a> – you can see it presented in <span><strong>yellow</strong></span> below:</p>


<p>We’ll find out how the shape and the <span><strong>orientation</strong></span> of the <span><strong>airfoil</strong></span> helps airplanes remain airborne. We’ll also learn about the behavior and properties of air and other flowing matter. In the demonstration below, you can see a fluid flowing around a <span><strong>gray cube</strong></span>. Using the slider to change just one <span><strong>property</strong></span> of this substance, we can end up with vastly different effects on the liveliness of that flow:</p>


<p>Over the course of this blog post we’ll build some intuitions for why these different effects happen to airfoils and other objects placed in flowing air. We’ll start this journey by looking at some of the methods we can use to visualize the motion of the air.</p>
<h2 id="visualizing-flow">Visualizing Flow<a href="https://ciechanow.ski/airfoil/#visualizing-flow" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>If you’ve ever been outside in a grassy area on a windy fall day, you may have witnessed something similar to the little scene seen below. The slider lets you control the <span><strong>speed of time</strong></span> to observe in detail how the <span><strong>falling leaves</strong></span> and the bending <span><strong>blades of grass</strong></span> are visibly affected by the wind sweeping through this area:</p>


<p>We intuitively understand that it’s the flowing air that pushes the vegetation around, but note that we only observe the effects that the wind has on other objects – we can’t see the motion of the air <em>itself</em>. I could show you a similarly windy scene without the <span><strong>grass</strong></span> and <span><strong>leaves</strong></span>, and I could try to convince you that there is something going on there, but that completely empty demonstration wouldn’t be very gratifying.</p>
<p>Since the air’s transparency prevents us from tracking its movement directly, we have to come up with some other ways that can help us see its motion. Thankfully, the little outdoor scene already provides us with some ideas.</p>
<p>Notice that as the wind hits a blade of grass, that blade naturally bends in the direction of the blowing gust, and the faster that gust, the stronger the bending. A&nbsp;single blade indicates the direction and speed of the flow of air in that area.</p>
<p>In the next demonstration we’re looking at the same grassy field from above. When seen from this perspective, all the <span><strong>blades</strong></span> form short lines that are locally aligned with the wind. The more leaned over a blade of grass is, the longer the line it forms. We can mimic this behavior with a collection of <span><strong>small arrows</strong></span> placed all over the area, as seen on the right side:</p>


<p>Each <span><strong>arrow</strong></span> represents the direction and the speed of the flow of air at that location – the longer the <span><strong>arrow</strong></span>, the faster the flow. In these windy conditions the flow varies from place to place and it also changes over time, which we can clearly see in the motion of the <span><strong>arrows</strong></span>.</p>
<p>Note that we have some flexibility in how the speed of wind corresponds to the length of an <span><strong>arrow</strong></span>. I adjusted the lengths of the <span><strong>arrows</strong></span> to prevent them from visually overlapping, but I also made sure to maintain their <em>relative</em> lengths – if one <span><strong>arrow</strong></span> is twice as long as the other, then the flow at that location is also twice as fast.</p>
<p>For visual clarity I’m also not packing the <span><strong>arrows</strong></span> as densely as the <span><strong>blades of grass</strong></span> are placed, but it’s important to note that <em>every</em> point in the flow has its own velocity which contributes to the complete velocity <a href="https://en.wikipedia.org/wiki/Vector_field"><em>field</em></a> present in this area. If we wanted to, we could draw a <span><strong>velocity arrow</strong></span> at any of the seemingly empty spots on the right side.</p>
<p>The arrows are convenient, but the grassy scene also has another aid for visualizing flows. Many light objects like <span><strong>leaves</strong></span>, flower petals, dust, or smoke are very easily influenced by the motion of the surrounding air. They quickly change their velocity to match the flow of the wind. We can replicate the behavior of these light objects with <span><strong>little markers</strong></span> that are pushed around by that flow. You can see them on the right side:</p>


<p>These little <span><strong>markers</strong></span> also show us the motion of the air. Each <span><strong>marker</strong></span> represents an object so small and light that it instantly picks up the speed of the surrounding airflow. We’d have a hard time seeing these miniscule specks at their actual sizes, so I’m drawing the <span><strong>markers</strong></span> as visible dots.</p>
<p>In fact, the motion of each <span><strong>marker</strong></span> is equivalent to the motion of the parcel of air right around it. If you <a href="#" onclick="grass3_f0();return false;">slow down time</a>, you’ll be able to see how each <span><strong>marker</strong></span> just moves in the direction of the <span><strong>arrows</strong></span> underneath it. I also made each <span><strong>marker</strong></span> leave a little ghost trail behind it – this lets us track the path the air, as represented by the <span><strong>marker</strong></span>, took on the way to its current position.</p>
<p>Let’s pause for a second to emphasize what the <span><strong>grass</strong></span>-like <span><strong>arrows</strong></span> and <span><strong>leaf</strong></span>-like <span><strong>markers</strong></span> represent –&nbsp;they both show the <em>velocity</em> of the flow of air, but in slightly different ways. An <span><strong>arrow</strong></span> is attached to its fixed point in space, so it represents the current direction and speed of the flow at <em>that location</em>. The whole collection of arrows lets us easily see what the entire flow is doing at the moment.</p>
<p>On the other hand, the little <span><strong>markers</strong></span> are actively following the flow, letting us see how the air is actually <em>moving</em> through space, with the ghosty trails giving us some historical overview of where this parcel of air has come from.</p>
<p>The two methods we’ve seen so far are very versatile, but sometimes we don’t care about the local direction of the flow, only its speed – in the middle of this grassy field one might get cold from a fast blowing wind regardless of the direction from which that wind is coming. This brings us the third way of visualizing flow:</p>


<p>In this method we show the <em>speed</em> of the airflow using colors of varying brightness – the faster the wind, the brighter the color. You can see the whole <strong><span>spectrum of colors</span></strong> in the scale below the plot.</p>
<p>This method shows the speed of the flow at <em>all</em> locations giving us a more fine-grained insight into the motion of air at the cost of the directional information. To help with that I’ll sometimes overlay the regular arrows on top to let us know where the flow is going as well.</p>
<p>You may have noticed that all these methods present a flat, two dimensional view of the flow. It’s based on the assumption that the wind in our little scene doesn’t change with elevation, and that it also doesn’t blow towards or away from the ground.</p>
<p>In reality, the air velocity could vary in all three dimensions, and that air could also flow upwards or downwards. Thankfully, the air flows we’ll consider in this article will be two dimensional and the simple flat drawings will suffice.</p>
<p>Before we finish this section, let me bring up visualization of a simple airflow, but this time I’ll give you some control over its <span><strong>direction</strong></span>, which you can change using the second slider. The first one once more controls the <span><strong>speed of time</strong></span>:</p>



<p>Don’t be misled by the frozen arrows, the wind is actually blowing there. Remember that the arrows represent the local velocity of the flow of air, so while the velocity doesn’t change, the <em>position</em> of each packet of does. You can see those changes by tracking the <span><strong>markers</strong></span> moving around with the flow. This demonstration represents a <a href="https://en.wikipedia.org/wiki/Fluid_dynamics#Steady_vs_unsteady_flow"><em>steady flow</em></a>, which means that its properties don’t change over time.</p>
<p>So far we’ve been exploring the notion of airflow’s velocity on a more intuitive level, with a general understanding that’s it’s “the air” moving around in some direction and at some speed. I illustrated that concept using simple <span><strong>arrows↑</strong></span>, <span><strong>markers&nbsp;•</strong></span>, and <strong><span>varying colors</span></strong>, but we’re now ready to investigate the details hiding behind those straightforward graphical representations.</p>
<p>To do that, we have to look at individual particles of air. Although I <a href="https://ciechanow.ski/sound/#air">briefly discussed</a> the particle nature of air before, this time around we’re going to take a closer look at the motion of these molecules, and what it means for airflow as a whole.</p>
<h2 id="velocity">Velocity<a href="https://ciechanow.ski/airfoil/#velocity" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>Let’s take a look at the air particles in a small, marked out volume of space seen in the demonstration below – you can drag the cube around to change the viewing angle. The slider controls the <span><strong>speed of time</strong></span>:</p>


<p>You’re witnessing the motion of over twelve thousand air particles. It may seem like a lot, but this cube is <em>extremely</em> <span>tiny</span>, its sides are only 80 nanometers long. To put this in perspective using more familiar sizes, if that cube’s side measured just <span>1 inch</span><span>1 centimeter</span>, it would contain around <span>410 quintillion, or 4.1×10<sup>20</sup></span><span>25 quintillion, or 2.5×10<sup>19</sup></span> particles.</p>
<p>The particles are zipping around in random directions, constantly entering and leaving this region. However, despite all this motion what you’re seeing here is a simulation of <em>still</em> air.</p>
<p>To understand how all this movement ends up creating still conditions, we first have to look at the velocity of each particle – I’ll visualize it with a small arrow in the direction of motion. To make things a easier to see, I’ll also <span><strong>highlight</strong></span> a few of the particles while fading out the rest of them:</p>


<p>The length of an arrow is proportional to the speed of a particle, so when you <a href="#" onclick="particles2_f0();return false;">freeze the time</a> you should be able to see how some particles are slower and some are faster. This speed variation follows a <a href="https://en.wikipedia.org/wiki/Maxwell%E2%80%93Boltzmann_distribution">certain distribution</a> that’s related to temperature – the warmer the air, the faster the motion of its particles.</p>
<p>At room temperature the average speed of a particle in air is an astonishing <span>1030&nbsp;mph</span><span>1650&nbsp;km/h</span>, which is many times higher than even <a href="https://en.wikipedia.org/wiki/Saffir%E2%80%93Simpson_scale#Category_5">the most severe hurricanes</a>. Given the size of the cube, this means that even at the <a href="#" onclick="particles2_f1();return false;">fastest speed of simulation</a> everything happens 11 <em>billions</em> time slower than in real life.</p>
<p>If you paid close attention, you may have also noticed that sometimes the particles randomly change direction and speed of their motion – this happens when molecules collide. Each particle experiences roughly ten billion collisions per second. We’ll get back to these interactions later on, but for now let’s try to figure out how all this turmoil creates still air.</p>
<p>Having just seen the small velocity arrows of individual particles, let’s calculate the <em>average</em> velocity of a group of three <span><strong>particles</strong></span>, using the process shown below. We first take the velocity arrows <span><strong>from</strong></span> <span><strong>each</strong></span> <span><strong>particle</strong></span> and place them head to toe, one after another. Then we connect the start of the <span><strong>first arrow</strong></span> with the end of the <span><strong>last arrow</strong></span> to create the <span><strong>sum</strong></span> of all velocities. Finally, we divide, or scale down, the length of this <span><strong>sum</strong></span> by the number of particles to get the <span><strong>average velocity</strong></span>:</p>

<p>In the next demonstration we’re repeating this whole procedure by tallying up all the particles inside the <span><strong>red box</strong></span>. You can change the <span><strong>size</strong></span> of that region with the second slider. The <strong>large arrow</strong> in the middle shows the average velocity of particles in the box. To make that <strong>central arrow</strong> visible, I’m making it much larger than the tiny arrows tied to particles:</p>



<p>The counter in the bottom part of the demonstration tracks the current number of particles in the <span><strong>red cube</strong></span>. That value fluctuates as the molecules enter and leave that region. While aggregating over a small number of particles creates a very <a href="#" onclick="particles3_f0();return false;">noisy readout</a>, it doesn’t take <em>that</em> many particles to get a much <a href="#" onclick="particles3_f1();return false;">steadier measure</a>.</p>
<p>Recall that the scale of the <strong>large central arrow</strong> is much larger than the scale of individual tiny arrows attached to each particle. Despite that increase in size, the <strong>arrow</strong> practically disappears when we average out a larger number of particles and we can clearly see that the average velocity of particles is more or less zero even in this extremely small volume.</p>
<p>In still conditions, all these motions in different directions average out to nothing. As some particles enter the area from a random direction, the others also leave it in a random way. The <em>bulk</em> of air doesn’t really go anywhere and the particles just meander in a random fashion.</p>
<p>An imperfect, but convenient analogy is to imagine a swarm of bees flying in the air. While all the individual insects are actively roaming around at different speeds, the group as a whole may steadily stay in one place.</p>
<p>All these experiments form the key to understanding what happens when wind sweeps through an area. In the demonstration below, we’re once again watching a small volume of space, but this time you can control the <span><strong>speed</strong></span> of the blowing wind:</p>



<p>Notice the <span>mph</span><span>km/h</span> speedometer in the bottom of the demonstration. This is not a mistake –&nbsp;even with <a href="#" onclick="particles4_f0();return false;">hurricane-level wind speeds</a> it’s very hard to see any difference in the motion of the particles. Perhaps you’ve managed to see the tiniest shifts in the small particle arrows as you drag the <span><strong>second slider</strong></span> around with <a href="#" onclick="particles4_f1();return false;">time paused</a>, but it’s difficult to even perceive from which direction the wind is blowing.</p>
<p>However, when we use the procedure of averaging the velocity of all the particles, we can reveal the motion of their group in the box of a given <span><strong>size</strong></span>, at a specific <span><strong>speed</strong></span> of the flow:</p>




<p>Because the motion of each individual particle is so disordered, we have to look at many of them at once to discern any universal characteristics. And when we do <a href="#" onclick="particles5_f0();return false;">just that</a>, from all the chaos emerges order.</p>
<p>It’s important to note that with this approach we’re tracking the velocity of the flow within the same region of space outlined by the <span><strong>red box</strong></span> – the molecules keep entering and leaving this area as the flow moves and the <strong>arrow</strong> in the middle shows the average velocity of the air’s particles in that <span><strong>area</strong></span>.</p>
<p>This is <em>exactly</em> what the grass-like arrows we’ve played with in the previous section represent – each one shows the average velocity of air particles in that local region of space. The <strong>big arrow</strong> we just saw in the middle of the swarm in the <span><strong>averaging red box</strong></span> is equivalent to each of the <span><strong>arrows</strong></span> seen below:</p>


<p>Naturally, the <span><strong>averaging box</strong></span> needs to be large enough to avoid the jitteriness related to aggregation of too few particles, but at any scale that we could care about the noisy readout completely disappears.</p>
<p>The <em>average</em> motion of particles is very different than the motion of each individual molecule. Even in very fast flows, many of the molecules move in the opposite direction than what the arrow indicates, but if we tally up all the particle motion, the air as a whole does make forward progress in the direction of velocity.</p>
<p>Up to this point, we’ve mostly looked at the flow of air by looking at wind and the way it moves through space, but what we consider a motion of air is relative. Let’s see how, by merely changing the point of view, we can create a motion of air in otherwise windless conditions.</p>
<h2 id="relative-velocity">Relative Velocity<a href="https://ciechanow.ski/airfoil/#relative-velocity" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>Let’s zoom away from the world of microscopic particles to look at the motion of larger bodies. In the demonstration below, you can see two different views of the same <span><strong>car</strong></span> driving in the left direction. In the top part, the camera stays firmly on the ground, but in the bottom part, the camera tracks the motion of the <span><strong>vehicle</strong></span>. If needed, you can restart the scene with the button in the bottom left corner or tweak the <span><strong>speed of time</strong></span> with the slider:</p>


<p>These two views show the exact same scene – we’re just changing what the camera is focusing on. As seen in the top part, from the perspective of the static camera, it’s only the <span><strong>car</strong></span> that has some velocity in the left direction.</p>
<p>On the other hand, from the perspective of the camera focused on the <span><strong>vehicle</strong></span>, the <span><strong>car</strong></span> doesn’t move, but <em>everything else</em> does. The poles and road markings all move to the right with a speed equal to that of the <span><strong>car</strong></span>. This shouldn’t come as a surprise from daily experience in any form of transportation – when you’re sitting in a moving vehicle, static things in the surrounding environment seem to move towards and past you.</p>
<p>The very same rules apply to any region of air – I’ve outlined some of them with dashed boxes <span></span>  up in the sky. For the observer on the ground that air is still, but from the <span><strong>car’s</strong></span> perspective, that air is moving.</p>
<p>With that in mind, let’s see the same scene, but this time I’ll add the familiar small arrows showing the air’s velocity as “seen” by the camera:</p>


<p>From the point of view of the <span><strong>car</strong></span>, as seen in the bottom view, the air is moving to the right, as if there was some wind blowing right at the <span><strong>vehicle</strong></span>. You’ve probably felt this many times by sticking your hand out the window – it feels no different than if you were standing still on the ground with the wind hitting your fingers.</p>
<p>In fact, there is absolutely no difference between “regular” wind and wind experienced by the <span><strong>car</strong></span>  or your hand sticking out the window – both are simply a motion of air relative to some object. This means that we can use our arrows to represent any motion of air, as long as we note what that motion is relative to.</p>
<p>You may have also noticed that the moving <span><strong>car</strong></span> affects the motion of air in its vicinity. Let me bring up the previous demonstration one more time:</p>


<p>In the top view, we can see how the front of the <span><strong>vehicle</strong></span> pushes the air forward, and how the air “bends” and speeds up around the shape of the <span><strong>car</strong></span> to roughly follow its shape, only to end up circling right behind the <span><strong>machine</strong></span>.</p>
<p>The same effects are seen in the bottom view – they’re just experienced differently. For example, the air right in front of the <span><strong>car</strong></span> slows down, while the air on top moves even faster than the rest of the undisturbed, distant air.</p>
<p>We’ll soon explore <em>why</em> the air behaves this way when flowing around an object, but for now let’s raise above the ground to see the motion of an <span><strong>airplane</strong></span> flying in the sky. We’ll use the familiar setup of a camera kept steady relative the ground, as seen in the top part, and a camera that follows the <span><strong>airplane</strong></span>, seen in the bottom part:</p>


<p>Before we continue, notice that it’s getting a little hard to pay close attention to what happens to the moving objects in the ground-fixed camera view – the bodies quickly leave the field of view of the demonstrations. For the rest of this article I’ll stick to the camera style seen in the bottom part of the demonstration – this will let us directly track the interaction between the object and the air that flows around that object.</p>
<p>From the point of view of the <span><strong>airplane</strong></span>, it also experiences a flow of incoming air as seen by the air “boxes” approaching the <span><strong>plane</strong></span>, which is very similar to the car example. What’s completely different from the car example is the fact that the <span><strong>airplane</strong></span> somehow stays suspended in the air, despite gravity pulling it down towards the ground. This means that there must be some other force acting on it to prevent the plane from falling from the sky.</p>
<p>Let’s compare these two vehicles by looking at the basic forces affecting their motion, starting with the diagram of forces acting on the <span><strong>car</strong></span>:</p>


<p>The down-pulling <span><strong>gravity force</strong></span> is counteracted by the <span><strong>reaction forces</strong></span> from the ground – they act through the <span><strong>car’s</strong></span> tires to prevent the <span><strong>car</strong></span> from sinking. The air drag and other forms of resistance <span><strong>push</strong></span> the car back, but the <span><strong>car’s</strong></span> tires powered by the engine keep <span><strong>propelling</strong></span> the car forward.</p>
<p>In my previous article I presented a <a href="https://ciechanow.ski/bicycle/#forces">more elaborate description</a> of the interplay between forces and objects, but to briefly recap here, if forces acting on an object are balanced, then that object will maintain its current velocity.</p>
<p>All forces on the <span><strong>car</strong></span> <em>are</em> balanced and the <span><strong>vehicle</strong></span> moves forward with constant speed, and it doesn’t move at all in the up or down direction – the <span><strong>object’s</strong></span> velocity is indeed constant.</p>
<p>Let’s draw a similar diagram of forces for the flying <span><strong>plane</strong></span>:</p>


<p>We still have the <span><strong>air drag</strong></span> that pushes the <span><strong>vehicle</strong></span> back, and the <span><strong>plane’s</strong></span> propeller powered by the engine keeps  <span><strong>pushing</strong></span> it forward. As a result the <span><strong>plane</strong></span> moves forward with constant speed.</p>
<p>We also have the down-pulling <span><strong>gravity</strong></span>. This time, however, that <span><strong>gravity</strong></span> is not countered by the reaction forces from the ground, but instead it’s balanced by <a href="https://en.wikipedia.org/wiki/Lift_(force)"><em>lift</em></a>, a <span><strong>force</strong></span> that pushes the <span><strong>plane</strong></span> up. When <span><strong>gravity</strong></span> and <span><strong>lift</strong></span> are equalized, the plane doesn’t move up or down either.</p>
<p>Airplanes create most of their lift with wings, which are carefully designed to generate that force. While length, area, and the overall geometry of the wings are very important, in this article we’ll focus on the shape of the cross-section of a wing which I highlighted below in <span><strong>yellow</strong></span>:</p>

<p>This is an <span><strong>airfoil</strong></span>, the protagonist of this article. This <span><strong>airfoil</strong></span> has a smooth, rounded front and a sharp trailing edge. Let’s take a closer look at the flow of air around this <span><strong>airfoil</strong></span> using the grass-like arrows that show the velocity of air at that location:</p>


<p>These arrows paint an interesting picture, but in the demonstration below I’ve also added the little leaf-like <strong>markers</strong> that track the motion of air parcels in the flow. I&nbsp;steadily release a whole line of them from the left side, but you can also <span>click</span><span>tap</span> anywhere in the flow to drop a <strong>marker</strong> at that location. You can do this in any demonstration that has a little hand symbol in the bottom right corner:</p>

<p>The <strong>markers</strong> show that the flow splits ahead of the <span><strong>airfoil</strong></span>, then it gently changes direction to glide above and below the <span><strong>shape</strong></span>. Moreover, the <strong>markers</strong> right in front of the <span><strong>airfoil</strong></span> gradually slow down and lag behind their neighbors. The air somehow senses the presence of the body.</p>
<p>It may be hard to see, but the top and bottom sections of this <span><strong>airfoil</strong></span> aren’t symmetric. This asymmetric design is very important, but right now it will needlessly complicate our discussion on how the flow around this shape arises.</p>
<p>To simplify things a little, let’s use a less complicated shape of a <em>symmetric</em> <span><strong>airfoil</strong></span> – you can see it in the demonstration below. I overlay the previous asymmetric shape with a <span>dashed</span> outline to show the difference between the two:</p>

<p>The motion of air around this airfoil is very similar – the flow changes its direction and speed when it passes around an object. Until now we’ve simply been observing that the flow changes to adapt to the shape of the body, but it’s finally time to understand <em>why</em> it happens. To explain that behavior we need to go back to the world of air particles to discuss the concept of pressure.</p>
<h2 id="pressure">Pressure<a href="https://ciechanow.ski/airfoil/#pressure" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>As we’ve discussed, even in the seemingly steady conditions the particles of air are zipping around at high speeds colliding with each other at an incredible rate. The surface of any object placed in the air will also experience these bounces.</p>
<p>In the demonstration below, you can see air particles bombarding a <span><strong>small box</strong></span>. Every time a collision happens I briefly mark it with a <strong>dark spot</strong> on the surface of that <span><strong>cube</strong></span>:</p>


<p>To understand the implications of these collisions, let’s first take a look at objects with more ordinary sizes. In the demonstration below, <span><strong>tennis balls</strong></span> are hitting a large <span><strong>cardboard box</strong></span> from the left and right side. By dragging the slider you can change the <span><strong>intensity</strong></span> of both streams of balls:</p>


<p>When a <span><strong>tennis ball</strong></span> hits the <span><strong>box</strong></span>, the collision imparts some force on it, causing the <span><strong>box</strong></span> to move. However, in this simulation the collisions from all the <span><strong>balls</strong></span> on each side balance each other out, so the <span><strong>box</strong></span> doesn’t make any consistent progress in either direction.</p>
<p>In real air, the situation is similar, but at vastly different scales. The mass of each particle constituting air is absolutely miniscule, so the impact of an individual collision on any object of meaningful size is completely imperceptible.</p>
<p>Moreover, each air particle hitting an object has a different speed, and it strikes the surface of that object at a different angle – some hit the object straight on, but some barely graze it. Due to the enormous number of these collisions happening at every instant of time, all these variations average out, and even a small section of surface of any body experiences uniform bombardment.</p>
<p>In aggregate, we say that the air exerts <a href="https://en.wikipedia.org/wiki/Pressure"><em>pressure</em></a> on any object present in that air. The magnitude of this pressure depends on the intensity of these collisions across an area.</p>
<p>Let’s see how this pressure manifests on our <span><strong>tiny cube</strong></span>. In the demonstration below, you can use the second slider to control the <span><strong>number of air molecules</strong></span> present in this volume:</p>



<p>The <span><strong>black arrows</strong></span> you see on the sides of the cube symbolize the magnitude of pressure on these walls. As we <em>uniformly</em> increase the <span><strong>number of particles</strong></span> in this volume, the intensity of collisions, and thus the <span><strong>pressure</strong></span>, also increases. Because the collisions happen at more or less the same rate on every side of the <span><strong>box</strong></span>, the net balance of forces is also maintained and the <span><strong>cube</strong></span> doesn’t move, regardless of how big or small the overall <span><strong>pressure</strong></span> is.</p>
<p>This is exactly what happens in the Earth’s atmosphere – everything is constantly squeezed by relatively high pressure caused by the barrage of countless air particles. That pressure is typically balanced either by an object’s material, which resists compression like a spring, or by the air itself that fills the insides of the object. When that inner air is removed, the seemingly innocuous atmospheric pressure <a href="https://www.youtube.com/watch?v=0N17tEW_WEU&amp;t=165s">reveals its might</a>.</p>
<p>The underlying particle nature also shows us that pressure is never negative. Without any particle collisions, we reach the lowest possible pressure of zero. Beyond that, any impacts on the surface of an object create some amount of positive pressure.</p>
<p>In the demonstrations we’ve seen so far, the <em>balanced</em> number of collisions on each wall was very important for keeping the objects steady. Unsurprisingly, more interesting things happen when this harmony isn’t maintained. Let’s first investigate this scenario using the <span><strong>tennis balls</strong></span>. In the demonstration below, the slider controls if it’s the <span><strong>left side or the right side</strong></span> that’s shooting more balls:</p>


<p>As you can see, if one of the sides has a <a href="#" onclick="balls2_f0();return false;">higher number of collisions</a>, the forces acting on the <span><strong>box</strong></span> are no longer balanced and the <span><strong>box</strong></span> starts to move.</p>
<p>The very same situation happens in air, which you can witness in the simulation below. Notice that the volume in which the <span><strong>tiny cube</strong></span> exists has more particles on one side than the other. Observe what happens to <span><strong>cube</strong></span> once you let <span><strong>the time run</strong></span> using the slider:</p>


<p>The higher number of particle collisions on one side of the <span><strong>cube</strong></span> creates higher <span><strong>pressure forces</strong></span> on that wall. The uneven forces end up pushing the <span><strong>block</strong></span> to the side. In this demonstration, the pressure re-balances after a while and the <span><strong>cube</strong></span> stops moving.</p>
<p>Intuitively, the air exerts an imbalanced net force on the <span><strong>cube</strong></span> only when different parts of that <span><strong>object</strong></span> experience different pressure – it’s the <em>spatial variation</em> in pressure that creates an acting net force. When the <em>difference</em> in pressure between any two points increases, the net force acting on the object also grows.</p>
<p>It’s easy to see that a larger number of collisions on the left side of an object would start to exert a net force pushing that object to the right, but, perhaps surprisingly, the same rules apply to any chunk of air itself.</p>
<p>In the demonstration below, I once again made one half of the test volume contain more particles than the other half. As you <span><strong>unpause</strong></span> the demonstration, observe the average velocity of molecules in the  <span><strong>marked out</strong></span> section of air:</p>



<p>The particles on the more occupied side can easily travel to the less crowded side, because there are fewer particles there to collide with and bounce back from. Additionally, each particle in the less populated section is more likely to hit a particle in the more populated section, which will typically cause that particle from the desolate side to bounce back where it came from.</p>
<p>The particles end up, on average, traveling from the area of high pressure to the area of lower pressure. Even though we don’t have any clean borders between different sections, we can still see the bulk of particles getting accelerated towards the less dense section.</p>
<p>Once again, the initial pressure differences in the test volume dissipate after a while. On their own, these freely suspended pressure variations quickly disappear, but we will soon see how, with the aid of airflow, these areas of different pressure can be sustained indefinitely.</p>
<p>In the examples we’ve been playing with, the notion of increased pressure came from an increased number of collisions, which in turn came from an increased number of particles in the area. This shows that, all other things being equal, pressure is tied to the local density of the air, which was very easy to perceive in an increased concentration of molecules.</p>
<p>However, the pressure can also grow due to increased average speed of the particles, which in turn comes with increased temperature. As particles get faster, each collision gets more impactful and it pushes on an object or other particles a bit harder, causing the overall pressure to also increase. In the demonstration below, we can simulate this with <span><strong>tennis balls</strong></span> hitting the <span><strong>cardboard box</strong></span> at the same rate, but with <span><strong>different speeds</strong></span>, which you can control with the slider:</p>


<p>As we make the <span><strong>balls</strong></span> on one side of the <span><strong>box</strong></span> <a href="#" onclick="balls3_f0();return false;">faster</a>, their impacts also become stronger and the <span><strong>package</strong></span> starts moving to the right, even though the <em>number</em> of collisions per second is equal on both sides.</p>
<p>The important point from these discussions is that air pressure exerts force on everything inside it, be it a solid object or any parcel of air. It’s a little unintuitive that the air itself both exerts the pressure and it also “feels” the pressure, but it’s all just a consequence of very rapid motions of particles and the collisions between them happening at an enormous rate.</p>
<p>Recall that even in small volumes of air there are billions of billions of particles, and each particle experiences roughly ten billion collisions per second. What we’ve simulated at a micro scale and in slow motion as countable, individual interactions, very quickly smooths out into a uniform and uninterrupted notion of force-exerting pressure.</p>
<p>This fact lets us abandon the molecules and their collisions yet again. It’s not a big loss, since counting the number and intensity of collisions was never convenient in the first place, but we can now investigate some other ways of visualizing pressure in a region of air.</p>
<h2 id="visualizing-pressure">Visualizing Pressure<a href="https://ciechanow.ski/airfoil/#visualizing-pressure" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>As we’ve seen in the particle simulations, pressure can vary from place to place. One of the most convenient ways to express this variation is to use colors of different intensities. Let’s see how that simple approach could work here. In the demonstration below, the dashed circles represent regions of high <span></span> and low <span></span> pressure – you can drag them around to change their position:</p>

<p>This map of pressure is colored with <strong><span data-color0="E3BFBB" data-color1="E5432E">varying shades of red</span></strong> as indicated by the scale below – the <span><strong>redder</strong></span> the color, the <span><strong>higher</strong></span> the pressure. The small triangle <span>▼</span> in the middle of the scale indicates the location of the  base, <em>static</em> pressure present in the atmosphere.</p>
<p>In this simulation we have complete control over where the different locations of <span><strong>lower</strong></span>  and <span><strong>higher</strong></span> pressure are. To make things more interesting, each draggable pressure circle has a different strength and range. You can infer this variation from color changes around these points.</p>
<p>Let’s put an <span><strong>airfoil</strong></span> in this area to see how it’s affected by the pressure of the surrounding air. The <span><strong>arrows</strong></span> seen below symbolize the force that pressure exerts on the surface of the <span><strong>airfoil</strong></span> at that location. They’re the exact same <span><strong>arrows</strong></span> that we’ve seen acting on the walls of the tiny yellow cube, here we just see them at a larger scale:</p>

<p>As you move around the locations of <span><strong>lower</strong></span>  and <span><strong>higher</strong></span> pressure, the <span><strong>forces</strong></span> acting on the surface of the <span><strong>airfoil</strong></span> also change, matching what we’ve seen with little cubes bombarded by air particles. The static pressure always exerts some base load, but in the areas of <span><strong>higher</strong></span> pressure the <span><strong>surface forces</strong></span> are higher, and in the areas of <span><strong>lower</strong></span> pressure the <span><strong>surface forces</strong></span> are lower than these base forces.</p>
<p>Note that you can also move the pressure circles into the <span><strong>airfoil</strong></span>, but it only serves as a convenience to let you customize the shape of the air pressure field <em>around</em> that body – we don’t particularly care about the pressure inside the solid itself.</p>
<p>When we tally up all the <span><strong>pressure forces</strong></span> acting on each piece of the <span><strong>airfoil’s</strong></span> surface, we end up with the <span><strong>net force</strong></span> acting on that object. In the demonstration below, I’m showing it with the <span><strong>big arrow</strong></span> at the center of the <span><strong>airfoil</strong></span>:</p>

<p>By changing the distribution of pressure around the <span><strong>airfoil</strong></span>, we can affect the <span><strong>total force</strong></span> that this object feels.</p>
<p>The reddish plots we’ve been looking at are correct, but a little inconvenient. Recall that <span><strong>final net force</strong></span> on the object depends only on the <em>differences</em> of pressure – when we uniformly increased the number of collisions on the walls of the tiny cube, it steadily remained in place.</p>
<p>This means that the static background pressure doesn’t matter for the <span><strong>cumulative forces</strong></span> acting on an object. It’s only the differences relative to that static pressure that affect the overall balance. This lets us overhaul our visual representation of pressure – we can use no color where the pressure has the static value, use <span><strong>blue color</strong></span> when the pressure is <span><strong>lower</strong></span> than the static pressure, and use <span><strong>red color</strong></span> when the pressure is <span><strong>higher</strong></span> than the static pressure:</p>

<p>This is the <em>exact</em> same distribution of pressure that we’ve just seen. All the pressure demos in this section are connected, and here we simply changed the reference point against which we present the pressure variation.</p>
<p>If we then throw in the <span><strong>airfoil</strong></span> back into the mix we can now also adjust the <span><strong>arrows</strong></span> representing the <span><strong>forces</strong></span> that the pressure exerts on the surface of that <span><strong>object</strong></span>:</p>

<p>The areas of <span><strong>higher</strong></span> pressure still seem to push on the surface of the <span><strong>airfoil</strong></span>, but the areas of <span><strong>lower</strong></span> pressure now seem to <em>pull</em> it. However, I need to emphasize once more that pressure <em>always</em> pushes on the object, and we can only talk about a pulling force when we discard that uniform, pushing contribution coming from the static pressure. In those “pulling” areas the pressure is still pushing, it just pushes less intensely.</p>
<p>I will also use the convenient terms of <span><strong>positive</strong></span> and <span><strong>negative</strong></span> pressure, but remember that this refers to their difference from the static pressure. The phrase “pressure lower than static pressure” is a mouthful, so the expression “negative pressure” is very handy, even when it hides the fact that pressure is always positive.</p>
<p>While the color variations used here show the true nature of the smoothly varying pressure changes, they make it a little hard to see how quickly those changes happen. To fix this, I’ll also draw the <a href="https://en.wikipedia.org/wiki/Contour_line"><em>contour lines</em></a> that join the locations of the same pressure – they’re very similar to lines of the same altitude you may have seen on maps:</p>

<p>Every point on one of those contour lines has the same value of pressure, and each subsequent line is drawn at the same increment of pressure – you can see this in the scale placed below the plot. This means that the closer the lines are together, the more quickly the pressure changes in that area.</p>
<p>The mathematical concept that describes the direction and rapidness of these changes is known as a <a href="https://en.wikipedia.org/wiki/Gradient"><em>gradient</em></a>. Informally, gradient describes how some property changes from one point to another, and, thankfully, this notion tracks closely with how this word is used in graphic design to describe smooth color changes. Wherever you see a <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E"> color gradient</span></strong> , this also implies that there is a <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">pressure gradient</span></strong>  – the pressure changes from place to place.</p>
<p>This spatial variation is particularly important for the motion of air. Recall that the air pressure differences don’t just exert forces on solid objects, but also on the air itself – any small parcel of air is subject to the same whims of pressure forces.</p>
<p>Those spatial variations in pressure end up pushing the air around, changing its velocity. Let’s see this in action using the little leaf-like <strong>markers</strong> that are moved around by pressure differences. In the demonstration below, I’m steadily releasing the <strong>markers</strong> from the left side –&nbsp;notice how their trajectory changes when you modify the pressure field:</p>


<p>You may still find it a little difficult to grasp how pressure differences affect the motion of a parcel of air. Luckily, we can draw parallels between the contour lines of pressure seen on these pressure maps and the contour lines of elevation seen on traditional maps. This lets us build a little pressure-landscape analogy.</p>
<p>In the demonstration below, the very same distribution of pressure is expressed as a mountainy landscape. <span><strong>Positive</strong></span> pressure lifts the ground above the base level and <span><strong>negative</strong></span> pressure depresses it below the base level. A parcel of air moves like a marble that loses speed when climbing uphill and accelerates when rolling downhill. You can drag the demo around to change the viewing angle:</p>


<p>Notice that when the pressure changes more rapidly and the contour lines are closer, the steepness of the corresponding hill or valley also increases, and so do the forces acting on a parcel of air. If the pressure is increasing by a large amount, it may even make the <strong>marker</strong> go back. This landscape analogy also shows that the static pressure doesn’t matter for the motion of air parcels, as any changes in static pressure would just lift all the areas by the same amount without changing their steepness.</p>
<p>When watching these air parcels move around, you may have noticed that things were a little bit off. For example, it’s possible for air parcels coming from different directions to arrive at the same location, and then continue to travel in different directions. You can see an example of that on the left side of the demonstration below, with the slider letting you <span><strong>scrub back and forth in time</strong></span>:</p>


<p>Recall that the markers always follow the local velocity of air, so the motion seen in the left part implies that the air at the <a href="#" onclick="parcels_crossing_f0();return false;">location of the meetup</a> of the <span><strong>two</strong></span> <span><strong>markers</strong></span>  has two different velocities at the same time, which is not realistic.</p>
<p>It’s worth pointing out that the situation seen on right side, where <span><strong>one marker</strong></span> merely <a href="#" onclick="parcels_crossing_f1();return false;">intersects</a> the <strong><span data-color0="F0D9C0" data-color1="E09F56">historical path</span></strong> of the <span><strong>other</strong></span>, <em>can</em> be realistic, as long as we’re dealing with an unsteady flow, where the <span><strong>velocity</strong></span> of the air at the crossing location has changed since the <span><strong>first marker</strong></span> was there. For steady conditions in which no changes occur over time, the scenario seen on the right is also not physically correct.</p>
<p>We’ll look at some unsteady flows later in the article, but for now we’re interested in steady conditions so the crossing paths of our <strong>markers</strong> indicate implausible velocities. Even more dubious result happen when we simulate the motion of these <strong>markers</strong> with an <span><strong>airfoil</strong></span> present in the flow:</p>


<p>For most distributions of pressure, the air markers will flow right through the <span><strong>body</strong></span>. This is clearly wrong! The demonstrations we’ve seen so far correctly represent what would happen to individual air parcels and bodies placed in these pressure fields, but those pressure fields themselves were completely made up and didn’t correspond to any physical reality. Our mistake was that we completely ignored any interactions between the pressure of the air and the <em>motion</em> of that air.</p>
<p>The flow of air, the pressure of air, and the shape of the objects placed in that air are all tied together – for a given incoming flow speed and the shape of the object, we can’t just arbitrarily arrange the pressure field like we did in our artificial demonstrations. Instead, that pressure field will arise on its own.</p>
<p>Let’s see a <em>real</em> distribution of pressure around this airfoil and witness how it affects the motion of air parcels around it:</p>

<p>The behavior of air parcels now matches our intuitive expectations – the <strong>markers</strong> don’t go through the body, and in these steady conditions they also don’t cross paths.</p>
<p>We’re now one step closer to understanding how the flow of air takes its shape to move around an <span><strong>airfoil</strong></span> – it’s the pressure differences that cause the flow to change its direction and speed.</p>
<p>The pressure field we’ve just seen clearly works – regions of <span><strong>lower</strong></span> and <span><strong>higher</strong></span> pressure guide the air around the airfoil. However, it’s still unclear how these areas emerged in the first place. Let’s try to follow nature’s path to see how this pressure distribution is created and sustained in a flow.</p>
<h2 id="airfoil-flow">Airfoil Flow<a href="https://ciechanow.ski/airfoil/#airfoil-flow" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>Before we start building the correct pressure field from scratch, let’s first establish two guiding principles that the flow around any object has to follow.</p>
<p>Firstly, the air can’t penetrate solid walls. A valid pressure field should either completely stop the flow at the surface of the object, or redirect that flow to make it travel in the direction perpendicular to the walls. This means that the markers that we track can never get inside the object.</p>
<p>Secondly, we also have the restrictions on the relative motion of the markers. For now we’ll only be interested in steady conditions, which means that the markers can’t cross their paths – we expect the ghostly historical trails to never intersect.</p>
<p>Let’s first focus on the pressure field in front of the airfoil. In the demonstration below, I created an <em>artificial</em> pressure field in that frontal region, you can control it using the slider:</p>


<p>It should quickly become clear that to prevent the approaching air from getting into the object, the pressure in the frontal region has to be <span><strong>positive</strong></span>, so that it pushes the incoming air away.</p>
<p>If that <span><strong>positive pressure</strong></span> in front is <a href="#" onclick="symmetric_airfoil_fvm4_f0();return false;">too low</a> the air can still erroneously flow through the object. If that pressure is <a href="#" onclick="symmetric_airfoil_fvm4_f1();return false;">too high</a>, the air parcels arriving at the airfoil will turn back and incorrectly cross paths with the incoming air. When the pressure is <a href="#" onclick="symmetric_airfoil_fvm4_f2();return false;">just right</a>, the air parcels don’t go through the wall, and, at least in front of the object, they also don’t cross their paths.</p>
<p>The faster the incoming flow, the higher the pushing force required to slow down and redirect the incoming air. In the demonstration below, you can also control the <span><strong>speed</strong></span> of that incoming air using the second slider:</p>



<p>While for <a href="#" onclick="symmetric_airfoil_fvm5_f0();return false;">slow flows</a>, only a small amount of <span><strong>positive pressure</strong></span>  is <a href="#" onclick="symmetric_airfoil_fvm5_f1();return false;">enough</a> to stop the incoming air, for <a href="#" onclick="symmetric_airfoil_fvm5_f2();return false;">fast flows</a>, the pressure in front of the airfoil has to become <a href="#" onclick="symmetric_airfoil_fvm5_f3();return false;">much higher</a>.</p>
<p>The pressure needed to stop air at a given velocity is known as <a href="https://en.wikipedia.org/wiki/Stagnation_pressure"><em>stagnation pressure</em></a> and it’s proportional to the <em>square</em> of that velocity – twice as high speed requires four times larger pressure.  Naturally, when there is <a href="#" onclick="symmetric_airfoil_fvm5_f4();return false;">no flow</a>, no pressure is required as the air no longer tries to flow through the object.</p>
<p>In the previous two demonstrations, we manually adjusted the pressure to get the correct result, but in nature this process happens on its own – it’s the flow <em>itself</em> that creates this region of increased pressure in front of the object.</p>
<p>As the incoming parcels of air arrive at the surface of the airfoil, they can’t continue going forward, but air parcels from further up ahead continuously want to keep flowing into this region. This compresses the air close to the object, which causes the pressure in front to increase, which then helps to slow down the incoming flow.</p>
<p>This mechanism is self-balancing – if the pressure is too low to push away the incoming air parcels, the air parcels will compact the existing air more, causing an increase in pressure. If the pressure is too high, it will easily push the incoming air away, which relieves the frontal area, causing the pressure to decrease. Any fluctuations quickly settle to an equilibrium that balances the pressure in the entire frontal region.</p>
<p>Let’s look at the distribution of the <span><strong>positive</strong></span> frontal pressure once more:</p>

<p>Notice that the <span><strong>positive pressure</strong></span> isn’t limited to just the close vicinity of the airfoil, but it spreads out much further ahead to gradually reach the value of the static pressure, far away from the airfoil itself.</p>
<p>All in all, we have a large area of increasing pressure that starts far away from the body and ends at its surface. Those pressure differences create a pressure “hill” that not only gradually slows the incoming air down, but it also redirects that air to flow around the object.</p>
<p>It seems that with our frontal pressure field we’ve easily completed our goal of preventing the air from flowing through the walls of the body. However, our second guideline of non-crossing marker paths is still not fulfilled – this condition is broken above and below the airfoil.</p>
<p>Let’s first try to rectify this manually. In the demonstration below, you can control the pressure in these two regions using the slider:</p>


<p>While <span><strong>positive</strong></span> values of pressure in those zones make the <a href="#" onclick="symmetric_airfoil_fvm6_f0();return false;">problem worse</a>, <span><strong>negative</strong></span> values get us <a href="#" onclick="symmetric_airfoil_fvm6_f1();return false;">much closer</a> to the expected behavior – in the top and bottom areas the <strong>markers</strong> no longer veer off into different directions. However, that pressure can’t bo <a href="#" onclick="symmetric_airfoil_fvm6_f2();return false;"><em>too</em> low</a>, otherwise it will pull the <strong>markers</strong> back into the body.</p>
<p>In real flow, these regions of <span><strong>lower</strong></span> pressure arise on their own, but the explanation for this phenomenon is a little less straightforward than what I’ve described for the area of <span><strong>positive</strong></span> pressure in the frontal region. We can get <em>some</em>, albeit a bit hand-wavy,  understanding by observing what happens to the air <strong>markers</strong> when those <span><strong>negative</strong></span> regions are <a href="#" onclick="symmetric_airfoil_fvm6_f3();return false;">missing</a>.</p>
<p>In that scenario, the incoming air parcels no longer reach those areas above and below the airfoil, causing some local depletion of air that has since left those zones. This decreases the pressure in those regions, and that lower pressure attracts the surrounding air to flow into those less occupied spaces.</p>
<p>If that lower pressure is <em>too</em> negative, more air will come in and the pressure will rise. If the pressure is not negative enough, those region will get depleted again. Once again, it’s the flow itself that creates the balancing system – without the flow no pressure differences would arise.</p>
<p>As we’ll see later on, in more extreme scenarios that negative pressure can alter the flow more dramatically, and the regions of “missing” air get filled through other means, but for now let’s close things up by tweaking the pressure in the rear part of the airfoil:</p>


<p>Some amount of <span><strong>positive</strong></span> pressure in the rear <a href="#" onclick="symmetric_airfoil_fvm7_f0();return false;">prevents</a> the air parcels from smashing into each other after leaving the airfoil. Intuitively, this pressure arises naturally from the flow, because as the air slides off from the ends of the top and bottom sides, it all arrives into the same region, creating some compression.</p>
<p>If that compressive pressure in the rear is too low, more air will manage to get in, which will further increase the pressure. If that pressure is too high, it will push the incoming air away, which depletes the area and the pressure decreases. The system balances itself yet again.</p>
<p>The quite informal description of these balances that I’ve presented can be formalized mathematically using the <a href="https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations"><em>Navier–Stokes equations</em></a>. These equations describe the motion of liquids and gasses, collectively known as <a href="https://en.wikipedia.org/wiki/Fluid"><em>fluids</em></a>, subject to various forces like gravity, or, most importantly for us, pressure.</p>
<p>Navier–Stokes equations are <a href="https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_existence_and_smoothness"><em>notoriously difficult</em></a> to solve analytically, but a lot of insight about the behavior of fluids can be gained with computer simulations with various degrees of complexity.</p>
<p>In this article, I’m also employing simulations to investigate the flow of air around objects. However, the computer models used here are quite simplified and they don’t reflect the <em>full</em> richness of physics involved in the motion of air. These <em>slow-motion</em> demonstrations are intended to present the broad strokes of the delicate interaction between the air and the airfoil, but I would advise against relying on them when building an airworthy airplane.</p>
<p>With all of these caveats in place, let’s get back to the pressure distribution around a symmetric airfoil. We’re done recreating the nature-made pressure field, but there is one small aspect that we haven’t yet accounted for.</p>
<p>For our experiments, I kept the pressure steady in time so that we could focus on its general outlines. In practice, a pressure field imposed by a fast flow around any object will experience some degree of instability, which you can see in the demonstration below. You can once more drop the markers at any location to track the flow in the area:</p>

<p>As the pressure builds up on one side, it redirects the flow, which changes the pressure again. The pressure ends up oscillating back and forth like a swing. The pressure distribution and the flow direction are once again at the mercy of their mutual balance, one affecting the other. We’ll soon see some other examples of these unstable behaviors.</p>
<p>As we’ve just seen, the variation in pressure doesn’t just happen in the close vicinity of the airfoil, but it stretches quite far away from the body itself. This means that the velocity of the flow is also affected quite far away from the shape.</p>
<p>However, when it comes to the forces exerted on the airfoil, it’s only the pressure right at the surface of the airfoil that matters. Let’s bring back the two tools we’ve used before: <span><strong>surface arrows</strong></span> that show how the air pushes or “pulls” on the airfoil, and the <span><strong>net force</strong></span> arrow that tallies up the net results of <span><strong>these forces</strong></span>:</p>

<p>As the pressure field fluctuates, the resulting <span><strong>net force</strong></span> also moves around. Let’s decompose this force into two different components, one  <span><strong>perpendicular</strong></span> to the flow, and one <span><strong>parallel</strong></span> to it:</p>

<p>The force acting in the direction perpendicular to the flow is known as <span><strong>lift</strong></span>, and the one acting in the direction of the flow is known as <span><strong>pressure drag</strong></span>, or <a href="https://en.wikipedia.org/wiki/Parasitic_drag#Form_drag"><em>form drag</em></a>. As the name implies, this component of drag is created by the distribution of pressure around the shape.</p>
<p>For this airfoil, the <span><strong>pressure drag</strong></span> is very tiny. While airfoils are specifically designed to minimize the overall drag, most of that force hindering their motion comes from another source – we’ll discuss it soon enough.</p>
<p>Notice that as this flow fluctuates, the <span><strong>lift</strong></span> force jumps around, but averaged over time the upward and downward swings of that <span><strong>force</strong></span> end up balancing each other. This airfoil in <em>this</em> configuration doesn’t generate any continuous lift.</p>
<p>This shouldn’t come as a surprise since this situation is completely symmetric, so the pressure forces on the upper and lower sides of the airfoil are, on average, completely balanced. However, there is an easy way to disturb that symmetry. In the demonstration below, we’re once again meeting the plain, symmetric airfoil, but this time we can gently <span><strong>tilt</strong></span> it using the slider:</p>


<p>The <span><strong>slider</strong></span> controls the so-called <a href="https://en.wikipedia.org/wiki/Angle_of_attack"><em>angle of attack</em></a>, which is spanned between some reference line on the body, like the one joining the front and back, and the direction of the incoming flow. I’m showing this angle right in the middle of the airfoil.</p>
<p>As we <a href="#" onclick="symmetric_airfoil_fvm20_f0();return false;">change</a> the <span><strong>angle of attack</strong></span>, the shape that the airflow “sees” is no longer symmetrical relative to the incoming direction of that flow. The velocity and pressure fields adapt in their mutual push and pull to form a new, asymmetric distribution. Notice that the stagnation point of <span><strong>high pressure</strong></span> has moved around, and the little markers that indicate the motion of air now travel on very different paths below and above and below the airfoil.</p>
<p>If we then put the <span><strong>pressure arrows</strong></span> back in, we can tally them all up to get the resulting <span><strong>lift</strong></span> and <span><strong>pressure drag</strong></span>. When compared to the previous simulation, I’m scaling down all the arrows to make them fit in the bounds of the demonstration:</p>


<p>When this symmetric airfoil is <a href="#" onclick="symmetric_airfoil_fvm21_f0();return false;">tilted up</a>, the asymmetric pressure distribution generates a <span><strong>lift force</strong></span> that pushes the object up. Conversely, for a <a href="#" onclick="symmetric_airfoil_fvm21_f1();return false;">downward tilted airfoil</a>, the pressure forces <span><strong>push</strong></span> the airfoil down.</p>
<p>Naturally, we’re typically interested in upward-pointing forces, and when the <span><strong>lift</strong></span> generated by the wings is equal to the weight of the plane, the plane will stay in the air without raising or falling to the ground – we’re finally flying.</p>
<p>Let’s plot the dependence between the <span><strong>lifting force</strong></span> and the <span><strong>angle of attack</strong></span> of an airfoil – you can see it in the right side of the demonstration below. Note that this plot presents time-averaged and <em>settled</em> values, so you may have to wait a little for the flow to normalize and the <span><strong>lift</strong></span> to start oscillating around the expected value:</p>


<p>Clearly, as the <span><strong>angle of attack</strong></span> increases, so does the generated <span><strong>lift</strong></span>. The same thing happens on the other end of the spectrum, where a more negative  <span><strong>angle of attack</strong></span> creates more negative lift. Note that for this symmetric airfoil the positive and negative sides of the diagram are just mirror images of each other, so let’s focus only on positive  <span><strong>angles of attack</strong></span>.</p>
<p>One could naively hope that we could keep increasing the  <span><strong>angle of attack</strong></span> to generate more and more <span><strong>lift</strong></span>. Let’s see what happens in practice:</p>


<p>Initially, the <span><strong>lift</strong></span> force indeed keeps increasing with the <span><strong>angle of attack</strong></span>, but at some point <a href="#" onclick="symmetric_airfoil_fvm23_f0();return false;">it plateaus</a>. Once that <em>critical</em> angle of attack is surpassed, the <span><strong>lift</strong></span> force <a href="#" onclick="symmetric_airfoil_fvm23_f1();return false;">starts to fall</a> after the flow <a href="#" onclick="symmetric_airfoil_fvm23_f2();return false;">fully develops</a>.</p>
<p>What we’re witnessing here is known as a <a href="https://en.wikipedia.org/wiki/Stall_(fluid_dynamics)"><em>stall</em></a>. The onset of a stall imposes limits on how much <span><strong>lift</strong></span> the wings of an airplane can generate from merely increasing the angle of attack.</p>
<p>Notice that when the stall happens, the pressure distribution on the upper part of the airfoil becomes very erratic – it’s not only the surface <span><strong>pressure arrows</strong></span> that are changing rapidly, but the whole pressure field in that area is very disturbed.</p>
<p>Let’s bring in the velocity arrows and markers to get a better feel on what’s going on in that region:</p>


<p>At high angles of attack, the flow above the upper part of the airfoil becomes very complicated. If you <span>click</span><span>tap</span> in that region to drop a few markers, you’ll notice that the air is trapped in various swirling eddies that are eventually shed to fly away with rest of the flow.</p>
<p>We’re witnessing <a href="https://en.wikipedia.org/wiki/Flow_separation"><em>flow separation</em></a>, where the main part of the flow detaches from the surface and doesn’t follow its shape anymore. The interactions in the complicated flow right above the airfoil affect the pressure field, which then <em>decreases</em> <span><strong>lift</strong></span>.</p>
<p>There is a lot going on there, but to understand how these effects arise we have to talk about a property that affects the flow of every fluid: viscosity.</p>
<h2 id="viscosity">Viscosity<a href="https://ciechanow.ski/airfoil/#viscosity" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>You might have heard the term <a href="https://en.wikipedia.org/wiki/Viscosity"><em>viscosity</em></a> used to describe “thickness” of different liquids, with a classic example that contrasts the slowness of the flow of honey to the rapidness of the flow of water.</p>
<p>Viscosity is also a property of gasses like air, but before I describe this concept more formally, we’ll first build an intuitive understanding of what viscosity is and what it does to the flow of different fluids.</p>
<p>In the demonstration below, the fluid flows in from the left side, but note that the flow in the top half is faster than the flow in the bottom half, which is reflected by the different lengths of the arrows. Dragging the slider to the left decreases the <span><strong>viscosity</strong></span> of the fluid, and dragging the slider to the right increases <span><strong>viscosity</strong></span>:</p>


<p>While we can see some changes to the arrows as we move the slider around, you probably agree that, for this flow, the arrow-based visualization isn’t very rewarding. Let’s add the color-based <strong><span>visualization of speed</span></strong> distribution in this flow:</p>


<p>We can now see how <span><strong>viscosity</strong></span> blends the speed variation between different sections of the fluid. For highly <a href="#" onclick="fdm2_f0();return false;">viscous fluids</a>, this mixing behavior spreads very easily and the initially distinct velocities of the two layers average out quite rapidly.</p>
<p>At <a href="#" onclick="fdm2_f1();return false;">lower viscosity</a> these two layers with different speeds remain quite separated. If you make the viscosity <a href="#" onclick="fdm2_f2();return false;">low enough</a>, you may even notice that, after a while, the flow develops some interesting wave-like phenomena – we’ll get back to these soon.</p>
<p>All this mixing behavior may remind you of a <a href="https://en.wikipedia.org/wiki/Diffusion"><em>diffusion</em></a> process, where some quantity, like temperature or concentration, evens out over time. Let’s see some basic diffusion in action. In the simulation below, I filled half of the bottle with with <span><strong>red-dyed water</strong></span>, while the other half is filled with <span><strong>blue-dyed water</strong></span>. The slider lets you control the <span><strong>speed of time</strong></span>:</p>


<p>As <a href="#" onclick="bottle_f0();return false;">time passes</a>, the sharp difference between the two layer blends more and more to eventually completely disappear. Clearly, there is some similarity between the diffusion of differently <span><strong>colored</strong></span> <span><strong>dyes</strong></span> and the averaging of velocity that we’ve seen in the earlier example.</p>
<p>In our flow demonstrations, <span><strong>viscosity</strong></span> seemed to have controlled the diffusion of velocity. To define it more precisely, <span><strong>viscosity</strong></span> controls the diffusion of <a href="https://en.wikipedia.org/wiki/Momentum"><em>momentum</em></a>, which is a product of velocity <em>and</em> mass. The simplified fluids we’re looking at have more or less constant density, so each equally-sized parcel of those fluids has the same mass. Therefore, if it makes things easier for you, wherever you see the word momentum you can think of velocity, but in more complex scenarios these differences can matter.</p>
<p>Let me bring in the previous flow simulation one more time:</p>


<p>You’ve probably noticed that, as the flow moves to the right, the size of this blended region increases. When the regions of fluid with different momentums meet for the first time, they barely have any time to average out, and the blending is minimal. As time passes, these regions of fluid get to average out more, similarly to how two different layers of dyed water mix more over time.</p>
<p>However, as time is passing, these parcels also <em>keep moving</em>, and that stronger blending happens further to the right. The downstream regions had more time to mix and average out, so the visible thickness of the blended region on the right side is also larger.</p>
<p>With higher <span><strong>viscosity</strong></span>, the size of blended region grows much more quickly, which lets us be more precise about our working definition – <span><strong>viscosity</strong></span> controls the <em>rate</em> of the diffusion of momentum.</p>
<p>So far we’ve only observed flows with nicely separated horizontal layers, but <span><strong>viscosity</strong></span> averages momentum between <em>any</em> two regions of fluids. In the demonstration below, you can witness how <span><strong>viscosity</strong></span> affects a swirly motion of fluid in a <a href="https://en.wikipedia.org/wiki/Vortex"><em>vortex</em></a>:</p>


<p>Notice that with <a href="#" onclick="fdm4_f0();return false;">high viscosity</a> any differences in velocity are very quickly diluted out into nothing, but with <a href="#" onclick="fdm4_f1();return false;">low viscosity</a> the revolving motion can survive for quite a while.</p>
<p><span><strong>Viscosity</strong></span> has a damping or smoothing effect that makes it much harder to sustain any large variation in a velocity field. Let’s see how this affects the motion of objects in fluids of various viscosity. In the demonstration below, we’re tracking a velocity field close to a very <span><strong>thin plate</strong></span> put directly in the stream of an incoming fluid of adjustable <span><strong>viscosity</strong></span>:</p>


<p>With <a href="#" onclick="fdm5_f0();return false;">high viscosity</a>, there is a large region of slow down around the <span><strong>plate</strong></span> that regains its speed fairly quickly behind the object. At <a href="#" onclick="fdm5_f1();return false;">lower viscosity</a> that surrounding region is much smaller, but it extends much further behind the <span><strong>plate</strong></span>. For <a href="#" onclick="fdm5_f2();return false;">very low viscosity</a> we’re once again seeing some more unusual behavior that we’ll get back to in a minute.</p>
<p>From the dark colors we can easily see that right by the surface of the <span><strong>plate</strong></span> the fluid doesn’t move at all – it sticks to that surface. This velocity difference between the halted flow at the wall and the moving outer flow gets smoothed out over time by <span><strong>viscosity</strong></span>, similar to how it blended in the flow between two different layers of fluid.</p>
<p>As before, with higher <span><strong>viscosity</strong></span>, the velocity averaging process becomes more rapid, and the blended region becomes more widespread. This averaging effect doesn’t just alter the velocity of fluid, but it also affects the plate. In some sense, the <span><strong>viscosity</strong></span> also wants to make the velocity of the surface of the <span><strong>plate</strong></span> to be more like the velocity of the surrounding flow.</p>
<p>The viscosity makes the flow want to pull the <span><strong>plate</strong></span> with it, which creates a <a href="https://en.wikipedia.org/wiki/Shear_force"><em>shearing force</em></a> that tries to slide the surface of this object away. The net effect is that that viscosity creates additional drag known as <a href="https://en.wikipedia.org/wiki/Skin_friction_drag"><em>skin friction drag</em></a> that wants to slow down any object moving in it.</p>
<p>All of these effects underline why highly viscous fluids are “thick”. Viscosity not only quickly averages any local differences in velocity, which prevents those fluids from flowing easily, but it also represses motion of objects in those fluids – you’ve likely experienced the difficulty of moving a spoon through a jar of honey.</p>
<p>The flow of any fluid exhibits tiny, random disturbances. In fluids with high viscosity, these variations are very quickly dispersed, so their motion is rarely erratic. Fluids with low viscosity aren’t as effective at damping motion, and these disturbances can grow to create oscillatory patterns. We’ve seen glimpses of them in the previous simulations, but here is another example:</p>


<p>At lower viscosity the flow becomes <a href="#" onclick="fdm10_f0();return false;">quite wave-y</a>. Those <a href="https://en.wikipedia.org/wiki/Kelvin%E2%80%93Helmholtz_instability"><em>instabilities</em></a> happen at the border of regions of fluid with different velocities, like where the slow wake behind a plate is in contact with the fast external flow. In those regions, any tiny random intrusion of slower flow into the faster flow can get magnified and rolled over like a wave.</p>
<p>In our discussion of the motion of air around an airfoil, we’ve seen how the flow, the pressure field, and the shape of the body have effects on each other. These influences can be quite dynamic in nature, with distributions of velocity and pressure swinging back and forth in a never-ending fight for dominance.</p>
<p>In the demonstration below, we can see a more dramatic example of these battles, where, depending on the <span><strong>viscosity</strong></span>, the flow around a <span><strong>gray cube</strong></span> can take many different forms:</p>


<p>With very <a href="#" onclick="fdm9_f0();return false;">high viscosity</a>, the flow is completely stable, but as <a href="#" onclick="fdm9_f1();return false;">viscosity decreases</a>, it starts to regularly oscillate from side to side, <a href="https://en.wikipedia.org/wiki/Vortex_shedding"><em>shedding vortices</em></a> in the process. At <a href="#" onclick="fdm9_f2();return false;">very low viscosity</a>, the motion becomes even more erratic.</p>
<p>While I can’t easily simulate it here, with further decrease in viscosity, the flow <a href="https://www.youtube.com/watch?v=c8zKWaxohng">can develop</a> full featured <a href="https://en.wikipedia.org/wiki/Turbulence"><em>turbulence</em></a> in which highly irregular and chaotic mixing motions occur at different scales. Turbulent flow stands in contrast to <a href="https://en.wikipedia.org/wiki/Laminar_flow"><em>laminar flow</em></a>, in which neighboring areas of fluid move in an orderly way past each other without any varying fluctuations.</p>
<p>Although we’ve put most of our focus on <span><strong>viscosity</strong></span>, which is often denoted with the Greek letter <strong>μ</strong>, the general behavior of the flow also depends on its velocity <strong>u</strong>, density <strong>ρ</strong>, and the size <strong>L</strong> of the body or container involved in the flow. These parameters are tied together by the <a href="https://en.wikipedia.org/wiki/Reynolds_number"><em>Reynolds number</em></a> <strong>Re</strong>:</p>
<p>
Re = <span>
    <span>ρ · u · L</span>
    <span>/</span>
    <span>μ</span> 
</span>
</p>
<p>Flows with the same Reynolds numbers exhibit similar behavior, which means that if we make the obstacle size <strong>L</strong> twice as large and we halve the speed of the flow <strong>u</strong>, the Reynolds number won’t change and neither will the characteristics of the flow – it will exhibit the same smooth or oscillatory motion.</p>
<p>The Reynolds number also “predicts” the onset of turbulence. When we increase the speed of the flow <strong>u</strong>, or decrease the viscosity <strong>μ</strong>, the Reynolds number rises. When it reaches a high enough value, turbulence is likely to occur.</p>
<p>Let’s quantify the difference in viscosity between different fluids. The precise values aren’t that important to us, but to briefly be a bit more formal, viscosity is expressed in units of pascal-seconds, or <strong>Pa·s</strong>. To let us use more manageable numbers, the following table uses millipascal-seconds, or <strong>mPa·s</strong>:</p>
<table>
<tbody><tr><td>honey</td><td>~10000 <span>mPa·s</span></td></tr>
<tr><td>olive oil</td><td>~100 <span>mPa·s</span></td></tr>
<tr><td>water</td><td>1.0 <span>mPa·s</span></td></tr>
<tr><td>air</td><td>0.018 <span>mPa·s</span></td></tr>
</tbody></table>
<p>These values are measured at <span>68 °F</span><span>20 °C</span>, but many fluids like oil get much less viscous with increased temperature. As expected, honey is significantly more viscous than water. Compared to water, the viscosity of air is around 50 times less still, but even a very low viscosity has effects on flow and its interaction with solid walls.</p>
<p>To understand how viscosity arises in gasses like air, we have to once more get back to the world of particles. So far we’ve been watching them from a distance, with individual collisions barely perceptible in the moving swarm. This time we’re going take a closer look at these interactions.</p>
<p>In the demonstration below, you can experience a simplified simulation of <span><strong>two</strong></span> <span><strong>molecules</strong></span> colliding in space. Each molecule represents nitrogen or oxygen – these two elements constitute the vast majority of air, and, in normal conditions, each one consists of two atoms.</p>
<p>You can drag the <span><strong>orange particle</strong></span> around, and once you let go I’ll automatically aim it so that it hits the <span><strong>blue particle</strong></span>. The speed of the <span><strong>orange molecule</strong></span> is four times larger than the speed of the <span><strong>blue one</strong></span>:</p>

<p>Notice that after the collision, it’s the <span><strong>orange molecule</strong></span> that’s slow, and it’s the <span><strong>blue one</strong></span> that’s fast. In this demonstration the two particles have the same mass and they collide straight on, so they simply end up trading velocities.</p>
<p>More generally, particles of different masses that strike each other at different angles will exchange some amount of momentum. Recall that the heavier the particle, or the faster it moves, the higher its momentum.</p>
<p>Let’s see how this behavior ends up affecting the average velocities of larger quantities of molecules. In the <span><strong>paused</strong></span> demonstration below, air molecules are grouped into two different parts. The air in the <span><strong>blue region</strong></span> has higher velocity than the air in the <span><strong>red region</strong></span>, which you can see in the black arrows showing the <span><strong>average velocity</strong></span> in those regions. Notice what happens to <span><strong>these averages</strong></span> as you let <span><strong>time flow</strong></span> by dragging the slider:</p>



<p>At the very beginning, the <span><strong>average velocities</strong></span> in these <span><strong>two</strong></span> <span><strong>sections</strong></span> are visibly different, but they quickly even out when fast particles from the <span><strong>blue region</strong></span> flow into the slower <span><strong>red region</strong></span>, and the slower particles from the <span><strong>red region</strong></span> move into the faster <span><strong>blue region</strong></span>, balancing the initial velocity differences.</p>
<p>Moreover, some of the faster particles collide with slower particles in the <span><strong>red region</strong></span> and some of the slower particles collide with faster particles from <span><strong>above</strong></span>. The faster particles lose some of their higher momentum, while the slower particles gain some of the momentum. All of these effects “dilute” some of those average velocity differences between the two regions.</p>
<p>You may also remember that when we observed a flow of fluid around a flat plate, that fluid wasn’t moving at all right on the surface of that plate, because it was stuck to it. Let’s see how this behavior may arise on a microscopic scale.</p>
<p>In the demonstration below, we’re watching the familiar air particles right next to the <span><strong>surface of an object</strong></span>. To make tracking easier, I’m <span><strong>highlighting</strong></span> some of the particles in the vicinity of this <span><strong>surface</strong></span>:</p>


<p>When seen at a very large magnification, this <span><strong>surface</strong></span>, like almost all surfaces, isn’t perfectly smooth and has various peaks and valleys. The particles hitting these irregularities get bounced in more or less random directions. Some of the unlucky molecules can even get stuck for a while in these local crevices.</p>
<p>Close to the <span><strong>surface</strong></span>, the random collisions with peaks and valleys prevent the particles from making bulk progress in <em>any</em> direction. The average velocity of the air flow by the wall is more or less zero. Some molecular interactions between the particles and the <span><strong>surface</strong></span> can also prevent the fluid from moving.</p>
<p>This sticking behavior is known as the <a href="https://en.wikipedia.org/wiki/No-slip_condition"><em>no‑slip condition</em></a> and it holds true for most typical flows of fluids that we experience day to day. It’s only in extreme conditions of very rarified gasses in the upper parts of the atmosphere or flows in microscopic capillaries that can break this assumption.</p>
<p>Let’s leave the world of particles behind for the last time and see how these two effects play an important role of influencing the airflow close to the surface of any object.</p>
<h2 id="boundary-layer">Boundary Layer<a href="https://ciechanow.ski/airfoil/#boundary-layer" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>Let’s take another look at a <span><strong>thin plate</strong></span>  placed in the stream of incoming fluid:</p>

<p>From this broader perspective, it’s hard to see how the flow interacts with the surface of that <span><strong>plate</strong></span>, because the effects of viscosity are limited to the region close to that surface. Let’s focus our attention on the small area that I’ve outlined with a <span>dashed</span> line, right in the top part of the <span><strong>plate</strong></span>. Here it is zoomed up close:</p>

<p>We can once more see that, due to the no-slip condition, the velocity is zero at the <span><strong>wall</strong></span>, and then it <strong><span>grows to meet</span></strong> the velocity of the flow further away from the surface itself. What we’re seeing here is known as the <a href="https://en.wikipedia.org/wiki/Boundary_layer"><em>boundary layer</em></a>, which spans the region between the <span><strong>surface</strong></span> of the object and the “outer” flow, which is mostly unaffected by the presence of the object.</p>
<p>Because the velocity in the boundary layer smoothly approaches the speed of the outer flow, it doesn’t have a well-defined end point. One of the choices is to agree that the boundary layer ends where the speed reaches 99% of the speed of the surrounding flow far away from the <span><strong>solid surface</strong></span>. Let me visualize this boundary in the flow using a <span><strong>dashed line</strong></span>:</p>

<p>As we move with the flow along the distance of the plate, the viscosity keeps averaging out the velocity differences, making the boundary layer thicker – this is similar to what we’ve seen at larger scales with highly viscous flows around objects.</p>
<p>Let’s quantify the distribution of speed in the boundary layer a little more precisely. In the demonstration below, I put the velocity arrows back in. I then connected the ends of these arrows with a thin line to show a <em>profile</em> of velocity at that location along the <span><strong>surface</strong></span>:</p>

<p>Notice that, initially, the velocity close to the wall increases almost linearly, but then it smoothly tapers to reach the speed of the external flow. The velocity profile close to the surface has a certain steepness, which I’m showing with the <span><strong>white dotted line</strong></span>. This line determines the amount of skin friction drag at that spot – the closer to the <span><strong>surface</strong></span>, or more horizontal, the <span><strong>line</strong></span> is, the higher the skin drag.</p>
<p>As the differences in velocity become less severe, the force with which viscosity wants to drag the surface with the flow also decreases. In the conditions present in the demonstration, the skin friction drag decreases over distance.</p>
<p>At this point you hopefully have an intuitive grasp of how viscosity affects the flow close to the surface of the object. From our earlier discussion, you may also remember that pressure differences also affect how the flow behaves, with parcels of air slowing down when climbing the hill of <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">increasing pressure</span></strong> and accelerating on the downhill of the <strong><span data-color0="E5432E" data-color1="dddddd" data-color2="3F90CD">decreasing pressure</span></strong>.</p>
<p>In the boundary layer flows we played with, the pressure distribution was more or less constant in the investigated region. Let’s see how the flow changes when we vary that pressure.</p>
<p>In the top part of the demonstration below we see the exact same view of velocity we’ve experimented with so far. In the bottom part of the demonstration below you can see the pressure distribution in the boundary layer, which you can change using the slider below.</p>


<p>If the pressure <a href="#" onclick="boundary4_f0();return false;">decreases</a> in the direction of the flow in the boundary layer, we say that the pressure gradient is <em>favorable</em>. <strong><span data-color0="E5432E" data-color1="dddddd" data-color2="3F90CD">Favorable pressure gradient</span></strong> accelerates the air, and the boundary layer doesn’t grow as quickly, since the slowdown caused by viscosity is opposed by that acceleration.</p>
<p>When the pressure <a href="#" onclick="boundary4_f1();return false;">increases</a> in the direction of the flow, we say that the pressure gradient is <em>adverse</em>. <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">Adverse pressure gradient</span></strong> pushes <em>against</em> the direction of motion of the air. Far away from the <span><strong>surface</strong></span>, the air has enough momentum that the adverse pressure merely slows the flow down. However, close to the <span><strong>surface</strong></span>, the flow in the boundary layer was slow in the first place, so a pushing <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">adverse pressure gradient</span></strong> may even reverse the direction of the flow.</p>
<p>When the flow in the boundary layer gets reversed, we say that the boundary layer <em>separates</em>. This region of reversed flow can form a sort of wedge that can lift the rest of the flow away from the <span><strong>surface</strong></span>.</p>
<p>Let’s take a step back from the subtleties of boundary layers to see how what we’ve learned corresponds to behavior of a flow around an airfoil. Let me once more bring up the demonstration that brought us here in the first place:</p>


<p>As we move across the surface of the airfoil, the <span><strong>high pressure</strong></span> at the stagnation point up front gradually decreases to reach <span><strong>minimum</strong></span> close to the “peak” of that curved surface. Across this transition the <strong><span data-color0="E5432E" data-color1="dddddd" data-color2="3F90CD">pressure gradient is favorable</span></strong>, and that distribution works in our favor – the boundary layer stays nicely attached to the surface.</p>
<p>However, as the air reaches the valley of the <span><strong>lowest pressure</strong></span>, it then has to start climbing back up to reach the <span><strong>slightly positive</strong></span> pressure in the rear of the airfoil. For <a href="#" onclick="symmetric_airfoil_fvm24a_f0();return false;">small values</a> of the  <span><strong>angle of attack</strong></span>, the pressure pit from which the air has to climb out is not very deep and the <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">adverse pressure gradient</span></strong> isn’t very strong, so the boundary layer remains attached.</p>
<p>As we <a href="#" onclick="symmetric_airfoil_fvm24a_f1();return false;">increase</a> the <span><strong>angle of attack</strong></span> of the airfoil, the pressure on top becomes <span><strong>lower</strong></span> and <span><strong>lower</strong></span>. For even <a href="#" onclick="symmetric_airfoil_fvm24a_f2();return false;">higher angles</a>, the <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">adverse pressure gradient</span></strong> becomes so strong that it  <a href="#" onclick="symmetric_airfoil_fvm24a_f3();return false;">eventually</a> reverses the flow in the boundary layer, creating separation. Let’s look at this region up close to see how the arrows of velocity in the separated region point in the other direction:</p>

<p>If you <span>click</span><span>tap</span> to add <strong>markers</strong> in the bottom right corner of the simulation you’ll notice that many of them move <em>against</em> the bulk of the flow – the boundary layer and the flow have separated.</p>
<p>We’ll get back to looking at airfoils soon enough, but we still have a few things to wrap up in the world of boundary layers.</p>
<p>The boundary layers we’ve looked at so far were <em>laminar</em> – the layers of fluid with different velocities flowed in an orderly way on top of each other. However, at higher flow speeds and over larger distances, or at high Reynolds numbers, the flow in the boundary layer transitions to a <em>turbulent</em> flow:</p>

<p>Be aware that what you’re seeing here is a very simplified simulation of a turbulent boundary layer. Turbulence is inherently three dimensional and it contains various evolving structures of different sizes that are extremely computationally expensive to evaluate in detail. Thankfully, you can find many videos of <a href="https://www.youtube.com/watch?v=Wr984EOmNaY">computer</a> <a href="https://www.youtube.com/watch?v=wXsl4eyupUY">simulations</a> and <a href="https://www.youtube.com/watch?v=e1TbkLIDWys">real flows</a> showing turbulent boundary layers.</p>
<p>While the laminar boundary layers we’ve seen in the past exhibited very organized flows, the turbulent one is very chaotic, with large and small swirls causing the flow to mix very rapidly. The transition from laminar to turbulent boundary layer happens spontaneously, but for a given flow speed, the location of the transition depends on surface roughness, steadiness of the flow outside of the boundary layer, and presence of pressure gradients.</p>
<p>At any given moment, the velocity profile in the turbulent boundary layer is very unsteady, but it can be averaged over time to get the mean distribution of speed. Let’s compare the <em>time-averaged</em> profiles of the <span><strong>laminar</strong></span> and <span><strong>turbulent</strong></span> boundary layers:</p>

<p>In the dynamic simulation of the <span><strong>turbulent</strong></span> boundary layer, we saw how the slower flow close to the <span><strong>surface</strong></span> rapidly mixed with the upper regions of the flow. This slows down those faster sections, and we need to go farther away from the <span><strong>surface</strong></span> for these sluggish intrusions to stop affecting the flow. For this reason, the <span><strong>turbulent</strong></span> boundary layer is thicker and grows faster than a <span><strong>laminar</strong></span> boundary layer.</p>
<p>On the other hand, the strong <span><strong>turbulent</strong></span>  mixing causes the fast external flow to get close to the body, so the overall velocity profile by the <span><strong>surface</strong></span> increases much more quickly in the <span><strong>turbulent</strong></span> case as opposed to <span><strong>laminar</strong></span> case – I’m showing that with <span><strong>white dotted lines</strong></span>.</p>
<p>Recall that the more horizontal the velocity profile at the <span><strong>surface</strong></span> of the object, the bigger the skin friction drag – a <span><strong>turbulent</strong></span>  boundary layer has higher skin friction drag than a <span><strong>laminar</strong></span> layer. Despite the cost of increased friction drag, a <span><strong>turbulent</strong></span>  boundary layer is often beneficial.</p>
<p>Because of that higher velocity closer to the surface, a turbulent boundary layer is more resistant to <strong><span data-color0="3F90CD" data-color1="dddddd" data-color2="E5432E">adverse pressure gradients</span></strong> and it can stay attached to the surface of an object for longer distances.</p>
<p>For some objects like golf balls, which purposefully make their boundary layer turbulent by roughing up the surface with little dimples, the delayed separation also decreases the <span><strong>pressure drag</strong></span> caused by uneven pressure distribution. That reduction more than compensates for the increased skin friction drag, making the dimply golf balls fly farther than equivalent smooth balls.</p>
<p>For airfoils, a turbulent boundary layer delays separation of the flow, which can help prevent stall at higher angles of attack, but at normal cruising conditions the increased skin friction becomes an important drawback. For many aerodynamic shapes in typical conditions, the skin friction frag is the primary contributor to the total drag that these objects experience.</p>
<p>As we’ve seen, by increasing the angle of attack on an airfoil, the lift force grows up to a certain limit, at which the boundary layer separates over most of the upper surface. By staying under this limit, a symmetric airfoil can safely generate lift force.</p>
<p>However, when it comes to  <span><strong>angle of attack</strong></span> and <span><strong>lift</strong></span>, the shape of an airfoil isn’t particularly unique in its <span><strong>lift</strong></span>-creation capabilities. Most simple elongated shapes generate <span><strong>lift</strong></span> when put in a flow at an  <span><strong>angle of attack</strong></span>. In the demonstration below, you can <span><strong>tilt</strong></span> a <span><strong>flat plate</strong></span> and see the forces exerted by the pressure field around it:</p>


<p>You may be surprised to see that, at small  <span><strong>angles of attack</strong></span>, this flat plate also generates <span><strong>lift</strong></span>. An airfoil-like shape is not a requirement for <span><strong>lift</strong></span> generation. After all, paper airplanes with their flat wings can fly just fine. <span><strong>Lift</strong></span> is just an outcome of the pressure distribution created and sustained by the flow.</p>
<p>Although it doesn’t take a sophisticated shape to generate lift at an angle of attack, a well-designed airfoil can often create more lift and with lower drag. In the last section of this article, we’ll explore how other variations to the shape of an airfoil can affect its characteristics.</p>
<h2 id="airfoil-shapes">Airfoil Shapes<a href="https://ciechanow.ski/airfoil/#airfoil-shapes" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>Let’s go back to the simple symmetric airfoil we’ve been playing with thus far. This time, however, we’re able to control its <span><strong>thickness</strong></span> using the slider:</p>


<p>Notice that as we increase the <span><strong>thickness</strong></span> of the airfoil, the pressure on the top and bottom sections of the shape becomes more <span><strong>negative</strong></span>. For this symmetric airfoil at 0° angle of attack the thickness doesn’t change much other than increasing the <span><strong>pressure drag</strong></span>.</p>
<p>However, if we break the symmetry of the shape, we can use thickness-dependence to make one side of the airfoil have a higher <span><strong>negative</strong></span> pressure than the other. In the demonstration below, you can control the “thickness” of the upper surface of the airfoil using the slider:</p>


<p>Notice that an asymmetric shape creates an asymmetric pressure distribution, which ends up creating <span><strong>lift</strong></span> without any changes to angle of attack. With some slight tweaking of this shape we finally recreated the asymmetric shape we first saw on the airplane in the early sections of this article.</p>
<p>Naturally, when combined with an increasing <span><strong>angle of attack</strong></span>, this airfoil will generate even more <span><strong>lift</strong></span> until it eventually reaches stalling conditions:</p>


<p>While symmetric airfoils are sometimes used in acrobatic airplanes, which often find themselves flying upside down, most typical planes use an asymmetric airfoil shape.</p>
<p>The underlying mechanism of <span><strong>lift</strong></span> generation by changing the <span><strong>angle of attack</strong></span> or by shaping the object differently is ultimately the same – we’re changing the placement and orientation of the surface of the body relative to the incoming flow. The flow reacts by changing the velocity and pressure distribution, and the resulting pressure field creates the forces on that object.</p>
<p>This all means that we have a lot of flexibility in how an airfoil is shaped, as long as the resulting pressure distribution fulfills the design goals of achieving a certain amount of lift while minimizing drag.</p>
<p>For example, in some applications it’s important to minimize the skin friction drag caused by a turbulent boundary layer. Some <em>laminar flow airfoils</em> achieve this by shaping the airfoil to move the “pit” of <span><strong>negative pressure</strong></span> further to the back of the airfoil:</p>


<p>The <strong><span data-color0="E5432E" data-color1="dddddd" data-color2="3F90CD">favorable pressure gradient</span></strong> between the front and the lowest pressure point extends over a longer distance across the surface of this airfoil, which, at least in principle, helps to keep the boundary layer laminar to keep the skin friction low.</p>
<p>Notice that even this unusual airfoil had a rounded front and a sharp back. The roundness of the front helps the air smoothly flow around this area at different  <span><strong>angles of attack</strong></span>, and the sharp back reduces the pressure drag by avoiding the separation of the flow.</p>
<p>The velocity of the flow around the airfoil is also a contributing factor to the design of the shape. Let’s look at the speed distribution in the flow around a simple asymmetric airfoil using the <strong><span>varying colors</span></strong> and <strong>markers</strong>:</p>


<p>The flow above the airfoil is faster than the incoming flow as indicated by brighter colors. The markers that start in the same line don’t end up sliding off the airfoil in the same formation – the ones on top are further ahead. This is particularly visible for <a href="#" onclick="airfoil_fvm3_f0();return false;">larger</a> values of the <span><strong>angles of attack</strong></span>.</p>
<p>This acceleration in the upper part becomes another point of consideration for airfoil design. While commercial airliners don’t fly faster than the speed of sound, the accelerated flow in the top part of an airfoil <em>can</em> break that barrier. This creates a shockwave that can sometimes be <a href="https://www.youtube.com/watch?v=HekbC6Pl4_Y">seen in flight</a>. Modern airliners use <a href="https://en.wikipedia.org/wiki/Supercritical_airfoil">supercritical airfoils</a> that are designed to reduce these drag-causing shockwaves by carefully controlling the speed of the flow around the wing.</p>
<p>Planes designed to fly <em>above</em> the speed of sound use <a href="https://en.wikipedia.org/wiki/Supersonic_airfoils"><em>supersonic airfoils</em></a> that are quite different from the shapes we’ve seen. These airfoils have a thin profile and their front edge is sharp and not rounded. <a href="https://en.wikipedia.org/wiki/Supersonic_speed"><em>Supersonic</em></a> flows of air are more complicated than what we’ve explored in this article, as variations in density and temperature become an important component of the behavior of the flow.</p>
<p>Many of the airfoils used today are designed specifically for the plane they’ll be used in. Moreover, that cross-sectional shape may change across the length of the wing. Real airplanes are three dimensional and the <a href="https://en.wikipedia.org/wiki/Wing_configuration">overall shape of the wings</a> also significantly affects the lift and drag of an airplane, but ultimately all the resulting forces are an outcome of interactions between the flow and the body.</p>
<h2 id="further-reading-and-watching">Further Reading and Watching<a href="https://ciechanow.ski/airfoil/#further-reading-and-watching" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>John Anderson’s <a href="https://www.mheducation.com/highered/product/fundamentals-aerodynamics-anderson/1264151926.html">Fundamentals of Aerodynamics</a> is a very well-written textbook on aerodynamics. Over the course of over a thousand pages, the author presents a classic exposition of the motion of fluids and their interactions with bodies put in those flows.</p>
<p><a href="https://www.wiley.com/en-us/Understanding+Aerodynamics%3A+Arguing+from+the+Real+Physics-p-9781119967514">Understanding Aerodynamics</a> by Doug McLean is a great textbook that takes a different approach of explaining aerodynamic phenomena using physical reasoning. For me, the crowning achievement of the publication is showing that many popular explanations of the origins of lift are either incorrect or they’re based on merely mathematically convenient theorems. The author’s <a href="https://www.youtube.com/watch?v=QKCK4lJLQHU">video lecture</a> gives an overview of some of these misconceptions.</p>
<p>In this article, I’m using <a href="https://en.wikipedia.org/wiki/Computational_fluid_dynamics"><em>computational fluid dynamics</em></a> to simulate the flow of air around different objects. For an approachable introduction to these methods I enjoyed Tony Saad’s <a href="https://www.youtube.com/playlist?list=PLEaLl6Sf-KIC7oet7zvNfW03aocrIq-s4">series of lectures</a> on the topic. For an alternative, and slightly more rigorous approach, Lorena Barba created <a href="https://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/">12 steps to Navier-Stokes</a>. That website is also accompanied by <a href="https://www.youtube.com/playlist?list=PL30F4C5ABCE62CB61">video lectures</a>.</p>
<p>Finally, YouTuber <a href="https://www.youtube.com/@braintruffle/videos">braintruffle</a> created a series of <a href="https://www.youtube.com/playlist?list=PLMoTR49uj6ld32zLVWmcGXaW7w2ey7Vh4">beautiful videos</a> that start with the behavior of fluids on a quantum scale and build up increasingly abstract models that can be used in more practical applications. The videos are packed with interesting takes on fluid mechanics, and they’re worth watching for their visuals alone.</p>
<h2 id="final-words">Final Words<a href="https://ciechanow.ski/airfoil/#final-words" arialabel="Anchor"><img src="https://ciechanow.ski/images/anchor.png" width="16px" height="8px"></a> </h2>
<p>If you were to sit on a flying airplane and look out the window to glance at its wings, you’d often have a hard time seeing anything going on. However, in that crisp clearness of air whose invisible flow sustains the varied pressure field, lies the hidden source of lift that overcomes the might of gravity to keep the plane safely above the ground.</p>
<p>Since the first human flight, we’ve now mastered the art of soaring in the skies by bending the flow of air to our will, using physical quantities like pressure and velocity to help shape our designs. These tangible concepts are ultimately just a manifestation of motions and collisions of billions of inanimate air particles that somehow conspire to assemble the forces we need.</p>
<p>I hope this deeper, technical exploration of airfoils hasn’t diminished your appreciation of the greatness of flight. Perhaps paradoxically, by seeing how all the pieces fit together, you’ll find the whole thing even more magical.</p>
    </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Testing the F-35C Tailhook (156 pts)]]></title>
            <link>https://the-engi-nerd.github.io/posts/welcome/</link>
            <guid>39525243</guid>
            <pubDate>Tue, 27 Feb 2024 15:32:36 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://the-engi-nerd.github.io/posts/welcome/">https://the-engi-nerd.github.io/posts/welcome/</a>, See on <a href="https://news.ycombinator.com/item?id=39525243">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="quarto-content">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main id="quarto-document-content">





<p>Welcome to the blog. As a way to kick things off, here’s a single-page version of my thread on the F-35C tailhook.</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-270015806.png"></p>
<p>The primary source for material here, aside from my own experiences, is “F-35C Carrier Suitability Testing” by Tony “Brick” Wilson, who gives us one of the greatest flexes in an AIAA paper I’ve ever seen.</p>
<div>
<figure>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-3796555614.png" width="800"></p>
</figure>
</div>
<p>The F-35C is the variant of the F-35 Joint Strike Fighter intended for use on CATOBAR (Catapult Assisted TakeOff, Barrier Assisted Recovery) aircraft carriers operated by the United States Navy. Aircraft are launched via steam or electromagnetic catapults, and on landing, the aircraft uses a tailhook to engage an arresting wire.</p>
<p>The F-35C hook is stowed in a bay that’s covered by clamshell doors during flight. The hook is electronically controlled and moved by hydraulics. And originally, it didn’t work.</p>
<div>
<figure>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-2200776526.png" width="500"></p>
</figure>
</div>
<p>We began serious carrier suitability testing with F-35C at Lakehurst NAS in the summer of 2011. We would hold our briefings in the same hangar that the Hindenburg was trying to reach when it crashed. This is a level of irony that took zoomers another ten years to reach.</p>
<div>
<figure>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-1904785528.png"></p>
</figure>
</div>
<p>After things like Jet Blast Deflector compatibility testing, we get down to testing the hook. Brick tells us, in a masterful understatement:</p>
<div>
<figure>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-4221285650.png"></p>
</figure>
</div>
<p>I was at Lakehurst for most of this testing and I never saw the hook engage the wire. It was interesting to see it play out this way. I heard it predicted first, by, of all people, one of our instrumentation technicians on November 6, 2010 That was the day CF-01 arrived at Pax. We went to look at the jet. I remember standing with this tech, looking at the hook.</p>
<p>“Boss,” he says to me, “This fucker ain’t gonna work. Look at this thing. It’s short, it’s too close to the wheels, and look at this dumbass hook shoe they got on it. If the wire don’t hit it exactly right, it’s just gonna go under the hook and you’ll bolter.”</p>
<p>The hook point is only a little over 7 feet from the main landing gear axle center. The amount of space in the airframe available is extremely tight, so this is what the designers had to work with.</p>
<p>Here’s pretty much what we were looking at in this conversation. <img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-4283089921.png"> Why was it like this?</p>
<p>Because the engineers at Northrop Grumman engineered to a computer model that can simulate how an arresting wire acts when used for arrestment, and the model was wrong.</p>
<p>I heard years later in the course of my job verifying and validating other F-35 simulations that the wire dynamics model supplied by the Navy had not been properly V&amp;V’d.&nbsp;I don’t know this for sure. However it happened, the model led to a design that didn’t work.</p>
<p>To understand <em>how</em> it didn’t work, back to Brick’s article. When an aircraft performing an arrestment lands, its main and/or nose tires hit the wire first. This starts a wave in the wire that moves away from the tire. Here’s an example, the nose gear has just hit the wire.</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-2195312844.png"></p>
<p>Now the aircraft continues to move forward. We have a wave propagating in both directions along the wire, causing the wire to lay mostly flat against the deck. <img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-3079080065.png"> The wire dynamics model said that what should happen next is that the wire rebounds off the deck. But what actually happened is that the wire stayed down and the hook skips over the wire. <img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-1366193013.png"> What isn’t shown is how FAST the hook would rebound into the jet. The readings were so high that the test team believed the accelerometer measuring hook up-swing was broken, but it wasn’t.</p>
<p>We tried, and tried, and tried over and over again to get the hook to work. While we were at Lakehurst, the program had a few days where we were forbidden from flying due to an IPP issue, but we kept trying roll-in arrestments at slower and slower speeds. None were successful. The hook kept rebounding at very high angular accelerations, damaging our very fragile tailhook instrumentation.</p>
<p>Many of my memories of August 2011 are of sitting on the concrete floor of a clamshell near the old F-8/F-14 engineering building, cleaning up damaged wiring and tediously calibrating rotational position sensors on the tailhook. All usually done late at night.</p>
<p>Hurricane Irene came through late in the month and made a direct line for us at Lakehurst. I came in to work one morning only to turn around and go back to my hotel and leave for home; the jet was sent back to PAX, ending our frustrating trials.</p>
<p>It was clear that the hook needed a redesign. The new hook has much stronger hold down damper, an all new damper for the upstroke, a better lateral limiter, and improved instrumentation that didn’t break on every arrestment. Here’s the new hook:</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-1416251102.png"></p>
<p>We also got an all new hook shoe! This one looks much more like a scoop, to catch the wire even if it’s down low on the deck. The new hook profile in red, old in blue, the arresting cable shown in purple:</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-1418549673.png"></p>
<p>Now, program note: for personal circumstances, I left the F-35 program entirely in mid 2012 only to return a little less than two years later to the same job. So I wasn’t there to see this in 2013:</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-3587626872.png"></p>
<p>The story most definitely does NOT end here though, because we had much struggle to go. We kept having issues with the bearing that the pitch pivot pin sits within. The pin is supposed to last at least 25 arrestments, but we were getting 1-2 before the pin sleeve would gall. We had a position sensor in that pin/sleeve to directly tell the control room the hook angular position during test, and all this pivot pin replacement meant I and a few other engineers spent a LOT of time working on CF-03’s tailhook.</p>
<p>During my years on the F-35 flight test program, I usually worked night shift engineering. Night shift had next to no meetings, it was pure flight support, problem solving, and performing instrumentation preflights for the next day’s test. Me and another engineer almost always worked together on the mission systems jets. CF-03 and its tailhook (as well as its launch bar and gear) instrumentation needed so much care and feeding that one of us would work just on CF-03. The other would take both BF-04 and BF-05.</p>
<p>We worked out some tricks for being able to replace a pitch pivot pin without having to re-perform a synchro (rotational position sensor) calibration. Why? because new calibrations mean new instrumentation projects, which took several days and required a display check. Engineers, saving your program time and money out of the sheer laziness of not wanting to make a new XML format for an instrumentation project. This is how progress is made in the world, I guess. Most of 2015 saw me paying more attention to instrumentation project efficiency (something that, I am pleasantly surprised to see, gets a 1 paragraph mention in “From Concept to Cockpit”) but in 2016 I returned to Lakehurst with CF-03 to perform the trickiest tailhook tests.</p>
<p>We were doing tests with full external weapons (modulo not having AIM-9X on the outer wing stations…the wing structure needed a fix) and trying for off-center arrestments.</p>
<p>One cloudy day in May 2016 we try for an arrestment as far off centerline as we can go.</p>
<p>Here’s the story of that day.</p>
<p>I am sitting in something called the MITS (Mobile Instrumentation/Telemetry System). It’s literally a giant 18 wheeler trailer the Navy made that’s a mini control room. I am watching my instrumentation health screen and listening to the test audio, while watching the video on a television in the trailer.</p>
<p>An F-35C coming in for landing is usually at about 11 to 13 degrees nose high so that when it hits the carrier deck, it looks something like this:</p>
<p><img src="https://the-engi-nerd.github.io/posts/welcome/images/clipboard-2481399230.png"></p>
<p>Everything seems normal at first. I hear our pilot call the ball. But then I see on the screen that he’s actually coming in almost flat, and sinking like a stone. I expect to hear “WAVE OFF” from the fellow pilot monitoring him on the ground, acting as the Landing Signal Officer. I don’t hear that. And horrified, I watch on the video screen as the F-35 hits the ground with all three tires at once, then bounces off the runway, hits the runway again. The control room is full of engineers muttering curses under their breaths and staring at their screens.</p>
<p>Through the hotmic channel we have recording everything our pilot says and hears, I hear him yell “FUCK!” but he keeps control somehow and gets back in the air. “Lightning seven three is airborne, going back around” he says on the radio. Someone in the control room says hat we have violated a flight test continuation criterion: we hit so hard that all the landing gear bottomed out. The lead test engineer gets on the radio and says “We need a gentle flared landing.” The pilot replies, “My body needs a gentle flared landing.”</p>
<p>Fortunately, once we come back around for a conventional landing, there is no further difficulty. But now, the post-flight debrief we we have to talk about what happened and decide what to do next.</p>
<p>An immediate review of the data shows that the landing was at such a high sink rate that it is as if we have taken the F-35C and dropped it from a height of 20 feet. We hit almost entirely flat, just 2 degrees angle of attack.</p>
<p>It turns out that somehow the fresnel lens optical landing system (FLOLS) we are using was not set properly. Thus when the pilot thought he was flying on glideslope, he was flying a much too steep path. The LSO did not realize what was happening until it was too late.</p>
<p>Much consultation has to be done with the leaders of the program at Fort Worth. The day is late, the jet is impounded by the crew chief and no one is to work on it until formal inspection criteria are created. They arrive the next morning in my email, 200+ steps that I must conduct just on my flight test instrumentation hardware. Other maintainers and engineers have their own thick inspection stacks. It takes three days. We discover that we need new landing gear, but that can’t happen at Lakehurst. ortunately it looks like the gear is good enough to support one more takeoff/landing cycle, so back to PAX it goes. The program decides to officially stop trying to chase the off-center arrrestments and wire only arrestments.</p>
<p>Thus ends the tailhook test saga.</p>



</main> <!-- /main -->

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Botanical gardens can cool city air by an average of 5°C (200 pts)]]></title>
            <link>https://newatlas.com/environment/surrey-cooling-effects-green-spaces-waterways/</link>
            <guid>39524164</guid>
            <pubDate>Tue, 27 Feb 2024 13:59:45 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://newatlas.com/environment/surrey-cooling-effects-green-spaces-waterways/">https://newatlas.com/environment/surrey-cooling-effects-green-spaces-waterways/</a>, See on <a href="https://news.ycombinator.com/item?id=39524164">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Temperatures around the world are <a href="https://newatlas.com/science/hottest-summer-record-2023-nasa/" data-cms-ai="0">on the rise</a>, with 2023 recently confirmed as the <a href="https://newatlas.com/environment/2023-hottest-year-on-record/" data-cms-ai="0">hottest since records began</a>. A new study has found that bringing nature into cities could help lower temperatures during heatwaves.</p><p>If you're lucky enough to live near forested areas, you'll know that one of the best ways to escape the mid-day heat while out rambling is to head for tree cover. Living in a concrete jungle might present fewer options than being out in the sticks, but even a visit to a local park or botanical garden could help you keep your cool.</p><p>In fact, research led by the University of Surrey in the UK has found that botanical gardens can lower the temperature of inner city air by as much as 5 °C. Wetlands and rain gardens are not far behind in the cooling stakes, at 4.7 and 4.5 °C respectively, trees planted along streets also lowered air temps by 3.8 °C while city parks managed 3.2 °C.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="The potential cooling effects of GBGI infrastructure across 10 categories selected in the study" width="996" height="653" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/693cf14/2147483647/strip/true/crop/996x653+0+0/resize/440x288!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 440w,https://assets.newatlas.com/dims4/default/e4cd2cb/2147483647/strip/true/crop/996x653+0+0/resize/800x524!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 800w,https://assets.newatlas.com/dims4/default/2245c57/2147483647/strip/true/crop/996x653+0+0/resize/1200x787!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 1200w,https://assets.newatlas.com/dims4/default/e8c2679/2147483647/strip/true/crop/996x653+0+0/resize/1920x1259!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/18d46a4/2147483647/strip/true/crop/996x653+0+0/resize/996x653!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/693cf14/2147483647/strip/true/crop/996x653+0+0/resize/440x288!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 440w,https://assets.newatlas.com/dims4/default/e4cd2cb/2147483647/strip/true/crop/996x653+0+0/resize/800x524!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 800w,https://assets.newatlas.com/dims4/default/2245c57/2147483647/strip/true/crop/996x653+0+0/resize/1200x787!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 1200w,https://assets.newatlas.com/dims4/default/e8c2679/2147483647/strip/true/crop/996x653+0+0/resize/1920x1259!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg 1920w" src="https://assets.newatlas.com/dims4/default/18d46a4/2147483647/strip/true/crop/996x653+0+0/resize/996x653!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F40%2Fb0%2Fb1e7462e421faa42df63e8454a8a%2F1-s2.0-S2666675824000262-fx1_lrg.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">The potential cooling effects of GBGI infrastructure across 10 categories selected in the study</figcaption><p>Prashant Kumar et al</p></div>
    
</figure>

                
            </div><p>"We have known for some time that green spaces and water can cool cities down," said Professor Prashant Kumar, founding director of the Global Center for Clean Air Research at the University of Surrey, as well as professor and chair in Air Quality and Health, and co-director at the Institute for Sustainability. "However, this study provides us the most comprehensive picture yet. What's more – we can explain why. From trees providing shade, to evaporating water cooling the air."</p><p>The paper notes that an air temperature of 40.3 °C (104.5 °F) broke records in the UK on July 19, 2022, some 62,862 deaths were linked to summer heat across Europe in the same year while the 2003 heatwave in Europe led to an economic loss of €16 billion thanks to drought and crop failures. The team of 29 scientists from the UK, Australia, Brazil, China, Hong Kong and the US also says that the IPCC reckons that "green and blue urban infrastructure elements are particularly effective in reducing air temperatures in cities."</p><p>From a pool of more than 27,000 research papers, the researchers selected 202 for meta-analysis based on a number of urban green-blue-grey infrastructure categories – including parks, engineered greening projects, wetlands, green walls, parks and botanical gardens.</p><p>Trees and plants, for example, help reduce heat by reducing the amount of direct sunlight reaching the ground, while also releasing moisture into the air. Water bodies cool the surrounding environment via "<a href="https://en.wikipedia.org/wiki/Evapotranspiration" target="_blank" data-cms-ai="0">evapotranspiration</a>, shading, the <a href="https://en.wikipedia.org/wiki/Albedo" target="_blank" data-cms-ai="0">albedo effect</a>, groundwater recharge and temperature buffering" and could also serve as heatsinks, cooling during daylight hours and offering warming potential at night. Green roofs and walls not only help insulate buildings, but also reduce heat absorption, and vegetation can serve as windbreaks for natural ventilation.</p><div data-align-center="">
                
                    <figure>
    
    
    
    


<p><img alt="Table showing the cooling effects of several green spaces and waterways" width="1396" height="687" data-image-size="articleImage" loading="lazy" data-srcset="https://assets.newatlas.com/dims4/default/14acb79/2147483647/strip/true/crop/1396x687+0+0/resize/440x217!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 440w,https://assets.newatlas.com/dims4/default/30fb544/2147483647/strip/true/crop/1396x687+0+0/resize/800x394!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 800w,https://assets.newatlas.com/dims4/default/075c8ea/2147483647/strip/true/crop/1396x687+0+0/resize/1200x591!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 1200w,https://assets.newatlas.com/dims4/default/9ae51da/2147483647/strip/true/crop/1396x687+0+0/resize/1920x945!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 1920w" data-src="https://assets.newatlas.com/dims4/default/0162ba8/2147483647/strip/true/crop/1396x687+0+0/resize/1396x687!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg" sizes="(min-width: 1240px) 800px, (min-width: 1024px) 95vw, 100vw" srcset="https://assets.newatlas.com/dims4/default/14acb79/2147483647/strip/true/crop/1396x687+0+0/resize/440x217!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 440w,https://assets.newatlas.com/dims4/default/30fb544/2147483647/strip/true/crop/1396x687+0+0/resize/800x394!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 800w,https://assets.newatlas.com/dims4/default/075c8ea/2147483647/strip/true/crop/1396x687+0+0/resize/1200x591!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 1200w,https://assets.newatlas.com/dims4/default/9ae51da/2147483647/strip/true/crop/1396x687+0+0/resize/1920x945!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg 1920w" src="https://assets.newatlas.com/dims4/default/0162ba8/2147483647/strip/true/crop/1396x687+0+0/resize/1396x687!/quality/90/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2F9a%2Ff4%2F33d5aa8f4112942745dea2f15e28%2Ftable.jpg">
</p>



    
    

    
        <div><figcaption itemprop="caption">Table showing the cooling effects of several green spaces and waterways</figcaption><p>University of Surrey</p></div>
    
</figure>

                
            </div><p>We've already seen a number of <a href="https://newatlas.com/architecture/la-serre-mvrdv/" data-cms-ai="0">architecture projects</a> <a href="https://newatlas.com/architecture/wonderwoods-vertical-forest/" data-cms-ai="0">around the world</a> employ <a href="https://newatlas.com/architecture/hangzhou-oil-refinery-factory-park/" data-cms-ai="0">large amounts</a> of <a href="https://newatlas.com/architecture/vincent-callebaut-green-line/" data-cms-ai="0">greenery</a> to both enliven facades and promote local cooling, along with beneficial landscaping. The researchers conclude by stating that "all urban green-blue-grey infrastructure types provide cooling benefits" and that "nature needs to be brought back into densifying and expanding cities and any opportunity to expand plant cover on the ground, podiums, wall and roofs must be taken."</p><p>But they also recognize that there isn't one simple solution to suit every locale, and much will depend on effective planning – "before selecting suitable urban green-blue-grey infrastructure interventions, it is necessary to assess the local context, environmental conditions, available resources and the budget to ensure their long-term effectiveness and avoid <a href="https://newatlas.com/trees-increase-smog-ozone-heat-wave/49607/" data-cms-ai="0">possible drawbacks</a>."</p><p>"Our paper confirms just how many ways there are to keep cool," added Professor Maria de Fatima Andrade of the Atmospheric Sciences Department at the University of Sao Paulo, Brazil. "But it also reveals how much work is left to do. Institutions around the world need to invest in the right research – because what’s very clear from our study is that there is no one-size-fits-all solution. It depends on what works for your community."</p><p>The study is open access via the journal <i><a href="https://www.sciencedirect.com/science/article/pii/S2666675824000262?via%3Dihub" target="_blank" data-cms-ai="0">The Innovation</a></i>.</p><p>Source: <a href="https://www.surrey.ac.uk/news/wetlands-parks-and-even-botanical-gardens-among-best-ways-cool-cities-during-heatwaves" target="_blank" data-cms-ai="0">University of Surrey</a></p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Boeing missing key elements of safety culture: FAA report (246 pts)]]></title>
            <link>https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report</link>
            <guid>39523813</guid>
            <pubDate>Tue, 27 Feb 2024 13:30:24 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report">https://www.ainonline.com/aviation-news/air-transport/2024-02-26/boeing-missing-key-elements-safety-culture-faa-report</a>, See on <a href="https://news.ycombinator.com/item?id=39523813">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-grid-variation="default" data-variation="more-in"><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-27/utah-medevac-program-aims-save-stricken-service-dogs"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Utah Medevac Program Aims To Save Stricken Service Dogs</h2><h3>It represents the third program of its kind in the U.S.</h3><p>Rotorcraft</p></div><p><img alt="K9 handlers with their charges by helicopter" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/362%20Intermountain%20Life%20Flight%20Canine.jpeg?h=413144ff&amp;itok=rv2wmtRm"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-26/argus-prism-sms-success-and-2024-helicopter-market"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Argus: Prism SMS Success and the 2024 Helicopter Market</h2><h3>Global helicopter activity was up almost 20 percent in 2023</h3><p>Rotorcraft</p></div><p><img alt="Airbus Heli" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf- 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf- 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Airbus-Heli_CDPH-7409-00057R.jpg?h=2361f00c&amp;itok=ztr84gf-"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-26/vita-aerospace-unveiling-rapid-extraction-device"><div data-sponsored="false" data-variation="media-top" data-blurred="true"><div><h2>Vita Aerospace Unveiling Rapid Extraction Device</h2><h3>The new device enables safer hoisting of unconscious victims</h3><p>Rotorcraft</p></div><p><img alt="Vita Pelican Rapid Extraction Device " loading="lazy" width="885" height="500" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=1080,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_885x500/public/2024-02/334web--HAI24_VitaAerospaceRED_mrosales_1595.jpg?h=aecc625a&amp;itok=fzlytipR"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-25/airbus-seeks-boost-output-and-performance"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Airbus Seeks To Boost Output and Performance</h2><h3>Rotorcraft accounted for the strongest earnings growth across the Airbus group</h3><p>Rotorcraft</p></div><p><img alt="Airbus Helicopters" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/EXPH-2142-003.jpg?h=9e7ae2bd&amp;itok=2txfqylj"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-23/union-balks-bringing-super-pumas-back-north-sea"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Union Balks at Bringing Super Pumas Back to North Sea</h2><h3>Calls it an 'insult' to the memory of 33 killed</h3><p>Safety</p></div><p><img alt="Airbus" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screenshot%202024-02-23%20at%208.55.43%20AM.png?h=7d56c686&amp;itok=vPoJKcAX"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-22/ainsight-bucket-list-professional-pilots"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><h2>AINsight: The Bucket List for Professional Pilots</h2><h3>Trying out new ways to fly is a sure way to recharge a pilot's batteries</h3><p>Training and Workforce</p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/business-aviation/2024-02-21/transport-canada-reissues-shoulder-harness-exemption"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Transport Canada Reissues Shoulder Harness Exemption</h2><h3>Pilots can fly without wearing shoulder harnesses above 10,000 feet</h3><p>Safety</p></div><p><img alt="pilots shot" loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/uploads/2022/01/334_pilots-ipadweb.jpg?h=cc766518&amp;itok=lKTqMgxi"></p></div></a></div><div><a href="https://www.ainonline.com/aviation-news/general-aviation/2024-02-21/firefighting-radio-system-now-available-latin-america"><div data-sponsored="false" data-variation="media-top" data-blurred="false"><div><h2>Firefighting Radio System Now Available in Latin America</h2><h3>First orders set to ship in six to eight weeks</h3><p>Avionics</p></div><p><img alt="A mockup of the new AEM radio." loading="lazy" width="700" height="395" decoding="async" data-nimg="1" srcset="https://www.ainonline.com/cdn-cgi/image/width=750,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD 1x, https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD 2x" src="https://www.ainonline.com/cdn-cgi/image/width=1920,format=webp/https://backend.ainonline.com/sites/default/files/styles/fpsc_700x395/public/2024-02/Screen%20Shot%202024-02-21%20at%201.56.30%20PM.png?h=2bc11df2&amp;itok=awBANAeD"></p></div></a></div></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Ant Geopolitics (158 pts)]]></title>
            <link>https://aeon.co/essays/the-strange-and-turbulent-global-world-of-ant-geopolitics</link>
            <guid>39523495</guid>
            <pubDate>Tue, 27 Feb 2024 12:57:43 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://aeon.co/essays/the-strange-and-turbulent-global-world-of-ant-geopolitics">https://aeon.co/essays/the-strange-and-turbulent-global-world-of-ant-geopolitics</a>, See on <a href="https://news.ycombinator.com/item?id=39523495">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>It is a familiar story: a small group of animals living in a wooded grassland begin, against all odds, to populate Earth. At first, they occupy a specific ecological place in the landscape, kept in check by other species. Then something changes. The animals find a way to travel to new places. They learn to cope with unpredictability. They adapt to new kinds of food and shelter. They are clever. And they are <em>aggressive</em>.</p>
<p>In the new places, the old limits are missing. As their population grows and their reach expands, the animals lay claim to more territories, reshaping the relationships in each new landscape by eliminating some species and nurturing others. Over time, they create the largest animal societies, in terms of numbers of individuals, that the planet has ever known. And at the borders of those societies, they fight the most destructive within-species conflicts, in terms of individual fatalities, that the planet has ever known.</p>
<p>This might sound like our story: the story of a hominin species, living in tropical Africa a few million years ago, becoming global. Instead, it is the story of a group of ant species, living in Central and South America a few hundred years ago, who spread across the planet by weaving themselves into European networks of exploration, trade, colonisation and war – some even stowed away on the 16th-century Spanish galleons that carried silver across the Pacific from Acapulco to Manila. During the past four centuries, these animals have globalised their societies alongside <span>our own.</span></p>
<figure><img alt="" loading="lazy" width="1080" height="1080" decoding="async" data-nimg="1" srcset="https://images.aeonmedia.co/user_image_upload/2864/insert-98657943-invicta51.jpg?width=1080&amp;quality=75&amp;format=auto 1x, https://images.aeonmedia.co/user_image_upload/2864/insert-98657943-invicta51.jpg?width=3840&amp;quality=75&amp;format=auto 2x" src="https://images.aeonmedia.co/user_image_upload/2864/insert-98657943-invicta51.jpg?width=3840&amp;quality=75&amp;format=auto"><figcaption><p>A polygyne population of red imported fire ants at Brackenridge Field. Austin, Texas, USA. Photo by Alexander Wild</p></figcaption></figure>
<p>It is tempting to look for parallels with human empires. Perhaps it is impossible <em>not</em> to see rhymes between the natural and human worlds, and as a science journalist I’ve contributed more than my share. But just because words rhyme, it doesn’t mean their definitions align. Global ant societies are not simply echoes of human struggles for power. They are something new in the world, existing at a scale we can measure but struggle to grasp: there are roughly 200,000 times more ants on our planet than the <span>100 billion</span> stars in the <span>Milky Way.</span></p>
<p>In late 2022, colonies of the most notorious South American export, the red fire ant (<em>Solenopsis invicta</em>) were unexpectedly <a href="https://www.sciencedirect.com/science/article/abs/pii/S0960982223009740" target="_blank" rel="noreferrer noopener">found</a> in Europe for the first time, alongside a river estuary close to the Sicilian city of Syracuse. People were shocked when a total of 88 colonies were eventually located, but the appearance of the red fire ant in Europe shouldn’t be a surprise. It was entirely predictable: another ant species from <span><em>S invicta</em></span><span>’s</span> native habitats in South America had already found its way to Europe.</p>
<p>What is surprising is how poorly we still understand global ant societies: there is a science-fiction epic going on under our feet, an alien geopolitics being negotiated by the <span>20 quadrillion</span> ants living on Earth today. It might seem like a familiar story, but the more time I spend with it, the less familiar it seems, and the more I want to resist relying on human analogies. Its characters are strange; its scales hard to conceive. Can we tell the story of global ant societies without simply retelling our own story?</p>
<p><span>S</span>ome animal societies hold together because their members recognise and remember one another when they interact. Relying on memory and experience in this way – in effect, trusting only friends – limits the size of groups to their members’ capacity to sustain personal relationships with one another. Ants, however, operate differently by forming what the ecologist Mark Moffett <a href="https://academic.oup.com/beheco/article/23/5/925/232024" target="_blank" rel="noreferrer noopener">calls</a> ‘anonymous societies’ in which individuals from the same species or group can be expected to accept and cooperate with each other even when they have never met before. What these societies depend on, Moffett writes, are ‘shared cues recognised by all its members’.</p>
<p>Recognition looks very different for humans and insects. Human society relies on networks of reciprocity and reputation, underpinned by language and culture. Social insects – ants, wasps, bees and termites – rely on chemical badges of identity. In ants, this badge is a blend of waxy compounds that coat the body, keeping the exoskeleton watertight and clean. The chemicals in this waxy blend, and their relative strengths, are genetically determined and variable. This means that a newborn ant can quickly learn to distinguish between nest mates and outsiders as it becomes sensitive to its colony’s unique scent. Insects carrying the right scent are fed, groomed and defended; those with the wrong one are rejected or fought.</p>
<p>Colonies spread without ever drawing boundaries because workers treat all of their own kind as allies</p>
<p>The most successful invasive ants, including the tropical fire ant (<em>Solenopsis geminata</em>) and red fire ant <span>(</span><span><em>S invicta</em></span><span>),</span> share this quality. They also share social and reproductive traits. Individual nests can contain many queens (in contrast to species with one queen per nest) who mate inside their home burrows. In single-queen species, newborn queens leave the nest before mating, but in unicolonial species, mated queens will sometimes leave their nest on foot with a group of workers to set up a new nest nearby. Through this budding, a network of allied and interconnected colonies begins to grow.</p>
<p>In their native ranges, these multi-nest colonies can <a href="https://www.jstor.org/stable/4095293" target="_blank" rel="noreferrer noopener">grow</a> to a few hundred metres across, limited by physical barriers or other ant colonies. This turns the landscape to a patchwork of separate groups, with each chemically distinct society fighting or avoiding others at their borders. Species and colonies coexist, without any prevailing over the others. However, for the ‘anonymous societies’ of unicolonial ants, as they’re known, transporting a small number of queens and workers to a new place can cause the relatively stable arrangement of groups to break down. As new nests are created, colonies bud and spread without ever drawing boundaries because workers treat all others of their own kind as allies. What was once a patchwork of complex relationships becomes a simplified, and unified, social system. The relative genetic homogeneity of the small founder population, replicated across a growing network of nests, ensures that members of unicolonial species tolerate each other. Spared the cost of fighting one another, these ants can live in denser populations, spreading across the land as a plant might, and turning their energies to capturing food and competing with other species. Chemical badges keep unicolonial ant societies together, but also allow those societies to rapidly expand.</p>
<p><span>A</span>ll five of the ants included in the International Union for the Conservation of Nature’s (IUCN) <a href="https://www.iucngisd.org/gisd/100_worst.php" target="_blank" rel="noreferrer noopener">list</a> of 100 of the world’s worst invasive alien species are unicolonial. Three of these species – the aforementioned red fire ant <span>(</span><span><em>S invicta</em></span><span>),</span> the Argentine ant (<em>Linepithema humile</em>) and the little fire ant (<em>Wasmannia auropunctata</em>) – are originally from Central and/or South America, where they are found sharing the same landscapes. It is likely that the first two species, at least, began their global expansion centuries ago on ships out of Buenos Aires. Some of these ocean journeys might have lasted longer than a single worker ant’s lifetime.</p>
<p>Unicolonial ants are superb and unfussy scavengers that can hunt animal prey, eat fruit or nectar, and tend insects such as aphids for the sugary honeydew they excrete. They are also adapted to living in regularly disrupted environments, such as river deltas prone to flooding (the ants either get above the waterline, by climbing a tree, for example, or gather into living rafts and float until it subsides). For these ants, disturbance is a kind of environmental reset during which territories have to be reclaimed. Nests – simple, shallow burrows – are abandoned and remade at short notice. If you were looking to design a species to invade cities, suburbs, farmland and any wild environment affected by humans, it would probably look like a unicolonial ant: a social generalist from an unpredictable, intensely competitive environment.</p>
<p>When these ants show up in other places, they can make their presence felt in spectacular fashion. An early example comes from the 1850s, when the big-headed ant (<em>Pheidole megacephala</em>), another species now listed on the IUCN’s <span>top 100,</span> found its way from Africa to the Madeiran capital of Funchal. ‘You eat it in your puddings, vegetables and soups, and wash your hands in a decoction of it,’ complained one British visitor in 1851. When the red fire ant <span>(</span><span><em>S invicta</em></span><span>),</span> probably the best-known unicolonial species, spread through the US farming communities surrounding the port of Mobile, Alabama in the 1930s, it wreaked havoc in different ways. ‘Some farmers who have heavily infested land are unable to hire sufficient help, and are forced to abandon land to the ants,’ was how <span>E O Wilson</span> in 1958 <a href="https://www.jstor.org/stable/24940940" target="_blank" rel="noreferrer noopener">described</a> the outcome of their arrival. Today, the red fire ant does billions of dollars of damage each year and inflicts its agonising bite on millions of people. But the largest colonies, and most dramatic moments in the global spread of ant societies, belong to the Argentine ant <span>(</span><span><em>L humile</em></span><span>).</span></p>
<p>New Zealand is the only country to have prevented the spread of the red fire ant</p>
<p>Looking at the history of this species’ expansion in the late 19th and early <span>20th centuries,</span> it can seem as if the spread of global trade was an Argentine ant plot for world domination. One outbreak <a href="https://link.springer.com/chapter/10.1007/978-3-319-74986-0_4" target="_blank" rel="noreferrer noopener">appeared</a> in Porto, following the 1894 Exhibition of the Islands and Colonies of Portugal. The insects had likely travelled on produce and wares displayed at the exhibition from Madeira – ornamental plants, which tend to travel with a clump of their home soil, are particularly good for transporting invasive species. In 1900, a Belfast resident, Mrs Corry, found a ‘dark army’ of the same species crossing her kitchen floor and entering the larder, where they covered a leg of mutton so completely that ‘one could scarcely find room for a pin-point’. In 1904, the US Bureau of Entomology dispatched a field agent, Edward Titus, to <a href="https://www.biodiversitylibrary.org/item/132322#page/81/mode/1up" target="_blank" rel="noreferrer noopener">investigate</a> a plague of Argentine ants in New Orleans. He heard reports of the ants crawling into babies’ mouths and nostrils in such numbers that they could be dislodged only by repeatedly dunking the infant in water. Other reports described the ants entering hospitals and ‘busily carrying away the sputum’ from a tuberculosis patient. When the species arrived on the French Riviera a few years later, holiday villas were abandoned and a children’s hospital was evacuated.</p>
<p>In December 1927, Italy’s king Vittorio <span>Emmanuel III </span>and its prime minister Benito Mussolini signed a law setting out the measures to be taken against the Argentine ant, splitting the cost equally with invaded provinces. The state’s effectiveness, or lack of it, is shown in the novella <em>The Argentine Ant</em> (1952) by Italo Calvino, one of Italy’s great postwar writers. Calvino, whose parents were plant biologists, sets his tale in an unnamed seaside town much like the one where he grew up, in the northwestern province of Liguria. The ant has outlasted both Mussolini and the monarchy, and saturates the unnamed town, burrowing underground (and into people’s heads). Some residents drench their houses and gardens with pesticides or build elaborate traps involving hammers covered in honey; others try to ignore or deny the problem. And then there is Signor Baudino, an employee of the Argentine Ant Control Corporation, who has spent <span>20 years</span> putting out bowls of molasses laced with a weak dose of poison. The locals suspect him of feeding the ants to keep himself in <span>a job.</span></p>
<p>In reality, people who found themselves living in the path of such ant plagues learned to stand the feet of their cupboards, beds and cots in dishes of kerosene. However, this was not a long-term solution: killing workers away from the nest achieves little when most, along with their queens, remain safe at home. Slower-acting insecticides (like Baudino’s poison), which workers take back to the nest and feed to queens, can be more effective. But because unicolonial workers can enter any number of nests in their network, each containing many queens, the chances of delivering a fatal dose gets much slimmer.</p>
<p>In the early 20th century, an intensive period in the human war against ants, pest-control researchers advocated using broad-spectrum poisons, most of which are now banned for use as pesticides, to set up barriers or fumigate nests. Nowadays, targeted insecticides can be effective for clearing relatively small areas. This has proved useful in orchards and vineyards (where the ants’ protection of sap-sucking insects makes them a hazard to crops) and in places such as the Galápagos or Hawaii <a href="https://bioone.org/journals/florida-entomologist/volume-88/issue-2/0015-4040_2005_088_0159_EOTLFA_2.0.CO_2/ERADICATION-OF-THE-LITTLE-FIRE-ANT-WASMANNIA-AUROPUNCTATA-HYMENOPTERA/10.1653/0015-4040(2005)088%5b0159:EOTLFA%5d2.0.CO;2.full" target="_blank" rel="noreferrer noopener">where</a> the ants <a href="https://link.springer.com/article/10.1023/B:BINV.0000010121.45225.cc" target="_blank" rel="noreferrer noopener">threaten</a> rare species. Large-scale eradications are a different matter, and few places have tried. New Zealand, the world leader in controlling invasive species, is the only country to have prevented the spread of the red fire ant, mostly by eradicating nests on goods arriving at airports and ports. The country is also home to a spaniel trained to sniff out Argentine ants nests and prevent the insects from reaching small islands important for seabirds.</p>
<p><span>H</span>uman inconvenience pales in comparison with the ants’ effects on other species. Exploring the countryside around New Orleans in 1904, Titus found the Argentine ant overwhelming the indigenous ant species, bearing away the corpses, eggs and larvae of the defeated to be eaten: ‘column after column of them arriving on the scene of battle’. Other entomologists at the time learned to recognise the disappearance of native ants as a sign of an invader’s arrival. Unicolonial species are aggressive, quick to find food sources and tenacious in defending and exploiting them. Unlike many ant species, in which a worker who finds a new food source returns to the nest to recruit other foragers, the Argentine ant <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0070888" target="_blank" rel="noreferrer noopener">enlists</a> other workers already outside the nest, thus recruiting foragers more quickly. However, the decisive advantage of unicolonial ant species lies in their sheer force of numbers, which is usually what decides ant conflicts. They often become the only ant species in invaded areas.</p>
<p>The effects of these invasions cascade through ecosystems. Sometimes, the damage is direct: on the Galápagos, fire ants prey on tortoise hatchlings and bird chicks, threatening their survival. In other cases, the damage falls on species that once relied on native ants. In California, the tiny Argentine ant (typically under <span>3 mm</span> long) has <a href="https://www.jstor.org/stable/2641040" target="_blank" rel="noreferrer noopener">replaced</a> the larger native species that once formed the diet of horned lizards, leaving the reptiles starving – it seems they do not recognise the much smaller invader as food. In the scrublands of the South African fynbos heathland, which has some of the most distinctive flora on Earth, many plants <a href="https://www.jstor.org/stable/1938311" target="_blank" rel="noreferrer noopener">produce</a> seeds bearing a fatty blob. Native ants ‘plant’ the seeds by carrying them into their nests, where they eat the fat and discard the rest. Argentine ants – almost certainly imported to South Africa around 1900 along with horses shipped from Buenos Aires by the British Empire to fight the Boer War – either ignore the seeds, leaving them to be eaten by mice, or strip the fat where it lies, leaving the seed on the ground. This makes it harder for endemic flora such as proteas to reproduce, tipping the balance towards invasive plants such as acacias and eucalypts.</p>
<p>In the past 150 years, the Argentine ant has spread to pretty much everywhere that has hot, dry summers and cool, wet winters. A single supercolony, possibly descended from as few as half a dozen queens, now <a href="https://www.pnas.org/doi/10.1073/pnas.092694199" target="_blank" rel="noreferrer noopener">stretches</a> along 6,000 kilometres of coastline in southern Europe. Another runs most of the length of California. The species has arrived in South Africa, Australia, New Zealand and Japan, and even <a href="https://www.antweb.org/description.do?rank=species&amp;genus=linepithema&amp;name=humile" target="_blank" rel="noreferrer noopener">reached</a> Easter Island in the Pacific and <span>St Helena</span> in the Atlantic. Its allegiances span oceans: workers from different continents, across millions of nests containing trillions of individuals, will <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3352483/" target="_blank" rel="noreferrer noopener">accept</a> each other as readily as if they had been born in the same nest. Workers of the world united, indeed. But not <em>completely</em> united.</p>
<p>As with inbred species everywhere, this may make them prone to disease</p>
<p>Expanding in parallel with the world-spanning supercolony are separate groups of the Argentine ant that bear different chemical badges – the legacy of other journeys from the homeland. Same species, different ‘smells’. In places where these distinct colonies come into contact, hostilities resume.</p>
<p>In Spain, one such colony holds a stretch of the coast of Catalonia. In Japan, four mutually hostile groups fight it out <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1472-4642.2012.00934.x" target="_blank" rel="noreferrer noopener">around</a> the port city of Kobe. The best-studied conflict zone is in southern California, a little north of San Diego, where the Very Large Colony, as the state-spanning group is known, shares a border with a separate group called the Lake Hodges colony, with a territory measuring just <span>30 kilometres</span> around. Monitoring this border for a six-month period between April and September 2004, a team of researchers <a href="https://onlinelibrary.wiley.com/doi/10.1111/j.1365-294X.2006.03038.x" target="_blank" rel="noreferrer noopener">estimated</a> that <span>15 million</span> ants died on a frontline a few centimetres wide and several kilometres long. There were times when each group seemed to gain ground, but over longer periods stalemate was the rule. Those seeking to control ant populations believe provoking similar conflicts might be a way to weaken invasive ants’ dominance. There are also hopes, for example, that artificial pheromones – chemical misinformation, in other words – might <a href="https://vcresearch.berkeley.edu/news/creating-new-trail-solve-old-problem" target="_blank" rel="noreferrer noopener">cause</a> colony mates to turn on one another, although no products have yet come to market.</p>
<p>In the very long term, the fate of unicolonial societies is unclear. A survey of Madeira’s ants between 2014 and 2021 <a href="https://jhr.pensoft.net/article/81624/" target="_blank" rel="noreferrer noopener">found</a>, contrary to fears that invasive ants would wipe the island clean of other insects, very few big-headed ants and, remarkably, no Argentine ants. Invasive ants are <a href="https://link.springer.com/article/10.1007/s10530-016-1214-2" target="_blank" rel="noreferrer noopener">prone</a> to population crashes for reasons that aren’t understood but may be related to genetic homogeneity: a single colony of Argentine ants in their homeland <a href="https://www.cell.com/trends/ecology-evolution/fulltext/S0169-5347(09)00089-5" target="_blank" rel="noreferrer noopener">contains</a> as much genetic diversity as the whole of California’s state-spanning supercolony. As with inbred species everywhere, this may make them prone to disease. Another potential issue is that the ants’ lack of discrimination about whom they help may also <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9810424/" target="_blank" rel="noreferrer noopener">favour</a> the evolution of free-riding ‘lazy workers’ in colonies, who selfishly prosper by exploiting their nest mates’ efforts. Though it’s assumed this uneven distribution of work may eventually lead to social breakdown, no examples have been found.</p>
<p>Unless natural selection turns against them, one of the most effective curbs on unicolonial ants is other unicolonial ants. In the southeastern United States, red fire ants seem to have <a href="https://pubmed.ncbi.nlm.nih.gov/15245397/" target="_blank" rel="noreferrer noopener">prevented</a> the Argentine ant forming a single vast supercolony as it has in California, instead returning the landscape to a patchwork of species. In southern Europe, however, the Argentine ant has had a century longer to establish itself, so, even if the fire ant does gain a European foothold, there’s no guarantee that the same dynamic will play out. In the southern US, red fire ants are themselves now being <a href="https://www.sciencedirect.com/science/article/pii/S0960982214003376" target="_blank" rel="noreferrer noopener">displaced</a> by the tawny crazy ant (<em>Nylanderia fulva</em>), another South American species, which has immunity to fire ant venom.</p>
<p><span>I</span>t is remarkable how irresistible the language of human warfare and empire can be when trying to describe the global history of ant expansion. Most observers – scientists, journalists, others – seem not to have tried. Human efforts to control ants are regularly described as a war, as is competition between invaders and native ants, and it is easy to see why comparisons are made between the spread of unicolonial ant societies and human colonialism. People have been drawing links between insect and human societies for millennia. But what people see says more about them than about insects.</p>
<p>A beehive is organised along similar lines to an ant nest, but human views of bee society tend to be benign and utopian. When it comes to ants, the metaphors often polarise, either towards something like communism or something like fascism – one mid-20th-century US eugenicist even used the impact of the Argentine ant as an argument for immigration control. For the entomologist Neil Tsutsui, who <a href="https://radiolab.org/podcast/226523-ants" target="_blank" rel="noreferrer noopener">studies</a> unicolonial ants at the University of California, Berkeley, insects are like Rorschach tests. Some people see his research as evidence that we should all get along, while others see the case for racial purity.</p>
<p>In addition to conflating a natural ‘is’ with a political ‘ought’, the temptations of ant anthropomorphism can also lead to a limited, and limiting, view of natural history. Surely the <a href="https://resjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-3032.1989.tb00947.x" target="_blank" rel="noreferrer noopener">habit</a> of worker ants in Argentine nests to kill nine-tenths of their queens every spring – seemingly clearing out the old to make way for the new – is enough to deter parallels between ant societies and human politics?</p>
<p>Unicolonial species can overwhelmingly alter ecological diversity when they arrive <span>somewhere new</span></p>
<p>The more I learn, the more I am struck by the ants’ strangeness rather than their similarities with human society. There is another way to be a globalised society – one that is utterly unlike our own. I am not even sure we have the language to convey, for example, a colony’s ability to take bits of information from thousands of tiny brains and turn it into a distributed, constantly updated picture of their world. Even ‘smell’ seems a feeble word to describe the ability of ants’ antennae to read chemicals on the air and on each other. How can we imagine a life where sight goes almost unused and scent forms the primary channel of information, where chemical signals show the way to food, or mobilise a response to threats, or distinguish queens from workers and the living from the <a href="https://www.pnas.org/doi/10.1073/pnas.0901270106" target="_blank" rel="noreferrer noopener">dead</a>?</p>
<p>As our world turns alien, trying to think like an alien will be a better route to finding the imagination and humility needed to keep up with the changes than looking for ways in which other species are like us. But trying to think like an ant, rather than thinking about how ants are like us, is not to say that I welcome our unicolonial insect underlords. Calamities follow in the wake of globalised ant societies. Most troubling among these is the way that unicolonial species can overwhelmingly alter ecological diversity when they arrive somewhere new. Unicolonial ants can turn a patchwork of colonies created by different ant species into a landscape dominated by a single group. As a result, textured and complex ecological communities become simpler, less diverse and, crucially, less different to each other. This is not just a process; it is an era. The current period in which a relatively small number of super-spreading animals and plants expands across Earth is sometimes called the Homogecene. It’s not a cheering word, presaging an environment that favours the most pestilential animals, plants and microbes. Unicolonial ants contribute to a more homogenous future, but they also speak to life’s ability to escape our grasp, regardless of how we might try to order and exploit the world. And there’s something hopeful about that, for the planet, if not <span>for us.</span></p>
<p>The scale and spread of ant societies is a reminder that humans should not confuse impact with control. We may be able to change our environment, but we’re almost powerless when it comes to manipulating our world exactly how we want. The global society of ants reminds us that we cannot know how other species will respond to our reshaping of the world, only that <span>they will.</span></p>
<p>If you want a parable of ants’ ability to mock human hubris, it’s hard to improve on the story of <span>Biosphere 2.</span> This giant terrarium in the Arizona desert, funded by a billionaire financier in the late 1980s, was intended as a grand experiment and model for long-distance space travel and colonisation. It was designed to be a self-sustaining living system, inhabited by eight people, with no links to the world’s atmosphere, water, soil. Except that, soon after it began operations in 1991, the black crazy ant (<em>Paratrechina longicornis</em>), a unicolonial species originally from southeast Asia, <a href="https://www.jstor.org/stable/3496865" target="_blank" rel="noreferrer noopener">found</a> a way in, reshaped the carefully engineered invertebrate community inside, and turned the place into a honeydew farm.</p>
<p>It is possible to be both a scourge and a marvel.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[SpaceX discloses cause of Starship anomalies as it clears an FAA hurdle (117 pts)]]></title>
            <link>https://arstechnica.com/space/2024/02/faa-closes-starship-inquiry-and-spacex-details-causes-of-november-accidents/</link>
            <guid>39523403</guid>
            <pubDate>Tue, 27 Feb 2024 12:48:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://arstechnica.com/space/2024/02/faa-closes-starship-inquiry-and-spacex-details-causes-of-november-accidents/">https://arstechnica.com/space/2024/02/faa-closes-starship-inquiry-and-spacex-details-causes-of-november-accidents/</a>, See on <a href="https://news.ycombinator.com/item?id=39523403">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <h4>
      RUD    —
</h4>
            
            <h2 itemprop="description">"Several engines began shutting down before one engine failed energetically."</h2>
                    </div><div itemprop="articleBody">
                                    
<figure>
  <img src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/F_VbEJcbQAAv3Ak-800x1199.jpg" alt="Starship launches on its second flight on November 18, 2023.">
      <figcaption><p><a href="https://cdn.arstechnica.net/wp-content/uploads/2023/11/F_VbEJcbQAAv3Ak-scaled.jpg" data-height="2560" data-width="1708">Enlarge</a> <span>/</span> Starship launches on its second flight on November 18, 2023.</p><p>SpaceX</p></figcaption>  </figure>

  




<!-- cache hit 345:single/related:60f3482123c7bd87bb91cca2a1c93565 --><!-- empty -->
<p>A little more than three months after the most recent launch of a Starship vehicle, which ended with both the booster and upper stage being lost in flight, the Federal Aviation Administration has closed its investigation of the mishap.</p>
<p>"SpaceX identified, and the FAA accepts, the root causes and 17 corrective actions documented in SpaceX’s mishap report," the federal agency said in a statement issued Monday. "Prior to the next launch, SpaceX must implement all corrective actions and receive a license modification from the FAA that addresses all safety, environmental and other applicable regulatory requirements."</p>
<p>SpaceX must still submit additional information to the FAA, which is responsible for the safety of people and property on the ground, before the agency completes its review of an application to launch Starship for a third time. The administrator for Commercial Space Transportation at the Federal Aviation Administration, Kelvin Coleman, <a href="https://arstechnica.com/space/2024/02/spacex-seeks-to-launch-starship-at-least-nine-times-this-year/">said last week</a> that early to mid-March is a reasonable timeline for the regulatory process to conclude.</p>
<p>A launch attempt is likely to follow soon after.</p>
<h2>What went wrong</h2>
<p>In conjunction with Monday's announcement, SpaceX released details for the first time of what happened to cause the November 18 launch to go awry.</p>
<p>In this update, SpaceX noted that the Super Heavy first stage of the rocket performed nominally, with all 33 Raptor engines on this massive rocket igniting successfully. The booster then performed a full-duration burn to reach stage separation. At this point, the upper stage executed a successful "hot staging" maneuver in which the Starship stage separated from the booster while some of the booster's engines were still firing.</p>
<p>For the Super Heavy booster, the next step was to perform a series of burns to make a soft landing in the Gulf of Mexico. As part of the initial burn, 13 of the rocket's engines were intended to fire.</p>
<p>"During this burn, several engines began shutting down before one engine failed energetically, quickly cascading to a rapid unscheduled disassembly of the booster," <a href="https://www.spacex.com/updates">SpaceX said</a>. "The vehicle breakup occurred more than three and a half minutes into the flight at an altitude of ~90 km over the Gulf of Mexico."</p>                                            
                                                        
<p>The problem was subsequently linked to a problem with supplying liquid oxygen to the Raptor engines.</p>
<p>"The most likely root cause for the booster RUD was determined to be filter blockage where liquid oxygen is supplied to the engines, leading to a loss of inlet pressure in engine oxidizer turbopumps that eventually resulted in one engine failing in a way that resulted in loss of the vehicle," the company stated. "SpaceX has since implemented hardware changes inside future booster oxidizer tanks to improve propellant filtration capabilities and refined operations to increase reliability."</p>
<h2>Starship vents</h2>
<p>As Super Heavy was experiencing these problems, the six Raptor engines on the Starship upper stage were burning nominally and pushing the vehicle along a flight path intended to take it nearly two-thirds of the way around Earth before splashing down near Hawaii. However, at about seven minutes after liftoff, a large vent of liquid oxygen occurred. There was excess liquid oxygen on the vehicle, SpaceX said, to gather data representative of future payload deployment missions. It needed to be released before Starship splashed down.</p>
<p>"A leak in the aft section of the spacecraft that developed when the liquid oxygen vent was initiated resulted in a combustion event and subsequent fires that led to a loss of communication between the spacecraft’s flight computers," the company said. "This resulted in a commanded shut down of all six engines prior to completion of the ascent burn, followed by the Autonomous Flight Safety System detecting a mission rule violation and activating the flight termination system, leading to vehicle breakup."</p>
<p>At the time, the vehicle had reached an altitude of 150 km, well into outer space, and had achieved a velocity of about 24,000 km/h. This is just short of orbital velocity, which is 28,000 km/h.</p>
<p>In its statement, SpaceX said it was implementing changes to the Super Heavy and Starship stages to account for these issues. The company is also seeking to improve the overall performance of Starship, with the addition of a new electronic Thrust Vector Control system for Starship’s upper-stage Raptor engines and more rapid propellant loading operations prior to launch.</p>
<p>SpaceX has four Starships in complete, or nearly complete, build stages. Should the next flight go smoothly, the company could begin to launch the world's largest rocket on a more frequent basis.</p>

                                                </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Easter eggs on Swiss maps (159 pts)]]></title>
            <link>https://www.atlasobscura.com/articles/swiss-map-secrets</link>
            <guid>39523187</guid>
            <pubDate>Tue, 27 Feb 2024 12:21:56 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.atlasobscura.com/articles/swiss-map-secrets">https://www.atlasobscura.com/articles/swiss-map-secrets</a>, See on <a href="https://news.ycombinator.com/item?id=39523187">Hacker News</a></p>
<div id="readability-page-1" class="page"><section id="article-body">
<p><span>Swiss humor. </span>Now there’s two words you don’t often see together. In fact, Google Trends<a href="https://trends.google.com/trends/explore?date=all&amp;geo=DK&amp;q=Swiss%20humor"> lists <em>zero</em> occurrences</a> of the phrase between 2004 and now. Even “German humor” produces a graph (albeit a rather flat one). But not only is there some evidence that Swiss comedy does exist, it might just be that being well-hidden is kind of its thing. Find it and laugh. Or don’t, and the joke’s on you!</p>
<p>That evidence, as it turns out, is cartographic. The Swiss Federal Office of National Topography,<a href="https://www.swisstopo.admin.ch/"> Swisstopo</a> for short, is a decidedly serious institution. Many serious things—time and money, for starters—depend on the accuracy of its maps. In the case of its mountain maps, actual lives hang in the balance. Yet in decades past, the austere institute’s maps have served as the canvas for a series of in-jokes among its more fun-loving cartographers.</p>
<figure><img src="https://img.atlasobscura.com/TF_h2N2cGkEVAU5ELOD91Wp4msaxylLzh8m1CevqMmc/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9kY2Q5MTk5NS02/NTg5LTQxMzQtYTlk/Zi0xNTkxZTFkM2Nh/YmQzNzc0ZmYzOGRm/NDFmZTVjNmRfc2Ft/dWVsLWZlcnJhcmEt/SUVIUEROazItOHct/dW5zcGxhc2guanBn.jpg" alt="Switzerland's geography proved to inspire map makers in surprising ways." width="auto" data-kind="article-image" id="article-image-100240" data-src="https://img.atlasobscura.com/TF_h2N2cGkEVAU5ELOD91Wp4msaxylLzh8m1CevqMmc/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9kY2Q5MTk5NS02/NTg5LTQxMzQtYTlk/Zi0xNTkxZTFkM2Nh/YmQzNzc0ZmYzOGRm/NDFmZTVjNmRfc2Ft/dWVsLWZlcnJhcmEt/SUVIUEROazItOHct/dW5zcGxhc2guanBn.jpg"><figcaption>Switzerland’s geography proved to inspire map makers in surprising ways. <a target="_blank" href="https://unsplash.com/photos/landscape-photography-of-mountain-during-daytime-IEHPDNk2-8w">Samuel Ferrara/Public Domain</a></figcaption></figure>
<p>These mapmakers played a game of wits against their superiors, the ones whose duties included checking the maps before publication. Over the years, the cartographers managed to slip in—on maps that were supposed to contain only dry topographic facts—drawings of an airplane, a fish, a marmot, a mountaineer, a face, a spider, even of a naked lady. Once discovered, these humorous additions were removed without pardon. At least, that’s how it used to be.</p>
<p>Either way, it doesn’t matter. Swisstopo is defeated by its own thoroughness. Its<a href="https://map.geo.admin.ch/"> map page</a> allows you not just to zoom in and out of the most recent maps but also to browse historical maps and thus revisit these “Easter eggs” that prove, however obliquely, the existence of a sense of humor among the mountains of Switzerland.</p>
<figure><img src="https://img.atlasobscura.com/ouhDVpuIXAo3SFvcq5PVwthqjPAkbzTO4k_6bBpsS_4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8xMjczZGVkYy01/OTZkLTRiNjMtYjJk/Zi04ZTZhNDI4ZGI0/ZDIzNzc0ZmYzOGRm/NDFmZTVjNmRfenVy/aWNoLWFpcnBvcnQu/anBn.jpg" alt="Topographic maps normally never have planes on them." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100239/image" data-kind="article-image" id="article-image-100239" data-src="https://img.atlasobscura.com/ouhDVpuIXAo3SFvcq5PVwthqjPAkbzTO4k_6bBpsS_4/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8xMjczZGVkYy01/OTZkLTRiNjMtYjJk/Zi04ZTZhNDI4ZGI0/ZDIzNzc0ZmYzOGRm/NDFmZTVjNmRfenVy/aWNoLWFpcnBvcnQu/anBn.jpg"><figcaption>Topographic maps normally never have planes on them. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The plane that disappeared—twice</strong></h3>
<p>In 1994, an anonymous cartographer at Swisstopo included an airplane in this map of Kloten, the international airport of Zürich. While it may seem only natural for airplanes to show up at airports, that is normally not the case on topographic maps.</p>
<p>The error remained undetected until a revision of the map in 2000, when the offending craft was erased. However, the plane reappeared on the 2007 map at exactly the same spot—the tarmac before Gate A—only to vanish again in 2013.</p>
<figure><img src="https://img.atlasobscura.com/mFxxMnNDB8tJkzL-zeT_e8ZOhhui_kPnDnPs2m523OY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYzVkYzliMy05/MWNiLTRhY2ItODFj/NS01NmQ4ODk4OGVj/NDgzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDAuanBn.jpg" alt="Despite having no head, the figure's filled-in lines leave little to the imagination." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100238/image" data-kind="article-image" id="article-image-100238" data-src="https://img.atlasobscura.com/mFxxMnNDB8tJkzL-zeT_e8ZOhhui_kPnDnPs2m523OY/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYzVkYzliMy05/MWNiLTRhY2ItODFj/NS01NmQ4ODk4OGVj/NDgzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDAuanBn.jpg"><figcaption>Despite having no head, the figure’s filled-in lines leave little to the imagination. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The Naked Lady of Künten</strong></h3>
<p>Possibly the oldest topographical Easter egg, and the current record holder for the longest-lasting one, is the Naked Lady of Künten. First appearing on the topographical map of 1954, the reclining figure wasn’t discovered until 2012. Admittedly, without head, arms and feet, she is hard to spot. Her odalisque-like forms are suggested by the curvature of a stream and an elongated green patch indicating vegetation.</p>
<p>The world—or at least that bit between Eggenrain and Sunnenberg—was put to right again in the 2013 edition of the local map. But it’s still easy to see how that particular distribution of topographic features could have inspired a lonely 1950s cartographer to pencil in something that wasn’t there.</p>
<figure><img src="https://img.atlasobscura.com/LEowyKb-jUX75vzlXCCd-XpNr-972ODbpHk0rXnK9ZI/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9jODliYzIzYS1l/MjZjLTRjODEtOThh/Yy0wYWVkYWE2ZjBk/ZGYzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDIuanBn.jpg" alt="Any serious federal office could not allow a stray fish on its maps." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100237/image" data-kind="article-image" id="article-image-100237" data-src="https://img.atlasobscura.com/LEowyKb-jUX75vzlXCCd-XpNr-972ODbpHk0rXnK9ZI/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9jODliYzIzYS1l/MjZjLTRjODEtOThh/Yy0wYWVkYWE2ZjBk/ZGYzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDIuanBn.jpg"><figcaption>Any serious federal office could not allow a stray fish on its maps. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>A Swiss fish in a French lake</strong></h3>
<p>It was never discovered who reshaped the aforementioned landscape feature into a female form. But the younger generation of Easter-eggers is known by name.</p>
<p>In 1980, Werner Leuenberger even went international. He drew a fish at the southern end of the <em>Lac de Remoray</em>, a small lake just across the Franco-Swiss border. The fish felt right at home among the lines marking out the area as swampy. However, it was caught five years later, and has been left off the map since 1986.</p>
<figure><img src="https://img.atlasobscura.com/B75neSF-cnLwT1-neEsUt3Kz6RdXLTNIhqArKjXXN1c/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy82ODhkNTllMy00/NjEwLTQ2ZTUtODJm/Yy00OTM1OGYxNDc2/YTQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDQuanBn.jpg" alt="The spider once aligned with a particularly dangerous part of the mountain." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100236/image" data-kind="article-image" id="article-image-100236" data-src="https://img.atlasobscura.com/B75neSF-cnLwT1-neEsUt3Kz6RdXLTNIhqArKjXXN1c/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy82ODhkNTllMy00/NjEwLTQ2ZTUtODJm/Yy00OTM1OGYxNDc2/YTQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDQuanBn.jpg"><figcaption>The spider once aligned with a particularly dangerous part of the mountain. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Attack of the giant Eiger spider</strong></h3>
<p>In 1981, Othmar Wyss inserted a spider near the top of the Eiger, one of Switzerland’s most iconic Alpine summits, at a location actually known by mountaineers as quite dangerous.</p>
<p>The giant spider survived for six years in the freezing cold. The snowfield that made up the spider’s body—and made the northern approach of the Eiger so hard—has apparently also disappeared in the intervening years.</p>
<figure><img src="https://img.atlasobscura.com/OmKdGctdFCdkTLWR-Z0xPqsbGh44cNg9vp-5d8bZwJE/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjhlMTliNy05/MWIxLTRiMzAtODgz/My1iZjE1NWRmM2Jm/MTczNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDUuanBn.jpg" alt="A face still leers on a mountain slope today." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100235/image" data-kind="article-image" id="article-image-100235" data-src="https://img.atlasobscura.com/OmKdGctdFCdkTLWR-Z0xPqsbGh44cNg9vp-5d8bZwJE/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy9mYjhlMTliNy05/MWIxLTRiMzAtODgz/My1iZjE1NWRmM2Jm/MTczNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDUuanBn.jpg"><figcaption>A face still leers on a mountain slope today. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Haunted monk trapped in a map</strong></h3>
<p>A rock formation on a slope of the Harder Kulm, a mountain near Interlaken, looks like a face. This is the <em>Hardermandli</em>, or “little Harder man.” Legend has it that he was a lecherous monk, condemned to look down on the place where he chased a girl to her death.</p>
<p>Cartographer Friedrich Siegfried extended the curse to cartography, for since 1980 and until this day, the Hardermandli also lives on the map.</p>
<figure><img src="https://img.atlasobscura.com/g2miEHJwmdwJQi70um2E7ys6xc9gdynMeJXGeqG0YQs/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8zZjRlN2JmMS1m/ZTQ0LTRlMmItYTBm/Yy03MTZkNDc5MmI2/YzQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDYuanBn.jpg" alt="Where's Waldo in Italy?" width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100234/image" data-kind="article-image" id="article-image-100234" data-src="https://img.atlasobscura.com/g2miEHJwmdwJQi70um2E7ys6xc9gdynMeJXGeqG0YQs/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8zZjRlN2JmMS1m/ZTQ0LTRlMmItYTBm/Yy03MTZkNDc5MmI2/YzQzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDYuanBn.jpg"><figcaption>Where’s Waldo in Italy? <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>Beats waiting for the Italians</strong></h3>
<p>For the 1997 map update, Mr. Siegfried etched the likeness of a mountaineer on the Italian side of a mountain slope near Val Müstair. Reportedly, he got tired of waiting on the data for the area, which his Italian counterparts were slow to provide, so he found a creative way to plug the gap. Topography, like nature, also abhors a vacuum, apparently.</p>
<p>Swisstopo seems to have taken to heart the cartographer’s slight against his Italian colleagues, because the mountaineer still appears on the contemporary map, in 1:100,000 scale at least.</p>
<figure><img src="https://img.atlasobscura.com/OmislREC2TDa1cme6jqtn7dlGEZA2d_DmVaP7eM0WMo/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYTk0YTI2NS1m/YWJiLTQ0YzctODBi/Zi03ZGYxYzU5Zjcw/OTEzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDkuanBn.jpg" alt="The marmot is the most recent Easter egg." width="auto" data-article-image-id="undefined" data-full-size-image="https://assets.atlasobscura.com/article_images/full/100233/image" data-kind="article-image" id="article-image-100233" data-src="https://img.atlasobscura.com/OmislREC2TDa1cme6jqtn7dlGEZA2d_DmVaP7eM0WMo/rt:fill/w:1200/el:1/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL2Fzc2V0/cy8yYTk0YTI2NS1m/YWJiLTQ0YzctODBi/Zi03ZGYxYzU5Zjcw/OTEzNzc0ZmYzOGRm/NDFmZTVjNmRfMjY0/MTY2NDkuanBn.jpg"><figcaption>The marmot is the most recent Easter egg. <a target="_blank" href="https://map.geo.admin.ch/">Swisstopo</a></figcaption></figure>
<h3><strong>The marmot of the Aletsch glacier</strong></h3>
<p>Swisstopo’s most famous map gag—or at least the most recent one to be revealed, in 2014—is the marmot, which has been hiding in a rock near the Aletsch glacier since it was put there by cartographer Paul Ehrlich in 2011, shortly before his retirement. The marmot is still there, and perhaps it and its fellow map oddities may be allowed to survive.</p>
<p>On its website, Swisstopo says that “these hidden drawings do not affect the accuracy and level of detail of our maps, nor on the safety and security of their users. They merely add a note of mystery to our nation’s maps.”</p>
<p>Are there any other gags hidden in the official maps of Switzerland? Swisstopo itself claims it has no knowledge of any other cartographic oddities. But knowing and not telling, that’s exactly the kind of thing they would find funny, isn’t it?</p>
<p><a href="https://bigthink.com/strange-maps/swiss-maps/"><em>This article</em></a><em> originally appeared on</em><a href="https://bigthink.com/?utm_source=syndication&amp;utm_medium=organicpartner&amp;utm_campaign=atlasobscura"> <em>Big Think</em></a><em>, home of the brightest minds and biggest ideas of all time.</em><a href="https://bigthink.com/subscribe/?utm_source=syndication&amp;utm_medium=organicpartner&amp;utm_campaign=atlasobscura"> <em>Sign up for Big Think’s newsletter.</em></a></p>
</section></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rainwater everywhere on the planet is unsafe to drink due to chemicals (2022) (213 pts)]]></title>
            <link>https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html</link>
            <guid>39522798</guid>
            <pubDate>Tue, 27 Feb 2024 11:27:17 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html">https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html</a>, See on <a href="https://news.ycombinator.com/item?id=39522798">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
										
<div data-thumb="https://scx1.b-cdn.net/csz/news/tmb/2022/rainwater-1.jpg" data-src="https://scx2.b-cdn.net/gfx/news/hires/2022/rainwater-1.jpg" data-sub-html="Credit: Pixabay/CC0 Public Domain">
        <figure>
            <img src="https://scx1.b-cdn.net/csz/news/800a/2022/rainwater-1.jpg" alt="rainwater" title="Credit: Pixabay/CC0 Public Domain" width="800" height="530">
             <figcaption>
                Credit: Pixabay/CC0 Public Domain
            </figcaption>        </figure>
    </div><p>Rainwater everywhere on the planet is unsafe to drink due to levels of toxic chemicals known as PFAS that exceed the latest guidelines, according to a new study by Stockholm University scientists.</p>


										      
																																	<p>Commonly known as 'forever chemicals' because they disintegrate extremely slowly, PFAS (per- and <a href="https://phys.org/tags/polyfluoroalkyl+substances/" rel="tag">polyfluoroalkyl substances</a>) were initially found in packaging, shampoo or makeup but have spread to our entire environment, including water and air.</p>
<p>"There is nowhere on Earth where the rain would be safe to drink, according to the measurements that we have taken," Ian Cousins, a professor at the university and the lead author of the study published in <i>Environmental Science and Technology</i>, told AFP.</p>
<p>A compilation of the data since 2010 that his team studied showed that "even in Antarctica or the Tibetan plateau, the levels in the rainwater are above the drinking water guidelines that the US EPA (Environmental Protection Agency) proposed", he said.</p>
<p>Normally considered pristine, the two regions still have PFAS levels "14 times higher" than the US drinking water guidelines.</p>
<p>The EPA recently lowered its PFAS guidelines significantly after discovering that the chemicals may affect the <a href="https://phys.org/tags/immune+response/" rel="tag">immune response</a> in children to vaccines, Cousins noted.</p>
<p>Once ingested, PFAS accumulate in the body.</p>
<p>According to some studies, exposure can also lead to problems with fertility, <a href="https://phys.org/tags/developmental+delays/" rel="tag">developmental delays</a> in children, increased risks of obesity or certain cancers (prostate, kidney and testicular), an increase in <a href="https://phys.org/tags/cholesterol+levels/" rel="tag">cholesterol levels</a>.<br>
—Planet 'irreversibly contaminated'—</p>

																																						
																																			<p>Cousins said PFAS were now "so persistent" and ubiquitous that they will never disappear from the planet.</p>
<p>"We have made the planet inhospitable to human life by irreversibly contaminating it now so that nothing is clean anymore. And to the point that's it's not clean enough to be safe", he said.</p>
<p>"We have crossed a planetary boundary", he said, referring to a central paradigm for evaluating Earth's capacity to absorb the impact of human activity.</p>
<p>However, Cousins noted that PFAS levels in people have actually dropped "quite significantly in the last 20 years" and "ambient levels (of PFAS in the environment) have been the same for the past 20 years".</p>
<p>"What's changed is the guidelines. They've gone down millions of times since the early 2000s, because we've learned more about the toxicity of these substances."</p>
<p>Cousins said we have to learn to live with it.</p>
<p>"I'm not super concerned about the everyday exposure in mountain or stream <a href="https://phys.org/tags/water/" rel="tag">water</a> or in the food. We can't escape it... we're just going to have to live with it."</p>
<p>"But it's not a great situation to be in, where we've contaminated the environment to the point where background exposure is not really safe."</p>

																																																					
																				<div>
																						<p><strong>More information:</strong>
												Per- and polyfluoroalkyl substances (PFAS) define a new planetary boundary for novel entities that has been exceeded, <i>Environmental Science &amp; Technology</i> (2022). <a data-doi="1" href="https://dx.doi.org/10.1021/acs.est.2c02765" target="_blank">DOI: 10.1021/acs.est.2c02765</a>
																						
																						</p>
																					</div>
                               											
																															 <p>
												  © 2022 AFP
											 </p>
										                                        
										<!-- print only -->
										<div>
											 <p><strong>Citation</strong>:
												Rainwater unsafe to drink due to chemicals: study (2022, August 10)
												retrieved 27 February 2024
												from https://phys.org/news/2022-08-rainwater-unsafe-due-chemicals.html
											 </p>
											 <p>
											 This document is subject to copyright. Apart from any fair dealing for the purpose of private study or research, no
											 part may be reproduced without the written permission. The content is provided for information purposes only.
											 </p>
										</div>
                                        
									</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Blender Open Movies. Featuring all the production files, assets, and artwork (200 pts)]]></title>
            <link>https://studio.blender.org/films/</link>
            <guid>39522499</guid>
            <pubDate>Tue, 27 Feb 2024 10:42:50 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://studio.blender.org/films/">https://studio.blender.org/films/</a>, See on <a href="https://news.ycombinator.com/item?id=39522499">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
    
    <p>The iconic Blender Open Movies. Featuring all the production files, assets, artwork, and never-seen-before content.</p>
  </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[I turned my open-source project into a full-time business (404 pts)]]></title>
            <link>https://docs.emailengine.app/how-i-turned-my-open-source-project-into/</link>
            <guid>39522348</guid>
            <pubDate>Tue, 27 Feb 2024 10:16:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://docs.emailengine.app/how-i-turned-my-open-source-project-into/">https://docs.emailengine.app/how-i-turned-my-open-source-project-into/</a>, See on <a href="https://news.ycombinator.com/item?id=39522348">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="site-main">
<article>

    <header>

        

        


        <section>

            <ul>
                <li>
                    <a href="https://docs.emailengine.app/author/andris/">
                        <img src="https://www.gravatar.com/avatar/0bc4849da2f3ad66fe4d3fa53a3d45f2?s=250&amp;d=mm&amp;r=x" alt="Andris Reinman">
                    </a>
                </li>
            </ul>

            <div>
                
                <p><time datetime="2024-02-27">Feb 27, 2024</time>
                        <span><span>•</span> 4 min read</span>
                </p>
            </div>

        </section>

            <figure>
                <img srcset="https://docs.emailengine.app/content/images/size/w300/2024/02/EmailEngine_logo_horiz.jpg 300w,
                            https://docs.emailengine.app/content/images/size/w600/2024/02/EmailEngine_logo_horiz.jpg 600w,
                            https://docs.emailengine.app/content/images/size/w1000/2024/02/EmailEngine_logo_horiz.jpg 1000w,
                            https://docs.emailengine.app/content/images/size/w2000/2024/02/EmailEngine_logo_horiz.jpg 2000w" sizes="(min-width: 1400px) 1400px, 92vw" src="https://docs.emailengine.app/content/images/size/w2000/2024/02/EmailEngine_logo_horiz.jpg" alt="How I turned my open-source project into a business">
            </figure>

    </header>

    <section>
        <p>When I started writing and publishing open-source software about 15 years ago, I was pretty radical about it. I only used permissive licenses like MIT or BSD, as all I cared about was reach. Using a copyleft license with strings attached seemed to hinder that reach. Getting another A-category company to use my open-source libraries like <a href="https://nodemailer.com/?ref=docs.emailengine.app">Nodemailer</a> was a badge of honor. I even went so far that when a founder of a major transactional email service sent me an email regarding Nodemailer and offered to make a donation to promote my efforts, I rejected it. I did not want to seem affected by one of the dominant providers because this would not be fair to other providers.</p><p>In hindsight, what a fool I was.</p><p>In any case, it changed years later when a startup using Nodemailer was acquired for half a billion dollars. I was financially not in a good place back then, and when I saw the news, I started to wonder – what did I get out of this? Sending email notifications was a huge part of that service, and they probably sent millions of email notifications a day using Nodemailer. At the very least, I saved them tons of developer hours by providing a free and solid library for sending these emails. I searched my mailbox for emails related to that company and found a single complaint about a feature. No pull requests, no donations, no nothing. And there was nowhere to complain either as I had knowingly given my software for the world to use with no requirements to compensate anything. My empty wallet was not happy about the turn of events.</p><p>So, when I started what eventually became <a href="https://emailengine.app/?ref=docs.emailengine.app">EmailEngine</a>, I tried to cover my back as much as possible. I released the software under the copyleft LGPL license. I also set up an automated CLA process so that no one was able to get their PR merged without signing a CLA first. Many people hate CLAs, and several persons opened a PR first but closed it once they realized that there was a CLA requirement. Well, to be honest, I didn't really care. For example, 98.1% of the code for Nodemailer was written by myself, and only 1.9% was from other contributors, so not getting PRs merged was not a major issue. For EmailEngine, after a year and a half of being published as open source, the same numbers were 99.8% vs 0.2%.</p><blockquote>I use <a href="https://cla-assistant.io/?ref=docs.emailengine.app">CLA assistant</a> for managing CLAs in my projects</blockquote><p>Obviously, I wanted to make some money from my new project, and my business plan was simple. I published the project (it was called IMAP API at that time) as an LGPL-licensed application. I also offered an MIT version, but to get that, you had to subscribe. The subscription fee was 250€ per year. My assumption was that companies - the main target for the software - do not like copyleft licenses and would convert to the permissive license once they see how useful the app is.</p><p>Well, it turns out my business plan was bonkers. I only gained a few paying subscribers, and it seemed to me those people weren't even using IMAP API. They just wanted to support my effort. It turned out that smaller companies did not care about the license at all, and larger companies were not using it. After a year and a half and 750€ in total revenue, I decided to jump ship — enough of providing free stuff.</p><p>I re-designed the UI of the app to look more professional and implemented a license key system. From that moment if you wanted to use EmailEngine (the new name for IMAP API), you needed a license key that was only available for paying subscribers. I also changed the license from LGPL to a commercial license. The source code is still published publicly on <a href="https://github.com/postalsys/emailengine?ref=docs.emailengine.app">GitHub</a>. It is no longer <em>open-source</em> by definition but <em>source-available</em>. This change of license was only possible due to requiring outside committers to sign a CLA from the start.</p><blockquote>I still publish MIT-licensed projects, but only for smaller tools, not larger projects. The goal of these tools is to promote my main effort. For example, I extracted the IMAP client functions from EmailEngine and published it under an MIT license as a generic IMAP client library for Node.js. This module &nbsp;(<a href="https://imapflow.com/?ref=docs.emailengine.app">ImapFlow</a>) is gaining steam in adoption as it is by far better than any pre-existing alternative. The documentation page sends about 100 visitors per month to EmailEngine's homepage, which is not much, but hey, it is free traffic, and sometimes these visitors do convert, making the effort fruitful.</blockquote><p>At first, there wasn't even a trial option. If you did not provide a valid license key 15 minutes after the application started, the app just stopped working.</p><p>I kept the price the same, 250€ per year, and during the first month, I sold 1750€ worth of subscriptions. That's like twice the amount I made in the previous year and a half, and it sealed the fate of the project. There was no going back.</p><p>Next, I started to increase the pricing; 250€ became 495€, then 695€ and 795€, and finally 895€. To my surprise, it did not mean getting fewer customers. I guess any sub-$1k amount for businesses is peanuts, so the only thing these price increases changed was improving the revenue.</p><p>The current MRR for EmailEngine is 6100€ and grows steadily, which in Estonia, where I live, allows me to pay myself a decent salary so that I can work on my project full-time. The only regret I have is that I did not start selling my software earlier and only published free, open-source software. Yes, I have some sponsors in GitHub, but it has never been a substantial amount, ranging from $50-$750 per month, depending on how many sponsors I happen to have. Selling to business customers is definitely more reliable and predictable than depending on the goodwill of random people.</p>
    </section>


</article>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why time seems to pass faster as we age (150 pts)]]></title>
            <link>https://invertedpassion.com/why-time-seems-to-pass-faster-as-we-age/</link>
            <guid>39522249</guid>
            <pubDate>Tue, 27 Feb 2024 10:02:44 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://invertedpassion.com/why-time-seems-to-pass-faster-as-we-age/">https://invertedpassion.com/why-time-seems-to-pass-faster-as-we-age/</a>, See on <a href="https://news.ycombinator.com/item?id=39522249">Hacker News</a></p>
<div id="readability-page-1" class="page"><article id="post-2155">
		<!-- .entry-header -->
	<div>
		
		
<p>1/&nbsp;I’ve been mega-obsessed with this feeling.</p>



<p>A year as a 36-year-old seems so much shorter as compared to when I was a kid or even as a teen.</p>



<p>It seems cosmically unfair – <strong>we have fewer years to live, and each year flies by faster.</strong></p>



<p>2/&nbsp;But, why is that happening?</p>



<p>My tentative conclusion is that <strong>it’s an unfortunate outcome of how evolution shaped our brain to be an efficient storage device.&nbsp;</strong></p>



<p>3/&nbsp;Our brain is a prediction device.</p>



<p>Its top job is to <strong>construct a model of the world so that we get a survival and reproductive edge.&nbsp;</strong></p>



<p>4/&nbsp;To be able to predict a phenomenon is to be able to control it and have power over it, so <strong>our brain is obsessed with predicting how things are going to go.</strong></p>



<p>It wants to be able to predict how mates are found, how money is made, what makes people laugh, and so on..&nbsp;<sup></sup></p>



<p>5/&nbsp;But it’s also efficient.</p>



<p>If an event has happened before, what’s the point of paying attention to it and storing it in memory?</p>



<p>Redundant storage is inefficient, so the b<strong>rain likely only pays attention to and memorises what’s new and surprising.&nbsp;</strong></p>



<p>6/&nbsp;As kids, everything is new and surprising.</p>



<p><strong>The world is full of learning opportunities, so the brain makes massive updates in memories.</strong></p>



<p>Full snapshots of your birthdays, vacations, days at school and so on.&nbsp;<sup></sup></p>



<p>7/&nbsp;<strong>Surprising information comes in droves every single day, so the brain simply paid a lot of attention</strong>, and hence you felt there were so-many-slices-of-time in a day.</p>



<p>It also stored that rich information in memory, so even looking back, days felt longer.&nbsp;</p>



<p>8/&nbsp;<strong>As we grow, new surprises become a merely tiny-patch on an old memory.</strong></p>



<p>Why store the full details of your N-th vacation when you can simply store the diff of it from your first one?&nbsp;<sup></sup></p>



<p>9/&nbsp;In other words, as we age, our memories and attention become low-fidelity versions of their former self.</p>



<p><strong>As patterns in life start repeating themselves, the slices-of-time that you notice and memorise become fewer and coarser.&nbsp;</strong></p>



<p>10/&nbsp;Naturally, <strong>if anyone asks where did time in your life go, you’d access your memory and find the majority of them relating to childhood</strong>, and very few from the recent times.</p>



<p>And that’s why time feels like it accumulated in the past, and not in the recent present.&nbsp;<sup></sup></p>



<p>11/&nbsp;The main culprit in time-speeding up is predictability.</p>



<p><strong>The more predictable your days are, the shorter they will feel.&nbsp;</strong></p>



<p>12/&nbsp;A thought experiment.</p>



<p>If you have a stable job, you can pretty much mentally time travel a full year and find your days to be similar.</p>



<p><strong>But if I ask you to imagine doing a PhD in Sanskrit at a foreign university, you would have no idea what your days are going to look like.&nbsp;</strong></p>



<p>13/&nbsp;So, <strong>predictability not just impacts perception of time in the present but also for the future.</strong></p>



<p>As kids, a vacation was full of surprising information, so it actually felt rich and long.</p>



<p>Now, your nth trip to Goa feels much shorter as you know what you’re going to do.&nbsp;<sup></sup></p>



<p>14/&nbsp;So, what to do? How to slow down time?</p>



<p><strong>The only approach I can think of is to break the predictability and actively plan to be (massively) surprised.</strong></p>



<p>Take on projects that you have no idea about.&nbsp;<sup></sup></p>



<p>15/&nbsp;<strong>Unfortunately, we are evolved to avoid exploring and taking risks as we age.</strong></p>



<p>Our brain pushes us to exploit more of the world we have come to understand better instead of pushing us to explore more.</p>



<p>But that’s precisely how you’ll make your years fly.&nbsp;<sup></sup></p>



<p>16/&nbsp;You need to ask yourself.</p>



<p>How do you want to answer how you lived your life?</p>



<p><strong>Long one, or the one that *feels* long?</strong></p>



<p>What’s more important to you?</p>



<p>17/ Interestingly, <strong>the solution to slowing down time is not boredom</strong> (as I thought). </p>



<p>Boredom is a negative state. The solution is to dive head-first into unknown territory. </p>



<p><strong>That is, to travel physically or mentally,</strong></p>



<p>18/ Note that <strong>we’re really good at grokking patterns / making predictive models. </strong></p>



<p>As soon as we figure out a winning condition for a game or the story plot, we lose interest in it.</p>



<p>19/ So <strong>an existential crisis is a spoiler alert for life. </strong></p>



<p>The brain with all its predictive models asks: is this all to life? </p>



<p>But it’s mistaken – <strong>it’s all only to the life that it has chosen to live.</strong></p>



<p>20/ A (radically) different life that it can’t predict will keep brain at its toes. </p>



<p>The key word here is “radically”. </p>



<p><strong>The smaller the change, the less memorable the time.</strong></p>



<hr>





<p><small><strong>Join 150k+ followers</strong></small><br>
<a href="https://twitter.com/paraschopra" data-show-count="true">Follow @paraschopra</a></p>







			</div><!-- .entry-content -->
						<!-- .entry-footer -->
		</article></div>]]></description>
        </item>
        <item>
            <title><![CDATA[React Labs: What We've Been Working On – February 2024 (105 pts)]]></title>
            <link>https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024</link>
            <guid>39522031</guid>
            <pubDate>Tue, 27 Feb 2024 09:33:21 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024">https://react.dev/blog/2024/02/15/react-labs-what-we-have-been-working-on-february-2024</a>, See on <a href="https://news.ycombinator.com/item?id=39522031">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>February 15, 2024 by <a href="https://twitter.com/en_JS" target="_blank" rel="nofollow noopener noreferrer">Joseph Savona</a>, <a href="https://twitter.com/rickhanlonii" target="_blank" rel="nofollow noopener noreferrer">Ricky Hanlon</a>, <a href="https://twitter.com/acdlite" target="_blank" rel="nofollow noopener noreferrer">Andrew Clark</a>, <a href="https://twitter.com/mattcarrollcode" target="_blank" rel="nofollow noopener noreferrer">Matt Carroll</a>, and <a href="https://twitter.com/dan_abramov" target="_blank" rel="nofollow noopener noreferrer">Dan Abramov</a>.</p>
<hr>
<p>In React Labs posts, we write about projects in active research and development. We’ve made significant progress since our <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023">last update</a>, and we’d like to share our progress.</p>
<div><h3><svg width="2em" height="2em" viewBox="0 0 72 72" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_40_48064)"><path d="M24 27C24 25.3431 25.3431 24 27 24H45C46.6569 24 48 25.3431 48 27C48 28.6569 46.6569 30 45 30H27C25.3431 30 24 28.6569 24 27Z" fill="currentColor"></path><path d="M24 39C24 37.3431 25.3431 36 27 36H39C40.6569 36 42 37.3431 42 39C42 40.6569 40.6569 42 39 42H27C25.3431 42 24 40.6569 24 39Z" fill="currentColor"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M12 18C12 13.0294 16.0294 9 21 9H51C55.9706 9 60 13.0294 60 18V54C60 58.9706 55.9706 63 51 63H21C16.0294 63 12 58.9706 12 54V18ZM21 15H51C52.6569 15 54 16.3431 54 18V54C54 55.6569 52.6569 57 51 57H21C19.3431 57 18 55.6569 18 54V18C18 16.3431 19.3431 15 21 15Z" fill="currentColor"></path></g><defs><clipPath id="clip0_40_48064"><rect width="72" height="72" fill="white"></rect></clipPath></defs></svg>Note</h3><div><p>React Conf 2024 is scheduled for May 15–16 in Henderson, Nevada! If you’re interested in attending React Conf in person, you can <a href="https://forms.reform.app/bLaLeE/react-conf-2024-ticket-lottery/1aRQLK" target="_blank" rel="nofollow noopener noreferrer">sign up for the ticket lottery</a> until February 28th.</p><p>For more info on tickets, free streaming, sponsoring, and more, see <a href="https://conf.react.dev/" target="_blank" rel="nofollow noopener noreferrer">the React Conf website</a>.</p></div></div>
<hr>
<h2 id="react-compiler">React Compiler <a href="#react-compiler" aria-label="Link for React Compiler " title="Link for React Compiler "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>React Compiler is no longer a research project: the compiler now powers instagram.com in production, and we are working to ship the compiler across additional surfaces at Meta and to prepare the first open source release.</p>
<p>As discussed in our <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023#react-optimizing-compiler">previous post</a>, React can <em>sometimes</em> re-render too much when state changes. Since the early days of React our solution for such cases has been manual memoization. In our current APIs, this means applying the <a href="https://react.dev/reference/react/useMemo"><code dir="ltr">useMemo</code></a>, <a href="https://react.dev/reference/react/useCallback"><code dir="ltr">useCallback</code></a>, and <a href="https://react.dev/reference/react/memo"><code dir="ltr">memo</code></a> APIs to manually tune how much React re-renders on state changes. But manual memoization is a compromise. It clutters up our code, is easy to get wrong, and requires extra work to keep up to date.</p>
<p>Manual memoization is a reasonable compromise, but we weren’t satisfied. Our vision is for React to <em>automatically</em> re-render just the right parts of the UI when state changes, <em>without compromising on React’s core mental model</em>. We believe that React’s approach — UI as a simple function of state, with standard JavaScript values and idioms — is a key part of why React has been approachable for so many developers. That’s why we’ve invested in building an optimizing compiler for React.</p>
<p>JavaScript is a notoriously challenging language to optimize, thanks to its loose rules and dynamic nature. React Compiler is able to compile code safely by modeling both the rules of JavaScript <em>and</em> the “rules of React”. For example, React components must be idempotent — returning the same value given the same inputs — and can’t mutate props or state values. These rules limit what developers can do and help to carve out a safe space for the compiler to optimize.</p>
<p>Of course, we understand that developers sometimes bend the rules a bit, and our goal is to make React Compiler work out of the box on as much code as possible. The compiler attempts to detect when code doesn’t strictly follow React’s rules and will either compile the code where safe or skip compilation if it isn’t safe. We’re testing against Meta’s large and varied codebase in order to help validate this approach.</p>
<p>For developers who are curious about making sure their code follows React’s rules, we recommend <a href="https://react.dev/reference/react/StrictMode">enabling Strict Mode</a> and <a href="https://react.dev/learn/editor-setup#linting">configuring React’s ESLint plugin</a>. These tools can help to catch subtle bugs in your React code, improving the quality of your applications today, and future-proofs your applications for upcoming features such as React Compiler. We are also working on consolidated documentation of the rules of React and updates to our ESLint plugin to help teams understand and apply these rules to create more robust apps.</p>
<p>To see the compiler in action, you can check out our <a href="https://www.youtube.com/watch?v=qOQClO3g8-Y" target="_blank" rel="nofollow noopener noreferrer">talk from last fall</a>. At the time of the talk, we had early experimental data from trying React Compiler on one page of instagram.com. Since then, we shipped the compiler to production across instagram.com. We’ve also expanded our team to accelerate the rollout to additional surfaces at Meta and to open source. We’re excited about the path ahead and will have more to share in the coming months.</p>
<h2 id="actions">Actions <a href="#actions" aria-label="Link for Actions " title="Link for Actions "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>We <a href="https://react.dev/blog/2023/03/22/react-labs-what-we-have-been-working-on-march-2023#react-server-components">previously shared</a> that we were exploring solutions for sending data from the client to the server with Server Actions, so that you can execute database mutations and implement forms. During development of Server Actions, we extended these APIs to support data handling in client-only applications as well.</p>
<p>We refer to this broader collection of features as simply “Actions”. Actions allow you to pass a function to DOM elements such as <a href="https://react.dev/reference/react-dom/components/form"><code dir="ltr">&lt;form/&gt;</code></a>:</p>
<!--$--><div dir="ltr"><pre><code><p><span>&lt;</span><span>form</span> <span>action</span>=<span>{</span><span>search</span><span>}</span><span>&gt;</span><br></p><p><span>&lt;</span><span>input</span> <span>name</span>=<span>"query"</span> <span>/&gt;</span><br></p><p><span>&lt;</span><span>button</span> <span>type</span>=<span>"submit"</span><span>&gt;</span>Search<span>&lt;/</span><span>button</span><span>&gt;</span><br></p><p><span>&lt;/</span><span>form</span><span>&gt;</span></p></code></pre></div><!--/$-->
<p>The <code dir="ltr">action</code> function can operate synchronously or asynchronously. You can define them on the client side using standard JavaScript or on the server with the  <a href="https://react.dev/reference/react/use-server"><code dir="ltr">'use server'</code></a> directive. When using an action, React will manage the life cycle of the data submission for you, providing hooks like <a href="https://react.dev/reference/react-dom/hooks/useFormStatus"><code dir="ltr">useFormStatus</code></a>, and <a href="https://react.dev/reference/react-dom/hooks/useFormState"><code dir="ltr">useFormState</code></a> to access the current state and response of the form action.</p>
<p>By default, Actions are submitted within a <a href="https://react.dev/reference/react/useTransition">transition</a>, keeping the current page interactive while the action is processing. Since Actions support async functions, we’ve also added the ability to use <code dir="ltr">async/await</code> in transitions. This allows you to show pending UI with the <code dir="ltr">isPending</code> state of a transition when an async request like <code dir="ltr">fetch</code> starts, and show the pending UI all the way through the update being applied.</p>
<p>Alongside Actions, we’re introducing a feature named <a href="https://react.dev/reference/react/useOptimistic"><code dir="ltr">useOptimistic</code></a> for managing optimistic state updates. With this hook, you can apply temporary updates that are automatically reverted once the final state commits. For Actions, this allows you to optimistically set the final state of the data on the client, assuming the submission is successful, and revert to the value for data received from the server. It works using regular <code dir="ltr">async</code>/<code dir="ltr">await</code>, so it works the same whether you’re using <code dir="ltr">fetch</code> on the client, or a Server Action from the server.</p>
<p>Library authors can implement custom <code dir="ltr">action={fn}</code> props in their own components with <code dir="ltr">useTransition</code>. Our intent is for libraries to adopt the Actions pattern when designing their component APIs, to provide a consistent experience for React developers. For example, if your library provides a <code dir="ltr">&lt;Calendar onSelect={eventHandler}&gt;</code> component, consider also exposing a <code dir="ltr">&lt;Calendar selectAction={action}&gt;</code> API, too.</p>
<p>While we initially focused on Server Actions for client-server data transfer, our philosophy for React is to provide the same programming model across all platforms and environments. When possible, if we introduce a feature on the client, we aim to make it also work on the server, and vice versa. This philosophy allows us to create a single set of APIs that work no matter where your app runs, making it easier to upgrade to different environments later.</p>
<p>Actions are now available in the Canary channel and will ship in the next release of React.</p>
<h2 id="new-features-in-react-canary">New Features in React Canary <a href="#new-features-in-react-canary" aria-label="Link for New Features in React Canary " title="Link for New Features in React Canary "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>We introduced <a href="https://react.dev/blog/2023/05/03/react-canaries">React Canaries</a> as an option to adopt individual new stable features as soon as their design is close to final, before they’re released in a stable semver version.</p>
<p>Canaries are a change to the way we develop React. Previously, features would be researched and built privately inside of Meta, so users would only see the final polished product when released to Stable. With Canaries, we’re building in public with the help of the community to finalize features we share in the React Labs blog series. This means you hear about new features sooner, as they’re being finalized instead of after they’re complete.</p>
<p>React Server Components, Asset Loading, Document Metadata, and Actions have all landed in the React Canary, and we’ve added docs for these features on react.dev:</p>
<ul>
<li>
<p><strong>Directives</strong>: <a href="https://react.dev/reference/react/use-client"><code dir="ltr">"use client"</code></a> and <a href="https://react.dev/reference/react/use-server"><code dir="ltr">"use server"</code></a> are bundler features designed for full-stack React frameworks. They mark the “split points” between the two environments: <code dir="ltr">"use client"</code> instructs the bundler to generate a <code dir="ltr">&lt;script&gt;</code> tag (like <a href="https://docs.astro.build/en/concepts/islands/#creating-an-island" target="_blank" rel="nofollow noopener noreferrer">Astro Islands</a>), while <code dir="ltr">"use server"</code> tells the bundler to generate a POST endpoint (like <a href="https://trpc.io/docs/concepts" target="_blank" rel="nofollow noopener noreferrer">tRPC Mutations</a>). Together, they let you write reusable components that compose client-side interactivity with the related server-side logic.</p>
</li>
<li>
<p><strong>Document Metadata</strong>: we added built-in support for rendering <a href="https://react.dev/reference/react-dom/components/title"><code dir="ltr">&lt;title&gt;</code></a>, <a href="https://react.dev/reference/react-dom/components/meta"><code dir="ltr">&lt;meta&gt;</code></a>, and metadata <a href="https://react.dev/reference/react-dom/components/link"><code dir="ltr">&lt;link&gt;</code></a> tags anywhere in your component tree. These work the same way in all environments, including fully client-side code, SSR, and RSC. This provides built-in support for features pioneered by libraries like <a href="https://github.com/nfl/react-helmet" target="_blank" rel="nofollow noopener noreferrer">React Helmet</a>.</p>
</li>
<li>
<p><strong>Asset Loading</strong>: we integrated Suspense with the loading lifecycle of resources such as stylesheets, fonts, and scripts so that React takes them into account to determine whether the content in elements like <a href="https://react.dev/reference/react-dom/components/style"><code dir="ltr">&lt;style&gt;</code></a>, <a href="https://react.dev/reference/react-dom/components/link"><code dir="ltr">&lt;link&gt;</code></a>, and <a href="https://react.dev/reference/react-dom/components/script"><code dir="ltr">&lt;script&gt;</code></a> are ready to be displayed. We’ve also added new <a href="https://react.dev/reference/react-dom#resource-preloading-apis">Resource Loading APIs</a> like <code dir="ltr">preload</code> and <code dir="ltr">preinit</code> to provide greater control for when a resource should load and initialize.</p>
</li>
<li>
<p><strong>Actions</strong>: As shared above, we’ve added Actions to manage sending data from the client to the server. You can add <code dir="ltr">action</code> to elements like <a href="https://react.dev/reference/react-dom/components/form"><code dir="ltr">&lt;form/&gt;</code></a>, access the status with <a href="https://react.dev/reference/react-dom/hooks/useFormStatus"><code dir="ltr">useFormStatus</code></a>, handle the result with <a href="https://react.dev/reference/react-dom/hooks/useFormState"><code dir="ltr">useFormState</code></a>, and optimistically update the UI with <a href="https://react.dev/reference/react/useOptimistic"><code dir="ltr">useOptimistic</code></a>.</p>
</li>
</ul>
<p>Since all of these features work together, it’s difficult to release them in the Stable channel individually. Releasing Actions without the complementary hooks for accessing form states would limit the practical usability of Actions. Introducing React Server Components without integrating Server Actions would complicate modifying data on the server.</p>
<p>Before we can release a set of features to the Stable channel, we need to ensure they work cohesively and developers have everything they need to use them in production. React Canaries allow us to develop these features individually, and release the stable APIs incrementally until the entire feature set is complete.</p>
<p>The current set of features in React Canary are complete and ready to release.</p>
<h2 id="the-next-major-version-of-react">The Next Major Version of React <a href="#the-next-major-version-of-react" aria-label="Link for The Next Major Version of React " title="Link for The Next Major Version of React "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>After a couple of years of iteration, <code dir="ltr">react@canary</code> is now ready to ship to <code dir="ltr">react@latest</code>. The new features mentioned above are compatible with any environment your app runs in, providing everything needed for production use. Since Asset Loading and Document Metadata may be a breaking change for some apps, the next version of React will be a major version: <strong>React 19</strong>.</p>
<p>There’s still more to be done to prepare for release. In React 19, we’re also adding long-requested improvements which require breaking changes like support for Web Components. Our focus now is to land these changes, prepare for release, finalize docs for new features, and publish announcements for what’s included.</p>
<p>We’ll share more information about everything React 19 includes, how to adopt the new client features, and how to build support for React Server Components in the coming months.</p>
<h2 id="offscreen-renamed-to-activity">Offscreen (renamed to Activity). <a href="#offscreen-renamed-to-activity" aria-label="Link for Offscreen (renamed to Activity). " title="Link for Offscreen (renamed to Activity). "><svg width="1em" height="1em" viewBox="0 0 13 13" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor" fill-rule="evenodd"><path d="M7.778 7.975a2.5 2.5 0 0 0 .347-3.837L6.017 2.03a2.498 2.498 0 0 0-3.542-.007 2.5 2.5 0 0 0 .006 3.543l1.153 1.15c.07-.29.154-.563.25-.773.036-.077.084-.16.14-.25L3.18 4.85a1.496 1.496 0 0 1 .002-2.12 1.496 1.496 0 0 1 2.12 0l2.124 2.123a1.496 1.496 0 0 1-.333 2.37c.16.246.42.504.685.752z"></path><path d="M5.657 4.557a2.5 2.5 0 0 0-.347 3.837l2.108 2.108a2.498 2.498 0 0 0 3.542.007 2.5 2.5 0 0 0-.006-3.543L9.802 5.815c-.07.29-.154.565-.25.774-.036.076-.084.16-.14.25l.842.84c.585.587.59 1.532 0 2.122-.587.585-1.532.59-2.12 0L6.008 7.68a1.496 1.496 0 0 1 .332-2.372c-.16-.245-.42-.503-.685-.75z"></path></g></svg></a></h2>
<p>Since our last update, we’ve renamed a capability we’re researching from “Offscreen” to “Activity”. The name “Offscreen” implied that it only applied to parts of the app that were not visible, but while researching the feature we realized that it’s possible for parts of the app to be visible and inactive, such as content behind a modal. The new name more closely reflects the behavior of marking certain parts of the app “active” or “inactive”.</p>
<p>Activity is still under research and our remaining work is to finalize the primitives that are exposed to library developers. We’ve deprioritized this area while we focus on shipping features that are more complete.</p>
<hr>
<p>In addition to this update, our team has presented at conferences and made appearances on podcasts to speak more on our work and answer questions.</p>
<ul>
<li>
<p><a href="https://react.dev/community/team#sathya-gunasekaran">Sathya Gunasekaran</a> spoke about the React Compiler at the <a href="https://www.youtube.com/watch?v=kjOacmVsLSE" target="_blank" rel="nofollow noopener noreferrer">React India</a> conference</p>
</li>
<li>
<p><a href="https://react.dev/community/team#dan-abramov">Dan Abramov</a> gave a talk at <a href="https://www.youtube.com/watch?v=zMf_xeGPn6s" target="_blank" rel="nofollow noopener noreferrer">RemixConf</a> titled “React from Another Dimension” which explores an alternative history of how React Server Components and Actions could have been created</p>
</li>
<li>
<p><a href="https://react.dev/community/team#dan-abramov">Dan Abramov</a> was interviewed on <a href="https://changelog.com/jsparty/311" target="_blank" rel="nofollow noopener noreferrer">the Changelog’s JS Party podcast</a> about React Server Components</p>
</li>
<li>
<p><a href="https://react.dev/community/team#matt-carroll">Matt Carroll</a> was interviewed on the <a href="https://www.buzzsprout.com/2226499/14462424-interview-the-two-reacts-with-rachel-nabors-evan-bacon-and-matt-carroll" target="_blank" rel="nofollow noopener noreferrer">Front-End Fire podcast</a> where he discussed <a href="https://overreacted.io/the-two-reacts/" target="_blank" rel="nofollow noopener noreferrer">The Two Reacts</a></p>
</li>
</ul>
<p>Thanks <a href="https://twitter.com/potetotes" target="_blank" rel="nofollow noopener noreferrer">Lauren Tan</a>, <a href="https://twitter.com/sophiebits" target="_blank" rel="nofollow noopener noreferrer">Sophie Alpert</a>, <a href="https://threads.net/someextent" target="_blank" rel="nofollow noopener noreferrer">Jason Bonta</a>, <a href="https://twitter.com/Eli_White" target="_blank" rel="nofollow noopener noreferrer">Eli White</a>, and <a href="https://twitter.com/_gsathya" target="_blank" rel="nofollow noopener noreferrer">Sathya Gunasekaran</a> for reviewing this post.</p>
<p>Thanks for reading, and <a href="https://conf.react.dev/" target="_blank" rel="nofollow noopener noreferrer">see you at React Conf</a>!</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Shittier: Code formatting tool that makes your code look terrible (101 pts)]]></title>
            <link>https://github.com/rohitdhas/shittier</link>
            <guid>39521944</guid>
            <pubDate>Tue, 27 Feb 2024 09:21:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/rohitdhas/shittier">https://github.com/rohitdhas/shittier</a>, See on <a href="https://news.ycombinator.com/item?id=39521944">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <nav aria-label="Global">
            <ul>
                <li>
      
      <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Actions&quot;,&quot;label&quot;:&quot;ref_cta:Actions;&quot;}" href="https://github.com/features/actions">
      
      <div>
        <p>Actions</p><p>
        Automate any workflow
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Packages&quot;,&quot;label&quot;:&quot;ref_cta:Packages;&quot;}" href="https://github.com/features/packages">
      
      <div>
        <p>Packages</p><p>
        Host and manage packages
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Security&quot;,&quot;label&quot;:&quot;ref_cta:Security;&quot;}" href="https://github.com/features/security">
      
      <div>
        <p>Security</p><p>
        Find and fix vulnerabilities
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Codespaces&quot;,&quot;label&quot;:&quot;ref_cta:Codespaces;&quot;}" href="https://github.com/features/codespaces">
      
      <div>
        <p>Codespaces</p><p>
        Instant dev environments
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Copilot&quot;,&quot;label&quot;:&quot;ref_cta:Copilot;&quot;}" href="https://github.com/features/copilot">
      
      <div>
        <p>Copilot</p><p>
        Write better code with AI
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Code review&quot;,&quot;label&quot;:&quot;ref_cta:Code review;&quot;}" href="https://github.com/features/code-review">
      
      <div>
        <p>Code review</p><p>
        Manage code changes
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Issues&quot;,&quot;label&quot;:&quot;ref_cta:Issues;&quot;}" href="https://github.com/features/issues">
      
      <div>
        <p>Issues</p><p>
        Plan and track work
      </p></div>

    
</a></li>

                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Product&quot;,&quot;action&quot;:&quot;click to go to Discussions&quot;,&quot;label&quot;:&quot;ref_cta:Discussions;&quot;}" href="https://github.com/features/discussions">
      
      <div>
        <p>Discussions</p><p>
        Collaborate outside of code
      </p></div>

    
</a></li>

            </ul>
          </div>
</li>


                <li>
      
      
</li>


                <li>
      
      <div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to GitHub Sponsors&quot;,&quot;label&quot;:&quot;ref_cta:GitHub Sponsors;&quot;}" href="https://github.com/sponsors">
      
      <div>
        <p>GitHub Sponsors</p><p>
        Fund open source developers
      </p></div>

    
</a></li>

            </ul>
          </div>
          <div>
            <ul>
                <li>
  <a data-analytics-event="{&quot;category&quot;:&quot;Header dropdown (logged out), Open Source&quot;,&quot;action&quot;:&quot;click to go to The ReadME Project&quot;,&quot;label&quot;:&quot;ref_cta:The ReadME Project;&quot;}" href="https://github.com/readme">
      
      <div>
        <p>The ReadME Project</p><p>
        GitHub community articles
      </p></div>

    
</a></li>

            </ul>
          </div>
          
      </div>
</li>


                <li>
    <a data-analytics-event="{&quot;category&quot;:&quot;Header menu top item (logged out)&quot;,&quot;action&quot;:&quot;click to go to Pricing&quot;,&quot;label&quot;:&quot;ref_cta:Pricing;&quot;}" href="https://github.com/pricing">Pricing</a>
</li>

            </ul>
          </nav>

        <div>
                


<qbsearch-input data-scope="repo:rohitdhas/shittier" data-custom-scopes-path="/search/custom_scopes" data-delete-custom-scopes-csrf="FWdhSicj8xYdT51_JVIn5kWdKJODi3yO7CZWYznamEXXNYV2Wsu-d0GlUY3bU1AV8OqkYcuoMI9yWv-pdqZRuw" data-max-custom-scopes="10" data-header-redesign-enabled="false" data-initial-value="" data-blackbird-suggestions-path="/search/suggestions" data-jump-to-suggestions-path="/_graphql/GetSuggestedNavigationDestinations" data-current-repository="rohitdhas/shittier" data-current-org="" data-current-owner="rohitdhas" data-logged-in="false" data-copilot-chat-enabled="false" data-blackbird-indexed-repo-csrf="<esi:include src=&quot;/_esi/rails_csrf_token_form_hidden?r=GADFAsfhtLPi4AGWUyBIe3vZLW5W1DoP3iWNLq72pgLEWYwdugd37OYDmwn8vqlBlJa%2B1sR3u6eK4X1OFrc0nGFaZRaTlKaDzn6IBQMpci0RUqN6BW8bDYqcUP14h3gjudh68CQi%2F57d3m2KgAf00Gniu2QxEdn0txpzzm0GZHhctTiiMuMhWLC4Rozaj3HzVgbCx9e9Hzx4enUbIbabEJYkcu0oqU9%2FWavrhy75ATGECGMdLaPvMZQgURbyxLRhB9o74KpWQWn2SWZ7B8f%2F3%2B8p5UW3W5NE1iMtAzoFyQGmQJXF2p3V%2BtCW%2FAko%2F%2Fa7KGaP9NM4mCsViuzMphL56e6xvNlA4eInBcjiD08lbHp3hiv4rmO9KcRaMC%2FiJWC5H%2Bc3v4HmB2BMrSnVbhvIO5T35omZ4M8iyAeH9cG5HnWXSDYd2WuboiXA9pSqOA%2Fy28r4TS2jTV%2BmGFtIXFgfMDBGHmcoRC3ndVZD4X7sDIYuTCKynQDOj679jmCS9RMJ%2BIg4LRB59e8Nu3bSKCs%3D--WDjLJ4mcd0AVgtlb--kvMXHQMqaKR02dgI1R4CFA%3D%3D&quot; />">
  <div data-modal-dialog-overlay="" data-action="click:qbsearch-input#searchInputContainerClicked">
  <modal-dialog data-action="close:qbsearch-input#handleClose cancel:qbsearch-input#handleClose" data-target="qbsearch-input.searchSuggestionsDialog" role="dialog" id="search-suggestions-dialog" aria-modal="true" aria-labelledby="search-suggestions-dialog-header" data-view-component="true">
      <h2 id="search-suggestions-dialog-header">Search code, repositories, users, issues, pull requests...</h2>
    
</modal-dialog></div>
  
  <div>
    
<dialog-helper>
  <dialog data-target="qbsearch-input.feedbackDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="feedback-dialog" aria-modal="true" aria-labelledby="feedback-dialog-title" aria-describedby="feedback-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="feedback-dialog-title">
        Provide feedback
      </h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="feedback-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>

    <custom-scopes data-target="qbsearch-input.customScopesManager">
    
<dialog-helper>
  <dialog data-target="custom-scopes.customScopesModalDialog" data-action="close:qbsearch-input#handleDialogClose cancel:qbsearch-input#handleDialogClose" id="custom-scopes-dialog" aria-modal="true" aria-labelledby="custom-scopes-dialog-title" aria-describedby="custom-scopes-dialog-description" data-view-component="true">
    <div data-view-component="true">
    <p>
      <h2 id="custom-scopes-dialog-title">
        Saved searches
      </h2>
        <h2 id="custom-scopes-dialog-description">Use saved searches to filter your results more quickly</h2>
    </p>
    
  </div>
      <scrollable-region data-labelled-by="custom-scopes-dialog-title">
        
      </scrollable-region>
      
</dialog></dialog-helper>
    </custom-scopes>
  </div>
</qbsearch-input>

            <p><a href="https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=rohitdhas%2Fshittier" data-hydro-click="{&quot;event_type&quot;:&quot;authentication.click&quot;,&quot;payload&quot;:{&quot;location_in_page&quot;:&quot;site header menu&quot;,&quot;repository_id&quot;:null,&quot;auth_type&quot;:&quot;SIGN_UP&quot;,&quot;originating_url&quot;:&quot;https://github.com/rohitdhas/shittier&quot;,&quot;user_id&quot;:null}}" data-hydro-click-hmac="5f28916b646394bb66688b8439e9abe704d2ffc13e75813cf2d6d977df415d3a" data-analytics-event="{&quot;category&quot;:&quot;Sign up&quot;,&quot;action&quot;:&quot;click to sign up for account&quot;,&quot;label&quot;:&quot;ref_page:/<user-name>/<repo-name>;ref_cta:Sign up;ref_loc:header logged out&quot;}">
              Sign up
            </a>
        </p></div>
      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Berlin TYPE: The official type for the city of Berlin (2020) (150 pts)]]></title>
            <link>https://www.hvdfonts.com/custom-cases/berlin-type</link>
            <guid>39521650</guid>
            <pubDate>Tue, 27 Feb 2024 08:42:22 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.hvdfonts.com/custom-cases/berlin-type">https://www.hvdfonts.com/custom-cases/berlin-type</a>, See on <a href="https://news.ycombinator.com/item?id=39521650">Hacker News</a></p>
Couldn't get https://www.hvdfonts.com/custom-cases/berlin-type: Error: Request failed with status code 403]]></description>
        </item>
        <item>
            <title><![CDATA[Submarine cables linking Africa-Asia-Europe severed (122 pts)]]></title>
            <link>https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165</link>
            <guid>39521626</guid>
            <pubDate>Tue, 27 Feb 2024 08:39:15 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165">https://en.globes.co.il/en/article-houthis-hit-underwater-communications-cables-1001472165</a>, See on <a href="https://news.ycombinator.com/item?id=39521626">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="F_Content">
        <h2 id="coteret_SubCoteret">"Globes" has learned that four submarine communications cables have been damaged in the Red Sea between Jeddah in Saudi Arabia and Djibouti in East Africa.</h2>


        <article>
            <p>Three months after the Houthis began attacking merchant ships, the Yemenite rebels have carried out another one of their threats. "Globes" has learned that four submarine communication cables have been damaged in the Red Sea between Jeddah in Saudi Arabia and Djibouti in East Africa.</p>
<p>According to the reports, these are cables from the companies AAE-1, Seacom, EIG and TGN. This is causing serious disruption of Internet communications between Europe and Asia, with the main damage being felt in the Gulf countries and India.</p>



<section id="moreArticles"> 
    <h2>RELATED ARTICLES</h2>
    
    
    

    <p>
        <a href="https://en.globes.co.il/en/article-egypt-asks-houthis-to-attack-only-israeli-ships-report-1001468870">
            <h3>Egypt asks Houthis to attack only Israeli ships - report</h3>
        </a>
    </p>

</section>



<p>Estimates are that the damage to communications activities is significant but not critical because other cables pass through the same region linking Asia, Africa and Europe that have not been hit. The repair of such a large number of underwater cables may take at least eight weeks according to estimates and involve exposure to risk from the Houthi terror organization. The telecommunications companies will be forced to look for companies that will agree to carry out the repair work and probably pay them a high risk premium.</p>
<p>EIG (European India Gateway) connects Southern Europe with Egypt, Saudi Arabia, Djibouti, the UAE and India. The underwater cable was laid by Tyco arm Alcatel-Lucent at a cost of $700 million and was the first cable stretching from the UK to India. Shares in EIG are held by a consortium including AT&amp;T, Saudi Telecom, Verizon, and India's Bharat Sanchar.</p>
<p>TGN Atlantic was laid by Tyco International in 2001 and sold to Indian company Tata Communications in 2005 for $130 million. The AAE-1 cable which has also been cut links East Asia to Europe via Egypt. The cable, which has a 40 terabyte per second capacity, links China with the west via countries belonging to the Chinese-Iranian axis including those countries and Pakistan and Qatar.</p>
<p>The Seacom cable links Europe, Africa and India as well as South Africa.</p>
<p>Senior executives at international communications and underwater cable companies have posted reports about the damage on LinkedIn and X.</p>
<p><em>Published by Globes, Israel business news - <a href="https://en.globes.co.il/">en.globes.co.il</a> - on February 26, 2024.</em></p>
<p><em>© Copyright of Globes Publisher Itonut (1983) Ltd., 2024.</em></p>
			<!-- f67 -->
        </article>
        
        
            

        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Open Letter to Tim Cook, Sabotaging Web Apps Is Indefensible (139 pts)]]></title>
            <link>https://letter.open-web-advocacy.org/</link>
            <guid>39521200</guid>
            <pubDate>Tue, 27 Feb 2024 07:35:41 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://letter.open-web-advocacy.org/">https://letter.open-web-advocacy.org/</a>, See on <a href="https://news.ycombinator.com/item?id=39521200">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
          <p><em>Dear Tim Cook,</em></p>
          <p>We write to express our concern at Apple’s decision to remove Web Apps (PWAs) from iOS and Safari in the European Union (EU), and to avail ourselves of our rights under the Digital Markets Act (DMA).</p>
          <p>Apple points to Web Apps as the open alternative to the App Store, and actions to remove them have created deep concern in the web community. iOS demoting Web Apps to shortcuts threaten data loss and undermine the web as a reliable platform for iOS users. These silently-introduced changes threaten critical features including integration with iOS, push notifications, unread count badging, and the ability to run full screen. Their removal will break Web Apps for students, governments, health care institutions, journalists, and startups.</p>
          <p>Entire categories of apps will no longer be viable on the web as a result. More troubling, we understand iOS will not include APIs for competing browsers to implement these features either. This will do vast, immediate, and ongoing harm to users, developers, and businesses, both inside and outside the EU.</p>
          <p>Apple’s justifications gesture toward security and privacy, but are at best unfounded. Web Apps provide safe computing that puts users in control through their browsers, and iOS opening up to competing browser engines will enhance, rather than erode, security and privacy. Web Apps powered by competing browsers can be safer and more capable than today’s apps, and removing support cannot be justified on security grounds. Apple’s arguments regarding the safety of competing browsers have been conclusively rejected by regulators worldwide, and this situation is no different.</p>
          <p>We, the undersigned “end users” and “business users”, avail ourselves of our rights under Articles 5 and 6 of the EU’s DMA. In particular, we assert our right under Article 6(7) ensuring businesses effective interoperability with the software features of the operating system. </p>
          <p>Pursuant to these rights, Apple is obligated to preserve the functionality to allow Safari and other iOS browsers to add Web Apps to the home screen, allow them to run in top-level activities (not in tabs), integrate with iOS settings and permissions, enable Push Notifications and homescreen icon badging, and to run fullscreen.</p>
          <p>Further, we assert that Apple’s proposed changes violate Article 13 of the DMA which prohibits anti-circumvention efforts by designated gatekeepers. Specifically, Article 13(6) which states:</p>

          <blockquote>
          <p>6. The gatekeeper <strong>shall not degrade the conditions or quality of any of the core platform services</strong> provided to <strong>business users</strong> or <strong>end users</strong> who <strong>avail themselves of the rights</strong> or choices laid down in Articles 5, 6 and 7, or ...</p>
          <cite><a rel="noreferrer" target="_blank" href="https://eur-lex.europa.eu/legal-content/EN/TXT/?toc=OJ%3AL%3A2022%3A265%3ATOC&amp;uri=uriserv%3AOJ.L_.2022.265.01.0001.01.ENG#:~:text=6.%C2%A0%C2%A0%C2%A0The%20gatekeeper%20shall%20not%20degrade,user%20interface%20or%20a%20part%20thereof.">EU Digital Markets Act, Article 13(6)</a> (emphasis added)</cite>
          </blockquote>

          <p>It is still possible for Apple to reverse course and preserve essential functionality iOS users and developers have relied on since 2007 when Steve Jobs introduced Web Apps for the iPhone. Degrading these features in iOS and Safari is not required by the DMA. We encourage Apple to engage with all stakeholders urgently, transparently, and in good faith to restore and enhance these essential capabilities.
          </p>
          
          <p>Preserving these features, making them available to competitors, and allowing browser choice worldwide is the only good-faith path forward, and we call on you to both comply with Apple’s legal obligations and to allow fair and effective competition on your platforms globally. Apple has the ability to compete on merit, rather than relying on lock-in and self-preferencing.</p>

          <p>Sincerely</p>
        </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Netlify just sent me a $104K bill for a simple static site (1361 pts)]]></title>
            <link>https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/</link>
            <guid>39520776</guid>
            <pubDate>Tue, 27 Feb 2024 06:29:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/">https://old.reddit.com/r/webdev/comments/1b14bty/netlify_just_sent_me_a_104k_bill_for_a_simple/</a>, See on <a href="https://news.ycombinator.com/item?id=39520776">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>So I received an email from Netlify last weekend saying that I have a $104,500.00 bill overdue. At first I thought this is a joke or some scam email but after checking my dashboard it seems like I am truly owing them 104K dollars:</p>

<p><a href="https://preview.redd.it/do33l577a2lc1.png?width=1091&amp;format=png&amp;auto=webp&amp;s=6718c5f3358e3df4c6e502fc7c7a558eec47a2cf">That's 190TB bandwidth in 4 days</a></p>

<p>So I was like 😅😅😅 and think okay maybe I got ddos attacked. Since Netlify charges 55$/100GB for the exceeding bandwith, the peak day Feb 16 has 33385/55 * 100GB = 60.7TB bandwidth in a day. I mean, it's not impossible but why attack a simple static site like mine? This site has been on Netlify for 4 years and is okay with the free tier. The monthly bandwith never exceeded even 10GB, and has only ~200 daily visitors.</p>

<p>I contacted their billing support and they responded me that they looked into it and the bandwidth came from some user agents, meaning it is a ddos attack. Then they say such cases happen and they usually charge their customer 20% on this. And since my amount is too large, they offer to discount to 5%, which means I still need to pay 5 thousand dollars.</p>

<p>This feels more like a scam to me. Why do serverless platforms like Netlify and Vercel not have ddos protection, or at least a spend limit? They should have alerted me if the spending skyrocketed. I checked my inbox and spam folder and found nothing. The only email is "Extra usage package purchased for bandwidth". It feels like they deliberately not support these features so that they can cash grab in situations like this.</p>

<p>The ddos attack was focused on a file on my site. Yes it's partly my fault to put a 3.44MB size sound file on my site rather than using a third-party platform like SoundCloud. But still this doesn't invalidate the point of having protection against such attacks, and limit the spending.</p>

<p>And yes I have migrated my site to Cloudflare. Learned my lesson and will never use Netlify (or even Vercel) again.</p>

<p>​</p>

<p>UPDATE: Thank you all for the suggestions I have posted this on <a href="https://news.ycombinator.com/item?id=39520776">HackerNews</a>.</p>

<p>​</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[The creator economy can't rely on Patreon (106 pts)]]></title>
            <link>https://joanwestenberg.com/blog/the-creator-economy-cant-rely-on-patreon</link>
            <guid>39520250</guid>
            <pubDate>Tue, 27 Feb 2024 04:45:30 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://joanwestenberg.com/blog/the-creator-economy-cant-rely-on-patreon">https://joanwestenberg.com/blog/the-creator-economy-cant-rely-on-patreon</a>, See on <a href="https://news.ycombinator.com/item?id=39520250">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-76f9a593074a73766bae">
  <p>I've been thinking about a friend who used to run a cooking channel on YouTube. When she first started, it was just her goofing around in her tiny apartment kitchen, sharing recipes and little cooking hacks. And people loved it. She was able to quit her office job and do her channel full-time.</p><p>Eventually, her viewers were asking for more recipes, more elaborate productions, and fancier kitchen gadgets to review. Suddenly, she needed some serious cash to keep it all going. She started taking on sponsors, then product placements, then brand deals. I watched her churn-out videos, just ads for knives, mixers, or meal kits. And I could see the light going out of her eyes a little.</p><p>She felt like she had no creative freedom or joy in her creations. And the only answer had been a pivot to direct financial support. But despite racking up tens of thousands of views per video, she struggled to convert more than a tiny fraction into paid subscribers.</p><p>Finally, we were grabbing coffee, and she just broke down. She asked me, "At what point am I selling out? And is it even worth it?"</p><p>I didn't have an answer. I don't think anyone does. </p><p>Creators who are burned out by renting space on someone else’s platform and playing the Shopping Channel game, squeezing dollars out of sponsored promotions, eventually shift toward a direct funding patronage model. </p><p>The promise of it is certainly attractive. </p><p>But it's just not realistic.</p><p>From Ghost to Patreon memberships and everything in between, there are more options than ever for artists, musicians, writers, and video producers to get paid directly by their audience. It's the <a href="https://kk.org/thetechnium/1000-true-fans/" target="_blank">1,000 true fans theory</a> that we've all been sold for the past 15 years - that all you need is a strong mailing list of people who give a shit, and a healthy living will follow.</p><p>Unfortunately, a theory is all it is. </p><p>Put simply, the numbers don't add up. Data from Patreon and Substack suggests the average conversion rate from follower to paying fan is about 5%. This means a creator would need a total fanbase of 20,000 followers to yield 1,000 paying supporters. And building a core fanbase of 20,000 engaged followers is extremely difficult in today's crowded creative landscape. </p>
</div><div data-block-type="2" data-border-radii="{&quot;topLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;topRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomLeft&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0},&quot;bottomRight&quot;:{&quot;unit&quot;:&quot;px&quot;,&quot;value&quot;:0.0}}" id="block-yui_3_17_2_1_1707109725824_15020">
  <p>Relying solely on organic user payments rarely provides reliable and adequate income. Creators soon discover building a subscriber base is far easier said than done. Though some succeed due to viral content or niche popularity, creators are more often stranded in the discouraging and disappointing gap between audience reach and monetisable support.</p><p>In a crowded market, the supply of content creators hoping to profit from their work directly outstrips demand. The number of YouTube channels, podcasts, Substack newsletters, and other independently produced media has exploded. The signal-to-noise ratio is utterly unhinged. Talented creators struggle to stand out and attract an audience, let alone convince fans to pay up regularly. It is statistically unlikely that any random podcast or YouTube channel will blow up in popularity to the point of replacing the creator's working salary through direct payments.</p><p>And even when virality strikes, all forms of content rely on easily substitutable free alternatives. YouTube and Apple Podcasts offer a nearly endless feed of free, on-demand videos on every topic imaginable. Convincing music and video fans to pay even a few dollars monthly for exclusive content requires immense effort when similar artists offer work for free. The same dynamic applies to written content. With so many high-quality blogs and news sites available at no cost, it's a hard sell to entice swathes of readers to pay for niche reporting or commentary.</p><p>The challenges of direct audience monetisation go beyond marketplace saturation and free rider problems.</p><p>The transactional ask inherent in requesting money damages community trust and goodwill. Turning fans into individual revenue streams backfires, breaking the genuine parasocial relationships creators build with their audiences. The shift from viewing fans as community members to income sources changes social dynamics in ways many find unpalatable.</p><p><a href="https://www.vox.com/culture/2024/2/1/24056883/tiktok-self-promotion-artist-career-how-to-build-following" target="_blank">Creators themselves dislike the constant hustle associated with extracting money from fans</a>. Having to endlessly pitch subscriber benefits and exclusive content takes mental bandwidth away from simply creating high-quality work. The non-stop social media promotion required to maintain income flows also detracts from production time. Burnout is common for independent creators struggling to balance business development and content creation.</p><p>This is not to say achieving direct fan funding is impossible. Writers like Ben Thompson at Stratechery prove that. Critically, though, it remains the exception rather than the norm.</p><p>More commonly, independent creators utilise audience monetisation to supplement other incomes. Partnering crowd-funding models with advertising revenue, part-time work, freelancing, grants, merchandising, or institutional funding helps mitigate the uncertainty of relying purely on consumers.</p><p>And that's a good thing. Hybrid income streams hedge against the risk of inconsistent audience support drying up. Viewing direct monetisation tools as only one part of a sustainable funding model keeps expectations realistic. With multiple paychecks, a drop in Patreon backers or newsletter subscribers only sinks part of the operation. </p><p>The primary objective in any creative work should always be developing and disseminating content that truly resonates with and engages your audience. The content should be meaningful, insightful, written, recorded or filmed for your audience. The people who give a shit. The people who care.</p><p>If anything obstructs this goal, it must be eliminated or re-evaluated, regardless of its nature, even if that means abandoning monetisation strategies. They should never compromise the quality and relevance of the content. The audience's need for meaningful interaction should always take precedence over any potential revenue generation.</p><p>And creators should be strategic and intentional in their approach, diversifying their income streams and not placing all their hopes on a single form of monetisation. It's the only way to build a resilient, sustainable creative model that can withstand the ebbs and flows of audience engagement and market trends.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Rapier is a set of 2D and 3D physics engines written in Rust (278 pts)]]></title>
            <link>https://rapier.rs/docs/</link>
            <guid>39519894</guid>
            <pubDate>Tue, 27 Feb 2024 03:39:02 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://rapier.rs/docs/">https://rapier.rs/docs/</a>, See on <a href="https://news.ycombinator.com/item?id=39519894">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><article><header></header><div><p>Rapier is a set of 2D and 3D physics engines written using the <a href="https://www.rust-lang.org/" target="_blank" rel="noopener noreferrer">Rust programming language</a>.
It targets applications requiring real-time physics like video games, animation, and robotics. It is designed to be fast,
stable, and optionally cross-platform deterministic. Rapier features include:</p><ul><li>Rigid-body collisions and forces.</li><li>Joint constraints.</li><li>Contact events and sensors.</li><li>Snapshotting.</li><li>Optional cross-platform determinism.</li><li>JavaScript bindings.</li><li>And more…</li></ul><p>Rapier is free and open-source, released under the Apache 2.0 license. It is developed by the <a href="https://dimforge.com/" target="_blank" rel="noopener noreferrer">Dimforge</a>
open-source company. You can support us by sponsoring us on <a href="https://github.com/sponsors/dimforge" target="_blank" rel="noopener noreferrer">GitHub sponsor</a>.</p><p><img src="https://www.dimforge.com/img/logo/logo_dimforge_full" alt="dimforge_logo"></p></div></article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[HNInternal: Ask HN: People who switched from GPT to their own models. How was it? (142 pts)]]></title>
            <link>https://news.ycombinator.com/item?id=39519692</link>
            <guid>39519692</guid>
            <pubDate>Tue, 27 Feb 2024 03:06:56 GMT</pubDate>
            <description><![CDATA[<p>See on <a href="https://news.ycombinator.com/item?id=39519692">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
            <tbody><tr id="39520739"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520739" href="https://news.ycombinator.com/vote?id=39520739&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>A couple of months ago I attended a presentation of an on-prem LLM. An audience member asked, if it was using OpenAI in any way.<p>The presenter, somewhat overeagerly, "Why not ask our new AI?" and went on to type: "Are you an independent  model or do you use OpenAI?"</p><p>To chat bot answered in flourish language that sure it was using ChatGPT as a backend. Which it was <i>not</i> and which was kind of the whole point of the presentation.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521180"><td></td></tr>
                  <tr id="39520705"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520705" href="https://news.ycombinator.com/vote?id=39520705&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Obviously talking my own book here, but we've helped dozens of customers make the transition from prompted GPT-4 or GPT-3.5 to their own fine-tuned models at OpenPipe.<p>The most common reaction I get is "wow, I didn't expect that to work so well with so little effort". For most tasks, a fine-tuned Mistral 7B will consistently outperform GPT-3.5 at a fraction of the cost, and for some use cases will even match or outperform GPT-4 (particularly for narrower tasks like classification, information extraction, summarization -- but a lot of folks have that kind of task). Some aggregate stats are in our blog: <a href="https://openpipe.ai/blog/mistral-7b-fine-tune-optimized">https://openpipe.ai/blog/mistral-7b-fine-tune-optimized</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521055"><td></td></tr>
            <tr id="39520787"><td></td></tr>
                <tr id="39520819"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520819" href="https://news.ycombinator.com/vote?id=39520819&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I don't think they've released a fine-tuning API, but we'll definitely support it once they do!</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520078"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520078" href="https://news.ycombinator.com/vote?id=39520078&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I fine-tuned an LLM to do technical stuff. It works pretty darn good. What I actually discovered is that when evaluating LLMs, it is surprisingly difficult to evaluate them. And, also, that GPT 4 isn't that great, in general.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520227"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520227" href="https://news.ycombinator.com/vote?id=39520227&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Could you provide more details on this matter? Specifically, I'm interested in knowing which base model you've utilized and the approach you've taken to fine-tune it. Your insights would be greatly appreciated and highly beneficial.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520289"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520289" href="https://news.ycombinator.com/vote?id=39520289&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>For narrow stuff you can do better job than base gpt4/mistral/etc model. You fine tune it with your very custom data, stuff that got didn’t seem to be trained on, it will generalize it well.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520315"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39520315" href="https://news.ycombinator.com/vote?id=39520315&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Have you done this? How did you do it?<p>I've been looking forward to someone providing a detailed guide on how to "fine tune it with your custom data" for ages!
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520567"><td></td></tr>
            <tr id="39520544"><td></td></tr>
            <tr id="39520486"><td></td></tr>
                <tr id="39521116"><td><table>  <tbody><tr>    <td indent="5"><img src="https://news.ycombinator.com/s.gif" height="1" width="200"></td><td>
      <center><a id="up_39521116" href="https://news.ycombinator.com/vote?id=39521116&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>You're not wrong. There's been a lot of drama over licensing and releasing datasets, and a lot of the LLM scene are just pitchmen and promoters with no better grasp over what they're doing than "trust me, it's better".<p>Like with "prompt engineering", a lot of people are just hiding how much of the heavy lifting is from base models and a fluke of the merge. The past few "secret" set leaks were low/no delta diffs to common releases.</p><p>I said it a year ago, but if we want to wowed, make this a job for MLIS holders and references librarians. Without thorough, thoughtful curation, these things are just toys in the wrong hands.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                                    <tr id="39520737"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520737" href="https://news.ycombinator.com/vote?id=39520737&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Maybe the key to a good universal LLM is having multiple fine tuned models for various domains. The user thinks he's querying a single model but really there's some mechanism that selecting the best model for his query out of say like 300 different possibilities.<p>This also helps distributes traffic as a side effect.</p><p>I guess the problem is how the conversation would flow. If the user changes topics from say art to quantum physics then asks a question about quantum physics and art then I'm not sure what the algorithm should do.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520877"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520877" href="https://news.ycombinator.com/vote?id=39520877&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>That is actually the same idea as the (now) popular "Mixture of Experts" approach.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520836"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520836" href="https://news.ycombinator.com/vote?id=39520836&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>The user could talk to an "expert opinion aggregator" model which in turn makes a bunch of queries to specialized models.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520578"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520578" href="https://news.ycombinator.com/vote?id=39520578&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>&gt; GPT 4 isn't that great, in general<p>same here, it doesn't adhere to explicit instructions, maybe one or two simple instructions are ok but not more complex ones
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39521179"><td></td></tr>
            <tr id="39520783"><td></td></tr>
                  <tr id="39520085"><td></td></tr>
            <tr id="39520096"><td></td></tr>
                <tr id="39520299"><td></td></tr>
                        <tr id="39520095"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520095" href="https://news.ycombinator.com/vote?id=39520095&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Running Mistral-Instruct-0.1 for call/email summarization, Mixtral for contract mining &amp; OpenChat to augment agentic chatbot equipped with RAG tools(Instruct again).<p>Experience has been great, INT8 tradeoffs are acceptable until hardware FP8(FP4 anyone?) becomes more widely &amp; cheaply available. On-prem costs have been absorbed already for few boxes of A100s &amp; legacy V100s running millions of such interactions.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                <tr id="39520327"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520327" href="https://news.ycombinator.com/vote?id=39520327&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>What are the trade-offs with INT8? I thought even the INT4 loss of accuracy was small and the INT8 loss almost unmeasurable.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520298"><td></td></tr>
                <tr id="39521331"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39521331" href="https://news.ycombinator.com/vote?id=39521331&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Basically augmenting users parsing PDFs &amp; looking to prefill values into Excel instead of typing it all out. Ex. Liabilities, time-period/frequencies mentioned, owners of clauses etc.</span></p></div></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520303"><td></td></tr>
                <tr id="39520528"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520528" href="https://news.ycombinator.com/vote?id=39520528&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I've tried code-llama with Ollama, along with Continue.dev and found it to be pretty good. The only downside is that I couldn't "productively" run the 70B version, even on my MBP with M3 Max with 36GB of RAM (which interestingly should be enough to hold quantized model weights). It was simply painfully slow. 34B one works good enough for most of my use-cases, so I am happy.</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520887"><td><table>  <tbody><tr>    <td indent="2"><img src="https://news.ycombinator.com/s.gif" height="1" width="80"></td><td>
      <center><a id="up_39520887" href="https://news.ycombinator.com/vote?id=39520887&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I tried to use codellama 34B and I think it is pretty bad. For Example I asked it to convert a comment into a docstring and it would hallucinate a whole function around it.</span></p></div></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520554"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520554" href="https://news.ycombinator.com/vote?id=39520554&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>deepseek-coder 6.7b is seriously impressive for how quickly it runs on an M1 Max. There’s a few spots where it still doesn’t fare quite as well as ChatGPT but it’s a small tradeoff considering that it’s fully local and doesn’t even spin up my laptop’s fans.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520687"><td></td></tr>
                  <tr id="39520729"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520729" href="https://news.ycombinator.com/vote?id=39520729&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I prefer to use local models when running data extraction or processing over 10k or more records. Hosted services would be slow and brittle at this point.<p>Mistral 7B fine-tunes (OpenChat is my favorite) just chug through the data and get the job done.</p><p>Details: using vLLM to run the models. Using ChatGPT-4 to condense information for complex prompts (that the local models will execute).</p><p>I think, the situation will just keep on getting better with each month.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39521123"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39521123" href="https://news.ycombinator.com/vote?id=39521123&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Im using mixtral 8x7b (q5) for my use cases, such as scripting, searching for ideas and or definitions that i allways need to factcheck.<p>Currently i use lmstudio on my m2 with 96gb ram. But i‘m looking into switchin to ollama or another oss solution.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520895"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520895" href="https://news.ycombinator.com/vote?id=39520895&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>Mixed results. I think llama2 in general is pretty bad, especially at anything else than english. I've had very good results with Mixtral for Chat.<p>Of course all of them feel like a Frankenstein compared to actual ChatGPT. They feel similar and work just as well until, sometimes, they put out complete and utter garbage or artifacts and you wonder if they skimped on fine-tuning.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520654"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520654" href="https://news.ycombinator.com/vote?id=39520654&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I tested a bunch of models while building <a href="https://double.bot/">https://double.bot</a> but ended up back on gpt4. Other models are fun to play with but it gets frustrating even if they miss 1/100 questions that gpt4 gets. I find that right now I get more value implementing features around the model that fixes all the GitHub copilot papercuts (autocomplete that closes brackets properly, auto import upon accepting suggestions, disable suggestions when writing comments to be less distracting, midline completions, etc etc)<p>Hopefully os models can catch-up to gpt4 in the next six months when we fixed all the low hanging fruit outside of the model itself
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520461"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520461" href="https://news.ycombinator.com/vote?id=39520461&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>We support both in our app and enterprise product. The APIs (OpenAI) vs libraries (i.e. llama.cpp for on-device) are so similar that the switch is basically transparent to the user. We're adding support for other platforms APIs soon, and everything we've looked so far is as easy to integrate as OpenAI - except Google that for some reason complicates everything on Google Cloud.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520592"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520592" href="https://news.ycombinator.com/vote?id=39520592&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>To add to this question, are there LLMs that I can run on my own data, that also can provide citations similar to the way phind.com does for their results? Even better if they are multilingual.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520665"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520665" href="https://news.ycombinator.com/vote?id=39520665&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>It’s been ok. I’m running llama2 7b and it’s … fine. The results I get from gpt4 aren’t much better. This for general tasks.<p>Mostly I think I need to use LLMs more effectively
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520297"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520297" href="https://news.ycombinator.com/vote?id=39520297&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>My 2024 prediction is we will see far more people moving off of openai once they encounter its cost and latency compared to (less proven/scaled) competitors. It’s often a speed versus quality tradeoff, and I’ve seen multiple providers 3x faster than OpenAI with far more than 1/3 the quality</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520598"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520598" href="https://news.ycombinator.com/vote?id=39520598&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>OpenAI currently leads the front on (commercial) AI, so I doubt people will switch. In fact most offerings become outdated pretty fast and everyone else tries to play catch up.<p>Imagine using a GPT-2 type model when everyone else is using GPT-4. Until the dust settles there's no point in investing in alt models imo, unless you're leading the research.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520352"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520352" href="https://news.ycombinator.com/vote?id=39520352&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>I greatly prefer to use ChatGPT-4 instead of 3.5 despite the slowness. Really a good feature for them to have would be to easily re-run a prompt on 4. However, the glitchiness of the service is kind of annoying.</span></p></div></td></tr>
        </tbody></table></td></tr>
                            <tr id="39520466"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520466" href="https://news.ycombinator.com/vote?id=39520466&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>We have a first pass with our own model and then escalate to gpt if we aren't sure of our own model's results.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520502"><td><table>  <tbody><tr>    <td indent="0"><img src="https://news.ycombinator.com/s.gif" height="1" width="0"></td><td>
      <center><a id="up_39520502" href="https://news.ycombinator.com/vote?id=39520502&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>Would be great if people could share their app demo, host &amp; model used/trained for better context.</span></p></div></td></tr>
        </tbody></table></td></tr>
            <tr id="39520497"><td></td></tr>
            <tr id="39520208"><td></td></tr>
                <tr id="39520457"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520457" href="https://news.ycombinator.com/vote?id=39520457&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>I think the llm utility[0] (the one from Simon, not Google) is probably the best quickstart experience you can find. Gives the option to connect to services via API or install/run local models.<p>As simple as</p><pre><code>  pip install llm
  # add the local plugin
  llm install llm-gpt4all
  # Download and run a prompt against the Orca Mini 7B model
  llm -m orca-mini-3b-gguf2-q4_0 'What is the capital of France?'
</code></pre>
Alternatively, you could use the llamafile[1] which is a tiny binary runner which gets packaged ontop of the multigigabyte models. Download the llamafile and you can launch it through your terminal or a web browser.<p>From the llamafile page, after you download the file, you can just launch it as</p><pre><code>  ./mistral-7b-instruct-v0.2.Q5_K_M.llamafile -ngl 9999 --temp 0.7 -p '[INST]Write a story about llamas[/INST]'
</code></pre>
[0] <a href="https://llm.datasette.io/en/stable/index.html" rel="nofollow">https://llm.datasette.io/en/stable/index.html</a><p>[1] <a href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a></p><p>Edit: added llm quickstart from the intro page
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520551"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520551" href="https://news.ycombinator.com/vote?id=39520551&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>If you don't care about the details of how those model servers work, then something that abstracts out the whole process like LM Studio or Ollama is all you need.<p>However, if you want to get into the weeds of how this actually works, I recommend you look up model quantization and some libraries like ggml[1] that actually do that for you.</p><p>[1] <a href="https://github.com/ggerganov/ggml">https://github.com/ggerganov/ggml</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
            <tr id="39520243"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520243" href="https://news.ycombinator.com/vote?id=39520243&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><br><div>
                  <p><span>You can try going get some pre-trained (sometimes, fine-tuned) models on HuggingFace, following their instructions. Good luck!</span></p></div></td></tr>
        </tbody></table></td></tr>
                <tr id="39520444"><td></td></tr>
                <tr id="39520517"><td><table>  <tbody><tr>    <td indent="3"><img src="https://news.ycombinator.com/s.gif" height="1" width="120"></td><td>
      <center><a id="up_39520517" href="https://news.ycombinator.com/vote?id=39520517&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>it's all pretty well put together nowadays honestly.<p>here's a dead simple way : (1) download LM Studio, install it[0] (2) download a model from within the client when prompted (3) have a ball.</p><p>the program is fairly intuitive, it takes care of finding the relevant files, and it can even accept addendum prompts and various ways to flavor or specialize answers.</p><p>Learn the basics there, take what you learn to a more 'industrial' playground later on.</p><p>[0]: <a href="https://lmstudio.ai/" rel="nofollow">https://lmstudio.ai/</a>
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                        <tr id="39520280"><td></td></tr>
            <tr id="39520294"><td></td></tr>
            <tr id="39520241"><td></td></tr>
            <tr id="39520686"><td><table>  <tbody><tr>    <td indent="1"><img src="https://news.ycombinator.com/s.gif" height="1" width="40"></td><td>
      <center><a id="up_39520686" href="https://news.ycombinator.com/vote?id=39520686&amp;how=up&amp;goto=item%3Fid%3D39519692"></a></center>    </td><td><p><span>The other answers are recommending paths which give you #1. less control and #2. projects with smaller eco-systems.<p>If you want a truly general purpose front-end for LLMs, the only good solution right now is oobabooga: <a href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p><p>All other alternatives have only small fractions of the features that oobabooga supports. All other alternatives only support a fraction of the LLM backends that oobabooga supports, etc.
              </p></span></p></td></tr>
        </tbody></table></td></tr>
                  <tr id="39520069"><td></td></tr>
            </tbody></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Is Tableau Dead? (132 pts)]]></title>
            <link>https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau</link>
            <guid>39519145</guid>
            <pubDate>Tue, 27 Feb 2024 01:54:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau">https://www.mergeyourdata.com/blog/is-tableau-dead-the-future-of-tableau</a>, See on <a href="https://news.ycombinator.com/item?id=39519145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><h2>Is Tableau Dead?</h2><p>No... and Yes.</p><p>The momentum and community around Tableau has been lost. But current and projected future revenue growth is solid. It's not the company that it used to be, but it'll be a major player as long as it's tied to Salesforce.</p><p>Similar to how no one celebrates Oracle products and shouts from the rooftops, Tableau will become a tool in the Salesforce toolbox that major players will continue to use.</p><hr noshade="" size="1"><p>I've been working with Tableau for more than 10 years. Back before there was Tableau Cloud (formerly Tableau Online). Before there was Tableau Prep. Before the Salesforce acquisition.</p><p>During this decade, you could feel the momentum. The product was up-and-coming. Developers, users, and executives were excited about the future possibilities of the tool. The insights it could and was uncovering.</p><p>The "koolaid" tasted good and was still nutritious.</p><p>But after the Salesforce acquisition, things started changing.</p><h3>Unofficial Leading and Lagging Indicators</h3><p>In 2019, Salesforce acquired Tableau. A heavyweight acquired an upcoming superstar. It would be like Michael Jordan bringing second year Lebron James onboard. Or maybe it is more like Darko Miličić getting drafted by the Detroit Pistons with Ben Wallace, Chauncey Billups, Richard Hamilton, and Tayshaun Prince on it?</p><blockquote><p>For non-NBA historians, Darko was considered a bust after getting brought on by the at-the-time NBA powerhouse, the Detroit Pistons. Lebron James meanwhile has had an incredibly successfuly career, living up to expectations.</p></blockquote><p>The 2019 acquisition itself was received with mixed emotions. Some were excited about the resources and market Tableau would get access to. Others were skeptical about the product's culture and vision with a new boss in place.</p><p>Both parties ended up being correct with their perspectives.</p><p>The leading indicators showed up quickly. Product updates that were long overdue started getting built. The community fragmented and lost previously active contributors.</p><p>The lagging indicators showed up after 2 to 3 years. People fed up with Tableau/Salesforce support (or lackthereof). Questions about the future of the company and momentum (like this post). Salesforce executives <a href="https://www.bnnbloomberg.ca/salesforce-guts-tableau-after-spending-15-7-billion-in-2019-deal-1.1866349" rel="noopener noreferrer nofollow" target="_blank">mentioning Tableau less than Slack or Mulesoft on their public calls</a>.</p><p>So depending on what you have focused on, Tableau is doing both great and heading in the wrong direction at the same time.</p><h3>Getting Past the Indicators</h3><p>When a company grows with koolaid, it also fails when there is a shortage of koolaid. But this is difficult to measure and see from a performance aspect until much later. <a href="https://www.geekwire.com/2023/salesforce-stock-spikes-14-after-beating-q4-estimates-tableau-revenue-grows-3-to-636m/" rel="noopener noreferrer nofollow" target="_blank">Revenue still looks great</a>. It's growing year over year.</p><p>But most Tableau executives have left as of now. We've personally had clients and prospects trading Tableau for Power BI, native data viz tools in their other software, or just simply dropping the software all together. </p><h2>The Future of Tableau</h2><p>The competition in the data visualization space is increasing. New companies that directly address the shortcomings of Tableau and similar data visualization tools are starting to take foot.</p><p>It'll take years for those companies to eat into any significant market share of Tableau. But like Salesforce's race against HubSpot; new incumbents have ample opportunity to succeed with better custom service and more targeted problems that it solves with data.</p><p>Some of these up and comers include more flexible solutions like <a href="https://omni.co/" rel="noopener noreferrer nofollow" target="_blank">Omni</a>. Others are more targeted toward simplicity like <a href="https://app.databox.com/datawall/0be921146ad72ee8c631430a5b9b39570625489f6?fp_ref=dan56" rel="noopener noreferrer nofollow" target="_blank">Databox</a>.</p><p>Meanwhile, Microsoft's Power BI is on a hot streak. It's already internally approved and accessible by companies on the Microsoft stack. It's "free" to start with and in Microsoft's customers' stack, so business users and developers alike can build POCs with real company data they have access to.</p><p>It's a real threat to the core business of Tableau. Many companies are reducing SaaS subscription costs and complexity after Covid made them run rampant. Microsoft already has relationships to nearly all of these large enterprise accounts of Tableau. And with the stickiness of Microsoft's products, they can take losses on Power BI to take Tableau market share (if they choose).</p><p>Obviously there are technical differences between the platforms, but that's not the real decider of whether someone chooses Tableau or Power BI. This is the only true short-term threat to Tableau's enterprise customers in my opinion.</p><h3>Where That Leaves Tableau</h3><p>Tableau seems like it's settling into an incumbent role similar to Qlik. I believe it'll be a consistent player at large institutions who typically go through complex project and procurement processes.</p><p>It'll be a necessary evil that people will view the same as Salesforce. The view that it's for the more complex use cases and expensive. But once you reach a certain growth point, it's the option you need to go with.</p><p>Companies that are already using Salesforce will expand their usage to Tableau as well. Similar to how Power BI is being adopted by companies who are already neck deep in the Microsoft stack. Similar to how Oracle sells multiple products under their umbrella once they have an inked relationship with a customer.</p><p>It will no longer be the hot new tool that will be embraced by SMBs. The community will not have the hope and excitement it had in the 2010s. Instead, they'll constantly voice frustration that no one in Salesforce cares about their customers and that they'll just be a line on a spreadsheet again. But it won't matter, because the Tableau revenue will continue to steadily grow as Salesforce digs its heels into current revenue streams.</p><p>Small players will snap up opportunities of frustrated former Tableau customers. They'lll grow until they're acquired by a bigger player wanting access to that market.</p><p>And the cycle will continue as it always does.</p><h3>Final Conclusion</h3><p>Tableau will continue to be used in Enterprise and in large government. Power BI will eat some of that market share (you might've witnessed this at your compay already). It'll be further moved into an "add-on" role for Salesforce instead of a standalone product.</p><p>SMBs will seek other solutions that are more flexible, easier to use, or cheaper. Salesforce won't care too much about this as they'll be focused on the larger accounts.</p><p>If you're a Tableau developer, your opportunities will be there, but will look more and more like a Qlik developer's opportunities.</p><p>The "good times" in the community have ended. #Datafam will become more like other incumbent communities. The magic is no longer there and that's ok. Nothing lasts forever.</p><hr noshade="" size="1"><p>This article is written by Dan Saavedra, Founder of <a href="http://mergeyourdata.com/" rel="noopener noreferrer nofollow" target="_blank">MergeYourData.com</a>. Identify your most profitable customer segment and double down on growing what works.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[A Font that Renders 14 types of Charts (142 pts)]]></title>
            <link>https://www.vectrotype.com/chartwell</link>
            <guid>39518964</guid>
            <pubDate>Tue, 27 Feb 2024 01:32:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.vectrotype.com/chartwell">https://www.vectrotype.com/chartwell</a>, See on <a href="https://news.ycombinator.com/item?id=39518964">Hacker News</a></p>
&lt;Unparsable&gt;]]></description>
        </item>
        <item>
            <title><![CDATA[Bpftop: Streamlining eBPF performance optimization (111 pts)]]></title>
            <link>https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771</link>
            <guid>39518791</guid>
            <pubDate>Tue, 27 Feb 2024 01:04:09 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771">https://netflixtechblog.com/announcing-bpftop-streamlining-ebpf-performance-optimization-6a727c1ae2e5?gi=223d75ac1771</a>, See on <a href="https://news.ycombinator.com/item?id=39518791">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><div><a href="https://netflixtechblog.medium.com/?source=post_page-----6a727c1ae2e5--------------------------------" rel="noopener follow"><div aria-hidden="false"><p><img alt="Netflix Technology Blog" src="https://miro.medium.com/v2/resize:fill:88:88/1*BJWRqfSMf9Da9vsXG9EBRQ.jpeg" width="44" height="44" loading="lazy" data-testid="authorPhoto"></p></div></a><a href="https://netflixtechblog.com/?source=post_page-----6a727c1ae2e5--------------------------------" rel="noopener  ugc nofollow"><div aria-hidden="false"><p><img alt="Netflix TechBlog" src="https://miro.medium.com/v2/resize:fill:48:48/1*ty4NvNrGg4ReETxqU2N3Og.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto"></p></div></a></div><p id="976a"><em>By </em><a href="https://www.linkedin.com/in/josefernandezmn/" rel="noopener ugc nofollow" target="_blank"><em>Jose Fernandez</em></a></p><p id="5c31">Today, we are thrilled to announce the release of <a href="https://github.com/Netflix/bpftop" rel="noopener ugc nofollow" target="_blank">bpftop</a>, a command-line tool designed to streamline the performance optimization and monitoring of eBPF programs. As Netflix increasingly adopts eBPF [<a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/extending-vector-with-ebpf-to-inspect-host-and-container-performance-5da3af4c584b">1</a>, <a rel="noopener ugc nofollow" target="_blank" href="https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96">2</a>], applying the same rigor to these applications as we do to other managed services is imperative. Striking a balance between eBPF’s benefits and system load is crucial, ensuring it enhances rather than hinders our operational efficiency. This tool enables Netflix to embrace eBPF’s potential.</p><figure></figure><h2 id="9661">Introducing bpftop</h2><p id="a934">bpftop provides a dynamic real-time view of running eBPF programs. It displays the average execution runtime, events per second, and estimated total CPU % for each program. This tool minimizes overhead by enabling performance statistics only while it is active.</p><figure></figure><p id="0afb">bpftop simplifies the performance optimization process for eBPF programs by enabling an efficient cycle of benchmarking, code refinement, and immediate feedback. Without bpftop, optimization efforts would require manual calculations, adding unnecessary complexity to the process. With bpftop, users can quickly establish a baseline, implement improvements, and verify enhancements, streamlining the process.</p><p id="f37b">A standout feature of this tool is its ability to display the statistics in time series graphs. This approach can uncover patterns and trends that could be missed otherwise.</p><h2 id="a0b7">How it works</h2><p id="c629">bpftop uses the <a href="https://elixir.bootlin.com/linux/v6.6.16/source/include/uapi/linux/bpf.h#L792" rel="noopener ugc nofollow" target="_blank">BPF_ENABLE_STATS</a> syscall command to enable global eBPF runtime statistics gathering, which is disabled by default to reduce performance overhead. It collects these statistics every second, calculating the average runtime, events per second, and estimated CPU utilization for each eBPF program within that sample period. This information is displayed in a top-like tabular format or a time series graph over a 10s moving window. Once bpftop terminates, it turns off the statistics-gathering function. The tool is written in Rust, leveraging the <a href="https://github.com/libbpf/libbpf-rs" rel="noopener ugc nofollow" target="_blank">libbpf-rs</a> and <a href="https://github.com/ratatui-org/ratatui" rel="noopener ugc nofollow" target="_blank">ratatui</a> crates.</p><h2 id="24a2">Getting started</h2><p id="33df">Visit the project’s <a href="https://github.com/Netflix/bpftop" rel="noopener ugc nofollow" target="_blank">GitHub page</a> to learn more about using the tool. We’ve open-sourced bpftop under the Apache 2 license and look forward to contributions from the community.</p></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Segmenting Comic book Frames (183 pts)]]></title>
            <link>https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html</link>
            <guid>39518202</guid>
            <pubDate>Mon, 26 Feb 2024 23:33:55 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html">https://vrroom.github.io/blog/2024/02/23/comic-frame-segmentation.html</a>, See on <a href="https://news.ycombinator.com/item?id=39518202">Hacker News</a></p>
<div id="readability-page-1" class="page"><div aria-label="Content">
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>This post is based on my project in my Computer Vision class last semester</em></p>

<h2 id="introduction">Introduction</h2>

<p>As I was learning classical techniques in my Computer Vision class, I came across a <a href="https://maxhalford.github.io/blog/comic-book-panel-segmentation/">blog post</a> by Max Halford on extracting frames from comic books. He developed a very interesting algorithm where he applied <em>Canny</em> to detect the boundary of frames, filled holes and fit bounding boxes to contiguous regions.</p>

<p>This elegant algorithm did the job very well but had its shortcomings. For one, it didn’t handle arbitrary, un-aligned polygons and didn’t work on <em>negative frames</em>, which didn’t have a boundary of their own, but rather were defined by the boundaries of neighboring frames.</p>

<p>Given the hype around <em>foundation models</em> for segmentation such as <a href="https://github.com/facebookresearch/segment-anything">SAM</a>, I approached this problem by procedurally generating a synthetic dataset of comic books and finetuning SAM to detect the corner points of frames.</p>

<div>
<table>
<tbody><tr>
<td>
  <img src="https://vrroom.github.io/assets/comic_frame_seg/comic_panels.png" width="300px">
</td>
</tr>
</tbody><caption>Failure cases of heuristic approaches: (Top) Frames from Pepper and Carrot by David Revoy are polygons and not axis-aligned bounding boxes. (Bottom) Negative frames may not have a well defined border. </caption>
</table>
</div>

<h2 id="procedural-comic-generator">Procedural Comic Generator</h2>

<p>There isn’t abundant data available for this problem. But that doesn’t mean that we should hold our head in our hands. A common technique that is widely used (see <a href="https://errollw.com/">Erroll Wood’s</a> work) is to procedurally generate training data.</p>

<p>In our case, this means simulating comic books. Note, we don’t really need to make gripping animations and tell a story, we just need to generate panels that look like comics from 50,000 feet. In order to do this, I wrote a procedural generator of layouts and assigned random boxes on an empty image. I filled these boxes with images sampled from the <a href="https://danbooru.donmai.us/">Danbooru</a> dataset.</p>

<p>In order to ensure that the sampled images were atleast semi-coherant, I used <a href="https://github.com/openai/CLIP">CLIP L/14 image encoder</a> to create an image index. While choosing images for a particular page, I sampled one image at random from Danbooru and filled the rest of the boxes using it’s k-nearest neighbors.</p>

<p>With this procedural generator, I had complete control of the size, shape and boundary properties of the box, which I could set appropriately to simulate <em>negative</em> and <em>polygonal</em> frames.</p>



<h2 id="comic-segmentation">Comic Segmentation</h2>

<p>I used SAM as the backbone for my model. SAM is the state-of-the-art image segmentation model. It consists of a heavy, compute expensive image encoder and a light-weight decoder, which answers segmentation queries. The heavy encoder encodes an image only once, after which multiple segmentation queries are answered cheaply. This division of labor is particularly useful for deployment, where an enterprise serving a user can optimize for both speed and costs by keeping the heavy encoder inference on the cloud and using the user’s device for light-weight inference.</p>

<p>Since SAM predicts dense, per pixel mask, I modified it to predict points instead. An overview of the model can be seen below. The procedurally generated comic frame is fed to the image encoder (whose weights remain unchanged during training). A point is randomly sampled from a frame and given as a query/prompt. The light-weight decoder is trained to recover the corners of the frame.</p>

<div>
<table>
<tbody><tr>
<td>
  <img src="https://vrroom.github.io/assets/comic_frame_seg/architecture.png" width="500px">
</td>
</tr>
</tbody><caption>Model Overview</caption>
</table>
</div>

<p>I learned two lessons while training this model. Firstly, it was important to canonicalize the order in which the corners of the frame were predicted. Without this, the model got conflicting signals on the ordering of corner points and never converged. Secondly, it was important to use L1 instead of L2 loss since L2 optimized very quickly without improving the quality of predictions.</p>

<h2 id="evaluation">Evaluation</h2>

<p>I compared my method against original SAM and Halford’s method. Note that Halford’s method is a bit disadvantaged in this comparison since my method also uses a query (set to the center of the ground truth frame to be predicted). Despite this, it is evident that our model trained on our procedurally generated dataset, generalizes on “real-world” comics (Pepper and Carrot abbrev. as P&amp;C), coming close to Halford in the process. It beats Halford on procedurally generated dataset (abbrev. Pr), since this dataset is designed to expose the flaws in the method.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>IoU (P&amp;C)</th>
      <th>PCK@0.1 (P&amp;C)</th>
      <th>L1 (P&amp;C)</th>
      <th>IoU (Pr)</th>
      <th>PCK@0.1 (Pr)</th>
      <th>L1 (Pr)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SAM</td>
      <td>0.42</td>
      <td>0.52</td>
      <td>0.37</td>
      <td>0.81</td>
      <td>0.94</td>
      <td>0.08</td>
    </tr>
    <tr>
      <td>Halford</td>
      <td><strong>0.93</strong></td>
      <td>0.96</td>
      <td><strong>0.04</strong></td>
      <td>0.47</td>
      <td>0.61</td>
      <td>0.47</td>
    </tr>
    <tr>
      <td>Ours</td>
      <td>0.88</td>
      <td><strong>0.98</strong></td>
      <td>0.05</td>
      <td><strong>0.88</strong></td>
      <td><strong>0.99</strong></td>
      <td><strong>0.03</strong></td>
    </tr>
  </tbody>
</table>

<p>Here, IoU simply measures the area of intersection over union of the ground truth and predicted frames. PCK@0.1 refers to the percentage of times, the predicted frame corner lies within certain radius of the ground truth frame corner (0.1 refers to the radius as a percentage of the diagonal of the comic page). L1 is simply the L1 distance between ground truth and predicted frames.</p>

<p>Below are some qualitative results which demonstrate that our method works on “real-world” comics. We run it in two modes. On the left, we interactively provide a query and the model produces the corners. On the right, we sample a bunch of query on the image, predict polygons and filter them using <em>non-maximal suppression</em> like the original SAM paper.</p>



<h2 id="final-thoughts">Final Thoughts</h2>

<p>There are still shortcomings to my method and it can often fail for complex, cluttered comic pages. But still, I like this approach to designing algorithm over composing OpenCV functions because it is often easier to see how to improve the dataset than to design new heuristics. Once you do that, you almost have a guarantee that the Neural Network machinery will get you the results.</p>

<p>The annotated Pepper and Carrot dataset that I used for evaluation can be found in my <a href="https://drive.google.com/file/d/1z8OE8TC8eupC6_ZNxUSVyfvk4rSkVIgE">drive link</a>. All my code and checkpoints are available in my <a href="https://github.com/Vrroom/segment-anything-comic">Github Repo</a>. If you think of any improvements to my approach, feel free to reach out!</p>

<hr>

  </div>
</article>

      </div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Cactus – A Modern Diablo II Version Switcher, Character Isolator, & Mod Manager (101 pts)]]></title>
            <link>https://github.com/fearedbliss/Cactus</link>
            <guid>39518161</guid>
            <pubDate>Mon, 26 Feb 2024 23:29:57 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/fearedbliss/Cactus">https://github.com/fearedbliss/Cactus</a>, See on <a href="https://news.ycombinator.com/item?id=39518161">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><div dir="auto"><h2 tabindex="-1" dir="auto">Cactus <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/61bd41ec7f1d30871fe47ccfcf5f3e765f49c72ede959e1a4a912d5150a857ca/68747470733a2f2f692e696d6775722e636f6d2f6d53775a4a6d722e706e67"><img src="https://camo.githubusercontent.com/61bd41ec7f1d30871fe47ccfcf5f3e765f49c72ede959e1a4a912d5150a857ca/68747470733a2f2f692e696d6775722e636f6d2f6d53775a4a6d722e706e67" height="35" data-canonical-src="https://i.imgur.com/mSwZJmr.png"></a></h2><a id="user-content-cactus-" aria-label="Permalink: Cactus " href="#cactus-"></a></div>
<p dir="auto"><em><strong>A Modern Version Switcher, Character Isolator, and Mod Manager for Diablo II (Original, Not Resurrected)</strong></em></p>
<p dir="auto"><em><strong>By: Jonathan Vasquez (fearedbliss)</strong></em></p>
<p dir="auto"><em><strong>Build: 2024-02-26-2000</strong></em></p>
<ul dir="auto">
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/videos" rel="nofollow">Videos</a></h4><a id="user-content-videos" aria-label="Permalink: Videos" href="#videos"></a></p>
</li>
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/PORTS-COLLECTION.md">Ports Collection</a></h4><a id="user-content-ports-collection" aria-label="Permalink: Ports Collection" href="#ports-collection"></a></p>
</li>
<li>
<p dir="auto"><h4 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/PATCH-NOTES.md">Patch Notes</a></h4><a id="user-content-patch-notes" aria-label="Permalink: Patch Notes" href="#patch-notes"></a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Download</h2><a id="user-content-download" aria-label="Permalink: Download" href="#download"></a></p>
<p dir="auto">The Cactus (Core) source code can be downloaded at the
<a href="https://github.com/fearedbliss/Cactus-Core">Cactus (Core)</a> repo.</p>
<p dir="auto">The complete Cactus package is available as a direct download from my server
(uptime is not guaranteed). <a href="https://www.7-zip.org/download.html" rel="nofollow">7-Zip</a> must be
used to extract the archive since I'm using Ultra compression. All releases are
hashed and PGP signed with the key: <strong><code>34DA 858C 1447 509E C77A D49F FB85 90B7 C4CA 5279</code></strong>,
which can be found at the link below.</p>
<ul dir="auto">
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z" rel="nofollow">Download Cactus</a></h3><a id="user-content-download-cactus" aria-label="Permalink: Download Cactus" href="#download-cactus"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://keys.openpgp.org/search?q=34DA+858C+1447+509E+C77A+D49F+FB85+90B7+C4CA+5279" rel="nofollow">PGP Public Key</a></h3><a id="user-content-pgp-public-key" aria-label="Permalink: PGP Public Key" href="#pgp-public-key"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z.SHA256.txt" rel="nofollow">Latest Release Hash</a></h3><a id="user-content-latest-release-hash" aria-label="Permalink: Latest Release Hash" href="#latest-release-hash"></a></p>
</li>
<li>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://xyinn.org/diablo/Cactus.7z.SHA256.txt.sig" rel="nofollow">Latest Release Hash Signature</a></h3><a id="user-content-latest-release-hash-signature" aria-label="Permalink: Latest Release Hash Signature" href="#latest-release-hash-signature"></a></p>
</li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">License</h2><a id="user-content-license" aria-label="Permalink: License" href="#license"></a></p>
<p dir="auto">Released under the <strong><a href="https://github.com/fearedbliss/Cactus/blob/main/LICENSE.txt">Simplified BSD License</a></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Requirements</h2><a id="user-content-requirements" aria-label="Permalink: Requirements" href="#requirements"></a></p>
<ul dir="auto">
<li><em><strong>Windows 7 or 10 (11+ will not be supported, read below for why)</strong></em></li>
<li><em><strong>.NET Framework 4.6.2+ (Cactus)</strong></em></li>
<li><em><strong>Visual C++ 2015 - 2022 Redistributable (x86) (DSOAL w/ OpenAL Soft)</strong></em></li>
</ul>
<p dir="auto"><h2 tabindex="-1" dir="auto">Screenshots</h2><a id="user-content-screenshots" aria-label="Permalink: Screenshots" href="#screenshots"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Light Mode</h3><a id="user-content-light-mode" aria-label="Permalink: Light Mode" href="#light-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2f251245ed1f4542ec5dcf5b745119e8b251a83fed79072062ca3153fb8c7982/68747470733a2f2f692e696d6775722e636f6d2f566e693333414b2e706e67"><img src="https://camo.githubusercontent.com/2f251245ed1f4542ec5dcf5b745119e8b251a83fed79072062ca3153fb8c7982/68747470733a2f2f692e696d6775722e636f6d2f566e693333414b2e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/Vni33AK.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dark Mode</h3><a id="user-content-dark-mode" aria-label="Permalink: Dark Mode" href="#dark-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0109a3b86ee3de3757ca29881ed7bd83ff7995f9a8f0d441f8c36d29e4167b81/68747470733a2f2f692e696d6775722e636f6d2f57457a6d4878432e706e67"><img src="https://camo.githubusercontent.com/0109a3b86ee3de3757ca29881ed7bd83ff7995f9a8f0d441f8c36d29e4167b81/68747470733a2f2f692e696d6775722e636f6d2f57457a6d4878432e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/WEzmHxC.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Out Of The Box Experience</h3><a id="user-content-out-of-the-box-experience" aria-label="Permalink: Out Of The Box Experience" href="#out-of-the-box-experience"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e6c498d0b0ccf490a4f0dc9d7eff9ab9c492e065e64c3be356cba3ecd763d52b/68747470733a2f2f692e696d6775722e636f6d2f384d57736b4a512e706e67"><img src="https://camo.githubusercontent.com/e6c498d0b0ccf490a4f0dc9d7eff9ab9c492e065e64c3be356cba3ecd763d52b/68747470733a2f2f692e696d6775722e636f6d2f384d57736b4a512e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/8MWskJQ.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Settings (19 Material Design Colors Available + Light/Dark Mode)</h3><a id="user-content-settings-19-material-design-colors-available--lightdark-mode" aria-label="Permalink: Settings (19 Material Design Colors Available + Light/Dark Mode)" href="#settings-19-material-design-colors-available--lightdark-mode"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/53903c6976f052f985ce10a899c8dfbfeba82e038b864a8473fcb36e0c6bd83a/68747470733a2f2f692e696d6775722e636f6d2f6d43614c666b762e706e67"><img src="https://camo.githubusercontent.com/53903c6976f052f985ce10a899c8dfbfeba82e038b864a8473fcb36e0c6bd83a/68747470733a2f2f692e696d6775722e636f6d2f6d43614c666b762e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/mCaLfkv.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Add Entry</h3><a id="user-content-add-entry" aria-label="Permalink: Add Entry" href="#add-entry"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1bda3411a7b88d62613a6e315d4d8f945077196023922cb73c41200bdeb3c6b6/68747470733a2f2f692e696d6775722e636f6d2f436959353844612e706e67"><img src="https://camo.githubusercontent.com/1bda3411a7b88d62613a6e315d4d8f945077196023922cb73c41200bdeb3c6b6/68747470733a2f2f692e696d6775722e636f6d2f436959353844612e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/CiY58Da.png"></a></p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Edit Entry</h3><a id="user-content-edit-entry" aria-label="Permalink: Edit Entry" href="#edit-entry"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/42cbc84e1f0b7e356bae148c2d543acf274cb393639fa8d72efe31579a131bb6/68747470733a2f2f692e696d6775722e636f6d2f6f6f6942576e482e706e67"><img src="https://camo.githubusercontent.com/42cbc84e1f0b7e356bae148c2d543acf274cb393639fa8d72efe31579a131bb6/68747470733a2f2f692e696d6775722e636f6d2f6f6f6942576e482e706e67" alt="Cactus" data-canonical-src="https://i.imgur.com/ooiBWnH.png"></a></p>
<p dir="auto"><h2 tabindex="-1" dir="auto">History</h2><a id="user-content-history" aria-label="Permalink: History" href="#history"></a></p>
<p dir="auto">Cactus started out as just a simple application that allowed you to
easily and efficiently Time Travel between every version of Diablo II
that ever came out, while maximizing disk space, and enabling full
character isolation between versions. However, even though Cactus itself
still is just that, the Cactus Repository has evolved to become a
centralized and historical archive, that aims to preserve every single
Diablo II version that exists (<em><strong>Official Retail</strong></em> and <em><strong>Official Beta</strong></em>
Releases), and has also become an ecosystem where mod developers can distribute
their mods on. Cactus is a complete rewrite from scratch of my previous
application called <em><strong>Bliss Version Switcher</strong></em>. However, since Cactus is
written in C#, it behaves as a native Windows application and allows it
to integrate natively with the system. On the other hand, Bliss Version
Switcher was written in Java and there were many limitations that lead to the
Cactus rewrite.</p>
<p dir="auto">This repository also includes several other utilities that I have either
created, or collected, which can help you play <em><strong>Vanilla</strong></em> Diablo II
better. All Cactus Platforms are <em><strong>Vanilla</strong></em> by default. The only fix
I made to all Platforms below 1.12 is to remove the CD requirement,
since modern computers no longer have a CD drive. Blizzard already did
this exact thing for all versions starting with 1.12. Other than that,
the only other modifications I provide are through
<a href="https://github.com/fearedbliss/Cactus/blob/main/README-SINGLING.md"><strong>Singling</strong></a>, which only contains
<em><strong>non-gameplay modifications</strong></em> and is <em><strong>completely opt-in</strong></em>.</p>
<p dir="auto">If you will be playing online, you should make a copy of the Vanilla Platform
and use that one to connect to Battle.net (for example, copying the <strong>1.14d</strong>
platform, and calling it something like <strong>1.14d BNET</strong>). You can use your other
platforms with the Singling changes for local play.</p>
<p dir="auto">The <strong>cnc-ddraw</strong> video renderer, and <strong>DSOAL w/ OpenAL Soft</strong> are included as
a <em><strong>shared resource</strong></em> for all platforms. cnc-ddraw is provided to improve
video compatibility for all versions between 1.00 - 1.13d. Blizzard removed
DirectDraw support starting with 1.14, and thus Cactus will only provide
video support for versions before that. DSOAL w/ OpenAL Soft is provided to
enable you to use the following lost in-game sound functionality: 3D Sound,
Environmental Effects, 3D Bias.</p>
<p dir="auto">Cactus can also be used for <em><strong>easily playing mods in an isolated and safe fashion.</strong></em>
Please check out the <em><strong><a href="https://github.com/fearedbliss/Cactus/blob/main/PORTS-COLLECTION.md">Cactus Ports Collection</a></strong></em> for a
list of mods that have either been ported to Cactus, attempted to be ported to
Cactus, or were made to run on Cactus natively. You'll also find the compatibility
statuses for each mod listed, and a dedicated page containing important info.</p>
<p dir="auto">Cactus requires a purchased copy of <em><strong>Diablo II (Original, Not Resurrected)</strong></em>
from Blizzard in order to have all of the game assets stored in the MPQs. Once
you have these, they will be reused for all platforms.</p>
<p dir="auto">For more information, please read the documentation below for anything you are
interested in exploring.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cactus Repository</h2><a id="user-content-cactus-repository" aria-label="Permalink: Cactus Repository" href="#cactus-repository"></a></p>
<p dir="auto">The following opt-in modifications and utilities are available in this repository:</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-SINGLING.md">Singling</a></h3><a id="user-content-singling" aria-label="Permalink: Singling" href="#singling"></a></p>
<p dir="auto">A collection of non-gameplay modifications and fixes in order to improve the
Vanilla Diablo II Single Player &amp; LAN Experience.</p>
<p dir="auto">To use Singling, simply copy the Singling files for the version you want to
play from the <strong><code>2. Singling/1. Files</code></strong> folder, and replace the ones for
the equivalent version in your Platforms directory. To revert, use the files
in <strong><code>2. Singling/2. Stock</code></strong> instead.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md">Renderers</a></h3><a id="user-content-renderers" aria-label="Permalink: Renderers" href="#renderers"></a></p>
<p dir="auto">Cactus includes and promotes the <strong><code>cnc-ddraw</code></strong> video renderer for
Diablo II versions <strong><code>1.00 - 1.13d</code></strong>, which can help you run the game
on newer systems with a higher window resolution (not a higher internal
resolution), and the ability to use shaders to upscale the quality of
the graphics in the game. Since Blizzard removed <strong><code>DirectDraw</code></strong>
support in <strong><code>1.14+</code></strong>, you'll need to find an alternative video
renderer for those versions.</p>
<p dir="auto">Please read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md"><strong><code>README-RENDERERS</code></strong></a> for
further explanation on this, for information on how to set it up, or for
any known technical limitations. Definitely read the
<strong><code>Recommendations</code></strong> section at least, or you will most likely
encounter crashes if you've never played versions before <strong><code>1.14</code></strong>
before. Blizzard has done major changes with how video configuration
works starting in <strong><code>1.14</code></strong>.</p>
<ul dir="auto">
<li><a href="https://github.com/CnCNet/cnc-ddraw"><strong><code>cnc-ddraw</code></strong></a> - This renderer
reimplements the DirectDraw API for GDI, OpenGL, and Direct3D to improve
compatibility with Windows XP - 10, and Wine. This renderer also supports
the use of custom shaders - which will allow you to upscale the game so it
looks a lot better - and even provides hotkeys (such as <strong><code>[Alt] + [Enter]</code></strong>)
to switch between full screen and window mode.</li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto"><a href="https://github.com/fearedbliss/Cactus/blob/main/README-3D-SOUND.md">3D Sound (DSOAL w/ OpenAL Soft)</a></h3><a id="user-content-3d-sound-dsoal-w-openal-soft" aria-label="Permalink: 3D Sound (DSOAL w/ OpenAL Soft)" href="#3d-sound-dsoal-w-openal-soft"></a></p>
<p dir="auto">Cactus includes the files required to allow you to enable the following
lost in-game sound functionality: <strong><code>3D Sound</code></strong>, <strong><code>Environmental Effects</code></strong>, and <strong><code>3D Bias</code></strong>.</p>
<p dir="auto">Please read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-3D-SOUND.md"><strong><code>README-3D-SOUND</code></strong></a> for more information.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Installation Instructions</h2><a id="user-content-installation-instructions" aria-label="Permalink: Installation Instructions" href="#installation-instructions"></a></p>
<ul dir="auto">
<li><a href="https://xyinn.org/diablo/videos/01.%20Cactus%20Installation%20Video.mp4" rel="nofollow"><strong>Cactus Setup &amp; Demo Video (2.6.2+) (37 Minutes)</strong></a></li>
</ul>
<p dir="auto"><h3 tabindex="-1" dir="auto">Required Files</h3><a id="user-content-required-files" aria-label="Permalink: Required Files" href="#required-files"></a></p>
<p dir="auto">After you finish installing Diablo II (or restoring the files from a backup),
you only need to keep the following files in your <strong><code>Diablo II Root Directory</code></strong>.
Everything else can be deleted since it will come from Cactus. Once you are done,
continue with the Cactus installation steps.</p>
<div data-snippet-clipboard-copy-content="- D2.LNG
- D2Char.mpq
- D2Data.mpq
- D2Exp.mpq      (1.07+)
- D2Music.mpq    (Not needed for 1.00)
- D2Sfx.mpq
- D2Speech.mpq
- D2Video.mpq
- D2XMusic.mpq   (1.07+)
- D2XTalk.mpq    (1.07+)
- D2XVideo.mpq   (1.07+)"><pre><code>- D2.LNG
- D2Char.mpq
- D2Data.mpq
- D2Exp.mpq      (1.07+)
- D2Music.mpq    (Not needed for 1.00)
- D2Sfx.mpq
- D2Speech.mpq
- D2Video.mpq
- D2XMusic.mpq   (1.07+)
- D2XTalk.mpq    (1.07+)
- D2XVideo.mpq   (1.07+)
</code></pre></div>
<p dir="auto"><h3 tabindex="-1" dir="auto">Install Cactus, Dependencies, and Prepare MPQs.</h3><a id="user-content-install-cactus-dependencies-and-prepare-mpqs" aria-label="Permalink: Install Cactus, Dependencies, and Prepare MPQs." href="#install-cactus-dependencies-and-prepare-mpqs"></a></p>
<p dir="auto">This section will help you install Cactus to the correct location, its dependencies,
and also help you fix your MPQs, so that they are compatible with the older versions
of Diablo II. I only test on and support <strong><code>Windows 7</code></strong> and <strong><code>Windows 10</code></strong>,
however these instructions will probably work on versions in between as well.</p>
<ol dir="auto">
<li>
<p dir="auto">Copy all of the files in the <strong><code>1. Files</code></strong> folder into your <strong><code>Diablo II Root Directory</code></strong>.</p>
<p dir="auto"><strong><code>Note:</code></strong> It's important that <strong><code>Cactus</code></strong> runs from inside your <strong><code>Diablo II Root Directory</code></strong>
or you will get weird behavior in various situations like running <strong><code>-direct -txt</code></strong> mods or
taking screenshots.</p>
</li>
<li>
<p dir="auto">If you need to fix your MPQs, then also copy the <strong><code>MpqFixer</code></strong> located in the <strong><code>3. Other</code></strong>
directory into your <strong><code>Diablo II Root Directory</code></strong>. This fix is only needed if you want to
play versions <strong><code>1.08 - 1.13d</code></strong> and you also happened to install Diablo II from the new
Blizzard Installer. If you are not planning to play those versions, or you installed
Diablo II from the original <strong><code>1.00, 1.03, 1.07</code></strong> discs, you don't need to fix your MPQs.</p>
<p dir="auto">You can then run the <strong><code>FIX_MPQS_RUN_AS_ADMIN.bat</code></strong> inside the <strong><code>MpqFixer</code></strong> directory
that you copied, as <strong><code>Administrator</code></strong>.</p>
</li>
<li>
<p dir="auto">Run the <strong><code>vc_redist.x86.exe</code></strong> file in the <strong><code>3. Other</code></strong> directory to install the
required libraries for <strong><code>3D Sound</code></strong>. If you run the game without these being installed,
you will get a <strong><code>VCRUNTIME140.dll</code></strong> error message.</p>
<p dir="auto"><strong><code>Note:</code></strong> If you don't want this functionality, just delete the <strong><code>dsoal-aldrv.dll</code></strong> and
<strong><code>dsound.dll</code></strong> from your <strong><code>Diablo II Root Directory</code></strong>.</p>
</li>
<li>
<p dir="auto"><strong><code>(Windows 7 Only)</code></strong> Cactus requires <strong><code>.NET Framework 4.6.2</code></strong> to function, but that version
does not come included in Windows 7 by default. You can run the
<strong><code>NET_Framework_4.6.2_Offline_Installer_for_Windows_7.exe</code></strong> file located in the <strong><code>3. Other</code></strong>
directory to install that dependency. Windows 10 provides this dependency by default.</p>
</li>
</ol>
<p dir="auto"><h3 tabindex="-1" dir="auto">Adding/Running A Platform</h3><a id="user-content-addingrunning-a-platform" aria-label="Permalink: Adding/Running A Platform" href="#addingrunning-a-platform"></a></p>
<ol dir="auto">
<li>Open <strong><code>Cactus</code></strong>.</li>
<li>Click <strong><code>Settings</code></strong> and set your <strong><code>Diablo II Root Directory</code></strong> to your Diablo II folder. <strong><code>(Example: C:\Games\Diablo II)</code></strong></li>
<li>Click <strong><code>Add</code></strong>.</li>
<li>Type in the name of the <strong><code>Platform</code></strong> you want to run. This should match a folder in the <strong><code>Platforms</code></strong>
folder. (Example: If you want to run <strong><code>1.09b</code></strong>, type <strong><code>1.09b</code></strong>).</li>
<li><strong><code>Optional (Recommended)</code></strong>: Type in a <strong><code>Label</code></strong> for this platform. If you label your platform, it will have its own dedicated
save directory, allowing you to have multiple entries using the same platform but with different save locations.
If you don't use a label, all the characters with this platform name will be stored in the same location (flat structure).
You can have multiple entries using the same platform with and without labels as well. A label cannot be
removed from an entry once created, but it can be renamed. A label cannot be added to an entry once created.</li>
<li>Enter the name of your <strong><code>Launcher</code></strong>. <strong><code>(Example: Game.exe)</code></strong></li>
<li>Enter the Flags you want (Example: <strong><code>-ns</code></strong>).</li>
<li>Click <strong><code>Add</code></strong>.</li>
<li>Select your newly added Platform and press <strong><code>Launch</code></strong>.</li>
</ol>
<p dir="auto">The game should start. If you are having video issues, please make sure you
have read the <a href="https://github.com/fearedbliss/Cactus/blob/main/README-RENDERERS.md"><strong><code>README-RENDERERS</code></strong></a> and ensure that
it was configured properly.</p>
<p dir="auto"><strong><code>NOTE</code></strong>: Make sure to leave the Cactus application running throughout your
play sessions (you can minimize it). Cactus keeps track of the running Diablo II
processes it launches as to protect you from accidentally switching to a different
platform, and causing your <strong><code>Save Path</code></strong> to be updated to an incorrect location.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Windows 11+ will not be supported.</h2><a id="user-content-windows-11-will-not-be-supported" aria-label="Permalink: Windows 11+ will not be supported." href="#windows-11-will-not-be-supported"></a></p>
<p dir="auto">Due to Microsoft mandating people to have an online connection and a Microsoft
account for Windows 11 (at the OOBE stage), any version over Windows 10 will
not be supported. Given the Xbox One has the same type of requirement, I'm not
surprised Microsoft is going in that direction.</p>
<ul dir="auto">
<li><a href="https://blogs.windows.com/windows-insider/2022/02/16/announcing-windows-11-insider-preview-build-22557/" rel="nofollow"><strong>Source 1 - February 16, 2022 - Windows 11 Insider Preview Build 22557</strong></a></li>
</ul>
<div data-snippet-clipboard-copy-content="Similar to Windows 11 Home edition, Windows 11 Pro edition now requires internet
connectivity during the initial device setup (OOBE) only. If you choose to setup
device for personal use, MSA will be required for setup as well. You can expect
Microsoft Account to be required in subsequent WIP flights."><pre><code>Similar to Windows 11 Home edition, Windows 11 Pro edition now requires internet
connectivity during the initial device setup (OOBE) only. If you choose to setup
device for personal use, MSA will be required for setup as well. You can expect
Microsoft Account to be required in subsequent WIP flights.
</code></pre></div>
<ul dir="auto">
<li><a href="https://blogs.windows.com/windows-insider/2022/05/05/announcing-windows-11-insider-preview-build-22616/" rel="nofollow"><strong>Source 2 - May 5, 2022 - Windows 11 Insider Preview Build 22616</strong></a></li>
</ul>
<div data-snippet-clipboard-copy-content="Previously, we shared new requirements for internet and MSA on the Windows 11
Pro edition. Today, Windows Insiders on Windows 11 Pro edition will now require
MSA and internet connectivity during the initial device setup (OOBE) only when
setting up for personal use. If you choose to setup device for Work or School,
there is no change, and it will work the same way as before."><pre><code>Previously, we shared new requirements for internet and MSA on the Windows 11
Pro edition. Today, Windows Insiders on Windows 11 Pro edition will now require
MSA and internet connectivity during the initial device setup (OOBE) only when
setting up for personal use. If you choose to setup device for Work or School,
there is no change, and it will work the same way as before.
</code></pre></div>
<p dir="auto">Windows 10 Pro doesn't have this requirement at all, and is the OS I primarily
use on my gaming computer. Windows 10 Home does have this requirement though,
but can be bypassed by unplugging your internet connection before the OOBE.
<em><strong>I'm including Windows 10 support due to there still being a direct path
to use the OS with no workarounds through the Pro edition.</strong></em> Windows 10 reaches
EOL on <a href="https://learn.microsoft.com/en-us/lifecycle/products/windows-10-home-and-pro" rel="nofollow">October 14, 2025</a>.</p>
<p dir="auto">However, I have gone dark already using my <strong>Dark Island</strong> strategy, which
means that I'm using Windows exclusively in <strong>Offline Mode</strong>, with all forms
of communication to the public internet disabled. I never allowed it to reach
the internet from the very beginning, including during the installation stage.
I only play <strong>Offline Single Player DRM Free Games</strong>, so this strategy works
for me, and it also means that the EOL status of Windows 10 will have no
consequences for me. I'm basically using Windows like a N64 or Gameboy,
which is why I sometimes call this machine a Wintendo. I never needed
internet for those systems then, and still don't today, yet I can still use
and play those games decades later. Going dark years before the EOL, gives me
plenty of time to make sure that I backup everything that I need, to reproduce
my environment post EOL. If you are interested in my Dark Island strategy,
you can read the next section.</p>
<p dir="auto">Furthermore, I will be exiting Windows development completely for newer versions
of Windows. I will continue to maintain Cactus/Singling/Etc for Windows 7 and 10.</p>
<p dir="auto">Please do not file any bug reports if you are running my software on Windows 11+,
they will be promptly closed as <strong><code>NOT SUPPORTED</code></strong>.</p>
<p dir="auto"><h3 tabindex="-1" dir="auto">Dark Island Strategy (Going Dark)</h3><a id="user-content-dark-island-strategy-going-dark" aria-label="Permalink: Dark Island Strategy (Going Dark)" href="#dark-island-strategy-going-dark"></a></p>
<p dir="auto">With this strategy, what we want to do is block Windows from accessing the
internet, but we still want LAN communication (for sideloading games and
other applications). If you don't need LAN communication, then you can
just disable all of your network interfaces, and use an external drive to
load apps into your airgapped machine. This would be the best approach, but
a bit less convenient for sideloading. The LAN-only approach gives you a
good balance of external isolation and internal convenience. You could
also do this at the firewall level, but for me it's just one computer,
isolated to just gaming. Security isn't the most important thing in this
environment. We can just change the Network Adapter settings in the OS.</p>
<p dir="auto">You can do this even without reinstalling, but if you want to have a fresh
installation of Windows 10 with no further updates being forced upon you
by Microsoft, I recommend re-installing Windows 10 and make sure that you
unplug your ethernet cable (and don't connect to wifi) before you start the
installer. You must use <strong>Windows 10 Pro</strong> (or a higher edition) since this
allows you to make a local offline account. Then follow the steps below:</p>
<ol dir="auto">
<li>Start -&gt; Settings</li>
<li>Network &amp; Internet -&gt; Advanced network settings -&gt; Change adapter options</li>
<li>Right click your ethernet adapter interface -&gt; Properties. (If you are on
wifi, then select your wireless interface). Basically anything that will be
connecting to your LAN and that you don't want internet access to work for.</li>
<li>Uncheck <strong>Internet Protocol Version 4 (TCP/IPv6)</strong> (or configure it if you
need it). I personally disabled IPv6 completely because I just need IPv4 for
this purpose, and I also don't want the machine to accidentally connect to
the internet, if I ever enable IPv6 on my modem (and/or if the ISP also has
working IPv6).</li>
<li>Right click <strong>"Internet Protocol Version 4 (TCP/IPv4)"</strong> -&gt; Properties</li>
<li>Check "Use the following IP address" (this should automatically also set
<strong>Use the following DNS server addresses</strong>).</li>
<li>Set the <strong>IP address</strong> to an IP on your local network that doesn't conflict
with your DHCP (maybe a static ip outside of the DHCP range). The
<strong>Subnet mask</strong> should automatically be set to <strong>255.255.255.0</strong>. Leave the
<strong>Default gateway</strong> and <strong>Preferred DNS Server</strong> empty.</li>
<li>Press OK and Close</li>
</ol>
<p dir="auto">That's basically it! Enjoy your life, be free, be happy. Let's play. Go dark!</p>
<p dir="auto">I've been using this approach using the
<strong><code>Win10_22H2_English_x64v1.iso (2023 Update)</code></strong> with hash
<strong><code>bbb1b234ea7f5397a1906ee59187087c78374f35</code></strong> and it works great. Another thing
to keep in mind is that since you are never allowing your OS to communicate to
the outside world, Windows won't be able to reach its activation servers, which
means that the activation timer won't start. You essentially have a free and
legal copy of Windows. Not only that, but you also don't need to deal with
forced updates, forced telemetry, and any other spying that's going on. With
this set up, you get a beautiful, quiet, stable, and privacy respecting version
of Windows 10, simply by disconnecting it from the internet completely.</p>
<p dir="auto"><h4 tabindex="-1" dir="auto">Optional VM (Not Recommended)</h4><a id="user-content-optional-vm-not-recommended" aria-label="Permalink: Optional VM (Not Recommended)" href="#optional-vm-not-recommended"></a></p>
<p dir="auto">Now at this point, we have two options:</p>
<ol dir="auto">
<li><strong>(Recommended)</strong>: Use another computer as your main computer (or for
downloading games for your <strong>Wintendo</strong>).</li>
<li><strong>(Poses a security risk)</strong>: Setup a Virtual Machine in
<a href="https://www.virtualbox.org/" rel="nofollow">VirtualBox</a> for downloading games and any
other internet related tasks. Due to the potential security risk, if you do
go with this approach, I recommend restricting your usage of the VM to just
downloading games from GOG. You may be fine using this strategy for normal
computing though if you know what you are doing.</li>
</ol>
<p dir="auto">If you switch your virtual machine's network configuration from <strong>NAT</strong> to
<strong>Bridged Adapter</strong>, you can share your host's network adapter with the guest
directly. The Windows host will still have settings that prevent it from
accessing the internet, and the guest will have its own completely different
settings that allows it to access the internet. This means that we can download
all of our games from GOG in the guest, and transfer it with no issues to our
host via the LAN network at full speeds. I had issues with transferring files
between the Host and Guest via VirtualBox's <strong>Shared Folders</strong>, for any games
that required a large amount of disk space. But transferring over LAN was fine
(i.e transferring the files from the guest to the NAS and then back to the
host). Due to the security risk of attacks escaping from the VM, I don't
recommend this approach. Use a separate machine instead for your regular
computing and keep the gaming machine as isolated as possible.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Template &amp; Labeling System</h2><a id="user-content-template--labeling-system" aria-label="Permalink: Template &amp; Labeling System" href="#template--labeling-system"></a></p>
<p dir="auto">The Cactus Template &amp; Labeling System allows you to be able to easily
start using a few pretty cool and very interesting workflows, while
allowing you to re-use your existing platforms, thus minimizing the
amount of disk space used.</p>
<p dir="auto">For example, let's say we want to play <strong><code>Solo Self Found</code></strong> as defined
as <strong><code>Only using items that the character has found with their own hands</code></strong>,
this pretty much means untwinked play. However, let's say you
are also ok with using mules as a form of an extended stash for your
main character. Thus, any items your main character finds, can be placed
in storage, and will only be used by that main character specifically.
If you were to do this manually, for each particular main character you
made, it would quickly get out of hand since all the main characters and
each individual main character's set of mules, would all be in the same
folder. This is where the Templating &amp; Labeling System kicks in. Now, we
could simply make a new entry in Cactus pointing to an existing
platform, and give it a particular label (Say the name of that specific
main character) and play the game. A dedicated save folder with the given
label will be created under the Saves directory for this platform.</p>
<p dir="auto">For example, I want to make a character called <strong><code>Isaac</code></strong>. Isaac will
have their own set of mule characters as well and we'll call them
<strong><code>Mule_A</code></strong>, <strong><code>Mule_B</code></strong>, and <strong><code>Mule_C</code></strong> for simplicity. This
character will also be on <strong><code>1.09b</code></strong>. Thus, we can make a new entry for
our <strong><code>1.09b</code></strong> platform with the label <strong><code>Isaac</code></strong>. Once we start the
game, we will have the following structure in our Diablo II root
directory (let's assume I already made the characters in-game):</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s
</code></pre></div>
<p dir="auto">As you can see, this entry is fully isolated using its platform and
label combination. Now, let's say you and your friends want to have some
type of tournament on <strong><code>1.09b</code></strong>. No problem! You can quickly add
another entry for the <strong><code>1.09b</code></strong> platform with another label, such as
<strong><code>Tournament 2022</code></strong> and start it up. The same exact <strong><code>1.09b</code></strong>
platform files that we used before will be re-used, and we will have a
new save directory. Let's create a new character called <strong><code>Bethany</code></strong>
and give Bethany a few mules as well. We'll call the mules the same as
before, and since they are isolated, we can reuse the same muling naming
scheme with no conflicts. So now our structure looks like this:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">This is just one of the new workflows that our Templating &amp; Labeling
System enables. This workflow requires a modified <strong><code>D2gfx.dll</code></strong> to
allow multiple instances of the game, allowing you to mule between
your main character and your mules via LAN. Singling provides this
feature for the versions it supports. For all other versions, you'll
need to find a copy of it online.</p>
<p dir="auto">Another workflow which I really like is using this labeling system to
separate my <strong><code>Classic</code></strong> and <strong><code>Expansion</code></strong> characters. By using two
separate labels to the same platform, we can have two separate save
paths for them. If we did this, we would have the following:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">You can pretty much label your platforms whatever you want as long as
the name can be used for the folder on your hard drive. If you are using
some illegal symbols like <strong><code>/</code></strong>, then Cactus will properly detect that
and give you the appropriate message so that you can fix it. You can
also omit the label if you want and that works perfectly fine with the
above scenarios. Let's say you wanted to have a <strong><code>1.09b</code></strong> platform and
have everything in there without caring about labels (essentially a flat
layout, although you can also have a flat layout with a label, it just
depends on how you want to organize stuff), go ahead and create an entry
with the platform name <strong><code>1.09b</code></strong> and leave the label blank. Launching
the game will just point the save path to the <strong><code>/Saves/1.09b/</code></strong> folder
and your files will be placed in there. Assuming we then created a
character called <strong><code>Leslie</code></strong>, we would then have the following
structure:</p>
<div data-snippet-clipboard-copy-content="/Platforms/1.09b/

/Saves/1.09b/
/Saves/1.09b/Leslie.d2s

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s"><pre><code>/Platforms/1.09b/

/Saves/1.09b/
/Saves/1.09b/Leslie.d2s

/Saves/1.09b/Classic/

/Saves/1.09b/Expansion/

/Saves/1.09b/Isaac/
/Saves/1.09b/Isaac/Isaac.d2s
/Saves/1.09b/Isaac/Mule_A.d2s
/Saves/1.09b/Isaac/Mule_B.d2s
/Saves/1.09b/Isaac/Mule_C.d2s

/Saves/1.09b/Bethany/
/Saves/1.09b/Bethany/Bethany.d2s
/Saves/1.09b/Bethany/Mule_A.d2s
/Saves/1.09b/Bethany/Mule_B.d2s
/Saves/1.09b/Bethany/Mule_C.d2s
</code></pre></div>
<p dir="auto">It's just another folder after all ;). The nice thing about this is that
since all of these are sharing the same platform, switching between
these entries is extremely fast since no files need to change on your
hard drive, but rather we simply just update the registry save path, and
you are back in the action. Have fun!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Backup System</h2><a id="user-content-backup-system" aria-label="Permalink: Backup System" href="#backup-system"></a></p>
<p dir="auto">By clicking the <strong><code>Backup</code></strong> button, Cactus will automatically create a backup
of the following files in the <strong><code>Backups</code></strong> directory, inside your
<strong><code>Diablo II Root Directory</code></strong>:</p>
<ul dir="auto">
<li><strong><code>/Platforms/</code></strong></li>
<li><strong><code>/Saves/</code></strong></li>
<li><strong><code>/Entries.json</code></strong></li>
<li><strong><code>/LastRequiredFiles.json</code></strong></li>
<li><strong><code>/Settings.json</code></strong></li>
</ul>
<p dir="auto">The <strong><code>Backups</code></strong> directory is considered a <strong><code>Protected Directory</code></strong> by Cactus,
and will not be deleted.</p>
<p dir="auto">Lastly, you can change the backup location to any location you have write access
to, by modifying the <strong><code>Backups Directory</code></strong> setting in the <strong><code>Settings</code></strong>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Using Cactus like a Service</h2><a id="user-content-using-cactus-like-a-service" aria-label="Permalink: Using Cactus like a Service" href="#using-cactus-like-a-service"></a></p>
<p dir="auto">Cactus has some basic tracking of processes that it launches in order to
ensure that there are no accidental launches of other versions or
combinations of the game that would cause your Save Path location to be
changed mid-game. Thus it is essential for Cactus to remain running
while you are playing. If you are always going to be running Diablo II
via Cactus, you may want to go into the Cactus Settings and toggle the
<strong><code>Minimize to System Tray</code></strong> option so that it doesn't take up space in
your taskbar. I personally like having it show in the taskbar but that's
just preference ;D.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Moving Cactus To A New Computer</h2><a id="user-content-moving-cactus-to-a-new-computer" aria-label="Permalink: Moving Cactus To A New Computer" href="#moving-cactus-to-a-new-computer"></a></p>
<p dir="auto">If you want to move all of your Platforms, Characters, and Diablo II folder
to another machine, you will need to:</p>
<ol dir="auto">
<li>Copy your entire Diablo II folder to your new computer.</li>
<li>Open <strong><code>Cactus</code></strong>.</li>
<li>Click <strong><code>Settings</code></strong> and change your <strong><code>Diablo II Root Directory</code></strong> to match the location on your new computer.</li>
<li>Click <strong><code>Reset</code></strong>.</li>
<li>Now <strong><code>Launch</code></strong> whatever Platform you want.</li>
</ol>
<p dir="auto">Clicking <strong><code>Reset</code></strong> will cause Cactus to reconfigure itself by removing
some files from your <strong><code>Diablo II Root Directory</code></strong> and wiping the
<strong><code>Last Ran</code></strong> box on the entry that has it. Once you launch the game,
the registry will point to the appropriate save location, and your platform
files will be copied back to your Diablo II root folder. In some rare cases
(only on first install), you may need to do a little bit of manual organization
in your Diablo II root folder to get things aligned properly. Once aligned, it's
all tracked and automatic.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Unlocking All Cinematics</h2><a id="user-content-unlocking-all-cinematics" aria-label="Permalink: Unlocking All Cinematics" href="#unlocking-all-cinematics"></a></p>
<p dir="auto">If you moved Cactus to a new computer, or you did a fresh install, you can unlock
all of the cinematics by doing the following:</p>
<ol dir="auto">
<li>Launch Diablo II, and then close it.</li>
<li>Open the Registry Editor (<strong><code>regedit.exe</code></strong>).</li>
<li>Go to <strong><code>Computer\HKEY_CURRENT_USER\SOFTWARE\Blizzard Entertainment\Diablo II</code></strong>.</li>
<li>Set the <strong><code>Aux Battle.net</code></strong> key to <strong><code>216.148.246.35</code></strong>.</li>
<li>Congrats! All of your cinematics are now unlocked.</li>
</ol>
<p dir="auto"><h2 tabindex="-1" dir="auto">Updating Files In The Platforms folder</h2><a id="user-content-updating-files-in-the-platforms-folder" aria-label="Permalink: Updating Files In The Platforms folder" href="#updating-files-in-the-platforms-folder"></a></p>
<p dir="auto">If you update any files in your Platforms folder, then click the
<strong><code>Reset</code></strong> button, and run it again. This will cause Cactus to
re-install the files with the new ones.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">OMAHGOD! My Characters Are Gone! Cactus Deleted Them!!!</h2><a id="user-content-omahgod-my-characters-are-gone-cactus-deleted-them" aria-label="Permalink: OMAHGOD! My Characters Are Gone! Cactus Deleted Them!!!" href="#omahgod-my-characters-are-gone-cactus-deleted-them"></a></p>
<p dir="auto">Cactus comes with <a href="https://github.com/fearedbliss/Cactus-Core/blob/master/Cactus/FileGenerator.cs">built in safety features</a>
specifically designed to protect critical directories and files, which
includes the save directories. Thus it is impossible for Cactus to have
deleted them. Cactus also only operates within the Diablo II root
directory so it also wouldn't be possible for Cactus to delete saves
that are in <strong><code>1.14d+</code></strong>'s new save directory that is in your personal
folder.</p>
<p dir="auto">Since Cactus is <strong><code>A Modern Version Switcher, Character Isolator, and Mod Manager</code></strong>,
it will update the registry location of where the game should look for the
saves. For example, if you are playing a <strong><code>Platform</code></strong> called
<strong><code>1.05b</code></strong> with a <strong><code>Label</code></strong> called <strong><code>Chinchilla</code></strong>, the files for
this platform would logically be located under <strong><code>Platforms/1.05b/</code></strong>,
and the saves would be located under <strong><code>Saves/1.05b/Chinchilla/</code></strong>. Both
directories are located inside your Diablo II folder. Thus, when the
game starts, your characters are properly isolated and protected. If
this is the first time you launched a game with Cactus, and you
previously just had a regular Diablo II installation, then it would seem
as if all your characters got deleted, or magically dissapeared.
However, they are simply located in the original location that your
computer saved them to. If you were playing <strong><code>1.14d+</code></strong>, they most
likely are located at:</p>
<p dir="auto"><strong><code>%USERPROFILE%/Saved Games/Diablo II</code></strong></p>
<p dir="auto">If you were playing <strong><code>1.13d</code></strong> or below, they are inside the Diablo II
folder itself under a folder called <strong><code>save</code></strong>.</p>
<p dir="auto">Lastly, always remember to keep backups when running Third Party Tools
or Modifications.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Cactus switches versions but I don't see the Diablo II window and there are no errors either.</h2><a id="user-content-cactus-switches-versions-but-i-dont-see-the-diablo-ii-window-and-there-are-no-errors-either" aria-label="Permalink: Cactus switches versions but I don't see the Diablo II window and there are no errors either." href="#cactus-switches-versions-but-i-dont-see-the-diablo-ii-window-and-there-are-no-errors-either"></a></p>
<p dir="auto">If switching versions with Cactus doesn't actually launch the game but you also
don't notice any errors, this could be an indication that either your Video Settings
are not correct, or that you may need to run Cactus in Admin Mode. I've noticed that
if I have Diablo II installed on the <strong><code>C:\</code></strong> drive (i.e <strong><code>C:\Games\Diablo II</code></strong>),
I would need to run Cactus at least once in Admin Mode for it to work properly, but
if I installed Diablo II on another drive (i.e <strong><code>D:\Games\Diablo II</code></strong>), it would
work fine without Admin privileges. I'm pretty sure this is due to the <strong><code>C:\</code></strong>
drive generally being a protected drive.</p>
<p dir="auto">Another thing to note is that I observed that I only needed to run Cactus once in Admin
Mode for this to "stick" and continue working even if I opened Cactus in the future without
Admin rights, although I haven't tested if this persists across reboots, but it possibly may.
I've also noticed that even when there was a problem, some versions would work,
and some wouldn't. Specifically versions <strong><code>1.00 - 1.06b</code></strong> worked, but <strong><code>1.07 - 1.13d</code></strong>
didn't.</p>
<p dir="auto">Lastly, make sure that there are no zombie Diablo II processes running in the background (Task Manager),
that can cause the version switcher to either not switch away or something else. I know that
the new telemetry executables added to <strong><code>1.14d</code></strong> (i.e <strong><code>SystemSurvey.exe</code></strong> and <strong><code>BlizzardError.exe</code></strong>)
are not needed for the game to actually function, and can lock the process for a bit after you close
the game. The Cactus platforms do not contain these files since I've deleted them, however, if you
are installing from a fresh copy of Diablo II from Blizzard, it will have them.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">File Read Error on Diablo II Start Up</h2><a id="user-content-file-read-error-on-diablo-ii-start-up" aria-label="Permalink: File Read Error on Diablo II Start Up" href="#file-read-error-on-diablo-ii-start-up"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9fcc249b278b9e2e694aa8faccbe1f57604a8139160791836b8d4f6d15347ea0/68747470733a2f2f692e696d6775722e636f6d2f44514d63486c4d2e706e67"><img src="https://camo.githubusercontent.com/9fcc249b278b9e2e694aa8faccbe1f57604a8139160791836b8d4f6d15347ea0/68747470733a2f2f692e696d6775722e636f6d2f44514d63486c4d2e706e67" alt="Error" data-canonical-src="https://i.imgur.com/DQMcHlM.png"></a></p>
<p dir="auto">If you are receiving the above error, it may be possible that some of
your MPQs are still hidden from Cactus' previous behavior before version
<strong><code>2.2.0</code></strong>. For newer versions of Cactus, Cactus will automatically
rename the following MPQs back to normal whenever you either
<strong><code>Launch</code></strong> a platform, or if you already have an entry that was
<strong><code>Last Ran</code></strong>, when you press the <strong><code>Reset</code></strong> button as well. If for
whatever reason that still doesn't work, you can go to your Diablo II
root directory and make sure that the following <strong><code>4</code></strong> Expansion MPQs
are properly named: <strong><code>d2exp.mpq</code></strong>, <strong><code>d2xvideo.mpq</code></strong>,
<strong><code>d2xmusic.mpq</code></strong>, and <strong><code>d2xtalk.mpq</code></strong>. If you see any of them with
the <strong><code>.bak</code></strong> extension, simply remove that extension and everything
should be good. If you are receiving this error but those files are in
place, then it is something else not related to Cactus.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Game Screenshots</h2><a id="user-content-game-screenshots" aria-label="Permalink: Game Screenshots" href="#game-screenshots"></a></p>
<p dir="auto"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2bcd27051aea669d8263a15495987d25d23905e23599f853d0b66576859bc5ab/68747470733a2f2f692e696d6775722e636f6d2f7575674d6e34382e6a7067"><img src="https://camo.githubusercontent.com/2bcd27051aea669d8263a15495987d25d23905e23599f853d0b66576859bc5ab/68747470733a2f2f692e696d6775722e636f6d2f7575674d6e34382e6a7067" alt="1.00" data-canonical-src="https://i.imgur.com/uugMn48.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b67074c3e3461a80c50335a0568122279399990288cb7ccbe8b75ec315290225/68747470733a2f2f692e696d6775722e636f6d2f393031753456372e6a7067"><img src="https://camo.githubusercontent.com/b67074c3e3461a80c50335a0568122279399990288cb7ccbe8b75ec315290225/68747470733a2f2f692e696d6775722e636f6d2f393031753456372e6a7067" alt="1.05b" data-canonical-src="https://i.imgur.com/901u4V7.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ab11b25948b1a162e3343f79e6d8684c5ecf6243977f5dd7da4beb9596f90b10/68747470733a2f2f692e696d6775722e636f6d2f7a48443973354c2e6a7067"><img src="https://camo.githubusercontent.com/ab11b25948b1a162e3343f79e6d8684c5ecf6243977f5dd7da4beb9596f90b10/68747470733a2f2f692e696d6775722e636f6d2f7a48443973354c2e6a7067" alt="1.07" data-canonical-src="https://i.imgur.com/zHD9s5L.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ec17e02d14eea316670ba382bc53ef47b99457ba0ae865c60d24ab56524d3900/68747470733a2f2f692e696d6775722e636f6d2f735569464b714e2e6a7067"><img src="https://camo.githubusercontent.com/ec17e02d14eea316670ba382bc53ef47b99457ba0ae865c60d24ab56524d3900/68747470733a2f2f692e696d6775722e636f6d2f735569464b714e2e6a7067" alt="1.08" data-canonical-src="https://i.imgur.com/sUiFKqN.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7e8fadc6127f9e29626aeb55efbb80bc7ddf5d103bf8c86c31bc8c77419fc248/68747470733a2f2f692e696d6775722e636f6d2f4a5a3962494f792e6a7067"><img src="https://camo.githubusercontent.com/7e8fadc6127f9e29626aeb55efbb80bc7ddf5d103bf8c86c31bc8c77419fc248/68747470733a2f2f692e696d6775722e636f6d2f4a5a3962494f792e6a7067" alt="1.09b" data-canonical-src="https://i.imgur.com/JZ9bIOy.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4ed78562f5126584f8220ac99fa296c1c7d6376a204b670f53fa0da305394cf7/68747470733a2f2f692e696d6775722e636f6d2f5677347961444d2e6a7067"><img src="https://camo.githubusercontent.com/4ed78562f5126584f8220ac99fa296c1c7d6376a204b670f53fa0da305394cf7/68747470733a2f2f692e696d6775722e636f6d2f5677347961444d2e6a7067" alt="1.10" data-canonical-src="https://i.imgur.com/Vw4yaDM.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ebd5045a12814db5f119bf833c358d294227de3ce920d1dd8479890338803b1f/68747470733a2f2f692e696d6775722e636f6d2f586951686558792e6a7067"><img src="https://camo.githubusercontent.com/ebd5045a12814db5f119bf833c358d294227de3ce920d1dd8479890338803b1f/68747470733a2f2f692e696d6775722e636f6d2f586951686558792e6a7067" alt="1.13d" data-canonical-src="https://i.imgur.com/XiQheXy.jpg"></a>
<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/581336626e892ea8db655cf3923a15d680ec730d7548005d61f3aca7fabe3aa3/68747470733a2f2f692e696d6775722e636f6d2f706f6a71334a772e6a7067"><img src="https://camo.githubusercontent.com/581336626e892ea8db655cf3923a15d680ec730d7548005d61f3aca7fabe3aa3/68747470733a2f2f692e696d6775722e636f6d2f706f6a71334a772e6a7067" alt="1.14d" data-canonical-src="https://i.imgur.com/pojq3Jw.jpg"></a></p>
</article></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Jalapeño Peppers Are Less Spicy Than Ever (2023) (253 pts)]]></title>
            <link>https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</link>
            <guid>39517145</guid>
            <pubDate>Mon, 26 Feb 2024 21:50:37 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/">https://www.dmagazine.com/food-drink/2023/05/why-jalapeno-peppers-less-spicy-blame-aggies/</a>, See on <a href="https://news.ycombinator.com/item?id=39517145">Hacker News</a></p>
<div id="readability-page-1" class="page"><div>
<p>It’s not just you: jalapeño peppers are less spicy and less predictable than ever before. As heat-seekers chase ever-fiercer varieties of pepper—Carolina reapers, scorpions, ghosts—the classic jalapeño is going in the opposite direction. And the long-term “de-spicification” of the jalapeño is a deliberate choice, not the product of a bad season of weather.</p>
<p>This investigation began in my own kitchen. After months of buying heat-free jalapeños, I started texting chefs around Dallas to see if they were having the same experience. Many agreed. One prominent chef favors serranos instead. Regino Rojas of <a href="https://directory.dmagazine.com/restaurants/revolver-taco-lounge/" target="_blank" rel="noreferrer noopener">Revolver Taco Lounge</a> suggested jalapeños are now “more veggie-like than chile.” Luis Olvera, owner of Trompo, said that jalapeños now have so much less heat that “I tell my staff, ‘I think my hands are just too damn sweet,’ because I can’t make salsa spicy enough anymore.”</p>
<p>To be fair, not everyone agreed with these views. One restaurateur wondered if jalapeños seem less hot because diners have become infatuated with habaneros and serranos. Wayne White, general manager at Hutchins BBQ, offered a middle ground. “I noticed during covid, the quality got really bad, but now to me they’re beautiful,” he said. “We did have a season during covid, you could tell they were pulling them too soon, they weren’t that ripe. But I ate a whole jalapeño the other day, just to eat one, and it lit me up.”</p>
<p>I searched the internet to see whether jalapeños are really getting milder, but only found shopping tips. Gardening websites offered savvier advice: that peppers grow hotter under stress. If they’re well-watered, they won’t produce as much capsaicin, the chemical that generates the sensation we know as spiciness. But even this explanation leaves unanswered questions. My sunny backyard, which produces ferocious peppers, is one thing. What about all the peppers in the grocery store?</p>
<p>Clearly, a real investigation was required. So I called Stephanie Walker, extension vegetable specialist at New Mexico State University, advisory board member of that university’s <a href="https://cpi.nmsu.edu/">Chile Pepper Institute</a>, and chair of the 2023 New Mexico Chile Conference.</p>
<p>“Other complaints have come my way,” Walker said at the start of our phone call. This turned out to be a comedic understatement: she has a massive, existential complaint about the state of the chile pepper industry. I got on the phone expecting to hear a prosaic story of weather patterns shifting, unusual rains in pepper-growing regions, or the spread of greenhouses. I would not have been surprised if she validated Rojas’ theory: that jalapeños are now grown to look pretty, shiny, and big, regardless of flavor. “Pesticides and other enhancing farming elements make them look beautiful but not really spicy,” Rojas suggested to me. </p>
<blockquote>
<p>People lost a lot of interest in tomatoes for a long time until heirlooms came back. Now we have the same thing with peppers.</p>
<cite>Stephanie Walker, New Mexico State University Chile Pepper Institute</cite></blockquote>
<p>There’s truth to all these theories, but Walker says they are only secondary factors.</p>
<p>“As more growers have adopted drip irrigation, more high-tech farming tools to grow the peppers, they’ll tend to be milder,” Walker told me first, as a sort of throat-clearing exercise before the real explanation. “But there’s more to it than that.”</p>
<p>The truth is more like a vast industrial scheme to make the jalapeño more predictable—and less hot.</p>
<h2 id="h-the-vast-jalapeno-conspiracy">The Vast Jalapeño Conspiracy</h2>
<p>Most jalapeños go straight to factories, for canned peppers, pickled pepper rings, salsas, cream sauces, dressings, flavored chips and crackers, dips, sausages, and other prepared foods. For all those companies, consistency is key. Think about the salsa world’s “mild,” “medium,” and “hot” labels.</p>
<p>According to <em>The Mexican Chile Pepper Cookbook</em> by Dave DeWitt and José Marmolejo, 60 percent of jalapeños are sent to processing plants, 20 percent are smoke-dried into chipotles, and just 20 percent are sold fresh. Since big processors are the peppers’ main consumers, big processors get more sway over what the peppers taste like.</p>
<p>“It was a really big deal when breeders [told the industry], ‘hey, look, I have a low-heat jalapeño,’ and then a low-heat but high-flavor jalapeño,” Walker explained. “That kind of became the big demand for jalapeños—low heat jalapeños—because most of them are used for processing and cooking. [Producers] want to start with jalapeños and add oleoresin capsicum.”</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2015/09/texasfriedbaconjalapenobites.jpg" alt="Photo of jalapeño pepper fritters from the Texas State Fair.">
</picture>
<figcaption>
<span>Would this jalapeño fritter from the 2015 State Fair of Texas taste less spicy if it was made today?</span>
</figcaption>
</figure>
</div>
<p>Oleoresin capsicum is an extract from peppers, containing pure heat. It’s the active ingredient in pepper spray. It’s also the active ingredient, in a manner of speaking, for processed jalapeños. The salsa industry, Walker said, starts with a mild crop of peppers, then simply adds the heat extract necessary to reach medium and hot levels. She would know; she started her career working for a processed-food conglomerate.</p>
<p>“I’ve worked in peppers in my entire life,” she told me. “Jalapeños were originally prized as being a hot pepper grown in the field. When we were making hot sauce in my previous job, we had the same problem, that you couldn’t predict the heat. When you’re doing a huge run of salsa for shipment, and you want a hot label, medium label, mild label, it’s really important to predict what kind of heat you’ll get. We tried a statistical design from the fields, and it just didn’t work, because mother nature throws stressful events at you or, sometimes, does not bring stress.”</p>
<p>The standardization of the jalapeño was rapidly accelerated by the debut, about 20 years ago, of the TAM II jalapeño line, a reliably big, shiny, fleshy pepper that can grow up to six inches long—with little to no heat. TAM II peppers have become some of the most popular in the processing business. The <a href="https://journals.ashs.org/hortsci/view/journals/hortsci/37/6/article-p999.xml">2002 paper</a> in <em>HortScience</em> trumpeted TAM II’s benefits: virus resistance, absence of dark spots, longer fruit with thicker flesh, earlier maturation, and, compared to a variety of jalapeño called Grande, less than 10 percent of the spiciness. TAMs grown in one location measured in at 1620 Scoville units, while those at another came in at just 1080, which is <a href="https://www.alimentarium.org/en/story/scoville-scale">milder than a poblano</a>.</p>
<p>In conclusion, the paper’s authors wrote, “The large, low-pungency fruit of ‘TMJ II’ will make it equally suited for fresh-market and processing uses.”</p>
<p>DeWitt, writing in his solo book <em>Chile Peppers: A Global History</em>, says TAM became widespread in Texas after its introduction. “It was much milder and larger than the traditional jalapeños, and genes of this mild pepper entered the general jalapeño pool. Cross-breeding caused the gene pool to become overall larger and milder.”</p>
<p>Since I know you’re wondering who the inventors are: the clue is in the name TAM II. The hot (but also not hot) new jalapeño is an invention of Texas A&amp;M University. Yes, Aggies took the spice out of life.</p>
<div>
<figure>
<picture>
<img decoding="async" loading="lazy" srcset="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 480w, https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg 768w" src="https://assets.dmagstatic.com/wp-content/uploads/2021/09/Hutchins-bbq.jpg" alt="Picture of a barbecue tray from Hutchins BBQ, including a bacon-wrapped jalapeño pepper.">
</picture>
<figcaption>
<span>A tray from Hutchins BBQ, including a Texas Twinkie.</span>
<span>Bret Redman</span>
</figcaption>
</figure>
</div>
<p>And yes, “II” means it’s a sequel. The original TAM came out much earlier and was profiled in <a href="https://www.csmonitor.com/1983/0607/060702.html">a 1983 article</a> in the <em>Christian Science Monitor</em>. At the time, the A&amp;M scientists estimated 800 acres were being grown nationally, and they told reporter Daniel Benedict that there was plenty of room left on the market for spicier stuff. (“For the hot-pepper lover, there’s something for him already.”) </p>
<p>After 40 years of the milder pepper enjoying increased popularity, virus resistance, higher yields, and a shiny new sequel, hotter pre-TAM jalapeños appear to have lost substantial ground. Exact statistics on planting demand are hard to obtain because growers do not want to tip off seed suppliers on how to price their products.</p>
<p>As the invention of TAM I and II suggests, “jalapeño” as a name does not connote a single breed or genetic line. There are varieties of jalapeño as there are of tomatoes. Mitla peppers are at the opposite end of the scale from TAMs, sometimes reaching 8000 Scoville units. (The A&amp;M paper derides Mitlas since they are often wonkily curved, and need more culling.) </p>
<p>In my interviews around Dallas, I learned many restaurateurs don’t know what breed their supplier is offering, or even that various breeds exist. At Hutchins BBQ, which employs four people full-time preparing around 7,000 jalapeños a week for its iconic brisket-stuffed Texas Twinkies, suppliers drop off peppers and the barbecue joint sorts through, picking the specimens they want and returning the rest. Hutchins deseeds the peppers to reduce any remaining heat.</p>
<p>For heat seekers, Walker recommends Mitla and Early jalapeños; they’re called “Early” not because they were picked early but because, as a breed, they grow quickly and are well-adapted to cooler environments.</p>
<h2>First heirloom tomatoes, next heirloom peppers?</h2>
<p>Walker compares the current state of the pepper industry with the world of American tomatoes, which were bred for hardiness in shipping, firmness, and canning. Only recently has an heirloom tomato revolution tried to cater directly to home cooks and chefs with tomato breeds that emphasize flavor and juiciness first.</p>
<p>“People lost a lot of interest in tomatoes for a long time until heirlooms came back,” Walker said. “Now we have the same thing with peppers. There’s a place for people to embrace heirloom peppers, the way that we have with tomatoes.”</p>
<p>For gardeners and small growers, the <a href="https://chilepepperinstitute.ecwid.com/Seeds-c85441005">Chile Pepper Institute</a> sells seeds but results will always be complicated, since a hot, dry summer can turn even TAM jalapeños into weapons, and a cool, wet season will result in pampered plants. But how can you find hotter peppers if you are shopping, or looking to supply your restaurant? </p>
<p>Walker’s best advice is to lobby suppliers and grocers for specific pepper breeds. Ask a produce manager or a supplier if you can get Early or Mitla peppers, or if the store can label its pepper breeds. And ignore the bogus factoids spread by many online shopping guides. I found a <a href="https://www.rachaelrayshow.com/articles/the-trick-to-picking-a-really-spicy-or-less-spicy-jalapeno-pepper">Rachael Ray Show</a> article claiming that bigger peppers are always spicier than smaller ones—which contradicts everything I had just learned about TAMs being deliberately engineered for size. Walker called that tip “misinformation.”</p>
<p>If lobbying your grocery managers sounds like a futile effort, look at the changes that have rippled through the tomato industry as breeders re-embrace heirlooms. Or look at the widespread adoption of a <a href="https://www.npr.org/sections/thesalt/2019/10/30/773457637/from-culinary-dud-to-stud-how-dutch-plant-breeders-built-our-brussels-sprouts-bo">less stinky breed of Brussels sprouts</a>, scientifically developed through a similar selective breeding process, which turned that vegetable from a punchline into a favorite.</p>
<p>“I think it’s a great opportunity for growers who really want to get into specializing in some of these heirloom varieties,” Walker said. </p>
<p>Let’s hope some farmers are reading this and yearning for the days when a jalapeño was a reliable source of spice. Those days can return.</p>
<div>
<h3>
<span>Get the SideDish Newsletter</span>
</h3>
<p>
Dallas' hottest dining news, recipes, and reviews served up fresh to your inbox each week.
</p>
</div>
<div>
<h3>
<span>Author</span>
</h3>
<div>
<figure>
<img loading="lazy" src="https://assets.dmagstatic.com/wp-content/uploads/2022/04/brian-reinhart-headshot-150x150.png" alt="Brian Reinhart">
</figure>
<div>
<h3>
<span>Brian Reinhart</span>
</h3>
<p><a href="https://www.dmagazine.com/writers/brian-reinhart/" target="_self">
View Profile
<span aria-label="Arrow">
<svg width="32" height="16" viewBox="0 0 32 16" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M25 0.929688L23.5 2.42969L28.0703 7H0V9H28.0703L23.5 13.5703L25 15.0703L32.0703 8L25 0.929688Z"></path></svg> </span>
</a>
</p></div>
<p>
Brian Reinhart became D Magazine's dining critic in 2022 after six years of writing about restaurants for the <em>Dallas Observer</em> and the <em>Dallas Morning News.</em>
</p>
</div>
</div>

</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Mistral Remove "Committing to open models" from their website (180 pts)]]></title>
            <link>https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</link>
            <guid>39517016</guid>
            <pubDate>Mon, 26 Feb 2024 21:36:25 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/">https://old.reddit.com/r/LocalLLaMA/comments/1b0o41v/top_10_betrayals_in_anime_history/</a>, See on <a href="https://news.ycombinator.com/item?id=39517016">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><p>Hey all, here's my brief business analysis.</p>

<p>It's an interesting move. I see them pullling a 180 and reframing their go2market framework from open weights to GDPR-compliant and enterprise-friendly. That niche definitely exists right now - but surely quickly also will be a market to conquer for OpenAI aswell. All of OpenAI's efforts point in this direction.</p>

<p>Then Mistral will be in the situation to be much more dependent on Microsoft for their marketing via Azure, since it's their only major inbound channel, than vice-versa. And for Microsoft it will not make sense long-term to have multiple satellite enterprises competing for the same segment, overhead in infrastructure, etc. This is a small sized bet for Microsoft, but Microsoft is known to constantly seek for portfolio synergies and optimization. We might very well see Mistral become merged to OpenAI a year or two down the road.</p>

<p>Nevertheless, every actor in the industry knows the power and ingenuity of the Open Source and research community. I strongly believe that OSS nevertheless will have an extremely strong role to play.</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[New York medical school eliminates tuition after $1B gift (451 pts)]]></title>
            <link>https://www.bbc.com/news/world-us-canada-68407453</link>
            <guid>39516927</guid>
            <pubDate>Mon, 26 Feb 2024 21:25:18 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.bbc.com/news/world-us-canada-68407453">https://www.bbc.com/news/world-us-canada-68407453</a>, See on <a href="https://news.ycombinator.com/item?id=39516927">Hacker News</a></p>
<div id="readability-page-1" class="page"><div><main id="main-content" data-testid="main-content"><article><header></header><div data-component="image-block"><figure><p><span><picture><source srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg.webp 976w" type="image/webp"><img alt="Dr Ruth Gottesman in 2016" srcset="https://ichef.bbci.co.uk/news/240/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 240w, https://ichef.bbci.co.uk/news/320/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 320w, https://ichef.bbci.co.uk/news/480/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 480w, https://ichef.bbci.co.uk/news/624/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 624w, https://ichef.bbci.co.uk/news/800/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 800w, https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg 976w" src="https://ichef.bbci.co.uk/news/976/cpsprodpb/8FD8/production/_132742863_gettyimages-536711758.jpg" width="976" height="549" loading="eager"></picture></span><span role="text"><span>Image source, </span>Getty Images</span></p><figcaption><span>Image caption, </span><p>Dr Ruth Gottesman studied learning disabilities and developed screening protocols at Albert Einstein College of Medicine.in the Bronx.</p></figcaption></figure></div><div data-component="text-block"><p><b>A New York City medical school will offer students free tuition following a $1bn donation from the 93-year-old widow of a major Wall Street investor. </b></p></div><div data-component="text-block"><p>The gift to Albert Einstein College of Medicine came from Dr Ruth Gottesman, a former professor at the Bronx school. </p></div><div data-component="text-block"><p>It is one of the largest ever donations made to a US school and is the largest ever made to a medical school. </p></div><div data-component="text-block"><p>The Bronx, New York City's poorest borough, is ranked as the unhealthiest of New York's 62 counties. </p></div><div data-component="text-block"><p>In a statement, university dean Dr Yaron Yomer said that the "transformational" gift "radically revolutionises our ability to continue attracting students who are committed to our mission, not just those who can afford it". </p></div><div data-component="text-block"><p>Tuition at the school is nearly $59,000 each year, leaving students with substantial debt.</p></div><div data-component="text-block"><p>The statement from Einstein noted students in their final year will be reimbursed for their spring 2024 tuition, and from August, all students, including those who are currently enrolled, will receive free tuition. </p></div><div data-component="text-block"><p>The donation "will free up and lift our students, enabling them to pursue projects and ideas that might otherwise be prohibitive", Dr Yomer added. </p></div><div data-component="text-block"><p>Dr Gottesman, now 93, began working at the school in 1968. She studied learning disabilities, ran literacy programmes and developed widely used screening and evaluation protocols.</p></div><div data-component="text-block"><p>Her late husband, David Gottesman, founded a prominent investment house and was an early investor in Berkshire Hathaway, Warren Buffet's multinational conglomerate. He died in September 2022 at the age of 96. </p></div><div data-component="text-block"><p>Dr Gottesman said in a statement that the doctors who train at Einstein go on to "provide the finest healthcare to communities here in the Bronx and all over the world".</p></div><div data-component="text-block"><p>"I am very thankful to my late husband, Sandy, for leaving these funds in my care, and l feel blessed to be given the great privilege of making this gift to such a worthy cause," she added.</p></div><div data-component="text-block"><p>About 50% of Einstein's first-year students are from New York, and approximately 60% are women. Statistics published by the school show that about 48% of its medical students are white, while 29% are Asian, 11% are Hispanic and 5% are black. </p></div><div data-component="text-block"><p>In an interview with the New York Times, she recalled that her late husband had left her a "whole portfolio of Berkshire Hathaway stock" when he died with the instructions to "do whatever you think is right with it". </p></div><div data-component="text-block"><p>"I wanted to fund students at Einstein so that they would receive free tuition," Dr Gottesman said she immediately realised. "There was enough money to do that in perpetuity." </p></div><div data-component="text-block"><p>She added that she occasionally wonders what her husband would have thought of the donation.</p></div><div data-component="text-block"><p>"I hope he's smiling and not frowning," she said. "He gave me the opportunity to do this, and I think he would be happy - I hope so." </p></div><section data-component="links-block"><p><h2>More on this story</h2></p></section></article></main></div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[Why Isn't Taxpayer-Funded U.S. Broadband Mapping Data Owned by the Public? (247 pts)]]></title>
            <link>https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</link>
            <guid>39516007</guid>
            <pubDate>Mon, 26 Feb 2024 19:49:14 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/">https://www.techdirt.com/2024/02/26/why-isnt-taxpayer-funded-u-s-broadband-mapping-data-owned-by-the-public/</a>, See on <a href="https://news.ycombinator.com/item?id=39516007">Hacker News</a></p>
<div id="readability-page-1" class="page"><div id="storywrap-432775">


<h3>from the <i>this-is-why-we-can't-have-nice-things</i> dept</h3>

<p>We’ve&nbsp;<a href="https://www.techdirt.com/2022/01/06/shitty-us-broadband-maps-are-feature-not-bug/">noted for decades</a>&nbsp;how, despite all the political lip service paid toward “bridging the digital divide,” the U.S.&nbsp;doesn’t <strong>truly</strong> know where broadband is or isn’t available. The FCC’s past broadband maps, which cost $350 million to develop, have long been accused of all but hallucinating competitors, making up available speeds, and excluding a key metric of competitiveness: price.</p>
<p>You only need to spend a few minutes plugging your address into the FCC’s&nbsp;<a href="https://broadband477map.fcc.gov/#/">old map</a>&nbsp;to notice how the agency <strong>comically</strong> overstates broadband competition and available speeds. After being mandated by Congress in 2020 by the Broadband DATA Act, the FCC struck a new, $44 million contract with a company named Costquest to develop&nbsp;<a href="https://broadbandmap.fcc.gov/home">a new map</a>. </p>
<p>While an improvement, the new map still has problems with over-stating coverage and available speeds (<a href="https://broadbandmap.fcc.gov/home">try it for yourself</a>). And the FCC still refuses to collect and share pricing data, which industry opposes because it would only work to further highlight monopolization, consolidation, and muted competition. </p>
<p>But there’s another problem. As broadband industry consultant Doug Dawson notes, <a href="https://potsandpansbyccg.com/2024/02/14/shouldnt-broadband-mapping-data-belong-to-the-public/">the public doesn’t even own the finalized broadband mapping data</a>. Costquest does:</p>
<blockquote>
<p><em>“…the FCC gave CostQuest the ability to own the rights to the mapping fabric, which is the database that shows the location of every home and business in the country that is a potential broadband customer. This is a big deal because it means that CostQuest, a private company, controls the portal for data needed by the public to understand who has or doesn’t have broadband.”</em></p>
</blockquote>
<p>In addition to the $44.9 million the FCC paid Costquest to create the maps, Costquest received another <strong>$49.9 million</strong> from the NTIA to provide the databases and maps for the $42 billion broadband subsidy and grant program (included in the 2021 infrastructure bill). Third parties (like states trying to shore up access to affordable broadband) have to pay Costquest even more money to access the data. </p>
<p>So it’s all been incredibly profitable for Costquest. But <strong>taxpayers are closing in on paying nearly half a billion dollars for broadband maps that not only still aren’t fully accurate, but which they can’t transparently access and don’t own despite paying for. </strong></p>
<p>That’s fairly insane any way you slice it, and as Dawson notes, it’s a detriment to the cash-strapped folks who could be helping expand access to affordable broadband (and helping fact-check the data):</p>
<blockquote>
<p><em>“Our industry is full of data geeks who could work wonders if they had free access to the mapping fabric database. There are citizen broadband committees and retired folks in every community who are willing to sift through the mapping data to understand broadband trends and to identify locations where ISPs have exaggerated coverage claims. But citizens willing to do this research are not going to pay the fees to get access to the data – and shouldn’t have to.”</em></p>
</blockquote>
<p>For decades, feckless and corrupt state and federal regulators turned a blind eye as regional telecom monopolies dominated the market and crushed all competition underfoot, resulting in spotty access, high prices, and terrible customer service. Usually under the pretense that “deregulation” (read: very little real consumer protection oversight) had resulted in immense innovation. </p>
<p>Not only did government not address (or often even acknowledge) that problem, they’re still proving somewhat incapable when it comes to transparently mapping its impact. </p>
<p>The $42 billion in subsidies flowing to many states to shore up access <a href="https://www.techdirt.com/2023/06/27/biden-re-announces-42-billion-investment-in-broadband-because-apparently-people-didnt-notice-the-first-time/">is a good thing</a>, but its impact will most assuredly be corrupted by feckless bureaucrats who can’t stand up to industry giants, aren’t keen on the idea of data transparency, and will lack the courage necessary to ensure giant monopolies with a history of fraud (like Comcast and AT&amp;T) don’t <a href="https://communitynets.org/content/charter-comcast-continue-dominate-state-grant-awards">pocket most of the funds</a>. </p>
<p>
Filed Under: <a href="https://www.techdirt.com/tag/bead-grants/" rel="tag">BEAD grants</a>, <a href="https://www.techdirt.com/tag/broadband/" rel="tag">broadband</a>, <a href="https://www.techdirt.com/tag/broadband-mapping/" rel="tag">broadband mapping</a>, <a href="https://www.techdirt.com/tag/competition/" rel="tag">competition</a>, <a href="https://www.techdirt.com/tag/duopolies/" rel="tag">duopolies</a>, <a href="https://www.techdirt.com/tag/fcc/" rel="tag">fcc</a>, <a href="https://www.techdirt.com/tag/gigabit-fiber/" rel="tag">gigabit fiber</a>, <a href="https://www.techdirt.com/tag/high-speed-internet/" rel="tag">high speed internet</a>, <a href="https://www.techdirt.com/tag/mapping/" rel="tag">mapping</a>, <a href="https://www.techdirt.com/tag/subsidies/" rel="tag">subsidies</a>, <a href="https://www.techdirt.com/tag/telecom/" rel="tag">telecom</a>
<br>
Companies: <a href="https://www.techdirt.com/company/costquest/" rel="category tag">costquest</a>
</p>
</div></div>]]></description>
        </item>
        <item>
            <title><![CDATA[On Light, Colors, Mixing Paints, and Numerical Optimization (177 pts)]]></title>
            <link>https://github.com/miciwan/PaintMixing</link>
            <guid>39515478</guid>
            <pubDate>Mon, 26 Feb 2024 19:02:19 GMT</pubDate>
            <description><![CDATA[<p>URL: <a href="https://github.com/miciwan/PaintMixing">https://github.com/miciwan/PaintMixing</a>, See on <a href="https://news.ycombinator.com/item?id=39515478">Hacker News</a></p>
<div id="readability-page-1" class="page"><div data-hpc="true"><article itemprop="text"><p dir="auto"><h2 tabindex="-1" dir="auto">On Light, Colors, Mixing Paints, and Numerical Optimization.</h2><a id="user-content-on-light-colors-mixing-paints-and-numerical-optimization" aria-label="Permalink: On Light, Colors, Mixing Paints, and Numerical Optimization." href="#on-light-colors-mixing-paints-and-numerical-optimization"></a></p>
<p dir="auto">This is a short write-up that is supposed to serve as a rough description of what's going on in the paint mixing tool in this depot.</p>
<p dir="auto">The tool is a virtual paint mixing tool and a solver that can generate recipes for creating a particular color out of existing paints. The tool comes with data for Kimera paints that I measured. The tool is a Python 3 program; it comes with all the sources, and if you have a Python distribution, you can just run it. There's also a Windows executable created with PyInstaller (see 'Releases', on the right). I can probably create a MacOS version too, if need be (edit: I actually added one; there's a .dmg file, and it does have something in it, and if you double-click it, it does show up, so it seems to work, but honestly, I barely use Mac, so it's hard for me to say if this is the right way, or is something more expected...)</p>
<p dir="auto">If you just want to grab the tool and play with it, that's about it! Have fun, and I hope you find it at least somewhat useful.</p>
<p dir="auto">But below, you'll find a more or less complete description of how it works (and when it doesn't). So, if you have a bit of time to spare, read on!</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Introduction</h2><a id="user-content-introduction" aria-label="Permalink: Introduction" href="#introduction"></a></p>
<p dir="auto">Very recently, I discovered miniature painting. I was never really into WH40K or anything related, but I have some fond memories of playing pen &amp; paper RPGs years ago, and after watching a bunch of YouTube videos, I thought it looked easy enough to try. I still suck at it, but I somehow really enjoy the tranquilizing experience of putting thin layers of paint onto 3 cm tall figurines.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/1%20-%20figureJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/1%20-%20figureJPG.JPG" width="70%"></a>
</p>
<p dir="auto">In my day job, I do real-time graphics engineering for video games, and it quickly turned out that a lot of the problems I deal with at work are very similar to those in miniature painting: you analyze how light behaves, how it interacts with different surfaces, how the eyes perceive it, etc. Of course, painting is not just engineering; it's <em>Art</em> after all (capital 'A'), but there seems to be a consensus that painters should understand these technical aspects, even just to know when they deliberately break them.</p>
<p dir="auto">There were a number of things that looked like fascinating research projects somewhere between miniatures and computer graphics, but one thing that sparked my particular interest was paints. Miniature paints usually come with these cryptic names: Skrag Brown, Tombstone Horror, or whatever. I don't really mind, but the producers never actually tell you what these colors actually are. And when you have limited experience, it's often hard to tell if a particular paint will work as some midtone or if it will be too dark. Many YouTube tutorial videos actually tell you which exact paints they use, but they most often come from different lines, some are immediately available, some are not, and for some, you need to wait - and I want to paint this very second! It seemed pretty clear that instead of buying all the possible paints, the more reasonable approach would be to pick some base paints and learn to mix them to get the colors that I need.</p>
<p dir="auto">For a beginner, there are, however, two problems. First: mixing paint is not a particularly intuitive process: sometimes you get something reasonable, sometimes you get muddy brown. Second: you need to know what color you actually want to get. Sure, there are some nice videos on how to color match, but if you don't have a good intuition of what skin tone you want to achieve, it's hard to tell if your mix needs more blue or red.</p>
<p dir="auto">Because of my engineering background, the solution seemed obvious: I would like to just pick a color on the screen (from, say, a photograph) and I want to know which paints, and how much of them, to mix to get it. I would also like to experiment with mixing paints without actually having to waste physical paint. For that, I need to somehow characterize the paints that I have, I need a model for simulating how they mix, and I need a numerical solver that will be able to minimize the error between the color that I want and a mix of some number of paints. These sorts of processes are something that I regularly go through and enjoy, so it looked like a perfect on-the-side project.</p>
<p dir="auto">Disclaimer here: yes, I know that in practice no one works that way. Especially if the solver gives you ratios like 88 parts of white, 3 parts of blue, and 2 parts of yellow - there's no way to mix something like this on a wet palette where you work with a minuscule amount of paint. But, at least to me, it's still useful to know that it's mostly white, with a touch of blue and yellow, so when I mix something on the palette, I'm not doing it completely blind. And yes, if you've been painting for some time, you learn these things, you get that intuition. But you need to get it somehow. Painting takes a lot of practice, so if I can do some experiments purely digitally, I'm totally up for it. And to be honest, it's all more of a cool side project rather than anything else.</p>
<p dir="auto">Just in case anyone else finds it useful, I thought I'll write up all the theoretical basis for a simple tool I developed for this and provide it together with a simple Python code. Since I just had a bit of free time (I got COVID), and I just got my set of Kimera paints (which are single pigment, so incredibly saturated colors, amazing for mixing), I spent a week on this, and you can read about the results here. As it might be read by people with a less technical background, I'm trying to keep it all pretty simple and self-contained, so all the information you need to understand it is here. I'm not sure how that worked out in the end, but if something is unclear, feel free to ping me and ask for details. None of it is actually any rocket science; it's mostly some high-school level math and physics (but if you're allergic, a warning: there is some math in there).</p>
<p dir="auto">So if you're curious about how it all works, details below.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Light</h2><a id="user-content-light" aria-label="Permalink: Light" href="#light"></a></p>
<p dir="auto">Light is an electromagnetic wave, oscillations of electrical and magnetic fields propagating in space. Human eyes are sensitive to wavelengths between roughly 400 and 700 nanometers, which we perceive as colors, from violet, through blue, green, yellow, orange, to red.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/2%20-%20spectrum.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/2%20-%20spectrum.JPG"></a>
</p>
<p dir="auto">Light that usually reaches our eyes is a mixture of many different wavelengths. Depending on the ratios between the amounts of particular wavelengths, we perceive the light as different colors. If it consists mostly of the shorter visible wavelengths, we'll see it as blue. If it's mostly longer wavelengths, it's going to be red. The more precise details are further down, but that's the general intuition.</p>
<p dir="auto">To reason about these characteristics in a more principled way, one useful tool is a so-called spectral power distribution (SPD for short). It's a function that, roughly speaking, describes how much of a particular wavelength is present in some radiation. It is usually plotted as a graph, with wavelength on the horizontal axis and some energy-related quantity on the vertical axis (so the stronger a particular wavelength is, the higher the plot).</p>
<p dir="auto">So the "generally blue" light might have an SPD like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/4%20-%20spectrum%20blue.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/4%20-%20spectrum%20blue.JPG" width="35%"></a>
</p>
<p dir="auto">and the "generally red" light might have it more like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG" width="35%"></a>
</p>
<p dir="auto">One particularly interesting family of SPDs are those of different light sources. You can take any light source and measure how much of the light it produces comes from particular wavelengths. There's this thing in physics called black body radiation that describes the SPD of a perfectly black body (so that it doesn't reflect any light, just generates it) heated to a particular temperature (all that actually led straight to quantum mechanics and the world we know today; actually analyzing spectra of starlight led to the understanding that the distant stars produce energy just like the sun, the lines appearing in the spectra of excited gases was another catalyst in the evolution of quantum mechanics, and shift in the spectra of the light from different galaxies led to the discovery that the universe is expanding; it's all in the spectrum). If you've ever come across these "2700K light", "5000K light" markings, they are exactly that - they describe the light color as the color of a black body radiator of a given temperature, in Kelvin.</p>
<p dir="auto">The SPD of a typical ~2800K incandescent light looks like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/5%20-%202856K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/5%20-%202856K.JPG" width="35%"></a>
</p>
<p dir="auto">and a ~4200K fluorescent light looks like this.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/6%20-%20fluorescent.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/6%20-%20fluorescent.JPG" width="35%"></a>
</p>
<p dir="auto">The SPD of what we consider sunlight is a fairly complex distribution that includes not only the actual SPD of the light generated by the sun but also the absorption and scattering of some of it occurring when the light passes through the atmosphere. Because the sunlight encounters different amounts of atmosphere on its way at different times of the day (less at noon, more in the evening), the sunlight SPD also depends on the time of day and atmospheric conditions. On a typical sunny day, it might look like this:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/7%20-%205000K.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/7%20-%205000K.JPG" width="35%"></a>
</p>
<p dir="auto">Because we're doing science here, everything needs to be standardized, measured, and quantified. For that reason, the CIE (International Commission on Illumination) introduced "standard illuminants" - a number of SPDs describing some very particular lights. Illuminant A represents a typical tungsten filament light bulb, a black-body radiator at 2856K. Illuminants B and C have become pretty much obsolete in favor of Illuminants D. There's a whole family of these; they describe daylight in different conditions - from more "warm" ones (D50, D55) to colder ones (D65, D70). The numbers 50/55/65/70 roughly correspond to a black-body temperature that would emit light of similar color (5000K, 5500K, 6500K, 7000K), but it's a longer topic and not particularly relevant here. There are also other illuminants (like E and F), but in most practical situations, the interesting ones are A, D50, and D65 (especially the last one).</p>
<p dir="auto">One last thing that seems fairly obvious, but is very important later on: light behavior is linear (in mathematical terms). If you take two lights with two SPDs and you turn them on at the same time, the resulting lighting will have an SPD that's the sum of the two components. If you make the light twice as bright, the resulting SPD will be two times greater.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Reflection</h2><a id="user-content-reflection" aria-label="Permalink: Reflection" href="#reflection"></a></p>
<p dir="auto">We rarely see light as it is generated by some source. Before it reaches our eyes, it usually bounces off things, and we register that indirect, reflected light.</p>
<p dir="auto">The way light interacts with surfaces is an incredibly complicated topic. The most basic principle is fairly simple and described by Fresnel's equations: light reaches the boundary between two mediums (say, air and an object) and some of it gets reflected off the boundary, and some of it gets refracted into the object. The angle between the reflected light and the normal to the surface (which is a direction perpendicular to the surface) is the same as the angle between the incident light and the normal (alpha on the figure below). How much of the light goes where, and the exact direction of the refracted light, depends on the index of refraction of both mediums (which describes how fast light travels in that particular medium compared to its speed in vacuum).</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/8%20-%20fresnel.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/8%20-%20fresnel.jpg"></a>
</p>
<p dir="auto">Unfortunately, this only describes reflection off a perfectly smooth, mirror surface - nothing like anything you see in reality. And it only describes the first reflection off a boundary. But light can bounce around off the microscopic roughnesses of the surface and go into the object in a different place. Or it can do it multiple times. Or it can go into the object, bounce around there, and go out (or not, there's a boundary when going outside to the air as well). And all this is ignoring any wave phenomena - diffraction, interference, etc.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/9%20-%20complex%20interactions.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/9%20-%20complex%20interactions.jpg"></a>
</p>
<p dir="auto">Physics, optics, and related fields have tried to simplify all these concepts and created multiple models for describing and quantifying these effects. Some are simpler, some are very complex. Computer graphics loves them because they allow us to render realistic-looking images on a computer.</p>
<p dir="auto">From the perspective of a miniature painter, the simplest way of looking at the light-material interaction is to split it into two components. I will call them "diffuse" and "specular" because these are the terms used in computer graphics, which I'm used to.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10%20-%20diffuse%20spec.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10%20-%20diffuse%20spec.jpg"></a>
</p>
<p dir="auto">The "specular" component of the reflection is everything that happens on the actual boundary between the air and the object. Some of the light bounces off it. Generally, it follows the law of reflection, so the angle between the direction the light falls onto the object and the normal is the same as the angle between the direction the light is reflected at and the normal (the normal is the direction perpendicular to the surface). I say "generally" because if the surface is not perfectly smooth, the light will be scattered in different directions - the rougher the surface, the more scattered it will be - but generally, it will be around that reflected direction. One very important bit: in the case of non-metal materials, light reflected this way does not change its color. The reflected light will have the same SPD as the one falling on the object. Interestingly, this behavior is very similar for most non-metals. To the extent that in computer graphics, we often just treat all non-metal surfaces the same way: they can be rough or smooth, but they reflect the same amount of light: no matter if it's plastic, skin, or concrete. It's a very decent approximation. Metals, due to their atomic structure, are different. When the light reflects off their surface in a specular reflection, it actually changes color. That's why gold is yellow and copper is orange.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/10.5%20-%20rough%20spec%20vs%20smooth%20spec.png" width="40%"></a>
</p>
<p dir="auto">The "diffuse" component is all the light that goes into the object and then some of it gets out and is, generally, scattered uniformly in all directions (or rather: let's just assume it for simplicity, that's a good enough approximation). It doesn't matter how the surface is viewed; its diffuse lighting is the same from all angles (unlike specular, which is strongly visible when viewed from that one particular direction, and not much when viewed from others). Not all the light that gets into the object gets scattered out. Some of it is absorbed and turned into heat. There's also an important difference between metals and non-metals when it comes to the diffuse component. For non-metals, the spectrum of the reflected light very much depends on the object: after all, the light goes into the surface, bounces around, and comes out - so it picks up some of the characteristics of the object. For metals, there's no diffuse component at all. The light doesn't go out; it's either absorbed or bounces off specularly. And even though, technically, the diffuse component is not "reflection", but rather a form of scattering occurring over short distances within the material, I will oftentimes just say "diffuse reflection" for simplicity.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/11%20-%20diffuse%20subsurface.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/11%20-%20diffuse%20subsurface.jpg"></a>
</p>
<p dir="auto">Side note: even though it's safe to treat the diffuse reflection as the same in all directions, its brightness still depends on the amount of light <em>falling</em> onto the given part of the object. And this amount is related to the angle between the normal of the surface and the direction towards the light: the bigger the angle, the less light the given part of the object receives (actually, the amount of light is the same; it's just distributed over a larger area, which makes "light per area" smaller, and this is what we consider "brightness"). In physics, this is called Lambert's law. In many miniature painting tutorials, people talk about shading basic shapes in a particular way: spheres have round shading towards the light, cylinders have highlights along the axis, and cubes/surfaces have generally flat lighting, depending on their orientation. This is a practical application of Lambert's law. Spheres have smoothly changing normals in all possible directions, so their brightest area is going to be in sections facing the light. Normals of a cylinder change as you go around the circle but are the same as you move along the axis, so the entire length of the cylinder is shaded the same. Flat planes have a constant normal, so every point gets the same lighting.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/12%20-%20NdotL.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/12%20-%20NdotL.jpg"></a>
</p>
<p dir="auto">(Side note to the side note: this is, of course, only true if we assume that the direction towards the light is the same on the entire surface! But is it? If we consider sunlight, the source is so far away that we can safely think that yes, every point gets the light from the same direction - it's a directional light. But for other sources, the position of the light matters too. And if the light is not a simple point but rather something larger, it all gets complicated even more).</p>
<p dir="auto">For paints, we can focus on the diffuse component only. Miniature paints dry pretty matte, so the surface of the dried paint is pretty rough. The specular component of the reflection is very faint and does brighten the surface a bit with light color, but the main characteristics of the appearance come from the diffuse part. I'll go into some theories describing what is going on with light in the paint layer later on, but for now, we can look at the macroscopic effect: light with some particular SPD falls onto the paint layer, and some of it gets scattered uniformly in all directions, and some get absorbed. We can compute the ratio of that scattered light to the incident light. This is called reflectance and, just like SPD, is a spectral characteristic: objects reflect different wavelengths differently. Some wavelengths are reflected more, some are absorbed more. This is what determines the color of the object. If an object absorbs short wavelengths and reflects and scatters long ones, it will appear reddish. If it reflects short wavelengths and absorbs long ones, it will be bluish. This is, of course, assuming that the illumination is uniform in all wavelengths. If there are no long wavelengths in the incoming lighting and the object reflects only long wavelengths, it won't look red; it will look black, as it will not reflect any light. The actual light that we see reflected diffusely off the object is a product of the object's reflectance and the SPD of the incoming light:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/13%20-%20reflection%20eq.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/13%20-%20reflection%20eq.png"></a>
</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/14%20-%20D65%20times%20(%20yo%20+%20magenta)..JPG"></a>
  <span>=</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/D65.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/D65.JPG"></a>  
  <span>*</span>
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/3%20-%20spectrum%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/3%20-%20spectrum%20red.JPG"></a>     
</p>
<p dir="auto">Not surprisingly, the same object can look very differently depending on the light used for illumination. And you can reason about the color of the object in different illumination using the above principles: if the object is blue in white light but is illuminated by yellowish-red candlelight, it will appear greenish.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The eye</h2><a id="user-content-the-eye" aria-label="Permalink: The eye" href="#the-eye"></a></p>
<p dir="auto">The next component of the whole system is the human eye: the light that gets reflected off objects reaches the eye, and we can see it. In the most simplistic view, the eye focuses the incoming light on the retina, which contains light-sensitive cells. There are two families of these cells: rods and cones. Rods respond strongest to light in the green part of the spectrum, they are responsible for seeing in low-light conditions, and they do not contribute to color vision - so we'll ignore them here. Our ability to see colors comes from cones. People with no form of color blindness have three types of cones, usually called L, M, and S - for long, medium, and short. They most strongly respond to the light with long-red wavelengths (L), medium-green (M), and short-blue (S).</p>
<p dir="auto">Just like SPD and object reflectance, we can define spectral sensitivity. It describes how strongly an eye (or any other sensor, camera, or similar) responds to light of a particular wavelength. It shows how excited the given type of cone is to see light of a particular frequency (pun intended) - the higher the sensitivity, the stronger the signal generated by the cone when it receives the same amount of energy. Here are the plots of sensitivity of three different types of human cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/16%20-%201920px-Cones_SMJ2_E.svg.png" width="35%"></a>
</p>
<p dir="auto">A cone excited by light produces a signal, but the actual wavelength that caused that excitation is irrelevant. The signal will be the same if, say, the M cone receives X amount of energy at 550nm, twice the X amount of energy at 500nm, or X energy at 500nm and X at 600nm. Technically, all the incoming energy at a particular wavelength is multiplied by the sensitivity of the sensor for that wavelength and added (integrated, but let's keep things simple) for all wavelengths to produce the cell signal. This actually leads to an interesting phenomenon called metamerism, where different distributions of incoming light can look the same to a human eye, just because they generate the same excitations in the cones.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/17%20-%20excitation.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/17%20-%20excitation.jpg"></a>
</p>
<p dir="auto">We now have a full picture: light is emitted by some source (source SPD), gets reflected by an object (source SPD multiplied by the object reflectance) and is registered by a cone in the eye (source SPD, multiplied by object reflectance, multiplied by the cone sensitivity, and added/integrated for all wavelengths). That last operation essentially involves calculating the area below the graph on the right-hand side (marked in red) - the larger the area, the greater the cone excitation, but the exact shape of that graph is irrelevant.</p>
<p dir="auto">Because our eyes contain three types of cones, the light generates three different signals that our brain interprets as color. Because there are three types of cones, the space of colors that people can see is intrinsically three-dimensional: visible colors can be described by three numbers, instead of having to deal with the entire spectral information (yes, this is where RGB comes from, but we'll get there...).</p>

<p dir="auto">All this was heavily investigated by researchers in the early 20th century. Measuring SPDs of different lights is pretty straightforward; you split the light with a prism or some diffraction grating and look at the rainbow of colors. Measuring reflectance is also relatively simple: you measure the SPD of some light, then shine it onto an object, measure the SPD of the reflected light, and divide one by the other. Measuring the sensitivity of the human eye is unfortunately really challenging. How would it work? It is very challenging to accurately tell if some particular light is 2 times or only 1.5 times brighter than some other light. Especially since the human vision system is (again) incredibly complex: rods and cones are one thing, but there's tons of processing going on later in the brain, that includes adaptation to light, color, and tons of other things (there are models for these effects too, Color Appearance Models if you want to read about some details, but it's way beyond the scope of anything here). But clever people figured out a way. They designed so-called "color matching experiments". (The actual sensitivities of the eye were actually measured in the 1950s, by Ragnar Granit, and he was awarded a Nobel Prize for this work).</p>
<p dir="auto">In the color matching experiments, a person is shown a color on one side of a screen. It's a pure color, generated by a light of a very narrow range of wavelengths. They then have to match that color by mixing three "base" lights on the opposite side of the screen - also pure colors of some particular wavelengths (they were chosen to be red, green, and blue, as it was already known that these are the colors that give peak response of the eye cells). Since not every pure color could be mixed from these three base lights, the researchers added the option to add some of the mixing light to the target color, which would effectively act as a negative mixing weight. By showing people the colors for every possible wavelength in the visible light, the researchers measured how much of the intensity of these three reference lights is needed to get the impression of the same color.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/19%20-%20color%20matching.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/19%20-%20color%20matching.jpg" width="35%"></a>
</p>
<p dir="auto">It's not quite the spectral sensitivity of a cell in the eye, but for all practical purposes, it's just as good: we can now quantify the color of any pure wavelength with three numbers. And because light is linear, any mixture made from different wavelengths can be described as some weighted sum of these triplets. So, <em>any visible color</em> can be precisely characterized by just three numbers.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/20%20-%20CIE1931_RGBCMF2.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/20%20-%20CIE1931_RGBCMF2.png" width="35%"></a>
</p>
<p dir="auto">The curves derived in the color matching experiments are called the r/g/b matching curves. The experiments were conducted in the 1920s, and back then, negative numbers were a big no-no when it came to numerical calculations. But because not all colors could be matched by additive mixing, some colors required adding light to the target color, resulting in some of the values in these curves being negative. So, the same CIE that standardized illuminants figured out that it would be good to transform these curves mathematically, so they are always positive. This would represent the mixing of some imaginary lights that cannot exist in practice. This would mean that some of the combinations of the numbers would represent colors that cannot exist, but they thought it's a good deal to get rid of these negatives. They also wanted one of the numbers to represent the general "brightness" of the color (that corresponds to the response of the rods). So, they did some mathematical magic and, based on the r/g/b matching curves, formulated the X/Y/Z matching curves.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/21%20-%202560px-CIE_1931_XYZ_Color_Matching_Functions.svg.png" width="35%"></a>
</p>
<p dir="auto">And these X/Y/Z curves are the basis of modern colorimetry. They are still used today when describing color in any scientific or technical application. They form the basis for the XYZ color space, where every color has the three X/Y/Z values, and the Y value is the color brightness. The X/Y/Z matching curves behave exactly like the spectral sensitivities - but instead of being the sensitivities of cells in our eyes, they are just some mathematical abstract sensitivities, not describing anything in particular (well, the Y curve does represent the overall sensitivity of the eye to brightness). But since they are just some numbers, it doesn't really change anything; it's all just some abstract math anyway.</p>
<p dir="auto">We went from a continuous spectrum of visible light to three numbers we can do operations with. We can take light with the XYZ_1 color and add it to the light with XYZ_2 color, and we will get the (X_1 + X_2), (Y_1 + Y_2), (Z_1 + Z_2) color. We can take the light with XYZ_1 color and make it two times stronger, and we will get 2X_1, 2Y_1, 2*Z_1 color. This is called a color space. Technically, it's not really correct to multiply the components together - say, have the reflectance stored as XYZ and multiply it by the light XYZ, but it actually works pretty well (look at most of the modern CGI or video games; we all do this all the time. The only commercial renderer that actually works with spectral quantities and does it correctly is Manuka from Weta, used in "Avatar," "Planet of the Apes" series, and other movies).</p>
<p dir="auto">Fun fact: you might think that the matching curves were derived after massive studies involving thousands of volunteers. Quite far from reality, actually. The original experiments were performed by two people: W. David Wright with 10 (as in "ten") observers and John Guild with 7 ("seven") observers. Yes, the whole modern colorimetry is based on what 17 people saw in the 1920s, most likely friends and families of the two gentlemen.</p>
<p dir="auto">Later on, there were some revisions to these curves - most notably the 1964 10-degree standard observer (vs. the original 1931 2-degree standard observer): later researchers realized that due to all the adaptation in the visual system, it makes more sense for the test patches to take a larger part of the field of view, so they performed new experiments and created new curves that are slightly different. But long story short, the 2-degree, 1931 curves are still most commonly used today.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/22%20-%201931%20vs%201964%20xyz.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/22%20-%201931%20vs%201964%20xyz.jpg" width="35%"></a>
</p>
<p dir="auto">All the color spaces used in modern applications are derived from the XYZ. Some are just a linear transformation of the XYZ - so, just multiplying individual components by some constants and adding them in different combinations (like sRGB), while some involve some non-linear operations (computing powers of some expressions involving XYZ, or division) to make the values more "perceptually uniform" - so that differences in numbers describe similar changes in actual perceived colors (like Lab).</p>
<p dir="auto">Two important color spaces derived from XYZ are Yxy and sRGB. The first one uses Y to describe brightness and xy (which are X/(X+Y+Z) and Y/(X+Y+Z)) to describe the chromaticity, just the hue and saturation of the color. Oftentimes, all the visible colors are visualized in the xy plane as the visual locus, a space of all the colors that people can see. It is a convenient way of visualizing gamuts: subsets of the visible color that can be produced in some particular way, on screen or in print.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/23%20-%20YxyJPG.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/23%20-%20YxyJPG.JPG" width="35%"></a>
</p>
<p dir="auto">sRGB is a subset of XYZ, widely used in modern display technologies. It defines three primary colors (R, G, B) that are used to represent other colors and also specifies a "white point," which is considered the color white. The primaries were chosen so they can be physically realized in a technically feasible manner. However, the range of colors that can be represented in sRGB is somewhat limited because it relies on these base colors and negative weights cannot be used; monitors work by emitting light, not absorbing it. Ironically, the visual locus plots of all possible colors, when viewed on a typical screen, cannot fully display a wide range of these colors. Although sRGB is prevalent in the industry, it is gradually being replaced by standards with wider gamuts, such as Rec2020 in modern HDR displays. It's important to note that RGB values only make sense within the context of a specific color space and cannot be directly transferred between different spaces. For example, many cameras can capture photos in Adobe RGB, which has a slightly larger gamut than sRGB. To display these colors accurately on a standard monitor, they need to be converted to sRGB through additional calculations. This is the purpose of color profiles in software like Photoshop, which allow you to specify the color space you're working in and the color space of your input images, with the software handling all necessary conversions.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/24%20-%20sRGB%20gamut.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/24%20-%20sRGB%20gamut.JPG" width="35%"></a>
</p>
<p dir="auto">On the other hand, CMYK, commonly used in printing, cannot accurately describe general light color. It is designed to describe reflectance rather than light emission. CMYK colors, to be precise, require the definition of a specific illuminant and how individual components absorb and reflect light. It's not really a color space but more of a color model - a way to process color, rather than a precise mathematical method for defining and operating with colors.</p>
<p dir="auto">This is generally the case with any color system based on reflected light, including painted miniatures. To reason about their colors, we need to assume a certain illumination. Due to the widespread use of sRGB, the most sensible choice for an illuminant in such situations is D65, which is cold, white lighting and is the white point in sRGB, so (255, 255, 255) on a typical screen.</p>
<p dir="auto">To determine the color of paint and display it on a screen, the full process involves:</p>
<ul dir="auto">
<li>Taking spectral values for the D65 illuminant (this is publicly available, tabulated data).</li>
<li>Multiplying it by the spectral reflectivity of the paint.</li>
<li>Multiplying the result by the X, Y, and Z matching curves (again, publicly available).</li>
<li>Adding (integrating) the spectral values across all wavelengths to get X, Y, Z values of the light reflected off the paint layer.</li>
<li>Converting the XYZ values to sRGB with some simple math and displaying it on the screen.</li>
</ul>
<p dir="auto">You might notice that because of that integration across wavelengths, there's really no way of going back: many different spectral distributions can give the same XYZ values (the phenomenon of metamerism mentioned earlier). So, given the sRGB values, there's no straightforward way to retrieve the full spectrum back. However, we would like to overcome this limitation, so we'll try to work around the problem.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Pigments</h2><a id="user-content-pigments" aria-label="Permalink: Pigments" href="#pigments"></a></p>
<p dir="auto">The only missing bit to get the paint mixing software is the model for actually mixing the paints. You might think that maybe if we take 50% of paint A and 50% of paint B, the resulting mix will have the 50-50 mix of reflectance of paint A and paint B (or technically: that paint mixing is linear). Well, bad news, of course, it isn't linear. It's actually highly nonlinear, and to get something meaningful, we need to dive into radiative transport theory and differential equations. But no worries, we'll only skim the surface.</p>
<p dir="auto">We previously considered the diffuse reflection as light going into the object, bouncing around, scattering a bit, being absorbed a bit, and going out. Now, we will look into this process in just a bit more detail.</p>
<p dir="auto">The physical theory that describes processes like this is called radiative transport theory. It can describe phenomena like the blue color of the sky or the color of mixed paints, or more generally the effects of energy traveling through some form of medium. On the most basic level, it deals with two effects: absorption of radiation (light for us) and scattering. Absorption means that some of the light gets absorbed by particles of the medium and is converted to heat. As the light passes through the medium, there's less and less of it; it gets attenuated. Media that mostly absorb light appear translucent. If light goes through them, it can change color (because certain wavelengths can be absorbed more than others), but otherwise appears unchanged. If absorption is the main effect of the medium, it can be characterized by the Beer-Lambert law (which is a specific form of the radiative transfer equation), which states that the extinction of light is exponential with distance: the more light goes in, the more of it is absorbed on a unit distance. The absorption is fairly simple to model; after all, if the light is absorbed, it is gone, and we don't have to deal with it anymore.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/25%20-%20absorbtion%20and%20scattering.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/25%20-%20absorbtion%20and%20scattering.jpg" width="70%"></a>
</p>
<p dir="auto">The other effect is scattering. When light hits a particle of the medium, it can be scattered - bounce off in some other direction. And this is where the majority of the complexity comes from. Depending on the medium, the way its particles scatter light is different. Some media scatter predominantly forward (so the light direction is just slightly affected), some scatter uniformly (so whatever direction light comes from, it can get scattered in any other direction). On top of that, when light is scattered, it travels further and interacts with more particles, undergoing more scattering events. And of course, it all happens together with absorption, so scattered light might be scattered more, and then absorbed, and so on. This results in, once again, incredibly complex systems. Thousands of academic papers have been published on various ways of solving the radiative transfer equations for some particular setups of light, medium, and so on.</p>
<p dir="auto">Luckily for us, mixing colors is a pretty common problem in various industries, so it has been extensively studied too. To make things a bit simpler, usually, a number of simplifications are made: instead of dealing with some general objects, people analyze transport within very thin horizontal slabs of material. They are considered infinite, to avoid having to deal with any problems on the boundaries, but that's perfectly fine, as for our diffuse reflection, the light doesn't really go very far to the side within the material. They also assume that the scattering function is uniform, so when light is scattered, it bounces in any random direction with no preference. For regular, no-effect paints (so non-metallic paints), that's a totally valid assumption. Lastly, they assume circular symmetry along the vertical axis - which means that it all behaves the same way no matter how around the surface we look at it - the surface properties are the same (material is isotropic) and the lighting is the same too (for instance, the sample is illuminated in the same way from every direction). With all these assumptions, you can simplify the radiative transfer equations and reason about two separate "streams" ("fluxes") of lighting: one going down, into deeper layers, and one going up, and eventually leaving the paint layer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/26%20-%20two%20diffuse%20fluxes.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/26%20-%20two%20diffuse%20fluxes.jpg" width="70%"></a>
</p>
<p dir="auto">Given a setup like this, the radiative transport equation actually simplifies to something reasonable that can be solved by hand (if you remember how to solve systems of differential equations, of course - don't look at me here, though). If, on top of that, you assume that the thickness of the object is big enough to not pass any light through (for instance: the paint fully covers the substrate below), you get a fairly simple formula that gives you the reflectance as a function of two parameters describing scattering and absorption.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20two%20flux%20reflectance.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20two%20flux%20reflectance.png" width="40%"></a>
</p>
<p dir="auto">The K coefficient describes the absorption, and the S coefficients describe scattering. Just like SPDs, reflectance, and sensitivities, they are spectrally varying: so they are different for every wavelength.</p>
<p dir="auto">This was done, although in a bit less principled fashion, in the 1920s by Kubelka and Munk, forming what is known nowadays as "Kubelka-Munk theory". It got revised later, in the framework of radiative transfer theory, in the form of two-flux theories (K-M is sort of two diffuse fluxes), and extended in three- and multi-flux theories. If you're curious, there's a great book by Georg Klein, "Industrial Color Physics," that dives into details. For practical purposes, when working with regular diffuse, non-effect paints, two-flux or Kubelka-Munk is generally considered enough. One thing that I don't cover here are the Saunderson corrections - these are additional factors that actually take the light specularly reflected on the surface of the paint into account - and reduce the amount of light that goes into the paint layer (and out of it).</p>
<p dir="auto">And this is one of those rare cases where things are actually simple. Mixing of media is linear with respect to K and S coefficients. The mixture of 50-50 paints with coefficients K1 and K2 will give 0.5K1 + 0.5K2, the same with S. In general:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/27%20-%20k%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/27%20-%20k%20sum.png" width="40%"></a>
</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/28%20-%20s%20sum.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/28%20-%20s%20sum.png" width="40%"></a>
</p>
<p dir="auto">(the w1 and w2 weights need to add up to 1.0)</p>
<p dir="auto">Now, we only need to get these K and S coefficients from somewhere.</p>
<p dir="auto">The equation for reflectance gives us reflectance as a function of K and S, but we can rearrange it to get, for instance, K based on S and reflectance. We know how to measure reflectance, so if we only figure out where to get S from, we could compute K. The trick is to take one of the paints - usually white - and just assume that it has S equal to 1.0 for all wavelengths. We then measure its reflectance and compute K from that.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/29%20-%20k%20from%20s.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/29%20-%20k%20from%20s.png" width="30%"></a>
</p>
<p dir="auto">Now for any other paint, we also need K and S, but we cannot arbitrarily set their S to be 1.0. However, we can mix them with that white which will act as our reference value. This gives us two measurements: one of the raw paint (so-called "masstone") and one mixed with white in some proportions (that we of course need to know, but that's easy, we can just weigh the components). This, together with the equations above, gives us two equations and two unknowns: K and S for the new paint (it's a separate system of equations for every wavelength). This is trivial to solve and gives us everything we need to know to model paint mixing digitally.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/30%20-%20k%20and%20s%20from%20mix.png"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/30%20-%20k%20and%20s%20from%20mix.png" width="110%"></a>
</p>
<p dir="auto">Big equation, sorry :( But it's pretty simple really. We know R(eflectance) for paint and for the mix of the paint with white (this is what we measure). We know K and S for white (we just calculated it above, setting S to 1.0), and we know how much of both white and paint we put into the mix (w(eight)_paint and w(eight)_white), so it's really just a system of two equations with two unknowns (K and S for the paint). I marked all the things that we know in blue and the unknowns in red.</p>
<p dir="auto">In the real world, usually more than one mix is done, usually around five, in different proportions, to get a better estimate of the K and S coefficients. This requires slightly more complex math, but these are technical details (although important if you're trying to do it on a budget, see below).</p>
<p dir="auto">Given all this theoretical background our course of action is:</p>
<ul dir="auto">
<li>take some number of paints, preferably saturated paints that will give us a wide range of mixed colors</li>
<li>measure the reflectance of a fully opaque white layer, assume the S for that paint is equal to 1.0 for all wavelengths and derive K (the reflectance of white paint will generally be fairly uniform and close to 1.0, but not quite, and we actually need to measure it properly)</li>
<li>for every other paint in the set prepare a fully covering patch of the paint straight from the pot and its mix with white in some measured proportions.</li>
<li>measure the reflectance of both patches, derive K and S coefficients for that paint</li>
<li>we can now take any paints, in any proportions, mix their K and S coefficients</li>
<li>from the mix, we can compute the reflectance, use D65 illuminant to virtually illuminate it, and convert it to XYZ and later to sRGB</li>
<li>given this machinery we can do numerical optimization to find the best combination of paints that gives us a given sRGB color</li>
</ul>
<p dir="auto">And that's pretty much what I did. Let's dig into practical details.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Measurements</h2><a id="user-content-measurements" aria-label="Permalink: Measurements" href="#measurements"></a></p>
<p dir="auto">The crucial part of the whole above operation is the measurement of the spectral reflectance of the color samples. You need a spectrophotometer for that. There are some lab-grade ones, and there are handheld ones, aiming for industry (for validating if the plastic coming out from your injection moulder is the right color, or if your printing press prints the color that the customer ordered). They can do all sorts of interesting things, measure according to different standards (there's an ISO for that, 13655 if I remember correctly) compute color differences, some have filters etc. They have one thing in common: they are pretty expensive. The decent ones start at a few thousand dollars (though you can get the nix spectro - <a href="https://www.nixsensor.com/nix-spectro-2/" rel="nofollow">https://www.nixsensor.com/nix-spectro-2/</a> - that looks interesting and might be just enough - and it's only a bit over one grand - but I never tested it so it's hard for me to judge its accuracy) - so generally not the money you want to spend on a hobby project that you're going to forget about in a month. But luckily, a while ago, I actually constructed a handheld spectrophotometer at work. We needed some particular type of measurement, that's very rarely used in practice, so regular handheld devices do not support it. It was an interesting project on its own, but I'll spare you the details. The heart of the device is the Hamamatsu C12666MA sensor, some analog-to-digital converter and a microcontroller to drive all this. The reference lighting is provided by a wide spectrum LED, which is not really perfect - as the lighting below 410 nm and above 680nm is a bit weak, making readings in these ranges less reliable - but it was totally fine for us. During all the calibration procedures I went through when making these devices (I made something like 8 of them total) I got a set of calibrated color samples that came with full spectral reflectance data - so I was able to compare it with the ones produced by my device. They were all within 1-2% for each wavelength, so I generally trust the results. And I know the exact conditions used for measuring the calibration data, which happen to be the exact conditions used for typical industrial measurements of paint samples, so even though I know my device is not quite ideal for measurements like this, I know it's not too far off either.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/31%20-%20spectophotometer.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/31%20-%20spectophotometer.jpg" width="70%"></a>
</p>
<p dir="auto">For my set of paints, I picked the Kimera set (the one with 13 base colors). Kimera paints are great because they are single pigment, and the manufacturer actually tells you what pigment this is. And they are super heavy on pigment, so you don't need a lot to create a fully opaque layer. And because of all this simplicity, I trust they are consistent - after all, it's just some amount of given pigment and medium. That gives me some confidence that two bottles of white will actually behave the same, which is important if you want any reproducibility. Not that I want to complain here, but it's very much not the case with, for instance, Citadel or Vallejo paints. Even ignoring all these silly names, their consistency is absolutely random. Sometimes they are fine straight from the pot, sometimes they need tons of water to flow at all. If I'm working on some recipe for a color, I need to be certain that different samples of the same paint will behave the same. Neither Citadel nor Vallejo seems to guarantee that, which is a bit of a bummer because Kimeras are incredibly hard to get where I live (the land of the free and the home of the brave). I ended up getting them directly from Italy, which took over a month to get, and I'm definitely not a patient person (but, surprisingly, even with global shipping, it was still cheaper to get them from Italy than from the local distributors! so hey, order them from Pegaso World directly!).</p>
<p dir="auto">I took a piece of plywood, primed it black, and painted samples of all the colors for the set. They are around 3cm x 3cm, which corresponds to the measurement area of my spectrophotometer (it's actually smaller, but it illuminates an area of around that size, so I wanted it to be uniform). I took the paint straight out of the bottle, without diluting it at all, and painted an opaque layer. Or rather, tried to paint an opaque layer. Some of these pigments are just naturally translucent, especially two yellows, and getting an opaque layer was a nightmare. It took ages to put and dry multiple thick layers of paint, and honestly, in some spots, there's still a bit of substrate peeking through. Well, we'll have to live with that. In a professional setting, paint is applied on special draw-down charts with a draw-down bar that has a precisely controlled gap. You put a blob of paint and smear it down, leaving a layer of a particular thickness (see here: <a href="https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970" rel="nofollow">https://www.byk-instruments.com/en/Physical-Properties/Paint-Application/Manual-Film-Applicators/Film-Applicator%2C-1-Gap/c/p-5970</a>). I don't really care about the precise thickness, but it would be great to have it uniform. The draw-down bar, however, is around $400 - which seems a bit excessive for a piece of metal that I'm going to use once. So I used a brush (tbh, I did order some knock-off, no-name draw-down bar from China, it was $50, that's about as much as I'm willing to spend to test it - we'll see how well it does, but it's going to be a while until it gets here).</p>
<p dir="auto">Next, I mixed every paint with white, measuring the exact ratios. I measured by weight, just because it's much easier. I have a precise scale, with resolution up to one hundredth of a gram, but measuring small volumes of thick liquid is difficult. This means that the final recipes will be given by weight, but honestly I don't think these paints differ in density too much (and all this is so hand-wavy, that even if they do, it won't make much difference in practice anyway). You could probably weigh the paint bottles from a fresh set and figure out these densities, but I didn't, and now it's too late, since I only ordered one set. I only mixed each paint with white in one ratio. This is the minimum number of mixes you need to calculate the K and S coefficients, but having more would be better. But of course, to have more of these mixes, you need more paint, and, as I said, I only ordered one set, and I would actually want to paint something with them too, not only do some experiments. Of course, the biggest problem is white, which you need a lot for all these mixes, but just pure white is also out of stock everywhere around here, and I was too impatient to wait for it to ship from Italy again.</p>
<p dir="auto">This is what it all looked like:</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/32%20-%20PXL_20240128_204751800.jpg"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/32%20-%20PXL_20240128_204751800.jpg" width="70%"></a>
</p>
<p dir="auto">Like a real scientist, I measured every patch three times and averaged the results. I then wrote a piece of Python code to do all the math for mixing K and S and calculating reflectance and then final color when illuminated by D65 from that.</p>
<p dir="auto">Here are the spectral reflectivities of two paints (green and red)</p>
<p dir="auto">  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20green.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20green.JPG"></a>  
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/33%20-%20red.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/33%20-%20red.JPG"></a>
</p>

<p dir="auto">And just like you would expect, if you mix yellow and blue (and a bit of white, blues from Kimera set are dark as hell straight out of the bottle), you get green.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/34%20-%20blue%20and%20yelllow.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/34%20-%20blue%20and%20yelllow.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Solver</h2><a id="user-content-solver" aria-label="Permalink: Solver" href="#solver"></a></p>
<p dir="auto">With all this, writing a solver that produces a given color from a set of paints is fairly straightforward. When you take some set of paints, the process of finding their relative amounts in the mix that give you color closest to some prescribed one is called optimization. We start with some mixing weights, compute the result and analyze how to change the weights to get a result somewhat closer to what we want. Rinse and repeat. This is a pretty common operation nowadays, particularly in the machine learning world. All the DALL-Es, Stable Diffusions, and other ChatGPTs go through a training phase, which is in fact a giant optimization problem. Here, we only have a couple of weights, so the problem is pretty trivial and can be quickly solved even using off-the-shelf solvers, for instance, ones from the Python scipy library.</p>
<p dir="auto">There are two details here. First is the so-called "loss function" (that's the machine learning lingo, but people call it error function, objective function, cost function, and probably a bunch of other names too). This is a function that tells us how different some intermediate result is from the target. It is usually assumed to be decreasing as two values get closer (so the process becomes "minimization" of the loss function, trying to find its minimum), but it's not always the case. It usually doesn't matter at all because even if it gets larger when the values are closer, you can always multiply it by -1. One of the most common loss functions is the L2 loss: you compute the differences between the values, square them (hence the '2' in the name; the squaring bit makes sure that if the difference becomes negative, it counts the same way as if it was positive), and if you have multiple components, you add them together. You get one number that describes some measure of the difference between two arbitrary sets of values. L2 loss has some nice properties (it differentiates to a set of linear functions, which can be solved trivially), but in general, it doesn't really represent anything in particular. Yes, a larger L2 loss means that things are more different, but it's not like when L2 loss is smaller, the results are actually better (for however you define "better"). That's why designing good loss functions in modern machine learning is an art on its own, involving combining different factors, sometimes in very creative ways. Our case is fairly simple, so I just went with L2 loss in sRGB: so after computing the color of a mix of paints, I compute the differences in the red, green, and blue channels, I square and add them, and I try to find a mix that gives me the smallest such number. It seems like the industry usually does it in Lab color space because it's more perceptually uniform: so even if the result is not perfect (if your method gets the loss to zero, it doesn't really matter what loss you use - you get the issues when your loss is not quite zero), it's "perceptually" closer to the desired color. What I noticed in practice is that if you're trying to match a color that's impossible to get with a set of base paints, the result is not good no matter which loss you use, so I don't bother and just do it in sRGB.</p>
<p dir="auto">The other issue is a bit of combinatorics in our problem. The same color can be obtained from different mixes of paints. Of course, ones that use fewer paints are generally better than the ones that use more. But sometimes using two paints can get you almost there, and while using three can get you all the way there, it doesn't really make sense to add that additional one just to fix this 1% error. And how do you even decide which actual two paints you would mix? If you have a blue and yellow paint, you can optimize for the ratios that give you something closest to the green that you want, but how would you know to mix yellow and blue in the first place? In big machine learning, there are strategies for such problems: for instance, incorporating L0 loss in your final loss function - which is the number of non-zero weights - so that solutions that have fewer weights would be preferable. Of course, then comes the issue of how much of the actual difference in color one extra paint is worth and you need to come up with some arbitrary weighting factors. Because we don't really have that many paints, we can do something simpler: just take all possible combinations of one, two, three, four paints, optimize all these mixes and find the best ones. And since there are also some other factors involved, I just decided to show all best three one, two, three, and four paint combinations. I don't filter the results in any way, so it might happen that the three paint combinations can be worse than two paint ones (I always ensure that if a paint is used for mixing, some of it, even a minuscule amount, ends up in the recipe) - when a color can be obtained with just two paints, and anything extra just makes it worse. The results are also presented just as they come out of the optimizer, with three decimal places - so you might totally get recipes like 0.8 yellow + 0.01 black. The 80:1 ratio is totally useless if you want to apply it precisely in painting, but again, it's more of a guideline that it's yellow with a tiny hint of black. Don't get too attached to these numbers, they are just some help, nothing more.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/35%20-%20single%20recipe.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/35%20-%20single%20recipe.JPG" width="70%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">The tool</h2><a id="user-content-the-tool" aria-label="Permalink: The tool" href="#the-tool"></a></p>
<p dir="auto">The tool is written in Python, with the UI done in PyQt. Most of the boilerplate code was actually generated by ChatGPT - as PyQt is something I have absolutely zero interest in learning. ChatGPT is brilliant in helping you navigate these sorts of libraries that you're not familiar with, saving tons of time. I cannot recommend it enough. It was actually able to produce setup code for the layout of the application based on a rough sketch from Paint I pasted into the conversation - how cool is that?</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/36%20-%20whole%20too.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/36%20-%20whole%20too.JPG" width="70%"></a>
</p>
<p dir="auto">The left list shows you list of all the paints in the database, with their names, colors as XYZ and sRGB. The checkboxes allow you to enable and disable individual paint for the recipe solver.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/37%20-%20base%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/37%20-%20base%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">The next list is the mixing list. You can drag paints from main list onto it to add them to the mix, or the other way round to remove them. Paints on the mixing list have a slider that controls their amount in the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/38%20-%20used%20paints.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/38%20-%20used%20paints.JPG" width="30%"></a>
</p>
<p dir="auto">Next pane shows you the spectral plot of reflectance of the mix. Each component is shown, as well as the reflectance of the mix. The top button show the color of the mix.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/39%20-%20reflectance.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/39%20-%20reflectance.JPG" width="30%"></a>
</p>
<p dir="auto">Below is the chromaticity plot with all the components and the mixed color marked as well.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/40%20-%20xy.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/40%20-%20xy.JPG" width="30%"></a>
</p>
<p dir="auto">Last, there's the recipe solver pane: you can pick a color with a picker (and PyQt picker lets you pick a screen color directly too) and then hit "solve," which will kick off the solver. It will solve for 1/2/3/4 paint mixes and add them all to the list below. Each recipe has a background of the color mixed according to the recipe, and the overall background is always the target color, so you can compare them side by side. Solve can take a while, even though it is parallelized, but the results are added to the list as they become available. You can double click on any recipe, and it will be put into the paint mixer.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/41%20-%20recipe%20list.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/41%20-%20recipe%20list.JPG" width="30%"></a>
</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">Tests and conclusions</h2><a id="user-content-tests-and-conclusions" aria-label="Permalink: Tests and conclusions" href="#tests-and-conclusions"></a></p>
<p dir="auto">So! How does all this compare to reality? Actually decently well. I haven't tested it all super thourouglhly, but in all the test I did it behaved more-less like expected. Some examples (ignore anything odd below 400nm, the measurement device is not particularly reliable there):</p>
<p dir="auto">A 1 to 1 mix of yellow oxide and magenta (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_1.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_1.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 10 mix of blue red shade and orange (white: measured, colored: predicted)</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_2.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_2.JPG" width="30%"></a>
</p>
<p dir="auto">They are actually really close! Given how sloppy all this was, that's quite encouraging!</p>
<p dir="auto">Here are some not so accurate results:</p>
<p dir="auto">A 1 to 2 to 2 mix of blue red shade, cold yellow and white. Predicted reflectance in long wavelengths is overestimated, making the predicted result brighter</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_3.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_3.JPG" width="30%"></a>
</p>
<p dir="auto">A 1 to 1 mix of cold yellow and orange - similarly, the predicted result is slightly brigher than in reality.</p>
<p dir="auto">
  <a target="_blank" rel="noopener noreferrer" href="https://github.com/miciwan/PaintMixing/blob/main/images/mix_4.JPG"><img src="https://github.com/miciwan/PaintMixing/raw/main/images/mix_4.JPG" width="30%"></a>
</p>
<p dir="auto">The main offender seems to be yellow, mostly the cold variation. I think the problem comes from it's high translucency. I tried hard to get fully covering, opaque layer, but I'm not quite sure how well it worked in the end. Interestingly other people doing similar work (for instance Yoshio Okumura here: <a href="https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses" rel="nofollow">https://repository.rit.edu/cgi/viewcontent.cgi?article=5896&amp;context=theses</a>) ran into exact same problems with the same pigments. It looks like yellows just need more love. I might try playing with it a bit more, but tbh, the difference is not that huge in practice, the hue matches more-less and I'm not after exact recipes anyway.</p>
<p dir="auto">One thing missing in the model used for mixing are so-called Saunderson correction coefficients. They account for specular reflection on the surface of the measured sample. Testing how much this would improve the accuracy is another excersise for the future.</p>
<p dir="auto">If you got to that point, congratulations! Grab the tool, play with it, and if you have any questions, shoot me an email at <a href="mailto:miciwan@gmail.com">miciwan@gmail.com</a>.</p>
<p dir="auto"><h2 tabindex="-1" dir="auto">License and image credits</h2><a id="user-content-license-and-image-credits" aria-label="Permalink: License and image credits" href="#license-and-image-credits"></a></p>
<p dir="auto">All the rambling, all the images not specifically mentioned below, all the data in the depot are licensed under Creative Commons BY 4.0 (so you can do whatever with it, including using it for commercial purposed, you just need to say where you took it from).</p>
<p dir="auto">Image of the smooth vs rough specular reflection is from "Real-Time Rendering 4th edition". The publisher allowed to use all the figures from the book for non-commercial purposes under fair use. I <em>believe</em> that this actually counts as fair use, but tbh, I'm not 100 percent sure. But I'm one of the authors of this book, so maybe the publisher will be kind enough not to sue me if this is not the case.</p>
<p dir="auto">The image of the spectral sensitivities of human cones is by Vanessaezekowitz, from en.wikipedia, <a href="https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg" rel="nofollow">https://en.wikipedia.org/wiki/Trichromacy#/media/File:Cones_SMJ2_E.svg</a> under CC BY 3.0</p>
<p dir="auto">The image of the rgb matching curved is by Marco Polo, from wikipedia, <a href="https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg" rel="nofollow">https://en.wikipedia.org/wiki/File:CIE1931_RGBCMF.svg</a> under Public Domain</p>
<p dir="auto">The image of the 1931 XYZ matching curves is by Acdx, from wikipedia, <a href="https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg" rel="nofollow">https://en.wikipedia.org/wiki/CIE_1931_color_space#/media/File:CIE_1931_XYZ_Color_Matching_Functions.svg</a> under CC BY-SA 4.0</p>
</article></div></div>]]></description>
        </item>
    </channel>
</rss>